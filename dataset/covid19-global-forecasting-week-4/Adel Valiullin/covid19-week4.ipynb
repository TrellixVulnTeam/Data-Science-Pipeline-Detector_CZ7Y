{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\n\n\n\ndef rmsle(y, y_pred):\n        assert len(y) == len(y_pred)\n        terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n        return (sum(terms_to_sum) * (1.0/len(y))) ** 0.5\n    \n\ndef fix_target(frame, key, target, new_target_name=\"target\"):\n    import numpy as np\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n        if current_group == previous_group:\n                if current_value<previous_value:\n                    current_value=previous_value\n                    target[i] =current_value\n\n\n        target[i] =max(0,target[i] )#correct negative values\n\n    frame[new_target_name] = np.array(target)\n    \n    \ndef rate(frame, key, target, new_target_name=\"rate\"):\n    import numpy as np\n\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    rate=[1.0 for k in range (len(target))]\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                if previous_value!=0.0:\n                     rate[i]=current_value/previous_value\n\n                 \n        rate[i] =max(1,rate[i] )#correct negative values\n\n    frame[new_target_name] = np.array(rate)\n    \ndef get_data_by_key(dataframe, key, key_value, fields=None):\n    mini_frame=dataframe[dataframe[key]==key_value]\n    if not fields is None:                \n        mini_frame=mini_frame[fields].values\n        \n    return mini_frame\n\ndirectory=\"/kaggle/input/covid19-global-forecasting-week-4/\"\nmodel_directory=\"/kaggle/input/modelv2-dir/model\"\ngeo_dir=None#\"/kaggle/input/covid19-forecasting-metadata/\"\nextra_stable_columns=None#[\"lat\",\"lon\",\"population\",\"area\",\"density\"] #\"continent\"\ngroup_by_columns=None #[\"Country_Region\"]#[\"continent\"]\n\nif not group_by_columns is None and \"continent\" in group_by_columns:\n    assert not geo_dir is None\n\ntrain=pd.read_csv(directory + \"train.csv\", parse_dates=[\"Date\"] , engine=\"python\")\ntest=pd.read_csv('/kaggle/input/datatestadd2/test_new.csv', parse_dates=[\"Date\"], engine=\"python\")\n\ntrain[\"key\"]=train[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\ntest[\"key\"]=test[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\nif not geo_dir is None:\n    region_metadata=pd.read_csv(geo_dir+\"region_metadata.csv\")\n    train=pd.merge(train,region_metadata, how=\"left\", left_on=[\"Province_State\",\"Country_Region\"], right_on=[\"Province_State\",\"Country_Region\"] )\n    test=pd.merge(test,region_metadata, how=\"left\", left_on=[\"Province_State\",\"Country_Region\"], right_on=[\"Province_State\",\"Country_Region\"] )\n    train.to_csv(directory + \"train_plus_geo.csv\", index=False)\n\n    \n    \ngroup_names=None\n\nif  not group_by_columns is None and len(group_by_columns)>0:\n    group_names=[]\n    for group in group_by_columns:\n        groupss=train[[\"Date\", group,target1,target2]]\n        grp=groupss.groupby([\"Date\", group], as_index=False).sum()\n        grp.columns=[\"Date\", group,group +\"_\" + target1,group +\"_\" + target2 ]\n        train=pd.merge(train,grp, how=\"left\", left_on=[\"Date\", group,group], right_on=[\"Date\", group,group] )\n        #group_names+=[group +\"_\" + target1,group +\"_\" + target2]\n        for gr in [group +\"_\" + target1,group +\"_\" + target2]:\n            rate(train, key, gr, new_target_name=\"rate_\" +gr ) \n            group_names+=[\"rate_\" +gr]\n        train.to_csv(directory + \"train_plus_groups.csv\", index=False)\n        \n        \n    \n#last day in train\nmax_train_date=train[\"Date\"].max()\nmax_test_date=test[\"Date\"].max()\nhorizon=  (max_test_date-max_train_date).days\nprint (\"horizon\", int(horizon))\nprint (\"max_train_date\", (max_train_date))\nprint (\"max_test_date\", (max_test_date))\n\n#test_new=pd.merge(test,train, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] )\n#train.to_csv(directory + \"transfomed.csv\")\n\ntarget1=\"ConfirmedCases\"\ntarget2=\"Fatalities\"\n\nkey=\"key\"\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fix_target(train, key, target1, new_target_name=target1)\nfix_target(train, key, target2, new_target_name=target2)\n\nrate(train, key, target1, new_target_name=\"rate_\" +target1 )\nrate(train, key, target2, new_target_name=\"rate_\" +target2 )\nunique_keys=train[key].unique()\nprint(len(unique_keys))\n\n\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef get_lags(rate_array, current_index, size=20):\n    lag_confirmed_rate=[-1 for k in range(size)]\n    for j in range (0, size):\n        if current_index-j>=0:\n            lag_confirmed_rate[j]=rate_array[current_index-j]\n        else :\n            break\n    return lag_confirmed_rate\n\ndef days_ago_thresold_hit(full_array, indx, thresold):\n        days_ago_confirmed_count_10=-1\n        if full_array[indx]>thresold: # if currently the count of confirmed is more than 10\n            for j in range (indx,-1,-1):\n                entered=False\n                if full_array[j]<=thresold:\n                    days_ago_confirmed_count_10=abs(j-indx)\n                    entered=True\n                    break\n                if entered==False:\n                    days_ago_confirmed_count_10=100 #this value would we don;t know it cross 0      \n        return days_ago_confirmed_count_10 \n    \n    \ndef ewma_vectorized(data, alpha):\n    sums=sum([ (alpha**(k+1))*data[k] for  k in range(len(data)) ])\n    counts=sum([ (alpha**(k+1)) for  k in range(len(data)) ])\n    return sums/counts\n\ndef generate_ma_std_window(rate_array, current_index, size=20, window=3):\n    ma_rate_confirmed=[-1 for k in range(size)]\n    std_rate_confirmed=[-1 for k in range(size)] \n    \n    for j in range (0, size):\n        if current_index-j>=0:\n            ma_rate_confirmed[j]=np.mean(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])\n            std_rate_confirmed[j]=np.std(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])           \n        else :\n            break\n    return ma_rate_confirmed, std_rate_confirmed\n\ndef generate_ewma_window(rate_array, current_index, size=20, window=3, alpha=0.05):\n    ewma_rate_confirmed=[-1 for k in range(size)]\n\n    \n    for j in range (0, size):\n        if current_index-j>=0:\n            ewma_rate_confirmed[j]=ewma_vectorized(rate_array[max(0,current_index-j-window+1 ):current_index-j+1, ], alpha)           \n        else :\n            break\n    \n    #print(ewma_rate_confirmed)\n    return ewma_rate_confirmed\n\n\ndef get_target(rate_col, indx, horizon=33, average=3, use_hard_rule=False):\n    target_values=[-1 for k in range(horizon)]\n    cou=0\n    for j in range(indx+1, indx+1+horizon):\n        if j<len(rate_col):\n            if average==1:\n                target_values[cou]=rate_col[j]\n            else :\n                if use_hard_rule and j +average <=len(rate_col) :\n                     target_values[cou]=np.mean(rate_col[j:j +average])\n                else :\n                    target_values[cou]=np.mean(rate_col[j:min(len(rate_col),j +average)])\n                   \n            cou+=1\n        else :\n            break\n    return target_values\n\n\ndef dereive_features(frame, confirmed, fatalities, rate_confirmed, rate_fatalities, \n                     horizon ,size=20, windows=[3,7], days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10], \n                    extra_data=None, groups_data=None, windows_group=[3,7], size_group=20,\n                    days_back_confimed_group=[1,10,100]):\n    targets=[]\n    if not extra_data is None:\n        assert len(extra_stable_columns)==extra_data.shape[1]\n        \n    if not groups_data is None:\n        assert len(group_names)==groups_data.shape[1]        \n        \n    names=[\"lag_confirmed_rate\" + str(k+1) for k in range (size)]\n    for day in days_back_confimed:\n        names+=[\"days_ago_confirmed_count_\" + str(day) ]\n    for window in windows:        \n        names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n        names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n        names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n        \n        \n    names+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\n    for day in days_back_fatalities:\n        names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \n    for window in windows:        \n        names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n        names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n        names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]        \n    names+=[\"confirmed_level\"]\n    names+=[\"fatalities_level\"]   \n    if not extra_data is None: \n        names+=[k for k in extra_stable_columns]\n    if not groups_data is None:  \n         for gg in range (groups_data.shape[1]):\n             names+=[\"lag_rate_group_\"+ str(gg+1) + \"_\" + str(k+1) for k in range (size_group)]    \n             for day in days_back_confimed_group:\n                names+=[\"days_ago_grooupcount_\" + str(gg+1) + \"_\" + str(day) ]             \n             for window in windows_group:        \n                names+=[\"ma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]\n                names+=[\"std_group_\" + str(gg+1)+ \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]  \n                #names+=[\"ewma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size)]  \n\n            \n    names+=[\"confirmed_plus\" + str(k+1) for k in range (horizon)]    \n    names+=[\"fatalities_plus\" + str(k+1) for k in range (horizon)]  \n    \n    #names+=[\"current_confirmed\"]\n    #names+=[\"current_fatalities\"]    \n    \n    features=[]\n    for i in range (len(confirmed)):\n        row_features=[]\n        #####################lag_confirmed_rate       \n        lag_confirmed_rate=get_lags(rate_confirmed, i, size=size)\n        row_features+=lag_confirmed_rate\n        #####################days_ago_confirmed_count_10\n        for day in days_back_confimed:\n            days_ago_confirmed_count_10=days_ago_thresold_hit(confirmed, i, day)               \n            row_features+=[days_ago_confirmed_count_10] \n        #####################ma_rate_confirmed       \n        #####################std_rate_confirmed \n        for window in windows:\n            ma3_rate_confirmed,std3_rate_confirmed= generate_ma_std_window(rate_confirmed, i, size=size, window=window)\n            row_features+= ma3_rate_confirmed   \n            row_features+= std3_rate_confirmed          \n            ewma3_rate_confirmed=generate_ewma_window(rate_confirmed, i, size=size, window=window, alpha=0.05)\n            row_features+= ewma3_rate_confirmed              \n        #####################lag_fatalities_rate   \n        lag_fatalities_rate=get_lags(rate_fatalities, i, size=size)\n        row_features+=lag_fatalities_rate\n        #####################days_ago_confirmed_count_10\n        for day in days_back_fatalities:\n            days_ago_fatalitiescount_2=days_ago_thresold_hit(fatalities, i, day)               \n            row_features+=[days_ago_fatalitiescount_2]     \n        #####################ma_rate_fatalities       \n        #####################std_rate_fatalities \n        for window in windows:        \n            ma3_rate_fatalities,std3_rate_fatalities= generate_ma_std_window(rate_fatalities, i, size=size, window=window)\n            row_features+= ma3_rate_fatalities   \n            row_features+= std3_rate_fatalities  \n            ewma3_rate_fatalities=generate_ewma_window(rate_fatalities, i, size=size, window=window, alpha=0.05)\n            row_features+= ewma3_rate_fatalities                  \n        ##################confirmed_level\n        confirmed_level=0\n        \n        \"\"\"\n        if confirmed[i]>0 and confirmed[i]<1000:\n            confirmed_level= confirmed[i]\n        else :\n            confirmed_level=2000\n        \"\"\"   \n        confirmed_level= confirmed[i]\n        row_features+=[confirmed_level]\n        ##################fatalities_is_level\n        fatalities_is_level=0\n        \"\"\"\n        if fatalities[i]>0 and fatalities[i]<100:\n            fatalities_is_level= fatalities[i]\n        else :\n            fatalities_is_level=200            \n        \"\"\"\n        fatalities_is_level= fatalities[i]\n        \n        row_features+=[fatalities_is_level] \n        \n        if not extra_data is None:    \n            row_features+=extra_data[i].tolist()\n            \n        if not groups_data is None:  \n          for gg in range (groups_data.shape[1]): \n             ## lags per group\n             this_group=groups_data[:,gg].tolist()\n             lag_group_rate=get_lags(this_group, i, size=size_group)\n             row_features+=lag_group_rate           \n             #####################days_ago_confirmed_count_10\n             for day in days_back_confimed_group:\n                days_ago_groupcount_2=days_ago_thresold_hit(this_group, i, day)               \n                row_features+=[days_ago_groupcount_2]     \n             #####################ma_rate_fatalities       \n             #####################std_rate_fatalities \n             for window in windows_group:        \n                ma3_rate_group,std3_rate_group= generate_ma_std_window(this_group, i, size=size_group, window=window)\n                row_features+= ma3_rate_group   \n                row_features+= std3_rate_group             \n            \n            \n        #######################confirmed_plus target\n        confirmed_plus=get_target(rate_confirmed, i, horizon=horizon)\n        row_features+= confirmed_plus          \n        #######################fatalities_plus target\n        fatalities_plus=get_target(rate_fatalities, i, horizon=horizon)\n        row_features+= fatalities_plus \n        ##################current_confirmed\n        #row_features+=[confirmed[i]]\n        ##################current_fatalities\n        #row_features+=[fatalities[i]]        \n        \n          \n\n        \n        features.append(row_features)\n        \n    new_frame=pd.DataFrame(data=features, columns=names).reset_index(drop=True)\n    frame=frame.reset_index(drop=True)\n    frame=pd.concat([frame, new_frame], axis=1)\n    #print(frame.shape)\n    return frame\n    \n    \ndef feature_engineering_for_single_key(frame, group, key, horizon=33, size=20, windows=[3,7], \n                                       days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10],\n                                      extra_stable_=None, group_nams=None,windows_group=[3,7], \n                                       size_group=20, days_back_confimed_group=[1,10,100]):\n    \n    mini_frame=get_data_by_key(frame, group, key, fields=None)\n    \n    mini_frame_with_features=dereive_features(mini_frame, mini_frame[\"ConfirmedCases\"].values,\n                                              mini_frame[\"Fatalities\"].values, mini_frame[\"rate_ConfirmedCases\"].values, \n                                               mini_frame[\"rate_Fatalities\"].values, horizon ,size=size, windows=windows,\n                                              days_back_confimed=days_back_confimed, days_back_fatalities=days_back_fatalities,\n                                              extra_data=mini_frame[extra_stable_].values if not extra_stable_ is None else None,\n                                              groups_data=mini_frame[group_nams].values if not group_nams is None else None,\n                                              windows_group=windows_group, size_group=size_group, \n                                              days_back_confimed_group=days_back_confimed_group)\n    #print (mini_frame_with_features.shape[0])\n    return mini_frame_with_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ntrain_frame=[]\nsize=10\nwindows=[3]\ndays_back_confimed=[1,5,10,20,50,100,250,500,1000]\ndays_back_fatalities=[1,2,5,10,20,50]\n\nsize_group=10\nwindows_group=[3,5]\ndays_back_confimed_group=[1,10,100]\n\n\n#print (len(train['key'].unique()))\nfor unique_k in tqdm(unique_keys):\n    mini_frame=feature_engineering_for_single_key(train, key, unique_k, horizon=horizon, size=size, \n                                                  windows=windows, days_back_confimed=days_back_confimed,\n                                                  days_back_fatalities=days_back_fatalities,\n                                                  extra_stable_=extra_stable_columns if extra_stable_columns is not None and len(extra_stable_columns)>0 else None,\n                                     group_nams=group_names,windows_group=windows_group, \n                                     size_group=size_group, days_back_confimed_group=days_back_confimed_group\n                                                 ).reset_index(drop=True) \n    #print (mini_frame.shape[0])\n    train_frame.append(mini_frame)\n    \ntrain_frame = pd.concat(train_frame, axis=0).reset_index(drop=True)\n#train_frame.to_csv(directory +\"all\" + \".csv\", index=False)\nnew_unique_keys=train_frame['key'].unique()\nfor kee in new_unique_keys:\n    if kee not in unique_keys:\n        print (kee , \" is not there \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.externals import joblib\n\ndef predict(xtest,input_name=None):\n   #print (type(yt))\n   # create array object to hold predictions \n  \n   baggedpred=np.array([ 0.0 for d in range(0, xtest.shape[0])]) \n   model=  joblib.load( input_name) \n   preds=model.predict(xtest)               \n   baggedpred+=preds\n\n   return baggedpred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnames=[\"lag_confirmed_rate\" + str(k+1) for k in range (size)]\nfor day in days_back_confimed:\n    names+=[\"days_ago_confirmed_count_\" + str(day) ]\nfor window in windows:        \n    names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n    names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n    names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n\n\nnames+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\nfor day in days_back_fatalities:\n    names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \nfor window in windows:        \n    names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n    names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n    names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]        \nnames+=[\"confirmed_level\"]\nnames+=[\"fatalities_level\"]   \nif not extra_stable_columns is None and len(extra_stable_columns)>0: \n    names+=[k for k in extra_stable_columns]  \n    \nif not group_names is None:  \n     for gg in range (len(group_names)):\n         names+=[\"lag_rate_group_\"+ str(gg+1) + \"_\" + str(k+1) for k in range (size_group)]    \n         for day in days_back_confimed_group:\n            names+=[\"days_ago_grooupcount_\" + str(gg+1) + \"_\" + str(day) ]             \n         for window in windows_group:        \n            names+=[\"ma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]\n            names+=[\"std_group_\" + str(gg+1)+ \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size_group)]  \n            #names+=[\"ewma_group_\" + str(gg+1) + \"_\" + str(window) + \"_rate_\" + str(k+1) for k in range (size)]      \n      \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef decay_4_first_10_then_1_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        if j<10:\n            arr[j]=1. + (max(1,array[j])-1.)/4.\n        else :\n            arr[j]=1.\n    return arr\n\t\ndef decay_16_first_10_then_1_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        if j<10:\n            arr[j]=1. + (max(1,array[j])-1.)/16.\n        else :\n            arr[j]=1.\n    return arr\t\n            \ndef decay_2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/2.\n    return arr \n\ndef decay_4_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/4.\n    return arr \t\n\t\ndef acceleratorx2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)*2.\n    return arr \n\n\n\ndef decay_1_5_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)/1.5\n    return arr            \n         \n         \ndef stay_same_f(array):\n    arr=[1.0 for k in range(len(array))]      \n    for j in range(len(array)):\n        arr[j]=1.\n    return arr   \n\ndef decay_2_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)/2.\n    arr12= (max(1,arr[-12])-1.)/12. \n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\ndef decay_4_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)/4.\n    arr12= (max(1,arr[-12])-1.)/12. \n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\ndef linear_last_12_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=max(1,array[j])\n    arr12= (max(1,arr[-12])-1.)/12. \n    \n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n    \ndecay_4_first_10_then_1 =[ \"Heilongjiang_China\",\"Liaoning_China\",\"Shanghai_China\"]#, \"Hong Kong_China\"\n\ndecay_4_first_10_then_1_fatality=[]\n\ndecay_16_first_10_then_1 =[\"Beijing_China\",\"Fujian_China\",\"Guangdong_China\",\"Shandong_China\",\"Sichuan_China\",\"Zhejiang_China\"]\ndecay_16_first_10_then_1_fatality=[]\n\ndecay_4=[\"nan_Bhutan\",\"nan_Burundi\",\"nan_Cabo Verde\",\"Prince Edward Island_Canada\",\n\"nan_Central African Republic\",\"Inner Mongolia_China\",\"nan_Maldives\"]\ndecay_4_fatality=[\"nan_Congo (Kinshasa)\"]\n\ndecay_2 =[\"nan_Congo (Kinshasa)\",\"Faroe Islands_Denmark\",\"nan_Eritrea\",\"French Guiana_France\",\"nan_Korea, South\",\"nan_MS Zaandam\"]\ndecay_2_fatality=[]\n\nstay_same=[\"nan_Diamond Princess\",\"nan_Timor-Leste\"]\n\t\t   \nstay_same_fatality=[\"Beijing_China\",\"Fujian_China\",\"Guangdong_China\",\"Shandong_China\",\n\"Sichuan_China\",\"Zhejiang_China\", \"Heilongjiang_China\",\"Liaoning_China\",\"Shanghai_China\"]#\n\nnormal=[]\nnormal_fatality=[\"nan_Korea, South\",\"New York_US\"]\n\ndecay_4_last_12_linear_inter =[ \"Greenland_Denmark\",\"nan_Dominica\",\"nan_Equatorial Guinea\",\"nan_Eswatini\",\"New Caledonia_France\",\n\"Saint Barthelemy_France\",\"St Martin_France\",\"nan_Gambia\",\"nan_Grenada\",\"nan_Holy See\",\"nan_Mauritania\",\"nan_Namibia\",\"nan_Nicaragua\"\n,\"nan_Papua New Guinea\",\"nan_Saint Lucia\",\"nan_Saint Vincent and the Grenadines\",\"nan_Seychelles\",\"nan_Sierra Leone\",\"nan_Somalia\",\"nan_Suriname\",\n\"Anguilla_United Kingdom\",\"British Virgin Islands_United Kingdom\",\"Montserrat_United Kingdom\",\"Turks and Caicos Islands_United Kingdom\",\"nan_Zimbabwe\"\n                              , \"Hong Kong_China\",\"Curacao_Netherlands\"]\n\ndecay_4_last_12_linear_inter_fatality=[]\n\ndecay_2_last_12_linear_inter =[ \"nan_Chad\",\n\"nan_Congo (Brazzaville)\",\"nan_Fiji\",\"French Polynesia_France\",\"nan_Gabon\",\n\"nan_Guyana\",\"nan_Laos\",\"nan_Nepal\",\"Sint Maarten_Netherlands\",\n\"nan_Saint Kitts and Nevis\",\"nan_Sudan\",\"nan_Syria\",\"nan_Tanzania\",\n\"Bermuda_United Kingdom\",\"Cayman Islands_United Kingdom\",\"nan_Zambia\",\"Northwest Territories_Canada\",\"Yukon_Canada\"\n,\"nan_Mongolia\",\"nan_Uganda\"]\ndecay_2_last_12_linear_inter_fatality=[]\n\nacceleratorx2=[]\nacceleratorx2_fatality=[]\n\n\nwarm_st=['nan_Angola','nan_Antigua and Barbuda','Northern Territory_Australia','nan_Bahamas',\n'nan_Bangladesh','nan_Belize','nan_Benin','nan_Botswana','nan_Burundi','nan_Cabo Verde','nan_Cameroon',\n'nan_Central African Republic','nan_Chad','Hong Kong_China',\"nan_Cote d'Ivoire\",'nan_Cuba','Greenland_Denmark',\n'nan_Dominica','nan_Equatorial Guinea','nan_Eritrea','nan_Eswatini','nan_Fiji','French Polynesia_France','New Caledonia_France',\n'Saint Barthelemy_France','St Martin_France','nan_Gabon','nan_Gambia','nan_Grenada','nan_Guyana','nan_Haiti','nan_Holy See',\n'nan_Honduras','nan_Ireland','nan_Korea, South','nan_Laos','nan_Liberia','nan_Libya','nan_Maldives','nan_Mali',\n'nan_Mauritania','nan_Mauritius','nan_Mongolia','nan_Mozambique','nan_Namibia','nan_Nepal','Aruba_Netherlands',\n'nan_Nicaragua','nan_Niger','nan_Papua New Guinea','nan_Saint Kitts and Nevis','nan_Saint Lucia',\n'nan_Saint Vincent and the Grenadines','nan_Seychelles','nan_Sierra Leone','nan_Somalia',\n'nan_Spain','nan_Sudan','nan_Suriname','nan_Syria','nan_Tanzania','nan_Togo','nan_Uganda','Anguilla_United Kingdom',\n'Bermuda_United Kingdom','British Virgin Islands_United Kingdom','Channel Islands_United Kingdom',\n'Gibraltar_United Kingdom','Isle of Man_United Kingdom','Montserrat_United Kingdom','nan_United Kingdom',\n'Turks and Caicos Islands_United Kingdom','nan_Uzbekistan','nan_Zimbabwe',\n]\n\n\ndecay_1_5 =[\"nan_Angola\" ,\"nan_Antigua and Barbuda\",\"Montana_US\",\"Nebraska_US\",\"nan_Bangladesh\",\"Illinois_US\"\n,\"Northern Territory_Australia\",\"nan_Bahamas\",\"nan_Bahrain\",\"nan_Barbados\" ,\"nan_Belize\",\"nan_Benin\",\n\t\"nan_Botswana\",\"nan_Brunei\",\"Manitoba_Canada\",\"New Brunswick_Canada\",\"Saskatchewan_Canada\",\n\t\"nan_Cote d'Ivoire\",\"nan_France\",\"nan_Guinea-Bissau\",\"nan_Haiti\",\"nan_Italy\",\"nan_Libya\",\"nan_Malta\",\"nan_Mauritius\",\n\t\"Aruba_Netherlands\",\"nan_Niger\",\"nan_Spain\",\"nan_Togo\",\"Guam_US\",\"Iowa_US\",\"Idaho_US\",\"Connecticut_US\",\"California_US\",\"New York_US\",\"Virgin Islands_US\",\n\t\"Channel Islands_United Kingdom\",\"Gibraltar_United Kingdom\",\"Isle of Man_United Kingdom\",\"nan_United Kingdom\",'nan_Burma']\n\ndecay_1_5_fatality=[\"nan_Cameroon\",\"nan_Mali\",\"nan_Cuba\",\"Delaware_US\",\"District of Columbia_US\",\n\"Kansas_US\",\"Louisiana_US\",\"Michigan_US\",\"New Mexico_US\",\"Ohio_US\",\"Oklahoma_US\",\"Pennsylvania_US\",\"Puerto Rico_US\",\"Rhode Island_US\",\n\"South Dakota_US\" ,\"Tennessee_US\",\"Texas_US\",\"Vermont_US\",\"Virginia_US\",\"West Virginia_US\",\"nan_Uzbekistan\"]\n\nlinear_last_12=[\"nan_Honduras\",\"nan_Ireland\",\"Colorado_US\",\"nan_Liberia\",\"nan_Mozambique\"]\nlinear_last_12_fatality=[]\n\n\ntr_frame=train_frame\n\nfeatures_train=tr_frame[names].values   \n\nstandard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\nstandard_fatalities_train=tr_frame[\"Fatalities\"].values\ncurrent_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n\n     \n\nfeatures_cv=[]\nname_cv=[]\nstandard_confirmed_cv=[]\nstandard_fatalities_cv=[]\nnames_=tr_frame[\"key\"].values\ntraining_horizon=int(features_train.shape[0]/len(unique_keys)) \nprint(\"training horizon = \",training_horizon)\nfor dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n    features_cv.append(features_train[dd])\n    name_cv.append(names_[dd])\n    standard_confirmed_cv.append(standard_confirmed_train[dd])\n    standard_fatalities_cv.append(standard_fatalities_train[dd])\n    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n    \n \n\nfeatures_cv=np.array(features_cv)\npreds_confirmed_cv=np.zeros((features_cv.shape[0],horizon))\npreds_confirmed_standard_cv=np.zeros((features_cv.shape[0],horizon))\n\npreds_fatalities_cv=np.zeros((features_cv.shape[0],horizon))\npreds_fatalities_standard_cv=np.zeros((features_cv.shape[0],horizon))\n\noveral_rmsle_metric_confirmed=0.0\n\nfor j in range (preds_confirmed_cv.shape[1]):\n\n    this_features_cv=features_cv                          \n\n    preds=predict(features_cv, input_name=model_directory +\"confirmed\"+ str(j%30))\n    preds_confirmed_cv[:,j]=preds\n    print (\" modelling confirmed, case %d, , original cv %d and after %d \"%(j,this_features_cv.shape[0],this_features_cv.shape[0])) \n\npredictions=[] \nfor ii in range (preds_confirmed_cv.shape[0]):\n    current_prediction=standard_confirmed_cv[ii]\n    if current_prediction==0 :\n        current_prediction=0.1   \n    this_preds=preds_confirmed_cv[ii].tolist()\n    name=name_cv[ii]\n    reserve=this_preds[0]\n    #overrides\n\n\t\n    if name in normal:\n        this_preds=this_preds\t\n\t\n    elif name in decay_4_first_10_then_1:\n        this_preds=decay_4_first_10_then_1_f(this_preds)\n\t\t\n    elif name in decay_16_first_10_then_1:\n        this_preds=decay_16_first_10_then_1_f(this_preds)\t\t\n\t\t\n    elif name in decay_4_last_12_linear_inter:\n        this_preds=decay_4_last_12_linear_inter_f(this_preds)\t\t        \n          \n    elif name in decay_4:\n        this_preds=decay_4_f(this_preds)\n\n    \n    elif name in decay_2:\n        this_preds=decay_2_f(this_preds)\n        \n    elif name in decay_2_last_12_linear_inter:\n        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n        \n    elif name in decay_1_5:\n        this_preds=decay_1_5_f(this_preds)        \n        \n    elif name in linear_last_12:\n        this_preds=linear_last_12_f(this_preds)\n        \n    elif name in acceleratorx2:\n        this_preds=acceleratorx2_f(this_preds)         \n\n        \n    elif name in stay_same or  \"China\" in name:\n        this_preds=stay_same_f(this_preds)      \n\n    if name in warm_st:\n        this_preds[0]=reserve\n    for j in range (preds_confirmed_cv.shape[1]):\n                current_prediction*=max(1,this_preds[j])\n                preds_confirmed_standard_cv[ii][j]=current_prediction\n\n\nfor j in range (preds_confirmed_cv.shape[1]):\n\n    this_features_cv=features_cv\n                             \n    preds=predict(features_cv, input_name=model_directory +\"fatal\"+ str(j%30))\n    preds_fatalities_cv[:,j]=preds\n    print (\" modelling fatalities, case %d, original cv %d and after %d \"%( j,this_features_cv.shape[0],this_features_cv.shape[0])) \n\npredictions=[]\nfor ii in range (preds_fatalities_cv.shape[0]):\n    current_prediction=standard_fatalities_cv[ii]\n    if current_prediction==0 and standard_confirmed_cv[ii]>400:\n        current_prediction=0.1\n        \n    this_preds=preds_fatalities_cv[ii].tolist()\n    name=name_cv[ii]\n    reserve=this_preds[0]\n    #overrides\n   \n    ####fatality special\n\t\n    if name in normal_fatality:\n        this_preds=this_preds\t \t\n\t\n    elif name in decay_4_first_10_then_1_fatality:\n        this_preds=decay_4_first_10_then_1_f(this_preds) \n\t\t\n    elif name in decay_16_first_10_then_1_fatality:\n        this_preds=decay_16_first_10_then_1_f(this_preds)\n        \n    elif name in decay_4_last_12_linear_inter_fatality:\n        this_preds=decay_4_last_12_linear_inter_f(this_preds)\t\t            \n\n    elif name in decay_4_fatality:\n        this_preds=decay_4_f(this_preds)\t\t\n\t\t\n    elif name in decay_2_fatality:\n        this_preds=decay_2_f(this_preds)        \n\n    elif name in decay_2_last_12_linear_inter_fatality:\n        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n        \n    elif name in decay_1_5_fatality:\n        this_preds=decay_1_5_f(this_preds)   \t\n \n    elif name in linear_last_12_fatality:\n        this_preds=linear_last_12_f(this_preds) \n         \n    elif name in acceleratorx2_fatality:\n        this_preds=acceleratorx2_f(this_preds)\n        \n    elif name in stay_same_fatality:     \n        this_preds=stay_same_f(this_preds) \n        \n     \n    ####general   \n    elif name in normal:\n        this_preds=this_preds\t   \n\t\n    elif name in decay_4_first_10_then_1:\n        this_preds=decay_4_first_10_then_1_f(this_preds)\n\t\t\n    elif name in decay_16_first_10_then_1:\n        this_preds=decay_16_first_10_then_1_f(this_preds)\t\t\n        \n    elif name in decay_4_last_12_linear_inter:\n        this_preds=decay_4_last_12_linear_inter_f(this_preds)\t\t               \n\n    elif name in decay_4:\n        this_preds=decay_4_f(this_preds)        \n\t\t\n    elif name in decay_2:\n        this_preds=decay_2_f(this_preds)\n        \n    elif name in decay_2_last_12_linear_inter:\n        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n        \n    elif name in decay_1_5:\n        this_preds=decay_1_5_f(this_preds)        \n        \n    elif name in linear_last_12:\n        this_preds=linear_last_12_f(this_preds) \n        \n    elif name in acceleratorx2:\n        this_preds=acceleratorx2_f(this_preds)                 \n        \n    elif name in stay_same or  \"China\" in name:\n        this_preds=stay_same_f(this_preds)         \n       \n    if name in warm_st:\n        this_preds[0]=reserve    \n    \n    for j in range (preds_fatalities_cv.shape[1]):\n                if current_prediction==0 and  (preds_confirmed_standard_cv[ii][j]>400 or \"Malta\" in name or \"Somalia\" in name):\n                    current_prediction=1.\n                if j==0 and \"nan_Antigua and Barbuda\" in name:\n                    current_prediction=2.\n                if j==0 and 'nan_Burma' in name:\n                    current_prediction=3.                    \n                current_prediction*=max(1,this_preds[j])\n                preds_fatalities_standard_cv[ii][j]=current_prediction","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"key_to_confirmed_rate={}\nkey_to_fatality_rate={}\nkey_to_confirmed={}\nkey_to_fatality={}\nprint(len(features_cv), len(name_cv),len(standard_confirmed_cv),len(standard_fatalities_cv)) \nprint(preds_confirmed_cv.shape,preds_confirmed_standard_cv.shape,preds_fatalities_cv.shape,preds_fatalities_standard_cv.shape) \n\nfor j in range (len(name_cv)):\n    \n    key_to_confirmed_rate[name_cv[j]]=preds_confirmed_cv[j,:].tolist()\n    #print(key_to_confirmed_rate[name_cv[j]])\n    key_to_fatality_rate[name_cv[j]]=preds_fatalities_cv[j,:].tolist()\n    key_to_confirmed[name_cv[j]]  =preds_confirmed_standard_cv[j,:].tolist()  \n    key_to_fatality[name_cv[j]]=preds_fatalities_standard_cv[j,:].tolist()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new=train[[\"Date\",\"ConfirmedCases\",\"Fatalities\",\"key\",\"rate_ConfirmedCases\",\"rate_Fatalities\"]]\n\ntest_new=pd.merge(test,train_new, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] ).reset_index(drop=True)\ntest_new","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillin_columns(frame,key_column, original_name, training_horizon, test_horizon, unique_values, key_to_values):\n    keys=frame[key_column].values\n    original_values=frame[original_name].values.tolist()\n    print(len(keys), len(original_values), training_horizon ,test_horizon,len(key_to_values))\n    \n    for j in range(unique_values):\n        current_index=(j * (training_horizon +test_horizon )) +training_horizon \n        current_key=keys[current_index]\n        values=key_to_values[current_key]\n        co=0\n        for g in range(current_index, current_index + test_horizon):\n            original_values[g]=values[co]\n            co+=1\n    \n    frame[original_name]=original_values\n \n\nall_days=int(test_new.shape[0]/len(unique_keys))\n\ntr_horizon=all_days-horizon\nprint(all_days,tr_horizon, horizon )\n\nfillin_columns(test_new,\"key\", 'ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed)    \nfillin_columns(test_new,\"key\", 'Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality)   \nsubmission=test_new[[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"]]\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['Country_Region']=='Iran']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2020-04-09 real: 4110 => row 6114\ntest[test['Country_Region']=='Iran']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[6106:6149]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#IR\n# pdIR = ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([test, submission], axis=0)\ndf.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.Country_Region == 'Russia']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsubmission.to_csv( \"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}