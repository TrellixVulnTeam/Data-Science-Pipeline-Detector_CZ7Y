{"cells":[{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport plotly\nplotly.offline.init_notebook_mode()\n\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n%matplotlib inline\n\nfrom tqdm import tqdm\n\ndef RMSLE(pred,actual):\n    return np.sqrt(np.mean(np.power((np.log(pred+1)-np.log(actual+1)),2)))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport xgboost as xgb\n\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\n\nregion_metadata = pd.read_csv(\"/kaggle/input/covid19-forecasting-metadata/region_metadata.csv\")\nregion_date_metadata = pd.read_csv(\"/kaggle/input/covid19-forecasting-metadata/region_date_metadata.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on = [\"Province_State\", \"Country_Region\", \"Date\"], how = \"left\")\ndisplay(train.head())\ntest = test[~test.Date.isin(train.Date.unique())]\ndisplay(test.head())\n\ndf = pd.concat([train, test], sort = False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df[\"geo\"] = df.Country_Region.astype(str) + \": \" + df.Province_State.astype(str)\ndf.loc[df.Province_State.isna(), \"geo\"] = df[df.Province_State.isna()].Country_Region\n\ndf.ConfirmedCases = df.groupby(\"geo\")[\"ConfirmedCases\"].cummax()\ndf.Fatalities = df.groupby(\"geo\")[\"Fatalities\"].cummax()\n\ndf = df.merge(region_metadata, on = [\"Country_Region\", \"Province_State\"])\ndf = df.merge(region_date_metadata, on = [\"Country_Region\", \"Province_State\", \"Date\"], how = \"left\")\ndf.continent = LabelEncoder().fit_transform(df.continent)\ndf.Date = pd.to_datetime(df.Date, format = \"%Y-%m-%d\")\ndf.sort_values([\"geo\", \"Date\"], inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"DAYS_SINCE_CASES = [1, 10, 50, 100, 500, 1000, 5000, 10000]\nmin_date_train = np.min(df[~df.Id.isna()].Date)\nmax_date_train = np.max(df[~df.Id.isna()].Date)\n\nmin_date_test = np.min(df[~df.ForecastId.isna()].Date)\nmax_date_test = np.max(df[~df.ForecastId.isna()].Date)\n\nn_dates_test = len(df[~df.ForecastId.isna()].Date.unique())\n\nprint(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\nprint(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))\n\n# creating lag features\nfor lag in range(1, 41):\n    df[f\"lag_{lag}_cc\"] = df.groupby(\"geo\")[\"ConfirmedCases\"].shift(lag)\n    df[f\"lag_{lag}_ft\"] = df.groupby(\"geo\")[\"Fatalities\"].shift(lag)\n    df[f\"lag_{lag}_rc\"] = df.groupby(\"geo\")[\"Recoveries\"].shift(lag)\n\nfor case in DAYS_SINCE_CASES:\n    df = df.merge(df[df.ConfirmedCases >= case].groupby(\"geo\")[\"Date\"].min().reset_index().rename(columns = {\"Date\": f\"case_{case}_date\"}), on = \"geo\", how = \"left\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def prepare_features(df, gap):\n    \n    df[\"perc_1_ac\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap}_rc\"]) / df[f\"lag_{gap}_cc\"]\n    df[\"perc_1_cc\"] = df[f\"lag_{gap}_cc\"] / df.population\n    \n    df[\"diff_1_cc\"] = df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 1}_cc\"]\n    df[\"diff_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] - df[f\"lag_{gap + 2}_cc\"]\n    df[\"diff_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] - df[f\"lag_{gap + 3}_cc\"]\n    \n    df[\"diff_1_ft\"] = df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 1}_ft\"]\n    df[\"diff_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] - df[f\"lag_{gap + 2}_ft\"]\n    df[\"diff_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] - df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"diff_123_cc\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 3}_cc\"]) / 3\n    df[\"diff_123_ft\"] = (df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 3}_ft\"]) / 3\n\n    df[\"diff_change_1_cc\"] = df.diff_1_cc / df.diff_2_cc\n    df[\"diff_change_2_cc\"] = df.diff_2_cc / df.diff_3_cc\n    \n    df[\"diff_change_1_ft\"] = df.diff_1_ft / df.diff_2_ft\n    df[\"diff_change_2_ft\"] = df.diff_2_ft / df.diff_3_ft\n\n    df[\"diff_change_12_cc\"] = (df.diff_change_1_cc + df.diff_change_2_cc) / 2\n    df[\"diff_change_12_ft\"] = (df.diff_change_1_ft + df.diff_change_2_ft) / 2\n    \n    df[\"change_1_cc\"] = df[f\"lag_{gap}_cc\"] / df[f\"lag_{gap + 1}_cc\"]\n    df[\"change_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] / df[f\"lag_{gap + 2}_cc\"]\n    df[\"change_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] / df[f\"lag_{gap + 3}_cc\"]\n\n    df[\"change_1_ft\"] = df[f\"lag_{gap}_ft\"] / df[f\"lag_{gap + 1}_ft\"]\n    df[\"change_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] / df[f\"lag_{gap + 2}_ft\"]\n    df[\"change_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] / df[f\"lag_{gap + 3}_ft\"]\n\n    df[\"change_123_cc\"] = df[f\"lag_{gap}_cc\"] / df[f\"lag_{gap + 3}_cc\"]\n    df[\"change_123_ft\"] = df[f\"lag_{gap}_ft\"] / df[f\"lag_{gap + 3}_ft\"]\n    \n    for case in DAYS_SINCE_CASES:\n        df[f\"days_since_{case}_case\"] = (df[f\"case_{case}_date\"] - df.Date).astype(\"timedelta64[D]\")\n        df.loc[df[f\"days_since_{case}_case\"] < gap, f\"days_since_{case}_case\"] = np.nan\n\n    df[\"country_flag\"] = df.Province_State.isna().astype(int)\n    df[\"density\"] = df.population / df.area\n    \n    # target variable is log of change from last known value\n    df[\"target_cc\"] = np.log1p(df.ConfirmedCases) - np.log1p(df[f\"lag_{gap}_cc\"])\n    df[\"target_ft\"] = np.log1p(df.Fatalities) - np.log1p(df[f\"lag_{gap}_ft\"])\n    \n    features = [\n        f\"lag_{gap}_cc\",\n        f\"lag_{gap}_ft\",\n        f\"lag_{gap}_rc\",\n        \"perc_1_ac\",\n        \"perc_1_cc\",\n        \"diff_1_cc\",\n        \"diff_2_cc\",\n        \"diff_3_cc\",\n        \"diff_1_ft\",\n        \"diff_2_ft\",\n        \"diff_3_ft\",\n        \"diff_123_cc\",\n        \"diff_123_ft\",\n        \"diff_change_1_cc\",\n        \"diff_change_2_cc\",\n        \"diff_change_1_ft\",\n        \"diff_change_2_ft\",\n        \"diff_change_12_cc\",\n        \"diff_change_12_ft\",\n        \"change_1_cc\",\n        \"change_2_cc\",\n        \"change_3_cc\",\n        \"change_1_ft\",\n        \"change_2_ft\",\n        \"change_3_ft\",\n        \"change_123_cc\",\n        \"change_123_ft\",\n        \"days_since_1_case\",\n        \"days_since_10_case\",\n        \"days_since_50_case\",\n        \"days_since_100_case\",\n        \"days_since_500_case\",\n        \"days_since_1000_case\",\n        \"days_since_5000_case\",\n        \"days_since_10000_case\",\n        \"country_flag\",\n        \"lat\",\n        \"lon\",\n        \"continent\",\n        \"population\",\n        \"area\",\n        \"density\",\n        \"target_cc\",\n        \"target_ft\"\n    ]\n    \n    return df[features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def build_predict_lgbm(df_train, df_test, gap):\n    \n    df_train.dropna(subset = [\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace = True)\n    \n    target_cc = df_train.target_cc\n    target_ft = df_train.target_ft\n    \n    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n    \n    df_train.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    df_test.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    \n    categorical_features = [\"continent\"]\n    \n    dtrain_cc = lgb.Dataset(df_train, label = target_cc, categorical_feature = categorical_features)\n    dtrain_ft = lgb.Dataset(df_train, label = target_ft, categorical_feature = categorical_features)\n\n    model_cc = lgb.train(LGB_PARAMS, train_set = dtrain_cc, num_boost_round = 200)\n    model_ft = lgb.train(LGB_PARAMS, train_set = dtrain_ft, num_boost_round = 200)\n    \n    # inverse transform from log of change from last known value\n    y_pred_cc = np.expm1(model_cc.predict(df_test, num_boost_round = 200) + np.log1p(test_lag_cc))\n    y_pred_ft = np.expm1(model_ft.predict(df_test, num_boost_round = 200) + np.log1p(test_lag_ft))\n    \n    return y_pred_cc, y_pred_ft, model_cc, model_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def predict_mad(df_test, gap, val = False):\n    \n    df_test[\"avg_diff_cc\"] = (df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) / 3\n    df_test[\"avg_diff_ft\"] = (df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) / 3\n\n    if val:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) / VAL_DAYS\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) / VAL_DAYS\n    else:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) / n_dates_test\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) / n_dates_test\n\n    return y_pred_cc, y_pred_ft","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"SEED = 24\n\nLGB_PARAMS = {\"objective\": \"regression\",\n              \"num_leaves\": 5,\n              \"learning_rate\": 0.013,\n              \"bagging_fraction\": 0.91,\n              \"feature_fraction\": 0.81,\n              \"reg_alpha\": 0.13,\n              \"reg_lambda\": 0.13,\n              \"metric\": \"rmse\",\n              \"seed\": SEED\n             }\nVAL_DAYS = 7\nMAD_FACTOR = 0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_train = df[~df.Id.isna()]\ndf_test_full = df[~df.ForecastId.isna()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_preds_val = []\ndf_preds_test = []\n\nfor date in df_test_full.Date.unique():\n    \n    print(\"[INFO] Date:\", date)\n    \n    # ignore date already present in train data\n    if date in df_train.Date.values:\n        df_pred_test = df_test_full.loc[df_test_full.Date == date, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].rename(columns = {\"ConfirmedCases\": \"ConfirmedCases_test\", \"Fatalities\": \"Fatalities_test\"})\n    else:\n        df_test = df_test_full[df_test_full.Date == date]\n        \n        gap = (pd.Timestamp(date) - max_date_train).days\n        \n        if gap <= VAL_DAYS:\n            val_date = max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")\n\n            df_build = df_train[df_train.Date < val_date]\n            df_val = df_train[df_train.Date == val_date]\n            \n            X_build = prepare_features(df_build, gap)\n            X_val = prepare_features(df_val, gap)\n            \n            y_val_cc_lgb, y_val_ft_lgb, _, _ = build_predict_lgbm(X_build, X_val, gap)\n            y_val_cc_mad, y_val_ft_mad = predict_mad(df_val, gap, val = True)\n            \n            df_pred_val = pd.DataFrame({\"Id\": df_val.Id.values,\n                                        \"ConfirmedCases_val_lgb\": y_val_cc_lgb,\n                                        \"Fatalities_val_lgb\": y_val_ft_lgb,\n                                        \"ConfirmedCases_val_mad\": y_val_cc_mad,\n                                        \"Fatalities_val_mad\": y_val_ft_mad,\n                                       })\n\n            df_preds_val.append(df_pred_val)\n\n        X_train = prepare_features(df_train, gap)\n        X_test = prepare_features(df_test, gap)\n\n        y_test_cc_lgb, y_test_ft_lgb, model_cc, model_ft = build_predict_lgbm(X_train, X_test, gap)\n        y_test_cc_mad, y_test_ft_mad = predict_mad(df_test, gap)\n        \n        if gap == 1:\n            model_1_cc = model_cc\n            model_1_ft = model_ft\n            features_1 = X_train.columns.values\n        elif gap == 14:\n            model_14_cc = model_cc\n            model_14_ft = model_ft\n            features_14 = X_train.columns.values\n        elif gap == 28:\n            model_28_cc = model_cc\n            model_28_ft = model_ft\n            features_28 = X_train.columns.values\n\n        df_pred_test = pd.DataFrame({\"ForecastId\": df_test.ForecastId.values,\n                                     \"ConfirmedCases_test_lgb\": y_test_cc_lgb,\n                                     \"Fatalities_test_lgb\": y_test_ft_lgb,\n                                     \"ConfirmedCases_test_mad\": y_test_cc_mad,\n                                     \"Fatalities_test_mad\": y_test_ft_mad,\n                                    })\n    \n    df_preds_test.append(df_pred_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df.merge(pd.concat(df_preds_val, sort = False), on = \"Id\", how = \"left\")\ndf = df.merge(pd.concat(df_preds_test, sort = False), on = \"ForecastId\", how = \"left\")\n\nrmsle_cc_lgb = np.sqrt(mean_squared_error(np.log1p(df[~df.ConfirmedCases_val_lgb.isna()].ConfirmedCases), np.log1p(df[~df.ConfirmedCases_val_lgb.isna()].ConfirmedCases_val_lgb)))\nrmsle_ft_lgb = np.sqrt(mean_squared_error(np.log1p(df[~df.Fatalities_val_lgb.isna()].Fatalities), np.log1p(df[~df.Fatalities_val_lgb.isna()].Fatalities_val_lgb)))\n\nrmsle_cc_mad = np.sqrt(mean_squared_error(np.log1p(df[~df.ConfirmedCases_val_mad.isna()].ConfirmedCases), np.log1p(df[~df.ConfirmedCases_val_mad.isna()].ConfirmedCases_val_mad)))\nrmsle_ft_mad = np.sqrt(mean_squared_error(np.log1p(df[~df.Fatalities_val_mad.isna()].Fatalities), np.log1p(df[~df.Fatalities_val_mad.isna()].Fatalities_val_mad)))\n\nprint(\"LGB CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_lgb, 2))\nprint(\"LGB FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_lgb, 2))\nprint(\"LGB Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_lgb + rmsle_ft_lgb) / 2, 2))\nprint(\"MAD CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_mad, 2))\nprint(\"MAD FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_mad, 2))\nprint(\"MAD Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_mad + rmsle_ft_mad) / 2, 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test = df.loc[~df.ForecastId.isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n                                                     \"ConfirmedCases_test\", \"ConfirmedCases_test_lgb\", \"ConfirmedCases_test_mad\",\n                                                     \"Fatalities_test\", \"Fatalities_test_lgb\", \"Fatalities_test_mad\"]].reset_index()\n\ntest[\"ConfirmedCases\"] = 0.3 * test.ConfirmedCases_test_lgb + 0.7 * test.ConfirmedCases_test_mad\ntest[\"Fatalities\"] = 0.25 * test.Fatalities_test_lgb + 0.75 * test.Fatalities_test_mad\n\n# Since LGB models don't predict these countries well\ntest.loc[test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"]), \"ConfirmedCases\"] = test[test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"])].ConfirmedCases_test_mad.values\ntest.loc[test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"]), \"Fatalities\"] = test[test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"])].Fatalities_test_mad.values\n\ntest.loc[test.Date.isin(df_train.Date.values), \"ConfirmedCases\"] = test[test.Date.isin(df_train.Date.values)].ConfirmedCases_test.values\ntest.loc[test.Date.isin(df_train.Date.values), \"Fatalities\"] = test[test.Date.isin(df_train.Date.values)].Fatalities_test.values\n\nsub0 = test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\nsub0.ForecastId = sub0.ForecastId.astype(int)\n\nsub0.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train, test = train_ts, test_ts\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"display(train.head())\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ntrain['Province_State'].fillna('', inplace=True)\ntest['Province_State'].fillna('', inplace=True)\ntrain['Date'] =  pd.to_datetime(train['Date'])\ntest['Date'] =  pd.to_datetime(test['Date'])\ntrain = train.sort_values(['Country_Region','Province_State','Date'])\ntest = test.sort_values(['Country_Region','Province_State','Date'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"FirstDate = train.groupby('Country_Region').min()['Date'].unique()[0]\ntrain['Last Confirm'] = train['ConfirmedCases'].shift(1)\nwhile train[(train['Last Confirm'] > train['ConfirmedCases']) & (train['Date'] > FirstDate)].shape[0] > 0:\n    train['Last Confirm'] = train['ConfirmedCases'].shift(1)\n    train['Last Fatalities'] = train['Fatalities'].shift(1)\n    train.loc[(train['Last Confirm'] > train['ConfirmedCases']) & (train['Date'] > FirstDate),'ConfirmedCases'] = train.loc[(train['Last Confirm'] > train['ConfirmedCases']) & (train['Date'] > FirstDate),'Last Confirm']\n    train.loc[(train['Last Fatalities'] > train['Fatalities']) & (train['Date'] > FirstDate),'Fatalities'] = train.loc[(train['Last Fatalities'] > train['Fatalities']) & (train['Date'] > FirstDate),'Last Fatalities']\n    train['Last Confirm'] = train['ConfirmedCases'].shift(1)\n    train['Last Fatalities'] = train['Fatalities'].shift(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_model import ARIMA","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"feature_day = [1,20,50,100,200,500,1000]\ndef CreateInput(data):\n    feature = []\n    for day in feature_day:\n        #Get information in train data\n        data.loc[:,'Number day from ' + str(day) + ' case'] = 0\n        if (train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['ConfirmedCases'] < day)]['Date'].count() > 0):\n            fromday = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['ConfirmedCases'] < day)]['Date'].max()        \n        else:\n            fromday = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].min()       \n        for i in range(0, len(data)):\n            if (data['Date'].iloc[i] > fromday):\n                day_denta = data['Date'].iloc[i] - fromday\n                data['Number day from ' + str(day) + ' case'].iloc[i] = day_denta.days \n        feature = feature + ['Number day from ' + str(day) + ' case']\n    \n    return data[feature]\npred_data_all = pd.DataFrame()\nwith tqdm(total=len(train['Country_Region'].unique())) as pbar:\n    for country in train['Country_Region'].unique():\n    #for country in ['Vietnam']:\n        for province in train[(train['Country_Region'] == country)]['Province_State'].unique():\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\"ignore\")\n                df_train = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]\n                df_test = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n                X_train = CreateInput(df_train)\n                y_train_confirmed = df_train['ConfirmedCases'].ravel()\n                y_train_fatalities = df_train['Fatalities'].ravel()\n                X_pred = CreateInput(df_test)\n\n                feature_use = X_pred.columns[0]\n                for i in range(X_pred.shape[1] - 1,0,-1):\n                    if (X_pred.iloc[0,i] > 0):\n                        feature_use = X_pred.columns[i]\n                        break\n                idx = X_train[X_train[feature_use] == 0].shape[0] \n\n                adjusted_X_train = X_train[idx:][feature_use].values.reshape(-1, 1)\n                adjusted_y_train_confirmed = y_train_confirmed[idx:]\n                adjusted_y_train_fatalities = y_train_fatalities[idx:] \n                idx = X_pred[X_pred[feature_use] == 0].shape[0]    \n                adjusted_X_pred = X_pred[idx:][feature_use].values.reshape(-1, 1)\n\n                pred_data = test[(test['Country_Region'] == country) & (test['Province_State'] == province)]\n                max_train_date = train[(train['Country_Region'] == country) & (train['Province_State'] == province)]['Date'].max()\n                min_test_date = pred_data['Date'].min()\n                model = SARIMAX(adjusted_y_train_confirmed, order=(1,1,0), \n                                measurement_error=True).fit(disp=False)\n                y_hat_confirmed = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                y_train_confirmed = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['ConfirmedCases'].values\n                y_hat_confirmed = np.concatenate((y_train_confirmed,y_hat_confirmed), axis = 0)\n                model = SARIMAX(adjusted_y_train_fatalities, order=(1,1,0), \n                                measurement_error=True).fit(disp=False)\n                y_hat_fatalities = model.forecast(pred_data[pred_data['Date'] > max_train_date].shape[0])\n                y_train_fatalities = train[(train['Country_Region'] == country) & (train['Province_State'] == province) & (train['Date'] >=  min_test_date)]['Fatalities'].values\n                y_hat_fatalities = np.concatenate((y_train_fatalities,y_hat_fatalities), axis = 0)\n                pred_data['ConfirmedCases_hat'] =  y_hat_confirmed\n                pred_data['Fatalities_hat'] = y_hat_fatalities\n                pred_data_all = pred_data_all.append(pred_data)\n        pbar.update(1)\ndf_val = pd.merge(pred_data_all,train[['Date','Country_Region','Province_State','ConfirmedCases','Fatalities']],on=['Date','Country_Region','Province_State'], how='left')\ndf_val.loc[df_val['Fatalities_hat'] < 0,'Fatalities_hat'] = 0\ndf_val.loc[df_val['ConfirmedCases_hat'] < 0,'ConfirmedCases_hat'] = 0\ndf_val_3 = df_val.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub1 = df_val_3\nsubmission = sub1[['ForecastId','ConfirmedCases_hat','Fatalities_hat']]\nsubmission.columns = ['ForecastId','ConfirmedCases','Fatalities']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\nsub_df = sub0.copy()\nfor t in TARGETS:\n    sub_df[t] = np.expm1(np.log1p(submission[t].values)*0.4 + np.log1p(sub0[t].values)*0.6)\n\nsub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# sub0.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub0.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}