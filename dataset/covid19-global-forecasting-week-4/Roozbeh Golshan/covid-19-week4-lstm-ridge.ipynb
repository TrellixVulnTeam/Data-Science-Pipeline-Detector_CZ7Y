{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\nfrom scipy import integrate, optimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge, RidgeCV\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":true},"cell_type":"code","source":"counntries_df = pd.read_csv(\"/kaggle/input/countries-data/enriched_covid_19.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()\n#train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ConfirmedCases']=train_df['ConfirmedCases'].replace([-1], 0)\ntrain_df['Fatalities']=train_df['Fatalities'].replace([-1], 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df['ConfirmedCases']==-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train_df[\"key\"]=train_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\ntest_df[\"key\"]=test_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n\n#test_new=pd.merge(test,train, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] )\n#train.to_csv(directory + \"transfomed.csv\")\n\ntarget1=\"ConfirmedCases\"\ntarget2=\"Fatalities\"\nkey=\"key\"\ndef rate(frame, key, target, new_target_name=\"rate\"):\n   \n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    rate=[1.0 for k in range (len(target))]\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                if previous_value!=0.0:\n                     rate[i]=current_value/previous_value\n\n                 \n        rate[i] =max(1,rate[i] )#correct negative values\n\n    frame[new_target_name] = np.array(rate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"all_data_train_test=train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"all_data_train_test['mean_rate_case_last7']=1.00000000\nall_data_train_test['mean_rate_case_last3']=1.00000000\nall_data_train_test['mean_rate_case_each7']=1.00000000\nall_data_train_test['mean_rate_fat_last7']=1.00000000\nall_data_train_test['mean_rate_fat_last3']=1.00000000\nall_data_train_test['mean_rate_fat_each7']=1.00000000\nall_data_train_test['max_rate_case']=1.00000000\nall_data_train_test['min_rate_case']=1.00000000\nall_data_train_test['std_rate_case']=1.00000000\nall_data_train_test['mode_rate_case']=1.00000000\nall_data_train_test['range_rate_case']=1.00000000\nall_data_train_test['max_to_min_rate_case']=1.00000000\nall_data_train_test['max_rate_fat']=1.00000000\nall_data_train_test['min_rate_fat']=1.00000000\nall_data_train_test['std_rate_fat']=1.00000000\nall_data_train_test['mode_rate_fat']=1.00000000\nall_data_train_test['mean_rate_case']=1.00000000\nall_data_train_test['mean_rate_fat']=1.00000000\nall_data_train_test['range_rate_fat']=1.00000000       \nall_data_train_test['max_to_min_rate_fat']=1.00000000   \nall_data_train_test['cases_prev']=0  \nall_data_train_test['fat_prev']=0   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"rate(all_data_train_test, key, target1, new_target_name=\"rate_\" +target1)\nrate(all_data_train_test, key, target2, new_target_name=\"rate_\" +target2)\nall_data_train_test.head()\nall_data_train_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def mean_rate_each(df, country, col,tar):\n    target1=df[col][df['key']==country].values.tolist()\n    mean_rate_case_each7=[1.0 for k in range (len(target1))]\n    j=6\n    for i in range(1, len(target1) - 1):\n        if (i+j)<=(len(target1)):\n            \n            current_values1=[target1[i+j - 1],target1[i+j-2],target1[i+j -3],target1[i+j -4],target1[i+j -5],target1[i+j -6],target1[i+j -7]]\n           \n            mean_rate_case_each7[i+j-1]=np.mean(current_values1)\n        else:\n            break\n                                         \n    df[tar][df['key']==country]=np.array(mean_rate_case_each7)\n            \n    \n   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"group_keys = all_data_train_test['key'].values.tolist()\nfor coc in group_keys:\n    mean_rate_each(all_data_train_test, coc, col='rate_ConfirmedCases',tar='mean_rate_case_each7')\n    mean_rate_each(all_data_train_test, coc, col='rate_Fatalities',tar='mean_rate_fat_each7')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"all_data_train_test['Province_State'].fillna('NONE', inplace=True)\nall_data_train_test['ConfirmedCases'].fillna(0, inplace=True)\nall_data_train_test['Fatalities'].fillna(0, inplace=True)\nall_data_train_test['Id'].fillna(-1, inplace=True)\n#all_data_train_test['ForecastId'].fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def infection_from_first_confirmed_cases(train_df,country):\n    \n    confirmed_cases = train_df[(train_df['Country_Region']==country_dict[country]) & train_df['ConfirmedCases']!=0].groupby(['Date']).agg({'ConfirmedCases':['sum']})\n    fatalities_cases = train_df[(train_df['Country_Region']==country_dict[country]) & train_df['ConfirmedCases']!=0].groupby(['Date']).agg({'Fatalities':['sum']})\n    total_cases = confirmed_cases.join(fatalities_cases)\n    country_cases = [i for i in total_cases.ConfirmedCases['sum'].values]\n    country_cases_filter = country_cases[0:70] \n       \n    return country_cases_filter ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the mean foe last seven vlaues for fatalities and confirmed cases\ngroup_keys = all_data_train_test['key'].values.tolist()\nfor k in group_keys:\n    all_data_train_test['mean_rate_case_last7'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].tail(7).mean()\n    all_data_train_test['mean_rate_fat_last7'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k)&(all_data_train_test['Fatalities']!=0)].tail(7).mean()\n    all_data_train_test['mean_rate_case_last3'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].tail(3).mean()\n    all_data_train_test['mean_rate_fat_last3'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k)&(all_data_train_test['Fatalities']!=0)].tail(3).mean()\n    all_data_train_test['std_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].std()\n    all_data_train_test['max_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].max()\n    all_data_train_test['min_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].min()\n    all_data_train_test['mode_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].mode()\n    all_data_train_test['mean_rate_case'][all_data_train_test['key']==k]=all_data_train_test['rate_ConfirmedCases'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)].mean()\n    all_data_train_test['std_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].std()\n    all_data_train_test['max_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].max()\n    all_data_train_test['min_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].min()\n    all_data_train_test['mode_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].mode()\n    all_data_train_test['mean_rate_fat'][all_data_train_test['key']==k]=all_data_train_test['rate_Fatalities'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)].mean()\n   # all_data_train_test['max_to_min_rate_case'][all_data_train_test['key']==k]=(all_data_train_test['max_rate_case'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)])/(all_data_train_test['min_rate_case'][(all_data_train_test['key']==k) & (all_data_train_test['ConfirmedCases']!=0)])\n    #all_data_train_test['max_to_min_rate_fat'][all_data_train_test['key']==k]=(all_data_train_test['max_rate_fat'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)])/(all_data_train_test['min_rate_fat'][(all_data_train_test['key']==k) & (all_data_train_test['Fatalities']!=0)])\n    \n#all_data_train_test['cases_prev']=all_data_train_test['ConfirmedCases'].shift()\n#all_data_train_test['fat_prev']=all_data_train_test['Fatalities'].shift()                   \n\nall_data_train_test['max_to_min_rate_case']=all_data_train_test['max_rate_case']/all_data_train_test['min_rate_case']\nall_data_train_test['max_to_min_rate_fat']=all_data_train_test['max_rate_fat']/all_data_train_test['min_rate_fat']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data_train_test['mean_rate_case_last7']=(all_data_train_test[(all_data_train_test['rate_ConfirmedCases']!=0) & (all_data_train_test['ConfirmedCases']!=0)].groupby(['key'])).tail(7).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"all_data_train_test['ConfirmedCases'][all_data_train_test['ConfirmedCases']==-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data_train_test['cases_prev'] = all_data_train_test.groupby(\"key\")[\"ConfirmedCases\"].shift()\nall_data_train_test['fat_prev'] = all_data_train_test.groupby(\"key\")[\"Fatalities\"].shift()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for lag in range(1, ((train_df['Date'].max()-train_df['Date'].min()).days)):            \n    all_data_train_test[f\"lag_{lag}_cc\"] = all_data_train_test.groupby(\"key\")[\"ConfirmedCases\"].shift(lag)\n    all_data_train_test[f\"lag_{lag}_ft\"] = all_data_train_test.groupby(\"key\")[\"Fatalities\"].shift(lag)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"all_data_train_test[['mean_rate_case_last7','ConfirmedCases','rate_ConfirmedCases']][all_data_train_test['Country_Region']=='Qatar']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nall_data_train_test[all_data_train_test['Country_Region']=='Qatar'].tail(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"counntries_df.rename(columns={'Country/Region':'Country_Region','Province/State':'Province_State'},inplace = True)\ncounntries_df[\"key\"]=counntries_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"counntries_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"contries_data_columns=['Province_State', 'Country_Region', 'Lat', 'Long', 'Date',\n       'ConfirmedCases', 'Fatalities', 'age_0-4', 'age_5-9', 'age_10-14','key',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"grouped_countries=counntries_df.groupby(['Country_Region'], as_index=False).agg({\"Lat\": \"max\",\"Long\": \"max\",'total_pop': \"max\",\n                                                                                         'smokers_perc': \"max\",'density': \"max\",'urbanpop': \"max\",\n                                                                                         'hospibed': \"max\",'lung': \"max\",'femalelung': \"max\",\n                                                                                         'malelung': \"max\",'restrictions': \"max\",'quarantine': \"max\",\n                                                                                         'schools': \"max\",'age_0-4': \"max\",'age_100+': \"max\",'age_5-9': \"max\",\n                                                                                         'age_95-99': \"max\",'age_90-94': \"max\",'age_85-89': \"max\",\n                                                                                         'age_80-84': \"max\",'age_75-79': \"max\",'age_70-74': \"max\",\n                                                                                         'age_65-69': \"max\",'age_60-64': \"max\",'age_55-59': \"max\",\n                                                                                         'age_50-54': \"max\",'age_45-49': \"max\",'age_40-44': \"max\",\n                                                                                         'age_35-39': \"max\",'age_30-34': \"max\",'age_25-29': \"max\",\n                                                                                         'age_20-24': \"max\",'age_15-19': \"max\",'age_10-14': \"max\",\n                                                                                         })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_countries=counntries_df.groupby(['key'], as_index=False).agg({\"Lat\": \"max\",\"Long\": \"max\",'total_pop': \"max\",\n                                                                                         'smokers_perc': \"max\",'density': \"max\",'urbanpop': \"max\",\n                                                                                         'hospibed': \"max\",'lung': \"max\",'femalelung': \"max\",\n                                                                                         'malelung': \"max\",'restrictions': \"max\",'quarantine': \"max\",\n                                                                                         'schools': \"max\",'age_0-4': \"max\",'age_100+': \"max\",'age_5-9': \"max\",\n                                                                                         'age_95-99': \"max\",'age_90-94': \"max\",'age_85-89': \"max\",\n                                                                                         'age_80-84': \"max\",'age_75-79': \"max\",'age_70-74': \"max\",\n                                                                                         'age_65-69': \"max\",'age_60-64': \"max\",'age_55-59': \"max\",\n                                                                                         'age_50-54': \"max\",'age_45-49': \"max\",'age_40-44': \"max\",\n                                                                                         'age_35-39': \"max\",'age_30-34': \"max\",'age_25-29': \"max\",\n                                                                                         'age_20-24': \"max\",'age_15-19': \"max\",'age_10-14': \"max\",\n                                                                                         })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"merged_train_df = pd.merge(all_data_train_test,\n                 grouped_countries[['Lat', 'Long',\n        'age_0-4', 'age_5-9', 'age_10-14','key',\n       'age_15-19', 'age_20-24', 'age_25-29', 'age_30-34', 'age_35-39',\n       'age_40-44', 'age_45-49', 'age_50-54', 'age_55-59', 'age_60-64',\n       'age_65-69', 'age_70-74', 'age_75-79', 'age_80-84', 'age_85-89',\n       'age_90-94', 'age_95-99', 'age_100+', 'total_pop', 'smokers_perc',\n       'density', 'urbanpop', 'hospibed', 'lung', 'femalelung', 'malelung',\n       'restrictions', 'quarantine', 'schools']],\n                 on='key',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"merged_train_df['Date']= pd.to_datetime(merged_train_df['Date']) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_train_df['confirmed_to_pop']=merged_train_df['ConfirmedCases']/(merged_train_df['total_pop']*1000)\nmerged_train_df['fat_to_pop']=merged_train_df['Fatalities']/(merged_train_df['total_pop']*1000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"merged_train_df[['confirmed_to_pop','ConfirmedCases','total_pop']][merged_train_df['Country_Region']=='Saudi Arabia']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"scrolled":false},"cell_type":"code","source":"merged_train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"merged_train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_train_df[merged_train_df.isnull().any(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"merged_train_df['Province_State'].fillna('NONE', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"merged_train_df.fillna(0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"merged_train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"number_c = merged_train_df['Country_Region']\ncountries = (merged_train_df['Country_Region'])\ncountry_dict = dict(zip(countries, number_c)) \ncountry_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"merged_train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"merged_train_df['Date_num'] = pd.to_datetime(merged_train_df['Date'])\nmerged_train_df['Date_num'] = merged_train_df['Date_num'].dt.strftime(\"%m%d\")\nmerged_train_df['Date_num']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"merged_test_df['Date_num'] = pd.to_datetime(merged_test_df['Date'])\nmerged_test_df['Date_num'] = merged_test_df['Date_num'].dt.strftime(\"%m%d\")\nmerged_test_df['Date_num']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"merged_train_df['Date_num']=merged_train_df['Date_num'].astype('int')\nmerged_test_df['Date_num']=merged_test_df['Date_num'].astype('int')\nmerged_train_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Combined","execution_count":null},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"merged_train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"#'rate_ConfirmedCases','rate_Fatalities',\n\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmerged_train_df[['mean_rate_case_last7', 'mean_rate_case_each7','mean_rate_case_last3','mean_rate_fat_last3',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat','cases_prev', 'fat_prev',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']] = mms.fit_transform(merged_train_df[['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case','cases_prev', 'fat_prev',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat','mean_rate_case_last3','mean_rate_fat_last3',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport xgboost as xgb\n\nfrom tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm\ndef get_cpmp_sub(save_oof=False, save_public_test=False):\n    train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n    train['Province_State'].fillna('', inplace=True)\n    train['Date'] = pd.to_datetime(train['Date'])\n    train['day'] = train.Date.dt.dayofyear\n    #train = train[train.day <= 85]\n    train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n    train.fillna(0, inplace=True)\n    train['ConfirmedCases']=train['ConfirmedCases'].replace([-1], 0)\n    train['Fatalities']=train['Fatalities'].replace([-1], 0)\n    train\n\n    test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\n    test['Province_State'].fillna('', inplace=True)\n    test['Date'] = pd.to_datetime(test['Date'])\n    test['day'] = test.Date.dt.dayofyear\n    test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n    test.fillna(0, inplace=True)\n    test\n\n    day_min = train['day'].min()\n    train['day'] -= day_min\n    test['day'] -= day_min\n\n    min_test_val_day = test.day.min()\n    max_test_val_day = train.day.max()\n    max_test_day = test.day.max()\n    num_days = max_test_day + 1\n\n    min_test_val_day, max_test_val_day, num_days\n\n    train['ForecastId'] = -1\n    test['Id'] = -1\n    test['ConfirmedCases'] = 0\n    test['Fatalities'] = 0\n\n    debug = False\n\n    data = pd.concat([train,\n                      test[test.day > max_test_val_day][train.columns]\n                     ]).reset_index(drop=True)\n    if debug:\n        data = data[data['geo'] >= 'France_'].reset_index(drop=True)\n    #del train, test\n    gc.collect()\n\n    dates = data[data['geo'] == 'France_'].Date.values\n\n    if 0:\n        gr = data.groupby('geo')\n        data['ConfirmedCases'] = gr.ConfirmedCases.transform('cummax')\n        data['Fatalities'] = gr.Fatalities.transform('cummax')\n\n    geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n    num_geo = geo_data.shape[0]\n    geo_data\n\n    geo_id = {}\n    for i,g in enumerate(geo_data.index):\n        geo_id[g] = i\n\n\n    ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n    Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n\n    if debug:\n        cases = ConfirmedCases.values\n        deaths = Fatalities.values\n    else:\n        cases = np.log1p(ConfirmedCases.values)\n        deaths = np.log1p(Fatalities.values)\n\n\n    def get_dataset(start_pred, num_train, lag_period): #63,5,14\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)#(59,1,64)\n        lag_cases = np.vstack([cases[:, d - lag_period : d] for d in days])\n        lag_deaths = np.vstack([deaths[:, d - lag_period : d] for d in days])\n        target_cases = np.vstack([cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([deaths[:, d : d + 1] for d in days])\n        geo_ids = np.vstack([geo_ids_base for d in days])\n        country_ids = np.vstack([country_ids_base for d in days])\n        return lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days\n\n    def update_valid_dataset(data, pred_death, pred_case):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = cases[:, day:day+1]\n        new_target_deaths = deaths[:, day:day+1] \n        new_geo_ids = geo_ids  \n        new_country_ids = country_ids  \n        new_days = 1 + days\n        return new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, new_geo_ids, new_country_ids, new_days\n\n    def fit_eval(lr_death, lr_case, data, start_lag_death, end_lag_death, num_lag_case, fit, score):\n        lag_cases, lag_deaths, target_cases, target_deaths, geo_ids, country_ids, days = data\n\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], country_ids])\n        X_death = np.hstack([lag_deaths[:, -num_lag_case:], country_ids])\n        X_death = np.hstack([lag_cases[:, -start_lag_death:-end_lag_death], lag_deaths[:, -num_lag_case:], country_ids])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n            if 0:\n                keep = (y_death > 0).ravel()\n                X_death = X_death[keep]\n                y_death = y_death[keep]\n                y_death_prev = y_death_prev[keep]\n            lr_death.fit(X_death, y_death)\n        y_pred_death = lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -num_lag_case:], geo_ids])\n        X_case = lag_cases[:, -num_lag_case:]\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            lr_case.fit(X_case, y_case)\n        y_pred_case = lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        if score:\n            death_score = val_score(y_death, y_pred_death)\n            case_score = val_score(y_case, y_pred_case)\n        else:\n            death_score = 0\n            case_score = 0\n\n        return death_score, case_score, y_pred_death, y_pred_case\n\n    def train_model(train, valid, start_lag_death, end_lag_death, num_lag_case, num_val, score=True):\n        alpha = 3\n        lr_death = Ridge(alpha=alpha, fit_intercept=False)\n        lr_case = Ridge(alpha=alpha, fit_intercept=True)\n\n        (train_death_score, train_case_score, train_pred_death, train_pred_case,\n        ) = fit_eval(lr_death, lr_case, train, start_lag_death, end_lag_death, num_lag_case, fit=True, score=score)\n\n        death_scores = []\n        case_scores = []\n\n        death_pred = []\n        case_pred = []\n\n        for i in range(num_val):\n\n            (valid_death_score, valid_case_score, valid_pred_death, valid_pred_case,\n            ) = fit_eval(lr_death, lr_case, valid, start_lag_death, end_lag_death, num_lag_case, fit=False, score=score)\n\n            death_scores.append(valid_death_score)\n            case_scores.append(valid_case_score)\n            death_pred.append(valid_pred_death)\n            case_pred.append(valid_pred_case)\n\n            if 0:\n                print('val death: %0.3f' %  valid_death_score,\n                      'val case: %0.3f' %  valid_case_score,\n                      'val : %0.3f' %  np.mean([valid_death_score, valid_case_score]),\n                      flush=True)\n            valid = update_valid_dataset(valid, valid_pred_death, valid_pred_case)\n\n        if score:\n            death_scores = np.sqrt(np.mean([s**2 for s in death_scores]))\n            case_scores = np.sqrt(np.mean([s**2 for s in case_scores]))\n            if 0:\n                print('train death: %0.3f' %  train_death_score,\n                      'train case: %0.3f' %  train_case_score,\n                      'val death: %0.3f' %  death_scores,\n                      'val case: %0.3f' %  case_scores,\n                      'val : %0.3f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n            else:\n                print('%0.4f' %  case_scores,\n                      ', %0.4f' %  death_scores,\n                      '= %0.4f' % ( (death_scores + case_scores) / 2),\n                      flush=True)\n        death_pred = np.hstack(death_pred)\n        case_pred = np.hstack(case_pred)\n        return death_scores, case_scores, death_pred, case_pred\n\n    countries = [g.split('_')[0] for g in geo_data.index]\n    countries = pd.factorize(countries)[0]\n\n    country_ids_base = countries.reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n    country_ids_base.shape\n\n    geo_ids_base = np.arange(num_geo).reshape((-1, 1))\n    ohe = OneHotEncoder(sparse=False)\n    geo_ids_base = 0.1 * ohe.fit_transform(geo_ids_base)\n    geo_ids_base.shape\n\n    def val_score(true, pred):\n        pred = np.log1p(np.round(np.expm1(pred) - 0.2))\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n    def val_score(true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n\n#14,6,5,14\n\n    start_lag_death, end_lag_death = 14, 6,\n    num_train = 7\n    num_lag_case = 14\n    lag_period = max(start_lag_death, num_lag_case)  #14\n\n    def get_oof(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta\n        last_train = start_val - 1\n        num_val = max_test_val_day - start_val + 1\n        print(dates[start_val], start_val, num_val)\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = train[['Date', 'Id', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[sub.day >= start_val]\n        sub = sub[['Id', 'ConfirmedCases', 'Fatalities']].copy()\n        return sub\n\n#'2020-03-27', '2020-03-24', '2020-03-21', '2020-03-18'\n#'2020-03-22', '2020-03-19', '2020-03-16', '2020-03-13'\n    if save_oof:\n        for start_val_delta, date in zip(range(3, -8, -3),\n                                  ['2020-03-22', '2020-03-19', '2020-03-16', '2020-03-13']):\n            print(date, end=' ')\n            oof = get_oof(start_val_delta)\n            oof.to_csv('../submissions/cpmp-%s.csv' % date, index=None)\n\n    def get_sub(start_val_delta=0):   \n        start_val = min_test_val_day + start_val_delta  #64\n        last_train = start_val - 1   #63\n        num_val = max_test_val_day - start_val + 1 #11\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = 14  #14\n        train_data = get_dataset(last_train, num_train, lag_period)#63,5,14\n        valid_data = get_dataset(start_val, 1, lag_period) #64,1,14\n        _, _, val_death_preds, val_case_preds = train_model(train_data, valid_data, \n                                                            start_lag_death, end_lag_death, num_lag_case, num_val)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        return sub\n        return sub\n\n\n    known_test = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']\n              ].merge(test[['geo', 'day', 'ForecastId']], how='left', on=['geo', 'day'])\n    known_test = known_test[['ForecastId', 'ConfirmedCases', 'Fatalities']][known_test.ForecastId.notnull()].copy()\n    known_test\n\n    unknow_test = test[test.day > max_test_val_day]\n    unknow_test\n\n    def get_final_sub():   \n        start_val = max_test_val_day + 1\n        last_train = start_val - 1\n        num_val = max_test_day - start_val + 1\n        print(dates[last_train], start_val, num_val)\n        num_lag_case = num_val + 3\n        train_data = get_dataset(last_train, num_train, lag_period)\n        valid_data = get_dataset(start_val, 1, lag_period)\n        (_, _, val_death_preds, val_case_preds\n        ) = train_model(train_data, valid_data, start_lag_death, end_lag_death, num_lag_case, num_val, score=False)\n\n        pred_deaths = Fatalities.iloc[:, start_val:start_val+num_val].copy()\n        pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = ConfirmedCases.iloc[:, start_val:start_val+num_val].copy()\n        pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n        print(unknow_test.shape, pred_deaths.shape, pred_cases.shape)\n\n        sub = unknow_test[['Date', 'ForecastId', 'geo', 'day']]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        #sub = sub.fillna(0)\n        sub = sub[['ForecastId', 'ConfirmedCases', 'Fatalities']]\n        sub = pd.concat([known_test, sub])\n        return sub\n\n    if save_public_test:\n        sub = get_sub()\n    else:\n        sub = get_final_sub()\n    return sub","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"merged_train_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"markdown","source":"# LGB","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime as dt, timedelta\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\ntrain_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\n\ntrain_df['cases_prev']=train_df['ConfirmedCases'].shift()\ntrain_df['fat_prev']=train_df['Fatalities'].shift() \ntrain_df[\"key\"]=train_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n\nsub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n\nsub_df[\"key\"]=sub_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n\n'''coo_d = merged_train_df.groupby(\"Country_Region\")[['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case','mean_rate_case_last3','mean_rate_fat_last3',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']].mean().reset_index()'''\n\ncoo_d = merged_train_df.groupby('key')[['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case','mean_rate_case_last3','mean_rate_fat_last3',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']].mean().reset_index()\n\nloc_group = ['key']\ndef preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n     #   df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n    #    df[\"weekend\"] = df[\"Date\"].dt.dayofweek//5\n\n        df = df.merge(coo_d, how=\"left\", on='key')\n        df[\"Lat\"] = (df[\"Lat\"] // 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] // 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\nfeatures2=['Id', 'Province_State', 'Country_Region', 'Date','key',\n       'mean_rate_case_last7', 'mean_rate_case_each7', 'mean_rate_fat_last7',\n       'mean_rate_fat_each7', 'max_rate_case', 'min_rate_case','mean_rate_case_last3','mean_rate_fat_last3',\n       'std_rate_case', 'mode_rate_case', 'range_rate_case','ConfirmedCases',\n       'Fatalities','cases_prev','fat_prev','max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat',\n       'Lat', 'Long', 'max_to_min_rate_fat', 'rate_ConfirmedCases',\n       'rate_Fatalities', 'total_pop', 'smokers_perc', 'density']\nmerged_train_df2 = pd.DataFrame(columns=features2)\n\nmerged_train_df2=merged_train_df[features2]\nsub_df = preprocess(sub_df)\nmerged_train_df2['Date']= pd.to_datetime(merged_train_df2['Date']) \n#x_train = merged_train_df2\nx_train = preprocess(train_df)\nx_test = sub_df\nx_train.loc[x_train[\"Date\"]<x_test['Date'].min(),\"split\"] = \"train\"\nx_train.loc[x_train[\"Date\"]>=x_test['Date'].min(),\"split\"] = \"test\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.shape,x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"merged_train_df2.columns[~merged_train_df2.columns.isin(sub_df.columns)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"x_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"scrolled":true},"cell_type":"code","source":"train_df['Date'].max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"scrolled":true},"cell_type":"code","source":"x_test['Date'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"features = ['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat','mean_rate_case_last3','mean_rate_fat_last3',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']\ndef train_model(df, label, base_label, features=features, **kwargs):\n    X_train = df.loc[df[\"split\"] == \"train\"][features]\n    y_train = np.log(df.loc[df[\"split\"] == \"train\"][label] + 1)\n    b_train = np.log(df.loc[df[\"split\"] == \"train\"][base_label] + 1)\n    X_test = df.loc[df[\"split\"] == \"test\"][features]\n    y_test = np.log(df.loc[df[\"split\"] == \"test\", label] + 1)\n    b_test = np.log(df.loc[df[\"split\"] == \"test\", base_label] + 1)\n    print(kwargs)\n    model = lgb.LGBMRegressor(**kwargs)\n    model.fit(X_train, y_train, init_score = b_train)\n    y_pred = model.predict(X_test)\n    print(np.sqrt(mean_squared_error(y_test, y_pred + b_test)))\n   # print(len(X_test))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"lgb_model_cases = train_model(x_train,\"ConfirmedCases\",'cases_prev',features,\n                                   max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=1000,\n                                   subsample=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(lgb_model_cases.feature_importances_,x_train[features])), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_model_fatalities = train_model(x_train, \"Fatalities\",'fat_prev',features, \n                                   max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8\n                                  )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(lgb_model_fatalities.feature_importances_,x_train[features])), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"X_train = x_train[features]\ny_train = np.log(x_train[\"ConfirmedCases\"] + 1)\nb_train = np.log(x_train[\"cases_prev\"] + 1)\ncases_model = lgb.LGBMRegressor(max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8\n                               )\ncases_model.fit(X_train, y_train, init_score = b_train)\n\nX_train = x_train[features]\ny_train = np.log(x_train[\"Fatalities\"] + 1)\nb_train = np.log(x_train[\"fat_prev\"] + 1)\nfatalities_model = lgb.LGBMRegressor(max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8)\nfatalities_model.fit(X_train, y_train, init_score = b_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"sub_df.shape,merged_train_df2.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"merged_train_df2.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"merged_train_df2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train_df['Date']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merged_train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\nsub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\ndf[\"key\"]=train_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\nsub_df[\"key\"]=sub_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\nfeatures_columns = ['Id', 'Province_State', 'Country_Region', 'Date', 'ConfirmedCases',\n       'Fatalities','mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case','mean_rate_case_last3','mean_rate_fat_last3',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']\n\ncoo_df = merged_train_df.groupby(\"key\")[['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat','mean_rate_case_last3','mean_rate_fat_last3',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']].mean().reset_index()\n   # coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\nloc_group = [\"key\"]\n   # merged_train_df['Date']= pd.to_datetime(merged_train_df['Date']) \n  #  merged_train_df[\"days\"] = (merged_train_df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n\ndef preprocess(df):\n    df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n  #  df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n    #    df[\"weekend\"] = df[\"Date\"].dt.dayofweek//5\n\n    df = df.merge(coo_df, how=\"left\", on=\"key\")\n    df[\"Lat\"] = (df[\"Lat\"] // 30).astype(np.float32).fillna(0)\n    df[\"Long\"] = (df[\"Long\"] // 60).astype(np.float32).fillna(0)\n\n    for col in loc_group:\n        df[col].fillna(\"none\", inplace=True)\n    return df\nmerged_train_df2=pd.DataFrame(columns=features_columns)\nmerged_train_df2=merged_train_df[features_columns].copy()\n    #df = merged_train_df\ndf = preprocess(df)\nsub_df = preprocess(sub_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ntest_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n\n#train_df['Date']= pd.to_datetime(train_df['Date']) \ncountry_list = train_df ['Country_Region'].unique()\nkey_list=df['key'].unique()\n#train_df['Date']= pd.to_datetime(train_df['Date']) \nscoring_dates = test_df['Date'].unique()\npred_df = pd.DataFrame(columns=train_df.columns)\nfeatures = ['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case','mean_rate_case_last3','mean_rate_fat_last3',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']\n'''for date in scoring_dates.tolist():\n    print(date)\n    new_df = base_df.loc[base_df[\"Date\"] < date].copy()\n    curr_date_df = test.loc[test[\"Date\"] == date].copy()\n    curr_date_df[\"ConfirmedCases\"] = 0\n    curr_date_df[\"Fatalities\"] = 0\n    new_df = new_df.append(curr_date_df).reset_index(drop=True)\n    new_df = generate_features(new_df)\n    new_df[features] = new_df[features]\n    predictions = cases_model.predict(new_df[features]) + np.log(new_df[\"ConfirmedCases_yesterday\"] + 1)\n    new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"ConfirmedCases_yesterday\"]))\n    predictions = fatalities_model.predict(new_df[features]) + np.log(new_df[\"Fatalities_yesterday\"] + 1)\n    new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"Fatalities_yesterday\"])\n    new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n    new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n    new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n    pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n    if date not in base_df[\"Date\"].unique():\n        base_df = base_df.append(new_df.loc[new_df[\"Date\"] == date][base_df.columns.tolist()])'''\n\n'''for date in scoring_dates.tolist():\n    print(date)\n    new_df = merged_train_df2.loc[merged_train_df2[\"Date\"] < date].copy()\n    curr_date_df = sub_df.loc[sub_df[\"Date\"] == date].copy()\n    curr_date_df[\"ConfirmedCases\"] = 0\n    curr_date_df[\"Fatalities\"] = 0\n    curr_date_df[\"cases_prev\"] = 0\n    curr_date_df[\"fat_prev\"] = 0\n    new_df = new_df.append(curr_date_df).reset_index(drop=True)\n    #new_df['Date']= pd.to_datetime(new_df['Date']) \n    #new_df = preprocess(new_df)\n    #new_df[features] = new_df[features]\n \n    predictions = cases_model.predict(new_df[features]) + np.log(new_df[\"cases_prev\"] + 1)\n    new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"cases_prev\"]))\n    predictions = fatalities_model.predict(new_df[features]) + np.log(new_df[\"fat_prev\"] + 1)\n    new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"fat_prev\"])\n    new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n    new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n    new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n    pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n    \n    if date not in train_df[\"Date\"].unique():\n                \n        train_df = train_df.append(new_df.loc[new_df[\"Date\"] == date][train_df.columns.tolist()])'''\n#####################################################        \n'''sub = []\nfor date in scoring_dates.tolist():\n    \n    for country in country_list:\n        province_list = x_train.loc[x_train['Country_Region'] == country].Province_State.unique()\n        for province in province_list:\n            new_df = merged_train_df2.loc[merged_train_df2[\"Date\"] < date].copy()\n            curr_date_df = sub_df.loc[sub_df[\"Date\"] == date].copy()\n            curr_date_df[\"ConfirmedCases\"] = 0\n            curr_date_df[\"Fatalities\"] = 0\n            new_df = new_df.append(curr_date_df).reset_index(drop=True)\n            predictions = cases_model.predict(new_df[features]) + np.log(new_df[\"cases_prev\"] + 1)\n            new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"cases_prev\"]))\n            predictions = fatalities_model.predict(new_df[features]) + np.log(new_df[\"fat_prev\"] + 1)\n            new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"fat_prev\"])\n            new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n            new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n            new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n            pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n            \n        \n            \n            X_forecastId = sub_df.loc[(sub_df['Country_Region'] == country) & (sub_df['Province_State'] == province), ['ForecastId']]\n            X_forecastId = X_forecastId.values.tolist()\n            X_forecastId = [v[0] for v in X_forecastId]\n          \nfor j in range(len(sub_df['ForecastId'])):\n       \n     dic = { 'ForecastId': X_forecastId[j], 'ConfirmedCases': pred_df['ConfirmedCases'][j], 'Fatalities': pred_df['Fatalities'][j]}\n     sub.append(dic)\n\n# %% [code]\nsubmission = pd.DataFrame(sub)\nsubmission[['ForecastId','ConfirmedCases','Fatalities']].to_csv(path_or_buf='submission.csv',index=False)   '''\n######################################################################################################################################\n'''sub = []\nfor date in scoring_dates.tolist():\n    print(date)\n    \n    for key in key_list:\n              \n        new_df = merged_train_df2.loc[merged_train_df2[\"Date\"] < date].copy()\n        curr_date_df = sub_df.loc[sub_df[\"Date\"] == date].copy()\n        curr_date_df[\"ConfirmedCases\"] = 0\n        curr_date_df[\"Fatalities\"] = 0\n        new_df = new_df.append(curr_date_df).reset_index(drop=True)\n        predictions = cases_model.predict(new_df[features]) + np.log(new_df[\"cases_prev\"] + 1)\n        new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"cases_prev\"]))\n        predictions = fatalities_model.predict(new_df[features]) + np.log(new_df[\"fat_prev\"] + 1)\n        new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"fat_prev\"])\n        new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n        new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n        new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n        pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n            \n        \n            \n        X_forecastId = sub_df.loc[(sub_df['key'] == key), ['ForecastId']]\n        X_forecastId = X_forecastId.values.tolist()\n        X_forecastId = [v[0] for v in X_forecastId]\n          \nfor j in range(len(sub_df['ForecastId'])):\n       \n     dic = { 'ForecastId': X_forecastId[j], 'ConfirmedCases': pred_df['ConfirmedCases'][j], 'Fatalities': pred_df['Fatalities'][j]}\n     sub.append(dic)\n\n# %% [code]\nsubmission = pd.DataFrame(sub)\nsubmission[['ForecastId','ConfirmedCases','Fatalities']].to_csv(path_or_buf='submission.csv',index=False) '''\n###########################################################################################################################################\nsub = []\nfor date in scoring_dates.tolist():\n    print(date)\n    start=time.time()\n    for key in key_list:\n        if date < train_df['Date'].max():\n            \n            new_df = df.loc[df[\"Date\"] < date].copy()\n            curr_date_df = sub_df.loc[sub_df[\"Date\"] == date].copy()\n            curr_date_df[\"ConfirmedCases\"] = 0\n            curr_date_df[\"Fatalities\"] = 0\n            new_df = new_df.append(curr_date_df).reset_index(drop=True)\n            new_df['cases_prev']=new_df['ConfirmedCases'].shift()\n            new_df['fat_prev']=new_df['Fatalities'].shift() \n        \n            predictions = cases_model.predict(new_df[features]) + np.log(new_df[\"cases_prev\"] + 1)\n            new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"cases_prev\"]))\n            predictions = fatalities_model.predict(new_df[features]) + np.log(new_df[\"fat_prev\"] + 1)\n            new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"fat_prev\"])\n            new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n            new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n            new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n            pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n            \n        \n       \n        else:\n            #new_df = new_df.copy()\n            curr_date_df = sub_df.loc[sub_df[\"Date\"] == date].copy()\n            curr_date_df[\"ConfirmedCases\"] = 0\n            curr_date_df[\"Fatalities\"] = 0\n            new_df = new_df.append(curr_date_df).reset_index(drop=True)\n            new_df['cases_prev']=new_df['ConfirmedCases'].shift()\n            new_df['fat_prev']=new_df['Fatalities'].shift() \n        \n            predictions = cases_model.predict(new_df[features]) + np.log(new_df[\"cases_prev\"] + 1)\n            new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"cases_prev\"]))\n            predictions = fatalities_model.predict(new_df[features]) + np.log(new_df[\"fat_prev\"] + 1)\n            new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"fat_prev\"])\n            new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n            new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n            new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n            pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n            \n    print(time.time()-start)  \n            \nX_forecastId = sub_df.loc[(sub_df['key'] == key), ['ForecastId']]\nX_forecastId = X_forecastId.values.tolist()\nX_forecastId = [v[0] for v in X_forecastId]\n            \n                     \n            \n    \n          \nfor j in range(len(sub_df['ForecastId'])):\n          \n    dic = { 'ForecastId': X_forecastId[j], 'ConfirmedCases': pred_df['ConfirmedCases'][j], 'Fatalities': pred_df['Fatalities'][j]}\n    sub.append(dic)\n\n# %% [code]\nsubmission = pd.DataFrame(sub)\nsubmission[['ForecastId','ConfirmedCases','Fatalities']].to_csv(path_or_buf='submission.csv',index=False) \n########################################################################################################################################################\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"pred_df[pred_df['Country_Region']=='Italy']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Date'].min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"scrolled":true},"cell_type":"code","source":"pred_df[pred_df[\"Country_Region\"] == \"Italy\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"scrolled":true},"cell_type":"code","source":"new_df['predicted_cases'][new_df[\"Country_Region\"] == \"Italy\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"scrolled":true},"cell_type":"code","source":"sub_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\ntest['Date']= pd.to_datetime(test['Date']) \ntest = pd.merge(test, pred_df[[\"Province_State\", \"Country_Region\", \"Date\", \"ConfirmedCases\", \"Fatalities\"]], on=[\"Province_State\", \"Country_Region\", \"Date\"], how=\"left\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"scrolled":true},"cell_type":"code","source":"new_df[new_df['Country_Region']=='Qatar']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"Predicted_data = pd.merge(test  ,\n                 test_df[['ForecastId','Province_State','Country_Region','Date']],\n                 on='ForecastId',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ndf.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"merged_train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def get_nn_sub():\n    df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\n    df['ConfirmedCases']=df['ConfirmedCases'].replace([-1], 0)\n    df['Fatalities']=df['Fatalities'].replace([-1], 0)\n    sub_df = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n    sub_df[\"key\"]=sub_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n    df[\"key\"]=df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n    features_columns = ['Id', 'Province_State', 'Country_Region', 'Date', 'ConfirmedCases',\n       'Fatalities','mean_rate_case_last7', 'mean_rate_case_each7','cases_prev', 'fat_prev',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case','mean_rate_case_last3','mean_rate_fat_last3',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']\n\n    '''coo_df = merged_train_df.groupby(\"Country_Region\")[['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat','cases_prev', 'fat_prev',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']].mean().reset_index()'''\n    \n    coo_df = merged_train_df.groupby(\"Country_Region\")[['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case','mean_rate_case_last3','mean_rate_fat_last3',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long','cases_prev', 'fat_prev',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']].mean().reset_index()\n   # coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n    loc_group = [\"Province_State\", \"Country_Region\"]\n    #loc_group = [\"key\"]\n   # merged_train_df['Date']= pd.to_datetime(merged_train_df['Date']) \n  #  merged_train_df[\"days\"] = (merged_train_df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n\n    def preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n  #  df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n    #    df[\"weekend\"] = df[\"Date\"].dt.dayofweek//5\n\n        df = df.merge(coo_df, how=\"left\", on=\"Country_Region\")\n        #df = df.merge(coo_df, how=\"left\", on=\"key\")\n        df[\"Lat\"] = (df[\"Lat\"] // 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] // 60).astype(np.float32).fillna(0)\n\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n    merged_train_df2=pd.DataFrame(columns=features_columns)\n    merged_train_df2=merged_train_df[features_columns].copy()\n    #df = merged_train_df\n    df = preprocess(df)\n    sub_df = preprocess(sub_df)\n   # coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\n   # merged_train_df['Date']= pd.to_datetime(merged_train_df['Date']) \n  #  merged_train_df[\"days\"] = (merged_train_df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n\n\n \n\n    print(df.shape)\n\n    TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n    for col in TARGETS:\n        df[col] = np.log1p(df[col])\n\n    NUM_SHIFT = 7\n#'1-smokers_perc','density'\n#2-'smokers_perc','density','mean_rate_case_last7'\n#3-'smokers_perc','density','mean_rate_case_last7','mean_rate_fat_last7'\n#4-'smokers_perc','density','mean_rate_fat_last7'\n#5-'mean_rate_case_last7','mean_rate_fat_last7'\n#6- Lat and Long\n#7-'mean_rate_case_last7', 'mean_rate_fat_last7','std_rate_case','std_rate_fat','density','Lat','Long'\n#8----'mean_rate_case_last7', 'mean_rate_fat_last7','std_rate_case', 'std_rate_fat','mean_rate_case', 'mean_rate_fat','rate_ConfirmedCases','rate_Fatalities'\n#9-'std_rate_case','std_rate_fat','mean_rate_case', 'mean_rate_fat','rate_ConfirmedCases','rate_Fatalities'\n#10--------'mean_rate_case_last7', 'mean_rate_case_each7',  'mean_rate_fat_last7', 'mean_rate_fat_each7',   'rate_ConfirmedCases', 'rate_Fatalities'\n#11    'mean_rate_case_last7', 'mean_rate_case_each7','std_rate_case','std_rate_fat', 'max_to_min_rate_case', 'max_to_min_rate_fat',       'mean_rate_fat_last7', 'mean_rate_fat_each7'\n#12-----'mean_rate_case_last7', 'mean_rate_case_each7','mean_rate_case_last3','mean_rate_fat_last3', 'max_to_min_rate_case','max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities', 'mean_rate_case', 'mean_rate_fat','std_rate_fat','std_rate_case',              'mean_rate_fat_last7', 'mean_rate_fat_each7   \n    # we have to tune\n    \n    features = ['mean_rate_case_last7', 'mean_rate_case_each7','mean_rate_case_last3','mean_rate_fat_last3', 'max_to_min_rate_case',\n                'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities',\n                'mean_rate_fat_last7', 'mean_rate_fat_each7']\n  \n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            df[\"prev_{}_{}\".format(col, s)] = df.groupby(loc_group)[col].shift(s)\n            features.append(\"prev_{}_{}\".format(col, s))\n\n    df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=NUM_SHIFT)].copy()\n\n    TEST_FIRST = sub_df[\"Date\"].min() # pd.to_datetime(\"2020-03-13\") #\n    TEST_DAYS = (df[\"Date\"].max() - TEST_FIRST).days + 1\n\n    dev_df, test_df = df[df[\"Date\"] < TEST_FIRST].copy(), df[df[\"Date\"] >= TEST_FIRST].copy()\n\n    def nn_block(input_layer, size, dropout_rate, activation):\n        out_layer = KL.Dense(size, activation=None)(input_layer)\n        #out_layer = KL.BatchNormalization()(out_layer)\n        out_layer = KL.Activation(activation)(out_layer)\n        out_layer = KL.Dropout(dropout_rate)(out_layer)\n        return out_layer\n    '''def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 256,0, \"relu\")\n        hidden_layer = nn_block(hidden_layer,128, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer,64, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer,128, 0, \"relu\")\n        hidden_layer = nn_block(hidden_layer,64, 0.1, \"relu\") #0.05\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model'''\n    '''def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 128,0, \"relu\")\n        hidden_layer = nn_block(hidden_layer,64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer,32, 0.0, \"sigmoid\")\n\n        hidden_layer = nn_block(hidden_layer,32, 0.2, \"relu\") #0.05\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model'''\n\n\n    def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 256, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 64, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 64, 0.1, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model\n    '''def get_model():\n        inp = KL.Input(shape=(len(features),))\n\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")#64\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")#32\n        hidden_layer = nn_block(hidden_layer, 32, 0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[inp], outputs=out)\n        return model'''\n\n    get_model().summary()\n\n    def get_input(df):\n        return [df[features]]\n\n    NUM_MODELS = 10\n\n\n    def train_models(df, save=False):\n        models = []\n        for i in range(NUM_MODELS):\n            model = get_model()\n            model.compile(loss=\"mean_squared_error\", optimizer=Nadam(lr=1e-4))\n            hist = model.fit(get_input(df), df[TARGETS],\n                             batch_size=2048, epochs=500, verbose=0, shuffle=True)\n            if save:\n                model.save_weights(\"model{}.h5\".format(i))\n            models.append(model)\n        return models\n\n    models = train_models(dev_df)\n\n\n    prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n    def predict_one(df, models):\n        pred = np.zeros((df.shape[0], 2))\n        for model in models:\n            pred += model.predict(get_input(df))/len(models)\n        pred = np.maximum(pred, df[prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n\n    print([mean_squared_error(dev_df[TARGETS[i]], predict_one(dev_df, models)[:, i]) for i in range(len(TARGETS))])\n\n\n    def rmse(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def evaluate(df):\n        error = 0\n        for col in TARGETS:\n            error += rmse(df[col].values, df[\"pred_{}\".format(col)].values)\n        return np.round(error/len(TARGETS), 5)\n\n\n    def predict(test_df, first_day, num_days, models, val=False):\n        temp_df = test_df.loc[test_df[\"Date\"] == first_day].copy()\n        y_pred = predict_one(temp_df, models)\n\n        for i, col in enumerate(TARGETS):\n            test_df[\"pred_{}\".format(col)] = 0\n            test_df.loc[test_df[\"Date\"] == first_day, \"pred_{}\".format(col)] = y_pred[:, i]\n\n        print(first_day, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n        if val:\n            print(evaluate(test_df[test_df[\"Date\"] == first_day]))\n\n\n        y_prevs = [None]*NUM_SHIFT\n\n        for i in range(1, NUM_SHIFT):\n            y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n\n        for d in range(1, num_days):\n            date = first_day + timedelta(days=d)\n            print(date, np.isnan(y_pred).sum(), y_pred.min(), y_pred.max())\n\n            temp_df = test_df.loc[test_df[\"Date\"] == date].copy()\n            temp_df[prev_targets] = y_pred\n            for i in range(2, NUM_SHIFT+1):\n                temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = y_prevs[i-1]\n\n            y_pred, y_prevs = predict_one(temp_df, models), [None, y_pred] + y_prevs[1:-1]\n\n\n            for i, col in enumerate(TARGETS):\n                test_df.loc[test_df[\"Date\"] == date, \"pred_{}\".format(col)] = y_pred[:, i]\n\n            if val:\n                print(evaluate(test_df[test_df[\"Date\"] == date]))\n\n        return test_df\n\n    test_df = predict(test_df, TEST_FIRST, TEST_DAYS, models, val=True)\n    print(evaluate(test_df))\n\n    for col in TARGETS:\n        test_df[col] = np.expm1(test_df[col])\n        test_df[\"pred_{}\".format(col)] = np.expm1(test_df[\"pred_{}\".format(col)])\n\n    models = train_models(df, save=True)\n\n    sub_df_public = sub_df[sub_df[\"Date\"] <= df[\"Date\"].max()].copy()\n    sub_df_private = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()].copy()\n\n    pred_cols = [\"pred_{}\".format(col) for col in TARGETS]\n    #sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + pred_cols].rename(columns={col: col[5:] for col in pred_cols}), \n    #                                    how=\"left\", on=[\"Date\"] + loc_group)\n    sub_df_public = sub_df_public.merge(test_df[[\"Date\"] + loc_group + TARGETS], how=\"left\", on=[\"Date\"] + loc_group)\n\n    SUB_FIRST = sub_df_private[\"Date\"].min()\n    SUB_DAYS = (sub_df_private[\"Date\"].max() - sub_df_private[\"Date\"].min()).days + 1\n\n    sub_df_private = df.append(sub_df_private, sort=False)\n\n    for s in range(1, NUM_SHIFT+1):\n        for col in TARGETS:\n            sub_df_private[\"prev_{}_{}\".format(col, s)] = sub_df_private.groupby(loc_group)[col].shift(s)\n\n    sub_df_private = sub_df_private[sub_df_private[\"Date\"] >= SUB_FIRST].copy()\n\n    sub_df_private = predict(sub_df_private, SUB_FIRST, SUB_DAYS, models)\n\n    for col in TARGETS:\n        sub_df_private[col] = np.expm1(sub_df_private[\"pred_{}\".format(col)])\n\n    sub_df = sub_df_public.append(sub_df_private, sort=False)\n    sub_df[\"ForecastId\"] = sub_df[\"ForecastId\"].astype(np.int16)\n\n    return sub_df[[\"ForecastId\"] + TARGETS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1 = get_cpmp_sub()\nsub1['ForecastId'] = sub1['ForecastId'].astype('int')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sub2 = get_nn_sub()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sub1.sort_values(\"ForecastId\", inplace=True)\nsub2.sort_values(\"ForecastId\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nTARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n[np.sqrt(mean_squared_error(np.log1p(sub1[t].values), np.log1p(sub2[t].values))) for t in TARGETS]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"sub_df = sub1.copy()\nfor t in TARGETS:\n    sub_df[t] = np.expm1(np.log1p(sub1[t].values)*0.00+ np.log1p(sub2[t].values)*1).astype('int')\n    \nsub_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"Predicted_data = pd.merge(sub_df ,\n                 test_df[['ForecastId','Province_State','Country_Region','Date']],\n                 on='ForecastId',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"target1=\"ConfirmedCases\"\ntarget2=\"Fatalities\"\nkey=\"Country_Region\"\ndef trend(frame, key, target, new_target_name=\"trend\"):\n   \n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    trend=[1.0 for k in range (len(target))]\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                if previous_value!=0.0:\n                     trend[i]=current_value-previous_value\n\n                 \n        trend[i] =max(1,trend[i] )#correct negative values\n\n    frame[new_target_name] = np.array(trend)\ntrend(Predicted_data, key, target1, new_target_name=\"trend_\" +target1)\ntrend(Predicted_data, key, target2, new_target_name=\"trend_\" +target2)    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"Predicted_data[Predicted_data['Country_Region']=='Qatar']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confirmed_total_date = Predicted_data.groupby(['Date']).agg({'trend_ConfirmedCases':'sum','ConfirmedCases':'sum'})\nfatalities_total_date = Predicted_data.groupby(['Date']).agg({'trend_Fatalities':'sum','Fatalities':'sum'})\nprint(confirmed_total_date)\nprint(fatalities_total_date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"confirmed_date = train_df.groupby(['Date']).agg({'ConfirmedCases':['sum']})\nfatalities_date = train_df.groupby(['Date']).agg({'Fatalities':['sum']})\nprint(confirmed_date)\nprint(fatalities_date)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"scrolled":false},"cell_type":"code","source":"train_df[train_df['Country_Region']=='Qatar'].tail(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def countryplot(train_df,country):\n    \n    confirmed_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'ConfirmedCases':['sum']})\n    fatalities_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'Fatalities':['sum']})\n    total = confirmed_total.join(fatalities_total)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))\n    total.plot(ax=ax1)\n    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=90)\n    ax1.set_title(country + \" Forecast Confirmed Cases and Fatalities \", size=13,)\n    ax1.set_ylabel(\"Number of cases\", size=13)\n    ax1.set_xlabel(\"Date\", size=13)\n    #ax1.set_xticklabels(\"Date\",rotation= 90)\n    #plt.xticks(rotation=90)\n    ax1.grid()\n    fatalities_total.plot(ax=ax2, color='red')\n    plt.grid()\n    ax2.set_title(country+\" Forecast Fatalities\", size=13)\n    ax2.set_ylabel(\"Number of cases\", size=13)\n    ax2.set_xlabel(\"Date\", size=13)\n    plt.xticks(rotation=90)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"countrylist=['Qatar','Italy','Spain','United Arab Emirates','Egypt','Germany','France','Korea, South','Turkey','US','Saudi Arabia','United Kingdom']\nfor co in countrylist:\n    countryplot(Predicted_data,co)\n    #country='Italy'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def countryplottotal(train_df):\n      \n    confirmed_total = train_df.groupby(['Date']).agg({'ConfirmedCases':['sum']})\n    fatalities_total = train_df.groupby(['Date']).agg({'Fatalities':['sum']})\n    total = confirmed_total.join(fatalities_total)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))\n    total.plot(ax=ax1)\n    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=90)\n    ax1.set_title(\"World Wide Forecast Confirmed Cases and Fatalities \", size=13,)\n    ax1.set_ylabel(\"Number of cases\", size=13)\n    ax1.set_xlabel(\"Date\", size=13)\n    #ax1.set_xticklabels(\"Date\",rotation= 90)\n    #plt.xticks(rotation=90)\n    ax1.grid()\n    fatalities_total.plot(ax=ax2, color='red')\n    plt.grid()\n    ax2.set_title(\"World Wide Forecast Fatalities\", size=13)\n    ax2.set_ylabel(\"Number of cases\", size=13)\n    ax2.set_xlabel(\"Date\", size=13)\n    plt.xticks(rotation=90)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"countryplottotal(Predicted_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"def countryplothist(train_df,country):\n    \n    confirmed_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'trend_ConfirmedCases':['sum']})\n    fatalities_total = train_df[train_df['Country_Region']==country_dict[country]].groupby(['Date']).agg({'trend_Fatalities':['sum']})\n    total = confirmed_total.join(fatalities_total)\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17,7))\n    total.plot(kind='bar',ax=ax1)\n    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=90)\n    ax1.set_title(country + \" Forecast Confirmed Cases and Fatalities \", size=13,)\n    ax1.set_ylabel(\"Number of cases\", size=13)\n    ax1.set_xlabel(\"Date\", size=13)\n    #ax1.set_xticklabels(\"Date\",rotation= 90)\n    #plt.xticks(rotation=90)\n    ax1.grid()\n    fatalities_total.plot(kind='bar',ax=ax2, color='red')\n    plt.grid()\n    ax2.set_title(country+\" Forecast Fatalities\", size=13)\n    ax2.set_ylabel(\"Number of cases\", size=13)\n    ax2.set_xlabel(\"Date\", size=13)\n    plt.xticks(rotation=90)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"countrylist=['Qatar','Italy','Spain','United Arab Emirates','Egypt','Germany','France','Korea, South','Turkey','US','Saudi Arabia','United Kingdom']\nfor co in countrylist:\n    countryplothist(Predicted_data,co)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\nsubmission = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/submission.csv\")\n\ntrain_df['ConfirmedCases']=train_df['ConfirmedCases'].replace([-1], 0)\ntrain_df['Fatalities']=train_df['Fatalities'].replace([-1], 0)\n\ntrain_df[\"key\"]=train_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n#train_df['cases_prev']=train_df.groupby(\"key\")[\"ConfirmedCases\"].shift()\n#train_df['fat_prev']=train_df.groupby(\"key\")[\"Fatalities\"].shift()\n\ntrain_df['cases_prev'] = train_df.groupby(\"key\")[\"ConfirmedCases\"].shift()\ntrain_df['fat_prev'] = train_df.groupby(\"key\")[\"Fatalities\"].shift()\n\nsub_df = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\n\nsub_df[\"key\"]=sub_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n\ncoo_d = merged_train_df.groupby('key')[['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case','mean_rate_case_last3','mean_rate_fat_last3',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']].mean().reset_index()\nloc_group = ['key']\ndef preprocess(df):\n        df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n     #   df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n    #    df[\"weekend\"] = df[\"Date\"].dt.dayofweek//5\n\n        df = df.merge(coo_d, how=\"left\", on='key')\n        df[\"Lat\"] = (df[\"Lat\"] // 30).astype(np.float32).fillna(0)\n        df[\"Long\"] = (df[\"Long\"] // 60).astype(np.float32).fillna(0)\n        key_dummies = pd.get_dummies(df['key'])\n        df = pd.concat([df,key_dummies],axis=1)\n        for col in loc_group:\n            df[col].fillna(\"none\", inplace=True)\n        return df\n\n\n#x_train = merged_train_df2\nsub_df=preprocess(sub_df)\nx_train = preprocess(train_df)\nx_test = sub_df\nx_train.loc[x_train[\"Date\"]<x_test['Date'].min(),\"split\"] = \"train\"\nx_train.loc[x_train[\"Date\"]>=x_test['Date'].min(),\"split\"] = \"test\"       \n\nfeatureslist=x_train.columns.unique().tolist()\nfeatures=['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case','Alabama_US',\n 'Alaska_US',\n 'Alberta_Canada',\n 'Anguilla_United Kingdom',\n 'Anhui_China',\n 'Arizona_US',\n 'Arkansas_US',\n 'Aruba_Netherlands',\n 'Australian Capital Territory_Australia',\n 'Beijing_China',\n 'Bermuda_United Kingdom',\n 'Bonaire, Sint Eustatius and Saba_Netherlands',\n 'British Columbia_Canada',\n 'British Virgin Islands_United Kingdom',\n 'California_US',\n 'Cayman Islands_United Kingdom',\n 'Channel Islands_United Kingdom',\n 'Chongqing_China',\n 'Colorado_US',\n 'Connecticut_US',\n 'Curacao_Netherlands',\n 'Delaware_US',\n 'District of Columbia_US',\n 'Falkland Islands (Malvinas)_United Kingdom',\n 'Faroe Islands_Denmark',\n 'Florida_US',\n 'French Guiana_France',\n 'French Polynesia_France',\n 'Fujian_China',\n 'Gansu_China',\n 'Georgia_US',\n 'Gibraltar_United Kingdom',\n 'Greenland_Denmark',\n 'Guadeloupe_France',\n 'Guam_US',\n 'Guangdong_China',\n 'Guangxi_China',\n 'Guizhou_China',\n 'Hainan_China',\n 'Hawaii_US',\n 'Hebei_China',\n 'Heilongjiang_China',\n 'Henan_China',\n 'Hong Kong_China',\n 'Hubei_China',\n 'Hunan_China',\n 'Idaho_US',\n 'Illinois_US',\n 'Indiana_US',\n 'Inner Mongolia_China',\n 'Iowa_US',\n 'Isle of Man_United Kingdom',\n 'Jiangsu_China',\n 'Jiangxi_China',\n 'Jilin_China',\n 'Kansas_US',\n 'Kentucky_US',\n 'Liaoning_China',\n 'Louisiana_US',\n 'Macau_China',\n 'Maine_US',\n 'Manitoba_Canada',\n 'Martinique_France',\n 'Maryland_US',\n 'Massachusetts_US',\n 'Mayotte_France',\n 'Michigan_US',\n 'Minnesota_US',\n 'Mississippi_US',\n 'Missouri_US',\n 'Montana_US',\n 'Montserrat_United Kingdom',\n 'Nebraska_US',\n 'Nevada_US',\n 'New Brunswick_Canada',\n 'New Caledonia_France',\n 'New Hampshire_US',\n 'New Jersey_US',\n 'New Mexico_US',\n 'New South Wales_Australia',\n 'New York_US',\n 'Newfoundland and Labrador_Canada',\n 'Ningxia_China',\n 'North Carolina_US',\n 'North Dakota_US',\n 'Northern Territory_Australia',\n 'Northwest Territories_Canada',\n 'Nova Scotia_Canada',\n 'Ohio_US',\n 'Oklahoma_US',\n 'Ontario_Canada',\n 'Oregon_US',\n 'Pennsylvania_US',\n 'Prince Edward Island_Canada',\n 'Puerto Rico_US',\n 'Qinghai_China',\n 'Quebec_Canada',\n 'Queensland_Australia',\n 'Reunion_France',\n 'Rhode Island_US',\n 'Saint Barthelemy_France',\n 'Saint Pierre and Miquelon_France',\n 'Saskatchewan_Canada',\n 'Shaanxi_China',\n 'Shandong_China',\n 'Shanghai_China',\n 'Shanxi_China',\n 'Sichuan_China',\n 'Sint Maarten_Netherlands',\n 'South Australia_Australia',\n 'South Carolina_US',\n 'South Dakota_US',\n 'St Martin_France',\n 'Tasmania_Australia',\n 'Tennessee_US',\n 'Texas_US',\n 'Tianjin_China',\n 'Tibet_China',\n 'Turks and Caicos Islands_United Kingdom',\n 'Utah_US',\n 'Vermont_US',\n 'Victoria_Australia',\n 'Virgin Islands_US',\n 'Virginia_US',\n 'Washington_US',\n 'West Virginia_US',\n 'Western Australia_Australia',\n 'Wisconsin_US',\n 'Wyoming_US',\n 'Xinjiang_China',\n 'Yukon_Canada',\n 'Yunnan_China',\n 'Zhejiang_China',\n 'nan_Afghanistan',\n 'nan_Albania',\n 'nan_Algeria',\n 'nan_Andorra',\n 'nan_Angola',\n 'nan_Antigua and Barbuda',\n 'nan_Argentina',\n 'nan_Armenia',\n 'nan_Austria',\n 'nan_Azerbaijan',\n 'nan_Bahamas',\n 'nan_Bahrain',\n 'nan_Bangladesh',\n 'nan_Barbados',\n 'nan_Belarus',\n 'nan_Belgium',\n 'nan_Belize',\n 'nan_Benin',\n 'nan_Bhutan',\n 'nan_Bolivia',\n 'nan_Bosnia and Herzegovina',\n 'nan_Botswana',\n 'nan_Brazil',\n 'nan_Brunei',\n 'nan_Bulgaria',\n 'nan_Burkina Faso',\n 'nan_Burma',\n 'nan_Burundi',\n 'nan_Cabo Verde',\n 'nan_Cambodia',\n 'nan_Cameroon',\n 'nan_Central African Republic',\n 'nan_Chad',\n 'nan_Chile',\n 'nan_Colombia',\n 'nan_Congo (Brazzaville)',\n 'nan_Congo (Kinshasa)',\n 'nan_Costa Rica',\n \"nan_Cote d'Ivoire\",\n 'nan_Croatia',\n 'nan_Cuba',\n 'nan_Cyprus',\n 'nan_Czechia',\n 'nan_Denmark',\n 'nan_Diamond Princess',\n 'nan_Djibouti',\n 'nan_Dominica',\n 'nan_Dominican Republic',\n 'nan_Ecuador',\n 'nan_Egypt',\n 'nan_El Salvador',\n 'nan_Equatorial Guinea',\n 'nan_Eritrea',\n 'nan_Estonia',\n 'nan_Eswatini',\n 'nan_Ethiopia',\n 'nan_Fiji',\n 'nan_Finland',\n 'nan_France',\n 'nan_Gabon',\n 'nan_Gambia',\n 'nan_Georgia',\n 'nan_Germany',\n 'nan_Ghana',\n 'nan_Greece',\n 'nan_Grenada',\n 'nan_Guatemala',\n 'nan_Guinea',\n 'nan_Guinea-Bissau',\n 'nan_Guyana',\n 'nan_Haiti',\n 'nan_Holy See',\n 'nan_Honduras',\n 'nan_Hungary',\n 'nan_Iceland',\n 'nan_India',\n 'nan_Indonesia',\n 'nan_Iran',\n 'nan_Iraq',\n 'nan_Ireland',\n 'nan_Israel',\n 'nan_Italy',\n 'nan_Jamaica',\n 'nan_Japan',\n 'nan_Jordan',\n 'nan_Kazakhstan',\n 'nan_Kenya',\n 'nan_Korea, South',\n 'nan_Kosovo',\n 'nan_Kuwait',\n 'nan_Kyrgyzstan',\n 'nan_Laos',\n 'nan_Latvia',\n 'nan_Lebanon',\n 'nan_Liberia',\n 'nan_Libya',\n 'nan_Liechtenstein',\n 'nan_Lithuania',\n 'nan_Luxembourg',\n 'nan_MS Zaandam',\n 'nan_Madagascar',\n 'nan_Malawi',\n 'nan_Malaysia',\n 'nan_Maldives',\n 'nan_Mali',\n 'nan_Malta',\n 'nan_Mauritania',\n 'nan_Mauritius',\n 'nan_Mexico',\n 'nan_Moldova',\n 'nan_Monaco',\n 'nan_Mongolia',\n 'nan_Montenegro',\n 'nan_Morocco',\n 'nan_Mozambique',\n 'nan_Namibia',\n 'nan_Nepal',\n 'nan_Netherlands',\n 'nan_New Zealand',\n 'nan_Nicaragua',\n 'nan_Niger',\n 'nan_Nigeria',\n 'nan_North Macedonia',\n 'nan_Norway',\n 'nan_Oman',\n 'nan_Pakistan',\n 'nan_Panama',\n 'nan_Papua New Guinea',\n 'nan_Paraguay',\n 'nan_Peru',\n 'nan_Philippines',\n 'nan_Poland',\n 'nan_Portugal',\n 'nan_Qatar',\n 'nan_Romania',\n 'nan_Russia',\n 'nan_Rwanda',\n 'nan_Saint Kitts and Nevis',\n 'nan_Saint Lucia',\n 'nan_Saint Vincent and the Grenadines',\n 'nan_San Marino',\n 'nan_Sao Tome and Principe',\n 'nan_Saudi Arabia',\n 'nan_Senegal',\n 'nan_Serbia',\n 'nan_Seychelles',\n 'nan_Sierra Leone',\n 'nan_Singapore',\n 'nan_Slovakia',\n 'nan_Slovenia',\n 'nan_Somalia',\n 'nan_South Africa',\n 'nan_South Sudan',\n 'nan_Spain',\n 'nan_Sri Lanka',\n 'nan_Sudan',\n 'nan_Suriname',\n 'nan_Sweden',\n 'nan_Switzerland',\n 'nan_Syria',\n 'nan_Taiwan*',\n 'nan_Tanzania',\n 'nan_Thailand',\n 'nan_Timor-Leste',\n 'nan_Togo',\n 'nan_Trinidad and Tobago',\n 'nan_Tunisia',\n 'nan_Turkey',\n 'nan_Uganda',\n 'nan_Ukraine',\n 'nan_United Arab Emirates',\n 'nan_United Kingdom',\n 'nan_Uruguay',\n 'nan_Uzbekistan',\n 'nan_Venezuela',\n 'nan_Vietnam',\n 'nan_West Bank and Gaza',\n 'nan_Western Sahara',\n 'nan_Zambia',\n 'nan_Zimbabwe',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat','mean_rate_case_last3','mean_rate_fat_last3',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density',]\ndef train_model(df, label, base_label, features=features, **kwargs):\n    X_train = df.loc[df[\"split\"] == \"train\"][features]\n    y_train = np.log(df.loc[df[\"split\"] == \"train\"][label] + 1)\n    b_train = np.log(df.loc[df[\"split\"] == \"train\"][base_label] + 1)\n    X_test = df.loc[df[\"split\"] == \"test\"][features]\n    y_test = np.log(df.loc[df[\"split\"] == \"test\", label] + 1)\n    b_test = np.log(df.loc[df[\"split\"] == \"test\", base_label] + 1)\n    print(kwargs)\n    model = lgb.LGBMRegressor(**kwargs)\n    model.fit(X_train, y_train, init_score = b_train)\n    y_pred = model.predict(X_test)\n    print(np.sqrt(mean_squared_error(y_test, y_pred + b_test)))\n   # print(len(X_test))\n    return model\n\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"lgb_model_cases = train_model(x_train,\"ConfirmedCases\",'cases_prev',features,\n                                   max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=1000,\n                                   subsample=0.8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(lgb_model_cases.feature_importances_,x_train[features2])), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')\n\n\n\nlgb_model_fatalities = train_model(x_train, \"Fatalities\",'fat_prev',features2, \n                                   max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8\n                                  )\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(lgb_model_fatalities.feature_importances_,x_train[features2])), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"X_train = x_train[features2]\ny_train = np.log(x_train[\"ConfirmedCases\"] + 1)\nb_train = np.log(x_train[\"cases_prev\"] + 1)\ncases_model = lgb.LGBMRegressor(max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8\n                               )\ncases_model.fit(X_train, y_train, init_score = b_train)\n\nX_train = x_train[features2]\ny_train = np.log(x_train[\"Fatalities\"] + 1)\nb_train = np.log(x_train[\"fat_prev\"] + 1)\nfatalities_model = lgb.LGBMRegressor(max_depth=5,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=500,\n                                   subsample=0.8)\nfatalities_model.fit(X_train, y_train, init_score = b_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"train.csv\")\nsub_df = pd.read_csv(\"test.csv\")\ndf[\"key\"]=train_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\nsub_df[\"key\"]=sub_df[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n\n\ncoo_df = merged_train_df.groupby(\"key\")[['mean_rate_case_last7', 'mean_rate_case_each7',\n       'mean_rate_fat_last7', 'mean_rate_fat_each7', 'max_rate_case',\n       'min_rate_case', 'std_rate_case', 'mode_rate_case', 'range_rate_case',\n       'max_to_min_rate_case', 'max_rate_fat', 'min_rate_fat', 'std_rate_fat','mean_rate_case_last3','mean_rate_fat_last3',\n       'mode_rate_fat', 'mean_rate_case', 'mean_rate_fat', 'range_rate_fat','Lat','Long',\n       'max_to_min_rate_fat', 'rate_ConfirmedCases', 'rate_Fatalities','total_pop', 'smokers_perc', 'density']].mean().reset_index()\n   # coo_df = coo_df[coo_df[\"Country_Region\"].notnull()]\n\nloc_group = [\"key\"]\n   # merged_train_df['Date']= pd.to_datetime(merged_train_df['Date']) \n  #  merged_train_df[\"days\"] = (merged_train_df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n\ndef preprocess(df):\n    df[\"Date\"] = df[\"Date\"].astype(\"datetime64[ms]\")\n  #  df[\"days\"] = (df[\"Date\"] - pd.to_datetime(\"2020-01-01\")).dt.days\n    #    df[\"weekend\"] = df[\"Date\"].dt.dayofweek//5\n\n    df = df.merge(coo_df, how=\"left\", on=\"key\")\n    df[\"Lat\"] = (df[\"Lat\"] // 30).astype(np.float32).fillna(0)\n    df[\"Long\"] = (df[\"Long\"] // 60).astype(np.float32).fillna(0)\n    key_dummies = pd.get_dummies(df['key'])\n    df = pd.concat([df,key_dummies],axis=1)\n\n    for col in loc_group:\n        df[col].fillna(\"none\", inplace=True)\n    return df\n\ndf = preprocess(df)\nsub_df = preprocess(sub_df)\n\n\ntrain_df = pd.read_csv(\"train.csv\")\ntest_df = pd.read_csv(\"test.csv\")\n\n#train_df['Date']= pd.to_datetime(train_df['Date']) \ncountry_list = train_df ['Country_Region'].unique()\nkey_list=df['key'].unique()\n#train_df['Date']= pd.to_datetime(train_df['Date']) \n\n\nscoring_dates = test_df['Date'].unique()\npred_df = pd.DataFrame(columns=train_df.columns)\n\nsub = []\nfor date in scoring_dates.tolist():\n    print(date)\n    start=time.time()\n    \n    if date < train_df['Date'].max():\n            \n        new_df = df.loc[df[\"Date\"] < date].copy()\n        curr_date_df = sub_df.loc[sub_df[\"Date\"] == date].copy()\n        curr_date_df[\"ConfirmedCases\"] = 0\n        curr_date_df[\"Fatalities\"] = 0\n        new_df = new_df.append(curr_date_df).reset_index(drop=True)\n        new_df['cases_prev']=new_df.groupby(\"key\")[\"ConfirmedCases\"].shift()\n        new_df['fat_prev']=new_df.groupby(\"key\")[\"Fatalities\"].shift()\n        \n        \n    \n\n        predictions = cases_model.predict(new_df[features2]) + np.log(new_df[\"cases_prev\"] + 1)\n        new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"cases_prev\"]))\n        predictions = fatalities_model.predict(new_df[features2]) + np.log(new_df[\"fat_prev\"] + 1)\n        new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"fat_prev\"])\n        new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n        new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n        new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n        pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n            \n        \n       \n    else:\n            #new_df = new_df.copy()\n        curr_date_df = sub_df.loc[sub_df[\"Date\"] == date].copy()\n        curr_date_df[\"ConfirmedCases\"] = 0\n        curr_date_df[\"Fatalities\"] = 0\n        new_df = new_df.append(curr_date_df).reset_index(drop=True)\n        new_df['cases_prev']=new_df.groupby(\"key\")[\"ConfirmedCases\"].shift()\n        new_df['fat_prev']=new_df.groupby(\"key\")[\"Fatalities\"].shift()\n        \n        predictions = cases_model.predict(new_df[features2]) + np.log(new_df[\"cases_prev\"] + 1)\n        new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"cases_prev\"]))\n        predictions = fatalities_model.predict(new_df[features2]) + np.log(new_df[\"fat_prev\"] + 1)\n        new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"fat_prev\"])\n        new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n        new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n        new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n        pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n            \n    print(time.time()-start)  \n            \nX_forecastId = sub_df.loc[(sub_df['key'] == key), ['ForecastId']]\nX_forecastId = X_forecastId.values.tolist()\nX_forecastId = [v[0] for v in X_forecastId]\nfor j in range(len(sub_df['ForecastId'])):\n          \n    dic = { 'ForecastId': X_forecastId[j], 'ConfirmedCases': pred_df['ConfirmedCases'][j], 'Fatalities': pred_df['Fatalities'][j]}\n    sub.append(dic)\n\nsubmission = pd.DataFrame(sub)\nsubmission[['ForecastId','ConfirmedCases','Fatalities']].to_csv(path_or_buf='submission.csv',index=False) \nfrom Gaussiandistribution import Gaussian","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}