{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imports\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/test.csv\")\nsubmission_data_schema = pd.read_csv(\"/kaggle/input/covid19-global-forecasting-week-4/submission.csv\")\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['Date'] = pd.to_datetime(train_data['Date'])\nprint(train_data.describe())\nprint(max(train_data.Date))\nprint(test_data.head())\ntrain_data.drop(['Id'], axis=1, inplace=True)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on below analysis, there are two options:\n1. Build a combined model for all data with imputing Province_State where missing.\n2. Replace all Province_State with a value \"full_country_data\" so that the model can train this column indicating full country data.\n3. Build a separate model for countries which have Province_State and then another one for countries without state.\n(3) model could work much better since the 4 countries below could mislead the model.\nMight be worthwhile to start experimentation with (2)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_data.Country_Region.unique()))\ncountries_without_state = train_data[pd.isnull(train_data['Province_State'])]\nprint(len(countries_without_state.Country_Region.unique()))\ncountries_with_state = train_data.dropna(subset=['Province_State'])\nprint(countries_with_state.Country_Region.unique())\nfor country in countries_with_state.Country_Region.unique():\n    if country in countries_without_state.Country_Region.unique():\n        print(country)\n# 4 countries have both states data and country level data:\n# Denmark\n# France\n# Netherlands\n# United Kingdom","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Approach 2\n# Date features\nfrom datetime import datetime\ntrain_data['Day'] = train_data.apply(lambda row: row.Date.day, axis=1)\ntrain_data['Month'] = train_data.apply(lambda row: row.Date.month, axis=1)\n\n# Impute Province_State\ntrain_data['Province_State'] = train_data[['Province_State']].fillna('FULL_COUNTRY')\ntrain_data.tail(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stationarize ConfirmdedCases and Fatalities. This has to be done per region\noutput_country = pd.DataFrame()\ncount = 0\nfor country in train_data.Country_Region.unique():\n    country_data = train_data[train_data.Country_Region == country]\n    for state in country_data.Province_State.unique():\n        state_data = country_data[country_data.Province_State == state]\n        state_data['cc_stationary'] = state_data[['ConfirmedCases']].diff()\n        state_data['f_stationary'] = state_data[['Fatalities']].diff()\n        output_country = output_country.append(state_data)\n        count += state_data.shape[0]\ntrain_data = train_data.merge(output_country, on=['Date', 'Country_Region', 'Province_State', \n                                     'Day', 'Month', 'ConfirmedCases', 'Fatalities'])\ntrain_data.fillna(0, inplace=True)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Sparse Matrix somehow performs much better\nAverage RMSLE of 1.557 vs. 2+ for non sparse"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Evaluation using K-fold cross validation\n\n# # Using LabelBinarizer and sparse matrix\n# # Prepare features and prediction data\n# features_data = train_data[['Country_Region', 'Province_State', 'Day', 'Month']]\n# # LabelBinarize train_data\n# from sklearn import preprocessing\n# cr_lb = preprocessing.LabelBinarizer(sparse_output=True)\n# ps_lb = preprocessing.LabelBinarizer(sparse_output=True)\n# one_hot_countries = cr_lb.fit_transform(features_data[['Country_Region']])\n# one_hot_states = ps_lb.fit_transform(features_data[['Province_State']])\n\n# from scipy import sparse\n# training_input = features_data[['Day', 'Month']].values\n# training_input = sparse.hstack((training_input, one_hot_countries, one_hot_states))\n\n# # y_confirmed_cases = train_data[['ConfirmedCases']]\n# # y_fatalities = train_data[['Fatalities']]\n\n# y_confirmed_cases = train_data[['cc_stationary']]\n# y_fatalities = train_data[['f_stationary']]\n# y_confirmed_cases.clip(0, inplace=True)\n# y_fatalities.clip(0, inplace=True)\n\n# # train_test_split - 5 folds\n# from sklearn.model_selection import KFold\n# SPLIT_COUNT = 10\n# kf = KFold(n_splits=SPLIT_COUNT)\n\n# # Running XGBoosRegressor\n# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.metrics import mean_squared_log_error\n\n# average_error = 0\n# for train_index, test_index in kf.split(features_data):\n#     X_train, X_test = training_input.tocsr()[train_index,:], training_input.tocsr()[test_index,:]\n#     y_cc_train, y_cc_test = y_confirmed_cases.iloc[train_index], y_confirmed_cases.iloc[test_index]\n#     y_f_train, y_f_test = y_fatalities.iloc[train_index], y_fatalities.iloc[test_index]\n#     model_cc = xgb.XGBRegressor()\n#     model_cc.fit(X_train, y_cc_train)\n#     model_f = xgb.XGBRegressor()\n#     model_f.fit(X_train, y_f_train)\n#     predictions_cc = pd.DataFrame(model_cc.predict(X_test))\n#     predictions_f = pd.DataFrame(model_f.predict(X_test))\n#     predictions_cc.clip(0, inplace=True)\n#     predictions_f.clip(0, inplace=True)\n#     cc_error = np.sqrt(mean_squared_log_error(predictions_cc, y_cc_test))\n#     f_error = np.sqrt(mean_squared_log_error(predictions_f, y_f_test))\n#     median_error = (cc_error + f_error)/2\n#     average_error += median_error\n#     print(median_error)\n# print(average_error/SPLIT_COUNT)\n\n# # Average score Random forest: 1.7072718462044407\n# # Average score XGBoost: 1.5573904740299072\n\n# # Average after stationarization\n# # XGBoost 1.1587040498615342","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using LabelBinarizer and sparse matrix\n# Prepare features and prediction data\nfeatures_data = train_data[['Country_Region', 'Province_State', 'Day', 'Month']]\n# LabelBinarize train_data\nfrom sklearn import preprocessing\ncr_lb = preprocessing.LabelBinarizer(sparse_output=True)\nps_lb = preprocessing.LabelBinarizer(sparse_output=True)\none_hot_countries = cr_lb.fit_transform(features_data[['Country_Region']])\none_hot_states = ps_lb.fit_transform(features_data[['Province_State']])\n\nfrom scipy import sparse\ntraining_input = features_data[['Day', 'Month']].values\ntraining_input = sparse.hstack((training_input, one_hot_countries, one_hot_states))\n\nX_test = test_data\nX_test['Date'] = pd.to_datetime(X_test['Date'])\nX_test['Day'] = X_test.apply(lambda row: row.Date.day, axis=1)\nX_test['Month'] = X_test.apply(lambda row: row.Date.month, axis=1)\nX_test['Province_State'] = X_test[['Province_State']].fillna('FULL_COUNTRY')\nX_test = X_test[['Country_Region', 'Province_State', 'Day', 'Month']]\none_hot_c = cr_lb.transform(X_test[['Country_Region']])\none_hot_s = ps_lb.transform(X_test[['Province_State']])\ntesting_input = X_test[['Day', 'Month']].values\ntesting_input = sparse.hstack((testing_input, one_hot_c, one_hot_s))\n\ny_confirmed_cases = train_data[['cc_stationary']]\ny_fatalities = train_data[['f_stationary']]\ny_confirmed_cases.clip(0, inplace=True)\ny_fatalities.clip(0, inplace=True)\n\n# Running XGBoosRegressor\n\nmodel_cc = xgb.XGBRegressor()\nmodel_cc.fit(training_input, y_confirmed_cases)\nmodel_f = xgb.XGBRegressor()\nmodel_f.fit(training_input, y_fatalities)\npredictions_cc = pd.DataFrame(model_cc.predict(testing_input))\npredictions_f = pd.DataFrame(model_f.predict(testing_input))\npredictions_cc.clip(0, inplace=True)\npredictions_f.clip(0, inplace=True)\n# predictions_cc.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Submitting work of day 1\nX_test['PredictedConfirmedCases_s'] = predictions_cc\nX_test['PredictedFatalities_s'] = predictions_f\nX_test['ForecastId'] = test_data['ForecastId']\n\nLAST_TRAIN_DATE = '2020-04-01'\noutput_df = pd.DataFrame()\nfor country in train_data.Country_Region.unique():\n    country_data = X_test[X_test['Country_Region'] == country]\n    for state in country_data.Province_State.unique():\n        state_data = country_data[country_data['Province_State'] == state]\n        input_data = train_data[((train_data.Province_State == state) \n                                 & (train_data.Date == LAST_TRAIN_DATE)\n                                 & (train_data.Country_Region == country)\n                                )]\n        state_data.iloc[0, state_data.columns.get_loc('PredictedConfirmedCases_s')] = input_data.iloc[0].ConfirmedCases + state_data.iloc[0].PredictedConfirmedCases_s\n        state_data.iloc[0, state_data.columns.get_loc('PredictedFatalities_s')] = input_data.iloc[0].Fatalities + state_data.iloc[0].PredictedFatalities_s\n        state_data['ConfirmedCases'] = state_data[['PredictedConfirmedCases_s']].cumsum()\n        state_data['Fatalities'] = state_data[['PredictedFatalities_s']].cumsum()\n        output_df = output_df.append(state_data)\noutput_df['ConfirmedCases'] = output_df['ConfirmedCases'].apply(np.ceil)\noutput_df['Fatalities'] = output_df['Fatalities'].apply(np.ceil)\noutput_df.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output_df = output_df[['ForecastId', 'ConfirmedCases', 'Fatalities']]\noutput_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}