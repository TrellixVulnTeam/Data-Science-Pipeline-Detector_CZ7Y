{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install git+https://github.com/shijianjian/EfficientNet-PyTorch-3D\n!pip3 install torchsummary","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:19:00.567516Z","iopub.execute_input":"2021-10-06T08:19:00.567877Z","iopub.status.idle":"2021-10-06T08:19:20.078468Z","shell.execute_reply.started":"2021-10-06T08:19:00.567795Z","shell.execute_reply":"2021-10-06T08:19:20.077641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from efficientnet_pytorch_3d import EfficientNet3D\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:20:54.66995Z","iopub.execute_input":"2021-10-06T08:20:54.67071Z","iopub.status.idle":"2021-10-06T08:20:54.674528Z","shell.execute_reply.started":"2021-10-06T08:20:54.670669Z","shell.execute_reply":"2021-10-06T08:20:54.673708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport time\nimport glob\nimport os\nimport torch\nfrom torch import nn\nimport random\nfrom torch.utils import data as torch_data\nfrom sklearn import model_selection as sk_model_selection\nfrom torch.nn import functional as torch_functional\nimport torch.nn.functional as F\nimport pandas\nnp.random.seed(1)\ntorch.backends.cudnn.benchmark = True\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:20:54.898739Z","iopub.execute_input":"2021-10-06T08:20:54.899385Z","iopub.status.idle":"2021-10-06T08:20:54.907688Z","shell.execute_reply.started":"2021-10-06T08:20:54.899344Z","shell.execute_reply":"2021-10-06T08:20:54.906337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport math","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:20:55.235114Z","iopub.execute_input":"2021-10-06T08:20:55.235419Z","iopub.status.idle":"2021-10-06T08:20:55.243122Z","shell.execute_reply.started":"2021-10-06T08:20:55.235388Z","shell.execute_reply":"2021-10-06T08:20:55.242138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\nset_seed(12)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:20:55.584852Z","iopub.execute_input":"2021-10-06T08:20:55.585681Z","iopub.status.idle":"2021-10-06T08:20:55.659552Z","shell.execute_reply.started":"2021-10-06T08:20:55.585634Z","shell.execute_reply":"2021-10-06T08:20:55.658667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class sum_writer:\n    def __init__(self,folder):\n        if os.path.exists(folder):\n            print('Removing old events..')\n            for fil in os.listdir(folder):\n                os.remove(os.path.join(folder, fil))\n\n        self.writer =  SummaryWriter(folder)\n        \n    def close(self):\n        self.writer.flush()\n        self.writer.close()\n\n        \n    def add_info_new(self,msg,value1,value2):\n        self.writer.add_scalar(msg, value1, value2)\n        \ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef save_models(e,model,out_path,name):\n    save = {'model':model.state_dict(),'epoch':e+1}\n    #torch.save(save, os.path.join(out_path, '%s_%s_e%02d.pth' % (name,time.strftime(\"%d-%m-%Y-%H-%M-%S\"), e+1)))\n    torch.save(save, os.path.join(out_path, '%s_e%02d.pth' % (name, e+1)))\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy@k for the specified values of k\"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:20:55.932814Z","iopub.execute_input":"2021-10-06T08:20:55.933675Z","iopub.status.idle":"2021-10-06T08:20:55.950839Z","shell.execute_reply.started":"2021-10-06T08:20:55.933624Z","shell.execute_reply":"2021-10-06T08:20:55.949084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_all_slices(df,base_dir): \n    all_paths = []\n    for i in list(df['folder_id']):\n        i = os.path.join(base_dir,i)\n        all_paths.append(len(glob.glob(i+'/flair/*')))\n    return all_paths\n\ndef split_train_test(slices_list,folders_list,label_list,split_ratio=0.1):\n    test_size = int(len(slices_list)*split_ratio)\n    test_slices_list = slices_list[:test_size]\n    test_folders_list = folders_list[:test_size]\n    test_label_list = label_list[:test_size]\n    train_slices_list = slices_list[test_size:]\n    train_folders_list = folders_list[test_size:]\n    train_label_list = label_list[test_size:]\n    return train_slices_list,train_folders_list,train_label_list,test_slices_list,test_folders_list,test_label_list\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:20:56.299252Z","iopub.execute_input":"2021-10-06T08:20:56.299548Z","iopub.status.idle":"2021-10-06T08:20:56.307623Z","shell.execute_reply.started":"2021-10-06T08:20:56.299517Z","shell.execute_reply":"2021-10-06T08:20:56.306741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/rsnasubmissionresult/result.csv',dtype='str')\nbase_dir = '../input/classify-tumor-best/DATATUMORONLY_TRAIN/train'","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:20:56.649401Z","iopub.execute_input":"2021-10-06T08:20:56.650087Z","iopub.status.idle":"2021-10-06T08:20:56.679377Z","shell.execute_reply.started":"2021-10-06T08:20:56.650044Z","shell.execute_reply":"2021-10-06T08:20:56.678394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = df.iloc[:525,:]\ntest_df = df.iloc[526:,:]","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:20:57.020789Z","iopub.execute_input":"2021-10-06T08:20:57.021643Z","iopub.status.idle":"2021-10-06T08:20:57.031354Z","shell.execute_reply.started":"2021-10-06T08:20:57.021577Z","shell.execute_reply":"2021-10-06T08:20:57.030247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_slices_list = np.array(get_all_slices(train_df,base_dir))\ntest_slices_list = np.array(get_all_slices(test_df,base_dir))\n#slices_list = np.array(list(df['flair']))\ntrain_folders_list = np.array(list(train_df['folder_id']))\ntest_folders_list = np.array(list(test_df['folder_id']))\ntrain_label_list = np.array(list(train_df['MGMT_value']))\ntest_label_list = np.array(list(test_df['MGMT_value']))\nindexes = np.where((train_slices_list > 0 )&(train_slices_list < 51))\ntrain_slices_list = np.take(train_slices_list,indexes)[0]\ntrain_folders_list = np.take(train_folders_list,indexes)[0]\ntrain_label_list = np.take(train_label_list,indexes)[0]\nindexes = np.where((test_slices_list > 0 )&(test_slices_list < 51))\ntest_slices_list = np.take(test_slices_list,indexes)[0]\ntest_folders_list = np.take(test_folders_list,indexes)[0]\ntest_label_list = np.take(test_label_list,indexes)[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:20:57.297431Z","iopub.execute_input":"2021-10-06T08:20:57.298251Z","iopub.status.idle":"2021-10-06T08:21:08.081516Z","shell.execute_reply.started":"2021-10-06T08:20:57.298203Z","shell.execute_reply":"2021-10-06T08:21:08.080614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(torch_data.IterableDataset):\n    def __init__(self,slices_list,folders_list,label_list,width=256,height=256,batch_size=16,shuffle=True):\n        self.batch_size = batch_size\n        self.base_dir = '../input/classify-tumor-best/DATATUMORONLY_TRAIN/train'\n        self.width = width\n        self.crop_length = 224\n        self.height = height\n        self.tolerance = 5\n        self.shuffle = shuffle\n        self.intial_slices_list = slices_list\n        self.intial_folders_list = folders_list\n        self.intial_label_list = label_list\n        self.class_map = {'MGMT':1,'NONMGMT':0}\n        self.min_slice = 64 \n        self.normdata = 1\n        self.offset = 50 #for __len__\n        self.index_map = {v:k for k,v in self.class_map.items()}\n        self.get_groups()\n        self._get_size()\n        \n        print('Loader==>',len(self.intial_slices_list),self.class_map,self.index_map)\n        print('Grps:',[(K,len(V)) for K,V in self.group_indexes.items()])\n        print('Size:',self.size-self.offset)\n        \n    def get_groups(self):\n        limit = len(self.intial_slices_list)\n        tmp = np.copy(self.intial_slices_list)\n        i = 0\n        self.group_indexes = {}\n        \n        while i<limit:\n            if len(tmp)==0:\n                print('Empty TMP')\n                break\n            num = self.intial_slices_list[i]\n            ul = num+self.tolerance\n            ll = max(num-self.tolerance,0)\n            key = f'{ll}-{ul}'\n            lindexes = np.where(tmp >=ll)[0]\n            uindixes = np.where(tmp <= ul)[0]\n            neg_indexes = np.where(tmp==-1)[0]\n            indexes = np.intersect1d(lindexes,uindixes)\n            indexes = np.array(list(set(indexes)-set(neg_indexes)))\n\n            if len(indexes) == 0:\n                #print('Empty',i,key)\n                pass\n            else:\n                #print(i,key)\n                self.group_indexes[key] = indexes\n                #tmp = np.delete(tmp,indexes)\n                tmp[indexes] = -1\n            i+=1\n    \n\n    def __len__(self):\n        return self.size\n\n    def __iter__(self):\n        self.i = 0\n        #print('ITER....')\n        self.train_indexes = []\n        #print('Grps:',[(K,len(V)) for K,V in self.group_indexes.items()])\n        for key in self.group_indexes.keys():\n            bindexes = self.group_indexes[key]\n            if self.shuffle:\n                bindexes = np.random.choice(bindexes,len(bindexes),replace=False)\n            self.train_indexes.extend([bindexes[i:i+self.batch_size] for i in range(0,len(bindexes),self.batch_size)])\n                   \n        self.size = len(self.train_indexes)\n        #print('self.size',self.size,self.train_indexes)\n        return self\n    \n\n    def __next__(self):\n        if self.i >= self.size:\n            raise StopIteration\n        self.i += 1\n        #print('NEXT....')\n        return self.prepare_data(self.train_indexes[self.i-1])\n    \n    def prepare_data(self,random_indexes):\n        start =time.time()\n        labels = []\n        random_folder = np.take(self.intial_folders_list,random_indexes)\n        random_slices = np.take(self.intial_slices_list,random_indexes)\n        random_labels = np.take(self.intial_label_list,random_indexes)\n        self.max_depth = random_slices.max()\n        batch_x = self.__data_gen_batch(random_folder)\n        if self.normdata:\n            batch_x = batch_x/255.\n        #print(batch_x.shape)\n        batch_x = torch.tensor(batch_x.transpose(0,4,3,1,2)).float() #B,CH,D,H,W\n        #batch_x = torch.tensor(batch_x.transpose(0,4,1,2,3)).float() #B,CH,H,W,D\n        #print(batch_x.shape)\n        #padslic = self.min_slice-batch_x.shape[2]#B,CH,D,H,W\n        #padslic = self.min_slice-batch_x.shape[-1]#B,CH,H,W,D\n        #batch_x = F.pad(batch_x,(0,0,0,0,0,padslic),'constant',0)#B,CH,D,H,W\n        #batch_x = F.pad(batch_x,(0,padslic),'constant',0)#B,CH,H,W,D\n        \n        labels = torch.tensor(random_labels.astype(int),dtype=torch.long)\n        return batch_x,labels\n    \n\n    def __data_gen_image(self,folder_name):\n        flair_path = glob.glob(os.path.join(self.base_dir,folder_name,'flair/*'))\n        flair_path = sorted(flair_path,key=lambda x:x.split('-')[-1].split('.')[-2].zfill(3))\n        all_images = []\n        all_images = np.zeros(shape=(self.max_depth,self.height,self.height,1),dtype=np.float32)\n        for i,img_path in enumerate(flair_path):\n            img = cv2.imread(img_path,1)\n            img = cv2.resize(img,(self.width,self.height))\n            #img = image.img_to_array(img)\n            #all_images[i,] = np.expand_dims(img,axis=-1)\n        return np.transpose(all_images,(1,2,0,3))\n\n    def __data_gen_batch(self,folder_names):\n        batch_data = np.empty(shape=(len(folder_names),self.height,self.width,self.max_depth,3))\n        for i,patient_id in enumerate(folder_names):\n            batch_data[i,] = self.__data_gen_image(patient_id)\n        return batch_data\n    \n    def _get_size(self):\n        self.size = 0\n        self.size += self.offset\n        for k in self.group_indexes:\n            div, mod = divmod(len(self.group_indexes[k]), self.batch_size)\n            if mod!=0:\n                div +=1\n            self.size += div\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:21:08.083439Z","iopub.execute_input":"2021-10-06T08:21:08.083783Z","iopub.status.idle":"2021-10-06T08:21:08.11894Z","shell.execute_reply.started":"2021-10-06T08:21:08.083738Z","shell.execute_reply":"2021-10-06T08:21:08.117751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def worker_init_fn(_):\n    worker_info = torch.utils.data.get_worker_info()\n    dataset = worker_info.dataset\n    worker_id = worker_info.id\n    for K in dataset.group_indexes:\n        split_size = math.ceil(len(dataset.group_indexes[K]) / worker_info.num_workers)\n        dataset.group_indexes[K] = dataset.group_indexes[K][worker_id*split_size:(worker_id +1)*split_size]\n        #print('worker_id:',worker_id,',K',K,',Data',len(dataset.group_indexes[K]),'\\n')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:21:08.120761Z","iopub.execute_input":"2021-10-06T08:21:08.121238Z","iopub.status.idle":"2021-10-06T08:21:08.137936Z","shell.execute_reply.started":"2021-10-06T08:21:08.121189Z","shell.execute_reply":"2021-10-06T08:21:08.136785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_datagen = DataGenerator(train_slices_list,train_folders_list,train_label_list,batch_size=3,height=224,width=224,shuffle=True)\ntest_datagen = DataGenerator(test_slices_list,test_folders_list,test_label_list,batch_size=2,height=224,width=224,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:21:08.140491Z","iopub.execute_input":"2021-10-06T08:21:08.14089Z","iopub.status.idle":"2021-10-06T08:21:08.241382Z","shell.execute_reply.started":"2021-10-06T08:21:08.140843Z","shell.execute_reply":"2021-10-06T08:21:08.240333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(train_datagen, batch_size=None,pin_memory=False,num_workers=4,worker_init_fn=worker_init_fn)\nval_loader = torch.utils.data.DataLoader(test_datagen, batch_size=None,pin_memory=False,num_workers=2,worker_init_fn=worker_init_fn)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:21:08.242973Z","iopub.execute_input":"2021-10-06T08:21:08.243547Z","iopub.status.idle":"2021-10-06T08:21:08.249529Z","shell.execute_reply.started":"2021-10-06T08:21:08.243502Z","shell.execute_reply":"2021-10-06T08:21:08.248702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_loader))\nfor i,sample in enumerate(train_loader):\n    if i>10:\n        break\n    print(i,sample[0].shape,sample[1].shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T08:21:08.25142Z","iopub.execute_input":"2021-10-06T08:21:08.25208Z","iopub.status.idle":"2021-10-06T08:21:13.852387Z","shell.execute_reply.started":"2021-10-06T08:21:08.252035Z","shell.execute_reply":"2021-10-06T08:21:13.851378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        #self.net = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=1)\n        #self.net = torchvision.models.r2plus1d_18(pretrained=False, progress=True, num_classes=2)\n        self.net = torchvision.models.video.mc3_18(pretrained=False, progress=True, num_classes=2)\n        #n_features = self.net._fc.in_features\n        #self.net._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n    \n    def forward(self, x):\n        out = self.net(x)\n        return out\n    \nmodel = None\nmodel = Model()\nmodel.cuda()\nprint('Done')","metadata":{"execution":{"iopub.status.busy":"2021-10-06T07:22:45.138185Z","iopub.execute_input":"2021-10-06T07:22:45.138677Z","iopub.status.idle":"2021-10-06T07:22:51.94616Z","shell.execute_reply.started":"2021-10-06T07:22:45.138644Z","shell.execute_reply":"2021-10-06T07:22:51.945302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model, input_size=(3,100, 224,224))","metadata":{"execution":{"iopub.status.busy":"2021-10-06T07:22:55.869054Z","iopub.execute_input":"2021-10-06T07:22:55.869342Z","iopub.status.idle":"2021-10-06T07:22:58.968677Z","shell.execute_reply.started":"2021-10-06T07:22:55.869313Z","shell.execute_reply":"2021-10-06T07:22:58.967492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,90,95], gamma=0.1, last_epoch=-1, verbose=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T07:23:25.125746Z","iopub.execute_input":"2021-10-06T07:23:25.126038Z","iopub.status.idle":"2021-10-06T07:23:25.135978Z","shell.execute_reply.started":"2021-10-06T07:23:25.12601Z","shell.execute_reply":"2021-10-06T07:23:25.135047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in train_loader:\n    print(i[0].shape,i[1].shape)\n    break\n#inputs = torch.randn((5, 1, 224, 224, 64)).cuda()\ninputs = torch.randn((5, 3, 1,224, 224)).cuda()\nlabels = torch.tensor([0,1,0,0,1],dtype=torch.long).cuda()\nout = model(inputs)\nprint(out.shape)\nimg = i[0].cuda()\nprint(img.shape,img.dtype)\nout = model(img)\nprint(criterion(out,i[1].cuda()))","metadata":{"execution":{"iopub.status.busy":"2021-10-06T07:23:42.311053Z","iopub.execute_input":"2021-10-06T07:23:42.311897Z","iopub.status.idle":"2021-10-06T07:23:48.368831Z","shell.execute_reply.started":"2021-10-06T07:23:42.311864Z","shell.execute_reply":"2021-10-06T07:23:48.367642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eval_model(model,valloader,epoch,writer,val_global_iter):\n    model.eval()\n    dset = tqdm(enumerate(valloader),total=len(valloader))\n    avg_acc = 0\n    avg_loss = 0\n    \n    val_writer_freq = 10\n    val_total_loss = 0\n    val_total_acc = 0\n\n    for i,sample in dset:\n        val_global_iter+=1\n        #if i>10:\n        #    break\n        batch,label = sample\n        batch = batch.cuda()\n        label = label.cuda()\n        out = model(batch)\n        loss = criterion(out, label)\n        accs = accuracy(out.data, label, topk=(1,))\n        \n        val_total_loss += loss.item()\n        val_total_acc += accs[0].item()\n        avg_loss += loss.item()\n        avg_acc += accs[0].item()\n        if val_global_iter%val_writer_freq == 0:\n            writer.add_info_new(\"Loss/val\", value1=val_total_loss/val_writer_freq, value2=val_global_iter)\n            writer.add_info_new(\"ACC_Ori/val\",value1=val_total_acc/val_writer_freq, value2=val_global_iter)\n            val_total_loss = 0\n            val_total_acc = 0\n            \n        dset.set_description(f'ValEpoch:{epoch} | Loss:{loss.item():.4f} | Acc:{accs[0].item():.1f}')\n    print(f\"Avg Loss:{avg_loss/(i+1):.4f} | Avg Acc:{avg_acc/(i+1):.1f}\")\n    return val_global_iter","metadata":{"execution":{"iopub.status.busy":"2021-10-06T07:31:48.242664Z","iopub.execute_input":"2021-10-06T07:31:48.243565Z","iopub.status.idle":"2021-10-06T07:31:48.255886Z","shell.execute_reply.started":"2021-10-06T07:31:48.243527Z","shell.execute_reply":"2021-10-06T07:31:48.254252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"name = 'Eff_3d'\nout_path = './saved_model'\nos.makedirs(out_path,exist_ok=True)\nfolder = f'./logs/{name}'\nos.makedirs(folder,exist_ok=True)\nwriter = sum_writer(folder)\n\ntrain_global_iter = 0\ntrain_writer_freq = 10\ntrain_total_loss = 0\nval_global_iter = 0\ntrain_total_acc = 0\n\nfor epoch in range(100):\n    model.train()\n    dset = tqdm(enumerate(train_loader),total=len(train_loader))\n    avg_acc = 0\n    avg_loss = 0\n    for i,sample in dset:\n        train_global_iter+=1\n        #if i>10:\n        #    break\n        optimizer.zero_grad()\n        batch,label = sample\n        batch = batch.cuda()\n        label = label.cuda()\n        out = model(batch)\n        loss = criterion(out, label)\n        loss.backward()\n        accs = accuracy(out.data, label, topk=(1,))\n        \n        train_total_loss += loss.item()\n        train_total_acc += accs[0].item()\n        avg_loss += loss.item()\n        avg_acc += accs[0].item()\n        \n        optimizer.step()\n        optlr = get_lr(optimizer)\n        \n        if train_global_iter%train_writer_freq == 0:\n            writer.add_info_new(\"Loss/train\", value1=train_total_loss/train_writer_freq, value2=train_global_iter)\n            writer.add_info_new(\"LR/train\",value1=optlr, value2=train_global_iter)\n            writer.add_info_new(\"ACC_Ori/train\",value1=train_total_acc/train_writer_freq, value2=train_global_iter)\n            train_total_loss = 0\n            train_total_acc = 0\n            \n        dset.set_description(f'Epoch:{epoch} | LR:{optlr:.5f} | Loss:{loss.item():.4f} | Acc:{accs[0].item():.1f}')\n        #break\n    print(f\"Avg Loss:{avg_loss/(i+1):.4f} | Avg Acc:{avg_acc/(i+1):.1f}\")\n    if epoch%2 == 0:\n        save_models(epoch,model,out_path,name)\n    val_global_iter = eval_model(model,val_loader,epoch,writer,val_global_iter)\n    scheduler.step()\n    writer.add_info_new(\"EPOCH/train\",value1=epoch+1, value2=i*(epoch+1))\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-06T07:31:50.08964Z","iopub.execute_input":"2021-10-06T07:31:50.089964Z","iopub.status.idle":"2021-10-06T07:32:40.838539Z","shell.execute_reply.started":"2021-10-06T07:31:50.089933Z","shell.execute_reply":"2021-10-06T07:32:40.836332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}