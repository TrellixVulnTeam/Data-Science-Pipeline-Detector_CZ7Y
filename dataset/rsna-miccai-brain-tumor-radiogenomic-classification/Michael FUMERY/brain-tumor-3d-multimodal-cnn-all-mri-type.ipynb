{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![brain_baner](http://www.mf-data-science.fr/images/projects/brain_baner.jpg)","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\">Context</h1>\n\nThe goal of this competition, initiated by the **Radiological Society of North America *(RSNA)*** in partnership with the **Medical Image Computing and Computer Assisted Intervention Society *(the MICCAI Society)*** is to predict the methylation of the **MGMT promoter**, which is an important gene biomarker for treatment of brain tumors.\n\nThese predictions will be based on a database of **MRI *(magnetic resonance imaging)*** scans of several hundred patients.\n\n<h1 style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\">Data</h1>\n\nEach independent case has a dedicated folder identified by a five-digit number. Within each of these ‚Äúcase‚Äù folders, there are four sub-folders, each of them corresponding to each of the structural multi-parametric MRI (mpMRI) scans, in DICOM format. The exact mpMRI scans included are:\n\n- Fluid Attenuated Inversion Recovery (FLAIR)\n- T1-weighted pre-contrast (T1w)\n- T1-weighted post-contrast (T1Gd)\n- T2-weighted (T2)\n\n| ![brain_baner](http://www.mf-data-science.fr/images/projects/brain_tumor_types.png) | \n|:--:| \n| *Examples of the four MR sequence types included in this work* |\n\n<h1 style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\">Acknowledgement</h1>\n\nThis Notebook is inspired from *Ammar Alhaj Ali* work :\n- [üß†Brain Tumor 3D [Training]](https://www.kaggle.com/ammarnassanalhajali/brain-tumor-3d-training)\n- [üß†Brain Tumor 3D [Inference]](https://www.kaggle.com/ammarnassanalhajali/brain-tumor-3d-inference)","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\">Summary</h1>\n\n1. [Exploratory data analysis (EDA)](#section_1)      \n    1.1. [Submission sample & train.csv](#section_1_1)      \n    1.2. [MRI train data](#section_1_2)      \n    1.3. [Data cleaning](#section_1_3)      \n\n2. [Preprocessing](#section_2)      \n    2.1. [Crop and resize the images](#section_2_1)      \n    2.2. [Equalization CLAHE](#section_2_2)      \n    2.3. [Denoising filter](#section_2_3)      \n    2.4. [Global preprocessing function](#section_2_4)      \n    \n3. [Development of supervised models](#section_3)      \n    3.1. [Multimodal inputs CNN from scratch](#section_3_1)      \n    3.2. [Define loaders for images sequences 4 MRI types](#section_3_2)      \n    3.3. [Define folds](#section_3_3)      \n    3.4. [Keras custom data generator](#section_3_4)      \n    3.5. [Define CNN Multi-inputs model](#section_3_5)      \n    \n4. [Test of trained final model](#section_4)     \n5. [Try another approach: Transfer Learning](#section_5)","metadata":{}},{"cell_type":"markdown","source":"# <span style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\" id=\"section_1\">Exploratory data analysis (EDA)</span>","metadata":{}},{"cell_type":"markdown","source":"First, we have to load the usefull Python libraries :","metadata":{"execution":{"iopub.status.busy":"2021-08-04T14:37:44.731175Z","iopub.execute_input":"2021-08-04T14:37:44.731595Z","iopub.status.idle":"2021-08-04T14:37:44.739822Z","shell.execute_reply.started":"2021-08-04T14:37:44.731563Z","shell.execute_reply":"2021-08-04T14:37:44.737907Z"}}},{"cell_type":"code","source":"# Import Python libraries\nimport os\nimport glob\nimport shutil\nimport re\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport pydicom  as dicom\nimport cv2\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Input, Conv3D, MaxPool3D, GlobalAveragePooling3D, Dense, Dropout, BatchNormalization, concatenate\nfrom tensorflow.keras.models import Model\nfrom keras.callbacks import  ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils.vis_utils import plot_model\nfrom keras.metrics import AUC\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-11T09:14:55.78536Z","iopub.execute_input":"2021-09-11T09:14:55.785697Z","iopub.status.idle":"2021-09-11T09:15:01.159788Z","shell.execute_reply.started":"2021-09-11T09:14:55.785626Z","shell.execute_reply":"2021-09-11T09:15:01.158902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_1_1\">Submission sample & train.csv</span>\n\nWe will already look at the exemple **submission file** to see exactly what we need to predict in the end.","metadata":{}},{"cell_type":"code","source":"# Paths of the dataset\ninput_path = \"../input/rsna-miccai-brain-tumor-radiogenomic-classification/\"\nsample_file = \"sample_submission.csv\"\n\n# Load the submission sample dataset\nsample_submission = pd.read_csv(\n    input_path + sample_file)\nsample_submission.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:15:01.161422Z","iopub.execute_input":"2021-09-11T09:15:01.161779Z","iopub.status.idle":"2021-09-11T09:15:01.193928Z","shell.execute_reply.started":"2021-09-11T09:15:01.161742Z","shell.execute_reply":"2021-09-11T09:15:01.193218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of the subjects in this dataset appear to have a brain tumor. MGMT_Class = 0 refers to people who do not have the MGMT promoter methylation. MGMT_Class = 1 appears to be someone who has the MGMT promoter methylation. \n\nIt is a competition which gives **the probability of it in `MGMT_value`** feature. `BraTS21ID` is the patient's identification.\n\nNow, let's have a look to **train.csv** file :","metadata":{}},{"cell_type":"code","source":"train_labels_file = \"train_labels.csv\"\ntrain_labels = pd.read_csv(\n    input_path + train_labels_file)\ntrain_labels.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:15:01.196748Z","iopub.execute_input":"2021-09-11T09:15:01.197009Z","iopub.status.idle":"2021-09-11T09:15:01.211926Z","shell.execute_reply.started":"2021-09-11T09:15:01.196984Z","shell.execute_reply":"2021-09-11T09:15:01.211099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the data structure is the same as for the submission file, knowing that here **MGMT_value is indeed equal to 0 or 1 and no longer a probability**.\n\nLet's look at the distribution of the values of this variable in the train set :","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nfig = plt.figure(figsize=(8,6))\n# Countplot with Seaborn\nax = sns.countplot(data=train_labels,\n                   x=\"MGMT_value\")\n# Annotating bars\nfor p in ax.patches:\n    ax.annotate(\n        format(p.get_height(), '.0f'), \n               (p.get_x() + p.get_width() / 2., p.get_height()),\n        ha = 'center', va = 'center', \n        xytext = (0, 10), \n        textcoords = 'offset points')\n\nsns.despine(left=True, bottom=True)\nplt.title(\"MGMT value distribution in train labels\\n\",\n          fontsize=18, color=\"#0b0a2d\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:15:01.21474Z","iopub.execute_input":"2021-09-11T09:15:01.214981Z","iopub.status.idle":"2021-09-11T09:15:01.598661Z","shell.execute_reply.started":"2021-09-11T09:15:01.214955Z","shell.execute_reply":"2021-09-11T09:15:01.597819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution is almost equal between class 0 and class 1. The presence of MGMT is slightly higher. A total of **585 patients are tested in the train set**.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_1_2\">MRI train data</span>\n\nTrain data contains one record per patient. For each patient, four sub-files are available *(FLAIR, T1w, T1wCE and T2w)* in which the MRI image sequences are distributed.\n\n![data_structure](http://www.mf-data-science.fr/images/projects/data_structure.jpg)\n\nWe are going to take a look at what an MRI image looks like :","metadata":{}},{"cell_type":"code","source":"def plot_examples(row = 0, cat = 'FLAIR'):\n    '''\n    This function allows to display the MRI images of a \n    category entered in parameter for a given patient. \n    5 Random images are displayed simultaneously.\n    ***************************************************\n    PARAMETERS\n    ***************************************************\n    - row : integer\n        Line number corresponding to a patient \n        in the train_labels.csv file\n    - cat : string\n        MR sequence types to display. Can be in :\n        * FLAIR\n        * T1w\n        * T1wCE\n        * T2w\n    '''\n    folder = str(train_labels.loc[row, 'BraTS21ID']).zfill(5)\n    path_file = ''.join([input_path, 'train/', folder, '/', cat, '/'])\n    images = os.listdir(path_file)\n    \n    fig, axs = plt.subplots(1, 5, figsize=(30, 10))\n    fig.subplots_adjust(hspace = .2, wspace=.2)\n    axs = axs.ravel()\n    \n    for num in range(5):\n        data_file = dicom.dcmread(path_file+images[num])\n        img = data_file.pixel_array\n        axs[num].imshow(img, cmap='gray')\n        axs[num].set_title(cat+' '+images[num])\n        axs[num].set_xticklabels([])\n        axs[num].set_yticklabels([])\n        axs[num].grid(False)\n    \n    plt.suptitle(\"MRI \"+cat+\" Scan for patient \"+folder,\n                 fontsize=18, color=\"#0b0a2d\",\n                 x=.5, y=.8)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:15:01.599907Z","iopub.execute_input":"2021-09-11T09:15:01.600361Z","iopub.status.idle":"2021-09-11T09:15:01.611873Z","shell.execute_reply.started":"2021-09-11T09:15:01.600325Z","shell.execute_reply":"2021-09-11T09:15:01.611124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of FLAIR scans\nrow = 3\nplot_examples(row = row, cat = 'FLAIR')","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:15:01.613313Z","iopub.execute_input":"2021-09-11T09:15:01.613967Z","iopub.status.idle":"2021-09-11T09:15:02.511388Z","shell.execute_reply.started":"2021-09-11T09:15:01.613817Z","shell.execute_reply":"2021-09-11T09:15:02.510567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of T1wCE scans\nrow = 12\nplot_examples(row = row, cat = 'T1wCE')","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:15:02.512469Z","iopub.execute_input":"2021-09-11T09:15:02.512815Z","iopub.status.idle":"2021-09-11T09:15:03.297702Z","shell.execute_reply.started":"2021-09-11T09:15:02.512776Z","shell.execute_reply":"2021-09-11T09:15:03.296741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To better understand the representation of these MRI images, we can also **create an animation to visualize the sequence of images** of a certain category for a given patient.","metadata":{}},{"cell_type":"code","source":"rc('animation', html='jshtml')\n\ndef create_animation(row = 0, cat = 'FLAIR'):\n    '''\n    This function returns an animation of the MRI images \n    of a category entered as a parameter for a given patient.\n    ***************************************************\n    PARAMETERS\n    ***************************************************\n    - row : integer\n        Line number corresponding to a patient \n        in the train_labels.csv file\n    - cat : string\n        MR sequence types to display. Can be in :\n        * FLAIR\n        * T1w\n        * T1wCE\n        * T2w\n    '''\n    folder = str(train_labels.loc[row, 'BraTS21ID']).zfill(5)\n    path_file = ''.join([input_path, 'train/', folder, '/', cat])\n    t_paths = sorted(\n        glob.glob(os.path.join(path_file, \"*\")), \n        key=lambda x: int(x[:-4].split(\"-\")[-1]),\n    )\n    images = []\n    for filename in t_paths:\n        data_file = dicom.dcmread(filename)\n        data = data_file.pixel_array\n        data = data - np.min(data)\n        if np.max(data) != 0:\n            data = data / np.max(data)\n        data = (data * 255).astype(np.uint8)\n        if data.max() == 0:\n            continue\n        images.append(data)\n    \n    fig = plt.figure(figsize=(8, 8))\n    plt.axis('off')\n    im = plt.imshow(images[0], cmap=\"gray\", animated=True)\n\n    def animate_func(i):\n        im.set_array(images[i])\n        return [im]\n\n    animated = animation.FuncAnimation(\n        fig, animate_func, \n        frames = len(images), \n        interval = 1000//24)\n    return animated","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:15:03.30039Z","iopub.execute_input":"2021-09-11T09:15:03.300758Z","iopub.status.idle":"2021-09-11T09:15:03.311039Z","shell.execute_reply.started":"2021-09-11T09:15:03.300721Z","shell.execute_reply":"2021-09-11T09:15:03.310142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ani_1 = create_animation(row = 3, cat = 'FLAIR')","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-11T09:15:03.313285Z","iopub.execute_input":"2021-09-11T09:15:03.313746Z","iopub.status.idle":"2021-09-11T09:15:06.960803Z","shell.execute_reply.started":"2021-09-11T09:15:03.313708Z","shell.execute_reply":"2021-09-11T09:15:06.959891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ani_1","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:15:06.962064Z","iopub.execute_input":"2021-09-11T09:15:06.962384Z","iopub.status.idle":"2021-09-11T09:15:29.909457Z","shell.execute_reply.started":"2021-09-11T09:15:06.962349Z","shell.execute_reply":"2021-09-11T09:15:29.90624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are now going to check the number of DCM files to check if their number is the same for each category and for each patient. For that, we will complete a copy of train.csv with the calculated informations:","metadata":{}},{"cell_type":"code","source":"scan_categories = [\"FLAIR\",\"T1w\",\"T1wCE\",\"T2w\"]\ntrain_dataset = train_labels.copy()\n\nfor scan in scan_categories:\n    train_dataset[scan + \"_count\"] = [len(os.listdir(input_path + \"train/\" \n                                                      + str(p).zfill(5) \n                                                      + \"/\" + scan))\n                                       for p in train_dataset.BraTS21ID]","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:15:29.910817Z","iopub.execute_input":"2021-09-11T09:15:29.911266Z","iopub.status.idle":"2021-09-11T09:16:37.585484Z","shell.execute_reply.started":"2021-09-11T09:15:29.911222Z","shell.execute_reply":"2021-09-11T09:16:37.584641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (25,40))\nfor i, scan in enumerate(scan_categories):\n    ax = plt.subplot(4,1,i+1)\n    plt.xticks(rotation=70)\n    sns.countplot(x=train_dataset[scan + \"_count\"], ax=ax)\n    ax.set_title(\"Distribution of number of DCM file in {} scans\".format(scan),\n             fontsize=18, color=\"#0b0a2d\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:16:37.586763Z","iopub.execute_input":"2021-09-11T09:16:37.587076Z","iopub.status.idle":"2021-09-11T09:16:46.023881Z","shell.execute_reply.started":"2021-09-11T09:16:37.587041Z","shell.execute_reply":"2021-09-11T09:16:46.022998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that some values for each scan category are over-represented. On the other hand, the span ranges of the counters are important. This may be due, for example, to the use of different X-ray machines ...\n\nHowever, if we only consider patients with maximum values, the amount of data available may be too low for a complex machine learning algorithm.","metadata":{}},{"cell_type":"code","source":"train_dataset[(train_dataset[\"FLAIR_count\"] == int(train_dataset[\"FLAIR_count\"].mode()))\n              & (train_dataset[\"T1w_count\"] == int(train_dataset[\"T1w_count\"].mode()))\n              & (train_dataset[\"T1wCE_count\"] == int(train_dataset[\"T1wCE_count\"].mode()))\n              & (train_dataset[\"T2w_count\"] == int(train_dataset[\"T2w_count\"].mode()))]","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:16:46.025242Z","iopub.execute_input":"2021-09-11T09:16:46.025636Z","iopub.status.idle":"2021-09-11T09:16:46.058311Z","shell.execute_reply.started":"2021-09-11T09:16:46.025598Z","shell.execute_reply":"2021-09-11T09:16:46.057602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are only 151 patients whose scans contain the maximum of DCM images out of the 585 at the start. We therefore keep the entire dataset for the moment. Lets have a look to the **test dataset** :","metadata":{}},{"cell_type":"code","source":"test_dataset = pd.DataFrame(columns=[\"BraTS21ID\",\n                                     \"FLAIR_count\",\"T1w_count\",\n                                     \"T1wCE_count\",\"T2w_count\"])\ntest_dataset[\"BraTS21ID\"] = os.listdir(input_path + \"test/\")\nfor scan in scan_categories:\n    test_dataset[scan + \"_count\"] = [len(os.listdir(input_path + \"test/\" \n                                                      + str(p).zfill(5) \n                                                      + \"/\" + scan))\n                                       for p in test_dataset.BraTS21ID]","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:16:46.059353Z","iopub.execute_input":"2021-09-11T09:16:46.059688Z","iopub.status.idle":"2021-09-11T09:16:56.784269Z","shell.execute_reply.started":"2021-09-11T09:16:46.059662Z","shell.execute_reply":"2021-09-11T09:16:56.783439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:16:56.785642Z","iopub.execute_input":"2021-09-11T09:16:56.785996Z","iopub.status.idle":"2021-09-11T09:16:56.799077Z","shell.execute_reply.started":"2021-09-11T09:16:56.78596Z","shell.execute_reply":"2021-09-11T09:16:56.798373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize = (25,40))\nfor i, scan in enumerate(scan_categories):\n    ax = plt.subplot(4,1,i+1)\n    plt.xticks(rotation=70)\n    sns.countplot(x=test_dataset[scan + \"_count\"], ax=ax)\n    ax.set_title(\"Distribution of number of DCM file in TEST {} scans\".format(scan),\n             fontsize=18, color=\"#0b0a2d\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:16:56.800386Z","iopub.execute_input":"2021-09-11T09:16:56.800973Z","iopub.status.idle":"2021-09-11T09:16:58.979374Z","shell.execute_reply.started":"2021-09-11T09:16:56.800925Z","shell.execute_reply":"2021-09-11T09:16:58.978416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_1_3\">Data cleaning</span>\n\nIn the animation of the scans projected above, we notice that a certain number of images at the beginning or at the end of the sequence has a lot of black area.\n\nThese images will therefore be useless in our models and may even cause over-training.\n**When creating the image sequences, we will therefore start from the central image of each folder and we will then take the same number of images upstream and downstream**.\n\nLet's take an example from a single image :","metadata":{}},{"cell_type":"code","source":"# Path to sample image\nsample_img_path = ''.join([input_path, 'train/00005/FLAIR/Image-80.dcm'])\nsample_img = dicom.dcmread(sample_img_path)\nsample_img = sample_img.pixel_array\nplt.imshow(sample_img)\nprint(\"% of colored pixels : {:.2f}\".format(\n    np.sum(np.where(sample_img!=0,1,0)\n           /(sample_img.shape[0]*sample_img.shape[1]))*100))","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:16:58.980846Z","iopub.execute_input":"2021-09-11T09:16:58.98119Z","iopub.status.idle":"2021-09-11T09:16:59.198313Z","shell.execute_reply.started":"2021-09-11T09:16:58.981155Z","shell.execute_reply":"2021-09-11T09:16:59.197566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this example, only 7% of the image has colored pixels.      \nWe will now look at the distribution of the **colorization rates of the images in the full FLAIR folder** :","metadata":{}},{"cell_type":"code","source":"img_colored = []\ng_path = ''.join([input_path, 'train/00005/FLAIR/'])\nfor imgs in os.listdir(g_path):\n    img_path = ''.join([g_path, imgs])\n    temp_img = dicom.dcmread(img_path)\n    temp_img = temp_img.pixel_array\n    colored_zone = round(np.sum(np.where(temp_img!=0,1,0)\n                           /(temp_img.shape[0]*temp_img.shape[1])),2)\n    img_colored.append(colored_zone)\n\nfig = plt.figure(figsize=(12,8))\nsns.countplot(x=img_colored)\nplt.xlabel(\"Rate of colored pixels\")\nplt.xticks(rotation=70)\nplt.title(\"Colored pixel rate distribution in 00005 FLAIR folder\\n\",\n          fontsize=18, color=\"#0b0a2d\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:16:59.200913Z","iopub.execute_input":"2021-09-11T09:16:59.201182Z","iopub.status.idle":"2021-09-11T09:17:00.825198Z","shell.execute_reply.started":"2021-09-11T09:16:59.201156Z","shell.execute_reply":"2021-09-11T09:17:00.824404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It can be seen that the majority of the images are completely black.\n\n# <span style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\" id=\"section_2\">Preprocessing</span>\nWe will be using several preprocessing techniques on our images.\n- Crop images to reduce black areas.\n- Resize images.\n- Application of a denoising filter.\n\n**We will not apply image equalization** as the different types of scans already have voluntary contrast variations.\n\n## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_2_1\">Crop and resize the images</span>","metadata":{}},{"cell_type":"code","source":"def crop_resize_img(img, scale=1.0, dim=(244,244)): \n    '''\n    Crop function to keep a central part of the 2D image \n    according to a defined scale and resize it to \n    specific dimension.\n    ****************************************************\n    PARAMETERS\n    ****************************************************\n    - img : 2D array\n        2D array of pixels in the image\n    - scale : float\n        Desired scale of the cropped image\n    - dim : Tuple\n        Tuple of integer with desired final width, height\n    '''\n    # Crop image\n    center_x, center_y = img.shape[1] / 2, img.shape[0] / 2\n    width_scaled, height_scaled = img.shape[1] * scale, img.shape[0] * scale\n    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n    img_cropped = img[int(top_y):int(bottom_y), int(left_x):int(right_x)]\n    \n    # Resize\n    img_cropped = cv2.resize(img_cropped, dim, interpolation = cv2.INTER_AREA)\n    return img_cropped","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:00.826378Z","iopub.execute_input":"2021-09-11T09:17:00.826877Z","iopub.status.idle":"2021-09-11T09:17:00.834156Z","shell.execute_reply.started":"2021-09-11T09:17:00.826839Z","shell.execute_reply":"2021-09-11T09:17:00.83329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\nax = plt.subplot(1,2,1)\nax.imshow(sample_img)\nax.set_title(\"Original image\")\nax1 = plt.subplot(1,2,2)\nimg_cropped = crop_resize_img(sample_img, 0.7, (244,244))\nax1.imshow(img_cropped)\nax1.set_title(\"Cropped (244,244) image with scale = 0.7\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:00.835328Z","iopub.execute_input":"2021-09-11T09:17:00.83586Z","iopub.status.idle":"2021-09-11T09:17:01.174848Z","shell.execute_reply.started":"2021-09-11T09:17:00.835799Z","shell.execute_reply":"2021-09-11T09:17:01.173891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_2_4\">Global preprocessing function</span>\n\nWe can now create a global preprocessing function that will be applied to our MRI images before entering the neural network models. This function will take over the various treatments seen previously.","metadata":{}},{"cell_type":"code","source":"def mri_preprocessor(\n    img, scale=.8, \n    dim=(240,240)):\n    \n    # Apply all preprocess\n    x = crop_resize_img(img,scale,dim)\n    \n    return x","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:01.176179Z","iopub.execute_input":"2021-09-11T09:17:01.176529Z","iopub.status.idle":"2021-09-11T09:17:01.181047Z","shell.execute_reply.started":"2021-09-11T09:17:01.176495Z","shell.execute_reply":"2021-09-11T09:17:01.180203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\nax = plt.subplot(1,2,1)\nax.imshow(sample_img)\nax.set_title(\"Original image\")\nax1 = plt.subplot(1,2,2)\nimg_pre = mri_preprocessor(sample_img, \n                           scale=.8, \n                           dim=(240,240))\nax1.imshow(img_pre)\nax1.set_title(\"Preprocess image\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:01.18237Z","iopub.execute_input":"2021-09-11T09:17:01.182955Z","iopub.status.idle":"2021-09-11T09:17:01.512658Z","shell.execute_reply.started":"2021-09-11T09:17:01.182919Z","shell.execute_reply":"2021-09-11T09:17:01.511724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\" id=\"section_3\">Development of supervised models</span>\n\nWe will test several models of neural networks and test the main metrics to determine the best model.\n\n- **CNN** from scratch (Baseline).\n- **Learning transfer**.\n- **Fine Tuning**.\n\nThe metrics tested will be the **accuracy** in train and validation.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_3_1\">Multimodal inputs CNN from scratch</span>\n\nAs part of this modeling, it is necessary to develop the models on each of the 4 types of scans available to patients:\n\n- Fluid Attenuated Inversion Recovery (FLAIR).\n- T1-weighted pre-contrast (T1w).\n- T1-weighted post-contrast (T1Gd).\n- T2-weighted (T2).\n\nAs a reminder, these models are stored in the variable:\n```Python\nscan_categories = [\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\"]\n```\n\nThe arrays of the patient images will be loaded into the variable X, the labels of MGMT_value into the variable y. To obtain a binary classification, we will use a global **sigmoid activation layer** to the results of our 4 models *(classifier layer)*.\n\nWe will test the models with **a sequence of images** *(as for a video)* by taking the **middle 24 images** *(not entirely black)* of each patient for each type of scan :","metadata":{}},{"cell_type":"code","source":"# Initial parameters\nIMAGE_SIZE = 120\nNUM_IMAGES = 24\nBATCH_SIZE = 4\n\nnum_folds = 5\nSelected_fold = 1 #1,2,3,4,5 ","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:01.516466Z","iopub.execute_input":"2021-09-11T09:17:01.516806Z","iopub.status.idle":"2021-09-11T09:17:01.520976Z","shell.execute_reply.started":"2021-09-11T09:17:01.51677Z","shell.execute_reply":"2021-09-11T09:17:01.519946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_labels\ntrain_df['BraTS21ID5'] = [format(x, '05d') for x in train_df.BraTS21ID]\ntrain_df[\"Fold\"]=\"train\"\ntrain_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:01.522881Z","iopub.execute_input":"2021-09-11T09:17:01.523263Z","iopub.status.idle":"2021-09-11T09:17:01.539226Z","shell.execute_reply.started":"2021-09-11T09:17:01.523227Z","shell.execute_reply":"2021-09-11T09:17:01.538469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = sample_submission\ntest['BraTS21ID5'] = [format(x, '05d') for x in test.BraTS21ID]\ntest.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:01.541001Z","iopub.execute_input":"2021-09-11T09:17:01.541336Z","iopub.status.idle":"2021-09-11T09:17:01.552753Z","shell.execute_reply.started":"2021-09-11T09:17:01.541298Z","shell.execute_reply":"2021-09-11T09:17:01.551707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_3_2\">Define loaders for images sequences 4 MRI types</span>","metadata":{}},{"cell_type":"code","source":"def load_dicom_image(path, img_size=IMAGE_SIZE, preproc=True):\n    data = dicom.read_file(path)\n    data = data.pixel_array\n        \n    if (preproc==True):\n        data = mri_preprocessor(\n            data,\n            scale=.8,\n            dim=(img_size,img_size))\n            \n    else:    \n        data = cv2.resize(data, (img_size, img_size))\n    \n    return data","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:01.554388Z","iopub.execute_input":"2021-09-11T09:17:01.554796Z","iopub.status.idle":"2021-09-11T09:17:01.561607Z","shell.execute_reply.started":"2021-09-11T09:17:01.554747Z","shell.execute_reply":"2021-09-11T09:17:01.560498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_img = dicom.read_file(\n    input_path+\"train/00046/FLAIR/Image-90.dcm\").pixel_array\npreproc_img = load_dicom_image(input_path+\"train/00046/FLAIR/Image-90.dcm\",\n                               preproc=True)\n\nfig = plt.figure(figsize=(12,8))\nax1 = plt.subplot(1,2,1)\nax1.imshow(sample_img, cmap=\"gray\")\nax1.set_title(f\"Original image shape = {sample_img.shape}\")\nax2 = plt.subplot(1,2,2)\nax2.imshow(preproc_img, cmap=\"gray\")\nax2.set_title(f\"Preproc image shape = {preproc_img.shape}\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:01.56313Z","iopub.execute_input":"2021-09-11T09:17:01.563635Z","iopub.status.idle":"2021-09-11T09:17:01.915091Z","shell.execute_reply.started":"2021-09-11T09:17:01.563594Z","shell.execute_reply":"2021-09-11T09:17:01.914317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dicom_images_3d(scan_id, mri_type=\"FLAIR\", num_imgs=NUM_IMAGES, img_size=IMAGE_SIZE, split=\"train\"):\n    \n        files = sorted(glob.glob(f\"{input_path}{split}/{scan_id}/{mri_type}/*.dcm\"), \n                   key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n\n        middle = len(files)//2\n        num_imgs2 = num_imgs//2\n        p1 = max(0, middle - num_imgs2)\n        p2 = min(len(files), middle + num_imgs2)\n        img3d = np.stack([load_dicom_image(f) for f in files[p1:p2]], axis=-1) \n        if img3d.shape[-1] < num_imgs:\n            n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n            img3d = np.concatenate((img3d,  n_zero), axis = -1)\n\n        if np.min(img3d) < np.max(img3d):\n            img3d = img3d - np.min(img3d)\n            img3d = img3d / np.max(img3d)\n            \n        return img3d","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:01.916321Z","iopub.execute_input":"2021-09-11T09:17:01.916836Z","iopub.status.idle":"2021-09-11T09:17:01.925753Z","shell.execute_reply.started":"2021-09-11T09:17:01.916797Z","shell.execute_reply":"2021-09-11T09:17:01.924943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = load_dicom_images_3d(\"00046\")\nprint(np.array(a).shape)\nimage = a[:, :, 12]\nprint(\"Dimension of the CT scan is:\", image.shape)\nplt.imshow(np.squeeze(image), cmap=\"gray\")","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:01.926727Z","iopub.execute_input":"2021-09-11T09:17:01.927168Z","iopub.status.idle":"2021-09-11T09:17:02.479732Z","shell.execute_reply.started":"2021-09-11T09:17:01.927131Z","shell.execute_reply":"2021-09-11T09:17:02.478061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks to this function, we obtain **4 ordered sequences *(one for each Scan type)* of 24 images of dimension 120 x 120** and the preprocessing has been applied.\n\nTo couple our four types of scans, we will use a **multi-modal approach** to create our model. We will integrate 4 different inputs for a single final classifier.","metadata":{}},{"cell_type":"markdown","source":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_3_3\">Define folds</span>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold,StratifiedKFold\nsfolder = StratifiedKFold(n_splits=5,random_state=13,shuffle=True)\nX = train_df[['BraTS21ID']]\ny = train_df[['MGMT_value']]\n\nfold_no = 1\nfor train, valid in sfolder.split(X,y):\n    if fold_no==Selected_fold:\n        train_df.loc[valid, \"Fold\"] = \"valid\"\n    fold_no += 1","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:02.483623Z","iopub.execute_input":"2021-09-11T09:17:02.483997Z","iopub.status.idle":"2021-09-11T09:17:02.504578Z","shell.execute_reply.started":"2021-09-11T09:17:02.483957Z","shell.execute_reply":"2021-09-11T09:17:02.503389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train=train_df[train_df.Fold==\"train\"]\ndf_valid=train_df[train_df.Fold==\"valid\"].iloc[:-1,:]\nprint(\"df_train=\",len(df_train),\"-- df_valid=\",len(df_valid))","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:02.506321Z","iopub.execute_input":"2021-09-11T09:17:02.506785Z","iopub.status.idle":"2021-09-11T09:17:02.524333Z","shell.execute_reply.started":"2021-09-11T09:17:02.50674Z","shell.execute_reply":"2021-09-11T09:17:02.522884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_3_4\">Keras custom data generator</span>\nThanks to the Sequence module of the Keras library, we are going to create a personalized image generator. This will prevent us from creating Numpy arrays or Tensors containing all the sequences which would quickly overload the memory.","metadata":{}},{"cell_type":"code","source":"from keras.utils import Sequence\nclass Dataset(Sequence):\n    def __init__(self,df,is_train=True,batch_size=BATCH_SIZE,shuffle=False):\n        self.idx = df[\"BraTS21ID\"].values\n        self.paths = df[\"BraTS21ID5\"].values\n        self.y =  df[\"MGMT_value\"].values\n        self.is_train = is_train\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n    def __len__(self):\n        return math.ceil(len(self.idx)/self.batch_size)\n   \n    def __getitem__(self,ids):\n        id_path= self.paths[ids]\n        batch_paths = self.paths[ids * self.batch_size:(ids + 1) * self.batch_size]\n        \n        if self.y is not None:\n            batch_y = self.y[ids * self.batch_size: (ids + 1) * self.batch_size]\n        \n        if self.is_train:\n            self.full_X_mri = []\n            for mri_type in scan_categories:\n                list_x =  [load_dicom_images_3d(x,split=\"train\",mri_type=mri_type) for x in batch_paths]\n                batch_X = np.stack(list_x, axis=0)\n                self.full_X_mri.append(np.array(batch_X))\n            return self.full_X_mri,np.array(batch_y)\n        else:\n            self.full_X_mri = []\n            for mri_type in scan_categories:\n                list_x =  load_dicom_images_3d(id_path,split=\"test\",mri_type=mri_type)\n                batch_X = np.stack(np.expand_dims(list_x, axis=0))\n                self.full_X_mri.append(np.array(batch_X))\n            return self.full_X_mri\n    \n    def on_epoch_end(self):\n        if self.shuffle and self.is_train:\n            ids_y = list(zip(self.idx, self.y))\n            tf.random_shuffle(ids_y)\n            self.idx, self.y = list(zip(*ids_y))","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:02.527492Z","iopub.execute_input":"2021-09-11T09:17:02.528762Z","iopub.status.idle":"2021-09-11T09:17:02.554285Z","shell.execute_reply.started":"2021-09-11T09:17:02.528693Z","shell.execute_reply":"2021-09-11T09:17:02.553021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset(df_train,batch_size=BATCH_SIZE)\nvalid_dataset = Dataset(df_valid,batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:02.559875Z","iopub.execute_input":"2021-09-11T09:17:02.560359Z","iopub.status.idle":"2021-09-11T09:17:02.570144Z","shell.execute_reply.started":"2021-09-11T09:17:02.56032Z","shell.execute_reply":"2021-09-11T09:17:02.568874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once the generators are created, we can project an image to check:","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,8))\n\nfor i in range(1):\n    images, label = train_dataset[i]\n    print(\"Number of scan type is:\", len(images))\n    print(\"Dimension of the CT FLAIR scan is:\", images[1].shape)\n    print(\"label=\",label)\n    print(\"\\n\")\n    \n    ax1 = plt.subplot(1,4,1)\n    ax1.imshow(images[0][1][:,:,15], cmap=\"gray\")\n    ax1.set_title(\"FLAIR, 15th image\")\n    ax2 = plt.subplot(1,4,2)\n    ax2.imshow(images[1][1][:,:,15], cmap=\"gray\")\n    ax2.set_title(\"T1w, 15th image\")\n    ax3 = plt.subplot(1,4,3)\n    ax3.imshow(images[2][1][:,:,15], cmap=\"gray\")\n    ax3.set_title(\"T1wCE, 15th image\")\n    ax4 = plt.subplot(1,4,4)\n    ax4.imshow(images[3][1][:,:,15], cmap=\"gray\")\n    ax4.set_title(\"T2w, 15th image\")","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:02.57547Z","iopub.execute_input":"2021-09-11T09:17:02.575853Z","iopub.status.idle":"2021-09-11T09:17:06.888215Z","shell.execute_reply.started":"2021-09-11T09:17:02.575817Z","shell.execute_reply":"2021-09-11T09:17:06.887435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span style=\"color:#3c99dc; font-size:18px; text-transform: uppercase; font-weight:bold\" id=\"section_3_5\">Define CNN Multi-inputs model</span>","metadata":{}},{"cell_type":"code","source":"def get_model(inputs):\n    \"\"\"Build a 3D convolutional neural network model.\"\"\"\n     \n    x = Conv3D(filters=16, kernel_size=3, activation=\"relu\", padding=\"same\")(inputs)\n    x = MaxPool3D(pool_size=2)(x)\n    x = BatchNormalization()(x)\n    \n    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n    x = MaxPool3D(pool_size=2)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.1)(x)\n    \n    x = Conv3D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n    x = MaxPool3D(pool_size=2)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.1)(x)\n    \n    x = Conv3D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\")(x)\n    x = MaxPool3D(pool_size=2)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n\n    return x","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:24:10.234886Z","iopub.execute_input":"2021-09-11T09:24:10.235589Z","iopub.status.idle":"2021-09-11T09:24:10.243555Z","shell.execute_reply.started":"2021-09-11T09:24:10.235548Z","shell.execute_reply":"2021-09-11T09:24:10.242624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the four modal input\nFLAIR_input = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, NUM_IMAGES, 1))\nFLAIR_model = get_model(FLAIR_input)\n\nT1w_input = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, NUM_IMAGES, 1))\nT1w_model = get_model(T1w_input)\n\nT1wCE_input = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, NUM_IMAGES, 1))\nT1wCE_model = get_model(T1wCE_input)\n\nT2w_input = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, NUM_IMAGES, 1))\nT2w_model = get_model(T2w_input)\n\n# Concat models and add final block\ncnn = concatenate(\n    [FLAIR_model, T1w_model, T1wCE_model, T2w_model])\n\ncnn = GlobalAveragePooling3D()(cnn)\ncnn = Dense(units=256, activation=\"relu\")(cnn)\ncnn = Dropout(0.3)(cnn)\n\n# Output layer\noutput_layer = Dense(units = 1, activation = 'sigmoid')(cnn)\n\n# Final model\ncnn_model = Model(inputs=[FLAIR_input, \n                          T1w_input, \n                          T1wCE_input, \n                          T2w_input], \n                  outputs=[output_layer],\n                  name=\"multi3dcnn\")\n\n# Compile final model\ncnn_model.compile(loss='binary_crossentropy',\n                  optimizer=keras.optimizers.Adam(learning_rate=0.001),\n                  metrics=[\"accuracy\"])\n\n# Define callbacks.\nmodel_save = ModelCheckpoint(f'Brain_3d_multimodal.h5', \n                             save_best_only = True, \n                             monitor = 'val_loss', \n                             mode = 'min', verbose = 1)\nearly_stop = EarlyStopping(monitor = 'val_loss', \n                           patience = 10, mode = 'min', verbose = 1,\n                           restore_best_weights = True)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:24:22.718839Z","iopub.execute_input":"2021-09-11T09:24:22.71915Z","iopub.status.idle":"2021-09-11T09:24:23.071456Z","shell.execute_reply.started":"2021-09-11T09:24:22.719119Z","shell.execute_reply":"2021-09-11T09:24:23.070656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot model diagram\nplot_model(cnn_model, show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-11T09:17:09.196031Z","iopub.status.idle":"2021-09-11T09:17:09.196406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we **train the multi-input model** with the network defined above:","metadata":{}},{"cell_type":"code","source":"#tf.config.experimental_run_functions_eagerly(True)\nepochs = 50\ncnn_model.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=epochs, \n    shuffle=False,\n    #verbose=2,\n    callbacks=[model_save, early_stop])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-09-11T09:17:09.197455Z","iopub.status.idle":"2021-09-11T09:17:09.198042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(20, 7))\nax = ax.ravel()\n\nfor i, metric in enumerate([\"accuracy\",\"loss\"]):\n    ax[i].plot(cnn_model.history.history[metric])\n    ax[i].plot(cnn_model.history.history[\"val_\" + metric])\n    ax[i].set_title(\"Model {}\".format(metric))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(metric)\n    ax[i].legend([\"train\", \"val\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-08T11:52:50.967844Z","iopub.execute_input":"2021-09-08T11:52:50.96852Z","iopub.status.idle":"2021-09-08T11:52:51.44271Z","shell.execute_reply.started":"2021-09-08T11:52:50.968479Z","shell.execute_reply":"2021-09-08T11:52:51.441736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\" id=\"section_4\">Test of trained final model</span>\n\nThe best model was saved during training. We will therefore load it and test the predictions on the file and the submission images.","metadata":{}},{"cell_type":"code","source":"test_df = sample_submission.copy()\ntest_df['BraTS21ID5'] = [format(x, '05d') for x in test_df.BraTS21ID]","metadata":{"execution":{"iopub.status.busy":"2021-09-08T11:52:51.444086Z","iopub.execute_input":"2021-09-08T11:52:51.444423Z","iopub.status.idle":"2021-09-08T11:52:51.450333Z","shell.execute_reply.started":"2021-09-08T11:52:51.444384Z","shell.execute_reply":"2021-09-08T11:52:51.449526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = Dataset(test_df,is_train=False,batch_size=1)\nprint(len(test_dataset))\nprint(np.array(test_dataset).shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T12:46:47.361101Z","iopub.execute_input":"2021-09-08T12:46:47.361431Z","iopub.status.idle":"2021-09-08T12:48:08.229943Z","shell.execute_reply.started":"2021-09-08T12:46:47.3614Z","shell.execute_reply":"2021-09-08T12:48:08.228373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1):\n    images= test_dataset[i]\n    print(\"Number of scan type is:\", len(images))\n    print(\"Dimension of the CT FLAIR scan is:\", images[1].shape)\n    plt.imshow(images[1][0,:,:,15], cmap=\"gray\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T12:49:35.16711Z","iopub.execute_input":"2021-09-08T12:49:35.167425Z","iopub.status.idle":"2021-09-08T12:49:35.64963Z","shell.execute_reply.started":"2021-09-08T12:49:35.167394Z","shell.execute_reply":"2021-09-08T12:49:35.648844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = cnn_model.predict(test_dataset)\npreds = preds.reshape(-1)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T12:48:08.231431Z","iopub.execute_input":"2021-09-08T12:48:08.231784Z","iopub.status.idle":"2021-09-08T12:48:32.371564Z","shell.execute_reply.started":"2021-09-08T12:48:08.231732Z","shell.execute_reply":"2021-09-08T12:48:32.37069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(preds.shape)\nprint(preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'BraTS21ID':sample_submission['BraTS21ID'],'MGMT_value':preds})\nsubmission.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-08T12:53:30.930685Z","iopub.execute_input":"2021-09-08T12:53:30.931058Z","iopub.status.idle":"2021-09-08T12:53:30.944618Z","shell.execute_reply.started":"2021-09-08T12:53:30.931026Z","shell.execute_reply":"2021-09-08T12:53:30.943838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The submission.csv file will be used for the competition *(evaluation with AUC under ROC curve)*. We can also look at the **distribution of the predicted probabilities** :","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 8))\nplt.hist(submission[\"MGMT_value\"])\nplt.title(\"Predicted probabilites distribution on test set\", \n          fontsize=18, color=\"#0b0a2d\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-08T12:54:48.524425Z","iopub.execute_input":"2021-09-08T12:54:48.524753Z","iopub.status.idle":"2021-09-08T12:54:48.735077Z","shell.execute_reply.started":"2021-09-08T12:54:48.52472Z","shell.execute_reply":"2021-09-08T12:54:48.734091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#0b0a2d; font-size:24px; text-transform: uppercase; font-weight:bold\" id=\"section_5\">Try another approach: Transfer Learning</span>\n\nIn order to complete these models, we will try another approach using **Transfer Learning methods**. We will use a pre-trained deep model to detect the features *(like EfficientNet ...)* and an LSTM layer for the final classification on the matrices obtained. This approach is available in the Notebook :\n\n<span style=\"font-size:18px\">[üß†Brain Tumor - Transfert Learning MRI - All MRI](https://www.kaggle.com/michaelfumery/brain-tumor-transfert-learning-mri-all-mri/)</span>\n\n<span style=\"color:red; font-size:18px\">Don't forget to **upvote** if this Notebook helped you!</span>","metadata":{}}]}