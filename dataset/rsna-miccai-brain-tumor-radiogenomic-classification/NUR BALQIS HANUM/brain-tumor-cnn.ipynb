{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-23T04:56:39.528431Z","iopub.execute_input":"2022-06-23T04:56:39.528758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport cv2 as cv\nfrom path import Path\nimport os \nimport glob\nimport tensorflow_hub as hub\nimport os \nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df= pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv')\nsample_df = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dicom(path):\n    dicom=pydicom.read_file(path)\n    data=dicom.pixel_array\n    data=data-np.min(data)\n    if np.max(data) != 0:\n        data=data/np.max(data)\n    data=(data*255).astype(np.uint8)\n    return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def listdirs(folder):\n    return [d for d in os.listdir(folder) if os.path.isdir(os.path.join(folder, d))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir='../input/rsna-miccai-brain-tumor-radiogenomic-classification/train'\ntrainset=[]\ntrainlabel=[]\ntrainidt=[]\nfor i in tqdm(range(len(train_df))):\n    idt=train_df.loc[i,'BraTS21ID']\n    idt2=('00000'+str(idt))[-5:]\n    path=os.path.join(train_dir,idt2,'T1wCE')              \n    for im in os.listdir(path):\n        img=load_dicom(os.path.join(path,im)) \n        img=cv.resize(img,(64,64)) \n        image=img_to_array(img)\n        image=image/255.0\n        trainset+=[image]\n        trainlabel+=[train_df.loc[i,'MGMT_value']]\n        trainidt+=[idt]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dir='../input/rsna-miccai-brain-tumor-radiogenomic-classification/test'\ntestset=[]\ntestidt=[]\nfor i in tqdm(range(len(sample_df))):\n    idt=sample_df.loc[i,'BraTS21ID']\n    idt2=('00000'+str(idt))[-5:]\n    path=os.path.join(test_dir,idt2,'T1wCE')               \n    for im in os.listdir(path):   \n        img=load_dicom(os.path.join(path,im))\n        img=cv.resize(img,(64,64)) \n        image=img_to_array(img)\n        image=image/255.0\n        testset+=[image]\n        testidt+=[idt]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=np.array(trainlabel)\nY_train=to_categorical(y)\nX_train=np.array(trainset)\nX_test=np.array(testset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.models.Sequential()\nmodel.add(keras.layers.Conv2D(filters=64,kernel_size=(4,4),input_shape=(64,64,1),activation='relu',kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Conv2D(filters=64,kernel_size=(4,4),activation='relu',kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(keras.layers.Dropout(0.20))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Conv2D(filters=64,kernel_size=(4,4),activation='relu',kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(keras.layers.Dropout(0.25))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(100,activation=\"relu\",kernel_initializer=\"he_normal\"))\nmodel.add(keras.layers.Dense(2,\"softmax\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\",\n              optimizer = \"RMSprop\",metrics=[\"accuracy\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback = keras.callbacks.EarlyStopping(monitor='loss', patience=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = model.fit(X_train, Y_train,epochs=100, batch_size=64, verbose=1,callbacks=[callback])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_ac = hist.history['accuracy']\nget_los = hist.history['loss']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = range(len(get_ac))\nplt.plot(epochs, get_ac, 'g', label='Accuracy of Training data')\nplt.plot(epochs, get_los, 'r', label='Loss of Training data')\nplt.title('Training data accuracy and loss')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred=model.predict(X_test)\npred=np.argmax(y_pred,axis=1)\nresult=pd.DataFrame(testidt)\nresult[1]=pred\nresult.columns=['BraTS21ID','MGMT_value']\nresult2=result.groupby('BraTS21ID',as_index=False).mean()\nresult2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result2['BraTS21ID']=sample_df['BraTS21ID']\nresult2['MGMT_value']=result2['MGMT_value'].apply(lambda x:round(x*10)/10)\nresult2.to_csv('submission.csv',index=False)\nresult2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 5))\nplt.hist(submission[\"MGMT_value\"]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}