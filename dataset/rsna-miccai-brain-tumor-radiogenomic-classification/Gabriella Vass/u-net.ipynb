{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## EDA + Data Preprocessing - from Pratyush's notebook","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport os, glob, random, cv2, glob, pydicom","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:27.851201Z","iopub.execute_input":"2021-12-01T21:34:27.851854Z","iopub.status.idle":"2021-12-01T21:34:28.17413Z","shell.execute_reply.started":"2021-12-01T21:34:27.851814Z","shell.execute_reply":"2021-12-01T21:34:28.172346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:28.181377Z","iopub.execute_input":"2021-12-01T21:34:28.181917Z","iopub.status.idle":"2021-12-01T21:34:28.205941Z","shell.execute_reply.started":"2021-12-01T21:34:28.181876Z","shell.execute_reply":"2021-12-01T21:34:28.205233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:28.207268Z","iopub.execute_input":"2021-12-01T21:34:28.20759Z","iopub.status.idle":"2021-12-01T21:34:28.215024Z","shell.execute_reply.started":"2021-12-01T21:34:28.207554Z","shell.execute_reply":"2021-12-01T21:34:28.214075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:28.216729Z","iopub.execute_input":"2021-12-01T21:34:28.217065Z","iopub.status.idle":"2021-12-01T21:34:28.245802Z","shell.execute_reply.started":"2021-12-01T21:34:28.217032Z","shell.execute_reply":"2021-12-01T21:34:28.245004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.MGMT_value.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:28.248027Z","iopub.execute_input":"2021-12-01T21:34:28.251583Z","iopub.status.idle":"2021-12-01T21:34:28.267379Z","shell.execute_reply.started":"2021-12-01T21:34:28.251547Z","shell.execute_reply":"2021-12-01T21:34:28.266149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sample_path = '../input/rsna-miccai-brain-tumor-radiogenomic-classification/train'\nlen(os.listdir(train_sample_path)), df.BraTS21ID.nunique()","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:28.269116Z","iopub.execute_input":"2021-12-01T21:34:28.269787Z","iopub.status.idle":"2021-12-01T21:34:28.631996Z","shell.execute_reply.started":"2021-12-01T21:34:28.269748Z","shell.execute_reply":"2021-12-01T21:34:28.631254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_dicom_xray(path):\n    data = pydicom.read_file(path).pixel_array\n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:28.633181Z","iopub.execute_input":"2021-12-01T21:34:28.633861Z","iopub.status.idle":"2021-12-01T21:34:28.639783Z","shell.execute_reply.started":"2021-12-01T21:34:28.633816Z","shell.execute_reply":"2021-12-01T21:34:28.638916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# following function took from: https://www.kaggle.com/ihelon/brain-tumor-eda-with-animations-and-modeling?scriptVersionId=68202876&cellId=11\ndef visualize_sample(\n    brats21id, \n    mgmt_value,\n    slice_i,\n    types=(\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\")\n):\n    plt.figure(figsize=(16, 5))\n    patient_path = os.path.join(\n        train_sample_path, \n        str(brats21id).zfill(5),\n    )\n    for i, t in enumerate(types, 1):\n        t_paths = sorted(\n            glob.glob(os.path.join(patient_path, t, \"*\")), \n            key=lambda x: int(x[:-4].split(\"-\")[-1]),\n        )\n        data = read_dicom_xray(t_paths[int(len(t_paths) * slice_i)])\n        plt.subplot(1, 4, i)\n        plt.imshow(data, cmap=\"gray\")\n        plt.title(f\"{t}\", fontsize=16)\n        plt.axis(\"off\")\n\n    plt.suptitle(f\"MGMT_value: {mgmt_value}\", fontsize=16)\n    plt.show()\n    \n    \nfor i in random.sample(range(df.shape[0]), 2):\n    visualize_sample(df.iloc[i][\"BraTS21ID\"], df.iloc[i][\"MGMT_value\"], slice_i=0.5)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:28.640997Z","iopub.execute_input":"2021-12-01T21:34:28.641442Z","iopub.status.idle":"2021-12-01T21:34:29.941822Z","shell.execute_reply.started":"2021-12-01T21:34:28.641387Z","shell.execute_reply":"2021-12-01T21:34:29.940208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Preprocessing DICOM into 2D Slices as Generator\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomFlip,RandomRotation,RandomTranslation\n\n# Keras Data Augmentation\naugmentation_layers = tf.keras.Sequential(\n    [\n        RandomRotation(factor=0.01),\n        RandomTranslation(height_factor=0.0, width_factor=0.1),\n    ],\n    name='keras_augment_layers'\n)\n\n# More manual data augmentation\ndef preprocessing_image(img, augment=True):   \n    img = tf.cast(img, tf.float32) / 255.0\n\n    # only true for train set \n    if augment:\n        # augment each slices \n        # todo: integrate better technique \n        splitted_img = tf.split(img, input_depth, axis=-1)\n\n        augment_img = []\n        for each_img in splitted_img:\n            img = tf.repeat(each_img, repeats=3, axis=-1)\n            img = tf.image.random_flip_left_right(img)\n            img = tf.image.random_saturation(img, 0.9, 1.3)\n            img = tf.image.random_contrast(img, 0.8, 1.2)\n            img = tf.image.random_brightness(img, 0.2)\n            img, _, _ = tf.split(img, 3, axis=-1)\n            img = tfa.image.random_cutout(tf.expand_dims(img, 0),\n                                          mask_size=(20, 20), \n                                          constant_values=0)\n            augment_img.append(img)\n            \n        img = tf.concat(augment_img, axis=-1)\n    img = tf.reshape(img, [input_height, input_width, input_depth])\n    return img\n\ndef get_data_generator(data, is_train=False, shuffle=True, augment=False, repeat=True, batch_size=32):\n    if repeat: \n        data = data.repeat()\n    \n    if shuffle:\n        data = data.shuffle(batch_size * 10)\n        \n    data = data.map(lambda x, y: (preprocessing_image(x, augment), y), num_parallel_calls=AUTO)\n    data = data.batch(batch_size, drop_remainder=is_train)\n    \n    if shuffle:\n        data = data.map(lambda x, y: (augmentation_layers(x), y), num_parallel_calls=AUTO) \n    \n    data = data.prefetch(AUTO)\n    return data ","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:29.943463Z","iopub.execute_input":"2021-12-01T21:34:29.944003Z","iopub.status.idle":"2021-12-01T21:34:38.952436Z","shell.execute_reply.started":"2021-12-01T21:34:29.94396Z","shell.execute_reply":"2021-12-01T21:34:38.950594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data loader \nclass BrainTumorGenerator(tf.keras.utils.Sequence):\n    def __init__(self, dicom_path, data, is_train=True):\n        self.is_train = is_train # to control training/validation/inference part         \n        self.data = data\n        self.dicom_path = dicom_path\n        self.label = self.data['MGMT_value']\n  \n    def __len__(self):\n        return self.data['BraTS21ID'].shape[0]\n    \n    def __getitem__(self, index):\n        patient_ids = f\"{self.dicom_path}/{str(self.data['BraTS21ID'][index]).zfill(5)}/\"\n   \n        channel = []\n        for t in (\"FLAIR\", \"T1w\", \"T1wCE\", \"T2w\"): \n            t_paths = sorted(\n                glob.glob(os.path.join(patient_ids, t, \"*\")), \n                key=lambda x: int(x[:-4].split(\"-\")[-1]),\n            )\n            \n            # pick 15 slices \n            K = 15\n            # computing strt, and end index \n            strt_idx = (len(t_paths) // 2) - (K // 2)\n            end_idx = (len(t_paths) // 2) + (K // 2)\n            # slicing extracting elements with 3 intervals \n            r = t_paths[strt_idx + 3: end_idx + 3: 3]\n    \n            # removing black borders \n            # and add multi-modal features maps / channel depth\n            threshold = 0\n            for i in r:\n                image = self.read_dicom_xray(i)\n                temp_image = image\n                \n                rows = np.where(np.max(temp_image, 0) > threshold)[0]\n                if rows.size:\n                    cols = np.where(np.max(temp_image, 1) > threshold)[0]\n                    image = image[cols[0]: cols[-1] + 1, rows[0]: rows[-1] + 1]\n                else:\n                    image = image[:1, :1]\n                \n                channel.append(cv2.resize(image, (input_height, input_width)))\n                break # remove it for r-times frames for each series\n                    \n        if self.is_train:\n            return np.array(channel).T, self.label.iloc[index,]\n        else:\n            return np.array(channel).T\n    \n    def read_dicom_xray(self, path):\n        data = pydicom.read_file(path).pixel_array\n        data = data - np.min(data)\n        data = data / np.max(data)\n        data = (data * 255).astype(np.uint8)\n        return data","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:38.954009Z","iopub.execute_input":"2021-12-01T21:34:38.954261Z","iopub.status.idle":"2021-12-01T21:34:38.969807Z","shell.execute_reply.started":"2021-12-01T21:34:38.954226Z","shell.execute_reply":"2021-12-01T21:34:38.969056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor index, (train_index, val_index) in enumerate(skf.split(X=df.index, \n                                                           y=df.MGMT_value)):\n    df.loc[val_index, 'fold'] = index\n    \nprint(df.groupby(['fold', df.MGMT_value]).size())","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:38.970775Z","iopub.execute_input":"2021-12-01T21:34:38.971019Z","iopub.status.idle":"2021-12-01T21:34:39.631123Z","shell.execute_reply.started":"2021-12-01T21:34:38.970987Z","shell.execute_reply":"2021-12-01T21:34:39.630415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fold_generator(fold):\n    # for way one - data generator\n    train_labels = df[df.fold != fold].reset_index(drop=True)\n    val_labels = df[df.fold == fold].reset_index(drop=True)\n    \n    return (\n        BrainTumorGenerator(train_sample_path, train_labels),\n        BrainTumorGenerator(train_sample_path, val_labels)\n    )\n\n# first fold \ntrain_gen, val_gen = fold_generator(0)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:39.632438Z","iopub.execute_input":"2021-12-01T21:34:39.632856Z","iopub.status.idle":"2021-12-01T21:34:39.641379Z","shell.execute_reply.started":"2021-12-01T21:34:39.632821Z","shell.execute_reply":"2021-12-01T21:34:39.640678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# params \nAUTO = tf.data.AUTOTUNE\ninput_height = 312\ninput_width = 312\ninput_depth = 4\nbatch_size = 3\nfold = 0\n\ntrain_data = tf.data.Dataset.from_generator(\n    lambda: map(tuple, train_gen),\n    (tf.float32, tf.float32),\n    (\n        tf.TensorShape([input_height, input_width, input_depth]),\n        tf.TensorShape([]),\n    ),\n)\n\n# generate train sets \ntrain_generator = get_data_generator(train_data, is_train=True, repeat=False, shuffle=True, augment=True, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:39.643749Z","iopub.execute_input":"2021-12-01T21:34:39.643991Z","iopub.status.idle":"2021-12-01T21:34:41.689642Z","shell.execute_reply.started":"2021-12-01T21:34:39.643962Z","shell.execute_reply":"2021-12-01T21:34:41.688862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train generator visualization \nx, y = next(iter(train_generator))\nprint(x.shape, y.shape)  \nplt.figure(figsize=(35, 15))\nfor i in range(input_depth):\n    plt.subplot(1, input_depth, i + 1)\n    plt.imshow(x[0 ,:, :, i], cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.title(y[0].numpy())","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:41.693251Z","iopub.execute_input":"2021-12-01T21:34:41.693528Z","iopub.status.idle":"2021-12-01T21:34:54.396845Z","shell.execute_reply.started":"2021-12-01T21:34:41.69349Z","shell.execute_reply":"2021-12-01T21:34:54.393589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wrapping sequence generator to tf.data API \nval_data = tf.data.Dataset.from_generator(\n    lambda: map(tuple, val_gen),\n    (tf.float32, tf.float32),\n    (\n        tf.TensorShape([input_height, input_width, input_depth]),\n        tf.TensorShape([]),\n    ),\n)\n\n# generate validation sets \nvalid_generator = get_data_generator(val_data, is_train=True, \n                                     shuffle=False, repeat=False, augment=False, \n                                     batch_size=batch_size)\n\n# visualization \nx, y = next(iter(valid_generator))\nprint(x.shape, y.shape)  \nplt.figure(figsize=(35, 15))\nfor i in range(input_depth):\n    plt.subplot(1, input_depth, i + 1)\n    plt.imshow(x[0 ,:, :, i], cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.title(y[0].numpy())","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:54.398237Z","iopub.execute_input":"2021-12-01T21:34:54.398674Z","iopub.status.idle":"2021-12-01T21:34:56.165808Z","shell.execute_reply.started":"2021-12-01T21:34:54.398638Z","shell.execute_reply":"2021-12-01T21:34:56.164156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x,y = next(iter(train_generator))\nprint(x.shape, y.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:56.167268Z","iopub.execute_input":"2021-12-01T21:34:56.167716Z","iopub.status.idle":"2021-12-01T21:34:57.963603Z","shell.execute_reply.started":"2021-12-01T21:34:56.16768Z","shell.execute_reply":"2021-12-01T21:34:57.962804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def final_generator(data_gen):\n    while True:\n        for x,y in iter(data_gen):\n            newy = {'clf': y, 'recon': x}\n            yield x,newy\n        \n        '''newy = {'clf': y, 'recon': tf.reshape(x, (312,312,4))}\n        yield tf.reshape(x, (312,312,4)),newy'''","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:57.965078Z","iopub.execute_input":"2021-12-01T21:34:57.965591Z","iopub.status.idle":"2021-12-01T21:34:57.971362Z","shell.execute_reply.started":"2021-12-01T21:34:57.96555Z","shell.execute_reply":"2021-12-01T21:34:57.970634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gen_tr = final_generator(train_generator)\ngen_va = final_generator(valid_generator)\n\nx,y = next(gen_tr)\n\nprint(x.shape, y['clf'].shape, y['recon'].shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:57.972931Z","iopub.execute_input":"2021-12-01T21:34:57.973627Z","iopub.status.idle":"2021-12-01T21:34:59.395633Z","shell.execute_reply.started":"2021-12-01T21:34:57.973587Z","shell.execute_reply":"2021-12-01T21:34:59.394913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2D U-net:","metadata":{}},{"cell_type":"code","source":"# dual loss: \n#from tensorflow.keras import Input, Model \n#from tensorflow.keras.layers import Conv3D, GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.applications import *\n\nimport os, numpy as np, pandas as pd, matplotlib.pyplot as plt\nfrom tensorflow import losses, optimizers\nfrom tensorflow.keras import Input, Model, models, layers, callbacks, utils, metrics\n\n# --- Define kwargs dictionary\nkwargs = {\n    'kernel_size': (3, 3),\n    'padding': 'same'}\n\n# --- Define lambda functions\nconv = lambda x, filters, strides : layers.Conv2D(filters=filters, strides=strides, **kwargs)(x)\nnorm = lambda x : layers.BatchNormalization()(x)\nrelu = lambda x : layers.ReLU()(x)\ntran = lambda x, filters, strides : layers.Conv2DTranspose(filters=filters, strides=strides, **kwargs)(x)\n\n# --- Define stride-1, stride-2 blocks\nconv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\nconv2 = lambda filters, x : relu(norm(conv(x, filters, strides=2)))\n\ntran2 = lambda filters, x : relu(norm(tran(x, filters, strides=2)))\n","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:59.396551Z","iopub.execute_input":"2021-12-01T21:34:59.396797Z","iopub.status.idle":"2021-12-01T21:34:59.409159Z","shell.execute_reply.started":"2021-12-01T21:34:59.396765Z","shell.execute_reply":"2021-12-01T21:34:59.408277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define inputs:\ninput_dim = (input_height, input_width, input_depth)\ninput_tensor = Input(input_dim, name='input2d')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:59.410527Z","iopub.execute_input":"2021-12-01T21:34:59.411037Z","iopub.status.idle":"2021-12-01T21:34:59.419989Z","shell.execute_reply.started":"2021-12-01T21:34:59.410997Z","shell.execute_reply":"2021-12-01T21:34:59.419298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:59.421397Z","iopub.execute_input":"2021-12-01T21:34:59.421689Z","iopub.status.idle":"2021-12-01T21:34:59.43148Z","shell.execute_reply.started":"2021-12-01T21:34:59.421654Z","shell.execute_reply":"2021-12-01T21:34:59.430823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Define contracting layers\nl1 = conv1(8, input_tensor) #312\nl2 = conv1(16, conv2(16, l1)) #156\nl3 = conv1(32, conv2(32, l2)) #78\nl4 = conv1(48, conv2(48, l3)) #39\n#print(l4.shape)\n#l5 = conv1(64, conv2(64, l4))\n\n# --- Define expanding layers\n#l6  = tran2(48, l5)\n#\n#l7  = tran2(32, conv1(48, l6))\nl7  = tran2(32, l4)\n#print(l6.shape)\nl8  = tran2(16, conv1(32, l7))\nl9  = tran2(8,  conv1(16, l8))\nl10 = conv1(8,  l9)\n#print(l10.shape)\n\n# --- Define survival prediction\nh0 = layers.Flatten()(l4)\nh1 = layers.Dense(32, activation='relu')(h0)\n\n# --- Define all logits\nlogits = {}\nlogits['clf'] = layers.Dense(1, activation='sigmoid', name='clf')(h1) # prob use relu here instead of sigmoid cuz \n# sig is for pixel by pixel prediction\nfinal = layers.Conv2D(filters=4, name='recon', **kwargs)(l10) # 389 376\nlogits['recon'] = final #layers.Flatten()(final)\n\nprint(f\"\\nSurvival logits: {logits['clf'].shape}\")\nprint(f\"Tumor logits: {logits['recon'].shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:59.433726Z","iopub.execute_input":"2021-12-01T21:34:59.43391Z","iopub.status.idle":"2021-12-01T21:34:59.729163Z","shell.execute_reply.started":"2021-12-01T21:34:59.433889Z","shell.execute_reply":"2021-12-01T21:34:59.728464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Create model\ntf.keras.backend.clear_session()\nmodel = Model(inputs=input_tensor, outputs=logits)\n#model.summary()\n\n# --- Compile model\nmodel.compile(\n    optimizer=optimizers.Adam(learning_rate=1e-3),\n    loss={\n        'clf': losses.BinaryCrossentropy(from_logits = True),\n        'recon': losses.BinaryCrossentropy(from_logits = True)},\n    metrics={\n        'clf': metrics.AUC()}, \n             #metrics.BinaryAccuracy(name='bacc')],\n    experimental_run_tf_function=False)\n\n# define callbacks.\ncheckpoint_cb = callbacks.ModelCheckpoint(\n    \"model.h5\", monitor='val_auc', \n    mode='max', save_best_only=True\n)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:59.730434Z","iopub.execute_input":"2021-12-01T21:34:59.730831Z","iopub.status.idle":"2021-12-01T21:34:59.760051Z","shell.execute_reply.started":"2021-12-01T21:34:59.730794Z","shell.execute_reply":"2021-12-01T21:34:59.759437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Define callbacks\ndef lrdecay(epoch):\n    lr = 2e-4\n    if epoch > 32:\n        lr *= 1e-5\n    elif epoch > 16:\n        lr *= 2e-5\n    elif epoch > 8:\n        lr *= 1e-4\n    return lr\n\nlrdecay = callbacks.LearningRateScheduler(lrdecay)\n\n\nestop = callbacks.EarlyStopping(monitor='loss', patience=8, mode='min')\n# note: wanted to train to clearer convergence but kept running out of ram, hence the harsher estop penalty\n\ntensorboard_callback = callbacks.TensorBoard('./logs')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:34:59.761196Z","iopub.execute_input":"2021-12-01T21:34:59.761435Z","iopub.status.idle":"2021-12-01T21:35:00.237375Z","shell.execute_reply.started":"2021-12-01T21:34:59.761394Z","shell.execute_reply":"2021-12-01T21:35:00.236278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitting the model \n\nepochs = 50\nmodel.fit(\n    gen_tr, \n    steps_per_epoch = 250,\n    epochs=epochs,\n    validation_data=gen_va,\n    validation_steps = 250,\n    callbacks = [lrdecay, estop])#,\n    #callbacks=[checkpoint_cb])","metadata":{"execution":{"iopub.status.busy":"2021-12-01T21:35:00.24107Z","iopub.execute_input":"2021-12-01T21:35:00.241527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2d Cnn - from Pratyush's notebook","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import Input, Model \nfrom tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.applications import *\n\ninput_dim = (input_height, input_width, input_depth)\ninput_tensor = Input(input_dim, name='input2d')\nefnet = DenseNet121(weights=None, \n                       include_top = False, \n                       input_shape=(input_height, input_width, 3))\nmapping3feat = Conv2D(3, (3, 3), padding='same', use_bias=False)(input_tensor)\n\noutput = efnet(mapping3feat)\noutput = GlobalAveragePooling2D()(output)\noutput = Dense(1, activation='sigmoid')(output)\n\nprint(output.shape)\n\ntf.keras.backend.clear_session()\nmodel = Model(input_tensor, output)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras \nfrom tensorflow.keras.optimizers import Adam, SGD, RMSprop\nfrom tensorflow_addons.optimizers import RectifiedAdam, Lookahead\n\n# compiling \nmodel.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n    optimizer=Adam(learning_rate=1e-3),\n    metrics=[tf.keras.metrics.AUC(), \n             tf.keras.metrics.BinaryAccuracy(name='bacc')],\n)\n\n# define callbacks.\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    \"model.h5\", monitor='val_auc', \n    mode='max', save_best_only=True\n)\n\n\n# fitting the model \nepochs = 5\nmodel.fit(\n    train_generator, \n    epochs=epochs,\n    validation_data=valid_generator, \n    callbacks=[checkpoint_cb], verbose=2\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}