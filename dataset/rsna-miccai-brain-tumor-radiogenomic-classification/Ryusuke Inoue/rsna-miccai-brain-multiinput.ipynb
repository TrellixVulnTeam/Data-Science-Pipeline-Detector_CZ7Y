{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport sys\nimport os\nimport datetime\nimport argparse\nimport cv2\nimport numpy as np\nimport gc\nfrom numpy.core.fromnumeric import _trace_dispatcher\nimport pandas as pd\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom pathlib import Path\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras.layers import Activation,PReLU\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras import initializers\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_preds = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/sample_submission.csv')\nweightdatapath = Path(\"../input/weight-multi-20210919\")\ntestdatapaht = \"../input/rsna-miccai-brain-tumor-radiogenomic-classification/test\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"height = 256\nwidth = 256\nchannel = 3\nbatch_size = 32\nepochs = 400\nseed = 26\nepoch = \"0924\"\nviews = ['FLAIR', 'T1w', 'T1wCE', 'T2w']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=200):\n    tf.random.set_seed(seed)\n\n    # optional\n    # for numpy.random\n    np.random.seed(seed)\n    # for built-in random\n    random.seed(seed)\n    # for hash seed\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\nset_seed(seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Mish(Activation):\n\n    def __init__(self, activation, **kwargs):\n        super(Mish, self).__init__(activation, **kwargs)\n        self.__name__ = 'Mish'\n\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\n\nget_custom_objects().update({'Mish': Mish(mish)})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Siren(Activation):\n\n    def __init__(self, activation, **kwargs):\n        super(Siren, self).__init__(activation, **kwargs)\n        self.__name__ = 'Siren'\n\ndef siren(inputs):\n    return 1.0 / (1.0 + tf.math.exp(-inputs))\n\nget_custom_objects().update({'Siren': Siren(siren)})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ARelu(Activation):\n\n    def __init__(self, activation, **kwargs):\n        super(ARelu, self).__init__(activation, **kwargs)\n        self.__name__ = 'ARelu'\n\ndef arelu(x, alpha=0.90, beta=2.0):\n    alpha = tf.clip_by_value(alpha, clip_value_min=0.01, clip_value_max=0.99)\n    beta  = 1 + tf.math.sigmoid(beta)\n    return tf.nn.relu(x) * beta - tf.nn.relu(-x) * alpha\n\nget_custom_objects().update({'ARelu': ARelu(arelu)})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Celu(Activation):\n\n    def __init__(self, activation, **kwargs):\n        super(Celu, self).__init__(activation, **kwargs)\n        self.__name__ = 'Celu'\n\ndef celu(x,alpha=2.0 ):\n    mask_greater = tf.cast(tf.greater_equal(x,0),tf.float32) * x\n    mask_smaller = tf.cast(tf.less(x,0),tf.float32) * x\n    middle = alpha * ( tf.exp(tf.divide(mask_smaller,alpha)) - 1)\n    return middle + mask_greater\n\nget_custom_objects().update({'Celu': Celu(celu)})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TanhExp(Activation):\n\n    def __init__(self, activation, **kwargs):\n        super(TanhExp, self).__init__(activation, **kwargs)\n        self.__name__ = 'TanhExp'\n\ndef tanhexp(x):\n  return x * tf.math.tanh(tf.math.exp(x))\n\nget_custom_objects().update({'TanhExp': TanhExp(tanhexp)})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Silu(tf.keras.layers.Layer):\n  def __init__(self, num_outputs):\n    super(Silu, self).__init__()\n    self.num_outputs = num_outputs\n\n  def build(self, input_shape):\n    self.kernel = self.add_variable(\"kernel\", \n                                    shape=[int(input_shape[-1]), \n                                           self.num_outputs])\n\n  def call(self, input):\n    return tf.nn.silu(input)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_imgs(idx,view, ignore_zeros=True):\n    imgs = {}\n    \n    save_ds = []\n    dir_path = os.walk(os.path.join(\n        testdatapaht, idx, view\n    ))\n    for path, subdirs, files in dir_path:\n        for name in files:\n            image_path = os.path.join(path, name) \n            pyds = pydicom.filereader.dcmread(image_path)\n            slope = float(pyds.RescaleSlope)\n            intercept = float(pyds.RescaleIntercept)\n            img = intercept + pyds.pixel_array * slope\n            img = cv2.resize(img,[height,width])\n            save_ds.append(np.array(img))\n    if len(save_ds) == 0:\n        save_ds = np.zeros((1,256,256))\n    imgs = np.array(save_ds)\n    return imgs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim = (height,width)\nt_imgs = np.empty((channel, *dim))\ndef data_generation(ID,view,is_Train=True):\n    'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n\n    # Store sample\n    idx = str(ID).zfill(5)\n    imgs = load_imgs(idx,view, ignore_zeros=False)\n    t_size = imgs.shape\n    t_a = math.ceil(t_size[0] / 3)\n    t_img =imgs[:t_a]\n    t_imgs[0] = t_img.mean(axis=0) * 0.25\n    t_img =imgs[t_a:t_size[0] - t_a]\n    t_imgs[1] = t_img.mean(axis=0) * 0.4\n    t_img =imgs[t_size[0] - t_a:]\n    t_imgs[2] = t_img.mean(axis=0) * 0.25\n    #img_ = imgs.mean(axis=0)\n    img_ = t_imgs.transpose(1,2,0)\n    return img_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def serialize_example_test(feature0):\n  feature = {\n      'image': _bytes_feature(feature0.tobytes())\n  }\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return example_proto.SerializeToString()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(4):\n    with tf.io.TFRecordWriter(str(\"./\") + str(\"brain_test_\" + views[i] + \".tfrec\")) as writer:\n        for x in df_preds[\"BraTS21ID\"]:\n            img = data_generation(x,views[i],False)\n            example = serialize_example_test(\n                img)\n            writer.write(example)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deserialize_example(serialized_string):\n    # 特徴量を記述するディクショナリを作成\n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string)\n    }\n    parsed_record = tf.io.parse_single_example(serialized_string, image_feature_description)\n    #image = tf.io.decode_image(parsed_record['image'], expand_animations = False, dtype=tf.float64, channels=3)\n    image = tf.io.decode_raw(parsed_record['image'], tf.float64)\n    image = tf.reshape(image,[height,width,channel])\n    return image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def argument_image_tw2_val(img):\n  #img = tf.image.adjust_brightness(img, delta=3)\n  #img = tf.image.adjust_contrast(img, 0.3)\n\n  img = tf.cast(img, tf.float32) / 255.0\n  return img","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RegressionModel(tf.keras.Model):\n    def __init__(self,**kwargs):\n        super(RegressionModel, self).__init__(**kwargs)\n        self.conv1 = tf.keras.layers.Conv2D(128, 5, strides=2, kernel_initializer=initializers.TruncatedNormal(), activation=\"TanhExp\")\n        self.bn1   = tf.keras.layers.BatchNormalization()\n        self.rule1 = tf.keras.layers.PReLU(alpha_initializer=tf.initializers.constant(0.15))\n        #self.rule1 = Silu(0)\n        #self.rule1_1 = FReLU()\n        #self.rule1 = tf.keras.layers.LeakyReLU(alpha=0.3)\n        self.max1  = tf.keras.layers.MaxPooling2D(5)\n\n        self.conv2 = tf.keras.layers.Conv2D(128, 5, strides=2, kernel_initializer=initializers.TruncatedNormal(), activation=\"TanhExp\")\n        self.bn2   = tf.keras.layers.BatchNormalization()\n        self.rule2 = tf.keras.layers.PReLU(alpha_initializer=tf.initializers.constant(0.15))\n        #self.rule2_1 = FReLU()\n        #self.rule2 = Silu(0)\n        #self.rule2 = tf.keras.layers.LeakyReLU(alpha=0.3)\n        self.max2  = tf.keras.layers.MaxPooling2D(5)\n\n        self.conv3 = tf.keras.layers.Conv2D(128, 5, strides=2, kernel_initializer=initializers.TruncatedNormal(), activation=\"TanhExp\")\n        self.bn3   = tf.keras.layers.BatchNormalization()\n        self.rule3 = tf.keras.layers.PReLU(alpha_initializer=tf.initializers.constant(0.15))\n        #self.rule3_1 = FReLU()\n        #self.rule3 = Silu(0)\n        #self.rule2 = tf.keras.layers.LeakyReLU(alpha=0.3)\n        self.max3  = tf.keras.layers.MaxPooling2D(5)      \n\n        self.conv4 = tf.keras.layers.Conv2D(128, 5, strides=2, kernel_initializer=initializers.TruncatedNormal(), activation=\"TanhExp\")\n        self.bn4   = tf.keras.layers.BatchNormalization()\n        self.rule4 = tf.keras.layers.PReLU(alpha_initializer=tf.initializers.constant(0.15))\n        #self.rule4_1 = FReLU()\n        #self.rule4 = Silu(0)\n        #self.rule2 = tf.keras.layers.LeakyReLU(alpha=0.3)\n        self.max4  = tf.keras.layers.MaxPooling2D(5)\n\n        self.conv0 = tf.keras.layers.Conv2D(128, 5, strides=2, kernel_initializer=initializers.TruncatedNormal(), activation=\"TanhExp\")\n        self.bn0   = tf.keras.layers.BatchNormalization()\n        self.rule0 = tf.keras.layers.PReLU(alpha_initializer=tf.initializers.constant(0.15))\n        #self.rule1 = Silu(0)\n        #self.rule1_1 = FReLU()\n        #self.rule1 = tf.keras.layers.LeakyReLU(alpha=0.3)\n        self.max0  = tf.keras.layers.MaxPooling2D(5)\n\n        para_relu = tf.keras.layers.LeakyReLU(alpha=0.4)\n        para_relu2 = tf.keras.layers.LeakyReLU(alpha=0.8)\n        #para_relu = Silu(0)\n        self.dence256_1 = tf.keras.layers.Dense(256, kernel_initializer=initializers.TruncatedNormal(),activation=\"ARelu\")\n        self.dence256_1_1 = tf.keras.layers.Dense(256, kernel_initializer='he_normal',activation=\"Mish\")\n        self.dence128_1 = tf.keras.layers.Dense(128, kernel_initializer='he_normal',activation=\"swish\")\n        self.dence64_1 = tf.keras.layers.Dense(64, kernel_initializer='he_normal',activation=\"Celu\")\n        self.dropoup5_1 = tf.keras.layers.Dropout(0.3)\n        self.dropoup4_1 = tf.keras.layers.Dropout(0.3)\n        self.dropoup3_1 = tf.keras.layers.Dropout(0.2)\n\n        self.dence256_2 = tf.keras.layers.Dense(256, kernel_initializer=initializers.TruncatedNormal(),activation=\"ARelu\")\n        self.dence256_2_1 = tf.keras.layers.Dense(256, kernel_initializer='he_normal',activation=\"Mish\")\n        self.dence128_2 = tf.keras.layers.Dense(128, kernel_initializer='he_normal',activation=\"swish\")        \n        self.dence64_2 = tf.keras.layers.Dense(64, kernel_initializer='he_normal',activation=\"Celu\")\n        self.dropoup5_2 = tf.keras.layers.Dropout(0.3)\n        self.dropoup4_2 = tf.keras.layers.Dropout(0.3)\n        self.dropoup3_2 = tf.keras.layers.Dropout(0.2)\n\n        self.dence256_3 = tf.keras.layers.Dense(256, kernel_initializer=initializers.TruncatedNormal(),activation=\"ARelu\")\n        self.dence256_3_1 = tf.keras.layers.Dense(256, kernel_initializer='he_normal',activation=\"Mish\")\n        self.dence128_3 = tf.keras.layers.Dense(128, kernel_initializer='he_normal',activation=\"swish\")\n        self.dence64_3 = tf.keras.layers.Dense(64, kernel_initializer='he_normal',activation=\"Celu\")\n        self.dropoup5_3 = tf.keras.layers.Dropout(0.3)\n        self.dropoup4_3 = tf.keras.layers.Dropout(0.3)\n        self.dropoup3_3 = tf.keras.layers.Dropout(0.2)\n\n        self.dence256_4 = tf.keras.layers.Dense(256, kernel_initializer=initializers.TruncatedNormal(),activation=\"ARelu\")\n        self.dence256_4_1 = tf.keras.layers.Dense(256, kernel_initializer='he_normal',activation=\"Mish\")\n        self.dence128_4 = tf.keras.layers.Dense(128, kernel_initializer='he_normal',activation=\"swish\")\n        self.dence64_4 = tf.keras.layers.Dense(64, kernel_initializer='he_normal',activation=\"Celu\")\n        self.dropoup5_4 = tf.keras.layers.Dropout(0.3)\n        self.dropoup4_4 = tf.keras.layers.Dropout(0.3)\n        self.dropoup3_4 = tf.keras.layers.Dropout(0.2)\n\n        self.dence256 = tf.keras.layers.Dense(256, kernel_initializer=initializers.TruncatedNormal(),activation=\"Mish\")\n        self.dence256_X = tf.keras.layers.Dense(256, kernel_initializer=initializers.TruncatedNormal(),activation=\"gelu\")\n        self.dence128 = tf.keras.layers.Dense(128, kernel_initializer='he_normal',activation=\"Mish\")\n        self.dence128_X = tf.keras.layers.Dense(128, kernel_initializer='he_normal',activation=\"selu\")               \n        self.dence64 = tf.keras.layers.Dense(64, kernel_initializer='he_normal',activation=\"gelu\")\n        self.dence32 = tf.keras.layers.Dense(32, kernel_initializer='he_normal',activation=\"Siren\")\n        self.dropoup5 = tf.keras.layers.Dropout(0.5)\n        self.dropoup4 = tf.keras.layers.Dropout(0.4)\n        self.dropoup3 = tf.keras.layers.Dropout(0.3)\n        self.dropoup2 = tf.keras.layers.Dropout(0.2)\n        self.dropoup = tf.keras.layers.Dropout(0.1)\n        self.dence1 = tf.keras.layers.Dense(1, activation='sigmoid')\n\n        self.flatten = tf.keras.layers.Flatten()\n    \n    def call(self, input_tensor, training=True):\n        # forward pass: block 1 \n        x1 = input_tensor[0]\n        x1 = self.conv1(x1)\n        x1 = self.bn1(x1)\n        x1 = self.rule1(x1)\n        x1 = self.max1(x1)\n        x1 = self.dence256_1(x1)\n        x1 = self.dropoup5_1(x1)\n        x1 = self.dence256_1_1(x1)\n        #x1 = self.dropoup3_1(x1)\n        x1 = self.dence128_1(x1)\n        #x1 = self.rule1_1(x1)\n        x1 = self.dence64_1(x1)\n        x1 = self.dropoup3_1(x1)\n\n        x2 = input_tensor[1]\n        x2 = self.conv2(x2)\n        x2 = self.bn2(x2)\n        x2 = self.rule2(x2)\n        x2 = self.max2(x2)\n        x2 = self.dence256_2(x2)\n        x2 = self.dropoup5_2(x2)\n        x2 = self.dence256_2_1(x2)\n        x2 = self.dropoup3_2(x2)\n        x2 = self.dence128_2(x2)\n        #x2 = self.rule2_1(x2)\n        x2 = self.dence64_2(x2)\n        x2 = self.dropoup3_2(x2)\n\n        x3 = input_tensor[2]\n        x3 = self.conv3(x3)\n        x3 = self.bn3(x3)\n        x3 = self.rule3(x3)\n        x3 = self.max3(x3)\n        x3 = self.dence256_3(x3)\n        x3 = self.dropoup5_3(x3)\n        x3 = self.dence256_3_1(x3)\n        x3 = self.dropoup3_3(x3)\n        x3 = self.dence128_3(x3)\n        #x3 = self.rule3_1(x3)\n        x3 = self.dence64_3(x3)\n        x3 = self.dropoup3_3(x3)\n\n        x4 = input_tensor[3]\n        x4 = self.conv4(x4)\n        x4 = self.bn4(x4)\n        x4 = self.rule4(x4)\n        x4 = self.max4(x4)\n        x4 = self.dence256_4(x4)\n        x4 = self.dropoup5_4(x4)\n        x4 = self.dence256_4_1(x4)\n        x4 = self.dropoup3_4(x4)\n        x4 = self.dence128_4(x4)\n        #x4 = self.rule4_1(x4)\n        x4 = self.dence64_4(x4)\n        x4 = self.dropoup3_4(x4)\n\n        x = tf.keras.layers.Concatenate()([x1,x2,x3, x4])\n        x = self.dence256(x)\n        #x = self.dropoup3(x)\n        x = self.conv0(x)\n        x = self.bn0(x)\n        x = self.rule0(x)\n        x = self.max0(x)\n        x = self.dence128_X(x)\n        x = self.flatten(x)\n        #x = self.dence128(x)\n        #x = self.dropoup(x)\n        x = self.dence64(x)\n        #x = self.dropoup2(x)\n        x = self.dence32(x)\n        #x = self.dropoup(x)\n        return self.dence1(x) \n\n    def train_step(self, data):\n        x ,y = data\n        with tf.GradientTape() as tape:\n            predictions = self(x, training=True)\n            loss = self.compiled_loss(y,predictions,regularization_losses=self.losses)\n            print(y,predictions)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        self.compiled_metrics.update_state(y, predictions)\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, data):\n        # Unpack the data\n        x, y = data\n        # Compute predictions\n        y_pred = self(x, training=False)\n        # Updates the metrics tracking the loss\n        self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n        # Update the metrics.\n        self.compiled_metrics.update_state(y, y_pred)\n        # Return a dict mapping metric names to current value.\n        # Note that it will include the loss (tracked in self.metrics).\n        return {m.name: m.result() for m in self.metrics}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n#opt = tf.keras.optimizers.SGD(learning_rate=1e-5, decay=1e-6, momentum=0.9, nesterov=True)\n#opt = tf.keras.optimizers.RMSprop(lr=1e-5, rho=0.9, epsilon=None, decay=0.0)\nopt = tf.keras.optimizers.Adam(learning_rate=1e-5, beta_1=0.9, beta_2=0.999)\nf_pre=[]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testset0 = tf.data.TFRecordDataset(str(\"./\") +  str(\"brain_test_FLAIR.tfrec\")).map(deserialize_example).map(argument_image_tw2_val)\ntestset1 = tf.data.TFRecordDataset(str(\"./\") +  str(\"brain_test_T1w.tfrec\")).map(deserialize_example).map(argument_image_tw2_val)\ntestset2 = tf.data.TFRecordDataset(str(\"./\") +  str(\"brain_test_T1wCE.tfrec\")).map(deserialize_example).map(argument_image_tw2_val)\ntestset3 = tf.data.TFRecordDataset(str(\"./\") +  str(\"brain_test_T2w.tfrec\")).map(deserialize_example).map(argument_image_tw2_val)\n\ninput_a = tf.keras.Input(shape=(height,width, channel), name=\"input_a\")\ninput_b = tf.keras.Input(shape=(height,width, channel), name=\"input_b\")\ninput_c = tf.keras.Input(shape=(height,width, channel), name=\"input_c\")\ninput_d = tf.keras.Input(shape=(height,width, channel), name=\"input_d\")\n\nmodel = RegressionModel()\nmodel([input_a,input_b,input_c,input_d])\n\nmodel.compile(\n  optimizer=opt, \n  loss=loss_func,\n  metrics=[tf.keras.metrics.AUC(), tf.keras.metrics.BinaryCrossentropy()]\n  )\n\ntest_ds = tf.data.Dataset.zip(((testset0,testset1,testset2,testset3),)).batch(10)\n\nfor f in range(5): #Fold    \n    t_weight = \"weight-Regression-multi_fold_0\" +str(f) + \"-\" + epoch + \".ckpt\"\n    model.load_weights(Path(weightdatapath,t_weight))\n    f_pre.append(model.predict(test_ds, batch_size=batch_size))\n    tf.keras.backend.clear_session()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre = np.array(f_pre)\nfinpre = pre.mean(axis=0)\n\nsubfilename = \"submission.csv\"\nsub = pd.DataFrame(finpre, columns=['MGMT_value'])\n\ndf_preds[\"MGMT_value\"] = sub\n\ndf_preds.to_csv(\n    subfilename,\n    index=False\n    )","metadata":{},"execution_count":null,"outputs":[]}]}