{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro\nIn my this code,I try to use 4 pre-trainmode,5 Fold weight for each MRI-TYPE.And,I converte Test Data to TFRecord.  \nBecause,without TFRecord,Notebook is timeout after submittion.  \n**Version 2.0**\nDisplay Images each MRI-TYPES","metadata":{}},{"cell_type":"code","source":"!pip install ../input/tensorflowprobability0122/tensorflow_probability-0.12.2-py2.py3-none-any.whl\n!pip3 install ../input/kerasapplications -q\n!pip3 install ../input/image-classifiers/image_classifiers-1.0.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:16:50.033538Z","iopub.execute_input":"2021-09-06T13:16:50.033866Z","iopub.status.idle":"2021-09-06T13:18:17.580872Z","shell.execute_reply.started":"2021-09-06T13:16:50.033837Z","shell.execute_reply":"2021-09-06T13:18:17.579844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport json\nimport argparse\nimport datetime\nimport random\nfrom pathlib import Path\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import log_loss\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport math\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow_probability as tfp\nfrom classification_models.keras import Classifiers","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:33:30.466556Z","iopub.execute_input":"2021-09-06T13:33:30.466891Z","iopub.status.idle":"2021-09-06T13:33:30.473681Z","shell.execute_reply.started":"2021-09-06T13:33:30.466861Z","shell.execute_reply":"2021-09-06T13:33:30.472651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data\nweight-20210903-full\" is weigth. 4 MRI-TYPE * 5 Fold * 4 model *2 =  160 files","metadata":{}},{"cell_type":"code","source":"df_preds = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/sample_submission.csv')\nweightdatapath = Path(\"../input/weight-20210903-full\")\ntestdatapaht = \"../input/rsna-miccai-brain-tumor-radiogenomic-classification/test\"","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:18:23.117936Z","iopub.execute_input":"2021-09-06T13:18:23.118293Z","iopub.status.idle":"2021-09-06T13:18:23.132085Z","shell.execute_reply.started":"2021-09-06T13:18:23.118255Z","shell.execute_reply":"2021-09-06T13:18:23.131293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"height = 256\nwidth = 256\nchannel = 3\nbatch_size = 16\nseed = 46\nviews = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\n\nMRI_TYPE = 0","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:18:23.133952Z","iopub.execute_input":"2021-09-06T13:18:23.134351Z","iopub.status.idle":"2021-09-06T13:18:23.139505Z","shell.execute_reply.started":"2021-09-06T13:18:23.134313Z","shell.execute_reply":"2021-09-06T13:18:23.138357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=200):\n    tf.random.set_seed(seed)\n\n    np.random.seed(seed)\n\n    random.seed(seed)\n\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\nset_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:18:23.140973Z","iopub.execute_input":"2021-09-06T13:18:23.141454Z","iopub.status.idle":"2021-09-06T13:18:23.149836Z","shell.execute_reply.started":"2021-09-06T13:18:23.141408Z","shell.execute_reply":"2021-09-06T13:18:23.148981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_imgs(idx,view, ignore_zeros=True):\n    imgs = {}\n    \n    save_ds = []\n    dir_path = os.walk(os.path.join(\n        testdatapaht, idx, view\n    ))\n    for path, subdirs, files in dir_path:\n        for name in files:\n            image_path = os.path.join(path, name) \n            pyds = pydicom.filereader.dcmread(image_path)\n            slope = float(pyds.RescaleSlope)\n            intercept = float(pyds.RescaleIntercept)\n            img = intercept + pyds.pixel_array * slope\n            img = cv2.resize(img,[height,width])\n            save_ds.append(np.array(img))\n    if len(save_ds) == 0:\n        save_ds = np.zeros((1,256,256))\n    imgs = np.array(save_ds)\n    return imgs","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:18:23.151066Z","iopub.execute_input":"2021-09-06T13:18:23.151678Z","iopub.status.idle":"2021-09-06T13:18:23.161338Z","shell.execute_reply.started":"2021-09-06T13:18:23.151641Z","shell.execute_reply":"2021-09-06T13:18:23.160292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert Gray-Scale images to 3 Channel images\nDICOM is Gray-Scale.But Pre-Train models' input is 3channel.  \nSo,I divided the whole into one-third and get mean each part.  \nAnd I multiplied 0.3 or 0.4 so that total is 1.  \nI call this my method **\"Hazigin Method\"** :-)","metadata":{}},{"cell_type":"code","source":"dim = (height,width)\nt_imgs = np.empty((channel, *dim))\ndef data_generation(ID,view,is_Train=True):\n    idx = str(ID).zfill(5)\n    imgs = load_imgs(idx,view, ignore_zeros=False)\n    t_size = imgs.shape\n    t_a = math.ceil(t_size[0] / 3)\n    t_img =imgs[:t_a]\n    t_imgs[0] = t_img.mean(axis=0) * 0.3\n    t_img =imgs[t_a:t_size[0] - t_a]\n    t_imgs[1] = t_img.mean(axis=0) * 0.4\n    t_img =imgs[t_size[0] - t_a:]\n    t_imgs[2] = t_img.mean(axis=0) * 0.3\n    img_ = t_imgs.transpose(1,2,0)\n    return img_","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:18:23.162689Z","iopub.execute_input":"2021-09-06T13:18:23.163152Z","iopub.status.idle":"2021-09-06T13:18:23.174382Z","shell.execute_reply.started":"2021-09-06T13:18:23.163115Z","shell.execute_reply":"2021-09-06T13:18:23.173502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Argumentation\nThis argumentation is to that Images are too dark are bright, and  too bright are dark.","metadata":{}},{"cell_type":"code","source":"def argument_image_val(img):\n    hsv_tf = tf.image.rgb_to_hsv(img)\n    a = tf.transpose(hsv_tf,[2,0,1])\n    a = tf.reshape(a[2],[256,256]) \n    t_v = tf.cond( tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)>200,lambda:tf.cast(0,tf.float32),lambda:tf.cast((200 - tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95))*0.04,tf.float32))\n    t_c = tf.cond( tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)>200,lambda:tf.cast(0,tf.float32),lambda:tf.cast((200 - tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95))*0.005,tf.float32))   \n    t_v_o = tf.cond( tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)>300,lambda:tf.cast((tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95) - 300)*0.5,tf.float32),lambda:tf.cast(0,tf.float32))\n\n    img = tf.image.adjust_brightness(img, delta=(2.8+ t_v - t_v_o))\n    aa = tf.cond( (0.3 + t_c) < 2,lambda:tf.cast((0.3 + t_c),tf.float32),lambda:tf.cast(2,tf.float32))   \n\n    img = tf.image.adjust_contrast(img, tf.cast(aa,tf.float32))\n    img = tf.image.adjust_saturation(img, 1.2)\n    img = tf.cast(img, tf.float32) / 255.0\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:18:23.176442Z","iopub.execute_input":"2021-09-06T13:18:23.177028Z","iopub.status.idle":"2021-09-06T13:18:23.190893Z","shell.execute_reply.started":"2021-09-06T13:18:23.17699Z","shell.execute_reply":"2021-09-06T13:18:23.190163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _bytes_feature(value):\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() \n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:18:23.193144Z","iopub.execute_input":"2021-09-06T13:18:23.193716Z","iopub.status.idle":"2021-09-06T13:18:23.20358Z","shell.execute_reply.started":"2021-09-06T13:18:23.19368Z","shell.execute_reply":"2021-09-06T13:18:23.202815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def serialize_example_test(feature0):\n  feature = {\n      'image': _bytes_feature(feature0.tobytes())\n  }\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return example_proto.SerializeToString()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:18:23.204854Z","iopub.execute_input":"2021-09-06T13:18:23.20525Z","iopub.status.idle":"2021-09-06T13:18:23.212729Z","shell.execute_reply.started":"2021-09-06T13:18:23.2052Z","shell.execute_reply":"2021-09-06T13:18:23.211956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TFRecord\nConvert Test Data to TFRecord for each MRI type","metadata":{}},{"cell_type":"code","source":"for i in range(4):\n    with tf.io.TFRecordWriter(str(\"./\") + str(\"brain_test_\" + views[i] + \".tfrec\")) as writer:\n        for x in df_preds[\"BraTS21ID\"]:\n            img = data_generation(x,views[i],False)\n            example = serialize_example_test(\n                img)\n            writer.write(example)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:18:23.213774Z","iopub.execute_input":"2021-09-06T13:18:23.214986Z","iopub.status.idle":"2021-09-06T13:25:07.777171Z","shell.execute_reply.started":"2021-09-06T13:18:23.214958Z","shell.execute_reply":"2021-09-06T13:25:07.776306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def deserialize_example(serialized_string):\n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string)\n    }\n    parsed_record = tf.io.parse_single_example(serialized_string, image_feature_description)\n    image = tf.io.decode_raw(parsed_record['image'], tf.float64)\n    image = tf.reshape(image,[height,width,channel])\n    return image","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:25:07.778579Z","iopub.execute_input":"2021-09-06T13:25:07.778919Z","iopub.status.idle":"2021-09-06T13:25:07.786946Z","shell.execute_reply.started":"2021-09-06T13:25:07.778885Z","shell.execute_reply":"2021-09-06T13:25:07.786026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def _parse_image_function(example_proto): \n  return tf.io.parse_single_example(example_proto, image_feature_description)","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:32:30.917384Z","iopub.execute_input":"2021-09-06T13:32:30.917727Z","iopub.status.idle":"2021-09-06T13:32:30.924469Z","shell.execute_reply.started":"2021-09-06T13:32:30.917694Z","shell.execute_reply":"2021-09-06T13:32:30.923638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Disp Images[FLAIR]\nDisp FLAIR Image with brightness.","metadata":{}},{"cell_type":"code","source":"MRI_TYPE = 0\nraw_image_dataset = tf.data.TFRecordDataset(str(\"./\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\"))\n\nimage_feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string)\n}\n\nparsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n\nfig = plt.figure(figsize=(35, 20))\npage = 1\ni=0\nfor idx in range(page*15,(page + 1)*15):\n    ax = fig.add_subplot(3, 5, i+1)\n    ax.tick_params(labelbottom=False, bottom=False)\n    ax.tick_params(labelleft=False, left=False)\n    for parsed_record in parsed_image_dataset.take(idx):\n        img = tf.io.decode_raw(parsed_record['image'], tf.float64)\n        img = tf.reshape(img,[256,256,3])\n\n    hsv_tf = tf.image.rgb_to_hsv(img)\n    a = tf.transpose(hsv_tf,[2,0,1])\n    a = tf.reshape(a[2],[256,256]) \n    b  = tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)\n\n    img = argument_image_val(img)\n    ax.imshow(img)\n    ax.set_title(str(idx) + \":\" + str(b.numpy()),color=\"Black\")\n    i = i+1\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:39:51.051871Z","iopub.execute_input":"2021-09-06T13:39:51.052195Z","iopub.status.idle":"2021-09-06T13:39:54.866717Z","shell.execute_reply.started":"2021-09-06T13:39:51.052164Z","shell.execute_reply":"2021-09-06T13:39:54.860133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Disp Images[T1w]\nDisp T1w Image with brightness.","metadata":{}},{"cell_type":"code","source":"MRI_TYPE = 1\nraw_image_dataset = tf.data.TFRecordDataset(str(\"./\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\"))\n\nimage_feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string)\n}\n\nparsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n\nfig = plt.figure(figsize=(35, 20))\npage = 2\ni=0\nfor idx in range(page*15,(page + 1)*15):\n    ax = fig.add_subplot(3, 5, i+1)\n    ax.tick_params(labelbottom=False, bottom=False)\n    ax.tick_params(labelleft=False, left=False)\n    for parsed_record in parsed_image_dataset.take(idx):\n        img = tf.io.decode_raw(parsed_record['image'], tf.float64)\n        img = tf.reshape(img,[256,256,3])\n\n    hsv_tf = tf.image.rgb_to_hsv(img)\n    a = tf.transpose(hsv_tf,[2,0,1])\n    a = tf.reshape(a[2],[256,256]) \n    b  = tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)\n\n    img = argument_image_val(img)\n    ax.imshow(img)\n    ax.set_title(str(idx) + \":\" + str(b.numpy()),color=\"Black\")\n    i = i+1\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:39:35.599663Z","iopub.execute_input":"2021-09-06T13:39:35.599984Z","iopub.status.idle":"2021-09-06T13:39:39.849629Z","shell.execute_reply.started":"2021-09-06T13:39:35.599954Z","shell.execute_reply":"2021-09-06T13:39:39.848589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Disp Images[T1wCE]\nDisp T1wCE Image with brightness.","metadata":{}},{"cell_type":"code","source":"MRI_TYPE = 2\nraw_image_dataset = tf.data.TFRecordDataset(str(\"./\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\"))\n\nimage_feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string)\n}\n\nparsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n\nfig = plt.figure(figsize=(35, 20))\npage = 3\ni=0\nfor idx in range(page*15,(page + 1)*15):\n    ax = fig.add_subplot(3, 5, i+1)\n    ax.tick_params(labelbottom=False, bottom=False)\n    ax.tick_params(labelleft=False, left=False)\n    for parsed_record in parsed_image_dataset.take(idx):\n        img = tf.io.decode_raw(parsed_record['image'], tf.float64)\n        img = tf.reshape(img,[256,256,3])\n\n    hsv_tf = tf.image.rgb_to_hsv(img)\n    a = tf.transpose(hsv_tf,[2,0,1])\n    a = tf.reshape(a[2],[256,256]) \n    b  = tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)\n\n    img = argument_image_val(img)\n    ax.imshow(img)\n    ax.set_title(str(idx) + \":\" + str(b.numpy()),color=\"Black\")\n    i = i+1\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:39:19.693493Z","iopub.execute_input":"2021-09-06T13:39:19.693812Z","iopub.status.idle":"2021-09-06T13:39:23.806154Z","shell.execute_reply.started":"2021-09-06T13:39:19.693778Z","shell.execute_reply":"2021-09-06T13:39:23.805188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Disp Images[T2w]\nDisp T2w Image with brightness.","metadata":{}},{"cell_type":"code","source":"MRI_TYPE = 3\nraw_image_dataset = tf.data.TFRecordDataset(str(\"./\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\"))\n\nimage_feature_description = {\n    'image': tf.io.FixedLenFeature([], tf.string)\n}\n\nparsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n\nfig = plt.figure(figsize=(35, 20))\npage = 4\ni=0\nfor idx in range(page*15,(page + 1)*15):\n    ax = fig.add_subplot(3, 5, i+1)\n    ax.tick_params(labelbottom=False, bottom=False)\n    ax.tick_params(labelleft=False, left=False)\n    for parsed_record in parsed_image_dataset.take(idx):\n        img = tf.io.decode_raw(parsed_record['image'], tf.float64)\n        img = tf.reshape(img,[256,256,3])\n\n    hsv_tf = tf.image.rgb_to_hsv(img)\n    a = tf.transpose(hsv_tf,[2,0,1])\n    a = tf.reshape(a[2],[256,256]) \n    b  = tfp.stats.percentile(tf.experimental.numpy.ravel(a),q=95)\n\n    img = argument_image_val(img)\n    ax.imshow(img)\n    ax.set_title(str(idx) + \":\" + str(b.numpy()),color=\"Black\")\n    i = i+1\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-06T13:39:01.443085Z","iopub.execute_input":"2021-09-06T13:39:01.443448Z","iopub.status.idle":"2021-09-06T13:39:06.073186Z","shell.execute_reply.started":"2021-09-06T13:39:01.443419Z","shell.execute_reply":"2021-09-06T13:39:06.072198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassificationModel_senet154(tf.keras.Model):\n    def __init__(self):\n        super(ClassificationModel_senet154, self).__init__()\n        ResNet18, preprocess_input = Classifiers.get('senet154')\n        base_model = ResNet18((height,width, channel), include_top=False, weights='../input/image-classifiers/senet154_imagenet_1000_no_top.h5')\n        base_model.trainable = False\n        self.efficient_net = base_model\n        self.avgpooling = tf.keras.layers.GlobalAveragePooling2D()\n        self.dence64 = tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',activation='relu')\n        self.dence32 = tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',activation='relu')\n        self.efficient_net = base_model\n        self.dropoup = tf.keras.layers.Dropout(0.2)\n        self.dence1 = tf.keras.layers.Dense(1, activation='sigmoid')\n\n                \n    def call(self, input_tensor, training=True):\n        x = self.efficient_net(input_tensor)\n        #x = self.dence64(x)\n        x = self.avgpooling(x)\n        x = self.dence64(x)\n        x = self.dropoup(x)\n        x = self.dence32(x)\n        return self.dence1(x) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassificationModel_ResNet152V2(tf.keras.Model):\n    def __init__(self):\n        super(ClassificationModel_ResNet152V2, self).__init__()\n        base_model = tf.keras.applications.resnet_v2.ResNet152V2(weights=\"../input/image-classifiers/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\",include_top=False,pooling='avg',input_shape=(height,width, 3))\n        base_model.trainable = False\n        self.efficient_net = base_model\n        self.dence64 = tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',activation='relu')\n        self.dence32 = tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',activation='relu')\n        self.efficient_net = base_model\n        self.dropoup = tf.keras.layers.Dropout(0.2)\n        self.dence1 = tf.keras.layers.Dense(1, activation='sigmoid')\n\n                \n    def call(self, input_tensor, training=True):\n        x = self.efficient_net(input_tensor)\n        x = self.dence64(x)\n        x = self.dropoup(x)\n        x = self.dence32(x)\n        return self.dence1(x) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassificationModel_seresnext101(tf.keras.Model):\n    def __init__(self):\n        super(ClassificationModel_seresnext101, self).__init__()\n        ResNet18, preprocess_input = Classifiers.get('seresnext101')\n        base_model = ResNet18((height,width, channel), include_top=False, weights='../input/image-classifiers/seresnext101_imagenet_1000_no_top.h5')\n        base_model.trainable = False\n        self.efficient_net = base_model\n        self.avgpooling = tf.keras.layers.GlobalAveragePooling2D()\n        self.dence64 = tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',activation='relu')\n        self.dence32 = tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',activation='relu')\n        self.efficient_net = base_model\n        self.dropoup = tf.keras.layers.Dropout(0.2)\n        self.dence1 = tf.keras.layers.Dense(1, activation='sigmoid')\n\n                \n    def call(self, input_tensor, training=True):\n        x = self.efficient_net(input_tensor)\n        x = self.avgpooling(x)\n        x = self.dence64(x)\n        x = self.dropoup(x)\n        x = self.dence32(x)\n        return self.dence1(x) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ClassificationModel_seresnet152(tf.keras.Model):\n    def __init__(self):\n        super(ClassificationModel_seresnet152, self).__init__()\n        ResNet18, preprocess_input = Classifiers.get('seresnet152')\n        base_model = ResNet18((height,width, channel), include_top=False, weights='../input/image-classifiers/seresnet152_imagenet_1000_no_top.h5')\n        base_model.trainable = False\n        self.efficient_net = base_model\n        self.avgpooling = tf.keras.layers.GlobalAveragePooling2D()\n        self.dence64 = tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',activation='relu')\n        self.dence32 = tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',activation='relu')\n        self.efficient_net = base_model\n        self.dropoup = tf.keras.layers.Dropout(0.2)\n        self.dence1 = tf.keras.layers.Dense(1, activation='sigmoid')\n\n                \n    def call(self, input_tensor, training=True):\n        x = self.efficient_net(input_tensor)\n        x = self.avgpooling(x)\n        x = self.dence64(x)\n        x = self.dropoup(x)\n        x = self.dence32(x)\n        return self.dence1(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_func = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nopt = tf.keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.999)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_Model=[\"ResNet152V2\",\"senet154\",\"seresnet152\",\"seresnext101\"]\nt_pre=[] #For each MRI type data\nm_pre=[] #For each pre-train mode data\nf_pre=[] #For each Fold data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Choice Models\nThe model to be applied to each MRI is selected from the top two models with good CV scores.","metadata":{}},{"cell_type":"code","source":"#FLAIR\nMRI_TYPE = 0\nfor m in [1,3]: #Select MODELs\n    if m == 0:\n        model = ClassificationModel_ResNet152V2()\n    elif m == 1:\n        model = ClassificationModel_senet154()\n    elif m == 2:\n        model = ClassificationModel_seresnet152()\n    elif m == 3:\n        model = ClassificationModel_seresnext101()\n\n    model.compile(\n        optimizer=opt, \n        loss=loss_func,\n        metrics=['accuracy']\n        )\n    \n    for f in range(5): #Fold \n        test_dataset = tf.data.TFRecordDataset(str(\"./\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\")).map(deserialize_example).map(argument_image_val).batch(batch_size)\n        t_weight = \"weight-\" + weight_Model[m] + \"-\" + views[MRI_TYPE] + \"_fold_0\" +str(f) + \"-0903.ckpt\"\n        model.load_weights(Path(weightdatapath,t_weight))\n        f_pre.append(model.predict(test_dataset, batch_size=batch_size))\n        tf.keras.backend.clear_session()\n\n    pre = np.array(f_pre)\n    m_pre.append(pre.mean(axis=0))\n    f_pre.clear()\n\npre = np.array(m_pre)\nt_pre.append(pre.mean(axis=0))\nm_pre.clear()\ndel model\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#T1w\nMRI_TYPE = 1\nf_pre.clear()\nfor m in [0,1]: #Select MODELs\n    if m == 0:\n        model = ClassificationModel_ResNet152V2()\n    elif m == 1:\n        model = ClassificationModel_senet154()\n    elif m == 2:\n        model = ClassificationModel_seresnet152()\n    elif m == 3:\n        model = ClassificationModel_seresnext101()\n\n    model.compile(\n        optimizer=opt, \n        loss=loss_func,\n        metrics=['accuracy']\n        )\n    \n    for f in range(5): #Fold \n        test_dataset = tf.data.TFRecordDataset(str(\"./\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\")).map(deserialize_example).map(argument_image_val).batch(batch_size)\n        t_weight = \"weight-\" + weight_Model[m] + \"-\" + views[MRI_TYPE] + \"_fold_0\" +str(f) + \"-0903.ckpt\"\n        model.load_weights(Path(weightdatapath,t_weight))\n        f_pre.append(model.predict(test_dataset, batch_size=batch_size))\n        tf.keras.backend.clear_session()\n\n    pre = np.array(f_pre)\n    m_pre.append(pre.mean(axis=0))\n    f_pre.clear()\n\npre = np.array(m_pre)\nt_pre.append(pre.mean(axis=0))\nm_pre.clear()\ndel model\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#T1wCE\nMRI_TYPE = 2\nf_pre.clear()\nfor m in [1,2]: #Select MODELs\n    if m == 0:\n        model = ClassificationModel_ResNet152V2()\n    elif m == 1:\n        model = ClassificationModel_senet154()\n    elif m == 2:\n        model = ClassificationModel_seresnet152()\n    elif m == 3:\n        model = ClassificationModel_seresnext101()\n\n    model.compile(\n        optimizer=opt, \n        loss=loss_func,\n        metrics=['accuracy']\n        )\n    \n    for f in range(5): #Fold \n        test_dataset = tf.data.TFRecordDataset(str(\"./\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\")).map(deserialize_example).map(argument_image_val).batch(batch_size)\n        t_weight = \"weight-\" + weight_Model[m] + \"-\" + views[MRI_TYPE] + \"_fold_0\" +str(f) + \"-0903.ckpt\"\n        model.load_weights(Path(weightdatapath,t_weight))\n        f_pre.append(model.predict(test_dataset, batch_size=batch_size))\n        tf.keras.backend.clear_session()\n\n    pre = np.array(f_pre)\n    m_pre.append(pre.mean(axis=0))\n    f_pre.clear()\n\npre = np.array(m_pre)\nt_pre.append(pre.mean(axis=0))\nm_pre.clear()\ndel model\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#T2w\nMRI_TYPE = 3\nf_pre.clear()\nfor m in [0,2]: #Select MODELs\n    if m == 0:\n        model = ClassificationModel_ResNet152V2()\n    elif m == 1:\n        model = ClassificationModel_senet154()\n    elif m == 2:\n        model = ClassificationModel_seresnet152()\n    elif m == 3:\n        model = ClassificationModel_seresnext101()\n\n    model.compile(\n        optimizer=opt, \n        loss=loss_func,\n        metrics=['accuracy']\n        )\n    \n    for f in range(5): #Fold \n        test_dataset = tf.data.TFRecordDataset(str(\"./\") + str(\"brain_test_\" + views[MRI_TYPE] + \".tfrec\")).map(deserialize_example).map(argument_image_val).batch(batch_size)\n        t_weight = \"weight-\" + weight_Model[m] + \"-\" + views[MRI_TYPE] + \"_fold_0\" +str(f) + \"-0903.ckpt\"\n        model.load_weights(Path(weightdatapath,t_weight))\n        f_pre.append(model.predict(test_dataset, batch_size=batch_size))\n        tf.keras.backend.clear_session()\n\n    pre = np.array(f_pre)\n    m_pre.append(pre.mean(axis=0))\n    f_pre.clear()\n\npre = np.array(m_pre)\nt_pre.append(pre.mean(axis=0))\nm_pre.clear()\ndel model\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre = np.array(t_pre)\nfinpre = pre.mean(axis=0)\n\nsubfilename = \"submission.csv\"\nsub = pd.DataFrame(finpre, columns=['MGMT_value'])\n\ndf_preds[\"MGMT_value\"] = sub\n\ndf_preds.to_csv(\n    subfilename,\n    index=False\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Request\n**If you find this Code useful, please upvote !!**","metadata":{}}]}