{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys \nimport json\nimport glob\nimport random\nimport collections\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport pydicom\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nfrom torch import nn\nfrom torch.utils import data as torch_data\nfrom sklearn import model_selection as sk_model_selection\nfrom torch.nn import functional as torch_functional\nimport torch.nn.functional as F\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-03T03:59:34.259932Z","iopub.execute_input":"2021-10-03T03:59:34.260289Z","iopub.status.idle":"2021-10-03T03:59:36.836493Z","shell.execute_reply.started":"2021-10-03T03:59:34.260187Z","shell.execute_reply":"2021-10-03T03:59:36.835668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.exists(\"../input/rsna-miccai-brain-tumor-radiogenomic-classification\"):\n    data_directory = '../input/rsna-miccai-brain-tumor-radiogenomic-classification'\n    pytorch3dpath = \"../input/efficientnetpyttorch3d/EfficientNet-PyTorch-3D\"\nelse:\n    data_directory = '/media/roland/data/kaggle/rsna-miccai-brain-tumor-radiogenomic-classification'\n    pytorch3dpath = \"EfficientNet-PyTorch-3D\"\n    \nmri_types = ['FLAIR','T1w','T1wCE','T2w']\nSIZE = 256\nNUM_IMAGES = 64\n\nsys.path.append(pytorch3dpath)\nfrom efficientnet_pytorch_3d import EfficientNet3D","metadata":{"execution":{"iopub.status.busy":"2021-10-03T03:59:36.837887Z","iopub.execute_input":"2021-10-03T03:59:36.838208Z","iopub.status.idle":"2021-10-03T03:59:36.874696Z","shell.execute_reply.started":"2021-10-03T03:59:36.838173Z","shell.execute_reply":"2021-10-03T03:59:36.87392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_dicom_image(path, img_size=SIZE):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    if np.min(data)==np.max(data):\n        data = np.zeros((img_size,img_size))\n        return data\n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data / np.max(data)\n    \n    #data = (data * 255).astype(np.uint8)\n    data = cv2.resize(data, (img_size, img_size))\n    return data\n\ndef load_dicom_images_3d(scan_id, num_imgs=NUM_IMAGES, img_size=SIZE, mri_type=\"FLAIR\", split=\"train\"):\n\n    files = sorted(glob.glob(f\"{data_directory}/{split}/{scan_id}/{mri_type}/*.dcm\"))\n    \n    middle = len(files)//2\n    num_imgs2 = num_imgs//2\n    p1 = max(0, middle - num_imgs2)\n    p2 = min(len(files), middle + num_imgs2)\n    img3d = np.stack([load_dicom_image(f) for f in files[p1:p2]]).T \n    if img3d.shape[-1] < num_imgs:\n        n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n        img3d = np.concatenate((img3d,  n_zero), axis = -1)\n            \n    return np.expand_dims(img3d,0)\n\nload_dicom_images_3d(\"00000\").shape","metadata":{"execution":{"iopub.status.busy":"2021-10-03T03:59:36.876599Z","iopub.execute_input":"2021-10-03T03:59:36.876947Z","iopub.status.idle":"2021-10-03T03:59:37.591389Z","shell.execute_reply.started":"2021-10-03T03:59:36.876912Z","shell.execute_reply":"2021-10-03T03:59:37.590615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\nset_seed(3407)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T03:59:37.59289Z","iopub.execute_input":"2021-10-03T03:59:37.593144Z","iopub.status.idle":"2021-10-03T03:59:37.655951Z","shell.execute_reply.started":"2021-10-03T03:59:37.593118Z","shell.execute_reply":"2021-10-03T03:59:37.655184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(f\"{data_directory}/train_labels.csv\")\ndisplay(train_df)\n\ndf_train, df_valid = sk_model_selection.train_test_split(\n    train_df, \n    test_size=0.2, \n    random_state=177, \n    stratify=train_df[\"MGMT_value\"],\n)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-03T03:59:37.657567Z","iopub.execute_input":"2021-10-03T03:59:37.657821Z","iopub.status.idle":"2021-10-03T03:59:37.690774Z","shell.execute_reply.started":"2021-10-03T03:59:37.657795Z","shell.execute_reply":"2021-10-03T03:59:37.689874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.tail()","metadata":{"execution":{"iopub.status.busy":"2021-10-03T03:59:37.692055Z","iopub.execute_input":"2021-10-03T03:59:37.692612Z","iopub.status.idle":"2021-10-03T03:59:37.702337Z","shell.execute_reply.started":"2021-10-03T03:59:37.69257Z","shell.execute_reply":"2021-10-03T03:59:37.701258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch_data.Dataset):\n    def __init__(self, paths, targets=None, mri_type=None, label_smoothing=0.01, split=\"train\"):\n        self.paths = paths\n        self.targets = targets\n        self.mri_type = mri_type\n        self.label_smoothing = label_smoothing\n        self.split = split\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):\n        scan_id = self.paths[index]\n        if self.targets is None:\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=self.split)\n        else:\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], split=\"train\")\n\n        if self.targets is None:\n            return {\"X\": torch.tensor(data).float(), \"id\": scan_id}\n        else:\n            y = torch.tensor(abs(self.targets[index]-self.label_smoothing), dtype=torch.float)\n            return {\"X\": torch.tensor(data).float(), \"y\": y}\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=1)\n        n_features = self.net._fc.in_features\n        self.net._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n    \n    def forward(self, x):\n        out = self.net(x)\n        return out\n\n# class Model(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.net1 = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=1)\n#         self.net2 = EfficientNet3D.from_name(\"efficientnet-b1\", override_params={'num_classes': 2}, in_channels=1)\n#         n_features = self.net1._fc.in_features\n#         self.net1._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n#         self.net2._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n#         self.classifier = nn.Linear(2, 1)\n    \n#     def forward(self, x):\n#         out1 = self.net1(x)\n#         out2 = self.net2(x)\n#         out = torch.cat((out1, out2), dim=1)\n#         out = self.classifier(out)\n#         return out\n    \n    \n\nclass Trainer:\n    def __init__(\n        self, \n        model, \n        device, \n        optimizer, \n        criterion\n    ):\n        self.model = model\n        self.device = device\n        self.optimizer = optimizer\n        self.criterion = criterion\n\n        self.best_valid_score = np.inf\n        self.n_patience = 0\n        self.lastmodel = None\n        \n    def fit(self, epochs, train_loader, valid_loader, save_path, patience):        \n        for n_epoch in range(1, epochs + 1):\n            self.info_message(\"EPOCH: {}\", n_epoch)\n            \n            train_loss, train_time = self.train_epoch(train_loader)\n            valid_loss, valid_auc, valid_time = self.valid_epoch(valid_loader)\n            \n            self.info_message(\n                \"[Epoch Train: {}] loss: {:.4f}, time: {:.2f} s            \",\n                n_epoch, train_loss, train_time\n            )\n            \n            self.info_message(\n                \"[Epoch Valid: {}] loss: {:.4f}, auc: {:.4f}, time: {:.2f} s\",\n                n_epoch, valid_loss, valid_auc, valid_time\n            )\n\n            # if True:\n            # if self.best_valid_score < valid_auc: \n            if self.best_valid_score > valid_loss: \n                self.save_model(n_epoch, save_path, valid_loss, valid_auc)\n                self.info_message(\n                     \"auc improved from {:.4f} to {:.4f}. Saved model to '{}'\", \n                    self.best_valid_score, valid_loss, self.lastmodel\n                )\n                self.best_valid_score = valid_loss\n                self.n_patience = 0\n            else:\n                self.n_patience += 1\n            \n            if self.n_patience >= patience:\n                self.info_message(\"\\nValid auc didn't improve last {} epochs.\", patience)\n                break\n            \n    def train_epoch(self, train_loader):\n        self.model.train()\n        t = time.time()\n        sum_loss = 0\n\n        for step, batch in enumerate(train_loader, 1):\n            X = batch[\"X\"].to(self.device)\n            targets = batch[\"y\"].to(self.device)\n            self.optimizer.zero_grad()\n            outputs = self.model(X).squeeze(1)\n            \n            loss = self.criterion(outputs, targets)\n            loss.backward()\n\n            sum_loss += loss.detach().item()\n\n            self.optimizer.step()\n            \n            message = 'Train Step {}/{}, train_loss: {:.4f}'\n            self.info_message(message, step, len(train_loader), sum_loss/step, end=\"\\r\")\n        \n        return sum_loss/len(train_loader), int(time.time() - t)\n    \n    def valid_epoch(self, valid_loader):\n        self.model.eval()\n        t = time.time()\n        sum_loss = 0\n        y_all = []\n        outputs_all = []\n\n        for step, batch in enumerate(valid_loader, 1):\n            with torch.no_grad():\n                X = batch[\"X\"].to(self.device)\n                targets = batch[\"y\"].to(self.device)\n\n                outputs = self.model(X).squeeze(1)\n                loss = self.criterion(outputs, targets)\n\n                sum_loss += loss.detach().item()\n                y_all.extend(batch[\"y\"].tolist())\n                outputs_all.extend(outputs.tolist())\n\n            message = 'Valid Step {}/{}, valid_loss: {:.4f}'\n            self.info_message(message, step, len(valid_loader), sum_loss/step, end=\"\\r\")\n            \n        y_all = [1 if x > 0.5 else 0 for x in y_all]\n        auc = roc_auc_score(y_all, outputs_all)\n        \n        return sum_loss/len(valid_loader), auc, int(time.time() - t)\n    \n    def save_model(self, n_epoch, save_path, loss, auc):\n        self.lastmodel = f\"{save_path}-e{n_epoch}-loss{loss:.3f}-auc{auc:.3f}.pth\"\n        torch.save(\n            {\n                \"model_state_dict\": self.model.state_dict(),\n                \"optimizer_state_dict\": self.optimizer.state_dict(),\n                \"best_valid_score\": self.best_valid_score,\n                \"n_epoch\": n_epoch,\n            },\n            self.lastmodel,\n        )\n    \n    @staticmethod\n    def info_message(message, *args, end=\"\\n\"):\n        print(message.format(*args), end=end)\n        \ndef predict(modelfile, df, mri_type, split):\n    #print(\"Predict:\", modelfile, mri_type, df.shape)\n    df.loc[:,\"MRI_Type\"] = mri_type\n    data_retriever = Dataset(\n        df.index.values, \n        mri_type=df[\"MRI_Type\"].values,\n        split=split\n    )\n\n    data_loader = torch_data.DataLoader(\n        data_retriever,\n        batch_size=4,\n        shuffle=False,\n        num_workers=8,\n    )\n   \n    model = Model()\n    model.to(device)\n    \n    checkpoint = torch.load(modelfile)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    \n    y_pred = []\n    ids = []\n\n    for e, batch in enumerate(data_loader,1):\n        print(f\"{e}/{len(data_loader)}\", end=\"\\r\")\n        with torch.no_grad():\n            tmp_pred = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n            if tmp_pred.size == 1:\n                y_pred.append(tmp_pred)\n            else:\n                y_pred.extend(tmp_pred.tolist())\n            ids.extend(batch[\"id\"].numpy().tolist())\n            \n    preddf = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred}) \n    preddf = preddf.set_index(\"BraTS21ID\")\n    return preddf","metadata":{"execution":{"iopub.status.busy":"2021-10-03T03:59:37.704578Z","iopub.execute_input":"2021-10-03T03:59:37.705021Z","iopub.status.idle":"2021-10-03T03:59:37.740907Z","shell.execute_reply.started":"2021-10-03T03:59:37.704982Z","shell.execute_reply":"2021-10-03T03:59:37.740047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_mri_type(df_train, df_valid, mri_type,checkpoint_path):\n    if mri_type==\"all\":\n        train_list = []\n        valid_list = []\n        for mri_type in mri_types:\n            df_train.loc[:,\"MRI_Type\"] = mri_type\n            train_list.append(df_train.copy())\n            df_valid.loc[:,\"MRI_Type\"] = mri_type\n            valid_list.append(df_valid.copy())\n\n        df_train = pd.concat(train_list)\n        df_valid = pd.concat(valid_list)\n    else:\n        df_train.loc[:,\"MRI_Type\"] = mri_type\n        df_valid.loc[:,\"MRI_Type\"] = mri_type\n\n    print(df_train.shape, df_valid.shape)\n    display(df_train.head())\n    \n    train_data_retriever = Dataset(\n        df_train[\"BraTS21ID\"].values, \n        df_train[\"MGMT_value\"].values, \n        df_train[\"MRI_Type\"].values\n    )\n\n    valid_data_retriever = Dataset(\n        df_valid[\"BraTS21ID\"].values, \n        df_valid[\"MGMT_value\"].values,\n        df_valid[\"MRI_Type\"].values\n    )\n\n    train_loader = torch_data.DataLoader(\n        train_data_retriever,\n        batch_size=4,\n        shuffle=True,\n        num_workers=8,\n    )\n\n    valid_loader = torch_data.DataLoader(\n        valid_data_retriever, \n        batch_size=4,\n        shuffle=False,\n        num_workers=8,\n    )\n\n    model = Model()\n    \n\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.to(device)\n\n#     #print(model)\n\n#     optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n#     #optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n\n#     criterion = torch_functional.binary_cross_entropy_with_logits\n\n#     trainer = Trainer(\n#         model, \n#         device, \n#         optimizer, \n#         criterion\n#     )\n\n#     history = trainer.fit(\n#         0, \n#         train_loader, \n#         valid_loader, \n#         f\"{mri_type}\", \n#         0,\n#     )\n    \n    return model\n\nmodelfiles = None\n\n# if not modelfiles:\n# modelfiles = [train_mri_type(df_train, df_valid, 'FLAIR','../input/efficientnet3d-with-one-mri-type/FLAIR-e2-loss0.696-auc0.605.pth'),\n#               train_mri_type(df_train, df_valid, 'T1w','../input/efficientnet3d-with-one-mri-type/T1w-e2-loss0.718-auc0.579.pth'),\n#               train_mri_type(df_train, df_valid, 'T1wCE','../input/efficientnet3d-with-one-mri-type/T1wCE-e6-loss0.683-auc0.633.pth'),\n#               train_mri_type(df_train, df_valid, 'T2w','../input/efficientnet3d-with-one-mri-type/T2w-e8-loss0.658-auc0.677.pth'),\n#              ]\n#print(modelfiles)","metadata":{"execution":{"iopub.status.busy":"2021-10-03T03:59:37.743889Z","iopub.execute_input":"2021-10-03T03:59:37.744358Z","iopub.status.idle":"2021-10-03T03:59:37.756248Z","shell.execute_reply.started":"2021-10-03T03:59:37.74432Z","shell.execute_reply":"2021-10-03T03:59:37.755274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# modelfiles = [\n#     '../input/efficientnet3d684/T1w-e7-loss0.685-auc0.555.pth',\n#     '../input/efficientnet3d684/T1wCE-e6-loss0.683-auc0.633.pth',\n#     '../input/efficientnet3d684/T2w-e8-loss0.658-auc0.677.pth',\n# ]\nmodelfiles = [\n    '../input/efficientnet3d684/T1w-e7-loss0.685-auc0.555.pth',\n    '../input/efficientnet3d684/T1wCE-e6-loss0.683-auc0.633.pth',\n    '../input/efficientnet3d684/T2w-e8-loss0.658-auc0.677.pth',\n]\n\nmri_types = ['T1w', \n             'T1wCE',\n             'T2w',]","metadata":{"execution":{"iopub.status.busy":"2021-10-03T03:59:37.758027Z","iopub.execute_input":"2021-10-03T03:59:37.758587Z","iopub.status.idle":"2021-10-03T03:59:37.766988Z","shell.execute_reply.started":"2021-10-03T03:59:37.758505Z","shell.execute_reply":"2021-10-03T03:59:37.766075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid = df_valid.set_index(\"BraTS21ID\")\ndf_valid[\"MGMT_pred\"] = 0\nfor m, mtype in zip(modelfiles,  mri_types):\n    pred = predict(m, df_valid, mtype, \"train\")\n    df_valid[\"MGMT_pred\"] += pred[\"MGMT_value\"]\ndf_valid[\"MGMT_pred\"] /= len(modelfiles)\nauc = roc_auc_score(df_valid[\"MGMT_value\"], df_valid[\"MGMT_pred\"])\nprint(f\"Validation ensemble AUC: {auc:.4f}\")\nsns.displot(df_valid[\"MGMT_pred\"])","metadata":{"execution":{"iopub.status.busy":"2021-10-03T03:59:37.76953Z","iopub.execute_input":"2021-10-03T03:59:37.769831Z","iopub.status.idle":"2021-10-03T04:01:23.017273Z","shell.execute_reply.started":"2021-10-03T03:59:37.769806Z","shell.execute_reply":"2021-10-03T04:01:23.016083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nimport glob\nimport random\nimport collections\n\nimport numpy as np\nimport pandas as pd\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom tqdm.notebook import tqdm\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers\n\n\n\nTYPES = [\"FLAIR\", \"T1w\", \"T2w\", \"T1wCE\"]\nWHITE_THRESHOLD = 10 # out of 255\nEXCLUDE = [109, 123, 709]\n\n\ntrain_df = pd.read_csv(\"../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv\")\ntest_df = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/sample_submission.csv')\ntrain_df = train_df[~train_df.BraTS21ID.isin(EXCLUDE)]\ndef load_dicom(path, size = 224):\n    ''' \n    Reads a DICOM image, standardizes so that the pixel values are between 0 and 1, then rescales to 0 and 255\n    \n    Note super sure if this kind of scaling is appropriate, but everyone seems to do it. \n    '''\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    if np.max(data) != 0:\n        data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return cv2.resize(data, (size, size))\n\ndef get_all_image_paths(brats21id, image_type, folder='train'): \n    '''\n    Returns an arry of all the images of a particular type for a particular patient ID\n    '''\n    assert(image_type in TYPES)\n    \n    patient_path = os.path.join(\n        \"../input/rsna-miccai-brain-tumor-radiogenomic-classification/%s/\" % folder, \n        str(brats21id).zfill(5),\n    )\n\n    paths = sorted(\n        glob.glob(os.path.join(patient_path, image_type, \"*\")), \n        key=lambda x: int(x[:-4].split(\"-\")[-1]),\n    )\n    \n    num_images = len(paths)\n    \n    start = int(num_images * 0.25)\n    end = int(num_images * 0.75)\n\n    interval = 3\n    \n    if num_images < 10: \n        interval = 1\n    \n    return np.array(paths[start:end:interval])\n\ndef get_all_images(brats21id, image_type, folder='train', size=225):\n    return [load_dicom(path, size) for path in get_all_image_paths(brats21id, image_type, folder)]\nIMAGE_SIZE = 128\n\ndef get_all_data_for_train(image_type):\n    global train_df\n    \n    X = []\n    y = []\n    train_ids = []\n\n    for i in tqdm(train_df.index):\n        x = train_df.loc[i]\n        images = get_all_images(int(x['BraTS21ID']), image_type, 'train', IMAGE_SIZE)\n        label = x['MGMT_value']\n\n        X += images\n        y += [label] * len(images)\n        train_ids += [int(x['BraTS21ID'])] * len(images)\n        assert(len(X) == len(y))\n    return np.array(X), np.array(y), np.array(train_ids)\n\ndef get_all_data_for_test(image_type):\n    global test_df\n    \n    X = []\n    test_ids = []\n\n    for i in tqdm(test_df.index):\n        x = test_df.loc[i]\n        images = get_all_images(int(x['BraTS21ID']), image_type, 'test', IMAGE_SIZE)\n        X += images\n        test_ids += [int(x['BraTS21ID'])] * len(images)\n\n    return np.array(X), np.array(test_ids)\n\nX_test, testidt = get_all_data_for_test('T1wCE')\nfile_path = '../input/rsna-miccai-2dcnn-inference/best_model.h5'\nmodel_best = tf.keras.models.load_model(filepath=file_path)\nsample = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/sample_submission.csv')\n\ny_pred = model_best.predict(X_test)\n\npred = np.argmax(y_pred, axis=1)\n\nresult=pd.DataFrame(testidt)\nresult[1]=pred\nresult.columns=['BraTS21ID','MGMT_value']\nresult2 = result.groupby('BraTS21ID',as_index=False).mean()\nresult2['BraTS21ID'] = sample['BraTS21ID']","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:01:23.019266Z","iopub.execute_input":"2021-10-03T04:01:23.019641Z","iopub.status.idle":"2021-10-03T04:01:54.197675Z","shell.execute_reply.started":"2021-10-03T04:01:23.019601Z","shell.execute_reply":"2021-10-03T04:01:54.196475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2_pred = result2['MGMT_value']","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:01:54.199985Z","iopub.execute_input":"2021-10-03T04:01:54.200351Z","iopub.status.idle":"2021-10-03T04:01:54.206291Z","shell.execute_reply.started":"2021-10-03T04:01:54.200313Z","shell.execute_reply":"2021-10-03T04:01:54.204987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Scalers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\n\n# Models\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n#Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nimport pydicom\nimport random\nimport matplotlib.pyplot as plt\nimport glob\n\n# directory setting\nINPUT = '../input/rsna-miccai-brain-tumor-radiogenomic-classification'\n\ntrain_lab = pd.read_csv(INPUT + '/' + 'train_labels.csv')\nsample_sub = pd.read_csv(INPUT + '/' + 'sample_submission.csv')\n\nprint('Train labels')\ntrain_lab\n\ntemp = train_lab['BraTS21ID'] + 100000\nitem_id = []\nfor i in range(len(train_lab)):\n    item_id = item_id + [str(temp[i])[-5:]]\nprint('Number of samples in training data')\nlen(item_id)   # 585\n\nfrom tqdm import tqdm\n\ndef AddDif1(df):\n    df2 = df\n    df2['d1'] = df['c11'] - df['c12']\n    df2['d2'] = df['c12'] - df['c13']\n    df2['d3'] = df['c13'] - df['c14']\n    df2['d4'] = df['c14'] - df['c15']\n    df2['d5'] = df['c15'] - df['c16']\n    return df2\n    \n\n\ntrain_lab = pd.read_csv('../input/train-data/Table_trainA.csv')\ntrain_lab2 = train_lab[train_lab['c2'] != 0]\ntrain_data = train_lab2.reset_index()\ntrain_data = AddDif1(train_data)\ntrain_data\n\ntemp = sample_sub['BraTS21ID'] + 100000\nitem_id = []\nfor i in range(len(sample_sub)):\n    item_id = item_id + [str(temp[i])[-5:]]\nprint('Number of samples in test data')\nlen(item_id)   # 87\n\ntrain_lab = sample_sub\n\nprint(i, 'number of images', 'intensity', 'volume', 'average', 'Gmin', 'Gmax', 'Gmax-average', 'CmaxName')# for i in range(len(item_id[:10])):\n# for i in range(len(item_id[:1])):\nfor i in tqdm(range(len(item_id[:]))):\n    item_fol = os.listdir(INPUT + '/test/' + item_id[i] + '/FLAIR')\n    item_fol2 = []\n    for j in item_fol:\n#         k = 'A' + j[6:len(j)-4]\n        k = 1000 + int(j[6:len(j)-4])\n        item_fol2 = item_fol2 + [k]\n    item_fols = sorted(item_fol2)\n    volume = 0\n    intensity = 0\n    vac = 0\n    Gmax = 0\n    Gmin = 0\n    Amax = 0\n    Imax = 0\n    area_prev = 0\n    sumN_prev = 0\n    changeMax = 0\n    maxName ='none'\n    AmaxName ='none'\n    ImaxName ='none'\n    CmaxName ='00000'\n    for j in item_fols:\n        l = str(j-1000)\n        path = INPUT + '/test/' + item_id[i] + '/FLAIR/Image-' + l + '.dcm'\n        dicom = pydicom.read_file(path)\n        data = dicom.pixel_array\n        sumN = np.sum(data)\n        sumN_plus = sumN - sumN_prev\n        sumN_prev = sumN\n        if sumN > Imax:\n            Imax = sumN\n            ImaxName = j\n        maxN = np.max(data)\n        if maxN > Gmax:\n            Gmax = maxN\n            maxName = j\n        minN = np.min(data)\n        if minN < Gmin:\n            Gmin = minN\n        zerocount = np.count_nonzero(data == 0)\n        area = np.count_nonzero(data != 0)\n        area_plus = area - area_prev\n        area_prev = area\n        if area > Amax:\n            Amax = area\n            AmaxName = j\n        change = -(sumN_plus/area_plus)\n        if change > changeMax:\n            changeMax = change\n            CmaxName = j\n        intensity = intensity + sumN\n        volume = volume + area\n        vac = vac + zerocount\n#         print(i, j, sumN, maxN, minN, zerocount, area, area_plus, sumN_plus, change)\n    average = intensity/volume\n    train_lab.loc[i,'c1'] = len(item_fol)\n    train_lab.loc[i,'c2'] = int(intensity)\n    train_lab.loc[i,'c3'] = volume\n    train_lab.loc[i,'c4'] = vac\n    train_lab.loc[i,'c5'] = volume+vac\n    train_lab.loc[i,'c6'] = average\n    train_lab.loc[i,'c7'] = Gmin\n    train_lab.loc[i,'c8'] = Gmax\n    train_lab.loc[i,'c9'] = Gmax-average\n    train_lab.loc[i,'c10'] = 'Image-' + str(int(CmaxName) -1000) + '.dcm'\n#     train_lab.loc[i,['c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']] = (len(item_fol), intensity, volume, vac, volume+vac, average, Gmin, Gmax, int(Gmax-average))\n#     train_lab.loc[i,['c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']] = (len(item_fol), intensity, volume, vac, volume+vac, average, Gmin, Gmax, int(Gmax-average))\n#    print(i, len(item_fol), intensity, volume, average, Gmin, Gmax, Gmax-average, 'Image-' + str(int(CmaxName) -1000) + '.dcm')\n\n#     train_lab.c1[i] = len(item_fol)\n\n# print(item_id[:1], maxName, AmaxName, ImaxName)\ntrain_lab.head(10)\n\nprint(i, 'number of images', 'intensity', 'volume', 'average', 'Gmin', 'Gmax', 'Gmax-average', 'CmaxName')# for i in range(len(item_id[:10])):\n# for i in range(len(item_id[:1])):\nfor i in tqdm(range(len(item_id[:]))):\n    item_fol = os.listdir(INPUT + '/test/' + item_id[i] + '/FLAIR')\n    item_fol2 = []\n    for j in item_fol:\n#         k = 'A' + j[6:len(j)-4]\n        k = 1000 + int(j[6:len(j)-4])\n        item_fol2 = item_fol2 + [k]\n    item_fols = sorted(item_fol2)\n#     volume = 0\n#     intensity = 0\n#     vac = 0\n#     Gmax = 0\n#     Gmin = 0\n#     Amax = 0\n#     Imax = 0\n#     area_prev = 0\n#     sumN_prev = 0\n#     changeMax = 0\n#     maxName ='none'\n#     AmaxName ='none'\n#     ImaxName ='none'\n#     CmaxName ='00000'\n    P50 = 0\n    P60 = 0\n    P70 = 0\n    P80 = 0\n    P90 = 0\n    P95 = 0\n    F2 = 0\n    F3 = 0\n    F4 = 0\n    F5 = 0\n    F6 = 0\n    val50 = train_lab['c9'][i] * 0.5 + train_lab['c6'][i]\n    val60 = train_lab['c9'][i] * 0.6 + train_lab['c6'][i]\n    val70 = train_lab['c9'][i] * 0.7 + train_lab['c6'][i]\n    val80 = train_lab['c9'][i] * 0.8 + train_lab['c6'][i]\n    val90 = train_lab['c9'][i] * 0.9 + train_lab['c6'][i]\n    val95 = train_lab['c9'][i] * 0.95 + train_lab['c6'][i]\n    F2val = train_lab['c6'][i] * 2\n    F3val = train_lab['c6'][i] * 3\n    F4val = train_lab['c6'][i] * 4\n    F5val = train_lab['c6'][i] * 5\n    F6val = train_lab['c6'][i] * 6\n    for j in item_fols:\n        l = str(j-1000)\n        path = INPUT + '/test/' + item_id[i] + '/FLAIR/Image-' + l + '.dcm'\n        dicom = pydicom.read_file(path)\n        data = dicom.pixel_array\n#         sumN = np.sum(data)\n#         sumN_plus = sumN - sumN_prev\n#         sumN_prev = sumN\n#         if sumN > Imax:\n#             Imax = sumN\n#             ImaxName = j\n#         maxN = np.max(data)\n#         if maxN > Gmax:\n#             Gmax = maxN\n#             maxName = j\n#         minN = np.min(data)\n#         if minN < Gmin:\n#             Gmin = minN\n        count50 = np.count_nonzero(data > val50)\n        count60 = np.count_nonzero(data > val60)\n        count70 = np.count_nonzero(data > val70)\n        count80 = np.count_nonzero(data > val80)\n        count90 = np.count_nonzero(data > val90)\n        count95 = np.count_nonzero(data > val95)\n        countF2 = np.count_nonzero(data > F2val)\n        countF3 = np.count_nonzero(data > F3val)\n        countF4 = np.count_nonzero(data > F4val)\n        countF5 = np.count_nonzero(data > F5val)\n        countF6 = np.count_nonzero(data > F6val)\n#         area = np.count_nonzero(data != 0)\n#         area_plus = area - area_prev\n#         area_prev = area\n#         if area > Amax:\n#             Amax = area\n#             AmaxName = j\n#         change = -(sumN_plus/area_plus)\n#         if change > changeMax:\n#             changeMax = change\n#             CmaxName = j\n#         intensity = intensity + sumN\n#         volume = volume + area\n        P50 = P50 + count50\n        P60 = P60 + count60\n        P70 = P70 + count70\n        P80 = P80 + count80\n        P90 = P90 + count90\n        P95 = P95 + count95\n        F2 = F2 + countF2\n        F3 = F3 + countF3\n        F4 = F4 + countF4\n        F5 = F5 + countF5\n        F6 = F6 + countF6\n#         print(i, j, sumN, maxN, minN, zerocount, area, area_plus, sumN_plus, change)\n#     average = intensity/volume\n#     train_lab.loc[i,'c1'] = len(item_fol)\n#     train_lab.loc[i,'c2'] = int(intensity)\n#     train_lab.loc[i,'c3'] = volume\n#     train_lab.loc[i,'c4'] = vac\n#     train_lab.loc[i,'c5'] = volume+vac\n#     train_lab.loc[i,'c6'] = int(average)\n#     train_lab.loc[i,'c7'] = Gmin\n#     train_lab.loc[i,'c8'] = Gmax\n#     train_lab.loc[i,'c9'] = int(Gmax-average)\n#     train_lab.loc[i,'c10'] = 'Image-' + str(int(CmaxName) -1000) + '.dcm'\n    c3val = train_lab['c3'][i]\n    train_lab.loc[i,'c11'] = P50 * 1e7 / c3val\n    train_lab.loc[i,'c12'] = P60 * 1e7 / c3val\n    train_lab.loc[i,'c13'] = P70 * 1e7 / c3val\n    train_lab.loc[i,'c14'] = P80 * 1e7 / c3val\n    train_lab.loc[i,'c15'] = P90 * 1e7 / c3val\n    train_lab.loc[i,'c16'] = P95 * 1e7 / c3val\n    train_lab.loc[i,'c17'] = F2 *  1e7 / c3val\n    train_lab.loc[i,'c18'] = F3 *  1e7 / c3val\n    train_lab.loc[i,'c19'] = F4 * 1e7 / c3val\n    train_lab.loc[i,'c20'] = F5 * 1e7 / c3val\n    train_lab.loc[i,'c21'] = F6 * 1e7 / c3val\n#     train_lab.loc[i,['c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']] = (len(item_fol), intensity, volume, vac, volume+vac, average, Gmin, Gmax, int(Gmax-average))\n#     train_lab.loc[i,['c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']] = (len(item_fol), intensity, volume, vac, volume+vac, average, Gmin, Gmax, int(Gmax-average))\n#     print(i, (P50, P60, P70, P80, P90, P95, \n#     print(i, (F2, F3, F4, F5, F6)*1e7/train_lab['c3'][i])\n\n#     train_lab.c1[i] = len(item_fol)\n\n# print(item_id[:1], maxName, AmaxName, ImaxName)\ntrain_lab.head(10)\n\nsample_sub = train_lab     # <=== Caution!\n\nsample_sub.to_csv('Table_testA.csv')\ntest_data = sample_sub\ntest_data = AddDif1(test_data)\noutput = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/sample_submission.csv')\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\ny = train_data[\"MGMT_value\"]\n\nfeatures = [\"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c8\", \"c9\", \n#              \"c11\", \"c12\", \"c13\", \"c14\", \"c15\", \"c16\",\n            \"d1\", \"d2\", \"d3\", \"d4\", \"d5\"]\n#              \"c17\", \"c18\", \"c19\", \"c20\", \"c21\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=500, max_depth=7, random_state=42)\nmodel.fit(X, y)\n\ny_score1 = model.predict_proba(X)[:,1]\npredictions = model.predict_proba(X_test)[:,1]\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\n\nparams = {\n          'num_leaves': 5,\n#           'min_child_weight': 0.7,\n#           'feature_fraction': 0.01,\n#            'bagging_fraction': 0.8,\n#           'min_data_in_leaf': 5,\n#           'objective': 'binary',\n          'max_depth': 6,\n          'learning_rate': 0.15,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 3407,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n#           'reg_alpha': 0,\n#           'reg_lambda': 0.6485237330340494,\n          'random_state': 47,\n         }\n\n\nNFOLDS = 5\nfolds = KFold(n_splits=NFOLDS)\n\ny = train_data[\"MGMT_value\"]\nfeatures = [\"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c8\", \"c9\", \n#              \"c11\", \"c12\", \"c13\", \"c14\", \"c15\", \"c16\",\n            \"d1\", \"d2\", \"d3\", \"d4\", \"d5\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\ncolumns = X.columns\nsplits = folds.split(X, y)\ny_preds = np.zeros(X_test.shape[0])\ny_oof = np.zeros(X.shape[0])\nscore = 0\n\nfeature_importances = pd.DataFrame()\nfeature_importances['feature'] = columns\n  \nfor fold_n, (train_index, valid_index) in enumerate(splits):\n    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    dtrain = lgb.Dataset(X_train, label=y_train)\n    dvalid = lgb.Dataset(X_valid, label=y_valid)\n\n    clf = lgb.train(params, dtrain, 1000, valid_sets = [dtrain, dvalid], verbose_eval=20, early_stopping_rounds=100)\n    \n    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n    \n    y_pred_valid = clf.predict(X_valid)\n    y_oof[valid_index] = y_pred_valid\n    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n    \n    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n    y_preds += clf.predict(X_test) / NFOLDS\n    \n    del X_train, X_valid, y_train, y_valid\n    gc.collect()\n    \nprint(f\"\\nMean AUC = {score}\")\nprint(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")\n\nimport seaborn as sns\n\nfeature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(folds.n_splits)]].mean(axis=1)\nfeature_importances.to_csv('feature_importances.csv')\n\nplt.figure(figsize=(16, 16))\nsns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\nplt.title('50 TOP feature importance over {} folds average'.format(folds.n_splits));\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\ny_true1 = y\n# # y_score1 = model.predict_proba(X)[:,1]\n# y_score1 = gd.best_estimator_.predict_proba(X)[:,1]\n\nroc1 = roc_curve(y_true1, y_score1)\n\nfpr1, tpr1, thresholds1 = roc_curve(y_true1, y_score1)\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\ny_pred = np.round(y_score1, decimals = 0)\ntn1, fp1, fn1, tp1 = confusion_matrix(y_true1, y_pred).ravel()\nac1 = accuracy_score(y_true1, y_pred)\npr1 = precision_score(y_true1, y_pred)\nrc1 = recall_score(y_true1, y_pred)\nsp1 = tn1/(fp1+tn1)\nf11 = f1_score(y_true1, y_pred)\nphi1 = (tp1*tn1-fp1*fn1)/np.sqrt((tp1+fn1)*(tp1+fp1)*(tn1+fn1)*(tn1+fp1))\n\n\n\nplt.figure(figsize=(6,6))\nplt.plot((0,1), (0,1), color=\"black\", linestyle=\"--\")\nplt.plot(fpr1, tpr1, linewidth=3)#, marker='o')\n# plt.plot(fpr2, tpr2, linewidth=3)#, marker='o')\n# plt.plot(fpr3, tpr3, linewidth=3)#, marker='o')\nplt.tick_params(direction='in')\nplt.xlabel('FPR: False positive rate')\nplt.ylabel('TPR: True positive rate')\n# plt.legend(['train', 'valid', 'test'], loc='lower right')\nplt.grid()\nROC1=roc_auc_score(y_true1, y_score1)\n# ROC2=roc_auc_score(y_true2, y_score2)\n# ROC3=roc_auc_score(y_true3, y_score3)\nprint(ROC1)#,ROC2,ROC3)\n\n# predictions = model.predict_proba(X_test)[:,1]\n# predictions = gd.best_estimator_.predict_proba(X_test)[:,1]\noutput['MGMT_value'] = y_preds * 0.5 +predictions*0.5\noutput","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:01:54.208947Z","iopub.execute_input":"2021-10-03T04:01:54.209415Z","iopub.status.idle":"2021-10-03T04:04:04.028015Z","shell.execute_reply.started":"2021-10-03T04:01:54.209378Z","shell.execute_reply":"2021-10-03T04:04:04.026817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\nimport numpy as np\nimport sys\nnp.set_printoptions(threshold=sys.maxsize)\n\nimport pandas as pd\nimport os\nimport random\ndf = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv', header=0, names=['id','value'], dtype=object)\ndf = df[~df.id.isin([\"00109\", \"00123\", \"00709\"])]\n\n#https://stackoverflow.com/a/4836734/8245487\ndef natural_sort(l): \n    convert = lambda text: int(text) if text.isdigit() else text.lower()\n    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n    return sorted(l, key=alphanum_key)\n\nimport os\nimport pydicom\nimport pandas as pd\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom tqdm import tqdm\nimport binascii\nfrom PIL import Image\n\nINPUT = '../input/rsna-miccai-brain-tumor-radiogenomic-classification'\n\nif not os.path.exists('./train'):\n    os.makedirs('./train')\n    \n\nif not os.path.exists('./test'):\n    os.makedirs('./test')\n\n\ndef get_dicom_files(input_dir, dataset='train'):\n    for subdir, dirs, files in os.walk(f\"{input_dir}/{dataset}\"):\n        if len(files) == 0:\n            continue\n        filename = natural_sort(files)[len(files)//2] #take middle most image -- FLAIR DCM file per training item.\n        filepath = os.path.join(subdir, filename)\n        \n        if filepath.endswith(\".dcm\") and \"FLAIR\" in filepath:\n            cur_id = subdir.split('/')[-2]\n            outpath = os.path.join(f'./{dataset}',f'{cur_id}.png')\n            \n            process_dicom(filepath, outpath)\n\ndef process_dicom(path, outpath):\n    dicom = pydicom.read_file(path)\n    data = apply_voi_lut(dicom.pixel_array, dicom)\n    if dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    \n    height = len(data)\n    width = len(data[0])\n    \n    pixels_out = []\n    for row in data:\n        pixels_out.extend(row)\n    assert(len(pixels_out) == height * width)\n    \n    image_out = Image.new('L', (width, height))\n    image_out.putdata(pixels_out)\n    image_out.save(outpath)\n\nget_dicom_files(INPUT, 'train')\nget_dicom_files(INPUT, 'test')\n\n\n#     final = pd.DataFrame(final)\n#     final.to_csv(f\"{args['output']}/dicom_meta_{args['dataset']}.csv\", index=False)\n\nfor id_num in df.id:\n    full_path = './train/{}.png'.format(id_num)\n    df.loc[df.id == id_num, 'file'] = full_path\n\ndls = ImageDataLoaders.from_df(df, item_tfms=Resize(224), bs=64, label_col =1, fn_col=2, path='')\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self, pretrained=False):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        return x\n    \n\nlearn = cnn_learner(dls, Net, metrics=[error_rate, accuracy], model_dir=\"/tmp/model/\").to_fp16()\nlearn.lr_find()\nlearn.fit_one_cycle(10, lr_max=1e-2, cbs=ShortEpochCallback())\n\ndf_test = pd.DataFrame(columns=['id', 'value'])\ndf_test.id = os.listdir(\"../input/rsna-miccai-brain-tumor-radiogenomic-classification/test/\")\n\nfor id_num in df_test.id:\n    full_path = './test/{}.png'.format(id_num)\n    prediction = learn.predict(full_path)\n    print(prediction)\n    probability = prediction[2][1].item()\n    print(probability)\n    df_test.loc[df_test.id==id_num, 'value'] = probability\n    \ndf_test = df_test.rename(columns={'id':'BraTS21ID','value':'MGMT_value'})","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:04:04.030165Z","iopub.execute_input":"2021-10-03T04:04:04.030517Z","iopub.status.idle":"2021-10-03T04:06:18.862358Z","shell.execute_reply.started":"2021-10-03T04:04:04.030479Z","shell.execute_reply":"2021-10-03T04:06:18.861221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(f\"{data_directory}/sample_submission.csv\", index_col=\"BraTS21ID\")\nmri_types = ['T1w', \n             'T1wCE',\n             'T2w',]\n\nsubmission[\"MGMT_value\"] = 0\nfor m, mtype in zip(modelfiles, mri_types):\n    \n    pred = predict(m, submission, mtype, split=\"test\")\n    submission[\"MGMT_value\"] += pred[\"MGMT_value\"]\n    print(modelfiles,pred[\"MGMT_value\"])\n\nsubmission[\"MGMT_value\"] /= len(modelfiles)\n# submission[\"MGMT_value\"] = (submission[\"MGMT_value\"].values*0.8 + model2_pred.values*0.1 +df_test[\"MGMT_value\"].values*0.1)*0.98 +output['MGMT_value'].values*0.02\nsubmission[\"MGMT_value\"] = submission[\"MGMT_value\"].values*0.6 + model2_pred.values*0.4\nsubmission[\"MGMT_value\"].to_csv(\"submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:06:18.864851Z","iopub.execute_input":"2021-10-03T04:06:18.865945Z","iopub.status.idle":"2021-10-03T04:07:45.648645Z","shell.execute_reply.started":"2021-10-03T04:06:18.865901Z","shell.execute_reply":"2021-10-03T04:07:45.647482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(submission[\"MGMT_value\"])","metadata":{"execution":{"iopub.status.busy":"2021-10-03T04:07:45.651146Z","iopub.execute_input":"2021-10-03T04:07:45.65231Z","iopub.status.idle":"2021-10-03T04:07:45.981025Z","shell.execute_reply.started":"2021-10-03T04:07:45.65227Z","shell.execute_reply":"2021-10-03T04:07:45.979978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}