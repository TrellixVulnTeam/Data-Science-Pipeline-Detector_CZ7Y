{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"7345e9cc-0fea-3f32-8f0f-145bb6e906d4"},"source":"AllState Competition for Stat 198 @ UC Berkeley"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f213bda8-d83d-8193-7f27-f1eb374d57d9"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n#import plotting libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d96721a0-5f6d-dd80-f66f-1522ef1e1d14"},"outputs":[],"source":"Load Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"089db371-3394-e8f7-444b-598f73d79b04"},"outputs":[],"source":"# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Read raw data from the file\n\nimport pandas #provides data structures to quickly analyze data\n#Since this code runs on Kaggle server, data can be accessed directly in the 'input' folder\n#Read the train dataset\ndataset = pandas.read_csv(\"../input/train.csv\") \n\n#Read test dataset\ndataset_test = pandas.read_csv(\"../input/test.csv\")\n#Save the id's for submission file\nID = dataset_test['id']\n#Drop unnecessary columns\ndataset_test.drop('id',axis=1,inplace=True)\n\n#Print all rows and columns. Dont hide any\npandas.set_option('display.max_rows', None)\npandas.set_option('display.max_columns', None)\n\n#Display the first five rows to get a feel of the data\nprint(dataset.head(5))\n\n#Learning : cat1 to cat116 contain alphabets"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"81b03a0a-8887-9c79-e149-60c3c4cafda9"},"outputs":[],"source":"Statistics\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e0b646f5-14e1-3d35-7ead-19e13b50ceac"},"outputs":[],"source":"#EXPLORATION STATISTICS: Find the skew of the distribution\nprint(dataset.skew())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"547a1b24-4aff-2801-6665-2b9ec44446b9"},"outputs":[],"source":"#EXPLORATION GRAPHS: Box and Density plots for each variable to get a thorough understanding \n\n#range of features considered\nsplit = 116 \n\n#number of features considered\nsize = 15\n\n#create a dataframe with only continuous features\ndata=dataset.iloc[:,split:] \n\n#get the names of all the columns\ncols=data.columns \n\n#Plot violin for all attributes in a 7x2 grid\nn_cols = 2\nn_rows = 7\n\nfor i in range(n_rows):\n    fg,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(12, 8))\n    for j in range(n_cols):\n        sns.violinplot(y=cols[i*n_cols+j], data=dataset, ax=ax[j])\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"3cc117a8-2f0a-4930-d110-6571544ce56a"},"source":"Data Cleaning and Feature Engineering"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"93231ea9-4d03-4085-80e0-20e90ce7bdaa"},"outputs":[],"source":"#Skew Correction: we see that the skew is mostly corrected when the graph is displayed"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"50d68732-19f2-05e4-0211-db5c6224e658"},"outputs":[],"source":"#log1p function applies log(1+x) to all elements of the column\ndataset[\"loss\"] = numpy.log1p(dataset[\"loss\"])\n#visualize the transformed column\nsns.violinplot(data=dataset,y=\"loss\")  \nplt.show()\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"3754bf58-a13e-0a11-b330-a21ef1507ae5"},"source":"Feature Engineering"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4a8e6d0f-28d0-c034-fbe0-03612729deb4"},"outputs":[],"source":"#need to do"},{"cell_type":"markdown","metadata":{"_cell_guid":"03245ede-7ae3-f14f-3c4c-a34bfd7abe56"},"source":"Split into Train and Validation\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3dda4061-66d2-91da-6a35-17bea224a9de"},"outputs":[],"source":"#get the number of rows and columns\nr, c = dataset_encoded.shape\n\n#create an array which has indexes of columns\ni_cols = []\nfor i in range(0,c-1):\n    i_cols.append(i)\n\n#Y is the target column, X has the rest\nX = dataset_encoded[:,0:(c-1)]\nY = dataset_encoded[:,(c-1)]\ndel dataset_encoded\n\n#Validation chunk size\nval_size = 0.1\n\n#Use a common seed in all experiments so that same chunk is used for validation\nseed = 0\n\n#Split the data into chunks\nfrom sklearn import cross_validation\nX_train, X_val, Y_train, Y_val = cross_validation.train_test_split(X, Y, test_size=val_size, random_state=seed)\ndel X\ndel Y\n\n#All features\nX_all = []\n\n#List of combinations\ncomb = []\n\n#Dictionary to store the MAE for all algorithms \nmae = []\n\n#Scoring parameter\nfrom sklearn.metrics import mean_absolute_error\n\n#Add this version of X to the list \nn = \"All\"\n#X_all.append([n, X_train,X_val,i_cols])\nX_all.append([n, i_cols])"},{"cell_type":"markdown","metadata":{"_cell_guid":"d3928c77-3df2-cec9-e819-95496f151c77"},"source":"Running a Model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"464766c4-eaff-2725-db51-f1271088fd72"},"outputs":[],"source":"#Evaluation of various combinations of LinearRegression\n\n#Import the library\nfrom sklearn.linear_model import LinearRegression\n\n#uncomment the below lines if you want to run the algo\n##Set the base model\n#model = LinearRegression(n_jobs=-1)\n#algo = \"LR\"\n#\n##Accuracy of the model using all features\n#for name,i_cols_list in X_all:\n#    model.fit(X_train[:,i_cols_list],Y_train)\n#    result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n#    mae.append(result)\n#    print(name + \" %s\" % result)\n#comb.append(algo)\n\n#Result obtained after running the algo. Comment the below two lines if you want to run the algo\nmae.append(1278)\ncomb.append(\"LR\" )    \n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#MAE achieved is 1278"},{"cell_type":"markdown","metadata":{"_cell_guid":"4e074204-b6b7-2a03-fa81-b56e62d6cb7d"},"source":"Running another model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76fc8f7b-e3bc-ffad-e8c1-5d8a204ca936"},"outputs":[],"source":"#Evaluation of various combinations of KNN\n\n#Import the library\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#Add the N value to the below list if you want to run the algo\nn_list = numpy.array([])\n\nfor n_neighbors in n_list:\n    #Set the base model\n    model = KNeighborsRegressor(n_neighbors=n_neighbors,n_jobs=-1)\n    \n    algo = \"KNN\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % n_neighbors )\n\nif (len(n_list)==0):\n    mae.append(1745)\n    comb.append(\"KNN\" + \" %s\" % 1 )\n    \n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Very high computation time\n#Best estimated performance is 1745 for n=1"},{"cell_type":"markdown","metadata":{"_cell_guid":"8d125282-c598-9ee6-f784-e8c400ef94d3"},"source":"Model Evaluation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"480d0558-205f-60c6-22e4-d59f4fa00ffc"},"outputs":[],"source":"#Evaluation of various combinations of XGB\n\n#Import the library\nfrom xgboost import XGBRegressor\n\n#Add the n_estimators value to the below list if you want to run the algo\nn_list = numpy.array([])\n\nfor n_estimators in n_list:\n    #Set the base model\n    model = XGBRegressor(n_estimators=n_estimators,seed=seed)\n    \n    algo = \"XGB\"\n\n    #Accuracy of the model using all features\n    for name,i_cols_list in X_all:\n        model.fit(X_train[:,i_cols_list],Y_train)\n        result = mean_absolute_error(numpy.expm1(Y_val), numpy.expm1(model.predict(X_val[:,i_cols_list])))\n        mae.append(result)\n        print(name + \" %s\" % result)\n        \n    comb.append(algo + \" %s\" % n_estimators )\n\nif (len(n_list)==0):\n    mae.append(1169)\n    comb.append(\"XGB\" + \" %s\" % 1000 )    \n    \n##Set figure size\n#plt.rc(\"figure\", figsize=(25, 10))\n\n##Plot the MAE of all combinations\n#fig, ax = plt.subplots()\n#plt.plot(mae)\n##Set the tick names to names of combinations\n#ax.set_xticks(range(len(comb)))\n#ax.set_xticklabels(comb,rotation='vertical')\n##Plot the accuracy for all combinations\n#plt.show()    \n\n#Best estimated performance is 1169 with n=1000# Make predictions using XGB as it gave the best estimated performance        \n\nX = numpy.concatenate((X_train,X_val),axis=0)\ndel X_train\ndel X_val\nY = numpy.concatenate((Y_train,Y_val),axis=0)\ndel Y_train\ndel Y_val\n\nn_estimators = 1000\n\n#Best model definition\nbest_model = XGBRegressor(n_estimators=n_estimators,seed=seed)\nbest_model.fit(X,Y)\ndel X\ndel Y\n#Read test dataset\ndataset_test = pandas.read_csv(\"../input/test.csv\")\n#Drop unnecessary columns\nID = dataset_test['id']\ndataset_test.drop('id',axis=1,inplace=True)\n\n#One hot encode all categorical attributes\ncats = []\nfor i in range(0, split):\n    #Label encode\n    label_encoder = LabelEncoder()\n    label_encoder.fit(labels[i])\n    feature = label_encoder.transform(dataset_test.iloc[:,i])\n    feature = feature.reshape(dataset_test.shape[0], 1)\n    #One hot encode\n    onehot_encoder = OneHotEncoder(sparse=False,n_values=len(labels[i]))\n    feature = onehot_encoder.fit_transform(feature)\n    cats.append(feature)\n\n# Make a 2D array from a list of 1D arrays\nencoded_cats = numpy.column_stack(cats)\n\ndel cats\n\n#Concatenate encoded attributes with continuous attributes\nX_test = numpy.concatenate((encoded_cats,dataset_test.iloc[:,split:].values),axis=1)\n\ndel encoded_cats\ndel dataset_test\n\n#Make predictions using the best model\npredictions = numpy.expm1(best_model.predict(X_test))\ndel X_test\n# Write submissions to output file in the correct format\nwith open(\"submission.csv\", \"w\") as subfile:\n    subfile.write(\"id,loss\\n\")\n    for i, pred in enumerate(list(predictions)):\n        subfile.write(\"%s,%s\\n\"%(ID[i],pred))"},{"cell_type":"markdown","metadata":{"_cell_guid":"62fb6852-2cd3-5bdf-d53e-cf3540401684"},"source":"Model Interpretation"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"418891a5-0635-9194-8590-3a140e255ea6"},"outputs":[],"source":"#need to do\n4 points for interpretation of any kind\n3 points for thoroughness (did you fully grasp what your model is telling you about the data?)\n3 points for comparing different models \n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"42ce28d5-54c5-6370-2b53-f10f9a2f81bd"},"outputs":[],"source":"Model Evaluation\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9485f001-6271-c196-2377-62d173461c9b"},"outputs":[],"source":"#need to do\n\n7 points for correct out-of-sample test accuracy to the public leaderboard accuracy (within a reasonable threshold)  \n3 points for demonstrating that your local test accuracy is correlated with the public leaderboard accuracy\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}