{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"29da0e9e-da0e-902f-4df0-d7a606442792"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport glob\nimport cv2\nimport datetime\nimport pandas as pd\nimport time\nimport warnings\n\nfrom __future__ import print_function\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"85e5aefa-30b6-e47a-fc86-c9367aaead60"},"outputs":[],"source":"def get_im_cv2(path):\n    img = cv2.imread(path)\n    resized = cv2.resize(img, (32, 32), cv2.INTER_LINEAR)\n    return resized\n\ndef get_im_tf(path, img_rows, img_cols, color_type=1):\n    img = tf.read_file(path)\n    # Load as grayscale\n    if color_type == 1:\n        img = tf.image.decode_jpeg(img, channels=1)\n    elif color_type == 3:\n        img = tf.image.decode_jpeg(img, channels=0)\n    # Reduce size\n    resized = tf.image.resize_images(img, (32,32), method= 0,align_corners=False)\n    return resized\n\n\ndef load_train(img_rows, img_cols, color_type=1):\n    x_train = []\n    x_train_id = []\n    y_train = []\n    #start_time = time.time()\n\n    print('Read train images')\n    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n    for fld in folders:\n        index = folders.index(fld)\n        print('Load folder {} (Index: {})'.format(fld, index))\n        path = os.path.join('..', 'input', 'train', fld, '*.jpg')\n        files = glob.glob(path)\n        for fl in files:\n            flbase = os.path.basename(fl)\n            img = get_im_cv2(fl)\n            x_train.append(img)\n            x_train_id.append(flbase)\n            y_train.append(index)\n\n    #print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n    return x_train, y_train, x_train_id\n\n\ndef load_test():\n    path = os.path.join('..', 'input', 'test_stg1', '*.jpg')\n    files = sorted(glob.glob(path))\n\n    X_test = []\n    X_test_id = []\n    for fl in files:\n        flbase = os.path.basename(fl)\n        img = get_im_cv2(fl)\n        X_test.append(img)\n        X_test_id.append(flbase)\n\n    return X_test, X_test_id\n\ndef to_categorical(y, nb_classes=None):\n    \"\"\"Converts a class vector (integers) to binary class matrix.\n    E.g. for use with categorical_crossentropy.\n    # Arguments\n        y: class vector to be converted into a matrix\n            (integers from 0 to nb_classes).\n        nb_classes: total number of classes.\n    # Returns\n        A binary matrix representation of the input.\n    \"\"\"\n    y = np.array(y, dtype='int').ravel()\n    if not nb_classes:\n        nb_classes = np.max(y) + 1\n    n = y.shape[0]\n    categorical = np.zeros((n, nb_classes))\n    categorical[np.arange(n), y] = 1\n    return categorical"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f6b9069-1e0c-2339-b1a7-6e65c27c42e0"},"outputs":[],"source":"train_data, train_target, train_id = load_train(32,32,1)\ntest_data, test_id = load_test()\n\n\nx_train = np.array(train_data, dtype=np.uint8)\ny_train = np.array(train_target, dtype=np.uint8)\n#x_train = x_train.transpose((0, 1, 2))\n\nx_test = np.array(test_data, dtype=np.uint8)\n#x_test = x_test.transpose((0, 3, 1, 2))\n\n# Normalizing the data\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n#x_train.reshape()\nprint(y_train.shape)\nprint('X_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"34e3fb2f-1e90-a168-39e4-a452e5c6611e"},"outputs":[],"source":"# Parameters\nlearning_rate = 0.001\ntraining_iters = 200000\nbatch_size = 128\ndisplay_step = 10\n\n# Network Parameters\nn_width = 32 # Fishes data input (img shape: 28*28)\nn_height=32\nn_classes = 8 # Fishes total classes (0-9 digits)\ndropout = 0.75 # Dropout, probability to keep units\n\n\n# tf Graph input\nimages_batch = tf.placeholder(tf.float32, [None,n_width,n_height,3])\nlabels_batch = tf.placeholder(tf.float32, [None, n_classes])\ny_train = to_categorical(y_train, n_classes)\nprint(y_train.shape)\nkeep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26ee5aee-d0e4-2fa9-2740-7914f7fa350e"},"outputs":[],"source":"def conv2d(x, W, b, strides=1):\n    # Conv2D wrapper, with bias and relu activation\n    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n    x = tf.nn.bias_add(x, b)\n    return tf.nn.relu(x)\n\n\ndef maxpool2d(x, k=2):\n    # MaxPool2D wrapper\n    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n                          padding='VALID')\n\n\n# Create model\ndef conv_net(x, weights, biases, dropout):\n    # Reshape input picture\n    print(\"Pablo0\")\n    #x = tf.reshape(x, shape=[-1, 32, 32, 3])\n    \n    print(\"Pablo1\")\n\n    # Convolution Layer\n    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n    # Max Pooling (down-sampling)\n    conv1 = maxpool2d(conv1, k=2)\n\n    # Convolution Layer\n    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n    # Max Pooling (down-sampling)\n    conv2 = maxpool2d(conv2, k=2)\n\n    # Fully connected layer\n    # Reshape conv2 output to fit fully connected layer input\n    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n    fc1 = tf.nn.relu(fc1)\n    # Apply Dropout\n    fc1 = tf.nn.dropout(fc1, dropout)\n\n    # Output, class prediction\n    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n    return out"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"89c7245a-61ff-5b60-6639-58f35dcdc46b"},"outputs":[],"source":"# Store layers weight & bias\nweights = {\n    # 5x5 conv, 1 input, 32 outputs\n    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n    # 5x5 conv, 32 inputs, 64 outputs\n    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n    # fully connected, 7*7*64 inputs, 1024 outputs\n    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n    # 1024 inputs, 10 outputs (class prediction)\n    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n}\n\nbiases = {\n    'bc1': tf.Variable(tf.random_normal([32])),\n    'bc2': tf.Variable(tf.random_normal([64])),\n    'bd1': tf.Variable(tf.random_normal([1024])),\n    'out': tf.Variable(tf.random_normal([n_classes]))\n}\n\n# Construct model\npred = conv_net(images_batch, weights, biases, keep_prob)\n\n# Define loss and optimizer\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=labels_batch))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\nprint(\"prediction\")\n# Evaluate model\ncorrect_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(labels_batch, 1))\nprint(\"prediction\")\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initializing the variables\ninit = tf.global_variables_initializer()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a427728e-8177-ba44-1401-9d6bb72c467d"},"outputs":[],"source":"# Launch the graph\nwith tf.Session() as sess: \n    sess.run(init) \n    for _ in range(1000):  \n              \n            indices = np.random.choice(3777, batch_size)\n            print(indices.size)\n            X_batch, Y_batch = x_train[indices], y_train[indices]\n            print(X_batch.size)\n            print(Y_batch)\n            sess.run(optimizer, feed_dict={images_batch: X_batch, labels_batch: Y_batch,\n                                       keep_prob: dropout})\n            \n            print(Y_batch.size)               \n            if step % display_step == 0:\n            # Calculate batch loss and accuracy\n               loss, acc = sess.run([cost, accuracy], feed_dict={images_batch: X_batch,\n                                                              labels_batch: Y_batch,\n                                                              keep_prob: 1.})     \n            print(acc+ \" \")  \n            step += 1\n    \n    print(\"Optimization Finished!\")\n            \n \n\n\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}