{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The Nature Conservancy Fisheries Monitoring","metadata":{}},{"cell_type":"markdown","source":"https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nimport zipfile\n\nprint(tf.__version__)","metadata":{"_kg_hide-output":true,"_uuid":"784aaee522ae53eff2274a388350b1b1dd60649b","execution":{"iopub.status.busy":"2022-04-21T11:55:15.303461Z","iopub.execute_input":"2022-04-21T11:55:15.303876Z","iopub.status.idle":"2022-04-21T11:55:20.221511Z","shell.execute_reply.started":"2022-04-21T11:55:15.303782Z","shell.execute_reply":"2022-04-21T11:55:20.220604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install py7zr","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:55:20.223214Z","iopub.execute_input":"2022-04-21T11:55:20.223472Z","iopub.status.idle":"2022-04-21T11:55:30.628847Z","shell.execute_reply.started":"2022-04-21T11:55:20.223448Z","shell.execute_reply":"2022-04-21T11:55:30.627829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Загружаем разметку","metadata":{"_uuid":"263db29157d5633d6f9e7340ab5efec72c677b66"}},{"cell_type":"code","source":"with zipfile.ZipFile(\"../input/the-nature-conservancy-fisheries-monitoring/train.zip\",\"r\") as z:\n    z.extractall(\".\")\nwith zipfile.ZipFile(\"../input/the-nature-conservancy-fisheries-monitoring/test_stg1.zip\",\"r\") as z:\n    z.extractall(\".\")","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:55:30.630601Z","iopub.execute_input":"2022-04-21T11:55:30.63099Z","iopub.status.idle":"2022-04-21T11:55:49.22648Z","shell.execute_reply.started":"2022-04-21T11:55:30.630949Z","shell.execute_reply":"2022-04-21T11:55:49.225662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import py7zr\nwith py7zr.SevenZipFile(\"../input/the-nature-conservancy-fisheries-monitoring/test_stg2.7z\", mode='r') as z:\n    z.extractall()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:55:49.22878Z","iopub.execute_input":"2022-04-21T11:55:49.229354Z","iopub.status.idle":"2022-04-21T11:57:41.865774Z","shell.execute_reply.started":"2022-04-21T11:55:49.229317Z","shell.execute_reply":"2022-04-21T11:57:41.864832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nfrom glob import glob\n\nTRAIN_PREFIX = '../working/train'\n\ndef load_annotations():\n    boxes = dict()\n    for path in tqdm(glob('../input/fishboxes/boxes/*.json')):\n        label = os.path.basename(path).split('_', 1)[0]\n        with open(path) as src:\n            for annotation in json.load(src):\n                basename = os.path.basename(annotation['filename'])\n                annotation['filename'] = os.path.join(\n                    TRAIN_PREFIX, label.upper(), basename)\n                for rect in annotation['annotations']:\n                    rect['x'] += rect['width'] / 2\n                    rect['y'] += rect['height'] / 2\n                    rect['class'] = label\n                if os.path.isfile(annotation['filename']):\n                    boxes.setdefault(label, []).append(annotation)\n    return boxes\n\ndef draw_boxes(annotation, rectangles=None, image_size=None):\n    \n    def _draw(img, rectangles, scale_x, scale_y, color=(0, 255, 0)):\n        for i, rect in enumerate(rectangles):\n            pt1 = (int((rect['x'] - rect['width'] / 2) * scale_x),\n                   int((rect['y'] - rect['height'] / 2) * scale_y))\n            pt2 = (int((rect['x'] + rect['width'] / 2) * scale_x),\n                   int((rect['y'] + rect['height'] / 2) * scale_y))\n            img = cv2.rectangle(img.copy(), pt1, pt2, \n                                color=color, thickness=10)\n            img = cv2.putText(img.copy(), annotation['annotations'][i]['class'], tuple(np.array(pt1)+[0,-7]), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 4 )\n        return img\n    \n    def __draw(img, rectangles, scale_x, scale_y, color=(0, 255, 0)):\n        for i, rect in enumerate(rectangles):\n            pt1 = (int((rect['x'] - rect['width'] / 2) * scale_x),\n                   int((rect['y'] - rect['height'] / 2) * scale_y))\n            pt2 = (int((rect['x'] + rect['width'] / 2) * scale_x),\n                   int((rect['y'] + rect['height'] / 2) * scale_y))\n            img = cv2.rectangle(img.copy(), pt1, pt2, \n                                color=color, thickness=3)\n            img = cv2.putText(img.copy(), counts['class'][int(rect['label'])] + ': ' + str(rect['label']), tuple(np.array(pt1)+[0,-7]), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 4 )\n        return img\n    \n    scale_x, scale_y = 1., 1.\n    \n    img = cv2.imread(annotation['filename'], cv2.IMREAD_COLOR)[...,::-1]\n    if image_size is not None:\n        scale_x = 1. * image_size[0] / img.shape[1]\n        scale_y = 1. * image_size[1] / img.shape[0]\n        img = cv2.resize(img, image_size)\n        \n    img = _draw(img, annotation.get('annotations', []), scale_x, scale_y)\n    \n    if rectangles is not None:\n        img = __draw(img, rectangles, 1., 1., (255, 0, 0))\n\n    return img","metadata":{"_uuid":"76496f443d36d16b961aeef10b365e3822b06a2b","execution":{"iopub.status.busy":"2022-04-21T11:57:41.869377Z","iopub.execute_input":"2022-04-21T11:57:41.869727Z","iopub.status.idle":"2022-04-21T11:57:41.887293Z","shell.execute_reply.started":"2022-04-21T11:57:41.869691Z","shell.execute_reply":"2022-04-21T11:57:41.88645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Визуализируем разметку","metadata":{}},{"cell_type":"code","source":"boxes = load_annotations()  # загружаем разметку детекций","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:41.890018Z","iopub.execute_input":"2022-04-21T11:57:41.890524Z","iopub.status.idle":"2022-04-21T11:57:42.093733Z","shell.execute_reply.started":"2022-04-21T11:57:41.890487Z","shell.execute_reply":"2022-04-21T11:57:42.092864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts = pd.DataFrame(\n    [(k, len(v)) for k, v in boxes.items()],\n    columns=['class', 'count'])\n\nfish_classes = counts['class'].values\nprint(fish_classes)\ncounts","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:42.095137Z","iopub.execute_input":"2022-04-21T11:57:42.095693Z","iopub.status.idle":"2022-04-21T11:57:42.126179Z","shell.execute_reply.started":"2022-04-21T11:57:42.095653Z","shell.execute_reply":"2022-04-21T11:57:42.125361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot as plt\n\n\nannotation = boxes['lag'][5]\nimg = draw_boxes(annotation)\n\nplt.figure(figsize=(6, 6), dpi=240)\nplt.imshow(img)\nplt.title('{} {}x{}'.format(\n    annotation['filename'], img.shape[0], img.shape[1]));","metadata":{"_uuid":"bb49d388931db2cbd5d8f08b9104299ca90a8c5a","execution":{"iopub.status.busy":"2022-04-21T11:57:42.127806Z","iopub.execute_input":"2022-04-21T11:57:42.128158Z","iopub.status.idle":"2022-04-21T11:57:42.860593Z","shell.execute_reply.started":"2022-04-21T11:57:42.128122Z","shell.execute_reply":"2022-04-21T11:57:42.859543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Распределение размеров разметки","metadata":{"_uuid":"0caa7502a0deb8e5e18773b6d2be8ed2f8d0dd4f"}},{"cell_type":"code","source":"annotations = sum([box['annotations']\n                   for box in sum(boxes.values(), [])], [])\n\nwidths = [rect['width'] for rect in annotations]\nheights = [rect['height'] for rect in annotations]\n\nplt.hist(widths)\nplt.hist(heights);","metadata":{"_uuid":"8db3e3e9aa63c1216d3a1f13526d74ab3abe31a8","execution":{"iopub.status.busy":"2022-04-21T11:57:42.861793Z","iopub.execute_input":"2022-04-21T11:57:42.8621Z","iopub.status.idle":"2022-04-21T11:57:43.080746Z","shell.execute_reply.started":"2022-04-21T11:57:42.862062Z","shell.execute_reply":"2022-04-21T11:57:43.079845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Экстрактор признаков","metadata":{"_uuid":"b01ffd790e6e7a81bfee104faa4bfa84ad7597c8"}},{"cell_type":"code","source":"# from tensorflow.keras.applications.resnet50 import ResNet50 #2, EfficientNetB3\nfrom tensorflow.keras.applications.vgg16 import VGG16\n\n# IMG_HEIGHT = 750\n# IMG_WIDTH = 1200\n\nIMG_HEIGHT = 736\nIMG_WIDTH =  1184\n\nfeatures = VGG16(weights='imagenet',\n                       include_top=False,\n                       input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n\n# дообучаем последние 5 слоев\nfor layer in features.layers[:-5]:\n    layer.trainable = False\n    \nfeature_tensor = features.layers[-1].output\nprint(feature_tensor.shape)","metadata":{"_uuid":"f3e6d68bd5e0c8a97319185f25a3b25673482606","execution":{"iopub.status.busy":"2022-04-21T11:57:43.081936Z","iopub.execute_input":"2022-04-21T11:57:43.082268Z","iopub.status.idle":"2022-04-21T11:57:46.084808Z","shell.execute_reply.started":"2022-04-21T11:57:43.082232Z","shell.execute_reply":"2022-04-21T11:57:46.083869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Сетка якорей (anchor grid)","metadata":{"_uuid":"ac4e46072546760703354bd43fa09c5d6bd69fb9"}},{"cell_type":"code","source":"FEATURE_SHAPE = (feature_tensor.shape[1],\n                 feature_tensor.shape[2])\nprint(FEATURE_SHAPE)\n\nGRID_STEP_H = IMG_HEIGHT / FEATURE_SHAPE[0]\nGRID_STEP_W = IMG_WIDTH / FEATURE_SHAPE[1]\n\nANCHOR_WIDTH = 150.\nANCHOR_HEIGHT = 150. \n\n# сетка якорей, размер определяется соотношением \n# размера входного изображения и размером тензора признаков\nANCHOR_CENTERS = np.mgrid[GRID_STEP_H/2:IMG_HEIGHT:GRID_STEP_H,\n                          GRID_STEP_W/2:IMG_WIDTH:GRID_STEP_W]\n\nANCHOR_CENTERS.shape","metadata":{"_uuid":"9d267722da49aad29f4bd4b625473295c3293e5f","execution":{"iopub.status.busy":"2022-04-21T11:57:46.086152Z","iopub.execute_input":"2022-04-21T11:57:46.086698Z","iopub.status.idle":"2022-04-21T11:57:46.097136Z","shell.execute_reply.started":"2022-04-21T11:57:46.086657Z","shell.execute_reply":"2022-04-21T11:57:46.096129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = counts.shape[0]\nnum_classes","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:46.09852Z","iopub.execute_input":"2022-04-21T11:57:46.099147Z","iopub.status.idle":"2022-04-21T11:57:46.107261Z","shell.execute_reply.started":"2022-04-21T11:57:46.099106Z","shell.execute_reply":"2022-04-21T11:57:46.106155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.special import softmax","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:46.108703Z","iopub.execute_input":"2022-04-21T11:57:46.109123Z","iopub.status.idle":"2022-04-21T11:57:46.1165Z","shell.execute_reply.started":"2022-04-21T11:57:46.109083Z","shell.execute_reply":"2022-04-21T11:57:46.115486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def iou(rect, x_scale, y_scale, anchor_x, anchor_y,\n        anchor_w=ANCHOR_WIDTH, anchor_h=ANCHOR_HEIGHT):\n    \n    rect_x1 = (rect['x'] - rect['width'] / 2) * x_scale\n    rect_x2 = (rect['x'] + rect['width'] / 2) * x_scale\n    \n    rect_y1 = (rect['y'] - rect['height'] / 2) * y_scale\n    rect_y2 = (rect['y'] + rect['height'] / 2) * y_scale\n    \n    anch_x1, anch_x2 = anchor_x - anchor_w / 2, anchor_x + anchor_w / 2\n    anch_y1, anch_y2 = anchor_y - anchor_h / 2, anchor_y + anchor_h / 2\n    \n    dx = (min(rect_x2, anch_x2) - max(rect_x1, anch_x1))\n    dy = (min(rect_y2, anch_y2) - max(rect_y1, anch_y1))\n    \n    intersection = dx * dy if (dx > 0 and dy > 0) else 0.\n    \n    anch_square = (anch_x2 - anch_x1) * (anch_y2 - anch_y1)\n    rect_square = (rect_x2 - rect_x1) * (rect_y2 - rect_y1)\n    union = anch_square + rect_square - intersection\n    \n    return intersection / union\n\ndef encode_anchors(annotation, img_shape, iou_thr=0.5):\n    encoded = np.zeros(shape=(FEATURE_SHAPE[0],\n                              FEATURE_SHAPE[1], 11), dtype=np.float32)\n    x_scale = 1. * IMG_WIDTH / img_shape[1]\n    y_scale = 1. * IMG_HEIGHT / img_shape[0]\n    for rect in annotation['annotations']:\n        scores = []\n        label = fish_classes == rect['class']\n                \n        for row in range(FEATURE_SHAPE[0]):\n            for col in range(FEATURE_SHAPE[1]):\n                anchor_x = ANCHOR_CENTERS[1, row, col]\n                anchor_y = ANCHOR_CENTERS[0, row, col]\n                score = iou(rect, x_scale, y_scale, anchor_x, anchor_y)\n                scores.append((score, anchor_x, anchor_y, row, col))\n        \n        scores = sorted(scores, reverse=True)\n        if scores[0][0] < iou_thr:\n            scores = [scores[0]]  # default anchor\n        else:\n            scores = [e for e in scores if e[0] > iou_thr]\n            \n        for score, anchor_x, anchor_y, row, col in scores:\n            dx = (anchor_x - rect['x'] * x_scale) / ANCHOR_WIDTH\n            dy = (anchor_y - rect['y'] * y_scale) / ANCHOR_HEIGHT\n            dw = (ANCHOR_WIDTH - rect['width'] * x_scale) / ANCHOR_WIDTH\n            dh = (ANCHOR_HEIGHT - rect['height'] * y_scale) / ANCHOR_HEIGHT\n            encoded[row, col] = np.array([*label, 1., dx, dy, dw, dh])\n\n    return encoded\n\ndef _sigmoid(x):\n    return 1. / (1. + np.exp(-x))\n\n#keras.activations.softmax(x, axis=-1)\n\ndef decode_prediction(prediction, conf_thr=0.1):\n    rectangles = []\n    for row in range(FEATURE_SHAPE[0]):\n        for col in range(FEATURE_SHAPE[1]):\n            \n            label = np.empty(6)\n            \n            label[0], label[1], label[2], label[3], label[4], label[5], conf, dx, dy, dw, dh = prediction[row, col]\n            conf = _sigmoid(conf)\n            label = softmax(label)\n\n            if conf > conf_thr:\n                anchor_x = ANCHOR_CENTERS[1, row, col]\n                anchor_y = ANCHOR_CENTERS[0, row, col]\n                rectangles.append({'x': anchor_x - dx * ANCHOR_WIDTH,\n                                   'y': anchor_y - dy * ANCHOR_HEIGHT,\n                                   'width': ANCHOR_WIDTH - dw * ANCHOR_WIDTH,\n                                   'height': ANCHOR_HEIGHT - dh * ANCHOR_HEIGHT,\n                                   'conf': conf,\n                                   'label': np.argmax(_sigmoid(label)),\n                                   'labels': label })\n    return rectangles\n\ndef non_max_suppression(rectangles, max_output_size, iou_threshold=0.5):\n    if not rectangles:\n        return rectangles\n    \n    boxes = [[r['y'],\n              r['x'],\n              r['y'] + r['height'],\n              r['x'] + r['width']] for r in rectangles]\n    scores = [r['conf'] for r in rectangles]\n    indices = tf.image.non_max_suppression(np.array(boxes),\n                                           np.array(scores),\n                                           max_output_size,\n                                           iou_threshold)\n    \n    return [rectangles[i] for i in indices]","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.status.busy":"2022-04-21T11:57:46.117986Z","iopub.execute_input":"2022-04-21T11:57:46.119474Z","iopub.status.idle":"2022-04-21T11:57:46.143559Z","shell.execute_reply.started":"2022-04-21T11:57:46.119444Z","shell.execute_reply":"2022-04-21T11:57:46.142451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Валидация энкодинга/декодинга якорей","metadata":{}},{"cell_type":"code","source":"annotation = boxes['shark'][3]\n\nencoded = encode_anchors(annotation,\n                         img_shape=(IMG_HEIGHT, IMG_WIDTH),\n                         iou_thr=0.1)\n\ndecoded = decode_prediction(encoded, conf_thr=0.7)\ndecoded = sorted(decoded, key = lambda e: -e['conf'])\n\nplt.figure(figsize=(6, 6), dpi=240)\nplt.imshow(draw_boxes(annotation, decoded))\nplt.title('{} {}x{}'.format(\n    annotation['filename'], img.shape[0], img.shape[1]));","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:46.14572Z","iopub.execute_input":"2022-04-21T11:57:46.146303Z","iopub.status.idle":"2022-04-21T11:57:46.974633Z","shell.execute_reply.started":"2022-04-21T11:57:46.146265Z","shell.execute_reply":"2022-04-21T11:57:46.973855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Функция потерь","metadata":{"_uuid":"69b77cba0531fd1ed1c2634e929bee24ac068f7a"}},{"cell_type":"code","source":"K = tf.keras.backend\n\ndef confidence_loss(y_true, y_pred):\n    conf_loss = K.binary_crossentropy(y_true[..., 6], \n                                      y_pred[..., 6],\n                                      from_logits=True)\n    return conf_loss\n\ndef smooth_l1(y_true, y_pred):\n    abs_loss = K.abs(y_true[..., -4:] - y_pred[..., -4:])\n    square_loss = 0.5 * K.square(y_true[..., -4:] - y_pred[..., -4:])\n    mask = K.cast(K.greater(abs_loss, 1.), 'float32')\n    total_loss = (abs_loss - 0.5) * mask + 0.5 * square_loss * (1. - mask)\n    return K.sum(total_loss, axis=-1)\n\ndef classification_loss(y_tr, y_pr, alpha=0.25, gamma=2.0):\n    \n    \"\"\"Focal Loss\"\"\"\n    y_true = y_tr[..., :6]\n    y_pred = y_pr[..., :6]\n\n    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(\n        labels=y_true, logits=y_pred\n    )\n    probs = tf.nn.softmax(y_pred)\n    alpha = tf.where(tf.equal(y_true, 1.0), alpha, (1.0 - alpha))\n    pt = tf.where(tf.equal(y_true, 1.0), probs, 1 - probs)\n    loss = alpha * tf.pow(1.0 - pt, gamma) * cross_entropy\n    \n    return tf.reduce_sum(loss, axis=-1)\n\ndef class_loss(y_tr, y_pr):\n    \n    y_true = y_tr[..., :6]\n    y_pred = y_pr[..., :6]\n    \n    cross_entropy = K.categorical_crossentropy(y_true[..., :6], \n                                               y_pred[..., :6],\n                                               from_logits=True)\n#     tf.nn.sigmoid_cross_entropy_with_logits(\n#         labels=y_true, logits=y_pred\n#         )\n    return cross_entropy\n\ndef total_loss(y_true, y_pred, neg_pos_ratio=3):\n    batch_size = K.shape(y_true)[0]\n    \n    y_true = K.reshape(y_true, (batch_size, -1, 11))\n    y_pred = K.reshape(y_pred, (batch_size, -1, 11))\n    \n    # TODO: добавьте функцию потерь для классификации детекции\n    cls_loss = classification_loss(y_true, y_pred)\n    #cls_loss = class_loss(y_true, y_pred)\n\n\n    # confidence loss\n    conf_loss = confidence_loss(y_true, y_pred)\n    \n    # smooth l1 loss\n    loc_loss = smooth_l1(y_true, y_pred)\n    \n    # positive examples loss\n    pos_conf_loss = K.sum(conf_loss * y_true[..., 6], axis=-1)\n    pos_class_loss = K.sum(cls_loss * y_true[..., 6], axis=-1)\n    pos_loc_loss = K.sum(loc_loss * y_true[..., 6], axis=-1)\n\n    \n    \n    # negative examples loss\n    anchors = K.shape(y_true)[1]\n    num_pos = K.sum(y_true[..., 6], axis=-1)\n    num_pos_avg = K.mean(num_pos)\n    num_neg = K.min([neg_pos_ratio * (num_pos_avg) + 1., K.cast(anchors, 'float32')])\n    \n    # hard negative mining\n    neg_conf_loss, _ = tf.nn.top_k(conf_loss * (1. - y_true[..., 6]),\n                                   k=K.cast(num_neg, 'int32'))\n\n    neg_conf_loss = K.sum(neg_conf_loss, axis=-1)\n    \n    # total conf loss\n    total_conf_loss = (neg_conf_loss + pos_conf_loss) / (num_neg + num_pos + 1e-32)\n    cls_loss = pos_class_loss / (num_pos + 1e-32)\n    loc_loss = pos_loc_loss / (num_pos + 1e-32)\n    \n    return total_conf_loss + 0.5 * loc_loss + cls_loss\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:46.975904Z","iopub.execute_input":"2022-04-21T11:57:46.976371Z","iopub.status.idle":"2022-04-21T11:57:47.000561Z","shell.execute_reply.started":"2022-04-21T11:57:46.976334Z","shell.execute_reply":"2022-04-21T11:57:46.999704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Загрузка данных","metadata":{}},{"cell_type":"code","source":"from random import shuffle\n\ndef load_img(path, target_size=(IMG_WIDTH, IMG_HEIGHT)):\n    img = cv2.imread(path, cv2.IMREAD_COLOR)[...,::-1]\n    img_shape = img.shape\n    img_resized = cv2.resize(img, target_size)\n    return img_shape, tf.keras.applications.resnet_v2.preprocess_input(img_resized.astype(np.float32))\n\ndef data_generator(boxes, batch_size=32):\n    boxes = sum(boxes.values(), [])\n    while True:\n        shuffle(boxes)\n        for i in range(len(boxes)//batch_size):\n            X, y = [], []\n            for j in range(i*batch_size,(i+1)*batch_size):\n                img_shape, img = load_img(boxes[j]['filename'])\n                # TODO: добавьте one-hot encoding в разметку для классов\n                #print('boxes[j]', boxes[j])\n                y_ = encode_anchors(boxes[j], img_shape)\n                y.append(y_)\n                X.append(img)\n            yield np.array(X), np.array(y)","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:47.002013Z","iopub.execute_input":"2022-04-21T11:57:47.002512Z","iopub.status.idle":"2022-04-21T11:57:47.013224Z","shell.execute_reply.started":"2022-04-21T11:57:47.002472Z","shell.execute_reply":"2022-04-21T11:57:47.012265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Добавляем выход детектора","metadata":{}},{"cell_type":"code","source":"output = tf.keras.layers.BatchNormalization()(feature_tensor)\n\n# TODO: добавьте выходы для классификации детекции\noutput = tf.keras.layers.Conv2D(11,\n                                kernel_size=(1, 1), \n                                activation='linear',\n                                kernel_regularizer='l2')(output)\n\nmodel = tf.keras.models.Model(inputs=features.inputs, outputs=output)\n","metadata":{"_uuid":"a8904e155a9d956eecccd59a9b104bcfd20c44e1","execution":{"iopub.status.busy":"2022-04-21T11:57:47.014723Z","iopub.execute_input":"2022-04-21T11:57:47.015132Z","iopub.status.idle":"2022-04-21T11:57:47.046324Z","shell.execute_reply.started":"2022-04-21T11:57:47.01509Z","shell.execute_reply":"2022-04-21T11:57:47.045688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Обучение","metadata":{}},{"cell_type":"code","source":"lr=3e-3\ndef lr_exp_decay(epoch, lr):\n    k = 0.5\n    return lr * np.exp(-k*epoch)\n\nbatch_size = 32\nsteps_per_epoch = sum(map(len, boxes.values()), 0) / batch_size\n\ngen = data_generator(boxes, batch_size=batch_size)\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    'fishdetector.hdf5',\n    monitor='loss',\n    verbose=47,  \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto',\n    save_freq=10)\n\nadam = tf.keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999)\nmodel.compile(optimizer=adam, \n              loss=total_loss,\n              metrics=[confidence_loss, classification_loss, smooth_l1])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-21T11:57:47.047729Z","iopub.execute_input":"2022-04-21T11:57:47.048089Z","iopub.status.idle":"2022-04-21T11:57:47.064977Z","shell.execute_reply.started":"2022-04-21T11:57:47.048052Z","shell.execute_reply":"2022-04-21T11:57:47.064143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(gen, \n          steps_per_epoch=steps_per_epoch,\n          epochs=2,\n          callbacks=[checkpoint, tf.keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=1)])","metadata":{"_uuid":"88d52a3e5e2f887dcf4cb295c62c3820c97f0db9","execution":{"iopub.status.busy":"2022-04-21T11:57:47.066286Z","iopub.execute_input":"2022-04-21T11:57:47.066671Z","iopub.status.idle":"2022-04-21T12:04:27.732658Z","shell.execute_reply.started":"2022-04-21T11:57:47.066634Z","shell.execute_reply":"2022-04-21T12:04:27.725796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Результат работы детектора","metadata":{}},{"cell_type":"code","source":"#model.load_weights('../input/fish-data/fishdetector(2).hdf5')","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:04:27.734991Z","iopub.execute_input":"2022-04-21T12:04:27.735401Z","iopub.status.idle":"2022-04-21T12:04:27.742335Z","shell.execute_reply.started":"2022-04-21T12:04:27.735359Z","shell.execute_reply":"2022-04-21T12:04:27.741318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"annotation = boxes['shark'][1]\n\n_, sample_img = load_img(annotation['filename'])\npred = model.predict(np.array([sample_img,]))\n\ndecoded = decode_prediction(pred[0], conf_thr=0.1)\n\ndecoded = non_max_suppression(decoded,\n                              max_output_size=1,\n                              iou_threshold=0.75)\n\nplt.figure(figsize=(6, 6), dpi=240)\nimg = draw_boxes(annotation, decoded, (IMG_WIDTH, IMG_HEIGHT))\nplt.imshow(img)\nplt.title('Предсказание модели {}x{}'.format(*img.shape));","metadata":{"_uuid":"6d6d87b271aad9b6f5135870c464f613fce3a31c","execution":{"iopub.status.busy":"2022-04-21T12:04:27.752865Z","iopub.execute_input":"2022-04-21T12:04:27.753219Z","iopub.status.idle":"2022-04-21T12:04:29.664971Z","shell.execute_reply.started":"2022-04-21T12:04:27.753184Z","shell.execute_reply":"2022-04-21T12:04:29.664133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Визуализируем предсказание на тесте","metadata":{}},{"cell_type":"code","source":"test_images = glob('../working//test_stg1/*.jpg')[:4]\n\n\nplt.figure(figsize=(6, 4 * len(test_images)), dpi=240)\n\nfor i, filename in enumerate(test_images):\n    _, sample_img = load_img(filename)\n\n    pred = model.predict(np.array([sample_img,]))\n    decoded = decode_prediction(pred[0], conf_thr=0.1)\n    decoded = non_max_suppression(decoded,\n                                  max_output_size=1,\n                                  iou_threshold=0.5)\n    plt.subplot(len(test_images), 1, i + 1)\n    img = draw_boxes({'filename': filename}, decoded, (IMG_WIDTH, IMG_HEIGHT))\n    plt.imshow(img)\n    plt.title('Предсказание на тесте {}'.format(filename.split('/')[-1]));","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:04:29.666143Z","iopub.execute_input":"2022-04-21T12:04:29.666558Z","iopub.status.idle":"2022-04-21T12:04:32.959261Z","shell.execute_reply.started":"2022-04-21T12:04:29.666526Z","shell.execute_reply":"2022-04-21T12:04:32.957796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Агрегация результатов","metadata":{}},{"cell_type":"code","source":"# TODO: предскажите класс рыбы для фотографии из тестовой выборки\n#\n# Подготовьте файл с предсказаниями вероятностей для каждой фотографии:\n# image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\n# img_00001.jpg,1,0,0,0,0,...,0\n# img_00002.jpg,0.3,0.1,0.6,0,...,0","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:04:32.960798Z","iopub.execute_input":"2022-04-21T12:04:32.961448Z","iopub.status.idle":"2022-04-21T12:04:32.966051Z","shell.execute_reply.started":"2022-04-21T12:04:32.961405Z","shell.execute_reply":"2022-04-21T12:04:32.965186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fish_classes","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:04:32.967592Z","iopub.execute_input":"2022-04-21T12:04:32.968158Z","iopub.status.idle":"2022-04-21T12:04:32.976992Z","shell.execute_reply.started":"2022-04-21T12:04:32.968121Z","shell.execute_reply":"2022-04-21T12:04:32.976078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_predictions():\n    ptable = pd.DataFrame(columns=['image', 'ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK','YFT'])\n    \n    for i, file in enumerate(tqdm(glob('../working/test_stg1/*.jpg'))):\n        bn = os.path.basename(file)\n        _, sample_img = load_img(file)\n        \n        pred = model.predict(np.array([sample_img,]))[0]\n        decoded = decode_prediction(pred, conf_thr=0.015)\n        decoded = non_max_suppression(decoded,\n                              max_output_size=1,\n                              iou_threshold=0.5)\n        decoded = decoded[0]['labels']\n\n        ptable.loc[i, 'image'] = bn\n        ptable.loc[i, 'ALB'] = decoded[5]\n        ptable.loc[i, 'BET'] = decoded[1]\n        ptable.loc[i, 'DOL'] = decoded[4]\n        ptable.loc[i, 'LAG'] = decoded[3]\n        ptable.loc[i, 'SHARK'] = decoded[0]\n        ptable.loc[i, 'YFT'] = decoded[2]\n        \n        ptable.loc[i, 'NoF'] = 0.123081\n        ptable.loc[i, 'OTHER'] = 0.079142\n\n#         i += 1    \n    \n    for j, file in enumerate(tqdm(glob('../working/test_stg2/*.jpg'))):\n        bn = os.path.basename(file)\n        \n        bn = \"test_stg2/\" + bn\n        _, sample_img = load_img(file)\n        \n        pred = model.predict(np.array([sample_img,]))[0]\n\n        decoded = decode_prediction(pred, conf_thr=0.015)\n        decoded = non_max_suppression(decoded,\n                              max_output_size=1,\n                              iou_threshold=0.5)\n        \n        decoded = decoded[0]['labels']\n\n        ptable.loc[i + j, 'image'] = bn\n        ptable.loc[i + j, 'ALB'] = decoded[5]\n        ptable.loc[i + j, 'BET'] = decoded[1]\n        ptable.loc[i + j, 'DOL'] = decoded[4]\n        ptable.loc[i + j, 'LAG'] = decoded[3]\n        ptable.loc[i + j, 'SHARK'] = decoded[0]\n        ptable.loc[i + j, 'YFT'] = decoded[2]\n        \n        ptable.loc[i + j, 'NoF'] = 0.123081\n        ptable.loc[i + j, 'OTHER'] = 0.079142\n\n    return ptable   ","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:04:32.978627Z","iopub.execute_input":"2022-04-21T12:04:32.979395Z","iopub.status.idle":"2022-04-21T12:04:32.997794Z","shell.execute_reply.started":"2022-04-21T12:04:32.979351Z","shell.execute_reply":"2022-04-21T12:04:32.996895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_table = make_predictions()","metadata":{"execution":{"iopub.status.busy":"2022-04-21T12:04:32.999408Z","iopub.execute_input":"2022-04-21T12:04:33.000135Z","iopub.status.idle":"2022-04-21T12:45:53.383175Z","shell.execute_reply.started":"2022-04-21T12:04:33.000093Z","shell.execute_reply":"2022-04-21T12:45:53.382238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_table1 = pd.concat([pred_table, pred_table.iloc[-1:]]).reset_index().drop(['index'], axis=1)\npred_table1","metadata":{"execution":{"iopub.status.busy":"2022-04-21T13:06:46.29558Z","iopub.execute_input":"2022-04-21T13:06:46.295928Z","iopub.status.idle":"2022-04-21T13:06:46.321556Z","shell.execute_reply.started":"2022-04-21T13:06:46.295896Z","shell.execute_reply":"2022-04-21T13:06:46.320468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_table1.to_csv(\"submission.csv\", index=False)\nprint(os.listdir(\"./\"))","metadata":{"execution":{"iopub.status.busy":"2022-04-21T13:06:51.576301Z","iopub.execute_input":"2022-04-21T13:06:51.576643Z","iopub.status.idle":"2022-04-21T13:06:51.756119Z","shell.execute_reply.started":"2022-04-21T13:06:51.576612Z","shell.execute_reply":"2022-04-21T13:06:51.755207Z"},"trusted":true},"execution_count":null,"outputs":[]}]}