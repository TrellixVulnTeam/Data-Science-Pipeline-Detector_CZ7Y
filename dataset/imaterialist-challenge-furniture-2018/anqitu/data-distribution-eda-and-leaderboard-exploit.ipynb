{"cells":[{"metadata":{"_uuid":"25bbc6e395e376ad5b86aee4cc6a294fb464f801","_cell_guid":"6911bd72-5002-4614-9c8b-0e94ec865f9f"},"cell_type":"markdown","source":"After some exploratory analysis of the train, test and validation data, I find some problems with the data distribution which might play an important role in our model building."},{"metadata":{"_uuid":"21eb04594497aec4fb983a42aa73c3b9cd44c8a4"},"cell_type":"markdown","source":"# UPDATE: More interesting findings from exploiting leaderboard score.\nThe previous version (the code below) finds the different distribution of labels in train and test dataset - labels have drastic difference in frequency in train data, but same frequency in validation data . Thus, it was recommended to combine train and test because it was assumed the test dataset should have similar distribution as train dataset, ie. similar to train set where label 42 is the most frequent, the majority of test set should also be 42. \nHowever, if you try submitting two files - one with all label 42, one with all label 1 - you will get similar score - 0.99322 and 0.99348 for each. After simple calculation - (1-0.99322) * 128 = 0.86784, (1-0.99322) * 128 = 0.83456 - it seems that the test set has similar distribution of label as the validation data which has same frequency of each label. \nAll in all, if you want to validate, a better one might be to simulate the distribution of test set - have same distribution of each label.\nThe rest of this kernel shows how I found the distribution of labels and a simple EDA of the data."},{"metadata":{"collapsed":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"code","source":"raw_data_path = \"../input\"\nimport time\nscript_start_time = time.time()\n\nimport pandas as pd\nimport numpy as np\nimport gc\nimport json\n\npd.set_option('display.max_rows', 600)\npd.set_option('display.max_columns', 50)\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"},"cell_type":"markdown","source":"## 1. Load data (convert json to csv)"},{"metadata":{"_uuid":"7c3720a81922c80fdecdf131990828b9ec868328","_cell_guid":"56cacbe4-a764-4a55-ba27-7063fd5ce2ff","trusted":false,"collapsed":true},"cell_type":"code","source":"print('%0.2f min: Start loading data'%((time.time() - script_start_time)/60))\n\ntrain={}\ntest={}\nvalidation={}\nwith open('%s/train.json'%(raw_data_path)) as json_data:\n    train= json.load(json_data)\nwith open('%s/test.json'%(raw_data_path)) as json_data:\n    test= json.load(json_data)\nwith open('%s/validation.json'%(raw_data_path)) as json_data:\n    validation = json.load(json_data)\n\nprint('Train No. of images: %d'%(len(train['images'])))\nprint('Test No. of images: %d'%(len(test['images'])))\nprint('Validation No. of images: %d'%(len(validation['images'])))\n\n# JSON TO PANDAS DATAFRAME\n# train data\ntrain_img_url=train['images']\ntrain_img_url=pd.DataFrame(train_img_url)\ntrain_img_url['url'] = train_img_url['url'].apply(lambda r: r[0])\ntrain_ann=train['annotations']\ntrain_ann=pd.DataFrame(train_ann)\ntrain_img_url.head()\ntrain=pd.merge(train_img_url, train_ann, on='image_id', how='inner')\n\n# test data\ntest=pd.DataFrame(test['images'])\ntest['url'] = test['url'].apply(lambda r: r[0])\n\n\n# Validation Data\nval_img_url=validation['images']\nval_img_url=pd.DataFrame(val_img_url)\nval_img_url['url'] = val_img_url['url'].apply(lambda r: r[0])\nval_ann=validation['annotations']\nval_ann=pd.DataFrame(val_ann)\nvalidation=pd.merge(val_img_url, val_ann, on='image_id', how='inner')\n\nprint('%0.2f min: Finish loading data'%((time.time() - script_start_time)/60))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ea4e9412ced9ce1774f7fecfbbb2a5ef57a4be5","_cell_guid":"504027ad-7ceb-45a1-a3ee-22f8c2edb9f8"},"cell_type":"markdown","source":"## 2. Check data (size, NA, duplicates...)"},{"metadata":{"_uuid":"01834c9b15880699b55f41caabbbd41cfd3ee113","_cell_guid":"456b0e7f-1fa9-450e-9f35-89dd24507d17"},"cell_type":"markdown","source":"The self defined function I used is quite useful for the first step of data analysis. We get many information from them.\n- There are 128 labels.\n- There are no duplicated data in each data set.\n- However, there are 7 duplicated url in all dataset (which will be investigated later)."},{"metadata":{"_uuid":"50b9fc57b27df01f18c9f2c1951a5a44d48e8a3e","_cell_guid":"c60ea770-5b69-468c-899e-66cd6c8e76e9","trusted":false,"collapsed":true},"cell_type":"code","source":"# Findings: There are duplicated url\ndatas = {'train': train, 'test': test, 'validation': validation}\ntotal_url = []\ndataset_url = {}\nfor data_name, data in datas.items():\n    print('%s shape: %s'%(data_name, str(data.shape)))\n    print('Unique:')\n    print(data.nunique()) # Unique values\n    print('NA:')\n    print(data.isnull().sum()) # No missing values\n    print(data.describe())\n    total_url = total_url + data['url'].tolist()\n    dataset_url[data_name] = data['url'].tolist()\n    print('-'*50)\n\nprint('Total images: %d'%(len(total_url)))\nprint('Total unique images: %d'%(len(set(total_url))))\nprint('Duplicated url: %d'%(len(total_url) - len(set(total_url))))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6cc27deb0c7e87bced479cfc1175f210d73d233c","_cell_guid":"f4a0c4ac-b75b-41ec-acf6-667d8d440d1d","trusted":false},"cell_type":"code","source":"# #Save as csv -----------------------------------------------------------------\n# for data_name, data in datas.items():\n#     data.to_csv('%s/%s.csv'%(processed_data_dir, data_name), index = False)\n# print('%0.2f min: Finish saving raw data as csv'%((time.time() - script_start_time)/60))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45ea5fc2c5ad7375da7a545a9a1c8f5c06a25d7f","_cell_guid":"3b1aacc2-69fb-4344-b0cf-71486754b494"},"cell_type":"markdown","source":"## 3. Visualization for further exploration"},{"metadata":{"_uuid":"f61c6e372da2cc63546cca54d430a3a9756745f2","_cell_guid":"2e60a23f-9921-48d8-a93c-46d6870b2de6","trusted":false,"collapsed":true},"cell_type":"code","source":"# 3. Exploratory Data Analysis =================================================\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport plotly.plotly as py\nimport cufflinks as cf\ncf.set_config_file(offline=True, world_readable=True, theme='ggplot')\nplt.rcParams[\"figure.figsize\"] = 12,8\nsns.set(rc={'figure.figsize':(12,8)})\nplt.style.use('fivethirtyeight')\n\npd.set_option('display.max_rows', 600)\npd.set_option('display.max_columns', 50)\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69c06b3f0fc926f30c314346099416711dfe6cf7","_cell_guid":"f9588d81-bf03-4a82-946d-232e38bbb5e6"},"cell_type":"markdown","source":"### 3.1 Lets find the duplicated url first"},{"metadata":{"_uuid":"5a3f55388a0fdd52c93c971d66690cda510cf164","_cell_guid":"3c9c2e80-47a9-4dbb-9e9c-fe838578307d","trusted":false,"collapsed":true},"cell_type":"code","source":"# 3.1 Try to find the duplicated url -----------------------------------------------\n# Findings: Labels are different\nfrom itertools import product\ncombinations = list(product(*[datas.keys(), datas.keys()]))\nfor comb in combinations:\n    print('%s inter %s: %d | %d'%(comb[0], comb[1], len(set(dataset_url[comb[0]])), len(set(dataset_url[comb[0]]).intersection(set(dataset_url[comb[1]])))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"101ab5588269ac6a269daf0b3cd6707cba454aea","_cell_guid":"4310aa7d-f0d9-4c3e-bcb5-6992b51c9d7e"},"cell_type":"markdown","source":"As shows by the intersection, the duplicated url are in validation and train. However, they have different labels despite the same url."},{"metadata":{"_uuid":"bb6ea7d4e08ba6395cf2556929698c84ea5bc2a0","_cell_guid":"033352be-8bc4-4b0f-9e9a-6e95290aef0c","trusted":false,"collapsed":true},"cell_type":"code","source":"# Confirm the duplicated url\nduplicated = train[['url']].merge(validation[['url']], how = 'inner')\nduplicated = duplicated.merge(validation[['url', 'label_id']], on = 'url',how = 'left').rename(columns = {'label_id': 'label_id_val'})\nduplicated = duplicated.merge(train[['url', 'label_id']], on = 'url',how = 'left').rename(columns = {'label_id': 'label_id_train'})\nprint(duplicated)\nduplicated_url = duplicated['url']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30e60f414209a4ebef42c049f8e36f31fa406a7c","_cell_guid":"d775acbb-3903-47f1-aa1d-6b6d1ec11c50"},"cell_type":"markdown","source":"Lets display the image for duplicated url"},{"metadata":{"_uuid":"7d42d7e04f6467de4c3332f086b822707a2d3f47","_cell_guid":"da94b8d6-96ce-484c-a16d-c65330af52ce","trusted":false,"collapsed":true},"cell_type":"code","source":"# Display images with duplicated url\nfrom IPython.display import Image\nfrom IPython.core.display import HTML\n\ndef display_image(urls):\n    img_style = \"width: 180px; margin: 0px; float: left; border: 1px solid black;\"\n    images_list = ''.join([f\"<img style='{img_style}' src='{u}' />\" for _, u in urls.iteritems()])\n    display(HTML(images_list))\ndisplay_image(duplicated_url)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84f54ee64c8a2176814217b05d0a5e0a430aa46e","_cell_guid":"18a75986-8028-474e-a886-e621cee10e60"},"cell_type":"markdown","source":"Well, I guess the lebels represents the table or laptop."},{"metadata":{"_uuid":"8bac4e2518cc0323acbd671194e97c63b1809276","_cell_guid":"0b8a1447-e1e0-4e89-b311-a648b0223174"},"cell_type":"markdown","source":"### 3.2 Frequency of labels"},{"metadata":{"_uuid":"911e00dce4b7399d6b8dbc876a7ff0511d041444","_cell_guid":"9b3adeea-3eb1-410d-a601-ce9b348ede0f"},"cell_type":"markdown","source":"This section let us realize the different frequency of labels in train and validation:\nAll labels appears for 50 times in validation\nMost labels appears for 1000-2000 times in train. Some appear for 4000 times."},{"metadata":{"_uuid":"ab5981eb8b4a497c4b77b07034babd5b1ce54de1","_cell_guid":"0b77de6e-3a0b-4368-b334-915bf82b9f11","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.distplot(train['label_id'])\nsns.distplot(validation['label_id'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34a783ca2a64cfee357c9fcaa3ce0286a1d05549","_cell_guid":"7ea95b1a-b141-4623-8e69-5aaebdce2114","trusted":false,"collapsed":true},"cell_type":"code","source":"train_Label_count = train['label_id'].value_counts().reset_index().rename(columns = {'index': 'label_id', 'label_id': 'label_id_count_train'})\nvalidation_Label_count = validation['label_id'].value_counts().reset_index().rename(columns = {'index': 'label_id', 'label_id': 'label_id_count_val'})\nlabel_count = train_Label_count.merge(validation_Label_count, on = 'label_id', how = 'right').fillna(0)\nlabel_count['label_id_freq_train'] = label_count['label_id_count_train'] / train.shape[0]\nlabel_count['label_id_freq_val'] = label_count['label_id_count_val'] / validation.shape[0]\nprint(label_count.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce9744d6191de36ce0a47b770436003fed657c1f","_cell_guid":"53d42acc-202d-4d95-b926-a93dcf0a1f23","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.distplot(label_count['label_id_freq_train'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39f3ba39f77a47ce2675f685a4e5d74948327a71","_cell_guid":"33b2eb91-1dec-494a-8c87-29ee93bed2e3","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.distplot(label_count['label_id_freq_val'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c766222cfd3a2a461c0c66976f318a3f5cd0536","_cell_guid":"3ae28bcd-0d6c-4d53-a5d2-2fb6086aa1fe"},"cell_type":"markdown","source":"### 3.3 Distribution of labels\nThe plots of label_id against index shows label_id are grouped together in train, while randomly in validation"},{"metadata":{"_uuid":"a5e1447f4c5f613726b8ea6b1d9294aeb90c8b94","_cell_guid":"3cc7a264-fdfe-4178-b6b2-62ac0a3fced7","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.plot(train['label_id'], '.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bdd46597a2214fb2f2384eb0e7934f0d9db39a7","_cell_guid":"c4346679-519e-45c9-aea7-e04f1fa1916b","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.plot(validation['label_id'], '.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6ef147d2365c106db7f5990635de7a661768314","_cell_guid":"70db28eb-01d4-4203-a8ee-cbd3aa9dce05"},"cell_type":"markdown","source":"## 3.4 Images of the most frequent label "},{"metadata":{"_uuid":"75ec30deb6843604dfb351b0e65ed25baea7a730","_cell_guid":"7d793fa8-eff3-4443-a592-87fcc58f265e"},"cell_type":"markdown","source":"Label 20 is the most frequent. I guess it is 'bottle'."},{"metadata":{"_uuid":"480871969f1293c4c3145dad49235ca9f8650428","_cell_guid":"b5e65f17-5925-4292-84f1-c257b8319873","trusted":false,"collapsed":true},"cell_type":"code","source":"print(label_count.sort_values(['label_id_count_train']).tail())\nurl_label20 = train[train['label_id']==20]['url'][:10]\ndisplay_image(url_label20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf20b3add24a7d230a9fd8d53a731fa952ad3007","_cell_guid":"c72a28bd-9030-47ab-9113-f829ddbd0d74"},"cell_type":"markdown","source":"Label 83 is the least frequent. I guess it is 'table''"},{"metadata":{"_uuid":"6a297ac14995b6d3aa753d14130260d2364811a1","_cell_guid":"b08648b2-3ffb-4f99-9ea6-c335bf55bf8e","trusted":false,"collapsed":true},"cell_type":"code","source":"print(label_count.sort_values(['label_id_count_train']).head())\nurl_label83 = train[train['label_id']==83]['url'][:10]\ndisplay_image(url_label83)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eea82859627b5fa69ea0a2e93135bbfea1d1ffdf","_cell_guid":"6112ff05-5178-4809-afe0-7a3aef6d46d8","trusted":false,"collapsed":true},"cell_type":"code","source":"print('%0.2f min: Finish running scipt'%((time.time() - script_start_time)/60))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1eeba59642fe2f74c4b754b423f48292fadc682a","_cell_guid":"15d1a1fb-ac90-4514-8b25-175edcb640b2"},"cell_type":"markdown","source":"# Note for model building: \n- The validation set is not a desirable validation data.\n- There is a need to combine the data and get our own validation data.\n- There are 7 duplicated url in train and validation where some have different lab"},{"metadata":{"_uuid":"26dee9cd201fa1816b2725e54ee6c4cf8739a170","_cell_guid":"c506fd6d-9403-45fe-9d07-04bfbc269a0b"},"cell_type":"markdown","source":"## 4. Construct Validation data after combining"},{"metadata":{"_uuid":"a8493fdfd607ead59b1e6b8e8ded00556839fe51","_cell_guid":"2fe49042-1fb7-42f6-91ee-c952df3d4a6c"},"cell_type":"markdown","source":"Do not forget about the duplicated url and label when combine train and validation"},{"metadata":{"_uuid":"1564089082b218906076973df9c9600ceb1a4fb2","_cell_guid":"7b2990e2-435b-4959-839d-7a3ac3c47380","trusted":false,"collapsed":true},"cell_type":"code","source":"all = pd.concat([train, validation])\nprint(all[all[['url', 'label_id']].duplicated()])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cb73ef9686f47ba995d23939a910be4fd2653f1","_cell_guid":"532b51c8-633b-4934-bfdc-b12d40bfa919"},"cell_type":"markdown","source":"Double Check duplicates"},{"metadata":{"_uuid":"9b1764d60cc76aefcb944c5cb7901b2d7a3eb383","_cell_guid":"a3af35e6-86d4-4786-bd26-f856445bafe1","trusted":false,"collapsed":true},"cell_type":"code","source":"all = all.drop_duplicates(all[['url', 'label_id']])\nprint(all[all[['url', 'label_id']].duplicated()])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78459a0007df7940f67ca7146d7d1a5bd6e84ab7","_cell_guid":"6f8784b5-ab96-40ec-ace6-c1060635a334"},"cell_type":"markdown","source":"For a more representative validation data, let's split the combined data based on the label distribution with stratification."},{"metadata":{"collapsed":true,"_uuid":"e31d20e7f86e8d569305d1138d600ea28bb60973","_cell_guid":"7fe50aef-1e47-4069-baa1-e914e471585e","trusted":false},"cell_type":"code","source":"X = all[['url']]\ny = all[['label_id']]\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, stratify = y, test_size = 0.3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e1628c5d673e6fb474b15fdffc8f1f5a77778c55","_cell_guid":"bdcb1833-f79a-4328-9bf1-75c4198533fe"},"cell_type":"markdown","source":"Plot the distribution to see out work"},{"metadata":{"_uuid":"7e1566e7980147c9399cae17725e713b30e21cf6","_cell_guid":"3609c54b-54e0-4928-afd8-940c7efb06ae","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.distplot(y_train['label_id'])\nsns.distplot(y_val['label_id'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1972988cb76a0bbd032f8ab23c7b3f5aa8cd7301","_cell_guid":"d272488c-7cfc-4417-8722-1dcb9fb00e76"},"cell_type":"markdown","source":"Save the newly generated train and validation data"},{"metadata":{"collapsed":true,"_uuid":"40e380bd3282e2670667f4ed976f58319986b0c0","_cell_guid":"50bccbba-bc9e-4e57-bb09-a0dcadea5706","trusted":true},"cell_type":"code","source":"# train = pd.concat([X_train, y_train], axis = 1)\n# validation = pd.concat([X_val, y_val], axis = 1)\n# datas = {'train': train, 'validation': validation}\n# for data_name, data in datas.items():\n#     data[['url', 'label_id']].to_csv('%s/%s.csv'%(processed_data_dir, data_name), index = False)","execution_count":1,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}