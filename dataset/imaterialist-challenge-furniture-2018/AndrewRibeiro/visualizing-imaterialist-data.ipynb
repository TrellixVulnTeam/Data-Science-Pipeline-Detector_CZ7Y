{"cells":[{"metadata":{"_cell_guid":"6e2a83f1-1135-4a27-a882-208565dc4d5a","_uuid":"ff0ce783883f9f94bf875f52b9e22c7c166964a3"},"cell_type":"markdown","source":"$$\n\\huge\\text{Visualizing iMaterialist Data}\\\\\n\\large\\text{March 2018}\\\\\n\\text{Andrew Riberio @ https://github.com/Andrewnetwork}\n$$\n<img width=\"700\" height=\"300\" src=\"https://i.imgur.com/HcJJmzj.jpg\" />\nIn this notebook we will visualize data from the Kaggle challenge *iMaterialist Challenge (Furniture) at FGVC5*. \n\n**NOTE**: In order for the interactive componnents of this kernel to function, you must either fork this kernel or download it to your local machine which has the required environment and dependencies. The simplest method is to fork the kernel here on kaggle. "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport json\n\n# Libraries for displying the data. \nfrom IPython.core.display import HTML \nfrom ipywidgets import interact\nfrom IPython.display import display\n\n%matplotlib inline","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"cd64708b-676c-42f9-b2b3-3320d2188bc8","_uuid":"f8f02f2ae9cde02c0984b4bb8f0e761aad86cad9"},"cell_type":"markdown","source":"## Loading the data"},{"metadata":{"_cell_guid":"b34b41d1-68bd-4fea-8df2-77380c60de8a","_uuid":"774c9aed4ee371c6ed894746c10526309b3c50e2"},"cell_type":"markdown","source":"We use the json library to load the data into python dictionary objects. "},{"metadata":{"_cell_guid":"70d557fa-26b6-48bf-8180-05765462d923","_uuid":"26814b04f6ce0d4102ebce5afa35f5d2b812cdea","collapsed":true,"trusted":true},"cell_type":"code","source":"training   = json.load(open(\"../input/train.json\"))\ntest       = json.load(open(\"../input/test.json\"))\nvalidation = json.load(open(\"../input/validation.json\"))","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"d4e39d34-3822-4786-8fb1-0c55c9c2d306","_uuid":"2b8ebc673ac50a37c908fea52cb8743a37c17a15"},"cell_type":"markdown","source":"We itterate over the json dictionaries loaded in above and produce a pandas dataframe for the training, validation, and test data. "},{"metadata":{"_cell_guid":"54fe508c-3a0f-41c8-9329-04e589e9356c","_uuid":"adcb3a16fc5ba3c3b1de97ede24f4617e9d8f67f","collapsed":true,"trusted":true},"cell_type":"code","source":"# A function to be mapped over the json dictionary. \ndef joinFn(dat):\n    return [dat[0][\"url\"][0], dat[1][\"label_id\"]]\n\ntrainingDF   = pd.DataFrame(list(map(joinFn, zip(training[\"images\"],training[\"annotations\"]))),columns=[\"url\",\"label\"])\nvalidationDF = pd.DataFrame(list(map(joinFn, zip(validation[\"images\"],validation[\"annotations\"]))),columns=[\"url\",\"label\"])\ntestDF       = pd.DataFrame(list(map(lambda x: x[\"url\"],test[\"images\"])),columns=[\"url\"])","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"53d85726-bb87-4822-bcc9-139eba91fc95","_uuid":"dff1cabc2f0f741ffebb38ee8b4d08bca15d354c","trusted":true},"cell_type":"code","source":"trainingDF","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8f365089dcf3bed5c00594bb09693002c77ed36"},"cell_type":"code","source":"validationDF.head()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5d9c01ca0d6a9ec87aa63688c731fc42260b307e"},"cell_type":"code","source":"testDF.head()","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"914e9bfe-c457-4df9-b437-281351280fd8","_uuid":"6292caacb7a89e25e5c888d671597d6335808962"},"cell_type":"markdown","source":"## Basic Visualization "},{"metadata":{"_cell_guid":"7ca628e0-2f1e-44f3-94e0-b7fa35d4a339","_uuid":"16d8baaf8be2913804a5a0afb72fc5cf2be9bae0","trusted":true},"cell_type":"code","source":"print(\"Number of classes: {0}\".format(len( trainingDF[\"label\"].unique())))","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"a710f893-cddb-4765-ba14-6ca8e6715bf3","_uuid":"366cef26441bfd5aafd2bb66648ae9e5fe9f2ce4","trusted":true},"cell_type":"code","source":"trainingDF[\"label\"].value_counts().plot(kind='bar',figsize=(40,10),title=\"Number of Training Examples Versus Class\").title.set_size(40)","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"1e57628a-c907-4d4f-9654-750250273e18","_uuid":"2cff6ca2979a07799f77d21f023e054139958428"},"cell_type":"markdown","source":"In the above chart we can see that we have a heavily skewed dataset in respect the the number of training examples per class. Class 20 has about 4,000 examples where class 83 has less than 500. \n\nWe use the next funciton to view examples of each class in the training data. The overlayed number is the class label. "},{"metadata":{"_cell_guid":"e062e3de-c653-43db-846a-1d7434075033","_uuid":"494272e608b3342608f56dcc38bdb94689485bc3","scrolled":false,"trusted":true},"cell_type":"code","source":"def displayExamples(exampleIndex=0):\n    outHTML = \"<div>\"\n    for label in range(1,129):\n        img_style = \"width: 180px;height:180px; margin: 0px; float: left; border: 1px solid black;\"\n        captionDiv = \"<div style='position:absolute;right:30px;color:red;font-size:30px;background-color:grey;padding:5px;opacity:0.5'>\"+str(label)+\"</div>\"\n        outHTML += \"<div style='position:relative;display:inline-block'><img style='\"+img_style+\"' src='\"+trainingDF[trainingDF.label == label].iloc[exampleIndex][0]+\"'/>\"+captionDiv+\"</div>\"\n    outHTML += \"</div>\"\n    display(HTML(outHTML))\n\ndisplayExamples()","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"4ff332d8-8879-42f5-aec7-86b08eded886","_uuid":"08094eeffda4b9305feadcddb81d23939cc54118"},"cell_type":"markdown","source":"We can use the following function to view examples for a particular category/class. "},{"metadata":{"_cell_guid":"cb504ab7-69c9-4b14-8095-4f8da01b7cb8","_kg_hide-output":false,"_uuid":"9cf3617aba56a6242ba9110a1548fb8858d6baba","scrolled":false,"trusted":true},"cell_type":"code","source":"def displayCategoryExamples(category=0,nExamples=20):\n    outHTML = \"<div>\"\n    for idx in range(0,nExamples):\n        img_style = \"width: 180px;height:180px; margin: 0px; float: left; border: 1px solid black;\"\n        outHTML += \"<div style='position:relative;display:inline-block'><img style='\"+img_style+\"' src='\"+trainingDF[trainingDF.label == category].iloc[idx][0]+\"'/></div>\"\n    outHTML += \"</div>\"\n    display(HTML(outHTML))\n    \ndisplayCategoryExamples(7)","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"1ac4ab35-00fa-47ae-bf1e-20d49596c229","_uuid":"48ec1db69186a799d91255fb083b27bbf0d9de60"},"cell_type":"markdown","source":"Just from looking at examples in class 7, TVs, we can note a few things: \n* Some are natural images with the TVs included in their surroundings. \n* Some are artifical images of cartoon TVs. \n* Some images have digital artifacts overlayed upon the image ( like the date of the image ). \n* There are different angles provided. \n\nLet's look at another category to see if this type of variablity is consistent across the classes. \n"},{"metadata":{"_cell_guid":"b9b37a05-fdf8-4247-ba88-a28a196a8721","_uuid":"59e3460501bba6769736dc83369ee83c5afb519c","trusted":true},"cell_type":"code","source":"displayCategoryExamples(24)","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"4c725b80-38ee-43b9-8185-2d5520d90114","_uuid":"6553d6580dc438c6eecd2b8c65ccb9690af8f539","collapsed":true},"cell_type":"markdown","source":"It does seem like there is a huge variablity within each class. In the next section you will be able to interactively pan around the dataset. This requires you to be running this notebook as a fork, as a static rendering will not include interactive elements. There is no preloading done here, so it may take some time for all the images to load. "},{"metadata":{"_cell_guid":"2947657b-662f-4d29-bc61-6e5de60dbea1","_uuid":"c36e8dccfc958218536c0bf95ed6ff01fe50764b","trusted":true},"cell_type":"code","source":"def visCat(cat=1):\n    displayCategoryExamples(cat,40)\n    \ninteract(visCat, cat=(1,128))","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"3efea7a4-d0df-4457-887d-5f380c98b0b6","_uuid":"2d23f3841c49381554945a136844c9170801862c"},"cell_type":"markdown","source":"## Next Steps\nFor futher visualization we would need to download the images which we are unable to do in a contained Kaggle kernel. There are a few download scripts available in the kernels section. It would be nice if someone could download the images and upload a partial set of the training, validation, and test data so we can do more analysis in these notebooks. \n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}