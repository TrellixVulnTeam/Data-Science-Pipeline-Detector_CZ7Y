{"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"file_extension":".py","version":"3.6.3","nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","mimetype":"text/x-python"},"kernelspec":{"display_name":"Python 3","name":"python3","language":"python"}},"cells":[{"metadata":{},"cell_type":"markdown","source":"# Class Prediction on the Test Set\n\nAfter running BingQing Wei's \"XGBoost with Context\" kernel, you should have a new file in your working directory called xgb_model.  Below is code you can use to produce a file called \"classes.csv\".  Once you have this file, you should be able to run another one of my kernels, \"Better Late Than Never\", which will produce an output file that you can submit.  If you use his model out of the box you should get a LB score of .9910.  If you change boosts (the third parameter in the line that starts with \"model = xgb.train(...)\") to 220 instead of 50, you should get a LB score of .9911.  Note that changing from 50 to 220 will make the runtime significantly longer."},{"outputs":[],"metadata":{"_cell_guid":"e5d4a529-673a-4a2f-9e53-c1d89258d4b4","collapsed":true,"_uuid":"378f34c60320c764b06dee5a393fa46c9bf90a69"},"cell_type":"code","execution_count":null,"source":"import pandas as pd\nimport numpy as np\nimport os\nimport pickle\nimport gc\nimport xgboost as xgb\nimport numpy as np\nimport re\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nmax_num_features = 10\npad_size = 1\nboundary_letter = -1\nspace_letter = 0\n#max_data_size = 320000\n\nout_path = r'.'\ndf = pd.read_csv(r'en_test_2.csv')\n\nx_data = []\n#y_data =  pd.factorize(df['class'])\n#labels = y_data[1]\n#y_data = y_data[0]\ngc.collect()\nfor x in df['before'].values:\n    x_row = np.ones(max_num_features, dtype=int) * space_letter\n    for xi, i in zip(list(str(x)), np.arange(max_num_features)):\n        x_row[i] = ord(xi)\n    x_data.append(x_row)\n\ndef context_window_transform(data, pad_size):\n    pre = np.zeros(max_num_features)\n    pre = [pre for x in np.arange(pad_size)]\n    data = pre + data + pre\n    neo_data = []\n    for i in np.arange(len(data) - pad_size * 2):\n        row = []\n        for x in data[i : i + pad_size * 2 + 1]:\n            row.append([boundary_letter])\n            row.append(x)\n        row.append([boundary_letter])\n        neo_data.append([int(x) for y in row for x in y])\n    return neo_data\n\n#x_data = x_data[:max_data_size]\n#y_data = y_data[:max_data_size]\nx_data = np.array(context_window_transform(x_data, pad_size))\ngc.collect()\nx_data = np.array(x_data)\n#y_data = np.array(y_data)\n\nprint('Total number of samples:', len(x_data))\n#print('Use: ', max_data_size)\n#x_data = np.array(x_data)\n#y_data = np.array(y_data)\n\nprint('x_data sample:')\nprint(x_data[0])\n#print('y_data sample:')\n#print(y_data[0])\n#print('labels:')\n#print(labels)\n\nmodel = xgb.Booster({'nthread': 4})\nmodel.load_model('xgb_model')\n\nlabels = [u'PLAIN', u'PUNCT', u'DATE', u'LETTERS', u'CARDINAL', u'VERBATIM',\n       u'DECIMAL', u'MEASURE', u'MONEY', u'ORDINAL', u'TIME', u'ELECTRONIC',\n       u'DIGIT', u'FRACTION', u'TELEPHONE', u'ADDRESS']\n\ndtest = xgb.DMatrix(x_data)\npreds = model.predict(dtest)\npreds2 = [labels[int(x)] for x in preds]\npreds3 = np.array(preds2)\npreds4 = pd.DataFrame(preds3.reshape(len(preds3), 1))\npreds4.to_csv(os.path.join(out_path, 'classes.csv'))\n\n"}],"nbformat_minor":1,"nbformat":4}