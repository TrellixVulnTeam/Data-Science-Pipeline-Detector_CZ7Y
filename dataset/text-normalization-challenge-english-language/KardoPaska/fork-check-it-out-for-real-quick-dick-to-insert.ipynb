{"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"ab034cbc2149bacffa3bd425178cb94d815fe406","_cell_guid":"390315bc-fecf-489c-bc29-407fbe668c0d"},"source":"1. (1)  **READ THE TRAIN SHIT FILE**\n1. (2)  **FIGURE OUT WHAT'S DIFFERENT**\n1. (3)  **LOOK AT NOT SAME SHIT**\n1. (4)  **FIGURE OUT WHICH SHIT IS THE SAME**\n1. (5)  **BUILD SOME DICKS**\n1. (6)  **FUCKING MISSING SHIT**\n1. (7)  **LOOK AT THE FUCKING MISSING SHIT**\n1. (8)  **BUILD SOME MORE DICKS**\n1. (9)  **FIGURE OUT WHERE NOT SAME SHIT HAS DIFFERENT SHIT**\n1. (10)  **LOOK AT THAT SHIT**\n1. (11)  **LOOK AT THAT SHIT AGAIN BUT DIFFERENT**\n1. (12)  **FIND WHERE THE AFTER PARTY HAS CAPITAL LETTERS AND FIGURE OUT WTF**\n1. (13)  **THROW AN ERROR**"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"bd59c14c0d386e130c290a754e7174ea058f4b56","_cell_guid":"371661d6-7455-40a6-b68e-d51dc4ce2b78"},"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport re, string, collections\nfrom fuzzywuzzy import fuzz\nfrom tqdm import tqdm\nfrom IPython.display import display\n\n# Read en_train.csv  file.\ntrain_df = pd.read_csv(filepath_or_buffer=\"../input/en_train.csv\", encoding=\"utf-8\", dtype={'class':'category'})\n#train_df.head()"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"bd4d2c11c346252c36b1e3e88552548f6ac2ab7c","_cell_guid":"2458b9fc-ae89-4a54-920b-a4c16d0a772a"},"source":""},{"cell_type":"markdown","metadata":{"_uuid":"6ab3def19b50595a0a09e0d115429324d9e2d6fa","_cell_guid":"e7744594-82be-403f-b983-a9be6bfa4c6d"},"source":"**test shit**"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"35e7c1223a6fa112e9ccc236e80a72255eadf83c","_cell_guid":"dac5c660-aa3a-4da1-bdc7-b60ef4a49ff6"},"source":"test_df = pd.read_csv(filepath_or_buffer=\"../input/en_test.csv\", encoding=\"utf-8\", dtype={'class':'category'})\ntest_df.head()"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b43aecc12d891c13033469ed4b7d8322f440911e","_cell_guid":"8d2cc40e-3ab0-477e-8bb7-49a3f966a485"},"source":"submitshit = [str(x) + '_' + str(y) for x,y in zip(tqdm(np.array(test_df['sentence_id'])), np.array(test_df['token_id']))]"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2fad0b7c38ba459209fb2f386ab029c970298bd0","_cell_guid":"f06bf8a9-710e-48bf-8c93-a74e48ff259b"},"source":"print(submitshit[:10])\n"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"dea9642a90d465ba5e377549f469c2b22afeacc0","_cell_guid":"058ada17-5f8b-43f1-89e7-5afaf78dc22f"},"source":"test_before = np.array(test_df['before'])\ntest_before_DICK = dict(zip(np.array(test_df.index), test_before))\n\ndisplay(test_before[:10])"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b0d15d67c10fa273864a17f05b15f92a7b76fb5e","_cell_guid":"83699e73-05f0-4c7a-8d9e-0e5d86c65517"},"source":"arr_test_before_str_only = [x for x in test_before if type(x) == type(str())]\nthing = [x.isdigit() for x in tqdm(arr_test_before_str_only)]\nprint(sum(thing))"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"bb93fb66921312912ec68cd3deb804456bfe1d9a","_cell_guid":"2e0f3165-0085-43ad-af10-335436066aae"},"source":""},{"cell_type":"markdown","metadata":{"_uuid":"ea2a785eb9684ffbde3e8f3e6d304fa17aee330d","_cell_guid":"761690ab-883c-402e-89c0-a0150a3a3df9"},"source":"(2)  **FIGURE OUT WHAT'S DIFFERENT AND WHICH SHIT SAMA SAMA**"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1e6031be57e0ac7cfaf3262a1a52f92f59d3c6aa","_cell_guid":"db244dd9-f295-422a-853f-75c248991213"},"source":"#np.array(train_df['before'])\narr_after = np.array(train_df['after'])\n\nidx_not_same = list()\nfor each_after, each_beforeiter in zip(tqdm(arr_after), train_df['before'].iteritems()):\n    if each_after != each_beforeiter[1]:\n        idx_not_same.append(each_beforeiter[0])\nprint(str(len(idx_not_same)) + ' NOT SAME out of ' + str(len(train_df)) +' total (' + str(len(idx_not_same) / len(train_df)) + ' %)')\n\nidx_are_same = set(train_df.index) - set(idx_not_same)\nprint(str(len(idx_are_same)) + ' ARE SAME out of ' + str(len(train_df)) +' total (' + str(len(idx_are_same) / len(train_df)) + ' %)')"},{"cell_type":"markdown","metadata":{"_uuid":"3a3e05487117e9b474fa6b382791ee98300b9a57","_cell_guid":"52538678-2dc9-4317-8422-177d8b8a8fcc"},"source":"> (5)  **BUILD AND STROKE ALL THE  DICKS**"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"f95a18cc2b9cb9c093397c234e9b8013d2323971","_cell_guid":"4085412d-8ad7-4569-ab7a-9a83553ff9f0"},"source":"# find rows where before is the same but after is different\n# find rows where after is the same but before is different\n# and class is different?\n#build a before dick\ntrain_df_idx = np.array(train_df.index)\narr_before = np.array(train_df['before'])\nbefore_DICK = dict()\nfor each_idx, each_before in zip(tqdm(train_df_idx), arr_before):\n    if each_before in before_DICK:\n        before_DICK[each_before].append(each_idx)\n    else:\n        before_DICK[each_before] = [each_idx]\n            \n# build a after dick\nafter_DICK = dict()\nfor each_idx, each_after in zip(tqdm(train_df_idx), arr_after):\n    if each_after in after_DICK:\n        after_DICK[each_after].append(each_idx)\n    else:\n        after_DICK[each_after] = [each_idx]\n\n# build a classy dick\narr_class = np.array(train_df['class'])\nclassy_DICK = dict()\nfor each_idx, each_class in zip(tqdm(train_df_idx), arr_class):\n    if each_class in classy_DICK:\n        classy_DICK[each_class].append(each_idx)\n    else:\n        classy_DICK[each_class] = [each_idx] \n\n# build index to after dick\nidx_after_DICK = dict(zip(np.array(train_df.index), np.array(train_df['after'])))        "},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"517bfe5a1e8dc1a92f5ad2ed95601c12001a17a8","_cell_guid":"ea69908a-f98e-4ae7-85a0-519d825ef716"},"source":"idx_classy_DICK = dict(zip(np.array(train_df.index), np.array(train_df['class'])))        "},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"4fc44be87b66222c6c4e0096462aca06c1e82060","_cell_guid":"1fed98ac-a10d-48aa-bcc6-f78164282da7"},"source":"#cardinal_df = train_df.loc[classy_DICK['CARDINAL']]\n#digit_df = train_df.loc[classy_DICK['DIGIT']]\n#verbatim_df = train_df.loc[classy_DICK['VERBATIM']]\n#display(cardinal_df.head())\n#display(digit_df.head())\n#display(verbatim_df.head())\n\ncdv_df = train_df.loc[classy_DICK['CARDINAL'] + classy_DICK['DIGIT'] + classy_DICK['VERBATIM']]\ndisplay(cdv_df.head())\narr_cdv_before = np.array(cdv_df['before'])\narr_cdv_after = np.array(cdv_df['after'])\n\n#cdv_singlechar = [(x,y) for x,y in zip(tqdm(np.array(cdv_df.index)), arr_cdv_before) if len(y) == 1]\n#print(cdv_singlechar[:10])\n\ncdv_before_after_DICK = dict()\ncdv_multiple_afters = list()\nfor each_idx, each_before, each_after in zip(tqdm(np.array(cdv_df.index)), arr_cdv_before, arr_cdv_after):\n    if len(each_before) != 1:\n        pass\n    elif each_before in cdv_before_after_DICK:\n        if cdv_before_after_DICK[each_before] != each_after:\n            cdv_multiple_afters.append((each_idx, each_before, each_after))\n    else:\n        cdv_before_after_DICK[each_before] = each_after\nprint(len(cdv_multiple_afters))\nprint(cdv_multiple_afters[:10])"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1c0b631e5f5c70895b14fd89ae6c005e1435463b","_cell_guid":"f7b66c88-b5b7-47ef-8b43-0b095cf6fcc9"},"source":"print(set([(y,z) for x,y,z in cdv_multiple_afters]))\nprint(len(cdv_before_after_DICK))"},{"cell_type":"markdown","metadata":{"_uuid":"9f432ba36faf37c084e9ebe04268a4692126ec66","_cell_guid":"16d78bda-6def-4cda-9006-77d2a800d8bc"},"source":"(6)  **FUCKING MISSING SHIT**"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6b0e4422882e4225f0aff6a24c5943395f154c6e","_cell_guid":"0c7be710-bbfd-4fa0-9229-d1f7a9f4ebcf"},"source":"print('    '.join([str(x) for x in [len(before_DICK), len(after_DICK), len(classy_DICK)]]))\n\n# fucking missing shit\nafter_dick_nan_idxs = after_DICK.pop(np.nan, None)\nbefore_dick_nan_idxs = before_DICK.pop(np.nan, None)\n"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8bf009fc54fbf973817b6861f1fbe3ae1d665a8a","_cell_guid":"f875565a-9bda-4d02-843c-2920b42a40f5"},"source":"#test_before\n# are train_before's unique? what?\nlen(train_df['before']) == len(set(train_df['before']))"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"2f8ad0aebaf02fc3595a4ecf9f267784eaaf8fcd","_cell_guid":"6eed3065-638c-481b-8a72-5632d6c868f1"},"source":"# fuckkkk duuuuude"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3317365ce3ac5421c4cbf77c2df6be74fd7c1d85","_cell_guid":"2f0e7481-28b7-48f0-99ec-1bc19a3a7aed"},"source":"collections.Counter(train_df['before']).most_common(10)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"21ab3886573afd14dbe020e2953a7d597ca8defe","_cell_guid":"45716203-e767-4362-beb7-1a7dfaa76567"},"source":"#build a short DICK haha\n# where only got uniques\n#train_df_ARE_same = train_df.loc[idx_are_same]\n#display(train_df_ARE_same[:10])\nbeforeDICK_length = list()\nwith tqdm(total=len(before_DICK)) as pbar:\n    for key, value in before_DICK.items():\n        beforeDICK_length.append(len(before_DICK[key]))\n        pbar.update()\nprint(collections.Counter(beforeDICK_length).most_common(10))        \n"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"ee252507cc7785f0e4563dbe75a433b183ceb23e","_cell_guid":"a0c9914f-acca-437f-bb3e-7a7412753bc3"},"source":"# build a dick where it's longer before and shorter after... yeah cuz it busted a nut\n#train_df_ARE_same_idx = np.array(train_df_ARE_same.index)\n#arr_ARE_same_before = np.array(train_df_ARE_same['before'])\nbefore_after_DICK = dict()\nfor each_idx, each_before in zip(tqdm(train_df_idx), arr_before):\n    if each_before in before_after_DICK:\n        before_after_DICK[each_before].append(idx_after_DICK[each_idx])\n    else:\n        before_after_DICK[each_before] = [idx_after_DICK[each_idx]]"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2c8030b228554c85503640c1b34abf247c8c3420","_cell_guid":"6517675d-d421-4280-bcff-dd3069d29bd1"},"source":"\nbefore_with_same_after = list()\nbefore_with_different_after = list()\nbefore_with_mostcommon_notsame_different_after = list()\nwith tqdm(total=len(before_after_DICK)) as pbar:\n    for key, value in before_after_DICK.items():\n        if len(set(value)) == 1:\n            before_with_same_after.append((key, value[0], len(value)))\n        else:\n            cafters, ccounts  = zip(*collections.Counter(value).most_common(3))\n            before_with_different_after.append((key, cafters, ccounts))\n            mostcommon_notsame = key\n            for each_cafters in cafters:\n                if each_cafters == key:\n                    pass\n                else:\n                    mostcommon_notsame = each_cafters\n                    break\n            before_with_mostcommon_notsame_different_after.append((key, mostcommon_notsame))\n        pbar.update()"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"8dc8cab8539f6bcf976bd23fd4fa5fe2dea5d81b","_cell_guid":"2579337c-5a70-4dc3-ade9-6dbe1d132ded"},"source":"pd.DataFrame(before_with_different_after)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b0be12d19759c33cea11af88a37f6edbadefd5ca","_cell_guid":"1a10d01e-ee0f-4bbc-9d94-cbd991cddea2"},"source":"pd.DataFrame(before_with_same_after, columns=['before', 'after', 'occurences'])"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"743e8073028efc2930d6aab9b03be9a5a0747fb5","_cell_guid":"f8bc6e70-9e86-4663-8505-98d3e4ea8933"},"source":"pd.DataFrame(before_with_mostcommon_notsame_different_after)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"scrolled":true,"_uuid":"508019c7d4ffbf85425462fbf1eaa5e7c08e3784","_cell_guid":"3bbe8314-5312-4e37-b2d0-79fdcfa56ca5"},"source":"# find rows where before is the same but after is different\n# find rows where after is the same but before is different\n# and class is different?\n#before_with_different_after = list()\n#most_common_after_of_before = list()\n#with tqdm(total=len(before_DICK)) as pbar:\n#    for key, value in before_DICK.items():\n#        #unique_afters_of_before = train_df.loc[value, 'after'].unique() #slow as shit\n#        get_that_shit = [idx_after_DICK.get(x) for x in value]\n#        unique_afters_of_before = set(get_that_shit)\n#        pbar.update()\n#        if len(unique_afters_of_before) != 1:\n#            cafters, ccounts = zip(*collections.Counter(get_that_shit).most_common())\n#            most_common_after_of_before.append((key, cafters[0])) #before, after\n#            before_with_different_after.append((key, value, unique_afters_of_before))\n#\n#print(len(before_with_different_after))\n#print(len(most_common_after_of_before))\n#print(len(most_common_after_of_before))\n#most_common_after_of_before[:10]\n"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"0615f261555803a8e5d495bcabc6f8077375b821","_cell_guid":"7c19f705-f1e8-4cd0-a472-bcec7cf6843f"},"source":"#intersection? \n\ntesttrain_intersection = set(test_df['before']).intersection(set(train_df['before']))\nprint(str(len(testtrain_intersection)) + ' or ' + str(100 * len(testtrain_intersection) / len(test_df['before'])) + \" %\")\nprint(list(testtrain_intersection)[:10])"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2613fd97358d6269f556812c5827d240741b4931","_cell_guid":"ac4e6c70-7875-47c2-9bbc-ef72bed85039"},"source":"# 8 nans?\ntestypes = [type(x) for x in test_before]\ntest_before_nonan = [x for x in list(test_before) if type(x) == type(str())]\ntest_before_nanbool = [type(x) == type(str()) for x in test_before]\n\nsubmitshit_nonan = np.array(submitshit)[np.where(test_before_nanbool)]\nsubmitshit_isnan = np.array(submitshit)[np.where(np.logical_not(test_before_nanbool))]\nprint(collections.Counter(testypes).most_common())\nprint(len(submitshit_nonan))\nprint(len(submitshit_isnan))\n#howmany = np.sum(np.isnan(test_before))\n#print(howmany)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6634f719b19bacca3c32443ef969b9ad794323a2","_cell_guid":"7e35a1b8-75ed-45db-afc0-28e5f2151f8e"},"source":"np.logical_not(test_before_nanbool[:10])"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"aaae89df9f2dd8f7e0548faf8f1994a467ff2d18","_cell_guid":"b15a89b2-c61a-4531-813f-972412f85826"},"source":"# wow lots of single char entires?\nunicodepoint = [ord(x) if len(x) == 1 else [ord(y) for y in x] for x in test_before_nonan]\n\ntypes_unicodepoint = [type(x) for x in tqdm(unicodepoint)]\nprint(collections.Counter(types_unicodepoint).most_common())"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"d83b11dc5393b46bce12f437dff1f10c8e190bc5","_cell_guid":"6f63f50e-9a4a-42cc-8681-1d32097be3ee"},"source":"xxx, yyy, _ = zip(*before_with_same_after)\nbig_train_DICK = dict(zip(xxx, yyy))\nsmall_train_DICK = dict(before_with_mostcommon_notsame_different_after)\nprint(len(big_train_DICK))\nprint(len(small_train_DICK))"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1f18de781149ae51e52914a2ead281416a2e27a3","_cell_guid":"182405b4-6a5b-4b3f-a827-9db414773335"},"source":"submit_1stcol = list()\nsubmit_2ndcol = list()\nfor each_submit in submitshit_isnan:\n    submit_1stcol.append(each_submit)\n    submit_2ndcol.append('n a')\n\nnotfoundcount = 0    \nnotfoundcount2 = 0\nfor each_submit, each_testbefore in zip(tqdm(submitshit_nonan), test_before_nonan):\n    submit_1stcol.append(each_submit)\n    spititback = each_testbefore\n    nopunct = each_testbefore.translate({ord(c): None for c in string.punctuation})\n    if each_testbefore.isupper():\n        submit_2ndcol.append(' '.join(each_testbefore))\n    elif (each_testbefore.isalpha()) and not (each_testbefore.isalnum()):\n        submit_2ndcol.append(spititback)\n#    elif (nopunct.isnumeric()) | (nopunct.isalnum()):\n#        tryit = str()\n#        for each_char in nopunct:\n#            cdv_val = cdv_before_after_DICK.get(each_char)\n#            if cdv_val is None:\n#                tryit = spititback\n#                notfoundcount2 = notfoundcount2 + 1\n#                break\n#            else:\n#                tryit = tryit + ' ' + cdv_val\n#        submit_2ndcol.append(tryit.strip())\n    else:\n        DICK_val = small_train_DICK.get(each_testbefore)\n        if DICK_val is None:\n            DICK_val = big_train_DICK.get(each_testbefore)\n            #if DICK_val is None:\n                #DICK_val = nopunct\n            if DICK_val is None:\n                tryit = str()\n                for each_char in nopunct:\n                    cdv_val = cdv_before_after_DICK.get(each_char)\n                    if cdv_val is None:\n                        tryit = None\n                        notfoundcount2 = notfoundcount2 + 1\n                        break\n                    else:\n                        tryit = tryit + ' ' + cdv_val\n                if tryit is None:\n                    DICK_val = nopunct\n                else:\n                    DIVK_val = tryit.strip()\n                    notfoundcount = notfoundcount + 1\n        submit_2ndcol.append(DICK_val)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{},"source":"submit_2ndcol[:15]"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"272ebf5c5b1624bf84f126cd68e8a6768bbbd5a7","_cell_guid":"d8a96f91-9a07-405e-be69-e9388b0248b9"},"source":"Submitnans = submit_2ndcol[:8]\nSubmitnonnan = submit_2ndcol[8:]\nsubmit_2ndcol_quotes = Submitnans + ['\"' + x + '\"' if x is not None else x for x in Submitnonnan]"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"babcac372f2b05a278156f378dfac8ebfaab3303","_cell_guid":"558ed446-359c-46b9-8647-4cb88b358198"},"source":"print(notfoundcount)\nprint(notfoundcount2)\n#forsubmission = pd.DataFrame(list(zip(submit_1stcol, submit_2ndcol)), columns=['id', 'after'])\nforsubmission = pd.DataFrame(list(zip(submit_1stcol, submit_2ndcol_quotes)), columns=['id', 'after'])\ndisplay(forsubmission)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b9728aed8ee4e10639a62bf93f700111563c805c","_cell_guid":"8a184b17-bcb5-4afc-9ee2-41a1f8fcbbe6"},"source":"submitdate = 'submit_20171010.csv'\nforsubmission.to_csv(submitdate, encoding='utf-8', index=False)\nreaditback = pd.read_csv(submitdate, encoding='utf-8', nrows=10)\ndisplay(readitback)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"69fd44a0824bf31c936aba66a2c419d4a5549945","_cell_guid":"5b242335-d2c8-403f-878f-4a4fe2d9f022"},"source":"import gzip\nf_in = open(submitdate, 'rb')\nf_out = gzip.open(submitdate + '.gzip', 'wb')\nf_out.writelines(f_in)\nf_out.close()\nf_in.close()\nprint('i done')"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"f5d6f13db437dd392be01f6ac394518bfdac4334","_cell_guid":"ea5d9053-06f6-45ac-8913-b4b7249b6c19"},"source":"#THROW AN ERROR"}],"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"nbconvert_exporter":"python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.1","file_extension":".py","mimetype":"text/x-python"}}}