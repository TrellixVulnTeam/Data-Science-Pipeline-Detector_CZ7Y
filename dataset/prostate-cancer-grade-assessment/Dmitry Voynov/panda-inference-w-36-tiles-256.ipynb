{"cells":[{"metadata":{},"cell_type":"markdown","source":"## This notebook is distributed under MIT License","execution_count":null},{"metadata":{"_uuid":"54842c1e-6f25-40f5-9189-effcb413510b","_cell_guid":"d6459f8f-3850-4fb5-bda0-c650f44e92c3","trusted":true},"cell_type":"code","source":"import os\nimport sys\nsys.path = [\n    '../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master',\n] + sys.path\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom efficientnet_pytorch import model as enet\n\nfrom tqdm import tqdm_notebook as tqdm\nimport skimage.io","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c2f9307-e380-420d-b14a-2305cb0e7c7a","_cell_guid":"55d464ce-6ba8-4db2-a33f-334a89e75c5a","trusted":true},"cell_type":"code","source":"data_dir = '../input/prostate-cancer-grade-assessment'\ndf_train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(data_dir, 'test.csv'))\ndf_sub = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))\n\nmodel_dir = '../input/panda-base-model'\nimage_folder = os.path.join(data_dir, 'test_images')\nis_test = os.path.exists(image_folder) \nimage_folder = image_folder if is_test else os.path.join(data_dir, 'train_images')\n\ndf = df_test if is_test else df_train.loc[:3]\nprint(df.shape)\n\ntile_size = 256\nimage_size = 256\nn_tiles = 36\nbatch_size = 4\nnum_workers = 4\n\ndevice = torch.device('cuda')\n\nprint(image_folder)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e440a77-995e-41cc-9e12-204c4de0f9a5","_cell_guid":"f4a0a5cf-dfeb-4387-9b86-c7d1a4438968","trusted":true},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{"_uuid":"b27156c1-3541-42d5-8e72-4129ee9afb86","_cell_guid":"baa24457-b168-4b2a-9ee8-ca51c0f18ca2","trusted":true},"cell_type":"code","source":"class enetv2(nn.Module):\n    def __init__(self, backbone, out_dim):\n        super(enetv2, self).__init__()\n        self.enet = enet.EfficientNet.from_name(backbone)\n        self.myfc = nn.Linear(self.enet._fc.in_features, out_dim)\n        self.enet._fc = nn.Identity()\n\n    def extract(self, x):\n        return self.enet(x)\n\n    def forward(self, x):\n        x = self.extract(x)\n        x = self.myfc(x)\n        return x\n    \n    \ndef load_models(model_files):\n    models = []\n    for model_dict in model_files:\n        model_f = model_dict['name']\n        model_f = os.path.join(model_dir, model_f)\n        backbone = 'efficientnet-b0'\n        model = enetv2(backbone, out_dim=model_dict['out_dim'])\n        if model_dict['parallel']:\n            model = nn.DataParallel(model)\n        model.load_state_dict(torch.load(model_f, map_location=lambda storage, loc: storage), strict=True)\n        model.eval()\n        model.to(device)\n        models.append(model)\n        print(f'{model_f} loaded!')\n    return models\n\n\nmodel_files = [\n    {\n        'name': 'model_2_bce_0.881.pth',\n#         Trained on several GPUs in parallel\n        'parallel': True,\n#         Misconfigured while training, it should have predicted 5 bins\n        'out_dim': 6,\n        'n_tiles': 36\n    },\n    {\n        'name': 'model_0_bce_4x4_0.884.pth',\n        'parallel': False,\n        'out_dim': 5,\n        'n_tiles': 16\n    },\n    {\n        'name': 'model_2_bce_5x5_total_0.883.pth',\n        'parallel': False,\n        'out_dim': 5,\n        'n_tiles': 25\n    },\n    {\n        'name': 'model_3_bce_6x6_total_0.883.pth',\n        'parallel': False,\n        'out_dim': 5,\n        'n_tiles': 36\n    }\n]\n\nmodels = load_models(model_files)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee63922b-3196-4206-88fa-7e4cb3a29786","_cell_guid":"ca639470-917a-42d4-864b-9415b55898e6","trusted":true},"cell_type":"markdown","source":"# Dataset","execution_count":null},{"metadata":{"_uuid":"79fbdbfb-1cc4-44d1-aecd-7209ef2de573","_cell_guid":"7c5e75b2-d058-466e-a15c-79eb4089ed96","trusted":true},"cell_type":"code","source":"def get_tiles(img, n_tiles, mode=0):\n    result = []\n    h, w, c = img.shape\n    pad_h = (tile_size - h % tile_size) % tile_size\n    pad_w = (tile_size - w % tile_size) % tile_size\n    mode_pad_h = (tile_size * mode) // 4\n    mode_pad_w = (tile_size * mode) // 4\n    img2 = np.pad(img,[[pad_h // 2 + mode_pad_h, pad_h - pad_h // 2 + tile_size - mode_pad_h], \n                       [pad_w // 2 + mode_pad_w, pad_w - pad_w // 2 + tile_size - mode_pad_w], [0,0]], constant_values=255)\n    img3 = img2.reshape(\n        img2.shape[0] // tile_size,\n        tile_size,\n        img2.shape[1] // tile_size,\n        tile_size,\n        3\n    )\n\n    img3 = img3.transpose(0,2,1,3,4).reshape(-1, tile_size, tile_size,3)\n    n_tiles_with_info = (img3.reshape(img3.shape[0],-1).sum(1) < tile_size ** 2 * 3 * 255).sum()\n    if len(img3) < n_tiles:\n        img3 = np.pad(img3, [[0, n_tiles - len(img3)], [0, 0] , [0, 0] , [0, 0]], constant_values=255)\n    idxs = np.argsort(img3.reshape(img3.shape[0],-1).sum(-1))[:n_tiles]    \n    img3 = img3[idxs]\n    for i in range(len(img3)):\n        result.append({'img':img3[i], 'idx':i})\n    return result, n_tiles_with_info >= n_tiles\n\n\nclass PANDADataset(Dataset):\n    def __init__(self,\n                 df,\n                 image_size,\n                 n_tiles=n_tiles,\n                 tile_mode=0,\n                 rotate=False,\n                 rand=False,\n                 sub_imgs=False\n                ):\n\n        self.df = df.reset_index(drop=True)\n        self.image_size = image_size\n        self.n_tiles = n_tiles\n        self.tile_mode = tile_mode\n        self.rotate = rotate\n        self.rand = rand\n        self.sub_imgs = sub_imgs\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        img_id = row.image_id\n        \n        tiff_file = os.path.join(image_folder, f'{img_id}.tiff')\n        image = skimage.io.MultiImage(tiff_file)[1]\n        tiles, OK = get_tiles(image, n_tiles=self.n_tiles, mode=self.tile_mode)\n\n        if self.rand:\n            idxes = np.random.choice(list(range(self.n_tiles)), self.n_tiles, replace=False)\n        else:\n            idxes = list(range(self.n_tiles))\n        idxes = np.asarray(idxes) + self.n_tiles if self.sub_imgs else idxes\n\n        n_row_tiles = int(np.sqrt(self.n_tiles))\n        images = np.zeros((image_size * n_row_tiles, image_size * n_row_tiles, 3))\n        for h in range(n_row_tiles):\n            for w in range(n_row_tiles):\n                i = h * n_row_tiles + w\n    \n                if len(tiles) > idxes[i]:\n                    this_img = tiles[idxes[i]]['img']\n                else:\n                    this_img = np.ones((self.image_size, self.image_size, 3)).astype(np.uint8) * 255\n                this_img = 255 - this_img\n                h1 = h * image_size\n                w1 = w * image_size\n                \n                if self.rotate:\n                    # 90 degrees counter-clockwise rotation\n                    this_img = this_img.transpose(1, 0, 2)[::-1]\n                    \n                images[h1:h1+image_size, w1:w1+image_size] = this_img\n\n        images = images.astype(np.float32)\n        images /= 255\n        images = images.transpose(2, 0, 1)\n\n        return torch.tensor(images)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26d59d3a-1d40-4b71-b156-0ff2993dc75c","_cell_guid":"e773cb49-b777-4114-ab1a-9e800ff8d56f","trusted":true},"cell_type":"markdown","source":"# Prediction","execution_count":null},{"metadata":{"_uuid":"e2354707-3ee3-48cd-9c13-4cdda5e5a137","_cell_guid":"9555f68d-d49f-4f34-9ee1-5e339d2447a5","trusted":true},"cell_type":"code","source":"model_loaders = []\n\nfor model_dict, model in zip(model_files, models):\n    loaders = []\n    for mode in [0, 1, 2, 3]:\n        for rotate in [False, True]:\n            dataset = PANDADataset(df, image_size, n_tiles=model_dict['n_tiles'], tile_mode=mode, rotate=rotate)\n            loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n            loaders.append(loader)\n    \n    model_loaders.append((model, loaders))\nprint(f'{len(model_loaders) * len(model_loaders[0][1])} predictions in total')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22b38077-ce03-45f5-bff2-ca4e829dc9fd","_cell_guid":"7a0b3072-4e3e-4f8b-a5b5-3186fb2a07e3","trusted":true},"cell_type":"code","source":"LOGITSS = []\nwith torch.no_grad():\n    for model, loaders in model_loaders:\n        for loader in loaders:\n            LOGITS = []\n            for data in tqdm(loader):\n                data = data.to(device)\n                logits = model(data)\n                LOGITS.append(logits)\n            LOGITS = torch.cat(LOGITS).sigmoid().cpu().numpy().sum(1)\n            LOGITSS.append(LOGITS)\n\nLOGITS = np.array(LOGITSS).mean(0)\nPREDS = LOGITS.round()\n\ndf['isup_grade'] = PREDS.astype(int)\ndf[['image_id', 'isup_grade']].to_csv('submission.csv', index=False)\nprint(df.head())\nprint()\nprint(df.isup_grade.value_counts())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}