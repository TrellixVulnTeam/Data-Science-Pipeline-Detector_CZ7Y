{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, sys\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# tensorflow imports\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt, cv2\nimport tensorflow_io as tfio\nfrom tqdm import tqdm\n\n# get dataset path\nfrom kaggle_datasets import KaggleDatasets\n\n# regex\nimport re\n\n# PIL\nfrom PIL import Image\n\nfrom skimage import io\nfrom skimage import novice\nfrom skimage.transform import rescale, resize, downscale_local_mean\n\n# EXTRA RUN INSTRUCTIONS\n# change Settings -> Accelerator -> TPU v3-8 to use TPU (avoid burning through TPU hours when not in session)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-27T19:48:11.565907Z","iopub.execute_input":"2022-04-27T19:48:11.566231Z","iopub.status.idle":"2022-04-27T19:48:11.594843Z","shell.execute_reply.started":"2022-04-27T19:48:11.566195Z","shell.execute_reply":"2022-04-27T19:48:11.594026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nREFERENCE\n# Code is a mix of tutorials (kaggle and google codelabs tutorials) for the TPU dataloading\n# and Tensorflow tutorial for a basic cnn\n# Code has been modified to fit competition dataset, along with pre-processing to analyze the data\n\nTPU Tutorials:\nhttps://www.kaggle.com/docs/tpu\nhttps://www.kaggle.com/code/mgornergoogle/five-flowers-with-keras-and-xception-on-tpu/notebook\nhttps://codelabs.developers.google.com/codelabs/keras-flowers-tpu/#4\n\nAlong with, for CNN:\nhttps://www.tensorflow.org/tutorials/images/cnn\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-04-27T07:55:21.884481Z","iopub.execute_input":"2022-04-27T07:55:21.884744Z","iopub.status.idle":"2022-04-27T07:55:21.892945Z","shell.execute_reply.started":"2022-04-27T07:55:21.884717Z","shell.execute_reply":"2022-04-27T07:55:21.892228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA VISUALIZATION AND EXPLORATION\n\n# explore data","metadata":{"execution":{"iopub.status.busy":"2022-04-27T07:55:26.013598Z","iopub.execute_input":"2022-04-27T07:55:26.013973Z","iopub.status.idle":"2022-04-27T07:55:26.017136Z","shell.execute_reply.started":"2022-04-27T07:55:26.013947Z","shell.execute_reply":"2022-04-27T07:55:26.016468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ENABLE TPU\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T07:55:29.925047Z","iopub.execute_input":"2022-04-27T07:55:29.925486Z","iopub.status.idle":"2022-04-27T07:55:30.192499Z","shell.execute_reply.started":"2022-04-27T07:55:29.925456Z","shell.execute_reply":"2022-04-27T07:55:30.190734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SET HYPERPARAMETERS\n\n# ideal batch size is 128 per TPU core, (128 * 8 = 1024), TPU v3-8 the core count is 8\n#SWAP THIS IN FOR TPU RUN## BATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\nBATCH_SIZE = 16\n\n# size of TFRecord batches\nSIZE_OF_TFRECORD_BATCH = 820\n\n# learning rate, should increase learning rate with batch size\nLEARNING_RATE = 1\n\n# number of epochs\nNUMBER_OF_EPOCHS = 1\n\n# activation function\nACTIVATION_FUNCTION = \"relu\" \n\n# send multiple batches to the TPU at once\nSTEPS_PER_EXECUTION = 32\n\n# training data split\nTRAINING_DATA_SPLIT = .7\n\n# image size\n# ideal image size for TPUs is 512 x 512 or 256 x 256 for imagenet\nIMAGE_WIDTH = 512\nIMAGE_HEIGHT = 512\nIMAGE_CHANNELS = 3\nIMAGE_MAX_DIMENSION = IMAGE_WIDTH\nIMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n\n# AUTOTUNE for dataset\nAUTOTUNE = tf.data.AUTOTUNE\n\n# get directory path for google cloud services, where data is hosted\nGCS_PATH = KaggleDatasets().get_gcs_path()\n\n# list of filename locations on GCS\nIMAGE_FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/train_images/*.tiff\")\nMASK_FILENAMES = tf.io.gfile.glob(GCS_PATH + \"/train_label_masks/*.tiff\")","metadata":{"execution":{"iopub.status.busy":"2022-04-27T19:21:43.117431Z","iopub.execute_input":"2022-04-27T19:21:43.118084Z","iopub.status.idle":"2022-04-27T19:21:47.470354Z","shell.execute_reply.started":"2022-04-27T19:21:43.118033Z","shell.execute_reply":"2022-04-27T19:21:47.46953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA PRE-PROCESSING - LABEL DATA\n\n# list of image ids\nIMAGE_IDS = []\nfor image_filename in IMAGE_FILENAMES:\n    IMAGE_IDS.append(((re.search(\"(\\w*).tiff\", image_filename))[1]))\n\n# list of mask ids\nMASK_IDS = []\nfor mask_filename in MASK_FILENAMES:\n    MASK_IDS.append(((re.search(\"(\\w*)_mask.tiff\", mask_filename))[1]))\n\n# https://stackoverflow.com/questions/3462143/get-difference-between-two-lists\nIMAGE_IDS_WITH_NO_MASK = [x for x in IMAGE_IDS if x not in set(MASK_IDS)]\n\n# get training data csv into panda object\ntrain_csv = pd.read_csv(GCS_PATH + \"/train.csv\")\n\n# replace \"negative\" with 0+0 since they are equivalent\ntrain_csv['gleason_score'] = train_csv['gleason_score'].replace(\"negative\" ,\"0+0\")\n\n# remove all entries where image has no mask\nfor image_id_no_mask in IMAGE_IDS_WITH_NO_MASK:\n    train_csv = train_csv[train_csv.image_id != image_id_no_mask]\n    \n# get labels\nCATEGORIES = train_csv.gleason_score.unique()\nCATEGORIES_LENGTH = len(CATEGORIES)\n\n# print data preprocessing\nprint(\"Length of Images: \" + str(len(IMAGE_FILENAMES)))\nprint(\"Length of Masks: \" + str(len(MASK_FILENAMES)))\nprint(\"Length of Images with no Mask: \" + str(len(IMAGE_IDS_WITH_NO_MASK)))\nprint(\"Length of entries in train_csv: \" + str(len(train_csv)))\nprint()\nprint(\"Final Array:\")\nprint(train_csv)","metadata":{"execution":{"iopub.status.busy":"2022-04-27T18:55:16.760241Z","iopub.execute_input":"2022-04-27T18:55:16.760496Z","iopub.status.idle":"2022-04-27T18:55:24.037194Z","shell.execute_reply.started":"2022-04-27T18:55:16.760468Z","shell.execute_reply":"2022-04-27T18:55:24.036217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -BUILD DATASET AS TFRECORDS\n\ndef decode_tiff(filename):\n    # get the bits for file name\n    bits = tf.io.read_file(filename)\n    return bits\n\n# function to calculate downscale size while maintaing aspect ratio\ndef calculate_downscale_width_height(image_width, image_height, max_width, max_height):\n    # calculate downscale values\n    width_downscale = image_width / max_width\n    height_downscale = image_height / max_height\n    print(max_width)\n    print(max_height)\n    print(width_downscale)\n    print(height_downscale)\n    print(image_width)\n    print(image_height)\n    # compare downscale values to return new scale that approximates aspect ratio\n    if (width_downscale > height_downscale):\n        new_image_width = max_width\n        new_image_height = int(image_height / width_downscale)\n    elif(width_downscale > height_downscale):\n        new_image_width = int(image_width / height_downscale)\n        new_image_height = max_height\n    else:\n        new_image_width = max_width\n        new_image_height = max_height\n    if (new_image_width > max_width or new_image_height > max_height):\n        raise ValueError(\"Bad calc for downscale width/height for \" + str(image_width) + \"and\" + str(image_width))\n    return new_image_width, new_image_height\n\n# downscale mask\n# downscale image\n# create new 512x 512 plot\n# overlay image data\n\n# final image\n# apply convolution if desired\n\n# turn image into some 3 channel format\n\nimage = io.imread(\"../input/prostate-cancer-grade-assessment/train_images/006f6aa35a78965c92fffd1fbd53a058.tiff\")\nprint(\"../input/prostate-cancer-grade-assessment/train_images/006f6aa35a78965c92fffd1fbd53a058.tiff\")\nprint(calculate_downscale_width_height(7000, 7010, IMAGE_WIDTH, IMAGE_HEIGHT))\nimage_rescaled = resize(image, (512, 512), anti_aliasing=True)\nplt.imshow(image_rescaled)\nplt.show\n\n# first step is doing image transformations\n    # room for a convolve\n    # tiff to another 3 channel format\n    # downscale to desired level\n# additionally, if we want to put data in TRF + how we do that for TPU loading, which I think is page 4/5\n# and how to divide into training/testing datasets\n# potentially where we're image transform, rotate, and add dataset ordering or address class imbalances\n\n\n# should end up as history = (train_images, train_labels) and (test_images, test_labels)\n# images should be converted to an ndarray with shape (# of images, 512 x 512, 3)\n# labels should be converted to an ndarray with shape (# of images, 1)\n# can work backwards on this, may need to incorporate data streaming in here\n\n# train_images =\n# train_labels =\n# test_images =\n# test_labels =\n","metadata":{"execution":{"iopub.status.busy":"2022-04-27T20:10:13.030824Z","iopub.execute_input":"2022-04-27T20:10:13.031742Z","iopub.status.idle":"2022-04-27T20:10:17.131779Z","shell.execute_reply.started":"2022-04-27T20:10:13.031684Z","shell.execute_reply":"2022-04-27T20:10:17.130903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to input filename, and get array of \ndef decode_tiff(filename):\n    # get the bits for file name\n    bits = tf.io.read_file(filename)\n    # decode into tiff, array shape is [height, width, 4], 4 is RGBA\n    image_as_rgba = tfio.experimental.image.decode_tiff(bits)\n    # convert rgba to rgb\n    image_as_rgb = tfio.experimental.color.rgba_to_rgb(image_as_rgba)\n    return image_as_rgb\n\n# takes img as rgb and resizes to new size\ndef resize_image(image, newWidth, newHeight):\n    image = tf.image.resize(image, (newWidth, newHeight))\n    return image\n\ndef black_mask_over_image_rgb(image, mask):\n    black_pixel = [0,0,0]\n    for row in mask:\n        for column in row:\n            if(column == black_pixel):\n                image[row][column] = black_pixel\n    return mask\n\ndef convolution_on_image():\n    pass\n\ndef testFunction():\n    # get original image and mask\n    original_rgb_image = decode_tiff(image_filenames[0])\n    original_rgb_mask = decode_tiff(mask_filenames[0])\n    \n    # resize image\n    original_rgb_image_resized = resize_image(original_rgb_image, IMAGE_WIDTH, IMAGE_HEIGHT)\n    original_rgb_mask_resized = resize_image(original_rgb_mask, IMAGE_WIDTH, IMAGE_HEIGHT)\n    print(original_rgb_mask_resized)\n    \ntestFunction()\nprint(\"function finished\")\n\n### DATA PRE-PROCESSING - TRAINING SPLIT AND SET-UP FOR MODEL INTAKE\n\n# -BUILD DATASET AS TFRECORDS\nfilenames = tf.io.gfile.glob(GCS_PATH + \"/train_images/*.tiff\")\n\n# helper functions for TFRecord\ndef _bytes_feature(value):\n  #Returns a bytes_list from a string or byte\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy()\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n  #Returns a float_list from a float or double\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n  #Returns an int64_list from a bool / enum / int / uint\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef serialize_example(feature0, feature1, feature2):\n  feature = {\n      'image': _bytes_feature(feature0),\n      'image_id': _bytes_feature(feature1),\n      'label': _int64_feature(feature2)\n  }\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return examp\n\n# function to input filename, and get array of \ndef decode_tiff(filename):\n    # get the bits for file name\n    bits = tf.io.read_file(filename)\n    # decode into tiff, array shape is [height, width, 4], 4 is RGBA\n    image_as_rgba = tfio.experimental.image.decode_tiff(bits)\n    # convert rgba to rgb\n    image_as_rgb = tfio.experimental.color.rgba_to_rgb(image_as_rgba)\n    return image_as_rgb\n\n# input data in my_img_bytes, my_class, my_height, my_width, my_floats\nwith tf.python_io.TFRecordWriter(filename) as out_file:\n  feature = {\n    \"image\": _bytestring_feature([my_img_bytes]), # one image in the list\n    \"class\": _int_feature([my_class]),            # one class in the list\n    \"size\": _int_feature([my_height, my_width]),  # fixed length (2) list of ints\n    \"float_data\": _float_feature(my_floats)       # variable length  list of floats\n  }\n  tf_record = tf.train.Example(features=tf.train.Features(feature=feature))\n  out_file.write(tf_record.SerializeToString())\n\n\n# -RUN IMAGE PROCESSING CODE\n# ignore order of images for faster processing\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\n# get list of file addresses in gcs\nfilenames = tf.io.gfile.glob(GCS_PATH + \"/train_images/*.tfrec\")\n\n# define dataset, list of filenames, as tfrecord\ndataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n\ndataset = dataset.with_options(ignore_order)\n\n# dataset mapping\n#dataset = dataset.map()\n\n\nprint(dataset)\n\nimage_id = train_csv.image_id.values\n\ntotal_number_of_images = len(train_csv)\n\nfor i in range(12):\n    print('writing TFRecord')\n    OneTRecordFileBatch = min(SIZE_OF_TFRECORD_BATCH,total_image-i*SIZE_OF_TFRECORD_BATCH)\n    with tf.io.TFRecordWriter('TFRecord'+str(i)+'_'+str(OneTRecordFileBatch)+'.tfrec') as writer:\n        for k in tqdm(range(OneFiOneTRecordFileBatchle)):\n            img = cv2.imread('GCS_PATH + /train_images/' + str(image_id[SIZE_OF_TFRECORD_BATCH*i+k])+'.tiff')\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Fix incorrect colors\n            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 100))[1].tobytes()\n            TFRd = serialize_example(\n                img, \n                str.encode(image_id[SIZE_OF_TFRECORD_BATCH*i+k]),\n                train_csv.loc[train_csv.image_id==image_id[SIZE_OF_TFRECORD_BATCH*i+k],'gleason_score'].values[0]                           \n            )\n            writer.write(TFRd)\n\nim = run_imageset_transform(image_filenames[0], mask_filenames[0])\nim.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\ntotal_image = len(train_csv)\nfor i in range(12):\n    print('writing TFRecord')\n    OneFile = min(SIZE_OF_TFRECORD_BATCH,total_image-i*SIZE_OF_TFRECORD_BATCH)\n    with tf.io.TFRecordWriter('TFRecord'+str(i)+'_'+str(OneFile)+'.tfrec') as writer:\n        for k in tqdm(range(OneFile)):\n            img = cv2.imread('../input/panda-resized-train-data-512x512/train_images/train_images/'+str(image_id[SIZE_OF_TFRECORD_BATCH*i+k])+'.png')\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Fix incorrect colors\n            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 100))[1].tobytes()\n            TFRd = serialize_example(\n                img, \n                str.encode(image_id[SIZE_OF_TFRECORD_BATCH*i+k]),\n                train_csv.loc[train_csv.image_id==image_id[SIZE_OF_TFRECORD_BATCH*i+k],'gleason_score'].values[0]                           \n            )\n            writer.write(TFRd)\n\n# -HELPER FUNCTIONS\n# convert tiff image, enter filename, return array of image as rgb and label\ndef decode_tiff_full(filename):\n    # get the bits for file name\n    bits = tf.io.read_file(filename)\n    # decode into tiff, array shape is [height, width, 4], 4 is RGBA\n    image_as_rgba = tfio.experimental.image.decode_tiff(bits)\n    # convert rgba to rgb\n    image_as_rgb = tfio.experimental.color.rgba_to_rgb(image_as_rgba)\n    # get image id from filename\n    image_id_from_filename = (re.search(\"(\\w*).tiff\", filename))[1]\n    # find row in train_csv data where image id matches filename\n    entry = train_csv.loc[train_csv['image_id'] == image_id_from_filename]\n    # set the label to the gleason score\n    label = entry['gleason_score'].to_numpy()[0]\n    return image_as_rgb, label\n\n# helper functions for TFRecord\ndef _bytes_feature(value):\n  #Returns a bytes_list from a string or byte\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy()\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n  #Returns a float_list from a float or double\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n  #Returns an int64_list from a bool / enum / int / uint\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef serialize_example(feature0, feature1, feature2):\n  feature = {\n      'image': _bytes_feature(feature0),\n      'image_id': _bytes_feature(feature1),\n      'label': _int64_feature(feature2)\n  }\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return example_proto.SerializeToString()\n\n# process images into TFRecords\nlenames = image_filenames[:split]\n\nimage_id = train_csv.image_id.values\nSIZE = 885\ntotal_image = len(train_csv)\nfor i in range(12):\n    print('writing TFRecord')\n    OneFile = min(885,total_image-i*885)\n    with tf.io.TFRecordWriter('TFRecord'+str(i)+'_'+str(OneFile)+'.tfrec') as writer:\n        for k in tqdm(range(OneFile)):\n            img = cv2.imread('../input/panda-resized-train-data-512x512/train_images/train_images/'+str(image_id[885*i+k])+'.png')\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Fix incorrect colors\n            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 100))[1].tobytes()\n            TFRd = serialize_example(\n                img, \n                str.encode(image_id[885*i+k]),\n                train_csv.loc[train_csv.image_id==image_id[885*i+k],'label'].values[0]                           \n            )\n            writer.write(TFRd)\n\ndataset = 1 # load something\ndataset = dataset.shuffle(1000) # shuffle the dataset with a buffer of 1000\ndataset = dataset.cache() # cache the dataset in RAM or on disk\ndataset = dataset.repeat() # repeat the dataset indefinitely\ndataset = dataset.batch(128) # batch data elements together in batches of 128\ndataset = dataset.prefetch(AUTOTUNE) # prefetch next batch(es) while training\n\nfilenames_dataset = tf.data.Dataset.list_files(GCS_PATH + \"/train_images/*.tiff\")\n\n# configure streaming options\ndataset = dataset.shuffle(1000)\ndataset = dataset.cache()\ndataset = dataset.repeat()\ndataset = dataset.batch(BATCH_SIZE)\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\ndataset = dataset.prefetch(AUTOTUNE)\n\nfilenames = tf.io.gfile.glob(GCS_PATH + \"/train_images/*.tiff\")\n#dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\ndataset = dataset.with_options(ignore_order)\ndataset = dataset.map(read_TFRecord, num_parallel_calls=AUTOTUNE)\ndataset = force_images_sizes(dataset, (IMAGE_WIDTH, IMAGE_HEIGHT))\n\nprint(filenames[0])\nprint(decode_tiff(filenames[0]))\n\n# images should be converted to an ndarray with shape (# of images, 512 x 512, 3)\n# labels should be converted to an ndarray with shape (# of images, 1\ntrain_images = 1;\ntrain_labels = 1;\n\ntest_images = 1;\ntest_labels = 1;\n\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\n# configure streaming options\ndataset = dataset.shuffle(1000)\ndataset = dataset.cache()\ndataset = dataset.repeat()\ndataset = dataset.batch(BATCH_SIZE)\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\ndataset = dataset.prefetch(AUTOTUNE)\n\nfilenames = tf.io.gfile.glob(GCS_PATH + \"/train_images/*.tiff\")\n#dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE)\ndataset = dataset.with_options(ignore_order)\ndataset = dataset.map(read_TFRecord, num_parallel_calls=AUTOTUNE)\ndataset = force_images_sizes(dataset, (IMAGE_WIDTH, IMAGE_HEIGHT))\n\nprint(filenames[0])\nprint(decode_tiff(filenames[0]))\n\n####\n\n\n\n# convert to numpy for iteration\n# train_csv_array = train_csv.to_numpy()\n# print(train_csv_array)\n\n# build TFRecord reader\ndef read_TFRecord(example):\n    features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"class\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, features)\n    image = tfio.experimental.image.decode_tiff(example['image'], index=0, name=None)\n    class_label = tf.cast(example['class'], tf.int32)\n    return image\n    \n# set options\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\n# build training dataset\ntraining_dataset = tf.data.TFRecordDataset(training_filenames, num_parallel_reads=AUTO)\ntraining_dataset = dataset.with_options(ignore_order)\ntraining_dataset = dataset.map(read_TFRecord, num_parallel_calls=AUTO)\ntraining_dataset = force_image_sizes(dataset, (IMAGE_WIDTH, IMAGE_HEIGHT))\n\ntraining_dataset = load_dataset(training_filenames)\nprint(training_dataset) \n\n# training_dataste = tf.data.Dataset\n\n\ndef get_training_dataset():\n    dataset = load_dataset(training_filenames)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef input_\n\ntrain_labels = np.empty((IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS), int)\ntest_labels = np.empty_like((IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS))\n\n\nprint(train_labels)\n\n# for row in train_csv_array\n\n# train_images = \n# train_labels =\n\n# test_images =\n# test_labels =\n\n# image data loading and construct arrays\n# print(image_dataset)\n\n\n# images should be converted to an ndarray with shape (# of images, 512 x 512, 3)\n# labels should be converted to an ndarray with shape (# of images, 1)\n# train_images, train_labels and test_images, test_labels should be constructed\n\n# normalize pixel values between 0 and 1\n# train_images =\n# test_images =\n\n###########################\n\n# image_dataset = tf.data.TFRecordDataset(image_filenames)\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DATA VISUALIZATION AND EXPLORATION\n\n# explore data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SET AND RUN MODEL\n\n# run model design and model compile within TPU strategy scope, to prepare for TPU computation\nwith tpu_strategy.scope():\n    # MODEL DESIGN (PART 1)\n    model = tf.keras.models.Sequential()\n    # Convolutional layers\n    # 64 filters, (3,3) feature kernel, input image 512x512 w/ 3 channels\n    model.add(tf.keras.layers.Conv2D(64, (3,3), activation=ACTIVATION_FUNCTION, input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Conv2D(128, (3,3), activation=ACTIVATION_FUNCTION)\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Conv2D(128, (3,3), activation=ACTIVATION_FUNCTION)\n    # Dense Layers\n    model.add(tf.keras.layers.Flatten())\n    # determine model summary to get shape from last layer of (Conv2D), print(model.summary)\n    model.add(tf.keras.layers.Dense(64, activation=ACTIVATION_FUNCTION))\n    # final output should be 10, since we have 10 classes (these are the gleason_scores)\n    model.add(tf.keras.layers.Dense(categories_length))\n\n    # COMPILE MODEL (PART 2)\n    # set optimizer for learning rate for adam optimizer\n    model_optimizer = keras.optimizers.Adam(LEARNING_RATE)\n\n    # compile model\n    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'], steps_per_execution=STEPS_PER_EXECUTION)\n\n# build history\nhistory = model.fit(train_images, train_labels, epochs=NUMBER_OF_EPOCHS, validation_data=(test_images, test_labels))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL EVALUATION\n\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower_right')\n\ntest_loss, test_acc, model.evaluate()","metadata":{},"execution_count":null,"outputs":[]}]}