{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from zipfile import ZipFile \n  \nwith ZipFile('/kaggle/input/quora-insincere-questions-classification/embeddings.zip', 'r') as embd_zip: \n    print(embd_zip.namelist())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. * **DEBUT DU TRAVAIL SUR LES DONNES**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def DEBUG_DICTIONARY(dct, limit=10):\n    for i, key in enumerate(dct.keys()):\n        if i > limit: break\n        print(key, dct[key])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# conguration des donnees d entrainement et de test\ntrain_data, val_data = train_test_split(pd.read_csv('/kaggle/input/quora-insincere-questions-classification/train.csv'), test_size=0.2, random_state=42)\nsentences, targets = train_data['question_text'], train_data['target']\nval_sentences, val_targets = val_data['question_text'], val_data['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"targets.value_counts(), val_targets.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pour chaque mot - compter combien de il occure dans les phrases\ndef configure_sentences(sentences, lower = True):\n    words = {}\n    for sentence in sentences:\n        for word in sentence.split():\n            if lower: word = word.lower()\n            words[word] = words.get(word, 0) + 1\n    return words\n\nwords = configure_sentences(sentences)\n#frequence des mot dans les phrase \nDEBUG_DICTIONARY(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# coconnaitre quelle type de mot est le plus utilisee\nDEBUG_DICTIONARY({word: cnt for word, cnt in sorted(words.items(), key=lambda item: item[1], reverse=True)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# indexage des mot\ndef configure_words(words):\n    vocabulary = {}\n    for i, word in enumerate(words.keys()):\n        vocabulary[word] = i # vocabulary[i] = word\n    return vocabulary\n\nvocabulary = configure_words(words)\nDEBUG_DICTIONARY(vocabulary)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# retourne le minum le maximum le moyen la longueur et trace aussi l histograme de distrubission de longueur  \ndef configure_sentence_statistic(sentences):\n    def sentence_len(s):\n        return len(s.split())\n    \n    sentences.apply(sentence_len).plot(title='Sentence Length Distribution',y='Length Frequency',kind='hist', colormap='autumn', logy=True);\n    return np.min(sentences.apply(sentence_len)), np.round(np.mean(sentences.apply(sentence_len))), np.max(sentences.apply(sentence_len))\n\nmin, avrg, max = configure_sentence_statistic(sentences)\n\nprint('minimum sentence length {} - average sentence length {} - maximum sentence length {}'.format(min, avrg, max))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HIDDEN_SIZE = 30","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\n# Affiche comment target est distrubie \ndef configure_target_statistic(targets):\n    trg_cnt = targets.value_counts()\n    labels, sizes = (np.array(trg_cnt.index)), (np.array(100*(trg_cnt/trg_cnt.sum())))\n    py.iplot(go.Figure(data=[go.Pie(labels=labels, values=sizes)], layout=go.Layout(title='Distrubition de target',font=dict(size=15),width=500, height=500)))\n    return trg_cnt\n\nconfigure_target_statistic(targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filttrer les donnees suivant les parametre fourni\ndef filter_and_display_data(sentences, targets, target=0, min_len=5, max_len=30, limit=3):\n    result = []\n    for i, sentence in enumerate(sentences):\n        sent_len = len(sentence.split(' '))\n        if min_len <= sent_len and sent_len <= max_len:\n            if targets[i] == target:\n                result.append(sentence)\n                if len(result) >= limit: break\n    \n    if(len(result) ==- 0):\n        print('no such sequencies found.')\n        return\n    \n    print('{} {} sentences with length between {}-{}:\\n'.format(limit, 'GOOD' if target == 0 else 'BAD', min_len, max_len))\n    for i, s in enumerate(result):\n        print(str(i+1)+\")\",s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Affichage des expemple de nos donnees"},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=0, min_len=120, max_len=140)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1, min_len=120, max_len=140)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pour une longueur entre 120 et 140 il parrait que nos donnees sont difficile a determiner target meme pour un humain et pour target\n=1 on n a pas d exemples  donc il est pas bon pour nos donnnees d avoir toute les types des exemples basic mais ca ne fait rien "},{"metadata":{"trusted":true},"cell_type":"code","source":"# lit et retourne un dictionnaire cle: mot;ads and returns dictionary - key: word; valeur le vecteur attachee  (vec. length=300)\ndef confnigure_embeddings(embd_path):\n    word2vecs = {}\n    with ZipFile('/kaggle/input/quora-insincere-questions-classification/embeddings.zip') as embd_zip:\n        for embd in embd_zip.open(embd_path, 'r'):\n            word2vec = embd.decode().split(' ')\n            word2vecs[word2vec[0]] = np.asarray(word2vec[1:], dtype='float32')\n    return word2vecs\n            \nword2vecs = confnigure_embeddings('glove.840B.300d/glove.840B.300d.txt')\nDEBUG_DICTIONARY(word2vecs, limit=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dans chaque phrase remplace les mots par les  in each sentence replaces words with its own embedding vectors \ndef configure_word2vecs(sentences, word2vecs):\n    def configure_sentence(sentence, len=HIDDEN_SIZE):\n        return ([word2vecs.get(word.lower(), np.zeros(300)) for word in sentence.split()] + [np.zeros(300)]*len)[:len] \n    \n    return [configure_sentence(sentence) for sentence in sentences]\n\n# embedding_sentences = configure_word2vecs(sentences, word2vecs)\n# print(embedding_sentences[:10])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Debut de travail sur le modele "},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 256\nBATCHES = (len(sentences)+BATCH_SIZE-1)//BATCH_SIZE\n\nEPOCHS = 2 # gpu :(\nEMBD_SIZE = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpu = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngpu, torch.cuda.is_available()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#la longue courte memoire est la plus en adequation pour ce cas comme notre donnes d entrainemrnt est plus 1M et nos vecteurs attachee sont\n#de longueur 300 si nous convertons notre toute notre data dans tensor une fois on a besin de 16 gb de de ram et beauccoup plus de ressources pour entrainer cette data\n\n# Aussi nous utilisons une couche lineaire car on a pas des dependences difficile -comme on a dans tous les qui contient bad une target de 1 le plus souvent \n\n\nclass LSTM(nn.Module):\n    def __init__(self, input_dim=1, emb_dim=EMBD_SIZE, hid_dim=HIDDEN_SIZE, n_layers=1, output_dim=1, dropout=0.3):\n        super().__init__()\n        self.hid_dim, self.n_layers = hid_dim, n_layers\n        \n        # nn's\n        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, batch_first=True)\n        self.linear = nn.Linear(hid_dim, output_dim)\n        \n       \n        \n        \n    def forward(self, src):\n        outputs, (hidden, cell) = self.lstm(src)\n        return self.linear(hidden.reshape(-1, self.hid_dim))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# craete model - with lstm and linear layers\nmodel = LSTM().to(gpu)\n\n# init loss function\nloss_function = nn.BCEWithLogitsLoss().to(gpu) #nn.MSELoss()\n\n# init optimizer with learning rate 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#evalue et retourne l accuracy pour Y predit par le modele\ndef acc_function(y_pred, y_test):\n    y_pred = torch.round(torch.sigmoid(y_pred).to(gpu)).to(gpu)\n    correct = (y_pred == y_test).sum().float()\n    return torch.round(100*(correct/y_pred.shape[0]))\n\n# genere et retoune idx-ieme batch conmme torch tensor suivant les donneees fournies\ndef get_batch(sentences, targets, idx):\n    src = configure_word2vecs(sentences[BATCH_SIZE*idx:BATCH_SIZE*(idx+1)], word2vecs)\n    trg = np.asarray(targets[BATCH_SIZE*idx:BATCH_SIZE*(idx+1)], dtype='bool')\n    return torch.FloatTensor(src).to(gpu), torch.FloatTensor(trg).to(gpu)\n\n#evalue et retourne f1 pour y predis par le model\ndef f1_score(y_pred, y_test):\n    tp = (y_test * y_pred).sum().to(torch.float32)\n    tn = ((1 - y_test) * (1 - y_pred)).sum().to(torch.float32)\n    fp = ((1 - y_test) * y_pred).sum().to(torch.float32)\n    fn = (y_test * (1 - y_pred)).sum().to(torch.float32)\n    \n    epsilon = 1e-7 #pour feviter les crash\n    precision, recall = tp / (tp + fp + epsilon), tp / (tp + fn + epsilon)\n    \n    return 2*(precision*recall)/(precision + recall + epsilon)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DEBUT D ENTRAINEMET DU MODELE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# pret pour l entrainement\nmodel.train()\n\nVALIDATION_BATCHES = 10\n# validation du data pour la precision pendant l entrainement -mais prenant seulement VALIDATIONBATCHES pendant que toute la data est tres grande.\nval_sents = configure_word2vecs(val_sentences[:VALIDATION_BATCHES*BATCH_SIZE], word2vecs)\nval_targs = np.asarray(val_targets[:VALIDATION_BATCHES*BATCH_SIZE], dtype='bool')\n\nval_batch = torch.FloatTensor(val_sents).to(gpu)\nval_target = torch.FloatTensor(val_targs).to(gpu)\nprint(type(val_batch), val_batch.shape, type(val_targets), val_targets.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCHES, BATCH_SIZE, get_batch(sentences, targets, 0)[0].shape, get_batch(sentences, targets, 0)[1].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# entrainement \nfor e in range(EPOCHS):\n    # enregistre epoch loss et accuracy\n    epoch_loss, epoch_acc = 0, 0\n    for b in range(BATCHES):\n        # prend le batch courant de la data\n        X_batch, y_batch = get_batch(sentences, targets, b)\n        \n        # met le gradientsset a zero,avant de commencer la backpropagation - evitant la msnwue de direction pour le minumum .\n        optimizer.zero_grad()\n\n        #predit targets pour le bstch courant et apprend en la comparant avec la fonction loss.\n        y_pred = model(X_batch)\n        loss = loss_function(y_pred, y_batch.unsqueeze(1))\n        \n        #predit target pour la data de validation et evalue la precision\n        val_pred = model(val_batch)\n        acc = acc_function(val_pred, val_target.unsqueeze(1))\n\n        #grandients sont enregistree par tensors-lors de l appel de backward sur la fonction loss.\n        loss.backward()\n        \n        #met a jour les parametres du modele\n        optimizer.step()\n        \n        #ajoute bash et accuracy pour evaluer epoch loss/acc.\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n        if b == 0 or (b+1) % 100 == 0:\n            print(f'Epoch {(e+1)+0:03} | Batch {(b+1)+0:04}: | Loss: {epoch_loss/(b+1):.5f} | Acc: {epoch_acc/(b+1):.3f} | F1: {f1_score(val_pred, val_target.unsqueeze(1)):.3f}')\n            \n    print(f'Epoch {(e+1)+0:03}: | Epoch Loss: {epoch_loss/BATCHES:.5f} | Epoch Acc: {epoch_acc/BATCHES:.3f}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DEBUT DU TRAVAIL SUR LA DATA DU TEST"},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialisation du data de test\ntest_data = pd.read_csv('/kaggle/input/quora-insincere-questions-classification/test.csv')\nsentences, targets = test_data['question_text'], []\nTEST_BATCHES = (len(sentences)+BATCH_SIZE-1)//BATCH_SIZE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"min, avrg, max = configure_sentence_statistic(sentences)\n\nprint('minimum sentence length {} - average sentence length {} - maximum sentence length {}'.format(min, avrg, max))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sentences), len(targets), TEST_BATCHES","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PREDIT LA DATA DU TEST SUIVANT NOTRE MODELE"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    for b in range(TEST_BATCHES):\n        #obtenir le bash courant\n        X_batch = torch.FloatTensor(configure_word2vecs(sentences[BATCH_SIZE*b:BATCH_SIZE*(b+1)], word2vecs)).to(gpu)\n        \n        # predit bach selon notre modele entraine\n        trg = torch.round(torch.sigmoid(model(X_batch))).cpu().numpy().squeeze()\n        targets.extend(trg)\n        \n        if b == 0 or (b+1) % 100 == 0: print(f'Batch {(b+1)+0:04} predicted')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# enregistre les donnees a transmetre \ntest_targets = (np.array(targets) >= 0.5).astype(np.int)\n\nsubmit = pd.DataFrame({\"qid\": test_data['qid'], \"prediction\": test_targets})\nsubmit.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Affichage des resultats\nsubmit.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"comme il parrait de notre prediction il est comme les donnees d entrainement ,c est bon"},{"metadata":{},"cell_type":"markdown","source":"****voyons quelque exemple de nos prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"configure_target_statistic(submit['prediction'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"il parrait que notre modele marche bien"},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1, min_len=100, max_len=150)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filter_and_display_data(sentences, np.asarray(targets, dtype='int'), target=1, min_len=100, max_len=150)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"..s il n ya plus de longue sequence c est ok"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}