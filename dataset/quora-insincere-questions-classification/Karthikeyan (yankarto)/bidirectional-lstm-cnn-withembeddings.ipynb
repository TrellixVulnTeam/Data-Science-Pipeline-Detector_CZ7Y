{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Cleaning The texts using re library\nimport re\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nfrom nltk.stem.porter import PorterStemmer\nps=PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nmax_features = 50000\n\nfrom keras.preprocessing.sequence import pad_sequences\nmaxlen = 100\n\nimport gensim\nfrom gensim import corpora,models,similarities\n\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\ntest = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\nsubm = pd.read_csv('../input/quora-insincere-questions-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain, val_df = train_test_split(train, test_size=0.1, random_state=2018)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = train[\"question_text\"].fillna(\"_na_\").values\nval_X=val_df[\"question_text\"].fillna(\"_na_\").values\ntest1 = test[\"question_text\"].fillna(\"_na_\").values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embed_size = 300","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train1))\n\ntrain1 = tokenizer.texts_to_sequences(train1)\ntest1 = tokenizer.texts_to_sequences(test1)\nval_X=tokenizer.texts_to_sequences(val_X)\n\n\ntrain1 = pad_sequences(train1, maxlen=maxlen)\ntest1 = pad_sequences(test1, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\n\ntrain_y = train['target'].values\nval_y = val_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from zipfile import ZipFile\nfile_name='/kaggle/input/quora-insincere-questions-classification/embeddings.zip'\nz=ZipFile(file_name)\nprint(z.namelist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_FILE=z.extract('paragram_300_sl999/paragram_300_sl999.txt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nembedding_dict = {}\nfilename = os.path.join(os.getcwd()+'/paragram_300_sl999/paragram_300_sl999.txt')\nwith open(filename,encoding='utf8',errors='ignore') as f:\n    for line in f:\n        line = line.split()\n        token = line[0]\n        try:\n            coefs = np.asarray(line[1:], dtype='float32',)\n            embedding_dict[token] = coefs\n        except:\n            pass\nprint('The embedding dictionary has {} items'.format(len(embedding_dict)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"       \nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix_1 = np.zeros(shape=[nb_words, embed_size])\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n\ndel embedding_dict; gc.collect() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"S_DROPOUT = 0.4\nDROPOUT = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, CuDNNLSTM, concatenate\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Dropout, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom sklearn import metrics\nimport gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport random as rn\n\nbatch_size=512\n\nnp.random.seed(40)\nrn.seed(40)\ntf.random.set_seed(40)\n\nfrom keras.layers import Input, Dense, Embedding,LSTM\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Dropout\nfrom keras.models import Model\n   \n# def create_lstm():\ninput = Input(shape=(maxlen,))\n   \n# Embedding layer has fixed weights, so set 'trainable' to False\nx = Embedding(nb_words, embed_size, weights=[embedding_matrix_1], trainable=False)(input)\nx = Bidirectional(LSTM(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.2)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=input, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n   \n#     return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train1, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val_lstm_y = model.predict([val_X], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_lstm_y = model.predict([test1], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val_y = pred_val_lstm_y   # two random numbers\npred_test_y = pred_test_lstm_y\n\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 1)\n    res = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n\nfilter_sizes = [1,2,3,5]\nnum_filters = 36\n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix_1])(inp)\nx = SpatialDropout1D(S_DROPOUT)(x)\nx = Reshape((maxlen, embed_size, 1))(x)\n\nmaxpool_pool = []\nfor i in range(len(filter_sizes)):\n    conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n                                 kernel_initializer='he_normal', activation='elu')(x)\n    maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n\nz = Concatenate(axis=1)(maxpool_pool)   \nz = Flatten()(z)\nz = Dropout(DROPOUT)(z)\n\noutp = Dense(1, activation=\"sigmoid\")(z)\n\nmodel = Model(inputs=inp, outputs=outp)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train1, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val_cnn_y = model.predict([val_X], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_cnn_y = model.predict([test1], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del word_index, embedding_matrix, model, inp, x\nimport gc; gc.collect()\ntime.sleep(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_val_y = 0.6 * pred_val_lstm_y + 0.4 * pred_val_cnn_y  # two random numbers\npred_test_y = 0.6 * pred_test_lstm_y + 0.4 * pred_test_cnn_y\n\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_y = (pred_test_y > best_thresh).astype(int)\n# out_df = pd.DataFrame({\"qid\":test1[\"qid\"].values})\nsubm['prediction'] = pred_test_y\nsubm.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}