{"cells":[{"metadata":{"_uuid":"bc262cc486149b226f719d33f1a3369936fa3fca"},"cell_type":"markdown","source":"# Best Model and Result Analysis"},{"metadata":{"_uuid":"068757b458895f8a96b5492b609a9cc47504b90d"},"cell_type":"markdown","source":"## 1. Loading \n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#General packages\nimport os\nimport numpy as np #for linear algebra\nimport pandas as pd # for data processing\nfrom tqdm import tqdm #for progress information\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Load \"train\" dataset\ndata = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94cc0bbb87d225771f1eb99242f9322e02bba883"},"cell_type":"code","source":"#Load \"glove.840B.300d\" data. Glove is a data files which assigns to each possible word a vector to \"explain\" it in machine language\nembeddings_index = {}    #creates empty list\nglove = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt') #opens the test document for reading\nfor line in tqdm(glove): #for every line in this text do the following        (tqdm: and show the progress)\n    values = line.split(\" \")  #splits the string every time there is a space into seperate strings\n    word = values[0] #the first string in this text file is always the word\n    coefs = np.asarray(values[1:], dtype='float32') # the following strings are the \"explanation\"\n    embeddings_index[word] = coefs #the list is now filled with entries consisting of the word and the respective \"explanations\" (word vectors)\nglove.close() #closes the file such that is not possible to read it anymore\n\nprint('The dictionary contains %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"015cf90bf78a8b332a6862f4cd4066a72a881802"},"cell_type":"markdown","source":"## 3. Preprocessing data and building the embedding layer\n### Preprocessing the dataset\n"},{"metadata":{"trusted":true,"_uuid":"a82b459cb118b2fcd968b1c52937e5a211defcda"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#this time some portion of the data is used to test the model in the end\ntrain, test = train_test_split(data, \n                 test_size = 0.1, shuffle = True)\ntrain = train.reset_index(drop=True) \ntest = test.reset_index(drop = True)\nprint (\"The training dataset has the shape:\" , train.shape)\nprint (\"The test dataset has the shape:\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad3399132075e7ab71b56635c6a94fe5e141d3ed"},"cell_type":"code","source":"train2, validate = train_test_split(train, \n                 test_size = 0.05, shuffle = True)\ntrain = train2.reset_index(drop=True) \nvalidate = validate.reset_index(drop = True)\nprint (\"The training dataset has the shape:\" , train.shape)\nprint (\"The validation dataset has the shape:\", validate.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25bd68d79f013b9c09151ac2526b975434b29ae6"},"cell_type":"code","source":"X_train = train.iloc[:,1] #Takes all rows of the first column as new dataset\nY_train = np.array(train.iloc[:, 2]) #Takes all rows of the second column as new dataset\n\nX_validate = validate.iloc[:,1]\nY_validate = np.array(validate.iloc[:, 2])\n\nX_test = test.iloc[:,1]\nY_test = np.array(test.iloc[:, 2])\n\nprint(X_train.shape)\nprint(Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4531b037ffd6b8a205234086137917529f48a31"},"cell_type":"markdown","source":"### Fitting the dataset to the embedding layer\n"},{"metadata":{"trusted":true,"_uuid":"463cda27e889bdc6681a001fce7493b559004890"},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94e5ac2f0dcec9d0734c84774dc42f6d6a489b6a"},"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0d38b7a4bad74fbf0abb90e16ee74c3134f49781"},"cell_type":"code","source":"tokenizer_2 = Tokenizer(filters = puncts, lower = False)\ntokenizer_2.fit_on_texts(list(data[\"question_text\"]))\nword_index_2 = tokenizer_2.word_index\nprint('Found %s unique tokens.' % len(word_index_2))\n\nsequences_2 = tokenizer_2.texts_to_sequences(data[\"question_text\"])\nmaxlen_2 = len(max(sequences_2, key = len)) #max number of words in a question (längste sequenz aus tokenisierten wörtern)\n\nX_train_seq_2 = tokenizer_2.texts_to_sequences(X_train)\nX_train_seq_2 = pad_sequences(X_train_seq_2, maxlen=maxlen_2)\n\nX_validate_seq_2 = tokenizer_2.texts_to_sequences(X_validate)\nX_validate_seq_2 = pad_sequences(X_validate_seq_2, maxlen=maxlen_2)\n\nX_test_seq_2 = tokenizer_2.texts_to_sequences(X_test)\nX_test_seq_2 = pad_sequences(X_test_seq_2, maxlen=maxlen_2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81dbe3a14f72d649d5efed4babb80c9c5d446e4e"},"cell_type":"markdown","source":"### Creating the embedding matrix"},{"metadata":{"trusted":true,"_uuid":"041c77255611f5d5d1e4352171564cc5f4f2a656"},"cell_type":"code","source":"#Compute Embedding Matrix for the tweaked data\nembed_dim = 300 #da glove.840B.300d.txt bedeutet, dass 300d. vektor\nembedding_matrix_2 = np.zeros((len(word_index_2) + 1, embed_dim)) #creation of the numpy array\nfor word, i in tqdm(word_index_2.items()): #loop going through each word in word_index\n    embedding_vector = embeddings_index.get(word) #for each word the programm takes the respective vector and calls it embedding_vector\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix_2[i] = embedding_vector # vector gets inserted into the numpy array at the place where the word would stand according to the index.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f3aad33a9b0eda1880168ec7ea1131e4afb9d1c"},"cell_type":"code","source":"#Load into Keras Embedding layer\nfrom keras.layers.embeddings import Embedding\nembedding_layer_2 = Embedding(len(word_index_2) + 1,\n                            embed_dim,\n                            weights=[embedding_matrix_2],\n                            input_length=maxlen_2,\n                            trainable=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec6291fc908ffc7a1d2ca5fa44f76927ed6ab4a8"},"cell_type":"markdown","source":"## 4. Model\n\n### Preparing the model"},{"metadata":{"trusted":true,"_uuid":"4b4c5079f880073458c423198b2e18c9949e0934"},"cell_type":"code","source":"#Metric: F1 score: F1: wikipedia, umsetzung https://github.com/keras-team/keras/blob/53e541f7bf55de036f4f5641bd2947b96dd8c4c3/keras/metrics.py\nimport keras.backend as K #to use math functions like \"keras.backend.sum\"\ndef fmeasure (y_true, y_pred):\n    \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    \n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    \n    f1 = 5 * (precision*recall) / (4*precision+recall+K.epsilon())\n    \n    return f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a8ff7e802b8733e9272d578edfd7b1ca6693cbb"},"cell_type":"code","source":"from keras.models import Model #to build the Model\nfrom keras.layers import Dense, Input, Dropout, Activation, Bidirectional, CuDNNGRU\n\n# Third Model: Preprocessed Data, Improved Structure\nsequence_input_3 = Input(shape=(maxlen_2,), dtype='int32')\nembedded_sequences_3 = embedding_layer_2(sequence_input_3)\nX_3 = Bidirectional(CuDNNGRU(128, return_sequences=True))(embedded_sequences_3)\nX_3 = Dropout(0.5)(X_3)\nX_3 = Bidirectional(CuDNNGRU(128, return_sequences=False))(X_3)\nX_3 = Dropout(0.5)(X_3)\nX_3 = Dense(1)(X_3)\nX_3 = Activation('sigmoid')(X_3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59a2c0ed7abd2faaacdef110f01c69568d01f70e"},"cell_type":"markdown","source":"### Finalising the model"},{"metadata":{"trusted":true,"_uuid":"1990b2b733dabebd7ae965071d1c806496254645","scrolled":true},"cell_type":"code","source":"model_3 = Model(inputs = sequence_input_3, outputs=X_3)\nmodel_3.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"307bad6759bb09de0cfe9e61868f8e1acb66f155"},"cell_type":"code","source":"# Callbacks\nfrom keras.callbacks import EarlyStopping\n#Early Stopping -> Stop if the results do not improve anymore\nearly_stopping = EarlyStopping(monitor='val_fmeasure', min_delta=0.0001, patience=1, mode='max')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e6e828c362cb088ec9a7efa9c04f942bc619a81"},"cell_type":"code","source":"model_3.compile(loss= \"binary_crossentropy\", optimizer='adam', metrics= [fmeasure])\nhistory_3 = model_3.fit(X_train_seq_2, Y_train, validation_data=(X_validate_seq_2, Y_validate),\n          epochs= 8, batch_size = 512, shuffle = True, callbacks = [early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d42a8ff5f2f404fda2fa7b829e88c980ff8c963"},"cell_type":"markdown","source":"### Threshholding and Final Predicting"},{"metadata":{"trusted":true,"_uuid":"4f959480d09754f75185dbd0a80f424c7ced4486"},"cell_type":"code","source":"# Da nicht mehr im Keras framework, umgeschrieben für Numpy Arrays\ndef f_measure (y_true, y_pred):\n    \n    true_positives = np.sum(np.round(np.dot(y_true, y_pred)))\n    predicted_positives = np.sum(y_pred)\n    actual_positives = np.sum(y_true)\n    \n    precision = true_positives / (predicted_positives + 2e-7)\n    recall = true_positives / (actual_positives + 2e-7)\n    f1 = 5 * (precision*recall) / (4*precision+recall+ 2e-7)\n    \n    return f1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9379182a1e104dd4e022adcd14d52f744f4cc872"},"cell_type":"code","source":"Y_validate.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"770a10429299d24e1f8b186edc5c685dd25bb947"},"cell_type":"code","source":"prediction = model_3.predict([X_validate_seq_2], batch_size=1024, verbose=1)\n\nj = 0\nthresh_results = np.zeros(51)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    value = np.zeros(11)\n    for i in range(11):\n        value[i] = f_measure(Y_validate[i*5000:(i+1)*5000], (prediction[i*5000:(i+1)*5000]>thresh).astype(int))\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, np.mean(value)))\n    thresh_results[j] = np.mean(value)\n    j = j+1\n    \nbest_thresh = 0.10 + np.argmax(thresh_results)*0.01\nprint(\"Best threshhold :\", best_thresh)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80d0b168a7767f44ba5a099cef87ea135f6e79a3"},"cell_type":"code","source":"prediction_final = model_3.predict([X_test_seq_2], batch_size=1024, verbose=1)\nprediction_final = (prediction_final>best_thresh).astype(int).flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26990c8cad7c9197fa1e3551aeb797daf3661c0c"},"cell_type":"code","source":"prediction_final.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"273427e0131baa1cf1eb90f88192d0a7656237a8"},"cell_type":"markdown","source":"## 5. The Results"},{"metadata":{"_uuid":"ca3d6f43e3d9e07321e5deac43a2372837ecf1fd"},"cell_type":"markdown","source":"F1 on the test data is:"},{"metadata":{"trusted":true,"_uuid":"261fdb3783f14f240d43ed81f7e2f4a7da0323a3"},"cell_type":"code","source":"value = np.zeros(26)\nfor i in range(26):\n     value[i] = f_measure(Y_test[i*5000:(i+1)*5000], (prediction_final[i*5000:(i+1)*5000]))\nnp.mean(value)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c420921763fda1bad660503dff8839bf6abf16cc"},"cell_type":"markdown","source":"The Confusion Matrix looks as follows. The predictions are seen as rows and \"0\" means that it was predicted sincere, \"1\" that it was predicted insincere."},{"metadata":{"trusted":true,"_uuid":"3d01c0f1b10c2296aa66d6a33a3c8d4364113dc9"},"cell_type":"code","source":"m=pd.crosstab(prediction_final,Y_test, rownames = [\"prediction\"], colnames = [\"actual value\"])\nprint(\"Confusion matrix\")\nprint()\nprint(m)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a32ac4d8ffbe87460ce1cbb3d37d28ccaec497db"},"cell_type":"markdown","source":"In the following it will be shown which kind of labels the network got right and which kind of labels it got wrong"},{"metadata":{"trusted":true,"_uuid":"cf6ad6265c24f0f2e4b40b672f1970926e0506ff"},"cell_type":"code","source":"#building a dataframe in which for each test example there is question, actual class and predicted class\nresults = test\nresults[\"pred\"] = prediction_final\ntrue_pos = results.loc[(results['pred'] == 1) & (results['target'] == 1)]\ntrue_neg = results.loc[(results['pred'] == 0) & (results['target'] == 0)]\nfalse_pos = results.loc[(results['pred'] == 1) & (results['target'] == 0)]\nfalse_neg = results.loc[(results['pred'] == 0) & (results['target'] == 1)]\npd.options.display.max_colwidth = 300","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2490e8942661f1daae73810b3a39752d6482a661"},"cell_type":"markdown","source":"**True positives**"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f9e71e57eb8454510bba1df266193d11f07f1c43"},"cell_type":"code","source":"true_pos.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b11ab36837c0c1f02d5ea1c1b7b3cae7c1521c8"},"cell_type":"markdown","source":"**True negatives**"},{"metadata":{"trusted":true,"_uuid":"17bd528f6ac3604f9d4f157e8c5a6ee97dd4480a"},"cell_type":"code","source":"true_neg.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a41cd598749c2a7b69816bb1c059f12ad4caa7fd"},"cell_type":"markdown","source":"**False negatives**"},{"metadata":{"trusted":true,"_uuid":"a42e48f86cbc071c40915ee4768c892dc27e672c"},"cell_type":"code","source":"false_neg.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c48a7bd98f70bcece00688eaa628405821ec7fff"},"cell_type":"markdown","source":"**False positives**"},{"metadata":{"trusted":true,"_uuid":"15b1c36f8d0871c4a9f93d15269a0185d9619513"},"cell_type":"code","source":"false_pos.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}