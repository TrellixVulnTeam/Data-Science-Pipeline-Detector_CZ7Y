{"cells":[{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nfrom torchtext import data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Seeding\nSEED = 1234\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nstop_words = set(stopwords.words('english')) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT = data.Field(tokenize='spacy', batch_first=True, include_lengths=True, stop_words = stop_words)\nLABEL = data.LabelField(dtype = torch.float,batch_first=True)\n# https://pytorch.org/text/_modules/torchtext/data/field.html\n\nfields = [(None, None), ('text', TEXT), ('label', LABEL)]\n\ntraining_data = data.TabularDataset(path = '/kaggle/input/quora-insincere-questions-classification/train.csv', format = 'csv',fields = fields,skip_header = True)\n\nprint(vars(training_data.examples[0]))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ntrain_data, valid_data = training_data.split(split_ratio=0.7, random_state = random.seed(SEED))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#initialize glove embeddings\nTEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.100d\")  \nLABEL.build_vocab(train_data)\n\n#No. of unique tokens in text\nprint(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n\n#No. of unique tokens in label\nprint(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n\n#Commonly used words\nprint(TEXT.vocab.freqs.most_common(10))  \n\n#Word dictionary\nprint(TEXT.vocab.stoi)   \n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check whether cuda is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n\n#set batch size\nBATCH_SIZE = 64\n\n#Load an iterator\ntrain_iterator, valid_iterator = data.BucketIterator.splits(\n    (train_data, valid_data), \n    batch_size = BATCH_SIZE,\n    sort_key = lambda x: len(x.text),\n    sort_within_batch=True, # Required by torch.nn\n    device = device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass classifier(nn.Module):\n    \n    #define all the layers used in model\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers):\n        \n        #Constructor\n        super().__init__()        \n        \n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        #embedding layer\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n\n        # linear layer\n        self.rnn = nn.RNN(embedding_dim, \n                           hidden_dim, \n                           num_layers=num_layers, \n                           batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        \n        # activation function\n        self.act = nn.Sigmoid()\n\n\n\n\n    def forward(self, text, text_lengths):\n        \n        h0 = torch.zeros(self.num_layers, text.size(0), self.hidden_dim).requires_grad_()\n        h0 = h0.to(device)\n            \n        embedded = self.embedding(text)\n        # print('embedded size: ', embedded.size())\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True)\n        \n        packed_output, hidden = self.rnn(packed_embedded, h0.detach())\n        # print('hidden size: ', hidden.size())\n\n        dense_outputs = self.fc(hidden[-1,:,:])\n        out = self.act(dense_outputs)\n        return out\n    \n\n\n    \n   \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#define hyperparameters\nvocab_size= len(TEXT.vocab)\nembedding_dim = 100\nhidden_dim = 32  \noutput_dim = 1\nnum_layers = 1\n\n#instantiate the model\nmodel = classifier(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#architecture\nprint(model)\n\n#No. of trianable parameters\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \nprint(f'The model has {count_parameters(model):,} trainable parameters')\n\n#Initialize the pretrained embedding\npretrained_embeddings = TEXT.vocab.vectors\nmodel.embedding.weight.data.copy_(pretrained_embeddings)\n\nprint(pretrained_embeddings.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\n#define optimizer and loss\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.BCELoss()\n\n#define metric\ndef binary_accuracy(preds, y):\n    #round predictions to the closest integer\n    rounded_preds = torch.round(preds)\n    \n    correct = (rounded_preds == y).float() \n    acc = correct.sum() / len(correct)\n    return acc\n    \n#push to cuda if available\nmodel = model.to(device)\ncriterion = criterion.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, iterator, optimizer, criterion):\n    \n    #initialize every epoch \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    #set the model in training phase\n    model.train()  \n    \n    for batch in iterator:\n        #resets the gradients after every batch\n        optimizer.zero_grad()   \n        \n        #retrieve text and no. of words\n        text, text_lengths = batch.text   \n   \n        #convert to 1D tensor\n        predictions = model(text, text_lengths).squeeze()    \n        \n        #compute the loss\n        loss = criterion(predictions, batch.label)        \n        \n        #compute the binary accuracy\n        acc = binary_accuracy(predictions, batch.label)   \n        \n        #backpropage the loss and compute the gradients\n        loss.backward()       \n        \n        #update the weights\n        optimizer.step()      \n        \n        #loss and accuracy\n        epoch_loss += loss.item()  \n        epoch_acc += acc.item()    \n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    \n    #initialize every epoch\n    epoch_loss = 0\n    epoch_acc = 0\n\n    #deactivating dropout layers\n    model.eval()\n    \n    #deactivates autograd\n    with torch.no_grad():\n    \n        for batch in iterator:\n\n        \n            #retrieve text and no. of words\n            text, text_lengths = batch.text\n            \n            #convert to 1d tensor\n            predictions = model(text, text_lengths).squeeze()\n\n            #compute loss and accuracy\n            loss = criterion(predictions, batch.label)\n            acc = binary_accuracy(predictions, batch.label)\n            \n            #keep track of loss and accuracy\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_EPOCHS = 5\nbest_valid_loss = float('inf')\nimport time\n\n    \nfor epoch in range(N_EPOCHS):\n \n    start = time.time()\n    print('======= starting epoch ======= ', epoch)\n    \n    print('======= now tranining ======= ', epoch)\n    #train the model\n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    \n    print('======= now evaluating =======', epoch)\n    \n    #evaluate the model\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    \n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'saved_weights.pt')\n    \n    end = time.time()\n    \n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n    print(\"traninig time = {:.3f} \".format(end - start))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntest_data = data.TabularDataset(path = '/kaggle/input/quora-insincere-questions-classification/test.csv', format = 'csv',fields = fields,skip_header = True)\n\nprint(vars(test_data.examples[0])) \n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load weights\npath='/kaggle/working/saved_weights.pt' \nmodel.load_state_dict(torch.load(path));\nmodel.eval();\n\n#inference \nimport spacy\nnlp = spacy.load('en')\n\ndef predict(model, sentence):\n    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  #tokenize the sentence \n    indexed = [TEXT.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n    length = [len(indexed)]                                    #compute no. of words\n    tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n    tensor = tensor.unsqueeze(1).T                             #reshape in form of batch,no. of words\n    length_tensor = torch.LongTensor(length)                   #convert to tensor\n    prediction = model(tensor, length_tensor)                  #prediction \n    return prediction.item()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ntest_iterator = data.Iterator(\n    test_data,\n    batch_size=1)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\ndef test(model, iterator):\n    \n\n    #deactivating dropout layers\n    model.eval()\n    \n    num_correct = 0\n     \n    \n    #deactivates autograd\n    with torch.no_grad():\n    \n        for batch in iterator:\n\n            predicted = predict(model, batch.text)\n            \n            num_total += 1\n            if (predicted == batch.label):\n                num_correct += 1\n        \n    print(\"Test Accuracy = {:.3f}% \".format(num_correct/num_total))\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test(model,test_iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}