{"cells":[{"metadata":{},"cell_type":"markdown","source":"Hi, if you are a beginner in Tensorflow and would like to catch up the essence of Natural Language Processing (Like me), please upvote <3"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# importing the libraries\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the Deep Learning Libraries\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading the training data\ntraining_data = pd.read_csv('/kaggle/input/quora-insincere-questions-classification/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. We dont need the qid to train the model.\n2. The question_text is the text input that has to be fitted in the model along with the target\n3. The target has 2 classes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the qid\ntraining_data = training_data.drop(['qid'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a feature length that contains the total length of the question\ntraining_data['length'] = training_data['question_text'].apply(lambda s: len(s))\n# I used a basic way of utilizing a lambda function.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data['length']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now checking the mean length of the text for tokenizing the data.\nmin(training_data['length']), max(training_data['length']), round(sum(training_data['length'])/len(training_data['length']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"minimum length = 1 ?? looks like outliers, How can a question contain just a single word ? Let us do some preprocessing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data[training_data['length'] <= 9]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"oops...**Are these even complete questions ???**"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data = training_data.drop(training_data[training_data['length'] <= 9].index, axis = 0)\nmin(training_data['length']), max(training_data['length']), round(sum(training_data['length'])/len(training_data['length']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks sensible. Now lets check for missing values (if any)"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No Missing Values !\n\nLet us start the Deep Learning part now !"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tokenizing the text - Converting each word, even letters into numbers. \nmax_length = round(sum(training_data['length'])/len(training_data['length']))\ntokenizer = Tokenizer(num_words = max_length, \n                      filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n                     lower = True,\n                     split = ' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(training_data['question_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Actual Conversion takes place here.\nX = tokenizer.texts_to_sequences(training_data['question_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X), len(X[0]), len(X[1]), len(X[2]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see the lengths are not same. So Pad sequences are used. Pad sequences adds a specific value, usually 0, before or after the text sequence to make them equal in length"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pad_sequences(sequences = X, padding = 'pre', maxlen = max_length)\nprint(len(X), len(X[0]), len(X[1]), len(X[2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = training_data['target'].values\ny.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now the data is ready to be fed into the neural network. Now constructing the neural network NLP. \n\nI will create a neural network with minimum layers so that beginners like me can understand without complexity."},{"metadata":{"trusted":true},"cell_type":"code","source":"# LSTM Neural Network\nlstm = Sequential()\nlstm.add(Embedding(input_dim = max_length, output_dim = 120))\nlstm.add(LSTM(units = 120, recurrent_dropout = 0.2))\nlstm.add(Dropout(rate = 0.2))\nlstm.add(Dense(units = 120, activation = 'relu'))\nlstm.add(Dropout(rate = 0.1))\nlstm.add(Dense(units = 2, activation = 'softmax'))\n\nlstm.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_fitted = lstm.fit(X, y, epochs = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note: you can play with the hyperparameters to get the expected accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# importing the testing data\ntesting_data = pd.read_csv('/kaggle/input/quora-insincere-questions-classification/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting the data into tokens\nX_test = tokenizer.texts_to_sequences(testing_data['question_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(X_test), len(X_test[0]), len(X_test[1]), len(X_test[2]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# paddding the sequences\nX_test = pad_sequences(X_test, maxlen = max_length, padding = 'pre')\nprint(len(X_test), len(X_test[0]), len(X_test[1]), len(X_test[2]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are good to go !!"},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting the test set\nlstm_prediction = lstm.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creating a dataframe for submitting\nsubmission = pd.DataFrame(({'qid':testing_data['qid'], 'prediction':lstm_prediction}))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Thank you for viewing my kernel. Please comment if you have any creative ideas of doing traditional methods.\n\n*Let's Learn ! Let's Learn !* "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}