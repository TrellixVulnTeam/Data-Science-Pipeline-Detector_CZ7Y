{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Contents\n\n- **[Load Library](#Load-Library)**\n- **[Load Data](#Load-Data)**\n- **[EDA](#EDA)**\n- **[Feature Extraction](#Feature-Extraction)**\n- **[Modeling](#Modeling)**\n- **[Submission](#Submission)**","metadata":{}},{"cell_type":"markdown","source":"# Load Library","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option(\"display.max_colwidth\", 80)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\nfrom wordcloud import STOPWORDS\nfrom collections import defaultdict\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/quora-insincere-questions-classification/train.csv\")\nprint('Train Set Shape = {}'.format(train.shape))\ntrain.head(3)","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/quora-insincere-questions-classification/test.csv\")\nprint('Test Set Shape = {}'.format(test.shape))\ntest.head(3)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\")\nsubmission.head(3)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Target Distribution","metadata":{}},{"cell_type":"code","source":"print(f'the number of insincere questions is : {len(train[train[\"target\"]==1])} / {len(train)}')\nprint(f'the number of not-insincere questions is : {len(train[train[\"target\"]==0])} / {len(train)}')\ntrain_counts = train[\"target\"].value_counts()\nhv.Bars((train_counts.keys(), train_counts.values),\"Target Label\",\"Counts\").opts(width=600,height=400,title=\"Target Counts\",tools=['hover'])","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Null Ratio","metadata":{}},{"cell_type":"code","source":"print(f'the number of nulls in train set : {train.isnull().any().sum()}')\nprint(f'the number of nulls in test set : {test.isnull().any().sum()}')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## N-gram Frequencies","metadata":{}},{"cell_type":"code","source":"#ngram function\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_0 = train[train[\"target\"]==0]\ntrain_1 = train[train[\"target\"]==1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unigram","metadata":{}},{"cell_type":"code","source":"freq_dict_0_uni = defaultdict(int)\nfor sent in train_0[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict_0_uni[word] += 1\n\nfreq_dict_1_uni = defaultdict(int)\nfor sent in train_1[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict_1_uni[word] += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_0_uni = list(sorted(freq_dict_0_uni.items(), key=lambda x: x[1],reverse=True))\nbars_0_uni = hv.Bars(data_0_uni[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"unigram frequency with target=0\", color=\"red\")\ndata_1_uni = list(sorted(freq_dict_1_uni.items(), key=lambda x: x[1],reverse=True))\nbars_1_uni = hv.Bars(data_1_uni[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"unigram frequency with target=1\", color=\"blue\")\n\n(bars_0_uni + bars_1_uni).opts(opts.Bars(tools=['hover']))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bigram","metadata":{}},{"cell_type":"code","source":"freq_dict_0_bi = defaultdict(int)\nfor sent in train_0[\"question_text\"]:\n    for word in generate_ngrams(sent,n_gram=2):\n        freq_dict_0_bi[word] += 1\n\nfreq_dict_1_bi = defaultdict(int)\nfor sent in train_1[\"question_text\"]:\n    for word in generate_ngrams(sent,n_gram=2):\n        freq_dict_1_bi[word] += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_0_bi = list(sorted(freq_dict_0_bi.items(), key=lambda x: x[1],reverse=True))\nbars_0_bi = hv.Bars(data_0_bi[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"bigram frequency with target=0\", color=\"red\")\ndata_1_bi = list(sorted(freq_dict_1_bi.items(), key=lambda x: x[1],reverse=True))\nbars_1_bi = hv.Bars(data_1_bi[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"bigram frequency with target=1\", color=\"blue\")\n\n(bars_0_bi + bars_1_bi).opts(opts.Bars(tools=['hover']))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Trigram","metadata":{}},{"cell_type":"code","source":"freq_dict_0_tri = defaultdict(int)\nfor sent in train_0[\"question_text\"]:\n    for word in generate_ngrams(sent,n_gram=3):\n        freq_dict_0_tri[word] += 1\n\nfreq_dict_1_tri = defaultdict(int)\nfor sent in train_1[\"question_text\"]:\n    for word in generate_ngrams(sent,n_gram=3):\n        freq_dict_1_tri[word] += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_0_tri = list(sorted(freq_dict_0_tri.items(), key=lambda x: x[1],reverse=True))\nbars_0_tri = hv.Bars(data_0_tri[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"trigram frequency with target=0\", color=\"red\")\ndata_1_tri = list(sorted(freq_dict_1_tri.items(), key=lambda x: x[1],reverse=True))\nbars_1_tri = hv.Bars(data_1_tri[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"trigram frequency with target=1\", color=\"blue\")\n\n(bars_0_tri + bars_1_tri).opts(opts.Bars(tools=['hover']))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"## TF-IDF","metadata":{}},{"cell_type":"code","source":"def tfidf_features(text, _max_features=10, _max_ngrams=2):\n    tfidf = TfidfVectorizer(max_features=_max_features, use_idf=True, ngram_range=(1,_max_ngrams))\n    vec = tfidf.fit_transform(text).toarray()\n    tfidf_df = pd.DataFrame(vec, columns=[\"TFIDF_\" + n for n in tfidf.get_feature_names()])\n    return tfidf_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_train = tfidf_features(train[\"question_text\"])\ntrain = pd.concat([train, tfidf_train], axis=1)\ntfidf_test = tfidf_features(test[\"question_text\"])\ntest = pd.concat([test, tfidf_test], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## N-gram Feature","metadata":{}},{"cell_type":"code","source":"uni_0_words = [word_freq[0] for word_freq in data_0_uni[0:50]]\nuni_1_words = [word_freq[0] for word_freq in data_1_uni[0:50]]\nbi_0_words = [word_freq[0] for word_freq in data_0_bi[0:50]]\nbi_1_words = [word_freq[0] for word_freq in data_1_bi[0:50]]\ntri_0_words = [word_freq[0] for word_freq in data_0_tri[0:50]]\ntri_1_words = [word_freq[0] for word_freq in data_1_tri[0:50]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in [train,test]:\n    uni_0_feature = []\n    uni_1_feature = []\n    bi_0_feature = []\n    bi_1_feature = []\n    tri_0_feature = []\n    tri_1_feature = []\n    for line in df[\"question_text\"]:\n        uni_0_len = len([word for word in uni_0_words if word in line])\n        uni_1_len = len([word for word in uni_1_words if word in line])\n        bi_0_len = len([word for word in bi_0_words if word in line])\n        bi_1_len = len([word for word in bi_1_words if word in line])\n        tri_0_len = len([word for word in tri_0_words if word in line])\n        tri_1_len = len([word for word in tri_1_words if word in line])\n        \n        uni_0_feature.append(uni_0_len)\n        uni_1_feature.append(uni_1_len)\n        bi_0_feature.append(bi_0_len)\n        bi_1_feature.append(bi_1_len)\n        tri_0_feature.append(tri_0_len)\n        tri_1_feature.append(tri_1_len)\n    df[\"uni_0\"] = uni_0_feature\n    df[\"uni_1\"] = uni_1_feature\n    df[\"bi_0\"] = bi_0_feature\n    df[\"bi_1\"] = bi_1_feature\n    df[\"tri_0\"] = tri_0_feature\n    df[\"tri_1\"] = tri_1_feature\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(3)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Meta Feature","metadata":{}},{"cell_type":"code","source":"for df in [train,test]:\n    df[\"sent_len\"] = df[\"question_text\"].apply(lambda x: len(x.split()))\n    df[\"word_mean_len\"] = df[\"question_text\"].apply(lambda x: np.mean([len(i) for i in x.split()]))\n    df[\"punc_num\"] = df[\"question_text\"].apply(lambda x: len([c for c in x.split() if c in string.punctuation]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(3)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"# Submission","metadata":{}}]}