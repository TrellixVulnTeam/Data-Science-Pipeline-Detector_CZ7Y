{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#A lot of the preprocessing is based on Dieter's and Theio Vial's work.\n#https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\n#https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from wordcloud import WordCloud, STOPWORDS\nimport pyprind \nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\nimport gensim\nfrom gensim.models import KeyedVectors\nimport operator\nimport string\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/quora-insincere-questions-classification/train.csv', encoding='utf-8')#.drop('target',axis=1)\ntest = pd.read_csv('../input/quora-insincere-questions-classification/test.csv', encoding='utf-8')\ndf = pd.concat([train,test],axis=0)\nprint(f'''Train Shape: {train.shape}\nTest Shape: {test.shape}\nDf Shape:{df.shape}''')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.sample(frac=1)\nprint(f'Train Shape:{train.shape}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vocab(sentences, verbose=True):\n    sentences = sentences.apply(lambda x: x.split()).values\n    vocab={}    \n    for sentence in pyprind.prog_bar(sentences):\n        for word in sentence:\n            try:\n                vocab[word]+=1\n            except KeyError:\n                vocab[word]=1\n    return vocab          ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embed(file):\n    def get_coefs(word,*arr): \n        #print(word,arr)\n        return word, np.asarray(arr, dtype='float16')\n    \n    if file == '../input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='utf8') if len(o)>100)\n    elif file == '../input/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin':\n        embeddings_index = KeyedVectors.load_word2vec_format(file, binary=True)\n    elif file == '../input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin') if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n    return embeddings_index\nglove = '../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'\nparagram =  '../input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n#wiki_news = '../input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Extracting GloVe embedding\")\n#embed_glove = load_embed(glove)\nprint(\"Extracting Paragram embedding\")\nembed_paragram = load_embed(paragram)\n#print(\"Extracting FastText embedding\")\n#embed_fasttext = load_embed(wiki_news)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def check_coverage(vocab,embeddings_index):\n    known_words={}\n    unknown_words={}\n    nb_known_words=0\n    nb_unknown_words=0\n    for word in vocab.keys():\n        if word in embeddings_index.keys():\n            known_words[word]=embeddings_index[word]\n            nb_known_words += vocab[word]\n        elif word.capitalize() in embeddings_index.keys():\n            known_words[word] = embeddings_index[word.capitalize()]\n            nb_known_words += vocab[word]\n        elif word.lower() in embeddings_index.keys():\n            known_words[word] = embeddings_index[word.lower()]\n            nb_known_words += vocab[word]\n        elif word.upper() in embeddings_index.keys():\n            known_words[word] = embeddings_index[word.upper()]\n            nb_known_words += vocab[word]\n        else:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n    print(f'Found embeddings for {round((len(known_words)/len(vocab))*100,5)}% of the vocab\\nFound embeddings for {round((nb_known_words/(nb_known_words+nb_unknown_words))*100,5)}% of all text')\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1),reverse=True)#[::-1]\n    return unknown_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab = build_vocab(df['question_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"Glove : \")\n# oov_glove = check_coverage(vocab, embed_glove)\n# print(\"Paragram : \")\n# oov_paragram = check_coverage(vocab, embed_paragram)\n# #print(\"FastText : \")\n# #oov_fasttext = check_coverage(vocab, embed_fasttext)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\n    \"Trump's\" : 'trump is',\"'cause\": 'because','â€™': \"'\",',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n    'ain;t': 'am not','ain´t': 'am not','ain’t': 'am not',\"aren't\": 'are not','â€“': '-','â€œ':'\"',\n    'aren,t': 'are not','aren;t': 'are not','aren´t': 'are not','aren’t': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n    'can;t': 'cannot','can;t;ve': 'cannot have',\n    'can´t': 'cannot','can´t´ve': 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\n    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n    'couldn;t;ve': 'could not have','couldn´t': 'could not',\n    'couldn´t´ve': 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could´ve': 'could have',\n    'could’ve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn´t': 'did not',\n    'didn’t': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn´t': 'does not',\n    'doesn’t': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don´t': 'do not','don’t': 'do not',\n    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n    'hadn;t;ve': 'had not have','hadn´t': 'had not','hadn´t´ve': 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn´t': 'has not','hasn’t': 'has not',\n    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven´t': 'have not','haven’t': 'have not',\"he'd\": 'he would',\n    \"he'd've\": 'he would have',\"he'll\": 'he will',\n    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he´d': 'he would','he´d´ve': 'he would have','he´ll': 'he will',\n    'he´s': 'he is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’s': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n    'how;s': 'how is','how´d': 'how did','how´ll': 'how will','how´s': 'how is','how’d': 'how did','how’ll': 'how will',\n    'how’s': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n    'isn,t': 'is not','isn;t': 'is not','isn´t': 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it´d': 'it would','it´ll': 'it will','it´s': 'it is',\n    'it’d': 'it would','it’ll': 'it will','it’s': 'it is',\n    'i´d': 'i would','i´ll': 'i will','i´m': 'i am','i´ve': 'i have','i’d': 'i would','i’ll': 'i will','i’m': 'i am',\n    'i’ve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let´s': 'let us',\n    'let’s': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n    'mayn´t': 'may not','mayn’t': 'may not','ma´am': 'madam','ma’am': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn´t': 'might not',\n    'mightn’t': 'might not','might´ve': 'might have','might’ve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn´t': 'must not','mustn’t': 'must not','must´ve': 'must have',\n    'must’ve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn´t': 'need not','needn’t': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n    'oughtn´t': 'ought not','oughtn’t': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n    'shan,t': 'shall not','shan;t': 'shall not','shan´t': 'shall not','shan’t': 'shall not','sha´n´t': 'shall not','sha’n’t': 'shall not',\n    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she´d': 'she would','she´ll': 'she will',\n    'she´s': 'she is','she’d': 'she would','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn´t': 'should not','shouldn’t': 'should not','should´ve': 'should have',\n    'should’ve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n    'that;s': 'that is','that´d': 'that would','that´s': 'that is','that’d': 'that would','that’s': 'that is',\"there'd\": 'there had',\n    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n    'there´d': 'there had','there´s': 'there is','there’d': 'there had','there’s': 'there is',\n    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n    'they;ve': 'they have','they´d': 'they would','they´ll': 'they will','they´re': 'they are','they´ve': 'they have','they’d': 'they would','they’ll': 'they will',\n    'they’re': 'they are','they’ve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn´t': 'was not',\n    'wasn’t': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren´t': 'were not','weren’t': 'were not','we´d': 'we would','we´ll': 'we will',\n    'we´re': 'we are','we´ve': 'we have','we’d': 'we would','we’ll': 'we will','we’re': 'we are','we’ve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n    'what;s': 'what is','what;ve': 'what have','what´ll': 'what will',\n    'what´re': 'what are','what´s': 'what is','what´ve': 'what have','what’ll': 'what will','what’re': 'what are','what’s': 'what is',\n    'what’ve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n    'where;s': 'where is','where´d': 'where did','where´s': 'where is','where’d': 'where did','where’s': 'where is',\n    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n    'who´ll': 'who will','who´s': 'who is','who’ll': 'who will','who’s': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n    'won´t': 'will not','won’t': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn´t': 'would not',\n    'wouldn’t': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n    'you;re': 'you are','you´d': 'you would','you´ll': 'you will','you´re': 'you are','you’d': 'you would','you’ll': 'you will','you’re': 'you are',\n    '´cause': 'because','’cause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n    \"havn't\": 'have not',\"here’s\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you’ve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‘I\":'I','ᴀɴᴅ':'and','ᴛʜᴇ':'the','ʜᴏᴍᴇ':'home','ᴜᴘ':'up','ʙʏ':'by','ᴀᴛ':'at','…and':'and','civilbeat':'civil beat','TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','ᴄʜᴇᴄᴋ':'check','ғᴏʀ':'for','ᴛʜɪs':'this','ᴄᴏᴍᴘᴜᴛᴇʀ':'computer','ᴍᴏɴᴛʜ':'month','ᴡᴏʀᴋɪɴɢ':'working','ᴊᴏʙ':'job','ғʀᴏᴍ':'from','Sᴛᴀʀᴛ':'start','gubmit':'submit','CO₂':'carbon dioxide','ғɪʀsᴛ':'first','ᴇɴᴅ':'end','ᴄᴀɴ':'can','ʜᴀᴠᴇ':'have','ᴛᴏ':'to','ʟɪɴᴋ':'link','ᴏғ':'of','ʜᴏᴜʀʟʏ':'hourly','ᴡᴇᴇᴋ':'week','ᴇɴᴅ':'end','ᴇxᴛʀᴀ':'extra','Gʀᴇᴀᴛ':'great','sᴛᴜᴅᴇɴᴛs':'student','sᴛᴀʏ':'stay','ᴍᴏᴍs':'mother','ᴏʀ':'or','ᴀɴʏᴏɴᴇ':'anyone','ɴᴇᴇᴅɪɴɢ':'needing','ᴀɴ':'an','ɪɴᴄᴏᴍᴇ':'income',\n    'ʀᴇʟɪᴀʙʟᴇ':'reliable','ғɪʀsᴛ':'first','ʏᴏᴜʀ':'your','sɪɢɴɪɴɢ':'signing','ʙᴏᴛᴛᴏᴍ':'bottom','ғᴏʟʟᴏᴡɪɴɢ':'following','Mᴀᴋᴇ':'make',\n    'ᴄᴏɴɴᴇᴄᴛɪᴏɴ':'connection','ɪɴᴛᴇʀɴᴇᴛ':'internet','financialpost':'financial post', 'ʜaᴠᴇ':' have ', 'ᴄaɴ':' can ', 'Maᴋᴇ':' make ', 'ʀᴇʟɪaʙʟᴇ':' reliable ', 'ɴᴇᴇᴅ':' need ',\n    'ᴏɴʟʏ':' only ', 'ᴇxᴛʀa':' extra ', 'aɴ':' an ', 'aɴʏᴏɴᴇ':' anyone ', 'sᴛaʏ':' stay ', 'Sᴛaʀᴛ':' start', 'SHOPO':'shop',\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar = pyprind.ProgBar(df.shape[0], bar_char='█')\ndef clean_contractions(text, mapping):\n    #text_val = text.lower()\n    #map_val = \n    for word in mapping.keys():\n        word_val = word.lower()\n        if word in text:\n            text = text.replace(word, mapping[word])\n        elif word_val in text:\n            text = text.replace(word_val, mapping[word])\n#         elif word.lower() in text:\n#             text = text.replace(word, mapping[word])\n#         elif word.capitalize() in text:\n#             text = text.replace(word, mapping[word])\n#         elif word.upper() in text:\n#             text = text.replace(word, mapping[word])\n\n    bar.update()\n    return text\n#df['lowered_question'] = df['question_text'].apply(lambda x: x.lower())\n#df['treated_question'] = df['lowered_question'].apply(lambda x: clean_contractions(x, contraction_mapping))\n#train['lowered_question'] = train['question_text'].apply(lambda x: x.lower())\n#test['lowered_question'] = test['question_text'].apply(lambda x: x.lower())\n#train['treated_question'] = train['lowered_question'].apply(lambda x: clean_contractions(x, contraction_mapping))\n#test['treated_question'] = test['lowered_question'].apply(lambda x: clean_contractions(x, contraction_mapping))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_2 = df.copy()\n#df = df_2.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab = build_vocab(df['treated_question'])\n# print(\"Glove : \")\n# oov_glove = check_coverage(vocab, embed_glove)\n# print(\"Paragram : \")\n# oov_paragram = check_coverage(vocab, embed_paragram)\n# #print(\"FastText : \")\n# #oov_fasttext = check_coverage(vocab, embed_fasttext)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extra_punct = [\n#     ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n#     '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n#     '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',\n#     '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', '“', '★', '”',\n#     '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾',\n#     '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼', '⊕', '▼',\n#     '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n#     'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»',\n#     '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø',\n#     '¹', '≤', '‡', '√', '«', '»', '´', 'º', '¾', '¡', '§', '£', '₤']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n#punct = list(set(list(punct)+extra_punct+list(string.punctuation)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unknown_punct(embed, punct):\n    unknown_punct=''\n    for val in punct:\n        if val not in embed:\n            unknown_punct += val+' '\n    return unknown_punct\n#print(f'Glove:\\n{unknown_punct(embed_glove, punct)}')\nprint(f'Paragram:\\n{unknown_punct(embed_paragram, punct)}')\n#print(f'FastText:\\n{unknown_punct(embed_fasttext, punct)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar = pyprind.ProgBar(df.shape[0], bar_char='█')\ndef clean_puncts(text, mapping, punct):\n    for p in punct_mapping.keys():\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n    for p in specials.keys():\n        text = text.replace(p, specials[p])\n    bar.update()    \n    return text\n#df['treated_question'] = df['treated_question'].apply(lambda x: clean_puncts(x, punct_mapping, punct))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab = build_vocab(df['treated_question'])\n# print(\"Glove : \")\n# oov_glove = check_coverage(vocab, embed_glove)\n# print(\"Paragram : \")\n# oov_paragram = check_coverage(vocab, embed_paragram)\n# #print(\"FastText : \")\n# #oov_fasttext = check_coverage(vocab, embed_fasttext)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oov_paragram[:100]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'lnmiit':'limit', 'quorans': 'users of quora', 'mhcet':'competitive examination', 'swachh':'clean', 'chapterwise':'chapter wise', 'bitconnect':'cryptocurrency', 'bittrex':'cryptocurrency exchange','koinex':'cryptocurrency exchange', 'zebpay':'cryptocurrency exchange', 'binance':'cryptocurrency exchange', 'coinbase':'cryptocurrency exchange', 'brexit':'exit of UK from European Union', 'cryptocurrencies':'multiple cryptocurrency','redmi':'mobile phone', 'oneplus':'mobile phone company', 'pokémon':'anime', 'boruto':'anime', 'bhakts':'loyal followers', 'litecoin':'cryptocurreny','qoura':'quora','altcoin':'cryptocurreny','blockchains':'blockchain', 'airpods':'wireless earphones','zenfone':'mobile phone','altcoins':'multiple cryptocurrency','electroneum':'cryptocurreny','cryptocurreny':'blockchain based currency','iitians':'students','iitian':'student','gdpr':'data protection law'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bar = pyprind.ProgBar(df.shape[0], bar_char='█')\ndef correct_spell(text, dic):\n    for word in dic.keys():\n        text = text.replace(word, dic[word])\n    bar.update()\n    return text\n#df['treated_question'] = df['treated_question'].apply(lambda x: correct_spell(x, mispell_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# vocab = build_vocab(df['treated_question'])\n# print(\"Glove : \")\n# oov_glove = check_coverage(vocab, embed_glove)\n# print(\"Paragram : \")\n# oov_paragram = check_coverage(vocab, embed_paragram)\n# #print(\"FastText : \")\n# #oov_fasttext = check_coverage(vocab, embed_fasttext)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del embed_glove\n# del oov_glove\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['treated_question'] = train['question_text'].apply(lambda x: x.lower())\ntest['treated_question'] = test['question_text'].apply(lambda x: x.lower())\ntrain['treated_question'] = train['treated_question'].apply(lambda x: clean_contractions(x, contraction_mapping))\ntest['treated_question'] = test['treated_question'].apply(lambda x: clean_contractions(x, contraction_mapping))\ntrain['treated_question'] = train['treated_question'].apply(lambda x: clean_puncts(x, punct_mapping, punct))\ntest['treated_question'] = test['treated_question'].apply(lambda x: clean_puncts(x, punct_mapping, punct))\ntrain['treated_question'] = train['treated_question'].apply(lambda x: correct_spell(x, mispell_dict))\ntest['treated_question'] = test['treated_question'].apply(lambda x: correct_spell(x, mispell_dict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model, Sequential\nfrom keras.layers import Flatten,Dense, Embedding,  Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, Input, Dropout,Bidirectional, CuDNNGRU, GlobalAveragePooling1D, GlobalMaxPooling1D, LeakyReLU, Activation, CuDNNLSTM, SpatialDropout1D\n#Dense, Embedding, Bidirectional, CuDNNGRU, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, Input, Dropout\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"length_to_keep = 100000\n#train['treated_question'].map(len).mean()\nmax_len = 75","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def tokenize_data(text):\n#     t = Tokenizer(num_words=length_to_keep)\n#     t.fit_on_texts(text)\n#     text = t.texts_to_sequences(text)\n#     text = pad_sequences(text, maxlen=max_len)\n#     return text, t.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X, word_index = tokenize_data(train['question_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_treated_data(text, text_test):\n    t = Tokenizer(num_words=length_to_keep, filters='')\n    t.fit_on_texts(text)\n    text = t.texts_to_sequences(text)\n    text_test = t.texts_to_sequences(text_test)\n    text = pad_sequences(text, maxlen=max_len)\n    text_test = pad_sequences(text_test, maxlen=max_len)\n    return text, t.word_index, text_test\nX_treated, word_index_treated, X_test_treated = tokenize_treated_data(train['treated_question'], test['treated_question'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# , _ = tokenize_treated_data(test['treated_question'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_len_treated = len(word_index_treated)+1\ndef embed_matrix(embed_paragram, word_index, length_to_keep):\n    embeddings = np.stack(embed_paragram.values())\n    \n#    embeddings_mean, embeddings_std = embeddings.mean(), embeddings.std(ddof=1)\n#    print(embeddings_mean)\n     \n    embeddings_shape = embeddings.shape[1]\n    embedding_matrix = np.zeros((length_to_keep, 300))\n    \n    for word, i in word_index.items():\n        if i >= length_to_keep:\n            continue\n        embeddings_vector = embed_paragram.get(word)\n        if embeddings_vector is not None:\n            embedding_matrix[i] = embeddings_vector\n    return embedding_matrix\n\n\n#embedding = embed_matrix(embed_paragram, word_index, length_to_keep)\nembedding_treated = embed_matrix(embed_paragram, word_index_treated, length_to_keep)\n        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_treated.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#del embed_paragram\n\n#del embed_fasttext\n#del word_index\ndel word_index_treated\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# {k: embed_glove[k] for k in list(embed_glove)[:5]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_target = pd.read_csv('../input/quora-insincere-questions-classification/train.csv', encoding='utf-8')\ny = train['target'].values\ntrain.drop(columns='target',inplace=True)\n# del train_target\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train, X_chk, y_train, y_chk = train_test_split(X, y, test_size=0.1, random_state=0)\nX_t_train, X_t_chk, y_train_treated, y_chk_treated = train_test_split(X_treated, y, test_size=0.1, random_state=0)\nprint(f'Training on {X_t_train.shape[0]} texts')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def f1(y_true, y_pred):\n#     precision,recall,fscore,support = precision_recall_fscore_support(y_true,y_pred,average='macro')\n#     print (f'Precision : {precision}')\n#     print (f'Recall : {recall}')\n#     print (f'Fscore : {fscore}')\n#     print (f'Precision : {precision}')\n#     return fscore","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    \n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def make_model1(embedding_matrix, embed_size=300, loss='binary_crossentropy'):\n#     model = Sequential()\n#     model.add(Embedding(length_to_keep, embed_size, input_length=max_len,weights=[embedding_matrix], trainable=False))\n#     model.add(Conv1D(filters=2, kernel_size=4, activation='relu'))\n#     model.add(MaxPooling1D(pool_size=2))\n#     model.add(Flatten())\n#     model.add(Dense(10, activation='relu'))\n#     model.add(Dense(1, activation='sigmoid'))\n#     model.compile(loss=loss, optimizer=Adam(lr=0.1), metrics=['accuracy',f1])\n#     return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model1 = make_model1(embedding_treated)\n# model1.summary()\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def make_model(embedding_matrix, embed_size=300, loss='binary_crossentropy'):\n#     inp = Input(shape=(max_len,))\n#     filter_size = [2,3,4,5]\n#     num_kernels=600\n#     embed   = Embedding(length_to_keep, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n#     x       = embed\n#     x1      = Conv1D(kernel_size=filter_size[0], strides=1, filters=num_kernels, padding='valid', activation='relu', use_bias=True)(x)\n# #    line1   = LeakyReLU()(x1)\n#     x2      = Conv1D(kernel_size=filter_size[1], strides=1, filters=num_kernels, padding='valid', activation='relu', use_bias=True)(x)\n# #    line2   = LeakyReLU()(x2)\n#     x3      = Conv1D(kernel_size=filter_size[2], strides=1, filters=num_kernels, padding='valid', activation='relu', use_bias=True)(x)\n# #     x4      = Conv1D(kernel_size=filter_size[3], strides=1, filters=num_kernels, padding='valid', activation='relu', use_bias=True)(x)\n#     x1_max  = MaxPooling1D(pool_size=(2), strides=2, padding='valid')(x1)\n#     #x1_flat = Flatten()(x1_max)\n#     x2_max  = MaxPooling1D(pool_size=(2), strides=2, padding='valid')(x2)\n#     #x2_flat = Flatten()(x2_max)\n#     x3_max  = MaxPooling1D(pool_size=(2), strides=2, padding='valid')(x3)\n# #     #x3_flat = Flatten()(x3_max)\n# #     x4_max  = MaxPooling1D(pool_size=(max_len-filter_size[3]+1), strides=1, padding='valid')(x4)\n#     concat  = Concatenate(axis=1)([x1_max, x2_max, x3_max])#, x4_max])\n#     flat    = Flatten()(concat)\n#     drop    = Dropout(0.3)(flat)\n#     dense1  = Dense(128, activation=\"relu\")(drop)\n#     output  = Dense(1, activation=\"sigmoid\")(dense1)\n#     model   = Model(inputs=inp, outputs=output)\n#     model.compile(loss=loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\n#     return model\n# #model = make_model(embedding)\n# #model_treated = make_model(embedding_treated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def make_model(embedding_matrix, embed_size=300, loss='binary_crossentropy'):\n#     inp = Input(shape=(max_len,))\n#     filter_size = [2,3,4,5]\n#     num_kernels=64\n#     embed   = Embedding(length_to_keep, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n#     x       = embed\n#     x1      = Conv1D(kernel_size=filter_size[0], strides=1, filters=num_kernels, padding='valid', activation='relu', use_bias=True)(x)\n# #    line1   = LeakyReLU()(x1)\n#     x2      = Conv1D(kernel_size=filter_size[1], strides=1, filters=num_kernels, padding='valid', activation='relu', use_bias=True)(x)\n# #    line2   = LeakyReLU()(x2)\n#     x3      = Conv1D(kernel_size=filter_size[2], strides=1, filters=num_kernels, padding='valid', activation='relu', use_bias=True)(x)\n# #     x4      = Conv1D(kernel_size=filter_size[3], strides=1, filters=num_kernels, padding='valid', activation='relu', use_bias=True)(x)\n#     x1_max  = MaxPooling1D(pool_size=(max_len-filter_size[0]+1), strides=1, padding='valid')(x1)\n#     #x1_flat = Flatten()(x1_max)\n#     x2_max  = MaxPooling1D(pool_size=(max_len-filter_size[1]+1), strides=1, padding='valid')(x2)\n#     #x2_flat = Flatten()(x2_max)\n#     x3_max  = MaxPooling1D(pool_size=(max_len-filter_size[2]+1), strides=1, padding='valid')(x3)\n# #     #x3_flat = Flatten()(x3_max)\n# #     x4_max  = MaxPooling1D(pool_size=(max_len-filter_size[3]+1), strides=1, padding='valid')(x4)\n#     concat  = Concatenate(axis=1)([x1_max, x2_max, x3_max])#, x4_max])\n#     flat    = Flatten()(concat)\n#     drop    = Dropout(0.3)(flat)\n#     dense1  = Dense(64, activation=\"relu\")(drop)\n#     output  = Dense(1, activation=\"sigmoid\")(dense1)\n#     model   = Model(inputs=inp, outputs=output)\n#     model.compile(loss=loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\n#     return model\n# #model = make_model(embedding)\n# #model_treated = make_model(embedding_treated)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def make_model(embedding_matrix, embed_size=300, loss='binary_crossentropy'):\n#     inp    = Input(shape=(max_len,))\n#     x      = Embedding(length_to_keep, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n#     x      = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n#     x      = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n#     avg_pl = GlobalAveragePooling1D()(x)\n#     max_pl = GlobalMaxPooling1D()(x)\n#     concat = concatenate([avg_pl, max_pl])\n#     dense  = Dense(64, activation=\"relu\")(concat)\n#     drop   = Dropout(0.1)(concat)\n#     output = Dense(1, activation=\"sigmoid\")(concat)\n    \n#     model  = Model(inputs=inp, outputs=output)\n#     model.compile(loss=loss, optimizer=Adam(lr=0.0001), metrics=['accuracy', f1])\n#     return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_model(embedding_matrix, embed_size=300, loss='binary_crossentropy'):\n    inp = Input(shape=(max_len,))\n    embed= Embedding(length_to_keep, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    spatial_drop = SpatialDropout1D(0.3)(embed)\n    lstm1=(Bidirectional(CuDNNLSTM(256,return_sequences=True)))(spatial_drop)\n    lstm2=(Bidirectional(CuDNNGRU(256,return_sequences=True)))(lstm1)\n    maxpool1 = GlobalMaxPooling1D()(lstm1)\n    maxpool2 = GlobalMaxPooling1D()(lstm2)\n    conc = Concatenate(axis=1)([maxpool1,maxpool2])\n    #flat=Flatten()(lstm2)\n    #drop2 = Dropout(0.3)(conc)\n    dense1 = Dense(128,activation='relu')(conc)\n    output=Dense(1,activation='sigmoid')(dense1)\n    model  = Model(inputs=inp, outputs=output)\n    model.compile(loss=loss, optimizer=Adam(lr=0.001), metrics=['accuracy', f1])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_treated = make_model(embedding_treated)\nmodel_treated.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n#checkpoints = ModelCheckpoint('weights.hdf5', monitor=\"val_f1\", mode=\"max\", verbose=True, save_best_only=True)\n#reduce_lr = ReduceLROnPlateau(monitor='val_f1', factor=0.1, patience=2, verbose=1, min_lr=0.000001)\n# early_stopping = EarlyStopping(patience=2, verbose=1, monitor='val_f1', mode='max')\n# checkpoints_treated = ModelCheckpoint('treated_weights.hdf5', monitor=\"val_f1\", mode=\"max\", verbose=True, save_best_only=True)\n# reduce_lr_treated = ReduceLROnPlateau(monitor='val_f1', factor=0.1, patience=2, verbose=1, min_lr=0.1)\n\n\nearly_stopping = EarlyStopping(patience=5, verbose=1, monitor='val_loss', mode='min')\ncheckpoints_treated = ModelCheckpoint('treated_weights.hdf5', monitor=\"val_loss\", mode=\"min\", verbose=1, save_best_only=True)\nreduce_lr_treated = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, min_lr=0.00001, mode='min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import tensorflow as tf\nepochs = 4\nbatch_size = 128\n#tf.reset_default_graph()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a=3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# history = model_treated.fit(X_t_train, y_train_treated, batch_size=batch_size,\n#                             epochs=epochs, \n#                     validation_data=[X_t_chk, y_chk_treated], \n#                             verbose=2, callbacks=[checkpoints_treated, reduce_lr_treated, early_stopping])\n\nhistory = model_treated.fit(X_treated, y, batch_size=batch_size,\n                            epochs=epochs, \n                    validation_data=[X_t_chk, y_chk_treated], \n                            verbose=2, callbacks=[checkpoints_treated, reduce_lr_treated, early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(1,2,1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Test Loss')\nplt.subplot(1,2,2)\nplt.plot(history.history['acc'], label='Train Accuracy')\nplt.plot(history.history['val_acc'], label='Test Accuracy')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_hat = model_treated.predict(X_test_treated,verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\npred_val_y = model_treated.predict([X_t_chk], batch_size=1024, verbose=1)\nthresholds = []\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    res = f1_score(y_chk_treated, (pred_val_y > thresh).astype(int))\n    thresholds.append([thresh, res])\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n    \nthresholds.sort(key=lambda x: x[1], reverse=True)\nbest_thresh_1 = thresholds[0][0]\nprint(\"Best threshold: \", best_thresh_1)\n\n#y_pred = model_treated.predict(X_test_treated, batch_size=1024, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/quora-insincere-questions-classification/sample_submission.csv', encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = (y_hat > best_thresh_1).astype(int)\nsubmission['prediction'] = y_pred\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}