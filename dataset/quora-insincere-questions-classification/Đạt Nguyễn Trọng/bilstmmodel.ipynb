{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* Họ và tên : Nguyễn Trọng Đạt\n* MSSV : 19021240\n* Lớp : INT3405E_20","metadata":{"papermill":{"duration":0.096213,"end_time":"2022-01-08T09:57:21.028995","exception":false,"start_time":"2022-01-08T09:57:20.932782","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# 1. Mô tả bài toán","metadata":{"papermill":{"duration":0.074274,"end_time":"2022-01-08T09:57:21.194113","exception":false,"start_time":"2022-01-08T09:57:21.119839","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Giới thiệu bài toán\n1. Quora là một nền tảng cho phép mọi người học hỏi lẫn nhau. Trên Quora, mọi người có thể đặt câu hỏi và kết nối với những người khác, những người đóng góp thông tin chi tiết độc đáo và câu trả lời chất lượng.\n2. Một vấn đề tồn tại đối với bất kỳ trang web lớn nào hiện nay là làm thế nào để xử lý nội dung độc hại và gây chia rẽ.\n3. Quora muốn giải quyết vấn đề này trực tiếp để giữ cho nền tảng của họ trở thành một nơi mà người dùng có thể cảm thấy an toàn khi chia sẻ kiến thức của họ với thế giới.\n4. Một thách thức quan trọng là loại bỏ những câu hỏi thiếu chân thành - những câu hỏi được đặt ra dựa trên những tiền đề sai lầm hoặc có ý định đưa ra một tuyên bố hơn là tìm kiếm những câu trả lời hữu ích.\n5. Trong notebook này, đề xuất phát triển các mô hình xác định và gắn cờ cho các \"insincere questions\"\n","metadata":{"papermill":{"duration":0.074502,"end_time":"2022-01-08T09:57:21.342827","exception":false,"start_time":"2022-01-08T09:57:21.268325","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Thêm các thư viện cần thiết","metadata":{"papermill":{"duration":0.074254,"end_time":"2022-01-08T09:57:21.491271","exception":false,"start_time":"2022-01-08T09:57:21.417017","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\n%matplotlib inline\nimport matplotlib as mp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport collections\nfrom wordcloud import WordCloud,STOPWORDS\nimport re\nfrom tqdm import tqdm_notebook, tnrange\nfrom tqdm.auto import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize.toktok import ToktokTokenizer","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-01-08T09:57:21.601751Z","iopub.status.busy":"2022-01-08T09:57:21.600995Z","iopub.status.idle":"2022-01-08T09:57:29.665815Z","shell.execute_reply":"2022-01-08T09:57:29.664853Z","shell.execute_reply.started":"2022-01-07T17:17:23.48066Z"},"papermill":{"duration":8.126388,"end_time":"2022-01-08T09:57:29.665973","exception":false,"start_time":"2022-01-08T09:57:21.539585","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tham số ","metadata":{"papermill":{"duration":0.045889,"end_time":"2022-01-08T09:57:29.758559","exception":false,"start_time":"2022-01-08T09:57:29.71267","status":"completed"},"tags":[]}},{"cell_type":"code","source":"embedding_dim = 300 # chiều của vécto embedding từ\nvocab_size = 150000 #số lượng vocab từ lấy ra từ thư viện embedding\nmaxlen = 70 # chiều dài tối đa của 1 từ sau khi làm tròn\n\ntagset_size = 1 # số lượng nhãn\nbatch_size=512\n# Decide which device we want to run on\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:29.927292Z","iopub.status.busy":"2022-01-08T09:57:29.926686Z","iopub.status.idle":"2022-01-08T09:57:29.930531Z","shell.execute_reply":"2022-01-08T09:57:29.931241Z","shell.execute_reply.started":"2022-01-07T17:17:26.163319Z"},"papermill":{"duration":0.127248,"end_time":"2022-01-08T09:57:29.931394","exception":false,"start_time":"2022-01-08T09:57:29.804146","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Đảm bảo tính xác định\nVì ở phần sau, em dùng mô hình train trên GPU nên sẽ thiếu tính \"determinism\" sau mỗi lần train. Có thể hiểu nôm na rằng sau 2 lần train với cùng một bộ dữ liệu, chúng ta sẽ có 2 mô hình không hoàn toàn giống nhau, một cái tốt hơn và một cái cho ra kết quả tệ hơn. \\\nMặc dù sự sai khác của 2 mô hình này là không nhiều nhưng để ổn định hơn, em sẽ xử lý nó bằng cách sử dụng \"seed\". Seed là một điểm bắt đầu trong một chuỗi xác định, nó đảm bảo rằng khi ta dùng cùng một seed, kết quả cho ra qua các lần chạy đều giống nhau","metadata":{"papermill":{"duration":0.045264,"end_time":"2022-01-08T09:57:30.022649","exception":false,"start_time":"2022-01-08T09:57:29.977385","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def set_seed(seed_value=2022):\n    \"\"\"Set seed for reproducibility.\"\"\"\n\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n    torch.backends.cudnn.deterministic = True\nset_seed()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:30.119114Z","iopub.status.busy":"2022-01-08T09:57:30.118439Z","iopub.status.idle":"2022-01-08T09:57:30.123527Z","shell.execute_reply":"2022-01-08T09:57:30.123091Z","shell.execute_reply.started":"2022-01-07T17:17:27.806678Z"},"papermill":{"duration":0.05561,"end_time":"2022-01-08T09:57:30.123646","exception":false,"start_time":"2022-01-08T09:57:30.068036","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Phân tích dữ liệu\n","metadata":{"papermill":{"duration":0.045423,"end_time":"2022-01-08T09:57:30.214083","exception":false,"start_time":"2022-01-08T09:57:30.16866","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 2.1 Khảo sát dữ liệu\n","metadata":{"papermill":{"duration":0.044842,"end_time":"2022-01-08T09:57:30.304013","exception":false,"start_time":"2022-01-08T09:57:30.259171","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 2.1.1Đọc dữ liệu","metadata":{"papermill":{"duration":0.044843,"end_time":"2022-01-08T09:57:30.394227","exception":false,"start_time":"2022-01-08T09:57:30.349384","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\ntest_df = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:30.489873Z","iopub.status.busy":"2022-01-08T09:57:30.489379Z","iopub.status.idle":"2022-01-08T09:57:36.114668Z","shell.execute_reply":"2022-01-08T09:57:36.114169Z","shell.execute_reply.started":"2022-01-07T17:07:28.185747Z"},"papermill":{"duration":5.674837,"end_time":"2022-01-08T09:57:36.114801","exception":false,"start_time":"2022-01-08T09:57:30.439964","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1.2 Tổng quan về dữ liệu","metadata":{"papermill":{"duration":0.046406,"end_time":"2022-01-08T09:57:36.207851","exception":false,"start_time":"2022-01-08T09:57:36.161445","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:36.31197Z","iopub.status.busy":"2022-01-08T09:57:36.311293Z","iopub.status.idle":"2022-01-08T09:57:36.324534Z","shell.execute_reply":"2022-01-08T09:57:36.324085Z","shell.execute_reply.started":"2022-01-07T17:07:33.676351Z"},"papermill":{"duration":0.071064,"end_time":"2022-01-08T09:57:36.32465","exception":false,"start_time":"2022-01-08T09:57:36.253586","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.tail()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:36.426987Z","iopub.status.busy":"2022-01-08T09:57:36.426116Z","iopub.status.idle":"2022-01-08T09:57:36.431174Z","shell.execute_reply":"2022-01-08T09:57:36.430784Z","shell.execute_reply.started":"2022-01-07T17:07:33.701013Z"},"papermill":{"duration":0.059776,"end_time":"2022-01-08T09:57:36.431321","exception":false,"start_time":"2022-01-08T09:57:36.371545","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bộ dữ liệu có 3 cột là **qid** - id của câu hỏi, **question_text** - nội dung của câu hỏi và **target** - phân loại câu hỏi bằng hai giá trị là 0, 1 với 0 là các câu hỏi \"**sincere**\" và 1 là các câu hỏi \"**insincere**\"","metadata":{"papermill":{"duration":0.046928,"end_time":"2022-01-08T09:57:36.526261","exception":false,"start_time":"2022-01-08T09:57:36.479333","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:36.876095Z","iopub.status.busy":"2022-01-08T09:57:36.875229Z","iopub.status.idle":"2022-01-08T09:57:36.900223Z","shell.execute_reply":"2022-01-08T09:57:36.900804Z","shell.execute_reply.started":"2022-01-07T17:07:33.714126Z"},"papermill":{"duration":0.327882,"end_time":"2022-01-08T09:57:36.900999","exception":false,"start_time":"2022-01-08T09:57:36.573117","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Bộ dữ liệu có 1306122 câu hỏi \\\nCác câu hỏi \"sincere\"","metadata":{"papermill":{"duration":0.046499,"end_time":"2022-01-08T09:57:36.998438","exception":false,"start_time":"2022-01-08T09:57:36.951939","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df[train_df['target']==0].head()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:37.100606Z","iopub.status.busy":"2022-01-08T09:57:37.099671Z","iopub.status.idle":"2022-01-08T09:57:37.175148Z","shell.execute_reply":"2022-01-08T09:57:37.174739Z","shell.execute_reply.started":"2022-01-07T17:07:33.986985Z"},"papermill":{"duration":0.129247,"end_time":"2022-01-08T09:57:37.175297","exception":false,"start_time":"2022-01-08T09:57:37.04605","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Các câu hỏi \"insincere\"","metadata":{"papermill":{"duration":0.046931,"end_time":"2022-01-08T09:57:37.269662","exception":false,"start_time":"2022-01-08T09:57:37.222731","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df[train_df['target']==1].head()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:37.370495Z","iopub.status.busy":"2022-01-08T09:57:37.369732Z","iopub.status.idle":"2022-01-08T09:57:37.391591Z","shell.execute_reply":"2022-01-08T09:57:37.391992Z","shell.execute_reply.started":"2022-01-07T17:07:34.081199Z"},"papermill":{"duration":0.075036,"end_time":"2022-01-08T09:57:37.392131","exception":false,"start_time":"2022-01-08T09:57:37.317095","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Visualize data\n","metadata":{"papermill":{"duration":0.047898,"end_time":"2022-01-08T09:57:37.489146","exception":false,"start_time":"2022-01-08T09:57:37.441248","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 2.2.1 Thống kê về dữ liệu","metadata":{"papermill":{"duration":0.048273,"end_time":"2022-01-08T09:57:37.585111","exception":false,"start_time":"2022-01-08T09:57:37.536838","status":"completed"},"tags":[]}},{"cell_type":"code","source":"labels = collections.Counter(train_df['target']).keys()\namounts = collections.Counter(train_df['target']).values()\nlabels, amounts","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:38.082652Z","iopub.status.busy":"2022-01-08T09:57:38.081784Z","iopub.status.idle":"2022-01-08T09:57:38.084734Z","shell.execute_reply":"2022-01-08T09:57:38.085137Z","shell.execute_reply.started":"2022-01-07T17:07:34.109738Z"},"papermill":{"duration":0.45231,"end_time":"2022-01-08T09:57:38.085296","exception":false,"start_time":"2022-01-08T09:57:37.632986","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"insincere_data_train = train_df[train_df.target == 1]\nsincere_data_train = train_df[train_df.target == 0]\nprint(insincere_data_train.shape, sincere_data_train.shape)\nprint(\"--------------------------\")\n\nsns.countplot(x='target', data=train_df)","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:38.187816Z","iopub.status.busy":"2022-01-08T09:57:38.187063Z","iopub.status.idle":"2022-01-08T09:57:38.708161Z","shell.execute_reply":"2022-01-08T09:57:38.7086Z","shell.execute_reply.started":"2022-01-07T17:07:34.510866Z"},"papermill":{"duration":0.575394,"end_time":"2022-01-08T09:57:38.708754","exception":false,"start_time":"2022-01-08T09:57:38.13336","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sincere_percent= (len(train_df.question_text[train_df['target'] == 0]) /  len(train_df['question_text']) * 100)\ninsincere_percent= (len(train_df.question_text[train_df['target'] == 1]) / len(train_df['question_text']) * 100)\n__labels = 'sincere', 'insincere'\nsizes = [sincere_percent, insincere_percent]\nexplode = (0.1, 0)  # explode 1st slice\n\nplt.pie(sizes, explode=explode, labels=__labels,autopct='%1.1f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:38.815915Z","iopub.status.busy":"2022-01-08T09:57:38.815126Z","iopub.status.idle":"2022-01-08T09:57:38.981319Z","shell.execute_reply":"2022-01-08T09:57:38.981748Z","shell.execute_reply.started":"2022-01-07T17:07:34.855878Z"},"papermill":{"duration":0.222937,"end_time":"2022-01-08T09:57:38.981897","exception":false,"start_time":"2022-01-08T09:57:38.75896","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nhận thấy **\"sincere question\"** có hơn 1,2 triệu câu hỏi, chiếm 93.8% tập train, còn lại 6.2% cho **\"insincere question\"** với khoảng gần 81,000 câu hỏi. Nhận thấy lượng data có sự chênh lệnh lớn khi số lượng **\"sincere question\"** gấp 15 lần so với **\"insincere question\"**. Với lượng data bị mất cân bằng giữa các nhãn như thế này, chúng ta cần phải có một biện pháp khắc phục sự mất cân bằng để mô hình cho được kết quả tốt nhất.","metadata":{"papermill":{"duration":0.051216,"end_time":"2022-01-08T09:57:39.08385","exception":false,"start_time":"2022-01-08T09:57:39.032634","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 2.2.1 Phân tích câu trong data\nThống kê số kí tự trong từng câu","metadata":{"papermill":{"duration":0.050124,"end_time":"2022-01-08T09:57:39.183554","exception":false,"start_time":"2022-01-08T09:57:39.13343","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df['question_text'].str.len().hist()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:39.415478Z","iopub.status.busy":"2022-01-08T09:57:39.414634Z","iopub.status.idle":"2022-01-08T09:57:40.374751Z","shell.execute_reply":"2022-01-08T09:57:40.375423Z","shell.execute_reply.started":"2022-01-07T17:07:35.01102Z"},"papermill":{"duration":1.142246,"end_time":"2022-01-08T09:57:40.375578","exception":false,"start_time":"2022-01-08T09:57:39.233332","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Số lượng từ có trong 1 câu","metadata":{"papermill":{"duration":0.050127,"end_time":"2022-01-08T09:57:40.476522","exception":false,"start_time":"2022-01-08T09:57:40.426395","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_df['question_text'].str.split().map(lambda x: len(x)).hist()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:40.717337Z","iopub.status.busy":"2022-01-08T09:57:40.716393Z","iopub.status.idle":"2022-01-08T09:57:45.85929Z","shell.execute_reply":"2022-01-08T09:57:45.858849Z","shell.execute_reply.started":"2022-01-07T17:07:36.081786Z"},"papermill":{"duration":5.331084,"end_time":"2022-01-08T09:57:45.859422","exception":false,"start_time":"2022-01-08T09:57:40.528338","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Chúng ta có thể thấy độ dài các từ trong khoảng 0-20 chiếm phần đa dữ liệu . Max độ dài từ cũng chỉ là 70. Từ đó chúng ta chọn chiều dài biểu diễn vecto trong câu là 70\\\nmaxlen = 70 # max number of words in a question to use","metadata":{"papermill":{"duration":0.050826,"end_time":"2022-01-08T09:57:45.961929","exception":false,"start_time":"2022-01-08T09:57:45.911103","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Chúng ta cùng xem qua các từ xuất hiện nhiều nhất trong các câu hỏi của dữ liệu\n","metadata":{"papermill":{"duration":0.051493,"end_time":"2022-01-08T09:57:46.064985","exception":false,"start_time":"2022-01-08T09:57:46.013492","status":"completed"},"tags":[]}},{"cell_type":"code","source":"new_corpus = []\nquest = train_df['question_text'].str.split()\nquest = quest.values.tolist()\nnew_corpus = [word for q in quest for word in q]\ncounter = collections.Counter(new_corpus)\nmost = counter.most_common()\n\nx,y= [],[]\nfor word,count in most[:20]:\n    x.append(word)\n    y.append(count)\n\nsns.barplot(x=y,y=x).set(title='Các từ xuất hiện nhiều nhất trong dữ liệu Train')","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:46.300591Z","iopub.status.busy":"2022-01-08T09:57:46.299662Z","iopub.status.idle":"2022-01-08T09:57:54.491682Z","shell.execute_reply":"2022-01-08T09:57:54.492568Z","shell.execute_reply.started":"2022-01-07T17:07:41.754106Z"},"papermill":{"duration":8.376473,"end_time":"2022-01-08T09:57:54.492766","exception":false,"start_time":"2022-01-08T09:57:46.116293","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_corpus = []\nquest = test_df['question_text'].str.split()\nquest = quest.values.tolist()\nnew_corpus = [word for q in quest for word in q]\ncounter = collections.Counter(new_corpus)\nmost = counter.most_common()\n\nx,y= [],[]\nfor word,count in most[:20]:\n    x.append(word)\n    y.append(count)\n\nsns.barplot(x=y,y=x).set(title='Các từ xuất hiện nhiều nhất trong dữ liệu Test')","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:54.779989Z","iopub.status.busy":"2022-01-08T09:57:54.77907Z","iopub.status.idle":"2022-01-08T09:57:57.91921Z","shell.execute_reply":"2022-01-08T09:57:57.919711Z","shell.execute_reply.started":"2022-01-07T17:07:49.012421Z"},"papermill":{"duration":3.352087,"end_time":"2022-01-08T09:57:57.919875","exception":false,"start_time":"2022-01-08T09:57:54.567788","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Xem các cụm từ xuất hiện nhiều trong các câu hỏi","metadata":{"papermill":{"duration":0.052993,"end_time":"2022-01-08T09:57:58.026011","exception":false,"start_time":"2022-01-08T09:57:57.973018","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def bigrams_data(data):\n    bigrams = []\n    for question in data:\n        question = [w for w in question.lower().split() if w not in STOPWORDS] # skip từ stopword\n        if not question: \n            continue # skip question do nltk.bigrmas\n        bi = [b for b in nltk.bigrams(question)]\n        bi = [' '.join(w) for w in bi]\n        bigrams.extend(bi)\n    return bigrams\n\ndef draw_plt(data, title, bar_color, numberOfWordsInTop):\n    top_words = collections.Counter(data).most_common(numberOfWordsInTop) # 25 từ xuất hiện nhiều nhất\n\n    df_top = pd.DataFrame(top_words, columns=['word', 'count']).sort_values('count')\n\n    plt.barh(df_top['word'].values, df_top['count'].values, orientation='horizontal', color=bar_color)\n    plt.title(f'Top words in {title}')\n\nbigrams_sincere = bigrams_data(train_df[train_df['target']==0]['question_text'])\nbigrams_insincere = bigrams_data(train_df[train_df['target']==1]['question_text'])","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:57:58.14299Z","iopub.status.busy":"2022-01-08T09:57:58.142125Z","iopub.status.idle":"2022-01-08T09:58:09.564256Z","shell.execute_reply":"2022-01-08T09:58:09.563757Z","shell.execute_reply.started":"2022-01-07T17:07:52.734746Z"},"papermill":{"duration":11.484413,"end_time":"2022-01-08T09:58:09.564401","exception":false,"start_time":"2022-01-08T09:57:58.079988","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(1, 2, 1)\ndraw_plt(bigrams_sincere, 'Các cụm từ xuất hiện nhiều nhất trong câu sincere ', 'blue',25)\n\n\nplt.subplot(1, 2, 2)\ndraw_plt(bigrams_insincere, 'Các cụm từ xuất hiện nhiều nhất trong câu insincere', 'red',25)\n\nplt.subplots_adjust(right=3.0)\nplt.subplots_adjust(top=2.0)\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:58:12.306285Z","iopub.status.busy":"2022-01-08T09:58:12.28587Z","iopub.status.idle":"2022-01-08T09:58:13.915348Z","shell.execute_reply":"2022-01-08T09:58:13.915758Z","shell.execute_reply.started":"2022-01-07T17:08:03.505028Z"},"papermill":{"duration":4.297314,"end_time":"2022-01-08T09:58:13.915905","exception":false,"start_time":"2022-01-08T09:58:09.618591","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Khảo sát các từ xuất hiện nhiều trong các câu hỏi\nBiểu đồ word cloud","metadata":{"papermill":{"duration":0.054413,"end_time":"2022-01-08T09:58:14.02569","exception":false,"start_time":"2022-01-08T09:58:13.971277","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def cloud(docs, title):\n    wordcloud = WordCloud(width=800, height=400, collocations=False, background_color=\"white\").generate(\" \".join(docs))\n    fig = plt.figure(figsize=(10,7), facecolor='w')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.title(title, fontsize=25,color='k')\n    plt.tight_layout(pad=0)\n    plt.show()\ncloud(train_df[train_df['target']==0]['question_text'], \"Sincere question\")\ncloud(test_df['question_text'][train_df['target']==1], \"Insincere question\")","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:58:14.143953Z","iopub.status.busy":"2022-01-08T09:58:14.143124Z","iopub.status.idle":"2022-01-08T09:58:35.997917Z","shell.execute_reply":"2022-01-08T09:58:35.998366Z","shell.execute_reply.started":"2022-01-07T17:08:07.602864Z"},"papermill":{"duration":21.917838,"end_time":"2022-01-08T09:58:35.998557","exception":false,"start_time":"2022-01-08T09:58:14.080719","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Tiền xử lý dữ liệu\nTham khảo tại : https://www.kaggle.com/sunnymarkliu/more-text-cleaning-to-increase-word-coverage\n","metadata":{"papermill":{"duration":0.07212,"end_time":"2022-01-08T09:58:36.142638","exception":false,"start_time":"2022-01-08T09:58:36.070518","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 2.3.1 Làm sạch dữ liệu\n**clean_text** :xử lý các ký tự đặc biệt tồn tại trong văn bản đầu vào, các ký tự đặc biệt được lấy từ mảng puncts bên dưới. Hiểu đơn giản là bỏ đi các ký tự đặc biệt và xóa dấu câu\\\n**clean_number** : Xử lý các chữ số đầu vào, thay thế bằng các ký tự #s tại vì các thư viện embedding đã xử lý các số theo cách này\\\n**replace_typical_misspell**: xử lý các từ viết tắt trong câu đầu vào bằng **mispell_dict** tương ứng","metadata":{"papermill":{"duration":0.070646,"end_time":"2022-01-08T09:58:36.283058","exception":false,"start_time":"2022-01-08T09:58:36.212412","status":"completed"},"tags":[]}},{"cell_type":"code","source":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\npunct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\",\n                 \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '”': '\"', '“': '\"', \"£\": \"e\",\n                 '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta',\n                 '∅': '', '³': '3', 'π': 'pi', '\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\nfor p in puncts:\n    punct_mapping[p] = ' %s ' % p\n\np = re.compile('(\\[ math \\]).+(\\[ / math \\])')\np_space = re.compile(r'[^\\x20-\\x7e]')\n\n#\ndef remove_stopwords(text):\n    text = [word for word in text.split() if word not in STOPWORDS]\n    text = ' '.join(text)\n    return text\n\ndef clean_text(text):\n    # clean latex maths\n    text = p.sub(' [ math ] ', text)\n    # clean invisible chars\n    text = p_space.sub(r'', text)\n    # clean punctuations\n    for punct in punct_mapping:\n        if punct in text:\n            text = text.replace(punct, punct_mapping[punct])\n    #Remove stop word\n#     text = remove_stopwords(text)\n    return text\n\ndef clean_numbers(x):\n    if bool(re.search(r'\\d', x)):\n        x = re.sub('[0-9]{5,}', '#####', x)\n        x = re.sub('[0-9]{4}', '####', x)\n        x = re.sub('[0-9]{3}', '###', x)\n        x = re.sub('[0-9]{2}', '##', x)\n    return x\n\nmispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\",\n                \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n                \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n                \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\",\n                \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"I will have\", \"i'm\": \"i am\",\n                \"i've\": \"I have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n                \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n                \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\",\n                \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n                \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n                \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n                \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n                \"so've\": \"so have\", \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\",\n                \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n                \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n                \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n                \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\",\n                \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n                \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n                \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n                \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\",\n                \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n                \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n                \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n                \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n                \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n                \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color',\n                'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n                'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize',\n                'youtu ': 'youtube ', 'qoura': 'quora', 'sallary': 'salary', 'whta': 'what',\n                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doi': 'do I',\n                'thebest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation',\n                'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis',\n                'etherium': 'ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017',\n                '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess',\n                \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n                'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n#Removing Contractions/ chữa các từ chính tả\ndef replace_typical_misspell(text):\n    tokens = []\n    for token in text.split():\n        # replace contractions & correct misspells\n        token = mispell_dict.get(token.lower(), token)\n        tokens.append(token)\n    text = ' '.join(tokens)\n    return text\n    \n#Remove contraction / Xoá các từ contraction\ncontraction_map = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\ndef clean_contractions(text):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    \n    text = ' '.join([contraction_map[t] if t in contraction_map else t for t in text.split(\" \")])\n    return text\n    ","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:58:36.464534Z","iopub.status.busy":"2022-01-08T09:58:36.457715Z","iopub.status.idle":"2022-01-08T09:58:36.466626Z","shell.execute_reply":"2022-01-08T09:58:36.46616Z","shell.execute_reply.started":"2022-01-07T17:08:29.668413Z"},"papermill":{"duration":0.113682,"end_time":"2022-01-08T09:58:36.46674","exception":false,"start_time":"2022-01-08T09:58:36.353058","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.2 Chuẩn hoá dữ liệu\n1. Chuyển hết tất cả các ký tự trong câu hỏi về dạng viết thường\n2. Xử các ký tự đặc biệt khỏi từng câu\n3. Xử lý các chữ số nằm trong từng câu hỏi\n4. Thay thế các từ viết tắt thành dạng nguyên bản của chúng\n5. Thay thế các giá trị null trong cột questiontext bằng giá trị\n6. Thêm một số trường dữ liệu cho dataset\n7. Tokenize train data và test data : Chuyển text -> ma trận từ\n\n<img src = \"https://miro.medium.com/max/4800/0*c1o1ff1yupRvTLkY.png\" width=\"400\" height=\"400\">\\\n\n\n8. Padding data : Các mô hình yêu cầu cùng kích thước đầu vào -> Lấp đầy những câu có ít từ (ma trận ngắn)\n\n\n\n\n\n\n<img src = \"https://miro.medium.com/max/1332/0*KL-8g0HlN6tnlMDk.png\" width=\"400\" height=\"400\">\n\n9. Shuffle data: trộn data ngẫu nhiên\n\n","metadata":{"papermill":{"duration":0.097818,"end_time":"2022-01-08T09:58:36.634255","exception":false,"start_time":"2022-01-08T09:58:36.536437","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ntqdm.pandas()\n# Thêm một số trường dữ liệu\ndef add_features(df):\n    \n    df['question_text'] = df['question_text'].progress_apply(lambda x:str(x))\n    df['total_length'] = df['question_text'].progress_apply(len)\n    df['capitals'] = df['question_text'].progress_apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df.progress_apply(lambda row: float(row['capitals'])/float(row['total_length']),\n                                axis=1)\n    df['num_words'] = df.question_text.str.count('\\S+')\n    df['num_unique_words'] = df['question_text'].progress_apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n\n    return df\n\ndef load_and_prec():\n    print(\"Train shape : \",train_df.shape)\n    print(\"Test shape : \",test_df.shape)\n    \n    \n    # Lower\n    train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\n    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n\n    # Clean the text\n    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_text(x))\n    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n    # Clean numbers\n    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n    \n    # Clean speelings\n    train_df[\"question_text\"] = train_df[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n    test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n    \n    # Fill up the missing values\n    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n    \n    \n    ###################### Add Features ###############################\n    train = add_features(train_df)\n    test = add_features(test_df)\n    \n    features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n    test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n\n    ss = StandardScaler()\n    ss.fit(np.vstack((features, test_features)))\n    features = ss.transform(features)\n    test_features = ss.transform(test_features)\n    ###########################################################################\n\n    ## Tokenize the sentences\n    tokenizer = Tokenizer(num_words=vocab_size)\n    tokenizer.fit_on_texts(list(train_X))\n    train_X = tokenizer.texts_to_sequences(train_X)\n    test_X = tokenizer.texts_to_sequences(test_X)\n\n    ## Pad the sentences \n    train_X = pad_sequences(train_X, maxlen=maxlen)\n    test_X = pad_sequences(test_X, maxlen=maxlen)\n\n    ## Get the target values\n    train_y = train_df['target'].values\n    \n\n    #shuffling the data\n    np.random.seed(2022)\n    trn_idx = np.random.permutation(len(train_X))\n\n    train_X = train_X[trn_idx]\n    train_y = train_y[trn_idx]\n    features = features[trn_idx]\n    \n    return train_X, test_X, train_y, features, test_features, tokenizer.word_index","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:58:36.89145Z","iopub.status.busy":"2022-01-08T09:58:36.889253Z","iopub.status.idle":"2022-01-08T09:58:36.916453Z","shell.execute_reply":"2022-01-08T09:58:36.917626Z","shell.execute_reply.started":"2022-01-07T17:08:29.711244Z"},"papermill":{"duration":0.167454,"end_time":"2022-01-08T09:58:36.917832","exception":false,"start_time":"2022-01-08T09:58:36.750378","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train,features, test_features, word_index = load_and_prec() ","metadata":{"execution":{"iopub.execute_input":"2022-01-08T09:58:37.185103Z","iopub.status.busy":"2022-01-08T09:58:37.184276Z","iopub.status.idle":"2022-01-08T10:01:33.80338Z","shell.execute_reply":"2022-01-08T10:01:33.802433Z","shell.execute_reply.started":"2022-01-07T17:08:29.730683Z"},"papermill":{"duration":176.755573,"end_time":"2022-01-08T10:01:33.803545","exception":false,"start_time":"2022-01-08T09:58:37.047972","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:01:34.001441Z","iopub.status.busy":"2022-01-08T10:01:34.000629Z","iopub.status.idle":"2022-01-08T10:01:34.004905Z","shell.execute_reply":"2022-01-08T10:01:34.008723Z","shell.execute_reply.started":"2022-01-07T17:11:25.765276Z"},"papermill":{"duration":0.128463,"end_time":"2022-01-08T10:01:34.008922","exception":false,"start_time":"2022-01-08T10:01:33.880459","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.3 Embedding\n<img src = \"https://miro.medium.com/max/1400/0*rAgNKeRR6Dm_PR0b.png\" width=\"800\" height=\"800\">\nTrong notebook này em sử dụng 3 thư viện embedding từ có sẵn của Google-glove, Facebook-fasttext, paragram\nTham khảo : https://www.kaggle.com/suicaokhoailang/blending-with-linear-regression-0-688-lb/notebook","metadata":{"papermill":{"duration":0.12531,"end_time":"2022-01-08T10:01:34.264535","exception":false,"start_time":"2022-01-08T10:01:34.139225","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!unzip ../input/quora-insincere-questions-classification/embeddings.zip","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:01:34.513891Z","iopub.status.busy":"2022-01-08T10:01:34.513073Z","iopub.status.idle":"2022-01-08T10:05:14.690069Z","shell.execute_reply":"2022-01-08T10:05:14.690795Z","shell.execute_reply.started":"2022-01-07T17:11:25.774801Z"},"papermill":{"duration":220.293379,"end_time":"2022-01-08T10:05:14.691012","exception":false,"start_time":"2022-01-08T10:01:34.397633","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_glove(word_index):\n    EMBEDDING_FILE = './glove.840B.300d/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(vocab_size, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in tqdm(word_index.items()):\n        if i >= vocab_size: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n    \ndef load_fasttext(word_index):    \n    EMBEDDING_FILE = './wiki-news-300d-1M/wiki-news-300d-1M.vec'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    # word_index = tokenizer.word_index\n    nb_words = min(vocab_size, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in tqdm(word_index.items()):\n        if i >= vocab_size: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n\ndef load_para(word_index):\n    EMBEDDING_FILE = './paragram_300_sl999/paragram_300_sl999.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(vocab_size, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in tqdm(word_index.items()):\n        if i >= vocab_size: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    \n    return embedding_matrix","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:05:18.837031Z","iopub.status.busy":"2022-01-08T10:05:18.836073Z","iopub.status.idle":"2022-01-08T10:05:18.896313Z","shell.execute_reply":"2022-01-08T10:05:18.897036Z","shell.execute_reply.started":"2022-01-07T17:17:36.070338Z"},"papermill":{"duration":1.681552,"end_time":"2022-01-08T10:05:18.897309","exception":false,"start_time":"2022-01-08T10:05:17.215757","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed()\nembedding_matrix_1 = load_glove(word_index)\nembedding_matrix_2 = load_fasttext(word_index)\nembedding_matrix_3 = load_para(word_index)","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:05:19.53799Z","iopub.status.busy":"2022-01-08T10:05:19.537094Z","iopub.status.idle":"2022-01-08T10:14:58.628972Z","shell.execute_reply":"2022-01-08T10:14:58.628511Z","shell.execute_reply.started":"2022-01-07T17:17:37.668815Z"},"papermill":{"duration":579.430746,"end_time":"2022-01-08T10:14:58.629132","exception":false,"start_time":"2022-01-08T10:05:19.198386","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Xoá đi các dữ liệu đã giải nén trên ổ cứng","metadata":{"papermill":{"duration":0.086759,"end_time":"2022-01-08T10:14:58.802764","exception":false,"start_time":"2022-01-08T10:14:58.716005","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!rm -r ./glove.840B.300d\n!rm -r ./GoogleNews-vectors-negative300\n!rm -r ./wiki-news-300d-1M\n!rm -r ./paragram_300_sl999","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:14:58.999357Z","iopub.status.busy":"2022-01-08T10:14:58.998351Z","iopub.status.idle":"2022-01-08T10:15:02.669325Z","shell.execute_reply":"2022-01-08T10:15:02.668481Z","shell.execute_reply.started":"2022-01-07T17:27:02.355417Z"},"papermill":{"duration":3.780135,"end_time":"2022-01-08T10:15:02.669501","exception":false,"start_time":"2022-01-08T10:14:58.889366","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.mean([embedding_matrix_1, embedding_matrix_2, embedding_matrix_3], axis=0)\nnp.shape(embedding_matrix)","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:15:02.837069Z","iopub.status.busy":"2022-01-08T10:15:02.835936Z","iopub.status.idle":"2022-01-08T10:15:03.905061Z","shell.execute_reply":"2022-01-08T10:15:03.904655Z","shell.execute_reply.started":"2022-01-07T17:27:06.245991Z"},"papermill":{"duration":1.154568,"end_time":"2022-01-08T10:15:03.905221","exception":false,"start_time":"2022-01-08T10:15:02.750653","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self,dataset):\n        self.dataset = dataset\n\n    def __getitem__(self, index):\n        data, target = self.dataset[index]\n\n        return data, target, index\n    def __len__(self):\n        return len(self.dataset)","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:15:04.071667Z","iopub.status.busy":"2022-01-08T10:15:04.069981Z","iopub.status.idle":"2022-01-08T10:15:04.072332Z","shell.execute_reply":"2022-01-08T10:15:04.07277Z","shell.execute_reply.started":"2022-01-07T17:27:07.152121Z"},"papermill":{"duration":0.08802,"end_time":"2022-01-08T10:15:04.072907","exception":false,"start_time":"2022-01-08T10:15:03.984887","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Mô hình\n\n","metadata":{"papermill":{"duration":0.078238,"end_time":"2022-01-08T10:15:04.229295","exception":false,"start_time":"2022-01-08T10:15:04.151057","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 3.1 Cross Validation\nVấn đề : Bộ dataset chúng ta chỉ có 2 tập, train và test, không có tập val. Chúng ta cũng không được dùng tập train để kiểm thử mô hình, vì nó sẽ dẫn tới overfitting trên tập train. Vậy chúng ta sẽ lấy 1 phần của tập train ra làm tập validation. Nhưng, tập train của dữ liệu quá ít nhãn 1, việc lấy ra 1 phần của tập lỡ như hầu hết nhãn 1 đều nằm trong tập val này thì dữ liệu nhãn 1 ở tập train sẽ ít đi và dẫn đến thiếu dữ liệu train. Điều này dẫn đến mô hình không học được nhãn 1, có thể model không tốt . Để mô hình được huấn luyện tốt nhất, em đề xuất sử dụng Cross Validation.\n* Cross Validation là phương pháp chia nhỏ tập training ra thành N phần. Với mỗi lần train, mô hình sẽ sử dụng N-1 phần để train, sau đó test dựa trên 1 phần còn lại. Điều này sẽ giúp cho mô hình hạn chế gặp phải overfitting và giúp bạn tìm ra được những Hyper parameter tốt hơn.\n<img src = \"https://orig00.deviantart.net/fa87/f/2018/096/f/f/cross_validation_by_toiyeumayhoc-dc81ul7.png\" width=\"800\" height=\"800\">\n\n* Như hình chúng ta có thể thấy : Với mỗi lần train đầu, lấy 4 fold đầu tiên để train. Sau đó để val, sử dụng fold 5 để val. Qua lần train thứ 2, bạn lấy từ fold 2 đến fold 5 để train, rồi lại lấy fold 1 để val. Và đó, chính là Cross Validation.\n<img src = \"https://img00.deviantart.net/951a/i/2018/096/0/a/cross_validation_1_by_toiyeumayhoc-dc81vuz.jpg\" width=\"600\" height=\"600\">\n\n* Phương pháp đánh giá : Training data ta chia thành K phần. Sau đó train model K lần, mỗi lần train sẽ chọn 1 phần làm dữ liệu validation và K-1 phần còn lại làm dữ liệu training. Kết quả đánh giá model cuối cùng là trung bình cộng kết quả đánh giá của K lần train.\n\nEm chia dữ liệu thành 5 phần:","metadata":{"papermill":{"duration":0.07893,"end_time":"2022-01-08T10:15:04.386961","exception":false,"start_time":"2022-01-08T10:15:04.308031","status":"completed"},"tags":[]}},{"cell_type":"code","source":"splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=2022).split(x_train, y_train))","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:15:04.551457Z","iopub.status.busy":"2022-01-08T10:15:04.550656Z","iopub.status.idle":"2022-01-08T10:15:04.760964Z","shell.execute_reply":"2022-01-08T10:15:04.760076Z","shell.execute_reply.started":"2022-01-07T17:27:07.406147Z"},"papermill":{"duration":0.29489,"end_time":"2022-01-08T10:15:04.761107","exception":false,"start_time":"2022-01-08T10:15:04.466217","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 ModelBiLSTM","metadata":{"papermill":{"duration":0.080376,"end_time":"2022-01-08T10:15:04.921114","exception":false,"start_time":"2022-01-08T10:15:04.840738","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Tham khảo : https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.htmlhttps://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html","metadata":{"papermill":{"duration":0.079946,"end_time":"2022-01-08T10:15:05.079615","exception":false,"start_time":"2022-01-08T10:15:04.999669","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Mô hình\n\n* Mô hình gồm t input, các input được đưa vào mô hình đúng với thứ tự từ trong câu\n* Mỗi hình vuông được gọi là 1 state, đầu vào mỗi state là $ x_t $, $h_{t-1}$ với $h_t = f(W*x_t + U*h_{t-1})$. ($\\mathbf W$ là trọng số của đầu vào, $\\mathbf U$ là trọng số của trạng thái ẩn), $\\mathbb f$ là activation value như: sigmoid, tanh, ReLU,....\n* Có thể thấy $h_t$ mang cả thông tin từ hidden state trước \n* $h_0$ được thêm vào để cho chuẩn công thức nên thường được gán bằng 0 hoặc giá trị ngẫu nhiên\n* $y_t = g(V*h_t)$. V là trọng số của trạng thái ẩn sau khi tính đầu ra\n\n#### LSTM (Long short term memory)\n* Mạng RNN có yếu điểm là không mô tả học được chuỗi quá dài do hiện tượng triệt tiêu đạo hàm (vanishing gradient). Mạng LSTM ra đời khắc phục phần nào nhược điểm này bằng cách cho phép thông tin lan truyền trực tiếp hơn thông qua một biến trạng thái ô (cell state).\n\n* Mạng bộ nhớ dài-ngắn (Long Short Term Memory networks), thường được gọi là LSTM - là một dạng đặc biệt của RNN (Recurrent Neural Network), nó có khả năng học được các phụ thuộc xa. Chúng hoạt động cực kì hiệu quả trên nhiều bài toán khác nhau nên dần đã trở nên phổ biến như hiện nay.\n\n\n* LSTM được thiết kế để tránh được vấn đề phụ thuộc xa (long-term dependency). Việc nhớ thông tin trong suốt thời gian dài là đặc tính mặc định của chúng, chứ ta không cần phải huấn luyện nó để có thể nhớ được. Tức là ngay nội tại của nó đã có thể ghi nhớ được mà không cần bất kì can thiệp nào.\n\n<img  src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-16-15-51-05.png\" width=\"600\" height =\"400\">\n\n\n\n* Tại state t\n    * output: $c_t$  là cell state, $h_t$ là hidden state     \n    * input: $c_{t-1},h_{t-1}$. Ở đây $c$ là điểm mới so với RNN\n* Tính toán trong cell LSTM:\n    * Cổng quên (forget gate): $\\mathbf f_t = \\sigma(\\mathbf W_{f} \\mathbf x_t + \\mathbf U_{f}\\mathbf h_{t-1})$\n    * Cổng đầu vào (input gate): $\\mathbf i_t = \\sigma(\\mathbf W_{i} \\mathbf x_t + \\mathbf U_{i}\\mathbf h_{t-1})$\n    * Cổng đầu ra (output gate): $\\mathbf o_t = \\sigma(\\mathbf W_{o} \\mathbf x_t + \\mathbf U_{o}\\mathbf h_{t-1})$\n    * $\\tilde{\\mathbf c}_t = \\mathrm{tanh}(\\mathbf W_{c} \\mathbf x_t + \\mathbf U_{c}\\mathbf h_{t-1})$\n    * Cổng trạng thái ô (cell state): $\\mathbf c_{t} = \\mathbf f_t \\times \\mathbf c_{t-1} + \\mathbf i_t \\times \\tilde{\\mathbf c}_t$. Forget gate quyết định xem lấy bao nhiêu từ cell state trước và input gate sẽ quyết định lấy bao nhiêu từ input của state và hidden state của state trước\n    * $ \\mathbf h_t = \\mathrm{tanh}(c_t) \\times \\mathbf o_t $ , $\\mathbf y_t = \\phi_y(\\mathbf W_y \\mathbf h_t)$\n    \n    \n    \n* Chìa khóa của LSTM là trạng thái tế bào (cell state) - chính đường chạy thông ngang phía trên của sơ đồ hình vẽ.Trạng thái tế bào là một dạng giống như băng truyền. Nó chạy xuyên suốt tất cả các mắt xích (các nút mạng) và chỉ tương tác tuyến tính đôi chút. Vì vậy mà các thông tin có thể dễ dàng truyền đi thông suốt mà không sợ bị thay đổi. $c_t$ sẽ được hiệu chỉnh để học được chuỗi dài hơn\n\n\n\n\n* <img  src=\"https://i2.wp.com/nttuan8.com/wp-content/uploads/2019/06/cell_state.png?fit=1024%2C316&ssl=1\" width=\"600\" height =\"400\">\n\n\n* LSTM có khả năng bỏ đi hoặc thêm vào các thông tin cần thiết cho trạng thái tế báo, chúng được điều chỉnh cẩn thận bởi các nhóm được gọi là cổng (gate). Và đó là những khái niệm cốt lõi về LSTM.\n\n\n#### Bidirectional LSTM \n\n\n* Bidirectional LSTM là mô hình gồm hai LSTM: LSTM thứ nhất nhận đầu vào là chuỗi các từ theo thứ tự từ trái sang phải, LSTM còn lại nhận đầu vào là chuỗi các từ theo thứ tự từ phải sang trái. Cải thiện ngữ cảnh của mô hình, giúp mô hình học được tốt hơn\n\n<img src=\"https://www.researchgate.net/profile/Giovanni-Montana/publication/312250942/figure/fig2/AS:450835592290305@1484498989447/An-illustration-of-the-BiLSTM-architecture-for-joint-medical-entity-recognition-and.png\" width=\"400\" height =\"400\">\n\n#### Kiến trúc mô hình sử dụng\n* Lớp Embedding layer\n* BiLSTM\n* 2 lớp linear, với hàm kích hoạt relu\n* Dropout \n","metadata":{"papermill":{"duration":0.079287,"end_time":"2022-01-08T10:15:05.237806","exception":false,"start_time":"2022-01-08T10:15:05.158519","status":"completed"},"tags":[]}},{"cell_type":"code","source":"hidden_size = 64\n\nclass BiLSTM(nn.Module):\n    def __init__(self):\n        super(BiLSTM, self).__init__()\n        self.hidden_size = 64\n        drp = 0.1\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.lstm = nn.LSTM(embedding_dim, self.hidden_size, bidirectional=True, batch_first=True)\n        self.linear = nn.Linear(self.hidden_size*4 , 64)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(drp)\n        self.out = nn.Linear(64, tagset_size)\n    def forward(self, x):\n        #rint(x.size())*\n        h_embedding = self.embedding(x)\n        #_embedding = torch.squeeze(torch.unsqueeze(h_embedding, 0))*\n        h_lstm, _ = self.lstm(h_embedding)\n        avg_pool = torch.mean(h_lstm, 1)\n        max_pool, _ = torch.max(h_lstm, 1)\n        conc = torch.cat(( avg_pool, max_pool), 1)\n        conc = self.relu(self.linear(conc))\n        conc = self.dropout(conc)\n        out = self.out(conc)\n        return out","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:15:05.407933Z","iopub.status.busy":"2022-01-08T10:15:05.406236Z","iopub.status.idle":"2022-01-08T10:15:05.408633Z","shell.execute_reply":"2022-01-08T10:15:05.409112Z","shell.execute_reply.started":"2022-01-07T17:27:10.370425Z"},"papermill":{"duration":0.092839,"end_time":"2022-01-08T10:15:05.409284","exception":false,"start_time":"2022-01-08T10:15:05.316445","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# matrix for the out-of-fold predictions\ntrain_preds = np.zeros((len(x_train)))\n# matrix for the predictions on the test set\ntest_preds = np.zeros((len(test_df)))\n\n# always call this before training for deterministic results\n# seed_everything()\n\nx_test_cuda = torch.tensor(x_test, dtype=torch.long).to(device)\ntest = torch.utils.data.TensorDataset(x_test_cuda)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n\navg_losses_f = []\navg_val_losses_f = []\ntrain_loss = []\nvalid_loss = []\n\n# Tracking best validation accuracy\nbest_accuracy = 0","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:15:05.575792Z","iopub.status.busy":"2022-01-08T10:15:05.574801Z","iopub.status.idle":"2022-01-08T10:15:11.171481Z","shell.execute_reply":"2022-01-08T10:15:11.17096Z","shell.execute_reply.started":"2022-01-07T17:39:55.137684Z"},"papermill":{"duration":5.683188,"end_time":"2022-01-08T10:15:11.17161","exception":false,"start_time":"2022-01-08T10:15:05.488422","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Optimizers","metadata":{"papermill":{"duration":0.080203,"end_time":"2022-01-08T10:15:11.331728","exception":false,"start_time":"2022-01-08T10:15:11.251525","status":"completed"},"tags":[]}},{"cell_type":"code","source":"base_lr, max_lr = 0.001, 0.003\ndef optimizers(model):\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=max_lr)\n    return optimizer","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:15:11.496042Z","iopub.status.busy":"2022-01-08T10:15:11.495145Z","iopub.status.idle":"2022-01-08T10:15:11.497257Z","shell.execute_reply":"2022-01-08T10:15:11.497642Z","shell.execute_reply.started":"2022-01-07T17:27:16.464579Z"},"papermill":{"duration":0.086744,"end_time":"2022-01-08T10:15:11.49778","exception":false,"start_time":"2022-01-08T10:15:11.411036","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Loss Function","metadata":{"papermill":{"duration":0.079473,"end_time":"2022-01-08T10:15:11.656282","exception":false,"start_time":"2022-01-08T10:15:11.576809","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# define binary cross entropy loss\n# note that the model returns logit to take advantage of the log-sum-exp trick \n# for numerical stability in the loss\nloss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:15:11.821256Z","iopub.status.busy":"2022-01-08T10:15:11.820346Z","iopub.status.idle":"2022-01-08T10:15:11.822878Z","shell.execute_reply":"2022-01-08T10:15:11.822293Z","shell.execute_reply.started":"2022-01-07T17:27:16.471055Z"},"papermill":{"duration":0.086447,"end_time":"2022-01-08T10:15:11.823001","exception":false,"start_time":"2022-01-08T10:15:11.736554","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Training","metadata":{"papermill":{"duration":0.079247,"end_time":"2022-01-08T10:15:11.983035","exception":false,"start_time":"2022-01-08T10:15:11.903788","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 4.1 Create PyTorch DataLoader","metadata":{"papermill":{"duration":0.078495,"end_time":"2022-01-08T10:15:12.14013","exception":false,"start_time":"2022-01-08T10:15:12.061635","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_data_loader(x_train,y_train,train_idx, valid_idx,batch_size):\n    # split data in train / validation according to the KFold indeces\n    # also, convert them to a torch tensor and store them on the GPU (done with .cuda())\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    \n    \n    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.long).to(device)\n    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32).to(device)\n    \n    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.long).to(device)\n    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32).to(device)\n    \n    #############################################################################################\n    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n    \n    train = MyDataset(train)\n    valid = MyDataset(valid)\n    \n    ##No need to shuffle the data again here. Shuffling happens when splitting for kfolds.\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n    return train_loader,valid_loader","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:15:12.309162Z","iopub.status.busy":"2022-01-08T10:15:12.307416Z","iopub.status.idle":"2022-01-08T10:15:12.311801Z","shell.execute_reply":"2022-01-08T10:15:12.311084Z","shell.execute_reply.started":"2022-01-07T17:27:18.024401Z"},"papermill":{"duration":0.092878,"end_time":"2022-01-08T10:15:12.311917","exception":false,"start_time":"2022-01-08T10:15:12.219039","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Evaluate model","metadata":{"papermill":{"duration":0.080151,"end_time":"2022-01-08T10:15:12.471298","exception":false,"start_time":"2022-01-08T10:15:12.391147","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#eval model\ndef evaluate(model, valid_loader):\n    \n    # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n    model.eval()\n    # predict all the samples in y_val_fold batch per batch\n    valid_preds_fold = np.zeros(len(valid_idx))\n    avg_val_loss = 0\n    val_accuracy = []\n    \n    for i, (x_batch, y_batch, index) in enumerate(valid_loader):\n        \n        with torch.no_grad():\n            y_pred = model(x_batch).detach()\n        \n        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n        valid_preds_fold[i * batch_size:(i+1) * batch_size] = torch.special.expit(y_pred).cpu().numpy()[:, 0]\n        if i==0 and epoch == 0:\n            valid_loss.append(loss_fn(y_pred, y_batch))\n        train_preds[valid_idx] = valid_preds_fold\n        # Get the predictions\n        preds = torch.argmax(y_pred, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == y_batch).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n    accuracy = sum(val_accuracy)/len(val_accuracy)\n    \n    print('val_accuracy = {:.4f} '.format(accuracy))\n    return valid_preds_fold, avg_val_loss\n#predicts model\ndef predict(model, test_loader):\n    # predict all samples in the test set batch per batch\n    test_preds_fold = np.zeros((len(test_df)))\n    for i, (x_batch,) in enumerate(test_loader):\n        \n        y_pred = model(x_batch).detach()\n        test_preds_fold[i * batch_size:(i+1) * batch_size] = torch.special.expit(y_pred).cpu().numpy()[:, 0]\n    return test_preds_fold","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:15:12.640931Z","iopub.status.busy":"2022-01-08T10:15:12.640173Z","iopub.status.idle":"2022-01-08T10:15:12.642686Z","shell.execute_reply":"2022-01-08T10:15:12.642151Z","shell.execute_reply.started":"2022-01-07T17:39:41.668924Z"},"papermill":{"duration":0.092683,"end_time":"2022-01-08T10:15:12.642811","exception":false,"start_time":"2022-01-08T10:15:12.550128","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Training","metadata":{"papermill":{"duration":0.078926,"end_time":"2022-01-08T10:15:12.801478","exception":false,"start_time":"2022-01-08T10:15:12.722552","status":"completed"},"tags":[]}},{"cell_type":"code","source":"n_epochs = 5\nset_seed()\nfor i, (train_idx, valid_idx) in enumerate(splits):\n    train_loader,valid_loader = create_data_loader(x_train,y_train,train_idx, valid_idx,batch_size)\n    model = BiLSTM()\n\n    # make sure everything in the model is running on the GPU\n    model.to(device)\n    \n    optimizer = optimizers(model)\n    print(f'Fold {i + 1}')\n    for epoch in range(n_epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        start_time = time.time()\n        # set train mode of the model. This enables operations which are only applied during training like dropout\n        \n        model.train()\n        avg_loss = 0.  \n        for i, (x_batch, y_batch, index) in enumerate(train_loader):\n            \n            # Forward pass: compute predicted y by passing x to the model.           \n            y_pred = model(x_batch)\n            \n            # Compute and print loss.\n            loss = loss_fn(y_pred, y_batch)\n\n            # Before the backward pass, use the optimizer object to zero all of the\n            # gradients for the Tensors it will update (which are the learnable weights\n            # of the model)\n            optimizer.zero_grad()\n             # Backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n\n            # Calling the step function on an Optimizer makes an update to its parameters\n            optimizer.step()\n            avg_loss += loss.item()\n            if(epoch==0 and i==0):\n                train_loss.append(loss.item())\n        \n        # Calculate the average loss over the entire training data        \n        avg_loss = avg_loss / len(train_loader)    \n        \n        \n        # =======================================\n        #               Evaluation\n        # =======================================\n        if valid_loader is not None:\n            # After the completion of each training epoch, measure the model's\n            # performance on our validation set.\n            valid_preds_fold, avg_val_loss = evaluate(model, valid_loader)\n            elapsed_time = time.time() - start_time \n            print('Epoch {}/{} \\t avg_loss={:.4f} \\t avg_val_loss={:.4f} \\t time={:.2f}s'.format(\n                epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n        \n        \n    avg_losses_f.append(avg_loss)\n    avg_val_losses_f.append(avg_val_loss) \n    #predics test_data_loader\n    test_preds_fold = predict(model, test_loader)\n    test_preds += test_preds_fold / len(splits)\n    \n    ##predics train_data_loader\n    train_preds[valid_idx] = valid_preds_fold\n\nprint('All \\t loss={:.4f} \\t val_loss={:.4f} \\t '.format(np.average(avg_losses_f),np.average(avg_val_losses_f)))","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:15:12.973345Z","iopub.status.busy":"2022-01-08T10:15:12.972291Z","iopub.status.idle":"2022-01-08T10:33:07.040356Z","shell.execute_reply":"2022-01-08T10:33:07.040746Z","shell.execute_reply.started":"2022-01-07T17:43:54.740681Z"},"papermill":{"duration":1074.160502,"end_time":"2022-01-08T10:33:07.040907","exception":false,"start_time":"2022-01-08T10:15:12.880405","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\navg_losses_f_n=np.array(avg_losses_f)\navg_val_losses_f_n=np.array(avg_val_losses_f)\nplt.plot(avg_losses_f_n, label='avg_losses_f')\nplt.plot(avg_val_losses_f_n, label='avg_val_losses_f')\nplt.title(\"Avg Losses\")\nplt.legend()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:33:07.233697Z","iopub.status.busy":"2022-01-08T10:33:07.231357Z","iopub.status.idle":"2022-01-08T10:33:07.463419Z","shell.execute_reply":"2022-01-08T10:33:07.462975Z","shell.execute_reply.started":"2022-01-07T17:54:35.651408Z"},"papermill":{"duration":0.334954,"end_time":"2022-01-08T10:33:07.463549","exception":false,"start_time":"2022-01-08T10:33:07.128595","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots()\ntrain_loss = np.array(train_loss)\nvalid_loss = np.array(valid_loss)\nplt.plot(train_loss, label='train_loss ford 1')\nplt.plot(valid_loss, label='valid_loss ford 1')\nplt.title(\"Training Losses\")\nplt.legend()","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:33:07.789334Z","iopub.status.busy":"2022-01-08T10:33:07.788385Z","iopub.status.idle":"2022-01-08T10:33:08.002776Z","shell.execute_reply":"2022-01-08T10:33:08.002357Z","shell.execute_reply.started":"2022-01-07T17:54:38.211412Z"},"papermill":{"duration":0.447367,"end_time":"2022-01-08T10:33:08.002896","exception":false,"start_time":"2022-01-08T10:33:07.555529","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bestThresshold(y_train,train_preds):\n    tmp = [0,0,0] # idx, cur, max\n    delta = 0\n    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n        tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n        if tmp[1] > tmp[2]:\n            delta = tmp[0]\n            tmp[2] = tmp[1]\n    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n    return delta\ndelta = bestThresshold(y_train,train_preds)","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:33:08.210924Z","iopub.status.busy":"2022-01-08T10:33:08.21007Z","iopub.status.idle":"2022-01-08T10:33:24.43189Z","shell.execute_reply":"2022-01-08T10:33:24.43127Z","shell.execute_reply.started":"2022-01-07T17:54:40.622294Z"},"papermill":{"duration":16.33901,"end_time":"2022-01-08T10:33:24.432051","exception":false,"start_time":"2022-01-08T10:33:08.093041","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_test = y_train\ntrain_preds\ntrain_preds_test = (train_preds > delta).astype(int)\n\nunique, counts = np.unique(train_preds_test, return_counts=True)\ndict(zip(unique, counts))\n# np.amax(train_preds_test)\n","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:33:24.6171Z","iopub.status.busy":"2022-01-08T10:33:24.61623Z","iopub.status.idle":"2022-01-08T10:33:24.643256Z","shell.execute_reply":"2022-01-08T10:33:24.643672Z","shell.execute_reply.started":"2022-01-07T17:56:04.484444Z"},"papermill":{"duration":0.122228,"end_time":"2022-01-08T10:33:24.643819","exception":false,"start_time":"2022-01-08T10:33:24.521591","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_train_test, train_preds_test))","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:33:24.83203Z","iopub.status.busy":"2022-01-08T10:33:24.831093Z","iopub.status.idle":"2022-01-08T10:33:26.452361Z","shell.execute_reply":"2022-01-08T10:33:26.451853Z","shell.execute_reply.started":"2022-01-07T17:56:06.337449Z"},"papermill":{"duration":1.718999,"end_time":"2022-01-08T10:33:26.452492","exception":false,"start_time":"2022-01-08T10:33:24.733493","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(confusion_matrix(y_train_test, train_preds_test))","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:33:26.639673Z","iopub.status.busy":"2022-01-08T10:33:26.638739Z","iopub.status.idle":"2022-01-08T10:33:28.312249Z","shell.execute_reply":"2022-01-08T10:33:28.313526Z","shell.execute_reply.started":"2022-01-07T17:56:10.346711Z"},"papermill":{"duration":1.770691,"end_time":"2022-01-08T10:33:28.313743","exception":false,"start_time":"2022-01-08T10:33:26.543052","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test_df[['qid']].copy()\nsubmission['prediction'] = (test_preds > delta).astype(int)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:33:28.585863Z","iopub.status.busy":"2022-01-08T10:33:28.58497Z","iopub.status.idle":"2022-01-08T10:33:29.406201Z","shell.execute_reply":"2022-01-08T10:33:29.40732Z","shell.execute_reply.started":"2022-01-07T17:56:12.032495Z"},"papermill":{"duration":0.939937,"end_time":"2022-01-08T10:33:29.407675","exception":false,"start_time":"2022-01-08T10:33:28.467738","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head submission.csv","metadata":{"execution":{"iopub.execute_input":"2022-01-08T10:33:29.723102Z","iopub.status.busy":"2022-01-08T10:33:29.722291Z","iopub.status.idle":"2022-01-08T10:33:30.682739Z","shell.execute_reply":"2022-01-08T10:33:30.682189Z","shell.execute_reply.started":"2022-01-06T18:47:54.065748Z"},"papermill":{"duration":1.118077,"end_time":"2022-01-08T10:33:30.682865","exception":false,"start_time":"2022-01-08T10:33:29.564788","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}