{"cells":[{"metadata":{"_uuid":"04666f49ac27d4c1e6e0aab6fe295245bcc7b22e"},"cell_type":"markdown","source":"# Starter Code for fastText English Word Vectors Embedding\n\nThis kernel intends to be a starter code for anyone using the fastText Embedding. It uses Gensim to create a `KeyedVector` object (behavior similar to a dictionary). An example of tokenizing the data is also given."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport nltk\nfrom gensim.models import KeyedVectors\nfrom sklearn.datasets import fetch_20newsgroups","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20ccefecab3bca5c5f0dccdfe5befd2e663b5250"},"cell_type":"code","source":"print(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"FILE_PATH = '../input/fasttext-wikinews/wiki-news-300d-1M.vec'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"249b30328d7aa9effd7d7c42373c20481d8a3e12","scrolled":true},"cell_type":"code","source":"# Let's read the first few lines \nwith open(FILE_PATH) as f:\n    for _ in range(5):\n        print(f.readline()[:80])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fe6da8b88a544bb2a3055657f9fc5ddad76f2ff"},"cell_type":"markdown","source":"## Load the embedding"},{"metadata":{"trusted":true,"_uuid":"52ead221bd6a5058f2496a0f3b2d4eb878975cee"},"cell_type":"code","source":"# This may take a few mins\nkeyed_vec = KeyedVectors.load_word2vec_format(FILE_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f829dfaa3f63cb4bfa73bb2ed3b25d290ef10cee"},"cell_type":"code","source":"for word in ['hello', '!', '2', 'Turing', 'foobarz', 'hi!']:\n    print(word, \"is in the vocabulary:\", word in keyed_vec.vocab)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10c20e26f6e2eb85877d0f735560e3e7e6b6fa27"},"cell_type":"markdown","source":"### Retrieving a vector with the KeyedVector"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"1d9810361256a46147609c779562afbf85dcf8c1"},"cell_type":"code","source":"word_vec = keyed_vec.get_vector('foobar')\nprint(word_vec.shape)\nprint(word_vec[:25])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"180c439dcf858ca56393acad576eface1dc0233a"},"cell_type":"markdown","source":"### Creating Keras Embeddings"},{"metadata":{"trusted":true,"_uuid":"6f7dc3d679463988f9145aebf92ad86a288144fc"},"cell_type":"code","source":"keras_embedding = keyed_vec.get_keras_embedding()\nkeras_embedding.get_config()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c783d32b0472d79b8c55ccceb01cc9294258411"},"cell_type":"markdown","source":"## Applied Example: Prediction with scikit-learn"},{"metadata":{"trusted":true,"_uuid":"2fe738fcac46daf4679f3c5ba52d4c6fcbbf19ca"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4648711227b86ee9bccab27728c5c1ba16048052"},"cell_type":"code","source":"def mean_fasttext(arr, embedding_dim=300):\n    '''\n    Create the average of the fasttext embeddings from each word in a document. \n    Very slow function, needs to be optimized for larger datasets\n    '''\n    mean_vectors = []\n    for document in arr:\n        tokens = nltk.tokenize.word_tokenize(document)\n        vectors = [keyed_vec.get_vector(token) for token in tokens if token in keyed_vec.vocab]\n        if vectors:\n            mean_vec = np.vstack(vectors).mean(axis=0)\n            mean_vectors.append(mean_vec)\n        else:\n            mean_vectors.append(np.zeros(embedding_dim))\n    embedding = np.vstack(mean_vectors)\n    return embedding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5b987750cdbee6e69337f31130d9878edba8212"},"cell_type":"code","source":"data_sample = pd.read_csv('../input/quora-insincere-questions-classification/train.csv', nrows=6000)\ntrain_sample = data_sample[:5000]\ntest_sample = data_sample[5000:]\ntrain_sample.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"23667b8d0a81ee0271b29f3735de71687659d953"},"cell_type":"code","source":"X_train = mean_fasttext(train_sample[\"question_text\"].values)\nX_test = mean_fasttext(test_sample[\"question_text\"].values)\ny_train = train_sample['target'].values\ny_test = test_sample['target'].values\nprint(X_train.shape)\nprint(y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f8f02185142f7d894c9818f5fe8cb79e3e46a72"},"cell_type":"code","source":"model = LogisticRegression(solver='lbfgs')\nmodel.fit(X_train, y_train)\nprint(\"Train Score:\", f1_score(y_train, model.predict(X_train)))\nprint(\"Test Score:\", f1_score(y_test, model.predict(X_test)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}