{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Name:** Trương Thị Kiều Anh\n\n**Student ID:** 19021209\n\n**Class:** 2122I_INT3405E_20","metadata":{}},{"cell_type":"markdown","source":"# Problem Description","metadata":{}},{"cell_type":"markdown","source":"Quora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n* Input: \n  a question asked on Quora\n* Output:\n  0/1 (Yes/ No) - predicting whether a question asked on Quora is sincere or not","metadata":{}},{"cell_type":"markdown","source":"# Data analysis","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport gc\nimport re\nimport spacy\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import backend as K\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.initializers import Constant\nfrom keras.utils.vis_utils import plot_model\n\nimport torch\nimport torch.nn as nn\nfrom tensorflow.keras.optimizers import Adam\n\nfrom torch import LongTensor, FloatTensor, DoubleTensor\nfrom torch.utils.data import Dataset, DataLoader, sampler\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom tqdm.notebook import tqdm\nfrom IPython.core.display import display, HTML\ntqdm().pandas()\n\npd_ctx = pd.option_context('display.max_colwidth', 100)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:47:16.920562Z","iopub.execute_input":"2022-01-07T04:47:16.921096Z","iopub.status.idle":"2022-01-07T04:47:25.724184Z","shell.execute_reply.started":"2022-01-07T04:47:16.921059Z","shell.execute_reply":"2022-01-07T04:47:25.723511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data size:**\n1.306.122 question\n\n**Data fields**\n   * `qid` - unique question identifier\n   * `question_text` - Quora question text\n   * `target` - a question labeled \"insincere\" has a value of 1, otherwise 0\n   \n\n*No data is null or missing*   ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/quora-insincere-questions-classification/train.csv')\ndf.info()\n\ntest_df = pd.read_csv('/kaggle/input/quora-insincere-questions-classification/test.csv')\ntest_df.info()\n\ndf['word_count']= df.question_text.progress_apply(lambda x: len(x.split()))\nsincere_data = df[df['target']==0]\ninsincere_data = df[df['target']==1]\nprint(\"Sincere question\")\ndisplay(sincere_data.head())\nprint(\"Insincere question\")\ndisplay(insincere_data.head())","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:47:25.72599Z","iopub.execute_input":"2022-01-07T04:47:25.726717Z","iopub.status.idle":"2022-01-07T04:47:35.678614Z","shell.execute_reply.started":"2022-01-07T04:47:25.726678Z","shell.execute_reply":"2022-01-07T04:47:35.677877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get number of word in a sentences\nstatistic = pd.merge(\n    sincere_data[['word_count']].describe(percentiles=[.8, .9999]), \n    insincere_data[['word_count']].describe(percentiles=[.8, .9999]), \n    left_index=True, right_index=True, suffixes=('_sincere', '_insincere')\n)\ncolLabels = statistic.columns\ncellText = statistic.round(2).values\nrowLabels = statistic.index\n\nfig, axes = plt.subplots(nrows=1, ncols=2)\naxes[0] = fig.add_axes([0,0,1,1])\naxes[0].bar(['sincere question', 'insincere question'], df.target.value_counts())\nfor p in axes[0].patches:\n    width = p.get_width()\n    height = p.get_height()\n    percent = height / len(df)\n    x, y = p.get_xy() \n    axes[0].annotate(f'{percent:.2%}', (x + width/2, y + height + 0.01*len(df)), ha='center')\n# axes[1].axis('off')\nmpl_table = axes[1].table(cellText = cellText, colLabels=colLabels, rowLabels = rowLabels, bbox=[2, 0, 2, 1.5], )\nmpl_table.auto_set_font_size(False)\nmpl_table.set_fontsize(14)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:47:35.680092Z","iopub.execute_input":"2022-01-07T04:47:35.680581Z","iopub.status.idle":"2022-01-07T04:47:36.38271Z","shell.execute_reply.started":"2022-01-07T04:47:35.680543Z","shell.execute_reply":"2022-01-07T04:47:36.382039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Sincere question account for 93,81%\n* Insincere question account for 6,19%\n\n$\\rightarrow$ The dataset for training is unbalanced (positive results are 15 times more negative) create a misinterpretation of model quality. Then if we are using accuracy as a performance metric, it can be achieved very high without the model. For example, a random prediction given that all are in the majority group, the accuracy achieved is 93%. Therefore, for an imbalanced dataset, other performance metrics should be used.\n\n**Length of sentences in the dataset**\n* Sincere question: The average length of a sentence is 12,5; the longest sentence have 134 words; 80% of these sentence have less than or equal to 16 words. Most of the sentences have length under 53 words.\n* Insincere question: The average length of a sentence is 17,28; the longest sentence have 64 words; 80% of these sentence have less than or equal to 25 words. Most of the sentences have length under 54 words. Insincere question tend to bave longer length than sincere question.","metadata":{}},{"cell_type":"markdown","source":"## Word cloud","metadata":{}},{"cell_type":"code","source":"def cloud(docs, title):\n    wordcloud = WordCloud(width=800, height=400, collocations=False, background_color=\"white\").generate(\" \".join(docs))\n    fig = plt.figure(figsize=(10,7), facecolor='w')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.title(title, fontsize=25,color='k')\n    plt.tight_layout(pad=0)\n    plt.show()\ncloud(sincere_data.question_text, \"Sincere question\")\ncloud(insincere_data.question_text, \"Insincere question\")","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:47:36.383787Z","iopub.execute_input":"2022-01-07T04:47:36.384148Z","iopub.status.idle":"2022-01-07T04:48:00.279645Z","shell.execute_reply.started":"2022-01-07T04:47:36.38411Z","shell.execute_reply":"2022-01-07T04:48:00.27896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Follow some characteristics that can signify that a question is insincere:\n* Has a non-neutral tone\n    * Has an exaggerated tone to underscore a point about a group of people\n    * Is rhetorical and meant to imply a statement about a group of people\n* Is disparaging or inflammatory\n    * Suggests a discriminatory idea against a protected class of people, or seeks confirmation of a stereotype\n    * Makes disparaging attacks/insults against a specific person or group of people\n    * Based on an outlandish premise about a group of people\n    * Disparages against a characteristic that is not fixable and not measurable\n* Isn't grounded in reality\n    * Based on false information, or contains absurd assumptions\n* Uses sexual content (incest, bestiality, pedophilia) for shock value, and not to seek genuine answers\n\nWe can see, in sincere question, common nouns are prominent in the question while proper nouns primarily arise in insincere questions (American, Muslim, Trump,...); also the sexual, discriminatory word (woman, man, white, black, Muslism...)  \n","metadata":{}},{"cell_type":"markdown","source":"## Word n-grams Count Plot \n\nTo analyze closer to the dataset, let's use n-gram to see the most frequent words. It has to be ensure that all of the stopwords need to be eliminated from the counter before take frequent grams. Words are all lowercased before zipping a number of words (based on parameter n_gram value). Finally, a frequency dictionary is created and count all words appear.","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objs as go\nfrom collections import defaultdict\nfrom plotly import tools\nimport plotly.offline as py\n\nstopwords = set(STOPWORDS)\n    \n# N-gram generation\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n# Horizontal bar chart\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\ndef ngram_chart(n_gram = 1):\n    # Get the bar chart from sincere questions #\n    freq_dict = defaultdict(int)\n    for sent in sincere_data[\"question_text\"]:\n        for word in generate_ngrams(sent, n_gram):\n            freq_dict[word] += 1\n    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n    fd_sorted.columns = [\"word\", \"wordcount\"]\n    trace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n    # Get the bar chart from insincere questions #\n    freq_dict = defaultdict(int)\n    for sent in insincere_data[\"question_text\"]:\n        for word in generate_ngrams(sent, n_gram):\n            freq_dict[word] += 1\n    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n    fd_sorted.columns = [\"word\", \"wordcount\"]\n    trace1 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\n    # Creating two subplots\n    fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                              subplot_titles=[\"Frequent words of sincere questions\", \n                                              \"Frequent words of insincere questions\"])\n    fig.append_trace(trace0, 1, 1)\n    fig.append_trace(trace1, 1, 2)\n    fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n    py.iplot(fig, filename='word-plots')\n    return","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:48:00.282148Z","iopub.execute_input":"2022-01-07T04:48:00.282521Z","iopub.status.idle":"2022-01-07T04:48:00.311686Z","shell.execute_reply.started":"2022-01-07T04:48:00.282477Z","shell.execute_reply":"2022-01-07T04:48:00.310856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Unigram count plot\n* Some of the top words are common across both the classes like 'people', 'will', 'think' etc.\n* Many of top words in sincere questions (after excluding the both common ones) used for describe and comparison purpose: 'best', 'good', 'possible', 'need', etc.\n* The other top words in insincere questions (after excluding the both common ones) often involve matters that may be sensitive, controversial, statement: 'chinese', 'women', 'white', 'mulisiam', 'trump', etc.","metadata":{}},{"cell_type":"code","source":"ngram_chart(1)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:48:00.313117Z","iopub.execute_input":"2022-01-07T04:48:00.313409Z","iopub.status.idle":"2022-01-07T04:48:11.347981Z","shell.execute_reply.started":"2022-01-07T04:48:00.313357Z","shell.execute_reply":"2022-01-07T04:48:11.347278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Bigram count plot\n\n* Some of the top bigrams are common across both the classes, especially in sincere questions, people tend to find useful answer for method to do something: 'best way'. Therefore, they may need to provide informative question. So we can see a lot of words from different catergories: 'computer science', 'world war', 'tv shows', etc.\n* The top bigrams in insincere questions still involve matters that may be sensitive or controversial (and significantly related to people, religion, politics) such as 'donald trump', 'white people', 'black people', etc. This observation is quite reasonable and similar to characteristics (provided in the competition description) that can signify that a question is insincere.","metadata":{}},{"cell_type":"code","source":"ngram_chart(2)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:48:11.349429Z","iopub.execute_input":"2022-01-07T04:48:11.349915Z","iopub.status.idle":"2022-01-07T04:48:27.215151Z","shell.execute_reply.started":"2022-01-07T04:48:11.349877Z","shell.execute_reply":"2022-01-07T04:48:27.214367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Trigram count plot**\n\n* We can easily inspect many trigrams are combinations of popular uni- and bigrams, and we can get the characteristic of the 2 types now. So further plot count (n>3) is now unnecessary.\n* More detailed and informational phrases appear: 'black lives matter', 'kim jong un', etc. People's age were mentioned frequently in the questions, especially in insincere questions. Perhaps people really care a lot about characteristics such as physical, psychological, cognitive level of others - traits that are influenced by certain age.","metadata":{}},{"cell_type":"code","source":"ngram_chart(3)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:48:27.217909Z","iopub.execute_input":"2022-01-07T04:48:27.21812Z","iopub.status.idle":"2022-01-07T04:48:43.576954Z","shell.execute_reply.started":"2022-01-07T04:48:27.218092Z","shell.execute_reply":"2022-01-07T04:48:43.576218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"markdown","source":" ## Clean data \n \n* Replace math equations, links by \"MATHEQUATION\", \"URL\"\n* Make abbreviations complete\n* Correcting mispell words\n* Remove punctuation\n\n*I consider that removing stopword and lowercase is not a good idea in classification task. In the context of sentiment analysis, removing stop words can be problematic if context is affected. For example stop word corpus includes ‘not’, which is a negation that can alter the valence of the passage. In addition, proper nouns can have a big effect in context classification as we analysis above (insincere question have many controversial proper nouns such as \"Trump\", \"Indian\", \"Kim Jung Un\", etc. Therefore, the data will contain stopword and uppercase words which is necessary.*","metadata":{}},{"cell_type":"code","source":"contractions= {\"i'm\": 'i am',\"i'm'a\": 'i am about to',\"i'm'o\": 'i am going to',\"i've\": 'i have',\"i'll\": 'i will',\"i'll've\": 'i will have',\"i'd\": 'i would',\"i'd've\": 'i would have',\"Whatcha\": 'What are you',\"amn't\": 'am not',\"ain't\": 'are not',\"aren't\": 'are not',\"'cause\": 'because',\"can't\": 'can not',\"can't've\": 'can not have',\"could've\": 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have',\"daren't\": 'dare not',\"daresn't\": 'dare not',\"dasn't\": 'dare not',\"didn't\": 'did not','didn’t': 'did not',\"don't\": 'do not','don’t': 'do not',\"doesn't\": 'does not',\"e'er\": 'ever',\"everyone's\": 'everyone is',\"finna\": 'fixing to',\"gimme\": 'give me',\"gon't\": 'go not',\"gonna\": 'going to',\"gotta\": 'got to',\"hadn't\": 'had not',\"hadn't've\": 'had not have',\"hasn't\": 'has not',\"haven't\": 'have not',\"he've\": 'he have',\"he's\": 'he is',\"he'll\": 'he will',\"he'll've\": 'he will have',\"he'd\": 'he would',\"he'd've\": 'he would have',\"here's\": 'here is',\"how're\": 'how are',\"how'd\": 'how did',\"how'd'y\": 'how do you',\"how's\": 'how is',\"how'll\": 'how will',\"isn't\": 'is not',\"it's\": 'it is',\"'tis\": 'it is',\"'twas\": 'it was',\"it'll\": 'it will',\"it'll've\": 'it will have',\"it'd\": 'it would',\"it'd've\": 'it would have',\"kinda\": 'kind of',\"let's\": 'let us',\"luv\": 'love',\"ma'am\": 'madam',\"may've\": 'may have',\"mayn't\": 'may not',\"might've\": 'might have',\"mightn't\": 'might not',\"mightn't've\": 'might not have',\"must've\": 'must have',\"mustn't\": 'must not',\"mustn't've\": 'must not have',\"needn't\": 'need not',\"needn't've\": 'need not have',\"ne'er\": 'never',\"o'\": 'of',\"o'clock\": 'of the clock',\"ol'\": 'old',\"oughtn't\": 'ought not',\"oughtn't've\": 'ought not have',\"o'er\": 'over',\"shan't\": 'shall not',\"sha'n't\": 'shall not',\"shalln't\": 'shall not',\"shan't've\": 'shall not have',\"she's\": 'she is',\"she'll\": 'she will',\"she'd\": 'she would',\"she'd've\": 'she would have',\"should've\": 'should have',\"shouldn't\": 'should not',\"shouldn't've\": 'should not have',\"so've\": 'so have',\"so's\": 'so is',\"somebody's\": 'somebody is',\"someone's\": 'someone is',\"something's\": 'something is',\"sux\": 'sucks',\"that're\": 'that are',\"that's\": 'that is',\"that'll\": 'that will',\"that'd\": 'that would',\"that'd've\": 'that would have',\"em\": 'them',\"there're\": 'there are',\"there's\": 'there is',\"there'll\": 'there will',\"there'd\": 'there would',\"there'd've\": 'there would have',\"these're\": 'these are',\"they're\": 'they are',\"they've\": 'they have',\"they'll\": 'they will',\"they'll've\": 'they will have',\"they'd\": 'they would',\"they'd've\": 'they would have',\"this's\": 'this is',\"those're\": 'those are',\"to've\": 'to have',\"wanna\": 'want to',\"wasn't\": 'was not',\"we're\": 'we are',\"we've\": 'we have',\"we'll\": 'we will',\"we'll've\": 'we will have',\"we'd\": 'we would',\"we'd've\": 'we would have',\"weren't\": 'were not',\"what're\": 'what are',\"what'd\": 'what did',\"what've\": 'what have',\"what's\": 'what is',\"what'll\": 'what will',\"what'll've\": 'what will have',\"when've\": 'when have',\"when's\": 'when is',\"where're\": 'where are',\"where'd\": 'where did',\"where've\": 'where have',\"where's\": 'where is',\"which's\": 'which is',\"who're\": 'who are',\"who've\": 'who have',\"who's\": 'who is',\"who'll\": 'who will',\"who'll've\": 'who will have',\"who'd\": 'who would',\"who'd've\": 'who would have',\"why're\": 'why are',\"why'd\": 'why did',\"why've\": 'why have',\"why's\": 'why is',\"will've\": 'will have',\"won't\": 'will not',\"won't've\": 'will not have',\"would've\": 'would have',\"wouldn't\": 'would not',\"wouldn't've\": 'would not have',\"y'all\": 'you all',\"y'all're\": 'you all are',\"y'all've\": 'you all have',\"y'all'd\": 'you all would',\"y'all'd've\": 'you all would have',\"you're\": 'you are',\"you've\": 'you have',\"you'll've\": 'you shall have',\"you'll\": 'you will',\"you'd\": 'you would',\"you'd've\": 'you would have','jan.': 'january','feb.': 'february','mar.': 'march','apr.': 'april','jun.': 'june','jul.': 'july','aug.': 'august','sep.': 'september','oct.': 'october','nov.': 'november','dec.': 'december','I’m': 'I am','I’m’a': 'I am about to','I’m’o': 'I am going to','I’ve': 'I have','I’ll': 'I will','I’ll’ve': 'I will have','I’d': 'I would','I’d’ve': 'I would have','amn’t': 'am not','ain’t': 'are not','aren’t': 'are not','’cause': 'because','can’t': 'can not','can’t’ve': 'can not have','could’ve': 'could have','couldn’t': 'could not','couldn’t’ve': 'could not have','daren’t': 'dare not','daresn’t': 'dare not','dasn’t': 'dare not','doesn’t': 'does not','e’er': 'ever','everyone’s': 'everyone is','gon’t': 'go not','hadn’t': 'had not','hadn’t’ve': 'had not have','hasn’t': 'has not','haven’t': 'have not','he’ve': 'he have','he’s': 'he is','he’ll': 'he will','he’ll’ve': 'he will have','he’d': 'he would','he’d’ve': 'he would have','here’s': 'here is','how’re': 'how are','how’d': 'how did','how’d’y': 'how do you','how’s': 'how is','how’ll': 'how will','isn’t': 'is not','it’s': 'it is','’tis': 'it is','’twas': 'it was','it’ll': 'it will','it’ll’ve': 'it will have','it’d': 'it would','it’d’ve': 'it would have','let’s': 'let us','ma’am': 'madam','may’ve': 'may have','mayn’t': 'may not','might’ve': 'might have','mightn’t': 'might not','mightn’t’ve': 'might not have','must’ve': 'must have','mustn’t': 'must not','mustn’t’ve': 'must not have','needn’t': 'need not','needn’t’ve': 'need not have','ne’er': 'never','o’': 'of','o’clock': 'of the clock','ol’': 'old','oughtn’t': 'ought not','oughtn’t’ve': 'ought not have','o’er': 'over','shan’t': 'shall not','sha’n’t': 'shall not','shalln’t': 'shall not','shan’t’ve': 'shall not have','she’s': 'she is','she’ll': 'she will','she’d': 'she would','she’d’ve': 'she would have','should’ve': 'should have','shouldn’t': 'should not','shouldn’t’ve': 'should not have','so’ve': 'so have','so’s': 'so is','somebody’s': 'somebody is','someone’s': 'someone is','something’s': 'something is','that’re': 'that are','that’s': 'that is','that’ll': 'that will','that’d': 'that would','that’d’ve': 'that would have','there’re': 'there are','there’s': 'there is','there’ll': 'there will','there’d': 'there would','there’d’ve': 'there would have','these’re': 'these are','they’re': 'they are','they’ve': 'they have','they’ll': 'they will','they’ll’ve': 'they will have','they’d': 'they would','they’d’ve': 'they would have','this’s': 'this is','those’re': 'those are','to’ve': 'to have','wasn’t': 'was not','we’re': 'we are','we’ve': 'we have','we’ll': 'we will','we’ll’ve': 'we will have','we’d': 'we would','we’d’ve': 'we would have','weren’t': 'were not','what’re': 'what are','what’d': 'what did','what’ve': 'what have','what’s': 'what is','what’ll': 'what will','what’ll’ve': 'what will have','when’ve': 'when have','when’s': 'when is','where’re': 'where are','where’d': 'where did','where’ve': 'where have','where’s': 'where is','which’s': 'which is','who’re': 'who are','who’ve': 'who have','who’s': 'who is','who’ll': 'who will','who’ll’ve': 'who will have','who’d': 'who would','who’d’ve': 'who would have','why’re': 'why are','why’d': 'why did','why’ve': 'why have','why’s': 'why is','will’ve': 'will have','won’t': 'will not','won’t’ve': 'will not have','would’ve': 'would have','wouldn’t': 'would not','wouldn’t’ve': 'would not have','y’all': 'you all','y’all’re': 'you all are','y’all’ve': 'you all have','y’all’d': 'you all would','y’all’d’ve': 'you all would have','you’re': 'you are','you’ve': 'you have','you’ll’ve': 'you shall have','you’ll': 'you will','you’d': 'you would','you’d’ve': 'you would have'}\nmissing_spell = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'bitcoin', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization','electroneum':'bitcoin','nanodegree':'degree','hotstar':'star','dream11':'dream','ftre':'fire','tensorflow':'framework','unocoin':'bitcoin','lnmiit':'limit','unacademy':'academy','altcoin':'bitcoin','altcoins':'bitcoin','litecoin':'bitcoin','coinbase':'bitcoin','cryptocurency':'cryptocurrency','simpliv':'simple','quoras':'quora','schizoids':'psychopath','remainers':'remainder','twinflame':'soulmate','quorans':'quora','brexit':'demonetized','iiest':'institute','dceu':'comics','pessat':'exam','uceed':'college','bhakts':'devotee','boruto':'anime','cryptocoin':'bitcoin','blockchains':'blockchain','fiancee':'fiance','redmi':'smartphone','oneplus':'smartphone','qoura':'quora','deepmind':'framework','ryzen':'cpu','whattsapp':'whatsapp','undertale':'adventure','zenfone':'smartphone','cryptocurencies':'cryptocurrencies','koinex':'bitcoin','zebpay':'bitcoin','binance':'bitcoin','whtsapp':'whatsapp','reactjs':'framework','bittrex':'bitcoin','bitconnect':'bitcoin','bitfinex':'bitcoin','yourquote':'your quote','whyis':'why is','jiophone':'smartphone','dogecoin':'bitcoin','onecoin':'bitcoin','poloniex':'bitcoin','7700k':'cpu','angular2':'framework','segwit2x':'bitcoin','hashflare':'bitcoin','940mx':'gpu','openai':'framework','hashflare':'bitcoin','1050ti':'gpu','nearbuy':'near buy','freebitco':'bitcoin','antminer':'bitcoin','filecoin':'bitcoin','whatapp':'whatsapp','empowr':'empower','1080ti':'gpu','crytocurrency':'cryptocurrency','8700k':'cpu','whatsaap':'whatsapp','g4560':'cpu','payymoney':'pay money','fuckboys':'fuck boys','intenship':'internship','zcash':'bitcoin','demonatisation':'demonetization','narcicist':'narcissist','mastuburation':'masturbation','trignometric':'trigonometric','cryptocurreny':'cryptocurrency','howdid':'how did','crytocurrencies':'cryptocurrencies','phycopath':'psychopath','bytecoin':'bitcoin','possesiveness':'possessiveness','scollege':'college','humanties':'humanities','altacoin':'bitcoin','demonitised':'demonetized','brasília':'brazilia','accolite':'accolyte','econimics':'economics','varrier':'warrier','quroa':'quora','statergy':'strategy','langague':'language','splatoon':'game','7600k':'cpu','gate2018':'gate 2018','in2018':'in 2018','narcassist':'narcissist','jiocoin':'bitcoin','hnlu':'hulu','7300hq':'cpu','weatern':'western','interledger':'blockchain','deplation':'deflation', 'cryptocurrencies':'cryptocurrency', 'bitcoin':'blockchain cryptocurrency'}\n#Replace math equations, links by \"MATHEQUATION\", \"URL\"\ndef clean_tag(x):\n    if '[math]' in x:\n        x = re.sub('\\[math\\].*?math\\]', 'math equation', x) #replacing with [MATH EQUATION]\n    if 'http' in x or 'www' in x:\n        x = re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', 'url', x) #replacing with [url]\n    return x\n#Make abbreviations complete\ndef contraction_fix(word):\n    try:\n        a=contractions[word]\n    except KeyError:\n        a=word\n    return a\n#Correcting mispell words\ndef misspell_fix(word):\n    try:\n        a=missing_spell[word]\n    except KeyError:\n        a=word\n    return a\n\n\ndef clean_text(text):\n    text = clean_tag(text)\n    text = \" \".join([contraction_fix(w) for w in text.split()]) \n    text = \" \".join([misspell_fix(w) for w in text.split()]) \n    #Remove punctuation\n    text = re.sub(r'[^a-zA-Z0-9]', ' ', text) \n    return text\n\ndef apply_clean_text(question_text):\n    tmp = pd.DataFrame()\n    tmp['question_text'] = question_text;\n    tmp['clean'] = tmp.question_text.progress_map(clean_text)\n    with pd_ctx:\n        display(tmp)\n    return tmp['clean']\n\n\ntrainX_ques = apply_clean_text(df.question_text)\ntestX_ques = apply_clean_text(test_df.question_text)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:48:43.581202Z","iopub.execute_input":"2022-01-07T04:48:43.581463Z","iopub.status.idle":"2022-01-07T04:49:29.770851Z","shell.execute_reply.started":"2022-01-07T04:48:43.581429Z","shell.execute_reply":"2022-01-07T04:49:29.770151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding text to vectors\n\nTokenization is a way of separating a piece of text into smaller units called tokens. Here, each token is a word in sentences. I using `torchtext` to tokenize by go through the words in the entire document to build up a dictionary. Then, the words will be sorted by their frequency of occurrence. The words appear frequently have the lower the index. Then we will use this dictionary to transform each sentence in text form into a sequence of numbers.","metadata":{}},{"cell_type":"markdown","source":"1. Create a dictionary of the dataset\n    The list of special tokens (e.g., padding or eos) that will be prepended to the vocabulary in addition to an \\<unk> token.","metadata":{}},{"cell_type":"code","source":"import torchtext\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer(\"basic_english\")\ndef yield_tokens(data_iter):\n    for text in data_iter:\n        yield tokenizer(text)\n\nvocab = build_vocab_from_iterator(yield_tokens(testX_ques), specials=[\"<unk>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\n\n# len(vocab)\n# vocab(['here', 'is', 'an', 'example'])\n# vocab.lookup_token(1)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:49:29.772067Z","iopub.execute_input":"2022-01-07T04:49:29.772391Z","iopub.status.idle":"2022-01-07T04:49:35.752461Z","shell.execute_reply.started":"2022-01-07T04:49:29.772356Z","shell.execute_reply":"2022-01-07T04:49:35.751737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. Turn each question into a sequence of numbers.","metadata":{}},{"cell_type":"code","source":"# text_pipeline = lambda x: vocab(tokenizer(x))\n# text_pipeline('here is the an example')\n\ndef tokenize_ques(data_iter):\n    text_list = []\n    text_pipeline = lambda x: vocab(tokenizer(x))\n    for text in data_iter:\n        processed_text = text_pipeline(text)\n        text_list.append(processed_text)\n    return text_list\n\nword_sequences = tokenize_ques(testX_ques)\n\nprint(\"Length of 20 first word_sequences:\")\nprint(list(map(lambda x: len(x) ,word_sequences[:20])))\n\nprint(\"\\n20 first word_sequences:\")\nfor sequence in word_sequences[:20]:\n    print(sequence)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:49:35.753855Z","iopub.execute_input":"2022-01-07T04:49:35.754113Z","iopub.status.idle":"2022-01-07T04:49:42.640341Z","shell.execute_reply.started":"2022-01-07T04:49:35.75408Z","shell.execute_reply":"2022-01-07T04:49:42.639624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Padding and Truncating**\nWe can see in tokenize steps, these question don't have same length which can lead to difficult in trainning model. Hence, let's regularize sequences with padding and truncating:\nEach sequences will have the fixed length 60 as we analyze in section Data Analysis - 99.9% sentences have length less than or equal to 54 words.  \n* Padding: if the sequences shorter than the fixed length, adding 0 after the sequences.\n* Truncatting: if the sequences shorter than the fixed length, shorten by remove the balance of the sequences.\n* 'post': padding or truncatting at the end of the word","metadata":{}},{"cell_type":"code","source":"MAX_SENTENCE_LENGTH = 60 \nPADDING_TYPE = 'post' \nTRUNCATE_TYPE = 'post'\ndef create_sequence(word_sequences):\n    padded_word_sequences = pad_sequences(word_sequences, maxlen=MAX_SENTENCE_LENGTH, padding=PADDING_TYPE, truncating=TRUNCATE_TYPE)\n    return padded_word_sequences\npadded_sequences = create_sequence(word_sequences)\n\nprint(\"Array size:\",padded_sequences.shape)\n\nprint(\"Length of 20 first word_sequences:\")\nprint(list(map(lambda x: len(x) ,padded_sequences[:20])))\n\nprint(\"\\n10 first word_sequences:\")\nfor sequence in padded_sequences[:10]:\n    print(sequence)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:49:42.64168Z","iopub.execute_input":"2022-01-07T04:49:42.641918Z","iopub.status.idle":"2022-01-07T04:49:45.24307Z","shell.execute_reply.started":"2022-01-07T04:49:42.641885Z","shell.execute_reply":"2022-01-07T04:49:45.242395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split dataset to valid/train/test set\nIn this section, I defined `QuoraDataset` using data preparing functions above to process the dataset.I also split the dataset into train set and valid set used to give an estimate of model skill while tuning model’s hyperparameters.","metadata":{}},{"cell_type":"code","source":"class QuoraDataset(Dataset):\n    def __init__(self, dataset):\n        #contain all question in data\n        self.text = dataset.question_text\n        #target 0/1 for training data and - len for test and validation\n        self.target = dataset.target if \"target\" in dataset.columns else [-1]*len(dataset)\n        \n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, i):\n        target = [self.target[i]]\n        question = str(self.text[i])\n        question_id = create_sequence([vocab(tokenizer(question))])\n        return FloatTensor(target), question, question_id","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:49:45.244332Z","iopub.execute_input":"2022-01-07T04:49:45.244634Z","iopub.status.idle":"2022-01-07T04:49:45.252792Z","shell.execute_reply.started":"2022-01-07T04:49:45.244599Z","shell.execute_reply":"2022-01-07T04:49:45.251822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data.dataset import random_split\n\nBATCH_SIZE = 1024\n# df = df.sample(n=500, random_state=123).reset_index(drop=True)\nsplit = np.int32(0.8*len(df))\nvalid_data, training_data = df[split:], df[:split]\nvalid_data = valid_data.reset_index(drop=True)\nval_dataset = QuoraDataset(valid_data)\nval_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE,\n                        num_workers=0, shuffle=True)\n\ntraining_data = training_data.reset_index(drop=True)\ntrain_dataset = QuoraDataset(training_data)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE,\n                           num_workers=0, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:49:45.256992Z","iopub.execute_input":"2022-01-07T04:49:45.257221Z","iopub.status.idle":"2022-01-07T04:49:45.328938Z","shell.execute_reply.started":"2022-01-07T04:49:45.257197Z","shell.execute_reply":"2022-01-07T04:49:45.328168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model architecture\nThis section will describe model architectures are experimented to classify insincere questions and improve the evaluation metrics F1 score.","metadata":{}},{"cell_type":"code","source":"EMBEDDING_DIM = 300\nVOCAB_SIZE = len(vocab)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:49:45.330064Z","iopub.execute_input":"2022-01-07T04:49:45.332042Z","iopub.status.idle":"2022-01-07T04:49:45.33718Z","shell.execute_reply.started":"2022-01-07T04:49:45.332009Z","shell.execute_reply":"2022-01-07T04:49:45.335022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LSTM without pretrained - embedding \nThe first architecture used is LSTMs (Long short-term memory). LSTMs is a very special kind of recurrent neural network (RNN) which have powerful performance for many tasks in NLP . LSTMs have capable of learning long-term dependencies which solve the drawback short memory in RNN (conventional RNN have trouble with relating events that too far separated in time) by using gates to control memorizing process. LSTMs have a chain of repeating modules of neural network but has a different structure:\n<div>\n<img src=\"https://2.bp.blogspot.com/-xWdsykP1hUg/WNn2HNEC25I/AAAAAAAADHE/vkWmQl68AT4e70AgwCFPBL4GdKObqUylACLcB/s1600/fig04_2d_LSTM.png\" width=\"600\"/>\n</div>\n\n*In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. The pink circles represent pointwise operations, like vector addition, while the yellow boxes are learned neural network layers. Lines merging denote concatenation, while a line forking denote its content being copied and the copies going to different locations.*\n> RNN (Recurrent Neural Network) is a popular sequence model that has efficient performance for sequential data. RNN's can remember important things about the input they received by processing inputs in a sequential manner, where the context from the previous input is considered when computing the output of the current step. This allows the neural network to carry information over different time steps rather than keeping all the inputs independent of each other. This is why they're the preferred algorithm for sequential data like time series, speech, text, financial data, audio, video, weather and much more. Recurrent neural networks can form a much deeper understanding of a sequence and its context compared to other algorithms.However, a significant shortcoming that plagues the typical RNN is the problem of vanishing.Due to these issues, RNNs are unable to work with longer sequences and hold on to long-term dependencies, making them suffer from “short-term memory”.\n\n> <div>\n<img src=\"https://1.bp.blogspot.com/-6hyAXQfTrXY/WNn2G3CUtbI/AAAAAAAADHA/EaaANM6G1fg460fQccTNmwa8gp9k_IS7wCLcB/s1600/fig04_2c_LSTM.png\" width=\"600\"/>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"Now I define the model architecture that will ingest questions dataset.\nI'll keep it simple:\n* An embedding layer: basically a look-up table that converts each token in the dictionary to a vector with dense representation that will be learn and adjusted throughout training\n* A LSTM layers outputs three things: The consolidated output — of all hidden states in the sequence, Hidden state of the last LSTM unit — the final output, Cell state\n* A Linear layer and Dropout layers with rate 0.2\n\nExpected dimensions: I first pass the input (batch_size x max_len) through embedding layer which return (batch_size x max_len x embedding_size) matrix. Then through LSTM with hidden state will have dimension (batch_size x hidden_dim). Finally, the matrix is passing through linear layer to return the prediction.","metadata":{}},{"cell_type":"code","source":"class LSTM(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = torch.tensor(x).to(device)\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.lstm(x)\n        return self.linear(ht[-1])\n\npure_lstm_model=  LSTM(VOCAB_SIZE, EMBEDDING_DIM, 786)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:49:45.339707Z","iopub.execute_input":"2022-01-07T04:49:45.340638Z","iopub.status.idle":"2022-01-07T04:49:45.678606Z","shell.execute_reply.started":"2022-01-07T04:49:45.340593Z","shell.execute_reply":"2022-01-07T04:49:45.677792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pure_lstm_model","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:49:45.679879Z","iopub.execute_input":"2022-01-07T04:49:45.680138Z","iopub.status.idle":"2022-01-07T04:49:45.688101Z","shell.execute_reply.started":"2022-01-07T04:49:45.680105Z","shell.execute_reply":"2022-01-07T04:49:45.687323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### BiLSTM with pre-trained Word embedding \nI tend to improve the simple LSTMs model with pretrained Glove word-vectors and use Bidirectional LSTM. Instead of training word embeddings, we can use pre-trained Glove word vectors that have been trained on massive corpus and probably have better context captured and in addition, using Bidirectional LSTM making any neural network context to have the sequence information in both directions backwards (future to past) or forward(past to future).","metadata":{}},{"cell_type":"markdown","source":"1. Loading pretrained Glove word embedding","metadata":{}},{"cell_type":"code","source":"GLOVE_FILE = 'glove.840B.300d/glove.840B.300d.txt'\n# PARAGRAM_FILE =  'paragram_300_sl999/paragram_300_sl999.txt'\n# WIKI_FILE = 'wiki-news-300d-1M/wiki-news-300d-1M.vec'\n!unzip -n /kaggle/input/quora-insincere-questions-classification/embeddings.zip {GLOVE_FILE} -d .\n# !unzip -n /kaggle/input/quora-insincere-questions-classification/embeddings.zip {PARAGRAM_FILE} -d .\n# !unzip -n /kaggle/input/quora-insincere-questions-classification/embeddings.zip {WIKI_FILE} -d .\n\ndef load_glove_vectors(glove_file=GLOVE_FILE):\n    \"\"\"Load the glove word vectors\"\"\"\n    word_vectors = {}\n    with open(glove_file) as f:\n        for line in f:\n            split = line.split(\" \")\n            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n    return word_vectors\n\n# word_vecs = load_glove_vectors(GLOVE_FILE)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:40.10558Z","iopub.execute_input":"2022-01-07T04:52:40.106111Z","iopub.status.idle":"2022-01-07T04:57:05.776642Z","shell.execute_reply.started":"2022-01-07T04:52:40.106072Z","shell.execute_reply":"2022-01-07T04:57:05.775743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_emb_matrix(pretrained, vocab, emb_size = 300):\n    \"\"\" Creates embedding matrix from word vectors\"\"\"\n    vocab_size = len(vocab) + 2\n    vocab_to_idx = {}\n    dic = [\"\", \"UNK\"]\n    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n#     dic[\"UNK\"] = 1\n    i = 2\n    for i in range(vocab_size-2):\n        word = vocab.lookup_token(i)\n        if word in word_vecs:\n            W[i+2] = word_vecs[word]\n        else:\n            W[i+2] = np.random.uniform(-0.25,0.25, emb_size)\n        vocab_to_idx[word] = i+2\n        dic.append(word)\n        i = i+ 1   \n    return W, np.array(dic), vocab_to_idx","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:57:05.77875Z","iopub.execute_input":"2022-01-07T04:57:05.779281Z","iopub.status.idle":"2022-01-07T04:57:05.787851Z","shell.execute_reply.started":"2022-01-07T04:57:05.779242Z","shell.execute_reply":"2022-01-07T04:57:05.78716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pretrained_weights, dictionary, vocab2index = get_emb_matrix(word_vecs, vocab)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:57:56.503225Z","iopub.execute_input":"2022-01-07T04:57:56.503579Z","iopub.status.idle":"2022-01-07T04:57:56.554177Z","shell.execute_reply.started":"2022-01-07T04:57:56.503524Z","shell.execute_reply":"2022-01-07T04:57:56.553257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2. BiLSTM with pre-trained Glove word-vectors:\n   *In BiLSTM output we have hidden state from backwards and forwards.Therefore, we have to cat two tensor to the next layer*\n<div>\n<img src=\"https://www.researchgate.net/publication/343981315/figure/fig3/AS:938328841015298@1600726437710/Structure-of-bidirectional-long-short-term-memory-LSTM.png\" width=\"600\"/>\n</div>\n","metadata":{}},{"cell_type":"code","source":"class LSTM_glove_vecs(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n        self.embeddings.weight.requires_grad = False ## freeze embeddings\n        self.dropout = nn.Dropout(0.2)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = 2, batch_first=True, bidirectional = True)\n        self.linear = nn.Linear(hidden_dim*2, 1)   \n    def forward(self, x):\n        x = torch.tensor(x).to(device)\n        x = self.embeddings(x) #size: 1024, 60, 300 - batch_size x leng_sentence x embedding_size\n        lstm_out, (ht, ct) = self.lstm(x) #ht_size: 2, 1024, 128 - batch_size x hidden_size\n        x = torch.cat([ht[0],ht[-1]],dim=1) #shape: 1024, 256\n        return self.linear(x)\n\n# lstm_glove_model = LSTM_glove_vecs(VOCAB_SIZE+2, 300, 128, pretrained_weights)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:57:05.843192Z","iopub.status.idle":"2022-01-07T04:57:05.845821Z","shell.execute_reply.started":"2022-01-07T04:57:05.845576Z","shell.execute_reply":"2022-01-07T04:57:05.845604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RoBERTa - Pretrained Model\nI try the SOTA in NLP model - using powerful performance of pretrained model RoBERTa to train the dataset.\n\nRoBERTa stands for Robustly Optimized BERT Pre-training Approach. This model optimize the training of BERT architecture in order to take lesser time during pre-training.RoBERTa has almost similar architecture as compare to BERT, but in order to improve the results on BERT architecture, the Next Sentence Prediction (NSP) objective is removed, the model is trained with bigger batch sizes & longer sequences, dynamically changing the masking pattern.\n\n> BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. BERT was pretrained on two tasks: language modelling (15% of tokens were masked and BERT was trained to predict them from context) and next sentence prediction (BERT was trained to predict if a chosen next sentence was probable or not given the first sentence). As a result of the training process, BERT learns contextual embeddings for words. After pretraining, which is computationally expensive, BERT can be finetuned with less resources on smaller datasets to optimize its performance on specific tasks.\n<div>\n<img src=\"https://s3.us-west-2.amazonaws.com/secure.notion-static.com/2015b7e3-bb9f-4e07-a888-77b56a405a37/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220106%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220106T125505Z&X-Amz-Expires=86400&X-Amz-Signature=dd71699317b9310bc0d8cdc3ae80ecb35dc990f33c3ae8ac27a18597f2b83e9b&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject\" width=\"600\"/>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"Pretraining model need to import from `transformers`, then I have to define a new dataset for RoBerta `RoBertaDataset` to add new parameter `attention_mask` to emphasize which token to focus on because the sequence include both real token (will have `attention_mask = 1`) and padding token (`attention_mask = 0`)","metadata":{}},{"cell_type":"code","source":"import transformers\nfrom transformers import AutoModel, AutoTokenizer\ntransformers.__version__\nmodel_name = 'roberta-base'\n# instantiate model & tokenizer\n# model     = AutoModel.from_pretrained(model_name)\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\nclass RoBertaDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        #contain all question in data\n        self.text = data.question_text\n        #initialize tokenizer for dataset\n        self.data, self.tokenizer = data, tokenizer\n        #target 0/1 for training data and - len for test and validation\n        self.target = data.target if \"target\" in data.columns else [-1]*len(data)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, i):\n        pg, tg = 'post', 'post'\n        target = [self.target[i]]\n        question = str(self.text[i])\n        quest_ids = self.tokenizer.encode(question.strip())\n        #padding sentence\n        attention_mask_idx = len(quest_ids) - 1\n        # start of the sentence is 0\n        if 0 not in quest_ids: quest_ids = 0 + quest_ids\n        quest_ids = pad([quest_ids], maxlen=MAX_SENTENCE_LENGTH, value=1, padding=pg, truncating=tg)\n        #emphasize which token to focus on \n        attention_mask = np.zeros(MAX_SENTENCE_LENGTH)\n        attention_mask[1:attention_mask_idx] = 1\n        attention_mask = attention_mask.reshape((1, -1))\n        if 2 not in quest_ids: quest_ids[-1], attention_mask[-1] = 2, 0\n        return FloatTensor(target), LongTensor(quest_ids), LongTensor(attention_mask)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.454773Z","iopub.status.idle":"2022-01-07T04:52:30.455649Z","shell.execute_reply.started":"2022-01-07T04:52:30.455404Z","shell.execute_reply":"2022-01-07T04:52:30.455429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split = np.int32(0.8*len(dfta))\n# robert_valid_data, robert_training_data = dfta[split:], dfta[:split]\n# robert_valid_data = robert_valid_data.reset_index(drop=True)\n# robert_val_dataset = RoBertaDataset(robert_valid_data, tokenizer)\n# robert_val_loader = DataLoader(dataset=robert_val_dataset, batch_size=1024,\n#                         num_workers=0, shuffle=True)\n\n# robert_training_data = robert_training_data.reset_index(drop=True)\n# robert_train_dataset = RoBertaDataset(robert_training_data, tokenizer)\n# robert_train_loader = DataLoader(dataset=robert_train_dataset, batch_size=128,\n#                            num_workers=0, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.456687Z","iopub.status.idle":"2022-01-07T04:52:30.457543Z","shell.execute_reply.started":"2022-01-07T04:52:30.457287Z","shell.execute_reply":"2022-01-07T04:52:30.457311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Roberta model will have: roBERTa layer with its pretrained weights will be fine-tuned when feed through the model and add a custom (Dropout + Linear) head at the top to turn it into a binary text classifier. These custom layers will be trained from scratch.","metadata":{}},{"cell_type":"code","source":"class Roberta(nn.Module):\n    def __init__(self):\n        super(Roberta, self).__init__()\n        self.dropout = nn.Dropout(0.2)\n        self.linear = nn.Linear(768, 1)\n        self.roberta = AutoModel.from_pretrained(model_path_or_name)\n\n    def forward(self, input_token, att_mask):\n        input_token = input_token.view(-1, MAX_SENTENCE_LENGTH)\n        _, self.feat = self.roberta(input_token, att_mask, return_dict=False)\n        self.feat = self.dropout(self.feat)\n        return self.linear(self.feat)\n\n# roberta_model = Roberta()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.458578Z","iopub.status.idle":"2022-01-07T04:52:30.459293Z","shell.execute_reply.started":"2022-01-07T04:52:30.459057Z","shell.execute_reply":"2022-01-07T04:52:30.459082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training ","metadata":{}},{"cell_type":"markdown","source":"### Using F1 score\nData analysis show that the dataset for training is imbalanced. Therefore using accuracy as a performance metric can be achieved wrong evaluation of the model. Therefore,F1 score is a suitable measure of models tested with this imbalance classification datasets.\nFormula of F1: \n\n$F1 \\textrm{Score} = 2*\\frac{\\textrm{Precision*Recall}}{\\textrm{Precision+Recall}}$\n\nWhere $\\textrm{Recall} = \\frac{\\textrm{#True Positives}}{\\textrm{Relevant items}}$\n\nand $\\textrm{Precision} = \\frac{\\textrm{#True Positives}}{\\textrm{Total Positives}}$\n\nThese 2 elements can be represent in this picture:\n\n<div>\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" width=\"300\"/>\n</div>","metadata":{}},{"cell_type":"code","source":"def f1_score(y_pred, y_true):\n    y_true = y_true.squeeze()\n    y_pred = torch.round(nn.Sigmoid()(y_pred)).squeeze()\n    tp = (y_true * y_pred).sum().to(torch.float32)\n    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n    epsilon = 1e-7\n    recall = tp / (tp + fn + epsilon)\n    precision = tp / (tp + fp + epsilon)\n    return 2*(precision*recall) / (precision + recall + epsilon)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:57:05.847279Z","iopub.status.idle":"2022-01-07T04:57:05.847941Z","shell.execute_reply.started":"2022-01-07T04:57:05.84769Z","shell.execute_reply":"2022-01-07T04:57:05.847716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GPU ","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:57:05.849173Z","iopub.status.idle":"2022-01-07T04:57:05.849798Z","shell.execute_reply.started":"2022-01-07T04:57:05.849566Z","shell.execute_reply":"2022-01-07T04:57:05.849591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training \nI selected the learning rate 0.01 and batch size of 1024 for LSTMs model and 128 for RoBerta (because the model is quite big, training with big size make the gpu run out of memory). The valid set will train on bigger batch_size since it don't need gradient calculation.BCE is the loss function which is commonly used in binary classification tasks.\n","metadata":{}},{"cell_type":"code","source":"LEARNING_RATE = 0.001\nNUM_EPOCHS = 10\nMODEL_SAVE_PATH = 'insincerity_model.pt'\n\nglobal val_f1s; global train_f1s\nglobal val_losses; global train_losses\nglobal metric_lists\n\ndef train_quoraModel(model, train_loader, valid_loader):\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE) \n    val_losses, val_f1s = [], []\n    train_losses, train_f1s = [], []\n    model.to(device)\n    for epoch in range(NUM_EPOCHS):\n        print(\"EPOCH :\" + str(epoch+1))\n        batch = 1\n        model.train()  \n        for train_batch in tqdm(train_loader):\n            train_targ, train_ques, train_id = train_batch\n            train_targ = train_targ.to(device)\n            train_id = train_id.to(device)\n            train_preds = model.forward(train_id.squeeze(dim=1))\n            #for RoBerta model\n            #train_preds = model.forward(train_ques, train_id)\n            train_preds = train_preds.to(device)\n            train_loss = criterion(train_preds, train_targ)\n            train_f1 = f1_score(train_preds, train_targ)\n            f1 = np.round(train_f1.item(), 3)\n            optimizer.zero_grad()\n            train_loss.backward()\n            optimizer.step()\n            batch = batch + 1\n            if (batch + 1) % 100 == 0:\n                print(\n                    f\"Step [{batch + 1}], \"\n                    f\"F1Score [{f1}], \"\n                    f\"Loss: {train_loss.item():.4f}\"\n                )\n        val_loss, val_f1, val_points = 0, 0, 0\n\n        model.eval()\n        with torch.no_grad():\n            for val_batch in val_loader:\n                val_targ, val_ques, val_id = val_batch\n                val_targ = val_targ.to(device)\n                val_id = val_id.to(device)\n                val_preds = model.forward(val_id.squeeze(dim=1))\n                #for Roberta model\n                #val_preds = model.forward(val_ques, val_id)\n                val_points = val_points + len(val_targ)\n                val_loss = val_loss + criterion(val_preds, val_targ).item()\n                val_f1 = val_f1 + f1_score(val_preds, val_targ.squeeze(dim=1)).item()*len(val_preds)\n        val_f1 = val_f1/ val_points\n        val_loss = val_loss/ val_points\n        val_f1s.append(val_f1); train_f1s.append(train_f1.item())\n        val_losses.append(val_loss); train_losses.append(train_loss.item())\n    print(\"END TRAINING\")\n    \n    torch.save(model.state_dict(), MODEL_SAVE_PATH); del model; gc.collect()\n\n    metric_lists = [val_losses, train_losses, val_f1s, train_f1s]\n    metric_names = ['val_loss_', 'train_loss_', 'val_f1_', 'train_f1_']\n    for i, metric_list in enumerate(metric_lists):\n        for j, metric_value in enumerate(metric_list):\n            torch.save(metric_value, metric_names[i] + str(j) + '.pt')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-07T04:57:05.851048Z","iopub.status.idle":"2022-01-07T04:57:05.851673Z","shell.execute_reply.started":"2022-01-07T04:57:05.851427Z","shell.execute_reply":"2022-01-07T04:57:05.851452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quoraTrainning = train_quoraModel(pure_lstm_model, train_loader, val_loader)\n# quoraTrainning = train_quoraModel(lstm_glove_model, train_loader, val_loader)\n# from keras.preprocessing.sequence import pad_sequences as pad\n# quoraTrainning = train_quoraModel(roberta_model, robert_train_loader, robert_val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:57:05.852857Z","iopub.status.idle":"2022-01-07T04:57:05.853456Z","shell.execute_reply.started":"2022-01-07T04:57:05.853223Z","shell.execute_reply":"2022-01-07T04:57:05.853247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experimental results report","metadata":{}},{"cell_type":"code","source":"# val_f1s = [0] + [metric_value for metric_value in metric_lists[2]]\n# train_f1s = [0] + [metric_value for metric_value in metric_lists[3]]\n# val_losses = [0.25] + [metric_value for metric_value in metric_lists[0]]\n# train_losses = [0.25] + [metric_value for metric_value in metric_lists[1]]\nval_f1s = [0] + [torch.load('val_f1_{}.pt'.format(i)) for i in range(NUM_EPOCHS)]\n\ntrain_f1s = [0] + [torch.load('train_f1_{}.pt'.format(i)) for i in range(NUM_EPOCHS)]\nval_losses = [0.25] + [torch.load('val_loss_{}.pt'.format(i)) for i in range(NUM_EPOCHS)]\ntrain_losses = [0.25] + [torch.load('train_loss_{}.pt'.format(i)) for i in range(NUM_EPOCHS)]","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.468101Z","iopub.status.idle":"2022-01-07T04:52:30.468518Z","shell.execute_reply.started":"2022-01-07T04:52:30.468283Z","shell.execute_reply":"2022-01-07T04:52:30.468306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(val_losses)+1),\n                         y=val_losses, mode=\"lines+markers\", name=\"val\",\n                         marker=dict(color=\"indianred\", line=dict(width=.5,\n                                                                  color='rgb(0, 0, 0)'))))\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(train_losses)+1),\n                         y=train_losses, mode=\"lines+markers\", name=\"train\",\n                         marker=dict(color=\"darkorange\", line=dict(width=.5,\n                                                                   color='rgb(0, 0, 0)'))))\n\nfig.update_layout(xaxis_title=\"Epochs\", yaxis_title=\"Binary Cross Entropy\",\n                  title_text=\"Binary Cross Entropy vs. Epochs\", template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.469797Z","iopub.status.idle":"2022-01-07T04:52:30.470201Z","shell.execute_reply.started":"2022-01-07T04:52:30.469986Z","shell.execute_reply":"2022-01-07T04:52:30.470008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(val_f1s)+1),\n                         y=val_f1s, mode=\"lines+markers\", name=\"val\",\n                         marker=dict(color=\"indianred\", line=dict(width=.5,\n                                                                  color='rgb(0, 0, 0)'))))\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(train_f1s)+1),\n                         y=train_f1s, mode=\"lines+markers\", name=\"train\",\n                         marker=dict(color=\"darkorange\", line=dict(width=.5,\n                                                                   color='rgb(0, 0, 0)'))))\n\nfig.update_layout(xaxis_title=\"Epochs\", yaxis_title=\"F1 Score\",\n                  title_text=\"F1 Score vs. Epochs\", template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.471563Z","iopub.status.idle":"2022-01-07T04:52:30.471976Z","shell.execute_reply.started":"2022-01-07T04:52:30.471753Z","shell.execute_reply":"2022-01-07T04:52:30.471774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_insincerity(question, network):\n    pg, tg = 'post', 'post'\n    ins = {0: 'sincere', 1: 'insincere'}\n    print(question.strip())\n    quest_id = create_sequence([vocab(tokenizer(question))])\n    quest_id = torch.tensor(quest_id).to(device)\n#     print(quest_id)\n    network.to(device)\n    output = network.forward(quest_id)\n    return ins[int(np.round(nn.Sigmoid()(output.detach().cpu()).item()))]\n\nprint(predict_insincerity(\"How can I train roBERTa base on TPUs?\", pure_lstm_model))\nprint(predict_insincerity(\"Why is that stupid man the biggest dictator in the world?\", pure_lstm_model))","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.473146Z","iopub.status.idle":"2022-01-07T04:52:30.473933Z","shell.execute_reply.started":"2022-01-07T04:52:30.473691Z","shell.execute_reply":"2022-01-07T04:52:30.473715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The simple LSTMs result is showed above. \nThe simple LSTMs get the best score 0.60133 seem to get better result than more complex BiLSTMs with the best score only 0.59415\n![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/4e861996-1068-4256-82bc-2f0953a0f9e0/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220107%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220107T043835Z&X-Amz-Expires=86400&X-Amz-Signature=76db11d7319af280e2cbd0997e5a163c6257478a19ca593a43ecde26bc0c413a&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/adf13b8d-2fef-4c78-8c0a-d7b9de78ecd2/newplot_%284%29.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220107%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220107T041942Z&X-Amz-Expires=86400&X-Amz-Signature=012beb12cb1f4e8d787a85ebe5ce8ef5327d8a0ebc4de988a166d354d887d9d4&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22newplot%2520%284%29.png%22&x-id=GetObject)","metadata":{}},{"cell_type":"markdown","source":"RoBERTa run on GPU take 5 hours to complete. But the model converges to around 0.7 F1 Score nearly reach to the leaderboard score with only 3 layers. But in this pretrained model isn't included in provided kaggle competition. Hence this model will not allowed to submit in the competition.\n![](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/b8d38bb7-6e79-4647-9de0-90dee1afe140/newplot_%283%29.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220106%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220106T134914Z&X-Amz-Expires=86400&X-Amz-Signature=79ad5a024648918f07c330c4aee45b4378acdba155d0213a52990182490bf05c&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22newplot%2520%283%29.png%22&x-id=GetObject)","metadata":{}},{"cell_type":"markdown","source":"# Run on the test data","metadata":{}},{"cell_type":"code","source":"pure_lstm_model.eval()\ntest_preds = []\n\ntest_dataset = QuoraDataset(test_df)\ntest_loader = tqdm(DataLoader(test_dataset, batch_size=256))\ndef sigmoid(x):\n    return 1/(1 + np.exp(-x))\nwith torch.no_grad():\n    for batch in test_loader:\n        test_targ, test_ques, test_id = batch\n#         print(test_id)\n        test_pred = pure_lstm_model.forward(test_id.squeeze(dim=1))\n        test_preds.extend(test_pred.squeeze().detach().cpu().numpy())\n\ntest_preds = np.int32(np.round(sigmoid(np.array(test_preds))))","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.475315Z","iopub.status.idle":"2022-01-07T04:52:30.475724Z","shell.execute_reply.started":"2022-01-07T04:52:30.475509Z","shell.execute_reply":"2022-01-07T04:52:30.475531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = '../input/quora-insincere-questions-classification/'\nsample_submission = pd.read_csv(path + 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.477032Z","iopub.status.idle":"2022-01-07T04:52:30.477425Z","shell.execute_reply.started":"2022-01-07T04:52:30.477211Z","shell.execute_reply":"2022-01-07T04:52:30.477232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.prediction = test_preds","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.478738Z","iopub.status.idle":"2022-01-07T04:52:30.47914Z","shell.execute_reply.started":"2022-01-07T04:52:30.478926Z","shell.execute_reply":"2022-01-07T04:52:30.478947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.480695Z","iopub.status.idle":"2022-01-07T04:52:30.481103Z","shell.execute_reply.started":"2022-01-07T04:52:30.480889Z","shell.execute_reply":"2022-01-07T04:52:30.480911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-01-07T04:52:30.482499Z","iopub.status.idle":"2022-01-07T04:52:30.482906Z","shell.execute_reply.started":"2022-01-07T04:52:30.482681Z","shell.execute_reply":"2022-01-07T04:52:30.482703Z"},"trusted":true},"execution_count":null,"outputs":[]}]}