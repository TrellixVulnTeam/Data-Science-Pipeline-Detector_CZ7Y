{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nimport time\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_validate\nimport time\nfrom scipy import sparse\nimport matplotlib.pyplot as plt\nfrom matplotlib.legend_handler import HandlerLine2D\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import LinearSVC\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\n\n\nps = PorterStemmer()\n\n\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer\n\n    def __call__(self, wo):\n        return [self.wnl.lemmatize(z) for z in word_tokenize(wo)]\n\n\nclass Stemtfidf(TfidfVectorizer):\n    def build_analyzer(self):\n        analyzer = super(TfidfVectorizer, self).build_analyzer()\n        return lambda doc: (ps.stem(w) for w in analyzer(doc))\n\n\ndef vector(input, input2):\n    blorp = Stemtfidf(max_features=20000, tokenizer=nltk.word_tokenize, stop_words=\"english\", ngram_range=(1, 4))\n    tfidf = blorp.fit_transform(input['question_text'])\n    print(tfidf)\n    sparse.save_npz(\"vector.npz\", tfidf)\n    tfidf2 = blorp.transform(input2['question_text'])\n    sparse.save_npz(\"vector2.npz\", tfidf2)\n    return tfidf, tfidf2\n\n\ndef plot(train_results, temp):\n    train_results = np.asarray(train_results)\n    print(train_results)\n    print(max(train_results))\n    print(np.argmax(train_results))\n    line1, = plt.plot(temp, train_results, 'b', label=\"train\")\n    plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n    plt.ylabel('f-score')\n    plt.xlabel('alpha')\n    plt.show()\n\n\ndef n_estimate(tempy, tempy2):\n    tra = []\n    n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n    for x in range(len(n_estimators)):\n        f = n_estimators[x]\n        rf1 = RandomForestClassifier(n_estimators=f)\n        rf1.fit(tempy, tempy2)\n        tra.append(np.mean(cross_val_score(rf1, tempy, tempy2, cv=5)))\n        print(x)\n    plot(tra, n_estimators)\n\n\ndef lr(x_t, y_t, x_tes, y_tes):\n    tra = []\n    learn = [.01, .05, .1, .2, .3]\n    for x in range(len(learn)):\n        f = learn[x]\n        boost = xgb.XGBClassifier(max_depth=100, n_estimators=100, learning_rate=f, colsample_bytree=.72,\n                                  reg_alpha=4, objective='binary:logistic', subsample=0.75, n_jobs=-1).fit(x_t,\n                                                                                                                    y_t)\n        prediction = boost.predict(x_tes)\n        tra.append(f1_score(y_tes, prediction, average='macro'))\n        print(x)\n    plot(tra, learn)\n\n\ndef reg(x_t, y_t, x_tes, y_tes):\n    tra = []\n    reg_a = [1, 2, 3, 4]\n    for x in range(len(reg_a)):\n        f = reg_a[x]\n        boost = xgb.XGBClassifier(max_depth=100, n_estimators=100, learning_rate=0.2, colsample_bytree=.72,\n                                  reg_alpha=f, objective='binary:logistic', subsample=0.75, n_jobs=-1).fit(x_t,\n                                                                                                                    y_t)\n        prediction = boost.predict(x_tes)\n        tra.append(f1_score(y_tes, prediction, average='macro'))\n        print(x)\n    plot(tra, reg_a)\n\n\ndef alpha(x_t, y_t, x_tes, y_tes):\n    tra = []\n    alp = [.00001, .0001, .001, .01, .1, 1, 10, 100]\n    for x in range(len(alp)):\n        f = alp[x]\n        bnb = BernoulliNB(alpha=f)\n        bnb.fit(x_t, y_t)\n        prediction = bnb.predict(x_tes)\n        tra.append(f1_score(y_tes, prediction, average='macro'))\n        print(x)\n    plot(tra, alp)\n\n\ndef Cs(x_t, y_t, x_tes, y_tes):\n    tra = []\n    c = [.001, .01, .1, 1, 10, 100]\n    for x in range(len(c)):\n        f = c[x]\n        svm1 = LinearSVC(C=f)\n        svm1.fit(x_t, y_t)\n        prediction = svm1.predict(x_tes)\n        tra.append(f1_score(y_tes, prediction, average=\"macro\"))\n        print(x)\n    plot(tra, c)\n\n\ndef svm(x_t, y_t, z_t):\n    clf = LinearSVC(C=10, class_weight='balanced')\n    clf.fit(x_t, y_t)\n    return clf.predict(z_t)\n\n\ndef randforest(x_t, y_t, z_t):\n    rf = RandomForestClassifier(n_estimators=100, max_depth=100, n_jobs=-1)\n    rf.fit(x_t, y_t)\n    return rf.predict(z_t)\n\n\ndef gradboost(x_t, y_t, z_t):\n    xgb_model = xgb.XGBClassifier(max_depth=6, n_estimators=100, learning_rate=0.2, colsample_bytree=.72,\n                                  reg_alpha=3, objective='binary:logistic', subsample=0.75, n_jobs=-1).fit(x_t,\n                                                                                                           y_t)\n    return xgb_model.predict(z_t)\n\n\ndef MNB(x, y, z):\n    mnb = MultinomialNB(alpha=.0001)\n    mnb.fit(x, y)\n    return mnb.predict(z)\n\n\ndef naive(x, y, z):\n    nb = BernoulliNB(alpha=.0001)\n    nb.fit(x, y)\n    return nb.predict(z)\n\n\ndef linreg(x_t, y_t, z_t):\n    rege = LogisticRegression()\n    rege.fit(x_t, y_t)\n    return rege.predict(z_t)\n\n\ndef mlp():\n    clf = MLPClassifier()\n    clf.fit(x_train, y_train)\n    return clf.predict(x_test)\n    # parameters = {'solver': ['lbfgs', 'adam', 'sgd'], 'max_iter': [200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200],\n    #               'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes': (10,)}\n    # clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=1)\n    # clf.fit(x_train, y_train)\n    # print(clf.score(x_train, y_train))\n    # print(clf.best_params_)=\n\n\ndef resample():\n    return SMOTE(n_jobs=-1).fit_resample(x_train, y_train)\n\n\ntest = pd.read_csv('../input/quora-insincere-questions-classification/test.csv')\nqid = test\ntest = test.drop('qid', axis=1)\ntrain = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\ntrain = train.drop('qid', axis=1)\nprint(train.keys())\ntarget = train['target']\nprint(target.sum())\n\ntfidf_train, tfidf_test = vector(train, test)\n# tfidf_train = sparse.load_npz(\"vector.npz\")\n# tfidf_test = sparse.load_npz(\"vector2.npz\")\nprint(tfidf_train.shape)\nx_train, x_test, y_train, y_test = train_test_split(tfidf_train, target, test_size=0.3, random_state=27)\n# x_resmamp, y_resamp = resample()\n# n_estimate(x_train, y_train)\n# lr(x_train, y_train, x_test, y_test)\n# reg(x_train, y_train, x_test, y_test)\n# alpha(x_train, y_train, x_test, y_test)\n# Cs(x_train, y_train, x_test, y_test)\n\nprint(\"starting predict\")\nstart = time.time()\n# testy = naive(tfidf_train, target, tfidf_test)\ntesty = MNB(tfidf_train, target, tfidf_test)\n# testy = gradboost(tfidf_train, target, tfidf_test)\n# testy = linreg(tfidf_train, target, tfidf_test)\n# testy = svm(tfidf_train, target, tfidf_test)\n# print(testy)\n# np.savetxt(\"numpy.txt\", testy, fmt=\"%d\")\n# tes = naive(x_train, y_train, x_train)\n# tes = linreg()\n# tes = gradboost(x_train, y_train)\n# tes = svm(x_train, y_train)\n# tes = randforest(x_resmamp, y_resamp)\n# tes = mlp()\n# tes = naive()\nprint(time.time() - start)\nqid = qid.drop('question_text', axis=1)\nqid['prediction'] = testy\nprint(\"to_csv\")\nqid.to_csv('submission.csv', index=False)\n\n# print(confusion_matrix(y_test, tes))\n# print(\"accuracy\")\n# print(accuracy_score(y_test, tes))\n# print(\"f1\")\n# print(f1_score(y_test, tes, average='macro'))\n# print(classification_report(y_test, tes))\n# print(\"precision\")\n# print(precision_score(y_test, tes))\n# print(\"recall\")\n# print(recall_score(y_test, tes))\n# print(\"cross\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}