{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport pandas as pd\npd.set_option(\"display.max_columns\",None)\ntrain_df= pd.read_csv(\"../input/quora-insincere-questions-classification/train.csv\",index_col=False)\ntest_df = pd.read_csv(\"../input/quora-insincere-questions-classification/test.csv\",index_col=False)\ntest_id= test_df['qid']\n\ntarget = train_df['target']\ntrain_df = train_df.drop(['target'],axis=1)\n\ndf = pd.concat([train_df,test_df],axis=0)\ndf.head()\ndf.tail()\ndf.isnull().sum()\ndf.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import spacy\nnlp =spacy.load('en_core_web_lg')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['question_text']=df['question_text'].apply(lambda x: x.lower())\ndf['question_text']=df['question_text'].apply(lambda x: re.sub(\"[^a-z A-Z 0-9-]+\",'',x))\ndf['question_text']=df['question_text'].apply(lambda x: \" \".join(x.split()))\n\nimport unicodedata\ndef remove_accented_chars(x):\n    x = unicodedata.normalize('NFKD',x).encode('ascii','ignore').decode('utf-8','ignore')\n    return x\n\n# df['question_text']=df['question_text'].apply(remove_accented_chars(str(df['question_text'])))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lemmatization\ndef make_to_base(x):\n    x_list = []\n    docs=nlp.pipe(x, batch_size=32, disable=[\"parser\", \"ner\"])\n    for doc in docs:\n        lemma= [tok.lemma_ for tok in doc]\n#        print(lemma)\n        if lemma=='-PRON-' or lemma=='be':\n            lemma=lemma.text\n        x_list.append(lemma)\n    # return \"\".join(str(x_list))\n    # return \"\".join(str(x_list))\n    return x_list\n\n# x=['hi makes would john','dadsa']\n#make_to_base(x)\n\ndf['question_text'] = (make_to_base(df['question_text']))\n\ndf['question_text']=df['question_text'].apply(lambda x: re.sub(\"[^a-z A-Z]+\",'',str(x)))\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape[0]\nx = df.iloc[:train_df.shape[0],1]\ntest = df.iloc[train_df.shape[0]:,1]\ny = target\nx.shape,test.shape,y.shape\ntrain_df.shape,test_df.shape,target.shape\n# y = df.iloc[:,2]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(x,y,test_size=0.2,shuffle=True)\ntrain_x.shape, test_x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n# tfidf = TfidfVectorizer(ngram_range=(1,4),min_df=3,max_df=0.9,strip_accents='unicode',use_idf=True,smooth_idf=True,sublinear_tf=True,stop_words='english')\ntfidf = TfidfVectorizer(min_df=3,max_df=0.9,strip_accents='unicode',use_idf=True,smooth_idf=True,sublinear_tf=True)\ntfidf.fit(train_x)\ntrain_x= tfidf.transform(train_x)\ntest_x= tfidf.transform(test_x)\ntest= tfidf.transform(test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n#classifier = LogisticRegression(solver='lbfgs', dual=False, class_weight='balanced', C=0.5, max_iter=40)\nclassifier = LogisticRegression(max_iter=800)\n\nclassifier.fit(train_x, train_y)\n\nclassifier.score(train_x, train_y), classifier.score(test_x, test_y)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predicted= classifier.predict(test)\npredicted[1]\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df=pd.read_csv('../input/quora-insincere-questions-classification/sample_submission.csv',index_col=False)\nsample_df.head(2)\n\ndf_sub=pd.DataFrame(predicted,columns=sample_df.columns[1:])\ndf_sub.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sub1=pd.DataFrame(test_id)\ndf_sub1.head(2)\n\nfinal_sub=pd.concat([df_sub1,df_sub],axis=1)\nfinal_sub.to_csv('submission.csv',index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}