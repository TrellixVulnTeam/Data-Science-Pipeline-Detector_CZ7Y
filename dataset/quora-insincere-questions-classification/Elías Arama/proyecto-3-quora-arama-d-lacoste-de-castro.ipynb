{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom keras.engine.topology import Layer\nimport math\nimport operator \nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, TimeDistributed, CuDNNLSTM,Conv2D, SpatialDropout1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, concatenate, Flatten, Reshape, AveragePooling2D, Average, BatchNormalization\nfrom keras.models import Model\nfrom keras.layers import Wrapper\nimport keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport re\nimport gc\nfrom sklearn.preprocessing import StandardScaler\ntqdm.pandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocesamiento**"},{"metadata":{},"cell_type":"markdown","source":"Mappings y Dicts útiles"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n                       \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n                       \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n                       \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n                       \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                       \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\n                       \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n                       \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\",\n                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\",\n                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n\ncontraction_patterns = [ (r'won\\'t', 'will not'), (r'can\\'t', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'),\n                        (r'dont', 'do not'), (r'wont', 'will not') ]\n\npunct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\npunct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n\nmispell_dict = {'advanatges': 'advantages', 'irrationaol': 'irrational' , 'defferences': 'differences','lamboghini':'lamborghini','hypothical':'hypothetical', 'colour': 'color',\n                'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n                'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'qoura' : 'quora', 'sallary': 'salary', 'Whta': 'What',\n                'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do',\n                'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n                'pennis': 'penis', 'Etherium': 'Ethereum', 'etherium': 'ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota',\n                'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n                'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pokémon': 'pokemon'}\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Métodos para limpieza del texto"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_contractions(text, mapping):\n    specials = [\"’\", \"‘\", \"´\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\ndef replaceContraction(text):\n    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n    for (pattern, repl) in patterns:\n        (text, count) = re.subn(pattern, repl, text)\n    return text\ndef clean_text(x):\n    x = str(x)\n    for punct in \"/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n        x = x.replace(punct, '')\n    return x\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', ' number ', x)\n    x = re.sub('[0-9]{4}', ' number ', x)\n    x = re.sub('[0-9]{3}', ' number ', x)\n    x = re.sub('[0-9]{2}', ' number ', x)\n    return x\ndef clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters\n    for s in specials:\n        text = text.replace(s, specials[s])\n    return text\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Los datos"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Un vistazo a los datos\ntrain_df[train_df.target==1].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Statistical Features<p>\n    \"The statistical features were: length of the text, number of capital letters, number of exclamation/question/punctuation marks, number of special symbols, number of smileys, number of words, number of unique words and few derivatives.\""},{"metadata":{"trusted":true},"cell_type":"code","source":"def statistical_features(df):\n    stat_features = pd.DataFrame()\n    stat_features['txt_len'] = df['question_text'].progress_apply(lambda x: len(str(x))) # incluye espacios\n    stat_features['word_count'] = df['question_text'].progress_apply(lambda x: len(str(x).split(\" \")))\n    stat_features['!_count'] = df['question_text'].progress_apply(lambda x: x.count('!'))\n    stat_features['?_count'] = df['question_text'].progress_apply(lambda x: x.count('?'))\n    stat_features['upper_word_count'] = df['question_text'].progress_apply(lambda x: len([x for x in x.split() if x.isupper()]))\n    stat_features['unique_word_count'] = df['question_text'].progress_apply(lambda x: len(set(x.split())))\n    return stat_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df = statistical_features(train_df)\n#train_df[train_df.target==1].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_df = statistical_features(test_df)\n#test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Embeddings**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_embed(file):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n    if file == '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n        \n    return embeddings_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\nparagram =  '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\nprint(\"Extrayendo GloVe embedding...\")\nembed_glove = load_embed(glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Extrayendo Paragram embedding...\")\n#embed_paragram = load_embed(paragram)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"El vocabulario"},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\ndef check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) / len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Eliminar mayúsculas a palabras sin embedding"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Se agregaron {count} palabras al embedding\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train_df ,test_df])\nvocab = build_vocab(df['question_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"GloVe v2: \")\nadd_lower(embed_glove, vocab)\n#oov_glove = check_coverage(vocab_low, embed_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Paragram v2: \")\n#add_lower(embed_paragram, vocab)\n#oov_paragram = check_coverage(vocab_low, embed_paragram)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Aplicando limpieza de texto a los sets de entrenamiento y prueba"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Llevar a minúsculas\ntrain_df['treated_question'] = train_df['question_text'].progress_apply(lambda x: x.lower())\n# Contracciones\ntrain_df['treated_question'] = train_df['treated_question'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))\n# Caracteres especiales\ntrain_df['treated_question'] = train_df['treated_question'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n# Corregir ortografía\ntrain_df['treated_question'] = train_df['treated_question'].progress_apply(lambda x: correct_spelling(x, mispell_dict))\n\n# Llevar a minúsculas\ntest_df['treated_question'] = test_df['question_text'].progress_apply(lambda x: x.lower())\n# Contracciones\ntest_df['treated_question'] = test_df['treated_question'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))\n# Caracteres especiales\ntest_df['treated_question'] = test_df['treated_question'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n# Corregir ortografía\ntest_df['treated_question'] = test_df['treated_question'].progress_apply(lambda x: correct_spelling(x, mispell_dict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Tokenizer y padding"},{"metadata":{"trusted":true},"cell_type":"code","source":"## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\nmax_len = 100 # max number of words in a question to use","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_data(X, X_test):\n    t = Tokenizer(num_words=max_features, filters='', oov_token='<OOV>')\n    t.fit_on_texts(X)\n    X = t.texts_to_sequences(X)\n    X_test = t.texts_to_sequences(X_test)\n    X = pad_sequences(X, maxlen=max_len)\n    X_test = pad_sequences(X_test, maxlen=max_len)\n    return X, X_test, t.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, X_test, word_index = make_data(train_df['treated_question'], test_df['treated_question'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train-Validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['target'].values\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embedding Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_embed_matrix(embeddings_index, word_index, len_voc):\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    word_index = word_index\n    nb_words = min(len_voc, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= len_voc:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: \n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix_glove = make_embed_matrix(embed_glove, word_index, max_features)\ndel word_index\ndel embed_glove\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Extrayendo Paragram embedding...\")\nembed_paragram = load_embed(paragram)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(\"Paragram v2: \")\nadd_lower(embed_paragram, vocab)\n#oov_paragram = check_coverage(vocab_low, embed_paragram)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_, _, word_index = make_data(train_df['treated_question'], test_df['treated_question'])\nembedding_matrix_paragram = make_embed_matrix(embed_paragram, word_index, max_features)\ndel word_index\ndel embed_paragram\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix_final = np.sum([embedding_matrix_glove*0.7,embedding_matrix_paragram*0.3], axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Statistic features como input auxiliar"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_stats = statistical_features(train_df)\ntrain_stats[train_df.target==1].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stats = statistical_features(test_df)\ntest_stats.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalización de statistic features\nsc = StandardScaler()\ntrain_stats = sc.fit_transform(train_stats)\ntest_stats = sc.transform(test_stats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train-Validation Split sobre Statistic Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['target'].values\nX_train_stats, X_val_stats, _, _ = train_test_split(train_stats, y, test_size=0.1, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**El Modelo**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model():\n    optim = Adam(lr=0.0010)\n    \n    main_inp = Input(shape=(max_len,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix_final], trainable=True)(main_inp)\n    x = SpatialDropout1D(0.15)(x)\n    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n    x = Conv1D(filters=64, kernel_size=1)(x)\n    x = GlobalMaxPool1D()(x)\n    \n    stat_inp = Input(shape=(X_train_stats.shape[1],))\n    x_2 = Dense(64, activation=\"relu\")(stat_inp)\n    x_2 = Dropout(0.15)(x_2)\n    x_2 = Dense(32, activation=\"relu\")(x_2)\n    \n    x_f = concatenate([x, x_2])\n    x_f = Dense(128, activation=\"relu\")(x_f)\n    x_f = Dropout(0.15)(x_f)\n    x_f = BatchNormalization()(x_f)\n    x_f = Dense(1, activation=\"sigmoid\")(x_f)\n    \n    model = Model(inputs=[main_inp, stat_inp], outputs = x_f)\n    model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['binary_accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = model()\nprint(model1.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 512\nEPOCHS = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.fit([X_train,X_train_stats], y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=([X_val,X_val_stats],y_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cálculo del puntaje F1 para estimación de la precisión del modelo"},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1_score(pred_model):\n    max_t = 0\n    max_f1 = 0\n    for thresh in np.arange(0.1, 0.701, 0.01):\n        thresh = np.round(thresh, 2)\n        f1 = metrics.f1_score(y_val, (pred_model>thresh).astype(int))\n        #print(\"F1 score at threshold {0} is {1}\".format(thresh, f1))\n        if(f1>max_f1):\n            max_f1 = f1\n            max_t = thresh\n    print(max_t, max_f1) \n    return max_t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_model1 = model1.predict([X_val,X_val_stats], batch_size=1024, verbose=1)\nf1_score(pred_model1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = model()\nmodel2.fit([X_train,X_train_stats], y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=([X_val,X_val_stats],y_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_model2 = model2.predict([X_val,X_val_stats], batch_size=1024, verbose=1)\nf1_score(pred_model2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_model = pred_model1*0.5 + pred_model2*0.5\nmax_t = f1_score(pred_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Predicción sobre el Test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_model1_test = model1.predict([X_test,test_stats], batch_size=1024, verbose=1)\npred_model2_test = model2.predict([X_test,test_stats], batch_size=1024, verbose=1)\npred_model_test = pred_model1_test*0.5 + pred_model2_test*0.5\npred_test = (pred_model_test>max_t).astype(int)\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test\nout_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}