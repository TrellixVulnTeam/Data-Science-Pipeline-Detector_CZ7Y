{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os, re, gc, random\n\nimport warnings\nwarnings.filterwarnings('ignore') # to suppress some matplotlib deprecation warnings","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d8153dfa07ce541703d06feaed43c058f3334b9"},"cell_type":"code","source":"from nltk.tokenize import TweetTokenizer\nfrom gensim.models import KeyedVectors\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"597eea9d6295671bd36a809b579d12777738a392"},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport torch\nimport torchtext\nfrom torchtext import data, vocab\nfrom torchtext.data import Dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9ae58b80e7a871ca069fcefdce83d24c43948e5"},"cell_type":"code","source":"from tqdm import tqdm_notebook\ntorchtext.vocab.tqdm = tqdm_notebook # Replace tqdm to tqdm_notebook in module torchtext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7c04a817b5ba68498dc9b30638605da891ba6c4"},"cell_type":"code","source":"path = \"../input\"\nemb_path = \"../input/embeddings\"\nn_folds = 5\nbs = 512\ndevice = 'cuda'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ddd98901408dbe7c7fb9efdb0ec17cecd511864"},"cell_type":"code","source":"seed = 7777\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fd588f56c18a0993c7db729b7524f81dc016126"},"cell_type":"code","source":"tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"461f84f65cf5452f7d47cd07d4e7da984529c5bf"},"cell_type":"code","source":"mispell_dict = {   \n    \"can't\" : \"can not\", \"tryin'\":\"trying\",\n    \"'m\": \" am\", \"'ll\": \" 'll\", \"'d\" : \" 'd'\", \"..\": \"  \",\".\": \" . \", \",\":\" , \",\n    \"'ve\" : \" have\", \"n't\": \" not\",\"'s\": \" 's\", \"'re\": \" are\", \"$\": \" $\",\"’\": \" ' \",\n    \"y'all\": \"you all\", 'metoo': 'me too',\n    'colour': 'color', 'centre': 'center', 'favourite': 'favorite',\n    'travelling': 'traveling', 'counselling': 'counseling',\n    'centerd': 'centered',\n    'theatre': 'theater','cancelled': 'canceled','labour': 'labor',\n    'organisation': 'organization','wwii': 'world war 2', 'citicise': 'criticize',\n    'youtu ': 'youtube ','Qoura': 'Quora','sallary': 'salary','Whta': 'What',\n    'narcisist': 'narcissist','howdo': 'how do','whatare': 'what are',\n    'howcan': 'how can','howmuch': 'how much','howmany': 'how many', 'whydo': 'why do',\n    'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation',\n    'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'doI': 'do I',\n    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', \n    '2k17': '2017', '2k18': '2018','qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n    'airhostess': 'air hostess', \"whst\": 'what', 'watsapp':'whatsapp',\n    'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n    'demonetisation': 'demonetization','bigdata': 'big data',\n    'Quorans': 'Questions','quorans': 'questions','quoran':'question','Quoran':'Question',\n    # Slang and  abbreviation\n    'Skripal':'russian spy','Doklam':'Tibet', \n    'BNBR':'Be Nice Be Respectful', 'Brexit': 'British exit',\n    'Bhakts':'fascists','bhakts':'fascists','Bhakt':'fascist','bhakt':'fascist',\n    'SJWs':'Social Justice Warrior','SJW':'Social Justice Warrior',\n    'Modiji':'Prime Minister of India', 'Ra apist': 'Rapist', ' apist ':' ape ',\n    'wumao':'commenters','cucks': 'cuck', 'Strzok':'stupid phrase','strzok':'stupid phrase',\n    \n    ' s.p ': ' ', ' S.P ': ' ', 'U.s.p': '', 'U.S.A.': 'USA', 'u.s.a.': 'USA', 'U.S.A': 'USA',\n    'u.s.a': 'USA', 'U.S.': 'USA', 'u.s.': 'USA', ' U.S ': ' USA ', ' u.s ': ' USA ', 'U.s.': 'USA',\n    ' U.s ': 'USA', ' u.S ': ' USA ', ' fu.k': ' fuck', 'U.K.': 'UK', ' u.k ': ' UK ',\n    ' don t ': ' do not ', 'bacteries': 'batteries', ' yr old ': ' years old ', 'Ph.D': 'PhD',\n    'cau.sing': 'causing', 'Kim Jong-Un': 'The president of North Korea', 'savegely': 'savagely',\n    '2fifth': 'twenty fifth', '2third': 'twenty third',\n    '2nineth': 'twenty nineth', '2fourth': 'twenty fourth', '#metoo': 'MeToo',\n    'Trumpcare': 'Trump health care system', '4fifth': 'forty fifth', 'Remainers': 'remainder',\n    'Terroristan': 'terrorist', 'antibrahmin': 'anti brahmin','culturr': 'culture',\n    'fuckboys': 'fuckboy', 'Fuckboys': 'fuckboy', 'Fuckboy': 'fuckboy', 'fuckgirls': 'fuck girls',\n    'fuckgirl': 'fuck girl', 'Trumpsters': 'Trump supporters', '4sixth': 'forty sixth',\n    'weatern': 'western', '4fourth': 'forty fourth', 'emiratis': 'emirates', 'trumpers': 'Trumpster',\n    'indans': 'indians', 'mastuburate': 'masturbate', ' f**k': ' fuck', ' F**k': ' fuck', ' F**K': ' fuck',\n    ' u r ': ' you are ', ' u ': ' you ', '操你妈 ': 'fuck your mother', ' e.g.': ' for example',\n    'i.e.': 'in other words', '...': '.', 'et.al': 'elsewhere', 'anti-Semitic': 'anti-semitic',\n    ' f***': ' fuck', ' f**': ' fuc', ' F***': ' fuck', ' F**': ' fuck',\n    ' a****': ' assho', 'a**': 'ass', ' h***': ' hole', 'A****': 'assho', ' A**': ' ass', ' H***': ' hole',\n    ' s***': ' shit', ' s**': 'shi', ' S***': ' shit', ' S**': ' shi', ' Sh**': 'shit',\n    ' p****': ' pussy', ' p*ssy': ' pussy', ' P****': ' pussy',\n    ' p***': ' porn', ' p*rn': ' porn', ' P***': ' porn',' Fck': ' Fuck',' fck': ' fuck',  \n    ' st*up*id': ' stupid', ' d***': 'dick', ' di**': ' dick', ' h*ck': ' hack',\n    ' b*tch': ' bitch', 'bi*ch': ' bitch', ' bit*h': ' bitch', ' bitc*': ' bitch', ' b****': ' bitch',\n    ' b***': ' bitc', ' b**': ' bit', ' b*ll': ' bull',' FATF': 'Western summit conference',\n    'Terroristan': 'terrorist Pakistan', 'terroristan': 'terrorist Pakistan',\n    ' incel': ' involuntary celibates', ' incels': ' involuntary celibates', 'emiratis': 'Emiratis',\n    'weatern': 'western', 'westernise': 'westernize', 'Pizzagate': 'debunked conspiracy theory', 'naïve': 'naive',\n    ' HYPSM': ' Harvard, Yale, Princeton, Stanford, MIT', ' HYPS': ' Harvard, Yale, Princeton, Stanford',\n    'kompromat': 'compromising material', ' Tharki': ' pervert', ' tharki': 'pervert',\n    'Naxali ': 'Naxalite ', 'Naxalities': 'Naxalites','Mewani': 'Indian politician Jignesh Mevani', ' Wjy': ' Why',\n    'Fadnavis': 'Indian politician Devendra Fadnavis', 'Awadesh': 'Indian engineer Awdhesh Singh',\n    'Awdhesh': 'Indian engineer Awdhesh Singh', 'Khalistanis': 'Sikh separatist movement',\n    'madheshi': 'Madheshi','Stupead': 'stupid',  'narcissit': 'narcissist',\n}\n\ndef clean_latex(text):\n    \"\"\"\n    convert r\"[math]\\vec{x} + \\vec{y}\" to English\n    \"\"\"\n    # edge case\n    text = re.sub(r'\\[math\\]', ' LaTex math ', text)\n    text = re.sub(r'\\[\\/math\\]', ' LaTex math ', text)\n#     text = re.sub(r'\\\\', ' LaTex ', text)\n\n    pattern_to_sub = {\n        r'\\\\mathrm': ' LaTex math mode ',\n        r'\\\\mathbb': ' LaTex math mode ',\n        r'\\\\boxed': ' LaTex equation ',\n        r'\\\\begin': ' LaTex equation ',\n        r'\\\\end': ' LaTex equation ',\n        r'\\\\left': ' LaTex equation ',\n        r'\\\\right': ' LaTex equation ',\n        r'\\\\(over|under)brace': ' LaTex equation ',\n        r'\\\\text': ' LaTex equation ',\n        r'\\\\vec': ' vector ',\n        r'\\\\var': ' variable ',\n        r'\\\\theta': ' theta ',\n        r'\\\\mu': ' average ',\n        r'\\\\min': ' minimum ',\n        r'\\\\max': ' maximum ',\n        r'\\\\sum': ' + ',\n        r'\\\\times': ' * ',\n        r'\\\\cdot': ' * ',\n        r'\\\\hat': ' ^ ',\n        r'\\\\frac': ' / ',\n        r'\\\\div': ' / ',\n        r'\\\\sin': ' Sine ',\n        r'\\\\cos': ' Cosine ',\n        r'\\\\tan': ' Tangent ',\n        r'\\\\infty': ' infinity ',\n        r'\\\\int': ' integer ',\n        r'\\\\in': ' in ',\n    }\n    # post process for look up\n    pattern_dict = {k.strip('\\\\'): v for k, v in pattern_to_sub.items()}\n    # init re\n    patterns = pattern_to_sub.keys()\n    pattern_re = re.compile('(%s)' % '|'.join(patterns))\n\n    def _replace(match):\n        \"\"\"\n        reference: https://www.kaggle.com/hengzheng/attention-capsule-why-not-both-lb-0-694 # noqa\n        \"\"\"\n        try:\n            word = pattern_dict.get(match.group(0).strip('\\\\'))\n        except KeyError:\n            word = match.group(0)\n            print('!!Error: Could Not Find Key: {}'.format(word))\n        return word\n    return pattern_re.sub(_replace, text)\n\ndef correct_spelling(s, dic):\n    for key, corr in dic.items():\n        s = s.replace(key, dic[key])\n    return s\n\ndef tweet_clean(text):\n    text = re.sub(r'[^A-Za-z0-9!.,?$\\'\\\"]+', ' ', text) # remove non alphanumeric character\n#     text = re.sub(r'https?:/\\/\\S+', ' ', text) # remove links\n    return text #.lower()\n\ndef tokenizer(s): \n    s = clean_latex(s)\n    s = correct_spelling(s, mispell_dict)\n    s = tweet_clean(s)\n    return tknzr.tokenize(s)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f1a53a68e48048e6383bca70f5b58ad82b8058a"},"cell_type":"code","source":"def find_threshold(y_t, y_p, floor=-1., ceil=1., steps=41):\n    thresholds = np.linspace(floor, ceil, steps)\n    best_val = 0.0\n    for threshold in thresholds:\n        val_predict = (y_p > threshold)\n        score = f1_score(y_t, val_predict)\n        if score > best_val:\n            best_threshold = threshold\n            best_val = score\n    return best_threshold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbfa9dfccdac8100e265eeb0038a6bcea121c677"},"cell_type":"code","source":"def splits_cv(data, cv, y=None):\n    \n    for indices in cv.split(range(len(data)), y):\n        (train_data, val_data) = tuple([data.examples[i] for i in index] for index in indices)\n        yield tuple(Dataset(d, data.fields) for d in (train_data, val_data) if d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1acb71152dd8704c3e95725be2943c75e4a2561"},"cell_type":"code","source":"skf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed)\n\nscores = pd.read_csv('../input/train.csv')\ntarget = scores.target.values\nscores = scores.set_index('qid')\nscores.drop(columns=['question_text'], inplace=True)\n\nsubm = pd.read_csv('../input/test.csv')\nsubm = subm.set_index('qid')\nsubm.drop(columns='question_text', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9640b7fef754155e0162b2cd1e70cadf94c6ec6c"},"cell_type":"code","source":"# define the columns that we want to process and how to process\ntxt_field = data.Field(sequential=True, tokenize=tokenizer, include_lengths=False,  use_vocab=True)\nlabel_field = data.Field(sequential=False, use_vocab=False, is_target=True)\nqid_field = data.RawField()\n\ntrain_fields = [\n    ('qid', qid_field), # we dont need this, so no processing\n    ('question_text', txt_field), # process it as text\n    ('target', label_field) # process it as label\n]\ntest_fields = [\n    ('qid', qid_field), \n    ('question_text', txt_field), \n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92ea577a18faedfdccf5198d626c5c27861133ee"},"cell_type":"code","source":"# Loading csv file\ntrain_ds = data.TabularDataset(path=os.path.join(path, 'train.csv'), \n                           format='csv',\n                           fields=train_fields, \n                           skip_header=True)\n\ntest_ds = data.TabularDataset(path=os.path.join(path, 'test.csv'), \n                           format='csv',\n                           fields=test_fields, \n                           skip_header=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"211c926c66b7429f40bf8cf327b51c8bd6bc7ba3"},"cell_type":"code","source":"test_ds.fields['qid'].is_target = False\ntrain_ds.fields['qid'].is_target = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e5feaed1764517dbb5e51c21f09c939dbb77704"},"cell_type":"code","source":"test_loader = data.BucketIterator(test_ds, batch_size=bs, device='cuda',\n                                sort_key=lambda x: len(x.question_text),\n                                sort_within_batch=True, \n                                shuffle=False, repeat=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e64a4a40f2e23749d937004f00db7210cdd3d946","scrolled":true},"cell_type":"code","source":"class RecNN(nn.Module):\n    def __init__(self, embs_vocab, hidden_size, layers=1, dropout=0., bidirectional=False):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.bidirectional = bidirectional\n        self.num_layers = layers\n        self.emb = nn.Embedding(embs_vocab.size(0), embs_vocab.size(1))\n        self.emb.weight.data.copy_(embs_vocab) # load pretrained vectors\n        self.emb.weight.requires_grad = False # make embedding non trainable\n        \n        self.line = nn.Conv1d(300, 300, 1, bias=True) # Linear transformation of embedding vectors\n        \n        self.lstm = nn.LSTM(embs_vocab.size(1), self.hidden_size,\n                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n        \n        self.gru = nn.GRU(embs_vocab.size(1), self.hidden_size,\n                            num_layers=layers, bidirectional=bidirectional, dropout=dropout)\n        \n        self.out = nn.Linear(self.hidden_size*(bidirectional + 1), 32)\n        self.last = nn.Linear(32, 1)\n                \n    def forward(self, x):\n        \n        embs = self.emb(x)\n        lstm, (h, c) = self.lstm(embs)\n        \n        x = F.relu(self.line(embs.permute(1,2,0)), inplace=True).permute(2,0,1)\n        gru, h = self.gru(x, h)\n        lstm = lstm + gru\n        \n        lstm, _ = lstm.max(dim=0, keepdim=False) \n        out = self.out(lstm)\n        out = self.last(F.relu(out)).squeeze()\n        return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e566ecdb5f44cf32af12127f073021566d6e66dd"},"cell_type":"code","source":"def OOF_preds(test_df, target, embs_vocab, epochs = 4, alias='prediction', cv=skf,\n              loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean', pos_weight=(torch.Tensor([2.7])).to(device)),\n              bs = 512, embedding_dim = 300, bidirectional=True, n_hidden = 64):\n\n    print('Embedding vocab size: ', embs_vocab.size()[0])\n    \n    test_df[alias] = 0.\n    \n    for train, _ in splits_cv(train_ds, cv, target):\n        \n        train = data.BucketIterator(train, batch_size=bs, device=device,\n                        sort_key=lambda x: len(x.question_text),\n                        sort_within_batch=True,\n                        shuffle=True, repeat=False)\n\n        model = RecNN(embs_vocab, n_hidden, dropout=0., bidirectional=bidirectional).to(device)\n        \n        opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), 1e-3,\n                         betas=(0.75, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n        \n        print('\\n')\n        for epoch in range(epochs):      \n            y_true_train = np.empty(0)\n            y_pred_train = np.empty(0)\n            total_loss_train = 0          \n            model.train()\n            for (_, x), y in train:\n                y = y.type(dtype=torch.cuda.FloatTensor)\n                opt.zero_grad()\n                pred = model(x)\n                loss = loss_fn(pred, y)\n                loss.backward()\n                opt.step()\n\n                y_true_train = np.concatenate([y_true_train, y.cpu().data.numpy()], axis = 0)\n                y_pred_train = np.concatenate([y_pred_train, pred.cpu().squeeze().data.numpy()], axis = 0)\n                total_loss_train += loss.item()\n\n            tacc = f1_score(y_true_train, y_pred_train>0)\n            tloss = total_loss_train/len(train)\n            print(f'Epoch {epoch+1}: Train loss: {tloss:.4f}, F1: {tacc:.4f}')\n        \n        # Get prediction for test set\n        preds = torch.empty(0)\n        qids = []\n        for (y, x), _ in test_loader:\n            pred = model(x)\n            qids.append(y)\n            preds = torch.cat([preds, pred.detach().cpu()])\n            \n        # Save prediction of test set\n        preds = torch.sigmoid(preds).numpy()\n        qids = [item for sublist in qids for item in sublist]\n        test_df.at[qids, alias]  =  test_df.loc[qids][alias].values + preds/n_folds\n        \n        gc.enable();\n        del train\n        gc.collect();\n        \n    gc.enable();\n    del embs_vocab, model\n    gc.collect();     \n    return test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cb4c7f88da6b9c608a923f6119d5fac70cab7ffa"},"cell_type":"code","source":"def preload_gnews():\n    # Google..... bin file.......\n    vector_google = KeyedVectors.load_word2vec_format(os.path.join(emb_path, embs_file['gnews']), binary=True)\n\n    stoi = {s:idx for idx, s in enumerate(vector_google.index2word)}\n    itos = {idx:s for idx, s in enumerate(vector_google.index2word)}\n\n    cache='cache/'\n    path_cache = os.path.join(cache, 'GoogleNews-vectors-negative300.bin')\n    file_suffix = '.pt'\n    path_pt = path_cache + file_suffix\n\n    torch.save((itos, stoi, torch.from_numpy(vector_google.vectors), vector_google.vectors.shape[1]), path_pt)\n\n    \nembs_file = {}\nembs_file['wiki'] = 'wiki-news-300d-1M/wiki-news-300d-1M.vec'\nembs_file['gnews'] = 'GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\nembs_file['glove'] = 'glove.840B.300d/glove.840B.300d.txt'\nembs_file['gram'] = 'paragram_300_sl999/paragram_300_sl999.txt'\n\nembs_vocab = {}\n\n!mkdir cache\npreload_gnews()\n# specify the path to the localy saved vectors\nvec = vocab.Vectors(os.path.join(emb_path, embs_file['gnews']), cache='cache/')\n# build the vocabulary using train and validation dataset and assign the vectors\ntxt_field.build_vocab(train_ds, test_ds, max_size=350000, vectors=vec)\nembs_vocab['gnews'] = train_ds.fields['question_text'].vocab.vectors\n!rm -r cache\n\n# specify the path to the localy saved vectors\nvec = vocab.Vectors(os.path.join(emb_path, embs_file['wiki']), cache='cache/')\n# build the vocabulary using train and validation dataset and assign the vectors\ntxt_field.build_vocab(train_ds, test_ds, max_size=350000, vectors=vec)\nembs_vocab['wiki'] = train_ds.fields['question_text'].vocab.vectors\n\n# specify the path to the localy saved vectors\nvec = vocab.Vectors(os.path.join(emb_path, embs_file['glove']), cache='cache/')\n# build the vocabulary using train and validation dataset and assign the vectors\ntxt_field.build_vocab(train_ds, test_ds, max_size=350000, vectors=vec)\nembs_vocab['glove'] = train_ds.fields['question_text'].vocab.vectors\n\nprint('Embedding loaded, vocab size: ', embs_vocab['glove'].size()[0])\n!rm -r cache\ngc.enable()\ndel vec\ngc.collect(); ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6d9580087b3087d928afd4ead24aab495db2f6d6"},"cell_type":"code","source":"def fill_unknown(vector):\n    # fill from Glove\n    data = torch.zeros_like(vector)\n    data.copy_(vector)\n    idx = torch.nonzero(data.sum(dim=1) == 0)\n    data[idx] = embs_vocab['glove'][idx]\n    # fill from Wiki\n    idx = torch.nonzero(data.sum(dim=1) == 0)\n    data[idx] = embs_vocab['wiki'][idx]\n    # fill from GoogleNews\n    idx = torch.nonzero(data.sum(dim=1) == 0)\n    data[idx] = embs_vocab['gnews'][idx]\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11f59c3d8c209dc02d8a36a845f7c510e74dd6cc","scrolled":false},"cell_type":"code","source":"%%time\nsubm = OOF_preds(subm, target, epochs = 5, alias='wiki',\n#               loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean'), \n              embs_vocab=fill_unknown(embs_vocab['wiki']),\n              cv = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed),\n              embedding_dim = 300, bidirectional=True, n_hidden = 64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"991cb7359d8d497835f6677910329233e6750861","scrolled":false},"cell_type":"code","source":"%%time\nsubm = OOF_preds(subm, target, epochs = 5, alias='glove',\n              embs_vocab=fill_unknown(embs_vocab['glove']),\n              cv = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed+15),\n              bs = 512, embedding_dim = 300, bidirectional=True, n_hidden = 64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94ba7d9f06af2533780b352ef602351d40d0d762"},"cell_type":"code","source":"%%time\nsubm = OOF_preds(subm, target, epochs = 5, alias='gnews',\n              embs_vocab=fill_unknown(embs_vocab['gnews']),\n              cv = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed+25),\n              bs = 512, embedding_dim = 300, bidirectional=True, n_hidden = 64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2d3a79fb61ae30dc46ff201110cec6bbabf2c89"},"cell_type":"code","source":"subm.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"419491d50aed847a9047c8583bf094d1a38fbf92"},"cell_type":"code","source":"submission = np.mean(subm.values, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c6a41bd21bf783455fd11b63745ed454e0413f0"},"cell_type":"code","source":"subm['prediction'] = submission > 0.55\nsubm.prediction = subm.prediction.astype('int')\nsubm.to_csv('submission.csv', columns=['prediction'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}