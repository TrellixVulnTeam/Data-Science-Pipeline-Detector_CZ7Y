{"cells":[{"metadata":{},"cell_type":"markdown","source":"A simple neural network model using BiLSTM and pre-trained Glove embeddings."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nquestion_lines = list()\nlines = train['question_text'].values.tolist()\n\nfor line in lines:\n    tokens = word_tokenize(line)\n    tokens = [w.lower() for w in tokens]\n    table = str.maketrans('','',string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    words = [word for word in stripped if word.isalpha()]\n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words if not w in stop_words]\n    question_lines.append(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(question_lines)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length = 60\nEMBEDDING_DIM = 200\nmax_features = 50000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\nembeddings_index={}\nf = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt')\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:])\n    embeddings_index[word] = coefs\nf.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(question_lines)\nsequences = tokenizer_obj.texts_to_sequences(question_lines)\n\nword_index = tokenizer_obj.word_index\nprint(len(word_index))\nquestion_pad = pad_sequences(sequences,maxlen=max_length)\ntarget = train['target'].values\nprint(question_pad.shape)\nprint(target.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words,EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i>num_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import CuDNNLSTM, Dense, Bidirectional","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(num_words, EMBEDDING_DIM, input_length=max_length, weights=[embedding_matrix], trainable=False))\nmodel.add(Bidirectional(CuDNNLSTM(64, return_sequences=True))\nmodel.add(Bidirectional(CuDNNLSTM(64)))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model1.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"VALIDATION_SPLIT = 0.1\n\nindices = np.arange(question_pad.shape[0])\nnp.random.shuffle(indices)\nquestion_pad = question_pad[indices]\ntarget = target[indices]\nnum_validation_samples = int(VALIDATION_SPLIT*question_pad.shape[0])\n\nX_train_pad = question_pad[:-num_validation_samples]\ny_train = target[:-num_validation_samples]\nX_test_pad = question_pad[-num_validation_samples:]\ny_test = target[-num_validation_samples:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train_pad,y_train,batch_size=128,epochs=25,validation_data=(X_test_pad,y_test),verbose=1,callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"question_lines_test = list()\nlines_test = test['question_text'].values.tolist()\n\nfor line in lines_test:\n    tokens = word_tokenize(line)\n    tokens = [w.lower() for w in tokens]\n    table = str.maketrans('','',string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    words = [word for word in stripped if word.isalpha()]\n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words if not w in stop_words]\n    question_lines_test.append(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(question_lines_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(question_lines_test)\nsequences_test = tokenizer_obj.texts_to_sequences(question_lines_test)\n\nword_index_test = tokenizer_obj.word_index\nprint(len(word_index_test))\nquestion_pad_test = pad_sequences(sequences_test,maxlen=max_length)\nprint(question_pad_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_test = model.predict(question_pad_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_test = (target_test>0.5).astype(int)\nout_df = pd.DataFrame({\"qid\":test[\"qid\"].values})\nout_df['prediction'] = target_test\nout_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}