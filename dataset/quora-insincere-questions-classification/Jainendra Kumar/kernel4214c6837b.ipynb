{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dropout\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nSTOPWORDS = set(stopwords.words('english'))\nfrom tensorflow.keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/quora-insincere-questions-classification/train.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_description(description):\n    document = re.sub(r'\\W', ' ', str(description))\n    document = re.sub(' +', ' ', document)        \n    document = document.lower()\n    document = re.sub('\\(', \" \", document)\n    document = re.sub('\\)', \" \", document)\n    document = re.sub('-', \"\", document)\n    document = re.sub('&', \"\", document)\n    document = re.sub('&', \"\", document)\n    document = re.sub('|', \"\", document)\n    document = re.sub('\\/', \" \", document)\n    document = re.sub(\"\\'\", \"\", document)\n    document = re.sub('\\\"', \"\", document)\n    document = re.sub(',', \"\", document)\n    document = re.sub('[0-9]', \"\", document)\n    document = document.split()\n    document=' '.join( [w for w in document if len(w)>1 and (w.lower() not in STOPWORDS)] )\n    return document","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['question_text']=df['question_text'].apply(lambda x: clean_description(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_NB_WORDS = 50000\n# Max number of words in each query.\nMAX_SEQUENCE_LENGTH = 400\n# This is fixed.\nEMBEDDING_DIM = 100\n\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['question_text'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = tokenizer.texts_to_sequences(df['question_text'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = pd.get_dummies(df['target']).values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"  def createmodel():\n    model = Sequential()\n    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n    model.add(SpatialDropout1D(0.2))\n    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(2, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print(model.summary())\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = createmodel()\n\nepochs = 2\nbatch_size = 1028\n\nhistory = model.fit(X, Y, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test=pd.read_csv(\"/kaggle/input/quora-insincere-questions-classification/test.csv\")\ndf_test['question_text']=df_test['question_text'].apply(lambda x: clean_description(x))\n\ntokenizertest = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizertest.fit_on_texts(df_test['question_text'].values)\n\nX_test = tokenizer.texts_to_sequences(df_test['question_text'].values)\nX_test = pad_sequences(X_test, maxlen=MAX_SEQUENCE_LENGTH)\nynew = model.predict_classes(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission=pd.read_csv(\"/kaggle/input/quora-insincere-questions-classification/test.csv\")\ndf_submission[\"prediction\"]=ynew\ndf_submission=df_submission[[\"qid\",\"prediction\"]]\ndf_submission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}