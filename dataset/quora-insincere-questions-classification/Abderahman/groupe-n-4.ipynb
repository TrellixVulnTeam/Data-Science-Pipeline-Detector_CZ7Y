{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nimport re\nfrom sklearn.model_selection import train_test_split\ntqdm.pandas()\ntrain = pd.read_csv(\"../input/quora-insincere-questions-classification/train.csv\")\ntest = pd.read_csv(\"../input/quora-insincere-questions-classification/test.csv\")\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)\ntrain[:10].question_text\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \n          \"'\",  '&', '/', '[', ']', '>', '<', '%', '=', '#', '+', \n          '\\\\',  '§', '″', '′','¿','═']\n\ndef preprocessing(sentence):\n    \"\"\"\n    Applies preprocessing such as cleaning to a sentence\n    Edited from https://www.kaggle.com/hengzheng/pytorch-starter\n    \n    Args:\n    * sentence: Original sentence to be cleaned\n    \n    Returns:\n    Cleaned sentence\n    \"\"\"\n    \n    # Convert to lower case\n    sentence = sentence.lower()\n    \n    # Add spaces around special characters\n    for punct in puncts:\n        sentence = sentence.replace(punct, f' {punct} ')\n    \n    # Replace numbers by hash characters\n    sentence = re.sub('[0-9]{5,}', '#####', sentence)\n    sentence = re.sub('[0-9]{4}', '####', sentence)\n    sentence = re.sub('[0-9]{3}', '###', sentence)\n    sentence = re.sub('[0-9]{2}', '##', sentence)\n    \n    return sentence\ntrain, val = train_test_split(train, test_size=0.1, random_state=2019)\ntrain\ntrain.question_text = train.question_text.apply(preprocessing)\ntest.question_text = test.question_text.apply(preprocessing)\nval.question_text = val.question_text.apply(preprocessing)\ntrain.question_text[0]\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\nmax_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\n\n# Tokenize the sentences\ntokenizer = Tokenizer(num_words = max_features, # how many unique words to use \n                     filters='' # characters that will be filtered from the texts\n                     )\n\ntokenizer.fit_on_texts(list(train.question_text) + list(test.question_text)+list(val.question_text))\n\nX2 = tokenizer.texts_to_sequences(val.question_text)\nX = tokenizer.texts_to_sequences(train.question_text)\nX_test = tokenizer.texts_to_sequences(test.question_text)\n\n# Create vocabulary from all words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocabulary = tokenizer.word_index\nprint(f\"Tokenized sample tex: {X[0]}\\n\")\nprint({word: vocabulary[word] for word in (train.iloc[0].question_text).split(' ')[:-1]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nMAX_LENGTH = 100 # 745\n\nX2_embed = pad_sequences(X2, \n                  maxlen=MAX_LENGTH, # Max. number of words in sentence\n                  padding='pre' # Where to add padding\n                 )\n\nX_embed = pad_sequences(X, \n                  maxlen=MAX_LENGTH, # Max. number of words in sentence\n                  padding='pre' # Where to add padding\n                 )\n\nX_test_embed = pad_sequences(X_test, \n                       maxlen=MAX_LENGTH, # Max. number of words in sentence\n                       padding='pre' # Where to add padding\n                      )\nX_embed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[:10].question_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.models import Model\nfrom tensorflow.compat.v1.keras.layers import CuDNNGRU \nfrom keras.layers import Bidirectional, GlobalMaxPool1D\ninp = Input(shape=(100,))\nx = Embedding(max_features, 300)(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ty2= val['target'].values\nty= train['target'].values\nmodel.fit(X_embed, ty, batch_size=512, epochs=2,validation_data=(X2_embed, ty2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y = model.predict([X_test_embed], batch_size=1024, verbose=1)\npred_y1 = model.predict([X_test_embed], batch_size=1024, verbose=1)\npred_y2 = model.predict([X_test_embed], batch_size=1024, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test_y = 0.33*pred_y + 0.33*pred_y1 + 0.34*pred_y2\npred_test_y = (pred_test_y>0.35).astype(int)\nout_df = pd.DataFrame({\"qid\":test[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}