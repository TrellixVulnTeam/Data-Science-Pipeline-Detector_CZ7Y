{"cells":[{"metadata":{"id":"YTzRr-voh4jx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"72280e91-0937-4fdf-c1b1-a6dcc10e6613","trusted":true,"_uuid":"a050efbf0e26ab85fcdc2a00e2448d2661313985"},"cell_type":"code","source":"# Natural Language Processing\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport json\nimport string\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\nfrom keras import backend as K\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, GlobalAvgPool1D, concatenate\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom sklearn import model_selection\n\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.model_selection import GridSearchCV# for tuning parameter\nfrom sklearn.ensemble import RandomForestClassifier # for random forest classifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm # for Support Vector Machine\nfrom sklearn import metrics # for the check the error and accuracy of the model\n","execution_count":null,"outputs":[]},{"metadata":{"id":"TvbXh0CHlcaL","colab_type":"code","colab":{},"trusted":true,"_uuid":"779cc7dc380c8670d8aa565ce23ed95ee6ff4b57"},"cell_type":"code","source":"#from zipfile import ZipFile\n#file_name = \"train.csv.zip\"\n\n#with ZipFile(file_name,'r') as zip:\n#  zip.extractall()\n#  print(\"DOne\")","execution_count":null,"outputs":[]},{"metadata":{"id":"j52bn4yDh4j1","colab_type":"code","colab":{},"trusted":true,"_uuid":"9e1dcce0018d7920d0e9403bd917da0420450187"},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nsample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n## Parameters \nembed_size = 300 # how big is each word vector\nmax_features = 60000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 26 # max number of words in a question to use\nbatch_size = 3636\n\n## fill up the missing values\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)","execution_count":null,"outputs":[]},{"metadata":{"id":"E19XJ0i8h4j3","colab_type":"code","colab":{},"trusted":true,"_uuid":"71bb4c775eff83f268f21e159a2f796677b85562"},"cell_type":"code","source":"train_y = train_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"id":"OX2gmvSdh4j5","colab_type":"code","colab":{},"trusted":true,"_uuid":"3f188bd662f2916bbdc1daaed0e9f6c628f9fa84"},"cell_type":"code","source":"# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(train_X, train_y)\n\n# Predicting the Test set results\ny_pred = classifier.predict(train_X)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(train_y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"edi1qWp7h4j8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6ba59f82-0410-4ab2-ee99-2b12a8335eed","trusted":true,"_uuid":"dae9062e0f12152dd97ad75717a393824fc04d57"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nacc = accuracy_score(train_y, y_pred)\n\nprint(\"Accuracy on the Quora dataset: {:.2f}\".format(acc*100))","execution_count":null,"outputs":[]},{"metadata":{"id":"Iq-GDoV4h4j_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"6a305b54-3639-4a79-f45e-b6e348fc139a","trusted":true,"_uuid":"86bc592a68d70c8628d48308459f0d1a3548d13e"},"cell_type":"code","source":"from sklearn.metrics import classification_report\ntarget_names = ['0','1']\nprint(classification_report(y_pred,train_y, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"id":"_4TqxvG3h4kM","colab_type":"code","colab":{},"trusted":true,"_uuid":"e1f9d5b084d5225759b3bc95122bba73bf3ace7d"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split # to split the data into two parts\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn import metrics # for the check the error and accuracy of the model\n# Any results you write to the current directory are saved as output.\n# dont worry about the error if its not working then insteda of model_selection we can use cross_validation","execution_count":null,"outputs":[]},{"metadata":{"id":"IBUu7QnLh4kP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"44d5d6a7-c77c-439e-8ee0-dbcfd7df8da1","trusted":true,"_uuid":"18ba8e178ca7a189925d5cc9f85205a26dff549c"},"cell_type":"code","source":"# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(train_X, train_y)\n\n# Predicting the Test set results\ny_pred = classifier.predict(train_X)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nprint(metrics.accuracy_score(y_pred,train_y))","execution_count":null,"outputs":[]},{"metadata":{"id":"35ChNOAsq7dg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2e1eea10-78f8-4828-d662-970087da885a","trusted":true,"_uuid":"aa217dfdd6d1d29480591fc9361cfcae818f8654"},"cell_type":"code","source":"model_DT = DecisionTreeClassifier(criterion='entropy', random_state=0)\nmodel_DT.fit(train_X, train_y)\nprediction = model_DT.predict(train_X)\nmetrics.accuracy_score(prediction,train_y)","execution_count":null,"outputs":[]},{"metadata":{"id":"ew3tu_XdjDgf","colab_type":"code","colab":{},"trusted":true,"_uuid":"37c2897a1dad80d0402387adabe3e63f4768b1a2"},"cell_type":"code","source":"from sklearn.metrics import classification_report\ntarget_names = ['0','1']\nprint(classification_report(prediction,train_y, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"id":"LEuDOeJBvKRC","colab_type":"code","colab":{},"trusted":true,"_uuid":"333e9e8b54c94fccd078811decca0464eefc7529"},"cell_type":"code","source":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2) # You want cluster the passenger records into 2: Survived or Not survived\nkmeans.fit(train_X)\nprediction = kmeans.predict(train_X)\nmetrics.accuracy_score(prediction,train_y)","execution_count":null,"outputs":[]},{"metadata":{"id":"tjHA-Z9ajGRo","colab_type":"code","colab":{},"trusted":true,"_uuid":"00b3cf22589c30817d3882d8e66c5fe7fe59df3b"},"cell_type":"code","source":"from sklearn.metrics import classification_report\ntarget_names = ['0','1']\nprint(classification_report(prediction,train_y, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"id":"ccSihh0Hh4kR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"7d0f7deb-1751-4029-a4b6-4f7b63735998","trusted":true,"_uuid":"d95a7dbfefb6e7265ab92f945a95a79f2678a3e6"},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn import model_selection\neclf1 = VotingClassifier(estimators=[('model_GaussianNB', classifier),('model_DT', model_DT),('kmeans', kmeans)], voting='hard')\neclf1 = eclf1.fit(train_X, train_y)\nprediction=eclf1.predict(train_X)\nprint(metrics.accuracy_score(prediction,train_y),\"voting classifier hard method\")","execution_count":null,"outputs":[]},{"metadata":{"id":"HgMgZgMRjKUI","colab_type":"code","colab":{},"trusted":true,"_uuid":"17823caf46a477a62a928ff12b2bae2935b5e4a0"},"cell_type":"code","source":"from sklearn.metrics import classification_report\ntarget_names = ['0','1']\nprint(classification_report(prediction,train_y, target_names=target_names))","execution_count":null,"outputs":[]},{"metadata":{"id":"Z-j7dPczh4kB","colab_type":"code","colab":{},"trusted":true,"_uuid":"817dcac371641f41ebb4155cf7bf2c41605425e1"},"cell_type":"code","source":"ids = test_df[\"qid\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"Wf97mIGqh4kE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"538c3c46-aeb5-4bfc-f32d-041048c153f4","trusted":true,"_uuid":"93d54116d1c9eabac2f022e60803e5c3410823c6"},"cell_type":"code","source":"target = eclf1.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"id":"VjQnobAHh4kG","colab_type":"code","colab":{},"trusted":true,"_uuid":"4f47e6fa3ef75b029683d952d08b02efeb5dac60"},"cell_type":"code","source":"sample_submission[\"prediction\"]=target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7dce0d7c67cdc924ac0db47d1b73150879734b3"},"cell_type":"code","source":" sample_submission = sample_submission.loc[:, ~sample_submission.columns.str.contains('^Unnamed')]","execution_count":null,"outputs":[]},{"metadata":{"id":"9v4KAi2nh4kJ","colab_type":"code","colab":{},"trusted":true,"_uuid":"f01fad3036ab8bfac0e117e79b01fa415b27c8e1"},"cell_type":"code","source":"sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"314HPH_fn4JL","colab_type":"code","colab":{},"trusted":true,"_uuid":"aee499358772578d55635efd58037c70417e8eb3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d399d8f2fe792872d50626731e03cd34d5cab99a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"Quora Insincere Questions Classification.ipynb","version":"0.3.2","provenance":[],"private_outputs":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}