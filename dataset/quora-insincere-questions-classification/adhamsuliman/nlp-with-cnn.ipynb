{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Introduction**\n\nThe concept of Long Short Term Memory networks were first envisioned by Sepp Hochreiter and JÃ¼rgen Schmidhuber in 1997 which provided a huge advancement within the world of Reccurent Neural Networks. LSTMs utilize gates which learn to pass on certain parts of a given input to be utilized when determining an output. \n\n\nQuora is a website which hosts an online platform where questions can be asked and then be asnwered by the online community. The the community intends to answer questions posed by curious individuals from topics ranging from relgion to technology. A majority of the answers provided are well intedned, but infrequently, insencere responses are posted which provide no value to the question asked. If Quora is capable of identifying these types of responses, it can make sure it's community is a benefical environment for all users. The following analysis intends to identify inscencere questions that have been posted within the Quora Community by utilizing Convolutional Networks in tangent with LSTMs. \n\n\nBelow, we import the required modules for the analysis."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\n\nfrom sklearn.metrics import roc_curve, auc,  f1_score\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport keras\nfrom sklearn import metrics\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D, BatchNormalization, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D,CuDNNLSTM\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, TimeDistributed\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preproccessing**\n\nThe train data set consists of 1.31 million records for the training data where the commentary is preclassified as toxic or non toxic. The test data set consists of 376 thousand records.\n\nIn order for computers to be able to process english text, they must convert the sentences to vectors. We will first tokenize the sentences where each word is given a value. Each sentence must be the same length before entering the neural network which is why we utilize the pad_sequences function. \n\nWe will now read in the data and split it into train and test, tokenize the comments, and pad the comments for preprocessing."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#filename = '../input/googles-trained-word2vec-model-in-python/GoogleNews-vectors-negative300.bin.gz'\n#model = KeyedVectors.load_word2vec_format(filename, binary=True)\ndf = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=2018)\ntrain_df, val_df = train_test_split(train_df, test_size=0.34, random_state=2018)\n\n## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\n## fill up the missing values\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values\ntest_y = test_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Embedding**\n\nIt takes years for humans to comprehend the intriciacies of a language such as grammer, future and past tenses, and parts of speech. Though we could input our data into the Neural Netowrk immdeitaly, we can utilize anexternal dictionary which seek to expalin what each word utilized with our data set means. \n\nA glove embedding is an unsupervised algorithm which aims to produce vectors that represent all words within a given corpra. A glove is initiated by creating a matrix sized by the (unique number of words) x (unique number of words). The values within the matrix are indicative of how often a word is associated with another word. The matrix is then factorized to yield lower dimesnionality to produce a matrix (words) x (# of desired features). Though these features don't have a specific meaning, the more features produced, the more differentiation can be created between words. \n\nThe glove embedding utilized within this analysis was created by Standford by training on the corpra of Wikipedia2014 which contains 400k unique words within it's corpra. A function is created below to find the embeedings within the quora corpra data set utlizing standford's glove embeddings. "},{"metadata":{"trusted":true,"_uuid":"19fbaa59bf067e2f00e684e4505a8c9c27942bff"},"cell_type":"code","source":"EMBEDDING_FILE = '../input/glove840b300dtxt/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will now constrct the Neural Networks which utilizes both Convolutional Neural Networks and Recurrent Neural Networks in Keras. \n\n1. The comments within the train data set must first be translated into numerical embeddings utilizing Stanford's glove embedding.\n\n2. A 2 dimensional CNN is then used across the comments with the filter sizes set at the top.\n    \n    A. The first dimension looks at the number of words to utilize when learning to determine the classification of our output.\n        \n        a. The first filter size of 1 looks strictly at the word to determine the output.\n        \n        b. The next 4 filters look at the word and the words following it to determine the output. The   \n        filter allows the Conv2D to look at the next word, 2 words, 3 words, and 4 words respectively to \n        determine the classification of the given data.  \n        \n    B. The second dimension looks at the word embeddings of each of the words within our filter, which again consist of 300 vectors.\n    \n3. A Bidirectional LSTM is then utilized to determine the output.\n\n    A.The Bidirectional layer creates two LSTMs where one LSTM reads a comment from left to right and the other utilizes the other LSTM to read the comment from right to left. These LSTMs are used in tandem to determine the classification of the comment. \n    \n4. A simple layer of 100 neurons are then used before a final dense layer which will produce the perdicted classification of the comment.\n"},{"metadata":{"trusted":true,"_uuid":"2e0b59c6df9326cdd2a4c482b7fb778fc0a1a153"},"cell_type":"code","source":"filter_sizes = [1,2,3,5]\nnum_filters = 128\n\ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Reshape((maxlen, embed_size, 1))(x)\n\nmaxpool_pool = []\nfor i in range(len(filter_sizes)):\n    conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n                                 kernel_initializer='glorot_uniform', activation='relu')(x) \n    maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n\nz = Concatenate(axis=1)(maxpool_pool) \nz = TimeDistributed(Bidirectional(CuDNNLSTM(256)))(z)\nz = BatchNormalization()(z)\nz = Flatten()(z)\nz = Dense(100, activation=\"relu\")(z)\nz = Dropout(.5)(z)\nz = Dense(100, activation=\"relu\")(z)\nz = Dropout(.5)(z)\nz = Dense(10, activation=\"relu\")(z)\n\noutp = Dense(1, activation=\"sigmoid\")(z)\n\nmodel = Model(inputs=inp, outputs=outp)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will fit the model below."},{"metadata":{"trusted":true,"_uuid":"536115ac569f2f36bd49580552a2def1256434cf","scrolled":false},"cell_type":"code","source":"#from keras.callbacks import EarlyStopping, ModelCheckpoint\n#earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n#mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n\nhistory = model.fit(train_X, train_y, batch_size=512, epochs=3, validation_data=(val_X, val_y))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two visualizations below look at our train and validation acurracy and the binary crossentropy loss of the binary model. It is clear that the model begins to overfit our training data after the first epoch, so we'll rerun our fit with only 1 epoch.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n\ndef plot_history(history):\n    acc = history.history[\"acc\"]\n    val_acc = history.history[\"val_acc\"]\n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, \"b\", label=\"Training acc\")\n    plt.plot(x, val_acc, \"r\", label=\"Validation acc\")\n    plt.title(\"Training and validation accuracy\")\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, \"b\", label=\"Training loss\")\n    plt.plot(x, val_loss, \"r\", label=\"Validation loss\")\n    plt.title(\"Training and validation loss\")\n    plt.legend()\n    \nplot_history(history=history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_X, train_y, batch_size=512, epochs=1, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b079c1c62f3da23cf8b62b64f1b8246da4706cf7"},"cell_type":"code","source":"y_pred = model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23758b1f999559c34b179bdef855d889f527c25c"},"cell_type":"code","source":"fpr, tpr, thresholds =roc_curve(test_y, y_pred)\nroc_auc = auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With our final layer utlizing a sigmoid as it's activation function, the neural network will produce a value between 0 and 1. We can take a standard cutoff point of .5 which produces an f1 score below of .654 which would put this f1 score at the top 20th percentile of all submissions for this Kaggle competition.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred1 = np.where(y_pred > .5, 1, 0)\nprint(\"F1 score is equivalent to {}\".format(f1_score(test_y,y_pred1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(classification_report(test_y,y_pred1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model does quite a good job at corecctly classifiyin non-toxic comments with a precision of .98 and a recall of .97. What the model doesn't excel in is in classifying the toxic comments with a precision of .63 and a recall of .68. The business value for this model is to allow someone to manual check the predicted toxic comments and decide where they need to be removed from the forum. With a recall of .68, the model will only capture 68% of the truly toxic comments. We can increase this percentage by lowering the cutoff point for the model when considering if a comment is toxic.\n\nWe will lower the cutoff to .3. This will lower precision to .4 and recall will rise to .9. This means that out of all the comments the model predicted as toxic, 90% of the true toxic comments population will be captured. The cost for this is that only 40% of the comments our model predicted to be toxic will truly be toxic. A user will then have to manually identify the truly toxic comments from the predicted toxic comments because 60% of the predicted toxic comments are not toxic."},{"metadata":{"trusted":true,"_uuid":"f560ee446063d2b27f3d8cbfeb04dd81fb3ce978"},"cell_type":"code","source":"y_pred1 = np.where(y_pred > .3, 1, 0)\nfrom sklearn.metrics import classification_report\nprint(classification_report(test_y,y_pred1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c3faf2c3250e5f079ac692029a54cb6fcd37eea"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}