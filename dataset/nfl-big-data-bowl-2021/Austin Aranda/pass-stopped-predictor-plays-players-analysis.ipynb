{"cells":[{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# NFL 2018 Defense Analyzer"},{"metadata":{},"cell_type":"markdown","source":"# Importing Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"# acquire libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport re\n\n\n#explore libraries\nfrom scipy import stats\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport os\nplt.rc(\"figure\", figsize=(12, 7))\nplt.rc(\"font\", size=14)\n\n# model libraries\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n\n\npd.set_option('display.max_columns', None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_plays_data():\n    '''\n    This function retrieves the data from a csv saved locally containing the plays data\n    '''\n    df = pd.read_csv('../input/nfl-big-data-bowl-2021/plays.csv')\n    return df\n\nprint(\"Acquire.py Loaded Successfully\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prep_plays_data():\n    '''\n    This function retrieves calls the function that acquires \n    the plays csv and prepares it for an mvp\n    '''\n    # acquire the plays csv and save it as a dataframe\n    df = get_plays_data()\n    # keep only the useful columns for mvp\n    df = df[['playDescription', 'quarter', 'down', 'yardsToGo', 'possessionTeam',\n             'offenseFormation', 'personnelO', 'defendersInTheBox', 'numberOfPassRushers', \n             'personnelD', 'typeDropback', 'gameClock', 'absoluteYardlineNumber', 'epa',\n             'playType', 'passResult', 'playResult', 'gameId', 'playId']]\n    # filter out any data that is not a pass play\n    df = df[df.playType == 'play_type_pass']\n    # creates 0 or 1 for tradtional and scramble\n    df['typeDropback'].replace({'TRADITIONAL':0,'SCRAMBLE_ROLLOUT_RIGHT':1,\n                                 'SCRAMBLE':1,'DESIGNED_ROLLOUT_RIGHT':0,\n                                 'SCRAMBLE_ROLLOUT_LEFT':1,'DESIGNED_ROLLOUT_LEFT':0,\n                                 'UNKNOWN':0}, inplace=True)\n    # ranking the teams with the most cumulative passing yards\n    df['possessionTeam'].replace({'TB': .428528, 'PIT': .417436, 'KC': .392355, 'ATL': .382256, 'LA': .376048, 'GB': .349394, 'PHI': .362307,\n                                  'NE': .358086, 'NYG': .335902, 'CLE': .335819, 'IND': .371082, 'HOU': .316947, 'SF': .303620, 'OAK': .312559,\n                                  'CAR': .314215, 'MIN': .316367, 'NO': .334329, 'LAC': .303868, 'DAL': .295094, 'DET': .303620, 'CHI': .291038,\n                                  'CIN': .273655, 'DEN': .287230, 'BAL': .296998, 'JAX': .259252, 'NYJ': .335902, 'MIA': .243442, 'WAS': .253872,\n                                  'TEN': .246753, 'BUF': .232516, 'ARI': .209504, 'SEA': .234916}, inplace=True)  \n    \n    # cleaning up the pass result column to only pass complete and pass incomplete\n    df['passResult'].replace({'C': 0,'I' : 1, 'IN' : 1}, inplace=True)\n    # create a new column that extracts \n    # \"(number) RB, (number) TE, (number) WR\"\n    # and saves it as a temporary column\n    df['tempO'] = df.personnelO.str.extract(r'(\\d RB, \\d TE, \\d WR)')\n    # create a new column that extracts \n    # \"(number) DL, (number) LB, (number) DB\"\n    # and saves it as a temporary column\n    df['tempD'] = df.personnelD.str.extract(r'(\\d DL, \\d LB, \\d DB)')\n    # keeps the rows that contain only the string in tempO column\n    df = df[df.personnelO == df.tempO]\n    # keep the rows that contain only the string in tempD column\n    df = df[df.personnelD == df.tempD]\n    # create a temporary dataframe containing the personnelO \n    # column split by a comma and space\n    temp = df.personnelO.str.split(', ', expand = True)\n    # create a new column with the number of RB on the field\n    df['RB'] = temp[0].str.replace(r' RB', '')\n    # create a new column with the number of TE on the field\n    df['TE'] = temp[1].str.replace(r' TE', '')\n    # create a new column with the number of WR on the field\n    df['WR'] = temp[2].str.replace(r' WR', '')\n    # create a temporary dataframe containing the personnelD \n    # column split by a comma and space\n    temp = df.tempD.str.split(', ', expand = True)\n    # create a new column with the number of DL on the field\n    df['DL'] = temp[0].str.replace(r' DL', '')\n    # create a new column with the number of LB on the field\n    df['LB'] = temp[1].str.replace(r' LB', '')\n    # create a new column with the number of DB on the field\n    df['DB'] = temp[2].str.replace(r' DB', '')\n    # create dummies for offensive formation\n    formation = pd.get_dummies(df.offenseFormation)\n    # Classifying traditional and rollouts into normal dropbacks\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('DESIGNED_ROLLOUT_RIGHT', 'normal'))\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('TRADITIONAL', 'normal'))\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('DESIGNED_ROLLOUT_LEFT', 'normal'))\n    # Classifying all scrambles as scrambles\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('SCRAMBLE_ROLLOUT_RIGHT', 'scramble'))\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('SCRAMBLE', 'scramble'))\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('SCRAMBLE_ROLLOUT_LEFT', 'scramble'))\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('scramble_ROLLOUT_LEFT', 'scramble'))   \n    df = df.rename(columns = {'typeDropback' : 'QB_under_pressure', 'passResult' : 'pass_stopped', 'possessionTeam': 'team_by_comp_yds'})\n    # join all dataframes together\n    df = pd.concat([df, formation], axis = 1)\n    # drop temporary columns and duplicates\n    df = df.drop(columns = {'personnelO', 'personnelD', 'tempO', 'tempD', 'playType', 'offenseFormation'})\n    # reorder the index and drop the old index\n    # Changing datatype from object to int\n    df = df.astype({'DL':'int', 'LB':'int','DB':'int'})\n    # creating formation columns\n    df['four_three'] = np.where((df['DL'] == 4) & (df['LB'] == 3),1,0)\n    df['three_four'] = np.where((df['DL'] == 3) & (df['LB'] == 4),1,0)\n    df['nickel'] = np.where(df['DB'] == 5, 1, 0)\n    df['dime'] = np.where(df['DB'] == 6, 1, 0)\n    df = df.reset_index(drop=True)\n    df = df.dropna()\n    return df\n\n\n\n### Function for returning Passing Team Rank\n\ndef passing_team_rank():\n    # brings in the plays csv\n    plays = pd.read_csv('plays.csv')\n    # returns only pass plays\n    plays = plays[plays.playType == 'play_type_pass']\n    # groups by team and sums the offense play result regardless of penalties\n    team_rank = plays.groupby('possessionTeam')['offensePlayResult'].sum().reset_index()\n    # sorts the summed results from highest to lowest\n    team_rank = team_rank.sort_values(by='offensePlayResult', ascending=False)\n    # returns the team rank\n    return team_rank\n\n\ndef explore_plays_data():\n    '''\n    This function retrieves calls the function that acquires \n    the plays csv and prepares it for an mvp\n    '''\n    # acquire the plays csv and save it as a dataframe\n    df = get_plays_data()\n    # keep only the useful columns for mvp\n    df = df[['playDescription', 'quarter', 'down', 'yardsToGo', 'possessionTeam',\n             'offenseFormation', 'personnelO', 'defendersInTheBox', 'numberOfPassRushers', \n             'personnelD', 'typeDropback', 'gameClock', 'absoluteYardlineNumber', 'epa',\n             'playType', 'passResult', 'playResult']]\n    # creates 0 or 1 for tradtional and scramble\n    df['typeDropback'].replace({'TRADITIONAL':0,'SCRAMBLE_ROLLOUT_RIGHT':1,\n                                 'SCRAMBLE':1,'DESIGNED_ROLLOUT_RIGHT':0,\n                                 'SCRAMBLE_ROLLOUT_LEFT':1,'DESIGNED_ROLLOUT_LEFT':0,\n                                 'UNKNOWN':0}, inplace=True)\n    df = df.rename(columns = {'typeDropback' : 'QB_under_pressure', 'passResult' : 'pass_stopped', 'possessionTeam': 'team_by_comp_yds'})\n    # cleaning up the pass result column to only pass complete and pass incomplete\n    df['pass_stopped'].replace({'C': 0,'I' : 1, 'IN' : 1}, inplace=True)         \n    # filter out any data that is not a pass play\n    df = df[df.playType == 'play_type_pass']\n    # create a new column that extracts \n    # \"(number) RB, (number) TE, (number) WR\"\n    # and saves it as a temporary column\n    df['tempO'] = df.personnelO.str.extract(r'(\\d RB, \\d TE, \\d WR)')\n    # create a new column that extracts \n    # \"(number) DL, (number) LB, (number) DB\"\n    # and saves it as a temporary column\n    df['tempD'] = df.personnelD.str.extract(r'(\\d DL, \\d LB, \\d DB)')\n    # keeps the rows that contain only the string in tempO column\n    df = df[df.personnelO == df.tempO]\n    # keep the rows that contain only the string in tempD column\n    df = df[df.personnelD == df.tempD]\n    # create a temporary dataframe containing the personnelO \n    # column split by a comma and space\n    temp = df.personnelO.str.split(', ', expand = True)\n    # create a new column with the number of RB on the field\n    df['RB'] = temp[0].str.replace(r' RB', '')\n    # create a new column with the number of TE on the field\n    df['TE'] = temp[1].str.replace(r' TE', '')\n    # create a new column with the number of WR on the field\n    df['WR'] = temp[2].str.replace(r' WR', '')\n    #create a temporary dataframe containing the personnelD \n    #column split by a comma and space\n    temp = df.tempD.str.split(', ', expand = True)\n    # create a new column with the number of DL on the field\n    df['DL'] = temp[0].str.replace(r' DL', '')\n    # create a new column with the number of LB on the field\n    df['LB'] = temp[1].str.replace(r' LB', '')\n    # create a new column with the number of DB on the field\n    df['DB'] = temp[2].str.replace(r' DB', '')\n    # Changing datatype from object to int\n    df = df.astype({'DL':'int', 'LB':'int','DB':'int'})\n    # creating formation columns\n    df['four_three'] = np.where((df['DL'] == 4) & (df['LB'] == 3),1,0)\n    df['three_four'] = np.where((df['DL'] == 3) & (df['LB'] == 4),1,0)\n    df['nickel'] = np.where(df['DB'] == 5, 1, 0)\n    df['dime'] = np.where(df['DB'] == 6, 1, 0)\n    \n    # drop temporary columns and duplicates\n    df = df.drop(columns = {'tempO', 'tempD'})\n    df = df.reset_index(drop=True)\n    df = df.dropna()\n    # split df into test (30%) and train_validate (70%)\n    train_validate, test = train_test_split(df, test_size=.3, random_state=123, stratify = df.pass_stopped)\n\n    # split train_validate off into train (60% of 70% = 42%) and validate (40% of 70% = 28%)\n    train, validate = train_test_split(train_validate, test_size=.4, random_state=123, stratify = train_validate.pass_stopped)\n    return train, validate, test\n\n\n\n###################################################################################\n############################## PHASE 2 ############################################\n###################################################################################\n        \n        \n############################### prep plays csv to combine with weeks ##############\n\n\ndef prep_plays_for_weeks():\n    '''\n    This function retrieves calls the function that acquires \n    the plays csv and prepares it for weeks.csv.\n    This is the same as the prepare file above without \n    train, validate, test split.\n    '''\n    # acquire the plays csv and save it as a dataframe\n    df = get_plays_data()\n    # keep only the useful columns for mvp\n    df = df[['gameId', 'playId', 'playDescription', 'quarter', 'down', 'yardsToGo', 'possessionTeam',\n             'offenseFormation', 'personnelO', 'defendersInTheBox', 'numberOfPassRushers', \n             'personnelD', 'typeDropback', 'gameClock', 'absoluteYardlineNumber', 'epa',\n             'playType', 'passResult', 'playResult']]\n    # filter out any data that is not a pass play\n    df = df[df.playType == 'play_type_pass']\n    # creates 0 or 1 for tradtional and scramble\n    df['typeDropback'].replace({'TRADITIONAL':0,'SCRAMBLE_ROLLOUT_RIGHT':1,\n                                 'SCRAMBLE':1,'DESIGNED_ROLLOUT_RIGHT':0,\n                                 'SCRAMBLE_ROLLOUT_LEFT':1,'DESIGNED_ROLLOUT_LEFT':0,\n                                 'UNKNOWN':0}, inplace=True)\n    # ranking the teams with the most cumulative passing yards\n    df['possessionTeam'].replace({'TB': 1, 'PIT': 2, 'KC': 4, 'ATL': 3, 'LA': 5, 'GB': 7, 'PHI': 8,\n                                  'NE': 9, 'NYG': 10, 'CLE': 11, 'IND': 6, 'HOU': 12, 'SF': 17, 'OAK': 16,\n                                  'CAR': 15, 'MIN': 14, 'NO': 13, 'LAC': 19, 'DAL': 18, 'DET': 20, 'CHI': 22,\n                                  'CIN': 24, 'DEN': 23, 'BAL': 21, 'JAX': 25, 'NYJ': 26, 'MIA': 28, 'WAS': 27,\n                                  'TEN': 29, 'BUF': 31, 'ARI': 32, 'SEA': 30}, inplace=True)  \n    \n    # cleaning up the pass result column to only pass complete and pass incomplete\n    df['passResult'].replace({'C': 0,'I' : 1, 'IN' : 1}, inplace=True)\n    # create a new column that extracts \n    # \"(number) RB, (number) TE, (number) WR\"\n    # and saves it as a temporary column\n    df['tempO'] = df.personnelO.str.extract(r'(\\d RB, \\d TE, \\d WR)')\n    # create a new column that extracts \n    # \"(number) DL, (number) LB, (number) DB\"\n    # and saves it as a temporary column\n    df['tempD'] = df.personnelD.str.extract(r'(\\d DL, \\d LB, \\d DB)')\n    # keeps the rows that contain only the string in tempO column\n    df = df[df.personnelO == df.tempO]\n    # keep the rows that contain only the string in tempD column\n    df = df[df.personnelD == df.tempD]\n    # create a temporary dataframe containing the personnelO \n    # column split by a comma and space\n    temp = df.personnelO.str.split(', ', expand = True)\n    # create a new column with the number of RB on the field\n    df['RB'] = temp[0].str.replace(r' RB', '')\n    # create a new column with the number of TE on the field\n    df['TE'] = temp[1].str.replace(r' TE', '')\n    # create a new column with the number of WR on the field\n    df['WR'] = temp[2].str.replace(r' WR', '')\n    # create a temporary dataframe containing the personnelD \n    # column split by a comma and space\n    temp = df.tempD.str.split(', ', expand = True)\n    # create a new column with the number of DL on the field\n    df['DL'] = temp[0].str.replace(r' DL', '')\n    # create a new column with the number of LB on the field\n    df['LB'] = temp[1].str.replace(r' LB', '')\n    # create a new column with the number of DB on the field\n    df['DB'] = temp[2].str.replace(r' DB', '')\n    # create dummies for offensive formation\n    formation = pd.get_dummies(df.offenseFormation)\n    # Classifying traditional and rollouts into normal dropbacks\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('DESIGNED_ROLLOUT_RIGHT', 'normal'))\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('TRADITIONAL', 'normal'))\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('DESIGNED_ROLLOUT_LEFT', 'normal'))\n    # Classifying all scrambles as scrambles\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('SCRAMBLE_ROLLOUT_RIGHT', 'scramble'))\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('SCRAMBLE', 'scramble'))\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('SCRAMBLE_ROLLOUT_LEFT', 'scramble'))\n    df.typeDropback = df.typeDropback.apply(lambda value : str(value).replace('scramble_ROLLOUT_LEFT', 'scramble'))   \n    df = df.rename(columns = {'typeDropback' : 'QB_under_pressure', 'passResult' : 'pass_stopped', 'possessionTeam': 'team_by_comp_yds'})\n    # join all dataframes together\n    df = pd.concat([df, formation], axis = 1)\n    # drop temporary columns and duplicates\n    df = df.drop(columns = {'personnelO', 'personnelD', 'tempO', 'tempD', 'playType', 'offenseFormation'})\n    # reorder the index and drop the old index\n    # Changing datatype from object to int\n    df = df.astype({'DL':'int', 'LB':'int','DB':'int'})\n    # crreating formation columns\n    df['four_three'] = np.where((df['DL'] == 4) & (df['LB'] == 3),1,0)\n    df['three_four'] = np.where((df['DL'] == 3) & (df['LB'] == 4),1,0)\n    df['nickel'] = np.where(df['DB'] == 5, 1, 0)\n    df['dime'] = np.where(df['DB'] == 6, 1, 0)\n    df = df.reset_index(drop=True)\n    df = df.dropna()\n    return df\n\nprint(\"Prep.py Loaded Successfully\")\n\n\n################################### prep week csv ###########################\n\ndef filter_nfl_weeks():\n    '''\n    This function creates a copy of the weeks.csv\n    that only contain pass_forward\n    '''\n    \n    for i in range(1,18):\n        # read a week csv\n        df = pd.read_csv('../input/nfl-big-data-bowl-2021/week' + str(i) + '.csv')\n        # keep only 5 events from the df\n        df = df[(df.event == 'pass_forward')]\n        # fill null values in position to none\n        df.position = df.position.fillna('BALL')\n        # reset the index\n        df.reset_index(drop=True)\n        # save the df as a new csv\n        df.to_csv('week' + str(i) + 'filtered.csv', index=False)\n        # print the week number after you run through the above steps\n        print(f'{i}')\n    #had to drop playId '3640' because it was assigned pass_forward on two different frames\n    week9 = pd.read_csv('week9filtered.csv')\n    week9 = week9[week9.playId != 3640]\n    week9.to_csv('week9filtered.csv')\n    #had to drop plaId '2650' because it was assigned pass_forward on three different frames\n    week10 = pd.read_csv('week10filtered.csv')\n    week10 = week10[week10.playId != 2650]\n    week10.to_csv('week10filtered.csv')\n    \n\n################################## getting new features ###########################\n    \n    \ndef combine_week_and_plays(week_num):\n    '''\n    This function combines the week.csv's\n    with the plays data and returns\n    playid and week number, name and distance \n    of closest defender with their coordindates\n    '''\n    ##############################################################\n    #first we load our prepped plays and create a unique idetifier\n    ##############################################################\n    #loading prepped plays data\n    plays = prep_plays_for_weeks()\n    #changing gameid into string\n    plays.gameId = plays.gameId.astype(str)\n    #changing playid into string\n    plays.playId = plays.playId.astype(str)\n    #concat to create a unique identifier\n    plays['playid'] = plays.gameId + plays.playId\n    #drop old columns\n    plays = plays.drop(columns = {'gameId', 'playId'})\n    #drop any duplicates\n    plays.drop_duplicates(inplace=True)\n    ##############################################################\n    #second we load our week data and create a unique identifier\n    ##############################################################\n    #read in week data that contains only plays when pass is being released\n    week = pd.read_csv('week' + str(week_num) + 'filtered.csv')\n    #changing gameid into string\n    week.gameId = week.gameId.astype(str)\n    #changing playid into string\n    week.playId = week.playId.astype(str)\n    #concat to create a unique identifier\n    week['playid'] = week.gameId + week.playId\n    #drop old columns\n    week = week.drop(columns = {'gameId', 'playId'})\n    #drop any duplicates\n    week.drop_duplicates(inplace=True)\n    ##############################################################\n    #third we merge the dataframe\n    ##############################################################\n    #merge plays and week1 so we can have a play description\n    df = pd.merge(plays, week, left_on = 'playid', right_on = 'playid', how = 'inner')\n    #drop duplicates\n    df.drop_duplicates(inplace=True)\n    ##############################################################\n    #forth we extract the intended receiver from the play description\n    ##############################################################\n    #extracting names from play description\n    #second name will be the intended reciever\n    desc = df.playDescription.str.findall(r'(\\b[A-Z]+\\.\\b[A-Z]+\\w+)').apply(','.join)\n    #split the desc names by comma\n    temp = desc.str.split(',', expand = True)\n    #saving name of receiver\n    df['receiver_last_name'] = temp[1]\n    #splitting first and last name of player\n    temp2 = df.displayName.str.split(' ', expand = True)\n    #getting first initial\n    initial = temp2[0].astype(str).str[0]\n    #getting last name\n    last_name = temp2[1]\n    #saving player last name as first inital dot last name\n    df['player_last_name'] = initial + '.' + last_name\n    #filtering out the football\n    df = df[df.displayName != 'Football']\n    #resetting the index\n    df = df.reset_index(drop=True)\n    #labeling incorrect receivers with their surname\n    df.loc[(df.receiver_last_name == 'J.Smith'),'receiver_last_name'] = 'J.Smith-Schuster'\n    df.loc[(df.receiver_last_name == 'A.Seferian'),'receiver_last_name'] = 'A.Seferian-Jenkins'\n    df.loc[(df.receiver_last_name == 'R.Seals'),'receiver_last_name'] = 'R.Seals-Jones'\n    df.drop_duplicates(inplace=True)\n    #########################################################################################################\n    #fifth we create a function that will find the distance of the closest defender to the intended receiver\n    ########################################################################################################\n    newdf = pd.DataFrame(columns = ['playid', 'closest_dist', 'closest_x', 'closest_y', 'defender_receiver', 'week'])\n    playids = [play for play in df.playid.unique()]\n\n    #loop through each playid in playids\n    for play in playids:\n        #reset shortest distance\n        closest_distance = 100\n        #reset shortest x\n        closest_x = 0\n        #reset shortest y\n        closest_y = 0\n        #filter for all players in current play\n        current_play = df[df.playid == play]\n        #create a dataframe of offensive players\n        offense = current_play[(current_play.position == 'QB') | (current_play.position == 'RB') | (current_play.position == 'WR') | (current_play.position == 'FB') | (current_play.position == 'HB') | (current_play.position == 'TE')]\n        #create a dataframe of defensive players\n        defense = current_play[(current_play.position == 'CB') | (current_play.position == 'OLB') | (current_play.position == 'FS') | (current_play.position == 'SS') | (current_play.position == 'ILB') | (current_play.position == 'MLB') | (current_play.position == 'LB') | (current_play.position == 'DB') | (current_play.position == 'S') | (current_play.position == 'DL') | (current_play.position == 'DE') | (current_play.position == 'NT')]\n        #for x in coordinates of players\n        for name in defense.displayName:\n            if (offense.receiver_last_name == offense.player_last_name).any():\n                #retrieve y coordinate of this player\n                x = defense.loc[defense.displayName == name].x.item()\n                #retrieve y coordinate of this player\n                y = defense.loc[defense.displayName == name].y.item()\n                #retrive x coordinate of reciever\n                x1= offense.loc[offense.receiver_last_name == offense.player_last_name].x.item()\n                # retrieve y coordinate of reciever\n                y1= offense.loc[offense.receiver_last_name == offense.player_last_name].y.item()\n                #solve for distance\n                distance = ((x-x1)**2+(y-y1)**2)**(1/2)\n                #if the distance is the shortest distance\n                if distance < closest_distance:\n                    #save the distance\n                    closest_distance = distance\n                    #save the x coordinate\n                    closest_x = x\n                    #save the y coordinate\n                    closest_y = y\n                    #save the defender name\n                    def_name = name\n            else:\n                #fill with unrealistic values \n                closest_distance = 0\n                closest_x = 0\n                closest_y = 0\n                def_name = \"unknown\"\n        newdf = newdf.append({'playid': play, 'closest_dist': closest_distance, 'closest_x': closest_x, 'closest_y': closest_y, 'defender_receiver': def_name, 'week': week_num}, ignore_index=True)\n    return newdf\n\n\n################################ adding new features to original df ##########################\n\n\ndef combine_all_weeks_and_plays():\n    '''\n    This function creates new features from week.csv's\n    and adds them to original prep_plays_for_weeks\n    '''\n    #create new features from week 1\n    df = combine_week_and_plays(1)\n    print(1)\n    #create new features for remaining weeks\n    for i in range(2,18):\n        newdf = combine_week_and_plays(i)\n        #append new features from weeks to each other\n        df = df.append(newdf).reset_index(drop=True)\n        #print week number when done\n        print(i)\n    #load prepped plays df    \n    plays = prep_plays_for_weeks()\n    #changing gameid into string\n    plays.gameId = plays.gameId.astype(str)\n    #changing playid into string\n    plays.playId = plays.playId.astype(str)\n    #concat to create a unique identifier\n    plays['playid'] = plays.gameId + plays.playId\n    #drop old columns\n    plays = plays.drop(columns = {'gameId', 'playId'})\n    #drop any duplicates\n    plays.drop_duplicates(inplace=True)  \n    #merge new features with old\n    total_df = pd.merge(plays, df, left_on = 'playid', right_on = 'playid', how = 'inner')\n    return total_df\n\ndef get_weeksnplays_data():\n    \n    ''' This function will acquire the csv file needed to work with the season data, if there is not csv saved,\n    then it ill iterate through the function above and create one for you'''\n\n    if os.path.isfile('final.csv'):\n        df = pd.read_csv('final.csv')\n        df = df.drop(columns = {'Unnamed: 0'})\n        print('Dataframe Ready For Use')\n    else:\n        filter_nfl_weeks()\n        df = combine_all_weeks_and_plays()\n        df.to_csv('final.csv')\n    return df\n\n\n################################ finding top defenders in NFL ##########################\n\ndef top_defenders():\n    '''\n    This function will create a dataframe of the best defenders \n    in the NFL in regards to defending the intended receiver\n    '''\n    # loading whole dataframe\n    filter_nfl_weeks()\n    df = combine_all_weeks_and_plays()\n    # top 100 defenders directly involved in a pass play\n    total_plays = df[df.defender_receiver != 'unknown'].defender_receiver.value_counts().head(100)\n    # transform total_plays into a dataframe\n    total_plays = pd.DataFrame(total_plays)\n    # reset index\n    total_plays = total_plays.reset_index()\n    # rename columns\n    total_plays = total_plays.rename(columns = {'index': 'defender', 'defender_receiver': 'total_plays'})\n    # creating a temp df for passes stopped\n    temp = df[df.pass_stopped == 1]\n    #top 10 defenders who were directly involved in stopping the pass play\n    top_10 = temp[temp.defender_receiver != 'unknown'].defender_receiver.value_counts().head(10)\n    # transform top_10 into a dataframe\n    top_10 = pd.DataFrame(top_10)\n    # reset index\n    top_10 = top_10.reset_index()\n    # rename columns\n    top_10 = top_10.rename(columns = {'index': 'defender', 'defender_receiver': 'stopped_passes'})\n    # merging dataframes to find top defenders\n    top_defenders = pd.merge(top_10, total_plays, how= 'inner')\n    # finding precentage of passes stopped\n    top_defenders['stopped_pass_perc'] = (top_defenders.stopped_passes / top_defenders.total_plays).round(2)\n    # sorting values in dataframe\n    defenders = top_defenders.sort_values('stopped_pass_perc', ascending = False)\n    return defenders","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prep_nfl():\n    df = clean_season()\n    df['force_per_second'] = (((df.weight * 0.45359237)/ (9.8)) * (df.a * .9144)).round(2)\n    df['uniqueId'] = (df.gameId.astype(str) + df.playId.astype(str)).astype(int)\n    df2 = prep_plays_data()\n    df2['uniqueId'] = (df2.gameId.astype(str) + df2.playId.astype(str)).astype(int)\n    df = pd.merge(df, df2, how='left', on='uniqueId')\n    df = df.drop(columns = {'playId_y', 'gameId_y', 'pass_stopped_y'})\n    df = df.rename(columns = {'gameId_x': 'gameId','playId_x': 'playId', 'pass_stopped_x': 'pass_stopped'})\n    df = df.dropna()\n    df3 = get_weeksnplays_data()\n    df3['uniqueId'] = df3.playid.rename({'playid': 'uniqueId'}).astype(int)\n    df = pd.merge(df, df3, how='left', on='uniqueId')\n    df = df.drop(columns = {'week_y', 'playid', 'playDescription_y', 'quarter_y', 'down_y',\n                            'yardsToGo_y', 'team_by_comp_yds_y', 'defendersInTheBox_y',\n                            'numberOfPassRushers_y', 'QB_under_pressure_y', 'gameClock_y',\n                            'absoluteYardlineNumber_y', 'epa_y', 'pass_stopped_y', 'playResult_y',\n                            'RB_y', 'TE_y', 'WR_y', 'DL_y', 'LB_y', 'DB_y', 'EMPTY_y', 'I_FORM_y',\n                            'JUMBO_y', 'PISTOL_y', 'SHOTGUN_y', 'SINGLEBACK_y', 'WILDCAT_y',\n                            'four_three_y', 'three_four_y', 'nickel_y', 'dime_y'})\n    df = df.rename(columns = {'week_x': 'week', 'playDescription_x': 'playDescription',\n                              'quarter_x': 'quarter', 'down_x': 'down', 'yardsToGo_x': 'yardsToGo',\n                              'team_by_comp_yds_x': 'team_by_comp_yds', 'defendersInTheBox_x': 'defendersInTheBox',\n                              'numberOfPassRushers_x': 'numberOfPassRushers', 'QB_under_pressure_x': 'QB_under_pressure',\n                              'gameClock_x': 'gameClock','absoluteYardlineNumber_x': 'absoluteYardlineNumber',\n                              'epa_y': 'epa', 'pass_stopped_x': 'pass_stopped', 'playResult_x': 'playResult',\n                              'RB_x': 'RB', 'TE_x': 'TE', 'WR_x': 'WR', 'DL_x': 'DL', 'LB_x': 'LB', 'DB_x': 'DB',\n                              'EMPTY_x': 'EMPTY', 'I_FORM_x': 'I_FORM','JUMBO_x': 'JUMBO', 'PISTOL_x': 'PISTOL',\n                              'SHOTGUN_x': 'SHOTGUN', 'SINGLEBACK_x': 'SINGLEBACK', 'WILDCAT_x': 'WILDCAT',\n                              'four_three_x': 'four_three', 'three_four_x': 'three_four', 'nickel_x': 'nickel',\n                              'dime_x': 'dime', 'pass_stopped_x': 'pass_stopped', 'epa_x': 'epa'})\n    df = df.dropna()\n    df.to_csv('clean_nfl.csv')\n    print('Prep_NFL.py Loaded Successfully')\n    return df\n\ndef get_nfl_data():\n    \n    ''' This function will acquire the csv file needed to work with the season data, if there is not csv saved,\n    then it ill iterate through the function above and create one for you'''\n    \n    if os.path.isfile('clean_nfl.csv'):\n        df = pd.read_csv('clean_nfl.csv')\n        df = df.drop(columns = {'Unnamed: 0'})\n        print('Dataframe Ready For Use')\n    else:\n        df = prep_nfl()\n        print('Dataframe Ready For Use')\n    return df\n\nprint('Prep_NFL.py Imported Successfully')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"########################### ALT Prep Season function ############################\n\ndef prep_season():\n    '''\n    This function acquires the players csv and prepares\n    it to merge with other csv's\n    '''\n    \n    # Acquire the players csv\n    players= pd.read_csv('../input/nfl-big-data-bowl-2021/players.csv')\n    # Convert the birthdate to datetime to get rid of different date formats\n    players.birthDate = pd.to_datetime(players.birthDate)\n    # Creating a age column that takes the start date of the 2018 season and subtracts the birthdate\n    players['age'] = (pd.to_datetime('09/06/2018') - players.birthDate).astype('<m8[Y]')\n    # Function that converts heights\n    def conv_height(value):\n        if len(re.findall(r'(\\d+)-(\\d+)', value)) > 0:\n            feet = int(re.findall(r'(\\d+)-(\\d+)', value)[0][0])\n            inches = int(re.findall(r'(\\d+)-(\\d+)', value)[0][1])\n            return (feet * 12) + inches\n        else:\n            return value\n    # Changing height column to equal just inches\n    players['height'] = players.height.apply(conv_height)\n    players['height'] = players['height'].astype(int)\n    \n    # Bringing in the week csv's\n    df2 = players\n    week1 = pd.read_csv('../input/nfl-big-data-bowl-2021/week1.csv')\n    week1['week'] = 1\n    week2 = pd.read_csv('../input/nfl-big-data-bowl-2021/week2.csv')\n    week2['week'] = 2\n    week3 = pd.read_csv('../input/nfl-big-data-bowl-2021/week3.csv')\n    week3['week'] = 3\n    week4 = pd.read_csv('../input/nfl-big-data-bowl-2021/week4.csv')\n    week4['week'] = 4\n    week5 = pd.read_csv('../input/nfl-big-data-bowl-2021/week5.csv')\n    week5['week'] = 5\n    week6 = pd.read_csv('../input/nfl-big-data-bowl-2021/week6.csv')\n    week6['week'] = 6\n    week7 = pd.read_csv('../input/nfl-big-data-bowl-2021/week7.csv')\n    week7['week'] = 7\n    week8 = pd.read_csv('../input/nfl-big-data-bowl-2021/week8.csv')\n    week8['week'] = 8\n    week9 = pd.read_csv('../input/nfl-big-data-bowl-2021/week9.csv')\n    week9['week'] = 9\n    week10 = pd.read_csv('../input/nfl-big-data-bowl-2021/week10.csv')\n    week10['week'] = 10\n    week11 = pd.read_csv('../input/nfl-big-data-bowl-2021/week11.csv')\n    week11['week'] = 11\n    week12 = pd.read_csv('../input/nfl-big-data-bowl-2021/week12.csv')\n    week12['week'] = 12\n    week13 = pd.read_csv('../input/nfl-big-data-bowl-2021/week13.csv')\n    week13['week'] = 13\n    week14 = pd.read_csv('../input/nfl-big-data-bowl-2021/week14.csv')\n    week14['week'] = 14\n    week15 = pd.read_csv('../input/nfl-big-data-bowl-2021/week15.csv')\n    week15['week'] = 15\n    week16 = pd.read_csv('../input/nfl-big-data-bowl-2021/week16.csv')\n    week16['week'] = 16\n    week17 = pd.read_csv('../input/nfl-big-data-bowl-2021/week17.csv')\n    week17['week'] = 17\n    df1 = pd.concat([week1, week2, week3, week4, week5, week6, week7, week8, week9,\n                    week10, week11, week12, week13, week14, week15, week16, week17])\n    df = pd.merge(df1, df2, how='inner', on='displayName')\n    df = df.drop(columns = {'position_y', 'nflId_y'})\n    df = df.rename(columns = {'position_x':'position', 'nflId_x': 'nflId'})\n    \n    # adding columns to measure time taken to travel and force of players\n    df['time_since_last_x'] = (df.dis / df.s).round(4)        \n    # Calculate force by converting the weight to Kg's then divide by gravity (9.81 m/s^2) * acceleration\n    # This will provide a players force in Newtons\n    df['force_per_second'] = (((df.weight * 0.45359237)/ (9.8)) * (df.s / 1.094)).round(4)\n    \n    \n    # replacing the event column with target variable\n    df.drop(df.index[df['event'] == 'None'], inplace = True)\n    df.drop(df.index[df['event'] == 'ball_snap'], inplace = True)\n    df.drop(df.index[df['event'] == 'pass_forward'], inplace = True)\n    df.drop(df.index[df['event'] == 'pass_arrived'], inplace = True)\n    df.drop(df.index[df['event'] == 'tackle'], inplace = True)\n    df.drop(df.index[df['event'] == 'first_contact'], inplace = True)\n    df.drop(df.index[df['event'] == 'play_action'], inplace = True)\n    df.drop(df.index[df['event'] == 'out_of_bounds'], inplace = True)\n    df.drop(df.index[df['event'] == 'line_set'], inplace = True)\n    df.drop(df.index[df['event'] == 'man_in_motion'], inplace = True)\n    df.drop(df.index[df['event'] == 'touchdown'], inplace = True)\n    df.drop(df.index[df['event'] == 'pass_tipped'], inplace = True)\n    df.drop(df.index[df['event'] == 'pass_outcome_touchdown'], inplace = True)\n    df.drop(df.index[df['event'] == 'fumble'], inplace = True)\n    df.drop(df.index[df['event'] == 'shift'], inplace = True)\n    df.drop(df.index[df['event'] == 'fumble_defense_recovered'], inplace = True)\n    df.drop(df.index[df['event'] == 'handoff'], inplace = True)\n    df.drop(df.index[df['event'] == 'pass_shovel'], inplace = True)\n    df.drop(df.index[df['event'] == 'penalty_flag'], inplace = True)\n    df.drop(df.index[df['event'] == 'fumble_offense_recovered'], inplace = True)\n    df.drop(df.index[df['event'] == 'touchback'], inplace = True)\n    df.drop(df.index[df['event'] == 'penalty_accepted'], inplace = True)\n    df.drop(df.index[df['event'] == 'field_goal_blocked'], inplace = True)\n    df.drop(df.index[df['event'] == 'pass_lateral'], inplace = True)\n    df.drop(df.index[df['event'] == 'lateral'], inplace = True)\n    df.drop(df.index[df['event'] == 'snap_direct'], inplace = True)\n    df.drop(df.index[df['event'] == 'run_pass_option'], inplace = True)\n    df.drop(df.index[df['event'] == 'huddle_break_offense'], inplace = True)\n    df.drop(df.index[df['event'] == 'huddle_start_offense'], inplace = True)\n    df.drop(df.index[df['event'] == 'qb_strip_sack'], inplace = True)\n    df.drop(df.index[df['event'] == 'timeout_home'], inplace = True)\n    df.drop(df.index[df['event'] == 'qb_sack'], inplace = True)\n    df.drop(df.index[df['event'] == 'qb_spike'], inplace = True)\n    df.drop(df.index[df['event'] == 'run'], inplace = True)\n    df.drop(df.index[df['event'] == 'punt_fake'], inplace = True)\n    df.drop(df.index[df['event'] == 'field_goal_fake'], inplace = True)\n    df.drop(df.index[df['event'] == 'safety'], inplace = True)\n    df.drop(df.index[df['event'] == 'field_goal_play'], inplace = True)\n    df['event'].replace({'pass_outcome_caught': 0,'pass_outcome_incomplete' : 1,'pass_outcome_interception' : 1}, inplace=True)\n    df.reset_index(inplace=True)\n\n    # Dropping undefined route\n    df.drop(df.index[df['route'] == 'undefined'], inplace =True) \n    # Write DataFrame to csv file for future use\n    df.to_csv('season.csv')\n    print('CSV Successfully Created')\n    return df\n\n\ndef get_season_data():\n    \n    ''' This function will acquire the csv file needed to work with the season data, if there is not csv saved,\n    then it ill iterate through the function above and create one for you'''\n    \n    if os.path.isfile('season.csv'):\n        df = pd.read_csv('season.csv')\n        df = df.drop(columns = {'Unnamed: 0', 'index'})\n        print('Season Data Imported Successfully')\n    else:\n        df = prep_season()\n        df = df.drop(columns = {'index'})\n        print('Season Data Imported Successfully')\n    return df\n\ndef clean_season():\n    df = get_season_data()\n    df.route.fillna(value='NONE', inplace=True)\n    df = df.dropna()\n    df = df.rename(columns = {'event':'pass_stopped'})\n    # 1 is play shifted to left side of field, 0 is play shifted to right side\n    df['playDirection'] = df.playDirection.replace({'left': 1, 'right': 0})\n    df['is_home'] = df.team.replace({'home': 1, 'away': 0})\n    df = df.drop(columns = {'team'})\n    df['time_since_last_x'] = df.time_since_last_x.replace([np.inf, -np.inf], np.nan)\n    df['time_since_last_x'] = df.time_since_last_x.replace([np.inf, -np.inf], np.nan).dropna()\n    df['is_defense'] = df.position.replace({'QB': 0, 'SS': 1, 'WR': 0, 'FS': 1, 'RB': 0, 'MLB': 1, 'CB': 1, 'TE': 0,\n                                                 'LB': 1, 'FB': 0, 'OLB': 1,'HB': 0, 'ILB': 1, 'DL': 1, 'DB': 1,\n                                                 'S': 1, 'NT': 1, 'DE': 1, 'P': 0, 'LS': 0, 'K': 0, 'DT': 1})\n    return df\n\ndef get_viz(df):\n    agedf = df.groupby('age')['event'].sum().reset_index()\n    agedf = agedf.sort_values(by='event', ascending=False).head(7)\n    sns.barplot(data=agedf, x='age', y= 'event', palette = 'mako')\n    plt.title('Age and Incompletions', fontsize=13)\n    plt.xlabel('age', fontsize=13)\n    plt.ylabel('Incomplete Passes', fontsize=13)\n    plt.show()\n\n    ### College\n    collegedf = df.groupby('collegeName')['event'].sum().reset_index()\n    college20 = collegedf.sort_values(by='event', ascending=False).head(7)\n    sns.barplot(data=college20, x='collegeName', y= 'event',palette='mako' )\n    plt.title('College and Incompletions', fontsize=13)\n    plt.xlabel('College', fontsize=13)\n    plt.ylabel('Incomplete Passes', fontsize=13)\n    #plt.xticks(rotation=30)\n    plt.show()\n\n    ### Height\n    heightdf = df.groupby('height')['event'].sum().reset_index()\n    heightdf = heightdf.sort_values(by='event', ascending=False).head(7)\n    sns.barplot(data=heightdf, x='height', y= 'event', palette='mako')\n    plt.title('Height and Incompletions', fontsize=13)\n    plt.xlabel('height', fontsize=13)\n    plt.ylabel('Incomplete Passes', fontsize=13)\n    plt.xticks(rotation=30)\n    plt.show()\n\n    ### Weight\n    weightdf = df.groupby('weight')['event'].sum().reset_index()\n    weight20 = weightdf.sort_values(by='event', ascending=False).head(7)\n    sns.barplot(data=weight20, x='weight', y= 'event', palette='mako')\n    plt.title('Weight and Incompletions', fontsize=13)\n    plt.xlabel('Weight', fontsize=13)\n    plt.ylabel('Incomplete Passes', fontsize=13)\n    plt.xticks(rotation=30)\n    plt.show()\n    \n    \n    \n################################ finding top defenders in NFL ##########################\n\ndef top_defenders():\n    '''\n    This function will create a dataframe of the best defenders \n    in the NFL in regards to defending the intended receiver\n    '''\n    df = filter_nfl_weeks()\n    df = combine_all_weeks_and_plays()\n    # top 100 defenders directly involved in a pass play\n    total_plays = df[df.defender_receiver != 'unknown'].defender_receiver.value_counts().head(100)\n    # transform total_plays into a dataframe\n    total_plays = pd.DataFrame(total_plays)\n    # reset index\n    total_plays = total_plays.reset_index()\n    # rename columns\n    total_plays = total_plays.rename(columns = {'index': 'defender', 'defender_receiver': 'total_plays'})\n    # creating a temp df for passes stopped\n    temp = df[df.pass_stopped == 1]\n    #top 10 defenders who were directly involved in stopping the pass play\n    top_10 = temp[temp.defender_receiver != 'unknown'].defender_receiver.value_counts().head(10)\n    # transform top_10 into a dataframe\n    top_10 = pd.DataFrame(top_10)\n    # reset index\n    top_10 = top_10.reset_index()\n    # rename columns\n    top_10 = top_10.rename(columns = {'index': 'defender', 'defender_receiver': 'stopped_passes'})\n    # merging dataframes to find top defenders\n    top_defenders = pd.merge(top_10, total_plays, how= 'inner')\n    # finding precentage of passes stopped\n    top_defenders['stopped_pass_perc'] = (top_defenders.stopped_passes / top_defenders.total_plays).round(2)\n    # sorting values in dataframe\n    defenders = top_defenders.sort_values('stopped_pass_perc', ascending = False)\n    return defenders\n\n\n\nprint('Prep_Season.py Loaded Successfully')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Turn off warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# split_scale\n# import split_scale\n\n# libraries needed for preparing the data:\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer, PowerTransformer, RobustScaler, MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nimport sklearn.preprocessing\nfrom sklearn.cluster import KMeans\n\ndef train_validate_test(df):\n    '''\n    this function takes in a dataframe and splits it into 3 samples, \n    a test, which is 30% of the entire dataframe, \n    a validate, which is 28% of the entire dataframe,\n    and a train, which is 42% of the entire dataframe. \n    It then splits each of the 3 samples into a dataframe with independent variables\n    and a series with the dependent, or target variable. \n    The function returns 3 dataframes and 3 series:\n    X_train (df) & y_train (series), X_validate & y_validate, X_test & y_test. \n    '''\n    # split df into test (30%) and train_validate (70%)\n    train_validate, test = train_test_split(df, test_size=.3, random_state=123, stratify = df.pass_stopped)\n\n    # split train_validate off into train (60% of 70% = 42%) and validate (40% of 70% = 28%)\n    train, validate = train_test_split(train_validate, test_size=.4, random_state=123, stratify = train_validate.pass_stopped)\n\n\n    # split train into X (dataframe, drop target) & y (series, keep target only)\n    X_train = train.drop(columns= ['time', 'nflId', 'displayName',\n                                  'jerseyNumber', 'frameId', 'gameId', 'playId',\n                                  'route', 'week', 'birthDate'])\n    X_validate = validate.drop(columns= ['time', 'nflId', 'displayName',\n                                  'jerseyNumber', 'frameId', 'gameId', 'playId',\n                                  'route', 'week', 'birthDate'])\n    X_test = test.drop(columns= ['time', 'nflId', 'displayName',\n                                  'jerseyNumber', 'frameId', 'gameId', 'playId',\n                                  'route', 'week', 'birthDate'])\n\n    y_train = train[['pass_stopped']]\n    y_validate = validate[['pass_stopped']]\n    y_test = test[['pass_stopped']]\n    return X_train, y_train, X_validate, y_validate, X_test, y_test\n\n\ndef min_max_scale(X_train, X_validate, X_test):\n    '''\n    this function takes in 3 dataframes with the same columns, \n    a list of numeric column names (because the scaler can only work with numeric columns),\n    and fits a min-max scaler to the first dataframe and transforms all\n    3 dataframes using that scaler. \n    it returns 3 dataframes with the same column names and scaled values. \n    '''\n    # create the scaler object and fit it to X_train (i.e. identify min and max)\n    # if copy = false, inplace row normalization happens and avoids a copy (if the input is already a numpy array).\n    X_train = X_train.drop(columns = {'collegeName', 'position', 'pass_stopped',\n                                      'playDescription', 'uniqueId', 'gameClock',\n                                     'playResult', 'defender_receiver'})\n    X_validate = X_validate.drop(columns = {'collegeName', 'position', 'pass_stopped',\n                                            'playDescription', 'uniqueId', 'gameClock',\n                                           'playResult', 'defender_receiver'})\n    X_test = X_test.drop(columns = {'collegeName', 'position', 'pass_stopped',\n                                    'playDescription', 'uniqueId', 'gameClock',\n                                   'playResult', 'defender_receiver'})\n    scaler = MinMaxScaler(copy = True).fit(X_train)\n\n    X_train_scaled = scaler.transform(X_train)\n    X_validate_scaled = scaler.transform(X_validate)\n    X_test_scaled = scaler.transform(X_test)\n    X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_train.columns.values).set_index([X_train.index.values])\n    X_validate_scaled = pd.DataFrame(X_validate_scaled, columns =\n                                     X_validate.columns.values).set_index([X_validate.index.values])\n    X_test_scaled = pd.DataFrame(X_test_scaled, columns = X_test.columns.values).set_index([X_test.index.values])\n\n\n    return X_train_scaled, X_validate_scaled, X_test_scaled\n\ndef add_clusters(X_train_scaled, X_validate_scaled, X_test_scaled, X_train, X_validate, X_test):\n    X1_train = X_train_scaled[['height', 'weight', 'age', 'RB', 'TE', 'WR', 'DL',\n                               'LB', 'DB', 's', 'a', 'dis']]\n    X1_val = X_validate_scaled[['height', 'weight', 'age', 'RB', 'TE', 'WR', 'DL',\n                                'LB', 'DB', 's', 'a', 'dis']]\n    X1_test = X_test_scaled[['height', 'weight', 'age', 'RB', 'TE', 'WR', 'DL',\n                             'LB', 'DB', 's', 'a', 'dis']]\n    kmeans = KMeans(n_clusters=4)\n    kmeans.fit(X1_train)\n    X_train_scaled['pos_att_cluster'] = kmeans.predict(X1_train)\n    X_train['pos_att_cluster'] = kmeans.predict(X1_train)\n    X_validate_scaled['pos_att_cluster'] = kmeans.predict(X1_val)\n    X_validate['pos_att_cluster'] = kmeans.predict(X1_val)\n    X_test_scaled['pos_att_cluster'] = kmeans.predict(X1_test)\n    X_test['pos_att_cluster'] = kmeans.predict(X1_test)\n\n\n    X2_train = X_train_scaled[['x', 'y', 'dis', 'o', 'dir', 'playDirection', 'quarter', 'down',\n                               'yardsToGo', 'numberOfPassRushers', 'QB_under_pressure']]\n    X2_val = X_validate_scaled[['x', 'y', 'dis', 'o', 'dir', 'playDirection', 'quarter', 'down',\n                               'yardsToGo', 'numberOfPassRushers', 'QB_under_pressure']]\n    X2_test = X_test_scaled[['x', 'y', 'dis', 'o', 'dir', 'playDirection', 'quarter', 'down',\n                               'yardsToGo', 'numberOfPassRushers', 'QB_under_pressure']]\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(X2_train)\n    X_train_scaled['play_cluster'] = kmeans.predict(X2_train)\n    X_train['play_cluster'] = kmeans.predict(X2_train)\n    X_validate_scaled['play_cluster'] = kmeans.predict(X2_val)\n    X_validate['play_cluster'] = kmeans.predict(X2_val)\n    X_test_scaled['play_cluster'] = kmeans.predict(X2_test)\n    X_test['play_cluster'] = kmeans.predict(X2_test)\n\n    X3_train = X_train_scaled[['RB', 'TE', 'WR', 'DL', 'LB', 'DB', 'I_FORM', 'JUMBO',\n                               'PISTOL', 'SHOTGUN', 'SINGLEBACK', 'WILDCAT', 'four_three', 'three_four',\n                               'nickel', 'dime']]\n    X3_val = X_validate_scaled[['RB', 'TE', 'WR', 'DL', 'LB', 'DB', 'I_FORM', 'JUMBO',\n                               'PISTOL', 'SHOTGUN', 'SINGLEBACK', 'WILDCAT', 'four_three', 'three_four',\n                               'nickel', 'dime']]\n    X3_test = X_test_scaled[['RB', 'TE', 'WR', 'DL', 'LB', 'DB', 'I_FORM', 'JUMBO',\n                               'PISTOL', 'SHOTGUN', 'SINGLEBACK', 'WILDCAT', 'four_three', 'three_four',\n                               'nickel', 'dime']]\n    kmeans = KMeans(n_clusters=4)\n    kmeans.fit(X3_train)\n    X_train_scaled['offvsdef_cluster'] = kmeans.predict(X3_train)\n    X_train['offvsdef_cluster'] = kmeans.predict(X3_train)\n    X_validate_scaled['offvsdef_cluster'] = kmeans.predict(X3_val)\n    X_validate['offvsdef_cluster'] = kmeans.predict(X3_val)\n    X_test_scaled['offvsdef_cluster'] = kmeans.predict(X3_test)\n    X_test['offvsdef_cluster'] = kmeans.predict(X3_test)\n\n    X4_train = X_train_scaled[['QB_under_pressure', 'numberOfPassRushers', 'defendersInTheBox', 'force_per_second',\n                               'time_since_last_x', 's', 'a']]\n    X4_val = X_validate_scaled[['QB_under_pressure', 'numberOfPassRushers', 'defendersInTheBox', 'force_per_second',\n                               'time_since_last_x', 's', 'a']]\n    X4_test = X_test_scaled[['QB_under_pressure', 'numberOfPassRushers', 'defendersInTheBox', 'force_per_second',\n                               'time_since_last_x', 's', 'a']]\n    kmeans = KMeans(n_clusters=3)\n    kmeans.fit(X4_train)\n    X_train_scaled['def_react_cluster'] = kmeans.predict(X4_train)\n    X_train['def_react_cluster'] = kmeans.predict(X4_train)\n    X_validate_scaled['def_react_cluster'] = kmeans.predict(X4_val)\n    X_validate['def_react_cluster'] = kmeans.predict(X4_val)\n    X_test_scaled['def_react_cluster'] = kmeans.predict(X4_test)\n    X_test['def_react_cluster'] = kmeans.predict(X4_test)\n    \n    return X_train_scaled, X_validate_scaled, X_test_scaled\n\nprint('Wrangle_NFL.py Loaded Successfully')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Acquire "},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- We acquired the data from kaggle.com as several .csv's but the data itself is provided by nextgenstats.nfl.com"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"nfl = pd.read_csv('../input/nfl-big-data-bowl-2021/plays.csv')","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"nfl.head()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"nfl.shape","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"nfl.info()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"nfl.describe().T","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"num_cols = nfl.columns[[(nfl[col].dtype == 'int64') | (nfl[col].dtype == 'float64') for col in nfl.columns]]\nfor col in num_cols:\n    plt.hist(nfl[col])\n    plt.title(col)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"**Takeaways:**\n- There are some null values listed that will need some investigation\n- More plays are being ran in the second and fourth quarter\n- Less plays are being ran as the down gets greater\n- Yards to go is skewed right(makes sense)\n    - Less likely to lose yards than gain\n- Most plays begin between home 20 and away 20\n    - Hard to pin your opponent inside 20 for kickoff or punt\n- Defenders in the box is a normal distribution\n- Number of pass  rushers is a normal distribution\n- Scores are skewed right\n- Play result is skewed right slightly\n- epa is fairly normal distribution"},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Prepare"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- Create a function that will acquire the plays.csv and prepare it for exploration(prep_plays.py)\n- Keep only the useful columns that can help us determine the success of a defense(whether a pass was completed or not)\n    - `playDescription`, `quarter`, `down`, `yardsToGo`, `possessionTeam`, `offenseFormation`, `personnelO`, `defendersInTheBox`, `numberOfPassRushers`, `personnelD`, `typeDropback`, `gameClock`, `absoluteYardlineNumber`, `epa`, `playType`, `passResult`, `playResult`\n- Create a new column called `pass_stopped` \n    - Will change completion into 0\n    - Will change incomplete and interception into 1\n- Filter out data that is not a pass play(no fake punts, fake field goals, etc)\n- Create new columns that extract positions from offensive personnel\n    - RB, TE, WR\n- Create new columns that extract positions from defensive personnel\n    - DL, LB, DB\n- Rename `typeDropback` to `QB_under_pressure` and change values into normal or scramble\n- Rename `passResult` into `pass_stopped`\n- Create formations out of personnel on the field\n- Create `closest_dist`, `closest_x`, `closest_y`, and `defender_receiver` from player tracking data\n- Merge all dataframes together\n- Convert height and age to be uniform\n- Create `time_since_last_x` and `force_per_second` "},{"metadata":{},"cell_type":"markdown","source":"# Explore"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, validate, test = explore_plays_data()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alpha = .05","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Does the offense formation matter? i.e. (is a certain offensive formation harder to defend?)"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- $H_0$: There is no dependence between offensive formation and pass stopped\n- $H_a$: There is a dependence between offensive formation and pass stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"observed = pd.crosstab(train.offenseFormation, train.pass_stopped)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"chi2, p, degf, expected = stats.chi2_contingency(observed)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"if p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"offenseFormation\", hue=\"pass_stopped\", kind=\"count\", data=train, height=8, aspect=2)._legend.remove()\nplt.title('Do certain offensive formations have more more passes stopped than others?', size = 30)\nplt.xlabel('Offensive Formation', size = 16)\nplt.ylabel('Count', size = 20)\nplt.legend(labels = ('Pass Completed', 'Pass Stopped'), loc='center right', frameon=False, fontsize='x-large')\nplt.xticks([0, 1, 2, 3, 4, 5, 6], ['Shotgun', 'Empty', 'Singleback', 'I Formation', 'Pistol', 'Jumbo', 'Wildcat'], size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"**Takeaways:**\n- There does not seem to be a certain formation that will have there pass stopped more than others\n- After a statistical test, we can safely say that there is not dependence on stopping the play and the formation the offense is lined up in."},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train.groupby('offenseFormation').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train[train.pass_stopped ==1].groupby('offenseFormation').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Are passes stoped dependent on Down?"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- $H_0$: There is no dependence between down and pass stopped\n- $H_a$: There is a dependence between down and pass stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"observed = pd.crosstab(train.down, train.pass_stopped)\n\nchi2, p, degf, expected = stats.chi2_contingency(observed)\n\nif p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"sns.barplot(data=train,x='down', y='pass_stopped').set(ylim=(0, .55))\nplt.xlabel('Down')\nplt.ylabel('Pass Stopped %')\nplt.title(\"Are Passes Stopped dependent on Down?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"**Takeaway:**\n- There is a dependence between a pass being stopped and what down it is.\n- more passes are stopped on 3rd down with 4th down right behind it\n"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train.groupby('down').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train[train.pass_stopped ==1].groupby('down').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Are EPA values dramatically different for passes stopped vs. passes completed?"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- $H_0$: The EPA value is the same for passes completed and passes stopped\n- $H_a$: The EPA value is different for passes completed and passes stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"pass_completed = train[train.pass_stopped == 0]\npass_not_completed = train[train.pass_stopped == 1]\n\nt, p = stats.ttest_ind(pass_completed.epa, pass_not_completed.epa)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"if p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"plt.rc(\"figure\", figsize=(10, 6))\nsns.violinplot(train.pass_stopped, train.epa)\nplt.xlabel('')\nplt.xticks([0,1], ['Pass Completed', 'Pass Stopped'])\nplt.yticks(size = 24)\nplt.ylabel('EPA')\nplt.title(\"Are Passes Stopped dependent on EPA?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"print(f\"The EPA mean for passes completed is {pass_completed.epa.mean()}.\")\nprint(f\"The EPA minimum for passes completed is {pass_completed.epa.min()}.\")\nprint(f\"The EPA max for passes completed is {pass_completed.epa.max()}.\")","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"print(f\"The EPA mean for passes stopped is {pass_not_completed.epa.mean()}.\")\nprint(f\"The EPA minimum for passes stopped is {pass_not_completed.epa.min()}.\")\nprint(f\"The EPA max for passes stopped is {pass_not_completed.epa.max()}.\")","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"**Takeaways:**\n- On average the EPA is negative for passes stopped and the EPA is positive for passes completed\n- The pass is usually stopped when the EPA is negative but not always.\n- If the EPA is above 2.5 then it almost guarantees that the pass will be completed"},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Are passes stopped dependent on QB pressure?"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- $H_0$: There is no dependence between QB pressure and pass stopped\n- $H_a$: There is a dependence between QB pressure and pass stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"observed = pd.crosstab(train.QB_under_pressure, train.pass_stopped)\n\nchi2, p, degf, expected = stats.chi2_contingency(observed)\n\nif p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"sns.barplot(data=train,x='QB_under_pressure', y='pass_stopped').set(ylim=(0, .55))\nplt.xlabel('')\nplt.ylabel('Pass Stopped %')\nplt.xticks([0,1], ['No Pressure', 'Pressure Applied'])\nplt.title(\"Are Passes Stopped dependent on Pressure Applied to QB?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train.groupby('QB_under_pressure').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train[train.pass_stopped ==1].groupby('QB_under_pressure').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Are passes stopped dependent on how many Defenders are in the Box?"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- $H_0$: There is no dependence between defenders in the box and pass stopped\n- $H_a$: There is a dependence between defenders in the box and pass stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"observed = pd.crosstab(train.defendersInTheBox, train.pass_stopped)\n\nchi2, p, degf, expected = stats.chi2_contingency(observed)\n\nif p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"sns.barplot(data=train,x='defendersInTheBox', y='pass_stopped').set(ylim=(0, .55))\nplt.xlabel('Defenders in the Box')\nplt.ylabel('Pass Stopped %')\nplt.xticks([0,1,2,3,4,5,6,7,8,9], [1,2,3,4,5,6,7,8,9,10])\nplt.title(\"Are Passes Stopped dependent on the number of Defenders in the Box?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train.groupby('defendersInTheBox').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train[train.pass_stopped ==1].groupby('defendersInTheBox').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Are passes stopped dependent on how many DL?"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- $H_0$: There is no dependence between DL and pass stopped\n- $H_a$: There is a dependence between DL and pass stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"observed = pd.crosstab(train.DL, train.pass_stopped)\n\nchi2, p, degf, expected = stats.chi2_contingency(observed)\n\nif p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"sns.barplot(data=train,x='DL', y='pass_stopped').set(ylim=(0, .55))\nplt.xlabel('Number of DL')\nplt.ylabel('Pass Stopped %')\n# plt.xticks([0,1,2,3,4,5,6,7,8,9], [1,2,3,4,5,6,7,8,9,10])\nplt.title(\"Are Passes Stopped dependent on DL count?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train.groupby('DL').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train[train.pass_stopped ==1].groupby('DL').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Are passes stopped dependent on how many LB?"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- $H_0$: There is no dependence between LB and pass stopped\n- $H_a$: There is a dependence between LB and pass stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"observed = pd.crosstab(train.LB, train.pass_stopped)\n\nchi2, p, degf, expected = stats.chi2_contingency(observed)\n\nif p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"sns.barplot(data=train,x='LB', y='pass_stopped').set(ylim=(0, .55))\nplt.xlabel('Number of LB')\nplt.ylabel('Pass Stopped %')\n# plt.xticks([0,1,2,3,4,5,6,7,8,9], [1,2,3,4,5,6,7,8,9,10])\nplt.title(\"Are Passes Stopped dependent on LB count?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Are passes stopped dependent on how many DB?"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- $H_0$: There is no dependence between DB and pass stopped\n- $H_a$: There is a dependence between DB and pass stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"observed = pd.crosstab(train.DB, train.pass_stopped)\n\nchi2, p, degf, expected = stats.chi2_contingency(observed)\n\nif p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"sns.barplot(data=train,x='DB', y='pass_stopped').set(ylim=(0, .55))\nplt.xlabel('Number of DB')\nplt.ylabel('Pass Stopped %')\n# plt.xticks([0,1,2,3,4,5,6,7,8,9], [1,2,3,4,5,6,7,8,9,10])\nplt.title(\"Are Passes Stopped Dependent on DB count?\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train.groupby('DB').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"train[train.pass_stopped ==1].groupby('DB').pass_stopped.count()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Are passes stopped dependent on how defensive formation(Nickel)?"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- $H_0$: There is no dependence between Nickel formation and pass stopped\n- $H_a$: There is a dependence between Nickel formation and pass stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"observed = pd.crosstab(train.nickel, train.pass_stopped)\n\nchi2, p, degf, expected = stats.chi2_contingency(observed)\n\nif p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"sns.barplot(data=train,x='nickel', y='pass_stopped').set(ylim=(0, .40))\nplt.xlabel('')\nplt.ylabel('Pass Stopped %')\nplt.title(\"Is the Nickle Formation better at stopping the pass than other formations?\")\nplt.xticks([0,1], ['Other Formation', 'Nickle Formation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Are passes stopped dependent on how defensive formation(Dime)?"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"\n- $H_0$: There is no dependence between Dime formation and pass stopped\n- $H_a$: There is a dependence between Dime formation and pass stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"observed = pd.crosstab(train.dime, train.pass_stopped)\n\nchi2, p, degf, expected = stats.chi2_contingency(observed)\n\nif p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"sns.barplot(data=train,x='dime', y='pass_stopped').set(ylim=(0, .40))\nplt.xlabel('')\nplt.ylabel('Pass Stopped %')\nplt.title(\"Is the Dime Formation better at stopping the pass than other formations?\")\nplt.xticks([0,1], ['Other Formation', 'Dime Formation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Are passes stopped dependent on how defensive formation(4-3)?"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- $H_0$: There is no dependence between 4-3 formation and pass stopped\n- $H_a$: There is a dependence between 4-3 formation and pass stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"observed = pd.crosstab(train.four_three, train.pass_stopped)\n\nchi2, p, degf, expected = stats.chi2_contingency(observed)\n\nif p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"sns.barplot(data=train,x='four_three', y='pass_stopped').set(ylim=(0, .40))\nplt.xlabel('')\nplt.ylabel('Pass Stopped %')\nplt.title(\"Is the 4-3 Formation better at stopping the pass than other formations?\")\nplt.xticks([0,1], ['Other Formation', '4-3 Formation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Are passes stopped dependent on how defensive formation(3-4)?"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- $H_0$: There is no dependence between 3-4 formation and pass stopped\n- $H_a$: There is a dependence between 3-4 formation and pass stopped"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"observed = pd.crosstab(train.three_four, train.pass_stopped)\n\nchi2, p, degf, expected = stats.chi2_contingency(observed)\n\nif p < alpha:\n    print(\"We reject the null hypothesis\")\nelse:\n    print(\"We fail to reject the null hypothesis\")\np","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.barplot(data=train,x='three_four', y='pass_stopped').set(ylim=(0, .40))\nplt.xlabel('')\nplt.ylabel('Pass Stopped %')\nplt.title(\"Is the 3-4 Formation better at stopping the pass than other formations?\")\nplt.xticks([0,1], ['Other Formation', '3-4 Formation'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"## Incomplete Passes by Position"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"df = prep_season()\ndefensedf = df[df[\"position\"].isin([\"CB\", \"OLB\", \"SS\",\"FS\",\"ILB\",\"DE\",\"LB\",\"MLB\",\"S\",\"DT\",\"DL\",\"DB\"])]","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize']=(13,7)\nposdf = defensedf.groupby('position')['event'].sum().reset_index()\n#pos20 = posdf.sort_values(by='event', ascending=False)\nposdf = posdf.sort_values(by=['event'], ascending =False)\n#plt.grid()\nsns.set_style(\"darkgrid\")\nsns.barplot(data=posdf, x='position', y= 'event', palette='mako')\nsns.color_palette('Blues')\nplt.title('Position and Incompletions', fontsize=13)\nplt.xlabel('Defensive Position',fontsize=13)\nplt.ylabel('Incomplete Passes',fontsize=13)\nposdf","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"cbdf = defensedf[defensedf['position'] == 'CB']\nolbdf = defensedf[defensedf['position'] == 'OLB']\nssdf = defensedf[defensedf['position'] == 'SS']\nfsdf = defensedf[defensedf['position'] == 'FS']\nilbdf = defensedf[defensedf['position'] == 'ILB']","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true,"hidden":true},"cell_type":"markdown","source":"### Cornerback"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"get_viz(cbdf)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"**Takeaway:** \n\n**Ages:**\n-    23, 25, 27, 26, 28\n\n**Colleges:**\n-    Ohio state, Florida state, lsu, Alabama, Florida\n\n**Height:**\n-    71\", 72\", 73\", 70\", 69\"\n\n**Weight:**\n-    190lbs, 196lbs, 195lbs, 192lbs, 185lbs"},{"metadata":{"heading_collapsed":true,"hidden":true},"cell_type":"markdown","source":"### Outside Linebacker"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"get_viz(olbdf)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"**Takeaway:**\n\n**Ages:**\n- 25, 27, 23, 28, 26\n\n**College:**\n- Georgia, Florida state, Southern California, Kentucky\n\n**Height:**\n- 75\", 73\", 76\", 74\", 77\"\n\n**Weight:**\n- 250lbs, 255lbs, 265lbs, 240lbs, 235lbs"},{"metadata":{"heading_collapsed":true,"hidden":true},"cell_type":"markdown","source":"### Strong Safety"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"get_viz(ssdf)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"**Takeaway:**\n\n**Ages:**\n- 27, 24, 26, 30, 25\n\n**College:**\n- Ohio state, boston college, lsu, Georgia, Texas \n\n**Height:**\n- 72\",71\",73\",74\",70\"\n\n**Weight:**\n- 215lbs, 210lbs, 202lbs, 195lbs, 212lbs"},{"metadata":{"heading_collapsed":true,"hidden":true},"cell_type":"markdown","source":"### Free Safety"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"get_viz(fsdf)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"**Takeaway:**\n\n**Ages:**\n- 27, 25, 26, 22, 24\n\n**College:**\n- Utah, Rutgers, Alabama, South Carolina, ohio state\n\n**Height:**\n- 73\", 71\", 72\", 70\", 74\"\n\n**Weight:**\n- 205lbs, 195lbs, 212lbs, 202lbs, 14lbs"},{"metadata":{"heading_collapsed":true,"hidden":true},"cell_type":"markdown","source":"### Inside Linebacker"},{"metadata":{"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"get_viz(ilbdf)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true},"cell_type":"markdown","source":"**Takeaway:**\n\n**Ages:**\n- 23, 28, 26, 24, 29\n\n**College:**\n- Kentucky, Alabama, Washington, Florida state,stanford\n\n**Height:**\n- 73\", 72\", 74\", 75\", 76\"\n\n**Weight:**\n- 250lbs, 232lbs, 230lbs, 245lbs, 235lbs"},{"metadata":{},"cell_type":"markdown","source":"## Who are the top defenders?"},{"metadata":{},"cell_type":"markdown","source":"### Most Stopped Passes"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_defenders()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defenders with the Least Amount of Seperation"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"## Prep Data for Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"#load data for modeling\ndf = prep_nfl()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare data for modeling\nX_train, y_train, X_validate, y_validate, X_test, y_test = train_validate_test(df)\nX_train_scaled, X_validate_scaled, X_test_scaled = min_max_scale(X_train, X_validate, X_test)\nX_train_scaled, X_validate_scaled, X_test_scaled = add_clusters(X_train_scaled,\n                                                                X_validate_scaled, X_test_scaled,\n                                                                X_train,X_validate, X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_scaled = X_train_scaled[['epa', 'time_since_last_x', 'x', 'a', 'yardsToGo', 'down',\n                                  'absoluteYardlineNumber', 's', 'y', 'force_per_second',\n                                  'QB_under_pressure', 'closest_dist', 'closest_x', 'closest_y']]\nX_validate_scaled = X_validate_scaled[['epa', 'time_since_last_x', 'x', 'a', 'yardsToGo', 'down',\n                                  'absoluteYardlineNumber', 's', 'y', 'force_per_second',\n                                  'QB_under_pressure', 'closest_dist', 'closest_x', 'closest_y']]\nX_test_scaled = X_test_scaled[['epa', 'time_since_last_x', 'x', 'a', 'yardsToGo', 'down',\n                                  'absoluteYardlineNumber', 's', 'y', 'force_per_second',\n                                  'QB_under_pressure', 'closest_dist', 'closest_x', 'closest_y']] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"### Gradient Boost"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"# after running through several learning rates \n# from .0001 up to 100, 1 is the best parameter\nboost_params = {'learning_rate': [1]}","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#setting parameters and fitting model\nsearch = GridSearchCV(GradientBoostingClassifier(), boost_params, cv=5)\nsearch.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#predicting target variable\ny_pred = search.predict(X_train_scaled)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"print('>>>>>>>>>> Accuracy of Gradient Boost on TRAIN set: {:.4f}'\n     .format(search.score(X_train_scaled, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_train, y_pred))\ncm","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"#classification report\nclass_report = pd.DataFrame(classification_report(y_train, y_pred, output_dict=True))\nclass_report","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"### KNN"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#setting parameters and fitting model\nknn = KNeighborsClassifier(n_neighbors=5, weights='uniform')\nknn.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#predicting target variable\ny_pred = knn.predict(X_train_scaled)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"print('>>>>>>>>>> Accuracy of KNN classifier on TRAIN set: {:.2f}'\n     .format(knn.score(X_train_scaled, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_train, y_pred))\ncm","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#classification report\nreport = pd.DataFrame(classification_report(y_train, y_pred, output_dict=True))\nreport","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#setting parameters and fitting model\nlogit = LogisticRegression(C=1, class_weight={0:1, 1:99}, random_state=123, intercept_scaling=1, solver='lbfgs')\nlogit.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#predicting target variable\ny_pred = logit.predict(X_train_scaled)\n\nprint('>>>>>>>>>> Accuracy of Logistic Regression classifier on TRAIN set: {:.2f}'\n     .format(logit.score(X_train_scaled, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_train, y_pred))\ncm","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#classification report\nreport = pd.DataFrame(classification_report(y_train, y_pred, output_dict=True))\nreport","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#setting parameters and fitting model\nrf = RandomForestClassifier(bootstrap=True, \n                            class_weight=None, \n                            criterion='gini',\n                            min_samples_leaf=8,\n                            n_estimators=100,\n                            max_depth=15, \n                            random_state=123)\n\nrf.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#predicting target variable\ny_pred = rf.predict(X_train_scaled)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"print('>>>>>>>>>> Accuracy of Random Forest classifier on TRAIN set: {:.2f}'\n     .format(rf.score(X_train_scaled, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_train, y_pred))\ncm","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#classification report\nclass_report = pd.DataFrame(classification_report(y_train, y_pred, output_dict=True))\nclass_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validate"},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting target variable\ny_pred_val = search.predict(X_validate_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('>>>>>>>>>> Accuracy of Gradient Boost on VALIDATE set: {:.4f}'\n     .format(search.score(X_validate_scaled, y_validate)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_pred_val, y_validate))\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#classification report\nclass_report = pd.DataFrame(classification_report(y_validate, y_pred_val, output_dict=True))\nclass_report","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"### KNN"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#predicting target variable\ny_pred = knn.predict(X_validate_scaled)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"print('>>>>>>>>>> Accuracy of KNN classifier on VALIDATE set: {:.2f}'\n      .format(knn.score(X_validate_scaled, y_validate)))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_validate, y_pred))\ncm","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#classifiation report\nreport = pd.DataFrame(classification_report(y_validate, y_pred, output_dict=True))\nreport","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"#predicting target variable\ny_pred = rf.predict(X_validate_scaled)","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"print('>>>>>>>>>> Accuracy of Random Forest on VALIDATE set: {:.2f}'\n      .format(rf.score(X_validate_scaled, y_validate)))","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"cm = pd.DataFrame(confusion_matrix(y_validate, y_pred))\ncm","execution_count":null,"outputs":[]},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"report = pd.DataFrame(classification_report(y_validate, y_pred, output_dict=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test"},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting target variable\ny_pred_val = search.predict(X_test_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('>>>>>>>>>> Accuracy of Gradient Boost on TEST set: {:.4f}'\n      .format(search.score(X_test_scaled, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#confusion matrix\ncm = pd.DataFrame(confusion_matrix(y_pred_val, y_test))\ncm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#classification report\nclass_report = pd.DataFrame(classification_report(y_test, y_pred_val, output_dict=True))\nclass_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top Features for Gradient Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#obtaining names of features\ntop_feature = pd.DataFrame(X_train_scaled.columns)\n#adding importance measure of values\ntop_feature['values'] = search.best_estimator_.feature_importances_\n#finding top 10 features\ntop_feature.sort_values('values', ascending = False).head(15)","execution_count":null,"outputs":[]},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Conclusions"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- Our Gradient Boost Model was 96% accurate at predicting a pass being stopped.\n- EPA & closest_dist turned out to be significant features in our model.\n    - EPA was provided by Kaggle\n    - closest_dist was a feature engineered\n- Success in defending the pass truly depends on the defenders' ability to prevent separation from receiver and their reaction time.\n- When pressure is applied to the quarter back, the completion percentage significantly decreases.\n- Dime formation (6 defensive backs) had the best success in stopping the pass."},{"metadata":{"heading_collapsed":true},"cell_type":"markdown","source":"# Next Steps"},{"metadata":{"hidden":true},"cell_type":"markdown","source":"- work out mislabels and small bugs for closest_dist feature\n- use similar algorithm to find the distance of all cornerbacks to their defensive assignments i.e. WR, RB, TE, etc\n- further analyze the components of EPA to understand their influence on the model\n- explore trick plays to see if the same features carry over from the traditional offensive setup"},{"metadata":{"hidden":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}