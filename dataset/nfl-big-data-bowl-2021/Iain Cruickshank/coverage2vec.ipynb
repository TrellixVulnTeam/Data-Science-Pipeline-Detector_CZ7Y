{"cells":[{"metadata":{"papermill":{"duration":0.039735,"end_time":"2021-01-07T21:29:51.600474","exception":false,"start_time":"2021-01-07T21:29:51.560739","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Coverage2Vec: Towards a Defensive Performance Embedding Scheme\n\nIn this notebook, we will describe our Coverage2Vec model, which converts a defense's coverage into a real-valued vector, much like any other embedding model (i.e. Doc2Vec, Word2Vec, etc.) does. The embedding allows for efficient prediction of relevant metrics (i.e. completion likelihood or expected yards gained), investigation of patterns across defenses, and visualization of defensive scheme for analyst use. At a high-level, the steps to implement our model are as follows:\n\n1. **Data Processesing and Investigation**: we will combine elements from the 'players,' 'plays,' and 'games' data frames into a week dataframe. We will also format this new data frame to create training data where every row of the training data is a snapshot of the play and has a consistent structure (i.e. number of players). So, using the output of theis investigation, we will have a dataset that consists of a normalized vector for every play snapshot.\n2. **Creation of the Coverage2Vec model and model training**: In the second section, we will describe our coverage2vec model and the training procedure used to fit that model to the data created in the previous section. The output of this section will be the fully-trainined coverage2vec model\n3. **Investigation of coverages**: In this section, we will use the trained coverage2vec model along with all of the play snapshots to visually investigate various aspects of defensive coverages\n\n\nThis notebook is orgnaized into sections:\n\n1. [Data Processing and Investigation](#Data-Processing-and-Investigation)\n2. [Coverage2Vec model](#Coverage2Vec-model)\n3. [Investigation of Defensive Coverages using Embedded Plays](#Investigation-of-Defensive-Coverages-using-Embedded-Plays)\n4. [Appendix A: Code for processing the data](#Appendix-A:-Code-for-processing-the-data)\n5. [Appendix B: Code Implementation for Coverage2Vec Model](#Appendix-B:-Code-Implementation-for-Coverage2Vec-Model)"},{"metadata":{"papermill":{"duration":0.039197,"end_time":"2021-01-07T21:29:51.678546","exception":false,"start_time":"2021-01-07T21:29:51.639349","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Data Processing and Investigation\n\nIn this section, we will read in the data, investigate some properties of it, and process it for use in the coverage2vec model\n- First, we will import neccesary Python packages and then read in the data\n- Second, we will investigate aspects of the data, most notably the player postions present, number of unique plays, number of unique play snapshots, etc.\n\n**Note**: The full code for processing the data is in the appendix "},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-01-07T21:29:51.758417Z","iopub.status.busy":"2021-01-07T21:29:51.757612Z","iopub.status.idle":"2021-01-07T21:29:59.117039Z","shell.execute_reply":"2021-01-07T21:29:59.116422Z"},"papermill":{"duration":7.400484,"end_time":"2021-01-07T21:29:59.117174","exception":false,"start_time":"2021-01-07T21:29:51.71669","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"'''Read in the neccesary python packages'''\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np, pandas as pd, seaborn as sns, os\nfrom matplotlib import pyplot as plt\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.sparse import csr_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2021-01-07T21:29:59.284561Z","iopub.status.busy":"2021-01-07T21:29:59.283891Z","iopub.status.idle":"2021-01-07T21:29:59.847641Z","shell.execute_reply":"2021-01-07T21:29:59.846977Z"},"papermill":{"duration":0.605132,"end_time":"2021-01-07T21:29:59.847769","exception":false,"start_time":"2021-01-07T21:29:59.242637","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"''' Create variables for all the different data repositories, and read in the data'''\n\nsave_dir = \"/kaggle/working/\"\n\ngame_data  = pd.read_csv(\"../input/nfl-big-data-bowl-2021/games.csv\")\nplayer_data  = pd.read_csv(\"../input/nfl-big-data-bowl-2021/players.csv\")\nplay_data  = pd.read_csv(\"../input/nfl-big-data-bowl-2021/plays.csv\")\nplay_processed_data = np.load(\"../input/nfl-big-data-bowl-c2v-processed-data/play_processed_data.npy\")\nplay_data_df = pd.read_csv(\"../input/nfl-big-data-bowl-c2v-processed-data/processed_play_data_labels.csv\", index_col=0)\nplay_processed_data = play_processed_data[:,:64,:-180]\nplay_processed_data = np.transpose(play_processed_data, (0 ,2, 1))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.038011,"end_time":"2021-01-07T21:29:59.924679","exception":false,"start_time":"2021-01-07T21:29:59.886668","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Initial Investigation of the Data\nWe will first look at the nature of the players and the games. Namely, how many games are there for training on, and what types of players do we have telemetry data for, etc."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:00.018766Z","iopub.status.busy":"2021-01-07T21:30:00.017862Z","iopub.status.idle":"2021-01-07T21:30:00.021637Z","shell.execute_reply":"2021-01-07T21:30:00.022193Z"},"papermill":{"duration":0.058003,"end_time":"2021-01-07T21:30:00.022338","exception":false,"start_time":"2021-01-07T21:29:59.964335","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"player_pos = np.unique(player_data['position'])\nprint(\"The number of unique player positions is: {}, and they are: {}\".format(len(player_pos), player_pos))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-01-07T21:30:00.136269Z","iopub.status.busy":"2021-01-07T21:30:00.135404Z","iopub.status.idle":"2021-01-07T21:30:00.224191Z","shell.execute_reply":"2021-01-07T21:30:00.224739Z"},"papermill":{"duration":0.162778,"end_time":"2021-01-07T21:30:00.224909","exception":false,"start_time":"2021-01-07T21:30:00.062131","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(\"The number of unique plays is: {}\".format(len(np.unique(play_data['gameId'].astype(str)+\" \"+play_data['playId'].astype(str)))))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:00.310605Z","iopub.status.busy":"2021-01-07T21:30:00.309921Z","iopub.status.idle":"2021-01-07T21:30:00.31651Z","shell.execute_reply":"2021-01-07T21:30:00.317052Z"},"papermill":{"duration":0.05061,"end_time":"2021-01-07T21:30:00.317212","exception":false,"start_time":"2021-01-07T21:30:00.266602","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(\"The number of unique games is: {}\".format(len(np.unique(game_data['gameId']))))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:00.415246Z","iopub.status.busy":"2021-01-07T21:30:00.414125Z","iopub.status.idle":"2021-01-07T21:30:00.432189Z","shell.execute_reply":"2021-01-07T21:30:00.431606Z"},"papermill":{"duration":0.073503,"end_time":"2021-01-07T21:30:00.432318","exception":false,"start_time":"2021-01-07T21:30:00.358815","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(\"size of processed data: {}\".format(play_processed_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.039593,"end_time":"2021-01-07T21:30:00.511741","exception":false,"start_time":"2021-01-07T21:30:00.472148","status":"completed"},"tags":[]},"cell_type":"markdown","source":"So, we get close to 20,000 different plays and around 22 different types of positions in the data. Generally, there are about 64 snapshots per play, with the longest play being 269 snapshots (It was a 4th down in the 4th quarter, with a few seconds left on the clock, and the offense went for a hook and lateral... they didn't make the touchdown). \n\nIn order to use this data, we first decided on how to featurize the play snapshots in order to get a time series signal for each of the plays: So, for each play snapshot, we used the following fields:\n* Each player's postion (both offense and defensive players).\n* Each player's telemetry data, including `x, y, s, a, dis, o,` and `dir`. \nEach of the player's vectors were then concatenated into one vector to produce a 1x660 length vector for each play snapshot.\n\n\n"},{"metadata":{"papermill":{"duration":0.040027,"end_time":"2021-01-07T21:30:00.593802","exception":false,"start_time":"2021-01-07T21:30:00.553775","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Coverage2Vec model\n\nIn this section, we will delve into the design and training of the coverage2vec model. (**Note**: Code for the model and training of the model is available in Appendix B). Since we want to look at play data, and that play data is composed of play sanshots, which have a time dependency, we opted to look at a embedding model that can handle time sequences. Furthermore, since the data is often sparse in nature, due to the categorical features, like `position` and the lack of telemetry data for all players on the field for most plays, we opted for a 1D convolutional autoencoder. Some more information on these models and the 1D convolution can be found at: [How to Use Convolutional Neural Networks for Time Series Classification](https://towardsdatascience.com/how-to-use-convolutional-neural-networks-for-time-series-classification-56b1b0a07a57) and [1-d Convolutional Neural Networks for Time Series: Basic Intuition](https://boostedml.com/2020/04/1-d-convolutional-neural-networks-for-time-series-basic-intuition.html).\n\nTo create the feature space, we settled on having 100 snapshots per each play. For reference, the average number of snapshots per play is 64. If it had fewer snapshots than 100, it was paddedd with zeros, and if a play had more than 100 snapshots, it was downsmapled to 100. The following code displays a graphical depection of a play snapshot"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18,22))\nplt.imshow(np.transpose(play_processed_data, (0,2,1))[8697], cmap='gray', vmin=0, vmax=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case the y-axis is time and the x-axis are the players, where every 30 units on the x-axis is a unique player"},{"metadata":{"papermill":{"duration":0.040332,"end_time":"2021-01-07T21:30:00.674468","exception":false,"start_time":"2021-01-07T21:30:00.634136","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Coverage2Vec Model Architecture\n\nFor our autoencoder, we used a combination of 1-D convultional layers and fully connected layers. We also found that running over the data by the players dimensions, rather than the snapshot dimension (i.e. `(None, 100, 600)`  for `(None, 600, 100)`, produced better embeddings. The following code displays the summary of the coverage2vec model. "},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:00.758934Z","iopub.status.busy":"2021-01-07T21:30:00.758226Z","iopub.status.idle":"2021-01-07T21:30:00.790573Z","shell.execute_reply":"2021-01-07T21:30:00.791209Z"},"papermill":{"duration":0.076145,"end_time":"2021-01-07T21:30:00.79143","exception":false,"start_time":"2021-01-07T21:30:00.715285","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\nc2v = load_model(\"../input/c2v-trained-model/c2v_model_conv1D_trans_v2\")","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:00.877181Z","iopub.status.busy":"2021-01-07T21:30:00.87638Z","iopub.status.idle":"2021-01-07T21:30:00.901923Z","shell.execute_reply":"2021-01-07T21:30:00.902472Z"},"papermill":{"duration":0.069915,"end_time":"2021-01-07T21:30:00.902622","exception":false,"start_time":"2021-01-07T21:30:00.832707","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"c2v.encoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:00.988742Z","iopub.status.busy":"2021-01-07T21:30:00.988111Z","iopub.status.idle":"2021-01-07T21:30:01.014557Z","shell.execute_reply":"2021-01-07T21:30:01.015092Z"},"papermill":{"duration":0.071183,"end_time":"2021-01-07T21:30:01.015256","exception":false,"start_time":"2021-01-07T21:30:00.944073","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"c2v.decoder.summary()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.042352,"end_time":"2021-01-07T21:30:01.099861","exception":false,"start_time":"2021-01-07T21:30:01.057509","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Training the Model\n\nFor training the model, we used Mean Squared Error and randomly sampled out 5 games to act as a validation set. We then trained over the training set for 30 iterations. We repeated this process of taking out a validation set and training 50 times."},{"metadata":{"papermill":{"duration":0.042188,"end_time":"2021-01-07T21:30:01.184461","exception":false,"start_time":"2021-01-07T21:30:01.142273","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Investigation of Defensive Coverages using Embedded Plays\n\nIn this section, we display some of the output that a autoencoder, like coverage2vec, can produce. In particular, we will visualize a play and some teams to see the differences between the teams."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:01.276971Z","iopub.status.busy":"2021-01-07T21:30:01.276294Z","iopub.status.idle":"2021-01-07T21:30:03.915521Z","shell.execute_reply":"2021-01-07T21:30:03.914779Z"},"papermill":{"duration":2.688543,"end_time":"2021-01-07T21:30:03.915641","exception":false,"start_time":"2021-01-07T21:30:01.227098","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected = True)\nimport plotly.figure_factory as ff","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:04.035118Z","iopub.status.busy":"2021-01-07T21:30:04.026718Z","iopub.status.idle":"2021-01-07T21:30:04.110808Z","shell.execute_reply":"2021-01-07T21:30:04.110132Z"},"papermill":{"duration":0.151219,"end_time":"2021-01-07T21:30:04.110955","exception":false,"start_time":"2021-01-07T21:30:03.959736","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"game_id = 2018111110\n\nplot_df = play_data_df[play_data_df['gameId']==game_id]\nlatent_positions = c2v.encoder(play_processed_data[np.where(play_data_df['gameId']==game_id)])\nplot_df['x'] = latent_positions[:,0].numpy()\nplot_df['y'] = latent_positions[:,1].numpy()\nplot_df['z'] = latent_positions[:,2].numpy()\nplot_df['colors'] = plot_df['d_team'].map(lambda x: '#69BE28' if x =='SEA' else '#0080C6')\n\ntraces =[]\nfor team_name in plot_df['d_team'].unique():\n    df = plot_df[plot_df['d_team']==team_name]\n    trace = go.Scatter3d(surfacecolor='darkgrey',\n        x= df['x'],\n        y= df['y'],\n        z= df['z'],\n        name=team_name,\n        mode='markers',text=df['playResult'],\n         marker=dict(\n            color = df['colors'], \n            colorscale='Jet',\n            size= 5,\n            line=dict(\n                color= df['colors'],\n                width= 10\n            ),\n            opacity=0.8\n         )\n    )\n    traces.append(trace)\n\nlayout = go.Layout(\n    title = 'Visualization of Defensive Coverages for Seattle v. LA',\n    scene = dict(\n            xaxis = dict(title  = 'latent x-dimension'),\n            yaxis = dict(title  = 'latent y-dimension'),\n            zaxis = dict(title  = 'latent z-dimension')\n        )\n)\n\nfig = go.Figure(data = traces, layout = layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generally, for this 11 Novermber game, we can observe that Seattle has less variability in their coverages for the game than does LA. Their defensive coverages end up much closer together"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:04.232229Z","iopub.status.busy":"2021-01-07T21:30:04.212795Z","iopub.status.idle":"2021-01-07T21:30:04.236853Z","shell.execute_reply":"2021-01-07T21:30:04.23739Z"},"papermill":{"duration":0.08495,"end_time":"2021-01-07T21:30:04.237617","exception":false,"start_time":"2021-01-07T21:30:04.152667","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"team_id = 'PIT'\n\nplot_df = play_data_df[play_data_df['d_team']==team_id]\nlatent_positions = c2v.encoder(play_processed_data[np.where(play_data_df['d_team']==team_id)])\nplot_df['x'] = latent_positions[:,0].numpy()\nplot_df['y'] = latent_positions[:,1].numpy()\nplot_df['z'] = latent_positions[:,2].numpy()\n\ntrace1 = go.Scatter3d(surfacecolor='darkgrey',\n    x= plot_df['x'],\n    y= plot_df['y'],\n    z= plot_df['z'],\n    mode='markers',text=plot_df['playResult'],\n     marker=dict(\n        color = '#FFB612', \n        colorscale='Jet',\n        size= 5,\n        line=dict(\n            color= '#FFB612',\n            width= 10\n        ),\n        opacity=0.8\n     )\n)\n\nlayout = go.Layout(\n    title = 'Visualization of Defensive Coverage Plays for Pittsburgh',\n    scene = dict(\n            xaxis = dict(title  = 'latent x-dimension'),\n            yaxis = dict(title  = 'latent y-dimension'),\n            zaxis = dict(title  = 'latent z-dimension')\n        )\n)\n\nfig = go.Figure(data = trace1, layout = layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:04.345783Z","iopub.status.busy":"2021-01-07T21:30:04.340484Z","iopub.status.idle":"2021-01-07T21:30:04.367195Z","shell.execute_reply":"2021-01-07T21:30:04.366488Z"},"papermill":{"duration":0.087013,"end_time":"2021-01-07T21:30:04.367345","exception":false,"start_time":"2021-01-07T21:30:04.280332","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"team_id = 'CHI'\n\nplot_df = play_data_df[play_data_df['d_team']==team_id]\nlatent_positions = c2v.encoder(play_processed_data[np.where(play_data_df['d_team']==team_id)])\nplot_df['x'] = latent_positions[:,0].numpy()\nplot_df['y'] = latent_positions[:,1].numpy()\nplot_df['z'] = latent_positions[:,2].numpy()\n\ntrace2 = go.Scatter3d(surfacecolor='darkgrey',\n    x= plot_df['x'],\n    y= plot_df['y'],\n    z= plot_df['z'],\n    mode='markers',text=plot_df['playResult'],\n     marker=dict(\n        color = '#0B162A', \n        colorscale='Jet',\n        size= 5,\n        line=dict(\n            color= '#0B162A',\n            width= 10\n        ),\n        opacity=0.8\n     )\n)\n\nlayout = go.Layout(\n    title = 'Visualization of Defensive Coverage Plays for Chicago',\n    scene = dict(\n            xaxis = dict(title  = 'latent x-dimension'),\n            yaxis = dict(title  = 'latent y-dimension'),\n            zaxis = dict(title  = 'latent z-dimension')\n        )\n)\n\nfig = go.Figure(data = trace2, layout = layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:04.491042Z","iopub.status.busy":"2021-01-07T21:30:04.488033Z","iopub.status.idle":"2021-01-07T21:30:04.4964Z","shell.execute_reply":"2021-01-07T21:30:04.495853Z"},"papermill":{"duration":0.086003,"end_time":"2021-01-07T21:30:04.496521","exception":false,"start_time":"2021-01-07T21:30:04.410518","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"team_id = 'OAK'\n\nplot_df = play_data_df[play_data_df['d_team']==team_id]\nlatent_positions = c2v.encoder(play_processed_data[np.where(play_data_df['d_team']==team_id)])\nplot_df['x'] = latent_positions[:,0].numpy()\nplot_df['y'] = latent_positions[:,1].numpy()\nplot_df['z'] = latent_positions[:,2].numpy()\n\ntrace3 = go.Scatter3d(surfacecolor='darkgrey',\n    x= plot_df['x'],\n    y= plot_df['y'],\n    z= plot_df['z'],\n    mode='markers',text=plot_df['playResult'],\n     marker=dict(\n        color = '#000000', \n        colorscale='Jet',\n        size= 5,\n        line=dict(\n            color= '#000000',\n            width= 10\n        ),\n        opacity=0.8\n     )\n)\n\nlayout = go.Layout(\n    title = 'Visualization of Defensive Coverage Plays for Oakland',\n    scene = dict(\n            xaxis = dict(title  = 'latent x-dimension'),\n            yaxis = dict(title  = 'latent y-dimension'),\n            zaxis = dict(title  = 'latent z-dimension')\n        )\n)\n\nfig = go.Figure(data = trace3, layout = layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layout = go.Layout(\n    title = 'Visual Comparison of the Three Teams Embedded Plays',\n    scene = dict(\n            xaxis = dict(title  = 'latent x-dimension'),\n            yaxis = dict(title  = 'latent y-dimension'),\n            zaxis = dict(title  = 'latent z-dimension')\n        )\n)\n\nfig = go.Figure(data = [trace1, trace2, trace3], layout = layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As with an individual play, we can also see differences in coverages between teams, most especially between Pittsburgh and the other two. \n\nWe hope you enjoyed our notebook and than it prompts ideas for even better way to analyze coverages (like, maybe a defensive_player2vec??)! "},{"metadata":{"papermill":{"duration":0.043983,"end_time":"2021-01-07T21:30:04.584271","exception":false,"start_time":"2021-01-07T21:30:04.540288","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Appendix A: Code for processing the data"},{"metadata":{"papermill":{"duration":0.043277,"end_time":"2021-01-07T21:30:04.671365","exception":false,"start_time":"2021-01-07T21:30:04.628088","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Data Processesing Functions\n\n- We will need to pre-process, `preprocess_data`, the data so that things like all of the player telemetry data is normalized on 0-1 for use by the neural network embedding (preprocess_data)\n- We will also need a function that can take out the neccesary elements of the data `create_attributes` for the coverage2vec model and process each of the play snapshots into a standard vector\n- Finally we will need a function that processes the play snapshot vectors and turns them into a sequence (3-D tensor) ,`process_play_data`. We will use 100 as the number of snap shots per play and pad those with less than 100 snapshots with 0's and downsample those with more than 100 snapshots (which is aonly about ~100 plays)."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:04.764147Z","iopub.status.busy":"2021-01-07T21:30:04.763412Z","iopub.status.idle":"2021-01-07T21:30:04.767366Z","shell.execute_reply":"2021-01-07T21:30:04.766781Z"},"papermill":{"duration":0.052142,"end_time":"2021-01-07T21:30:04.767491","exception":false,"start_time":"2021-01-07T21:30:04.715349","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# def preprocess_data(telemetry_data, play_data, game_data):\n#     telemetry_data['play_uid'] = telemetry_data['gameId'].astype(str) +\"_\" + telemetry_data['playId'].astype(str)\n#     telemetry_data = telemetry_data[telemetry_data['nflId'].notna()] #cut out the football telemetry data\n#     play_data['play_uid'] = play_data['gameId'].astype(str) +\"_\" + play_data['playId'].astype(str)\n#     final_data = telemetry_data.merge(play_data.drop(columns=['gameId','playId']), how='left', on='play_uid')\n#     final_data = final_data.merge(game_data[['gameId', 'homeTeamAbbr', 'visitorTeamAbbr']], how='left', on='gameId')\n    \n#     '''Add in some columns to determine which players are on offense or defense, and a unique identifier for each snapshot of each play'''\n#     final_data['frame_uid'] = final_data['play_uid'].astype(str) +\"_\" + final_data['frameId'].astype(str)\n#     check_for_bad_plays_series = final_data.groupby('frame_uid').count().iloc[:,0]\n#     bad_play_frame_ids = check_for_bad_plays_series[check_for_bad_plays_series>22].index #Find those plays that have more than 22 men on the field so they can be removed\n#     final_data = final_data[~ final_data['frame_uid'].isin(bad_play_frame_ids)]\n#     final_data['offense'] = ((final_data['possessionTeam'] == final_data['homeTeamAbbr']) & (final_data['team'] == 'home')) | ((final_data['possessionTeam'] == final_data['visitorTeamAbbr']) & (final_data['team'] == 'away'))\n#     final_data['d_team'] = final_data.apply(lambda x : x['homeTeamAbbr'] if x['possessionTeam'] != x['homeTeamAbbr'] else x['visitorTeamAbbr'], axis=1)\n\n#     '''Normalize some of the data'''\n#     final_data['x'] /= 120 #normalize by length of field\n#     final_data['y'] /= 53.3 #normalize by width of field\n#     final_data['o'] /= 360.0 #normalize by number of degrees in a full circle\n#     final_data['dir'] /= 360.0 #normalize by number of degrees in a full circle\n#     final_data[['a','s']] = MinMaxScaler().fit_transform(final_data[['a','s']]) # use a Min-Max scaler to scale across that weeks' data to 0-1\n    \n#     return final_data","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:04.860525Z","iopub.status.busy":"2021-01-07T21:30:04.859541Z","iopub.status.idle":"2021-01-07T21:30:04.863597Z","shell.execute_reply":"2021-01-07T21:30:04.862992Z"},"papermill":{"duration":0.052722,"end_time":"2021-01-07T21:30:04.863745","exception":false,"start_time":"2021-01-07T21:30:04.811023","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# def create_attributes(play):\n#     o_players = sum(play['offense'])\n#     d_players = sum(~play['offense'])\n    \n#     if (o_players >11) | (d_players >11):\n#         return np.nan\n    \n#     possible_positions = ['CB', 'DB', 'DE', 'DT', 'FB', 'FS', 'HB', 'ILB', 'K', 'LB', 'LS',\n#         'MLB', 'NT', 'OLB', 'P', 'QB', 'RB', 'S', 'SS', 'TE', 'WR', 'DL']\n    \n#     possible_o_formations = ['I_FORM', 'SINGLEBACK', 'SHOTGUN', 'EMPTY', 'PISTOL', 'JUMBO',\n#        'WILDCAT']\n    \n#     if len(play) < 22:\n#         for i in range(22-len(play)):\n#             if o_players < 11:\n#                 play = play.append({'position':str(i), 'offense':1.0}, ignore_index=True)\n#                 o_players +=1\n#             elif d_players <11:\n#                 play = play.append({'position':str(i), 'offense':0.0}, ignore_index=True)\n#                 d_players +=1\n            \n#     positions = pd.get_dummies(play['position'])\n#     attribs = positions.T.reindex(possible_positions).T.fillna(0)\n#     attribs[['x','y','s','a','dis','o','dir','offense']] = play[['x','y','s','a','dis','o','dir','offense']]\n#     attribs.fillna(0, inplace=True)\n    \n#     quarter = play['quarter'].values[0]/4\n#     down = play['down'].values[0]/4\n#     time_left_on_clock = play_snapshot['gameClock'].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1])).values[0]/(15*60)\n    \n#     o_formation = pd.get_dummies(play_snapshot['offenseFormation'].iloc[0])\n#     o_formation = o_formation.T.reindex(possible_o_formations).T.fillna(0).values\n    \n#     attribs = attribs.values.flatten().astype(np.float32)\n#     attribs = np.append(attribs, [quarter, down, time_left_on_clock])\n#     attribs = np.array([quarter, down, time_left_on_clock])\n#     attribs = np.append(attribs, o_formation)\n    \n#     return attribs","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:04.960664Z","iopub.status.busy":"2021-01-07T21:30:04.959929Z","iopub.status.idle":"2021-01-07T21:30:04.962686Z","shell.execute_reply":"2021-01-07T21:30:04.963208Z"},"papermill":{"duration":0.056067,"end_time":"2021-01-07T21:30:04.963353","exception":false,"start_time":"2021-01-07T21:30:04.907286","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# def process_play_data(processed_data, uids, sequence_length):\n#     from tensorflow.keras.preprocessing.sequence import pad_sequences  \n#     play_uids = uids['play_uid'].unique()\n#     X = []\n#     for play_uid in play_uids:\n#         play = processed_data[np.where(uids['play_uid'] == play_uid)]\n#         play = play[np.argsort(uids[uids['play_uid'] == play_uid]['frameId']).values]\n#         if len(play) > sequence_length:\n#             idx_to_remove = np.round(np.linspace(0, play.shape[0] - 1, play.shape[0]-sequence_length)).astype(int)\n#             play = np.delete(play, idx_to_remove, axis=0)\n#         X.append(play)\n            \n#     X = pad_sequences(X, padding='post', dtype='float32', maxlen=sequence_length)\n\n#     return play_uids, X","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.042829,"end_time":"2021-01-07T21:30:05.050214","exception":false,"start_time":"2021-01-07T21:30:05.007385","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Continued Investigation of the Data\nlets look at an example of the player telemtry data that we are given. In general, this telemetry data should have all the coverage players' physical charateristics for each snapshot of each play."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:05.141494Z","iopub.status.busy":"2021-01-07T21:30:05.14078Z","iopub.status.idle":"2021-01-07T21:30:05.143096Z","shell.execute_reply":"2021-01-07T21:30:05.143563Z"},"papermill":{"duration":0.049955,"end_time":"2021-01-07T21:30:05.143738","exception":false,"start_time":"2021-01-07T21:30:05.093783","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# telemetry_data = pd.read_csv(\"../input/nfl-big-data-bowl-2021/week\"+str(5)+\".csv\")\n# telemetry_data = preprocess_data(telemetry_data, play_data, game_data)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:05.234172Z","iopub.status.busy":"2021-01-07T21:30:05.233441Z","iopub.status.idle":"2021-01-07T21:30:05.236772Z","shell.execute_reply":"2021-01-07T21:30:05.237346Z"},"papermill":{"duration":0.049861,"end_time":"2021-01-07T21:30:05.237498","exception":false,"start_time":"2021-01-07T21:30:05.187637","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# print(\"The number of unique play snapshots in this week is: {}\".format(len(telemetry_data['frame_uid'].unique())))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.042961,"end_time":"2021-01-07T21:30:05.32429","exception":false,"start_time":"2021-01-07T21:30:05.281329","status":"completed"},"tags":[]},"cell_type":"markdown","source":"So, we have for any given week around 80,000 unique play snapshots. When you combine that with the number of weeks (18) that is a lot of player telemetry data to work with!\n\nNow, lets get a look at what a given play snapshot in the telemetry data looks like"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:05.41519Z","iopub.status.busy":"2021-01-07T21:30:05.414489Z","iopub.status.idle":"2021-01-07T21:30:05.417369Z","shell.execute_reply":"2021-01-07T21:30:05.416869Z"},"papermill":{"duration":0.050013,"end_time":"2021-01-07T21:30:05.417493","exception":false,"start_time":"2021-01-07T21:30:05.36748","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# play_snapshot = telemetry_data[telemetry_data['frame_uid']==np.unique(telemetry_data['frame_uid'])[100]]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:05.50894Z","iopub.status.busy":"2021-01-07T21:30:05.508248Z","iopub.status.idle":"2021-01-07T21:30:05.511256Z","shell.execute_reply":"2021-01-07T21:30:05.510592Z"},"papermill":{"duration":0.050718,"end_time":"2021-01-07T21:30:05.511377","exception":false,"start_time":"2021-01-07T21:30:05.460659","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# play_snapshot ","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:05.604086Z","iopub.status.busy":"2021-01-07T21:30:05.603258Z","iopub.status.idle":"2021-01-07T21:30:05.605551Z","shell.execute_reply":"2021-01-07T21:30:05.60608Z"},"papermill":{"duration":0.050246,"end_time":"2021-01-07T21:30:05.606346","exception":false,"start_time":"2021-01-07T21:30:05.5561","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# attributes = create_attributes(play_snapshot)\n# print(attributes.shape)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.043146,"end_time":"2021-01-07T21:30:05.697872","exception":false,"start_time":"2021-01-07T21:30:05.654726","status":"completed"},"tags":[]},"cell_type":"markdown","source":"So, for this play from Indianapolis versus New England in Week 5, we have 14 players that have telemetry data, split between 8 on defense and 6 on offense. When we featurize this data, we get a 1x660 length vector. So, each play snapshot will be transformed in a vector of 660, real-valued numbers between 0 and 1.\n\n### Process data for use in the Coverage2Vec model\n\nThe following code featurizes all of the play snapshots, but is commented out as it takes a long time to run and will overload the memory of a standard Kaggle machine. It is included for completeness, and we will load in the processed data at the end."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:05.788718Z","iopub.status.busy":"2021-01-07T21:30:05.788077Z","iopub.status.idle":"2021-01-07T21:30:05.793243Z","shell.execute_reply":"2021-01-07T21:30:05.793792Z"},"papermill":{"duration":0.052292,"end_time":"2021-01-07T21:30:05.793983","exception":false,"start_time":"2021-01-07T21:30:05.741691","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"'''Read in all of the telemetry data from all of the weeks'''\n\n# telemetry_data = list()\n# for week in range(1,18):\n#     telemetry_data.append(pd.read_csv(\"/kaggle/week\"+str(week)+\".csv\"))\n# telemetry_data = pd.concat(telemetry_data)\n# telemetry_data = preprocess_data(telemetry_data, play_data, game_data)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:05.886632Z","iopub.status.busy":"2021-01-07T21:30:05.885937Z","iopub.status.idle":"2021-01-07T21:30:05.890513Z","shell.execute_reply":"2021-01-07T21:30:05.891209Z"},"papermill":{"duration":0.052079,"end_time":"2021-01-07T21:30:05.891353","exception":false,"start_time":"2021-01-07T21:30:05.839274","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"'''function to speed up featurization by doing it in parallel'''\n\n# def applyParallel(dfGrouped, func):\n#     from joblib import Parallel, delayed\n#     import multiprocessing\n#     retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n#     return retLst","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:05.984961Z","iopub.status.busy":"2021-01-07T21:30:05.984275Z","iopub.status.idle":"2021-01-07T21:30:05.987473Z","shell.execute_reply":"2021-01-07T21:30:05.9881Z"},"papermill":{"duration":0.051188,"end_time":"2021-01-07T21:30:05.988246","exception":false,"start_time":"2021-01-07T21:30:05.937058","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# processed_data = applyParallel(telemetry_data.groupby('frame_uid'), create_attributes)\n# processed_data = np.vstack(processed_data)\n# processed_data = processed_data.astype(np.float32)\n# data_df = telemetry_data[['frame_uid', 'play_uid', 'playId', 'gameId', 'event', 'frameId', 'd_team','playResult', 'epa', 'quarter', 'down', 'yardsToGo']].groupby('frame_uid').max()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:06.08265Z","iopub.status.busy":"2021-01-07T21:30:06.081868Z","iopub.status.idle":"2021-01-07T21:30:06.085353Z","shell.execute_reply":"2021-01-07T21:30:06.084673Z"},"papermill":{"duration":0.052158,"end_time":"2021-01-07T21:30:06.085467","exception":false,"start_time":"2021-01-07T21:30:06.033309","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# def enrich_data_df(df):\n#     ball_snap_idx = np.where(df['event']=='ball_snap')\n#     frame_of_ball_snap = np.max(df.iloc[ball_snap_idx]['frameId'].values)\n#     df['snap_action'] = df['frameId'].apply(lambda x: 'pre-snap' if x < frame_of_ball_snap else('post-snap' if x > frame_of_ball_snap else 'snap'))\n#     return df\n\n# data_df = data_df.groupby('play_uid').apply(enrich_data_df)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:06.177708Z","iopub.status.busy":"2021-01-07T21:30:06.177026Z","iopub.status.idle":"2021-01-07T21:30:06.184211Z","shell.execute_reply":"2021-01-07T21:30:06.183599Z"},"papermill":{"duration":0.054188,"end_time":"2021-01-07T21:30:06.184328","exception":false,"start_time":"2021-01-07T21:30:06.13014","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"'''look at statistics about the number of snapshots per play'''\n\n# data_df[['play_uid', 'frameId']].groupby('play_uid').count().describe()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:06.282055Z","iopub.status.busy":"2021-01-07T21:30:06.281169Z","iopub.status.idle":"2021-01-07T21:30:06.28434Z","shell.execute_reply":"2021-01-07T21:30:06.284854Z"},"papermill":{"duration":0.05531,"end_time":"2021-01-07T21:30:06.285","exception":false,"start_time":"2021-01-07T21:30:06.22969","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"'''Process the data to get timeseries data for all of the plays'''\n\n# play_uids, play_processed_data = process_play_data(processed_data, data_df[['play_uid','frameId']], 100)\n# play_data_df = pd.DataFrame(play_uids, columns=['play_uid']).merge(data_df.groupby('play_uid').max(), how='left', on='play_uid')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:06.379641Z","iopub.status.busy":"2021-01-07T21:30:06.378989Z","iopub.status.idle":"2021-01-07T21:30:06.382062Z","shell.execute_reply":"2021-01-07T21:30:06.382605Z"},"papermill":{"duration":0.052966,"end_time":"2021-01-07T21:30:06.382747","exception":false,"start_time":"2021-01-07T21:30:06.329781","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"'''Save out the processed data, since it takes a long time to process the ~1.2 million play snapshots'''\n\n# np.save(os.path.join(save_dir,\"processed_data.npy\"), processed_data)\n# data_df.to_csv(os.path.join(save_dir,\"processed_data_labels.csv\"))\n# np.save(os.path.join(save_dir,\"play_processed_data.npy\"), play_processed_data)\n# play_data_df.to_csv(os.path.join(save_dir,\"processed_play_data_labels.csv\"))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.044645,"end_time":"2021-01-07T21:30:06.473165","exception":false,"start_time":"2021-01-07T21:30:06.42852","status":"completed"},"tags":[]},"cell_type":"markdown","source":"# Appendix B: Code Implementation for Coverage2Vec Model\n\nIn this section, we have the tensorflow implementation of the coverage2vec model along with the code used to train the model. We also briefly compared the outputs of the model to a more simpler dimensionality reduction technique, PCA, to see if the coverage2vec does indeed produce better embeddings and less reconstruction error for the plays."},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:06.567373Z","iopub.status.busy":"2021-01-07T21:30:06.566659Z","iopub.status.idle":"2021-01-07T21:30:06.572151Z","shell.execute_reply":"2021-01-07T21:30:06.571599Z"},"papermill":{"duration":0.054019,"end_time":"2021-01-07T21:30:06.572275","exception":false,"start_time":"2021-01-07T21:30:06.518256","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras import layers, losses, regularizers\n# from tensorflow.keras.models import Model, save_model\n\n# class Coverage2Vec(Model):\n#     def __init__(self, latent_dim, sequence_length, feature_size):\n#         super(Coverage2Vec, self).__init__()\n\n#         self.latent_dim = latent_dim\n#         self.sequence_length = sequence_length\n#         self.feature_size= feature_size\n\n#         self.encoder = tf.keras.Sequential([\n#             layers.InputLayer(input_shape = (sequence_length, feature_size), name=\"input\"),\n#             layers.Conv1D(filters=100, kernel_size=30, activation ='relu', strides=30, name='encoder_hidden_1'),\n#             layers.Conv1D(filters=50, kernel_size=2, activation ='relu', strides=2, name='encoder_hidden_2'),\n#             layers.MaxPooling1D(pool_size=2),\n#             layers.Conv1D(filters=20, kernel_size=2, activation ='relu', strides=2, name='encoder_hidden_3'),\n#             layers.AveragePooling1D(pool_size=2, name='encoder_hidden_4'),\n#             layers.Flatten(),\n#             layers.Dense(3, activation='relu', name='embedding')\n#         ])\n#         self.decoder = tf.keras.Sequential([\n#             layers.Dense(10, activation='relu', name='decoder_hidden_1'),\n#             layers.Dense(100, activation='relu', name='decoder_hidden_2'),\n#             layers.Dense(330, activation='relu', name='decoder_hidden_3'),\n#             layers.Reshape((330, 1)),\n#             layers.BatchNormalization(),\n#             layers.Conv1DTranspose(filters=50, kernel_size=5, activation='relu', strides=2, name='decoder_hidden_4', padding='same'),\n#             layers.BatchNormalization(),\n#             layers.Conv1DTranspose(filters=feature_size, kernel_size=2, activation='sigmoid', strides=1, name='output', padding='same')\n#         ])\n\n#     def call(self, x):\n#         encoded = self.encoder(x)\n#         decoded = self.decoder(encoded)\n#         return decoded","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:06.666729Z","iopub.status.busy":"2021-01-07T21:30:06.66604Z","iopub.status.idle":"2021-01-07T21:30:06.668252Z","shell.execute_reply":"2021-01-07T21:30:06.66871Z"},"papermill":{"duration":0.051638,"end_time":"2021-01-07T21:30:06.668887","exception":false,"start_time":"2021-01-07T21:30:06.617249","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# latent_dim = 3\n\n# c2v = Coverage2Vec(latent_dim, play_processed_data.shape[1], play_processed_data.shape[2])\n# c2v.compile(optimizer='adam', loss='mse')","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:06.765598Z","iopub.status.busy":"2021-01-07T21:30:06.764949Z","iopub.status.idle":"2021-01-07T21:30:06.770694Z","shell.execute_reply":"2021-01-07T21:30:06.770052Z"},"papermill":{"duration":0.055861,"end_time":"2021-01-07T21:30:06.77085","exception":false,"start_time":"2021-01-07T21:30:06.714989","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# checkpoint_path = os.path.join(save_dir, \"cp.ckpt\")\n# checkpoint_dir = os.path.dirname(checkpoint_path)\n# NUM_FULL_ITERATIONS = 50\n# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n#                                                  save_weights_only=True,\n#                                                  verbose=1,\n#                                                  save_freq=200)\n\n# game_ids = play_data_df['gameId']\n# for i in range(NUM_FULL_ITERATIONS):\n#     val_game_ids = np.random.choice(game_ids, size=5, replace=False)\n#     X_val = play_processed_data[np.where(game_ids.isin(val_game_ids))]\n#     train_game_ids = game_ids[~np.isin(game_ids, val_game_ids)]\n    \n#     X_train =  play_processed_data[np.where(game_ids.isin(train_game_ids))]\n#     print(\"Iteration Number : {}, X_train size :{}, X_val size: {}\".format(i, X_train.shape, X_val.shape))\n#     c2v.fit(X_train, X_train,\n#                     epochs=50,\n#                     batch_size=400,\n#                     shuffle=True,\n#                     callbacks=[cp_callback],\n#                     validation_data=(X_val, X_val)\n#            )","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:06.867033Z","iopub.status.busy":"2021-01-07T21:30:06.866347Z","iopub.status.idle":"2021-01-07T21:30:06.869965Z","shell.execute_reply":"2021-01-07T21:30:06.86934Z"},"papermill":{"duration":0.0527,"end_time":"2021-01-07T21:30:06.870084","exception":false,"start_time":"2021-01-07T21:30:06.817384","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# c2v.save(os.path.join(save_dir, \"c2v_model\"))","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.045641,"end_time":"2021-01-07T21:30:06.963899","exception":false,"start_time":"2021-01-07T21:30:06.918258","status":"completed"},"tags":[]},"cell_type":"markdown","source":"### Check on embedding quality"},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:07.058121Z","iopub.status.busy":"2021-01-07T21:30:07.057474Z","iopub.status.idle":"2021-01-07T21:30:07.060693Z","shell.execute_reply":"2021-01-07T21:30:07.061228Z"},"papermill":{"duration":0.05227,"end_time":"2021-01-07T21:30:07.061377","exception":false,"start_time":"2021-01-07T21:30:07.009107","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# test = np.random.randint(low=0,high=play_processed_data.shape[0], size=10)\n# temp = c2v.encoder(play_processed_data[test])\n# recon = c2v.decoder(temp)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:07.156412Z","iopub.status.busy":"2021-01-07T21:30:07.155737Z","iopub.status.idle":"2021-01-07T21:30:07.158743Z","shell.execute_reply":"2021-01-07T21:30:07.158231Z"},"papermill":{"duration":0.052448,"end_time":"2021-01-07T21:30:07.158885","exception":false,"start_time":"2021-01-07T21:30:07.106437","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# temp","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:07.256076Z","iopub.status.busy":"2021-01-07T21:30:07.255406Z","iopub.status.idle":"2021-01-07T21:30:07.258178Z","shell.execute_reply":"2021-01-07T21:30:07.258879Z"},"papermill":{"duration":0.054582,"end_time":"2021-01-07T21:30:07.259064","exception":false,"start_time":"2021-01-07T21:30:07.204482","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"'''Print the reconstruction errors (in MAE) for the autoencoder for each of the plays'''\n# for i in range(recon.shape[0]):\n#     print(np.sum(np.abs(play_processed_data[test][i]-recon[i])))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:07.353339Z","iopub.status.busy":"2021-01-07T21:30:07.35268Z","iopub.status.idle":"2021-01-07T21:30:07.355865Z","shell.execute_reply":"2021-01-07T21:30:07.356509Z"},"papermill":{"duration":0.051938,"end_time":"2021-01-07T21:30:07.356666","exception":false,"start_time":"2021-01-07T21:30:07.304728","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# from sklearn.decomposition import PCA\n\n# pca = PCA(n_components=3)\n# pca.fit(np.vstack(play_processed_data))","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:07.453337Z","iopub.status.busy":"2021-01-07T21:30:07.452602Z","iopub.status.idle":"2021-01-07T21:30:07.456723Z","shell.execute_reply":"2021-01-07T21:30:07.456161Z"},"papermill":{"duration":0.052519,"end_time":"2021-01-07T21:30:07.45686","exception":false,"start_time":"2021-01-07T21:30:07.404341","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"# temp_pca = pca.transform(np.vstack(play_processed_data[test]))\n# temp_pca","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-07T21:30:07.553198Z","iopub.status.busy":"2021-01-07T21:30:07.55252Z","iopub.status.idle":"2021-01-07T21:30:07.556763Z","shell.execute_reply":"2021-01-07T21:30:07.556159Z"},"papermill":{"duration":0.054393,"end_time":"2021-01-07T21:30:07.556896","exception":false,"start_time":"2021-01-07T21:30:07.502503","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"'''Print the reconstruction errors (in MAE) for the PCA for each of the plays'''\n# recon_pca = pca.inverse_transform(temp_pca)\n\n# for i in range(recon.shape[0]):\n#     print(np.sum(np.abs(np.vstack(play_processed_data[test][i])-recon_pca[i])))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}