{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Notebook setup"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://cdn.tiebreaker.com/wp-content/uploads/2019/10/butt-fumble-768x444.png\" width=\"650px\"/>"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"pip install nb-black -qq","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"%load_ext nb_black","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"pip install parfit -qq","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport feather\nimport lightgbm as lgb\nimport shap\nimport warnings\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.pipeline import FeatureUnion\nfrom lightgbm import LGBMClassifier\nimport parfit.parfit as pf\nimport math\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"pd.set_option(\"display.max_rows\", 100)\n\n\ndef display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000):\n        with pd.option_context(\"display.max_columns\", 1000):\n            display(df)\n\n\ndef rmse(x, y):\n    return math.sqrt(((x - y) ** 2).mean())\n\n\ndef print_score(m, X_train, y_train, X_test, y_test, model_name):\n    columns = [\"model\", \"Precision train\", \"Precision test\", \"ROC test\"]\n    res = pd.Series(\n        [\n            model_name,\n            average_precision_score(y_train, m.predict(X_train)),\n            average_precision_score(y_test, m.predict(X_test)),\n            roc_auc_score(y_test, m.predict(X_test)),\n        ],\n        index=columns,\n    )\n    if hasattr(m, \"oob_score_\"):\n        print(\"RMSE oob\", m.oob_score_)\n    return res\n\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame(\n        {\"cols\": df.columns, \"imp\": m.feature_importances_}\n    ).sort_values(\"imp\", ascending=False)\n\n\ndef features_type(dataframe, number=0):\n    if number == 0:\n        feat = dataframe.select_dtypes(exclude=[\"object\"]).columns\n    else:\n        feat = dataframe.select_dtypes(include=[\"object\"]).columns\n    return feat\n\n\ndef ifnone(a, b):\n    return b if a is None else a\n\n\ndef make_date(df, date_field):\n    field_dtype = df[date_field].dtype\n    if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        field_dtype = np.datetime64\n    if not np.issubdtype(field_dtype, np.datetime64):\n        df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True)\n\n\ndef add_datepart(df, field_name, prefix, drop=True, time=False):\n    \"Helper function that adds columns relevant to a date in the column `field_name` of `df`.\"\n    make_date(df, field_name)\n    field = df[field_name]\n    attr = [\n        \"Year\",\n        \"Month\",\n        \"Day\",\n        \"Dayofweek\",\n        \"Dayofyear\",\n        \"Is_month_end\",\n        \"Is_month_start\",\n        \"Is_quarter_end\",\n        \"Is_quarter_start\",\n        \"Is_year_end\",\n        \"Is_year_start\",\n    ]\n    if time:\n        attr = attr + [\"Hour\", \"Minute\", \"Second\"]\n    for n in attr:\n        df[prefix + n] = getattr(field.dt, n.lower())\n    # Pandas removed `dt.week` in v1.1.10\n    week = (\n        field.dt.isocalendar().week\n        if hasattr(field.dt, \"isocalendar\")\n        else field.dt.week\n    )\n    df.insert(3, prefix + \"Week\", week)\n    mask = ~field.isna()\n    df[prefix + \"Elapsed\"] = np.where(\n        mask, field.values.astype(np.int64) // 10 ** 9, None\n    )\n    if drop:\n        df.drop(field_name, axis=1, inplace=True)\n    return df\n\n\ndef z_score(df):\n    df_std = df.copy()\n    for column in df_std.columns:\n        if df_std[column].max() > 1:\n            df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[\n                column\n            ].std()\n        else:\n            df_std[column] = df_std[column]\n\n    return df_std\n\n\ndef feature_plot(df, col):\n    perc_amt = pd.DataFrame(\n        (\n            df.groupby([col, \"win\"])[\"win\"].count()\n            / df.groupby([col])[\"win\"].count()\n            * 100\n        ).unstack(\"win\")\n    )\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0: \"Lose\", 1: \"Win\"}, inplace=True)\n\n    plt.figure(figsize=(12, 4))\n    plt.title(\"Win % and distribution against {}\".format(col, fontsize=15))\n    order0 = df[col].unique()\n    p1 = sns.countplot(data=df, hue=\"win\", x=col, palette=\"rocket\", order=order0)\n    p1.set_xticklabels(p1.get_xticklabels(), rotation=45)\n    p2 = p1.twinx()\n    p2 = sns.pointplot(x=col, y=\"Win\", data=perc_amt, order=order0, color=\"red\")\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preprocesing\n\nImporting games and play data, win/lose outcome. NFL 2018 season data, 16 teams, 221 game. The goal is to forecast if a home team will wins."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true,"_kg_hide-input":true},"cell_type":"code","source":"games_df = pd.read_csv(\"../input/nfl-big-data-bowl-2021/games.csv\", parse_dates=True)\n# players_df = pd.read_csv('../input/nfl-big-data-bowl-2021/players.csv', parse_dates = True)\nplays_df = pd.read_csv(\"../input/nfl-big-data-bowl-2021/plays.csv\", parse_dates=True)\nstats_df = pd.read_csv(\n    \"../input/nfl-play-2009-2018/NFL Play by Play 2009-2018 (v5).csv\", parse_dates=True\n)\nmissing_df = pd.read_csv(\n    \"../input/missing-win/Untitled spreadsheet - Sheet1.csv\",\n    parse_dates=True,\n    header=None,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Play dataset\nIt is the main dataset used for modelling. Attributes explanation:\n\n- **gameId**: Game identifier, unique (numeric)\n- **playId**: Play identifier, not unique across games (numeric)\n- **playDescription**: Description of play (text)\n- **quarter**: Game quarter (numeric)\n- **down**: Down (numeric)\n- **yardsToGo**: Distance needed for a first down (numeric)\n- **possessionTeam**: Team on offense (text), with a possession of a ball\n- **playType**: Outcome of dropback: sack or pass (text)\n- **yardlineSide**: 3-letter team code corresponding to line-of-scrimmage (text)\n- **yardlineNumber**: Yard line at line-of-scrimmage (numeric)\n- **offenseFormation**: Formation used by possession team (text)\n- **personnelO**: Personnel used by offensive team (text)\n- **defendersInTheBox**: Number of defenders in close proximity to line-of-scrimmage (numeric)\n- **numberOfPassRushers**: Number of pass rushers (numeric)\n- **personnelD**: Personnel used by defensive team (text)\n- **typeDropback**: Dropback categorization of quarterback (text)\n- **preSnapHomeScore**: Home score prior to the play (numeric)\n- **preSnapVisitorScore**: Visiting team score prior to the play (numeric)\n- **gameClock**: Time on clock of play (MM:SS)\n- **absoluteYardlineNumber**: Distance from end zone for possession team (numeric)\n- **penaltyCodes**: NFL categorization of the penalties that ocurred on the play. For purposes of this contest, the most important penalties are Defensive Pass Interference (DPI), Offensive Pass Interference (OPI), Illegal Contact (ICT), and Defensive Holding (DH). Multiple penalties on a play are separated by a ; (text)\n- **penaltyJerseyNumber**: Jersey number and team code of the player commiting each penalty. Multiple penalties on a play are separated by a ; (text)\n- **passResult**: Outcome of the passing play (C: Complete pass, I: Incomplete pass, S: Quarterback sack, IN: Intercepted pass, text)\n- **offensePlayResult**: Yards gained by the offense, excluding penalty yardage (numeric)\n- **playResult**: Net yards gained by the offense, including penalty yardage (numeric)\n- **epa**: Expected points added on the play, relative to the offensive team. Expected points is a metric that - -- estimates the average of every next scoring outcome given the play's down, distance, yardline, and time remaining (numeric)\n- **isDefensivePI**: An indicator variable for whether or not a DPI penalty ocurred on a given play (TRUE/FALSE)[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"display_all(plays_df.tail())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"stats_df = stats_df[stats_df[\"game_date\"] > \"2018-09-01\"].sort_values(\n    [\"game_id\", \"play_id\"]\n)\nmax_play = stats_df.groupby([\"game_id\"])[\"play_id\"].max()\nmax_play2 = pd.DataFrame(max_play).reset_index()\nstats_df1 = pd.merge(\n    stats_df.reset_index(), max_play2, how=\"inner\", on=[\"game_id\", \"play_id\"]\n)\nstats_df1[\"win\"] = np.where(\n    stats_df1[\"total_home_score\"] > stats_df1[\"total_away_score\"], 1, 0\n)\ngames_df = games_df.set_index(\"gameId\").join(\n    stats_df1[[\"game_id\", \"win\"]].set_index(\"game_id\")\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"missing_df.columns = [\n    \"gameId\",\n    \"gameDate\",\n    \"gameTimeEastern\",\n    \"homeTeamAbbr\",\n    \"visitorTeamAbbr\",\n    \"week\",\n    \"win\",\n]\nmissing_df.set_index(\"gameId\", inplace=True)\nmissing_df.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"games_df2 = games_df.dropna().copy()\ngames_df3 = games_df2.append(missing_df)\n\nplays_full = pd.merge(\n    games_df3.reset_index().rename(columns={\"index\": \"gameId\"}), plays_df, on=\"gameId\"\n)\nplays_full.set_index([\"gameId\", \"playId\"], inplace=True)\nplays_full.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"plays_full.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Final dataset for modelling with 31 attributes and 20258 rows."},{"metadata":{},"cell_type":"markdown","source":"# Data modelling\nModel goal is to predict which team will win a match in NFL league.\n## Baseline models\nDecision tree, Logistic regression and Random forest without feature engineering. The models are created with just a sample of attributes, like Home team, Away team, preSnapScore. Missing values are replaced with 0 since most of cases, values are missing because their are zeros. For example, penalty type is none if missing in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = pd.DataFrame(\n    [], columns=[\"model\", \"Precision train\", \"Precision test\", \"ROC test\"]\n)\nplays_full.sort_index(axis=0, inplace=True)\n\ny = plays_full[\"win\"]\n\nkeys = [\n    \"homeTeamAbbr\",\n    \"visitorTeamAbbr\",\n    \"passResult\",\n    \"offensePlayResult\",\n    \"preSnapHomeScore\",\n    \"preSnapVisitorScore\",\n    \"absoluteYardlineNumber\",\n    \"yardsToGo\",\n    \"possessionTeam\",\n]\nX = plays_full[keys].copy()\n\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n        (\"scaler\", StandardScaler()),\n    ]\n)\n\ncategorical_transformer = Pipeline(\n    steps=[\n        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n        (\"encoder\", OneHotEncoder()),\n    ]\n)\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, features_type(X, number=0)),\n        (\"cat\", categorical_transformer, features_type(X, number=1)),\n    ]\n)\n\nclf = Pipeline(\n    steps=[\n        (\"preprocessor\", preprocessor),\n        (\"model\", DecisionTreeClassifier(max_depth=2)),\n    ]\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\nclf.fit(X_train, y_train)\nmodel_score = model_score.append(\n    [\n        print_score(\n            clf, X_train, y_train, X_test, y_test, model_name=\"Base DecissionTree\"\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_lg = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"model\", LogisticRegression(n_jobs=-1))]\n)\n\nclf_lg.fit(X_train, y_train)\nmodel_score = model_score.append(\n    [print_score(clf_lg, X_train, y_train, X_test, y_test, model_name=\"Base LogReg\")],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf_rf = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"model\", RandomForestClassifier(n_jobs=-1))]\n)\n\nclf_rf.fit(X_train, y_train)\nmodel_score = model_score.append(\n    [\n        print_score(\n            clf_rf, X_train, y_train, X_test, y_test, model_name=\"Base Random Forest\"\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n    \"model__C\": [0.1, 1.0, 10, 100],\n}\n\ngrid_search = GridSearchCV(clf_lg, param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\nmodel_score = model_score.append(\n    [\n        print_score(\n            grid_search,\n            X_train,\n            y_train,\n            X_test,\n            y_test,\n            model_name=\"LogReg with GridSearch\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering\n\nSome attributes, like *'homeTeamAbbr'*, contains **NFL teams** abbreviations. In stead of **OneHotEncoding**, attributes are encoded by setting 2 if a team plays at home and 1 if a team plays away. This encoding decreases number of attributes comparing to **OneHotEncoding**. I consider it as a possible advantage since dataset is quite small and attributtes containing **NFL teams** are connected. Other attributes containing team abbreviation (like possession team) gets 1 if it is a home team.   \nOther categorical variables are encoded with **CatBooster** since some attribustes have high cardinality and this method is proven to be best for forest type models.  \nCustom trasformers are used in pipelines to transform and to model data."},{"metadata":{"trusted":true},"cell_type":"code","source":"plays_full.reset_index(inplace=True)\nplays_full.sort_values([\"gameId\", \"playId\", \"gameClock\"])\ny = plays_full[\"win\"]\nX0 = plays_full.drop([\"win\", \"playDescription\", \"gameTimeEastern\"], axis=1)\nX0 = add_datepart(X0, \"gameDate\", \"Date\")\n\nteams = X0[\"homeTeamAbbr\"].unique()\nhomeTeam = pd.DataFrame()\nvisitorTeam = pd.DataFrame()\nfor i in teams:\n    homeTeam[i] = [2 if j == i else 0 for j in X0[\"homeTeamAbbr\"].values]\n    visitorTeam[i] = [1 if j == i else 0 for j in X0[\"visitorTeamAbbr\"].values]\n\nplayTeam = homeTeam + visitorTeam\nplayTeam[\"possessionTeam\"] = [\n    1 if i == j else 0 for i, j in zip(X0[\"homeTeamAbbr\"], X0[\"possessionTeam\"])\n]\nplayTeam[\"yardlineSide\"] = [\n    1 if i == j else 0 for i, j in zip(X0[\"homeTeamAbbr\"], X0[\"yardlineSide\"])\n]\nX1 = X0.drop(\n    [\n        \"possessionTeam\",\n        \"homeTeamAbbr\",\n        \"visitorTeamAbbr\",\n        \"yardlineSide\",\n        \"DateElapsed\",\n    ],\n    axis=1,\n).join(playTeam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import TransformerMixin, BaseEstimator\n\nclass NumDataSelector(TransformerMixin, BaseEstimator):\n    \n    def fit(self, data, y=None):\n        return self\n        \n    def transform(self, data, y=None):\n        self.num_columns = features_type(data, number=0)\n        return data[self.num_columns]\n    \n#     def get_feature_names(self):\n#         return data[self.num_columns].columns.tolist()\n    \nclass CatDataSelector(TransformerMixin, BaseEstimator):\n    \n    def fit(self, data, y=None):\n        return self\n        \n    def transform(self, data, y=None):\n        self.cat_columns = features_type(data, number=1)\n        return data[self.cat_columns]\n    \n    def get_feature_names(self):\n        return data[self.cat_columns].columns.tolist()\n    \nclass CatEncoder(TransformerMixin, BaseEstimator):\n    \n    def __init__(self):\n        self._encoder = ce.CatBoostEncoder()\n    \n    def fit(self, data, y):\n        self._encoder.fit(data, y)\n        return self\n        \n    def transform(self, data, y=None):\n        self.encoded_data = pd.DataFrame(self._encoder.transform(data), columns=data.columns).add_suffix('_cb')\n        return self.encoded_data\n    \n    def get_feature_names(self):\n        return self.encoded_data.columns.tolist()\n            \n    \nclass SimpleImputerWrapper(TransformerMixin, BaseEstimator):\n\n    def __init__(self, fill_value):\n        self._fill_value = fill_value\n        self._imputer = SimpleImputer(fill_value=self._fill_value, strategy='constant')\n    \n    def fit(self, data, y=None):\n        self._imputer.fit(data)\n        return self\n    \n    def transform(self, data, y=None):\n        imputed_data = self._imputer.transform(data)\n        return pd.DataFrame(imputed_data, columns= data.columns)\n    \n    \nclass ZScaler(TransformerMixin, BaseEstimator):\n    \n    def fit(self, data, y=None):\n        return self\n    \n    def transform(self, data, y=None):\n        return z_score(data)\n\n#     def get_feature_names(self):\n#         return z_score(data).columns.tolist()\n    \nclass FeatureSelector(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, remove_feature_names=None):\n        self._remove_feature_names = remove_feature_names \n        \n    def fit(self, data, y=None):\n        return self\n        \n    def transform(self, data, y=None):\n        self._feature_names = data.columns.tolist()\n        \n        if self._remove_feature_names is not None:\n            self._feature_names = [feat for feat in self._feature_names if feat not in self._remove_feature_names]\n        \n        return data[self._feature_names]\n    \n#     def get_feature_names(self):\n#         return self._feature_names\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"drop_list = [\"DateYear\", \"DateWeek\", \"playId\", \"gameId\", \"week\"]\n\nnumeric_transformer = Pipeline(\n    steps=[\n        (\"num_columns\", NumDataSelector()),\n        (\"imputer\", SimpleImputerWrapper(fill_value=0)),\n        (\"selector\", FeatureSelector(remove_feature_names=drop_list)),\n        (\"scaler\", ZScaler()),\n    ]\n)\n\ncategorical_transformer = Pipeline(\n    steps=[\n        (\"cat_columns\", CatDataSelector()),\n        (\"imputer\", SimpleImputerWrapper(fill_value=\"missing\")),\n        (\"encoder\", CatEncoder()),\n    ]\n)\n\nfull_pipeline = [\n    (\n        \"prep\",\n        FeatureUnion(\n            transformer_list=[\n                (\"num_pipeline\", numeric_transformer),\n                (\"cat_pipeline\", categorical_transformer),\n            ]\n        ),\n    )\n]\n\nclf_rf1 = RandomForestClassifier(n_jobs=-1)\nclf_rf1_steps = full_pipeline.copy()\nclf_rf1_steps.append((\"RF Classifier\", clf_rf1))\n\nclf_rf1_pipe = Pipeline(steps=clf_rf1_steps)\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X1, y, test_size=0.3, shuffle=False\n)\n\nclf_rf1_pipe.fit(X_train.copy(), y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            clf_rf1_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"Random Forest with Feat1\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = list(features_type(X_valid.drop(drop_list, axis=1), number=0)) + list(\n    features_type(X_valid.drop(drop_list, axis=1), number=1)\n)\n\nfi = rf_feat_importance(clf_rf1_pipe.named_steps[\"RF Classifier\"], X_valid[col_names])\nfi[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_transformer2 = Pipeline(\n    steps=[\n        (\"num_columns\", NumDataSelector()),\n        (\"imputer\", SimpleImputerWrapper(fill_value=0)),\n        # ('selector', FeatureSelector(remove_feature_names=drop_list)),\n        (\"scaler\", ZScaler()),\n    ]\n)\n\npart_pipeline = [\n    (\n        \"prep\",\n        FeatureUnion(\n            transformer_list=[\n                (\"num_pipeline\", numeric_transformer2),\n                (\"cat_pipeline\", categorical_transformer),\n            ]\n        ),\n    )\n]\npart_pipeline1 = part_pipeline.copy()\npart_pipeline1.append((\"RF Classifier\", clf_rf1))\n\npart_pipe = Pipeline(steps=part_pipeline1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"imp_features = fi[\"cols\"][:20].values\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X1[imp_features], y, test_size=0.3, shuffle=False\n)\n\npart_pipe.fit(X_train.copy(), y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            part_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"Random Forest with Feat1 20\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter tunning\nParfit optimization is used for Random Forest hyperparameter tunning. The combination of *min_samples_leaf* and *max_features* gives the best score on validation set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"paramGrid = ParameterGrid(\n    {\n        \"min_samples_leaf\": [15, 25, 50, 100, 150, 175, 200, 225, 250],\n        \"max_features\": [\"sqrt\", \"log2\", 0.4, 0.5, 0.6, 0.7],\n        \"n_estimators\": [100, 500],\n        \"n_jobs\": [-1],\n    }\n)\n\nX_train_trans = pd.DataFrame(part_pipe.named_steps[\"prep\"].transform(X_train))\nX_valid_trans = pd.DataFrame(part_pipe.named_steps[\"prep\"].transform(X_valid))\n\nbest_model, best_score, all_models, all_scores = pf.bestFit(\n    RandomForestClassifier,\n    paramGrid,\n    X_train_trans,\n    y_train,\n    X_valid_trans,\n    y_valid,\n    metric=roc_auc_score,\n    scoreLabel=\"AUC\",\n)\nprint(best_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(best_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"clf_rf2 = RandomForestClassifier(\n    n_estimators=500, max_features=0.7, min_samples_leaf=225, n_jobs=-1\n)\nclf_rf2_steps = part_pipeline.copy()\nclf_rf2_steps.append((\"RF2 Classifier\", clf_rf2))\n\nclf_rf2_pipe = Pipeline(steps=clf_rf2_steps)\nclf_rf2_pipe.fit(X_train.copy(), y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            clf_rf2_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"Random Forest Hypertunning with Feat1 20\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some feature engineering, hyperparameter tuning and oob scoring gives the best RF model so far. ROC is around 0.65 which is not a good result. ROC = 0.5 is a random model."},{"metadata":{},"cell_type":"markdown","source":"## LGB model\nLBG is tried since it is considered to be one of the best in ML models class. Model hyperparameters are set based on previous models. Some hyperparameters search is done as well. "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"d_train = lgb.Dataset(X_train_trans, label=y_train)\nd_test = lgb.Dataset(X_valid_trans, label=y_valid)\n\nparams_lgb = {\n    \"max_bin\": 512,\n    \"min_data_in_leaf\": 225,\n    \"learning_rate\": 0.1,\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"num_leaves\": 80,\n    \"verbose\": -1,\n    \"min_data\": 10,\n    \"boost_from_average\": True,\n    \"bagging_fraction\": 0.7,\n}\n\nmodel = lgb.train(\n    params_lgb,\n    d_train,\n    10000,\n    valid_sets=[d_test],\n    early_stopping_rounds=50,\n    verbose_eval=1000,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            model,\n            X_train_trans,\n            y_train,\n            X_valid_trans,\n            y_valid,\n            model_name=\"LightGBM with Feat1 20\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"min = float(\"inf\")\npp = {}\niterations = 500\n# for i in range(iterations):\n# #     try:\n#     params = {}\n#     params['learning_rate'] = np.random.uniform(0, 0.4)\n#     params['boosting_type'] = 'gbdt'\n#     params['objective'] = 'binary'\n#     params['metric'] = 'rmse'\n#     params['sub_feature'] = np.random.uniform(0.3, 0.7)\n#     params['num_leaves'] = np.random.randint(100, 300)\n#     params['min_data'] = np.random.randint(150, 250)\n#     params['max_depth'] = np.random.randint(3, 9)\n#     clf = lgb.train(params, d_train, 10000, valid_sets=[d_test],\n#                     early_stopping_rounds=50, verbose_eval=1000)\n#     y_pred=clf.predict(X_valid_trans)\n#     mae=rmse(y_pred,y_valid)\n#     if mae < min:\n#         min = mae\n#         pp = params\n# #     except:\n# #         print('failed with')\n# #         print(params)\n\n# print('Minimum rmse is: ', min)\n# print('Used params', pp)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp = {\n    \"learning_rate\": 0.325229431778417,\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"rmse\",\n    \"sub_feature\": 0.5937219626468773,\n    \"num_leaves\": 281,\n    \"min_data\": 217,\n    \"max_depth\": 8,\n}\nmodel2 = lgb.train(\n    pp, d_train, 10000, valid_sets=[d_test], early_stopping_rounds=50, verbose_eval=1000\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            model2,\n            X_train_trans,\n            y_train,\n            X_valid_trans,\n            y_valid,\n            model_name=\"LightGBM Hypertunning with Feat1 20\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SHAP values\nTo find out what is inside LGB model"},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(model)\nexpected_value = explainer.expected_value\n\nselect = range(20)\ncol_names2 = list(features_type(X_valid, number=0)) + list(\n    features_type(X_valid, number=1)\n)\nX_valid_shap = pd.DataFrame(X_valid_trans)\nX_valid_shap.columns = col_names2\nfeatures = X_valid_shap.iloc[select]\nfeatures_display = X_valid_shap.loc[features.index]\n\nshap_values = explainer.shap_values(features)\nshap_interaction_values = explainer.shap_interaction_values(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.decision_plot(expected_value[0], shap_values[0], features_display)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, X_valid_shap, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values = explainer.shap_values(X_valid_shap)\nshap.dependence_plot(\"preSnapVisitorScore\", shap_values[0], X_valid_shap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.decision_plot(expected_value, shap_interaction_values, features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plots"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"tmp = pd.crosstab(X0['homeTeamAbbr'], y, normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'Lose', 1:'Win'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('Home Team Distributions', fontsize=22)\n\nplt.subplot(311)\ng = sns.boxenplot(x='homeTeamAbbr', y='preSnapHomeScore', hue='win', data=plays_full, order=tmp['homeTeamAbbr'])\nplt.legend(title='Home team win', loc='upper center')\n\ng.set_title(\"Home Team Scores\", fontsize=19)\ng.set_xlabel(\"Home Team Name\", fontsize=17)\ng.set_ylabel(\"Score\", fontsize=17)\n\nplt.subplot(312)\ngt = sns.pointplot(x='homeTeamAbbr', y='Win', data=tmp, color='purple', legend=False)\n\ngt.set_ylabel(\"% of Win\", fontsize=16)\ngt.set_title(\"Home Team Wins %\", fontsize=19)\ngt.set_xlabel(\"Home Team\", fontsize=17)\n\nplt.subplot(313)\ng3 = sns.boxenplot(x='homeTeamAbbr', y='epa', hue='win', \n              data=plays_full, order=tmp['homeTeamAbbr'])\ng3.set_title(\"Epa Distribuition by Team\", fontsize=20)\ng3.set_xlabel(\"Home Team Name\", fontsize=17)\ng3.set_ylabel(\"Epa\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"clf_xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=-1)\nclf_xgb_steps = full_pipeline.copy()\nclf_xgb_steps.append((\"XGBClassifier\", clf_xgb))\n\nclf_xgb_pipe = Pipeline(steps=clf_xgb_steps)\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X1, y, test_size=0.3, shuffle=False\n)\n\nclf_xgb_pipe.fit(X_train.copy(), y_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            clf_xgb_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"XGB with Feat1\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model2 = clf_xgb_pipe.named_steps['XGBClassifier']\n\nplt.figure(figsize=(8,8))\nxgb.plot_importance(model2, max_num_features=20).set_yticklabels(X_valid.columns.tolist())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dtrain = xgb.DMatrix(clf_xgb_pipe.named_steps[\"prep\"].transform(X_train), label=y_train)\ndvalid = xgb.DMatrix(clf_xgb_pipe.named_steps[\"prep\"].transform(X_valid), label=y_valid)\nparams = {\n    \"objective\": \"reg:logistic\",\n    \"colsample_bytree\": 0.5,\n    \"learning_rate\": 0.1,\n    \"max_depth\": 10,\n    \"alpha\": 10,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gridsearch_params = [\n#     (max_depth, min_child_weight)\n#     for max_depth in range(2, 6)\n#     for min_child_weight in range(20,81)\n# ]\n\n# num_boost_round = 48\n# rmse = float(\"Inf\")\n# best_params = None\n# for max_depth, min_child_weight in gridsearch_params:\n#     params['max_depth'] = max_depth\n#     params['min_child_weight'] = min_child_weight\n#     cv_results = xgb.cv(\n#         params,\n#         dtrain,\n#         num_boost_round=num_boost_round,\n#         nfold=5,\n#         early_stopping_rounds=10\n#     )\n#     mean_rmse = cv_results['test-rmse-mean'].min()\n#     boost_rounds = cv_results['test-rmse-mean'].argmin()\n#     if mean_rmse < rmse:\n#         rmse = mean_rmse\n#         best_params = (max_depth,min_child_weight)\n# print(\"Best params: {}, {}, RMSE: {}\".format(best_params[0], best_params[1], rmse))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params['max_depth'] = 5\n# params['min_child_weight'] = 20\n# gridsearch_params = [\n#     (subsample, colsample)\n#     for subsample in [i/10. for i in range(7,11)]\n#     for colsample in [i/10. for i in range(4,7)]\n# ]\n\n# num_boost_round = 50\n# rmse = float(\"Inf\")\n# best_params = None\n# for subsample, colsample in reversed(gridsearch_params):\n#     params['subsample'] = subsample\n#     params['colsample_bytree'] = colsample\n\n#     cv_results = xgb.cv(\n#         params,\n#         dtrain,\n#         num_boost_round=num_boost_round,\n#         nfold=5,\n#         early_stopping_rounds=10\n#     )\n#     mean_rmse = cv_results['test-rmse-mean'].min()\n#     boost_rounds = cv_results['test-rmse-mean'].argmin()\n#     if mean_rmse < rmse:\n#         rmse = mean_rmse\n#         best_params = (subsample, colsample)\n# print(\"Best params: {}, {}, RMSE: {}\".format(best_params[0], best_params[1], rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params['subsample'] = 1\n# params['colsample_bytree'] = 0.6\n\n# num_boost_round = 18\n# rmse = float(\"Inf\")\n# best_params = None\n# for eta in [.3, .2, .1, .05, .01, .005]:\n#     params['eta'] = eta\n#     cv_results = xgb.cv(\n#         params,\n#         dtrain,\n#         num_boost_round=num_boost_round,\n#         nfold=5,\n#         early_stopping_rounds=10\n#     )\n#     mean_rmse = cv_results['test-rmse-mean'].min()\n#     boost_rounds = cv_results['test-rmse-mean'].argmin()\n#     if mean_rmse < rmse:\n#         rmse = mean_rmse\n#         best_params = eta\n# print(\"Best params: {}, RMSE: {}\".format(best_params, rmse))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"params = {\n    \"max_depth\": 5,\n    \"eval_metric\": \"rmse\",\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 1000,\n    \"n_jobs\": -1,\n    \"num_boost_round\": 50,\n    \"min_child_weight\": 20,\n    \"subsample\": 1,\n    \"colsample_bytree\": 0.6,\n}\n\nclf_xgb2 = XGBClassifier(**params)\nclf_xgb2_steps = full_pipeline.copy()\nclf_xgb2_steps.append((\"XGBClassifier\", clf_xgb2))\n\nclf_xgb2_pipe = Pipeline(steps=clf_xgb2_steps)\nclf_xgb2_pipe.fit(X_train.copy(), y_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            clf_xgb2_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"XGB with Feat1 tunning\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Search CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"# gbm_param_grid = {\n#     'n_estimators': [25, 50, 75, 100, 150, 200],\n#     'max_depth': range(4, 6),\n#     'colsample_bytree': [0.3, 0.4, 0.5, 0.6],\n#     'learning_rate': [0.1, 0.2, 0.3],\n#     'min_child_weight': range(10, 200)\n# }\n# gbm = xgb.XGBClassifier(n_estimators=10)\n# randomized_mse = RandomizedSearchCV(param_distributions=gbm_param_grid, estimator=gbm,\n#                                     scoring='neg_mean_squared_error', n_iter=5, cv=4,\n#                                    verbose=1)\n# randomized_mse.fit(clf_xgb_pipe.named_steps['prep'].transform(X_train), y_train)\n\n# print(\"Best parameters found: \", randomized_mse.best_params_)\n# print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid Search CV"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ngbm_param_grid = {\n    \"n_estimators\": range(99, 100),\n    \"max_depth\": range(4, 5, 6),\n    \"colsample_bytree\": np.arange(0.48, 0.53, 0.1),\n    \"learning_rate\": [0.08, 0.1, 0.12],\n    \"min_child_weight\": range(137, 160),\n}\n\ngbm = xgb.XGBClassifier()\n# grid_mse = GridSearchCV(param_grid=gbm_param_grid, estimator=gbm,\n#                         scoring='neg_mean_squared_error', cv=4, verbose=1)\n# grid_mse.fit(clf_xgb_pipe.named_steps['prep'].transform(X_train), y_train)\n# print(\"Best parameters found: \", grid_mse.best_params_)\n# print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"clf_xgb3 = XGBRegressor(\n    n_estimators=99,\n    objective=\"reg:logistic\",\n    colsample_bytree=0.48,\n    learning_rate=0.1,\n    max_depth=4,\n    min_child_weight=137,\n)\nclf_xgb3_steps = full_pipeline.copy()\nclf_xgb3_steps.append((\"XGBClassifier\", clf_xgb3))\nclf_xgb3_pipe = Pipeline(steps=clf_xgb3_steps)\n\nclf_xgb3_pipe.fit(X_train.copy(), y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            clf_xgb3_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"XGB Grid search with Feat1\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature Engineering: Second Try\nCreate feature interaction and take out outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new = pd.DataFrame()\nX1.sort_values([\"gameId\", \"playId\"])\nX_new[\"score_ratio\"] = (X1.loc[:, \"preSnapHomeScore\"] + 0.01) / (\n    X1.loc[:, \"preSnapVisitorScore\"] + 0.01\n)\nX_new[\"score_diff\"] = X1.loc[:, \"preSnapHomeScore\"] - X1.loc[:, \"preSnapVisitorScore\"]\nX_new[\"yardsToGo_adj\"] = [\n    k if i == j else 50 - k\n    for i, j, k in zip(X0[\"homeTeamAbbr\"], X0[\"possessionTeam\"], X0[\"yardsToGo\"])\n]\n\npenalty_team = X1[\"penaltyJerseyNumbers\"].str.split(\" \", n=1, expand=True)\nX_new[\"penalty_team\"] = penalty_team[0].where(penalty_team[0] == X0[\"homeTeamAbbr\"], -1)\nX_new[\"penalty_team\"] = penalty_team[0].where(\n    penalty_team[0] == X0[\"visitorTeamAbbr\"], 1\n)\nX_new[\"jerseyNumber\"] = penalty_team[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"interactions_num = X1[\n    [\n        \"yardsToGo\",\n        \"down\",\n        \"defendersInTheBox\",\n        \"absoluteYardlineNumber\",\n        \"numberOfPassRushers\",\n        \"epa\",\n    ]\n]\ninteractions_cat = X1[[\"playType\", \"offenseFormation\", \"typeDropback\"]]\nfactor = X1[\"possessionTeam\"]\n\ninteractions_num1 = interactions_num.where(factor == 1, -interactions_num)\ninteractions_cat1 = pd.DataFrame()\nfor i in interactions_cat:\n    interactions_cat1[i + \"Inter\"] = (\n        interactions_cat[i].astype(\"str\") + \"_\" + factor.astype(\"str\")\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"drop_list = [\"DateYear\", \"DateWeek\", \"playId\", \"gameId\", \"week\"]\nX3 = X0.join(X_new)\nX3 = X3.drop(interactions_num.columns.tolist(), axis=1).join(interactions_cat1)\nX3 = X3.join(interactions_num1)\n\nclf_rf4_steps = full_pipeline.copy()\nclf_rf4_steps.append((\"RF2 Classifier\", clf_rf2))\nclf_rf4_pipe = Pipeline(steps=clf_rf4_steps)\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X3, y, test_size=0.3, shuffle=False\n)\nclf_rf4_pipe.fit(X_train.copy(), y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            clf_rf4_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"Random Forest with Feat2 tunning\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"clf_xgb2_pipe.fit(X_train.copy(), y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            clf_xgb2_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"XGB with Feat2 tunning\",\n        )\n    ],\n    ignore_index=True,\n)\ndisplay_all(model_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"clf_lgbm_steps = full_pipeline.copy()\nclf_lgbm = LGBMClassifier(**params_lgb)\nclf_lgbm_steps.append((\"LGBM Classifier\", clf_lgbm))\nclf_lgbm_pipe = Pipeline(steps=clf_lgbm_steps)\n\nclf_lgbm_pipe.fit(X_train.copy(), y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            clf_lgbm_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"LGBM with Feat2\",\n        )\n    ],\n    ignore_index=True,\n)\ndisplay_all(model_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = list(features_type(X_valid.drop(drop_list, axis=1), number=0)) + list(\n    features_type(X_valid, number=1)\n)\n\nfi = rf_feat_importance(\n    clf_lgbm_pipe.named_steps[\"LGBM Classifier\"], X_valid[col_names]\n)\nsns.barplot(fi[\"imp\"][:20], fi[\"cols\"][:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi = rf_feat_importance(clf_xgb2_pipe.named_steps[\"XGBClassifier\"], X_valid[col_names])\nsns.barplot(fi[\"imp\"][:20], fi[\"cols\"][:20])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking newly created attributes by plotting. It is expected to find out if they might be important to the model."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_plotting = X3.copy()\nX_plotting[\"win\"] = y.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [\"playType\", \"offenseFormation\", \"typeDropback\", \"passResult\"]:\n    feature_plot(X_plotting, i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Add cummulative columns\nA new brilliant idea: to add cummulative columns that model will have cummulative information about the game. This should ease to forecast if home team would win or lose. In the previous datasets a row contains just one 'play' information(many plays in one game). All time related attributes are dropped bicause tree based models do not grasp trend."},{"metadata":{"trusted":true},"cell_type":"code","source":"X5 = (\n    X_plotting.sort_values([\"gameId\", \"playId\", \"gameClock\"])\n    .reset_index(drop=True)\n    .copy()\n)\ncol_list4 = [c for c in X5.columns if c[:4] != \"Date\"]\ny = X_plotting.sort_values([\"gameId\", \"playId\", \"gameClock\"])[\"win\"]\nX6 = X5[col_list4].copy()\nnum_cols = list(features_type(X6, number=0))\nfor i in num_cols:\n    X6[i + \"cum\"] = X6.groupby([\"gameId\", \"possessionTeam\"])[i].cumsum(axis=0)\n\ncat_cols = list(features_type(X6, number=1))\nfor i in cat_cols:\n    X6[i + \"cum\"] = X6.groupby([\"gameId\", \"possessionTeam\"])[i].cumcount()\n\nX7 = X6.drop(\n    [\n        \"gameIdcum\",\n        \"playIdcum\",\n        \"possessionTeam\",\n        \"homeTeamAbbr\",\n        \"visitorTeamAbbr\",\n        \"yardlineSide\",\n        \"win\",\n        \"wincum\",\n        \"weekcum\",\n    ],\n    axis=1,\n).join(playTeam)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(\n    X7, y, test_size=0.3, shuffle=False\n)\nclf_rf4_pipe.fit(X_train.copy(), y_train)\nclf_xgb2_pipe.fit(X_train.copy(), y_train)\nclf_lgbm_pipe.fit(X_train.copy(), y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            clf_xgb2_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"XGB with Feat3\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score = model_score.append(\n    [\n        print_score(\n            clf_rf4_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"Random Forest with Feat3\",\n        )\n    ],\n    ignore_index=True,\n)\nmodel_score = model_score.append(\n    [\n        print_score(\n            clf_lgbm_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"LGBM with Feat3\",\n        )\n    ],\n    ignore_index=True,\n)\ndisplay_all(model_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col_names = list(\n    features_type(X_valid.drop([\"gameId\", \"playId\", \"week\"], axis=1), number=0)\n) + list(features_type(X_valid, number=1))\n\nfi_LGBM = rf_feat_importance(\n    clf_lgbm_pipe.named_steps[\"LGBM Classifier\"], X_valid[col_names]\n)\nfi_RF = rf_feat_importance(\n    clf_rf4_pipe.named_steps[\"RF2 Classifier\"], X_valid[col_names]\n)\nfi_XGB = rf_feat_importance(\n    clf_xgb2_pipe.named_steps[\"XGBClassifier\"], X_valid[col_names]\n)\n\nplt.figure(figsize=(20, 16))\nplt.suptitle(\"Feature importance\", fontsize=22)\n\nplt.subplot(311)\ng1 = sns.barplot(fi_LGBM[\"imp\"][:20], fi_LGBM[\"cols\"][:20])\ng1.set_title(\"LGB model\", fontsize=20)\ng1.set_xlabel(\"Importance\")\n\nplt.subplot(312)\ng2 = sns.barplot(fi_RF[\"imp\"][:20], fi_RF[\"cols\"][:20])\ng2.set_title(\"Random Forest model\", fontsize=20)\ng2.set_xlabel(\"Importance\")\n\nplt.subplot(313)\ng3 = sns.barplot(fi_XGB[\"imp\"][:20], fi_XGB[\"cols\"][:20])\ng3.set_title(\"XGB model\", fontsize=20)\ng3.set_xlabel(\"Importance\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold\nfrom sklearn.metrics import make_scorer\nfrom hyperopt import hp, fmin, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\n\nimport time\n\nX_train_trans = clf_rf4_pipe.named_steps[\"prep\"].transform(X_train)\nX_valid_trans = clf_rf4_pipe.named_steps[\"prep\"].transform(X_valid)\n\n\ndef objective(params):\n    time1 = time.time()\n    params = {\n        \"max_depth\": int(params[\"max_depth\"]),\n        \"gamma\": \"{:.3f}\".format(params[\"gamma\"]),\n        \"subsample\": \"{:.2f}\".format(params[\"subsample\"]),\n        \"reg_alpha\": \"{:.3f}\".format(params[\"reg_alpha\"]),\n        \"reg_lambda\": \"{:.3f}\".format(params[\"reg_lambda\"]),\n        \"learning_rate\": \"{:.3f}\".format(params[\"learning_rate\"]),\n        \"num_leaves\": \"{:.3f}\".format(params[\"num_leaves\"]),\n        \"colsample_bytree\": \"{:.3f}\".format(params[\"colsample_bytree\"]),\n        \"min_child_samples\": \"{:.3f}\".format(params[\"min_child_samples\"]),\n        \"feature_fraction\": \"{:.3f}\".format(params[\"feature_fraction\"]),\n        \"bagging_fraction\": \"{:.3f}\".format(params[\"bagging_fraction\"]),\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 3\n    count = 1\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=False)\n    y_oof = np.zeros(X_train.shape[0])\n    score_mean = 0\n    for tr_idx, val_idx in skf.split(X_train_trans, y_train):\n        clf = xgb.XGBClassifier(\n            n_estimators=600, verbose=True, tree_method=\"gpu_hist\", **params,\n        )\n        X_tr, X_vl = X_train_trans[tr_idx, :], X_train_trans[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        clf.fit(X_tr, y_tr)\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        score_mean += score\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 / 60,2)}\")\n    print(f\"Mean ROC_AUC: {score_mean / FOLDS}\")\n    del clf, X_tr, X_vl, y_tr, y_vl, score\n    return {\"loss\": -(score_mean / FOLDS), \"status\": STATUS_OK}\n\n\nspace = {\n    \"max_depth\": hp.choice(\"max_depth\", list(range(3, 7))),\n    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0.01, 0.4),\n    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0.01, 0.4),\n    \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.4),\n    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.4, 0.9),\n    \"gamma\": hp.uniform(\"gamma\", 0.01, 0.7),\n    \"num_leaves\": hp.choice(\"num_leaves\", list(range(20, 250, 10))),\n    \"min_child_samples\": hp.choice(\"min_child_samples\", list(range(100, 250, 10))),\n    \"subsample\": hp.choice(\"subsample\", [0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n    \"feature_fraction\": hp.uniform(\"feature_fraction\", 0.4, 0.8),\n    \"bagging_fraction\": hp.uniform(\"bagging_fraction\", 0.4, 0.9),\n}\n\ntrials = Trials()\nbest = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=250)\n\nbest_params = space_eval(space, best)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_params","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"clf_xgbg = XGBClassifier(**best_params)\nclf_xgbg_steps = full_pipeline.copy()\nclf_xgbg_steps.append((\"XGBClassifier\", clf_xgbg))\n\nclf_xgbg_pipe = Pipeline(steps=clf_xgbg_steps)\n\nclf_xgbg_pipe.fit(X_train.copy(), y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_score = model_score.append(\n    [\n        print_score(\n            clf_xgbg_pipe,\n            X_train,\n            y_train,\n            X_valid,\n            y_valid,\n            model_name=\"XGB hyperopt with Feat3\",\n        )\n    ],\n    ignore_index=True,\n)\ndisplay_all(model_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fi_XGB_hyp = rf_feat_importance(\n    clf_xgbg_pipe.named_steps[\"XGBClassifier\"], X_valid[col_names]\n)\n\nplt.figure()\nsns.barplot(fi_XGB_hyp[\"imp\"][:20], fi_XGB_hyp[\"cols\"][:20])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\nfrom scipy.cluster import hierarchy as hc\n\ncorr = np.round(scipy.stats.spearmanr(X_train_trans).correlation, 4)\ncorr_condensed = hc.distance.squareform(1 - corr)\nz = hc.linkage(corr_condensed, method=\"average\")\nfig = plt.figure(figsize=(16, 30))\ndendrogram = hc.dendrogram(\n    z, labels=X_train.columns, orientation=\"left\", leaf_font_size=16\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://static0.thesportsterimages.com/wordpress/wp-content/uploads/2018/06/NFL11.jpg?q=50&fit=crop&w=740&h=370\" width=\"850px\"/>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}