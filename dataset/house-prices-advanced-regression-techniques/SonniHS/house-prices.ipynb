{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Predicting house prices\n\n![](https://cdn.pixabay.com/photo/2017/04/14/17/17/houses-2230817_960_720.png)\n\n### About the kernel\n\n>**Note!** *My purpose with this kernel is primarily to try different ML algorithms and to learn about hyperparameter tuning, stacking and ensambling.*\n>*I am therefore not including much EDA in this kernel and I am not describing the data cleaning- and feature engineering (FE) in details. I will refer to other kernels instead, as many others have already done an outstanding job doing in these regards!*  \n\n\n### Contents:    \n* **1 Data**\n  * 1.1 Importing data\n  * 1.2 Exploratory Data Analysis (light)\n* **2 Data cleaning**  \n  * 2.1 Outliers  \n  * 2.2 Handling skew and null   \n  * 2.3 Light feature engineering  \n* **3 Data preparation: Encode and scale**   \n* **4 Model fit**  \n  * 4.1 Hyper parameter tuning with GridSearch  \n  * 4.2 Stacking  \n  * 4.3 Emsambling  \n* **5 Predictions**  \n  * 5.1 Comparison of models"},{"metadata":{},"cell_type":"markdown","source":"Credit goes to (they deserve an upvote!):   \nhttps://www.kaggle.com/shaygu/house-prices-begginer-top-7  \nhttps://www.kaggle.com/niteshx2/top-50-beginners-stacking-lgb-xgb   \nhttps://www.kaggle.com/iamprateek/my-submission-to-predict-sale-price/data   \nhttps://www.kaggle.com/jesucristo/1-house-prices-solution-top-1#Blending-Models"},{"metadata":{},"cell_type":"markdown","source":"# 1 Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Importing the libraries used for the first couple of steps\nimport pandas as pd\nimport numpy as np\n\n#AND suppressing warning; they tend to ruin an easy overview\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Importing data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def import_data(data):\n    df = pd.read_csv('../input/house-prices-advanced-regression-techniques/{}.csv'\n                     .format(data))\n    return df\n\ntrain = import_data('train')\ntrain = train.set_index('Id')\ntest = import_data('test')\nID = test[['Id']]\ntest = test.set_index('Id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 EDA (light)"},{"metadata":{},"cell_type":"markdown","source":"*Getting to know the dataset!!!*  \n\nWe want to get an overview of the dataset, so we know which measures to take in order to get a nice and clean dataset to feed our machine learning models with.  \n\nI am starting with answering questions such as the shape (rows, columns) of both the training and the testing dataset, how many null values are in each row etc. The purpose of this dataset is not EDA, so if you are looking for a comprehensive overview, I will recommend moving on to another kernel :-)"},{"metadata":{},"cell_type":"markdown","source":"How many rows and columns are we dealing with?"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"print(\n    'Train dataset has {} rows and {} columns\\nTest dataset has {} rows and {} columns'\n      .format(train.shape[0],\n              train.shape[1],\n              test.shape[0],\n              test.shape[1]\n             )\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train dataset has one more columns than the test dataset - this is the target value, SalePrice!  \n\nThe next step is to combine the two datasets for consistency, when I handle nulls and skew values and when encoding and scaling.  \n\nLet's also have a look at the data; which columns do we have, split into qualitative and quantitative columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining qualitative and quantitative columns\nqual_cols = [col for col in train.columns if train[col].dtype in ['object', 'str']]\nquan_cols = [col for col in train.columns if train[col].dtype not in ['object', 'str']]\n\nprint('Qualitative columns ({} columns):\\n{}\\n'.format(len(qual_cols), train[qual_cols].columns))\nprint('Quanitative columns ({} columns):\\n{}'.format(len(quan_cols), train[quan_cols].columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's get an overview of the first 5 rows of the dataset:**"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2 Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Outliers"},{"metadata":{},"cell_type":"markdown","source":"**Identifying outliers**  \n\nIdentifying most correlated features and using z-score to remove outliers.  \nOne solution is to remove outliers with a z-score > 3.  \nThis does not improve score, so it is not integrated, but the code is still found below.\n\nInstead I use data vizualization, a pairplot, to identify outliers and then filter them out.\n\n\nZ-scores:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\n# defining top correlated features\n\ncorrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.6]\n\nfor kol in top_corr_features:\n    z = np.abs(stats.zscore(train[kol]))\n    train['z'] = z\n    #train = train[train.z < 3.5]\n    train = train.drop('z', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top correlated features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing libraries for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Setting style to 'darkgrid'\nsns.set_style('darkgrid')\n\nplt.figure(figsize=(6,6))\ng = sns.heatmap(\n    train[top_corr_features].corr(), \n    annot = True, cmap = \"Blues\", \n    cbar = False, vmin = .5, \n    vmax = .7, square=True\n    )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using a pairplot to check that all obvious outliers are in fact removed"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train[top_corr_features], diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We set a couple of filters based on the above (see also other kernels for in depth review)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train = train[(train['SalePrice']<450000) & (train['GrLivArea']<4000) & (train.TotalBsmtSF<3000)]\ntrain = train[(train['GrLivArea']<4500)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Handling skew for target"},{"metadata":{},"cell_type":"markdown","source":"Let's first look into our target variable: SalePrice.  \n\nWhen skewed, we apply np.log1p to: *\"Return the natural logarithm of one plus the input array, element-wise.\"*"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import probplot\n\nprint('The sales prices are skewed quite a bit')\nsns.distplot(train.SalePrice, bins=20)\nplt.suptitle('Prices before normalizing')\nplt.show()\n\nprobplot(train.SalePrice, plot=plt)\nplt.show()\n\nprint('We therefore apply log1p to normalize the sales prices for prediction')\n#Normalizing sales price\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#let's also just defined our target as y:\ny = train['SalePrice'].values\n\nsns.distplot(train.SalePrice, bins=20)\nplt.suptitle('Normalized prices')\nplt.show()\n\nprobplot(train.SalePrice, plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am combining the datasets for consistency in the cleaning process.  \nSalePrice is dropped from the combined dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([train.drop(['SalePrice'], axis=1), test], axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Null values"},{"metadata":{},"cell_type":"markdown","source":"**Overview of null values in both test and train; top 10 for each**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def null_view(df):\n    null_view = df[[col for col in df.columns if df[col].isna().sum() > 0]].isna().sum().sort_values(ascending = False)\n    null_view = pd.DataFrame(null_view, columns=['n_nulls'])\n    null_view['pct_null'] = null_view.n_nulls.apply(lambda x: str(round((x/len(df))*100, 1)) +'%')\n    print(null_view.head(10))\nnull_view(test)\nprint('')\nnull_view(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Interpretations of null values\n#Credit: see link in the beginning of the kernel\n\ndf['MSSubClass'] = df['MSSubClass'].apply(str)\ndf['YrSold'] = df['YrSold'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)\n\n#Fill out values with most common value\ncommonNa = [\n    'MSZoning', 'Electrical',\n    'KitchenQual', 'Exterior1st',\n    'Exterior2nd', 'SaleType',\n    'LotFrontage', 'Functional'\n    ]\n\n#Fill with zero value\ntoZero = [\n    'MasVnrArea', 'GarageYrBlt',\n    'BsmtHalfBath', 'BsmtFullBath',\n    'GarageArea', 'GarageCars',\n    'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF'\n    ]\n\n#Fill with No data\ntoNoData = [\n    'PoolQC', 'MiscFeature', \n    'Fence','MasVnrType',\n    'FireplaceQu', 'GarageType', \n    'GarageFinish', 'GarageQual', \n    'GarageCond', 'BsmtQual', \n    'BsmtCond', 'BsmtExposure', \n    'BsmtFinType1', 'BsmtFinType2',\n    'Alley'\n    ]\n\n#Function fill missing values\ndef fillNa_fe(df):\n    df['Functional'] = df['Functional'].fillna('Typ')\n    for i in commonNa:\n        df[i] = df[i].fillna(df[i].mode()[0])\n    for i in toNoData:\n        df[i] = df[i].fillna('None')\n    for i in toZero:\n        df[i] = df[i].fillna(0)\n    \n    df.drop(['Utilities', 'PoolQC', 'MiscFeature'], axis=1, inplace=True)\n    \n    return df\n\n\ndf = fillNa_fe(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Skew for rest of dataset\nFollowing SalePrice, I will \"fix\" skew for all other quantitative columns with skre > 0.7."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Skew is normally >.5, but I have chosen 0.7 in this case\ndef skew_(df):\n    #columns which are skew-candidates\n    colls = [col for col in df.columns if df[col].dtype in ['int64','float']]\n    skews_df = [col for col in df[colls].columns if abs(df[col].skew()) > .7]\n\n    #function to correct skew\n    def skewfix(data, data2):\n        for i in data2:\n            data[i] = np.log1p(data[i])\n            return data\n    return skewfix(df, skews_df)\n\ndf = skew_(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Feature engineering, light  \nInspired by other kernels, I am creating several new variables by combining/altering elements of existing variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"def feat_eng(df):\n    #A little bit of festure engineering\n    df['totalSF'] = df['TotalBsmtSF'] \\\n                   + df['1stFlrSF'] \\\n                   + df['2ndFlrSF'] \\\n                   + df['GrLivArea']\n    \n    df['YrBltAndRemod']= df['YearBuilt'] + df['YearRemodAdd']\n    \n    df['Porch_SF'] = (df['OpenPorchSF'] \\\n                      + df['3SsnPorch'] \\\n                      + df['EnclosedPorch'] \\\n                      + df['ScreenPorch'] \\\n                      + df['WoodDeckSF'])\n    \n    df['Total_Bathrooms'] = (df['FullBath'] \\\n                             + (0.5 * df['HalfBath']) \\\n                             + df['BsmtFullBath'] \\\n                             + (0.5 * df['BsmtHalfBath']))   \n    \n    df['hpool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['h2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['hbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    \n    return df\n\ndf = feat_eng(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3 Data preparation"},{"metadata":{},"cell_type":"markdown","source":"In order to use qualitative variables I am encoding and scaling the variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n#We need to encode variables with categorical data:\nencoder = LabelEncoder()\nsc = StandardScaler()\n\ndef encode(df):\n    cat_df = [col for col in df.columns if df[col].dtype not in ['int','float']]\n    for col in cat_df:\n        df[col] = encoder.fit_transform(df[col])\n    df_ = sc.fit_transform(df)\n    df = pd.DataFrame(data=df_, columns = df.columns)\n    return df\n\n\n'''\nTesting dummies in this commit; scores better than encoding!\n\n'''\n\ndf = pd.get_dummies(df)\n\n#df = encode(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4 Model fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df.iloc[:train.shape[0],:]\ntest  = df.iloc[train.shape[0]:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Splitting dataset into test and train\nfrom sklearn.model_selection import train_test_split\n\nX = train.copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Hyper parameter tuning with GridSearch\n\n"},{"metadata":{},"cell_type":"markdown","source":"**Five models with gradient boost first defined:**  \n>Gradient Boost  \n>Light GBM  \n>XGBoost  \n>K-nearest neighbors  \n>Random forest"},{"metadata":{},"cell_type":"markdown","source":"**I have been going through several possibilities in gridsearch, below is only the latest \"best parameters\"**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import datetime\n\n#Import library for Gridsearch\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing regressor for each model, as below\nfrom lightgbm import LGBMRegressor\n\nprint('####################################################\\n{}\\tLightBGM'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparameters = {\n    'objective':['regression'],\n    'num_leaves':[5],\n    'learning_rate':[0.05], #0.005, \n    'n_estimators':[720], #, 4000, 5000 \n    'max_bin':[55], #, 300, 500\n    'max_depth':[2, 3],\n    'bagging_fraction':[.5, .8],\n    'bagging_freq':[5],\n    'bagging_seed':[9], #7\n    'feature_fraction':[0.2319],#0.2, \n    'feature_fraction_seed':[9] #5\n    }\n\nlight = LGBMRegressor()\nclf = GridSearchCV(light, parameters, verbose=0, iid=False)\nclf.fit(X_train, y_train)\nlightgbm = LGBMRegressor(**clf.best_params_)\n\nprint('\\nRegressor: \\n', lightgbm, '\\n')\nprint('{}\\tDone!\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nprint('####################################################\\n{}\\tXGBoost'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparams = {\n    'colsample_bytree': [0.4],\n    'gamma': [0.0],\n    'learning_rate': [0.01], \n    'max_depth': [3],\n    'min_child_weight': [2],\n    'n_estimators': [3460],\n    'seed': [36],\n    'subsample': [0.2],\n    'objective':['reg:squarederror'],\n    'reg_alpha':[0.00006],\n    'cale_pos_weight':[1],\n    'verbosity':[0]\n    }\n\ngbm = XGBRegressor()\nclf = GridSearchCV(gbm, params, verbose=0, iid=False)\nclf.fit(X_train,y_train)\nXGBoost = XGBRegressor(**clf.best_params_)\n\nprint('\\nRegressor: \\n', XGBoost, '\\n')\nprint('{}\\tDone!\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nprint('####################################################\\n{}\\tGradient Boost'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparameters = {\n    'n_estimators':[8000,],\n    'learning_rate':[0.01], \n    'max_depth':[2], \n    'max_features':['sqrt'], \n    'min_samples_leaf':[10], \n    'min_samples_split':[5], \n    'loss':['huber'], \n    'random_state' :[42],\n    }\n\ngbr = GradientBoostingRegressor()\nclf = GridSearchCV(gbr, parameters, verbose=0, iid=False)\nclf.fit(X_train, y_train)\ngbr = GradientBoostingRegressor(**clf.best_params_)\n\nprint('\\nRegressor: \\n', gbr, '\\n')\nprint('{}\\tDone!\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nprint('####################################################\\n{}\\tK-nearest neighbor'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparameters = {\n    'algorithm':['auto'],\n    'leaf_size':[2],\n    'weights':['uniform', 'distance'],\n    'metric':['euclidean', 'manhattan'],\n    'n_neighbors':[10],\n    }\n\nknn = KNeighborsRegressor()\nclf = GridSearchCV(knn, parameters, verbose=0, iid=False)\nclf.fit(X_train, y_train)\nknn = KNeighborsRegressor(**clf.best_params_)\n\nprint('\\nRegressor: \\n', knn, '\\n')\nprint('{}\\tDone!\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nprint('####################################################\\n{}\\tRandom forest'\n      .format(datetime.datetime.now().strftime('%H:%M')))\n\nparameters = {\n    'bootstrap': [True],\n    #'max_depth': [15],\n    'max_features': ['auto'],\n    'min_samples_leaf': [2],\n    'min_samples_split': [4],\n    'n_estimators': [1500],\n    'n_jobs':[-1],\n    'oob_score':[True]\n    }\n\nr_forest = RandomForestRegressor()\nclf = GridSearchCV(r_forest, parameters, verbose=0, iid=False)\nclf.fit(X_train, y_train)\nknn = RandomForestRegressor(**clf.best_params_)\n\nprint('\\nRegressor: \\n', knn, '\\n')\nprint('{}\\tDone!\\n####################################################'\n      .format(datetime.datetime.now().strftime('%H:%M')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Pipelines for following models defined:**  \n>Ridge  \n>Lasso  \n>Elasticnet  \n>SVR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n#Imporing library for makign pipeline\nfrom sklearn.pipeline import make_pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\n\nalphas_ridge = [\n    14.5, 14.6, 14.7, 14.8, 14.9, 15, \n    15.1, 15.2, 15.3, 15.4, 15.5, 15.6\n    ]\n\nridge = make_pipeline(\n    RobustScaler(), \n    RidgeCV(\n        alphas=alphas_ridge, \n        cv=kfolds\n        )\n    )\n\nprint('############################################################\\nRidge: \\n', \n      ridge, \n      '\\n############################################################\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LassoCV\n\nalphas_lasso = [\n    5e-05, 0.0001, 0.0002, 0.0003, \n    0.0004, 0.0005, 0.0006, 0.0007\n    ]\n\nlasso = make_pipeline(\n    RobustScaler(), \n    LassoCV(\n        alphas=alphas_lasso, \n        max_iter=1e7, \n        random_state=42, \n        cv=kfolds\n        )\n    )\n\nprint('############################################################\\nLasso: \\n', \n      lasso,\n      '\\n############################################################\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV\n\nalphas_ela = [\n    0.0001, 0.0002, 0.0003, \n    0.0004, 0.0005, 0.0006, 0.0007\n    ]\n\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nelasticnet = make_pipeline(\n    RobustScaler(), \n    ElasticNetCV(\n        max_iter=1e7, \n        alphas=alphas_ela, \n        cv=kfolds, \n        l1_ratio=e_l1ratio\n        )\n    ) \n\nprint('############################################################\\nElastic: \\n',\n      elasticnet, \n      '\\n############################################################\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.svm import SVR\n\nparameters = {\n   # 'kernel': ['sigmoid'],\n    'C':     [120],\n    'gamma': [0.0003]\n    }\n\nsvr_ = SVR()\nclf = GridSearchCV(svr_, parameters, verbose=0, iid=False)\nclf.fit(X_train, y_train)\n\nsvr = make_pipeline(\n    RobustScaler(), \n    SVR(**clf.best_params_)\n    )\n\nprint('############################################################\\nSVR: \\n', \n      svr, \n      '\\n############################################################\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.kernel_ridge import KernelRidge\nkrr = KernelRidge()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stacking the models:**  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing library for stacking\nfrom mlxtend.regressor import StackingCVRegressor\n\n#SVR, KNN and Random Forest removed!\n\nprint('Stacking: \\t stack_gen')\nstack_gen = StackingCVRegressor(\n    regressors=(ridge, lasso, elasticnet, lightgbm, XGBoost, gbr),\n    meta_regressor=XGBoost, \n    use_features_in_secondary=True\n    )\nprint('Done!\\n')\n\nprint('Stacking: \\t stack_gen2')\nstack_gen2 = StackingCVRegressor(\n    regressors=(ridge, lasso, elasticnet, lightgbm, XGBoost, gbr),\n    meta_regressor=lightgbm, \n    use_features_in_secondary=True\n    )\nprint('Done!\\n')\n\nprint('Stacking: \\t stack_gen3')\nstack_gen3 = StackingCVRegressor(\n    regressors=(ridge, lasso, elasticnet, lightgbm, XGBoost, gbr),\n    meta_regressor=ridge, \n    use_features_in_secondary=True\n    )\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Fitting the models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"models = {\n    'svr':svr,\n    'KernelRidge':krr,\n    'Ridge':ridge,\n    'Lightgbm':lightgbm,\n    'XGBoost':XGBoost,\n    'GradientBoost':gbr,\n    'Lasso':lasso,\n    'Elasticnet':elasticnet,\n    'Random forest':r_forest,\n    'KNN':knn\n    }\n\n#Importing metrics for calculation of error\nfrom sklearn import metrics\n\ndef calc(model): \n    return int(np.sqrt(metrics.mean_squared_error(\n        np.expm1(y_test), np.expm1(model.predict(X_test)))))\n\nliste = []\nfor one, two in models.items():\n    print('Fitting:\\t{}'.format(one))\n    two.fit(X_train, y_train)\n    print('Done! Error:\\t{}\\n'.format(calc(two)))\n    df = pd.DataFrame(\n        data=[calc(two)], \n        index=[one], \n        columns=['error']\n        )\n    liste.append(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resList = pd.concat(liste, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Stacking"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('stack_gen: \\t fitting...')\nstack_gen_model = stack_gen.fit(np.array(X_test), np.array(y_test))\nsg_error = int(np.sqrt(metrics.mean_squared_error(np.expm1(y_test), np.expm1(stack_gen_model.predict(np.array(X_test))))))\nprint('stack_gen: \\t done!')\nprint('error:\\t\\t', sg_error, '\\n')\n\nprint('stack_gen2: \\t fitting...')\nstack_gen_model2 = stack_gen2.fit(np.array(X_test), np.array(y_test))\nsg_error2 = int(np.sqrt(metrics.mean_squared_error(np.expm1(y_test), np.expm1(stack_gen_model2.predict(np.array(X_test))))))\nprint('stack_gen2: \\t done!')\nprint('error:\\t\\t', sg_error2, '\\n')\n\nprint('stack_gen3: \\t fitting...')\nstack_gen_model3 = stack_gen3.fit(np.array(X_test), np.array(y_test))\nsg_error3 = int(np.sqrt(metrics.mean_squared_error(np.expm1(y_test), np.expm1(stack_gen_model3.predict(np.array(X_test))))))\nprint('stack_gen3: \\t done!')\nprint('error:\\t\\t', sg_error3)\n\nsg = pd.DataFrame(index=['stack_gen'], data=sg_error, columns=['error'])\nsg2 = pd.DataFrame(index=['stack_gen2'], data=sg_error2, columns=['error'])\nsg3 = pd.DataFrame(index=['stack_gen3'], data=sg_error3, columns=['error'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resList = resList.append(sg)\nresList = resList.append(sg2)\nresList = resList.append(sg3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3 Ensambling  \n**Ensambling for a bleded prediction model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def blend(data):\n    return (\n        #(0.01 * svr.predict(data)) +\n        #(0.03 * krr.predict(data)) +\n        (0.08 * ridge.predict(data)) +\n        (0.11 * lightgbm.predict(data)) +\n        (0.13 * XGBoost.predict(data)) +\n        (0.10 * gbr.predict(data)) +\n        (0.15 * stack_gen_model.predict(np.array(data))) +\n        (0.16 * stack_gen_model2.predict(np.array(data))) +\n        (0.16 * stack_gen_model3.predict(np.array(data))) +\n        (0.06 * lasso.predict(data)) +\n        #(0.03 * knn.predict(data)) + \n        #(0.02 * r_forest.predict(data)) +\n        (0.05 * elasticnet.predict(data)) \n        )\n\nblended_predictions = blend(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5 Predictions  \n## 5.1 Comparing models  \n**Comparing the first 20 results for each model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def createFrame(data):\n    '''\n    Compare results\n    '''\n    index = index=import_data('test').reset_index()['Id']\n    df_ = {\n        'lightgbm':lightgbm.predict(data),\n        'XGBoost':XGBoost.predict(data),\n        'GBR':gbr.predict(data),\n        'svr':svr.predict(data),\n        'ridge':ridge.predict(data),\n        'KernelRidge':krr.predict(data),\n        'elastic':elasticnet.predict(data),\n        'lasso':lasso.predict(data),\n        'KNN':knn.predict(data),\n        'Random forest':r_forest.predict(data),\n        'stack_gen':stack_gen_model.predict(np.array(data)),\n        'stack_gen2':stack_gen_model2.predict(np.array(data)),\n        'stack_gen3':stack_gen_model3.predict(np.array(data)),\n        'blended':blend(data)\n        }\n    df = pd.DataFrame(df_, index=index)\n    df = df.applymap(lambda x: np.expm1(x))\n    return df\n\npreds = createFrame(test)\n\nfor model in preds.columns:\n    df = preds[[model]]\n    df.rename(columns={model:'SalePrice'}, inplace=True)\n    df.to_csv('{}.csv'.format(model))\n\npreds.round(1).head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = blend(X_test)\n\nbl = pd.DataFrame(\n    index=['blend'], \n    data=int(np.sqrt(metrics.mean_squared_error(np.expm1(y_test), np.expm1(x)))),\n    columns=['error']\n    )\nresList = resList.append(bl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All models with error"},{"metadata":{"trusted":true},"cell_type":"code","source":"resList2 = resList.reset_index()\n#resList2.sort_values('error', inplace=True)\nplt.figure(figsize=(13,6))\nax1 = sns.barplot(\n    x=resList2['index'], \n    y=resList2.error, \n    color='#00616f'\n    )\n\nfor i, rows in resList2.iterrows():\n    ax1.annotate(\n        round(rows['error'],1), \n        xy=(i, rows['error']), \n        xycoords='data', \n        ha='center', \n        color='black'\n        )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = pd.DataFrame(data=np.expm1(blend(test)), index=ID.Id, columns=['SalePrice'])\n\nq1 = res['SalePrice'].quantile(0.005)\nq2 = res['SalePrice'].quantile(0.995)\n\nres['SalePrice'] = res['SalePrice'].apply(lambda x: x if x > q1 else x*0.78)\nres['SalePrice'] = res['SalePrice'].apply(lambda x: x if x < q2 else x*1.10)\n\nres.to_csv('1_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res.reset_index().head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How are the predictions correlated?"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.heatmap(\n    preds.corr().round(4), \n    annot=True, \n    cmap=\"Blues\", \n    cbar=False, \n    square=True\n   )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}