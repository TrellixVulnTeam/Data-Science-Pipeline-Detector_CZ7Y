{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center>House price - EDA & Prediction üè†</center></h1>\n\n<h2>Table of Content</h2>\n\n* [Introduction](#0)\n    - [Librairies import and load the dataset](#0.1) \n* [Understand the Data](#1)\n* [Exploratory Data Analysis (EDA) üìä](#2)\n    - [Checking missing Data & Split categorical / Numerical features](#2.1)\n* [Data Preprocessing](#3)\n    - [Deal with redundant and useless features](#3.1)\n    - [Dealing with Outliers](#3.2)\n    - [Treating Missing Values](#3.3)\n    - [Feature Engineering](#3.3)\n* [Modeling and training](#4)\n* [Conclusion](#5)\n\n\n\n\n<a id=\"0\"></a> <br>\n\n# 0 Introduction üìí\n\nIn this notebook I will . I took inspiration from multiple notebooks in this competition and I wanted to apply notions and tools by myself. In this notebook there is nothing new, the purpose of this notebook is for me to practice data science tools and get use to it. \n\nIn this notebook I will perform the following tasks :\n\n1. Understand the data in this competition\n2. Explore the data with data visualisation tools\n3. Preprocess the data (scale, encode, deal with outliers/missing)\n4. Train a model and submit\n\n<a id=\"0.1\"></a> <br>\n\n## 0.1 Import Libraries and Load the Dataset üìö","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly\nimport os\nimport seaborn as sns\nfrom numpy import mean,std\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.utils import class_weight\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn import ensemble\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom catboost import CatBoostRegressor\n\n\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom xgboost import cv\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:18.337699Z","iopub.execute_input":"2022-02-21T16:35:18.338279Z","iopub.status.idle":"2022-02-21T16:35:21.651704Z","shell.execute_reply.started":"2022-02-21T16:35:18.338169Z","shell.execute_reply":"2022-02-21T16:35:21.650588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\",index_col = 'Id')\ntest_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\",index_col = 'Id')","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:21.653347Z","iopub.execute_input":"2022-02-21T16:35:21.653727Z","iopub.status.idle":"2022-02-21T16:35:21.752434Z","shell.execute_reply.started":"2022-02-21T16:35:21.653689Z","shell.execute_reply":"2022-02-21T16:35:21.751492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = pd.concat([train_data.drop(\"SalePrice\", axis=1),test_data], axis=0)\ny = train_data[['SalePrice']]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:21.754359Z","iopub.execute_input":"2022-02-21T16:35:21.754781Z","iopub.status.idle":"2022-02-21T16:35:21.777453Z","shell.execute_reply.started":"2022-02-21T16:35:21.754732Z","shell.execute_reply":"2022-02-21T16:35:21.776416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n\n# 1 Understand the data\n\nIn this section I will explore a bit the data and try to understand the different features in this dataset. The idea is to treat numerical and categorical independently since the tools to analyse each type of data are not the same.","metadata":{}},{"cell_type":"code","source":"numeric_ = X.select_dtypes(exclude=['object']).drop(['MSSubClass'], axis=1).copy()\nnumeric_.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:21.780763Z","iopub.execute_input":"2022-02-21T16:35:21.781064Z","iopub.status.idle":"2022-02-21T16:35:21.798995Z","shell.execute_reply.started":"2022-02-21T16:35:21.781035Z","shell.execute_reply":"2022-02-21T16:35:21.797916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"disc_num_var = ['OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n                'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']\n\ncont_num_var = []\nfor i in numeric_.columns:\n    if i not in disc_num_var:\n        cont_num_var.append(i)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:21.801989Z","iopub.execute_input":"2022-02-21T16:35:21.802306Z","iopub.status.idle":"2022-02-21T16:35:21.807499Z","shell.execute_reply.started":"2022-02-21T16:35:21.802255Z","shell.execute_reply":"2022-02-21T16:35:21.80641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_train = X.select_dtypes(include=['object']).copy()\ncat_train['MSSubClass'] = X['MSSubClass']   #MSSubClass is nominal\ncat_train.columns","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:21.808931Z","iopub.execute_input":"2022-02-21T16:35:21.809344Z","iopub.status.idle":"2022-02-21T16:35:21.827893Z","shell.execute_reply.started":"2022-02-21T16:35:21.809303Z","shell.execute_reply":"2022-02-21T16:35:21.826874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2\"></a> <br>\n# 2 Exploratory Data Analysis (EDA) üìä\n\nIn this part I will perform transormation on the data in order to deal with missing data and see if there are any obvious correlation between variables and the quality of the wine.\n\n<a id=\"2.1\"></a> <br>\n## 2.1 Checking missing Data & Split categorical / Numerical features\n\nI will split categorical and numerical features in order to preprocessed them (Normalized numerical features and label encoding for the categorical features)","metadata":{}},{"cell_type":"code","source":"cols = train_data.columns\nnum_cols = train_data._get_numeric_data().columns\nnum_cols = [e for e in num_cols if e not in ('OverallQual','OverallCond')]\ncat_cols = list(set(cols) - set(num_cols))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:21.829593Z","iopub.execute_input":"2022-02-21T16:35:21.829928Z","iopub.status.idle":"2022-02-21T16:35:21.835232Z","shell.execute_reply.started":"2022-02-21T16:35:21.829896Z","shell.execute_reply":"2022-02-21T16:35:21.834338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(14,15))\nfor index,col in enumerate(cont_num_var):\n    plt.subplot(6,4,index+1)\n    sns.boxplot(y=col, data=numeric_.dropna())\nfig.tight_layout(pad=1.0)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:21.837323Z","iopub.execute_input":"2022-02-21T16:35:21.837609Z","iopub.status.idle":"2022-02-21T16:35:24.518201Z","shell.execute_reply.started":"2022-02-21T16:35:21.837582Z","shell.execute_reply":"2022-02-21T16:35:24.517464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,15))\nfor index,col in enumerate(disc_num_var):\n    plt.subplot(5,3,index+1)\n    sns.countplot(x=col, data=numeric_.dropna())\nfig.tight_layout(pad=1.0)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:24.519502Z","iopub.execute_input":"2022-02-21T16:35:24.519753Z","iopub.status.idle":"2022-02-21T16:35:26.539649Z","shell.execute_reply.started":"2022-02-21T16:35:24.519729Z","shell.execute_reply":"2022-02-21T16:35:26.538884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(18,20))\nfor index in range(len(cat_train.columns)):\n    plt.subplot(9,5,index+1)\n    sns.countplot(x=cat_train.iloc[:,index], data=cat_train.dropna())\n    plt.xticks(rotation=90)\nfig.tight_layout(pad=1.0)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:26.540759Z","iopub.execute_input":"2022-02-21T16:35:26.54115Z","iopub.status.idle":"2022-02-21T16:35:33.354031Z","shell.execute_reply.started":"2022-02-21T16:35:26.541112Z","shell.execute_reply":"2022-02-21T16:35:33.352974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_train = train_data.select_dtypes(exclude=['object'])\ncorrelation = numeric_train.corr()\ncorrelation[['SalePrice']].sort_values(['SalePrice'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:33.355453Z","iopub.execute_input":"2022-02-21T16:35:33.355828Z","iopub.status.idle":"2022-02-21T16:35:33.382044Z","shell.execute_reply.started":"2022-02-21T16:35:33.355787Z","shell.execute_reply":"2022-02-21T16:35:33.381052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(20,20))\nfor index in range(len(numeric_train.columns)):\n    plt.subplot(10,5,index+1)\n    sns.scatterplot(x=numeric_train.iloc[:,index], y='SalePrice', data=numeric_train.dropna())\nfig.tight_layout(pad=1.0)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:33.383008Z","iopub.execute_input":"2022-02-21T16:35:33.383254Z","iopub.status.idle":"2022-02-21T16:35:39.078986Z","shell.execute_reply.started":"2022-02-21T16:35:33.38323Z","shell.execute_reply":"2022-02-21T16:35:39.077799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlation Matrix**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,12))\ncorrelation = numeric_.corr()\nsns.heatmap(correlation, mask = correlation <0.8, linewidth=0.5, cmap='Reds')","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:39.080391Z","iopub.execute_input":"2022-02-21T16:35:39.080671Z","iopub.status.idle":"2022-02-21T16:35:40.142017Z","shell.execute_reply.started":"2022-02-21T16:35:39.080642Z","shell.execute_reply":"2022-02-21T16:35:40.140846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This data visualisation give us tons of informations for both numerical and categorical values\n\n\n\nAfter this quick Data visualisation at the first sight some features doesn't impact much the sale price, thus to have a quick model I will remove the following categorical features : *['Street', 'RoofMatl', 'PoolQC', 'Alley', 'MiscFeature', 'Condition2', 'BsmtFinType2', 'Utilities', 'Fence']* and the following numerical features  *['MonthSold', 'YrSold', 'PoolArea', '3SsnPorch', 'MiscVal', 'EnclosedPorch', 'ScreenPorch',\n                                             'LowQualFinSF', 'BsmtFullBath','BsmtHalfBath','HalfBath','WoodDeckSF']*\n                                             \nMoreover, some \"numerical\" features at first appeared to be categorical in fact \n* Fireplaces\n* GarageCars\n* MSSubClass\n* LotRmsAbvGrd\n\nI need to treat them as categorical features. There are also **OverallCond** and **OverallQual** that need to be treated as categorical. However as they are already encoded as an ordinal feature we don't need to perform further modification on these features.\n\n<a id=\"3\"></a> <br>\n# 3. Data Preprocessing\n\nIn this section I will perform the following tasks : \n\n* Removing Redundant Features\n* Dealing with Outliers\n* Filling in missing values\n\n\nWe can see with the Confusion matrix above that some features are highly correlated with one another thus it does not add any information and it could even lower the \"power\" of the prediction model\n\nThere are also some features with too much missing values ( more than 1350 over 1450) so we can just drop these columns. We can also drop some useless features such as the month and the year the house was sold.\n<a id=\"3.1\"></a> <br>\n## 3.1 Deal with redundant and useless features","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(25,8))\nplt.title('Number of missing rows')\nmissing_count = pd.DataFrame(X.isnull().sum(), columns=['sum']).sort_values(by=['sum'],ascending=False).head(20).reset_index()\nmissing_count.columns = ['features','sum']\nsns.barplot(x='features',y='sum', data = missing_count)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:40.143363Z","iopub.execute_input":"2022-02-21T16:35:40.143646Z","iopub.status.idle":"2022-02-21T16:35:40.491705Z","shell.execute_reply.started":"2022-02-21T16:35:40.14362Z","shell.execute_reply":"2022-02-21T16:35:40.490557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation = numeric_.corr()\n\n#Redundant values\nX.drop(['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageCars'], axis=1, inplace=True) \n\n#Too much missing values\nX.drop(['PoolQC','MiscFeature','Alley'], axis=1, inplace=True)\n\n#Useless features\nX.drop(['MoSold','YrSold'], axis=1, inplace=True)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:40.493331Z","iopub.execute_input":"2022-02-21T16:35:40.493737Z","iopub.status.idle":"2022-02-21T16:35:40.518955Z","shell.execute_reply.started":"2022-02-21T16:35:40.493693Z","shell.execute_reply":"2022-02-21T16:35:40.517939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next cells remove the features with mostly 1 value in it.\nIf a column has more than 96% of an unique value we can say that this feature is useless since it doesn't add any information.","metadata":{}},{"cell_type":"code","source":"cat_col = X.select_dtypes(include=['object']).columns\noverfit_cat = []\nfor i in cat_col:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(X) * 100 > 96:\n        overfit_cat.append(i)\n\noverfit_cat = list(overfit_cat)\nX = X.drop(overfit_cat, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:40.520167Z","iopub.execute_input":"2022-02-21T16:35:40.520457Z","iopub.status.idle":"2022-02-21T16:35:40.567373Z","shell.execute_reply.started":"2022-02-21T16:35:40.520429Z","shell.execute_reply":"2022-02-21T16:35:40.566335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_col = X.select_dtypes(exclude=['object']).drop(['MSSubClass'], axis=1).columns\noverfit_num = []\nfor i in num_col:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(X) * 100 > 96:\n        overfit_num.append(i)\n\noverfit_num = list(overfit_num)\nX = X.drop(overfit_num, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:40.568587Z","iopub.execute_input":"2022-02-21T16:35:40.568858Z","iopub.status.idle":"2022-02-21T16:35:40.596384Z","shell.execute_reply.started":"2022-02-21T16:35:40.568831Z","shell.execute_reply":"2022-02-21T16:35:40.595396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2\"></a> <br>\n## 3.2 Dealing with Outliers\n\nRemoving outliers will prevent our models performance from being affected by extreme values.\nFrom our boxplot earlier, we have highlighted the following features with extreme outliers:\n\n* LotFrontage\n* LotArea\n* BsmtFinSF1\n* TotalBsmtSF\n* GrLivArea\n\nWe will remove the outliers based on certain threshold value.","metadata":{}},{"cell_type":"code","source":"out_col = ['LotFrontage','LotArea','BsmtFinSF1','TotalBsmtSF','GrLivArea']\nfig = plt.figure(figsize=(20,5))\nfor index,col in enumerate(out_col):\n    plt.subplot(1,5,index+1)\n    sns.boxplot(y=col, data=X)\nfig.tight_layout(pad=1.5)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:40.597822Z","iopub.execute_input":"2022-02-21T16:35:40.599514Z","iopub.status.idle":"2022-02-21T16:35:41.194172Z","shell.execute_reply.started":"2022-02-21T16:35:40.599463Z","shell.execute_reply":"2022-02-21T16:35:41.19299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.drop(train_data[train_data['LotFrontage'] > 200].index)\ntrain_data = train_data.drop(train_data[train_data['LotArea'] > 100000].index)\ntrain_data = train_data.drop(train_data[train_data['BsmtFinSF1'] > 4000].index)\ntrain_data = train_data.drop(train_data[train_data['TotalBsmtSF'] > 5000].index)\ntrain_data = train_data.drop(train_data[train_data['GrLivArea'] > 4000].index)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.195759Z","iopub.execute_input":"2022-02-21T16:35:41.196204Z","iopub.status.idle":"2022-02-21T16:35:41.214667Z","shell.execute_reply.started":"2022-02-21T16:35:41.196156Z","shell.execute_reply":"2022-02-21T16:35:41.213444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.3\"></a> <br>\n## 3.3 Treating Missing Values\n\nHow to handle missing values ? It will depends on the types of variable we have.\nFor numerical features we have multiple choices : we can take the mean, median or any other numerical value. For categorical value it is possible to fill with a \"None\" category or with the most_frequent category it's up to us.\n\nWe will also map and encode our categorical features. To do so, if we have ordinal features (categorical feature where there is an order between the categories such as : Beginner, Intermediate, Expert for instance we can just map 0 = Beginner, 1 = Intermediate, 2 = Expert. This way the model will treat 2 as bigger than 0 which is correct in this case.\n\nThe second type of categorical data are the characteristics, such as colour for instance. We cannot treat colours as ordered categories. In this case I will use the one hot encoder.\n","metadata":{}},{"cell_type":"code","source":"cat = ['GarageType','GarageFinish','BsmtFinType2','BsmtExposure','BsmtFinType1', \n       'GarageCond','GarageQual','BsmtCond','BsmtQual','FireplaceQu','Fence',\"KitchenQual\",\n       \"HeatingQC\",'ExterQual','ExterCond']\n\nX[cat] = X[cat].fillna(\"NA\")","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.216352Z","iopub.execute_input":"2022-02-21T16:35:41.216792Z","iopub.status.idle":"2022-02-21T16:35:41.231379Z","shell.execute_reply.started":"2022-02-21T16:35:41.21673Z","shell.execute_reply":"2022-02-21T16:35:41.230152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#categorical\ncols = [\"MasVnrType\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"Functional\"]\nX[cols] = X.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.mode()[0]))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.232829Z","iopub.execute_input":"2022-02-21T16:35:41.233141Z","iopub.status.idle":"2022-02-21T16:35:41.434998Z","shell.execute_reply.started":"2022-02-21T16:35:41.23311Z","shell.execute_reply":"2022-02-21T16:35:41.433919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for correlated relationship\nX['LotFrontage'] = X.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\nX['GarageArea'] = X.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean()))\nX['MSZoning'] = X.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n#numerical\ncont = [\"BsmtHalfBath\", \"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"MasVnrArea\"]\nX[cont] = X[cont] = X[cont].fillna(X[cont].mean())","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.436254Z","iopub.execute_input":"2022-02-21T16:35:41.436577Z","iopub.status.idle":"2022-02-21T16:35:41.478229Z","shell.execute_reply.started":"2022-02-21T16:35:41.436547Z","shell.execute_reply":"2022-02-21T16:35:41.477439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X['MSSubClass'] = X['MSSubClass'].apply(str)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.481773Z","iopub.execute_input":"2022-02-21T16:35:41.482521Z","iopub.status.idle":"2022-02-21T16:35:41.48918Z","shell.execute_reply.started":"2022-02-21T16:35:41.482479Z","shell.execute_reply":"2022-02-21T16:35:41.488005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\nfintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nexpose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\nfence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}\n\n\nord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu']\nfor col in ord_col:\n    X[col] = X[col].map(ordinal_map)\n    \nfin_col = ['BsmtFinType1','BsmtFinType2']\nfor col in fin_col:\n    X[col] = X[col].map(fintype_map)\n\nX['BsmtExposure'] = X['BsmtExposure'].map(expose_map)\nX['Fence'] = X['Fence'].map(fence_map)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.490669Z","iopub.execute_input":"2022-02-21T16:35:41.490955Z","iopub.status.idle":"2022-02-21T16:35:41.527717Z","shell.execute_reply.started":"2022-02-21T16:35:41.490927Z","shell.execute_reply":"2022-02-21T16:35:41.526686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.4 Feature Engineering\n\nIn this section I will perform a quick feature engineering process :\nThe idea is to add more relevant features such as the Total lot area, or surface of the house.","metadata":{}},{"cell_type":"code","source":"X['TotalLot'] = X['LotFrontage'] + X['LotArea']\nX['TotalBsmtFin'] = X['BsmtFinSF1'] + X['BsmtFinSF2']\nX['TotalSF'] = X['TotalBsmtSF'] + X['2ndFlrSF']\nX['TotalBath'] = X['FullBath'] + X['HalfBath']\nX['TotalPorch'] = X['OpenPorchSF'] + X['EnclosedPorch'] + X['ScreenPorch']","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.528943Z","iopub.execute_input":"2022-02-21T16:35:41.52922Z","iopub.status.idle":"2022-02-21T16:35:41.541041Z","shell.execute_reply.started":"2022-02-21T16:35:41.529192Z","shell.execute_reply":"2022-02-21T16:35:41.539961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Binary Columns**\n\nI will also include a more simple feature which is a binary column for some features. The idea is to indicate if a feature is present or not in the dataset.","metadata":{}},{"cell_type":"code","source":"colum = ['MasVnrArea','TotalBsmtFin','TotalBsmtSF','2ndFlrSF','WoodDeckSF','TotalPorch']\n\nfor col in colum:\n    col_name = col+'_bin'\n    X[col_name] = X[col].apply(lambda x: 1 if x > 0 else 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.542275Z","iopub.execute_input":"2022-02-21T16:35:41.54284Z","iopub.status.idle":"2022-02-21T16:35:41.559758Z","shell.execute_reply.started":"2022-02-21T16:35:41.542803Z","shell.execute_reply":"2022-02-21T16:35:41.558913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Converting Categorical to Numerical**\n\nI convert the categorical columns into one hot encoder as mentionned earlier, by using the get_dummies method. Because machine learning models learns from numerical data so it's better.\n","metadata":{}},{"cell_type":"code","source":"X = pd.get_dummies(X)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.562335Z","iopub.execute_input":"2022-02-21T16:35:41.562778Z","iopub.status.idle":"2022-02-21T16:35:41.603319Z","shell.execute_reply.started":"2022-02-21T16:35:41.562736Z","shell.execute_reply":"2022-02-21T16:35:41.602515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y[\"SalePrice\"] = np.log(y['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.604398Z","iopub.execute_input":"2022-02-21T16:35:41.604958Z","iopub.status.idle":"2022-02-21T16:35:41.610891Z","shell.execute_reply.started":"2022-02-21T16:35:41.604918Z","shell.execute_reply":"2022-02-21T16:35:41.610213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a> <br>\n# 4 Modeling and training\n\nIn this section I will prepare the data for training and perform the following tasks :\n\n* Split the data\n* Scale the data using robust scaler (that works fine with outliers)\n* Create and train 3 models\n* I will use cross validation and hyper parameters tuning on each model\n* And I will use ensembling method to combine the models","metadata":{}},{"cell_type":"code","source":"x = X.loc[train_data.index]\ny = y.loc[train_data.index]\ntest = X.loc[test_data.index]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.612Z","iopub.execute_input":"2022-02-21T16:35:41.612644Z","iopub.status.idle":"2022-02-21T16:35:41.633384Z","shell.execute_reply.started":"2022-02-21T16:35:41.612607Z","shell.execute_reply":"2022-02-21T16:35:41.632358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = x.select_dtypes(np.number).columns\ntransformer = RobustScaler().fit(x[cols])\nx[cols] = transformer.transform(x[cols])\ntest[cols] = transformer.transform(test[cols])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.634723Z","iopub.execute_input":"2022-02-21T16:35:41.635049Z","iopub.status.idle":"2022-02-21T16:35:41.898308Z","shell.execute_reply.started":"2022-02-21T16:35:41.635008Z","shell.execute_reply":"2022-02-21T16:35:41.897202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=2020)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.899342Z","iopub.execute_input":"2022-02-21T16:35:41.899623Z","iopub.status.idle":"2022-02-21T16:35:41.911746Z","shell.execute_reply.started":"2022-02-21T16:35:41.899596Z","shell.execute_reply":"2022-02-21T16:35:41.910599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror')","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:35:41.913197Z","iopub.execute_input":"2022-02-21T16:35:41.913705Z","iopub.status.idle":"2022-02-21T16:35:41.921156Z","shell.execute_reply.started":"2022-02-21T16:35:41.913667Z","shell.execute_reply":"2022-02-21T16:35:41.920434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_lst = {\n    'learning_rate' : [0.01, 0.1, 0.15, 0.3, 0.5],\n    'n_estimators' : [100, 500, 1000, 2000, 3000],\n    'max_depth' : [3, 6, 9],\n    'min_child_weight' : [1, 5, 10, 20],\n    'reg_alpha' : [0.001, 0.01, 0.1],\n    'reg_lambda' : [0.001, 0.01, 0.1]\n}\n\nxgb_reg = RandomizedSearchCV(estimator = xgb, param_distributions = param_lst,\n                              n_iter = 100, scoring = 'neg_root_mean_squared_error',\n                              cv = 5)\n       \nxgb_search = xgb_reg.fit(X_train, y_train)\n\n# XGB with tune hyperparameters\nbest_param = xgb_search.best_params_\nxgb = XGBRegressor(**best_param)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:55:08.933978Z","iopub.execute_input":"2022-02-18T15:55:08.934423Z","iopub.status.idle":"2022-02-18T15:55:17.150084Z","shell.execute_reply.started":"2022-02-18T15:55:08.934383Z","shell.execute_reply":"2022-02-18T15:55:17.146903Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm = LGBMRegressor(boosting_type='gbdt',objective='regression', max_depth=-1,\n                    lambda_l1=0.0001, lambda_l2=0, learning_rate=0.1,\n                    n_estimators=100, max_bin=200, min_child_samples=20, \n                    bagging_fraction=0.75, bagging_freq=5,\n                    bagging_seed=7, feature_fraction=0.8,\n                    feature_fraction_seed=7, verbose=-1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:36:37.174332Z","iopub.execute_input":"2022-02-21T16:36:37.174693Z","iopub.status.idle":"2022-02-21T16:36:37.180184Z","shell.execute_reply.started":"2022-02-21T16:36:37.174659Z","shell.execute_reply":"2022-02-21T16:36:37.178899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_lst = {\n    'max_depth' : [2, 5, 8, 10],\n    'learning_rate' : [0.001, 0.01, 0.1, 0.2],\n    'n_estimators' : [100, 300, 500, 1000, 1500],\n    'lambda_l1' : [0.0001, 0.001, 0.01],\n    'lambda_l2' : [0, 0.0001, 0.001, 0.01],\n    'feature_fraction' : [0.4, 0.6, 0.8],\n    'min_child_samples' : [5, 10, 20, 25]\n}\n\nlightgbm = RandomizedSearchCV(estimator = lgbm, param_distributions = param_lst,\n                              n_iter = 100, scoring = 'neg_root_mean_squared_error',\n                              cv = 5)\n       \nlightgbm_search = lightgbm.fit(X_train, y_train,verbose = -1)\n\n# LightBGM with tuned hyperparameters\nbest_param = lightgbm_search.best_params_\nlgbm = LGBMRegressor(**best_param,verbose = -1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T16:36:38.892826Z","iopub.execute_input":"2022-02-21T16:36:38.893157Z","iopub.status.idle":"2022-02-21T16:36:41.409743Z","shell.execute_reply.started":"2022-02-21T16:36:38.893127Z","shell.execute_reply":"2022-02-21T16:36:41.406729Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb = CatBoostRegressor(loss_function='RMSE', logging_level='Silent')","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:55:17.155246Z","iopub.status.idle":"2022-02-18T15:55:17.155844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_lst = {\n    'n_estimators' : [100, 300, 500, 1000, 1300, 1600],\n    'learning_rate' : [0.0001, 0.001, 0.01, 0.1],\n    'l2_leaf_reg' : [0.001, 0.01, 0.1],\n    'random_strength' : [0.25, 0.5 ,1],\n    'max_depth' : [3, 6, 9],\n    'min_child_samples' : [2, 5, 10, 15, 20],\n    'rsm' : [0.5, 0.7, 0.9],\n    \n}\n\ncatboost = RandomizedSearchCV(estimator = cb, param_distributions = param_lst,\n                              n_iter = 100, scoring = 'neg_root_mean_squared_error',\n                              cv = 5)\n\ncatboost_search = catboost.fit(X_train, y_train)\n\n# CatBoost with tuned hyperparams\nbest_param = catboost_search.best_params_\ncb = CatBoostRegressor(logging_level='Silent', **best_param)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:55:17.157143Z","iopub.status.idle":"2022-02-18T15:55:17.157759Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_cross_val(model, X, y):\n    score = cross_val_score(model, X, y, cv=5)\n    mean = score.mean()\n    return mean\n\ncb.fit(X_train, y_train)   \npreds = cb.predict(X_val) \npreds_test_cb = cb.predict(test)\nmae_cb = mean_absolute_error(y_val, preds)\nrmse_cb = np.sqrt(mean_squared_error(y_val, preds))\nscore_cb = cb.score(X_val, y_val)\ncv_cb = mean_cross_val(cb, x, y)\n\n\nxgb.fit(X_train, y_train)   \npreds = xgb.predict(X_val) \npreds_test_xgb = xgb.predict(test)\nmae_xgb = mean_absolute_error(y_val, preds)\nrmse_xgb = np.sqrt(mean_squared_error(y_val, preds))\nscore_xgb = xgb.score(X_val, y_val)\ncv_xgb = mean_cross_val(xgb, x, y)\n\n\nlgbm.fit(X_train, y_train)   \npreds = lgbm.predict(X_val) \npreds_test_lgbm = lgbm.predict(test)\nmae_lgbm = mean_absolute_error(y_val, preds)\nrmse_lgbm = np.sqrt(mean_squared_error(y_val, preds))\nscore_lgbm = lgbm.score(X_val, y_val)\ncv_lgbm = mean_cross_val(lgbm, x, y)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:55:17.159284Z","iopub.status.idle":"2022-02-18T15:55:17.159915Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I use blending method to achieve better prediction by using a proportion of each model.","metadata":{}},{"cell_type":"code","source":" def blend_models_predict(X, b, c, d):\n        return ((b* xgb.predict(X)) + (c * lgbm.predict(X)) + (d * cb.predict(X)))","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:55:17.161108Z","iopub.status.idle":"2022-02-18T15:55:17.161718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subm = np.exp(blend_models_predict(test, 0.40, 0.20, 0.40))\nsubmission = pd.DataFrame({'Id': test.index,\n                           'SalePrice': subm})\n\nsubmission.to_csv(\"../../kaggle/working/submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-18T15:55:17.162989Z","iopub.status.idle":"2022-02-18T15:55:17.163579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 Conclusion\n\nIn this notebook I have used different tools for the first time thanks to the notebook you will find in references. This way I have learned a lot in data science tools and how to ensemble different ML models. \n\nI have also learned to perform data visualisation and to interpret these visualisation as well as preprocessed the data before training and fit the model.\n\nAs I am still a beginner please feel free to suggest any improvements in the comments ! \n\n**References**\n\n\n> Thanks [@angqx95](https://www.kaggle.com/angqx95) for his great notebook (https://www.kaggle.com/angqx95/data-science-workflow-top-2-with-tuning) for this competition, I have learned a lot and I tried to reuse step by step what he has implemented.","metadata":{}}]}