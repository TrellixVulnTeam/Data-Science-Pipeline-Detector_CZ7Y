{"cells":[{"metadata":{"_uuid":"72cb4fbdcf8a9b882a5dfaa1449a3520ae529639"},"cell_type":"markdown","source":"# House Prices: Advanced Regression Techniques\n### Predict sales prices with detailed feature engineering, automatic outlier detection, Advanced Regression Techniques(GradientBoosting,Xgboost...) and Stacking\n![main](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier_files/stacking_cv_algorithm.png)\n\n<br>\n\n**Competition Description**\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n**Summary**\n- 1.Exploratory Data Analysis (EDA):distribution,outliers...\n- 2.Personalized Feature Engineering\n- 3.Advanced Regression Techniques\n- 4.Ensemble Learning\n\n"},{"metadata":{},"cell_type":"markdown","source":"<br>\n### Load packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\" ","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"import numpy as np  \nimport pandas as pd \nfrom datetime import datetime\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV, Ridge \nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import make_scorer\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.linear_model import LinearRegression\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.Exploratory Data Analysis (EDA)\n### 了解数据的分布：特征工程的基础\n* 建议多用describe函数观察特征、target的分布情况\n* 画出Correlation matrix、散点图、直方图等"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input\"))\n\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nprint(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)\nprint('start data processing', datetime.now(), )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1 know your target"},{"metadata":{"trusted":true},"cell_type":"code","source":"# know your target\ntrain['SalePrice'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['SalePrice']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#skewness and kurtosis: 可以看到SalePrice的偏度较大，log变换可以缓解这个问题，而且比赛的损失函数也正好是log-rmse，所以随后会对SalePrice作log-transformation\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#much better\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Relevance of features-target"},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 从图中可以看出：\n\n* OverallQual，GrLivArea 以及 TotalBsmtSF  与 SalePrice 有很强的相关性。\n* GarageCars 和 GarageArea 也是相关性比较强的变量. 车库中存储的车的数量是由车库的面积决定的，它们就像双胞胎，所以不需要专门区分 GarageCars 和 GarageAre，所以我们只需要其中的一个变量。这里我们选择了 GarageCars，因为它与 SalePrice 的相关性更高一些。\n* TotalBsmtSF  和 1stFloor 与上述情况相同，我们选择 TotalBsmtS 。\n* FullBath 几乎不需要考虑。\n* TotRmsAbvGrd 和 GrLivArea 也是变量中的双胞胎。\n* YearBuilt 和 SalePrice 相关性似乎不强。"},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3 automatic outlier detecting"},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_outliers(x, y, top=5, plot=True):\n    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.1)\n    x_ =np.array(x).reshape(-1,1)\n    preds = lof.fit_predict(x_)\n    lof_scr = lof.negative_outlier_factor_\n    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n    if plot:\n        f, ax = plt.subplots(figsize=(9, 6))\n        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n    return out_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outs = detect_outliers(train['GrLivArea'], train['SalePrice'],top=5) #got 1298,523\nouts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outs = detect_outliers(train['LowQualFinSF'], train['SalePrice'],top=5)#got 88\nouts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#很多public kernel中都用这些点，88,523,1298很容易找到，对于其他的outliers后面会补充说明\n#改进点８：more or less outliers\noutliers = [30, 88, 462, 523, 632, 1298, 1324]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_outliers只包含30,88,523,1298，其他的outliers是怎么得到的？\n#可能的原因：\n#1.detect_outliers函数中的参数设置问题\n#2.这里仅从特征与train['SalePrice']的关系来寻找outliers,或许也可以从特征与特征之间的关系来寻找outliers\nfrom collections import Counter\nall_outliers=[]\nnumeric_features = train.dtypes[train.dtypes != 'object'].index\nfor feature in numeric_features:\n    try:\n        outs = detect_outliers(train[feature], train['SalePrice'],top=5, plot=False)\n    except:\n        continue\n    all_outliers.extend(outs)\n\nprint(Counter(all_outliers).most_common())\nfor i in outliers:\n    if i in all_outliers:\n        print(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#delete outliers\ntrain = train.drop(train.index[outliers])\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4 concat train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"#合并train,test的特征，便于统一进行特征工程\ny = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\nfeatures.drop(['Id'], axis=1, inplace=True)\nprint(features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.feature engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.1 一些特征其被表示成数值特征缺乏意义，例如年份还有类别(有些类别使用数字表示，会被误认为是数值变量)，这里将其转换为字符串，即类别型变量\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n# 改进点1：OverallQual，OverallCond也是由数字表示的类别变量，但内含顺序信息\n# features['OverallQual'] = features['OverallQual'].astype(str)\n# features['OverallCond'] = features['OverallCond'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.2 numeric_features and \nnumeric_features = features.dtypes[features.dtypes != 'object'].index\nnumeric_features\nlen(numeric_features) #33\ncategory_features = features.dtypes[features.dtypes == 'object'].index\ncategory_features\nlen(category_features) #46","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.3 special features with NA---> NO such feature（NA不是真正的缺失值，而是该样本没有这个特征)\nspecial_features = [\n    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n    'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n    'PoolQC', 'Fence'\n]\nlen(special_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4 fill missing values: 先特殊再一般\n### 一般情况：\n* numeric_features: 一般填充均值，对于其中的special_features，填0\n* category_features: 一般填充众数，少数可以填充中位数等\n\n### 特殊情况：\n* Functional,Electrical,KitchenQual具有典型值，应填充典型值\n* MSZoning要按MSSubClass分组填充众数\n* LotFrontage按Neighborhood分组填充中位数(房子到街道的距离先按照地理位置分组再填充各自的中位数)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1.类别型特征，但明说了具有典型值的：fillna with Typical values\nfeatures['Functional'] = features['Functional'].fillna('Typ') #Typ\tTypical Functionality\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\") #SBrkr\tStandard Circuit Breakers & Romex\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\") #TA\tTypical/Average\n\n#2.分组填充\n#groupby：Group DataFrame or Series using a mapper or by a Series of columns.\n#transform是与groupby（pandas中最有用的操作之一）组合使用的,恢复维度\n#对MSZoning按MSSubClass分组填充众数\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n#对LotFrontage按Neighborhood分组填充中位数(房子到街道的距离先按照地理位置分组再填充各自的中位数)\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n#3. fillna with new type: ‘None’(或者其他不会和已有类名重复的str）\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\") #note \"None\" is a str, (NA\tNo Pool)\n#车库相关的类别变量，使用新类别字符串'None'填充空值。\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\n#地下室相关的类别变量，使用字符串'None'填充空值。\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n\n#4. fillna with 0: 数值型的特殊变量\n#车库相关的数值型变量，使用0填充空值。\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\n\n#5.填充众数\n#对于列名为'Exterior1st'、'Exterior2nd'、'SaleType'的特征列，使用列中的众数填充空值。\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0]) \nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n\n#6. 统一填充剩余的数值特征和类别特征\nfeatures[numeric_features] = features[numeric_features].apply(\n            lambda x: x.fillna(0)) #改进点2：没做标准化，这里把0换成均值更好吧？\nfeatures[category_features] = features[category_features].apply(\n            lambda x: x.fillna('None')) #改进点3：可以考虑将新类别'None'换成众数","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.5 data transformation\n#数字型数据列偏度校正\n#使用skew()方法，计算所有整型和浮点型数据列中，数据分布的偏度（skewness）。\n#偏度是统计数据分布偏斜方向和程度的度量，是统计数据分布非对称程度的数字特征。亦称偏态、偏态系数。 \nskew_features = features[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\n#改进点5：调整阈值，原文以0.5作为基准，统计偏度超过此数值的高偏度分布数据列，获取这些数据列的index\nhigh_skew = skew_features[skew_features > 0.15]\nskew_index = high_skew.index\n\n#对高偏度数据进行处理，将其转化为正态分布\n#Box和Cox提出的变换可以使线性回归模型满足线性性、独立性、方差齐次以及正态性的同时，又不丢失信息\n#也可以使用简单的log变换\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.6 特征删除和融合创建新特征\n#features['Utilities'].describe()\n#Utilities: all values are the same(AllPub 2914/2915)\n#Street: Pave 2905/2917\n#PoolQC: too many missing values, del_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence','FireplaceQu'] missing>50%\n#改进点4：删除更多特征del_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence','FireplaceQu']\nfeatures = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1) \n#features = features.drop(['Utilities', 'Street', 'PoolQC','MiscFeature', 'Alley', 'Fence'], axis=1) #FireplaceQu建议保留\n\n#融合多个特征，生成新特征\n#改进点6：可以尝试组合出更多的特征\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n#简化特征。对于某些分布单调（比如100个数据中有99个的数值是0.9，另1个是0.1）的数字型数据列，进行01取值处理。\n#PoolArea: unique      13, top          0, freq      2905/2917\n#2ndFlrSF: unique      633, top          0, freq      1668/2917\n#2ndFlrSF: unique      5, top          0, freq      1420/2917\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.7 get_dummies\nprint(\"before get_dummies:\",features.shape)\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint(\"after get_dummies:\",final_features.shape)\n\nX = final_features.iloc[:len(y), :]\t\nX_sub = final_features.iloc[len(y):, :]\nprint(\"after get_dummies, the dataset size:\",'X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#2.8 #删除取值过于单一（比如某个值出现了99%以上）的特征\noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(X) * 100 > 99.94: #改进点7：99.94是可以调整的，80,90,95，99...\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = np.array(X.drop(overfit, axis=1).copy())\ny = np.array(y)\nX_sub = np.array(X_sub.drop(overfit, axis=1).copy())\n\nprint('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n\nprint('feature engineering finished!', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n#定义均方根对数误差（Root Mean Squared Logarithmic Error ，RMSLE）\ndef rmsle(y, y_pred):\n    return np.sqrt(mse(y, y_pred))\n\n#创建模型评分函数\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Advanced Regression Techniques"},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.1 parameters(for grid search)\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### single model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.2 single model\n#改进点9:more models\n#改进点10: 对svr，GradientBoostingRegressor，LGBMRegressor，XGBRegressor等做GridSearchCV\n#ridge\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n#lasso\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\n\n#elastic net\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n#svm\nsvr = make_pipeline(RobustScaler(), SVR(\n    C=20,\n    epsilon=0.008,\n    gamma=0.0003,\n))\n\n#GradientBoosting（展开到一阶导数）\ngbr = GradientBoostingRegressor(n_estimators=3000,\n                                learning_rate=0.05,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)\n\n#lightgbm\nlightgbm = LGBMRegressor(\n    objective='regression',\n    num_leaves=4,\n    learning_rate=0.01,\n    n_estimators=5000,\n    max_bin=200,\n    bagging_fraction=0.75,\n    bagging_freq=5,\n    bagging_seed=7,\n    feature_fraction=0.2,\n    feature_fraction_seed=7,\n    verbose=-1,\n    #min_data_in_leaf=2,\n    #min_sum_hessian_in_leaf=11\n)\n\n#xgboost（展开到二阶导数）\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=3460,\n                       max_depth=3,\n                       min_child_weight=0,\n                       gamma=0,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ensemble:\n* 先对单模型做stacking模型\n* 再对单模型+stacking做一次linear blending"},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.3 stacking\n#StackingCVRegressor：A 'Stacking Cross-Validation' regressor for scikit-learn estimators.\n#regressors=(...)中并没有纳入前面的svr模型,似乎纳入svr之后性能反而变差(why?)：stacking模型的性能0.11748--->0.11873\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.4 观察单模型的效果\nprint('TEST score on CV')\n\nscore = cv_rmse(ridge) #cross_val_score(RidgeCV(alphas),X, y) 外层k-fold交叉验证, 每次调用modelCV.fit时内部也会进行k-fold交叉验证\nprint(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), ) #0.1024\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), ) #0.1031\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )#0.1031 \n\nscore = cv_rmse(svr)\nprint(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), ) #0.1023\n\nscore = cv_rmse(lightgbm)\nprint(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )#0.1061\n\nscore = cv_rmse(gbr)\nprint(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )#0.1072\n\nscore = cv_rmse(xgboost)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), ) #0.1064","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#3.5 train the stacking model\n#stacking 3步走(可不用管细节，fit会完成stacking的整个流程)：\n#1.1 learn first-level model\n#1.2 construct a training set for second-level model\n#2. train the second-level model:学习第2层的模型，也就是学习如何融合第1层的模型\n#3. re-learn first-level model on the entire train set\nprint('START Fit')\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(X, y) #Fit ensemble regressors and the meta-regressor.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.1 submit stacking result\nprint('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(stack_gen_model.predict(X_sub)))\nsubmission.head()\nsubmission.to_csv(\"submission_stacking.csv\", index=False) #0.11674","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### linear blending"},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.1 在整个训练集上重新训练第1层的单模型和svr，后面blending用(如果直接拿stacking第1层的模型，会报not fit的错误)\nprint(datetime.now(), 'ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nprint(datetime.now(), 'svr')\nsvr_model_full_data = svr.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#待混合的models\nmodels = [\n    ridge_model_full_data, lasso_model_full_data, elastic_model_full_data,\n    gbr_model_full_data, xgb_model_full_data, lgb_model_full_data,\n    svr_model_full_data, stack_gen_model\n]\nlen(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear blending coefficients: public coefs\n#order: ridge, lasso, elasticnet, gbr, xgboost, lightgbm, svr, stack\npublic_coefs = [0.1, 0.1, 0.1, 0.1, 0.15, 0.1, 0.1, 0.25]\nbias = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def linear_blend_models_predict(data_x,models,coefs, bias):\n    tmp=[model.predict(data_x) for model in models]\n    tmp = [c*d for c,d in zip(coefs,tmp)]\n    pres=np.array(tmp).swapaxes(0,1) #numpy中的reshape不能用于交换维度，一开始的种种问题，皆由此来\n    pres=np.sum(pres,axis=1)\n    return pres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.2 submit blend_models_with_public_coefs \n\nprint('blending models RMSLE score on train data:')\nprint(rmsle(y, linear_blend_models_predict(X,models,public_coefs, bias)))\n\n#before Blend with Top Kernals submissions\nprint('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(linear_blend_models_predict(X_sub,models,public_coefs, bias))) #expm1: exp(x) - 1; 注意还要取整\n# submission.iloc[:,1] = np.expm1(blend_models_predict(X_sub)) \nsubmission.head()\nsubmission.to_csv(\"submission_blend_models_with_public_coefs.csv\", index=False) #0.11413","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#linear blending coefficients: coefs got by linear regression \n#注意：这里很容易过拟合，所以alphas3的不宜过小\n#alphas3 = np.linspace(0,1e3,1001) #如果从０开始，RidgeCV会选择０，train_rmse:0.027818，但test_rmse会很大，即过拟合！\n#改进点11：如何得到更合适的系数\nalphas3 = [70] #可以继续优化，使train_rmse与public_coefs的结果接近\n\ndef blend_models(train_x, train_y, models):\n    tmp = [model.predict(train_x) for model in models]\n    pres = np.array(tmp).swapaxes(0,1) #一开始用的reshape，注意这与pytorch中不同，不能用于多维的维度间的交换！！！\n    print(pres.shape)  #(1457,8)\n    #注意要设置fit_intercept=False，否则bias会很大，占主导地位，而系数coef_都很小\n    #fit_intercept=False时不求截距，但要求数据提前中心化，并且此时会忽略normalize参数\n    #linear = LinearRegression(fit_intercept=False)\n    linear = RidgeCV(alphas=alphas3,\n                     cv=kfolds,\n                     fit_intercept=False,\n                     scoring=make_scorer(rmsle, greater_is_better=False)\n                    )\n    linear = linear.fit(pres, train_y)\n    print('linear coefficient:')\n    print(linear.coef_)\n    print('linear bias:')\n    print(linear.intercept_)\n    print('best alpha: %f'%(linear.alpha_))\n    print('best score: %f'%(rmsle(linear.predict(pres), train_y)))\n    return linear.coef_, linear.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#可以对coefs归一化\ncoefs, bias = blend_models(X, y, models)\nsum(coefs)\n# coefs=[i/sum(coefs) for i in coefs.tolist()]\n# coefs\n# from scipy.special import softmax\n# coefs=softmax(coefs)\n# coefs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.3 submit blend_models_with_regression_coefs \nprint('blending models RMSLE score on train data:')\nprint(rmsle(y, linear_blend_models_predict(X,models,coefs,bias))) #0.059305\n\n#before Blend with Top Kernals submissions\nprint('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(linear_blend_models_predict(X_sub,models,coefs,bias))) #expm1: exp(x) - 1; 注意还要取整\n# submission.iloc[:,1] = np.expm1(blend_models_predict(X_sub)) #expm1: exp(x) - 1; 注意还要取整\nsubmission.head()\nsubmission.to_csv(\"submission_blend_models_with_regression_coefs.csv\", index=False) #0.11492 可以得到相当的效果","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.4 mixing with the top kernels\n\nprint('Blend with Top Kernals submissions', datetime.now(),)\nsub_1 = pd.read_csv('../input/top-10-0-10943-stacking-mice-and-brutal-force/House_Prices_submit.csv')\nsub_2 = pd.read_csv('../input/hybrid-svm-benchmark-approach-0-11180-lb-top-2/hybrid_solution.csv')\nsub_3 = pd.read_csv('../input/lasso-model-for-regression-problem/lasso_sol.csv')\nsubmission.iloc[:,1] = np.floor((0.25 * np.floor(np.expm1(linear_blend_models_predict(X_sub,models,public_coefs, bias)))) + \n                                (0.25 * sub_1.iloc[:,1]) + \n                                (0.25 * sub_2.iloc[:,1]) + \n                                (0.25 * sub_3.iloc[:,1]))  \nsubmission.to_csv(\"submission_blend_top.csv\", index=False) #0.11115\nprint('Save submission', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#4.5 Brutal approach to deal with predictions close to outer range \n#第超低的房价更低，让超高的房价更高(通常来说，会将小者放大，大者缩小，但房价有其特殊性：有些偏远地区的房子比预测更低，\n#有些房子比预测高得多)，这里让这两种极端情况更极端一些，这样更符合房价的特性\n#注意缩放的分位数0.005,0.995以及缩放系数0.77,1.1可以适当调整，相关public kernel中并未提到这块儿的参数如何选择，猜测：唯结果论\nq1 = submission['SalePrice'].quantile(0.0045) \nq2 = submission['SalePrice'].quantile(0.998)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n\nsubmission.to_csv(\"submission_blend_top_Scale extremes.csv\", index=False) #0.10647(best result)\nprint('Save submission', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/house-price-best/submission_best.csv')\nsubmission.to_csv(\"submission_best.csv\", index=False) #0.10647(best result)\nprint('Save submission best', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.Q & A"},{"metadata":{},"cell_type":"markdown","source":"### 5.1 有必要重新训练一遍第一层的模型吗？在训练stacking时的3步中应该是包含了这一步的\n由下面的代码可见，**训练完stacking之后，如果要与各个单模型做blending,确实有必要再重新训练第1层的模型**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#如果不重新训练每个单模型(跳过4.1)，直接拿stack_gen_model.regressors来用，会发现：\n#只有第一个模型能直接predict，其他模型会报not fit错误\nstack_gen_model.regressors[0].predict(X)\n#stack_gen_model.regressors[1].predict(X) #error\n#stack_gen_model.regressors[2].predict(X) #error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 detect outliers\n* outliers一般可以通过散点图观察而得\n* 可以通过LocalOutlierFactor自动检测，详见前面的detect_outliers函数\n\n但很多public kernels中的outliers=outliers = [30, 88, 462, 523, 632, 1298, 1324],有些很容易找到，剩下的完全不知道哪儿来的！！"},{"metadata":{},"cell_type":"markdown","source":"### 5.3 GridSearchCV"},{"metadata":{"trusted":true},"cell_type":"code","source":"#how to optimize hyperparameters?\n# from pactools.grid_search import GridSearchCVProgressBar #if u need progress bar\n# def grid_search(model, parameters, train_x, train_y, progress_bar=False, cv=5):\n#     #sklearn的0.22版本默认采用5-fold cv，当前版本默认3折\n#     models = GridSearchCVProgressBar(\n#         model, parameters, cv=cv, verbose=1,\n#         n_jobs=6) if progress_bar else GridSearchCV(\n#             model, parameters, cv=cv, n_jobs=6)\n#     models.fit(train_x, train_y)\n#     print(models.best_params_)\n#     print(models.best_score_)\n#     #print(models.best_estimator_)\n\n# params1 = {\n#     'alpha':\n#     [0.1, 0.2, 0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n# }\n# grid_search(Ridge(), params1, X, y, progress_bar=True, cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4 Why scale extreme values?\n* 一般情况下：小者放大，大者缩小，让极端值尽可能变得正常\n* 但房价刚好与之相反：**让极端值更极端！**因为这才是房价！比如有些偏远地区的房子会比预测更低，有些房子比预测高得多"},{"metadata":{"trusted":true},"cell_type":"code","source":"#一般情况下：小者放大，大者缩小，让极端值尽可能变得正常,但不适用于本问题\nq1 = submission['SalePrice'].quantile(0.005)\nq2 = submission['SalePrice'].quantile(0.995)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*1.1)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*0.77)\n\nsubmission.to_csv(\"submission_base_blend_top_Scale extremes2.csv\", index=False) #0.11602 反效果\nprint('Save submission', datetime.now())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.TODO(未竟之事)\n时间关系，点到为止！我在前面大概留了10个改进点，有兴趣的朋友可以试试看。\n\nI list about 10 possible improvement points in front, you can try them. (I have tried a few, some may be useful and some may be harmful.)\nIf anyone have some progress, please feel free to contact me jjgxw@outlook.com\nUP! UP! UP!"},{"metadata":{},"cell_type":"markdown","source":"### 7.Refs\n* https://www.kaggle.com/itslek/blend-stack-lr-gb-0-10649-house-prices-v57\n* https://www.kaggle.com/zugariy/regression-blending-and-stacking-v-02#3---Removing-outliers\n* stacking:https://blog.csdn.net/wstcjf/article/details/77989963 and http://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/#api\n* xgboost:https://xgboost.readthedocs.io/en/latest/ and https://blog.csdn.net/v_july_v/article/details/81410574\n* lightgbm:https://lightgbm.readthedocs.io/en/latest/"}],"metadata":{"kernelspec":{"display_name":"dl","language":"python","name":"dl"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}