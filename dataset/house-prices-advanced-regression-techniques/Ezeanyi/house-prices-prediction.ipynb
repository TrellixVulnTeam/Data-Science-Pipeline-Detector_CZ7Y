{"cells":[{"metadata":{},"cell_type":"markdown","source":"## INTRODUCTION\nThe regularized linear models ridge, lasso and elasticnet,  and gradient boosting regression tree were used in fitting the data. Simple feature engineering, feature optimization, hyper parameter tuning and hybrid regression extended model performance to <b>LB score 0.11904 Top 5% </b>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.0 Data Preparation\nLoad data, identify missing values and outliers, and EDA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load libraries for data manipulation\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n# load visualization libraries\nimport matplotlib.pyplot as plt\nplt.style.use('classic')\n%matplotlib inline\nimport seaborn as sb\nsb.set()\n# load modelling libraries\nfrom sklearn.linear_model import Ridge,  Lasso, ElasticNet\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import  cross_val_score\n# warnings\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the train and test data sets\ntrain = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ntrain_len = len(train)\n# combine train and test data sets\ndata_all = train.append(test, sort=False, ignore_index=True)\nfeatures = data_all.shape[1]\n# check for duplicates\nidUnique = len(set(data_all.Id))\nidTotal = data_all.shape[0]\nidDupli = idTotal - idUnique\nprint(\"There are \" + str(idDupli) + \" duplicate IDs for \" + str(idTotal) + \" total entries\")\nprint(\"There are \" + str(features) + \" variables for \" + str(idTotal) + \" total entries\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 2919 observations with 81 columns. Excluding the target variable SalePrice and Id there are 79 independent variables. The train set has 1460 observations while the test set has 1459 observations, the target variable SalePrice is absent in test. The aim of this study is to train a model on the train set and use it to predict the target SalePrice of the test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# group variables in quantity(scale) and quality(categorical)\nquantity = [f for f in data_all.columns if data_all.dtypes[f] != 'object']\nquantity.remove('SalePrice')\nquantity.remove('Id')\nquality = [f for f in data_all.columns if data_all.dtypes[f] == 'object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(quantity)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 36 scale variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(quality)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 43 categorical variables","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1.1 Scale Missing Values\nImpute missing values for continous and count variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_quant = (data_all[quantity].isnull().sum()/data_all[quantity].isnull().count()).sort_values(ascending=False)\nmissing_quant = missing_quant[missing_quant > 0] * 100\nprint(\"There are {} quantitative features with  missing values :\".format(missing_quant.shape[0]))\nmissing_quant = pd.DataFrame({'Percent' :missing_quant})\nmissing_quant.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The LotFontage described as the linear feet of street connected to the property was absent in about 16% of the properties. This value may be absent due to measurement difficulty. GarageYearBlt with 5.45% missing values can be inability to collect correct info because less than 1% of the properties have no GarageArea. LotFrontage and GarageYrBlt will be filled with median values of the neighborhood to keep original distribution as much as possible. Other variables with less than 1% missing values will be filled with zero.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"np.where(pd.isnull(data_all.GarageArea))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Index no. 2576 has no GarageArea and GarageCars and consequently no GarageYrBlt and hence GarageYrBlt will be filled with zero.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill missing values\ndata_all['LotFrontage'] = data_all.groupby(['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))\ndata_all.at[2576, 'GarageYrBlt'] = 0\ndata_all['GarageYrBlt'] = data_all.groupby(['Neighborhood'])['GarageYrBlt'].apply(lambda x: x.fillna(x.median()))\nfor col in ['MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','GarageCars',\n            'GarageArea']:\n    data_all[col] = data_all[col].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2 Categorical Missing Values\nImpute missing values for nominal and ordinal variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_qual = (data_all[quality].isnull().sum()/data_all[quality].isnull().count()).sort_values(ascending=False)\nmissing_qual = missing_qual[missing_qual > 0] * 100\nprint(\"There are {} qualitative features with  missing values :\".format(missing_qual.shape[0]))\nmissing_qual = pd.DataFrame({'Percent' :missing_qual})\nmissing_qual.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are features explicitly stated as NA on a property, those features will be filled with None, while other NA not explicitly assigned in the data description will be filled with the mode in the Neighborhood.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill missing values\ndata_all['MSZoning'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['MSZoning'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['Utilities'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['Utilities'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['Exterior1st'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['Exterior1st'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['Exterior2nd'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['Exterior2nd'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['MasVnrType'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['MasVnrType'].apply(lambda x: x.fillna(x.value_counts().index[0]))\nfor col in ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish',\n           'GarageQual','GarageCond','Alley','Fence','PoolQC','MiscFeature']:\n    data_all[col] = data_all[col].fillna('None')\ndata_all['Electrical'] = data_all.groupby(['Neighborhood','MSSubClass' ])['Electrical'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['KitchenQual'] = data_all.groupby(['Neighborhood','MSSubClass' ])['KitchenQual'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['Functional'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['Functional'].apply(lambda x: x.fillna(x.value_counts().index[0]))\ndata_all['SaleType'] = data_all.groupby(['Neighborhood', 'MSSubClass'])['SaleType'].apply(lambda x: x.fillna(x.value_counts().index[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3 Outliers\nOutliers can adversely affect the result of data analysis. Dean De Cook author of Ames house dataset recommended the removal of some 5 outliers representing unsual sales in GrLivArea greater than 4000 square feet. Ref: www.amstat.org/publications/jse/v19n3/decock.pdf","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_all.iloc[np.where(data_all.GrLivArea > 4000)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 of the houses are outliers, large houses prized relatively low, while the 2 on top in the scatter are very large houses with commensurate sales . The 2 outliers in the train set will be removed while the other outlier ID 2550 is in the test set .","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter = sb.regplot(x='GrLivArea', y='SalePrice', fit_reg =False, data=data_all)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the outliers in the train set\ndata_all = data_all.drop(data_all[data_all['Id'] == 524].index)\ndata_all = data_all.drop(data_all[data_all['Id'] == 1299].index)\n# adjusting train len\ntrain_len = train_len - 2\n# fill the outlier in test set with the median\ndata_all.loc[2549,('GrLivArea')]  = data_all['GrLivArea'].median()\ndata_all.loc[2549,('LotArea')]  = data_all['LotArea'].median()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4 EDA\nTest independent scale variables for linearity and normality ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":" #### 1.4.1 Linearity Test","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#correlation matrix\ncorrmatrix = data_all[:train_len].corr()\nf, ax = plt.subplots(figsize=(30, 24))\nk = 36 \ncols = corrmatrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(data_all[:train_len][cols].values.T)\nsb.set(font_scale=1.0)\nhm = sb.heatmap(cm, cbar=True, annot=True, square=True, fmt='.1f', annot_kws={'size': 18}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The heatmap shows some features have good correlation with the target and also high collinearity between GrLivArea/TotRmsAbvGrd, GarageCars/GarageArea, TotalBsmtSF/1stFlrSF and YearBuilt/GarageYrBlt. However there are features with low correaltion with the target (i.e. absolute correlation between 0.0 and 3.0)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatterplot of dependent vs independent variables\nquantitative = [f for f in data_all.columns if f in cols]\nquantitative.remove('SalePrice')\nvarx = pd.melt(data_all, id_vars=['SalePrice'], value_vars=quantitative)\ngx = sb.FacetGrid(varx, col=\"variable\",  col_wrap=3, sharex=False, sharey=False, height=5)\ngx = gx.map(sb.regplot, \"value\", \"SalePrice\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The independent variables show weak, moderate and high lineaity with the target, some with increasing linearity with the target and some with decreasing linearity with the target. The following features show decreasing lineraity - OverallCond, BsmtFinSF2, LowQualFinSF, BsmtHalfBath, MiscVal and YrSold.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 1.4.2 Normality Test","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# distribution plot of dependent and independent variables\n#quantitative = [f for f in data_all.columns if f in cols]\n#varx = pd.melt(data_all, value_vars=quantitative)\n#gx = sb.FacetGrid(varx, col=\"variable\",  col_wrap=3, sharex=False, sharey=False, height=5)\n#gx = gx.map(sb.distplot, (\"value\"), fit=norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No distribution is normally distributed, skewness and kurtosis enable us to assess deviation from normality. The variables with moderate to high skew will need a transformation to make them approximately normally distributed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. 0 Data Manipulations\n\nCreate new variables, and transform features for modeling\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Feature Extraction\nCreate new features from existing features. There are 2 types of categorical variables, nominal and ordinal. The ordinal variables show some rank and will be encoded with numeric values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# impute ordinal data with numeric values\ndata_all['KitchenQual'].replace(['Ex','Gd','TA','Fa'],[4,3,2,1],inplace=True)\ndata_all['FireplaceQu'].replace(['Ex','Gd','TA','Fa','Po', 'None'],[6,5,4,3,2,1],inplace=True)\ndata_all['GarageQual'].replace(['Ex','Gd','TA','Fa','Po','None'],[6,5,4,3,2,1],inplace=True)\ndata_all['GarageCond'].replace(['Ex','Gd','TA','Fa','Po','None'],[6,5,4,3,2,1],inplace=True)\ndata_all['PoolQC'].replace(['Ex','Gd','TA','Fa','None'],[5,4,3,2,1],inplace=True)\ndata_all['ExterQual'].replace(['Ex','Gd','TA','Fa'],[4,3,2,1],inplace=True)\ndata_all['ExterCond'].replace(['Ex','Gd','TA','Fa','Po'],[5,4,3,2,1],inplace=True)\ndata_all['BsmtQual'].replace(['Ex','Gd','TA','Fa','Po','None'],[6,5,4,3,2,1],inplace=True)\ndata_all['BsmtCond'].replace(['Ex','Gd','TA','Fa','Po','None'],[6,5,4,3,2,1],inplace=True)\ndata_all['BsmtExposure'].replace(['Gd','Av','Mn','No','None'],[5,4,3,2,1],inplace=True)\ndata_all['HeatingQC'].replace(['Ex','Gd','TA','Fa','Po'],[5,4,3,2,1],inplace=True)\n# transform discrete features to  categorical feature\ndata_all['MSSubClass'] = data_all['MSSubClass'].astype(str)\ndata_all['YrSold'] =    data_all['YrSold'].astype(str)   \ndata_all['MoSold'] =    data_all['MoSold'].astype(str)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combime some old features to form new ones.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# combinations of old features\ndata_all['GarageScale'] = data_all['GarageCars'] * data_all['GarageArea']\ndata_all['GarageOrdinal'] = data_all['GarageQual'] + data_all['GarageCond']\ndata_all['AllPorch'] = data_all['OpenPorchSF'] + data_all['EnclosedPorch'] + data_all['3SsnPorch'] + data_all['ScreenPorch']\ndata_all['ExterOrdinal'] = data_all['ExterQual'] + data_all['ExterCond']\ndata_all['KitchenCombined'] = data_all['KitchenQual'] * data_all['KitchenAbvGr']\ndata_all['FireplaceCombined'] = data_all['FireplaceQu'] * data_all['Fireplaces']\ndata_all['BsmtOrdinal'] = data_all['BsmtQual'] + data_all['BsmtCond']\ndata_all['BsmtFinishedAll'] = data_all['BsmtFinSF1'] + data_all['BsmtFinSF2']\ndata_all['AllFlrSF'] = data_all['1stFlrSF'] + data_all['2ndFlrSF']\ndata_all['OverallCombined'] = data_all['OverallQual'] + data_all['OverallCond']\ndata_all['TotalFullBath'] = data_all['BsmtFullBath'] +  + data_all[\"FullBath\"] \ndata_all['TotalHalfBath'] = data_all[\"HalfBath\"] + data_all['BsmtHalfBath']\ndata_all['TotalSF'] = data_all['AllFlrSF'] + data_all['TotalBsmtSF']\ndata_all['YrBltAndRemod'] = data_all[\"YearRemodAdd\"] + data_all['YearBuilt']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Categorical Encoding\nEncode categoricals with one hot encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = [f for f in data_all.columns if data_all.dtypes[f] == 'object']\ncategorical = []\nfor i in data_all.columns:\n    if i in cat_features:\n        categorical.append(i)\ndata_cat = data_all[categorical]\ndata_cat = pd.get_dummies(data_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Features with zero values that can be described as almost 100% can cause overfitting and will be dropped. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop features with zeros > 99.95\noverfit = []\nfor i in data_cat.columns:\n    counts = data_cat[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(data_cat) * 100 > 99.95:\n        overfit.append(i)\ndata_cat = data_cat.drop(overfit, axis=1)\nprint(\"There are {} qualitative features with zeros > 99.95 :\".format(len(overfit)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3 Numeric Feature Optimization\nSelect features with minimal linearity and reduce skews to symetrical normal distribution","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.1 Linearity with Target\nSpearman correlation coeffient can be used to find correlation of rank data and skewed scale data. It can give more reliable result in comparing the scale and skewed data as it does not need to satisfy the linearity test. Adapted from https://kaggle.com/dgawlik/house-prices-eda","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric = [f for f in data_all.columns if data_all.dtypes[f] != 'object']\nnumeric.remove('Id')\n#calculate spearman corr. coef.\nfeatures = numeric\nspr = pd.DataFrame()\nspr['feature'] = features\nspr['spearman'] = [data_all[f].corr(data_all['SalePrice'], 'spearman') for f in features]\nspr = spr.sort_values('spearman')\nnumeric_cols = spr['feature'].tail(47)# select features with corr. approximately => 0.2\n#plot spearman corr. coef.\nplt.figure(figsize=(6, 0.25*len(features)))\nbar = sb.barplot(data=spr, y='feature', x='spearman', orient='h')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.3.2 Normality and Homogeneity of Variance\nHomogeneity of variance is simply having equal spreads between the predictor variable and the target variable. It ensures the error is same across all values of the predictor variable. This is acheived by reducing a skewed distribution to an approximately symmetrical normal distribution and can impact on linear model performance.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Log transformation of the target variable will reduce deviation from a normal distribution and reduce errors in predicting expensive and cheap houses.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Log transform the target\ndata_all[:train_len].loc[:,('SalePrice')] = np.log1p(data_all[:train_len]['SalePrice'])\n# test SalePrice for normality\nfig = plt.figure()\nres = stats.probplot(data_all[:train_len]['SalePrice'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Probability plot indicates SalePrice deviation from a normal distribution is not significant.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# gather numerical features\nnumerical = numeric_cols.values.tolist()\nnumerical.remove('SalePrice')\ndata_num = data_all[numerical].astype(float)\n# calculate skewness \nskewness = data_num.apply(lambda x: skew(x)).sort_values(ascending=False)\nskewness = skewness[abs(skewness) > 0.5]\nprint(\"There are {} numerical features with absolute Skew > 0.5 :\".format(skewness.shape[0]))\nskewness = pd.DataFrame({'Skew' :skewness})\nskewness.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use Box-Cox transformation to reduce skewed features to an approximately normal distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Box-Cox transformation\nfor i in skewness.index.tolist():\n    data_num[i] = boxcox1p(data_num[i], boxcox_normmax(data_num[i] + 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Manual skew reduction will be attempted for moderate and high skewed features. Features that cannot be reduced to < 1.0 will be dropped ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate skewness \nskewness = data_num.apply(lambda x: skew(x)).sort_values(ascending=False)\nskewness = skewness[abs(skewness) > 0.5]\n# transforming negative skewed features\nneg_skew = skewness[skewness < 0]\ndata_num.loc[:,('GarageOrdinal')] = data_num['GarageOrdinal']**1.1\ndata_num.loc[:,('GarageQual')] = data_num['GarageQual']**1.2\ndata_num.loc[:,('GarageCond')] = data_num['GarageCond']**1.3\ndata_num.loc[:,('HeatingQC')] = data_num['HeatingQC']**1.6\n# transforming positive skewed features\npos_skew = skewness[skewness > 0]\npos_skew1 = ['Fireplaces','BsmtFullBath','OpenPorchSF','FireplaceCombined','TotalHalfBath',\n             'WoodDeckSF','2ndFlrSF']\ndata_num.loc[:,pos_skew1] = np.sqrt(data_num[pos_skew1])\ndata_num.loc[:,('HalfBath')] = np.cbrt(data_num['HalfBath'])\ndata_num.loc[:,('MasVnrArea')] = np.log2(data_num['MasVnrArea']+1)\n# check high skewed features\nskewness = data_num.apply(lambda x: skew(x)).sort_values(ascending=False)\nskewness = skewness[abs(skewness) > 1.0]\n# Drop highly skewed features\ndata_num = data_num.drop(skewness.index.tolist(), axis=1)\nprint(\"There are {} numerical features with absolute Skew > 1.0 :\".format(skewness.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Graph of GrLivArea and SalePrice showing equal levels of SalePrice dispersion among the values of GrLivArea after both variable satisfaction of an approximately symmetrical normal distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scatter = sb.regplot(x='GrLivArea', y='SalePrice', fit_reg =False, data=data_all)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separate train and test sets for modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert numerical features to standard score\ndata_num = (data_num - data_num.mean())/data_num.std()\n# separate train and test sets\ndf_all = pd.concat([data_num, data_cat], axis = 1)\nfeatures = (data_cat.columns).append(data_num.columns)\ndf_train = df_all[:1458]\ndf_test = df_all[1458:]\ntarget = 'SalePrice'\nX = df_train[features]\ny = data_all[:1458][target]\nX_test = df_test[features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Modeling\nCross validation and model evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining RMSE\ndef rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring = \"neg_mean_squared_error\", cv = 5))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.1.1 Linear Regressors","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ridge CV and model fitting\nridge = Ridge(alpha = 14)\ncv_ridge = rmse_cv(ridge)\nridge.fit(X , y)\nprint('Ridge CV score min: ' + str(cv_ridge.min()) + ' mean: ' + str(cv_ridge.mean()) \n      + ' max: ' + str(cv_ridge.max()) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot ridge important coeffients\ncoefs = pd.Series(ridge.coef_, index = X.columns)\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                       coefs.sort_values().tail(10)])\nplt.figure(figsize=(6, 8))\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"10 Top and Bottom Coefficients\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso CV and model fitting\nlasso = Lasso(alpha = .00066)\ncv_lasso = rmse_cv(lasso)\nlasso.fit(X, y)\nprint('Lasso CV score min: ' + str(cv_lasso.min()) + ' mean: ' + str(cv_lasso.mean()) \n      + ' max: ' + str(cv_lasso.max()) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso features\ncoefs = pd.Series(lasso.coef_, index = X.columns)\nprint(\"Lasso selected \" + str(sum(coefs != 0)) + \" features and discarded \" +  str(sum(coefs == 0)) + \" features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3.1.2 Gradient Boosting Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# GradientBoostng CV and model fitting\ngbrt = GradientBoostingRegressor(n_estimators=1000, learning_rate=.03, max_depth=3, max_features=.04, min_samples_split=4,\n                                 min_samples_leaf=3, loss='huber', subsample=1.0, random_state=0)\ncv_gbrt = rmse_cv(gbrt)\ngbrt.fit(X, y)\nprint('GradientBoosting CV score min: ' + str(cv_gbrt.min()) + ' mean: ' + str(cv_gbrt.mean()) \n      + ' max: ' + str(cv_gbrt.max()) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Model Evaluation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\n# model blending \ndef blend_models(X):\n    return ((ridge.predict(X)) + (gbrt.predict(X)))/2\n\nridge_preds =  np.expm1(ridge.predict(X_test))\ngbrt_preds = np.expm1(gbrt.predict(X_test))\n\nridge_gbrt_preds = np.expm1(blend_models(X_test))\nhp_ridge_gbrt = pd.DataFrame({'Id':test.Id, 'SalePrice':ridge_gbrt_preds})\nhp_ridge_gbrt.to_csv('solution.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting relationship between ridge and gbrt predictions","execution_count":null},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(6, 4))\naxes = fig.add_subplot(1,1,1)\npredictions = pd.DataFrame({\"ridge\":ridge_preds, \"gbrt\":gbrt_preds})\nplt.plot(gbrt_preds, ridge_preds, '.')\naxes.set_xlabel('gbrt')\naxes.set_ylabel('ridge')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Beyond feature engineering hyper parameter tuning and hybrid regression extended model performance.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}