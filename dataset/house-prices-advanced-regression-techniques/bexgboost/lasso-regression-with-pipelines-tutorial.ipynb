{"cells":[{"metadata":{},"cell_type":"markdown","source":"# How to Use Sklearn Pipelines For Ridiculously Neat Code\n## Everything I love about Scikit-learn, all in one place\n<img src='https://cdn-images-1.medium.com/max/1200/1*fLcObhT_JaC-WD8FkvT3JQ.jpeg'></img>\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https://www.pexels.com/@abhiram2244?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Abhiram Prakash</a>\n        on \n        <a href='https://www.pexels.com/photo/man-sitting-on-edge-facing-sunset-915972/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels</a>\n    </strong>\n</figcaption>"},{"metadata":{},"cell_type":"markdown","source":"### Setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Why Do You Need a Pipeline?"},{"metadata":{},"cell_type":"markdown","source":"Data cleaning and preparation is easily the most time-consuming and boring task in machine learning. All ML algorithms are really fussy, some want normalized or standardized features, some want encoded variables and some want both. Then, there is also the issue of missing values which is always there.\n\nDealing with them is no fun at all, not to mention the added bonus that comes with repeating the same cleaning operations on all training, validation and test sets. Fortunately, Scikit-learn's `Pipeline` is a major productivity tool to facilitate this process, cleaning up code and collapsing all preprocessing and modeling steps into to a single line of code. Here, check this out:"},{"metadata":{},"cell_type":"markdown","source":"```python\n# Fit lasso regression\npipe_lasso.fit(X_train, y_train)\n\n# Predict on X_Test\npreds = pipe_lasso.predict(X_test)\n\n```"},{"metadata":{},"cell_type":"markdown","source":"Above, `pipe_lasso` is an instance of such pipeline where it fills the missing values in `X_train` as well as feature scale the numerical columns and one-hot encode categorical variables finishing up by fitting Lasso Regression. When you call `.predict` the same steps are applied to `X_test`, which is really awesome. "},{"metadata":{},"cell_type":"markdown","source":"Pipelines combine everything I love about Scikit-learn: conciseness, consistency and easy of use. So, without further ado, let me show how you can build your own pipeline in a few minutes."},{"metadata":{},"cell_type":"markdown","source":"### Intro to Scikit-learn Pipelines"},{"metadata":{},"cell_type":"markdown","source":"In this and coming sections, we will build the above `pipe_lasso` pipeline together for the [Ames Housing dataset](https://www.kaggle.com/c/home-data-for-ml-course/data) which is used for an [InClass competition](https://www.kaggle.com/c/home-data-for-ml-course/overview) on Kaggle. The dataset contains 81 variables on almost every aspect of a house and using these, you have to predict the house's price. Let's load the training and test sets:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\nX_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\ntrain.iloc[:, 70:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Everything except for the last column - `SalePrice` is used as features. Before we do anything, let's divide up the training data into train, validation sets. We will use the final `X_test` set for predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = train.drop('SalePrice', axis=1)\ny = train.SalePrice\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.3, random_state=1121218)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# All categorical columns\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_train[col]) == set(X_valid[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n\n# Drop categorical columns that will not be encoded\nX_train = X_train.drop(bad_label_cols, axis=1)\nX_valid = X_valid.drop(bad_label_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's do basic exploration of the training set:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe().T.iloc[:10] # All numerical cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.describe(include=np.object).T.iloc[:10] # All object cols","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"above_0_missing = X_train.isnull().sum() > 0\n\nX_train.isnull().sum()[above_0_missing]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"19 features have NaNs. "},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features = X_train.select_dtypes(include='number').columns.tolist()\nprint(f'There are {len(numerical_features)} numerical features:', '\\n')\nprint(numerical_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = X_train.select_dtypes(exclude='number').columns.tolist()\nprint(f'There are {len(categorical_features)} categorical features:', '\\n')\nprint(categorical_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, on to preprocessing. For numeric columns, we first fill the missing values with `SimpleImputer` using the mean and feature scale using `MinMaxScaler`. For categoricals, we will use `SimpleImputer` to fill the missing values with the mode of each column. Most importantly, we do all of these in a pipeline. Let's import everything:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create two small pipelines for both numeric and categorical features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='mean')),\n    ('scale', MinMaxScaler())\n])\n\ncategorical_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('one-hot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Set `handle_unknown` to `ignore` to skip previously unseen labels. Otherwise, `OneHotEncoder` throws an error if there exists labels in test set that are not in train set."},{"metadata":{},"cell_type":"markdown","source":"[`sklearn.pipeline.Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class takes a tuple of transformers for its `steps` argument. Each tuple should have this pattern:"},{"metadata":{},"cell_type":"markdown","source":"```\n('name_of_transformer`, transformer)\n```"},{"metadata":{},"cell_type":"markdown","source":"Then, each tuple is called a *step* containing a transformer like `SimpleImputer` and arbitrary name. Each step will be chained and applied to the passed DataFrame in the given order."},{"metadata":{},"cell_type":"markdown","source":"But, these two pipelines are useless if we don't tell which columns they should be applied to. For that, we will use another transformer - `ColumnTransformer`."},{"metadata":{},"cell_type":"markdown","source":"### Column Transformer"},{"metadata":{},"cell_type":"markdown","source":"By default, all `Pipeline` objects have `fit` and `transform` methods which can be used to transform the input array like this:"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_pipeline.fit_transform(X_train.select_dtypes(include='number'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above, we are using the new numeric preprocessor on `X_train` using `fit_transform`. We are specifying the columns with `select_dtypes`. But, using the pipelines in this way means we have to call each pipeline separately on selected columns which is not what we want. What we want is to have a single preprocessor that is able to perform both numeric and categorical transformations in a single line of code like this:"},{"metadata":{},"cell_type":"markdown","source":"```python\nfull_processor.fit_transform(X_train)\n```"},{"metadata":{},"cell_type":"markdown","source":"To achieve this, we will use `ColumnTransformer` class:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nfull_processor = ColumnTransformer(transformers=[\n    ('number', numeric_pipeline, numerical_features),\n    ('category', categorical_pipeline, categorical_features)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Remember that `numerical_features` and `categorical_features` contain the respective names of columns from `X_train`."},{"metadata":{},"cell_type":"markdown","source":"Similar to `Pipeline` class, `ColumnTransformer` takes a tuple of transformers. Each tuple should contain an arbitrary step name, the transformer itself and the list of column names that the transformer should be applied to. Here, we are creating a column transformer with 2 steps using both of our numeric and categorical preprocessing pipelines. Now, we can use it to fully transform the `X_train`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"full_processor.fit_transform(X_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that most transformers return `numpy` arrays which means index and column names will be dropped. \n\nFinally, we managed to collapse all preprocessing steps into a single line of code. However, we can go even further. We can combine preprocessing and modeling to have even neater code."},{"metadata":{},"cell_type":"markdown","source":"### Final Pipeline With an Estimator"},{"metadata":{},"cell_type":"markdown","source":"Adding an estimator (model) to a pipeline is as easy as creating a new pipeline which contains the above column transformer and the model itself. Let's import and instantiate `LassoRegression` and add it to a new pipeline with the `full_processor`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_absolute_error\n\nlasso = Lasso(alpha=0.1)\n\nlasso_pipeline = Pipeline(steps=[\n    ('preprocess', full_processor),\n    ('model', lasso)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Warning! The order of steps matter! The estimator should always be the last step for the pipeline to work correctly."},{"metadata":{},"cell_type":"markdown","source":"That's it! We can now call `lasso_pipeline` just like we call any other model. When we call `.fit`, the pipeline applies all transformations before fitting an estimator:"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = lasso_pipeline.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's evaluate our base model on the validation set (Remember, we have a separate testing set which we haven't touched so far):"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = lasso_pipeline.predict(X_valid)\nmean_absolute_error(y_valid, preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_pipeline.score(X_valid, y_valid)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great, our base pipeline works. Another great thing about pipelines is that they can be treated as any other model. In other words, we can plug it into anywhere where we would use Scikit-learn estimators. So, we will use the pipeline in a grid search to find the optimal hyperparameters in the next section."},{"metadata":{},"cell_type":"markdown","source":"### Using Your Pipeline Everywhere"},{"metadata":{},"cell_type":"markdown","source":"The main hyperparameter for `Lasso` is alpha which can range from 0 to infinity. For simplicity, we will only cross-validate on the values within 0 and 1 with steps of 0.05:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_dict = {'model__alpha': np.arange(0, 1, 0.05)}\n\nsearch = GridSearchCV(lasso_pipeline, param_dict, \n                      cv=10, \n                      scoring='neg_mean_absolute_error')\n\n_ = search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can get the best score and parameters for `Lasso`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best score:', abs(search.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best alpha:', search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see, best `alpha` is 0.95 which is the very end of our given interval, i. e. \\[0, 1) with a step of 0.05. We need to search again in case the best parameter lies in a bigger interval:"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_dict = {'model__alpha': np.arange(1, 200, 5)}\n\nsearch = GridSearchCV(lasso_pipeline, param_dict, \n                      cv=10, \n                      scoring='neg_mean_absolute_error')\n\n_ = search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best score:', abs(search.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best alpha:', search.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With best hyperparameters, we get a significant drop in MAE (which is good). Let's redefine our pipeline with `Lasso(alpha=76)`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(alpha=181)\n\nfinal_lasso_pipe = Pipeline(steps=[\n    ('preprocess', full_processor),\n    ('model', lasso)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fit it to `X_train`, validate on `X_valid` and submit predictions for the competition using `X_test`:"},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = final_lasso_pipe.fit(X_train, y_train)\npreds = final_lasso_pipe.predict(X_valid)\n\nmean_absolute_error(y_valid, preds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Conclusion"},{"metadata":{},"cell_type":"markdown","source":"In summary, pipelines introduce several advantages to your daily workflow such as compact and fast code, ease of use and in-place modification of multiple steps. In the examples, we used simple Lasso regression but the pipeline we created could be used for virtually any model out there. Go and use it to build something awesome!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}