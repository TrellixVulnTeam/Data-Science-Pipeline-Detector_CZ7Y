{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Regressions\n\nIn this notebook, I will try to understand the regressions.  \nEspecially, I want to find some answers to my questions, like what is the math behind them, how to use them and when to use them.  ","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing libraries and loading datasets","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Modelling\nfrom sklearn.linear_model import LinearRegression","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:32.64958Z","iopub.execute_input":"2022-05-05T20:21:32.65041Z","iopub.status.idle":"2022-05-05T20:21:32.655617Z","shell.execute_reply.started":"2022-05-05T20:21:32.650358Z","shell.execute_reply":"2022-05-05T20:21:32.654533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col=0)\ntest_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv', index_col=0)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:21:32.675554Z","iopub.execute_input":"2022-05-05T20:21:32.67608Z","iopub.status.idle":"2022-05-05T20:21:32.732755Z","shell.execute_reply.started":"2022-05-05T20:21:32.676041Z","shell.execute_reply":"2022-05-05T20:21:32.732008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Explore data","metadata":{}},{"cell_type":"code","source":"train_data.tail()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:32.734246Z","iopub.execute_input":"2022-05-05T20:21:32.734609Z","iopub.status.idle":"2022-05-05T20:21:32.761618Z","shell.execute_reply.started":"2022-05-05T20:21:32.734578Z","shell.execute_reply":"2022-05-05T20:21:32.760704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:32.763023Z","iopub.execute_input":"2022-05-05T20:21:32.763663Z","iopub.status.idle":"2022-05-05T20:21:32.86105Z","shell.execute_reply.started":"2022-05-05T20:21:32.763615Z","shell.execute_reply":"2022-05-05T20:21:32.860197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Columns: \\n{0} \".format(train_data.columns.tolist()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:32.863026Z","iopub.execute_input":"2022-05-05T20:21:32.863363Z","iopub.status.idle":"2022-05-05T20:21:32.868839Z","shell.execute_reply.started":"2022-05-05T20:21:32.863329Z","shell.execute_reply":"2022-05-05T20:21:32.867533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Basic data check","metadata":{}},{"cell_type":"code","source":"missing = train_data.isna()\npercent = (missing.sum()/missing.count()*100).sort_values(ascending=False)\nmissing_columns = percent[percent > 0].index.tolist() # Any\nprint('Columns which have missing values: \\n{0}'.format(missing_columns))\n#missing_columns = percent[percent > 10].index.tolist() # More than 10 percent\n#print('Columns which have more than 10% missing values: \\n{0}'.format(missing_columns))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:32.87027Z","iopub.execute_input":"2022-05-05T20:21:32.870518Z","iopub.status.idle":"2022-05-05T20:21:32.898064Z","shell.execute_reply.started":"2022-05-05T20:21:32.870474Z","shell.execute_reply":"2022-05-05T20:21:32.897226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicates = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:32.899553Z","iopub.execute_input":"2022-05-05T20:21:32.900337Z","iopub.status.idle":"2022-05-05T20:21:32.930302Z","shell.execute_reply.started":"2022-05-05T20:21:32.900287Z","shell.execute_reply":"2022-05-05T20:21:32.929506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Taking care of the missing data\n\nIt looks like I won't need all of those columns anyways, at least for my purposes (we will see). So, there should be no harm removing them for now.","metadata":{}},{"cell_type":"code","source":"train_data.drop(missing_columns, axis=1, inplace=True)\ntest_data.drop(missing_columns, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:21:32.93139Z","iopub.execute_input":"2022-05-05T20:21:32.932236Z","iopub.status.idle":"2022-05-05T20:21:32.939999Z","shell.execute_reply.started":"2022-05-05T20:21:32.932171Z","shell.execute_reply":"2022-05-05T20:21:32.939301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:32.940992Z","iopub.execute_input":"2022-05-05T20:21:32.94166Z","iopub.status.idle":"2022-05-05T20:21:32.97665Z","shell.execute_reply.started":"2022-05-05T20:21:32.941621Z","shell.execute_reply":"2022-05-05T20:21:32.975712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Regressions\n\nRegression models are used to estimate the relationship between a dependent variable (target) and one or more independent variables (features).  \nThe most common one is the linear regression model, which uses only one independent variable and corresponds to a straight line that most closely fits the data according to a mathematical criterion.  \nWhile linear regression models use a straight line, logistic and nonlinear regression models use a curved line.\n\n\nRegression analysis and models are mainly used for two purposes.\n* Prediction and forecasting.\n* To understand relationships between the independent and dependent variables.\n\n\n**References**  \nhttps://en.wikipedia.org/wiki/Regression_analysis  \nhttps://corporatefinanceinstitute.com/resources/knowledge/finance/regression-analysis/  ","metadata":{}},{"cell_type":"markdown","source":"# 6.1 Simple Linear Regression\n\nA simple linear regression model determines the relationship between a dependent variable and an independent variable.\n\nIt is expressed using the equation:  \n$Y = \\beta_0 + \\beta_1X_1 + \\epsilon$\n\nwhere:  \n$Y$ - Dependent variable  \n$X_1$ - Independent variable  \n$\\beta_0$ - Intercept  \n$\\beta_1$ - Slope  \n$\\epsilon$ - Residual (error)  \n\n**References**  \nhttps://www.scribbr.com/statistics/simple-linear-regression/  \nhttps://www.imsl.com/blog/what-is-regression-model  \nhttps://online.stat.psu.edu/stat462/node/91/","metadata":{}},{"cell_type":"code","source":"X = train_data[['OverallQual']]\ny = train_data['SalePrice']\n\nregressor = LinearRegression()\nregressor.fit(X, y)\ny_pred = regressor.predict(X)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:21:32.98065Z","iopub.execute_input":"2022-05-05T20:21:32.981185Z","iopub.status.idle":"2022-05-05T20:21:32.993963Z","shell.execute_reply.started":"2022-05-05T20:21:32.981146Z","shell.execute_reply":"2022-05-05T20:21:32.992871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(13, 6))\nplt.scatter(X, y, color='blue')\nplt.plot(X, y_pred, color='red')\nplt.title(\"Sale Price vs Overall Quality\\n\" +\n          \"Equation: Y = {0:.2f} + {1:.2f}X₁\".format(regressor.intercept_, regressor.coef_[0]))\nplt.xlabel(\"Overall Quality\")\nplt.ylabel(\"Sale Price\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:32.995436Z","iopub.execute_input":"2022-05-05T20:21:32.995849Z","iopub.status.idle":"2022-05-05T20:21:33.231753Z","shell.execute_reply.started":"2022-05-05T20:21:32.995817Z","shell.execute_reply":"2022-05-05T20:21:33.23065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## How does linear regression find the best fitting line?\n\nThe best fitting line is a straight line that represents the best approximation of the given data.  \nThe difference between the actual (observed) value and the predicted value for any data point is known as residual error.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(13, 6))\nplt.scatter(X, y, color='blue')\nplt.plot(X, y_pred, color='red', label=\"best fitting line\")\nax.hlines(y, X-0.2, X+0.2, color='green', label=\"residuals\")\nax.vlines(X, y, y_pred, color='green')\nplt.title(\"Sale Price vs Overall Quality\")\nplt.xlabel(\"Overall Quality\")\nplt.ylabel(\"Sale Price\")\nplt.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:33.233387Z","iopub.execute_input":"2022-05-05T20:21:33.233731Z","iopub.status.idle":"2022-05-05T20:21:33.6365Z","shell.execute_reply.started":"2022-05-05T20:21:33.23368Z","shell.execute_reply":"2022-05-05T20:21:33.635388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best fitting line is the one which has the smallest possible residual errors in the overall sense.  \nRegression analysis uses the “least squares method” to generate the best fitting line.  \nIt makes the sum of the squared prediction errors the smallest it can be.  \n\nFor each i-th point in the data set,  \n\n$\n\\begin{align}\nY_i &= \\beta_0 + \\beta_1X_i \\\\\n\\epsilon_i &= y_i - Y_i \\\\\nQ &= \\sum_{i=1}^{n} (y_i - Y_i)^2 \\\\\nQ &= \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1X_i))^2 \\\\\n\\end{align}\n$\n\nThe least squares estimates,  \n\n$\n\\begin{align}\n\\beta_0 &= \\overline{y} - \\beta_1\\overline{x} \\\\ \n\\beta_1 &= \\frac{\\sum_{i=1}^{n}{(x_i - \\overline{x})(y_i - \\overline{y})}}{\\sum_{i=1}^{n}{(x_i - \\overline{x})^2}}\n\\end{align}\n$\n\nwhere:  \n$y_i$ - Actual value  \n$Y_i$ - Predicted value  \n$Q$ - Residual Sum of Squares\n\n**References**  \nhttps://statisticsbyjim.com/glossary/ordinary-least-squares/  \nhttps://www.numpyninja.com/post/what-is-line-of-best-fit-in-linear-regression  \nhttps://medium.com/@rndayala/linear-regression-a00514bc45b0  \nhttps://online.stat.psu.edu/stat501/lesson/1/1.2  \nhttps://www.immagic.com/eLibrary/ARCHIVES/GENERAL/WIKIPEDI/W120529O.pdf  ","metadata":{}},{"cell_type":"markdown","source":"## Assumptions of Single Linear Regression\n\nThese assumptions are important conditions which should be met before the model is used to make the predictions.  \nIf these assumptions are violated, the results may be misleading.\n\nStill, it will be a waste of time to check those assumptions every time working on a new dataset. It is much easier to try the model (possibly multiple models) and see its accuracy. When the dataset has linear relationships, the model will give higher accuracy than the other models, and it is enough evidence that the assumptions are satisfied. Although the dataset doesn't have any linear relationships, a linear regression model can still be used. It will just perform poorly and will have an accuracy lower than the other models. Remember that, when searching for the most appropriate model for the dataset and for better predictions, they will be helpful.\n\nSince in this notebook, I am trying to understand the math. I will check all of them.","metadata":{}},{"cell_type":"code","source":"from statsmodels.formula.api import ols\nfit = ols(formula='SalePrice~OverallQual', data=train_data).fit()\npredictions = fit.predict()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-05-05T20:21:33.637735Z","iopub.execute_input":"2022-05-05T20:21:33.638013Z","iopub.status.idle":"2022-05-05T20:21:33.651338Z","shell.execute_reply.started":"2022-05-05T20:21:33.637981Z","shell.execute_reply":"2022-05-05T20:21:33.650298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linearity\n\nThere should be a linear relationship between the dependent variable and the independent variable. A straight line should be able to represent all points as well as possible.  \nThis assumption is easy to test with a scatter plot.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(13, 6))\nplt.scatter(X, y, color='blue')\nplt.plot(X, predictions, color='red')\nplt.title(\"Sale Price vs Overall Quality\")\nplt.xlabel(\"Overall Quality\")\nplt.ylabel(\"Sale Price\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:33.652461Z","iopub.execute_input":"2022-05-05T20:21:33.653359Z","iopub.status.idle":"2022-05-05T20:21:33.826424Z","shell.execute_reply.started":"2022-05-05T20:21:33.653317Z","shell.execute_reply":"2022-05-05T20:21:33.825389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normality\n\nThe residuals must be normally distributed.  \nIt is possible to understand normality by looking at the residuals histogram or using the p-value from the [Kolmogorov-Smirnov test](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.kstest_normal.html) for normality.\n\n> If the p-value is lower than some threshold, e.g. 0.05, then we can reject the Null hypothesis that the sample comes from a normal distribution.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(13, 6))\nsns.histplot(fit.resid, kde=True, color='blue')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:33.830188Z","iopub.execute_input":"2022-05-05T20:21:33.830535Z","iopub.status.idle":"2022-05-05T20:21:34.141331Z","shell.execute_reply.started":"2022-05-05T20:21:33.83048Z","shell.execute_reply":"2022-05-05T20:21:34.140091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.stats.diagnostic import kstest_normal\nlabels = ['Kolmogorov-Smirnov statistic', 'p-value']\ntest = kstest_normal(fit.resid)\nprint(dict(zip(labels, test)))\nprint(\"Since p-value is lower than 0.05, the assumption is satisfied.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:34.142874Z","iopub.execute_input":"2022-05-05T20:21:34.143199Z","iopub.status.idle":"2022-05-05T20:21:34.153378Z","shell.execute_reply.started":"2022-05-05T20:21:34.143154Z","shell.execute_reply":"2022-05-05T20:21:34.152365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Independence\n\nThe residuals should be independent. There should be no correlation between the consecutive residuals.  \nAutocorrelation is a characteristic of data in which the correlation between the values of the same variables is based on related objects. It violates the assumption of independence.  \nWe will perform a [Durbin-Watson test](https://www.statsmodels.org/stable/generated/statsmodels.stats.stattools.durbin_watson.html) to determine if either positive or negative correlation is present.\n\n> The test statistic is approximately equal to 2*(1-r) where r is the sample autocorrelation of the residuals.  \n> Thus, for r == 0, indicating no serial correlation, the test statistic equals 2.  \n> This statistic will always be between 0 and 4.  \n> The closer to 0 the statistic, the more evidence for positive serial correlation.  \n> The closer to 4, the more evidence for negative serial correlation.  ","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats.stattools import durbin_watson\ntest = durbin_watson(fit.resid)\nprint({'Durbin-Watson statistic': test})\nprint(\"Since statistic is almost 2, this assumption is also satisfied.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:34.155119Z","iopub.execute_input":"2022-05-05T20:21:34.155846Z","iopub.status.idle":"2022-05-05T20:21:34.166617Z","shell.execute_reply.started":"2022-05-05T20:21:34.1558Z","shell.execute_reply":"2022-05-05T20:21:34.165923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Homoscedasticity\n\nThe residuals must have constant variance.  \nHeteroscedasticity, the violation of homoscedasticity, occurs when there is no constant variance across the residuals.\n\nIt is possible to plot the residuals and see if the variance appears to be uniform or using the [Breusch-Pagan test](https://www.statsmodels.org/devel/generated/statsmodels.stats.diagnostic.het_breuschpagan.html) for test heteroscedasticity.\n\n> Statistics provides two p-values, Lagrange Multiplier and F test (widely used and basically equivalent).  \n> Heteroscedasticity is indicated if p-value < 0.05.  \n> https://en.wikipedia.org/wiki/Breusch%E2%80%93Pagan_test","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(13, 6))\nplt.scatter(x=predictions, y=fit.resid, color='blue')\nplt.title(\"Residuals vs Fitted\")\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Residuals\")\nplt.show()  ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:34.167868Z","iopub.execute_input":"2022-05-05T20:21:34.168833Z","iopub.status.idle":"2022-05-05T20:21:34.589707Z","shell.execute_reply.started":"2022-05-05T20:21:34.168767Z","shell.execute_reply":"2022-05-05T20:21:34.588457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.stats.diagnostic import het_breuschpagan\nlabels = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\ntest = het_breuschpagan(fit.resid, fit.model.exog)\nprint(dict(zip(labels, test)))\nprint(\"Since p-value is lower than 0.05, heteroscedasticity is assumed. Hence, model actually doesn't satify the assumption.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:34.591222Z","iopub.execute_input":"2022-05-05T20:21:34.591571Z","iopub.status.idle":"2022-05-05T20:21:34.601397Z","shell.execute_reply.started":"2022-05-05T20:21:34.591523Z","shell.execute_reply":"2022-05-05T20:21:34.600342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**References**  \nhttps://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression4.html  \nhttps://www.restore.ac.uk/srme/www/fac/soc/wie/research-new/srme/modules/mod2/6/index.html  \nhttps://www.statology.org/linear-regression-assumptions/  \nhttps://www.statsmodels.org/stable/stats.html","metadata":{}},{"cell_type":"markdown","source":"# 6.2 Multiple Linear Regression\n\nDifferent from simple linear regression models, multiple linear regression models use multiple independent variables that may affect the target variable.   \n\nModel is expressed using the equation:  \n$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2  + ... +  \\beta_nX_n + \\epsilon$\n\nwhere:  \n$Y$ – Dependent variable  \n$X_1$, $X_2$, $X_3$ – Independent variables  \n$\\beta_0$ – Intercept  \n$\\beta_1$, $\\beta_2$, $\\beta_3$ – Slopes  \n$\\epsilon$ – Residual (error)  \n\n**References**  \nhttps://www.imsl.com/blog/what-is-regression-model  \nhttps://corporatefinanceinstitute.com/resources/knowledge/finance/regression-analysis/  \nhttps://towardsdatascience.com/simple-and-multiple-linear-regression-with-python-c9ab422ec29c  ","metadata":{}},{"cell_type":"code","source":"X = train_data[['OverallQual', 'OverallCond']]\ny = train_data['SalePrice']\n\nregressor = LinearRegression()\nregressor.fit(X, y);","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:21:34.603174Z","iopub.execute_input":"2022-05-05T20:21:34.603972Z","iopub.status.idle":"2022-05-05T20:21:34.617167Z","shell.execute_reply.started":"2022-05-05T20:21:34.603888Z","shell.execute_reply":"2022-05-05T20:21:34.615886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OverallQual = X.values[:, 0]\nOverallCond = X.values[:, 1]\nSalePrice = y\n\nx_surf, y_surf = np.meshgrid(\n    np.linspace(OverallQual.min(), OverallQual.max(), 10),\n    np.linspace(OverallCond.min(), OverallCond.max(), 10))\nx_surf = x_surf.flatten()\ny_surf = y_surf.flatten()\ny_pred = regressor.predict(np.array([x_surf, y_surf]).T)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:21:34.618652Z","iopub.execute_input":"2022-05-05T20:21:34.619522Z","iopub.status.idle":"2022-05-05T20:21:34.633447Z","shell.execute_reply.started":"2022-05-05T20:21:34.61947Z","shell.execute_reply":"2022-05-05T20:21:34.632404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(14, 14))\nax = plt.axes(projection='3d')\nax.scatter(OverallQual, OverallCond, SalePrice, color='blue')\nax.plot_trisurf(x_surf, y_surf, y_pred, color='red', alpha=0.5)\nax.set_xlabel('Overall Quality', fontsize=12)\nax.set_ylabel('Overall Condition', fontsize=12)\nax.set_zlabel('Sale Price', fontsize=12)\nax.tick_params(axis='both', labelsize=8)\nax.view_init(elev=15, azim=120)\nplt.title(\"Sale Price vs Overall Quality and Overall Condition\\n\" +\n          \"Equation: Y = {0:.2f} + {1:.2f}X₁ + {0:.2f}X₂\".format(regressor.intercept_, regressor.coef_[0], regressor.coef_[1]))\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:34.637225Z","iopub.execute_input":"2022-05-05T20:21:34.637956Z","iopub.status.idle":"2022-05-05T20:21:34.997163Z","shell.execute_reply.started":"2022-05-05T20:21:34.63791Z","shell.execute_reply":"2022-05-05T20:21:34.996232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Assumptions of Multiple Linear Regression\n\nThere are five assumptions of the multiple linear regression. Again, when those assumptions are violated the results may be misleading.  \n\n* Linearity: There should be a linear relationship between the dependent variable and each independent variable.\n* Normality: The residuals must be normally distributed.\n* Independence: The residuals should be independent.\n* Homoscedasticity: The residuals must have constant variance.\n* No Multicollinearity: None of the independent variables are highly correlated with each other.\n\nIt is possible to use the model and compare its accuracy with the other models. When the multiple linear regression model has the higher accuracy than other models, it is possible to say that the dataset has some linear relationships and the assumptions are satisfied. Still, to find out the most appropriate model for the dataset, checking assumptions will be helpful.\n\nI have already tried the first four assumptions (for simple linear regression). So, I will check for the last one to understand the details.\n\n**References**  \nhttps://www.statology.org/multiple-linear-regression-assumptions/  \nhttps://jeffmacaluso.github.io/post/LinearRegressionAssumptions/  ","metadata":{}},{"cell_type":"markdown","source":"### No Multicollinearity\n\nIndependent variables shouldn't show multicollinearity, which occurs when the they are highly correlated.  \nIt is possible to test this assumption by plotting an heatmap of the correlations and examine the [Variance Inflation Factors (VIF)](https://www.statsmodels.org/dev/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html).\n\n**References**  \nhttps://corporatefinanceinstitute.com/resources/knowledge/other/multiple-linear-regression/  \nhttps://stackoverflow.com/questions/42658379/variance-inflation-factor-in-python  \nhttps://www.statology.org/how-to-calculate-vif-in-python/  ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (10,8))\nsns.heatmap(X.corr(), annot=True)\nplt.title('Correlation of Variables')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:34.998758Z","iopub.execute_input":"2022-05-05T20:21:34.99913Z","iopub.status.idle":"2022-05-05T20:21:35.234298Z","shell.execute_reply.started":"2022-05-05T20:21:34.999092Z","shell.execute_reply":"2022-05-05T20:21:35.233636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nX_constant = add_constant(X)\nVIF = [variance_inflation_factor(X_constant.values, i) for i in range(X_constant.shape[1])]\nprint(dict(zip(X_constant.columns, VIF)))\nprint(\"Since VIF is lower than 5 for the independent variables `OverallQual` and `OverallCond`, the assumption is satisfied.\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:35.235319Z","iopub.execute_input":"2022-05-05T20:21:35.235912Z","iopub.status.idle":"2022-05-05T20:21:35.248499Z","shell.execute_reply.started":"2022-05-05T20:21:35.235873Z","shell.execute_reply":"2022-05-05T20:21:35.247492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.3 Polynomial Regression\n\nLike simple linear regression, polynomial regression determines the relationship between a dependent variable and an independent variable.  \nDifferent than simple linear regression, it is modeled as an nth degree polynomial; hence it fits a non-linear relationship between an independent and a dependent variable.  \n\nModel is expressed using the equation:  \n$Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_1^2  + ... +  \\beta_nX_1^n + \\epsilon$\n\nwhere:  \n$Y$ – Dependent variable  \n$X_1$ – Independent variable   \n$\\beta_0$ – Intercept  \n$\\beta_1$, $\\beta_2$ – Slopes  \n$\\epsilon$ – Residual (error)  \n\n**References**  \nhttps://en.wikipedia.org/wiki/Polynomial_regression  \nhttps://towardsdatascience.com/polynomial-regression-bbe8b9d97491  \nhttps://www.w3schools.com/python/python_ml_polynomial_regression.asp  ","metadata":{}},{"cell_type":"markdown","source":"## Comparison between Polynomial and Simple Linear regression\n\nPolynomial regression helps fit the best line to non-linear data.  \nTo understand it easily, I will generate two models, one with simple linear regression and one with polynomial regression.  \nBy comparing the curves it will be easier to see which model will generate the best fitting curve.  ","metadata":{}},{"cell_type":"code","source":"X = train_data[['OverallQual']]\ny = train_data['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:21:35.249776Z","iopub.execute_input":"2022-05-05T20:21:35.250184Z","iopub.status.idle":"2022-05-05T20:21:35.25769Z","shell.execute_reply.started":"2022-05-05T20:21:35.250152Z","shell.execute_reply":"2022-05-05T20:21:35.25701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simple Linear Regression","metadata":{}},{"cell_type":"code","source":"linear = LinearRegression()\nlinear.fit(X, y)\ny_pred_linear = linear.predict(X)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:21:35.259216Z","iopub.execute_input":"2022-05-05T20:21:35.259753Z","iopub.status.idle":"2022-05-05T20:21:35.275602Z","shell.execute_reply.started":"2022-05-05T20:21:35.259708Z","shell.execute_reply":"2022-05-05T20:21:35.274855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(13, 6))\nplt.scatter(X, y, color='blue')\nplt.plot(X, y_pred_linear, color='red')\nplt.title(\"Sale Price vs Overall Quality (Linear Regression)\" +\n          \"\\nEquation: Y = {0:.2f} + {1:.2f}X₁\".format(linear.intercept_, linear.coef_[0]))\nplt.xlabel(\"Overall Quality\")\nplt.ylabel(\"Sale Price\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:35.27693Z","iopub.execute_input":"2022-05-05T20:21:35.277277Z","iopub.status.idle":"2022-05-05T20:21:35.513583Z","shell.execute_reply.started":"2022-05-05T20:21:35.277248Z","shell.execute_reply":"2022-05-05T20:21:35.512691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Polynomial Regression\n\nTo generate polynomial features, I will use [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class provided by scikit-learn.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=4)\nX_poly = poly.fit_transform(X)\n\npolynomial = LinearRegression()\npolynomial.fit(X_poly, y)\n\nX_grid = np.linspace(X.min(), X.max(), 10)\nX_poly = poly.fit_transform(X_grid)\ny_pred_polynomial = polynomial.predict(X_poly)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:21:35.514965Z","iopub.execute_input":"2022-05-05T20:21:35.515451Z","iopub.status.idle":"2022-05-05T20:21:35.529608Z","shell.execute_reply.started":"2022-05-05T20:21:35.515408Z","shell.execute_reply":"2022-05-05T20:21:35.5284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(13, 6))\nplt.scatter(X, y, color='blue')\nplt.plot(X_grid, y_pred_polynomial, color='red')\nplt.title(\"Sale Price vs Overall Quality (Polynomial Regression)\" +\n          \"\\nEquation: Y = {0:.2f} + {1:.2f}X₁ + {2:.2f}X₁² + {3:.2f}X₁³ + {4:.2f}X₁⁴\".format(\n            polynomial.intercept_, polynomial.coef_[1], polynomial.coef_[2], polynomial.coef_[3], polynomial.coef_[4]))\nplt.xlabel(\"Overall Quality\")\nplt.ylabel(\"Sale Price\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:35.531015Z","iopub.execute_input":"2022-05-05T20:21:35.531264Z","iopub.status.idle":"2022-05-05T20:21:35.769985Z","shell.execute_reply.started":"2022-05-05T20:21:35.531228Z","shell.execute_reply":"2022-05-05T20:21:35.769103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.4 Support Vector Regression","metadata":{}},{"cell_type":"code","source":"X = train_data[['OverallQual']]\ny = train_data['SalePrice'].values.reshape(-1, 1)\n\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX = sc_X.fit_transform(X)\ny = sc_y.fit_transform(y)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:21:35.771176Z","iopub.execute_input":"2022-05-05T20:21:35.771395Z","iopub.status.idle":"2022-05-05T20:21:35.783068Z","shell.execute_reply.started":"2022-05-05T20:21:35.771369Z","shell.execute_reply":"2022-05-05T20:21:35.781935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVR\nregressor = SVR(kernel='rbf')\nregressor.fit(X, y.ravel())\n\nX_grid = np.linspace(X.min(), X.max(), 10).reshape(10, 1)\ny_pred = regressor.predict(X_grid)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:21:35.784451Z","iopub.execute_input":"2022-05-05T20:21:35.784717Z","iopub.status.idle":"2022-05-05T20:21:35.909448Z","shell.execute_reply.started":"2022-05-05T20:21:35.784686Z","shell.execute_reply":"2022-05-05T20:21:35.908267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(13, 6))\nplt.scatter(sc_X.inverse_transform(X), sc_y.inverse_transform(y), color='blue')\nplt.plot(sc_X.inverse_transform(X_grid), sc_y.inverse_transform(y_pred), color='red')\nplt.title(\"Sale Price vs Overall Quality\")\nplt.xlabel(\"Overall Quality\")\nplt.ylabel(\"Sale Price\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:35.9109Z","iopub.execute_input":"2022-05-05T20:21:35.911755Z","iopub.status.idle":"2022-05-05T20:21:36.140173Z","shell.execute_reply.started":"2022-05-05T20:21:35.911704Z","shell.execute_reply":"2022-05-05T20:21:36.139168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.5 Decision Tree Regression","metadata":{}},{"cell_type":"code","source":"X = train_data[['OverallQual']]\ny = train_data['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:23:27.451261Z","iopub.execute_input":"2022-05-05T20:23:27.451939Z","iopub.status.idle":"2022-05-05T20:23:27.457037Z","shell.execute_reply.started":"2022-05-05T20:23:27.451905Z","shell.execute_reply":"2022-05-05T20:23:27.455934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=1)\nregressor.fit(X, y)\n\nX_grid = np.linspace(X.min(), X.max(), 10).reshape(10, 1)\ny_pred = regressor.predict(X_grid)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:23:28.27465Z","iopub.execute_input":"2022-05-05T20:23:28.275601Z","iopub.status.idle":"2022-05-05T20:23:28.287039Z","shell.execute_reply.started":"2022-05-05T20:23:28.275557Z","shell.execute_reply":"2022-05-05T20:23:28.28599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(13, 6))\nplt.plot(X_grid, y_pred, color='red')\nplt.scatter(X, y, color='blue')\nplt.title(\"Sale Price vs Overall Quality\")\nplt.xlabel(\"Overall Quality\")\nplt.ylabel(\"Sale Price\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:21:36.168293Z","iopub.execute_input":"2022-05-05T20:21:36.168724Z","iopub.status.idle":"2022-05-05T20:21:36.393954Z","shell.execute_reply.started":"2022-05-05T20:21:36.16869Z","shell.execute_reply":"2022-05-05T20:21:36.39278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6.6 Random Forest Regression","metadata":{}},{"cell_type":"code","source":"X = train_data[['OverallQual']]\ny = train_data['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:21:36.39535Z","iopub.execute_input":"2022-05-05T20:21:36.395671Z","iopub.status.idle":"2022-05-05T20:21:36.401121Z","shell.execute_reply.started":"2022-05-05T20:21:36.395626Z","shell.execute_reply":"2022-05-05T20:21:36.400229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators=10, random_state=1)\nregressor.fit(X, y)\n\nX_grid = np.linspace(X.min(), X.max(), 10).reshape(10, 1)\ny_pred = regressor.predict(X_grid)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T20:22:58.35003Z","iopub.execute_input":"2022-05-05T20:22:58.350331Z","iopub.status.idle":"2022-05-05T20:22:58.38298Z","shell.execute_reply.started":"2022-05-05T20:22:58.350298Z","shell.execute_reply":"2022-05-05T20:22:58.382051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(13, 6))\nplt.plot(X_grid, y_pred, color='red')\nplt.scatter(X, y, color='blue')\nplt.title(\"Sale Price vs Overall Quality\")\nplt.xlabel(\"Overall Quality\")\nplt.ylabel(\"Sale Price\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-05T20:22:59.993095Z","iopub.execute_input":"2022-05-05T20:22:59.993376Z","iopub.status.idle":"2022-05-05T20:23:00.1736Z","shell.execute_reply.started":"2022-05-05T20:22:59.993346Z","shell.execute_reply":"2022-05-05T20:23:00.172709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WORK IN PROGRESS","metadata":{}}]}