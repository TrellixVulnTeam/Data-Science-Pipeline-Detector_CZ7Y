{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=\"https://treenewal.com/wp-content/uploads/2020/11/oak-tree-care.png\" alt=\"decision tree picture\"></center>\n\n# <center> üå≥Decision trees - classification and regression trees (CART)üå≥ </center> \n\n**What you can expect from this notebook:** This is a follow-up to my notebook [ml from scratch: neural network and GD-optimizers](https://www.kaggle.com/code/vincentbrunner/ml-from-scratch-neural-network-and-gd-optimizers/notebook). In this notebook classification and regression tree algorithms are **explained and implemented from scratch** and used on the titanic and avocado-prices datasets.<br>\n\nIf you're a beginner and intimidated by all the terms like entropy and information gain, I can assure you it looks way harder than it actually is. You'll manage it easilyüí™ <br>\n\nSpecial thanks to [Suraj Jha](https://www.kaggle.com/surajjha101) and [Ashwin Shetgaonkar](https://www.kaggle.com/ashwinshetgaonkar) for providing helpful feedback on my last notebook, as well as to [Mar√≠lia Prata](https://www.kaggle.com/mpwolke) for suggesting using competition data.\n\n<div class=\"alert alert-block alert-info\">üëâIf you're just interested in the complete, with comments documented implementation of a CART algorithm + post pruning based on nested objects using just numpy and the copy module, feel free to click on show hidden code: </div>","metadata":{}},{"cell_type":"code","source":"#  libraries used:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nclass DecisionTreeClassifier():\n    \n    def __init__(self, method=\"gini\", max_depth=50, min_samples_split=20, max_features=None):\n        self.method = method\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.max_features = max_features\n\n        self.root_node = None\n        self.depth = 0\n        \n    def calc_entropy(self, X):\n        weighted_average_of_props = 0\n        for categ in np.unique(X):\n            prop = np.where(X == categ, 1, 0).sum() / len(X)\n            weighted_average_of_props += prop * np.log10(prop)\n        return -1 * weighted_average_of_props\n    \n    def calc_gini(self, X):\n        square_props = 0\n        for categ in np.unique(X):\n            prop = np.where(X == categ, 1, 0).sum() / len(X)\n            square_props += prop ** 2\n        return 1 - square_props\n    \n    def information_gain(self, S, S_l, S_r):\n        #  if entropy is used as impurity measurement\n        if self.method == \"entropy\":\n            S_impurity = self.calc_entropy(S)\n            S_l_impurity = self.calc_entropy(S_l)\n            S_r_impurity = self.calc_entropy(S_r)\n            \n        #  if gini index is used as impurity measurement (default due to being less computing power intensiv)\n        elif self.method == \"gini\":\n            S_impurity = self.calc_gini(S)\n            S_l_impurity = self.calc_gini(S_l)\n            S_r_impurity = self.calc_gini(S_r)\n            \n        else:\n            raise ValueError(\"expected method to be 'gini' or 'entropy'\")\n        \n        return S_impurity - (S_l_impurity * (len(S_l) / len(S)) \n                             + S_r_impurity * (len(S_r) / len(S)))\n    \n    def get_best_split(self, X, y):\n        best_var, best_value, best_score = None, None, 0\n        \n        if self.max_features == None:\n            variables = range(X.shape[1])\n        else:\n            variables = np.random.choice([i for i in range(X.shape[1])], size=self.max_features, replace=False)\n        \n        for var_i in variables:\n            for value in np.unique(X[:-1, var_i]):\n                score = self.information_gain(y,                        # targe before split\n                                              y[X[:, var_i] <= value],  # targe after split left child\n                                              y[X[:, var_i] > value])   # targe after split right child\n                if score > best_score:\n                    best_var, best_value, best_score = var_i, value, score\n                    \n        return best_var, best_value\n    \n    def build_tree(self, X, y):\n        best_var, best_value = self.get_best_split(X, y)\n        \n        #  check wether to split or create leaf node\n        if (best_var == None)|(len(X) < self.min_samples_split)|(self.depth >= self.max_depth):\n            return Leaf(np.unique(y)[np.argmax(np.unique(y, return_counts=True)[1])])\n        else:\n            self.depth += 1\n            return Node(self.build_tree(X[X[:, best_var] <= best_value], y[X[:, best_var] <= best_value]), # left child\n                        self.build_tree(X[X[:, best_var] > best_value], y[X[:, best_var] > best_value]), # right child\n                        (best_var, best_value)) # save split\n    \n    def get_prediction(self, node, row):\n        if node.type == \"Node\":\n            if row[node.split[0]] > node.split[1]:\n                return self.get_prediction(node.right_child, row)\n            else:\n                return self.get_prediction(node.left_child, row)\n        else:\n            return node.value\n        \n    def fit(self, X, y):\n        self.depth = 0\n        self.root_node = self.build_tree(X, y)\n                    \n    def predict(self, X):\n        predictions = [self.get_prediction(self.root_node, row) for row in X]\n        return np.array(predictions)\n    \n    #  functions added for \"visualising\" structure:\n    def get_tree_struckture(self, node, prev_layer):\n        if node.type == \"Node\":\n            print(f\"walked node on depth lvl. {prev_layer+1}\")\n            return self.get_tree_struckture(node.left_child, prev_layer+1), self.get_tree_struckture(node.right_child, prev_layer+1)\n        else:\n            print(f\"walked leaf on depth lvl. {prev_layer+1} with the value {node.value}\")\n                \n    def struckture(self):\n        self.get_tree_struckture(self.root_node, 0)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-19T17:05:22.376089Z","iopub.execute_input":"2022-05-19T17:05:22.377183Z","iopub.status.idle":"2022-05-19T17:05:22.423542Z","shell.execute_reply.started":"2022-05-19T17:05:22.377011Z","shell.execute_reply":"2022-05-19T17:05:22.422492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <center>Table of Contents üìÑ</center>\nThis notebook goes through all the principles neccessary to understand and code a classification or regression tree. The resulting models will be tested on actual datasets (no pseudo datasets this time^^).\n\n<p style=\"font-family: Verdana; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#1.\" style=\"color:#318a11\">1. basic intuition behind decision trees</a></p>\n\n<p style=\"font-family: Verdana; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#2.\" style=\"color:#318a11\">2. evaluating a split:</a></p>\n\n<p style=\"text-indent:10px; font-family: Verdana; font-size: 14px; letter-spacing: 2px; line-height:1.3\"><a href=\"#2.1.\" style=\"color:#318a11\">2.1. entropy:</a></p>\n\n<p style=\"text-indent:10px; font-family: Verdana; font-size: 14px; letter-spacing: 2px; line-height:1.3\"><a href=\"#2.2.\" style=\"color:#318a11\">2.2. gini index:</a></p>\n\n<p style=\"text-indent:10px; font-family: Verdana; font-size: 14px; letter-spacing: 2px; line-height:1.3\"><a href=\"#2.3.\" style=\"color:#318a11\">2.3. information gain:</a></p>\n\n<p style=\"font-family: Verdana; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#3.\" style=\"color:#318a11\">3. greedy splitting</a></p>\n\n<p style=\"font-family: Verdana; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#4.\" style=\"color:#318a11\">4. building a classification tree</a></p>\n\n<p style=\"font-family: Verdana; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#5.\" style=\"color:#318a11\">5. how to prevent overfitting</a></p>\n\n<p style=\"font-family: Verdana; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#6.\" style=\"color:#318a11\">6. putting it all together: <strong>classification on titanic dataset</strong></a></p>\n\n<p style=\"font-family: Verdana; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#7.\" style=\"color:#318a11\">7. how to use decision trees for regression</a></p>\n\n<p style=\"font-family: Verdana; font-size: 16px; font-weight: bold; letter-spacing: 2px; line-height:1.3\"><a href=\"#8.\" style=\"color:#318a11\">8. putting it all together: <strong>regression on house-price dataset</strong></a></p>\n\n<br>\n\n##### <div class=\"alert alert-block alert-info\">‚ö†Ô∏è <strong>Important:</strong> since making own visualisations would be too time-consuming for a first notebook I mainly embedded images from <strong>google image search</strong>. If you should find you're image here and <strong>want it to be removed</strong> please leave a comment or <strong>contact me</strong>. </div>","metadata":{}},{"cell_type":"markdown","source":"****\n<p id=\"1.\"></p>\n\n# <center>1. basic intuition behind decision trees</center>\nScince decision trees might just be the most intuitive ml model there is (after linear regression mabey), most of what it does can be understood by just looking at it's struckture:\n<center><img src=\"https://eloquentarduino.github.io/wp-content/uploads/2020/08/DecisionTree.png\" style=\"width:auto; height:500px\" alt=\"eloquentarduino.github.io\"></center>\n<center>image source: eloquentarduino.github.io</center>\n<br>\nStarting from the top, questions of binary nature are asked and a datapoint follows the path to the next question based on the answer to the previous question. In the example tree the first question is \"Age over 30?\", so if an input sample has the entry 45 in the age variable, it travels to the right.\n<details> \n  <summary>There another question is asked: \"No. of children < 2?\". Let's assume the input sample has entry 1 in the num. children variable, <strong>which path would it travel on next?</strong> (click to see answer) </summary>\n   <center><strong>‚Üí it travels along the left parts and results into the block \"Get Loan\"</strong></center>\n</details>\n<br>\n\nThis is the output of the tree/the prediction it makes. -> aaannd that's a decision tree, see totally easyüëå<br>\n\n**important points about the structure to take away:**\n* a decision tree is a ***chain of questions*** that separate the tree into ***subtrees***\n* separating data into two groups like that is called a ***split***\n* these splits are performed at ***nodes*** (blue blocks that ask questions)\n* at the end of such a chain of splits ***leaves or terminal nodes*** (blue blocks that output a statement) output the predicted value\n* --> a decision tree makes predictions by splitting data points into subgroups based on variable values\n\n**how these trees are built in order to output fitting predictions:**\n* to fit a tree/build it to predict a target variable, a whole dataset is used, not just a single sample\n* tree construction starts at the top node, also called the ***root node***\n* there the question is asked/the ***dataset gets split***, that separates the data best based on the target variable\n    * consider the datapoints with target [+, +, -, +, -, -, ¬∞, ¬∞]\n    * a good split would be [+, +, +] [-, -, -, ¬∞, ¬∞ ]\n* now that the data is split into 2 groups (the left one is called ***left child*** and the right one ***right child***), each group again can be split, this is repeated till every subgroup consists of data points that have the same target. \n* There a ***leaf/terminal node is created*** with that target value\n* if we now pass a sample through the tree, it gets \"sorted\" into one of these \"subgroups\" and given that the data is not random, it (probably) has the same target as that group<br>\n\n****\n<p id=\"2.\"></p>\n\n# <center>2. evaluating a split üßê</center>\nThe crucial part of building a decision tree is deciding at which variable and which value to split/which questions to ask when. To do that we need some way to evaluate a split/tell which split is better than another one. This subchapter will cover exactly that. So to start out, let's take a closer look at how splitting works. Below you see a dataset with 2 feature variables and one target/label variable. You can use the interactive elements to get an idea of how splitting the data according to criteria affects the distribution of the target variable:","metadata":{}},{"cell_type":"code","source":"data = {\"var1\":np.array([1, 2, 5, 3, 2, 1, 4]),     \n        \"var2\":np.array([5, 34, 23, 3, 42, 13, 54]), \n        \"target\":np.array([\"red\", \"blue\", \"red\", \"red\", \"blue\", \"blue\", \"red\"])}\npd.DataFrame(data).head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T17:05:24.338928Z","iopub.execute_input":"2022-05-19T17:05:24.339574Z","iopub.status.idle":"2022-05-19T17:05:24.368631Z","shell.execute_reply.started":"2022-05-19T17:05:24.33954Z","shell.execute_reply":"2022-05-19T17:05:24.367853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ipywidgets import widgets\n\nsplit = []\n#  function to visualise splits\ndef visualise_split(split, ax):\n    x = [i for i in range(len(split[0]))]+[i+len(split[0])+1 for i in range(len(split[1]))]    \n    split_total = np.concatenate([split[0],split[1]])\n    colors_split = np.where(split_total==\"blue\", [\"#1c46ff\" for i in range(len(split_total))], [\"#ff3333\" for i in range(len(split_total))])\n    ax.scatter(x, [0 for i in range(len(x))], 2000, color=colors_split)\n    ax.axvline(len(split[0]), color=\"black\")\n    ax.axes.xaxis.set_visible(False)\n    ax.axes.yaxis.set_visible(False)\n    ax.set_ylim(-0.01, 0.01)\n    ax.set_xlim(-1, len(split_total)+1)\n\ndef make_split(var, value):\n    split = [data[\"target\"][data[var] <= value], data[\"target\"][data[var] > value]]\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 4))\n\n    befor_split = data[\"target\"]\n    colors_split = np.where(befor_split==\"blue\", [\"#1c46ff\" for i in range(len(befor_split))], [\"#ff3333\" for i in range(len(befor_split))])\n    ax1 = plt.subplot(2, 3, 2)\n    ax1.scatter(range(len(befor_split)), [0 for i in range(len(befor_split))], 2000, color=colors_split)\n    ax1.axes.xaxis.set_visible(False)\n    ax1.axes.yaxis.set_visible(False)\n    ax1.set_ylim(-0.01, 0.01)\n    ax1.set_xlim(-1, len(befor_split))\n    ax1.set_title(\"target before split\", fontdict={'fontsize': 20})\n    \n    visualise_split(split, ax2)\n    ax2.set_title(\"target after split\", fontdict={'fontsize': 20})\n    \n    fig.tight_layout() \n    plt.show()\n\n# found a tutorial on getting the ipywidgets to work on kaggle here: https://www.kaggle.com/code/atorabi/intro-to-ipywidgets/notebook\nvariable_selector = widgets.Select(description=\"\", options=[\"var1\", \"var2\"], layout = widgets.Layout(width=\"50px\", height=\"50px\"))\nvalue_slider = widgets.SelectionSlider(options=np.unique(np.concatenate([data[\"var1\"], data[\"var2\"]])))\n\nw = widgets.interactive_output(make_split, {\"var\": variable_selector, \"value\": value_slider})\nwidgets.HBox(children = [variable_selector, widgets.VBox([value_slider, w])])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T17:05:25.119284Z","iopub.execute_input":"2022-05-19T17:05:25.119521Z","iopub.status.idle":"2022-05-19T17:05:25.539996Z","shell.execute_reply.started":"2022-05-19T17:05:25.119492Z","shell.execute_reply":"2022-05-19T17:05:25.539173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### <div class=\"alert alert-block alert-info\">‚ö†Ô∏è <strong>Important:</strong> After making this interactive visualisation I discovered that it <strong>doesn't work in viewer mode</strong>. Still when <strong>forking and going into edit mode</strong> the visualisation get's updated.</div>\n\nNow that the principle of splitting is hopfully cristal clear, three specific splits will be considered to see what makes a good and what makes a bad split.","metadata":{}},{"cell_type":"code","source":"#  and prepare 3 sample splits\nsplit1 = [data[\"target\"][data[\"var2\"] >= 12], data[\"target\"][data[\"var2\"] < 12]] # variable: var2, value: 12\nsplit2 = [data[\"target\"][data[\"var1\"] >= 3], data[\"target\"][data[\"var1\"] < 3]] # variable: var1, value: 3\nsplit3 = [data[\"target\"][data[\"var2\"] >= 20], data[\"target\"][data[\"var2\"] < 20]] # variable: var1, value: 4\n\nprint(\"sample splits:\\n\")\n#  visualising splits\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 1))\n\nvisualise_split(split1, ax1)\nax1.set_title(\"split 1: variable = var2, value = 12\", fontdict={'fontsize': 20})\nvisualise_split(split2, ax2)\nax2.set_title(\"split 2: variable = var1, value = 3\", fontdict={'fontsize': 20})\nvisualise_split(split3, ax3)\nax3.set_title(\"split 3: variable = var1, value = 4\", fontdict={'fontsize': 20})\n\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-05-19T17:05:25.588964Z","iopub.execute_input":"2022-05-19T17:05:25.589211Z","iopub.status.idle":"2022-05-19T17:05:25.757899Z","shell.execute_reply.started":"2022-05-19T17:05:25.589187Z","shell.execute_reply":"2022-05-19T17:05:25.757317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<details> \n  <summary>Consider the splits above, from what was covered till now, <strong>which split would be the best?</strong> (result in the optimal children, regarding the target) -> click to see answer</summary>\n   <center><strong>‚Üí split 2 results in groups that each contain nearly perfect homogenous target categories, this is refered to as <u>purity</u></strong></center>\n</details>\n<br>\n\n**How impure a group is, is determined by the homogeneity of its target/label:**\n* split 1: medium impurity\n* split 2: low impurity\n* split 3: high impurity\n\nSince the original goal of building a decision tree, is to split the data till it's divided into subgroups that share the same target, ***the lower the impurity, the better***.<br>\n\nYou can also think of it as **uncertainty**: high impurity (split 3) equals high uncertainty (take a random sample from the group, how certain are you about the target value of that sample?). Low impurity (split 2) equals low uncertainty (consider the groups split 2 results in: taking a random sample from one of the groups and knowing from which group it came, you can almost certainly know its target value)\n\n**so the goal of splitting is to reduce impurity and with that uncertainty.** Having said all that, it becomes clear what tools are needed to determine how good a split is:\n* an impurity/uncertainty measure\n* a formula to determine how much the whole split reduced impurity/uncertainty\n\nthe 2 most common impurity/uncertainty measures together with that formula will be explained and implemented in the following 3 subchapters.\n\n**impurity measure preface:**<br>\nsince we have looked at impurity on a quite abstract level till now, let's formulate it in more mathematical terms:\n* impurity/uncertainty is high if the categories are evenly spread -> ***high impurity ‚âô equal probabilities***\n* impurity/uncertainty is low if a category is \"dominating\" -> ***low impurity ‚âô high probability of one category (low probabilities of all others)***\n\nSo let's look at a binary example, and more specific the probabilitie of +: \n* [+++++,-] ‚âô low impurity -> high probability of +\n* [+++,---] ‚âô high impurity -> equal probability of + (and -)\n* [+,-----] ‚âô low impurity -> low probability of + (but due to that a high probability of -)\n\nSo ***an impurity measure must put out high values for equal probabilities and low values for unequal probabilities***\n\n##### <div class=\"alert alert-block alert-info\">‚ö†Ô∏è <strong>Attention:</strong> The next section will provide formulas and intuitions for them but to explain them <strong>mathematicaly precise</strong>, I'd have to fill the whole notebook with just one concept. So I suggest looking into other recources and skipping the next 3 subchapters <strong>if you wan't everything to be perfectly detailed and formaly/officialy right.</strong></div>","metadata":{}},{"cell_type":"markdown","source":"****\n\n<p id=\"2.1.\"></p>\n\n## <center>2.1. entropy</center>\n\n**Entropy** is one of the more important concepts in data science, so even tho the measure described in the next subchapter (Gini index) is more computationally efficient and therefore more often used for decision trees, entropy will be covered first in this notebook. Its formula is: <br>\n\n$\\large H=-\\sum p(x)*log(p(x))\\>\\>\\>\\>\\>$   or less often writen as:  $\\large \\>\\>\\>\\>\\> H=\\sum p(x)*log(p(\\frac{1}{x}))$<br>\n\nwhat this formula does, part for part will be discussed in this section together with 3 different perspectives to look at it.<br>\n\n#### **entropy as impurity measure:**<br>\nThe formula might look confusing at first but let's analyse it part for part:\n1. the whole term gets multiplied by -1 (why is covered later)\n2. a sum is taken over each ***category*** of the target variable\n3. the term that's calculated for each category can be further split up into 2 parts\n    * $p(x)$ this is the probability of the category occurring (the proportion as probability estimate). \n    * $log(p(x))$ \n    * -> All probabilities of all classes sum up to 1, so you can imagine this part as ***weighted average*** of $log(p(x))$ over all the categories \n\nLet's take a closer look at the $log(p(x))$ term:\n<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/17/Binary_logarithm_plot_with_ticks.svg/1200px-Binary_logarithm_plot_with_ticks.svg.png\" style=\"width:auto; height:300px\" alt=\"eloquentarduino.github.io\"></center>\n\nThe closer the probability of x is to 1, the closer $log(p(x))$ is to 0, the closer the probability of x is to 0, the closer $log(p(x))$ is to -infinity. So when taking the weighted average (weighing by the same probabilities), the following happens: low probabilities result in really low values for $log(p(x))$ but don't contribute much to the average due to being weighted by their low probability. High probabilities result in a value close to 0 for $log(p(x))$ and contribute much to the average.\n\n* So to achieve the lowest possible value with the $\\sum p(x)*log(p(x))$ term, all probabilities have to be equal.\n* And to achieve the value closest to 0, the probabilities have to be as unequal as possible (one high probability, small probabilities for all other categories)\n\n***sounds familiar?*** That's more or less what we wanted to achieve with an impurity measure in the first place, just that the value gets lower for impure and higher for pure values - a purity measure so to speak. But that's easy to correct **multiplying $\\sum p(x)*log(p(x))$ by -1 results in high values(close to 1) for impure data and low values(close to 0) for pure data**.\n\nLet's recall the **binary example** from earlier:<br>\n\nthe entropy formula can now be written as: $\\large H= -(p(x_1)*log(p(x_1)))-((1-p(x_1))*log(1-p(x_1)))$ where x_1 is one of the 2 categories. Plotting this as a graph would look like this:<br>\n\n<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/200px-Binary_entropy_plot.svg.png\"></center>\n\nso for the examples the entropy is:\n* [+++++,-] ‚âô rather low impurity -> entropy somewhere around 0.5 \n* [+++,---] ‚âô high impurity -> entropy is 1\n* [+,-----] ‚âô rather low impurity -> entropy  somewhere around 0.5 \n\n\n#### **entropy as uncertainty measure:**\n\nAs already mentioned in the last subchapter, you can view entropy as a measure of uncertainty. This is best demonstrated by looking at an example:\n* [+++++,-] ‚âô entropy somewhere around 0.5 -> you can be pretty certain that a random sample would be +, so uncertainty is rather low\n* [+++,---] ‚âô entropy is 1 -> you are completely uncertain if a random sample would be + or -, due to them having the same probability of occurring\n* [+,-----] ‚âô entropy somewhere around 0.5 -> you can be pretty certain that a random sample would be -, so uncertainty is rather low \n\n#### **entropy as information measure:**\n\nThis is the original viewpoint/approach to entropy and the one it was invented for by the godfather of information theory: Claude Shannon. The original intuition was that entropy calculates the amount of information in bits, that is conveyed on average when identifying the value of a random sample from a set of possible outcomes + probabilities.\n\nThe information content or surprisal of an event E is expressed by the term: $\\large I(E) = log_2(p(\\frac{1}{E}))$. If the probability of an event occuring is low, the amount of information conveyed when spotting its occurrence is high, if the probability of an event occurring is high, the amount of information conveyed when spotting its occurrence is low. It can be imagined as a pseudo-inverse of the probability<br>\n\nLooking at entropy when written as $\\large H=\\sum p(x)*log(p(\\frac{1}{x}))$ and taking into account that the expected value of a random variable is given by $\\large E=\\sum p(x_i)*x_i$, it becomes clear, that entropy is the **expected value of information content/suprisal.**\n\n<br>\n\nimplementation:","metadata":{}},{"cell_type":"code","source":"def calc_entropy(X):\n    weighted_average_of_props = 0\n    for categ in np.unique(X):\n        prop = np.where(X == categ, 1, 0).sum() / len(X)\n        weighted_average_of_props += prop * np.log2(prop) \n    return -1 * weighted_average_of_props\n\n# applied on our binary example:\nex1 = np.array([\"+\", \"+\", \"+\", \"+\", \"+\", \"-\"])\nex2 = np.array([\"+\", \"+\", \"+\", \"-\", \"-\", \"-\"])\nex3 = np.array([\"+\" ,\"-\", \"-\", \"-\", \"-\", \"-\"]) \n\nentropy_ex1 = calc_entropy(ex1)\nentropy_ex2 = calc_entropy(ex2)\nentropy_ex3 = calc_entropy(ex3)\n\nprint(f\"binary example:\\n\\n{ex1}: entropy = {entropy_ex1}\\n{ex2}: entropy = {entropy_ex2}\\n{ex3}: entropy = {entropy_ex3}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:26.524959Z","iopub.execute_input":"2022-05-19T17:05:26.5252Z","iopub.status.idle":"2022-05-19T17:05:26.534721Z","shell.execute_reply.started":"2022-05-19T17:05:26.525176Z","shell.execute_reply":"2022-05-19T17:05:26.533959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****\n\n<p id=\"2.2.\"></p>\n\n## <center>2.2. gini index</center>\n\nThe **Gini index** is given by $\\large gini=1-\\sum p(x)^2$ and has practically the same attributes as entropy just on a slightly different scale. That's why I won't go much further into details here.<br>\n\nStill let's at least take a look at the graph resulting from the binary example of the past, in comparison to entropy:\n\n<center><img src=\"https://cdn-images-1.medium.com/max/674/1*ovBMTgXvj3hmDHvqqxm87A.png\" style=\"width:auto; height:300px\" alt=\"medium.com\"></center>\n\n**main differences between gini and entropy:**\n* max value: gini 0.5 and entropy 1\n* computational efficience: entropy requires calculating a logarithm, therefore ***the gini index is more efficient***\n\n\n<br>\n\n\nimplementation:","metadata":{}},{"cell_type":"code","source":"def calc_gini(X):\n    square_props = 0\n    for categ in np.unique(X):\n        prop = np.where(X == categ, 1, 0).sum() / len(X)\n        square_props += prop ** 2\n    return 1 - square_props\n\n# applied on our binary example:\nex1 = np.array([\"+\", \"+\", \"+\", \"+\", \"+\", \"-\"])\nex2 = np.array([\"+\", \"+\", \"+\", \"-\", \"-\", \"-\"])\nex3 = np.array([\"+\" ,\"-\", \"-\", \"-\", \"-\", \"-\"]) \n\ngini_ex1 = calc_gini(ex1)\ngini_ex2 = calc_gini(ex2)\ngini_ex3 = calc_gini(ex3)\n\nprint(f\"binary example:\\n\\n{ex1}: gini index = {gini_ex1}\\n{ex2}: gini index = {gini_ex2}\\n{ex3}: gini index = {gini_ex3}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:27.593362Z","iopub.execute_input":"2022-05-19T17:05:27.593774Z","iopub.status.idle":"2022-05-19T17:05:27.601395Z","shell.execute_reply.started":"2022-05-19T17:05:27.593739Z","shell.execute_reply":"2022-05-19T17:05:27.600192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****\n\n<p id=\"2.3.\"></p>\n\n## <center>2.3. information gain</center>\n\nNow that the 2 most important uncertainty statistics were covered this subchapter will move back to the original question **how to evaluate a split using these statistics?**. <br>\n\nTo do that **two major problems** have to be solved:\n1. a split results in 2 children: somehow the impurity of both has to be considered \n2. To know how good a split is (in comparison to others) there has to be a measure of how much a split ***reduces impurity/uncertainty***\n\n<details> \n  <summary>Scince both problems can be solved quite easily, I'd suggest <strong>taking the time to think about solutions yourself</strong> before <strong>clicking here</strong> to see how information gain solves both</summary>\n   <ul>\n   <li>1. taking a <strong>weighted average</strong> of the seperate impurities of both children (weighted by the proportion of samples in the child(relative to data before splitting))</li>\n   <li>2. subtracting the weighted average of impurities after splitting from the impurity before splitting: <strong>by how much was impurity reduced by making that split?</strong>. This results in a value comparable over all splits: the split that reduces impurity/uncertainty the most in the best.</li>    \n   </ul>\n</details>\n<br>\n\nPut into a mathematic formula this results in: <br>\n\n$\\large Gain(S,A)=Entropy(S)-\\sum_{v\\>\\epsilon \\>val\\>A}\\frac{\\lvert S_v\\rvert}{\\lvert S\\rvert}Entropy(S_v)$\n\nlooks confusing at first but that's why the intuition was covered before: it does exactly what is mentioned above:\n* S always stands for a subset: $S$ is the original subset before splitting, $S_v$ is the subset v after splitting\n* the sum is taken over a term calculated on each S_v created by the split\n* the vertical lines around these subsets in the term $\\frac{\\lvert S_v\\rvert}{\\lvert S\\rvert}$ mean the *total number of samples in the subset*. \n* So the term $\\frac{\\lvert S_v\\rvert}{\\lvert S\\rvert}$ just calculates the proportion of S_v(the subset of a child) relative to the original subset S  -> this results in the previously discussed **weighted average**.\n* **note:** obviously the term weighted average is not the theoretical correct name, but in practice, it does basically that. \n* and that weighted average is taken over the impurity of the subset v: $Entropy(S_v)$ note: **you can plug in Gini instead of entropy in this formula**\n\n**why information gain?**<br>\nInformation gain is nothing but a synonym for the ***Kullback‚ÄìLeibler divergence*** of the distributions before and after splitting:<br>\n\n$IG(Y,X)=H(Y)-H(Y|X)$<br>\n\nIt is also called information gain due to it representing the amount of information \"gained\" when knowing the distribution of X in addition to the distribution of Y and looking at entropy from the original information theory perspective.","metadata":{}},{"cell_type":"code","source":"#  S : original subset, S_l : subset of left child, S_r : subset of right child\ndef information_gain(S, S_l, S_r, method):\n    #  if entropy is used as impurity measurement\n    if method == \"entropy\":\n        S_impurity = calc_entropy(S)\n        S_l_impurity = calc_entropy(S_l)\n        S_r_impurity = calc_entropy(S_r)\n\n    #  if gini index is used as impurity measurement (default due to being less computing power intensiv)\n    elif method == \"gini\":\n        S_impurity = calc_gini(S)\n        S_l_impurity = calc_gini(S_l)\n        S_r_impurity = calc_gini(S_r)\n\n    else:\n        raise ValueError(\"expected method to be 'gini' or 'entropy'\")\n    \n    return S_impurity - (S_l_impurity * (len(S_l) / len(S)) \n                        + S_r_impurity * (len(S_r) / len(S)))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:28.445433Z","iopub.execute_input":"2022-05-19T17:05:28.445768Z","iopub.status.idle":"2022-05-19T17:05:28.453735Z","shell.execute_reply.started":"2022-05-19T17:05:28.445743Z","shell.execute_reply":"2022-05-19T17:05:28.452226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With that concept implemented we are now able to **evaluate the splits** you saw earlier. This is done by calculating the information gain for each split:","metadata":{}},{"cell_type":"code","source":"original_subset = data[\"target\"]\n\ninf_gain1 = information_gain(original_subset, split1[0], split1[1], method=\"entropy\")\ninf_gain2 = information_gain(original_subset, split2[0], split2[1], method=\"entropy\")\ninf_gain3 = information_gain(original_subset, split3[0], split3[1], method=\"entropy\")\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 1))\n\nvisualise_split(split1, ax1)\nax1.set_title(\"split 1: variable = var2, value = 12, inf. gain = {:2f}\".format(inf_gain1), fontdict={'fontsize': 20})\nvisualise_split(split2, ax2)\nax2.set_title(\"split 2: variable = var1, value = 3, inf. gain = {:2f}\".format(inf_gain2), fontdict={'fontsize': 20})\nvisualise_split(split3, ax3)\nax3.set_title(\"split 3: variable = var1, value = 4, inf. gain = {:2f}\".format(inf_gain3), fontdict={'fontsize': 20})\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:29.198175Z","iopub.execute_input":"2022-05-19T17:05:29.198899Z","iopub.status.idle":"2022-05-19T17:05:29.37261Z","shell.execute_reply.started":"2022-05-19T17:05:29.198868Z","shell.execute_reply":"2022-05-19T17:05:29.372027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the splits can be ranked in the following order:<br>\n* split 2\n* split 1\n* split 3","metadata":{}},{"cell_type":"markdown","source":"****\n\n<p id=\"3.\"></p>\n\n# <center>3. greedy splitting</center>\n\nThe way the best split is found is often referred to as greedy splitting. This means considering every possible split and choosing the best one aka. the one that results in the highest information gain","metadata":{}},{"cell_type":"code","source":"class Spliter():\n    \n    def __init__(self, method=\"gini\"):\n        self.method = method\n        \n    def calc_entropy(self, X, sample_weights=None):\n        weighted_average_of_props = 0\n        for categ in np.unique(X):\n            prop = np.where(X == categ, 1, 0).sum() / len(X)\n            weighted_average_of_props += prop * np.log10(prop)\n        return -1 * weighted_average_of_props\n    \n    def calc_gini(self, X, sample_weights=None):\n        square_props = 0\n        for categ in np.unique(X):\n            prop = np.where(X == categ, 1, 0).sum() / len(X)\n            square_props += prop ** 2\n        return 1 - square_props\n    \n    def information_gain(self, S, S_l, S_r):\n        #  if entropy is used as impurity measurement\n        if self.method == \"entropy\":\n            S_impurity = self.calc_entropy(S)\n            S_l_impurity = self.calc_entropy(S_l)\n            S_r_impurity = self.calc_entropy(S_r)\n            \n        #  if gini index is used as impurity measurement (default due to being less computing power intensiv)\n        elif self.method == \"gini\":\n            S_impurity = self.calc_gini(S)\n            S_l_impurity = self.calc_gini(S_l)\n            S_r_impurity = self.calc_gini(S_r)\n            \n        else:\n            raise ValueError(\"expected method to be 'gini' or 'entropy'\")\n        \n        return S_impurity - (S_l_impurity * (len(S_l) / len(S)) \n                             + S_r_impurity * (len(S_r) / len(S)))\n    \n    def get_best_split(self, X, y):\n        best_var, best_value, best_score = None, None, 0\n        \n        variables = range(X.shape[1])\n        \n        #  iterate over each possible variable\n        for var_i in variables: \n            #  iterate over each possible value\n            for value in np.unique(X[:-1, var_i]):\n                score = self.information_gain(y,                        # targe before split\n                                              y[X[:, var_i] <= value],  # targe after split left child\n                                              y[X[:, var_i] > value])   # targe after split right child\n                \n                #  if the resulting split is better than the one before save it\n                if score > best_score:\n                    best_var, best_value, best_score = var_i, value, score\n                    \n        return best_var, best_value, best_score","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:30.618423Z","iopub.execute_input":"2022-05-19T17:05:30.618785Z","iopub.status.idle":"2022-05-19T17:05:30.630084Z","shell.execute_reply.started":"2022-05-19T17:05:30.618747Z","shell.execute_reply":"2022-05-19T17:05:30.628914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can not just evaluate the 3 given splits of the sample dataset, but find the best one:","metadata":{}},{"cell_type":"code","source":"splitter = Spliter()\n\nX = np.column_stack([data[\"var1\"], data[\"var2\"]])\ny = data[\"target\"]\n\nbest_var, best_value, best_score = splitter.get_best_split(X, y)\nsplit = (y[X[:, best_var] > best_value], y[X[:, best_var] <= best_value])\n\nfig, ax1 = plt.subplots(1, 1, figsize=(20, 2))\nvisualise_split(split, ax1)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:31.772889Z","iopub.execute_input":"2022-05-19T17:05:31.773132Z","iopub.status.idle":"2022-05-19T17:05:31.84291Z","shell.execute_reply.started":"2022-05-19T17:05:31.77311Z","shell.execute_reply":"2022-05-19T17:05:31.842139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****\n\n<p id=\"4.\"></p>\n\n# <center>4. building a classification tree üèóÔ∏è</center>\n\nIn this Section, the actual building/fitting process of a decision tree will be explained.\n\nThe main bullet point here is ***recursive splitting***.<br>\nFor anyone already having looked into recursive programming just a bit, this should be relatively straightforward. For everyone who hasn't, here is a short explanation of recursion:<br>\n* in contrast to iterative programming, no loops are used to repeatedly compute a pattern\n* instead, methods are used that recursively call themselves till some kind of exit/termination condition is fulfilled \n\nLet's recall how a decision tree is constructed:\n* the data repeatedly gets split into 2 subsets at the split resulting in the highest information gain\n* when no split is able to improve the uncertainty/results in an information gain higher than 0, a terminal node or leaf is created\n\nThis creates a pattern of splitting or creating leaves, that is repeatedly called on the subsets created by the last split. Before taking a look at the recursive implementation of this pattern, we have to determine some way of creating the tree-**struckture**. \n\nThis could be done by nesting lists or dictionaries, but also by nesting objects that represent the building blocks of a decision tree. I think that while the first one might be slightly more efficient/easier to save, the second one is more intuitive and easier to understand. So for this implementation, the following classes are going to be used:","metadata":{}},{"cell_type":"code","source":"#  these classes could be combined to one node class but I think it is easier to imagine when splitting into seperate node and leaf \"building-blocks\"\n\nclass Node():\n    def __init__(self, left_child, right_child, split, layer=None):\n        self.right_child = right_child\n        self.left_child = left_child\n        self.split = split\n        self.type = \"Node\"\n\nclass Leaf():\n    def __init__(self, value):\n        self.value = value\n        self.type = \"Leaf\"\n        ","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:32.537938Z","iopub.execute_input":"2022-05-19T17:05:32.538194Z","iopub.status.idle":"2022-05-19T17:05:32.543987Z","shell.execute_reply.started":"2022-05-19T17:05:32.53817Z","shell.execute_reply":"2022-05-19T17:05:32.543228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"They can be nested into each other by setting the left/right child of a node to another node which's left and right childs again contain nodes etc. till the leafs are reached. Combining this with the recursion idea, it results in something that would look like this:<br>","metadata":{}},{"cell_type":"code","source":"def build_tree(X, y):\n                best_var, best_value = get_best_split(X, y)\n\n                #  check wether to split or create leaf node\n                if (best_var == None):  # best var is None when information gain == 0 -> see subchapter information gain\n                    return Leaf(np.unique(y)[np.argmax(np.unique(y, return_counts=True)[1])])\n                else:\n                    return Node(build_tree(X[X[:, best_var] <= best_value], y[X[:, best_var] <= best_value]), # left child\n                                build_tree(X[X[:, best_var] > best_value], y[X[:, best_var] > best_value]), # right child\n                                (best_var, best_value)) # save split","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:33.427235Z","iopub.execute_input":"2022-05-19T17:05:33.42775Z","iopub.status.idle":"2022-05-19T17:05:33.434661Z","shell.execute_reply.started":"2022-05-19T17:05:33.427716Z","shell.execute_reply":"2022-05-19T17:05:33.434068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the children of the returned node get set to the same function of the corresponding subset the split results in. With that recursive element every node's children get set to Nodes calculated on the subsets resulting from the split etc. \n\nSo when calling build_tree() and saving it some kind of root node variable, the whole structure gets built and nested into this variable.","metadata":{}},{"cell_type":"markdown","source":"****\n\n<p id=\"5.\"></p>\n\n# <center>5. how to prevent overfitting</center>\n\nAt this point **everything needed to implement a decision tree was covered** but there is one small detail that would ruin the success of the model: overfitting. Till now the algorithm continues to split the data **till an additional split wouldn't result in a decrease of uncertainty/impurity**. This creates a model with **large variance** and small bias, meaning it **works often nearly perfect on the training data** but ***overfits*** on the test data. \n\n> The model hasn't learned to solve the **general problem** but to specifically classify the data points from the training data by increasing its variance till the chain of questions separates the training data nearly perfect but doesn't work on the real problem anymore\n\nTo prevent overfitting from happening, the variance of the model has to be reduced while keeping the bias as low as possible. The most important method to do this with decision trees is called ***pruning***. Pruning seeks to reduce the complexity/size of a decision tree by reducing the number of nodes and leaves in the tree. The idea is to cut or prevent the creation of nodes/leaves non-critical to the actual problem.\n\nPruning can be further split(üòâü•¥) into 2 categories: \n* **pre-pruning** (pruning while/before fitting the tree)\n* **post-pruning** (pruning after fitting the tree)\n\n#### **pre-pruning:**\n* prevents the creation of unnecessary leafs and nodes by introducing additional terminal/stop conditions to the recursive splitting algorithm. Some of the possible criteria/conditions are:\n    * maximal depth (the maximal number of nodes that are allowed to be created)\n    * minimal amount of samples in split (the minimal amount of samples to split at, if size of subset smaller/equal a leaf is created no matter the impurity)\n    * minimal amount of samples in leaf (self-explanatory)\n    * minimal gain to split (if maximal achievable information gain is below that threshold, a leaf is created)\n* is also referred to as the early stopping rule\n\nexample implementation of a few criteria:","metadata":{}},{"cell_type":"code","source":"max_depth = 20\nmin_samples_split = 20\n\ndepth = 0\ndef build_tree(X, y):\n    best_var, best_value = self.get_best_split(X, y)\n\n    #  check wether to split or create leaf node\n    if (best_var == None)|(len(X) < min_samples_split)|(depth >= max_depth):\n        return Leaf(np.unique(y)[np.argmax(np.unique(y, return_counts=True)[1])])\n    else:\n        depth += 1\n        return Node(self.build_tree(X[X[:, best_var] <= best_value], y[X[:, best_var] <= best_value]), # left child\n                    self.build_tree(X[X[:, best_var] > best_value], y[X[:, best_var] > best_value]), # right child\n                    (best_var, best_value)) # save split","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:34.488395Z","iopub.execute_input":"2022-05-19T17:05:34.489046Z","iopub.status.idle":"2022-05-19T17:05:34.496928Z","shell.execute_reply.started":"2022-05-19T17:05:34.489013Z","shell.execute_reply":"2022-05-19T17:05:34.496419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **post-pruning**\n* removes unnecessary nodes + leaves from the already constructed tree by utilising some algorithm\n* the probably most widely used algorithm for post-pruning is called **cost complexity pruning** where the best amount of cut nodes+leafs is determined by a cost function that weights the training error against the complexity\n\n**cost complexity pruning** is more elaborate to implement especially when using the system of nested objects, which this notebook goes for. So it can be covered in its own notebook in the future if there is any demand for that.","metadata":{}},{"cell_type":"markdown","source":"****\n\n<p id=\"6.\"></p>\n\n# <center>6. putting it all together: <strong>classification on titanic dataset</strong></center>\n\n**Finally enough was covered to make the final decision tree implementation üéâ.**\nThe implementations are going to be coded in a way that they can be used in similar fashion to the sklearn equivalents, so a fluid transition is as easy as possible. \n\n#### preparing the data\nLet's start out by loading the data, encoding the categorical features (cause the algorithm can just deal with numerical data), and splitting it into training and test set:","metadata":{}},{"cell_type":"code","source":"titanic_data = pd.read_csv(\"../input/titanic/train.csv\", usecols=[\"Survived\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]).dropna()\ntitanic_data_encoded = pd.get_dummies(titanic_data, columns=[\"Sex\", \"Embarked\"])\n\ntitanic_data_train = titanic_data_encoded.sample(frac=0.7)\ntitanic_data_test = titanic_data_encoded[~titanic_data_encoded.index.isin(titanic_data_train.index)]\ntitanic_data_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:35.957237Z","iopub.execute_input":"2022-05-19T17:05:35.95748Z","iopub.status.idle":"2022-05-19T17:05:36.004615Z","shell.execute_reply.started":"2022-05-19T17:05:35.957455Z","shell.execute_reply":"2022-05-19T17:05:36.003656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### creating the DecisionTreeClassifier class\nTo do this, all thats left to do, is to collect all the single concepts implemented so far and put them into a class, which results in:","metadata":{}},{"cell_type":"code","source":"class DecisionTreeClassifier():\n    \n    def __init__(self, method=\"gini\", max_depth=50, min_samples_split=20, max_features=None):\n        self.method = method\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.max_features = max_features\n\n        self.root_node = None\n        self.depth = 0\n        \n    def calc_entropy(self, X):\n        weighted_average_of_props = 0\n        for categ in np.unique(X):\n            prop = np.where(X == categ, 1, 0).sum() / len(X)\n            weighted_average_of_props += prop * np.log10(prop)\n        return -1 * weighted_average_of_props\n    \n    def calc_gini(self, X):\n        square_props = 0\n        for categ in np.unique(X):\n            prop = np.where(X == categ, 1, 0).sum() / len(X)\n            square_props += prop ** 2\n        return 1 - square_props\n    \n    def information_gain(self, S, S_l, S_r):\n        #  if entropy is used as impurity measurement\n        if self.method == \"entropy\":\n            S_impurity = self.calc_entropy(S)\n            S_l_impurity = self.calc_entropy(S_l)\n            S_r_impurity = self.calc_entropy(S_r)\n            \n        #  if gini index is used as impurity measurement (default due to being less computing power intensiv)\n        elif self.method == \"gini\":\n            S_impurity = self.calc_gini(S)\n            S_l_impurity = self.calc_gini(S_l)\n            S_r_impurity = self.calc_gini(S_r)\n            \n        else:\n            raise ValueError(\"expected method to be 'gini' or 'entropy'\")\n        \n        return S_impurity - (S_l_impurity * (len(S_l) / len(S)) \n                             + S_r_impurity * (len(S_r) / len(S)))\n    \n    def get_best_split(self, X, y):\n        best_var, best_value, best_score = None, None, 0\n        \n        if self.max_features == None:\n            variables = range(X.shape[1])\n        else:\n            variables = np.random.choice([i for i in range(X.shape[1])], size=self.max_features, replace=False)\n        \n        for var_i in variables:\n            for value in np.unique(X[:-1, var_i]):\n                score = self.information_gain(y,                        # targe before split\n                                              y[X[:, var_i] <= value],  # targe after split left child\n                                              y[X[:, var_i] > value])   # targe after split right child\n                if score > best_score:\n                    best_var, best_value, best_score = var_i, value, score\n                    \n        return best_var, best_value\n    \n    def build_tree(self, X, y):\n        best_var, best_value = self.get_best_split(X, y)\n        \n        #  check wether to split or create leaf node\n        if (best_var == None)|(len(X) < self.min_samples_split)|(self.depth >= self.max_depth):\n            return Leaf(np.unique(y)[np.argmax(np.unique(y, return_counts=True)[1])])\n        else:\n            self.depth += 1\n            return Node(self.build_tree(X[X[:, best_var] <= best_value], y[X[:, best_var] <= best_value]), # left child\n                        self.build_tree(X[X[:, best_var] > best_value], y[X[:, best_var] > best_value]), # right child\n                        (best_var, best_value)) # save split\n    \n    def get_prediction(self, node, row):\n        if node.type == \"Node\":\n            if row[node.split[0]] > node.split[1]:\n                return self.get_prediction(node.right_child, row)\n            else:\n                return self.get_prediction(node.left_child, row)\n        else:\n            return node.value\n        \n    def fit(self, X, y):\n        self.depth = 0\n        self.root_node = self.build_tree(X, y)\n                    \n    def predict(self, X):\n        predictions = [self.get_prediction(self.root_node, row) for row in X]\n        return np.array(predictions)\n    \n    #  functions added for \"visualising\" structure:\n    def get_tree_struckture(self, node, prev_layer):\n        if node.type == \"Node\":\n            print(f\"walked node on depth lvl. {prev_layer+1}\")\n            return self.get_tree_struckture(node.left_child, prev_layer+1), self.get_tree_struckture(node.right_child, prev_layer+1)\n        else:\n            print(f\"walked leaf on depth lvl. {prev_layer+1} with the value {node.value}\")\n                \n    def struckture(self):\n        self.get_tree_struckture(self.root_node, 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:36.888181Z","iopub.execute_input":"2022-05-19T17:05:36.888412Z","iopub.status.idle":"2022-05-19T17:05:36.908987Z","shell.execute_reply.started":"2022-05-19T17:05:36.888389Z","shell.execute_reply":"2022-05-19T17:05:36.908374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### fitting the classifier\nNow all that's left is to initialise a classifier object and fit it on the training data:","metadata":{}},{"cell_type":"code","source":"X_train = titanic_data_train.loc[:, titanic_data_train.columns!=\"Survived\"].to_numpy() # select everything but the target\ny_train = titanic_data_train.loc[:, \"Survived\"].to_numpy() # select the target\n\nclassifier = DecisionTreeClassifier()\nclassifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:38.089539Z","iopub.execute_input":"2022-05-19T17:05:38.089954Z","iopub.status.idle":"2022-05-19T17:05:38.46227Z","shell.execute_reply.started":"2022-05-19T17:05:38.089918Z","shell.execute_reply":"2022-05-19T17:05:38.461344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I added another recursive functions to walk the structure of the created tree. It's not that easy to read but it's at least something. The structure of the resulting tree looks like this:","metadata":{}},{"cell_type":"code","source":"classifier.struckture()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:39.415214Z","iopub.execute_input":"2022-05-19T17:05:39.415444Z","iopub.status.idle":"2022-05-19T17:05:39.431069Z","shell.execute_reply.started":"2022-05-19T17:05:39.41542Z","shell.execute_reply":"2022-05-19T17:05:39.430473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### evaluating the classifier\nNow the classifier can be evaluated on the test data:","metadata":{}},{"cell_type":"code","source":"X_test = titanic_data_test.loc[:, titanic_data_test.columns!=\"Survived\"].to_numpy() # select everything but the target\ny_test = titanic_data_test.loc[:, \"Survived\"].to_numpy() # select the target\n\n#  make predictions:\npredictions = classifier.predict(X_test)\n\n#  confusion matrix:\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ncm = confusion_matrix(y_test, predictions, labels=[0, 1])\ncm_displ = ConfusionMatrixDisplay(cm)\ncm_displ.plot()\nplt.show()\n\n#  calculate accuracy:\naccuracy = np.mean(predictions==y_test)\n\n#  calculate recall:\nrecall = cm[1, 1]/cm[1, :].sum() # of the total actual positives, how much were classified correctly\n\n#  calculate precision:\nprecision = cm[1, 1]/cm[:, 1].sum() # of all predicted positives, how much were True positives\n\n#  not that neccessary for this problem, but for the completeness:\nf1 = 2 * ((recall * precision)/(recall + precision)) \n\nprint(f\"accuracy = {accuracy},\\nrecall = {recall},\\nprecision = {precision},\\nf1-score = {f1}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T17:05:40.518998Z","iopub.execute_input":"2022-05-19T17:05:40.519306Z","iopub.status.idle":"2022-05-19T17:05:41.850221Z","shell.execute_reply.started":"2022-05-19T17:05:40.51928Z","shell.execute_reply":"2022-05-19T17:05:41.849321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### and there you go!\n**If you're a beginner I hope you have understood the concept of decision trees for classification by now. If yes I'd invite you to continue with the next subchapter where I'll try explaining how to use this concept for regression tasks. If no or that was enough till here I'd like to thank you for reading and wish you a great rest of your day!**","metadata":{}},{"cell_type":"markdown","source":"****\n\n<p id=\"7.\"></p>\n\n# <center>7. decision trees for regression </center>\n\nActually **not much changes**, when using decision trees for regression, cause all you do is predict values instead of categories. So you basically use the same classification structure but with many leaves, covering most of the value range of the target variable. Let's take a look at a regression problem:\n\n<center><img src=\"https://www.jcchouinard.com/wp-content/uploads/2021/09/image-70.png\"></center>\n<center> image source: jcchouinard.com</center>\n\nNote that instead of predicting values using a continuous line like in linear regression, the values are aggregated in main categories and then get predicted by the decision tree.\n\nTo modify the decision tree algorithm covered till here, in order to handle regression a few things have to be modified:\n\n* the function that is reduced: impurity for classification, ***sum of squared residuals*** for regression\n* the value of the leaf: mode for classification, ***mean*** for regression\n\nThe rest can basically stay the same, due to the regression problem being practically still treated as a large multiclass classification. Even the implemented **information gain** can be kept due to it, after replacing Gini with the sum of squared residuals, calculating how much a split reduces this error.","metadata":{}},{"cell_type":"code","source":"#  main changes:\n\n#  sum or squared residuals instead of gini or entropy\ndef calc_ssr(self, X, value):\n    sum_of_squared_residuals = np.square(X - value).sum()\n    return sum_of_squared_residuals\n\n#  assigning mean instead of mode to leaf\ndef placeholder():\n    return Leaf(y.mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-19T15:40:11.581519Z","iopub.execute_input":"2022-05-19T15:40:11.582434Z","iopub.status.idle":"2022-05-19T15:40:11.587472Z","shell.execute_reply.started":"2022-05-19T15:40:11.582397Z","shell.execute_reply":"2022-05-19T15:40:11.586581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****\n\n<p id=\"8.\"></p>\n\n# <center>8. putting it all together: <strong>regression on house-price dataset</strong></center>\n\nSo with that we can make the few neccessary changes to the DecionTreeClassifier class and turn it into a DecionTreeRegressor. But before that let's quickly prepare a regession dataset:\n\n#### preparing the data","metadata":{}},{"cell_type":"code","source":"house_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\nhouse_data_encoded = pd.get_dummies(house_data).dropna().reset_index(drop=True)\n\nhouse_data_train = house_data_encoded.sample(frac=0.7)\nhouse_data_test = house_data_encoded[~house_data_encoded.index.isin(house_data_train.index)]\nhouse_data_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:39:09.264449Z","iopub.execute_input":"2022-05-19T14:39:09.265267Z","iopub.status.idle":"2022-05-19T14:39:09.389843Z","shell.execute_reply.started":"2022-05-19T14:39:09.265224Z","shell.execute_reply":"2022-05-19T14:39:09.388997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### creating the DecisionTreeRegressor class\nThe DecisionTreeClassifier with the previously discussed modifications:","metadata":{}},{"cell_type":"code","source":"class DecisionTreeRegressor():\n    \n    def __init__(self, max_depth=500, min_samples_split=2, max_features=None):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split\n        self.max_features = max_features\n\n        self.root_node = None\n        self.depth = 0\n        \n    def calc_ssr(self, X, value):\n        sum_of_squared_residuals = np.square(X - value).sum()\n        return sum_of_squared_residuals\n    \n    def loss_reduction(self, S, S_l, S_r):  # not information gain anymore but basicaly the same\n        S_loss = self.calc_ssr(S, S.mean())\n        S_l_loss = self.calc_ssr(S_l, S_l.mean())\n        S_r_loss = self.calc_ssr(S_r, S_r.mean())\n        \n        return S_loss - (S_l_loss * (len(S_l) / len(S)) \n                         + S_r_loss * (len(S_r) / len(S)))\n    \n    def get_best_split(self, X, y):\n        best_var, best_value, best_score = None, None, 0\n        \n        if self.max_features == None:\n            variables = range(X.shape[1])\n        else:\n            variables = np.random.choice([i for i in range(X.shape[1])], size=self.max_features, replace=False)\n        \n        for var_i in variables:\n            for value in np.unique(X[:, var_i])[:-1]:\n                score = self.loss_reduction(y,                        # targe before split\n                                            y[X[:, var_i] <= value],  # targe after split left child\n                                            y[X[:, var_i] > value])   # targe after split right child\n                \n                if score > best_score:\n                    best_var, best_value, best_score = var_i, value, score\n\n        return best_var, best_value\n    \n    def build_tree(self, X, y):\n        best_var, best_value = self.get_best_split(X, y)\n        \n        #  check wether to split or create leaf node\n        if (best_var == None)|(len(X) < self.min_samples_split)|(self.depth >= self.max_depth):\n            return Leaf(y.mean())\n        else:\n            self.depth += 1\n            return Node(self.build_tree(X[X[:, best_var] <= best_value], y[X[:, best_var] <= best_value]), # left child\n                        self.build_tree(X[X[:, best_var] > best_value], y[X[:, best_var] > best_value]), # right child\n                        (best_var, best_value)) # save split\n    \n    def get_prediction(self, node, row):\n        try:\n            if row[node.split[0]] > node.split[1]:\n                return self.get_prediction(node.right_child, row)\n            else:\n                return self.get_prediction(node.left_child, row)\n        except AttributeError:\n            return node.value\n        \n    def fit(self, X, y):\n        self.depth = 0\n        self.root_node = self.build_tree(X, y)\n        \n    def predict(self, X):\n        predictions = [self.get_prediction(self.root_node, row) for row in X]\n        return np.array(predictions)\n    \n    #  functions added for \"visualising\" structure:\n    def get_tree_struckture(self, node, prev_layer):\n        if node.type == \"Node\":\n            print(f\"walked node on depth lvl. {prev_layer+1}\")\n            return self.get_tree_struckture(node.left_child, prev_layer+1), self.get_tree_struckture(node.right_child, prev_layer+1)\n        else:\n            print(f\"walked leaf on depth lvl. {prev_layer+1} with the value {node.value}\")\n                \n    def struckture(self):\n        self.get_tree_struckture(self.root_node, 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:52:50.104018Z","iopub.execute_input":"2022-05-19T14:52:50.104289Z","iopub.status.idle":"2022-05-19T14:52:50.126452Z","shell.execute_reply.started":"2022-05-19T14:52:50.104261Z","shell.execute_reply":"2022-05-19T14:52:50.12552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### fitting the regressor","metadata":{}},{"cell_type":"code","source":"X_train = house_data_train.loc[:, house_data_train.columns!=\"SalePrice\"].to_numpy() # select everything but the target\ny_train = house_data_train.loc[:, \"SalePrice\"].to_numpy() # select the target\n\nregressor = DecisionTreeRegressor(max_depth=1000, min_samples_split=2)\nregressor.fit(X_train, y_train) # will take a bit longer due to larger tree","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:52:51.334711Z","iopub.execute_input":"2022-05-19T14:52:51.335486Z","iopub.status.idle":"2022-05-19T14:53:06.513399Z","shell.execute_reply.started":"2022-05-19T14:52:51.335437Z","shell.execute_reply":"2022-05-19T14:53:06.512501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### evaluating the regressor\nRegression trees realy tend to easily overfit, which is visible when comparing train to test error:","metadata":{}},{"cell_type":"code","source":"X_test = house_data_test.loc[:, house_data_test.columns!=\"SalePrice\"].to_numpy() # select everything but the target\ny_test = house_data_test.loc[:, \"SalePrice\"].to_numpy() # select the target\n\n#  make predictions:\npredictions = regressor.predict(X_test)\n\n#  calculate rmse:\nrmse = np.sqrt(np.square(y_test - predictions)).mean()\n\n#  calculate R2:\nr2 = 1 - np.square(y_test - predictions).sum()/np.square(y_test - y_test.mean()).sum()\n\nprint(f\"testing data: root mean squared error = {rmse}\\nR-squared = {r2}\")\n\n#  make predictions:\npredictions_train = regressor.predict(X_train)\n\n#  calculate rmse:\nrmse_train = np.sqrt(np.square(y_train - predictions_train)).mean()\n\n#  calculate R2:\nr2_train = 1 - np.square(y_train - predictions_train).sum()/np.square(y_train - y_train.mean()).sum()\n\nprint(f\"training data: root mean squared error = {rmse_train}\\nR-squared = {r2_train}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:53:06.514892Z","iopub.execute_input":"2022-05-19T14:53:06.515099Z","iopub.status.idle":"2022-05-19T14:53:06.537314Z","shell.execute_reply.started":"2022-05-19T14:53:06.515074Z","shell.execute_reply":"2022-05-19T14:53:06.536409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So lets use the pre pruning parameter implemented earlyer to reduce overfitting:","metadata":{}},{"cell_type":"code","source":"regressor_2 = DecisionTreeRegressor(max_depth=150, min_samples_split=30)\nregressor_2.fit(X_train, y_train) # will take a bit longer due to larger tree","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:53:50.891633Z","iopub.execute_input":"2022-05-19T14:53:50.89194Z","iopub.status.idle":"2022-05-19T14:53:55.514946Z","shell.execute_reply.started":"2022-05-19T14:53:50.8919Z","shell.execute_reply":"2022-05-19T14:53:55.514064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  make predictions:\npredictions = regressor_2.predict(X_test)\n\n#  calculate rmse:\nrmse = np.sqrt(np.square(y_test - predictions)).mean()\n\n#  calculate R2:\nr2 = 1 - np.square(y_test - predictions).sum()/np.square(y_test - y_test.mean()).sum()\n\nprint(f\"testing data: root mean squared error = {rmse}\\nR-squared = {r2}\")\n\n#  make predictions:\npredictions_train = regressor_2.predict(X_train)\n\n#  calculate rmse:\nrmse_train = np.sqrt(np.square(y_train - predictions_train)).mean()\n\n#  calculate R2:\nr2_train = 1 - np.square(y_train - predictions_train).sum()/np.square(y_train - y_train.mean()).sum()\n\nprint(f\"training data: root mean squared error = {rmse_train}\\nR-squared = {r2_train}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-19T14:53:55.516402Z","iopub.execute_input":"2022-05-19T14:53:55.516639Z","iopub.status.idle":"2022-05-19T14:53:55.531826Z","shell.execute_reply.started":"2022-05-19T14:53:55.51661Z","shell.execute_reply":"2022-05-19T14:53:55.530944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As visible above the **training error increases while the test error decreases**. Still, the model is far from perfect. Here the already mentioned **post pruning** would probably help, but like already said, that's too much to cover in this notebook, would need its own one to explain in detail and implement.\n\n##### <div class=\"alert alert-block alert-info\">‚ö†Ô∏è <strong>Attention:</strong> Decision Trees are considered <strong>weak estimators</strong> and nearly never used on theire own. Still they create the base for most <strong>ensembling techniques</strong> that combine multiple weak estimators to achieve lower bias or variance depending on the method (p.e. Boosting as a technique to decrease bias). I plan on maybe making a notebook to ensembling techniques like random forrest, adaboost, gradient boosting + variants in the future so if there's any demand for that let me know.</div>","metadata":{}},{"cell_type":"markdown","source":"#### **and that's it for this nootebook, hope it helped someone, have a great day and happy learning! üëã**","metadata":{}}]}