{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><img src=https://quantdare.com/wp-content/uploads/2015/01/Random-forest-vs-simple-tree-1-800x420.jpg width=1000px alt=\"hilarious random forest visualisation\"></center>\n<center>don't tell me this isn't hilariously well-fitting to the topic, found it here: quantdare.com</center>\n\n# <center>Random forests‚öôÔ∏è</center>\n\n**What you can expect from this notebook:** Since I did a notebook on decision trees [here](https://www.kaggle.com/code/vincentbrunner/ml-from-scratch-decision-trees#1.-basic-intuition-behind-decision-trees), and decision tree's alone aren't much of a useful tool in most cases, I thought I just do something similar but with one of the most common ensembling algorithms which is ***based on decision trees***: random forests.\n\n<div class=\"alert alert-block alert-info\">üëâIf you're just interested in the complete, with comments documented implementation of a random forest regressor using just numpy and the copy module, feel free to click on show hidden code: </div>","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"#  used for the implementation of the algorithm\nimport numpy as np\nimport copy\n\nclass RandomForestRegressor:\n    def __init__(self, n_estimators, max_depth=None, min_samples_split=20, max_features=0.5, min_impurity_decrease=0):\n        self.n_estimators = n_estimators\n        #  with the max_features parameter the proportion of the randomly considered features at every split is determined\n        self.base_estimator = DecisionTreeRegressor(max_depth=max_depth, \n                                                    min_samples_split=min_samples_split, max_features=max_features, \n                                                    min_impurity_decrease=min_impurity_decrease)\n        self.estimators = None\n        \n    def fit(self, X, y):\n        self.estimators = []\n        #  repeat n times:\n        for estimator_i in range(self.n_estimators):\n            #  bootstrap the dataset\n            idx = np.random.randint(low=0, high=len(X), size=len(X))\n            X_bs = X[idx, :]\n            y_bs = y[idx]\n            \n            #  fit estimator on bootstraped sample\n            new_estimator = copy.copy(self.base_estimator)\n            new_estimator.fit(X_bs, y_bs)\n            \n            #  save estimator\n            self.estimators.append(new_estimator)\n    \n    def predict(self, X):\n        predictions = np.stack([estimator.predict(X) for estimator in self.estimators], axis=1)\n        return predictions.mean(axis=1)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-25T09:43:01.695933Z","iopub.execute_input":"2022-06-25T09:43:01.696416Z","iopub.status.idle":"2022-06-25T09:43:01.731777Z","shell.execute_reply.started":"2022-06-25T09:43:01.696321Z","shell.execute_reply":"2022-06-25T09:43:01.730799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****\n\n# intuition\nThe main idea behind the random forest algorithm is to **combine multiple decision trees** to make a **better estimator** than the individual decision trees, reducing the variance (but slightly increasing the bias).\n\n<center><img src=https://1.bp.blogspot.com/-Ax59WK4DE8w/YK6o9bt_9jI/AAAAAAAAEQA/9KbBf9cdL6kOFkJnU39aUn4m8ydThPenwCLcBGAsYHQ/s0/Random%2BForest%2B03.gif width=1000px></center>\n<center> image source: blog.tensorflow.org </center>\n<br>\n\nSince combining identical decision trees wouldn't make much sense, **randomness** is introduced to the construction of each tree. This is done in the following way:\n* training on a random selection of data points (around 2/3 of the original training set) -> this is done by bootstrapping/sampling from the training data with replacement\n* considering a random subset of features at each split while fitting the decision tree estimators\n\nTo make a prediction, each tree makes a prediction which then gets **aggregated** into one, final prediction.\n\nThis leads to a reduction in variance of the model compared to individual decision trees but also to a slight increase of the bias due to fitting the trees on less training data (**a more detailed explanation of the variance-reducing aspect of bagging can be found in the next section**)\n\n\n\n***this is the obvious strength of random forests compared to individual decision trees:*** decision trees are high variance models being very likely to overfit. By reducing the variance and still keeping the bias as low as possible the effectiveness of the model on testing data increases","metadata":{}},{"cell_type":"markdown","source":"****\n\n# Bagging: the ensembling method random forests are based on\n**Main goal: reducing variance**<br>\n**Combining estimators by aggregating results**\n\nBagging: ***bootstrap aggregation***<br>\n\nSteps:\n* Bootstrap sample (sample from training set with replacement) -> results in 2/3 of the data\n* Train estimator on bootstrapped sample\n* Aggregate predictions of estimators\n\n**Variance reducing aspect of Bagging:**\n* Assuming that all estimators have the same underlying probability distribution of predictions\n* Taking the prediction from each estimator and aggregating them is like aggregating a sample with sample size n where n is the number of estimators\n* This way the variance of the sample mean(the prediction of the bagging algorithm) can be written as $\\large SE^2=(\\frac{\\sigma}{\\sqrt{n}})^2=\\frac{\\sigma^2}{n}$ according to the central limit theorem\n* **As n/the number of estimators increases, the variance of the sample mean therefore shrinks and the sample mean becomes closer to the true population mean.**\n\n**Variance reducing aspect of Bagging taking the correlation of the estimators into account**(***not absolutely necessary***):<br>\n\n*Since the estimators are trained on the bootstraped samples which originate from the same trainingdata, there exists some amount of correlation between them.*\n* Therefore the formula above doesn't exactly describe the variance cause it assumes independance\n* The standart error can be ***roughly*** adjusted by multiplying by $\\large \\sqrt{\\frac{1+p}{1-p}}$ (not perfect but models it quite well)\n* so $\\large SE_{corrected}^2 = (\\frac{\\sigma\\sqrt{1+p}}{\\sqrt{n(1-p)}})^2 = \\frac{(1+p)\\sigma^2}{(1-p)n} = \\frac{p\\sigma^2+\\sigma^2}{(1-p)n}$ \n* $\\large \\lim \\limits_{p\\to1}\\frac{p\\sigma^2+\\sigma^2}{(1-p)n} = \\infty$\n* $\\large \\lim \\limits_{p\\to0}\\frac{p\\sigma^2+\\sigma^2}{(1-p)n} = \\frac{\\sigma^2}{n}$\n* This is quite intuitive: As the correlation decreases the variance becomes just the usual CLT formula for sampling variance. As the correlation becomes closer to 1, the variance becomes infinitely large, obviously that isn't actualy that way, cause the variance of the sample mean distribution can't get larger than the population variance. So actualy it **tends toward $\\sigma^2$**\n\n**This shows that in order to decrease variance with ensembling, the estimators have to be decorrelated as far as possible besides just ensembling a large amount of them**<br>\n-> that's why bagging bootstraps the training data\n","metadata":{}},{"cell_type":"markdown","source":"****\n\n# the algorithm step for step\n\nAs already mentioned in the last section, the random forest algorithm is a **bagging method**.<br>\n**To decorrelate the estimators further additional randomness is introduced in the training part of the individual decision trees.**\n\n**quick bagging recap:**\n* ***bagging = bootstrap aggregation***\n* **bootstrapping**: drawing samples from the training set like it would be the population -> generating \"new training sets\"\n    * then training multiple estimators on different bootstrap samples\n* **aggregation**: combining individual data points into one\n    * gets used to combine the individual predictions of the estimators\n    \nThe random forest uses that concept and adds an additional element of randomness to each individual tree by just considering a random group of n features at each split done in the decision tree (if you're not familiar with the concept of splitting in decision trees I'd recommend reading through the corresponding section in my notebook [\"ml from scratch: üå≥decision treesüå≥\"](https://www.kaggle.com/code/vincentbrunner/ml-from-scratch-decision-trees#1.-basic-intuition-behind-decision-trees)).\n\nSo the algorithm step for step looks something like the following (for regression):\n\n**initialise** $\\large M\\>\\epsilon\\>\\mathbb{N}$ <br>\n**initialise** $\\large n\\>\\epsilon\\>(1, N)$<br>\n\n* **for** $\\large \\>m...M$**:**<br>\n    1. **sample X from training set S with replacement (X ~ S) -> bootstrap sample**\n    2. **fit decision tree $\\large h_m(x)$ on bootstraped sample X considering n random features at each split** \n\n$\\large F(x) = \\frac{\\sum_{m=1}^Mh_m(x)}{M}$<br>\n\nwhere M is the number of estimators to be ensembled and n is the number of random features considered at each split.<br>\n\nFor classification, the predictions are aggregated by majority vote instead of the mean.","metadata":{}},{"cell_type":"markdown","source":"****\n\n# python implementation\n#### libraries used:","metadata":{}},{"cell_type":"code","source":"#  used for the implementation of the algorithm\nimport numpy as np # linear algebra \nimport copy # deep copies of objects -> estimators\n\n#  estimator to ensemble:\nfrom sklearn.tree import DecisionTreeRegressor\n\n#  used for data handeling\nimport pandas as pd # loading and transforming data\nfrom sklearn.model_selection import train_test_split # splitting data in train/test set","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:16:06.795238Z","iopub.execute_input":"2022-06-05T15:16:06.795765Z","iopub.status.idle":"2022-06-05T15:16:08.031954Z","shell.execute_reply.started":"2022-06-05T15:16:06.795708Z","shell.execute_reply":"2022-06-05T15:16:08.030958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### preparing the data:\nno large-scale feature engeneering but just preparing the data to work with the algorithm:\n* encode categories into numerical values:\n    * I went with label encoding due to it generating fewer columns than one-hot encoding and therefore making the fitting process faster. Also, I often found it to perform slightly better when dealing with tree-based methods\n* imputing missing values cause the algorithm can't handle nan's\n* **no feature scaling:** trees aren't affected by features on different scales, due to the nature of greedy splitting\n* **no feature selection:** trees perform their own feature selection when splitting so this is only required when you have large data and want to save time","metadata":{}},{"cell_type":"code","source":"house_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n\n# encode categorical features, fill nan values\nfor feature in house_data.columns:\n    if house_data[feature].dtype == \"object\":\n        #  categorical encoding: turns [a, b, b, c] into [1, 2, 2, 3]\n        house_data[feature] = house_data[feature].astype(\"category\").cat.codes \n        if house_data[feature].isna().sum() != 0:\n            #  impute missing values with the mode of the corresponding variable\n            house_data[feature].fillna(house_data[feature].mode(), inplace=True)\n    else:\n        if house_data[feature].isna().sum() != 0:\n            #  impute missing values with the mean of the corresponding variable\n            house_data[feature].fillna(house_data[feature].mean(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:16:08.033811Z","iopub.execute_input":"2022-06-05T15:16:08.03425Z","iopub.status.idle":"2022-06-05T15:16:08.175285Z","shell.execute_reply.started":"2022-06-05T15:16:08.034207Z","shell.execute_reply":"2022-06-05T15:16:08.174582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  spliting in train and test data\nfeatures = house_data.loc[:, house_data.columns!=\"SalePrice\"].to_numpy()\nlabels = house_data.loc[:, \"SalePrice\"].to_numpy()\n\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.3)\nhouse_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:16:08.176456Z","iopub.execute_input":"2022-06-05T15:16:08.17704Z","iopub.status.idle":"2022-06-05T15:16:08.218609Z","shell.execute_reply.started":"2022-06-05T15:16:08.176997Z","shell.execute_reply":"2022-06-05T15:16:08.217525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### implementing the algorithm:","metadata":{}},{"cell_type":"code","source":"class RandomForestRegressor:\n    def __init__(self, n_estimators, max_depth=None, min_samples_split=20, max_features=0.5, min_impurity_decrease=0):\n        self.n_estimators = n_estimators\n        #  with the max_features parameter the proportion of the randomly considered features at every split is determined\n        self.base_estimator = DecisionTreeRegressor(max_depth=max_depth, \n                                                    min_samples_split=min_samples_split, max_features=max_features, \n                                                    min_impurity_decrease=min_impurity_decrease)\n        self.estimators = None\n        \n    def fit(self, X, y):\n        #  initialise empty list to save estimators after fitting\n        self.estimators = []\n        #  repeat n times(where n is the amount of estimators):\n        for estimator_i in range(self.n_estimators):\n            #  bootstrap the dataset\n            idx = np.random.randint(low=0, high=len(X), size=len(X)) # random indexes -> sampled with replacement\n            X_bs = X[idx, :]\n            y_bs = y[idx]\n            \n            #  fit estimator on bootstraped sample\n            new_estimator = copy.copy(self.base_estimator)\n            new_estimator.fit(X_bs, y_bs)\n            \n            #  save estimator\n            self.estimators.append(new_estimator)\n    \n    def predict(self, X):\n        \"\"\"\n        * every estimator makes his predictions in the shape (len(X)) -> [a, b, ..., len(X)]\n        * stack prediction of estimators to have them row wise(each row corresponds to a sample) -> [[a1, a2], [b1, b2], ..., len(X)]\n        * averaging the rows to have one final prediction per sample -> [mean(a1, a2), mean(b1, b2), ..., len(X)]\n        \"\"\"\n        predictions = np.stack([estimator.predict(X) for estimator in self.estimators], axis=1) \n        return predictions.mean(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T17:19:55.888638Z","iopub.execute_input":"2022-06-18T17:19:55.889087Z","iopub.status.idle":"2022-06-18T17:19:55.924255Z","shell.execute_reply.started":"2022-06-18T17:19:55.888994Z","shell.execute_reply":"2022-06-18T17:19:55.92312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### fit and evaluate the model:","metadata":{}},{"cell_type":"code","source":"#  fit model\nregressor = RandomForestRegressor(200)\nregressor.fit(X_train, y_train)\n\n#  make predictions:\npredictions = regressor.predict(X_test)\n\n#  calculate rmse:\nrmse = np.sqrt(np.square(y_test - predictions)).mean()\n\n#  calculate R2:\nr2 = 1 - np.square(y_test - predictions).sum()/np.square(y_test - y_test.mean()).sum()\n\nprint(f\"testing data: root mean squared error = {rmse}\\nR-squared = {r2}\")\n\n#  make predictions:\npredictions_train = regressor.predict(X_train)\n\n#  calculate rmse:\nrmse_train = np.sqrt(np.square(y_train - predictions_train)).mean()\n\n#  calculate R2:\nr2_train = 1 - np.square(y_train - predictions_train).sum()/np.square(y_train - y_train.mean()).sum()\n\nprint(f\"training data: root mean squared error = {rmse_train}\\nR-squared = {r2_train}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T15:16:56.553131Z","iopub.execute_input":"2022-06-05T15:16:56.553499Z","iopub.status.idle":"2022-06-05T15:16:58.166887Z","shell.execute_reply.started":"2022-06-05T15:16:56.553469Z","shell.execute_reply":"2022-06-05T15:16:58.165967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**That's all for this notebook, have a great day and happy learning!üëã**","metadata":{}}]}