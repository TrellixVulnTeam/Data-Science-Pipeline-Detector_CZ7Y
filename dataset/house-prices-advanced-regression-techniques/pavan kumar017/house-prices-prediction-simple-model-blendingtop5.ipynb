{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# House Prices - Advanced Regression Techniques\nPredict sales prices and practice feature engineering, RFs, and gradient boosting","metadata":{}},{"cell_type":"markdown","source":"# Loading the Dataset","metadata":{}},{"cell_type":"markdown","source":"Dataset to downloaded from the below link\n\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques/data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\npd.pandas.set_option('display.max_columns',None)\ntrain_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data=pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:10.28103Z","iopub.execute_input":"2021-11-05T18:24:10.28197Z","iopub.status.idle":"2021-11-05T18:24:10.344906Z","shell.execute_reply.started":"2021-11-05T18:24:10.281925Z","shell.execute_reply":"2021-11-05T18:24:10.344178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:10.554887Z","iopub.execute_input":"2021-11-05T18:24:10.555649Z","iopub.status.idle":"2021-11-05T18:24:10.649558Z","shell.execute_reply.started":"2021-11-05T18:24:10.555602Z","shell.execute_reply":"2021-11-05T18:24:10.648827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see from the Training DataFrame above that there are around 80 variables and 1460 observations. So we have suffcient data points to train the model and canÂ expect a good score on the predictions.","metadata":{}},{"cell_type":"markdown","source":"# EDA\nOur data is now in the form of a Data Frame. The initial step in EDA is to identify any missing data and examine how they relate to the target variable/feature. That analysis is usually useful in determining how to replace missing values.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(25,10))\nsns.heatmap(train_data.isnull(), cmap=\"viridis\")","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:11.269365Z","iopub.execute_input":"2021-11-05T18:24:11.269829Z","iopub.status.idle":"2021-11-05T18:24:14.424302Z","shell.execute_reply.started":"2021-11-05T18:24:11.269791Z","shell.execute_reply":"2021-11-05T18:24:14.423485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The missing or NaN values are indicated by the yellow streaks in the preceding image. Although the majority of the columns in the data are complete, a few rows have more than 75% of the values missing, such as alley, fence, and a few others.","metadata":{}},{"cell_type":"code","source":"Id=train_data['Id']","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:14.425826Z","iopub.execute_input":"2021-11-05T18:24:14.426072Z","iopub.status.idle":"2021-11-05T18:24:14.432921Z","shell.execute_reply.started":"2021-11-05T18:24:14.42604Z","shell.execute_reply":"2021-11-05T18:24:14.432212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.heatmap(train_data.corr())\nplt.title('HeatMap- Correlation between predictor Variables')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:14.434591Z","iopub.execute_input":"2021-11-05T18:24:14.435436Z","iopub.status.idle":"2021-11-05T18:24:15.439619Z","shell.execute_reply.started":"2021-11-05T18:24:14.435398Z","shell.execute_reply":"2021-11-05T18:24:15.438921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.corr()['SalePrice'].sort_values()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:15.442117Z","iopub.execute_input":"2021-11-05T18:24:15.442641Z","iopub.status.idle":"2021-11-05T18:24:15.458559Z","shell.execute_reply.started":"2021-11-05T18:24:15.4426Z","shell.execute_reply":"2021-11-05T18:24:15.457815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nOverallQual and GrLivArea are highly connected with the SalePrice, according to the preceding series. This suggests that individuals are prepared to pay more for homes with more ground living space and residences of higher quality. Furthermore, an increase in the quantity of high-quality properties in the neighbourhood will raise the area's average house sale price.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.boxplot(x='OverallQual', y='SalePrice',data=train_data )\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:15.460025Z","iopub.execute_input":"2021-11-05T18:24:15.460333Z","iopub.status.idle":"2021-11-05T18:24:15.809615Z","shell.execute_reply.started":"2021-11-05T18:24:15.460296Z","shell.execute_reply":"2021-11-05T18:24:15.808946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The boxplot diagram clearly shows that overall quality is one of the primary factors influencing property prices.The median sale price had  simultaneously  increased as the Overallquality rating went up in the graph","metadata":{}},{"cell_type":"code","source":"plt.scatter( x='GrLivArea',y='SalePrice',data=train_data)\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:15.811009Z","iopub.execute_input":"2021-11-05T18:24:15.811517Z","iopub.status.idle":"2021-11-05T18:24:16.022696Z","shell.execute_reply.started":"2021-11-05T18:24:15.811478Z","shell.execute_reply":"2021-11-05T18:24:16.022015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The range of house sale prices has expanded as the ground living area has increased. Furthermore, we can detect some outliers on the right side of the graph.","metadata":{}},{"cell_type":"code","source":"numerical_col=[col for col in train_data.columns if train_data[col].dtypes!='O']\nnumerical_col.remove('Id')\nyear_col=['YearBuilt','YearRemodAdd','GarageYrBlt','YrSold']\ntrain_data.groupby('YrSold')['SalePrice'].mean().plot()\nplt.ylabel('SalePrice')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:16.024325Z","iopub.execute_input":"2021-11-05T18:24:16.024627Z","iopub.status.idle":"2021-11-05T18:24:16.279059Z","shell.execute_reply.started":"2021-11-05T18:24:16.024588Z","shell.execute_reply":"2021-11-05T18:24:16.278334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can observe from the graph above that the Saleprice trend has been irregular when compared to the year sold variable, implying that the selling price of a property has a poor relationship with the year in which it is sold.","metadata":{}},{"cell_type":"code","source":"for i in year_col:\n    data1=train_data.copy()\n    if i!= 'YrSold':\n        data1['new']=data1['YrSold']-data1[i]\n        sns.scatterplot(x='new',y='SalePrice',data=data1)\n        plt.xlabel('Number of years since'+' '+ i)\n        plt.title(i)\n        plt.show()\n        ","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:16.280445Z","iopub.execute_input":"2021-11-05T18:24:16.280852Z","iopub.status.idle":"2021-11-05T18:24:16.953062Z","shell.execute_reply.started":"2021-11-05T18:24:16.280813Z","shell.execute_reply":"2021-11-05T18:24:16.952329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the three graphs above, older houses have a lower selling price than newly built houses. Furthermore, properties with freshly built garages or recently re-modified residences had higher selling values, and the price declined as the years since these changes or garage construction increased.","metadata":{}},{"cell_type":"code","source":"discrete_col=[col for col in numerical_col if len(train_data[col].value_counts())< 20 and col not in year_col]\nfor i in discrete_col:\n    df1=train_data.copy()\n    df1.groupby(i)['SalePrice'].mean().plot.bar()\n    plt.ylabel('Sale Price')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:16.954942Z","iopub.execute_input":"2021-11-05T18:24:16.955601Z","iopub.status.idle":"2021-11-05T18:24:19.854748Z","shell.execute_reply.started":"2021-11-05T18:24:16.955557Z","shell.execute_reply":"2021-11-05T18:24:19.853994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plotted the SalePrice versus category factors in the figures above. Some attributes, such as OverallQuality, TotalRoomsAboveGround, Fireplaces, and GarageCars, have a considerable relationship with sales prices.","metadata":{}},{"cell_type":"code","source":"train_data.groupby(['YrSold','MoSold']).count()['SalePrice'].plot(kind='barh',figsize=(20,25))","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:19.85755Z","iopub.execute_input":"2021-11-05T18:24:19.857942Z","iopub.status.idle":"2021-11-05T18:24:20.988004Z","shell.execute_reply.started":"2021-11-05T18:24:19.857902Z","shell.execute_reply":"2021-11-05T18:24:20.987262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above barchart we have plotted the number of houses sold in each month of the respective year starting from January 2006 to July 2011.If clearly observed, there is a trend in the above graph the number of houses sold have dramatically increased in the month of may, june and july in every year.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Imputing Missing Values","metadata":{}},{"cell_type":"markdown","source":"As stated in the dataset description in kaggle, NA values in numerous fields such as Alley,Fence,FireplaceQu...etc imply that these features or amenities are lacking in the home, thus I replaced them with a 'None' value if it is categorical and 0 incase the variable is numerical.","metadata":{}},{"cell_type":"code","source":"train_data[\"MiscFeature\"] = train_data[\"MiscFeature\"].fillna(\"None\")\ntrain_data[\"Alley\"] = train_data[\"Alley\"].fillna(\"None\")\ntrain_data[\"Fence\"] = train_data[\"Fence\"].fillna(\"None\")\ntrain_data[\"FireplaceQu\"] = train_data[\"FireplaceQu\"].fillna(\"None\")\n\ntest_data[\"MiscFeature\"] = test_data[\"MiscFeature\"].fillna(\"None\")\ntest_data[\"Alley\"] = test_data[\"Alley\"].fillna(\"None\")\ntest_data[\"Fence\"] = test_data[\"Fence\"].fillna(\"None\")\ntest_data[\"FireplaceQu\"] = test_data[\"FireplaceQu\"].fillna(\"None\")\n\ntrain_data[\"MasVnrArea\"] = train_data[\"MasVnrArea\"].fillna(0)\ntest_data[\"MasVnrArea\"] = test_data[\"MasVnrArea\"].fillna(0)\ntrain_data[\"MasVnrType\"] = train_data[\"MasVnrType\"].fillna(\"None\")\ntest_data[\"MasVnrType\"] = test_data[\"MasVnrType\"].fillna(\"None\")\ntrain_data[\"PoolQC\"] = train_data[\"PoolQC\"].fillna(\"None\")\ntest_data[\"PoolQC\"] = test_data[\"PoolQC\"].fillna(\"None\")","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:20.989288Z","iopub.execute_input":"2021-11-05T18:24:20.989606Z","iopub.status.idle":"2021-11-05T18:24:21.008923Z","shell.execute_reply.started":"2021-11-05T18:24:20.989565Z","shell.execute_reply":"2021-11-05T18:24:21.008174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Basement_cat = (\"BsmtQual\" , \"BsmtCond\", \"BsmtExposure\" , \"BsmtFinType1\" , \"BsmtFinType2\")\nfor i in Basement_cat:\n    train_data[i] = train_data[i].fillna(\"None\")\n    test_data[i] = test_data[i].fillna(\"None\")\n    \nBasement_num = (\"BsmtFinSF1\" , \"BsmtFinSF2\" , \"BsmtUnfSF\", \"TotalBsmtSF\" ,\n\"BsmtFullBath\" , \"BsmtHalfBath\")\nfor i in Basement_num:\n    train_data[i] = train_data[i].fillna(0)\n    test_data[i] = test_data[i].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.01071Z","iopub.execute_input":"2021-11-05T18:24:21.011334Z","iopub.status.idle":"2021-11-05T18:24:21.028983Z","shell.execute_reply.started":"2021-11-05T18:24:21.011296Z","shell.execute_reply":"2021-11-05T18:24:21.028021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"garage_cat= (\"GarageType\" , \"GarageFinish\" , \"GarageQual\" ,\"GarageCond\")\nfor i in garage_cat:\n    train_data[i] = train_data[i].fillna('None')\n    test_data[i] = test_data[i].fillna('None')\n    \ngarage_num = (\"GarageYrBlt\" , \"GarageArea\" , \"GarageCars\")\nfor i in garage_num:\n    train_data[i] = train_data[i].fillna(0)\n    test_data[i] = test_data[i].fillna(0)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.030144Z","iopub.execute_input":"2021-11-05T18:24:21.030522Z","iopub.status.idle":"2021-11-05T18:24:21.04517Z","shell.execute_reply.started":"2021-11-05T18:24:21.030478Z","shell.execute_reply":"2021-11-05T18:24:21.044366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"LotFrontage\"] = train_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\ntest_data[\"LotFrontage\"] = test_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.04651Z","iopub.execute_input":"2021-11-05T18:24:21.046922Z","iopub.status.idle":"2021-11-05T18:24:21.070209Z","shell.execute_reply.started":"2021-11-05T18:24:21.046881Z","shell.execute_reply":"2021-11-05T18:24:21.069541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the above cell, I grouped the columns based on their Neighborhood and then used the median value to fill in the empty values in the LotFrontage column, because properties in similar neighbourhoods tend to have the same LotFrontage distance.","metadata":{}},{"cell_type":"code","source":"numeric_cols = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_cols = train_data.select_dtypes(include=['object']).columns.tolist()\nyear_col=['YearBuilt','YearRemodAdd','GarageYrBlt','YrSold']","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.071577Z","iopub.execute_input":"2021-11-05T18:24:21.071836Z","iopub.status.idle":"2021-11-05T18:24:21.080064Z","shell.execute_reply.started":"2021-11-05T18:24:21.071802Z","shell.execute_reply":"2021-11-05T18:24:21.079394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Values along with Count in the Categorical Columns','\\n')\nfor i in categorical_cols:\n    print(i)\n    print(train_data[i].value_counts(),'\\n')  ","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.081642Z","iopub.execute_input":"2021-11-05T18:24:21.082379Z","iopub.status.idle":"2021-11-05T18:24:21.157731Z","shell.execute_reply.started":"2021-11-05T18:24:21.082328Z","shell.execute_reply":"2021-11-05T18:24:21.157033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.drop(['Utilities','Street',\"PoolQC\"], axis = 1,inplace=True)\ntest_data.drop(['Utilities','Street',\"PoolQC\"], axis = 1,inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.159103Z","iopub.execute_input":"2021-11-05T18:24:21.159357Z","iopub.status.idle":"2021-11-05T18:24:21.16898Z","shell.execute_reply.started":"2021-11-05T18:24:21.159325Z","shell.execute_reply":"2021-11-05T18:24:21.16818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I removed the utilities,PoolQc and street features since more than 95 percent of the values in it had a single value, thus adding these features to the model is pointless due to the lack of variety in values.","metadata":{}},{"cell_type":"code","source":"train_data","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.172379Z","iopub.execute_input":"2021-11-05T18:24:21.172575Z","iopub.status.idle":"2021-11-05T18:24:21.251788Z","shell.execute_reply.started":"2021-11-05T18:24:21.172552Z","shell.execute_reply":"2021-11-05T18:24:21.25101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_counts = train_data.isnull().sum().sort_values(ascending=False)\nmissing_counts[missing_counts > 0]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.253324Z","iopub.execute_input":"2021-11-05T18:24:21.253588Z","iopub.status.idle":"2021-11-05T18:24:21.272012Z","shell.execute_reply.started":"2021-11-05T18:24:21.253552Z","shell.execute_reply":"2021-11-05T18:24:21.271089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_counts = test_data.isna().sum().sort_values(ascending=False)\nmissing_counts[missing_counts > 0]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.274694Z","iopub.execute_input":"2021-11-05T18:24:21.27489Z","iopub.status.idle":"2021-11-05T18:24:21.290444Z","shell.execute_reply.started":"2021-11-05T18:24:21.274865Z","shell.execute_reply":"2021-11-05T18:24:21.289464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could see that there are few more missing values in training and testing data to deal with. Below, I have used the simple imputer class with mean to deal with the numerical missing values and mode for the categorical ones.The SimpleImputer class provides fundamental mechanisms for filling in missing values. Missing values can be imputed using a constant value supplied, or by utilising the statistics (mean, median, or most common) of each column in which the missing values are placed.","metadata":{}},{"cell_type":"markdown","source":"Numerical columns","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nnumeric_cols.remove('SalePrice')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.291917Z","iopub.execute_input":"2021-11-05T18:24:21.292447Z","iopub.status.idle":"2021-11-05T18:24:21.407222Z","shell.execute_reply.started":"2021-11-05T18:24:21.292409Z","shell.execute_reply":"2021-11-05T18:24:21.406513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imputer1 = SimpleImputer(strategy='mean')\nimputer1.fit(train_data[numeric_cols])\ntrain_data[numeric_cols] = imputer1.transform(train_data[numeric_cols])\ntest_data[numeric_cols] = imputer1.transform(test_data[numeric_cols])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.410125Z","iopub.execute_input":"2021-11-05T18:24:21.410385Z","iopub.status.idle":"2021-11-05T18:24:21.438341Z","shell.execute_reply.started":"2021-11-05T18:24:21.410351Z","shell.execute_reply":"2021-11-05T18:24:21.437605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Categorical columns","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\ncategorical_cols = train_data.select_dtypes(include=['object']).columns.tolist()\nimputer1 = SimpleImputer(strategy='most_frequent')\nimputer1.fit(train_data[categorical_cols])\ntrain_data[categorical_cols] = imputer1.transform(train_data[categorical_cols])\ntest_data[categorical_cols] = imputer1.transform(test_data[categorical_cols])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.441649Z","iopub.execute_input":"2021-11-05T18:24:21.441921Z","iopub.status.idle":"2021-11-05T18:24:21.525972Z","shell.execute_reply.started":"2021-11-05T18:24:21.441886Z","shell.execute_reply":"2021-11-05T18:24:21.525114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target variable transformation","metadata":{}},{"cell_type":"markdown","source":"The term \"normality\" refers to the fact that the distribution of variables follows a normal pattern.\nDrawing a Histogram and a QQ plot is the simplest technique to check for normality.","metadata":{}},{"cell_type":"code","source":"sns.distplot(train_data['SalePrice'],bins=50)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.527088Z","iopub.execute_input":"2021-11-05T18:24:21.527674Z","iopub.status.idle":"2021-11-05T18:24:21.864297Z","shell.execute_reply.started":"2021-11-05T18:24:21.527635Z","shell.execute_reply":"2021-11-05T18:24:21.863562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pricing is right skewed, as shown in the graph above. Skewed data makes it harder for a model to detect a correct pattern in the data, which is why we must convert skew data to normal or Gaussian data. The log-transformation effectively eliminates \nskewness.","metadata":{}},{"cell_type":"code","source":"from scipy import stats\nstats.probplot(train_data['SalePrice'], plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:21.866296Z","iopub.execute_input":"2021-11-05T18:24:21.866562Z","iopub.status.idle":"2021-11-05T18:24:22.063576Z","shell.execute_reply.started":"2021-11-05T18:24:21.866527Z","shell.execute_reply":"2021-11-05T18:24:22.062853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])\nfrom scipy import stats\nstats.probplot(train_data['SalePrice'], plot=plt)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:22.064928Z","iopub.execute_input":"2021-11-05T18:24:22.065211Z","iopub.status.idle":"2021-11-05T18:24:22.274296Z","shell.execute_reply.started":"2021-11-05T18:24:22.065175Z","shell.execute_reply":"2021-11-05T18:24:22.273615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tvariable=train_data['SalePrice']\ntrain_data.drop('SalePrice',axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:22.276527Z","iopub.execute_input":"2021-11-05T18:24:22.277023Z","iopub.status.idle":"2021-11-05T18:24:22.283163Z","shell.execute_reply.started":"2021-11-05T18:24:22.276982Z","shell.execute_reply":"2021-11-05T18:24:22.282367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  ADDITIONAL FEATURES","metadata":{}},{"cell_type":"code","source":"yr_col=['YearBuilt','YearRemodAdd']\nfor i in yr_col:\n        train_data['NYS'+i]=train_data['YrSold']-train_data[i]\n        test_data['NYS'+i]=test_data['YrSold']-test_data[i]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:22.663551Z","iopub.execute_input":"2021-11-05T18:24:22.663957Z","iopub.status.idle":"2021-11-05T18:24:22.672093Z","shell.execute_reply.started":"2021-11-05T18:24:22.66391Z","shell.execute_reply":"2021-11-05T18:24:22.671344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I've included two new fields to the dataset: the number of years since the remodification and the year the home was built. As we can see from the figures in the EDA section, these two will aid the model in properly anticipating prices.","metadata":{}},{"cell_type":"markdown","source":"# Transforming  numerical variables that are categorical","metadata":{}},{"cell_type":"markdown","source":"Some of the numerical features are categorical, therefore I changed them to strings so that they would be taken into account when encoding the categorical columns. To identify these columns, I first built a list of numerical columns and then filtered the list for columns with fewer than 30 different values in the respective columns and that are not in the Year list.","metadata":{}},{"cell_type":"code","source":"num_cols = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\nyear_col=['YearBuilt','YearRemodAdd','GarageYrBlt','YrSold']\nnum_discrete_col=[col for col in num_cols if len(train_data[col].value_counts())<30 and col not in year_col]\ntrain_data[num_discrete_col]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:24.204203Z","iopub.execute_input":"2021-11-05T18:24:24.204835Z","iopub.status.idle":"2021-11-05T18:24:24.479148Z","shell.execute_reply.started":"2021-11-05T18:24:24.204792Z","shell.execute_reply":"2021-11-05T18:24:24.478381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"MSSubClass\"] = train_data[\"MSSubClass\"].apply(str)\ntest_data[\"MSSubClass\"] = test_data[\"MSSubClass\"].apply(str)\ntrain_data[\"YrSold\"] = train_data[\"YrSold\"].apply(str)\ntest_data[\"YrSold\"] = test_data[\"YrSold\"].apply(str)\ntrain_data[\"MoSold\"] = train_data[\"MoSold\"].apply(str)\ntest_data[\"MoSold\"] = test_data[\"MoSold\"].apply(str)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:24.480958Z","iopub.execute_input":"2021-11-05T18:24:24.481481Z","iopub.status.idle":"2021-11-05T18:24:24.501841Z","shell.execute_reply.started":"2021-11-05T18:24:24.48144Z","shell.execute_reply":"2021-11-05T18:24:24.501155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the final list MSSubClass is categorical as its values relate to the type of dwelling involved in the sale. Moreover, Yearsold has only 4 disticnt values and MonthSold indicates the month in which the house was sold","metadata":{}},{"cell_type":"markdown","source":"# Outliers ","metadata":{}},{"cell_type":"markdown","source":"The statistics and distribution of the input variables affect machine learning algorithms. Outliers in data can sabotage and mislead the training process. Longer training times, fewer accurate models, and, ultimately, inferior results follow.","metadata":{}},{"cell_type":"markdown","source":"I separated the list of numerical columns into two categories: discrete and continuous. All columns with fewer than 15 different values were placed in the discrete column list, while the remaining were placed in the continuous column list. Following that, I created distribution charts for continuous columns.","metadata":{}},{"cell_type":"code","source":"numerical_col=[col for col in train_data.columns if train_data[col].dtypes!='O']\ndiscrete_col=[col for col in numerical_col if len(train_data[col].value_counts())< 15 and col not in year_col]\ncont_col=[col for col in numerical_col if col not in discrete_col+year_col ]\nfor i in cont_col:\n    df1=train_data.copy()\n    df1[i].hist(bins=50)\n    plt.xlabel(i)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:29.805355Z","iopub.execute_input":"2021-11-05T18:24:29.805882Z","iopub.status.idle":"2021-11-05T18:24:35.946748Z","shell.execute_reply.started":"2021-11-05T18:24:29.805841Z","shell.execute_reply":"2021-11-05T18:24:35.946057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l=['LotFrontage','LotArea','BsmtUnfSF','TotalBsmtSF','1stFlrSF','GrLivArea','GarageArea']\nfor i in l:\n    Q1=train_data[i].quantile(0.25)\n    Q3=train_data[i].quantile(0.75)\n    IQR=Q3-Q1\n    W1=Q1-(1.5*IQR)\n    W2=Q3+(1.5*IQR)\n    for x in train_data[i]:\n        if x<W1:\n            train_data[i].replace(x,W1,inplace=True)\n        if x>W2:\n            train_data[i].replace(x,W2,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:35.948304Z","iopub.execute_input":"2021-11-05T18:24:35.94899Z","iopub.status.idle":"2021-11-05T18:24:36.007845Z","shell.execute_reply.started":"2021-11-05T18:24:35.948947Z","shell.execute_reply":"2021-11-05T18:24:36.00716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After evaluating the distributions and the unique values in continuous columns, I have chosen a few columns that may include outliers. I later substituted the outliers with the corresponding whisker values.","metadata":{}},{"cell_type":"markdown","source":"# Skewness","metadata":{}},{"cell_type":"markdown","source":"Because it must cope with rare occurrences on extreme values, skewed data reduces the model's capacity to explain typical cases.There are statistical models that are resistant to outliers, such as Tree-based models, however this limits the ability to test alternative models. As a result, it is necessary to convert the skewed data into something like a Gaussian or Normal distribution. This will enable us to test a greater number of statistical models.","metadata":{}},{"cell_type":"markdown","source":"To cope with the skweness in the data, I applied the boxcox transformation below.","metadata":{}},{"cell_type":"code","source":"cdata=pd.concat([train_data,test_data])\nnumerical_col=[col for col in train_data.columns if train_data[col].dtypes!='O']\nskew=cdata[numerical_col].skew().sort_values()\nskew_score=pd.DataFrame({'Skew' :skew})\nskew_score","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:36.008953Z","iopub.execute_input":"2021-11-05T18:24:36.009489Z","iopub.status.idle":"2021-11-05T18:24:36.046819Z","shell.execute_reply.started":"2021-11-05T18:24:36.009447Z","shell.execute_reply":"2021-11-05T18:24:36.046053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skewness_p = skew_score[(skew_score['Skew']) > 0.75]\nskewness_n = skew_score[(skew_score['Skew']) < -0.75]\n\nfrom scipy.special import boxcox1p\nskewed_features_p = skewness_p.index\nlam = 0.17\nfor feat in skewed_features_p:\n    cdata[feat] = boxcox1p(cdata[feat], lam)\n    \nskewed_features_n = skewness_n.index\nlam=2    \nfor feat in skewed_features_n:\n    cdata[feat] = boxcox1p(cdata[feat], lam)\n    \ntrain_data=cdata.iloc[0:1460]\ntest_data=cdata.iloc[1460:]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:36.048854Z","iopub.execute_input":"2021-11-05T18:24:36.04921Z","iopub.status.idle":"2021-11-05T18:24:36.067585Z","shell.execute_reply.started":"2021-11-05T18:24:36.04917Z","shell.execute_reply":"2021-11-05T18:24:36.06655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaling Numerical Values","metadata":{}},{"cell_type":"markdown","source":"Feature scaling is a technique for standardising the independent characteristics included in data within a specific range. If feature scaling is not performed, a machine learning algorithm will tend to weight bigger values as higher and consider smaller values as lower, regardless of the unit of measurement.I used robustscaler to scale the data since there were many columns with outliers, and robustscaler performs well when scaling data with a high number of outliers.This Scaler eliminates the median and scales the data based on the quantile range . The IQR is the difference between the first and third quartiles (25th and 75th quantiles) (75th quantile). Because this Scaler's centering and scaling statistics are based on percentiles, they are unaffected by a small number of large marginal outliers.","metadata":{}},{"cell_type":"code","source":"num_cols = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\ntrain_data[num_cols].describe().loc[['min', 'max']]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:36.068885Z","iopub.execute_input":"2021-11-05T18:24:36.069602Z","iopub.status.idle":"2021-11-05T18:24:36.192983Z","shell.execute_reply.started":"2021-11-05T18:24:36.069555Z","shell.execute_reply":"2021-11-05T18:24:36.192115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nscaler.fit(pd.concat([train_data[num_cols], test_data[num_cols]]))\ntrain_data[num_cols] = scaler.transform(train_data[num_cols])\ntest_data[num_cols] = scaler.transform(test_data[num_cols])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:36.196791Z","iopub.execute_input":"2021-11-05T18:24:36.197074Z","iopub.status.idle":"2021-11-05T18:24:36.237479Z","shell.execute_reply.started":"2021-11-05T18:24:36.197028Z","shell.execute_reply":"2021-11-05T18:24:36.236492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encode Categorical Columns","metadata":{}},{"cell_type":"markdown","source":"All input and output variables in machine learning models must be numeric. This implies that if your data contains categorical data, you must convert it to numbers before fitting and evaluating a model. When working with categorical data for machine learning algorithms, encoding is a needed pre-processing step.Some of the categorical columns had order, so I used label encoding on those, and the rest columns were encoded with getdummies function.","metadata":{}},{"cell_type":"markdown","source":"## LABEL ENCODING","metadata":{}},{"cell_type":"code","source":"cdata=pd.concat([train_data,test_data])\ncdata['LotShape']=cdata['LotShape'].map({'Reg':3,'IR3':0,'IR2':1,'IR1':2})\ncdata['LandSlope']=cdata['LandSlope'].map({'Gtl':0,'Mod':1,'Sev':2})\ncdata['ExterQual']=cdata['ExterQual'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ncdata['ExterCond']=cdata['ExterCond'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ncdata['BsmtQual']=cdata['BsmtQual'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ncdata['BsmtCond']=cdata['BsmtCond'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ncdata['BsmtExposure']=cdata['BsmtExposure'].map({'None':0,'No':1,'Mn':2,'Av':3,'Gd':4})\ncdata['BsmtFinType1']=cdata['BsmtFinType1'].map({'None':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ncdata['BsmtFinType2']=cdata['BsmtFinType2'].map({'None':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ncdata['HeatingQC']=cdata['HeatingQC'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ncdata['KitchenQual']=cdata['KitchenQual'].map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ncdata['FireplaceQu']=cdata['FireplaceQu'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ncdata['GarageFinish']=cdata['GarageFinish'].map({'None':0,'Unf':1,'RFn':2,'Fin':3})\ncdata['GarageQual']=cdata['GarageQual'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ncdata['GarageCond']=cdata['GarageCond'].map({'None':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ncdata['YrSold']=cdata['YrSold'].map({'2006.0':0,'2007.0':1,'2008.0':2,'2009.0':3,'2010.0':4})\ncdata['MoSold']=cdata['MoSold'].apply(float).apply(int)\ncdata['PavedDrive']=cdata['PavedDrive'].map({'N':0,'P':1,'Y':2})\ncdata['Fence']=cdata['Fence'].map({'None':0,'MnWw':1,'GdWo':2,'MnPrv':3,'GdPrv':4})","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:36.239334Z","iopub.execute_input":"2021-11-05T18:24:36.239635Z","iopub.status.idle":"2021-11-05T18:24:36.307546Z","shell.execute_reply.started":"2021-11-05T18:24:36.239597Z","shell.execute_reply":"2021-11-05T18:24:36.306846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataf=pd.get_dummies(cdata, drop_first=True)\ndataf.drop(['Id'],axis=1,inplace=True)\nftrain_data=dataf.iloc[0:1460]\nftest_data=dataf.iloc[1460:]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:36.308802Z","iopub.execute_input":"2021-11-05T18:24:36.309123Z","iopub.status.idle":"2021-11-05T18:24:36.348854Z","shell.execute_reply.started":"2021-11-05T18:24:36.309084Z","shell.execute_reply":"2021-11-05T18:24:36.348091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"train_inputs = ftrain_data.copy()\ntest_inputs = ftest_data.copy()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:37.411337Z","iopub.execute_input":"2021-11-05T18:24:37.412096Z","iopub.status.idle":"2021-11-05T18:24:37.416994Z","shell.execute_reply.started":"2021-11-05T18:24:37.412057Z","shell.execute_reply":"2021-11-05T18:24:37.415993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inputs['Id']=Id","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:37.824706Z","iopub.execute_input":"2021-11-05T18:24:37.824965Z","iopub.status.idle":"2021-11-05T18:24:37.830491Z","shell.execute_reply.started":"2021-11-05T18:24:37.824934Z","shell.execute_reply":"2021-11-05T18:24:37.829536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper parameter optimization","metadata":{}},{"cell_type":"markdown","source":"The hyperparameter configuration becomes more complicated as the model becomes more sophisticated. Hyperparameter combinations can have a big influence on the model's performance.","metadata":{}},{"cell_type":"markdown","source":"Initially I have experimented with leastsqaures,lasso and ridge regression and finally ended up picking ridge as it has performed well and had great accuracy compared to the other two.\nThe rest two models are XGboost and gradient boosting regressor. I have performed the hyper parameter optimization on these models using the RandomizedSearchCV","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:47.233079Z","iopub.execute_input":"2021-11-05T18:24:47.233724Z","iopub.status.idle":"2021-11-05T18:24:47.237584Z","shell.execute_reply.started":"2021-11-05T18:24:47.233683Z","shell.execute_reply":"2021-11-05T18:24:47.236893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL-1 RIDGE REGRESSION","metadata":{}},{"cell_type":"markdown","source":"## Regularization\nValue of alpha, which is a Ridge hyperparameter, which means that they are not learnt automatically by the model and must be adjusted manually. \nWe use RandomizedSearchCV to determine the best alpha for Ridge Regularization.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nparam_grid={}\nparam_grid['alpha'] = np.arange(0, 20, 0.1)\nmodel=Ridge()\nsearcher=RandomizedSearchCV(model,param_grid,n_iter=2,scoring='neg_mean_squared_error',verbose=1,cv=10)\nsearcher.fit(ftrain_data, Tvariable)\nprint(searcher.best_params_)\nprint(searcher.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:47.889958Z","iopub.execute_input":"2021-11-05T18:24:47.890517Z","iopub.status.idle":"2021-11-05T18:24:48.298901Z","shell.execute_reply.started":"2021-11-05T18:24:47.890475Z","shell.execute_reply":"2021-11-05T18:24:48.298202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let us explore the coefficients for each of the independent attributes\nridge_model = Ridge(alpha=9)\nridge_model.fit(ftrain_data,Tvariable)\nweights = ridge_model.coef_\nweights_df = pd.DataFrame({\n    'columns': ftrain_data.columns,\n    'weight': weights\n}).sort_values('weight', ascending=False)\nweights_df=pd.concat([weights_df.iloc[0:4],weights_df.iloc[216:]])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:24:48.300675Z","iopub.execute_input":"2021-11-05T18:24:48.301193Z","iopub.status.idle":"2021-11-05T18:24:48.324202Z","shell.execute_reply.started":"2021-11-05T18:24:48.30115Z","shell.execute_reply":"2021-11-05T18:24:48.323433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,7))\nsns.barplot(x='weight',y='columns',data=weights_df)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-05T17:45:06.9742Z","iopub.execute_input":"2021-11-05T17:45:06.975148Z","iopub.status.idle":"2021-11-05T17:45:07.223811Z","shell.execute_reply.started":"2021-11-05T17:45:06.975098Z","shell.execute_reply":"2021-11-05T17:45:07.223145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I made a graph showing the variable coefficients with the greatest magnitude. As anticipated, the model gave higher weightage to Groundliving area and OverallQuality.","metadata":{}},{"cell_type":"markdown","source":"# MODEL-2 XGBOOST","metadata":{}},{"cell_type":"markdown","source":"XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. It is not always adequate to depend just on the outcomes of a single machine learning model. Ensemble learning provides a methodical approach to combining the predictive capacity of numerous learners. The end result is a single model that aggregates the output of numerous models.","metadata":{}},{"cell_type":"code","source":"n_estimators = [850,900,950]\nmax_depth = [2, 3, 5, 10, 15]\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\n\n# Define the grid of hyperparameters to search\nparam_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    }\n\nfrom xgboost import XGBRegressor\nmy_model = XGBRegressor()\nXGB=RandomizedSearchCV(my_model,param_grid,n_iter=5,scoring='neg_mean_squared_error',verbose=2,cv=10)\nXGB.fit(ftrain_data,Tvariable)\nprint(XGB.best_params_)\nprint(XGB.best_score_)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-05T17:45:10.550117Z","iopub.execute_input":"2021-11-05T17:45:10.550389Z","iopub.status.idle":"2021-11-05T17:50:21.985216Z","shell.execute_reply.started":"2021-11-05T17:45:10.550355Z","shell.execute_reply":"2021-11-05T17:50:21.984412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL-3 GRADIENT BOOSTING REGRESSOR","metadata":{}},{"cell_type":"markdown","source":"Gradient boosting creates an additive mode by employing several fixed-size decision trees as weak learners or weak predictive models. The parameter n estimators determines how many decision trees will be utilised in the boosting phases. The gradient boosting approach is useful for training models for both regression and classification problems. Boosting Gradients To fit the model that predicts the continuous value, the regression procedure is utilised.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nmy_model2=GradientBoostingRegressor()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:03:14.235583Z","iopub.execute_input":"2021-11-05T18:03:14.23611Z","iopub.status.idle":"2021-11-05T18:03:14.239789Z","shell.execute_reply.started":"2021-11-05T18:03:14.236071Z","shell.execute_reply":"2021-11-05T18:03:14.239131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_estimators = [850,900,950]\nmax_depth = [2, 3, 5, 10, 15]\nlearning_rate=[0.05,0.1,0.15,0.20]\n\nparam_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate\n    }\n\nGBR=RandomizedSearchCV(my_model2,param_grid,n_iter=5,scoring='neg_mean_squared_error',verbose=2,cv=10)\nGBR.fit(ftrain_data,Tvariable)\nprint(GBR.best_params_)\nprint(GBR.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:03:14.60406Z","iopub.execute_input":"2021-11-05T18:03:14.604603Z","iopub.status.idle":"2021-11-05T18:17:05.632236Z","shell.execute_reply.started":"2021-11-05T18:03:14.604563Z","shell.execute_reply":"2021-11-05T18:17:05.631464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  KFold dataset","metadata":{}},{"cell_type":"markdown","source":"I produced a Kfold dataset with 5 splits to utilise in the model blending strategy described below.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\nK_train_data=train_inputs.copy()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:25:00.232559Z","iopub.execute_input":"2021-11-05T18:25:00.23285Z","iopub.status.idle":"2021-11-05T18:25:00.23994Z","shell.execute_reply.started":"2021-11-05T18:25:00.232804Z","shell.execute_reply":"2021-11-05T18:25:00.239156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K_train_data[\"kfold\"] = -1\nkf =KFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=ftrain_data)):\n    K_train_data[\"kfold\"].loc[valid_indicies]  = fold","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:25:00.990937Z","iopub.execute_input":"2021-11-05T18:25:00.991226Z","iopub.status.idle":"2021-11-05T18:25:01.010427Z","shell.execute_reply.started":"2021-11-05T18:25:00.99118Z","shell.execute_reply":"2021-11-05T18:25:01.009392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K_train_data['target']=Tvariable","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:25:01.533784Z","iopub.execute_input":"2021-11-05T18:25:01.534327Z","iopub.status.idle":"2021-11-05T18:25:01.53924Z","shell.execute_reply.started":"2021-11-05T18:25:01.534282Z","shell.execute_reply":"2021-11-05T18:25:01.538219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K_train_data","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:25:01.885605Z","iopub.execute_input":"2021-11-05T18:25:01.886324Z","iopub.status.idle":"2021-11-05T18:25:02.027434Z","shell.execute_reply.started":"2021-11-05T18:25:01.88628Z","shell.execute_reply":"2021-11-05T18:25:02.026668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL BLENDING","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:25:02.467564Z","iopub.execute_input":"2021-11-05T18:25:02.468217Z","iopub.status.idle":"2021-11-05T18:25:02.599039Z","shell.execute_reply.started":"2021-11-05T18:25:02.468169Z","shell.execute_reply":"2021-11-05T18:25:02.59829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I trained the model on the Kfold dataset above, leaving out a single fold in each iteration, and then predicted the home values on the left out fold as well as the test set. As a result, for each model, I have one prediction set for the train data and four prediction sets for the test data.","metadata":{}},{"cell_type":"code","source":"# MODEL 1\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nfor fold in range(5):\n    xtrain =  K_train_data[K_train_data.kfold != fold].reset_index(drop=True)\n    xvalid = K_train_data[K_train_data.kfold == fold].reset_index(drop=True)\n    xtest = test_inputs.copy()\n    \n    valid_ids = xvalid.Id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain.drop(['target','kfold','Id'],axis=1,inplace=True)\n    xvalid.drop(['target','kfold','Id'],axis=1,inplace=True)\n    model =XGBRegressor(n_estimators=900, min_child_weight= 2, max_depth= 5, learning_rate= 0.05)\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"Id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)\n\nt=pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\ndf=pd.DataFrame({'Id':t['Id'].apply(int),'pred_1':np.mean(np.column_stack(final_test_predictions), axis=1)})\ndf.to_csv(\"test_pred_1.csv\", index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:25:03.056014Z","iopub.execute_input":"2021-11-05T18:25:03.056633Z","iopub.status.idle":"2021-11-05T18:25:47.375745Z","shell.execute_reply.started":"2021-11-05T18:25:03.056591Z","shell.execute_reply":"2021-11-05T18:25:47.374992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL 2\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nfor fold in range(5):\n    xtrain =  K_train_data[K_train_data.kfold != fold].reset_index(drop=True)\n    xvalid = K_train_data[K_train_data.kfold == fold].reset_index(drop=True)\n    xtest = test_inputs.copy()\n    \n    valid_ids = xvalid.Id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain.drop(['target','kfold','Id'],axis=1,inplace=True)\n    xvalid.drop(['target','kfold','Id'],axis=1,inplace=True)\n    \n    model=Ridge(alpha=12.9)\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"Id\", \"pred_2\"]\nfinal_valid_predictions.to_csv(\"train_pred_2.csv\", index=False)\n\nt=pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\ndf=pd.DataFrame({'Id':t['Id'].apply(int),'pred_2':np.mean(np.column_stack(final_test_predictions), axis=1)})\ndf.to_csv(\"test_pred_2.csv\", index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:25:47.37743Z","iopub.execute_input":"2021-11-05T18:25:47.37768Z","iopub.status.idle":"2021-11-05T18:25:47.573376Z","shell.execute_reply.started":"2021-11-05T18:25:47.377647Z","shell.execute_reply":"2021-11-05T18:25:47.572513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MODEL 3\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nfor fold in range(5):\n    xtrain =  K_train_data[K_train_data.kfold != fold]\n    xvalid = K_train_data[K_train_data.kfold == fold]\n    xtest = test_inputs.copy()\n    \n    valid_ids = xvalid.Id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain.drop(['target','kfold','Id'],axis=1,inplace=True)\n    xvalid.drop(['target','kfold','Id'],axis=1,inplace=True)\n    \n    model=XGBRegressor(n_estimators=850, max_depth= 5, learning_rate=0.1)\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"Id\", \"pred_3\"]\nfinal_valid_predictions.to_csv(\"train_pred_3.csv\", index=False)\n\nt=pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\ndf=pd.DataFrame({'Id':t['Id'].apply(int),'pred_3':np.mean(np.column_stack(final_test_predictions), axis=1)})\ndf.to_csv(\"test_pred_3.csv\", index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:25:47.576928Z","iopub.execute_input":"2021-11-05T18:25:47.577356Z","iopub.status.idle":"2021-11-05T18:26:29.496771Z","shell.execute_reply.started":"2021-11-05T18:25:47.577318Z","shell.execute_reply":"2021-11-05T18:26:29.496026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After prediction sets are formed, I saved predictions made on train in a separate csv file for every model, and in the case of test data, I had taken the mean of four prediction sets and saved it. ","metadata":{}},{"cell_type":"code","source":"df1 = pd.read_csv(\"train_pred_1.csv\")\ndf2 = pd.read_csv(\"train_pred_2.csv\")\ndf3 = pd.read_csv(\"train_pred_3.csv\")\n\ndf_test1 = pd.read_csv(\"test_pred_1.csv\")\ndf_test2 = pd.read_csv(\"test_pred_2.csv\")\ndf_test3 = pd.read_csv(\"test_pred_3.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:26:29.498591Z","iopub.execute_input":"2021-11-05T18:26:29.498881Z","iopub.status.idle":"2021-11-05T18:26:29.51676Z","shell.execute_reply.started":"2021-11-05T18:26:29.498841Z","shell.execute_reply":"2021-11-05T18:26:29.51602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=K_train_data\ndf = df.merge(df1, on=\"Id\", how=\"left\")\ndf = df.merge(df2, on=\"Id\", how=\"left\")\ndf = df.merge(df3, on=\"Id\", how=\"left\")\n\ndf_test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ndf_test = df_test.merge(df_test1, on=\"Id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"Id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"Id\", how=\"left\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:26:56.927991Z","iopub.execute_input":"2021-11-05T18:26:56.928727Z","iopub.status.idle":"2021-11-05T18:26:56.999781Z","shell.execute_reply.started":"2021-11-05T18:26:56.92869Z","shell.execute_reply":"2021-11-05T18:26:56.999069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below I trained a new ridge model by using the prior predictions from three models on training data as features and the train data sale prices as the target variable. Later, I forecasted the Final SalePrices on test data by using this model with the prior predictions on test data as inputs.Also I used the same Kfold values where were created previously, which again results in 5 test prediction sets using the same concept of leaving out 1 fold in every iteration. I have taken the mean of this 5 test set predictions as the Final Predictions.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nuseful_features = [\"pred_1\", \"pred_2\", \"pred_3\"]\ndf_test = df_test[useful_features]\n\npredictions = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    model = Ridge()\n    model.fit(xtrain, ytrain)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    predictions.append(test_preds)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:26:59.673797Z","iopub.execute_input":"2021-11-05T18:26:59.674331Z","iopub.status.idle":"2021-11-05T18:26:59.721142Z","shell.execute_reply.started":"2021-11-05T18:26:59.674292Z","shell.execute_reply":"2021-11-05T18:26:59.72034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EXPONENTIAL TRANSFORMATION","metadata":{}},{"cell_type":"code","source":"predictions=np.exp(predictions)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:27:00.92136Z","iopub.execute_input":"2021-11-05T18:27:00.92161Z","iopub.status.idle":"2021-11-05T18:27:00.926287Z","shell.execute_reply.started":"2021-11-05T18:27:00.921583Z","shell.execute_reply":"2021-11-05T18:27:00.925538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:27:01.297546Z","iopub.execute_input":"2021-11-05T18:27:01.298348Z","iopub.status.idle":"2021-11-05T18:27:01.306164Z","shell.execute_reply.started":"2021-11-05T18:27:01.2983Z","shell.execute_reply":"2021-11-05T18:27:01.305049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FINAL PREDICTIONS","metadata":{}},{"cell_type":"code","source":"final_predictions=np.mean(np.column_stack(predictions), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:27:02.526463Z","iopub.execute_input":"2021-11-05T18:27:02.526717Z","iopub.status.idle":"2021-11-05T18:27:02.533496Z","shell.execute_reply.started":"2021-11-05T18:27:02.526689Z","shell.execute_reply":"2021-11-05T18:27:02.532552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t=pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nDf = pd.DataFrame({'Id':t['Id'].apply(int), 'SalePrice':(final_predictions)})\n","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:27:16.890582Z","iopub.execute_input":"2021-11-05T18:27:16.890916Z","iopub.status.idle":"2021-11-05T18:27:16.916209Z","shell.execute_reply.started":"2021-11-05T18:27:16.890882Z","shell.execute_reply":"2021-11-05T18:27:16.915496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Df.to_csv('Submission', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:27:17.688191Z","iopub.execute_input":"2021-11-05T18:27:17.688745Z","iopub.status.idle":"2021-11-05T18:27:17.700413Z","shell.execute_reply.started":"2021-11-05T18:27:17.688699Z","shell.execute_reply":"2021-11-05T18:27:17.699524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Df","metadata":{"execution":{"iopub.status.busy":"2021-11-05T18:27:18.408853Z","iopub.execute_input":"2021-11-05T18:27:18.409544Z","iopub.status.idle":"2021-11-05T18:27:18.420977Z","shell.execute_reply.started":"2021-11-05T18:27:18.409504Z","shell.execute_reply":"2021-11-05T18:27:18.420126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CONCLUSION","metadata":{}},{"cell_type":"markdown","source":"In this project, we covered topics such as Handling Missing Values, Feature Engineering, Hyperparameter Optimization, and Model Building. We also performed exploratory data analysis at the start to become acquainted with the data, which assisted us in creating more characteristics and removing those that were unnecessary. Finally, we performed the model blending part, which enhanced accuracy while increasing the model's complexity.Personally, I found that model blending had a lot of potential and was the most effective feature that helped me get to the top inÂ the competition.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}