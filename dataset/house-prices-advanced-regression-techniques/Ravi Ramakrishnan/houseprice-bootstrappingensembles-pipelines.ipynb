{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# General imports:-\nimport numpy as np;\nfrom scipy.stats import iqr;\nimport pandas as pd;\nimport re;\nfrom datetime import date\n\nimport matplotlib.pyplot as plt;\n%matplotlib inline\nimport seaborn as sns;\nsns.set_style('darkgrid');\n\nfrom termcolor import colored;\nfrom warnings import filterwarnings; filterwarnings(action= 'ignore');\n\nfrom tqdm.notebook import tqdm;","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-15T17:29:49.610738Z","iopub.execute_input":"2022-06-15T17:29:49.611142Z","iopub.status.idle":"2022-06-15T17:29:49.624335Z","shell.execute_reply.started":"2022-06-15T17:29:49.611111Z","shell.execute_reply":"2022-06-15T17:29:49.623439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model specific libraries:-\n# 1. Pipeline specific imports:-\nfrom sklearn_pandas import DataFrameMapper, gen_features, NumericalTransformer;\nfrom sklearn_pandas.pipeline import Pipeline as skpPipeline;\n\nfrom sklearn.compose import ColumnTransformer;\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion;\nfrom sklearn.base import BaseEstimator, TransformerMixin;\n\n# 2. Model step specific imports:-\nfrom sklearn.impute import SimpleImputer;\nfrom sklearn.preprocessing import FunctionTransformer, StandardScaler, OrdinalEncoder, OneHotEncoder;\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, RFE, mutual_info_regression;\n\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge;\nfrom sklearn.tree import DecisionTreeRegressor;\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor;\nfrom xgboost import XGBRegressor;\nfrom lightgbm import LGBMRegressor;\nfrom catboost import CatBoostRegressor;\nfrom sklearn.svm import SVR;\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score;","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:10.080828Z","iopub.execute_input":"2022-06-15T16:43:10.081289Z","iopub.status.idle":"2022-06-15T16:43:10.08998Z","shell.execute_reply.started":"2022-06-15T16:43:10.08126Z","shell.execute_reply":"2022-06-15T16:43:10.089098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Advanced Regression for house price prediction:-**\n","metadata":{}},{"cell_type":"code","source":"# Global variable initialization:-\ntarget = 'SalePrice';\n\n# Null cutoff for dropping columns:-\nnull_cutoff = 0.10;\n\n# Skewness cutoff for treatment and transform:-\nskew_cutoff = 0.50;\n\n# Correlation threshold for feature selection:-\nfeat_sel_threshold = 0.0;\n\n# Train-set sampling fraction for bootstrapping sampling:-\ntrain_frac = 0.95;","metadata":{"execution":{"iopub.status.busy":"2022-06-15T17:27:41.49192Z","iopub.execute_input":"2022-06-15T17:27:41.493033Z","iopub.status.idle":"2022-06-15T17:27:41.49828Z","shell.execute_reply.started":"2022-06-15T17:27:41.492989Z","shell.execute_reply":"2022-06-15T17:27:41.497164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section0:- Data loading and basic information\n\n*This section loads the train-test data sets and displays column information*","metadata":{}},{"cell_type":"code","source":"# loading relevant datasets:-\nxytrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', encoding= 'utf8');\nxtest   = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv',  encoding= 'utf8');\n\nsub_fl = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv',  \n                     encoding= 'utf8');\n\nprint(colored(f\"Train data columns are \\n{xytrain.columns}\", color = 'blue'));\nprint(colored(f\"\\nTrain data details\\n\", color = 'blue', attrs= ['dark', 'bold']));\nxytrain.info();","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:10.108032Z","iopub.execute_input":"2022-06-15T16:43:10.108838Z","iopub.status.idle":"2022-06-15T16:43:10.191557Z","shell.execute_reply.started":"2022-06-15T16:43:10.108792Z","shell.execute_reply":"2022-06-15T16:43:10.190477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section1:- Preprocessing and basic data checks\n\n*This section is used to gain insights into the features and then transform the target*","metadata":{}},{"cell_type":"code","source":"print(colored(f\"\\nData preprocessing and pre-pipeline steps\\n\", color= 'blue', attrs= ['dark', 'bold']));\n\n# Splitting the data into features and target, test data is already split into features.\nxtrain, ytrain = xytrain.drop([target, 'Id'], axis= 1), xytrain[[target]];\nxtest.drop('Id', axis= 1, errors= 'ignore', inplace= True);\n\n# Typecasting already encoded columns from the datasets for label encoding later:-\nnum_enc_feat_lst = ['OverallQual', 'OverallCond', 'MSSubClass'];\nxtrain[num_enc_feat_lst] = xtrain[num_enc_feat_lst].astype(str);\nxtest[num_enc_feat_lst] = xtest[num_enc_feat_lst].astype(str);\n\n# Ensuing null checks:-\nnull_feat_lst = xtrain.isna().sum(axis= 0);\nnull_feat_lst = null_feat_lst[null_feat_lst > 0].sort_values(ascending= False);\nnum_null_feat_drop_lst = null_feat_lst[null_feat_lst > null_cutoff* xtrain.index.max()].index;\n\nxtrain.drop(num_null_feat_drop_lst, axis= 1, inplace= True,  errors= 'ignore');\nxtest.drop(num_null_feat_drop_lst, axis= 1, inplace= True, errors= 'ignore');\n\nprint(colored(\"\\nNull features in the training set\\n\", color= 'blue', attrs= ['bold', 'dark']));\ndisplay(null_feat_lst);\nprint(colored(f\"\\nFeatures with nulls > {null_cutoff:.2%} to be dropped are \\n{list(num_null_feat_drop_lst)}\\n\", \n              color= 'blue', attrs= ['bold', 'dark']));\nprint('\\n');\n\n# Plotting the null feature list:-\nfig, ax= plt.subplots(1,1,figsize= (7,7));\nax = null_feat_lst.plot.bar(color= 'tab:blue'); \nax.set_title('Null features in the training set', color = 'tab:blue', fontsize= 16);\nax.axhline(y = round(null_cutoff* xtrain.index.max()), color = 'darkred', linewidth = 1.5);\nax.set_ylabel('Null values', color = 'tab:blue', fontsize= 8);\nax.set_xlabel('Features', color = 'tab:blue', fontsize= 8);\n\nplt.xticks(color = 'tab:blue', fontsize= 8, rotation= 90); plt.yticks(fontsize= 8, color = 'tab:blue');\nplt.tight_layout(); \nplt.show();\ndel fig, ax;\n\n# Collating numeric and character columns from the training set:-\nnum_feat_lst = list(xtrain.head(1).select_dtypes(include= np.number).columns);\nchar_feat_lst = list(xtrain.head(1).select_dtypes(exclude= np.number).columns);\n\nprint(colored(f\"\\nTraining features after dropping null columns are-\\n\", color= 'blue', attrs= ['dark', 'bold']));\nprint(colored(f\"\\nNumeric training features are-\\n{num_feat_lst}\\n\", color= 'blue', attrs= ['dark']));\nprint(colored(f\"\\nObject training features are-\\n{char_feat_lst}\\n\", color= 'blue', attrs= ['dark']));\n\n# Describing the numeric features in the training data:-\nprint(colored(f\"\\nTraining set description\\n\", color= 'blue', attrs= ['bold', 'dark']));\ndisplay(xtrain.describe().transpose().style.format('{:,.0f}'));","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:10.193872Z","iopub.execute_input":"2022-06-15T16:43:10.19456Z","iopub.status.idle":"2022-06-15T16:43:10.760921Z","shell.execute_reply.started":"2022-06-15T16:43:10.194511Z","shell.execute_reply":"2022-06-15T16:43:10.760051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting target column and assessing its distribution:-\nprint('\\n');\nfig, ax = plt.subplots(1,2,figsize= (20,8));\nsns.distplot(ytrain[target].values, color = 'tab:blue', ax= ax[0]);\nsns.distplot(np.log(ytrain[target]), color = 'tab:blue', ax= ax[1]);\nax[0].set_title(\"Target column distribution without logarithm transform\", color= 'tab:blue', fontsize = 16, \n                loc= 'center');\nax[1].set_title(\"Target column distribution with logarithm transform\", color= 'tab:blue', fontsize = 16, \n                loc= 'center');\nplt.tight_layout();\nplt.show();\ndel fig, ax;","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:10.762045Z","iopub.execute_input":"2022-06-15T16:43:10.7624Z","iopub.status.idle":"2022-06-15T16:43:11.511498Z","shell.execute_reply.started":"2022-06-15T16:43:10.762367Z","shell.execute_reply":"2022-06-15T16:43:11.510555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Segmenting features for the upcoming pipeline:-","metadata":{}},{"cell_type":"code","source":"# Segmenting columns in the feature table (xtrain/ xtest) into groups for null treatment:-\nchar_fill_none_feat = ['MasVnrType'];\nchar_mode_fill_feat = [col for col in char_feat_lst if col != 'MasVnrType'];\n\nnum_fill_0_feat = ['BsmtFinSF1','BsmtFinSF2','HalfBath','Fireplaces','OpenPorchSF',\n                   'GrLivArea','BedroomAbvGr','EnclosedPorch','BsmtFullBath',\n                   'FullBath','KitchenAbvGr','GarageCars','3SsnPorch','MasVnrArea',\n                   '1stFlrSF','BsmtHalfBath','ScreenPorch','2ndFlrSF','WoodDeckSF','PoolArea'];\nnum_fill_median_feat = ['LotArea','LowQualFinSF','MiscVal','BsmtUnfSF','GarageArea','MoSold','YearBuilt'];\nnum_passthrough_feat = ['TotRmsAbvGrd','GarageYrBlt', 'YearRemodAdd', 'TotalBsmtSF', 'YrSold'];\n\nnew_feat_lst = ['Tot_Baths', 'Prop_Area', 'Porch_WdDeck', 'Pool_Fl',\n                'Prop_BldSell_Cyc','Is_Remodelled','Is_ReMdlB4Sale'];\n\nstd_feat_lst = ['BsmtFinSF1','BsmtFinSF2','OpenPorchSF', 'GrLivArea','MasVnrArea',\n                '2ndFlrSF','WoodDeckSF','PoolArea','LotArea','LowQualFinSF', 'MiscVal',\n                'BsmtUnfSF','GarageArea', 'TotalBsmtSF', 'Prop_Area', 'Prop_BldSell_Cyc'];\n\nfeat_lst = char_fill_none_feat + char_mode_fill_feat + num_fill_0_feat + num_fill_median_feat + num_passthrough_feat;\ndt_feat_lst = [col for col in num_feat_lst if re.findall(r\"yr|year|mo|sold\", col.lower()) != []];\n\nprint(colored(f\"\"\"Total unique segmented columns are- {len(set(feat_lst))}\\n\"\"\",\n              color= 'blue', attrs= ['bold', 'dark']));\nprint(colored(f\"\"\"Total features in the training data are- {len(num_feat_lst+char_feat_lst)}\\n\"\"\",\n             color= 'red', attrs= ['bold', 'dark']));\nprint(colored(f\"\"\"Year-date features in the training data are-\\n{dt_feat_lst}\\n\"\"\",\n              color= 'blue', attrs= ['bold', 'dark']));\n\n# Reordering train-test objects to the new feature list:-\nxtrain = xtrain[feat_lst];\nxtest = xtest[feat_lst];\n\nprint(colored(f\"\\nNewly ordered feature list is \\n\", color= 'blue', attrs= ['dark', 'bold']));\nprint(colored(f\"{feat_lst}\\n\", color= 'blue'));","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:11.513955Z","iopub.execute_input":"2022-06-15T16:43:11.514588Z","iopub.status.idle":"2022-06-15T16:43:11.53414Z","shell.execute_reply.started":"2022-06-15T16:43:11.514543Z","shell.execute_reply":"2022-06-15T16:43:11.532878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section2:- Model pipeline development\n\n*This section creates classes and adjutant functions used to develop the data ppeline and the subsequent model*","metadata":{}},{"cell_type":"code","source":"# Creating new features from the existing columns in the dataset:-\ndef Make_Features(df, feat_lst = feat_lst):\n    \"\"\"\n    This function creates new features from the existing features to improve predictablity\n    1. Total bathrooms\n    2. Total basement square feet area (null treated)\n    3. Property total area\n    4. Pool flag\n    5. Property build to sale period- this is clipped to 0 and positive values to prevent non-intuitive negative values\n    6. Remodelled flag\n    7. Remodelled before sale flag\n    8. Fireplace and wood-porch flag \n    9. Null filling for garage year built with year-built\n    This function is used with the sklearn function transformer as it is a stateless transformation\n    \n    Inputs- df (dataframe):- null treated and encoded dataframe from the previous pipeline step\n    Returns- df (dataframe):- new dataframe with the new features\n    \"\"\";\n      \n    # Calculating total bathrooms:-    \n    df['Tot_Baths'] = df.FullBath + df.BsmtFullBath + ((df.HalfBath + df.BsmtHalfBath) * 0.5);\n \n    # Calculating total basement square feet area:-\n    df['TotalBsmtSF'] = df['TotalBsmtSF'].fillna(df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['BsmtUnfSF']);\n    \n    # Calculating total property area:-    \n    df['Prop_Area'] = (df.TotalBsmtSF + df.WoodDeckSF + df.GrLivArea +df['3SsnPorch']+\n                       df.OpenPorchSF + df.ScreenPorch + df.EnclosedPorch +\n                       df.MasVnrArea + df.GarageArea + df.PoolArea);\n    \n    #  Calculating all porch and wood decks:-   \n    df['Porch_WdDeck'] = \\\n    df.OpenPorchSF + df.EnclosedPorch + df.ScreenPorch + df.WoodDeckSF + df['3SsnPorch'];\n        \n    #  Calculating flag for pool:-\n    df['Pool_Fl'] = np.where(df.PoolArea>0, 1,0);\n    \n    # Calculating property build-sell cycle and filling year sold nulls:-    \n    df['YrSold'] = df['YrSold'].fillna(date.today().year - 1);\n    df['Prop_BldSell_Cyc'] = np.clip(df['YrSold'] - df['YearBuilt'], a_min= 0.0, a_max= None);\n    \n    # Creating flag for remodelling:-\n    df['Is_Remodelled'] = np.where(df.YearRemodAdd != df.YearBuilt, 1,0);\n    \n    # Creating flag for remodelling just before sale:-    \n    df['Is_ReMdlB4Sale'] = np.where(abs(df['YrSold'] - df['YearRemodAdd']) <=1, 1, 0);\n         \n    # Replacing null values in garage year built with year-built:-\n    df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['YearBuilt']);\n    \n    # Displaying null attributes after the treatment:-  \n    _ = df.isna().sum(axis=0);\n    print(colored(f'\\nNull columns after processing and feature addition\\n', color='blue', attrs= ['bold', 'dark']));\n    display(_.loc[_>0]); print('\\n');\n    del _;\n       \n    return df;","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:11.535984Z","iopub.execute_input":"2022-06-15T16:43:11.536667Z","iopub.status.idle":"2022-06-15T16:43:11.552418Z","shell.execute_reply.started":"2022-06-15T16:43:11.536604Z","shell.execute_reply":"2022-06-15T16:43:11.551375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dropping year and date columns from the dataset:-\nclass DtColDropper(BaseEstimator, TransformerMixin):\n    \"\"\"This class drops the data columns from the dataset after all feature processing is done.\"\"\" \n    \n    def __init__(self, cols=None):\n        \"\"\"This function initialises the date columns from the input- dt_feat_lst global variable for the drop\"\"\"\n        if not isinstance(cols, list): self.cols = cols;\n        else: self.cols = cols;\n\n    def fit(self, X: pd.DataFrame, y= None): return self;\n\n    def transform(self, X:pd.DataFrame, y= None):\n        X1 = X.copy();\n        return X1.loc[:, ~X1.columns.isin(self.cols)];","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:11.553846Z","iopub.execute_input":"2022-06-15T16:43:11.554793Z","iopub.status.idle":"2022-06-15T16:43:11.567023Z","shell.execute_reply.started":"2022-06-15T16:43:11.554755Z","shell.execute_reply":"2022-06-15T16:43:11.566063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identifying and removing skewed columns:-\nclass SkewVarXformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class is a part of the overall processing pipeline that removes skewness from numerical variables.\n    It checks for skewed numeric variables and tries to reduce it by taking log of skewed variables.\n    \"\"\";   \n    \n    def __init__(self, skew_cutoff=skew_cutoff): \n        self.skew_cutoff = skew_cutoff;\n    \n    def fit(self, X, y= None, **fit_params):\n        \"\"\"\n        This function calculates the skewness for all variables in the dataset\n        \n        Inputs- \n        self- current state of the class\n        X,y (dataframe):- Input dataframe for the function\n        fit_params (dict):- keyword arguments for the function, if desired\n        \n        Returns- \n        self- current state of the class (learns the skewness of numeric columns)\n        \"\"\"; \n        \n        # Unearthing numeric columns from the dataframe:-\n        global char_feat_lst;  \n        #  Calculating skewness:-       \n        self.skew_val = X.iloc[:, len(char_feat_lst):].skew();      \n        return self;\n    \n    def transform(self, X, y= None, **transform_param):\n        \"\"\"\n        This function transforms highly skewed variables with the log transform\n        \n        Inputs- \n        self- current state of the class\n        X,y (dataframe):- Input dataframe for the function\n        skew_cutoff (float):- cutoff for the skewness for logarithm transform\n        transform_params (dict):- keyword arguments for the function, if desired \n        \n        Returns- \n        X1 (dataframe):- Dataframe with the modified values         \n        \"\"\";\n        \n        X1 = X.copy();\n        xform_col_lst = list(self.skew_val.loc[abs(self.skew_val) > self.skew_cutoff].index);\n        X1[xform_col_lst] = np.log1p(X1[xform_col_lst].values);\n        \n        return X1;   ","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:11.568226Z","iopub.execute_input":"2022-06-15T16:43:11.568872Z","iopub.status.idle":"2022-06-15T16:43:11.578157Z","shell.execute_reply.started":"2022-06-15T16:43:11.568841Z","shell.execute_reply":"2022-06-15T16:43:11.577517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating custom class for correlation and then filtration of features:-\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    This class is designed to elicit high univariate-performing independent variables with the target and shortlist such variables.\n    2 selection methods are used- corr (correlation) and mutual_info (mutual information regression)\n    \"\"\";\n    \n    def __init__(self, feat_sel_threshold:float, sel_mthd_lbl:str): \n        self.feat_sel_threshold = feat_sel_threshold\n        self.sel_mthd_lbl = sel_mthd_lbl.lower()\n        \n    def fit(self,X, y, target = target, **fit_params):\n        \"\"\"\n        This function calculates the Pearson correlation/ mutual info regression between the features and the target column in the training data        \n        \"\"\";\n        if self.sel_mthd_lbl == 'corr':\n            xy = pd.concat((X, y), axis= 1);\n            self.feat_sel_mtrc = xy.corr().drop([target], axis= 0)[[target]];\n            \n        elif self.sel_mthd_lbl == 'mutual_info': \n            self.feat_sel_mtrc = pd.DataFrame(mutual_info_regression(X,y, random_state= 10), index= X.columns, columns= [target]); \n            \n        return self;\n    \n    def transform(self, X, y = None, target= target, **transform_params):\n        \"\"\"\n        This function selects the high-performing columns and retains them in the relevant data-set\n        \"\"\";       \n        X1 = X.copy();\n        sel_feat_lst = list(self.feat_sel_mtrc.loc[abs(self.feat_sel_mtrc[target]) >= self.feat_sel_threshold, :].index);\n        \n        return X1.loc[:, sel_feat_lst];","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:11.5795Z","iopub.execute_input":"2022-06-15T16:43:11.580096Z","iopub.status.idle":"2022-06-15T16:43:11.593156Z","shell.execute_reply.started":"2022-06-15T16:43:11.580055Z","shell.execute_reply":"2022-06-15T16:43:11.592302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This class transforms the target column based on the user-defined transform function:-\nclass TargetXformer(BaseEstimator, TransformerMixin):\n    \"\"\"This class transforms the target column in the train set and then defines the inverse transform for the predictions\"\"\"; \n    \n    def __init__(self, xform_func_lbl:str): self.xform_func_lbl = xform_func_lbl.lower()\n        \n    def fit(self, X:pd.DataFrame, y= None, **fit_params): \n        return self\n    \n    def transform(self, X:pd.DataFrame , y=None, target= target, **transform_params):\n        \"This function transforms the target column\";\n        X1= X.copy();\n        if self.xform_func_lbl == 'log': \n            X1['xform'] = np.log(X1[target]);\n            X1.drop(target, axis=1, inplace= True);\n            X1.rename({'xform': target}, axis= 1, inplace= True); \n        return X1;\n    \n    def inverse_transform(self, X, y=None, target= target, **inv_xform_params):\n        \"This function inverts the transform based on the transform function used\"; \n        X1 = X.copy();\n        if self.xform_func_lbl == 'log': X1 = np.exp(X1);\n        return X1;","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:11.594578Z","iopub.execute_input":"2022-06-15T16:43:11.595064Z","iopub.status.idle":"2022-06-15T16:43:11.607254Z","shell.execute_reply.started":"2022-06-15T16:43:11.595024Z","shell.execute_reply":"2022-06-15T16:43:11.606644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This is a pipeline designed with sklearn_pandas methods, refer the link below for more details:- ##\n\nhttps://github.com/scikit-learn-contrib/sklearn-pandas","metadata":{}},{"cell_type":"code","source":"# Null treatment and ordinal encoding with sklearn_pandas pipeline:-\nfill_mode_ord_enc_feat = \\\ngen_features(columns= [col.split(' ') for col in char_mode_fill_feat], \n             classes= [{'class': SimpleImputer, 'strategy':'most_frequent'},\n                      {'class': OrdinalEncoder, 'handle_unknown': 'use_encoded_value', 'unknown_value': 99}], \n             suffix= {});\n\nfill_0_num= gen_features(columns= [col.split(' ') for col in num_fill_0_feat], classes= [{'class': SimpleImputer, 'strategy':'constant'}], suffix= {});\n\nfill_median_num= gen_features(columns= [col.split(' ') for col in num_fill_median_feat], classes= [{'class': SimpleImputer, 'strategy':'median'}], suffix= {});\n\nnull_trmt_ord_enc = \\\nskpPipeline([(\"fill_None_mapper\", DataFrameMapper(default= None,df_out= True, input_df= True, \n                                                  features= [(char_fill_none_feat,[SimpleImputer(strategy= 'constant', fill_value= 'None'), \n                                                                                   OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = 99)])])),\n                   (\"fill_mode_mapper\", DataFrameMapper(features= fill_mode_ord_enc_feat, default= None,df_out= True, input_df= True)),\n                   (\"fill_0_mapper\", DataFrameMapper(features= fill_0_num, default= None,df_out= True, input_df= True)),\n                   (\"fill_median_mapper\", DataFrameMapper(features= fill_median_num, default= None,df_out= True, input_df= True))\n            ]);\n\n# Standardizing select numerical features:-\nstd_num_feat = DataFrameMapper(gen_features(columns = [col.split(' ') for col in std_feat_lst], classes = [StandardScaler], suffix= {}),\n                               default= None,df_out= True, input_df= True);","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:11.609924Z","iopub.execute_input":"2022-06-15T16:43:11.610488Z","iopub.status.idle":"2022-06-15T16:43:11.624296Z","shell.execute_reply.started":"2022-06-15T16:43:11.610446Z","shell.execute_reply":"2022-06-15T16:43:11.623686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The below pipeline uses sklearn and sklearn-pandas structures:-\n\ny_processor = Pipeline([('target_xform', TargetXformer(xform_func_lbl= 'log')),\n                        ('tgt_zscorer', StandardScaler())]);\n\nx_procesor = Pipeline([('null_trmt_ord_enc', null_trmt_ord_enc), \n                       ('new_feat_dev', FunctionTransformer(Make_Features)),\n                       ('drop_dt_feat', DtColDropper(cols= dt_feat_lst)),\n                       ('skew_var_xform', SkewVarXformer(skew_cutoff= skew_cutoff)),\n                       ('z_scorer', std_num_feat),\n                       ('feat_sel', FeatureSelector(feat_sel_threshold= feat_sel_threshold, \n                                                    sel_mthd_lbl= 'corr'))                    \n                      ]);","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:11.625502Z","iopub.execute_input":"2022-06-15T16:43:11.626034Z","iopub.status.idle":"2022-06-15T16:43:11.639176Z","shell.execute_reply.started":"2022-06-15T16:43:11.625995Z","shell.execute_reply":"2022-06-15T16:43:11.638354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(colored(f\"Training set pipeline invocation:-\", color= 'red', attrs= ['bold', 'dark']));\ny1= pd.DataFrame(y_processor.fit_transform(ytrain), columns = [target]);\nx1= x_procesor.fit_transform(xtrain, y1);\n\nprint(colored(f\"Test set pipeline invocation:-\", color= 'red', attrs= ['bold', 'dark']));\nxt= x_procesor.transform(xtest);\n\nprint(colored(f\"Dataframes x1-y1 are the pipeline outputs for the model training, xt for test\\n\", \n              color= 'red', attrs= ['bold', 'dark']));","metadata":{"execution":{"iopub.status.busy":"2022-06-15T16:43:11.640562Z","iopub.execute_input":"2022-06-15T16:43:11.641073Z","iopub.status.idle":"2022-06-15T16:43:12.473989Z","shell.execute_reply.started":"2022-06-15T16:43:11.641031Z","shell.execute_reply":"2022-06-15T16:43:12.473105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section3:- Model Training\n\nThis is an update in my latest version. \n\nI hereby use a bootstrapping sampling of say, n-model samples with a 95% training data-set and execute one/ more ensemble models per candidate. The remaining 5% of the training data is considered as an in-sample validation set. Finally, I consider the mean/ median of the entire set of candidates as my submission predictions (on the test-set).","metadata":{}},{"cell_type":"code","source":"print(colored(f\"\\nSubmission sample file:-\\n\", color= 'blue', attrs= ['bold', 'dark']));\ndisplay(sub_fl.head(5).style.format({'SalePrice': '{:,.0f}'}));\n\n# Creating output dataframe to store the predictions in line with the submission sample:-\nmdl_pred_prf = pd.DataFrame(data= None, index= sub_fl.Id, columns= None);","metadata":{"execution":{"iopub.status.busy":"2022-06-15T17:08:40.009973Z","iopub.execute_input":"2022-06-15T17:08:40.010837Z","iopub.status.idle":"2022-06-15T17:08:40.022343Z","shell.execute_reply.started":"2022-06-15T17:08:40.0108Z","shell.execute_reply":"2022-06-15T17:08:40.021209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Train_Ensembles(mthd:str, n_estimators: np.int16= 500, nb_mdl: np.int16 = 500):\n    \"\"\"\n    This function implements the below routine- \n    1. Sample the dataset into train and test components based on random seed\n    2. Invoke the model method\n    3. Train the model on the train-set\n    4. Accumulate test-set predictions and collate in output table\n    \n    Inputs- \n    1. mthd- (string):-  model method \n    2. n_estimators (int):- number of trees\n    3. nb_mdl (int):- number of candidates \n    \"\"\"\n    \n    global train_frac;\n    \n    for mdl_nb in tqdm(range(nb_mdl), desc = f'\\nModel training progress\\n'):\n        xtr = x1.sample(frac= train_frac, random_state = mdl_nb);\n        ytr = y1.loc[xtr.index];   \n        xdev, ydev = x1.loc[~x1.index.isin(xtr.index)], y1.loc[~y1.index.isin(ytr.index)];\n\n        if mthd == 'LGBM': \n            mdl = LGBMRegressor(n_estimators= n_estimators, max_depth= 7, n_jobs = -1, \n                                learning_rate = 0.08, objective= 'regression', metric = ['rmse']);\n        elif mthd == 'XgBoost':\n            mdl = XGBRegressor(n_estimators= n_estimators, max_depth= 9,n_jobs= -1, learning_rate = 0.08,\n                              eval_metric  = ['rmse']);\n\n        elif mthd == 'CatBoost':\n            mdl = CatBoostRegressor(learning_rate = 0.08, max_depth = 9, eval_metric = 'RMSE');\n\n        mdl.fit(xtr, ytr, eval_set= [(xtr, ytr), (xdev, ydev)], \n                early_stopping_rounds = int(n_estimators/10),verbose= int(n_estimators/10));\n\n        mdl_pred_prf.loc[:, f\"{mthd}_{mdl_nb}\"] = \\\n        y_processor.inverse_transform(np.expand_dims(mdl.predict(xt),1));","metadata":{"execution":{"iopub.status.busy":"2022-06-15T17:29:57.743845Z","iopub.execute_input":"2022-06-15T17:29:57.7445Z","iopub.status.idle":"2022-06-15T17:29:57.756268Z","shell.execute_reply.started":"2022-06-15T17:29:57.744465Z","shell.execute_reply":"2022-06-15T17:29:57.755464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implementing the model training routine\nTrain_Ensembles(mthd = 'XgBoost', n_estimators= 500, nb_mdl= 500);\nTrain_Ensembles(mthd = 'LGBM', n_estimators= 500, nb_mdl= 500);\nTrain_Ensembles(mthd = 'CatBoost', n_estimators= 500, nb_mdl= 500);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section4:- Preparing submission files","metadata":{}},{"cell_type":"code","source":"# Collating model prediction columns:-\nmdl_pred_prf['SalePrice'] = np.mean(mdl_pred_prf, axis=1);\nmdl_pred_prf[['SalePrice']].reset_index().to_csv('Submission.csv', index= False);","metadata":{"execution":{"iopub.status.busy":"2022-06-15T17:34:34.280463Z","iopub.execute_input":"2022-06-15T17:34:34.281313Z","iopub.status.idle":"2022-06-15T17:34:34.301346Z","shell.execute_reply.started":"2022-06-15T17:34:34.281276Z","shell.execute_reply":"2022-06-15T17:34:34.300409Z"},"trusted":true},"execution_count":null,"outputs":[]}]}