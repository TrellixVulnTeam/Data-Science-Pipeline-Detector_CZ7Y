{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error,mean_squared_log_error\nimport sklearn\nimport sklearn.feature_selection\nfrom sklearn.preprocessing import OneHotEncoder\nimport seaborn as sns\n\n# load the datasets into dataframe\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv',index_col='Id')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv',index_col='Id')\nsample = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\nY = train['SalePrice']\n\ntrain = train.drop(['SalePrice'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:09.854867Z","iopub.execute_input":"2021-05-23T01:22:09.85525Z","iopub.status.idle":"2021-05-23T01:22:09.932868Z","shell.execute_reply.started":"2021-05-23T01:22:09.855212Z","shell.execute_reply":"2021-05-23T01:22:09.93173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hello everyone, this is my attempt at the housing prices. I will explain my thought process on how I designed my model, and areas that I think can still be improved. Feel free to fork this notebook for your own attempt and try to improve the model yourself! If this notebook has helped you, please upvote and leave a comment if you notice any mistakes!\n\nCheers,\nEugene","metadata":{}},{"cell_type":"code","source":"#Here we are just taking a look at what variables are there, and what datatypes are there\nN,M = train.shape\nprint('Number of Samples',N,'Number of Features',M)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:11.125033Z","iopub.execute_input":"2021-05-23T01:22:11.125442Z","iopub.status.idle":"2021-05-23T01:22:11.157396Z","shell.execute_reply.started":"2021-05-23T01:22:11.125404Z","shell.execute_reply":"2021-05-23T01:22:11.156533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_2,m_2 = test.shape\n\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:12.109023Z","iopub.execute_input":"2021-05-23T01:22:12.10942Z","iopub.status.idle":"2021-05-23T01:22:12.137714Z","shell.execute_reply.started":"2021-05-23T01:22:12.109385Z","shell.execute_reply":"2021-05-23T01:22:12.136948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I think the first thing that I noticed was that there are lot of NaNs in some of the features. This can be quite a problem, since NaNs can imply a lack of knowledge for that sample. However given that we have so many features, it is possible that we can remove a few features due to missing values.","metadata":{}},{"cell_type":"code","source":"percent_missing = train.isnull().mean() * 100 \npercent_missing = percent_missing.sort_values()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:13.218384Z","iopub.execute_input":"2021-05-23T01:22:13.218752Z","iopub.status.idle":"2021-05-23T01:22:13.23268Z","shell.execute_reply.started":"2021-05-23T01:22:13.218722Z","shell.execute_reply":"2021-05-23T01:22:13.231483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\nplt.bar(np.arange(len(percent_missing)), percent_missing)\nplt.ylabel('Percentage of Missing Values')\nplt.xticks(np.arange(len(percent_missing)),percent_missing.keys(),rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:13.779832Z","iopub.execute_input":"2021-05-23T01:22:13.780181Z","iopub.status.idle":"2021-05-23T01:22:15.028275Z","shell.execute_reply.started":"2021-05-23T01:22:13.780151Z","shell.execute_reply":"2021-05-23T01:22:15.027189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So clearly, there are a few features where there a large number of samples are missing values. Rather than just removing them outright, I took a slightly different approach. I am first going measure the mutual information of each feature for our target variable and then select the features that pass a cutoff. To do this, we are first going to normalize and transform the values before computing the mutual information.","metadata":{}},{"cell_type":"code","source":"\n#Normalization and feature selection\nmutual = []\n\n\ntrain_x = np.zeros((N,1))\ntest_x = np.zeros((N_2,1))\nfor name in train.columns:\n    \n    #Categorical Features\n\n    if train[name].dtypes == 'O':\n        \n        #Label Encoder to embed our categories\n        le = preprocessing.LabelEncoder()\n        \n        \n        #print(name,train[name].unique())\n        \n        #Rather than filling our missing values with known quantities, I am simply gonna leave it as a seperate category\n        train[name].fillna('missing',inplace=True)\n        \n        train[name] = train[name].astype(str)\n        Canadate = le.fit_transform(train[name])\n        Canadate = Canadate.reshape(-1, 1)\n        \n        ##Now we are going to calculate the mutual information \n        m  = sklearn.feature_selection.mutual_info_regression(Canadate,Y)[0]\n        mutual += [m]\n        \n        #informational cutoff of 0.1\n        if m > 0.1:\n            print('Variable Added')\n            print('Type:',train[name].dtypes,'Variable',name,'Most Frequent Label:',train[name].mode().values,'Number of Labels:', len(train[name].unique()))\n            #print(OneHotEncoder().fit_transform(Canadate).toarray())\n            \n            #Finally since they are categorical variables, we are going to perform one_hot_encoding on them\n            One_hot = OneHotEncoder()\n            train_x = np.append(train_x,One_hot.fit_transform(Canadate).toarray(),axis=1)\n            \n            #Same thing as above but for the test set\n            test[name].fillna(train[name].mode().values[0],inplace=True)\n            test[name] = test[name].astype(str)\n\n            Canadate = le.transform(test[name])\n            Canadate = Canadate.reshape(-1, 1)\n            test_x = np.append(test_x,One_hot.transform(Canadate).toarray(),axis=1)\n            \n            \n            \n    elif train[name].dtypes == 'int' or train[name].dtypes == 'float':\n        \n        #Now continuous variables, we are going to use a robust scaler\n        norm = preprocessing.RobustScaler()\n        \n        #Since we are going normalize each variable, we are filling the Nans with the mean value\n        train[name].fillna(train[name].mean(),inplace=True)\n        \n        \n        Canadate = norm.fit_transform(train[name].values.reshape(-1, 1))\n        \n        #Outlier Cutoff of 4 and -2, feel free to play around with this\n        Canadate[Canadate > 4] = 4\n        Canadate[Canadate < -2] = -2\n        \n        #Again mutual information\n        m  = sklearn.feature_selection.mutual_info_regression(Canadate,Y)[0]\n        mutual += [m]\n        \n        \n        if m > 0.1:\n            \n            \n            print('Variable Added')\n            print('Type:',train[name].dtypes,'Variable',name,'Average Value',train[name].mean())\n            train_x = np.append(train_x,Canadate,axis=1)\n            \n            \n            \n            #Repeat for test set\n            test[name].fillna(train[name].mean(),inplace=True)\n            Canadate = norm.transform(test[name].values.reshape(-1, 1))\n            Canadate[Canadate > 4] = 4\n            Canadate[Canadate < -2] = -2\n            #print(min(Canadate))\n            #print(max(Canadate))\n            test_x = np.append(test_x,Canadate,axis=1)\n            \n            \ntrain_x= train_x[:,1:]\ntest_x = test_x[:,1:]","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:15.663851Z","iopub.execute_input":"2021-05-23T01:22:15.664254Z","iopub.status.idle":"2021-05-23T01:22:16.868543Z","shell.execute_reply.started":"2021-05-23T01:22:15.66422Z","shell.execute_reply":"2021-05-23T01:22:16.867473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Final Training Shape:',train_x.shape)\nprint('Final Test Shape:',test_x.shape)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:16.869835Z","iopub.execute_input":"2021-05-23T01:22:16.870105Z","iopub.status.idle":"2021-05-23T01:22:16.875949Z","shell.execute_reply.started":"2021-05-23T01:22:16.870079Z","shell.execute_reply":"2021-05-23T01:22:16.874863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's take a look at the mutual information of each variable. We see that there are quite a few standouts and quite a few variables that show some promise. Some might argue that the 0.1 cutoff was too lenient or too harsh. That might be true, but the variables are already quite correlated, and so the addition of lesser informative variable most likely won't help too much.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,6))\n\nplt.bar(np.arange(79), mutual)\nplt.xticks(np.arange(79),train.columns,rotation=90)\nplt.ylabel('Mutual Information')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:18.875399Z","iopub.execute_input":"2021-05-23T01:22:18.875804Z","iopub.status.idle":"2021-05-23T01:22:20.083573Z","shell.execute_reply.started":"2021-05-23T01:22:18.875767Z","shell.execute_reply":"2021-05-23T01:22:20.082751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets scale our target variable. I tried several methods, but the log method won in the end and produced the best results.","metadata":{}},{"cell_type":"code","source":"\n#Price_scale =  preprocessing.StandardScaler()\n#Y = Price_scale.fit_transform(Y.values.reshape(-1,1))\nY =  np.log1p(Y)\nX_train,X_val,y_train,y_val = train_test_split(train_x,Y,test_size = 0.1,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:22.195777Z","iopub.execute_input":"2021-05-23T01:22:22.196279Z","iopub.status.idle":"2021-05-23T01:22:22.202607Z","shell.execute_reply.started":"2021-05-23T01:22:22.196236Z","shell.execute_reply":"2021-05-23T01:22:22.201774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model Design:**\n\nWe are going to make use of two models; ElasticNet and XGBregressor. Our final predictions are going to an average of the two models. One could add more models and most likely improve on the accuracy of the final prediction.","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nlr = linear_model.ElasticNetCV(cv=5,n_alphas=10,l1_ratio=[.1, .5, .7, .9, .95, .99, 1])\n\nxg_reg = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 20000)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:23.595209Z","iopub.execute_input":"2021-05-23T01:22:23.595736Z","iopub.status.idle":"2021-05-23T01:22:23.599908Z","shell.execute_reply.started":"2021-05-23T01:22:23.595703Z","shell.execute_reply":"2021-05-23T01:22:23.599109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting\nxg_reg.fit(X_train,y_train)\nlr.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:24.557311Z","iopub.execute_input":"2021-05-23T01:22:24.557674Z","iopub.status.idle":"2021-05-23T01:22:41.183685Z","shell.execute_reply.started":"2021-05-23T01:22:24.557643Z","shell.execute_reply":"2021-05-23T01:22:41.182586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#validation testing\ny_train_pred = xg_reg.predict(X_val)\ny_train_pred_l = lr.predict(X_val)","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:43.561919Z","iopub.execute_input":"2021-05-23T01:22:43.562275Z","iopub.status.idle":"2021-05-23T01:22:43.58838Z","shell.execute_reply.started":"2021-05-23T01:22:43.562229Z","shell.execute_reply":"2021-05-23T01:22:43.587046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(1)\nplt.title('XGBRegressor')\nplt.plot(np.exp(y_val),np.exp(y_train_pred),'.')\nplt.xlabel('True Values')\nplt.ylabel('Predictions')\nprint(mean_squared_log_error(np.exp(y_val),np.exp(y_train_pred)))\nplt.figure(2)\nplt.title('ElasticNetCV')\nplt.plot(np.exp(y_val),np.exp(y_train_pred_l),'.')\nprint(mean_squared_log_error(np.exp(y_val),np.exp(y_train_pred_l)))\nplt.xlabel('True Values')\nplt.ylabel('Predictions')","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:45.30249Z","iopub.execute_input":"2021-05-23T01:22:45.302868Z","iopub.status.idle":"2021-05-23T01:22:45.611975Z","shell.execute_reply.started":"2021-05-23T01:22:45.302834Z","shell.execute_reply":"2021-05-23T01:22:45.611218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#predicting our test sets\ny_pred_l=lr.predict(test_x)\ny_pred=xg_reg.predict(test_x)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:49.125954Z","iopub.execute_input":"2021-05-23T01:22:49.126659Z","iopub.status.idle":"2021-05-23T01:22:49.234515Z","shell.execute_reply.started":"2021-05-23T01:22:49.12662Z","shell.execute_reply":"2021-05-23T01:22:49.233174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Taking the average\npred_avg= np.mean([y_pred_l,y_pred],axis=0)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:22:51.548922Z","iopub.execute_input":"2021-05-23T01:22:51.549284Z","iopub.status.idle":"2021-05-23T01:22:51.554267Z","shell.execute_reply.started":"2021-05-23T01:22:51.549239Z","shell.execute_reply":"2021-05-23T01:22:51.553333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Submission\nsubmission=pd.DataFrame()\nsubmission['Id']=test.index\nsubmission['SalePrice']=np.exp(pred_avg)\nsubmission.to_csv(\"submission.csv\",index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:24:33.399789Z","iopub.execute_input":"2021-05-23T01:24:33.40014Z","iopub.status.idle":"2021-05-23T01:24:33.415323Z","shell.execute_reply.started":"2021-05-23T01:24:33.40011Z","shell.execute_reply":"2021-05-23T01:24:33.414377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-05-23T01:24:29.687296Z","iopub.execute_input":"2021-05-23T01:24:29.687684Z","iopub.status.idle":"2021-05-23T01:24:29.701539Z","shell.execute_reply.started":"2021-05-23T01:24:29.687653Z","shell.execute_reply":"2021-05-23T01:24:29.700287Z"},"trusted":true},"execution_count":null,"outputs":[]}]}