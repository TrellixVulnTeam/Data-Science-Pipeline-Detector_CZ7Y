{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"version":"3.6.3","name":"python","mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","file_extension":".py"}},"cells":[{"metadata":{"_execution_state":"idle","_cell_guid":"ad9041bf-ae22-4a7d-84b2-6a1652b23805","_uuid":"310ea9f79c0253324d759f21fd0e07a181ad6030"},"cell_type":"markdown","source":"Kaggle is an excellent platform to work on examples and improve your Machine Learning Skills. Last month, I started reading about Machine Learning and jumped into the example in Kaggle to apply my knowledge. \n\nThe below example is for the dataset in House Prices: Advanced Regression Techniques on Kaggle. You have Train and Test data which has been provided and you need to submit the predicted prices for the Test dataset to Kaggle. \n\nI have a thing for Python, I guess she was my first girlfriend in the world of Machine Learning and as you all know it is very difficult to forget the first love. Hence here I have used Python to write my code. \n\nKaggle Score - 0.14389 Public Leaderboard - 1023 (at time of finalising this notebook)\n\nNumerical features have only been selected and feature selection has been done primarily on correlation coffecient. "},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Import the necessary Python Packages. \nimport pandas as pd \nimport numpy as np \nimport seaborn as sns \nimport matplotlib as plt \nimport sklearn\n\n# Set ipython's max row display\npd.set_option('display.max_row', 10000)\n\n#Setting to print all the values in array\nnp.set_printoptions(threshold=np.nan)\n\n# Set iPython's max column width to 50\npd.set_option('display.max_columns', 500)","metadata":{"_execution_state":"idle","_cell_guid":"70a6e37a-f277-439b-ada7-67b37b98265c","collapsed":true,"_uuid":"9b97f5c1c2e76432d884b7fe2cbc1c8eaf9b5f0a"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Import Dataset downloaded from the Kaggle https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\n\ntraindata = pd.read_csv('../input/train.csv')\ntestdata = pd.read_csv('../input/test.csv')","metadata":{"_uuid":"292ae7ca967442190c58b0fdac05d8515a4e8a56","_cell_guid":"705a3383-3ef8-42e4-aaa8-0b7419aa4a1e","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Let us try to understand more about the data.\ntraindata.info()","metadata":{"_uuid":"87a4f626a7ef632415c50507d9afbe3ed5cb3b6a","_cell_guid":"5ed240b3-a3e8-45f0-bc53-5cb8f9572f61","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"total = traindata.isnull().sum().sort_values(ascending=False)\npercent = (traindata.isnull().sum()/traindata.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","metadata":{"_uuid":"0485305c014d31674999967f43ea9bed297df378","_cell_guid":"3a41ea9d-3d9d-4a4f-92b1-0944bb6078a3","collapsed":true,"scrolled":false}},{"metadata":{"_uuid":"f33148064452e1c52e7c53b1b6a4848c39b7104a","_cell_guid":"07cef745-9084-48de-960f-f2ac027c8762"},"cell_type":"markdown","source":"So we see that there are so columns which have lot of missing data. I would get rid of any such columns which have too many nulls as they seem to be very in-significant for the sale price prediction. I would get rid of 'Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature' columns here. For the rest we will decide if we want to fill them up with something, if we select them as a feature for our prediction. "},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Get ridding of the columns with lot of missing data\ntraindata = traindata.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1)\ntestdata = testdata.drop(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1)","metadata":{"_uuid":"0f3efe7df30d7404dac3d542669557cdc66fc99a","_cell_guid":"0d358b98-fdf0-4ea3-8f65-96d24799f91d","collapsed":true}},{"metadata":{"_uuid":"1a1a4e43c8db21ccc946c3169c551a4435d2e6a2","_cell_guid":"b8419884-6c7e-484a-a2d4-36b11e25e0e2"},"cell_type":"markdown","source":"Next, we would like to understand the select only the numerical features for our analysis for now. Let us filter out all the numerical columns and create a new dataset. In future I will also try to take into account the columns which are non-numeric and might impact the prices. We would use the Pearson correlation coefficient between the Sale Price and the numerical features."},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#First Deal with the numerical variables then move to the categorical string variables.\n#Create Data set with numerical variables\nnum_trainData = traindata.select_dtypes(include = ['int64', 'float64'])\nnumcol = ['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'MoSold', 'YrSold', 'SalePrice']","metadata":{"_uuid":"ac7c75336714fcb3240285ec0520a608687b2b09","_cell_guid":"2d7927b0-25c3-4a5e-aba4-26c5b56500e4","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Find out correlation with numerical features\ntraindata_corr = num_trainData.corr()['SalePrice'][:-1]\ngolden_feature_list = traindata_corr[abs(traindata_corr) > 0].sort_values(ascending = False)\nprint(\"Below are {} correlated values with SalePrice:\\n{}\".format(len(golden_feature_list), golden_feature_list))","metadata":{"_uuid":"e8f1536439ade92fe839d630fc0b30ec94b7b79a","_cell_guid":"16eda906-553a-4c45-b667-9b7394879452","collapsed":true}},{"metadata":{"_uuid":"b2399c8ca8b3e564c499579a084ebac5164de3f1","_cell_guid":"e0d9a7e4-8a1c-42b6-b338-572c80390daf"},"cell_type":"markdown","source":"I would first consider the following columns: \n\nOverallQual      0.790982\nGrLivArea        0.708624\nGarageCars       0.640409\nGarageArea       0.623431\nTotalBsmtSF      0.613581\n1stFlrSF         0.605852\nFullBath         0.560664\nTotRmsAbvGrd     0.533723\nYearBuilt        0.522897\nYearRemodAdd     0.507101\n\nNow OverallQual, GrLivArea seem to have a very strong correlation with SalePirce. I would definetly consider them. \nHowever before taking call on rest of the columns I would see if features are also correated to each other. So lets create a heatmap and see the results. "},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Create heatmap for correlated numerical variables\n%matplotlib inline\ntraindata_corrheatmap = num_trainData.corr()\ncols = traindata_corrheatmap.nlargest(10, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(num_trainData[cols].values.T)\nsns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)","metadata":{"_uuid":"34d53625a04843a4149582c66a0165d705ecd2cb","_cell_guid":"d14be2c8-4a98-4db5-b735-5c249ab1d125","collapsed":true,"scrolled":true}},{"metadata":{"_uuid":"047928653e23412f9facb319fb780eace9419428","_cell_guid":"28b8c96c-0549-4395-aa16-563e1169fd77"},"cell_type":"markdown","source":"So above heatmap clearly shows thet Garage Cars and Garage Area are co-related to each other. Hence does not make sense to take both into Feature list. I will go for Garage Area. Similarly I see that 1stFlrSF and TotalBsmtSF are closely related. Also GrLiv Area and TotRmsAbvGrd seem to quite strongly Correlated. I would go with following features:\n'OverallQual','MSSubClass', 'KitchenAbvGr','OverallCond', 'GrLivArea', 'EnclosedPorch', 'GarageArea','TotalBsmtSF', \n\nNow why MSSubClass and OverallCond ? A negative correlation means that there is an inverse relationship between two variables - when one variable decreases, the other increases. Hence the features. We can select other as well, but for now let's see how these look.\n\nNext let's try to understand more about are star of the show 'SalePrice'. We would like to understand the distribution of data.\n\nSalePrice does not look very normal here. There is 'peakedness', positive skewness and the data does not follow the diagonal line. Let us also try solve the problem here and see how does the data look. I also would like to see how does the data look like for GrLivArea and TotalBsmtSF "},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Understand the distribution of the Sale Price\ntraindata['SalePrice'].describe()","metadata":{"_uuid":"4887ffaa0a7d702ceb5094b82e1569ad3a0615f5","_cell_guid":"4592c12e-2dbb-4597-a468-02d9a0b8b8ac","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"traindata['SalePrice'].skew()","metadata":{"_uuid":"a0a792d53f6870bb7fb3689460214afb6352dec6","_cell_guid":"e85d3250-d90e-4a60-a846-780fe85ad0df","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"traindata['SalePrice'].kurtosis()","metadata":{"_uuid":"6851a53e8c9ef77cf6cc8d2a29a89d1d59465c36","_cell_guid":"708d3b16-e972-4b91-900f-0e6ac9b85528","collapsed":true}},{"metadata":{"_uuid":"1e98e1108e320eaf72e9b39d88ae9de37215951d","_cell_guid":"19eea52a-907a-420b-9d6b-ca1017042799"},"cell_type":"markdown","source":"The Sale Prices dont seem to be evenly distributed and is deviating from normal distribution. Also the peakedness is quite high and has a positive skewness."},{"execution_count":null,"cell_type":"code","outputs":[],"source":"sns.distplot(traindata['SalePrice'], color = 'b', bins = 100)","metadata":{"_uuid":"7ae20a6a578625f3e93abce08102524879cc6c5d","_cell_guid":"fe376923-5054-400e-af0c-3f26d7d677a1","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"from scipy import stats\nimport matplotlib.pyplot as plt\nres = stats.probplot(traindata['SalePrice'], plot=plt)","metadata":{"_uuid":"49886ba39532611bb841fc9504e31d8cdc6ccf06","_cell_guid":"170e4903-55ff-4181-9227-f1644452abe1","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"sns.distplot(np.log(traindata['SalePrice']), color = 'r', bins = 100)","metadata":{"_uuid":"5621753e1d4d5bd80967a3d2ea2a5af3e75ff89c","_cell_guid":"c1e00c50-a3a7-4ba6-b97d-10e0a0bb9159","collapsed":true,"scrolled":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"res = stats.probplot(np.log(traindata['SalePrice']), plot=plt)","metadata":{"_uuid":"145cfe8daa4e5980e22ca1edc5bc5290fbd6befc","_cell_guid":"90620fc4-2981-4393-b608-be88a25f2843","collapsed":true}},{"metadata":{"_uuid":"6e2d5d75b750d31ae2e33be6ca415f3b0790607d","_cell_guid":"2fb5e5aa-f083-4ac3-a939-567cf56fd55d"},"cell_type":"markdown","source":"So what did above was, first I checked the distribution of the Sale Price Data which was available in the data set provided by Kaggle. And I see that the data is not normally distributed. \n\nSo I log transformed the sale prices and checked the distribution agains. The data now looks normally distributed and we would keep this in mind when we go for our prediction. The Same approach has been taken for GrLivingArea and TotalBsmtSF. We would log transform the data for both these features as well. "},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Understand the behaviour of data in GrLivArea\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nres = stats.probplot(traindata['GrLivArea'], plot=plt)","metadata":{"_uuid":"c34bd47dc92e88cc661ba5f035a3da2d3c4e20b8","_cell_guid":"87866ce3-2e8c-480e-98f0-6f9721b4a4be","collapsed":true,"scrolled":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"sns.distplot(traindata['GrLivArea'], color = 'b', bins = 100)","metadata":{"_uuid":"d5b8422c5cc0f42578091cfb5aada4481dce09a5","_cell_guid":"ec4459f8-97ec-4579-a7f6-8155f317d65f","collapsed":true,"scrolled":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"sns.distplot(np.log(traindata['GrLivArea']), color = 'b', bins = 100)","metadata":{"_uuid":"792870f0cf59dcafc02ca620d439e2fa8a7a98e4","_cell_guid":"22b4a60f-2578-4a77-a8df-dca5ce96c6e6","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"from scipy import stats\nres = stats.probplot(np.log(traindata['GrLivArea']), plot=plt)","metadata":{"_uuid":"14187cc6b7c43bcbfda90e54ad1076de6c2d11fb","_cell_guid":"ec124db8-67d0-4c1b-a4d1-f7aea5b6260b","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Understand the skewness of TotalBsmtSF\nsns.distplot(traindata['TotalBsmtSF'], color = 'b', bins = 100)","metadata":{"_uuid":"d46222eb1b4334b33ee11a9f4cc1a329d0357b13","_cell_guid":"9c9ac29b-468c-4582-82d7-16af1354db9b","collapsed":true}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"res = stats.probplot(traindata['TotalBsmtSF'], plot=plt)","metadata":{"_uuid":"923a7687daed85c08a7f1fb2031d9ccaf46fab2a","_cell_guid":"b1443835-bd3c-42c8-9660-f4fc3ed1d1f0","collapsed":true,"scrolled":true}},{"metadata":{"_uuid":"f8225ce7acf81df3fe52a56a51fa4ba8d167dd48","_cell_guid":"548b772a-a1d6-487f-89bd-e02d08a8612d"},"cell_type":"markdown","source":"Let's also try to visualize the features we selected and see what kind of relation do they share with Sale Price. From the graphs you would see that they are quite linear in nature. I could also see some outliers there and upon deep diging I found that there were some properties which were quite huge but the prices were very low. I would rather delete those rows. We can also see what other outliers exist to improve the model in future. But for now let's move ahead. "},{"execution_count":null,"cell_type":"code","outputs":[],"source":"traindata.plot.scatter(x = 'GrLivArea', y = 'SalePrice')\n\ntraindata.plot.scatter(x = 'GarageArea', y = 'SalePrice')\n\ntraindata.plot.scatter(x = 'TotalBsmtSF', y = 'SalePrice')\n\ntraindata.plot.scatter(x = '1stFlrSF', y = 'SalePrice')\n\nsns.boxplot(x = 'OverallQual', y = 'SalePrice', data = traindata)\n\nsns.boxplot(x = 'GarageCars', y = 'SalePrice', data = traindata)\n\nsns.boxplot(x = 'FullBath', y = 'SalePrice', data = traindata)\n\nsns.boxplot(x = 'TotRmsAbvGrd', y = 'SalePrice', data = traindata)\n\nsns.boxplot(x = 'YearBuilt', y = 'SalePrice', data = traindata)\n\nsns.boxplot(x = 'YearRemodAdd', y = 'SalePrice', data = traindata)","metadata":{"_execution_state":"idle","_cell_guid":"02ddf8e8-a962-41bd-a777-d95ac02b9741","collapsed":true,"_uuid":"24a2156994e5bda3cb563eda562c32c2234a76ce"}},{"metadata":{"_uuid":"8b7f5b6c71db6eade07e79325adb1f7d90257897","_cell_guid":"174b6a88-4058-44be-8736-95988b8b2b74"},"cell_type":"markdown","source":"I think we are good for now. Based on our Exploratory data analysis I believe we could finalise on some of the features which we see have quite high impact on the Sale Prices. Let's go ahead and delete the outliers now and also log transform the features GrLivArea, SalePrices and TotalBsmtSF. We would also fill the missing values in both train and test dataset with some defaults for now. \n\nI have used the XGBoost and Linear Regression here. The values predicted by both the algorithms were then averaged out for the final predictions. The output was then generated in the form of a csv file and posted on Kaggle Leaderboard. Honestly, I think there is still lot of room to improve the algorithm. As I learn more techniques, I will come back and improve this piece. I intend to add some scoring sections in the code below to see the R SQuare, Adjusted R Square, OLS, Cross Validation Score. Hopefully I will do this very soon. "},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Delete the outliers\ntraindata = traindata.drop(traindata[traindata['Id'] == 1299].index)\ntraindata = traindata.drop(traindata[traindata['Id'] == 524].index)","metadata":{"_execution_state":"idle","_cell_guid":"ed23dee2-e184-45ab-ab54-cd60f87c99ee","collapsed":true,"_uuid":"326818a63f2ce94cfcf170890b16803f7dbe3d67"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#On basis of EDA we did earlier, filter out the variable we want to use for predicting the sale price\nfinaldata = traindata.filter(['OverallQual','MSSubClass', 'KitchenAbvGr','OverallCond', 'GrLivArea', 'EnclosedPorch', 'GarageArea','TotalBsmtSF',  'YearBuilt', 'SalePrice'], axis = 1)\nfinaltest = testdata.filter(['OverallQual','MSSubClass', 'KitchenAbvGr', 'OverallCond','GrLivArea', 'EnclosedPorch', 'GarageArea','TotalBsmtSF',  'YearBuilt'], axis = 1)","metadata":{"_execution_state":"idle","_cell_guid":"8e08244f-79fb-4b03-8303-52c0b78b7894","collapsed":true,"_uuid":"244dc82d708f46ecba0f3bf27fa58d54c4afaedd"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Handle mising values in test data \nfinaltest.loc[finaltest.GarageArea.isnull(), 'GarageArea'] = 0\nfinaltest.loc[finaltest.TotalBsmtSF.isnull(), 'TotalBsmtSF'] = 0","metadata":{"_execution_state":"idle","_cell_guid":"c377e65f-a232-4795-9e07-3f22f1c652d6","collapsed":true,"_uuid":"2003f052ca1388d35bdef1d2890a12aea558ad7c"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Transform Sale Price and GrLivArea to reduce standardize the data \nfinaldata['SalePrice'] = np.log(finaldata['SalePrice'])\nfinaldata['GrLivArea'] = np.log(finaldata['GrLivArea'])\nfinaltest['GrLivArea'] = np.log(finaltest['GrLivArea'])","metadata":{"_execution_state":"idle","_cell_guid":"dc6cbd71-4eb3-4e02-860c-8c35bf3b2689","collapsed":true,"_uuid":"d9eda0d04ae1d7075eee5abe156ecfa9a20740b4"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Find out the columns which are missing in final data \ntotal = finaldata.isnull().sum().sort_values(ascending=False)\npercent = (finaldata.isnull().sum()/finaldata.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","metadata":{"_execution_state":"idle","_cell_guid":"fa35b2c1-6c56-4c00-959f-f414c78f547b","collapsed":true,"_uuid":"ff119f86a5f786843918a2add23aa68bf72b7ecd"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Splt into predictor and variable\nxtrain = finaldata.iloc[:, :-1].values\nytrain = finaldata.iloc[:,9].values\nxtest = finaltest.iloc[:, :9].values","metadata":{"_execution_state":"idle","_cell_guid":"98278004-ae5c-431c-b60f-b365d7174aec","collapsed":true,"_uuid":"587c53b59166a757aa0bd50a09237a524c613f38"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Prediction Model\nimport xgboost as xgb\nregr = xgb.XGBRegressor()\nregr.fit(xtrain, ytrain)\n\n#Calculate the score for the XGBoost Model\nregr.score(xtrain,ytrain)\n\n# Run predictions using XGBoost\ny_pred = regr.predict(xtrain)\n\n#Predict the prices for Test Data Set\ny_test = regr.predict(xtest)","metadata":{"_execution_state":"idle","_cell_guid":"d4500219-1d3c-45c3-b729-29396b0f5878","collapsed":true,"_uuid":"fb7c91d7aaecd18bc3ee67bc8ab887a7cba16e86"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"##Fit Linear Regression Model \nfrom sklearn.linear_model import LinearRegression \nregressor = LinearRegression()\nregressor.fit(xtrain, ytrain)\n\n#Calculate score for the Linear Regression model\nregressor.score(xtrain,ytrain)\n\n#Predict Value of the house using Linear Regression\nytrainpred = regressor.predict(xtrain)\n\n#Predict Value of the house on test data set \nytestpred = regressor.predict(xtest)","metadata":{"_execution_state":"idle","_cell_guid":"a783246f-3efb-437d-9671-37fdc8fd8a72","collapsed":true,"_uuid":"c5544b7183b1035f7e927770f260d5fd32403639"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Average out the predicted value from XGBoost and Linear Regression\nfinalpred = (y_test+ytestpred)/2\nfinalpred = np.exp(finalpred)","metadata":{"_execution_state":"idle","_cell_guid":"1c99e6b8-6576-47fa-8b42-9774f1a4948b","collapsed":true,"_uuid":"24b42a65b7b09a713570aa64de474d318d2b4e26"}},{"execution_count":null,"cell_type":"code","outputs":[],"source":"#Output to csv\n\nmy_submission = pd.DataFrame(finalpred, index=testdata[\"Id\"], columns=[\"SalePrice\"])\nmy_submission.to_csv('submission.csv', header=True, index_label='Id')","metadata":{"_execution_state":"idle","_cell_guid":"1fbb45a4-1cad-4675-86b5-3766c6d1356e","collapsed":true,"_uuid":"20814985956924dc51cdb78d0cbc0c2d3112dcc7"}}],"nbformat":4,"nbformat_minor":1}