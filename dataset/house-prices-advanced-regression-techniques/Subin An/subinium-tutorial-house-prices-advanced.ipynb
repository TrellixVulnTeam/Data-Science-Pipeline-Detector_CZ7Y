{"cells":[{"metadata":{"_uuid":"a94173df95f5bb3d0f14d711d4b8ebd67e5cfc6f"},"cell_type":"markdown","source":"# [수비니움 캐글 따라하기] 주택 가격 예측 : Advanced\n\n본 커널은 다음과 같은 커널을 추가적으로 공부하며, Stacking 테크닉을 이용해봅니다.\n\n- [Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)\n\nThanks to @serigne for awesome kernel.\n\n이번엔  주택 가격 예측 문제를 stacking 테크닉을 사용하여 좋은 결과를 내는 과정입니다.\n\nStacking과  앙상블, 스태킹을 하는 부분에 있어서 class로 만들어 사용하는 것과 데이터의 분포를 맞추는 과정 등을 배울 수 있습니다.\n또한 교차 검증도 따로 함수로 만들어 사용하는 등 저에게는 배울 점이 많은 커널이 었습니다.\n\n하지만 비교적 사전 지식이 많이 필요한 커널입니다. 내용 자체가 친절한 커널은 아닙니다.\n하이퍼 파라미터의 설정이 왜 그렇게 됬는지 설명이 부족하고, 코드에 대한 설명 또한 부족합니다.\n\n그렇기에 머신러닝을 접한지 얼마 안된 분에게는 추천하지 않을 것 같지만, 스태킹과 앙상블 과정을 따라하기에는 좋은 커널이었습니다.\n\n> 이 문제의 앙상블과 스태킹 등의 기법에 있어서는 [파이썬 머신러닝 완벽 가이드] 책에도 상세하게 기술되어있습니다. \n\n더 많은 정보를 업로드하고 있으니 많은 좋아요와 구독 부탁드립니다.\n\n- **블로그** : [안수빈의 블로그](https://subinium.github.io)\n- **페이스북** : [어썸너드 수비니움](https://www.facebook.com/ANsubinium)\n- **유튜브** : [수비니움의 코딩일지](https://www.youtube.com/channel/UC8cvg1_oB-IDtWT2bfBC2OQ)"},{"metadata":{"_uuid":"e2e78e8eae41c7d7f446bc07d19aa65e99272339"},"cell_type":"markdown","source":"#### 특성 공학\n\n- Inputing missing values\n- Transforming : 수치형 -> 범주형\n- Label Encoding : 범주형 -> 서수형\n- Box Cox Transformation : 비대칭 분포 데이터에 대해 좀 더 좋은 결과(리더보드와 cross-validation에서)를 낼 수 있음\n- Getting dummy variables : 범주형 특성\n\n#### 알고리즘\n\n본 커널에서는 stacking/emsemble을 하기 전 다음과 같은 모델을 선택하고, cross-validate합니다..\n\n- sklearn based model\n- DMLS's XGBoost\n- Microsoft's LightGBM\n\n또한 stacking 및 앙상블 방법으로 가장 간단한 방법(평균)과 비교적 어려운 방법으로 총 2가지 방법으로 예측값을 높입니다.\n"},{"metadata":{"_uuid":"0a583efa2b734ebdbed79fc5eb5a2a070fe3cf89"},"cell_type":"markdown","source":"## 라이브러리, 데이터 확인하기\n\n우선 필요한 라이브러리를 불러오고, 데이터의 형태를 가볍게 확인해봅시다."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport warnings\nwarnings.filterwarnings('ignore') # warnings 무시\n%matplotlib inline\n\n# sns Theme \nsns.set_style('darkgrid') \n\n# 소수점 표현 제한\npd.set_option('display.float_format', lambda x : '{:.3f}'.format(x))\n\n# 디렉토리 내, 사용가능 파일 체크 \nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# 데이터 읽기\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c948c2521d31248e3950b92704cb063d68b0da7"},"cell_type":"code","source":"# 데이터체크\nprint(train_df.shape, test_df.shape)\ntrain_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"070143ab4157b1b2a03087e474e0c42436e441fb"},"cell_type":"markdown","source":"가격 특성을 제외하면 80개의 특성을 가지고 있습니다. 그 외에는 데이터 분석과 상관 없는 Id  특성도 있습니다."},{"metadata":{"trusted":true,"_uuid":"51dcee5da41f133a8f331064fb3fa6374797bc36"},"cell_type":"code","source":"# Save the 'Id' comlumn\ntrain_ID = train_df['Id']\ntest_ID = test_df['Id']\n\n# drop the 'Id' column since it's unnecessary for the prediction process\ntrain_df.drop('Id', axis=1, inplace=True)\ntest_df.drop('Id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f4a05db04373859d3ff6dc0dc1a0cb676a276d9"},"cell_type":"markdown","source":"## Data Processing\n\n### Outliers\n\n공식 [Documetation](http://jse.amstat.org/v19n3/decock.pdf)에서는 데이터에 이상 치가 있다고 언급하고 있습니다.\n그 이상치를 찾아 어떻게 처리할 것인가가 이 문제의 핵심이 될 수 있습니다."},{"metadata":{"trusted":true,"_uuid":"c290d2eef0c43132cbc057a5e54e7e1c3894e7cf"},"cell_type":"code","source":"fig, ax = plt.subplots()\n\nax.scatter(x=train_df['GrLivArea'], y=train_df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec1612b9d1671b75d160e5c3cf6de03774ae2a61"},"cell_type":"markdown","source":"이 그래프를 보면 낮은 SalePrice에서 그래프와 다르게 이상한 2개의 데이터를 확인할 수 있습니다.\n이 데이터를 더 나은 훈련을 위해 삭제하겠습니다."},{"metadata":{"trusted":true,"_uuid":"212af0eb2df44135f336f833728c6719108870b2"},"cell_type":"code","source":"#Deleting outlier\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index) \n\n#Check the Graph again\nfig, ax = plt.subplots()\nax.scatter(x=train_df['GrLivArea'], y=train_df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbe77c76b05966e98b0ac31b8e74f270fdbf4cfd"},"cell_type":"markdown","source":"이상치 제거는 훈련에 있어 매우 좋은 방법입니다.\n\n이상치를 일일히 찾아 제거하지 않는 이유는 다음과 같습니다.\n\n- 이 외에도 훈련 데이터에 이상치가 많을 수 있지만, 테스트 데이터에도 이상치가 있을 수 있습니다.\n- 그렇다면 오히려 모델에 안좋은 영향을 미칠 수 있습니다.\n\n이런 이상치를 모두 제거하는 대신에 후에 모델에서 이런 데이터를 제어하는 방법을 배울 것입니다."},{"metadata":{"_uuid":"463552f1c75a1b21d58452831a334d693d647323"},"cell_type":"markdown","source":"### Target Variable\n\n**SalePrice**는 우리가 예측해야 하는 타겟 값입니다. 이제 이 데이터부터 먼저 분석해보겠습니다.\n\n다음 그래프를 이해하기 위해는 다음과 같은 선지식이 필요합니다.\n\n- Q- Q Plot : 간단하게 두 데이터 집단 간의 분포를 체크\n\n**reference** \n\n더 많은 정보는 아래의 링크를 참고하시면 됩니다.\n\n- [Q-Q wiki](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot)\n- [Q-Q plot 한글 블로그 자료 : sw4r님](http://blog.naver.com/PostView.nhn?blogId=sw4r&logNo=221026102874&parentCategoryNo=&categoryNo=43&viewDate=&isShowPopularPosts=true&from=search)"},{"metadata":{"trusted":true,"_uuid":"84b4dc0a3e7a46483a06b7a02634b52851b90a35"},"cell_type":"code","source":"sns.distplot(train_df['SalePrice'], fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint(mu, sigma)\n\n# 분포를 그래프에 그려봅시다\nplt.legend(['Normal dist. ($\\mu$={:.2f} and $\\sigma$={:.2f})'.format(mu,sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n# QQ-plot을 그려봅시다.\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56757206d7e2a2d8041f66728a77c528b36b3176"},"cell_type":"markdown","source":"분포가 오른쪽으로 치우친 것을 알 수 있습니다.\n일반적으로 선형 모델은 분포가 균형잡힌 상태에 더 용이하므로 데이터를 좀 더 손보면 좋을 것 같습니다.\n\n#### Log-transformation of the target variable \n\n데이터의 정규화를 위해 numpy의 `log1p` (모든 column의 원소에  $log(1+x)$) 함수를 사용하여 데이터를 처리해보도록 하겠습니다."},{"metadata":{"trusted":true,"_uuid":"05037de97fe75d3a91ef9840a2b49f9f8838742b"},"cell_type":"code","source":"train_df['SalePrice'] = np.log1p(train_df['SalePrice'])\n\n# 위에서와 같은 코드로 똑같이 분포를 확인해봅니다.\nsns.distplot(train_df['SalePrice'], fit=norm)\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint(mu, sigma)\nplt.legend(['Normal dist. ($\\mu$={:.2f} and $\\sigma$={:.2f})'.format(mu,sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd6da28f2b118da23b82c2123bfac220451576f1"},"cell_type":"markdown","source":"이제 정규분포에 매우 근접하게 값들이 바뀐 것을 알 수 있습니다."},{"metadata":{"trusted":true,"_uuid":"8b34278f8559ac9a6d2e99f64947e43bb26c2062"},"cell_type":"markdown","source":"## Feature Engineering\n\n우선 처리하기에 앞서 데이터를 하나로 묶어서 사용하겠습니다."},{"metadata":{"trusted":true,"_uuid":"c60a1b9226c253a853ca5b4380331797c27d6ac4"},"cell_type":"code","source":"ntrain = train_df.shape[0]\nntest = test_df.shape[0]\n\ny_train = train_df.SalePrice.values\n\nall_data = pd.concat((train_df, test_df)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6b13f84edbc2595abde5e46b853209afce770c4"},"cell_type":"markdown","source":"### Missing Data\n\n이제 전체 데이터에서 빈 부분을 확인해보도록 하겠습니다."},{"metadata":{"trusted":true,"_uuid":"0c5703b7bb86ef74cecd492c947abe72435d1a2a"},"cell_type":"code","source":"all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n\nmissing_data = pd.DataFrame({\"Missing Ratio\" : all_data_na})\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b85075e3afc697691b0cebb9643198252ce514af"},"cell_type":"markdown","source":"이제 이 데이터를 시각화합니다."},{"metadata":{"trusted":true,"_uuid":"0c1063bf775ab8ab0cf23ebf3e53850753b3e4ce"},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15,12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Feature', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4de6b37cffa46e37c0d11a7fb8670c1778bcc31"},"cell_type":"markdown","source":"### Data Correlation\n\nheatmap을 이용하면 각 요소의 상관관계를 시각화하여, 더 직관적으로 볼 수 있습니다.\n여기서 초점으로 봐야하는 요소는 지금 구하고자하는 값인 SalePrice와 다른 요소간의 상관관계입니다."},{"metadata":{"trusted":true,"_uuid":"2bbf4a66094503d94f29d3e97305a159a7c30e7c"},"cell_type":"code","source":"corrmat = train_df.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"105b21af40028d114ff139834cfa348229d233a7"},"cell_type":"markdown","source":"### Inputing missing values\n\n이제 누락된 값을 채워넣도록 합시다. 이제부터 누락된 값이 있는 특성들을 하나씩 차례로 볼 차례입니다.\n\n- **PoolQC** : 디스크립션에 따르자면  NA값은 \"No Pool\"을 의미한다고 합니다. 99%의 값을 채워줍시다."},{"metadata":{"trusted":true,"_uuid":"fd71919c2bd0006dc04917d74ffb10c2bfc106dd"},"cell_type":"code","source":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37d989a4ccf7ab303d8e2b31db5a416bcb159cf4"},"cell_type":"markdown","source":"- **MiscFeature** : 디스크립션에 따르면 NA는 \"no misc feature\"이라고 합니다.\n이 외에도 **Alley, Fence, FireplaceQu**도 비슷하게 ~없다 이므로 함께 처리합니다."},{"metadata":{"trusted":true,"_uuid":"8669e3998a9d445cb8c5e242f70de715f751f644"},"cell_type":"code","source":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3e2891939810c2b4ef75bf546056cd1a8fecd6c"},"cell_type":"markdown","source":"- **LotFrontage** : 거리와 집의 거리 요소로, 이 값들은 이웃들의 거리와 유사한 값을 가질 것입니다. 손실된 값들은 이웃들의 중앙값으로 채워넣겠습니다."},{"metadata":{"trusted":true,"_uuid":"f6f43cc29c46555749f0922e54c7d8585d9bd192"},"cell_type":"code","source":"all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x : x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d71ec160f504db535edca0b3423ef249b6e0f5ba"},"cell_type":"markdown","source":"- **GarageType, GarageFinish, GarageQual and GarageCond** : 이 부분도 None으로 처리하겠습니다."},{"metadata":{"trusted":true,"_uuid":"858b95bc3ae1c00eeac56e7b05c06ce7d7fa3717"},"cell_type":"code","source":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bc03a6002b5322999c3bd1bccc4fda8a875cd52"},"cell_type":"markdown","source":"다시 빈데이터들을 'None' 또는 0으로 처리합니다."},{"metadata":{"trusted":true,"_uuid":"bd4503a4a068aa9114afe5dc3df2b0e3c5c729cd"},"cell_type":"code","source":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b5074adb731630e7905c83184a4d410aeb6592b"},"cell_type":"code","source":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46254415b86b39ea23e7069a7014a7487afa3e36"},"cell_type":"code","source":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19fb3d2d04af1e4ec0a58675195def8c2cb3b0f9"},"cell_type":"code","source":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"afcfa67314b4d21ca6682b6f27a1c1d0b5f44054"},"cell_type":"markdown","source":" - **MSZoning (The general zoning classification)** : RL이 최빈값으로 빈 부분은 RL로 채웁니다. mode 메서드는 가장 많이 나타나는 값을 자동으로 선택해줍니다."},{"metadata":{"trusted":true,"_uuid":"246ea7c5a282ffe13ddd3a7a8cb9146776a45cc3"},"cell_type":"code","source":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69396b1b6839a5af00c576d267ec7c2e95d4dae7"},"cell_type":"markdown","source":"- **Utilities** : 이 데이터는 모든 값이 \"AllPub\"으로 되어있고, 한개가 \"NoSeWa\" 그리고 2개의 NA값이 있습니다. 이는 예측하는 데에 있어 전혀 유용하지 않을 것 같으니 drop합시다."},{"metadata":{"trusted":true,"_uuid":"5d246cbdd240d322f9ebbb6c0469aef0025ad4a2"},"cell_type":"code","source":"all_data = all_data.drop(['Utilities'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5afd240716a7619d02f825b8df83e55cf9d1ef22"},"cell_type":"markdown","source":"- **Functional** : 디스크립션에 의하면  NA는 typical을 의미한다고 합니다."},{"metadata":{"trusted":true,"_uuid":"3c45b478662a31f5fd30bd297e556f915e07939d"},"cell_type":"code","source":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"109a02a7f474e7ea02f2d92970605bcb24cd3b53"},"cell_type":"markdown","source":"- **Electrical, KitchenQual, Exterior1st and Exterior2nd, SaleType** :  이 데이터 모두 최빈값으로 채워줍시다."},{"metadata":{"trusted":true,"_uuid":"3b2dc1df803b4178ac4d65f20ad1e112abb3e749"},"cell_type":"code","source":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e432b2c11ce4b8969bb8e588ca745bdce59f1ec"},"cell_type":"code","source":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"380605090448128e7d2ad3440fdf167f1c2a2e62"},"cell_type":"code","source":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44952d22131e58d1a65023da90da7b1aa04fcdb0"},"cell_type":"code","source":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9854e9e073ee3d1231e80b66d219580225586508"},"cell_type":"code","source":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98569ed98ce57e79ce8abf85344d52035db6f9ed"},"cell_type":"markdown","source":"마지막으로 아직까지 채우지 못한 데이터가 있는지 체크해봅시다."},{"metadata":{"trusted":true,"_uuid":"e3d074e4d5996e81cdedde7cb1258528d7345281"},"cell_type":"code","source":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49f89e9430741b014a606eb29399c54d732dd1b0"},"cell_type":"markdown","source":"### More features engineering\n\n#### Transforming some numerical variable that are really categorical\n\n수치형 값들 중 범주형인 특성들을 변환합시다."},{"metadata":{"trusted":true,"_uuid":"fc8d0e9cbf547c6591ec8c5d672ce71ff86ea86e"},"cell_type":"code","source":"# MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n# Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].apply(str)\n\n# Year and Month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c81be08bb3a43d47633340d63a9444159761060"},"cell_type":"markdown","source":"#### Label Encoding some categorical variables that may contain information in their ordering set\n\n범주형 데이터를 label encoding으로 변환합니다."},{"metadata":{"trusted":true,"_uuid":"7496f931927e29a2560fb5b54538fba819e79932"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa48e2ae722963e55fa8ed9c5415f87c67d3e7ba"},"cell_type":"markdown","source":"#### Adding one more important feature\n\n주택 가격에서 중요한 요소 중 하나는 집의 가용 평수입니다. 그렇기에 이런 특성을 basement + 1층 + 2층 공간으로 새로운 특성을 하나 만들겠습니다."},{"metadata":{"trusted":true,"_uuid":"57a785712165c0c5905708c0ed379331eb5583ff"},"cell_type":"code","source":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4df3de7fa0d670ef69a11341254b86e59177607d"},"cell_type":"markdown","source":"#### Skewed feature"},{"metadata":{"trusted":true,"_uuid":"111267aaf5f770505730a91af1553756f2dc4bad"},"cell_type":"code","source":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# 수치형 데이터에서 skewness 체크\nskewed_feats = all_data[numeric_feats].apply(lambda x : skew(x.dropna())).sort_values(ascending=False)\n\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04c9904035a3590cf362e2d0b508ce86319c5fa1"},"cell_type":"markdown","source":"#### Box Cox Transformation of (highly) skewed features\n\nBox-Cox Transformation은 정규 분포가 아닌 데이터를 정규 분포 형태로 변환하는 방법 중 하나입니다.\n\n보다 자세한 내용은 다음을 참고하길 바랍니다.\n\n**Reference**\n\n- [Box Cox Transformation](https://en.wikipedia.org/wiki/Power_transform)"},{"metadata":{"trusted":true,"_uuid":"647f4e1561d994a66d8b265035b039b686afebf1"},"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa9867bd3032cf8cba4ff0035ef5b6b597adc19e"},"cell_type":"markdown","source":"#### Getting dummy categorical features\n\n범주형 데이터를 get_dummies를 이용하여 변환합니다. 그리고 다시 train_df와 test_df로 나누겠습니다."},{"metadata":{"trusted":true,"_uuid":"3e4c1086ffa05c154c45308e95acbe3430aa5691"},"cell_type":"code","source":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)\n\ntrain_df = all_data[:ntrain]\ntest_df = all_data[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3e26f44b8ae8018dc790522c41f71bf43f7647d"},"cell_type":"markdown","source":"##  Modeling\n\n###  Import Libraries \n\n이제 모델들의 라이브러리를 import합시다."},{"metadata":{"trusted":true,"_uuid":"2a885aa355a644ee9c30b0b069448267d30a5144"},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1644ae359acdecd6719baf4ad39874507c7a127e"},"cell_type":"markdown","source":"### Define a cross validation strategy\n\n이 커널에서는 **cross_val_score** 함수를 사용합니다. 하지만 이 함수는 순서를 섞지 않기 때문에 검증의 정도를 높이기 위해 K-fold를 사용하여 검증의 정확도를 더 높입니다."},{"metadata":{"trusted":true,"_uuid":"3b62ec9347b7b7bffe2f114fa223420495129e6c"},"cell_type":"code","source":"# Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_df.values)\n    rmse = np.sqrt(-cross_val_score(model, train_df.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c03db0f43984d630047084c4e0b6ccf8bfe6285"},"cell_type":"markdown","source":"## Base models\n\n- **LASSO Regression**\n이 모델은 이상치에 매우 민감합니다. 그렇기에 이런 이상치를 좀 더 규제하기 위해 pipeline에  `RobustScaler()` 메서드를 이용합니다.\n\n- **Elastic Net Regression**\n이 모델 또한 이상치를 위해 똑같이 합니다."},{"metadata":{"trusted":true,"_uuid":"e71793047665cfe3042b97f3e20b9d399400ba7e"},"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb233a57296a70f6c37bc7ab1d8bbfc9e39d9e52"},"cell_type":"markdown","source":"- **Kernel Ridge Regression**"},{"metadata":{"trusted":true,"_uuid":"ba70287903cd839db92831669382867b2cf388ef"},"cell_type":"code","source":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"029c16cf84447f688fadf3fa16ea11d36707ef72"},"cell_type":"markdown","source":"- **Gradient Boosting Regression**\n\n**huber** 손실 함수로 이상치를 관리합니다. 이 손실함수는 다른 손실함수에 비해 이상치에 대해 민감하지 않습니다.\n\nReference\n\n- [huber wiki](https://en.wikipedia.org/wiki/Huber_loss)\n\n"},{"metadata":{"trusted":true,"_uuid":"d8f235f430cc3137e36fb3315ad3c8572d9d1f99"},"cell_type":"code","source":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4,\n                                   max_features='sqrt', min_samples_leaf=15, min_samples_split=10,\n                                   loss='huber', random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1c6edf3ff37aed601e6e33b94d467a57fb55414"},"cell_type":"markdown","source":"- **XGBoost** \n\n각 매개변수, 즉 하이퍼 파라미터 설정은 bayesian optimization을 사용했다고 하였습니다.\n\n저는 아직 그 사용을 잘 모르겠으나 다음을 참고하면 됩니다. [link](https://github.com/fmfn/BayesianOptimization)\n"},{"metadata":{"trusted":true,"_uuid":"76df40658de4427791b5545ae83b9610763c4ab0"},"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e841a63718ea202698bf665708260c62e66ec83"},"cell_type":"markdown","source":"- **LightGBM**"},{"metadata":{"trusted":true,"_uuid":"2b5cf3697e6aea221b23e368319b63612d82f515"},"cell_type":"code","source":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d83b89e84c02f9a0ab68385fc56951858bc9e56f"},"cell_type":"markdown","source":"### Base models scores\n\n이제 이 모델들을 이용해 교차 검증을 통해 score를 구해봅시다."},{"metadata":{"trusted":true,"_uuid":"e857157b6b9d7810758edaef4fee269450e55edc"},"cell_type":"code","source":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8835ea4b5fc78bbdcf123c740320ee87a5a90c96"},"cell_type":"code","source":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8dfa61e381d3d3c7d02e91b3ecc03e79debdc19"},"cell_type":"code","source":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0df7c8f82c0a7f59f1103b3f28770ac37ba09a31"},"cell_type":"code","source":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8762562a60b4fc10221035aee93fdf6390381b9"},"cell_type":"code","source":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adbcbad7f6ddaa96bf1422c57b38c4cd743fe01c"},"cell_type":"code","source":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55efcc325c802e086dcafb0d21356272c1de05c8"},"cell_type":"markdown","source":"## Stacking models\n\n### Simplest Stacking approach : Averaging base models\n\n우선 모델들의 성능을 평균하여 사용하는 것으로 시작해봅시다.\n**class**를 만들어 캡슐화하고, 코드를 재사용할 수 있게 만들어봅시다.\n"},{"metadata":{"trusted":true,"_uuid":"0c8daeec5065fbe6573a0213cefe09f2238c58ba"},"cell_type":"code","source":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1afe3512e715026527406ffccb24839224f12c0"},"cell_type":"markdown","source":"### Averaged base models score\n\n이제 **ENet, GBoost, KRR and lasso**를 이용해 score를 내봅시다. 다른 모델을 추가해도 됩니다."},{"metadata":{"trusted":true,"_uuid":"49c2f1b83e8131f8cacca359a93f756d30d8e864"},"cell_type":"code","source":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\"Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab9778ac575a9bdb1655900de6a9bf823960f927"},"cell_type":"markdown","source":"### Less simple Stacking : Adding a Meta-model\n\n이 방법은 meta model을 추가하고, base model들의 평균과 이 out-of-folds 예측을 이용하여 meta-model을 훈련시킵니다.\n\n기본적인 흐름은 다음과 같습니다.\n\n1. 훈련 데이터를 분리된 데이터셋 train, holdout으로 나눕니다.\n2. train 데이터로 훈련을 하고\n3. holdout 데이터로 테스트 합니다.\n4. 3)을 통해 예측값을 구하고, **meta model**을 통해 그 예측 값으로 모델을 학습합니다.\n\n첫 세 단계는 순서대로 진행하면 됩니다. 만약 5-fold stacking을 한다면, 5-folds를 예시로 들어봅니다.\n\n그렇다면 훈련 데이터를 5개로 나누고, 총 5번의 반복문을 진행하면 됩니다. 각 반복문은 4 folds로 훈련을 진행하고, 나머지 1 fold를 예측합니다.\n\n그렇다면 5번의 반복문이 끝나면 모든 데이터는 out-of-folds 예측값을 가지게 되고, 이제 이 값들을 이용해 meta model의 입력으로 사용합니다. (4)\n\n예측 부분에 있어 테스트 데이터에서 모든 모델의 예측값을 평균내고, 이를 **meta-features**로 사용하여 meta-model로 마지막 예측값을 만듭니다.\n\n![Faron's image](http://i.imgur.com/QBuDOjs.jpg)\n\nimage taken from [Faron](https://www.kaggle.com/getting-started/18153#post103381)\n"},{"metadata":{"trusted":true,"_uuid":"f80a411d9ca224d429693ba4e78b5db31dd7990f"},"cell_type":"markdown","source":"### Stacking averaged Models Class\n\n다음과 같은 pseudo 코드를 구현했다고 생각하면 됩니다.\n\n베이스 모델의 예측값을 하나의 특성으로 사용하여 최종 분류를 만든다고 이해하는 것이 가장 좋습니다.\n\n![](https://cdn-images-1.medium.com/max/1600/0*GXMZ7SIXHyVzGCE_.)"},{"metadata":{"trusted":true,"_uuid":"edbbcc0db74421b7f25f40f9b5cd227070af0e97"},"cell_type":"code","source":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # base_models_는 2차원 배열입니다.\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    # 각 모델들의 평균값을 사용합니다.\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e3e816f765e20aa7a48c407ab8603864c7103ed"},"cell_type":"markdown","source":"### Stacking Averaged models Score\n\n성능을 비교하기 위해 같은 모델을 이용하여 score를 만들어보겠습니다."},{"metadata":{"trusted":true,"_uuid":"aacccb96cea0621aff2bdbcbadd26e0e175976e2"},"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(\n    base_models=(ENet, GBoost, KRR),\n    meta_model=(lasso)\n)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"236ea8f21800cb7a8f3f787970f53ea22cfe8dc6"},"cell_type":"markdown","source":"meta learner을 이용하여 더 나은 점수를 받을 수 있었습니다."},{"metadata":{"_uuid":"8a956bc09efa71cda32b0113cd93627b79caf28f"},"cell_type":"markdown","source":"## Emsembling StackedRegressor, XGBoost and LightGBM\n\n위에서 만든 XGBoost와 LightBGM을 이용하여 최종 결과를 만들겠습니다.\n\n우선 rmsle 함수를 먼저 정의합니다."},{"metadata":{"trusted":true,"_uuid":"d6079e7992932f430f1fb8333c0dfe1282767516"},"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"906c955cc6aac493a3456d6c6a540580023413fd"},"cell_type":"markdown","source":"### Final Training and Prediction\n\n `expm1` 함수는 초기에 정규화를 위해 사용한 `log1p`함수의 역함수입니다.\n\n**StackedRegressor**:"},{"metadata":{"trusted":true,"_uuid":"ad11169143b056e36060a7089696c70d9a3fcc47"},"cell_type":"code","source":"stacked_averaged_models.fit(train_df.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train_df.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test_df.values))\nprint(rmsle(y_train, stacked_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6fa8072dd181f18b0510c9d399a50989c46210d5"},"cell_type":"markdown","source":"**XGBoost** :"},{"metadata":{"trusted":true,"_uuid":"976dd6f79d2d46df967521458782652d2a40fe63"},"cell_type":"code","source":"model_xgb.fit(train_df, y_train)\nxgb_train_pred = model_xgb.predict(train_df)\nxgb_pred = np.expm1(model_xgb.predict(test_df))\nprint(rmsle(y_train, xgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7812e9e812d7a2343c19ffc8be7e62dc07364c60"},"cell_type":"markdown","source":"**LightGBM** :"},{"metadata":{"trusted":true,"_uuid":"1e076ff827411292775e32baea7679850a1c5b09"},"cell_type":"code","source":"model_lgb.fit(train_df, y_train)\nlgb_train_pred = model_lgb.predict(train_df)\nlgb_pred = np.expm1(model_lgb.predict(test_df.values))\nprint(rmsle(y_train, lgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3fd083926d7a2eed19e870406690fb9b4ca06b5"},"cell_type":"code","source":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87e324a90ee8207993b469c4b020abe4edabffd3"},"cell_type":"markdown","source":"**Ensemble prediction** :\n\n앙상블에 사용한 가중치에 대해서 원 저자는 다음과 같은 답변을 했습니다. 가중치를 Stacked Regressor에 크게 한 이유는 다음과 같습니다.\n\nBased on their cross-validation scores (and a bit of trial and errors ^^ )\n\nYou can see for instance in this version of the notebook the following CV mean scores :\n\n- StackedRegressor score : 0.1085\n- Xgboost score: 0.1196\n- LGBM score: 0.1159\n\nThis helps to define the weights. However you may also want to define an optimization function in order to find more optimal weights."},{"metadata":{"trusted":true,"_uuid":"4e20609b0024f665e607ae64560cecc0dca8d0ac"},"cell_type":"code","source":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d50293618cb8acfa1b2ce3ceae7d1895280a107"},"cell_type":"markdown","source":"**Submission**\n\n이제 마지막으로 제출만 남았습니다."},{"metadata":{"trusted":true,"_uuid":"d360c4e10613a8d83fa2cbbb09f5452a3c9c1c77"},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58b0cf278417234cce39393b5fb648c02b079fb7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}