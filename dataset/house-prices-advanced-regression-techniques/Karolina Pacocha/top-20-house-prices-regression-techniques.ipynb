{"cells":[{"metadata":{},"cell_type":"markdown","source":"House Price competition is a very good way to introduce feature engineering and regression models. I'm gonna explore the data and make something with them and also imput missing values. Feature engineering is an important part of machine learning process so I want to spend more time for this part. I'm gonna try I few models and tell you which work the best with train dataset from this competition. Please consider upvoting if this is useful to you :)"},{"metadata":{},"cell_type":"markdown","source":"**Import the Libraries**"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport math \nnp.random.seed(2019)\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost.sklearn import XGBRegressor\n\nimport statsmodels\n\n#!pip install ml_metrics\nfrom ml_metrics import rmsle\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nprint(\"done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Import Data**"},{"metadata":{},"cell_type":"markdown","source":"I'm adding here 'train' variable in order to check in the easiest way which observations are from train and test dataset because I'm gonna join train and test datasets."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"def read_and_concat_dataset(training_path, test_path):\n    train = pd.read_csv(training_path)\n    train['train'] = 1\n    test = pd.read_csv(test_path)\n    test['train'] = 0\n    data = train.append(test, ignore_index=True)\n    return train, test, data\n\ntrain, test, data = read_and_concat_dataset('../input/train.csv', '../input/test.csv')\ndata = data.set_index('Id')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09e9a2cea7a903a160f9123f1d65904506760a53","trusted":true},"cell_type":"code","source":"data.columns[data.isnull().sum()>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are a few variables with NaN value but in these cases 'NaN' means something else than missing value. For example 'NaN' in 'GarageCond' means that this house hasn't a garage. I'm gonna change 'NaN' values to 'None' string. "},{"metadata":{},"cell_type":"markdown","source":"##**Fixing variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def filling_missing_values(data,variable, new_value):\n    data[variable] = data[variable].fillna(new_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filling_missing_values(data,'GarageCond','None')\nfilling_missing_values(data,'GarageQual','None')\nfilling_missing_values(data,'FireplaceQu','None')\nfilling_missing_values(data,'BsmtCond','None')\nfilling_missing_values(data,'BsmtQual','None')\nfilling_missing_values(data,'PoolQC','None')\nfilling_missing_values(data,'MiscFeature','None')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"224f963793b26e4d0c9e77481804b725b5ba2e4d"},"cell_type":"markdown","source":"MSSubClass is not a numerical variables, so let's transform it to caterogical variable."},{"metadata":{"_uuid":"6cad0f4c549eda4088932d2a73d0fd6ae4693242","trusted":true},"cell_type":"code","source":"data['MSSubClass'][data['MSSubClass'] == 20] = '1-STORY 1946 & NEWER ALL STYLES'\ndata['MSSubClass'][data['MSSubClass'] == 30] = '1-STORY 1945 & OLDER'\ndata['MSSubClass'][data['MSSubClass'] == 40] = '1-STORY W/FINISHED ATTIC ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 45] = '1-1/2 STORY - UNFINISHED ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 50] = '1-1/2 STORY FINISHED ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 60] = '2-STORY 1946 & NEWER'\ndata['MSSubClass'][data['MSSubClass'] == 70] = '2-STORY 1945 & OLDER'\ndata['MSSubClass'][data['MSSubClass'] == 75] = '2-1/2 STORY ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 80] = 'SPLIT OR MULTI-LEVEL'\ndata['MSSubClass'][data['MSSubClass'] == 85] = 'SPLIT FOYER'\ndata['MSSubClass'][data['MSSubClass'] == 90] = 'DUPLEX - ALL STYLES AND AGES'\ndata['MSSubClass'][data['MSSubClass'] == 120] = '1-STORY PUD (Planned Unit Development) - 1946 & NEWER'\ndata['MSSubClass'][data['MSSubClass'] == 150] = '1-1/2 STORY PUD - ALL AGES'\ndata['MSSubClass'][data['MSSubClass'] == 160] = '2-STORY PUD - 1946 & NEWER'\ndata['MSSubClass'][data['MSSubClass'] == 180] = 'PUD - MULTILEVEL - INCL SPLIT LEV/FOYER'\ndata['MSSubClass'][data['MSSubClass'] == 190] = '2 FAMILY CONVERSION - ALL STYLES AND AGES'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc4d81a798f45771091c82f4dcc0e4feaa7f5693"},"cell_type":"markdown","source":"A few categorical variables are ordinal variables, so let's fix them. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def fixing_ordinal_variables(data, variable):\n    data[variable][data[variable] == 'Ex'] = 5\n    data[variable][data[variable] == 'Gd'] = 4\n    data[variable][data[variable] == 'TA'] = 3\n    data[variable][data[variable] == 'Fa'] = 2\n    data[variable][data[variable] == 'Po'] = 1\n    data[variable][data[variable] == 'None'] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fixing_ordinal_variables(data,'ExterQual')\nfixing_ordinal_variables(data,'ExterCond')\nfixing_ordinal_variables(data,'BsmtCond')\nfixing_ordinal_variables(data,'BsmtQual')\nfixing_ordinal_variables(data,'HeatingQC')\nfixing_ordinal_variables(data,'KitchenQual')\nfixing_ordinal_variables(data,'FireplaceQu')\nfixing_ordinal_variables(data,'GarageQual')\nfixing_ordinal_variables(data,'GarageCond')\nfixing_ordinal_variables(data,'PoolQC')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"..and one more but in different way."},{"metadata":{"trusted":true},"cell_type":"code","source":"data['PavedDrive'][data['PavedDrive'] == 'Y'] = 3\ndata['PavedDrive'][data['PavedDrive'] == 'P'] = 2\ndata['PavedDrive'][data['PavedDrive'] == 'N'] = 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##**Missing values**"},{"metadata":{},"cell_type":"markdown","source":"First of all I'm gonna look how many variables have less than 50 missing values and fix it. Then I'll look how about variables with more than 50 missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"colu = data.columns[(data.isnull().sum()<50) & (data.isnull().sum()>0)]\nfor i in colu:\n    print(data[colu].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colu = data.columns[data.isnull().sum()>=50]\nfor i in colu:\n    print(data[colu].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm putting 0 in GarageArea, GarageFinish, GarageType, GarageYrBlt and GarageCars where houses don't have garage. "},{"metadata":{"trusted":true},"cell_type":"code","source":"filling_missing_values(data, 'GarageArea',0)\nfilling_missing_values(data, 'GarageCars',0)\ndata['GarageFinish'][(data.GarageFinish.isnull()==True) & (data.GarageCond==0)] =0\ndata['GarageType'][(data.GarageType.isnull()==True) & (data.GarageCond==0)] =0\ndata['GarageYrBlt'][(data.GarageYrBlt.isnull()==True) & (data.GarageCond==0)] =0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm gonna put 0 in MiscVal for house which don't have any MiscFeature and 'None' value for house with 0 in MiscValue and some value in MiscFeature."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data[['MiscFeature','MiscVal']][(data.MiscFeature=='None') & (data.MiscVal>0)])\ndata.MiscVal.loc[2550] = 0\n\nprint(data[['MiscFeature','MiscVal']][(data.MiscVal==0) & (data.MiscFeature!='None')])\nc=data[['MiscFeature','MiscVal']][(data.MiscVal==0) & (data.MiscFeature!='None')].index\ndata.MiscFeature.loc[c] = 'None'","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37777618ddb6639537f468935a9128617d8f879a"},"cell_type":"markdown","source":"Now I'm gonna write two functions to help me in imputing missing values in variables. I'm using here Random Forest Regressor and Classifier. "},{"metadata":{"_uuid":"219d4a0ef9e4e78bb6ab91a481ca09c56aa4ef14","trusted":true},"cell_type":"code","source":"def inputing(variab):\n    y = data[variab]\n    data2 = data.drop([variab],axis=1)\n    col = data2.columns[data2.isnull().sum()==0]\n    data2 = data2[col]\n    data2 = pd.get_dummies(data2)\n    c_train = y[y.notnull()==True].index\n    y_train = y[c_train]\n    columny = data2.columns\n    X_train = data2[columny].loc[c_train]\n    c_test = y[y.notnull()!=True].index\n    y_test = y[c_test]\n    X_test = data2[columny].loc[c_test]\n    #Model\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    #Filling missing data\n    y_pred = pd.Series(y_pred, index=c_test)\n    data[variab].loc[c_test] = y_pred.loc[c_test]\n    \ndef inputingnum(variab):\n    y = data[variab]\n    data2 = data.drop([variab],axis=1)\n    col = data2.columns[data2.isnull().sum()==0]\n    data2 = data2[col]\n    data2 = pd.get_dummies(data2)\n    c_train = y[y.notnull()==True].index\n    y_train = y[c_train]\n    columny = data2.columns\n    X_train = data2[columny].loc[c_train]\n    c_test = y[y.notnull()!=True].index\n    y_test = y[c_test]\n    X_test = data2[columny].loc[c_test]\n    #Model\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    #Filling missing data\n    y_pred = pd.Series(y_pred, index=c_test)\n    data[variab].loc[c_test] = y_pred.loc[c_test]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's imput missing values using two functions which I wrote. In KitchenQual, BsmtFullBath and BsmtHalfBath cases I'm gonna use Regressor model and convert them to integer."},{"metadata":{"_uuid":"fe93fb347e6047bf37af0d35faad5b807f52a8c7","scrolled":false,"trusted":true},"cell_type":"code","source":"inputing(variab='Electrical')\ninputing(variab='Exterior2nd')\ninputing(variab='Exterior1st')\ninputing(variab='MasVnrType')\ninputing(variab='Functional')\ninputing(variab='MSZoning')\ninputing(variab='SaleType')\ninputing(variab='Alley')\ninputing(variab='BsmtExposure')\ninputing(variab='BsmtFinType1')\ninputing(variab='BsmtFinType2')\ninputing(variab='Fence')\n\ninputingnum(variab='KitchenQual')\ndata['KitchenQual'] = data.KitchenQual.astype(int)\ninputingnum(variab='BsmtFullBath')\ndata['BsmtFullBath'] = data.BsmtFullBath.astype(int)\ninputingnum(variab='BsmtHalfBath')\ndata['BsmtHalfBath'] = data.BsmtHalfBath.astype(int)\n\ninputingnum(variab='TotalBsmtSF')\ninputingnum(variab='BsmtFinSF1')\ninputingnum(variab='BsmtFinSF2')\ninputingnum(variab='MasVnrArea')\ninputingnum(variab='BsmtUnfSF')\ninputingnum(variab='LotFrontage')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data['Utilities'].value_counts())\ndata  = data.drop(['Utilities'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns[data.isnull().sum()>0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's everything about imputing missing values."},{"metadata":{"_uuid":"47cb7aab40f3150f2905968333ef4202de09d5e5"},"cell_type":"markdown","source":"Let's understand a data set variable after variable, check basic statistics and drop a few outliers. I'll also drop variables with little differentiation. "},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\nplt.figure(figsize=(15,8))\nsns.distplot(data['SalePrice'][data.SalePrice.isnull()==False], fit= norm,kde=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##**Dropping outliers**"},{"metadata":{},"cell_type":"markdown","source":"On the scatter charts, I checked which observations could be considered outliers and I decided to delete them.\nI must be very careful because I don't want to remove observations from the test set."},{"metadata":{},"cell_type":"markdown","source":"For example, let's look at scatter plot of SalePrice and Lot Frontage."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.plot.scatter(x='LotFrontage',y='SalePrice'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dropping_outliers(data, condition):\n    #put condition with with reference to the data table, use brackets and (& |) operators, remember about you can drop observation only from train dataset\n    condition_to_drop = data[condition].index\n    data = data.drop(condition_to_drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dropping_outliers(data, (data.SalePrice<100000) & (data.train==1) & (data.LotFrontage>150))\ndropping_outliers(data, (data.LotFrontage>200) & (data.train==1))\ndropping_outliers(data, (data.SalePrice>700000) & (data.train==1))\ndropping_outliers(data, (data.SalePrice>700000) & (data.train==1))\ndropping_outliers(data, (data.LotArea>60000) & (data.train==1))\ndropping_outliers(data, (data.MasVnrArea>1450) & (data.train==1))\ndropping_outliers(data, (data.BedroomAbvGr==8) & (data.train==1))\ndropping_outliers(data, (data.KitchenAbvGr==3) & (data.train==1))\ndropping_outliers(data, (data['3SsnPorch']>400) & (data.train==1))\ndropping_outliers(data, (data.LotArea>100000) & (data.train==1))\ndropping_outliers(data, (data.MasVnrArea>1300) & (data.train==1))\ndropping_outliers(data, (data.BsmtFinSF1>2000) & (data.train==1) & (data.SalePrice<300000))\ndropping_outliers(data, (data.BsmtFinSF2>200) & (data.SalePrice>350000)  & (data.train==1))\ndropping_outliers(data, (data.BedroomAbvGr==8) & (data.train==1))\ndropping_outliers(data, (data.KitchenAbvGr==3) & (data.train==1))\ndropping_outliers(data, (data.TotRmsAbvGrd==2) & (data.train==1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CentalAir variable needs transformation to binary variable."},{"metadata":{"_uuid":"16618950e1835ffc688002633e4d7742fd7557c1","trusted":true},"cell_type":"code","source":"#CentralAir\nprint(data['CentralAir'].value_counts())\ndata['CentralAir'] = pd.Series(np.where(data['CentralAir'].values == 'Y', 1, 0),\n          data.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##**Feature engineering**"},{"metadata":{},"cell_type":"markdown","source":"* 2ndFloor - if the house has a second floor\n* Floors - total area of the first and second floor\n* TotBath - how many bathrooms house has\n* Porch - total area of the porch\n* TotalSF - total area of the house\n* Pool - if the house has a swimming pool\n* Bsmt - if the house has a basement\n* Garage - if the house has a garage\n* Fireplace - if the house has a fireplace\n* Remod - if the house was renovated\n* NewHouse - if the house is new\n* Age - ages of house\n"},{"metadata":{"_uuid":"c3e07c482caf498f1d2b49e770428dd40be3ba56","trusted":true},"cell_type":"code","source":"data['2ndFloor'] = pd.Series(np.where(data['2ndFlrSF'].values == 0, 0, 1),data.index)\ndata['Floors'] = data['1stFlrSF'] + data['2ndFlrSF']\ndata = data.drop(['1stFlrSF'],axis=1)\ndata = data.drop(['2ndFlrSF'],axis=1)\ndata['TotBath'] = data['FullBath'] + (0.5 * data['HalfBath']) + data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath'])\ndata['Porch'] = data['OpenPorchSF'] + data['3SsnPorch'] + data['EnclosedPorch'] + data['ScreenPorch']\ndata['TotalSF'] = data['BsmtFinSF1'] + data['BsmtFinSF2'] + data['Floors'] \ndata['Pool'] = pd.Series(np.where(data['PoolArea'].values == 0, 0, 1),data.index)\ndata['Bsmt'] = pd.Series(np.where(data['TotalBsmtSF'].values == 0, 0, 1),data.index)\ndata['Garage'] = pd.Series(np.where(data['GarageArea'].values == 0, 0, 1),data.index)\ndata['Fireplace'] = pd.Series(np.where(data['Fireplaces'].values == 0, 0, 1),data.index)\ndata['Remod'] = pd.Series(np.where(data['YearBuilt'].values == data['YearRemodAdd'].values, 0, 1),data.index)\ndata['NewHouse'] = pd.Series(np.where(data['YearBuilt'].values == data['YrSold'].values, 1, 0),data.index)\ndata['Age'] = data['YrSold'] - data['YearRemodAdd']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm gonna drop more observations."},{"metadata":{"trusted":true},"cell_type":"code","source":"c = data[(data['Floors']>4000) & (data.train==1)].index\ndata = data.drop(c)\nc = data[(data['SalePrice']>500000) & (data['TotalSF']<3500) & (data.train==1)].index\ndata = data.drop(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Droping a few variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['PoolQC'],axis=1)\ndata = data.drop(['GrLivArea'],axis=1)\ndata = data.drop(['Street'],axis=1)\ndata = data.drop(['GarageYrBlt'],axis=1)\ndata = data.drop(['PoolArea'],axis=1)\ndata = data.drop(['MiscFeature'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9478e67caa8c598fb313e397ed0d764b8b915f55"},"cell_type":"markdown","source":"**Preparing to modeling:**\n- dummies variables\n- two data frames with independent variables for train and test set\n- vector y with Sale Price variable for train set"},{"metadata":{},"cell_type":"markdown","source":"##**Modeling:**\n\n- XGB Regressor\n- Decision Tree Regressor\n- Random Forest Regressor\n- LASSO Regression\n- Ridge Regression\n- Linear Regression\n\n\nFor each model I tuned the parameters using loops and each model contains SalePrice variable tranformed to logarithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"Results = pd.DataFrame({'Model': [],'RMSLE': []})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrainX, testX, trainY, testY = train_test_split(data[data.SalePrice.isnull()==False].drop('SalePrice',axis=1),data.SalePrice[data.SalePrice.isnull()==False],test_size=0.30, random_state=2019)\ntrainY = np.log(trainY)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XGBoost Regressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost.sklearn import XGBRegressor\n\nmodel = XGBRegressor(learning_rate=0.001,n_estimators=4600,\n                                max_depth=7, min_child_weight=0,\n                                gamma=0, subsample=0.7,\n                                colsample_bytree=0.7,\n                                scale_pos_weight=1, seed=27,\n                                reg_alpha=0.00006)\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\n\nres = pd.DataFrame({\"Model\":['XGBoost'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree Regressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nmodel = DecisionTreeRegressor(max_depth=6)\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\n\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Decision Tree'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=1500,\n                                max_depth=6)\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Random Forest'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LASSO Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.0005)\n\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['LASSO'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stepwise Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n\nX2 = sm.add_constant(trainX)\no=0\nfor i in X2.columns:\n    o+=1\n    print(o)\n    model = sm.OLS(trainY, X2.astype(float))\n    model = model.fit()\n    p_values = pd.DataFrame(model.pvalues)\n    p_values = p_values.sort_values(by=0, ascending=False)\n    if float(p_values.loc[p_values.index[0]])>=0.05:\n        X2=X2.drop(p_values.index[0],axis=1)\n    else:\n        break\n\nkolumny = X2.columns\ntestX2 = sm.add_constant(testX)\ntestX2 = testX2[kolumny]\n\ny_pred = model.predict(testX2)\ny_pred = np.exp(y_pred)\n\n\nres = pd.DataFrame({\"Model\":['Stepwise Regression'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ridge Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=0.0005)\n\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Ridge'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear Regression**\n\nWhen you change alpha to 0 value in LASSO, you have simple Linear Regression model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0)\n\nmodel.fit(trainX,trainY)\ny_pred = model.predict(testX)\ny_pred = np.exp(y_pred)\nprint(rmsle(testY, y_pred))\n\nres = pd.DataFrame({\"Model\":['Linear Regression'],\n                    \"RMSLE\": [rmsle(testY, y_pred)]})\nResults = Results.append(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##**Results**"},{"metadata":{"trusted":true},"cell_type":"code","source":"Results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LASSO Regression model gives the best results. This model helps me to get 0.12903 (RMSLE) on competition test dataset and it gives me place in 20% best results on Leaderboard."},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX = data[data.SalePrice.isnull()==False].drop(['SalePrice','train'],axis=1)\ntrainY = data.SalePrice[data.SalePrice.isnull()==False]\ntestX = data[data.SalePrice.isnull()==True].drop(['SalePrice','train'],axis=1)\ntrainY = np.log(trainY)\nmodel = Lasso(alpha=0.0005)\nmodel.fit(trainX, trainY)\ntest = data[data.train==0]\ntest['SalePrice'] = model.predict(testX)\ntest['SalePrice'] = np.exp(test['SalePrice'] )\ntest = test.reset_index()\ntest[['Id','SalePrice']].to_csv(\"submissionLASSO.csv\",index=False)\nprint(\"done1\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}