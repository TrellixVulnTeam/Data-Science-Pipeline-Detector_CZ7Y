{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>House Prices Predictions using Keras</center></h1>\n<img src=\"https://i.ytimg.com/vi/LvfbopVq-WE/maxresdefault.jpg\" width=\"500\" height=\"600\">\n<br/>\n\n<h2>References</h2>\n\n[House Prices EDA, Lasso & LightGBM](https://www.kaggle.com/mviola/house-prices-eda-lasso-lightgbm-0-11635)\n\n[ANN House Price Prediction](https://www.kaggle.com/ppsheth91/ann-keras-hyper-parameter-tuning-price-prediction)"},{"metadata":{},"cell_type":"markdown","source":"<h2><center>In this notebook we are going to predict prices using Neural Network</center></h2>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.preprocessing import StandardScaler # To standardize the data\nfrom sklearn.ensemble import IsolationForest # To find and eliminate the outliers.\nfrom keras.models import Sequential # Sequential Neural Network\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping # Early Stopping Callback in the NN\nfrom keras.optimizers import Adam # Optimizer used in the NN\nfrom kerastuner.tuners import RandomSearch # HyperParameter Tunining\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading the Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\ny = train['SalePrice'].values\ndata = pd.concat([train,test],axis=0,sort=False)\ndata.drop(['SalePrice'],axis=1,inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Descriptive Statistics"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"column_data_type = []\nfor col in data.columns:\n    data_type = data[col].dtype\n    if data[col].dtype in ['int64','float64']:\n        column_data_type.append('numeric')\n    else:\n        column_data_type.append('categorical')\nplt.figure(figsize=(15,5))\nsns.countplot(x=column_data_type)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_values = data.isnull().sum()\nmissing_values = missing_values[missing_values > 0].sort_values(ascending = False)\nmissing_values\nNAN_col = list(missing_values.to_dict().keys())\nmissing_values_data = pd.DataFrame(missing_values)\nmissing_values_data.reset_index(level=0, inplace=True)\nmissing_values_data.columns = ['Feature','Number of Missing Values']\nmissing_values_data['Percentage of Missing Values'] = (100.0*missing_values_data['Number of Missing Values'])/len(data)\nmissing_values_data\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Filling NAN Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['BsmtFinSF1'].fillna(0, inplace=True)\ndata['BsmtFinSF2'].fillna(0, inplace=True)\ndata['TotalBsmtSF'].fillna(0, inplace=True)\ndata['BsmtUnfSF'].fillna(0, inplace=True)\ndata['Electrical'].fillna('FuseA',inplace = True)\ndata['KitchenQual'].fillna('TA',inplace=True)\ndata['LotFrontage'].fillna(data.groupby('1stFlrSF')['LotFrontage'].transform('mean'),inplace=True)\ndata['LotFrontage'].interpolate(method='linear',inplace=True)\ndata['MasVnrArea'].fillna(data.groupby('MasVnrType')['MasVnrArea'].transform('mean'),inplace=True)\ndata['MasVnrArea'].interpolate(method='linear',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in NAN_col:\n    data_type = data[col].dtype\n    if data_type == 'object':\n        data[col].fillna('NA',inplace=True)\n    else:\n        data[col].fillna(data[col].mean(),inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding New Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Total_Square_Feet'] = (data['BsmtFinSF1'] + data['BsmtFinSF2'] + data['1stFlrSF'] + \n                                                                 data['2ndFlrSF'] + data['TotalBsmtSF'])\n\ndata['Total_Bath'] = (data['FullBath'] + (0.5 * data['HalfBath']) + data['BsmtFullBath'] + \n                                                                  (0.5 * data['BsmtHalfBath']))\n\ndata['Total_Porch_Area'] = (data['OpenPorchSF'] + data['3SsnPorch'] + \n                                                data['EnclosedPorch'] + data['ScreenPorch'] + data['WoodDeckSF'])\n\ndata['SqFtPerRoom'] = data['GrLivArea'] / (data['TotRmsAbvGrd'] + data['FullBath'] +\n                                                       data['HalfBath'] + data['KitchenAbvGr'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# One Hot Encoding for Categorical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Splitting train and test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data[:1460].copy()\ntest = data[1460:].copy()\ntrain['SalePrice'] = y\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Extracting Top Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_features = train.corr()[['SalePrice']].sort_values(by=['SalePrice'],ascending=False).head(30)\nplt.figure(figsize=(5,10))\nsns.heatmap(top_features,cmap='rainbow',annot=True,annot_kws={\"size\": 16},vmin=-1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now that we have extracted the top features that influnces the SalePrice we would see their distribution to find outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_data(col, discrete=False):\n    if discrete:\n        fig, ax = plt.subplots(1,2,figsize=(14,6))\n        sns.stripplot(x=col, y='SalePrice', data=train, ax=ax[0])\n        sns.countplot(train[col], ax=ax[1])\n        fig.suptitle(str(col) + ' Analysis')\n    else:\n        fig, ax = plt.subplots(1,2,figsize=(12,6))\n        sns.scatterplot(x=col, y='SalePrice', data=train, ax=ax[0])\n        sns.distplot(train[col], kde=False, ax=ax[1])\n        fig.suptitle(str(col) + ' Analysis')\n    \nprint('Plot Function is ready to use')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data('OverallQual',True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We see there are two outliers with 10 overall quality and price less than 200000."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(train[(train['OverallQual'] == 10) & (train['SalePrice'] < 200000)].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data('Total_Square_Feet')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## This seems more or less appropriate distribution with no outliers whatsoever."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data('GrLivArea')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Again no outliers that can be eliminated."},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data('Total_Bath')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Here we clearly see two outliers that have Total_Bath more than 4 but with sale price less than 200000."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(train[(train['Total_Bath'] > 4) & (train['SalePrice'] < 200000)].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_data('TotalBsmtSF')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Her as well we see 1 clear outlier that has TotalBsmtSF more than 3000 but sale price less than 300000."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(train[(train['TotalBsmtSF'] > 3000) & (train['SalePrice'] < 400000)].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## After resetting the index,this is the final train data that we get"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.reset_index()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Outlier elimination through Isolation Forest!!\n### We use this algorithm since it would be difficult to go through all the features and eliminate the outliers manually but it was important to do it for the features that have higher correlation with the SalePrice"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = IsolationForest(max_samples = 100, random_state = 42)\nclf.fit(train)\ny_noano = clf.predict(train)\ny_noano = pd.DataFrame(y_noano, columns = ['Top'])\ny_noano[y_noano['Top'] == 1].index.values\n\ntrain = train.iloc[y_noano[y_noano['Top'] == 1].index.values]\ntrain.reset_index(drop = True, inplace = True)\nprint(\"Number of Outliers:\", y_noano[y_noano['Top'] == -1].shape[0])\nprint(\"Number of rows without outliers:\", train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling the features using Sklearn Standard Scalar"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.copy()\nX.drop(['SalePrice'],axis=1,inplace=True)\ny = train['SalePrice'].values\nX.shape,y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scale = StandardScaler()\nX = scale.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODELLING\n"},{"metadata":{},"cell_type":"markdown","source":"### We would use Random Algorithm from keras for hyper-parameter tuning of the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(hp):\n    model = Sequential()\n    for i in range(hp.Int('layers', 2, 10)):\n        model.add(Dense(units=hp.Int('units_' + str(i),\n                                            min_value=32,\n                                            max_value=512,\n                                            step=32),\n                               activation='relu'))\n    model.add(Dense(1))\n    model.compile(\n        optimizer=Adam(\n            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n        loss='mse',\n        metrics=['mse'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner = RandomSearch(\n    build_model,\n    objective='val_mse',\n    max_trials=10,\n    executions_per_trial=3,\n    directory='model_dir',\n    project_name='House_Price_Prediction')\ntuner.search_space_summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tuner.search(X[1100:],y[1100:],batch_size=128,epochs=200,validation_data=validation_data=(X[:1100],y[:1100]))\n# model = tuner.get_best_models(1)[0]\n\n# After implementing this and tuning further we get the below model that I have implemented separately.Won't be running this here.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(320, input_dim=X.shape[1], activation='relu'))\n    model.add(Dense(384, activation='relu'))\n    model.add(Dense(352, activation='relu'))\n    model.add(Dense(448, activation='relu'))\n    model.add(Dense(160, activation='relu'))\n    model.add(Dense(160, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1))\n    # Compile model\n    model.compile(optimizer=Adam(learning_rate=0.0001), loss = 'mse')\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We would be using early stopping callback and would use 1/10th of the training data as validation to estimate the optimum number of epochs that would prevent overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nhistory = model.fit(x=X,y=y,\n          validation_split=0.1,\n          batch_size=128,epochs=1000, callbacks=[early_stop])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)\nlosses.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model() # Resetting the model.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the model with full training data and optimum number of epochs!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x=X,y=y,\n          batch_size=128,epochs=170)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = pd.DataFrame(model.history.history)\nlosses.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction & Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.evaluate(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = scale.transform(test)\nresult = model.predict(X_test)\nresult = pd.DataFrame(result,columns=['SalePrice'])\nresult.head()\nresult['Id'] = test['Id']\nresult = result[['Id','SalePrice']]\nresult.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Note: I would just like to say that Keras is not the most suitable model for this problem since the dataset given in this problem is not sufficient!!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}