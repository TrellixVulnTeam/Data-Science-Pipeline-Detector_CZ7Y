{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop id columns\ntrain = train.drop('Id', axis = 1)\ntest = test.drop('Id', axis = 1)\n\n#treatind year fields as objects to classify them as categorical data\ntrain['YearBuilt'] =  train['YearBuilt'].astype(str)\ntrain['YearRemodAdd'] =  train['YearRemodAdd'].astype(str)\ntrain['YrSold'] =  train['YrSold'].astype(str)\ntest['YearBuilt'] =  test['YearBuilt'].astype(str)\ntest['YearRemodAdd'] =  test['YearRemodAdd'].astype(str)\ntest['YrSold'] =  test['YrSold'].astype(str)\n\n#dependent and independent variables\ntrain_independent = train.iloc[:, :-1]\ntrain_dependent = train.iloc[:, -1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#categorical and numerical features\nnumerical_features = []\ncategorical_features = []\nfor column in train_independent.columns:\n    if train_independent[column].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n        numerical_features.append(column)\n    elif train_independent[column].dtype == object:\n        categorical_features.append(column)\n\ncolumns = categorical_features + numerical_features\ntrain_independent = train[columns]\ntest = test[columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#merging test and train independent variables\nfeatures = pd.concat([train_independent, test], axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train features :', train_independent.shape)\nprint('train target :', train_dependent.shape)\nprint('test features :', test.shape)\nprint('train dataset :', train.shape)\nprint('all features :', features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Numerical features with null values\nnull = features[numerical_features].isna().sum().sort_values(ascending = False)\nnull_values = pd.DataFrame(null)\nnull_values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **lotfrantage**:\n    We could check feature with highest correlation w/lotfrontage and fill the missing values with mean of lot frontage of houses grouped by the feature with the highest correlation but this leaves out the categorical variables which have not been encoded yet.\n    So, neighborhood makes the most sense.\n* **GarageYrBlt**, **GarageCars**, **GarageArea**, **GarageQual**: going to assume no garages.\n* **Bsmts**: assuming no basements.\n* **MasonVeneer Area**: No Type, no area."},{"metadata":{"trusted":true},"cell_type":"code","source":"features['GarageYrBlt'] = features['GarageYrBlt'].fillna(0)\nfeatures['MasVnrArea'] = features['MasVnrArea'].fillna(0)\nfeatures['BsmtFullBath'] = features['BsmtFullBath'].fillna(0)\nfeatures['BsmtHalfBath'] = features['BsmtHalfBath'].fillna(0)\nfeatures['TotalBsmtSF'] = features['TotalBsmtSF'].fillna(0)\nfeatures['BsmtUnfSF'] = features['BsmtUnfSF'].fillna(0)\nfeatures['BsmtFinSF2'] = features['BsmtFinSF2'].fillna(0)\nfeatures['GarageCars'] = features['GarageCars'].fillna(0)\nfeatures['GarageArea'] = features['GarageArea'].fillna(0)\nfeatures['BsmtFinSF1'] = features['BsmtFinSF1'].fillna(0)\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\n\n#checking for anymore missing values\nfeatures[numerical_features].isna().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Categorical features with null values\nnull = features[categorical_features].isna().sum().sort_values(ascending = False)\nnull_values = pd.DataFrame(null)\nnull_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features['PoolQC'] = features['PoolQC'].fillna('Null')\nfeatures['MiscFeature'] = features['MiscFeature'].fillna('Null')\nfeatures['Alley'] = features['Alley'].fillna('Null')\nfeatures['Fence'] = features['Fence'].fillna('Null')\nfeatures['FireplaceQu'] = features['FireplaceQu'].fillna('Null')\nfeatures['GarageCond'] = features['GarageCond'].fillna('Null')\nfeatures['GarageQual'] = features['GarageQual'].fillna('Null')\nfeatures['GarageFinish'] = features['GarageFinish'].fillna('Null')\nfeatures['GarageType'] = features['GarageType'].fillna('Null')\nfeatures['BsmtExposure'] = features['BsmtExposure'].fillna('Null')\nfeatures['BsmtCond'] = features['BsmtCond'].fillna('Null')\nfeatures['BsmtQual'] = features['BsmtQual'].fillna('Null')\nfeatures['BsmtFinType2'] = features['BsmtFinType2'].fillna('Null')\nfeatures['BsmtFinType1'] = features['BsmtFinType1'].fillna('Null')\nfeatures['Utilities'] = features['Utilities'].fillna('Null')\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(features['KitchenQual'].mode()[0])\nfeatures['Functional'] = features['Functional'].fillna(features['Functional'].mode()[0])\nfeatures['MasVnrType'] = features['MasVnrType'].fillna(features['MasVnrType'].mode()[0])\nfeatures['Electrical'] = features['Electrical'].fillna(features['Electrical'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\nfeatures['MSZoning'] = features['MSZoning'].fillna(features['MSZoning'].mode()[0])\n\n#checking for remaining null values\nfeatures[categorical_features].isna().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Generation"},{"metadata":{},"cell_type":"markdown","source":"* Original Age\n* Age since Remodelling\n* Extra Rooms\n* Floors_Area\n* Bathrooms\n* Walled Area\n* Porch Area\n* Occupied area\n* Basement Size\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"features['age'] = features['YrSold'].astype(int) - features['YearBuilt'].astype(int)\nfeatures['remod_age'] = features['YrSold'].astype(int) - features['YearRemodAdd'].astype(int)\nfeatures['extra_rooms'] = features['TotRmsAbvGrd'] - features['BedroomAbvGr'] - features['KitchenAbvGr']\nfeatures['floors_area'] = features['1stFlrSF'] + features['1stFlrSF']\nfeatures['total_bathrooms'] = features['FullBath'] + (0.5 * features['HalfBath']) + features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath'])\nfeatures['porch_area'] = features['WoodDeckSF'] + features['OpenPorchSF'] + features['EnclosedPorch'] + features['3SsnPorch'] + features['ScreenPorch'] + features['PoolArea']\nfeatures['walled_area'] = features['TotalBsmtSF'] +features['GrLivArea']\nfeatures['TotalOccupiedArea'] = features['walled_area'] + features['porch_area']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Skewness"},{"metadata":{"trusted":true},"cell_type":"code","source":"#dependent variable\nf, ax = plt.subplots(figsize=(9, 8))\nsns.distplot(train_dependent, bins = 20, color = 'Magenta')\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Right tailed."},{"metadata":{"trusted":true},"cell_type":"code","source":"# log transformation\ntrain_dependent = np.log1p(train_dependent)\n\n#after transformation\nf, ax = plt.subplots(figsize=(9, 8))\nsns.distplot(train_dependent, bins = 20, color = 'Magenta')\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Other numerical variables\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nskew_features = features[numerical_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize skewed features with boxcox transformation\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Multicolinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"#defining numerical features again to include the added features for the correlation plot to be plotted.\nnumerical_features = []\nfor column in train_independent.columns:\n    if train_independent[column].dtype in ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']:\n        numerical_features.append(column)\n\nnew_train_set = pd.concat([features.iloc[:len(train_dependent), :], train_dependent], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"method from https://www.kaggle.com/pcbreviglieri/enhanced-house-price-predictions/notebook#Enhanced-House-Price-Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation_map(f_data, f_feature, f_number):\n    f_most_correlated = f_data.corr().nlargest(f_number,f_feature)[f_feature].index\n    f_correlation = f_data[f_most_correlated].corr()\n    \n    f_mask = np.zeros_like(f_correlation)\n    f_mask[np.triu_indices_from(f_mask)] = True\n    with sns.axes_style(\"white\"):\n        f_fig, f_ax = plt.subplots(figsize=(12, 10))\n        f_ax = sns.heatmap(f_correlation, mask=f_mask, vmin=0, vmax=1, square=True,\n                           annot=True, annot_kws={\"size\": 10}, cmap=\"BuPu\")\n\n    plt.show()\n\ncorrelation_map(new_train_set, 'SalePrice', 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping features with high correlation with other independent variables to avoid chances of multicolinearity.\nfeatures = features.drop(['GarageCars','1stFlrSF', 'walled_area'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.get_dummies(features).reset_index(drop=True)\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The extra number of columns can be seen after encoding."},{"metadata":{},"cell_type":"markdown","source":"#### dropping columns with predominant 0 values"},{"metadata":{},"cell_type":"markdown","source":"method from https://www.kaggle.com/pcbreviglieri/enhanced-house-price-predictions/notebook#Enhanced-House-Price-Predictions again."},{"metadata":{"trusted":true},"cell_type":"code","source":"features_to_be_dropped = []\nfor feature in features.columns:\n    all_value_counts = features[feature].value_counts()\n    zero_value_counts = all_value_counts.iloc[0]\n    if zero_value_counts / len(features) > 0.995:\n        features_to_be_dropped.append(feature)\nprint('\\nFeatures with predominant zeroes:\\n')\nprint(features_to_be_dropped)\n\nfeatures = features.drop(features_to_be_dropped, axis=1).copy()\nfeatures.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Transformations"},{"metadata":{"trusted":true},"cell_type":"code","source":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','MiscVal', 'floors_area']\n\nfeatures = logs(features, log_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['OverallQual', 'LotFrontage_log', \n              'TotalBsmtSF_log', 'GrLivArea_log',\n              'GarageArea_log', 'floors_area_log']\nfeatures = squares(features, squared_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Reconstructing train and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = features.iloc[:len(train_dependent), :]\nx_test = features.iloc[len(train_dependent):, :]\ny_train = train_dependent\ntrain_set = pd.concat([x_train, y_train], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train features:', x_train.shape)\nprint('train target:', y_train.shape)\nprint('test features:', x_test.shape)\nprint('train set:', train_set.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Selection, Stacking, Fitting and Preicting."},{"metadata":{},"cell_type":"markdown","source":"***gradientboostingregressor, xgbregressor, randomforestregressor, adaboostregressor, ridgecv, stackngcvregressor***"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\nfrom mlxtend.regressor import StackingCVRegressor\nimport xgboost\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n#randomforest\nrf = RandomForestRegressor(n_estimators=300, random_state=0)\n\n#adaboost\nada = AdaBoostRegressor(learning_rate = 0.05, loss =  'linear', n_estimators = 100 , random_state = 0)\n\n#xgb\nxgb = XGBRegressor(learning_rate=0.01, n_estimators=6000, max_depth=3, min_child_weight=0, gamma=0, subsample=0.7, colsample_bytree=0.7,\n                       objective='reg:squarederror', nthread=-1, scale_pos_weight=1, seed=27, reg_alpha=0.00006, random_state=0)\n\n#ridgecv\nkfolds = KFold(n_splits=10, shuffle=True, random_state=0)\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=[13.5, 14, 14.5, 15, 15.5], cv=kfolds))\n\n#gradient\ngrad = GradientBoostingRegressor(n_estimators=4000, learning_rate=0.01, max_depth=4, max_features='sqrt', min_samples_leaf=15, \n                                 min_samples_split=10, loss='huber', random_state=0)\n\n#stackcv \nstackcv = StackingCVRegressor(regressors=(rf, ada, xgb, \n                                          ridge, grad),\n                              meta_regressor=xgb,\n                              use_features_in_secondary=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#individual performance\nscores_rf = -1 * cross_val_score(rf, x_train, y_train, cv=5, scoring='neg_mean_absolute_error')\nscores_ada = -1 * cross_val_score(ada, x_train, y_train, cv=5, scoring='neg_mean_absolute_error')\nscores_xgb = -1 * cross_val_score(xgb, x_train, y_train, cv=5, scoring='neg_mean_absolute_error')\nscores_ridge = -1 * cross_val_score(ridge, x_train, y_train, cv=5, scoring='neg_mean_absolute_error')\nscores_grad = -1 * cross_val_score(grad, x_train, y_train, cv=5, scoring='neg_mean_absolute_error')\nprint('random forest mae:', scores_rf.mean())\nprint('Ada boost:', scores_ada.mean())\nprint('xgboost:', scores_xgb.mean())\nprint('ridgecv:', scores_ridge.mean())\nprint('Gradient Boosting:', scores_grad.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adaboost kinda sucks"},{"metadata":{"trusted":true},"cell_type":"code","source":"#fitting\nrf_fit = rf.fit(x_train, y_train)\nada_fit = ada.fit(x_train, y_train)\nxgb_fit = xgb.fit(x_train, y_train)\nridge_fit = ridge.fit(x_train, y_train)\ngrad_fit = grad.fit(x_train, y_train)\n\nstackcv_fit = stackcv.fit(np.array(x_train), np.array(y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blend = [0.1994, 0.0000, 0.2031, 0.2017, 0.2032, 0.2043]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#blending                \ny_pred = np.expm1((blend[0] * rf_fit.predict(x_test)) +\n                  (blend[1] * ada_fit.predict(x_test)) +\n                  (blend[2] * xgb_fit.predict(x_test)) +\n                  (blend[3] * ridge_fit.predict(x_test)) +\n                  (blend[4] * grad_fit.predict(x_test)) +\n                  (blend[5] * stackcv_fit.predict(np.array(x_test))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\nsubmission.iloc[:, 1] = np.round_(y_pred)\nsubmission.to_csv(\"submission_z.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}