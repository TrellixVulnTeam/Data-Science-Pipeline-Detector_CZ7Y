{"cells":[{"metadata":{},"cell_type":"markdown","source":"This Kernel is meant to solve the Housing Prices Challenge in a simple way and produce an acceptable leaderboard score. We will apply Linear Regression and machine learning models known for handling structured data.\n\n> Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price.\n\nResources used to guide my personal learning and application throughout this House Price project:\n* [Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)\n* [Getting Started with Kaggle: House Prices Competition](https://www.dataquest.io/blog/kaggle-getting-started/)\n* [Regularized Linear Models](https://www.kaggle.com/apapiu/regularized-linear-models)"},{"metadata":{},"cell_type":"markdown","source":"**Notebook Content:**\n1. Imports\n2. Exploratory Data Analysis\n3. Transforming and Engineering Feautures"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# visualiation tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# sci-kit learn tools\nfrom scipy.stats import skew\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# given data imports\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# copies of DS for manipulation (we will use this for remainder of project)\ntrain_df = train.copy()\ntest_df = test.copy()\n\n# copies for EDA purposes\nEDA_train = train.copy()\nEDA_test = test.copy()\n\n# combine data sets to avoid dimension misalignment\nall_data = pd.concat((train_df.loc[:,'MSSubClass':'SaleCondition'],\n                      test_df.loc[:,'MSSubClass':'SaleCondition']))\n\nprint(train_df.shape, test_df.shape, all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop target (dependent variable) from training dataframe\nactual_y = train_df['SalePrice']\n#train_df = train_df.drop('SalePrice', axis=1)\n\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exploratory Data Analysis**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from Abhinand \"Predicting HousingPrices: Simple Approach\" Kernel\ndef show_all(df):\n    #This fuction lets us view the full dataframe\n    with pd.option_context('display.max_rows', 100, 'display.max_columns', 100):\n        display(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_all(train_df.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can see that there exists many qualitative and missing values^\n> \n> Let's take a look at the skewness of SalePrice to see if a log transformation will be necessary for linear regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 6))\nsns.distplot(actual_y)\nprint(\"Skew is: \", actual_y.skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> You can see that the data is skewed. We will attempt to log-transform the data to bring the skew number closer to 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"log_actual_y = np.log(actual_y)\nf, ax = plt.subplots(figsize=(12, 6))\nsns.distplot(log_actual_y)\n\nprint(\"Skew is: \", log_actual_y.skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> A skew value closer to 0 means that we have improved the skewness of the data. You can see from the plot that the logged data resembles a normal distribution!"},{"metadata":{},"cell_type":"markdown","source":"We will handle quantitative and qualitative features seperately. We will begin with Quantitative features. We will examine correlation between actual SalePrice and quantitative features."},{"metadata":{"trusted":true},"cell_type":"code","source":"quant = train.select_dtypes(include=[np.number])\nquant.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Examine correlations between SalePrice and target"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = quant.corr()\nprint(corr['SalePrice'].sort_values(ascending=False)[:5])\nprint(corr['SalePrice'].sort_values(ascending=False)[-5:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now see the top 5 most postiviely correlated features with SalePrice. \n> If your dataset has perfectly positive or negative attributes then there is a high chance that the performance of the model will be impacted by a problem called — “Multicollinearity”\n\nLets take a look at a correlation heatmap."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_map = train_df.corr()\nfig, ax = plt.subplots(figsize=(20,16))\nsns.heatmap(corr_map, vmax=.8, square=True, annot=True, fmt='.1f')\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can see that 'GrLivArea' and 'TotRmsAbvGrd', 'TotalBsmtSF' and '1stFlrSF', 'YearBuilt' and 'GarageYrBlt' have high correlations. **These correlations are so strong that it can indicate a situation of multicollinearity**.\n\nLets take a moment to visualize these highly correlated numeric features (and later trim outliers).\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 10 high correlation to SalePrice matrix\nn = 10\ncols = corr_map.nlargest(n, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nfig, ax = plt.subplots(figsize=(10,8))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 'GarageCars' and 'GarageArea' are like twins. So we just need one. We will choose 'GarageCars' because it has a stronger correlation to 'SalePrice'!\n* 'TotalBsmtSF' and '1stFlrSF' are also twins. We will choose 'TotalBsmtSF' because of higher correlation to SP\n* 'TotRmsAbvGrd' and 'GrLivArea' are also twins. We will choose 'GrLivArea'\n\n> * (Drop 'GarageArea')\n> * (Drop '1stFlrSF')\n> * (Drop 'TotRmsAbvGrd')"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.OverallQual.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pivot table to further investigate relationship between 'OverallQual' and 'SalePrice'\nquality_pivot = train.pivot_table(index='OverallQual', values='SalePrice', aggfunc=np.median)\nquality_pivot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(8, 4))\nsns.lineplot(x='OverallQual', y = train_df.SalePrice, color='green',data=train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We can see that as the overall quality of the house increases so does the price of the house. This is an excellent variable for our model.\n\nNow we will take a look at 'GrLivArea'"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6), dpi=80)\nplt.scatter(x = train_df['GrLivArea'], y = log_actual_y)\nplt.ylabel('LogSalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Outliers can affect a regression model by pulling our estimated regression line further away from the true population regression line. So, we’ll remove those observations from our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove outliers and update EDA_train\nEDA_train = EDA_train[EDA_train['GrLivArea'] < 4000]\n\nplt.figure(figsize=(8, 6), dpi=80)\nplt.scatter(x = EDA_train['GrLivArea'], y = np.log(EDA_train.SalePrice))\nplt.xlim(-200,6000) # keeps same scale as first scatter plot\nplt.ylabel('LogSalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets do the same for all data\n#all_data = all_data[all_data['GrLivArea'] < 4000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will take a look at garage area."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6), dpi=80)\nplt.scatter(x = train_df['GarageArea'], y = np.log(train_df.SalePrice))\nplt.ylabel('LogSalePrice')\nplt.xlabel('GarageArea')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove outliers and update train_df\nEDA_train = EDA_train[EDA_train['GarageArea'] < 1200]\n\nplt.figure(figsize=(8, 6), dpi=80)\nplt.scatter(x = EDA_train['GarageArea'], y = np.log(EDA_train.SalePrice))\nplt.xlim(-50,1475)\nplt.ylabel('LogSalePrice')\nplt.xlabel('GarageArea')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instead of removing outlier rows, lets try to impute them with a value\n# dropping rows with outliers is misaligning my data and preventing submission\n#all_data = all_data[all_data['GarageArea'] < 1200]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can do more investigation on other variable outliers at a later date. Now we will take a look at missing values and begin imputation process. (Do not forget it will soon be time to combine two data sets to avoid dimension misalignment)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of missing values in each column of training data\nmissing_vals = (train_df.isnull().sum())\nprint(missing_vals[missing_vals > 0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will drop all variables with a high amount of missing values. Why? None of these variables seem to be important or considered when deciding to buy a house (and that's probably why they have so many missing values). \n> Drop: 'MiscFeature', 'Fence', 'PoolQC', 'FireplaceQu', 'Alley'\n\nIn regards to the Garage-related variables with missing values, we already have a garage variable with a high correlation to SalePrice. That variable alone will do the trick so we will delete all Garage variables with missing data.\n> Drop: 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond'\n\nThe same logic applies to Bsmt variables. Also MasVnr variables correlate heavily with 'OverallQual' so we will delete those.\n\nWe will delete everything except for Electrical.\n\n> General intution here was gathered from [Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)\n"},{"metadata":{},"cell_type":"markdown","source":"*We will now begin an analysis on the normality of some of our very important features. Let's note our most important variables thus far:*\n> * OverallQual\n> * GarageCars (recall all other garage variables have been dropped)\n> * TotalBsmtSF (recall all other Bsmt variables have been dropped)\n> * GrLivArea"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GrLivArea\nf, ax = plt.subplots(figsize=(8, 4))\nsns.distplot(train_df['GrLivArea'])\nprint(\"Skew is: \", train_df['GrLivArea'].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Holy skew! Let's transform."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['GrLivArea'] = np.log(train_df['GrLivArea'])\n\nf, ax = plt.subplots(figsize=(8, 4))\nsns.distplot(train_df['GrLivArea'])\nprint(\"Skew is: \", train_df['GrLivArea'].skew())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TotalBsmtSF\nf, ax = plt.subplots(figsize=(8, 4))\nsns.distplot(train_df['TotalBsmtSF'])\nprint(\"Skew is: \", train_df['TotalBsmtSF'].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> There does exists methods to log-transform complicated variables that contain: values equal to zero. We will visit this at a later date."},{"metadata":{},"cell_type":"markdown","source":"**Transforming and Engineering Features**\n> Start with applying all drops and transformations noted in EDA section. \n> For this section I reference: [Regularized Linear Models](https://www.kaggle.com/apapiu/regularized-linear-models)"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will begin by applying log transformation to skewed numeric features\nnum_data = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskew_data = all_data[num_data].apply(lambda x: skew(x.dropna()))\nskew_data = skew_data[skew_data > 0.75]\nskew_data = skew_data.index\n\nall_data[skew_data] = np.log1p(all_data[skew_data])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the sake of working quick, we will encode all qualitative variables with dummy representations. At a later point we will re-visit qualitative variables with a more granular approach."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop all features with missing values, noted above : keep electrical\nall_data = all_data.drop((missing_vals[missing_vals > 1]).index,1)\n#all_data = all_data.drop(all_data.loc[all_data['Electrical'].isnull()].index)\n\n# fix few number of missing vals in test set\nall_data = all_data.fillna(all_data.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.get_dummies(all_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop variables noted in EDA section\ndrop_me = ['GarageArea', '1stFlrSF', 'TotRmsAbvGrd']\nall_data = all_data.drop(drop_me, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# quick look under the hood\nshow_all(all_data.head())\nprint(all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split concatonated data into train and test dataframes\n\ny = np.log1p(train_df[\"SalePrice\"])\ntrain_df = train_df.drop('SalePrice', axis=1)\nX_train = all_data[:train_df.shape[0]]\nX_test = all_data[train_df.shape[0]:]\n\n\nX_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Build the Model**\n>  We will attempt to apply the following models:\n> * Linear Regression\n> * Lasso Regression\n> * Random Forests"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Root-Mean-Squared-Error (RMSE) evaluation metric\nfrom sklearn.model_selection import cross_val_score\n\n# from \"Regularized Linear Models\" w/ cross validation\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Linear Regression !\nfrom sklearn import linear_model\n\nlinear_model = linear_model.LinearRegression()\nlr_model = linear_model.fit(X_train, y)\n\nrmse_cv(lr_model).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LassoCV !\nfrom sklearn.linear_model import LassoCV\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nlasso_model = LassoCV(alphas = [1, 0.1, 0.001, 0.0005, 0.005, 0.0001, 0.5, 0.2]).fit(X_train, y)\nrmse_cv(lasso_model).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(random_state=42, max_depth = 6, n_jobs = 5)\nrf_model.fit(X_train, y)\n\nrmse_cv(rf_model).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_pred = np.expm1(lr_model.predict(X_test))\nlasso_pred = np.expm1(lasso_model.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Submit!"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submit!\noutput = pd.DataFrame({\"id\":test.Id, \"SalePrice\":lasso_pred})\noutput.to_csv(\"lasso_solution.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}