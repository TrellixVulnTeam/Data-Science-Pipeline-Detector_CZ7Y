{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nEstimating the sale prices of houses is one of the basic projects to have on our Data Science CV. By finishing this Kernel, we will be able to predict continuous variables using various types of regressor algorithm. In this work, we want to perform the steps of data analysis and build a House price prediction model in the simplest, easiest and straight way, according to which:\n\n+ Understand the problem: We will look at each variable and analyze philosophy about their meaning and importance to this problem.\n\n+ We'll focus on 'SalePrice' variable and try to know a little more about it, making the simplest adjustment to be able to apply basic Machine Learning.\n\n+ Independent variable: we will try to understand the relationship of the dependent variable and the independent variable.\n\n+ Basic data cleaning: We will clean up the data set and process the missing data, outliers and categorize variables. We don't go in the same direction as all the other Kagglers did by merging the train set and the test set BUT we do separately, assuming the test set is unknown, the cleaning goal of the test data set is just to prediction manipulation is performed.\n\n+ Statists: We will check to see if our data meets the assumptions required by most variable multivariate techniques.\n\n# Now, it's time to have fun!","metadata":{}},{"cell_type":"markdown","source":"We are going to break everything into logical steps that allow us to ensure the cleanest, most realistic data for our model to make accurate predictions from. The layout of the Notebook is summarized as below:\n\n# Section I: DATA PREPROCESSING & EDA\n\n1. Importing Data and Libraries\n2. Data cleaning: dealing with NaN or Null or missing data\n3. Data visualization, variable correlations: key variable parameters?\n4. Statistical (if any)\n\n# Section II: HOUSE PRICE MODEL\n\n1. Feature Selection, data handling & data Split\n2. Data Labeling\n3. Data spliting: training and testing\n4. Selected \"Best Model\"\n5. Model's Parameters tuning\n6. Submission\n7. Conclusion","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section I: DATA PREPROCESSING & EDA","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing Data and Libraries","metadata":{}},{"cell_type":"markdown","source":"Using the ‘read_csv’ function provided by the Pandas package, we can import the data into our python environment. After importing the data, we can use the ‘head’ function to get a glimpse of our dataset.","metadata":{}},{"cell_type":"code","source":"d_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\nd_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the summary above, we have total 1460 rows of data, 80 and 81 columns for the test and the data sets. Before we get into dealing with missing data, we will explore our dataset:\n\n+ We have to check which one have impact on the target value?\n+ But, wow ! 80 columns, so we would love to show all columns and rows, because it's easier to follow & check, by setting the following ...","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, importing some basic Librairies we might use ...","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport matplotlib.style as style\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data cleaning: dealing with NaN or Null or missing data\n\nIn this step, we start with removing all the null/NaN values that contain in our dataset. We can do this in Python using the ‘dropna’ function. We have different approaches to dealing with missing data, in this work, we assumpte that:\n\n+ We will drop all columns where the data missing ratio > 20%. We could observe below that \"Alley\", \"FireplaceQu\", \"Fence\", \"MiscFeature\", \"PoolQC\" are probaly the 5 first columns we will remove. Also, \"LotFrontage\" could be the next column that we have to consider.\n+ Dealing with the numerical missing data by replacing column's mean value, and the object columns will be considered in the next section.","metadata":{}},{"cell_type":"markdown","source":"Now, using the ‘describe’ function we can get a statistical view of the data like mean, median, standard deviation, and so on.","metadata":{}},{"cell_type":"code","source":"d_train.describe(include='all').T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yes, we could confirm our observation above in this Table.","metadata":{}},{"cell_type":"code","source":"d_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, Quick check on the test data set ...","metadata":{}},{"cell_type":"code","source":"d_test.isnull().sum().sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The missing values are very similar in both datasets. So, we decide to remove 5 columns in both datasets.","metadata":{}},{"cell_type":"markdown","source":"Now, we fill all NaN values (on the numerical columns) using the mean value of corresponding columns, by applying the fillna, and do not forget to implement the same action on the test data set.","metadata":{}},{"cell_type":"code","source":"d_train['YrSold'] = d_train['YrSold'].apply(str)\nd_train['MoSold'] = d_train['MoSold'].apply(str)\nd_test['YrSold'] = d_test['YrSold'].apply(str)\nd_test['MoSold'] = d_test['MoSold'].apply(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_test.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categorical data\ncat_cols=np.array(d_test.columns[d_test.dtypes == object])\n\nfor feature in cat_cols:\n    d_train[feature].fillna(d_train[feature].mode()[0], inplace=True)\n    d_test[feature].fillna(d_test[feature].mode()[0], inplace=True)    \n\n# categorical data\nnum_cols=np.array(d_test.columns[d_test.dtypes != object])\nfor feature in num_cols:\n    d_train = d_train.fillna(0)\n    d_test = d_test.fillna(0)\n    \nd_train = d_train.fillna(\"Other\")\nd_test = d_test.fillna(\"Other\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we feel better with the current dataset and take a look over the data trend by using the subplots ...","metadata":{}},{"cell_type":"code","source":"d_train.plot(subplots=True, sharex = True, figsize=(20,50))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could observe that: 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'KitchenAbvGr', '3SsnPorch', 'PoolArea', 'MiscVal' are VERY unbalanced, and we shoudl to find out a solution to handle with these columns in the modeling section.","metadata":{}},{"cell_type":"markdown","source":"# 3. Data visualization, variable correlations: key variable parameters?","metadata":{}},{"cell_type":"markdown","source":"Before processing null values in the object columns, we try to check the correlation between SalePrice and all other numerical variables by using the corr() function:","metadata":{}},{"cell_type":"code","source":"d_train.corr()['SalePrice'].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could see that, there are two main groups of correlated variables: POSITIVE and NEGATIVE.\nNow, we want to see something more beautiful, such as graphics. ","metadata":{}},{"cell_type":"markdown","source":"# Data Visualization\n\nIn this process, we are going to produce three different types of charts including heatmap, scatter plot, and a distribution plot. Heatmaps are very useful to find relations between two variables in a dataset. Heatmap can be easily produced using the ‘heatmap’ function provided by the seaborn package in python.","metadata":{}},{"cell_type":"code","source":"style.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,30))\n\n## Plotting heatmap. Generate a mask for the lower triangle (taken from seaborn example gallery)\nmask = np.zeros_like(d_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(d_train.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\nplt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NOT BAD at ALL !\n\nYEAH ! We concluded some first observations that the SALEPRICE seem to be strongly-POSITIVE correlated to:\n\n+ OverallQual\n+ TotalBsmtSF\n+ 1stFlrSF\n+ GrLivArea\n+ GarageCars, and\n+ GarageArea\n\nwhich means that as one variable increases, the SalePrice value also increases. OK, let's stop here to select these variables to analyse, there are probably other variable that should be considered in deep.\n\nThe main issue of the current problem is to be the choice of the right FEATURES and related to the TARGET value and NOT only the definition of complex relationships between them, we will discuss deeper in the next section.","metadata":{}},{"cell_type":"markdown","source":"# (a) SalePrice\n- the property's sale price in dollars. This is the target variable that we are trying to predict.\n\nDistribution plots are very useful to check how well a variable is distributed in the dataset. Let’s now produce a distribution plot using the ‘distplot’ combined with the 'boxplot' function to check the distribution of the ‘SalePrice’ variable in the dataset.","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"ticks\")\nx = d_train['SalePrice']\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)}, figsize=(12,7))\n\nsns.boxplot(x, ax=ax_box)\nsns.distplot(x, ax=ax_hist)\nplt.axvline(x = x.mean(), c = 'red')\nplt.axvline(x = x.median(), c = 'green')\n\nax_box.set(yticks=[])\nsns.despine(ax=ax_hist)\nsns.despine(ax=ax_box, left=True)\nplt.show()\nprint(\"Skewness: %f\" % d_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % d_train['SalePrice'].kurt())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ Red line in histogram indicates the mean of the SalePrice and the Green line indicates the median.\n+ Looking at the kurtosis score, we can see that there is a very nice peak. However, looking at the skewness score, we can see that the SalePrices deviate from the normal distribution. \n+ We want our data to be as \"normal\" as possible. This is just because the Machine Learning DOES  LIKE ONLY the NORMAL DISTRIBUTION.\n+ For conclusion, this is a right skewed distribution or called a positive skew distribution. That’s because the tail is longer on the positive direction of the number line. A histogram is right skewed if the peak of the histogram veers to the left. Therefore, the histogram’s tail has a positive skew to the right.\n\nLet's check a simplest way to correct the distribution of SalePrice by taking logarithm of the value.","metadata":{}},{"cell_type":"code","source":"sns.set(style=\"ticks\")\n\nx = (np.log1p(d_train['SalePrice']))\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (.15, .85)}, figsize=(12,7))\n\nsns.boxplot(x, ax=ax_box)\nsns.distplot(x, ax=ax_hist)\nplt.axvline(x = x.mean(), c = 'red')\nplt.axvline(x = x.median(), c = 'green')\n\nax_box.set(yticks=[])\nsns.despine(ax=ax_hist)\nsns.despine(ax=ax_box, left=True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We \"feel\" MUCH BETTER with the conversion of SalePrice into LOGARITH function, it's NOT PERFECT yet, but this is one of the simplest way to obtain a NORMAL DISTRIBUTION funtion, so we will apply this in the next section do train the model in the modeling section. ","metadata":{}},{"cell_type":"markdown","source":"# (b) OverallQual: \nOverall material and finish quality","metadata":{}},{"cell_type":"code","source":"fig = px.box(d_train, x=\"OverallQual\", y=\"SalePrice\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the OverallQua increases, price of the houses also increase. That makes sense.","metadata":{}},{"cell_type":"code","source":"yprop = 'SalePrice'\nxprop = 'OverallQual'\nh= 'LotArea'\npx.scatter(d_train, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (c) OverallCond: \nOverall condition rating","metadata":{}},{"cell_type":"code","source":"yprop = 'SalePrice'\nxprop = 'LotArea'\nh= 'OverallCond'\npx.scatter(d_train, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_train = d_train.drop(d_train[(d_train['SalePrice']>740000) & (d_train['SalePrice']<756000)].index).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (d) TotalBsmtSF: \nTotal square feet of basement area","metadata":{}},{"cell_type":"markdown","source":"Like heatmap, a scatter plot is also used to observe linear relations between two variables in a dataset. In a scatter plot, the dependent variable is marked on the x-axis and the independent variable is marked on the y-axis. In our case, the ‘SalePrice’ attribute is the dependent variable, and every other are the independent variables.","metadata":{}},{"cell_type":"code","source":"df = px.data.gapminder()\nfig = px.scatter(d_train, y=\"SalePrice\", x=\"LotArea\", size=\"SalePrice\", color=\"TotalBsmtSF\",\n           hover_name=\"LotArea\", log_x=True, log_y=True, size_max=20)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (e) 1stFlrSF: \nFirst Floor square feet","metadata":{}},{"cell_type":"code","source":"df = px.data.iris()\nfig = px.scatter(d_train, x=\"1stFlrSF\", y=\"SalePrice\", color=\"GarageCars\", marginal_y=\"violin\",\n           marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_train = d_train.drop(d_train[(d_train['1stFlrSF']>4690) & (d_train['1stFlrSF']<4700)].index).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (f) GrLivArea: \nAbove grade (ground) living area square feet","metadata":{}},{"cell_type":"code","source":"sns.jointplot(data=d_train, x='GrLivArea', y='SalePrice', kind='reg', height=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As recommended by the author of the data, Outlinear in the GrLivArea should be removed. The author stated that “I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these five unusual observations) before assigning it to students.” It makes sense that people would pay for the more living area. What doesn't make sense is the two datapoints in the bottom-right of the plot. We need to take care of this! What we will do is remove these outliers manually.","metadata":{}},{"cell_type":"code","source":"d_train = d_train.drop(d_train[(d_train['GrLivArea']>4000) & (d_train['SalePrice']<250000)].index).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (g) GarageCars: \nSize of garage in car capacity","metadata":{}},{"cell_type":"code","source":"fig = px.violin(d_train, y=\"SalePrice\", x=\"GarageCars\", color=None, box=True, points=\"all\", hover_data=d_train.columns)\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Suprising! 4-car garages result in less Sale Price? That doesn't make much sense. Let's remove these outliers.","metadata":{}},{"cell_type":"code","source":"d_train = d_train.drop(d_train[(d_train['GarageCars']>3) & (d_train['SalePrice']<290000)].index).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (h) GarageArea: \nSize of garage in square feet","metadata":{}},{"cell_type":"code","source":"fig = px.scatter(d_train, x=\"GarageArea\", y=\"SalePrice\", color=\"OverallCond\", marginal_y=\"violin\",\n           marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again with the top & bottom two data-points. Let's remove these outliers.","metadata":{}},{"cell_type":"code","source":"d_train = d_train.drop(d_train[(d_train['GarageArea']>1240) & (d_train['GarageArea']<1400)].index).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# (k) LotArea: \nLot size in square feet and other variables.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=[15,20])\nfeafures = ['LotArea','MSSubClass','OverallQual','OverallCond','ExterQual','ExterCond','BsmtQual','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual']\nn=1\nfor f in feafures:\n    plt.subplot(6,2,n)\n    sns.boxplot(x=f,y='SalePrice',data = d_train)\n    plt.title(\"Sale Price in function of {}\".format(f))\n    n=n+1\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+ As we can see from all the above representation that many factors are affecting the prices of the house, like square feet which increases the price of the house and even location influencing the prices of the house.\n+ Now that we are familiar with all these representation and can tell our own story let us move and create a model to which would predict the price of the house based upon the other factors. ","metadata":{}},{"cell_type":"markdown","source":"# 4. Statistical (if any)","metadata":{}},{"cell_type":"markdown","source":"In this section, we will check some hypothesis on the influence of independent variables on the target parameter. \n+ Hypothesis is checked at level of signidicant of 5%\n+ Test statistic parameters are calculated using the ttest_ind from scipy.stats Library.","metadata":{}},{"cell_type":"code","source":"from scipy.stats import ttest_ind\n\ndef Series_stats(var, category, prop1, prop2):\n# Step 1: State the null and alternative hypothesis and select a level of significance is 5% or 0.05\n# Step 2: Collect data and calculate the values of test statistic\n    s1 = d_train[(d_train[category]==prop1)][var]\n    s2 = d_train[(d_train[category]==prop2)][var]\n    t, p = ttest_ind(s1,s2,equal_var = False)\n\n    print(\"Two-sample t-test: t={}, p={}\".format(round(t,5),p))\n# Step 3: Compare the probability associated with the test statistic with level of significance specified\n    if ((p < 0.05) and (np.abs(t) > 1.96)):\n        print(\"\\n REJECT the Null Hypothesis and state that: \\n at 5% significance level, the mean {} of {}-{} and {}-{} are not equal.\".format(var, prop1, category, prop2, category))\n        print(\"\\n YES, the {} of {}-{} differ significantly from {}-{} in the current dataset.\".format(var, prop1, category, prop2, category))\n        print(\"\\n The mean value of {} for {}-{} is {} and for {}-{} is {}\".format(var, prop1, category, round(s1.mean(),2), prop2, category, round(s2.mean(),2)))\n    else:\n        print(\"\\n FAIL to Reject the Null Hypothesis and state that: \\n at 5% significance level, the mean {} of {} - {} and {} - {} are equal.\".format(var, prop1, category, prop2, category))\n        print(\"\\n NO, the {} of {}-{} NOT differ significantly from {}-{} in the current dataset\".format(var, prop1, category, prop2, category))\n        print(\"\\n The mean value of {} for {}-{} is {} and for {}-{} is {}\".format(var, prop1, category, round(s1.mean(),2), prop2, category, round(s2.mean(),2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(a) Doe the SalePrice of House that OverallQual of 1 and 2 are equal?","metadata":{}},{"cell_type":"code","source":"Series_stats('SalePrice','OverallQual',1,10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"(b) Does the SalePrice of LotArea (Lot size in square feet) 8450 and 13175 sqf are equal?","metadata":{}},{"cell_type":"code","source":"Series_stats('SalePrice','LotArea',8450,13175)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Street: Type of road access","metadata":{}},{"cell_type":"code","source":"Series_stats('SalePrice','Street','Pave', 'Grvl')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section II: HOUSE PRICE MODEL","metadata":{}},{"cell_type":"markdown","source":"In this Kernel, we do not discuss in deep about the Models' parameters, we just applied the standard or refer to previous recommendations. Let's copy the database.","metadata":{}},{"cell_type":"markdown","source":"# 1. Feature Selection, data handling\n\nAs we said before, in this process we are going to define the ‘X_train’ variable (independent variable) and the ‘y_train’ variable (dependent variable). After defining the variables, we will use them to split the data into a train set and test set. Splitting the data can be done using the ‘train_test_split’ function provided by scikit-learn in Python.","metadata":{}},{"cell_type":"code","source":"d_test.Functional.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One of our most time consuming operations when doing this Kernel is processing the data in order to perform House-price prediction step using the testdataset. There are a lot of null or NaN values or object variables present in the test file that don't appear in the train file and we got errors during LabelEncoding or final prediction step. That is why, we proceed to the next step: checking the difference between train set and test data set.","metadata":{}},{"cell_type":"code","source":"Check = pd.DataFrame(index=None, columns=['Feature','Missing from Test to Train', 'Items'])\ncols=np.array(d_test.columns[d_test.dtypes == object])\nfor fe in cols:\n    listtrain = d_train[fe]\n    listtest = d_test[fe]\n    Check = Check.append(pd.Series({'Feature':fe, 'Missing from Test to Train': len(set(listtest).difference(listtrain)), 'Items':set(listtest).difference(listtrain) }),ignore_index=True )\nCheck","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the first setp, all missing values in the object column are replace with the most common value in the column.","metadata":{}},{"cell_type":"markdown","source":"Now, check to confirm again, if there is any NaN or missing value in the datasets!","metadata":{}},{"cell_type":"code","source":"d_train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_test.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, check again to make sure before going to the next step.","metadata":{}},{"cell_type":"code","source":"Check = pd.DataFrame(index=None, columns=['Feature','Missing from Test to Train', 'Items'])\ncols=np.array(d_test.columns[d_test.dtypes == object])\nfor fe in cols:\n    listtrain = d_train[fe]\n    listtest = d_test[fe]\n    Check = Check.append(pd.Series({'Feature':fe, 'Missing from Test to Train': len(set(listtest).difference(listtrain)), 'Items':set(listtest).difference(listtrain) }),ignore_index=True )\nCheck","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this stage, we decided to select 61 variables and remove all following columns from the model: 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'KitchenAbvGr', '3SsnPorch', 'PoolArea', 'MiscVal'.","metadata":{}},{"cell_type":"code","source":"f_train = ['MSSubClass', 'MSZoning', 'LotArea', 'Street','LotShape', 'LandContour', 'Utilities', 'LotConfig',\n           'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', \n           'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n           'MasVnrType', 'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n           'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', \n           'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath', \n           'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'GarageType',\n           'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', \n           'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition', 'SalePrice']\nf_test = ['MSSubClass', 'MSZoning', 'LotArea', 'Street','LotShape', 'LandContour', 'Utilities', 'LotConfig',\n           'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', \n           'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n           'MasVnrType', 'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n           'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC', \n           'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BsmtFullBath', 'FullBath', 'HalfBath', \n           'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'GarageType',\n           'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond', 'PavedDrive', \n           'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition']\ndf_train = pd.DataFrame(d_train, columns=f_train)\ndf_test = pd.DataFrame(d_test, columns=f_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import norm, skew\n\nnumeric_feats = df_test.dtypes[df_test.dtypes != 'object'].index\nskewed_feats = df_test[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_feats[abs(skewed_feats) > 1]\nhigh_skew","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in high_skew.index:\n    df_train[feature] = np.log1p(df_train[feature])\n    df_test[feature] = np.log1p(df_test[feature])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take a deepcopy on both full datasets, and then map all objects columns by applying the map(str) function.","metadata":{}},{"cell_type":"code","source":"import copy\ntrain=copy.deepcopy(df_train)\ntest=copy.deepcopy(df_test)\n\ncols=np.array(df_train.columns[df_train.dtypes != object])\nfor i in train.columns:\n    if i not in cols:\n        train[i]=train[i].map(str)\n        test[i]=test[i].map(str)\ntrain.drop(columns=cols,inplace=True)\ntest.drop(columns=np.delete(cols,len(cols)-1),inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data Labeling\n\nAs you might know by this setp, we can’t have text in our data if we’re going to run any kind of model on it. So before we can run a model, we need to make this data ready for the model and to convert this kind of categorical text data into model-understandable: \"numerical data\", we use the Label Encoder class.","metadata":{}},{"cell_type":"code","source":"df_train.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\n\n# build dictionary function\ncols = np.array(df_train.columns[df_train.dtypes != object])\nd    = defaultdict(LabelEncoder)\n\n# only for categorical columns apply dictionary by calling fit_transform \ntrain = train.apply(lambda x: d[x.name].fit_transform(x))\ntest  = test.apply(lambda x: d[x.name].transform(x))\ntrain[cols] = df_train[cols]\ntest[np.delete(cols,len(cols)-1)]=df_test[np.delete(cols,len(cols)-1)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let see our final results on data processing results !","metadata":{}},{"cell_type":"code","source":"train.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['YrBltAndRemod']=test['YearBuilt']+test['YearRemodAdd']\ntest['TotalSF']=test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF']\ntest['Total_sqr_footage'] = (test['BsmtFinSF1'] + test['1stFlrSF'] + test['2ndFlrSF'])\ntest['Total_Bathrooms'] = (test['FullBath'] + (0.5 * test['HalfBath']) +test['BsmtFullBath'] )\ntest['Total_porch_sf'] = (test['OpenPorchSF'] + test['EnclosedPorch'] +test['WoodDeckSF'])\n\ntrain['YrBltAndRemod']=train['YearBuilt']+train['YearRemodAdd']\ntrain['TotalSF']=train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\ntrain['Total_sqr_footage'] = (train['BsmtFinSF1']  +train['1stFlrSF'] + train['2ndFlrSF'])\ntrain['Total_Bathrooms'] = (train['FullBath'] + (0.5 * train['HalfBath']) +train['BsmtFullBath'] )\ntrain['Total_porch_sf'] = (train['OpenPorchSF'] + train['EnclosedPorch'] +train['WoodDeckSF'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Spliting: Training and Testing\n\nWe split our dataset into training, testing data with a 90:10 split ratio (As learned from school, this ratio should be 70:30 or 80:20, but we experience here this ratio is better :) ). The splitting was done by picking at random which results in a balance between the training data and testing data amongst the whole dataset. This is done to avoid overfitting and enhance generalization. Finaly, we selected 61 characters in the dataset to train the model.","metadata":{}},{"cell_type":"markdown","source":"But, we have to import Libaries for this section first !","metadata":{}},{"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor,AdaBoostRegressor,BaggingRegressor, RandomForestRegressor\nimport xgboost as xgb\nimport lightgbm as lgb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the errors function, that help to calculate the accuracy of each model.","metadata":{}},{"cell_type":"code","source":"def Errors(model, X_train, y_train, X_test, y_test):\n    ATrS =  model.score(X_train,y_train)\n    ATeS = model.score(X_test,y_test)\n    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n    MSE = mean_squared_error(y_test, y_pred)\n    return ATrS, ATeS, RMSE, MSE","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Models (Regressor)","metadata":{}},{"cell_type":"markdown","source":"We use train data and test data: train data to train our machine and test data to see if it has learnt the data well or not.\n\nAnd DO NOT forget to fixing \"skewed\" features. Here, we fix all of the skewed data to be more normal so that our models will be more accurate when making predictions: HOPELY :)\n\nAnd, we create a DataFrame to store all the calculation results, including model name and errors.","metadata":{}},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(columns=['SalePrice']).values\ny = np.log1p(train[\"SalePrice\"])\nZ = test.values\n\nscaler = preprocessing.StandardScaler().fit(X)\nscaler.transform(X) \nscaler.transform(Z)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.00001, random_state = 12)\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size = 0.25, random_state = 12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Acc = pd.DataFrame(index=None, columns=['model','Root Mean Squared  Error','Accuracy on Traing set','Accuracy on Testing set', 'Mean square error'])\nregressors = [['DecisionTreeRegressor',DecisionTreeRegressor()],\n              ['XGBRegressor', XGBRegressor()],\n              ['CatBoostRegressor', CatBoostRegressor(verbose= False)],\n              ['LGBMRegressor',lgb.LGBMRegressor()],\n              ['GradientBoostingRegressor',GradientBoostingRegressor()],\n              ['ExtraTreesRegressor',ExtraTreesRegressor()]]\n\nfor mod in regressors:\n    name = mod[0]\n    model = mod[1]\n    model.fit(X_train1,y_train1)\n    y_pred = model.predict(X_test1)\n    ATrS, ATeS, RMSE, MSE = Errors(model, X_train1, y_train1, X_test1, y_test1)\n    Acc = Acc.append(pd.Series({'model':name, 'Root Mean Squared  Error': RMSE,'Accuracy on Traing set':ATrS,'Accuracy on Testing set':ATeS, 'Mean square error':MSE}),ignore_index=True )\n    \nAcc.sort_values(by='Mean square error')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So now, we have train data, test data. After fitting our data to different models we can check the score of our data and the prediction is MUCH LOWER than our aim of 90%. So how do we achieve that target?\n\nIn this Kernel, we used a different method, which is very important for weak prediction models such as this. This might seem to be a bit advanced but if understood is a really brilliant tool to enable better predictions.\n\nFor building a prediction model, many experts use Gradient Boosting regression, CatbootRegressor, ... and we will check these models in the next section.\n\nFor illustration purpose, we defined a function to compare the acutal and predicted SalePrice on the same Graphic.","metadata":{}},{"cell_type":"code","source":"def Graph_prediction(n, y_actual, y_predicted):\n    y = np.exp(y_actual)\n    y_total = np.expm1(y_predicted)\n    number = n\n    aa=[x for x in range(number)]\n    plt.figure(figsize=(25,10)) \n    plt.plot(aa, y[:number], '.', label=\"actual\")\n    plt.plot(aa, y_total[:number], 'o', label=\"prediction\")\n    plt.xlabel('SalePrice prediction of first {} Houses'.format(number), size=15)\n    plt.legend(fontsize=15)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Model's Parameters tuning","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter tuning has to with setting the value of parameters that the algorithm cannot learn on its own. As such, these are constants that you set as the researcher. The problem is that you are not any better at knowing where to set these values than the computer. Therefore, the process that is commonly used is to have the algorithm use several combinations  of values until it finds the values that are best for the model. Having said this, there are several hyperparameters we need to tune, and they are as follows.\n\n+ number of estimators: The number of estimators is show many trees to create. The more trees the more likely to overfit. \n+ learning rate: The learning rate is the weight that each tree has on the final prediction.\n+ subsample: Subsample is the proportion of the sample to use.\n+ max depth: Max depth was explained previously.\n\nWhat we will do now is make an instance of the GradientBoostingRegressor. Next, we will create our grid with the various values for the hyperparameters. We will then take this grid and place it inside GridSearchCV function so that we can prepare to run our model.","metadata":{}},{"cell_type":"code","source":"GBR = GradientBoostingRegressor(n_estimators=8000, learning_rate=0.003, max_depth=4, max_features='sqrt', min_samples_leaf=10,\n                                min_samples_split=5, loss='huber', random_state =42)  \n\nGBR.fit(X_train,y_train)\nATrS, ATeS, RMSE, MSE = Errors(GBR, X_train1, y_train1, X_test1, y_test1)\nprint(\"Root Mean Squared: {}, Accuracy Train set: {},Accuracy Test set: {}, Mean square error: {}\".format(RMSE, ATrS, ATeS, MSE))\nresult1 = GBR.predict(Z)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\ngbr = GradientBoostingRegressor()\nparams = {'loss': ['ls','huber'], \n          'learning_rate': [0.01, 0.012, 0.015], \n          'max_depth': [2, 3, 4], \n          'min_samples_leaf' : [9, 10, 12],\n          'min_samples_split' : [2, 3, 4]}\n#gs = GridSearchCV(estimator = gbr, param_grid = params, scoring = 'explained_variance', cv = 10, n_jobs = -1)\n#gs.fit(X_train,y_train)\n#print(\"Best Score:\", gs.best_score_)\n#print(\"Best Parameters :\",gs.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostRegressor\nimport numpy as np\n\ntrain_data = X_train\ntrain_labels = y_train\n\nmodel = CatBoostRegressor()\n\ngrid = {'iterations': [7000, 8000],'learning_rate': [0.001, 0.0045, 0.01, 0.1],\n        'depth': [2, 3, 4],'l2_leaf_reg': [1, 2],'random_seed': [12]}\n\n#grid_search_result = model.grid_search(grid, X=train_data, y=train_labels, plot=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {'iterations': 12000,'learning_rate': 0.008,'depth': 6,'l2_leaf_reg': 2,'eval_metric':'RMSE',\n          'verbose': False,'random_seed': 12}\n         \nCBR = CatBoostRegressor(**params)\nCBR.fit(X_train,y_train)\n\nATrS, ATeS, RMSE, MSE = Errors(CBR, X_train1, y_train1, X_test1, y_test1)\nprint(\"Root Mean Squared: {}, Accuracy Train set: {},Accuracy Test set: {}, Mean square error: {}\".format(RMSE, ATrS, ATeS, MSE))\nresult2 = CBR.predict(Z)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test)\n\ndef xgb_evaluate(learning_rate, max_depth, gamma, subsample, colsample_bytree, reg_alpha):\n    params = {'learning_rate':learning_rate,\n              'max_depth': int(max_depth),\n              'gamma': gamma,\n              'subsample':subsample,\n              'colsample_bytree': colsample_bytree,\n              'reg_alpha':reg_alpha}\n    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n    \n    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\nmodel = xgb.XGBRegressor()\noptimizer = BayesianOptimization(xgb_evaluate, {'learning_rate':(0.005, 0.03),\n                                                'max_depth':(2, 4),\n                                                'gamma':(0., 0.3),\n                                                'subsample':(0.5,1),\n                                                'colsample_bytree':(0.3,0.8),\n                                                'reg_alpha':(0.005, 0.02)})\n# Use the expected improvement acquisition function to handle negative numbers\n# Optimally needs quite a few more initiation points and number of iterations\n#optimizer.maximize(init_points=5, n_iter=15)\n#optimizer.max","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"XGBR = xgb.XGBRegressor(colsample_bytree=0.46, gamma=0.047, learning_rate=0.05, max_depth=4, min_child_weight=1.8, \n                 n_estimators=5000,reg_alpha=0.46, reg_lambda=0.85,subsample=0.52, random_state = 7, nthread = -1)\n\nXGBR.fit(X_train,y_train)\n    \nATrS, ATeS, RMSE, MSE = Errors(XGBR, X_train1, y_train1, X_test1, y_test1)\nprint(\"Root Mean Squared: {}, Accuracy Train set: {},Accuracy Test set: {}, Mean square error: {}\".format(RMSE, ATrS, ATeS, MSE))\nresult3 = XGBR.predict(Z)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LGBMR = lgb.LGBMRegressor(objective='regression', num_leaves=5,learning_rate=0.01, n_estimators=4000,max_bin=200, \n                         bagging_fraction=0.8,bagging_freq=4, bagging_seed=8,feature_fraction=0.2,feature_fraction_seed=10,\n                         min_sum_hessian_in_leaf = 15,verbose=-1,random_state=12)\nLGBMR.fit(X_train,y_train)\n    \nATrS, ATeS, RMSE, MSE = Errors(LGBMR, X_train1, y_train1, X_test1, y_test1)\nprint(\"Root Mean Squared: {}, Accuracy Train set: {},Accuracy Test set: {}, Mean square error: {}\".format(RMSE, ATrS, ATeS, MSE))\nresult4 = LGBMR.predict(Z)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Submission","metadata":{}},{"cell_type":"code","source":"Graph_prediction(300, y_train, GBR.predict(X_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = np.expm1((result1 + result2 + result3 + result4)/4)\nsub = pd.DataFrame()\nsub = pd.DataFrame({'Id':d_test.Id,'SalePrice':result}) \nsub.to_csv('submission.csv',index=False)\nsub.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Conclusion\n\nThis Kernel investigates different models for housing price prediction. Different types of Machine Learning methods including CatBoostRegressor, GradientBoostingRegressor and LightGBM and two techniques in machine learning are compared and analyzed for optimal solutions. Eventhough all of those methods achieved desirable results, different models have their own pros and cons. \n\nThe GradientBoostingRegressor is probably the best one and has been selected for this problem. The BayesianOptimization method is simple but performsa lot better than the three other availabel methods due to the generalization.\n\nFinally, the CatBoostRegressor is the best choice when parametrerization is the top priority.","metadata":{}}]}