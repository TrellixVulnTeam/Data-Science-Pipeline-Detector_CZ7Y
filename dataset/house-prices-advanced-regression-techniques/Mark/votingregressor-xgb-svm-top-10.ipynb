{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Notebook have following structure:\n1. Exploratory data analysis\n1. Data cleaning\n1. Feature engineering\n1. Train part\n1. Hyperoptimization\n1. Test part","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.options.display.max_rows = None\npd.options.display.max_columns = None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory data analysis","metadata":{}},{"cell_type":"markdown","source":"Let's load train (and later test) data and have a look on it. In train dataset exist target variable 'SalePrice', in test dataset - no.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntrain_df.shape","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe().transpose()","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.hist(figsize=(20, 20), bins=20);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(26, 16))\nsns.heatmap(train_df.corr(), cmap='rocket', annot=True, fmt=f'0.1', cbar=False);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look  little bit closer to our target feature.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nsns.distplot(train_df['SalePrice']);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data is right-skewed, let's see if log of price can handle with outliers.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 4))\nsns.distplot(np.log(train_df['SalePrice']));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yes, logarithmic 'SalePrice' looks better due to normal distribution and I will use LogPrice as target variable.","metadata":{}},{"cell_type":"code","source":"train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add price logarithm to dataset\ntrain_df['LogPrice'] = np.log(train_df['SalePrice'])\n\n# and remove SalePrice \ntrain_df = train_df.drop('SalePrice', axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation target feature with others features\ntrain_df.corr()['LogPrice'].sort_values(ascending=False)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see on some features with strong correlation.","metadata":{}},{"cell_type":"code","source":"sns.barplot(x='OverallQual', y='LogPrice', data=train_df);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x='GrLivArea', y='LogPrice', data=train_df);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x='GarageArea', y='LogPrice', data=train_df);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x='TotalBsmtSF', y='LogPrice', data=train_df);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x='LotFrontage', y='LogPrice', data=train_df);","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x='LotArea', y='LogPrice', data=train_df);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data cleaning","metadata":{}},{"cell_type":"markdown","source":"So, I'm going to concatenate train and test data in order to avoid duplicating code when I will be cleaning data. And later I will saparate it before training part.","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save train and test ID for final prediction on test part\ntest_id = test_df.pop('Id')\ntrain_id = train_df.pop('Id')\n\n# Save train length \nn_train = train_df.shape[0]\n\n# Set target variable and drop it from dataset\nlabels = train_df.pop('LogPrice')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatenate train and test part\ndf = pd.concat([train_df, test_df], axis=0)\ndf.reset_index(inplace=True, drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape, train_df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check empty values\npd.DataFrame({'Amount': df.isnull().sum(),\n             'Percent': (df.isnull().sum() / len(df)) *100}).sort_values(by='Percent', ascending=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In PoolQC, MiscFeature, Alley, Fence most of data is missing. I'm goind to drop all these columns.","metadata":{}},{"cell_type":"code","source":"df = df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FireplaceQu\ndf['Fireplaces'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['FireplaceQu'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems empty FireplaceQu in houses without Fireplace at all. I fill it with NA.","metadata":{}},{"cell_type":"code","source":"df['FireplaceQu'].fillna('NA', inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LotFrontage \nsns.distplot(df['LotFrontage'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For 'LotFrontage' we have 486 empty records, there are too many to delete. And we can see that there are no values  equal to 0. So we can try to fill these empty values with median (because we have some outliers).","metadata":{}},{"cell_type":"code","source":"lot_frontage_median = df['LotFrontage'].median()\ndf['LotFrontage'] = df['LotFrontage'].fillna(lot_frontage_median)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Garages' features\ndf[df['GarageYrBlt'].isnull()].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nWhere 'GarageYrBlt' equals 'NaN', there other empty 'garage' values is empty, as well. Follow desciription data it means no garage.\n","metadata":{}},{"cell_type":"code","source":"# Numerical features replace with number\ndf['GarageYrBlt'] = df['GarageYrBlt'].fillna(0) \n\n# Features replace with 'NA'\nfor column in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    df[column] = df[column].fillna('NA')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bsmts' features\ndf[df['BsmtExposure'].isnull()].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same situation as above: where 'BsmtExposure' is 'null', there other 'Bsmt' features are 'null', as well.","metadata":{}},{"cell_type":"code","source":"for column in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df[column] = df[column].fillna('NA')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MasVnrType\ndf[df['MasVnrType'].isnull()].head() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same here. Where 'MasVnrType' is null, there MasVnrArea - 0","metadata":{}},{"cell_type":"code","source":"df['MasVnrType'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['MasVnrType'] = df['MasVnrType'].fillna('None')\ndf['MasVnrArea'] = df['MasVnrArea'].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rest columns have a few empty values, but as I concatenate train and test part I can't delete it. <br>\nI'm going to fill rest values with 0, 'None' or with most occurred value.","metadata":{}},{"cell_type":"code","source":"# MSZoning\ndf['MSZoning'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['MSZoning'] = df['MSZoning'].fillna(df['MSZoning'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BsmtBaths\ndf['BsmtHalfBath'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['BsmtFullBath'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['BsmtFullBath'] = df['BsmtFullBath'].fillna(0)\ndf['BsmtHalfBath'] = df['BsmtHalfBath'].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functional\ndf['Functional'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Functional'] = df['Functional'].fillna(df['Functional'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Electrical\ndf['Electrical'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utilities\ndf['Utilities'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Utilities'] = df['Utilities'].fillna(df['Utilities'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TotalBsmtSF\ndf[df['TotalBsmtSF'].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TotalBsmtSf has strong correlation with 1stFlrSf","metadata":{}},{"cell_type":"code","source":"df['TotalBsmtSF'] = df['TotalBsmtSF'].fillna(df['1stFlrSF'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BsmtUnfSf, BsmtFinSF2, BsmtFinSF1\ndf['BsmtUnfSF'] = df['BsmtUnfSF'].fillna(0)\ndf['BsmtFinSF2'] = df['BsmtFinSF2'].fillna(0)\ndf['BsmtFinSF1'] = df['BsmtFinSF1'].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Garage\ndf[df['GarageCars'].isnull()]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['GarageCars'] = df['GarageCars'].fillna(0)\ndf['GarageArea'] = df['GarageArea'].fillna(0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rest empty categorical values fill with mode\ndf['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\ndf['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])\ndf['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])\ndf['KitchenQual'] = df['KitchenQual'].fillna(df['KitchenQual'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({'Amount': df.isnull().sum(),\n              'Percent': (df.isnull().sum() / len(df)) *100}).sort_values(by='Percent', ascending=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature engineering","metadata":{}},{"cell_type":"markdown","source":"\n\nSo, now is no more empty values. Let's transform categorical data to numbers.\nI will use 3 methods:\n\n*     Label encoder \n*     One hot encoding\n*     And for features with clear scale I will map these features\n\nGenerally we have 3 main feature types with clear scale, I will separate these features depends which scale their have.","metadata":{}},{"cell_type":"code","source":"qual_columns = ['GarageCond', 'GarageQual', 'FireplaceQu', 'KitchenQual', 'HeatingQC', \n           'BsmtCond', 'BsmtQual', 'ExterCond', 'ExterQual'] \nbsmt_columns = ['BsmtFinType2', 'BsmtFinType1'] \nexposure_columns = ['BsmtExposure']\n\nqual_rates = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\nbsmtype_rates = {'GLQ': 5, 'ALQ': 4, 'BLQ': 3, 'Rec': 2, 'LwQ': 1, 'Unf': -1, 'NA': 0}\nexposure_rates = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': -1, 'NA': 0}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Map features with clear scale\nfor feats, rate in ((qual_columns, qual_rates),  (bsmt_columns, bsmtype_rates), (exposure_columns, exposure_rates)):\n    for feat in feats:\n        df[feat] = df[feat].map(rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's encode rest of categorical features with LabelEncoder and OneHotEncoding. I will use pandas function **factorize** and **get_dummies**, it gives the same result as LebelEncoder and OneHotEncoder from sklearn.","metadata":{}},{"cell_type":"code","source":"# LabelEncoder \nencode = ['Functional', 'CentralAir', 'PavedDrive', 'GarageFinish', 'Street', 'LandSlope']\n\nfor feat in encode:\n    df['{0}_cat'.format(feat)] = pd.factorize(df[feat])[0]\n\n# OneHotEncoding\ncategorical_features = [x for x in df.select_dtypes(include=np.object).columns if x not in encode]\n\nfor feat in categorical_features:\n    dummies = pd.get_dummies(df[feat], prefix='{0}'.format(feat), drop_first=True)\n    df = pd.concat([df, dummies], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's add some new features.","metadata":{}},{"cell_type":"code","source":"df['BsmtFin'] = df['BsmtFinSF1'] + df['BsmtFinSF2'] \ndf['TotalBsmt'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will separate data on train and test datasets and I will store test data for final prediction. <br>","metadata":{}},{"cell_type":"code","source":"train_set = df[:n_train]\ntest_set = df[n_train:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Machine learing part","metadata":{}},{"cell_type":"code","source":"# import necessary libraries\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import ExtraTreesRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will use all numerical features for training models  (labels is LogPrice). I will try a few models and pick the bests. ","metadata":{}},{"cell_type":"code","source":"# Set X and y\nX = train_set[train_set.select_dtypes(include=np.number).columns].values\n\n# Normalise features\nscalar = MinMaxScaler()\nX_scaled = scalar.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create list of models\nlasso_model = Lasso()\nelastic_model = ElasticNet()\nsvr_model = SVR()\ntree_model = ExtraTreesRegressor()\nxgb_model = XGBRegressor()\nknn_model = KNeighborsRegressor()\n\nmodels = {'lasso_model': lasso_model,\n         'elastic_model': elastic_model,\n         'svr_model': svr_model,\n         'tree_model': tree_model,\n         'xgb_model': xgb_model,\n         'knn_model': knn_model}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_validation(model, X, y):\n    \"Check model with cross validation\"\n    score = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n    cross_score = np.sqrt(-score)\n    return round(np.mean(cross_score), 4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check models with cross validation\nmodels_evaluation = {}\nfor model_name, model in models.items():\n    models_evaluation[model_name] = cross_validation(model, X_scaled, labels)\n    \npd.DataFrame(data=models_evaluation.items(), columns=['Model', 'RMSE']).sort_values(by='RMSE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm going to unite SVM and XGB and train it with VotingRegressor. But first I will improve model's parameters.","metadata":{}},{"cell_type":"markdown","source":"## Hyperoptimization","metadata":{}},{"cell_type":"markdown","source":"So let's try to achieve a little bit more. I'm going to improve model using: <br>\n\n* Features importances (leave only significant features).\n* Search better parameters for models ","metadata":{}},{"cell_type":"code","source":"xgb_model.fit(X_scaled, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get features importances\nfeatures_list = sorted(zip(xgb_model.feature_importances_, train_set.select_dtypes(include=np.number).columns), reverse=True)\nfeatures_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Leave only useful features\nimp_feats = [feat for (n, feat) in features_list if n > 0.001]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set X with new feature set\nX = train_set[imp_feats].values\nX_scaled = scalar.fit_transform(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_validation(xgb_model, X_scaled, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Search better parameters for xgb_model \nparam_grid = {'n_estimators': np.arange(100, 1500),\n             'learning_rate': np.arange(0.01, 1, 0.01),\n             'max_depth': np.arange(1, 20),\n             'colsample_bytree': np.arange(0, 1, 0.1)}\n\nrandom_search = RandomizedSearchCV(xgb_model, param_grid, cv=10, scoring='neg_mean_squared_error', n_iter=100)\nrandom_search.fit(X_scaled, labels)\n\nbest_xgb = random_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_validation(best_xgb, X_scaled, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Search better parameters for svm model \nsvr_params = {'C': np.arange(1, 30),\n             'kernel': ('linear', 'poly', 'rbf', 'sigmoid')}\n\nhyperopt_svr = RandomizedSearchCV(svr_model, svr_params, cv=10, scoring='neg_mean_squared_error', n_iter=100)\nhyperopt_svr.fit(X_scaled, labels)\n\nbest_svr = hyperopt_svr.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_validation(best_svr, X_scaled, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, these steps improved our models. Now let's combine better models to one and train our final model.","metadata":{}},{"cell_type":"code","source":"# Ensemble better models\nvoting_reg = VotingRegressor(estimators=[('xgb', best_xgb), ('svr', best_svr)])\n\ncross_validation(voting_reg, X_scaled, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Final model training \nvoting_reg.fit(X_scaled, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test part","metadata":{}},{"cell_type":"code","source":"# Get X for test part\nX_test = test_set[imp_feats].values\nX_test_scaled = scalar.transform(X_test)\n\n# Make prediction\ny_pred = voting_reg.predict(X_test_scaled)\n\n# Convert LopPrice to normal and save it to csv in order to upload on Kaggle\ntest_file = pd.DataFrame({'Id': test_id, 'SalePrice': np.exp(y_pred)})\ntest_file.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, I achieved 0.12089 score for test set in Kaggle competition (Kaggle use RMSLE metric).","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}