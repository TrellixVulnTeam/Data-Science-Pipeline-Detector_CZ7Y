{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"markdown","source":"# Dimensionality Reduction\n* Dimensionality reduction is the process of reducing the available features. \n* Model could not be applied on entire set of features directly which may lead to spurious predictions and generalization issues.\n* In order to prevent these issues dimensionality reduction is applied.\n\n## Need for dimensionality reduction\nDimensionality reduction prevents overfitting. \n* Overfitting is when the model memorizes the data and fails to generalize. \n* Overfitted model could not be applied to the real world problems due to its generalization problem.\n\n## Types of Dimensionality Reduction\n* **Feature Selection**: Feature selection methods attempts to reduce the features by discarding the least important features.\n* **Feature Extraction**: Feature extraction methods attempts to reduce the features by combining the features and transforming it to the specified number of features.\n\n## Feature Selection\n1. Filter methods\n2. Wrapper methods\n3. Embedded methods\n4. Feature Importance\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import the required libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.feature_selection import SelectKBest, SelectFromModel, f_regression, chi2\nfrom sklearn.feature_selection import RFE, RFECV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost\n\npd.set_option('max.rows',500)\npd.set_option('max.columns',80)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Segregate the numeric and categoric columns'''\nnumeric_cols = ['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual',\n       'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n       'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'MoSold', 'YrSold', 'SalePrice']\n\ncategoric_cols = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load the preprocessed data\nThe training data has been preprocessed already. The preprocessing steps involved are,\n1. MICE Imputation\n2. Log transformation\n3. Square root transformation\n4. Ordinal Encoding\n5. Target Encoding\n6. Z-Score Normalization\n\nFor detailed implementation of the above mentioned steps refer my notebook on data preprocessing: \n\n[Notebook Link](https://www.kaggle.com/srivignesh/data-preprocessing-for-house-price-prediction) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessed_train = pd.read_csv('../input/preprocessed-train-data/preprocessed_train_data.csv')\nx_train, y_train = preprocessed_train[preprocessed_train.columns[:-1]], preprocessed_train[preprocessed_train.columns[-1]]\npreprocessed_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Segregate numerical and categorical features'''\ntrain_numeric = preprocessed_train[numeric_cols[:-1]]\nx_train_numeric, y_train = train_numeric, preprocessed_train[numeric_cols[-1]]\ntrain_categoric = preprocessed_train[categoric_cols]\nx_train_categoric, y_train = train_categoric, preprocessed_train[numeric_cols[-1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n# 1. Filter Methods\n\nFilter methods select the features independent of the model used.\nIt can use the following methods to select the useful set of features,\n* Correlation for numeric columns\n* Chi2 association for categoric columns\n\n## Select K Best in sklearn\n\n**F_Regression:**\n\nF_Regression is used for numeric columns. F_Regression consists of 2 steps:\n\n* Correlation is computed using each feature with the target.\n* The correlation is then converted to an F score then to a p-value.\n\n$Correlation = \\frac{\\sum (x_{i} - x) (y_{i} - y)}{\\sigma _{x} \\sigma _{y}} $\n\n**Chi2:**\n* Chi2 is used for testing the association between categorical columns.\n\n${\\chi}^2 = \\frac{\\sum \\limits_{i=1}^{c} \\sum \\limits_{j=1}^{r} (o_{ij} - e_{ij})^2 }{e_{ij}}$\n\n$o_{ij}$ = Observed frequency\n\n$e_{ij}$ = expected frequency","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Use f_regression for numeric columns'''\nskb_numeric = SelectKBest(score_func = f_regression, k= 30)\nskb_numeric.fit(x_train_numeric, y_train)\n'''Get Support (Boolean array) for the columns from the instance'''\ncolumns_selected_skb_numeric = x_train_numeric.columns[skb_numeric.get_support()]\nx_train_skb_numeric = pd.DataFrame(skb_numeric.transform(x_train_numeric), columns = columns_selected_skb_numeric, index = x_train_numeric.index )\nx_train_skb_numeric.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Chi2 test doesn't support negative values so square the dataset. Negative values are present in the dataset due to Z-Score normalization'''\nx_train_categoric_sqr = x_train_categoric ** 2\n'''Use chi2 for categoric columns'''\nskb_categoric = SelectKBest(score_func = chi2, k= 30)\nskb_categoric.fit(x_train_categoric_sqr, y_train)\n'''Get Support (Boolean array) for the columns from the instance'''\ncolumns_selected_skb_categoric = x_train_categoric_sqr.columns[skb_categoric.get_support()]\nx_train_skb_categoric = pd.DataFrame(skb_categoric.transform(x_train_categoric), columns = columns_selected_skb_categoric, index =x_train_categoric.index )\nx_train_skb_categoric.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Concatenate the selected features of numeric and categoric columns'''\nx_train_skb = pd.concat([x_train_skb_numeric ,x_train_skb_categoric], axis =1)\nx_train_skb.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Wrapper methods\n\nWrapper methods make use of an estimator to select the useful set of features. The techniques available are,\n* Recursive Feature Elimination\n* Recursive Feature Elimination Cross Validation\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Recursive Feature Elimination (RFE)\n\n* The estimator that is provided to RFE assigns weights to features (e.g., the coefficients), RFE recursively eliminates subset of features which have low weights assigned to it.\n* The estimator is trained with the initial set of features. The estimator might have attributes such as coef_ or feature_importances_. With that attribute of the estimator we find the weights of each feature.\n* The least weighted features are removed from the current set of features. This procedure is repeated on the removed set until the specified number of features to select is finally reached.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = xgboost.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\nrfe = RFE(estimator= xgb_model, n_features_to_select = 25)\nrfe.fit(x_train,y_train)\n'''Select the columns that are already selected by RFE'''\ncolumns_selected_rfe = x_train.columns[rfe.support_]\nx_train_rfe = pd.DataFrame(rfe.transform(x_train), columns = columns_selected_rfe, index = x_train.index)\nx_train_rfe.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Recursive Feature Elimination Cross Validation (RFECV)\n* RFECV is very similar to RFE but it uses Cross Validation at each training phase and finally outputs optimal number of columns to select.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfecv = RFECV(estimator=xgb_model)\nrfecv.fit(x_train, y_train)\n'''Select the columns that are already selected by RFECV'''\ncolumns_selected_rfecv = x_train.columns[rfecv.support_]\nx_train_rfecv = pd.DataFrame(rfecv.transform(x_train), columns = columns_selected_rfecv, index = x_train.index)\nx_train_rfecv.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Embedded Methods\nEmbedded methods select features during the training process itself. \n* The coefficients of features become zero when the importance of that feature is low and therefore that feature is not utilized to make predictions.\n \n## LASSO regression\nLASSO stands for **Least Absolute Shrinkage and Selection Operator**\n\n![](https://www.statisticshowto.com/wp-content/uploads/2015/09/lasso-regression.png)\n\n$\\lambda$ = Penalty (Tuning Parameter)\n\nWhen $\\lambda$ = 0 no parameters are eliminated and when $\\lambda$ = 1 it is equal to linear regression.\n\n* The parameter estimates are found by minimizing this cost function.\n* When the coefficient estimates are less than $\\lambda / 2$ the coefficients become zero.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(alpha = 0.3)\n'''fit a LASSO model'''\nlasso.fit(x_train, y_train)\n'''To only select based on max_features, set threshold=-np.inf. Set prefit = True if the model is already fitted to the dataset.'''\nsfm_lasso = SelectFromModel(estimator=lasso, prefit= True, max_features=65, threshold=-np.inf)\nlasso_selected_columns = x_train.columns[sfm_lasso.get_support()]\nx_train_lasso = pd.DataFrame(sfm_lasso.transform(x_train), columns = lasso_selected_columns, index = x_train.index)\nx_train_lasso.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Feature Importance\nThe feature importances is calculated after fitting the model to the entire set of features which assigns weights to each of the features. \n* The model might have attributes such as coef_ or feature_importances_ which help to select the subset of features. Using this the least important features are pruned.\n\n## Select From Model in sklearn","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''Any regressor can be used. Here Decision Tree is used'''\ndec_tree_model = DecisionTreeRegressor()\ndec_tree_model.fit(x_train, y_train)\n\n'''To only select based on max_features, set threshold=-np.inf. Set prefit = True if the model is already fitted to the dataset.'''\nsfm = SelectFromModel(estimator=dec_tree_model, prefit= True, max_features=65, threshold=-np.inf)\n\n'''Selected columns'''\ncolumns_selected_sfm = preprocessed_train.columns[:-1][sfm.get_support()]\nx_train_sfm = pd.DataFrame(sfm.transform(x_train), columns = columns_selected_sfm, index = x_train.index)\nx_train_sfm.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}