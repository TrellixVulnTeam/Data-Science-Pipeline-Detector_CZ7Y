{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>House prices prediction</h1>\n\n<img src=\"https://olegleyz.github.io/images/header.jpg\" alt=\"Header\" width=\"800\"/><br>\n\nThis kernel is going to describe the method by which I predicted the selling prices of houses. <br>\nThis method will include the following steps:\n<ul>\n<li>Observation</li>\n<li>Dealing with missing values</li>\n<li>Fixing skewness</li>\n<li>Adding new features</li>\n<li>Modeling</li>\n</ul>"},{"metadata":{},"cell_type":"markdown","source":"<h3>Importing necessary libraries and reading datasets</h3>"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom scipy.stats import norm, skew\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n\ntarget_name = 'SalePrice'\ndataset_train_raw = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndataset_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\ndataset_train_raw","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test data should contain the same type of data of the training set to preprocess them in the same way. The easiest way to solve this problem is to concatenate train and test datasets, preprocess, and then divide them again. It would also be a good idea to drop out the features that definitely do not affect the price, in our case - 'Id'."},{"metadata":{"trusted":true},"cell_type":"code","source":"ignore_feature = ['Id']\ny_train = dataset_train_raw[target_name]\ndataset_train = dataset_train_raw.drop([target_name] + ignore_feature, axis=1, inplace=False)\ndataset_test.drop(ignore_feature, axis=1, inplace=True)\n\nall_data = pd.concat([dataset_train, dataset_test], axis=0, sort=False)\nall_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Observation</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_train = dataset_train_raw.corr()\nsb.set(font_scale=0.5)\nplt.figure(figsize=(15, 10))\nax = sb.heatmap(correlation_train, annot=True, annot_kws={'size': 10}, fmt='.1f', cmap='PiYG', linewidths=.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the multicollinearity still exists in various features. However, we will keep them for now for the sake of learning and let the regularization models do the clean up later on."},{"metadata":{},"cell_type":"markdown","source":"<h3>Dealing with missing values</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nsb.heatmap(all_data.isnull(), yticklabels=False, cbar=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At first it may seem that there are a lot of missing values, but in fact, some of them were omitted on purpose, and I made a separate category for them."},{"metadata":{"trusted":true},"cell_type":"code","source":"specially_missed = ['Alley',\n                    'PoolQC',\n                    'MiscFeature',\n                    'Fence',\n                    'FireplaceQu',\n                    'GarageType',\n                    'GarageFinish',\n                    'GarageQual',\n                    'GarageCond',\n                    'BsmtQual',\n                    'BsmtCond',\n                    'BsmtExposure',\n                    'BsmtFinType1',\n                    'BsmtFinType2',\n                    'MasVnrType']\n\nfor feature in specially_missed:\n    all_data[feature] = all_data[feature].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If the missing feature is numerical, then fill it with zero, or it would also be a good idea to fill it with the median of the feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_missed = ['BsmtFinSF1',\n                  'BsmtFinSF2',\n                  'BsmtUnfSF',\n                  'TotalBsmtSF',\n                  'BsmtFullBath',\n                  'BsmtHalfBath',\n                  'GarageYrBlt',\n                  'GarageArea',\n                  'GarageCars',\n                  'MasVnrArea']\n\nfor feature in numeric_missed:\n    all_data[feature] = all_data[feature].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's change important numerical characteristics that take a small number of variants of values (for example, year) into categorical ones by casting them to strings."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fill in the remaining missing values with the values that are most common for this feature. "},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['Functional'] = all_data['Functional'].fillna('Typ')\nall_data['Utilities'] = all_data['Utilities'].fillna('AllPub')\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna('TA')\nall_data['Electrical'] = all_data['Electrical'].fillna('SBrkr')\n\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\nall_data['MSZoning'] = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\nsb.heatmap(all_data.isnull(), yticklabels=False, cbar=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No more missing values!"},{"metadata":{},"cell_type":"markdown","source":"<h3>Fixing skewness</h3>"},{"metadata":{},"cell_type":"markdown","source":"Let's create a histogram to see if the target variable (SalePrice) is normally distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(14, 9))\nsb.distplot(y_train, kde=True, hist=True, fit=norm)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, target variable is not normally distributed, let's try to fix it by using log."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = np.log1p(y_train)\n\nplt.subplots(figsize=(14, 9))\nsb.distplot(y_train, kde=True, hist=True, fit=norm)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's fix the high skewness in the rest of the values "},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_feats[abs(skewed_feats) > 0.5]\nhigh_skew","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in high_skew.index:\n    all_data[feature] = np.log1p(all_data[feature])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Adding new features</h3>"},{"metadata":{},"cell_type":"markdown","source":"Just adding new more significant features based on old minor features "},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\nall_data['SqFtPerRoom'] = all_data['GrLivArea'] / (all_data['TotRmsAbvGrd'] + all_data['FullBath'] +\n                                                       all_data['HalfBath'] + all_data['KitchenAbvGr'])\n\nall_data['TotalHomeQuality'] = all_data['OverallQual'] + all_data['OverallCond']\n\nall_data['TotalBathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) +\n                                  all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Processed dataset</h3>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_all = pd.get_dummies(all_data)\nX_train = X_all[:len(y_train)]\nX_test = X_all[len(y_train):]\n\nX_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h3>Modeling</h3>"},{"metadata":{},"cell_type":"markdown","source":"To begin with, let's conduct a superficial analysis of the models in order to weed out the inappropriate in this task. "},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xg\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression, ElasticNet, Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\nfrom mlxtend.regressor import StackingRegressor\n\n\nkf = KFold(n_splits=8, random_state=42, shuffle=True)\n\ndef cv_rmse(model):\n    return -cross_val_score(model, X_train, y_train, scoring='neg_root_mean_squared_error', cv=kf)\n\nmodels = ['Linear', 'SVR', 'Random_Forest', 'XGBR', 'Cat_Boost', 'Ridge', 'Elastic_Net', 'Lasso', 'Stack']\nscores = []\n\nlin = LinearRegression()\nscore_lin = cv_rmse(lin)\nscores.append(score_lin.mean())\n\nsvr = SVR()\nscore_svr = cv_rmse(svr)\nscores.append(score_svr.mean())\n\nrfr = RandomForestRegressor()\nscore_rfr = cv_rmse(rfr)\nscores.append(score_rfr.mean())\n\nxgb = xg.XGBRegressor()\nscore_xgb = cv_rmse(xgb)\nscores.append(score_xgb.mean())\n\ncatb = CatBoostRegressor(verbose=0, allow_writing_files=False)\nscore_catb = cv_rmse(catb)\nscores.append(score_catb.mean())\n\nrid = Ridge()\nscore_rid = cv_rmse(rid)\nscores.append(score_rid.mean())\n\nel = ElasticNet()\nscore_el = cv_rmse(el)\nscores.append(score_el.mean())\n\nlas = Lasso()\nscore_las = cv_rmse(las)\nscores.append(score_las.mean())\n\nstack_gen = StackingRegressor(regressors=(CatBoostRegressor(verbose=0, allow_writing_files=False),\n                                          Ridge(),\n                                          xg.XGBRegressor(),\n                                          RandomForestRegressor()),\n                              meta_regressor=CatBoostRegressor(verbose=0, allow_writing_files=False),\n                              use_features_in_secondary=True)\nscore_stack_gen = cv_rmse(stack_gen)\nscores.append(score_stack_gen.mean())\n\ncv_score = pd.DataFrame(models, columns=['Regressors'])\ncv_score['RMSE_mean'] = scores\ncv_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 11))\nsb.barplot(cv_score['Regressors'], cv_score['RMSE_mean'])\nplt.xlabel('Regressors', fontsize=16)\nplt.ylabel('CV_Mean_RMSE', fontsize=16)\nplt.xticks(rotation=45, fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's move on to optimizing the hyperparameters of the models that showed the best results. "},{"metadata":{},"cell_type":"markdown","source":"<h4>XGBR</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = {}\n\ndef xgbr(X_train, y_train, X_test):\n    xgbrM = xg.XGBRegressor()\n    params = {'max_depth': [3, 4, 5, 6, 7, 8],\n              'min_child_weight': [0, 4, 5, 6, 7, 8],\n              'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.25, 0.8, 1],\n              'n_estimators': [10, 30, 50, 100, 200, 400, 1000]}\n\n    grid_search_xg = RandomizedSearchCV(estimator=xgbrM, scoring='neg_root_mean_squared_error', param_distributions=params, n_iter=200, cv=4, verbose=2,\n                                         random_state=42, n_jobs=-1)\n    grid_search_xg.fit(X_train, y_train)\n    xgbrModel = grid_search_xg.best_estimator_\n    print('Best params(XGBR):',grid_search_xg.best_params_)\n    print('RMSE(XGBR):', -grid_search_xg.best_score_)\n    return xgbrModel\n\n#xgbrModel = xgbr(X_train, y_train, X_test)\nxgbrModel = xg.XGBRegressor(n_estimators=400, min_child_weight=5, max_depth=7, learning_rate=0.05)\nxgbrModel.fit(X_train, y_train)\npredictions['XGBR'] = xgbrModel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>Ridge</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def ridge(X_train, y_train, X_test):\n    alpha_ridge = {'alpha': [-3, -2, -1, 1e-15, 1e-10, 1e-8, 1e-5, 1e-4, 1e-3, 1e-2, 0.5, 1, 1.5, 2, 3, 4, 5, 10, 20, 30, 40]}\n\n    rd = Ridge()\n    grid_search_rd = GridSearchCV(estimator=rd, scoring='neg_root_mean_squared_error', param_grid=alpha_ridge, cv=4, n_jobs=-1, verbose=3)\n    grid_search_rd.fit(X_train, y_train)\n    ridgeModel = grid_search_rd.best_estimator_\n    print('Best params(Ridge):', grid_search_rd.best_params_)\n    print('RMSE(Ridge):', -grid_search_rd.best_score_)\n    return ridgeModel\n\n#ridgeModel = ridge(X_train, y_train, X_test)\nridgeModel = Ridge(alpha=10)\nridgeModel.fit(X_train, y_train)\npredictions['Ridge'] = ridgeModel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>Cat Boost</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def catBoost(X_train, y_train, X_test):\n    catM = CatBoostRegressor(verbose=0, allow_writing_files=False)\n    params = {'learning_rate': [0.01, 0.05, 0.005, 0.0005],\n              'depth': [4, 6, 10],\n              'l2_leaf_reg': [1, 2, 3, 5, 9]}\n\n    grid_search_cat = RandomizedSearchCV(estimator=catM, scoring='neg_root_mean_squared_error', param_distributions=params, n_iter=10, cv=4, verbose=2,\n                                     random_state=42, n_jobs=-1)\n    grid_search_cat.fit(X_train, y_train)\n    catModel = grid_search_cat.best_estimator_\n    print('Best params(CatBoost):',grid_search_cat.best_params_)\n    print('RMSE(CatBoost):', -grid_search_cat.best_score_)\n    return catModel\n\n#catModel = catBoost(X_train, y_train, X_test)\ncatModel = CatBoostRegressor(verbose=0, allow_writing_files=False, learning_rate=0.05, l2_leaf_reg=2, depth=4)\ncatModel.fit(X_train, y_train)\npredictions['CatBoost'] = catModel.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>Blending models</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_prediction = 0.25 * predictions['XGBR'] + 0.35 * predictions['CatBoost'] + 0.4 * predictions['Ridge']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h4>Result</h4>"},{"metadata":{"trusted":true},"cell_type":"code","source":"result = pd.DataFrame([len(y_train) + 1 + i for i in range(len(X_test))], columns=['Id'])\nresult[target_name] = np.expm1(final_prediction)\nresult.to_csv('result.csv', index=False, header=True)\nresult","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}