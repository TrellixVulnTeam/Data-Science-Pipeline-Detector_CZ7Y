{"cells":[{"metadata":{},"cell_type":"markdown","source":"#  <font color='blue'> House Prices : Data cleaning, viz and modeling  </font>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import packages\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import accuracy_score\nimport math\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectFromModel\nfrom time import time\nimport time \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import  SVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, VotingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## <font color='blue'>  Loading The Datasets </font>"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ntrain = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n#Creating a copy of the train and test datasets\nc_test  = test.copy()\nc_train  = train.copy()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* <font color='blue'>  Getting information about train dataset </font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"c_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* <font color='blue'>  Getting information about test dataset </font>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"c_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. We have 81 columns.\n2. Our target variable is SalePrice.\n3. Id is just an index that we can drop but we will need it in the final submission.\n1. We have many missing values \n\n * * * * we have 79 features in our dataset.\n\n"},{"metadata":{},"cell_type":"markdown","source":"\n* <font color='blue'>  Concat Train and Test datasets </font>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"c_train['train']  = 1\nc_test['train']  = 0\ndf = pd.concat([c_train, c_test], axis=0,sort=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n* <font color='red'>  Calculating the percentage of missing values of each feature </font>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Percentage of NAN Values \nNAN = [(c, df[c].isna().mean()*100) for c in df]\nNAN = pd.DataFrame(NAN, columns=[\"column_name\", \"percentage\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n* <font color='blue'>  Features with more than 50% of missing values. </font>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"NAN = NAN[NAN.percentage > 50]\nNAN.sort_values(\"percentage\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can drop PoolQC, MiscFeature, Alley and Fence features because they have more than 80% of missing values.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop PoolQC, MiscFeature, Alley and Fence features\ndf = df.drop(['Alley','PoolQC','Fence','MiscFeature'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we will select numerical and categorical features \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"object_columns_df = df.select_dtypes(include=['object'])\nnumerical_columns_df =df.select_dtypes(exclude=['object'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Categorical Features** :\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"object_columns_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Numerical Features** :"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_columns_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Deeling with **categorical** feature "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of null values in each feature\nnull_counts = object_columns_df.isnull().sum()\nprint(\"Number of null values in each column:\\n{}\".format(null_counts))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We will fill -- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageType, GarageFinish, GarageQual, FireplaceQu, GarageCond** -- with \"None\" (Take a look in the data description).\n*  We will fill the rest of features with th most frequent value (using its own most frequent value)\n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"columns_None = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','GarageType','GarageFinish','GarageQual','FireplaceQu','GarageCond']\nobject_columns_df[columns_None]= object_columns_df[columns_None].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_with_lowNA = ['MSZoning','Utilities','Exterior1st','Exterior2nd','MasVnrType','Electrical','KitchenQual','Functional','SaleType']\n#fill missing values for each column (using its own most frequent value)\nobject_columns_df[columns_with_lowNA] = object_columns_df[columns_with_lowNA].fillna(object_columns_df.mode().iloc[0])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we have a clean categorical features\n* In the next step we will deal with the **numerical** features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Number of null values in each feature\nnull_counts = numerical_columns_df.isnull().sum()\nprint(\"Number of null values in each column:\\n{}\".format(null_counts))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Fill GarageYrBlt and LotFrontage\n1. Fill the rest of columns with 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"print((numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt']).median())\nprint(numerical_columns_df[\"LotFrontage\"].median())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. * So we will fill the year with 1979 and the Lot frontage with 68"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"numerical_columns_df['GarageYrBlt'] = numerical_columns_df['GarageYrBlt'].fillna(numerical_columns_df['YrSold']-35)\nnumerical_columns_df['LotFrontage'] = numerical_columns_df['LotFrontage'].fillna(68)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Fill the rest of columns with 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_columns_df= numerical_columns_df.fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We finally end up with a clean dataset\n\n"},{"metadata":{},"cell_type":"markdown","source":"* After making some plots we found that we have some colums with low variance so we decide to delete them"},{"metadata":{"trusted":true},"cell_type":"code","source":"object_columns_df['Utilities'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Utilities'].value_counts() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_columns_df['Street'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Street'].value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_columns_df['Condition2'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Condition2'].value_counts() \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_columns_df['RoofMatl'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['RoofMatl'].value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_columns_df['Heating'].value_counts().plot(kind='bar',figsize=[10,3])\nobject_columns_df['Heating'].value_counts() #======> Drop feature one Type\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_columns_df = object_columns_df.drop(['Heating','RoofMatl','Condition2','Street','Utilities'],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Now we will create some new features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])\nnumerical_columns_df['Age_House'].describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Negatif = numerical_columns_df[numerical_columns_df['Age_House'] < 0]\nNegatif\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Like we see here tha the minimun is -1 ???\n* It is strange to find that the house was sold in 2007 before the YearRemodAdd 2009.\nSo we decide to change the year of sold to 2009\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_columns_df.loc[numerical_columns_df['YrSold'] < numerical_columns_df['YearBuilt'],'YrSold' ] = 2009\nnumerical_columns_df['Age_House']= (numerical_columns_df['YrSold']-numerical_columns_df['YearBuilt'])\nnumerical_columns_df['Age_House'].describe()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* TotalBsmtBath : Sum of :\nBsmtFullBath and  1/2 BsmtHalfBath\n\n* TotalBath : Sum of :\nFullBath and 1/2 HalfBath\n\n* TotalSA : Sum of : \n1stFlrSF and 2ndFlrSF and basement area\n\n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_columns_df['TotalBsmtBath'] = numerical_columns_df['BsmtFullBath'] + numerical_columns_df['BsmtFullBath']*0.5\nnumerical_columns_df['TotalBath'] = numerical_columns_df['FullBath'] + numerical_columns_df['HalfBath']*0.5 \nnumerical_columns_df['TotalSA']=numerical_columns_df['TotalBsmtSF'] + numerical_columns_df['1stFlrSF'] + numerical_columns_df['2ndFlrSF']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_columns_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now the next step is to encode categorical features\n**"},{"metadata":{},"cell_type":"markdown","source":"* **Ordinal categories features** - Mapping from 0 to N"},{"metadata":{"trusted":true},"cell_type":"code","source":"bin_map  = {'TA':2,'Gd':3, 'Fa':1,'Ex':4,'Po':1,'None':0,'Y':1,'N':0,'Reg':3,'IR1':2,'IR2':1,'IR3':0,\"None\" : 0,\n            \"No\" : 2, \"Mn\" : 2, \"Av\": 3,\"Gd\" : 4,\"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6\n            }\nobject_columns_df['ExterQual'] = object_columns_df['ExterQual'].map(bin_map)\nobject_columns_df['ExterCond'] = object_columns_df['ExterCond'].map(bin_map)\nobject_columns_df['BsmtCond'] = object_columns_df['BsmtCond'].map(bin_map)\nobject_columns_df['BsmtQual'] = object_columns_df['BsmtQual'].map(bin_map)\nobject_columns_df['HeatingQC'] = object_columns_df['HeatingQC'].map(bin_map)\nobject_columns_df['KitchenQual'] = object_columns_df['KitchenQual'].map(bin_map)\nobject_columns_df['FireplaceQu'] = object_columns_df['FireplaceQu'].map(bin_map)\nobject_columns_df['GarageQual'] = object_columns_df['GarageQual'].map(bin_map)\nobject_columns_df['GarageCond'] = object_columns_df['GarageCond'].map(bin_map)\nobject_columns_df['CentralAir'] = object_columns_df['CentralAir'].map(bin_map)\nobject_columns_df['LotShape'] = object_columns_df['LotShape'].map(bin_map)\nobject_columns_df['BsmtExposure'] = object_columns_df['BsmtExposure'].map(bin_map)\nobject_columns_df['BsmtFinType1'] = object_columns_df['BsmtFinType1'].map(bin_map)\nobject_columns_df['BsmtFinType2'] = object_columns_df['BsmtFinType2'].map(bin_map)\n\nPavedDrive =   {\"N\" : 0, \"P\" : 1, \"Y\" : 2}\nobject_columns_df['PavedDrive'] = object_columns_df['PavedDrive'].map(PavedDrive)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Will we use One hot encoder to encode the rest of categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Select categorical features\nrest_object_columns = object_columns_df.select_dtypes(include=['object'])\n#Using One hot encoder\nobject_columns_df = pd.get_dummies(object_columns_df, columns=rest_object_columns.columns) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_columns_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Concat Categorical(after encoding) and numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = pd.concat([object_columns_df, numerical_columns_df], axis=1,sort=False)\ndf_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = df_final.drop(['Id',],axis=1)\n\ndf_train = df_final[df_final['train'] == 1]\ndf_train = df_train.drop(['train',],axis=1)\n\n\ndf_test = df_final[df_final['train'] == 0]\ndf_test = df_test.drop(['SalePrice'],axis=1)\ndf_test = df_test.drop(['train',],axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Separate Train and Targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"target= df_train['SalePrice']\ndf_train = df_train.drop(['SalePrice'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting The Data into Train and Test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_val,y_train,y_val = train_test_split(df_train,target,test_size=0.33,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Implementing The regressors comparing accuracies."},{"metadata":{"trusted":true},"cell_type":"code","source":"def acc_summary(pipeline, X_train, y_train, X_val, y_val):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    y_pred = sentiment_fit.predict(X_val)\n    rmse = math.sqrt(metrics.mean_squared_error(y_val, y_pred))\n    print(\"root mean squre error : {0:.2f}\".format(rmse))\n    #print(\"train and test time: {0:.2f}s\".format(train_test_time))\n    print(\"-\"*80)\n    return rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [ \n        'Gradient Boosting Regressor',  \n        \"Bagging Regressor\",\n        \"AdaBoost Regressor\", \n        \"K Nearest Neighbour Regressor\",\n         \"Decison Tree Regressor\",\n         \"Random Forest Regressor\",\n        \"Gaussian Process Regressor\",\n        \"XGB Regressor\",\n        \"LGBM Regressor\"\n         ]\nregressors = [\n    \n    GradientBoostingRegressor(), \n    BaggingRegressor(),\n    AdaBoostRegressor(),\n    KNeighborsRegressor(),\n    DecisionTreeRegressor(),\n    RandomForestRegressor(),\n    GaussianProcessRegressor(),\n    XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.02, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2000,\n             n_jobs=1, nthread=None, objective='reg:linear',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1),\n     LGBMRegressor(objective='regression', \n                                       num_leaves=4, #was 3\n                                       learning_rate=0.01, \n                                       n_estimators=11000, #8000\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.4, # 'was 0.2'\n                                       )\n\n        ]\n\nzipped_clf = zip(names,regressors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def regressor_comparator(X_train,y_train,X_val,y_val,regressor=zipped_clf): \n    result = []\n    for n,r in regressor:\n        checker_pipeline = Pipeline([\n            ('regressor', r)\n        ])\n        print(\"Validation result for {}\".format(n))\n        #print(r)\n        clf_acc= acc_summary(checker_pipeline,X_train, y_train, X_val, y_val)\n        result.append((n,clf_acc))\n    return result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"regressor_comparator(X_train,y_train,X_val,y_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Thankyou for Reading..."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}