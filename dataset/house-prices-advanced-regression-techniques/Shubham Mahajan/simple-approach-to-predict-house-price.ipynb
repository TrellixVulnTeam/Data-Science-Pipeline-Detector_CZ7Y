{"cells":[{"metadata":{},"cell_type":"markdown","source":"**This competition is all about predict house price based on 79 columns.**\n\nThis notebook is just simple approach to solve this dataset.\n\nAnd there is brief explnation about filling null values."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pandas import Series, DataFrame\nimport matplotlib.pyplot as plt  # for plots\n%matplotlib inline\nimport seaborn as sns # For advance plots and graphs\n\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# to view all columns\nfrom IPython.display import display\npd.options.display.max_columns = None\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read data\ntrain = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n# combine train and test data set\ndata = pd.concat([train, test],ignore_index=True, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the shape of  train dataset', train.shape)\nprint('the shape of  test dataset', test.shape)\nprint('the shape of data', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 'Id' has no relation with sale price.  \n- So drop 'Id' column "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- **Now start EDA..**\n\n- First check target column.. target column is 'Sale price'\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['SalePrice'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness in SalePrice :\", train['SalePrice'].skew())\nprint(\"Kurtosis in SalePrice :\", train['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(train['SalePrice'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Target column is right skwed. So log transformation is requried."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['SalePrice'] = np.log(train[\"SalePrice\"]+1)\n#check distribution after log transform.\nsns.distplot(train['SalePrice'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now move on toward the predictors."},{"metadata":{"trusted":true},"cell_type":"code","source":"#check correlation in features\ncor = train.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(cor,cmap=\"Blues\", vmax=0.9)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking high correlatinal features to 'SalePrice'\ncor[abs(cor['SalePrice'].values) >= 0.5]['SalePrice'].sort_values(ascending=False)[1:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- So this features have good correlation with target variable\n- so check this their now realtion"},{"metadata":{"trusted":true},"cell_type":"code","source":"# OverllQual is catagorical feature so draw boxplot\nfig = plt.figure(constrained_layout=True, figsize=(8,5))\nsns.boxplot(train['OverallQual'], train['SalePrice'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(constrained_layout=True, figsize=(8,5))\nsns.scatterplot(train['GrLivArea'], train['SalePrice'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see this two points may be outliers cause as GrLivArea is increase price also increase.\n\nBut this two points behave abnormal."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<12.5)].index)\n\ntrain.reset_index(drop = True, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sctter plot after removing outliers.\nfig = plt.figure(constrained_layout=True, figsize=(8,5))\nsns.scatterplot(train['GrLivArea'], train['SalePrice'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(constrained_layout=True, figsize=(8,5))\nsns.boxplot(train['GarageCars'], train['SalePrice'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(train['GarageArea'], train['SalePrice'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(train['TotalBsmtSF'], train['SalePrice'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(constrained_layout=True, figsize=(8,5))\nsns.boxplot(train['FullBath'], train['SalePrice'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Form above plots we can see 'GrLivArea' and 'SalePrice' have good relation.\n- There are few outliers.\n- For now we will keep outliers, we will figure out it later."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot fig sizing. \nimport matplotlib.style as style\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train.corr(), cmap=sns.diverging_palette(20, 220, n=200), mask = mask, annot=True, center = 0, );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see there is multicollinearity between some features.\n- This can figure it later."},{"metadata":{},"cell_type":"markdown","source":" **Now we are done with most of the Feature Analysis, Let's Beging with the Feature Engineering!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# again concat train and test cause we have done changes in train dataset. \ndata = pd.concat((train, test)).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Check values in each column"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = data.columns\n\nfor col in columns:\n    print(data[col].value_counts())\n    print('\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now check missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_tot = data.isnull().sum().sort_values(ascending = False)\nmissing_per = ((data.isnull().sum()/data.isnull().count())*100).sort_values(ascending = False)\nmissing_data = pd.concat([missing_tot, missing_per] , axis=1, keys=['Total', 'Percent'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data.head(40)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now separate the catgorical freatues and numerical features "},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cat = data.select_dtypes('object')\ndata_int =  data.select_dtypes(['int64','float64'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cat.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_cat.isnull().sum().sort_values(ascending =False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d = data_cat.isnull().sum().sort_values(ascending =False)\nd.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- To decide about null values I have done old type analysis by using paper and pen\n- For example, Like in 'PoolQc' column 2908 null values, but in as per data description\n  PoolQC: Pool quality\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tAverage/Typical\n       Fa\tFair\n       NA\tNo Pool\n- Means null values are reprsenting No pool so fill them by 'None'\n- One more example, if 'GarageType' is null and as per data description\n\n       2Types\tMore than one type of garage\n       Attchd\tAttached to home\n       Basment\tBasement Garage\n       BuiltIn\tBuilt-In (Garage part of house - typically has room above garage)\n       CarPort\tCar Port\n       Detchd\tDetached from home\n       NA\tNo Garage\n- Means null values neans no Garage so  fill them by 'None'\n- And the features related to Garage like 'GarageYrBlt', 'GarageFinish', 'GarageCars','GarageArea','GarageQual','GarageCond' will follow same lead.\n- Like in ('GarageCars':  Size of garage in car capacity) if there is now Garage means 'GarageCars' null represnts '0'.\n- As we can see 'GarageCars' column in above  value count code. \n      2.0    1593\n      1.0     776\n      3.0     373\n      0.0     157\n      4.0      16\n      5.0       1\n \n- Use data desciption for fill null values\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill null values by none cause this null represpents they are not avaliable on site so fill by 'None'\ncolumns1 = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond',\n       'GarageQual', 'GarageFinish', 'GarageType', 'BsmtCond', 'BsmtExposure',\n       'BsmtQual', 'BsmtFinType2', 'BsmtFinType1', 'MasVnrType']\n\n# fill by mode\ncolumns2 = ['MSZoning','Functional', 'Utilities', 'Electrical', 'KitchenQual', 'SaleType',\n       'Exterior2nd', 'Exterior1st']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in columns1:\n    data_cat[col].fillna('None',inplace =True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col  in columns2:\n    data_cat[col].fillna(data[col].mode()[0],inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d2 = data_int.isnull().sum().sort_values(ascending=False)\nd2.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns3 = ['LotFrontage', 'GarageYrBlt', 'MasVnrArea', 'BsmtFullBath',\n       'BsmtHalfBath', 'TotalBsmtSF', 'BsmtUnfSF', 'GarageArea', 'GarageCars',\n       'BsmtFinSF2', 'BsmtFinSF1']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in columns3:\n    data_int[col].fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([data_cat,data_int],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Since these column are actually a category , using a numerical number will lead the model to assume\n# that it is numerical , so we convert to string .\ndf['MSSubClass'] = df['MSSubClass'].apply(str)\ndf['YrSold'] = df['YrSold'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)\ndf.drop('SalePrice', axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Check now is there ant missing value remain"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_tot = df.isnull().sum().sort_values(ascending = False)\nmissing_per = ((df.isnull().sum()/df.isnull().count())*100).sort_values(ascending = False)\nmissing_data = pd.concat([missing_tot, missing_per] , axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Looking at Skewed Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n\nskewed_feats = df[numeric_feats].apply(lambda x: x.skew()).sort_values(ascending=False)\nskewed_feats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Box Cox Transformation on Skewed Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nhigh_skew = skewed_feats[skewed_feats > 0.5]\nskew_index = high_skew.index\n\n# Normalise skewed features\nfor i in skew_index:\n    df[i] = boxcox1p(df[i], boxcox_normmax(df[i] + 1))\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Encoding the finalized features"},{"metadata":{"trusted":true},"cell_type":"code","source":"final_features = pd.get_dummies(df).reset_index(drop=True)\nprint('Features size:', df.shape)\nfinal_features.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Split data into two parts for training and testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"nrow_train = train.shape[0]\n\nX_train = final_features[:nrow_train]\nX_test = final_features[nrow_train:]\nY = train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Import models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\n\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# Kernel Ridge Regression : made robust to outliers\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n# LASSO Regression : made robust to outliers\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n                    alphas=alphas2,random_state=42, cv=kfolds))\n\n# Elastic Net Regression : made robust to outliers\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, \n                         alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fit the training data X_train,Y \nprint('Elasticnet')\nelastic_model = elasticnet.fit(X_train, Y)\nprint('Lasso')\nlasso_model = lasso.fit(X_train, Y)\nprint('Ridge')\nridge_model = ridge.fit(X_train, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model blending function using fitted models to make predictions\ndef blend_models(X):\n    return ((elastic_model.predict(X)) + (lasso_model.predict(X)) + (ridge_model.predict(X)))/3\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:,1] = np.expm1(blend_models(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fix outleir predictions\nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission_regression1.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This above thing I have done by taking refrence to other kernals."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This is my first approach with taking help some kernals.**\n\n**For more accurate results we can use multiple alogrithums.**\n\n**We can also do some feature engineering.**\n\n**If you like anything in this kernal then please don't forget to upvote.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}