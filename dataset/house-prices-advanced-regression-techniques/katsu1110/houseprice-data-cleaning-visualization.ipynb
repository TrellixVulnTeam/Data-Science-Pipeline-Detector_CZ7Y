{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.3","mimetype":"text/x-python","file_extension":".py"}},"nbformat":4,"cells":[{"metadata":{"_cell_guid":"418938e3-4fa2-48a8-8024-bcdff33aae2f","_uuid":"824be3aa14678a8e989a0c7a0b33c20ddad56cff"},"cell_type":"markdown","source":"The aim of this notebook is to perform basic exploratory data analysis (EDA), which in this competition include:\n\n- deal with nan\n- feature selection\n- normality check\n- convert strings to categorical values via LabelEncoder\n- correlation matrix\n- feature importance\n- outlier deletion & normalization\n- dimensionality reduction\n\nI also show the basic of the so-called 'stacked-emsemble model' to boost our prediction accuracy!\n- stacked-emsemble model using 'XGBoost', 'neural network, and 'support vector machine regression (SVR)' \n- submission"},{"metadata":{},"cell_type":"markdown","source":"First, let's start with looking at what's inside of the given csv file."},{"metadata":{"_cell_guid":"6fa40af3-42d9-48b0-9cf9-22496d95eb7d","_uuid":"2e0e640cb45790af651356329e51f9ede0bfac26"},"execution_count":null,"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n# for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain.head()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"91f4e382-ce2b-4d2f-a965-85514b9f58ab","_uuid":"ba80ffdf025c631a0c84850c8f2216fb53e21e35"},"cell_type":"markdown","source":"There seems to be several data types. "},{"metadata":{"_cell_guid":"d920fe7b-8788-4e44-a4e7-9154d2580ef1","_uuid":"1493835487aa6051df25b2fc66ca5cdccc4c9e75"},"execution_count":null,"source":"print(train.dtypes)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"a7520e56-d379-47dd-8946-4e1e1f16b244","_uuid":"c3bfa89dceb044478e96e347d548506b4e3e11c5"},"cell_type":"markdown","source":"Yes, there are...but what is 'object'? Let's just have a look at one example 'SaleCondition'."},{"metadata":{"_cell_guid":"ceac34c8-7831-4838-bc2d-b43710d9dcbb","_uuid":"0fc2dda5c6487eb9da74f8895764d8cbc69be8a5"},"execution_count":null,"source":"print(train['SaleCondition'].unique())","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"84e8d061-c693-4057-8f2b-8849ea315ac1","_uuid":"2d8caac10da4c69aea3190a8841994310c39b6b0"},"cell_type":"markdown","source":"Essentially, 'object' is string. There needs to be a way to convert string to float or int. This is where **LabelEncoder** kicks in."},{"metadata":{"_cell_guid":"9b6d5593-3981-44d1-857d-056eefba6288","_uuid":"8ab762cc12506924fa76fd93fdea246b2a4e0f23"},"execution_count":null,"source":"# string label to categorical values\nfrom sklearn.preprocessing import LabelEncoder\n\nfor i in range(train.shape[1]):\n    if train.iloc[:,i].dtypes == object:\n        lbl = LabelEncoder()\n        lbl.fit(list(train.iloc[:,i].values) + list(test.iloc[:,i].values))\n        train.iloc[:,i] = lbl.transform(list(train.iloc[:,i].values))\n        test.iloc[:,i] = lbl.transform(list(test.iloc[:,i].values))\n\nprint(train['SaleCondition'].unique())","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"05eb0083-eeb1-4d13-b3a8-a137d5b9b685","_uuid":"00db5f99981d88c6c56c16ba5b493de08f9b1a4e"},"cell_type":"markdown","source":"In this way we can convert strings to categorical values.\n\nNow let's see whether there are nan in the data. "},{"metadata":{"_cell_guid":"dd6fa56e-7d23-4ad4-9e4c-076e4e39eaad","_uuid":"dfe0f866f5ecdcdd8cc49d8cdd1a5aff5c2cca9a"},"execution_count":null,"source":"# search for missing data\nimport missingno as msno\nmsno.matrix(df=train, figsize=(20,14), color=(0.5,0,0))","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"911cfefc-f584-4fe1-941e-15d8a88ce2ed","_uuid":"95d664a6c5bd29627a0b19b0981a60086f24d96f"},"cell_type":"markdown","source":"The white elements have nan there. There seems to be some columns containing nan. "},{"metadata":{"_cell_guid":"5f9d3f3b-c1bd-497a-96a9-57e534a25273","_uuid":"b961d5aa5268095dfa2db98577ed7c301e7865fe"},"execution_count":null,"source":"# Which columns have nan?\nprint('training data+++++++++++++++++++++')\nfor i in np.arange(train.shape[1]):\n    n = train.iloc[:,i].isnull().sum() \n    if n > 0:\n        print(list(train.columns.values)[i] + ': ' + str(n) + ' nans')\n\nprint('testing data++++++++++++++++++++++ ')\nfor i in np.arange(test.shape[1]):\n    n = test.iloc[:,i].isnull().sum() \n    if n > 0:\n        print(list(test.columns.values)[i] + ': ' + str(n) + ' nans')\n","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"784a76e9-647e-4b96-9d25-9149324c8d5c","_uuid":"d1220bf29da117559b01f7d69029d7d10477c918"},"cell_type":"markdown","source":"Actually, there are many columns in the test set to have nan. 'LotFrontage', 'MasVnrArea' and 'GarageYrBlt' have many nans in the both sets, so we just drop them. For others, there are a few nans in each column, so we just replace them with the median of the corresponding column."},{"metadata":{"_cell_guid":"456119b5-0b3e-4855-835f-8a036f10b057","_uuid":"7f102f44b7687c89d4d75776cc69313d2d8677c0"},"cell_type":"markdown","source":"Let's organize the data a bit better."},{"metadata":{"_cell_guid":"b078b15d-c36e-4138-882d-d1421522c486","_uuid":"a741b61bb9da3a6a9f78acd56298d86acd4b7898"},"execution_count":null,"source":"# keep ID for submission\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# split data for training\ny_train = train['SalePrice']\nX_train = train.drop(['Id','SalePrice'], axis=1)\nX_test = test.drop('Id', axis=1)\n\n# dealing with missing data\nXmat = pd.concat([X_train, X_test])\nXmat = Xmat.drop(['LotFrontage','MasVnrArea','GarageYrBlt'], axis=1)\nXmat = Xmat.fillna(Xmat.median())\n\n# check whether there are still nan\nimport missingno as msno\nmsno.matrix(df=Xmat, figsize=(20,14), color=(0.5,0,0))","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"d22dce52-1b13-4214-a662-402ea98c0151","_uuid":"d0d7f71f7a65bfe9707fbb89bfa92e48391dda0b"},"cell_type":"markdown","source":"OK, there are no nan any more! Let's check our columns again."},{"metadata":{"_cell_guid":"896408e5-8c7b-4057-8f42-39f4c39251b9","_uuid":"ccfbd4ecbf984e08f9685b50ca5931ec2be0bbd4"},"execution_count":null,"source":"print(Xmat.columns.values)\nprint(str(Xmat.shape[1]) + ' columns')","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"0933fa2c-14bd-48e4-8578-1b6085430a58","_uuid":"e26562ba3c8c50aa82f87a1f371b0f71fd201438"},"cell_type":"markdown","source":"There are many '...SF'. Maybe we make a new feature which takes the sum of the all."},{"metadata":{"_cell_guid":"780390a9-4daa-46be-b682-3c9de135dbe7","_uuid":"afed341b737e7325b4d0935ae33abb0b4d48f7b9"},"execution_count":null,"source":"# add a new feature 'total sqfootage'\nXmat['TotalSF'] = Xmat['TotalBsmtSF'] + Xmat['1stFlrSF'] + Xmat['2ndFlrSF']\nprint('There are currently ' + str(Xmat.shape[1]) + ' columns.')","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"7c8c0c7f-d1b7-4bb0-954d-3d20da0a23e7","_uuid":"3738d1d93911258d5a52cffd26e3038c21d3293e"},"cell_type":"markdown","source":"Now let's have a look at the target distribution. As this is a regression task, we want the target to be normally distributed."},{"metadata":{"_cell_guid":"b60a3e72-444a-464b-a146-3159380adb4d","_uuid":"5aeac84155f975cc458269d42d50ce7209185e67"},"execution_count":null,"source":"# normality check for the target\nax = sns.distplot(y_train)\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"f137a09e-d252-4fc5-80a7-80b53b16f73e","_uuid":"fdf1149ae18db047c7f50d1dfa9485905bcd11be"},"cell_type":"markdown","source":"Well, it is right-skewed. We use log-transform to make them normally distributed."},{"metadata":{"_cell_guid":"28a6a34f-f2a4-4074-a191-0ae0223cf2bb","_uuid":"0e0f5c2489210ab4877de4bb448ed29f25f7699c"},"execution_count":null,"source":"# log-transform the dependent variable for normality\ny_train = np.log(y_train)\n\nax = sns.distplot(y_train)\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"83e0a507-4f7d-4482-b730-349b9a5f37cf","_uuid":"1fb20f67dd59b0c80587d505e3b2af930f136ab7"},"cell_type":"markdown","source":"This is (approximately) normal distribution! As a custom, let's have a look at correlation matrix."},{"metadata":{"_cell_guid":"d135c11f-80dc-4a6b-9455-b9ca614ecc8f","_uuid":"20d3a1c5150eb6aa05bdc6996f4e0af3152a399b"},"execution_count":null,"source":"# train and test\nX_train = Xmat.iloc[:train.shape[0],:]\nX_test = Xmat.iloc[train.shape[0]:,:]\n\n# Compute the correlation matrix\ncorr = X_train.corr()\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"deec738d-d763-4a09-ad4f-fdc137bd5e82","_uuid":"6a29815044ff7fda21ece0402e201d3bd73efa40"},"cell_type":"markdown","source":"Which features are important? Let a random forest regressor tell us about it."},{"metadata":{"_cell_guid":"7bbde82a-a38b-45e5-898f-f36247f0627a","scrolled":true,"_uuid":"b325bbbd06dc32508a93e0d2f0132144a75bb722"},"execution_count":null,"source":"# feature importance using random forest\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=80, max_features='auto')\nrf.fit(X_train, y_train)\nprint('Training done using Random Forest')\n\nranking = np.argsort(-rf.feature_importances_)\nf, ax = plt.subplots(figsize=(11, 9))\nsns.barplot(x=rf.feature_importances_[ranking], y=X_train.columns.values[ranking], orient='h')\nax.set_xlabel(\"feature importance\")\nplt.tight_layout()\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"74dfa198-7f65-4c23-ab14-0a5bf422aa34","_uuid":"8a7ea641b192d30f6870b636cf0604678faf8750"},"cell_type":"markdown","source":"Surprisingly, only two features are dominant: 'OverallQual' and 'TotalSF'. So instead of using all the 77 features, maybe just using the top 30 features is good enough (dimensionality reduction, in a way).\n\nHere, we make a new feature called 'Interaction': simply the multiplication between the top 2 features. Also, we normalize the data via z-scoring."},{"metadata":{"_cell_guid":"310e84f4-f6ca-4d26-a218-8a55baa3584d","_uuid":"ec9ea713b6ca9b179739971605f79d48d1c15f8c"},"execution_count":null,"source":"# use the top 30 features only\nX_train = X_train.iloc[:,ranking[:30]]\nX_test = X_test.iloc[:,ranking[:30]]\n\n# interaction between the top 2\nX_train[\"Interaction\"] = X_train[\"TotalSF\"]*X_train[\"OverallQual\"]\nX_test[\"Interaction\"] = X_test[\"TotalSF\"]*X_test[\"OverallQual\"]\n\n# zscoring\nX_train = (X_train - X_train.mean())/X_train.std()\nX_test = (X_test - X_test.mean())/X_test.std()\n    \n# heatmap\nf, ax = plt.subplots(figsize=(11, 5))\ncmap = sns.cubehelix_palette(light=1, as_cmap=True)\nsns.heatmap(X_train, cmap=cmap)\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"cc8fc4a7-098d-4ba8-b1ea-1277bcbfba4d","_uuid":"22f311f7f4c18bd599fe1e764e8dd5ee39ed0ac1"},"cell_type":"markdown","source":"Let' see how the important features are related to our target \"SalePrice\"."},{"metadata":{"_cell_guid":"2d5446e9-5c3e-4738-b962-fd5e736ad8d4","_uuid":"246fc7c3e5c15146ef0233fa567c4ef6deeed3b0"},"execution_count":null,"source":"# relation to the target\nfig = plt.figure(figsize=(12,7))\nfor i in np.arange(30):\n    ax = fig.add_subplot(5,6,i+1)\n    sns.regplot(x=X_train.iloc[:,i], y=y_train)\n\nplt.tight_layout()\nplt.show()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"c379f162-5e5e-4eba-a629-efd7eba1c44c","_uuid":"259af7661ede0bd1aad89c3af5fa3a6d9486eb59"},"cell_type":"markdown","source":"Basically we can see clear linear relationships in many panels. We do not see a lot of outliers, which is good. Maybe let's just remove some outlier data points found in the 'totalSF' and 'GrLivArea', which are apparently out of the linear regression line."},{"metadata":{"_cell_guid":"749f5ca2-ace5-4ee0-9f8f-2b0f7c437d38","collapsed":true,"_uuid":"4934b88aa12cfb8c8a46a8f608e556d7e1b82aae"},"execution_count":null,"source":"# outlier deletion\nXmat = X_train\nXmat['SalePrice'] = y_train\nXmat = Xmat.drop(Xmat[(Xmat['TotalSF']>5) & (Xmat['SalePrice']<12.5)].index)\nXmat = Xmat.drop(Xmat[(Xmat['GrLivArea']>5) & (Xmat['SalePrice']<13)].index)\n\n# recover\ny_train = Xmat['SalePrice']\nX_train = Xmat.drop(['SalePrice'], axis=1)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"2d239dec-11d4-48b9-99d5-1279a57f965a","_uuid":"d40faa086c6a6915a9bef2f102911fa7189c883b"},"cell_type":"markdown","source":"Now it is time to have fun with the stacked-emsembling model! \n\nThe intuitive idea of 'emsembling' is that we may be able to get better prediction performance if we average predictions by multiple models, rather than relying on a single model. There are always pros and cons in machine learning algorithms, so averaging multiple model predictions may compensate one another.\n\nHere we use three models: XGBoost, Neural Network, and SVR.\n\nLet's start with XGBoost. There are some parameters in XGBoost that can be optimized. We use 'GridSearch' to explore which combination of parameters yield the best prediction in this dataset."},{"metadata":{"_cell_guid":"16a417e4-976d-400a-9980-ad41fe85fcc0","_uuid":"d566baf912742709e267deb65b59fa4f71883e0d"},"execution_count":null,"source":"# XGBoost\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\nprint(\"Parameter optimization\")\nxgb_model = xgb.XGBRegressor()\nreg_xgb = GridSearchCV(xgb_model,\n                   {'max_depth': [2,4,6],\n                    'n_estimators': [50,100,200]}, verbose=1)\nreg_xgb.fit(X_train, y_train)\nprint(reg_xgb.best_score_)\nprint(reg_xgb.best_params_)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e307ebb3-5468-4c74-80db-e5e5dc3a7e14","_uuid":"1215a9c262b65d1dff0a380ee9c288b7cdfcd746"},"cell_type":"markdown","source":"Moving on to neural network: we simply use Keras for easy implementation of multi-layer perceptron. \n\nWe can again take advantage of grid search, but that requires Keras' wrapper 'KerasRegressor' (or 'KerasClassifier') to utilize scikit_learn's gird search. Here we try to select the best ones with respect to 'optimizer', 'batch_size', and 'epochs'. "},{"metadata":{"_cell_guid":"f57e9886-8159-4385-8d60-f661e565b979","_uuid":"f254ab3a60247c2e3b12125a7bea43c60737875e"},"execution_count":null,"source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\ndef create_model(optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    return model\n\nmodel = KerasRegressor(build_fn=create_model, verbose=0)\n# define the grid search parameters\noptimizer = ['SGD','Adam']\nbatch_size = [10, 30, 50]\nepochs = [10, 50, 100]\nparam_grid = dict(optimizer=optimizer, batch_size=batch_size, epochs=epochs)\nreg_dl = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\nreg_dl.fit(X_train, y_train)\n\nprint(reg_dl.best_score_)\nprint(reg_dl.best_params_)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"e1d2dd60-0386-4de2-a125-2bf165efa22d","_uuid":"939c60504ec94b98bf38e8646de9b11e1075b900"},"cell_type":"markdown","source":"Finally we implement our last regressor, which is SVR (support vector regressior). We can do that easily using sklearn. Again, we use grid search to optimize some of the SVR's hyperparameters."},{"metadata":{"_cell_guid":"fa151de2-a1ee-4325-ae90-ae5127c16271","_uuid":"48eae5b6797f6e6e3f97c93ade41b52651f127e7"},"execution_count":null,"source":"# SVR\nfrom sklearn.svm import SVR\n\nreg_svr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n                               \"gamma\": np.logspace(-2, 2, 5)})\nreg_svr.fit(X_train, y_train)\n\nprint(reg_svr.best_score_)\nprint(reg_svr.best_params_)","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"022436d4-67a0-4657-8850-a80f93ebc6fd","_uuid":"924c0162166dc299f92e92e1c4169591c5139b59"},"cell_type":"markdown","source":"Alright, so far we have three models. Emsemble methods are simply averaging predicted results across models to improve overall accuracy. This method is based on the idea that prediction by each model may be independent of one another and thus averaging predictions across models may compensate one another and yield better solution.\n\nStacking methods are part of emsemble methods, but they use weighted averaging of predictions by models. The idea is, of course, you want to assign more weights on better models and less weights on worse ones, based on the dataset. To determine the weights, we treat predictions by models as new features (predictors) and train a linear model to predict our targets.\n\nHere is the very nice documentation about the stacked-emsemble model.\n'https://blogs.sas.com/content/subconsciousmusings/2017/05/18/stacked-ensemble-models-win-data-science-competitions/'\n\nSo let's first make a new matrix where each column represents prediction by each model."},{"metadata":{"_cell_guid":"1bacc4fd-bf4e-428d-902a-e3246d1fcb35","_uuid":"e619333c55753fc4acfa54e2bbb60cd6789d44ed"},"execution_count":null,"source":"# second feature matrix\nX_train2 = pd.DataFrame( {'XGB': reg_xgb.predict(X_train),\n     'NN': reg_dl.predict(X_train).ravel(),\n     'SVR': reg_svr.predict(X_train),\n    })\nX_train2.head()","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"3329a75d-275d-4367-9b7f-73226ca83e32","_uuid":"a14f3e87f223bc384d0896a65fe977a9e7a82ffd"},"cell_type":"markdown","source":"We train a linear regressor for this new training matrix and predict our target! We use Lasso GLM to avoid overfitting. We predict our target 'SalePrice', but do not forget to convert it back to ordinal scale! It has been in log-scale for normality. "},{"metadata":{"_cell_guid":"82d7e8df-818d-49ed-8672-3deb51706a1e","collapsed":true,"_uuid":"5a2abc8092eb0345e4e9ed712265e9295a012fbd"},"execution_count":null,"source":"# second-feature modeling using linear regression\nfrom sklearn import linear_model\n\nreg = linear_model.LinearRegression()\nreg.fit(X_train2, y_train)\n\n# prediction using the test set\nX_test2 = pd.DataFrame( {'XGB': reg_xgb.predict(X_test),\n     'DL': reg_dl.predict(X_test).ravel(),\n     'SVR': reg_svr.predict(X_test),\n    })\n\n# Don't forget to convert the prediction back to non-log scale\ny_pred = np.exp(reg.predict(X_test2))","cell_type":"code","outputs":[]},{"metadata":{"_cell_guid":"2ba671ad-3e4f-4191-acb5-70444f50262a","_uuid":"e31dcc9789c8f5e55de7e6e6e2b7e7bbd85e6cd0"},"cell_type":"markdown","source":"Finally, we submit our prediction!"},{"metadata":{"_cell_guid":"68b75cb3-e282-4ed6-af52-4cff68dcdc94","collapsed":true,"_uuid":"934fbfecf7033864bde6cee3ccf873058c74bda4"},"execution_count":null,"source":"# submission\nsubmission = pd.DataFrame({\n    \"Id\": test_ID,\n    \"SalePrice\": y_pred\n})\nsubmission.to_csv('houseprice.csv', index=False)","cell_type":"code","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that, here we used a grid-search to optimize hyperparameters, but it is still not good enough because explored parameter spaces are still narrow. We could still optimize hyperparameters and go further up by:D "}],"nbformat_minor":1}