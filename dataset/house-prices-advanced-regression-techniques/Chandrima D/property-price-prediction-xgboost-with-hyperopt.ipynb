{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ndf= pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Pre-processing**\n\nFirst, we will do missing data analysis to see which variables have almost all NULL values. Then we will decide whether to discard them altogether or to impute them.","metadata":{}},{"cell_type":"code","source":"# Missing Data Pattern in Training Data\nimport seaborn as sns\nsns.heatmap(df.isnull(), cbar=False, cmap='PuBu')","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visibly, 'PoolQC' and 'Alley' have almost all NULL values. Now, we will check exact percentages of missing values for the variables.","metadata":{}},{"cell_type":"code","source":"# Detailed Information on Missing Data\ntotal = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\nmissing = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a rule-of-thumb, we usually discard variables with > 50% missing values, but here, I have discarded all variables having > 40% missing values to improve the quality of the model. For rest of the variables, simple imputation strategy has been used. Continuous variables have been imputed with mean values and categorical variables have been imputed with mode values respectively. ","metadata":{}},{"cell_type":"code","source":"# Drop the fields having > 40% missing values to avoid bias in the model\ndf.drop(['Alley'],axis=1,inplace=True)\ndf.drop(['PoolQC'],axis=1,inplace=True)\ndf.drop(['MiscFeature'],axis=1,inplace=True)\ndf.drop(['Fence'],axis=1,inplace=True)\ndf.drop(['FireplaceQu'],axis=1,inplace=True)\n\n# Impute continuous var with Mean; Impute categorical var with Mode\ndf['LotFrontage']=df['LotFrontage'].fillna(df['LotFrontage'].mean())\ndf['GarageCond']=df['GarageCond'].fillna(df['GarageCond'].mode()[0])\ndf['GarageType']=df['GarageType'].fillna(df['GarageType'].mode()[0])\ndf['GarageYrBlt']=df['GarageYrBlt'].fillna(df['GarageYrBlt'].mean())\ndf['GarageFinish']=df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])\ndf['GarageQual']=df['GarageQual'].fillna(df['GarageQual'].mode()[0])\ndf['BsmtExposure']=df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])\ndf['BsmtFinType2']=df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0])\ndf['BsmtFinType1']=df['BsmtFinType1'].fillna(df['BsmtFinType1'].mode()[0])\ndf['BsmtCond']=df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])\ndf['BsmtQual']=df['BsmtQual'].fillna(df['BsmtQual'].mode()[0])\ndf['MasVnrArea']=df['MasVnrArea'].fillna(df['MasVnrArea'].mean())\ndf['MasVnrType']=df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])\ndf['Electrical']=df['Electrical'].fillna(df['Electrical'].mode()[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, categorical variables have to be label-encoded to convert them into continuous features.","metadata":{}},{"cell_type":"code","source":"# Convert categorical features into continuous features in Train Data\nfrom sklearn.preprocessing import LabelEncoder\nlencoders = {}\nfor col in df.select_dtypes(include=['object']).columns:\n    lencoders[col] = LabelEncoder()\n    df[col] = lencoders[col].fit_transform(df[col])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extensive Feature Selection using Wrapper Methods**\n\nI am using various tree-based algorithms like: Random Forest, Decision Tree and Extra Trees classifiers to perform Feature Selection. For making the process more extensive, I have tried getting the results for each of these classifiers with max_features = 'log2' as well as 'auto'.  ","metadata":{}},{"cell_type":"code","source":"# Feature Selection using Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nx_t = df.drop('SalePrice', axis=1)\ny_t = df['SalePrice']\nclf_1 = SelectFromModel(RandomForestClassifier(n_estimators=100, max_features='log2', max_depth = 4))\nclf_2 = SelectFromModel(RandomForestClassifier(n_estimators=100, max_features='auto', max_depth = 4))\nclf_1.fit(x_t, y_t)\nclf_2.fit(x_t, y_t)\nsel_feat_1 = x_t.columns[(clf_1.get_support())]\nsel_feat_2 = x_t.columns[(clf_2.get_support())]\nprint(sel_feat_1)\nprint(sel_feat_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Selection using Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nx_t = df.drop('SalePrice', axis=1)\ny_t = df['SalePrice']\nclf_3 = SelectFromModel(DecisionTreeClassifier(max_features='log2'))\nclf_4 = SelectFromModel(DecisionTreeClassifier(max_features='auto'))\nclf_3.fit(x_t, y_t)\nclf_4.fit(x_t, y_t)\nsel_feat_3 = x_t.columns[(clf_3.get_support())]\nsel_feat_4 = x_t.columns[(clf_4.get_support())]\nprint(sel_feat_3)\nprint(sel_feat_4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Selection using Extra Trees Classifier \nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nx_t = df.drop('SalePrice', axis=1)\ny_t = df['SalePrice']\nclf_5 = SelectFromModel(ExtraTreesClassifier(max_features='log2'))\nclf_6 = SelectFromModel(ExtraTreesClassifier(max_features='auto'))\nclf_5.fit(x_t, y_t)\nclf_6.fit(x_t, y_t)\nsel_feat_5 = x_t.columns[(clf_5.get_support())]\nsel_feat_6 = x_t.columns[(clf_5.get_support())]\nprint(sel_feat_5)\nprint(sel_feat_6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extensive Feature Importance Visualization**\n\nNow, we will visualize the relative importance (w.r.t. target variable 'SalePrice') of these subsets of selected features. I have used extensive visualizations for three subsets I got in previous step using Random Forest, Decision Tree and Extra Trees classifiers respectively. ","metadata":{}},{"cell_type":"code","source":"# Visualize Relative Feature Importance\nfrom yellowbrick.features import FeatureImportances\n\nclf_1 = RandomForestClassifier()\nx_train_1 = df[['LotFrontage', 'LotArea', 'Neighborhood', 'Condition1','Condition2',\n                'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle',\n                'Exterior1st', 'Exterior2nd', 'MasVnrArea', 'ExterQual', 'BsmtFinSF1','BsmtFinSF2',\n                'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n                'FullBath', 'BedroomAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional',\n                'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n                '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']] \ny_train = df['SalePrice']\nviz_1 = FeatureImportances(clf_1)\nviz_1.fit(x_train_1, y_train)\nviz_1 = FeatureImportances(clf_1)\nviz_1.fit(x_train_1, y_train)\nviz_1.poof()\n\nclf_2 = DecisionTreeClassifier()\nx_train_2 = df[['LotFrontage', 'LotArea', 'LotShape', 'LotConfig', 'Neighborhood',\n                'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n                'Exterior1st', 'Exterior2nd', 'MasVnrArea', 'BsmtExposure',\n                'BsmtFinType1', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC',\n                '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'BedroomAbvGr','TotRmsAbvGrd',\n                'Fireplaces','GarageType','GarageYrBlt', 'GarageFinish', 'GarageArea', 'WoodDeckSF',\n                'OpenPorchSF', 'MoSold', 'YrSold']]\nviz_2 = FeatureImportances(clf_2)\nviz_2.fit(x_train_2, y_train)\nviz_2 = FeatureImportances(clf_2)\nviz_2.fit(x_train_2, y_train)\nviz_2.poof()\n\nclf_3 = ExtraTreesClassifier()\nx_train_3 = df[['MSSubClass', 'LotFrontage', 'LotArea', 'LotShape', 'LotConfig',\n       'Neighborhood', 'OverallQual', 'OverallCond', 'YearBuilt',\n       'YearRemodAdd', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'MasVnrArea', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtUnfSF',\n       'TotalBsmtSF', 'HeatingQC', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n       'BsmtFullBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n       'GarageYrBlt', 'GarageFinish', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'MoSold', 'YrSold']]\nviz_3 = FeatureImportances(clf_3)\nviz_3.fit(x_train_3, y_train)\nviz_3 = FeatureImportances(clf_3)\nviz_3.fit(x_train_3, y_train)\nviz_3.poof()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will combine all the features to store it in 'df_train' DataFrame.","metadata":{}},{"cell_type":"code","source":"df_train = df[['Id','MSSubClass','LotFrontage','LotArea','LotShape','LotConfig', 'Neighborhood','HouseStyle',\n        'Condition1','OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd','RoofStyle',\n        'Exterior1st', 'Exterior2nd', 'MasVnrType','MasVnrArea', 'ExterQual', 'BsmtExposure','BsmtFinType1',\n        'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC','1stFlrSF', '2ndFlrSF', 'GrLivArea',\n        'FullBath', 'BsmtFullBath','BedroomAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional','Fireplaces',\n        'GarageType','GarageYrBlt', 'GarageCars', 'GarageFinish', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n        'ScreenPorch','MoSold', 'YrSold','SalePrice']] \n# Included 'Id' and 'SalePrice' to complete the working feature set for training ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** Though some features ('3SsnPorch', 'PoolArea', 'MiscVal') have been selected as important through the selection classifiers, they are not considered here (because they have unary values). Features having unary values are redundant and they should be discarded. 'Condition2' is discarded because it has very low importance (almost close to zero).","metadata":{}},{"cell_type":"markdown","source":"Now, we will be checking correlation among the Selected Features. If we observe very high correlation (0.95 - 1.00) between any pair of selected features, we will discard one and keep one (from that pair). This step is required to avoid **\"Multicollinearity\"**.   ","metadata":{}},{"cell_type":"code","source":"# Check correlation matrix for the selected features\nimport numpy as np\nimport matplotlib.pyplot as plt\nmask = np.zeros_like(df_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(20,20))\nsns.heatmap(df_train.corr(), mask = mask, vmin = -1, annot = False, cmap = 'RdYlGn')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"High correlation has been observed for the following pair of features:\n* 'Exterior1st' and 'Exterior2nd'\n* 'TotalBsmtSF' and '1stFlrSF'\n* 'GrLivArea' and 'TotRmsAbvGrd'\n* 'OverallQual' and 'SalePrice'\n* 'GarageYrBlt' and 'YearBuilt'\nHowever, in none of these pairs, correlation is as high as > 0.95, so we are not discarding any variable and keeping every selected feature in 'df_train' intact.","metadata":{}},{"cell_type":"markdown","source":"Now, we will check **skewness** of the selected variables if any variable is **deviating from normality**. ","metadata":{}},{"cell_type":"code","source":"# Checking skewness of the selected features. If highly skewed, we should either discard or transform it.\nskewness_of_features=[]\nfor col in df_train:\n        skewness_of_features.append(df_train[col].skew())\nprint(skewness_of_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"'LotArea' is highly skewed variable having skewness values > 10. But discarding it altogether would be tricky. Earlier, while plotting Feature Importance, we observed that this feature has very high importance. ","metadata":{}},{"cell_type":"code","source":"# Feature Transformation\n# Checking if log-transformation can reduce its skewness\ndf_train['LotArea']=np.log(df_train['LotArea'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if skewness have been closer to zero now. If yes, then normality is restored.\nprint(df_train['LotArea'].skew())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting into train (selected features less target var) and test (target var) DataFrames:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nx_train = df_train[['Id','MSSubClass','LotFrontage','LotArea','LotShape','LotConfig', 'Neighborhood','HouseStyle',\n        'Condition1','OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd','RoofStyle',\n        'Exterior1st', 'Exterior2nd', 'MasVnrType','MasVnrArea', 'ExterQual', 'BsmtExposure','BsmtFinType1',\n        'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC','1stFlrSF', '2ndFlrSF', 'GrLivArea',\n        'FullBath', 'BsmtFullBath','BedroomAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional','Fireplaces',\n        'GarageType','GarageYrBlt', 'GarageCars', 'GarageFinish', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n        'ScreenPorch', 'MoSold', 'YrSold']] ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = df_train['SalePrice']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extensive Hyper-Parameter tuning using Hyperopt Library**\n\nI am using XGBoost regressor model for finally predicting the house prices. Here, for quick and effective hyper-parameter tuning, I am using Hyperopt library. Hyperopt library uses TPE algorithm (Tree of Parzen Estimators).","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom hyperopt import hp, tpe, fmin\nfrom sklearn.model_selection import cross_val_score\n\nvalgrid = {'n_estimators':hp.quniform('n_estimators', 1500, 2500, 25),\n         'gamma':hp.uniform('gamma', 0.01, 0.05),\n         'base_score':hp.uniform('base_score',0.6,0.9),\n         'learning_rate':hp.uniform('learning_rate', 0.00001, 0.03),\n         'max_depth':hp.quniform('max_depth', 3,8,1),\n         'subsample':hp.uniform('subsample', 0.50, 0.95),\n         'colsample_bytree':hp.uniform('colsample_bytree', 0.50, 0.95),\n         'colsample_bylevel':hp.uniform('colsample_bylevel', 0.50, 0.95),\n         'colsample_bynode':hp.uniform('colsample_bynode', 0.50, 0.95),\n         'reg_lambda':hp.uniform('reg_lambda', 1, 20)\n        }\n\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']),\n             'gamma': params['gamma'],\n             'base_score': params['base_score'],\n             'learning_rate': params['learning_rate'],\n             'max_depth': int(params['max_depth']),\n             'subsample': params['subsample'],\n             'colsample_bytree': params['colsample_bytree'],\n             'colsample_bylevel': params['colsample_bylevel'],\n             'colsample_bynode': params['colsample_bynode'],  \n             'reg_lambda': params['reg_lambda']}\n    \n    xb_a= xgb.XGBRegressor(**params)\n    score = cross_val_score(xb_a, x_train, y_train, scoring='neg_mean_squared_error', cv=5, n_jobs=-1).mean()\n    return -score\n\nbestP = fmin(fn= objective, space= valgrid, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Storing best hyper-parameters for XGB Regresssor into \"bestP\" DataFrame","metadata":{}},{"cell_type":"code","source":"print(bestP)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we will perform the same steps for Test Dataset as well","metadata":{}},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\ntotal = df_test[['MSSubClass','LotFrontage','LotArea','LotShape','LotConfig', 'Neighborhood','HouseStyle',\n        'Condition1','OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd','RoofStyle',\n        'Exterior1st', 'Exterior2nd', 'MasVnrType','MasVnrArea', 'ExterQual', 'BsmtExposure','BsmtFinType1',\n        'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC','1stFlrSF', '2ndFlrSF', 'GrLivArea',\n        'FullBath', 'BsmtFullBath','BedroomAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional','Fireplaces',\n        'GarageType','GarageYrBlt', 'GarageCars', 'GarageFinish', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n        'ScreenPorch', 'MoSold', 'YrSold']].isnull().sum().sort_values(ascending=False)\n\n# Missing Value analysis for Test Data\npercent = (df_test.isnull().sum()/df_test.isnull().count()).sort_values(ascending=False)\nmissing_test = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_test.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing Value imputation for Test Data\ndf_test['LotFrontage']=df_test['LotFrontage'].fillna(df_test['LotFrontage'].mean())\ndf_test['GarageFinish']=df_test['GarageFinish'].fillna(df_test['GarageFinish'].mode()[0])\ndf_test['GarageYrBlt']=df_test['GarageYrBlt'].fillna(df_test['GarageYrBlt'].mean())\ndf_test['GarageType']=df_test['GarageType'].fillna(df_test['GarageType'].mode()[0])\ndf_test['BsmtExposure']=df_test['BsmtExposure'].fillna(df_test['BsmtExposure'].mode()[0])\ndf_test['BsmtFinType1']=df_test['BsmtFinType1'].fillna(df_test['BsmtFinType1'].mode()[0])\ndf_test['MasVnrType']=df_test['MasVnrType'].fillna(df_test['MasVnrType'].mode()[0])\ndf_test['MasVnrArea']=df_test['MasVnrArea'].fillna(df_test['MasVnrArea'].mean()) \ndf_test['BsmtFullBath']=df_test['BsmtFullBath'].fillna(df_test['BsmtFullBath'].mode()[0]) #Imp.\ndf_test['Functional']=df_test['Functional'].fillna(df_test['Functional'].mode()[0])\ndf_test['KitchenQual']=df_test['KitchenQual'].fillna(df_test['KitchenQual'].mode()[0])\ndf_test['GarageArea']=df_test['GarageArea'].fillna(df_test['GarageArea'].mean())\ndf_test['BsmtUnfSF']=df_test['BsmtUnfSF'].fillna(df_test['BsmtUnfSF'].mean())\ndf_test['Exterior2nd']=df_test['Exterior2nd'].fillna(df_test['Exterior2nd'].mode()[0])\ndf_test['Exterior1st']=df_test['Exterior1st'].fillna(df_test['Exterior1st'].mode()[0])\ndf_test['BsmtFinSF1']=df_test['BsmtFinSF1'].fillna(df_test['BsmtFinSF1'].mean())\ndf_test['BsmtFinSF2']=df_test['BsmtFinSF2'].fillna(df_test['BsmtFinSF2'].mean())\ndf_test['TotalBsmtSF']=df_test['TotalBsmtSF'].fillna(df_test['TotalBsmtSF'].mean())\ndf_test['GarageCars']=df_test['GarageCars'].fillna(df_test['GarageCars'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** 'BsmtFullBath' is a binary variable, so it has been imputed with mode values instead of mean values.","metadata":{}},{"cell_type":"code","source":"df_test['LotArea']=np.log(df_test['LotArea'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# One-Hot encoding to convert categ. vars to Numeric vars for Test Data\nfrom sklearn.preprocessing import LabelEncoder\ndf_test_work = df_test[['Id','MSSubClass','LotFrontage','LotArea','LotShape','LotConfig', 'Neighborhood','HouseStyle',\n        'Condition1','OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd','RoofStyle',\n        'Exterior1st', 'Exterior2nd', 'MasVnrType','MasVnrArea', 'ExterQual', 'BsmtExposure','BsmtFinType1',\n        'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF', 'TotalBsmtSF', 'HeatingQC','1stFlrSF', '2ndFlrSF', 'GrLivArea',\n        'FullBath', 'BsmtFullBath','BedroomAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional','Fireplaces',\n        'GarageType','GarageYrBlt', 'GarageCars', 'GarageFinish', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n        'ScreenPorch', 'MoSold', 'YrSold']]\n\nlencoders_test = {}\nfor colt in df_test_work.select_dtypes(include=['object']).columns:\n    lencoders_test[colt] = LabelEncoder()\n    df_test_work[colt] = lencoders_test[colt].fit_transform(df_test_work[colt])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction using XGB model having the best set of hyperparameters\nimport pandas as pd_out\nimport xgboost \nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nreg_final = xgboost.XGBRegressor(base_score=bestP['base_score'], \n            colsample_bylevel=bestP['colsample_bylevel'], colsample_bynode=bestP['colsample_bynode'], \n            colsample_bytree=bestP['colsample_bytree'], #eval_metric='rmse', \n            gamma=bestP['gamma'], learning_rate=bestP['learning_rate'],\n            max_depth=int(bestP['max_depth']), n_estimators=int(bestP['n_estimators']), \n            #n_jobs=0,num_parallel_tree=1, objective='reg:squarederror', \n            random_state=42, reg_lambda=bestP['reg_lambda'], subsample=bestP['subsample'])\n#X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=123)\n#reg_final.fit(X_train,Y_train, early_stopping_rounds=5, eval_set=[(X_test, Y_test)], verbose=False)\nreg_final.fit(x_train,y_train)\ny_pred = reg_final.predict(df_test_work)\nprediction = pd_out.DataFrame(y_pred)\noutput = pd_out.concat([df_test_work['Id'],prediction], axis=1)\noutput.columns=['Id','SalePrice']\noutput.to_csv('HousePrice_submission.csv', index=False)\nprint(\"Submission successful\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}