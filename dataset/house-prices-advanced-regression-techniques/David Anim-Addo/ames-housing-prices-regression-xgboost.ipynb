{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ames Housing Prices Regression - XGBoost","metadata":{}},{"cell_type":"markdown","source":"# 1. Introduction\n\nPresented here is a data science challenge featuring the Ames Housing Prices [dataset](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt). This dataset contains information related to the price of houses in Ames, Iowa, U.S.A. The goal is to use regression to estimate the price of unlabeled houses. The presence of different numerical and categorical data types within the set make it a good exercise for exploratory data analysis (EDA).  \n\nIn this notebook, the data types will be split into the following categories: \n* Numerical\n    * Continuous\n    * Discrete \n* Categorical\n    * Nominal \n    * Ordinal \n\nThese will be examined using statistical methods and combined into a final dataset for regression training. The regression models to be evaluated here are: \n1. Ridge Regression\n2. Random Forest Regression\n3. XGBoost Regression","metadata":{}},{"cell_type":"markdown","source":"## 1.1 Import Libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-28T03:35:47.823478Z","iopub.execute_input":"2021-08-28T03:35:47.823835Z","iopub.status.idle":"2021-08-28T03:35:47.834226Z","shell.execute_reply.started":"2021-08-28T03:35:47.823784Z","shell.execute_reply":"2021-08-28T03:35:47.833145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import mean_squared_log_error, r2_score\nfrom statistics import mean, mode, median\nimport statsmodels.api as sma\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy.stats import kurtosis\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nfrom matplotlib import rcParams\nfrom platform import python_version\nprint('Python Version: ', python_version())","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:47.835917Z","iopub.execute_input":"2021-08-28T03:35:47.836302Z","iopub.status.idle":"2021-08-28T03:35:47.845478Z","shell.execute_reply.started":"2021-08-28T03:35:47.836264Z","shell.execute_reply":"2021-08-28T03:35:47.844532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2 Configure Settings","metadata":{}},{"cell_type":"code","source":"RANDOM_SEED = 123\nrcParams['figure.figsize'] = (10, 6)\nsns.set_theme(palette='muted', style='whitegrid')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:47.847369Z","iopub.execute_input":"2021-08-28T03:35:47.848163Z","iopub.status.idle":"2021-08-28T03:35:47.85727Z","shell.execute_reply.started":"2021-08-28T03:35:47.848123Z","shell.execute_reply":"2021-08-28T03:35:47.856503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Load the Training and Test Files","metadata":{}},{"cell_type":"code","source":"path = '../input/house-prices-advanced-regression-techniques/train.csv'\ndf = pd.read_csv(path)\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:47.86067Z","iopub.execute_input":"2021-08-28T03:35:47.860979Z","iopub.status.idle":"2021-08-28T03:35:47.91918Z","shell.execute_reply.started":"2021-08-28T03:35:47.860934Z","shell.execute_reply":"2021-08-28T03:35:47.917978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path_test = '../input/house-prices-advanced-regression-techniques/test.csv'\ndf_test = pd.read_csv(path_test)\nprint(df_test.shape)\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:47.921197Z","iopub.execute_input":"2021-08-28T03:35:47.921602Z","iopub.status.idle":"2021-08-28T03:35:47.979756Z","shell.execute_reply.started":"2021-08-28T03:35:47.921561Z","shell.execute_reply":"2021-08-28T03:35:47.978716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's investigate the types of data contained within the training and test sets. You can see they contain a combination of objects, integers, and float values. This will be important when we separate the data into categories.","metadata":{}},{"cell_type":"code","source":"print(df.dtypes.value_counts(), end='\\n' * 2)\nprint(df_test.dtypes.value_counts())","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:47.981208Z","iopub.execute_input":"2021-08-28T03:35:47.981609Z","iopub.status.idle":"2021-08-28T03:35:47.990701Z","shell.execute_reply.started":"2021-08-28T03:35:47.981569Z","shell.execute_reply":"2021-08-28T03:35:47.989579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. EDA of Target Feature","metadata":{}},{"cell_type":"code","source":"# Check for null values in the target\ndf['SalePrice'].isnull().any()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:47.992483Z","iopub.execute_input":"2021-08-28T03:35:47.992976Z","iopub.status.idle":"2021-08-28T03:35:48.002011Z","shell.execute_reply.started":"2021-08-28T03:35:47.992936Z","shell.execute_reply":"2021-08-28T03:35:48.000869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['SalePrice'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:48.00396Z","iopub.execute_input":"2021-08-28T03:35:48.00449Z","iopub.status.idle":"2021-08-28T03:35:48.017844Z","shell.execute_reply.started":"2021-08-28T03:35:48.004449Z","shell.execute_reply":"2021-08-28T03:35:48.016522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we see the `SalePrice` descriptive statistics. The mean is greater than the median which can indicate a positive or right skewed kurtosis. We can confirm this by visualizing the data. ","metadata":{}},{"cell_type":"code","source":"print(f'Kurtosis: {kurtosis(df.SalePrice)}')\nax = sns.histplot(data=df, x='SalePrice', kde=True)\nax.xaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:48.02148Z","iopub.execute_input":"2021-08-28T03:35:48.021949Z","iopub.status.idle":"2021-08-28T03:35:48.351468Z","shell.execute_reply.started":"2021-08-28T03:35:48.021902Z","shell.execute_reply":"2021-08-28T03:35:48.350529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The histogram plot confirms our expectation. This skewed distribution can decline model performance, but a log transformation may reduce kurtosis and improve model performance.  ","metadata":{}},{"cell_type":"code","source":"print(f'Kurtosis: {kurtosis(np.log(df.SalePrice))}')\nsns.histplot(data=df, x=np.log(df['SalePrice']), kde=True)\nplt.xlabel('SalePrice (Log Scale)')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:48.353363Z","iopub.execute_input":"2021-08-28T03:35:48.353751Z","iopub.status.idle":"2021-08-28T03:35:48.96134Z","shell.execute_reply.started":"2021-08-28T03:35:48.353714Z","shell.execute_reply":"2021-08-28T03:35:48.960351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The log transformation was effective at reducing kurtosis in the `SalePrice`.  We will use this later when the models are evaluated. Our next task is to assess any missing values in the rest of the dataset.","metadata":{}},{"cell_type":"markdown","source":"# 3. Data Cleaning of Train and Test Sets\n\nWe'll set up a small function to return null values from our training and test data.","metadata":{}},{"cell_type":"code","source":"def null_table(data):    \n    null_list = []\n\n    for i in data:\n        if data[i].isnull().any():\n            null_list.append(data[i].isnull().value_counts())\n    \n    return pd.DataFrame(pd.concat(null_list, axis=1).T)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:48.962759Z","iopub.execute_input":"2021-08-28T03:35:48.96313Z","iopub.status.idle":"2021-08-28T03:35:48.968153Z","shell.execute_reply.started":"2021-08-28T03:35:48.963092Z","shell.execute_reply":"2021-08-28T03:35:48.96737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review null values for the training data\ndf_null = null_table(df)\nprint(df_null.shape)\ndf_null","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:48.969388Z","iopub.execute_input":"2021-08-28T03:35:48.969938Z","iopub.status.idle":"2021-08-28T03:35:49.021207Z","shell.execute_reply.started":"2021-08-28T03:35:48.969901Z","shell.execute_reply":"2021-08-28T03:35:49.020346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems there are many missing values, but a closer review of the variable description shows that here the null values have meaning. For example, categorical features describing the basement or garage will be null if the house has neither. However, this is not the case for all the categorical or numerical features. ","metadata":{}},{"cell_type":"code","source":"# The categorical features are filled with the most frequent value\ndf['Electrical'] = df.Electrical.fillna(mode(df.Electrical)) \ndf['MasVnrType'] = df.MasVnrType.fillna(mode(df.MasVnrType)) \n# The continuous feature is filled with the median value\ndf['MasVnrArea'] = df.MasVnrArea.fillna(median(df.MasVnrArea)) \n# The discrete feature is filled with zeroes where there is no garage\ndf['GarageYrBlt'] = df.GarageYrBlt.fillna(0).astype(int) ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:49.022327Z","iopub.execute_input":"2021-08-28T03:35:49.022698Z","iopub.status.idle":"2021-08-28T03:35:49.031719Z","shell.execute_reply.started":"2021-08-28T03:35:49.022661Z","shell.execute_reply":"2021-08-28T03:35:49.030765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Review null values for the test data\ndf_test_null = null_table(df_test)\nprint(df_test_null.shape)\ndf_test_null","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:49.033148Z","iopub.execute_input":"2021-08-28T03:35:49.033584Z","iopub.status.idle":"2021-08-28T03:35:49.094992Z","shell.execute_reply.started":"2021-08-28T03:35:49.033492Z","shell.execute_reply":"2021-08-28T03:35:49.094053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the test data there are more features where the null values are *not* meaningful, so we need to fill these missing entries. The test data will be cleaned using the same method as the training data.","metadata":{}},{"cell_type":"code","source":"# The categorical features are filled with the most frequent value\ndf_test['MSZoning'] = df_test.MSZoning.fillna(mode(pd.concat([df['MSZoning'], df_test['MSZoning']], axis=0)))\ndf_test['Utilities'] = df_test.Utilities.fillna(mode(pd.concat([df['Utilities'], df_test['Utilities']], axis=0)))\ndf_test['Exterior1st'] = df_test.Exterior1st.fillna(mode(pd.concat([df['Exterior1st'], df_test['Exterior1st']], axis=0)))\ndf_test['Exterior2nd'] = df_test.Exterior2nd.fillna(mode(pd.concat([df['Exterior2nd'], df_test['Exterior2nd']], axis=0)))\ndf_test['MasVnrType'] = df_test.MasVnrType.fillna(mode(pd.concat([df['MasVnrType'], df_test['MasVnrType']], axis=0)))\ndf_test['BsmtFullBath'] = df_test.BsmtFullBath.fillna(mode(pd.concat([df['BsmtFullBath'], df_test['BsmtFullBath']], axis=0)))\ndf_test['BsmtHalfBath'] = df_test.BsmtHalfBath.fillna(mode(pd.concat([df['BsmtHalfBath'], df_test['BsmtHalfBath']], axis=0)))\ndf_test['KitchenQual'] = df_test.KitchenQual.fillna(mode(pd.concat([df['KitchenQual'], df_test['KitchenQual']], axis=0)))\ndf_test['Functional'] = df_test.Functional.fillna(mode(pd.concat([df['Functional'], df_test['Functional']], axis=0)))\ndf_test['SaleType'] = df_test.SaleType.fillna(mode(pd.concat([df['SaleType'], df_test['SaleType']], axis=0)))\ndf_test['GarageCars'] = df_test.GarageCars.fillna(mode(pd.concat([df['GarageCars'], df_test['GarageCars']], axis=0)))\n# The continuous features are filled with the median value\ndf_test['GarageArea'] = df_test.GarageArea.fillna(median(pd.concat([df['GarageArea'], df_test['GarageArea']], axis=0)))\ndf_test['MasVnrArea'] = df_test.MasVnrArea.fillna(median(pd.concat([df['MasVnrArea'], df_test['MasVnrArea']], axis=0)))\ndf_test['BsmtFinSF1'] = df_test.BsmtFinSF1.fillna(median(pd.concat([df['BsmtFinSF1'], df_test['BsmtFinSF1']], axis=0)))\ndf_test['BsmtFinSF2'] = df_test.BsmtFinSF2.fillna(median(pd.concat([df['BsmtFinSF2'], df_test['BsmtFinSF2']], axis=0)))\ndf_test['BsmtUnfSF'] = df_test.BsmtUnfSF.fillna(median(pd.concat([df['BsmtUnfSF'], df_test['BsmtUnfSF']], axis=0)))\ndf_test['TotalBsmtSF'] = df_test.TotalBsmtSF.fillna(median(pd.concat([df['TotalBsmtSF'], df_test['TotalBsmtSF']], axis=0)))\n# The discrete feature is filled with zeroes where there is no garage\ndf_test['GarageYrBlt'] = df_test.GarageYrBlt.fillna(0).astype(int) ","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:49.096313Z","iopub.execute_input":"2021-08-28T03:35:49.096657Z","iopub.status.idle":"2021-08-28T03:35:49.140058Z","shell.execute_reply.started":"2021-08-28T03:35:49.096624Z","shell.execute_reply":"2021-08-28T03:35:49.139292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(null_table(df).shape)\nprint(null_table(df_test).shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:49.142151Z","iopub.execute_input":"2021-08-28T03:35:49.142489Z","iopub.status.idle":"2021-08-28T03:35:49.213484Z","shell.execute_reply.started":"2021-08-28T03:35:49.142463Z","shell.execute_reply":"2021-08-28T03:35:49.212424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to fill the meaningful null data in the categorical features. We'll name the category `NA`, since this is used in the variable description for houses lacking certain attributes.","metadata":{}},{"cell_type":"code","source":"# Fill NA data for categorical features\ndf[[\n    'Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n    'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n    'GarageType', 'GarageFinish', 'GarageQual', \n    'GarageCond', 'PoolQC', 'Fence', 'MiscFeature'\n]] = df[[\n    'Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n    'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n    'GarageType', 'GarageFinish', 'GarageQual', \n    'GarageCond', 'PoolQC', 'Fence', 'MiscFeature'\n]].fillna('NA')\n\ndf_test[[\n    'Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n    'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n    'GarageType', 'GarageFinish', 'GarageQual', \n    'GarageCond', 'PoolQC', 'Fence', 'MiscFeature'\n]] = df_test[[\n    'Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n    'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n    'GarageType', 'GarageFinish', 'GarageQual', \n    'GarageCond', 'PoolQC', 'Fence', 'MiscFeature'\n]].fillna('NA')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:49.214685Z","iopub.execute_input":"2021-08-28T03:35:49.215004Z","iopub.status.idle":"2021-08-28T03:35:49.236333Z","shell.execute_reply.started":"2021-08-28T03:35:49.21497Z","shell.execute_reply":"2021-08-28T03:35:49.235555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data cleaning stage is completed by dropping `LotFrontage`. Although clever imputation is possible here, there are other features that describe similar aspects of the housing property without a lot of missing entries.","metadata":{}},{"cell_type":"code","source":"df = df.drop(['LotFrontage'], axis=1)\ndf_test = df_test.drop(['LotFrontage'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:49.237504Z","iopub.execute_input":"2021-08-28T03:35:49.237855Z","iopub.status.idle":"2021-08-28T03:35:49.245682Z","shell.execute_reply.started":"2021-08-28T03:35:49.237819Z","shell.execute_reply":"2021-08-28T03:35:49.244736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll gather all the continuous features into one dataframe for analysis and do the same for the discrete, nominal, and ordinal feature types.","metadata":{}},{"cell_type":"markdown","source":"# 4. EDA of Continuous Numerical Features","metadata":{}},{"cell_type":"code","source":"# Create a dataset of only continuous data\ndf_continuous = df[[\n    'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', \n    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n    'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n    'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \n    'MiscVal'\n]]\n\ndf_test_continuous = df_test[[\n    'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', \n    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n    'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n    'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \n    'MiscVal'\n]]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:49.246874Z","iopub.execute_input":"2021-08-28T03:35:49.247271Z","iopub.status.idle":"2021-08-28T03:35:49.257549Z","shell.execute_reply.started":"2021-08-28T03:35:49.247233Z","shell.execute_reply":"2021-08-28T03:35:49.256734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_continuous.shape)\ndf_continuous.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:49.258821Z","iopub.execute_input":"2021-08-28T03:35:49.259244Z","iopub.status.idle":"2021-08-28T03:35:49.284086Z","shell.execute_reply.started":"2021-08-28T03:35:49.259205Z","shell.execute_reply":"2021-08-28T03:35:49.282647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test_continuous.shape)\ndf_test_continuous.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:49.290533Z","iopub.execute_input":"2021-08-28T03:35:49.290784Z","iopub.status.idle":"2021-08-28T03:35:49.315187Z","shell.execute_reply.started":"2021-08-28T03:35:49.29076Z","shell.execute_reply":"2021-08-28T03:35:49.314348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next task is  to visualize the continuous features using the median sale price for color coding. This can show us the importance of certain features in terms of high or low sale prices. Also the coefficient of determination (r-squared) will be calculated to complement our observations.","metadata":{}},{"cell_type":"code","source":"# Create a boolean feature for houses sold above the median price\ndf['PriceAbvMedian'] = df['SalePrice'].apply(lambda x: 0 if x < median(df.SalePrice) else 1)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:49.317533Z","iopub.execute_input":"2021-08-28T03:35:49.317779Z","iopub.status.idle":"2021-08-28T03:35:49.8585Z","shell.execute_reply.started":"2021-08-28T03:35:49.317756Z","shell.execute_reply":"2021-08-28T03:35:49.857533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot all continuous features\nplt.figure(figsize=(18, 40))\n\nfor i, j in enumerate(df_continuous.columns):\n    plt.subplot(10, 2, i + 1)\n    ax = sns.scatterplot(data=df, x=f'{j}', y='SalePrice', hue='PriceAbvMedian')\n    ax.yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))\n    plt.title(f'Sale Price vs. {j}')\n    # Plot the best fit line\n    m, b = np.polyfit(x=df[j], y=df.SalePrice, deg=1)\n    plt.plot(df[j], m * df[j] + b, c='red')\n    # Plot the r-squared value\n    corr_matrix = np.corrcoef(df[j], df.SalePrice)    \n    corr_xy = corr_matrix[0,1]\n    r_squared = round((corr_xy ** 2), 4)    \n    plt.legend(labels=[f'R^2 = {r_squared}', 'Price Above Median', 'Price Below Median'], framealpha=0.5, loc=0)\n    \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:49.863033Z","iopub.execute_input":"2021-08-28T03:35:49.864468Z","iopub.status.idle":"2021-08-28T03:35:55.583996Z","shell.execute_reply.started":"2021-08-28T03:35:49.864344Z","shell.execute_reply":"2021-08-28T03:35:55.583209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`GrLivArea` has the highest r-squared score. We'll plot a correlation table and sort by the `SalePrice` to better understand how these features rank in relation to the `SalePrice`. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5, 10))\nsns.heatmap(pd.DataFrame(pd.concat([df['SalePrice'], df_continuous], axis=1).corr()[['SalePrice']].sort_values(by=['SalePrice'], ascending=False)), annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:55.585232Z","iopub.execute_input":"2021-08-28T03:35:55.585725Z","iopub.status.idle":"2021-08-28T03:35:55.965468Z","shell.execute_reply.started":"2021-08-28T03:35:55.585689Z","shell.execute_reply":"2021-08-28T03:35:55.964646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A function is created to return a kurtosis table for each feature. We can understand the skewness of each feature which will help us decide which scaling method to use.","metadata":{}},{"cell_type":"code","source":"def get_kurtosis(data):\n    df_kurt = pd.DataFrame()\n    df_kurt['Variable'] = data.columns\n\n    for i in data.columns:\n        df_kurt['Kurtosis'] = [kurtosis(data[f'{i}']) for i in data.columns]\n\n    return df_kurt.sort_values(by=['Kurtosis'], ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:55.966793Z","iopub.execute_input":"2021-08-28T03:35:55.967162Z","iopub.status.idle":"2021-08-28T03:35:55.974843Z","shell.execute_reply.started":"2021-08-28T03:35:55.967123Z","shell.execute_reply":"2021-08-28T03:35:55.974054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_kurtosis(df_continuous)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:55.976354Z","iopub.execute_input":"2021-08-28T03:35:55.976757Z","iopub.status.idle":"2021-08-28T03:35:56.084602Z","shell.execute_reply.started":"2021-08-28T03:35:55.976721Z","shell.execute_reply":"2021-08-28T03:35:56.083555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kurtosis is high in features such as `MiscVal` and `LotAreaa`, but low in `BsmtUnfSF`. We'll move on to the next feature group and see what we can learn about the data.","metadata":{}},{"cell_type":"markdown","source":"# 5. EDA of Discrete Numerical Features","metadata":{}},{"cell_type":"code","source":"# Create a dataset of only discrete data\ndf_discrete = df[[\n    'YearBuilt', 'YearRemodAdd', 'BsmtFullBath', 'BsmtHalfBath', \n    'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n    'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageYrBlt', \n    'MoSold', 'YrSold'\n]]\n\ndf_test_discrete = df_test[[\n    'YearBuilt', 'YearRemodAdd', 'BsmtFullBath', 'BsmtHalfBath', \n    'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n    'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageYrBlt', \n    'MoSold', 'YrSold'\n]]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:56.086079Z","iopub.execute_input":"2021-08-28T03:35:56.08648Z","iopub.status.idle":"2021-08-28T03:35:56.095461Z","shell.execute_reply.started":"2021-08-28T03:35:56.086425Z","shell.execute_reply":"2021-08-28T03:35:56.094532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_discrete.shape)\ndf_discrete.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:56.096822Z","iopub.execute_input":"2021-08-28T03:35:56.097273Z","iopub.status.idle":"2021-08-28T03:35:56.115387Z","shell.execute_reply.started":"2021-08-28T03:35:56.097234Z","shell.execute_reply":"2021-08-28T03:35:56.114495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test_discrete.shape)\ndf_test_discrete.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:56.116622Z","iopub.execute_input":"2021-08-28T03:35:56.117005Z","iopub.status.idle":"2021-08-28T03:35:56.136418Z","shell.execute_reply.started":"2021-08-28T03:35:56.116967Z","shell.execute_reply":"2021-08-28T03:35:56.135505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use the median price color coding again, this time with a histogram to identify any trends in the discrete data.","metadata":{}},{"cell_type":"code","source":"# Plot all discrete features\nplt.figure(figsize=(30, 18))\n\nfor i, j in enumerate(df_discrete.columns):\n    count, bin_edges = np.histogram(df[f'{j}']) # Acquire bin edges\n    plt.subplot(3, 5, i + 1)\n    sns.histplot(data=df, x=f'{j}', bins=bin_edges, hue='PriceAbvMedian')\n    plt.title(f'Count vs. {j}')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:35:56.137737Z","iopub.execute_input":"2021-08-28T03:35:56.13811Z","iopub.status.idle":"2021-08-28T03:36:00.604739Z","shell.execute_reply.started":"2021-08-28T03:35:56.138074Z","shell.execute_reply":"2021-08-28T03:36:00.603847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 10))\nsns.heatmap(pd.DataFrame(pd.concat([df['SalePrice'], df_discrete], axis=1).corr()[['SalePrice']].sort_values(by=['SalePrice'], ascending=False)), annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:00.605979Z","iopub.execute_input":"2021-08-28T03:36:00.606489Z","iopub.status.idle":"2021-08-28T03:36:01.114609Z","shell.execute_reply.started":"2021-08-28T03:36:00.606447Z","shell.execute_reply":"2021-08-28T03:36:01.113726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_kurtosis(df_discrete)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:01.118634Z","iopub.execute_input":"2021-08-28T03:36:01.120713Z","iopub.status.idle":"2021-08-28T03:36:01.218952Z","shell.execute_reply.started":"2021-08-28T03:36:01.120672Z","shell.execute_reply":"2021-08-28T03:36:01.218139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. EDA of Nominal Categorical Features","metadata":{}},{"cell_type":"code","source":"# Create a dataset for only nominal data\ndf_nominal = df[[\n    'Alley', 'GarageType', 'MiscFeature',\n    'MSZoning', 'MSSubClass', 'Street', 'LandContour',\n    'LotConfig', 'Neighborhood', 'Condition1', 'Condition2',\n    'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n    'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n    'Heating', 'CentralAir', 'SaleType', 'SaleCondition'    \n]]\n\ndf_test_nominal = df_test[[\n    'Alley', 'GarageType', 'MiscFeature',\n    'MSZoning', 'MSSubClass', 'Street', 'LandContour',\n    'LotConfig', 'Neighborhood', 'Condition1', 'Condition2',\n    'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n    'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n    'Heating', 'CentralAir', 'SaleType', 'SaleCondition'    \n]]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:01.22282Z","iopub.execute_input":"2021-08-28T03:36:01.224948Z","iopub.status.idle":"2021-08-28T03:36:01.236871Z","shell.execute_reply.started":"2021-08-28T03:36:01.224907Z","shell.execute_reply":"2021-08-28T03:36:01.235952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_nominal.shape)\ndf_nominal.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:01.241472Z","iopub.execute_input":"2021-08-28T03:36:01.244133Z","iopub.status.idle":"2021-08-28T03:36:01.2858Z","shell.execute_reply.started":"2021-08-28T03:36:01.244092Z","shell.execute_reply":"2021-08-28T03:36:01.28494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test_nominal.shape)\ndf_test_nominal.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:01.289631Z","iopub.execute_input":"2021-08-28T03:36:01.291827Z","iopub.status.idle":"2021-08-28T03:36:01.334955Z","shell.execute_reply.started":"2021-08-28T03:36:01.291787Z","shell.execute_reply":"2021-08-28T03:36:01.334095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The nominal features are analyzed using a boxplot.  This will give us information on the skewness, range, and median values of the feature categories.","metadata":{}},{"cell_type":"code","source":"# Plot all nominal features\nplt.figure(figsize=(15, 60))\n\nfor i, j in enumerate(df_nominal.columns):\n    plt.subplot(12, 2, i + 1)\n    ax = sns.boxplot(data=df, x=f'{j}', y=df.SalePrice)\n    ax.yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))\n    plt.xticks(rotation=45)\n    plt.title(f'SalePrice vs. {j}')\n    \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:01.339042Z","iopub.execute_input":"2021-08-28T03:36:01.341382Z","iopub.status.idle":"2021-08-28T03:36:09.320365Z","shell.execute_reply.started":"2021-08-28T03:36:01.341338Z","shell.execute_reply":"2021-08-28T03:36:09.319405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Transformation of the nominal data will be done by ordinal encoding. This dataset already contains a number of features that describe similar elements of the housing property. \n\nOne-Hot-Encoding could worsen this problem by adding redundant features to the dataset. This could increase [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) and make it more difficult to determine the effects of individual features on the target variable. ","metadata":{}},{"cell_type":"code","source":"# Encode the nominal data into numerical data\nencoder_nominal = OrdinalEncoder()\ndf_nominal_ord = pd.DataFrame(encoder_nominal.fit_transform(df_nominal))\ndf_nominal_ord.columns = [\n    'Alley', 'GarageType', 'MiscFeature',\n    'MSSubClass', 'MSZoning', 'Street', 'LandContour',\n    'LotConfig', 'Neighborhood', 'Condition1', 'Condition2',\n    'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n    'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n    'Heating', 'CentralAir', 'SaleType', 'SaleCondition'\n]  \n\nencoder_nominal_test = OrdinalEncoder()\ndf_test_nominal_ord = pd.DataFrame(encoder_nominal_test.fit_transform(df_test_nominal))\ndf_test_nominal_ord.columns = [\n    'Alley', 'GarageType', 'MiscFeature',\n    'MSSubClass', 'MSZoning', 'Street', 'LandContour',\n    'LotConfig', 'Neighborhood', 'Condition1', 'Condition2',\n    'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n    'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n    'Heating', 'CentralAir', 'SaleType', 'SaleCondition'\n]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:09.32195Z","iopub.execute_input":"2021-08-28T03:36:09.322289Z","iopub.status.idle":"2021-08-28T03:36:09.380482Z","shell.execute_reply.started":"2021-08-28T03:36:09.322255Z","shell.execute_reply":"2021-08-28T03:36:09.379693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_nominal_ord.shape)\ndf_nominal_ord.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:09.381774Z","iopub.execute_input":"2021-08-28T03:36:09.38215Z","iopub.status.idle":"2021-08-28T03:36:09.420677Z","shell.execute_reply.started":"2021-08-28T03:36:09.382112Z","shell.execute_reply":"2021-08-28T03:36:09.419599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test_nominal_ord.shape)\ndf_test_nominal_ord.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:09.422787Z","iopub.execute_input":"2021-08-28T03:36:09.423216Z","iopub.status.idle":"2021-08-28T03:36:09.460822Z","shell.execute_reply.started":"2021-08-28T03:36:09.423175Z","shell.execute_reply":"2021-08-28T03:36:09.459732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 10))\nsns.heatmap(pd.DataFrame(pd.concat([df['SalePrice'], df_nominal_ord], axis=1).corr()[['SalePrice']].sort_values(by=['SalePrice'], ascending=False)), annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:09.46238Z","iopub.execute_input":"2021-08-28T03:36:09.462784Z","iopub.status.idle":"2021-08-28T03:36:10.038299Z","shell.execute_reply.started":"2021-08-28T03:36:09.462745Z","shell.execute_reply":"2021-08-28T03:36:10.037442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_kurtosis(df_nominal_ord)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:10.039657Z","iopub.execute_input":"2021-08-28T03:36:10.040041Z","iopub.status.idle":"2021-08-28T03:36:10.202023Z","shell.execute_reply.started":"2021-08-28T03:36:10.040002Z","shell.execute_reply":"2021-08-28T03:36:10.201013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. EDA of Ordinal Categorical Features","metadata":{}},{"cell_type":"code","source":"# Create a dataset of only ordinal data\ndf_ordinal = df[[\n    'BsmtQual', 'BsmtCond', 'BsmtExposure', \n    'GarageFinish', 'GarageQual', 'GarageCond',\n    'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n    'LotShape', 'Utilities', 'LandSlope', \n    'OverallQual', 'OverallCond', 'ExterQual', \n    'ExterCond', 'HeatingQC', 'Electrical', \n    'KitchenQual', 'Functional', 'PavedDrive', \n    'PoolQC', 'Fence'\n]]\n\ndf_test_ordinal = df_test[[\n    'BsmtQual', 'BsmtCond', 'BsmtExposure', \n    'GarageFinish', 'GarageQual', 'GarageCond',\n    'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n    'LotShape', 'Utilities', 'LandSlope', \n    'OverallQual', 'OverallCond', 'ExterQual', \n    'ExterCond', 'HeatingQC', 'Electrical', \n    'KitchenQual', 'Functional', 'PavedDrive', \n    'PoolQC', 'Fence'\n]]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:10.203617Z","iopub.execute_input":"2021-08-28T03:36:10.203985Z","iopub.status.idle":"2021-08-28T03:36:10.213363Z","shell.execute_reply.started":"2021-08-28T03:36:10.203946Z","shell.execute_reply":"2021-08-28T03:36:10.212422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_ordinal.shape)\ndf_ordinal.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:10.214878Z","iopub.execute_input":"2021-08-28T03:36:10.215271Z","iopub.status.idle":"2021-08-28T03:36:10.246612Z","shell.execute_reply.started":"2021-08-28T03:36:10.215213Z","shell.execute_reply":"2021-08-28T03:36:10.245516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test_ordinal.shape)\ndf_test_ordinal.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:10.248292Z","iopub.execute_input":"2021-08-28T03:36:10.248771Z","iopub.status.idle":"2021-08-28T03:36:10.277809Z","shell.execute_reply.started":"2021-08-28T03:36:10.248731Z","shell.execute_reply":"2021-08-28T03:36:10.276719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot all ordinal features\nplt.figure(figsize=(15, 60))\n\nfor i, j in enumerate(df_ordinal.columns):\n    plt.subplot(12, 2, i + 1)\n    ax = sns.boxplot(data=df, x=f'{j}', y=df.SalePrice)\n    ax.yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))\n    plt.xticks(rotation=45)\n    plt.title(f'SalePrice vs. {j}')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:10.279335Z","iopub.execute_input":"2021-08-28T03:36:10.279906Z","iopub.status.idle":"2021-08-28T03:36:16.33736Z","shell.execute_reply.started":"2021-08-28T03:36:10.279867Z","shell.execute_reply":"2021-08-28T03:36:16.336367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use the `OrdinalEncoder` again to transform the ordinal data. This will make it so the model can understand degrees of order or class among the feature variables. ","metadata":{}},{"cell_type":"code","source":"# Encode the ordinal data into numerical data\nencoder = OrdinalEncoder()\ndf_ordinal = pd.DataFrame(encoder.fit_transform(df_ordinal))\ndf_ordinal.columns = [    \n    'BsmtQual', 'BsmtCond', 'BsmtExposure', \n    'GarageFinish', 'GarageQual', 'GarageCond',\n    'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n    'LotShape', 'Utilities', 'LandSlope', \n    'OverallQual', 'OverallCond', 'ExterQual', \n    'ExterCond', 'HeatingQC', 'Electrical', \n    'KitchenQual', 'Functional', 'PavedDrive', \n    'PoolQC', 'Fence'\n]\n\nencoder_test = OrdinalEncoder()\ndf_test_ordinal = pd.DataFrame(encoder_test.fit_transform(df_test_ordinal))\ndf_test_ordinal.columns = [    \n    'BsmtQual', 'BsmtCond', 'BsmtExposure', \n    'GarageFinish', 'GarageQual', 'GarageCond',\n    'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n    'LotShape', 'Utilities', 'LandSlope', \n    'OverallQual', 'OverallCond', 'ExterQual', \n    'ExterCond', 'HeatingQC', 'Electrical', \n    'KitchenQual', 'Functional', 'PavedDrive', \n    'PoolQC', 'Fence'\n]","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:16.338833Z","iopub.execute_input":"2021-08-28T03:36:16.339193Z","iopub.status.idle":"2021-08-28T03:36:16.391068Z","shell.execute_reply.started":"2021-08-28T03:36:16.339153Z","shell.execute_reply":"2021-08-28T03:36:16.390289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_ordinal.shape)\ndf_ordinal.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:16.392389Z","iopub.execute_input":"2021-08-28T03:36:16.392744Z","iopub.status.idle":"2021-08-28T03:36:16.427996Z","shell.execute_reply.started":"2021-08-28T03:36:16.392708Z","shell.execute_reply":"2021-08-28T03:36:16.426937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test_ordinal.shape)\ndf_test_ordinal.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:16.429756Z","iopub.execute_input":"2021-08-28T03:36:16.430145Z","iopub.status.idle":"2021-08-28T03:36:16.466515Z","shell.execute_reply.started":"2021-08-28T03:36:16.430107Z","shell.execute_reply":"2021-08-28T03:36:16.465643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5, 10))\nsns.heatmap(pd.DataFrame(pd.concat([df['SalePrice'], df_ordinal], axis=1).corr()[['SalePrice']].sort_values(by=['SalePrice'], ascending=False)), annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:16.467849Z","iopub.execute_input":"2021-08-28T03:36:16.468377Z","iopub.status.idle":"2021-08-28T03:36:17.049219Z","shell.execute_reply.started":"2021-08-28T03:36:16.468336Z","shell.execute_reply":"2021-08-28T03:36:17.048461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_kurtosis(df_ordinal)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:17.050519Z","iopub.execute_input":"2021-08-28T03:36:17.050857Z","iopub.status.idle":"2021-08-28T03:36:17.204027Z","shell.execute_reply.started":"2021-08-28T03:36:17.050822Z","shell.execute_reply":"2021-08-28T03:36:17.203233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Preprocessing of the Training Dataset","metadata":{}},{"cell_type":"markdown","source":"We now have 4 different datasets. They will be concatenated into a single training dataset for processing.","metadata":{}},{"cell_type":"code","source":"# Concatenate all the datasets into one\ndf_concat = pd.concat([ \n    df_continuous, \n    df_discrete,\n    df_ordinal, \n    df_nominal_ord\n], axis=1).astype(float)\n\ndf_test_concat = pd.concat([ \n    df_test_continuous, \n    df_test_discrete, \n    df_test_ordinal, \n    df_test_nominal_ord\n], axis=1).astype(float)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:17.20548Z","iopub.execute_input":"2021-08-28T03:36:17.205873Z","iopub.status.idle":"2021-08-28T03:36:17.217999Z","shell.execute_reply.started":"2021-08-28T03:36:17.205833Z","shell.execute_reply":"2021-08-28T03:36:17.21712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training and test datasets should have the same shape.","metadata":{}},{"cell_type":"code","source":"print(df_concat.shape)\ndf_concat.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:17.219456Z","iopub.execute_input":"2021-08-28T03:36:17.219842Z","iopub.status.idle":"2021-08-28T03:36:17.257635Z","shell.execute_reply.started":"2021-08-28T03:36:17.219802Z","shell.execute_reply":"2021-08-28T03:36:17.256509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test_concat.shape)\ndf_test_concat.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:17.259097Z","iopub.execute_input":"2021-08-28T03:36:17.259499Z","iopub.status.idle":"2021-08-28T03:36:17.297902Z","shell.execute_reply.started":"2021-08-28T03:36:17.259459Z","shell.execute_reply":"2021-08-28T03:36:17.296903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.1 Calculate the Variance Inflation Factors\n\nThe [variance inflation factor](https://en.wikipedia.org/wiki/Variance_inflation_factor) (VIF) tells us the degree to which an estimated regression coefficient's variance increases because of collinearity. This is applied when we want to know how strongly our data features correlate with each other. Too strong of a correlation between them can affect regression results. We begin this process by building a function to return a VIF table.","metadata":{}},{"cell_type":"code","source":"# Get the VIF statistics\ndef get_vif(data):\n    \n    df_vars = pd.DataFrame(np.log1p(data))\n    # Include the intercept constant \n    df_vars_const = sma.add_constant(df_vars)\n    df_vif = pd.DataFrame()\n    df_vif['Feature'] = df_vars_const.columns\n    df_vif['VIF'] = [variance_inflation_factor(df_vars_const.values, i) for i in range(df_vars_const.shape[1])]\n    # Return the 25 highest VIF values\n    return df_vif.sort_values(by=['VIF'], ascending=False).head(25)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:17.299245Z","iopub.execute_input":"2021-08-28T03:36:17.299633Z","iopub.status.idle":"2021-08-28T03:36:17.305816Z","shell.execute_reply.started":"2021-08-28T03:36:17.299596Z","shell.execute_reply":"2021-08-28T03:36:17.304897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_vif(df_concat)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:17.307193Z","iopub.execute_input":"2021-08-28T03:36:17.307849Z","iopub.status.idle":"2021-08-28T03:36:18.269366Z","shell.execute_reply.started":"2021-08-28T03:36:17.307797Z","shell.execute_reply":"2021-08-28T03:36:18.268272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are different guidelines about the best threshold for VIF:\n* VIF < 10\n* VIF < 5\n* VIF < 2.5\n\nIn regards to this notebook, we'll accept VIF values less than five. The VIF table shows eight features above this threshold, some are exceptionally high.  We'll set up a diagonal correlation matrix to visualize the strength of collinear relationships in the data.","metadata":{}},{"cell_type":"code","source":"# Create a mask for the upper triangle\nmask = np.triu(np.ones_like(df_concat.corr(), dtype=bool))\n\nplt.figure(figsize=(20, 20))\nsns.heatmap(df_concat.corr(), mask=mask, linewidths=1, linecolor='black')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:18.274475Z","iopub.execute_input":"2021-08-28T03:36:18.274901Z","iopub.status.idle":"2021-08-28T03:36:22.678672Z","shell.execute_reply.started":"2021-08-28T03:36:18.274859Z","shell.execute_reply":"2021-08-28T03:36:22.677861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"High correlation between features can occur when they are describing attributes that are too similar. When we look at the eight features above the VIF threshold we see two themes, house area and building year. \n\n`GrLivArea` describes the house ground living area but there are many other features such as `1stFlrSF` and `2ndFlrSF` that also describe house area, but they don't describe it uniquely enough to avoid strong collinearity.\n\nSince `GrLivArea` has a strong correlation with the target feature, it makes sense to drop other features that describe house area in some way but are not as strongly correlated to the target. This would include features such as `FullBath` which would affect the total living area of a house.\n\nAs seen in the correlation matrix, `YearBuilt` also struggles from multicollinearity. In this case, we'll try dropping `YearBuilt` and retain the features it shares collinearity with. This is because features like `Foundation` and `PavedDrive` are not similar even though they correlate well with `YearBuilt`.  \n\n`MSZoning` will also be dropped. It has strong correlations to `BldgType` and `HouseStyle`, but a weak correlation to the target feature. ","metadata":{}},{"cell_type":"code","source":"df_concat = df_concat.drop([\n    'WoodDeckSF', 'OpenPorchSF', 'MasVnrArea',\n    'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF',\n    '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n    'MSZoning', 'GarageCars', 'GarageArea',\n    'FullBath', 'YearBuilt', 'TotRmsAbvGrd'\n], axis=1)\n\ndf_test_concat = df_test_concat.drop([\n    'WoodDeckSF', 'OpenPorchSF', 'MasVnrArea',\n    'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF',\n    '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n    'MSZoning', 'GarageCars', 'GarageArea',\n    'FullBath', 'YearBuilt', 'TotRmsAbvGrd'\n], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:22.683865Z","iopub.execute_input":"2021-08-28T03:36:22.684124Z","iopub.status.idle":"2021-08-28T03:36:22.693573Z","shell.execute_reply.started":"2021-08-28T03:36:22.684099Z","shell.execute_reply":"2021-08-28T03:36:22.692674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've completed our multicollinearity preprocessing, let's run the VIF calculations on the edited training set to see if anything changed.","metadata":{}},{"cell_type":"code","source":"get_vif(df_concat)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:22.695409Z","iopub.execute_input":"2021-08-28T03:36:22.695892Z","iopub.status.idle":"2021-08-28T03:36:23.808711Z","shell.execute_reply.started":"2021-08-28T03:36:22.695855Z","shell.execute_reply":"2021-08-28T03:36:23.807838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The VIF table shows all feature values are now in the acceptable range. Our edits successfully reduced multicollinearity in the data. Now it's time to train our models.","metadata":{}},{"cell_type":"code","source":"print(df_concat.shape)\ndf_concat.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:23.810053Z","iopub.execute_input":"2021-08-28T03:36:23.810581Z","iopub.status.idle":"2021-08-28T03:36:23.874348Z","shell.execute_reply.started":"2021-08-28T03:36:23.810539Z","shell.execute_reply":"2021-08-28T03:36:23.873351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_test_concat.shape)\ndf_test_concat.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:36:23.878954Z","iopub.execute_input":"2021-08-28T03:36:23.879374Z","iopub.status.idle":"2021-08-28T03:36:23.932731Z","shell.execute_reply.started":"2021-08-28T03:36:23.879318Z","shell.execute_reply":"2021-08-28T03:36:23.931716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8.2 Perform Feature Scaling\n\nWith the presence of kurtosis throughout the data we'll scale our features using `log(1 + x)`. Our target feature will be scaled using `log(x)`. For prediction, the standard exponential function will be used to return the original price values.","metadata":{}},{"cell_type":"code","source":"# Scale the features \nX_features = np.log1p(df_concat)\nX_test_features = np.log1p(df_test_concat)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:42:23.319638Z","iopub.execute_input":"2021-08-28T03:42:23.319972Z","iopub.status.idle":"2021-08-28T03:42:23.328552Z","shell.execute_reply.started":"2021-08-28T03:42:23.319941Z","shell.execute_reply":"2021-08-28T03:42:23.327565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_labels = df['SalePrice'].astype(float)\nprint(Y_labels.shape)\nY_labels.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:42:23.658068Z","iopub.execute_input":"2021-08-28T03:42:23.658347Z","iopub.status.idle":"2021-08-28T03:42:23.666378Z","shell.execute_reply.started":"2021-08-28T03:42:23.658323Z","shell.execute_reply":"2021-08-28T03:42:23.665502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Evaluation of Regression Models","metadata":{}},{"cell_type":"markdown","source":"The evaluation function we create will assess our models using ground truth regression plots and return a metric table for root mean squared log error (RMSLE), r-squared, and the cross validation mean.","metadata":{}},{"cell_type":"code","source":"def evaluate_model(name, model, X_train, Y_train):            \n    \n    x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=RANDOM_SEED)    \n    # Scale the target feature using log(x)\n    model.fit(x_train, np.log(y_train))      \n    # Use the exponential function to return predicted sale prices\n    pred_train = np.exp(model.predict(x_train))\n    pred_val = np.exp(model.predict(x_val))   \n    \n    rmsle_train = np.sqrt(mean_squared_log_error(y_train, pred_train))\n    rmsle_val = np.sqrt(mean_squared_log_error(y_val, pred_val))\n    r2_train = r2_score(y_train, pred_train)\n    r2_val = r2_score(y_val, pred_val)\n    cvs = cross_val_score(model, X_train, np.log(Y_train), cv=5)\n    \n    # Plot the ground truth \n    f, ax = plt.subplots(figsize=(7, 7))\n    plt.scatter(y_train, pred_train, label='Training')\n    plt.scatter(y_val, pred_val, label='Validation')  \n    ax.xaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))\n    ax.yaxis.set_major_formatter(ticker.StrMethodFormatter('${x:,.0f}'))\n    plt.plot([0, 8e5], [0, 8e5], c='red')\n    plt.legend()\n    plt.xlabel('Sale Price')\n    plt.xticks(rotation=45)\n    plt.ylabel('Sale Price')\n    plt.title(f'{name}: Ground Truth')\n    plt.show()   \n\n    table = pd.DataFrame(\n                            [rmsle_train, rmsle_val, r2_train, r2_val, mean(cvs)],\n                            index=['RMSLE Training', 'RMSLE Validation', 'R^2 Training', 'R^2 Validation', 'CV Mean'],\n                            columns=[name]\n    )\n    \n    return table","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:42:23.794029Z","iopub.execute_input":"2021-08-28T03:42:23.794319Z","iopub.status.idle":"2021-08-28T03:42:23.805158Z","shell.execute_reply.started":"2021-08-28T03:42:23.794288Z","shell.execute_reply":"2021-08-28T03:42:23.804178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll define our regressors and evaluate them. Ridge regression will be our baseline model since it can handle multiple features better than a standard linear least squares model.","metadata":{}},{"cell_type":"code","source":"rr = Ridge(random_state=RANDOM_SEED)\nrfr = RandomForestRegressor(random_state=RANDOM_SEED)\nxgb = XGBRegressor(random_state=RANDOM_SEED)\n\nregressors = [rr, rfr, xgb]\nmodel_names = ['RidgeRegressor','RandomForestRegressor', 'XGBRegressor']\nmetrics = []\n\nfor name, reg in zip(model_names, regressors):\n    metrics.append(evaluate_model(name, reg, X_features, Y_labels))\n\ndf_results = pd.concat(metrics, axis=1)    \ndf_results","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:42:23.97005Z","iopub.execute_input":"2021-08-28T03:42:23.970307Z","iopub.status.idle":"2021-08-28T03:42:33.887798Z","shell.execute_reply.started":"2021-08-28T03:42:23.970284Z","shell.execute_reply":"2021-08-28T03:42:33.886994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The best performing model is the `XBGRegressor`. A comparison of the training and validation metrics show some overfitting. To confront this problem we'll try some hyperparameter tuning. We'll select some modest parameters for `GridSearchCV` and see if this improves our model performance.","metadata":{}},{"cell_type":"code","source":"import time\nstart_time = time.time()\n\nparams = {\n    'n_estimators': [50, 100, 300],\n    'max_depth': [3, 6, 9],\n    'subsample': [0.5, 0.8]\n}\n\ngsearch = GridSearchCV(\n    estimator=XGBRegressor(\n        learning_rate=0.1,\n        # Use GPU Accelerator\n        tree_method='gpu_hist', \n        n_jobs=-1,\n        random_state=RANDOM_SEED),\n    param_grid=params,\n    scoring='r2',\n    n_jobs=-1,\n    cv=5\n)\n\ngsearch.fit(X_features, np.log(Y_labels))\nprint(f'Time Elapsed: {time.time() - start_time}s')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:42:33.889264Z","iopub.execute_input":"2021-08-28T03:42:33.88955Z","iopub.status.idle":"2021-08-28T03:43:51.849844Z","shell.execute_reply.started":"2021-08-28T03:42:33.889523Z","shell.execute_reply":"2021-08-28T03:43:51.848875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the best parameters for the model and best score\nprint(f'Best Params: {gsearch.best_params_}')\nprint(f'Best Score: {gsearch.best_score_}')","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:43:51.851775Z","iopub.execute_input":"2021-08-28T03:43:51.852342Z","iopub.status.idle":"2021-08-28T03:43:51.858144Z","shell.execute_reply.started":"2021-08-28T03:43:51.852299Z","shell.execute_reply":"2021-08-28T03:43:51.857176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll now take the best estimator from `GridSearchCV` and evaluate it with our function.","metadata":{}},{"cell_type":"code","source":"evaluate_model('XGBRegressor Tuned', gsearch.best_estimator_, X_features, Y_labels)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:43:51.860093Z","iopub.execute_input":"2021-08-28T03:43:51.86065Z","iopub.status.idle":"2021-08-28T03:43:54.310534Z","shell.execute_reply.started":"2021-08-28T03:43:51.860611Z","shell.execute_reply":"2021-08-28T03:43:54.309451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The plot and metrics table shows a reduction in overfitting. This is only with a modest tuning, where a more aggressive tune could yield better results. We are now ready for the final phase of our data science task.","metadata":{}},{"cell_type":"markdown","source":"# 10. Predict Sale Price from the Test Dataset","metadata":{}},{"cell_type":"markdown","source":"The best regressor model will be fit on the full training dataset and used to predict house sale prices from the test dataset. Lastly, a table for the predictions will be created and submitted for evaluation.","metadata":{}},{"cell_type":"code","source":"# Use the best estimator as the regressor\nreg = gsearch.best_estimator_\nreg.fit(X_features, np.log(Y_labels))","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:43:54.312094Z","iopub.execute_input":"2021-08-28T03:43:54.312487Z","iopub.status.idle":"2021-08-28T03:43:54.691013Z","shell.execute_reply.started":"2021-08-28T03:43:54.312446Z","shell.execute_reply":"2021-08-28T03:43:54.690161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict the sale price\npredictions = reg.predict(np.log1p(df_test_concat))\n# Create a dataframe from sale price predictions and test id column\ndf_predictions = pd.DataFrame(np.exp(predictions))\ndf_predictions.columns = ['SalePrice']\n\nsubmission = pd.concat([df_test['Id'], df_predictions.astype(float)], axis=1)\nprint(submission.shape)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:43:54.692307Z","iopub.execute_input":"2021-08-28T03:43:54.692706Z","iopub.status.idle":"2021-08-28T03:43:54.716322Z","shell.execute_reply.started":"2021-08-28T03:43:54.692668Z","shell.execute_reply":"2021-08-28T03:43:54.715237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-28T03:43:54.717665Z","iopub.execute_input":"2021-08-28T03:43:54.718023Z","iopub.status.idle":"2021-08-28T03:43:54.729217Z","shell.execute_reply.started":"2021-08-28T03:43:54.717986Z","shell.execute_reply":"2021-08-28T03:43:54.728566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}