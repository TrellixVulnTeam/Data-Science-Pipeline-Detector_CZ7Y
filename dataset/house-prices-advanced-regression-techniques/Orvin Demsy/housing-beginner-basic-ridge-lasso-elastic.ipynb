{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Housing Price for Beginner using Basic Ridge, Lasso, Elastic","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-29T06:44:11.574812Z","iopub.execute_input":"2021-06-29T06:44:11.575545Z","iopub.status.idle":"2021-06-29T06:44:11.592159Z","shell.execute_reply.started":"2021-06-29T06:44:11.575466Z","shell.execute_reply":"2021-06-29T06:44:11.590955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Acknowledgement \n\nNotebooks from which I inspired the most:\n- https://www.kaggle.com/houcembenmansour/house-price-prediction\n- https://www.kaggle.com/rbyron/simple-linear-regression-models\n- https://www.kaggle.com/apapiu/regularized-linear-models","metadata":{}},{"cell_type":"markdown","source":"# Objective:\nThe goal of this work is to build a model that can correctly predict `SalePrice`  \nThis notebook is targeted to give exposure to beginner (myself) to work with continuous target variable, we will only implement basic model such as:\n- Simple Linear Regression (OLS)\n- Ridge Regression (Regression with L2 Regularization)\n- Lasso Regression (Regression with L1 Regularization)\n- Elastic (Regression with combination of L1 and L2)\n\nWe will use RMSE as a metric","metadata":{}},{"cell_type":"code","source":"# Basic\nimport pandas as pd\nimport numpy as np\nimport random\n\n# Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\n# Misc.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:12.183754Z","iopub.execute_input":"2021-06-29T06:44:12.184149Z","iopub.status.idle":"2021-06-29T06:44:12.190187Z","shell.execute_reply.started":"2021-06-29T06:44:12.184115Z","shell.execute_reply":"2021-06-29T06:44:12.189076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest  = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\nsample= pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:12.290549Z","iopub.execute_input":"2021-06-29T06:44:12.290901Z","iopub.status.idle":"2021-06-29T06:44:12.371623Z","shell.execute_reply.started":"2021-06-29T06:44:12.29087Z","shell.execute_reply":"2021-06-29T06:44:12.370553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA\n\nThis section will explore the data, goal:\n- get to know the dataset, how many features etc.\n- quick overview of feature correlation with dependent variable","metadata":{}},{"cell_type":"markdown","source":"## Getting to know data","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:13.182443Z","iopub.execute_input":"2021-06-29T06:44:13.18284Z","iopub.status.idle":"2021-06-29T06:44:13.225863Z","shell.execute_reply.started":"2021-06-29T06:44:13.182806Z","shell.execute_reply":"2021-06-29T06:44:13.224999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Data shape: ', train.shape)\nprint('There are %d instances' %train.shape[0])\nprint('There are %d features' %train.shape[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:13.37737Z","iopub.execute_input":"2021-06-29T06:44:13.377726Z","iopub.status.idle":"2021-06-29T06:44:13.383681Z","shell.execute_reply.started":"2021-06-29T06:44:13.377694Z","shell.execute_reply":"2021-06-29T06:44:13.382792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation heatmap\nGoal: see the correlation between each features, correlation range between -1 ant 1, the values near -1 or 1 shows stronge negative correlation or positive correlation respectively, while weak correlation indicated by values closer to zero. The goal is to see which features has high correlation (either positive or negative) with `SalePrice`","metadata":{}},{"cell_type":"code","source":"# Create correlation matrix\ncorrmat = train.corr()\n# corrmat\n\nplt.figure(figsize=(10, 10))\nax = sns.heatmap(corrmat, square=True, vmax=1, vmin=-1)\nax.set_title('Correlation Heatmap of Housing Pricing Train data')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:15.461955Z","iopub.execute_input":"2021-06-29T06:44:15.46236Z","iopub.status.idle":"2021-06-29T06:44:16.661168Z","shell.execute_reply.started":"2021-06-29T06:44:15.462326Z","shell.execute_reply":"2021-06-29T06:44:16.660421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For now we only need to focus on the last row `SalePrice`:\n- positively correlated : `OverallQual`, `GrLivArea`\n- negatively correlated : `None`\nThere are no variales that has high negative correlation with `SalePrice` \nLet see correlation values of those two variable in positive correlated and plot them to see what patterns they have","metadata":{}},{"cell_type":"code","source":"train['OverallQual']","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:17.630185Z","iopub.execute_input":"2021-06-29T06:44:17.630718Z","iopub.status.idle":"2021-06-29T06:44:17.639478Z","shell.execute_reply.started":"2021-06-29T06:44:17.630687Z","shell.execute_reply":"2021-06-29T06:44:17.638586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set seaborn theme\nsns.set(style='darkgrid', palette='muted')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:20.89813Z","iopub.execute_input":"2021-06-29T06:44:20.898638Z","iopub.status.idle":"2021-06-29T06:44:20.904386Z","shell.execute_reply.started":"2021-06-29T06:44:20.898604Z","shell.execute_reply":"2021-06-29T06:44:20.903496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of positive correlation\ntarget = 'SalePrice'\nvar1 = 'OverallQual'\nvar2 = 'GrLivArea'\n\n\nfig, (ax1, ax2)  = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\nsns.boxplot(x=var1, y=target, data=train, ax=ax1)\nax1.set_title('Correlation values %.3f' %corrmat.loc[target, var1])\nsns.scatterplot(x=var2, y=target, data=train, ax=ax2)\nax2.set_title('Correlation values %.3f' %corrmat.loc[target, var2])\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:21.979325Z","iopub.execute_input":"2021-06-29T06:44:21.979925Z","iopub.status.idle":"2021-06-29T06:44:22.701486Z","shell.execute_reply.started":"2021-06-29T06:44:21.979872Z","shell.execute_reply":"2021-06-29T06:44:22.700634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is examples of and definition of positive correlation, value of one variable increase as the other increase, the same logic applies to negative correlation.  \nWe will further observed which feature has high positive correlation and choose them as our features to train model","metadata":{}},{"cell_type":"markdown","source":"## Observe missing values","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:31.364863Z","iopub.execute_input":"2021-06-29T06:44:31.365457Z","iopub.status.idle":"2021-06-29T06:44:31.37161Z","shell.execute_reply.started":"2021-06-29T06:44:31.365404Z","shell.execute_reply":"2021-06-29T06:44:31.370565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total = train.isna().sum().sort_values(ascending=False)\npercent = total/len(train)\n\nmissing = pd.concat([total, percent], axis=1)\nmissing.columns = ['total', 'percentage']\n\n# let's also see corresponding correlation values\ncorr_tmp = corrmat.SalePrice\ncorr_tmp.name = 'corrval'\nmissingcorr = missing.merge(corr_tmp, how='outer', left_index=True,right_index=True).sort_values(by='percentage', ascending=False)\nmissingcorr.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:40.580977Z","iopub.execute_input":"2021-06-29T06:44:40.581555Z","iopub.status.idle":"2021-06-29T06:44:40.611983Z","shell.execute_reply.started":"2021-06-29T06:44:40.581505Z","shell.execute_reply":"2021-06-29T06:44:40.611173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What can we can we learn here?:\n- Notice that not all variables has correlation values, this indicates that corresponding features are categorical (non-numeric features)\n- The first four feature contains lots of missing data > 80%, we can delete those feature entirely and pretend they don't exist\n- Same thing goes for `LotFrontage` and `FireplaceQu`, also notice that it has low correlation, so dropping it entirely won'e be a problem\n- Interesting percentage values for features `GarageXXX`, notice they have save number, those missing values the belong to same instances, for now let's just drop them, plus the correlation values is not that high\n- The same logic applies to `BsmtXX` and `MasVnrXX`, although `MasVnrXX` has relatively low missing values, but for now let's just drop its column\n- `Electrical` only has one missing values, in this case we can delete the row","metadata":{}},{"cell_type":"code","source":"# Dropping columns\ndel_cols = missingcorr[missingcorr['percentage'] > missingcorr.loc['Electrical', 'percentage']].index\ndel_cols\nprint('Initial data shape:', train.shape)\n\n# The train_nona refers to train data without NaN values\ntrain_nona = train.drop(columns=del_cols)\nprint('After dropping columns:', train_nona.shape)\n\ntrain_nona = train_nona.dropna(axis=0, how='any')\nprint('After dropping instance of `Electrical`: ', train_nona.shape)\n\nprint('Total missing values in data after cleaning: ', train_nona.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:41.974755Z","iopub.execute_input":"2021-06-29T06:44:41.975139Z","iopub.status.idle":"2021-06-29T06:44:42.003559Z","shell.execute_reply.started":"2021-06-29T06:44:41.975102Z","shell.execute_reply":"2021-06-29T06:44:42.00258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\nThis section will cover the following:\n- pick which features to build our model, based on highest correlation values\n- check normality of features, normalized them if needed","metadata":{}},{"cell_type":"markdown","source":"## Pick important features\nLet's check the correlation matrix once again and sort them descendingly","metadata":{}},{"cell_type":"code","source":"highcorr = corrmat.SalePrice.sort_values(ascending=False)\n\n# Pick correlation values that are > 0.5\nhighcorr = highcorr[highcorr > 0.5]\nhighcorr","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:44.343513Z","iopub.execute_input":"2021-06-29T06:44:44.343893Z","iopub.status.idle":"2021-06-29T06:44:44.353594Z","shell.execute_reply.started":"2021-06-29T06:44:44.343858Z","shell.execute_reply":"2021-06-29T06:44:44.35252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we got some meaningful features that probably best to train our model, but do we need them all?  \nLet's take a look at correlation heatmap with these features one more time","metadata":{}},{"cell_type":"code","source":"# Correlation matrix with above features\nhighcorrmat = corrmat.loc[highcorr.index, highcorr.index]\n\nhighcorrmat","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:45.883467Z","iopub.execute_input":"2021-06-29T06:44:45.883836Z","iopub.status.idle":"2021-06-29T06:44:45.911589Z","shell.execute_reply.started":"2021-06-29T06:44:45.883803Z","shell.execute_reply":"2021-06-29T06:44:45.910802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation heatmap\nplt.figure(figsize=(8, 5))\nsns.heatmap(highcorrmat, annot=True, fmt='.2f')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:46.81682Z","iopub.execute_input":"2021-06-29T06:44:46.817193Z","iopub.status.idle":"2021-06-29T06:44:47.725603Z","shell.execute_reply.started":"2021-06-29T06:44:46.817164Z","shell.execute_reply":"2021-06-29T06:44:47.724754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let me introduce **multicollinearity**, this term means that there are high correlation between two independent variables/predictors/features, this can be cause a problem when we train our model. The example from above would be `GarageCars` and `GarageArea`, those two variables are highly correlated, this means we can drop one variable and keep the other. This makes the train data less redundant and also reduce its dimension, such that making our model less complex.\n\nAnother example of multicollinearity from above heatmap:\n- `TotalBsmtSF` and `1stFlrSF`\n- `GrLivArea` and `TotRmsAbvGrd`\n\nIntuitively, we understand why they have high correlations, because essentially both variables indicates the same thing, for example, the number of cars you can fit in your garage (`GarageCars`) implicitly dictates the area of the garage itself (`GarageArea`). The same goes for other pairs\n\nSo how to decide which feature to drop among those pair? For now, let's pick feature with higher correlation to our target variable, we pick:\n- `GarageCars`\n- `TotalBsmtSF`\n- `GrLivArea`\n\nAnd we drop: `GarageArea`, `1stFlrSF`, `TotRmsAbvGrd`  \nFor now, let's make it simplere and remove those in lowest three as well: `YearBuilt`, `YearRemodAdd`\n\nLet's remove those features","metadata":{}},{"cell_type":"code","source":"# Create new dataframe containing selected features \ndrop_cols = ['GarageArea', '1stFlrSF', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd']\n\n# Select highest correlated features\nsel_train = train_nona[highcorrmat.index]\n\n# Remove feature with multicollinearity\nsel_train = sel_train.drop(columns=drop_cols)\n\nsel_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:48.234854Z","iopub.execute_input":"2021-06-29T06:44:48.235477Z","iopub.status.idle":"2021-06-29T06:44:48.244015Z","shell.execute_reply.started":"2021-06-29T06:44:48.23544Z","shell.execute_reply":"2021-06-29T06:44:48.242971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pairplot & Outliers\nWe can see the plot between features altogether using pair plot, it helps us to see if patterns or outliers that may exist","metadata":{}},{"cell_type":"code","source":"sns.pairplot(sel_train)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:50.934899Z","iopub.execute_input":"2021-06-29T06:44:50.935265Z","iopub.status.idle":"2021-06-29T06:44:59.07946Z","shell.execute_reply.started":"2021-06-29T06:44:50.935235Z","shell.execute_reply":"2021-06-29T06:44:59.078687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Few observations regarding the plot against `SalePrice`:\n- There are two highest values in `GrLivArea` that doesn't follow the trends\n- There are one instances with highest values in `TotalBsmtSF` that doesn't follow the trends\n\nLet's delete those values\n\nThere some three instance in `GarageCars` when it equals to 4, that doesn't follow the trends, but we will ignore it now","metadata":{}},{"cell_type":"code","source":"# Check index of those instance \nprint(sel_train['GrLivArea'].sort_values()[-2:].index)\nprint(sel_train['TotalBsmtSF'].sort_values()[-1:].index)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:44:59.080684Z","iopub.execute_input":"2021-06-29T06:44:59.081107Z","iopub.status.idle":"2021-06-29T06:44:59.087923Z","shell.execute_reply.started":"2021-06-29T06:44:59.081064Z","shell.execute_reply":"2021-06-29T06:44:59.087192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can just delete the instance from GrLivArea\nsel_train = sel_train.drop(index=sel_train['GrLivArea'].sort_values()[-2:].index)\n\n# Check to see if thsoe outliers have been removed\nfig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12, 5))\nfig.suptitle('After Removing Outliers')\nsns.scatterplot(x='GrLivArea', y='SalePrice', data=sel_train, ax=ax1)\nsns.scatterplot(x='TotalBsmtSF', y='SalePrice', data=sel_train, ax=ax2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:15.456476Z","iopub.execute_input":"2021-06-29T06:45:15.457062Z","iopub.status.idle":"2021-06-29T06:45:15.966433Z","shell.execute_reply.started":"2021-06-29T06:45:15.456991Z","shell.execute_reply":"2021-06-29T06:45:15.965226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normality\nI consider this section to be quite advance, since we actually can proceed building model using data from before.\nBut since I've just learned about it, I thought I might as well put it here.\n\nThis section will check whether or not each features with numeric values follows normal distribution.  \nThis is done because data with normal distribution is favorable in machine learning settings.   \nHere we will apply log + 1 transformation to convert non-normal distribution to normal distribution.\n\nWe will also do this for columns with continuos values, i.e. `SalePrice`, `GrLivArea` and `TotalBsmtSF`","metadata":{}},{"cell_type":"markdown","source":"### `SalePrice`","metadata":{}},{"cell_type":"code","source":"from scipy.stats import norm","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:19.728504Z","iopub.execute_input":"2021-06-29T06:45:19.728859Z","iopub.status.idle":"2021-06-29T06:45:19.732548Z","shell.execute_reply.started":"2021-06-29T06:45:19.72882Z","shell.execute_reply":"2021-06-29T06:45:19.731667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.SalePrice, kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.SalePrice, plot = ax2)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:21.162244Z","iopub.execute_input":"2021-06-29T06:45:21.162858Z","iopub.status.idle":"2021-06-29T06:45:21.753087Z","shell.execute_reply.started":"2021-06-29T06:45:21.162741Z","shell.execute_reply":"2021-06-29T06:45:21.752225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see `SalePrice` is not normal, the distribution has positive skewness, and qq plot shows it doesn't follows diagonal line. Let's apply log transformation!","metadata":{}},{"cell_type":"code","source":"# Applying log transformation\nsel_train['SalePrice'] = np.log1p(sel_train.SalePrice)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:22.831993Z","iopub.execute_input":"2021-06-29T06:45:22.832546Z","iopub.status.idle":"2021-06-29T06:45:22.837842Z","shell.execute_reply.started":"2021-06-29T06:45:22.83251Z","shell.execute_reply":"2021-06-29T06:45:22.836951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See effect of log transformation \nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.SalePrice, kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.SalePrice, plot = ax2)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:23.024172Z","iopub.execute_input":"2021-06-29T06:45:23.024526Z","iopub.status.idle":"2021-06-29T06:45:23.582344Z","shell.execute_reply.started":"2021-06-29T06:45:23.024497Z","shell.execute_reply":"2021-06-29T06:45:23.581267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### `GrLivArea`","metadata":{}},{"cell_type":"code","source":"# Plot GrLiveArea\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.GrLivArea, kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.GrLivArea, plot = ax2)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:23.584436Z","iopub.execute_input":"2021-06-29T06:45:23.584881Z","iopub.status.idle":"2021-06-29T06:45:24.140657Z","shell.execute_reply.started":"2021-06-29T06:45:23.584834Z","shell.execute_reply":"2021-06-29T06:45:24.139427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Same phenomenon as before, `GrLivArea` also experience positive skewness","metadata":{}},{"cell_type":"code","source":"# Apply log transformation\nsel_train.GrLivArea = np.log(sel_train.GrLivArea)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:24.142296Z","iopub.execute_input":"2021-06-29T06:45:24.14261Z","iopub.status.idle":"2021-06-29T06:45:24.149156Z","shell.execute_reply.started":"2021-06-29T06:45:24.142581Z","shell.execute_reply":"2021-06-29T06:45:24.148102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See effect of log transformation \nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.GrLivArea, kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.GrLivArea, plot = ax2)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:24.150466Z","iopub.execute_input":"2021-06-29T06:45:24.1509Z","iopub.status.idle":"2021-06-29T06:45:24.750147Z","shell.execute_reply.started":"2021-06-29T06:45:24.150868Z","shell.execute_reply":"2021-06-29T06:45:24.749198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### `TotalBsmtSF`","metadata":{}},{"cell_type":"code","source":"# Plot TotalBsmtSF\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.TotalBsmtSF, kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.TotalBsmtSF, plot = ax2)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:24.754317Z","iopub.execute_input":"2021-06-29T06:45:24.754749Z","iopub.status.idle":"2021-06-29T06:45:25.511666Z","shell.execute_reply.started":"2021-06-29T06:45:24.754699Z","shell.execute_reply":"2021-06-29T06:45:25.510957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that couple of values are zeros, these values can be transformed using log.   \nTo solve this, we can only apply log transformation for the non-zero values.","metadata":{}},{"cell_type":"code","source":"# transform data\nsel_train['TotalBsmtSF'] = np.log1p(sel_train.TotalBsmtSF)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:25.512886Z","iopub.execute_input":"2021-06-29T06:45:25.513346Z","iopub.status.idle":"2021-06-29T06:45:25.517498Z","shell.execute_reply.started":"2021-06-29T06:45:25.513315Z","shell.execute_reply":"2021-06-29T06:45:25.516741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See effect of log transformation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\nsns.distplot(sel_train.TotalBsmtSF[sel_train.TotalBsmtSF>0], kde=True, fit=norm, ax=ax1)\n_ = stats.probplot(sel_train.TotalBsmtSF[sel_train.TotalBsmtSF>0], plot = ax2)\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:25.522824Z","iopub.execute_input":"2021-06-29T06:45:25.523194Z","iopub.status.idle":"2021-06-29T06:45:26.106338Z","shell.execute_reply.started":"2021-06-29T06:45:25.523161Z","shell.execute_reply":"2021-06-29T06:45:26.105397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the target variable from the predictors\ny = sel_train.SalePrice\nX = sel_train.drop(columns='SalePrice')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:26.108129Z","iopub.execute_input":"2021-06-29T06:45:26.108487Z","iopub.status.idle":"2021-06-29T06:45:26.114554Z","shell.execute_reply.started":"2021-06-29T06:45:26.108456Z","shell.execute_reply":"2021-06-29T06:45:26.113477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# Models\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:26.131453Z","iopub.execute_input":"2021-06-29T06:45:26.131869Z","iopub.status.idle":"2021-06-29T06:45:26.534241Z","shell.execute_reply.started":"2021-06-29T06:45:26.131824Z","shell.execute_reply":"2021-06-29T06:45:26.533226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:26.536438Z","iopub.execute_input":"2021-06-29T06:45:26.536756Z","iopub.status.idle":"2021-06-29T06:45:26.543863Z","shell.execute_reply.started":"2021-06-29T06:45:26.536718Z","shell.execute_reply":"2021-06-29T06:45:26.54301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ordinary Least Square (OLS), Regression\nFirst is let's use ordinary least square (OLS) model, this is basic linear regression without regularization.   \nThe definition of regularization will not be discussed extensively in this course.","metadata":{}},{"cell_type":"code","source":"# Function to compute RMSE (Root Mean Squared Error), using 5-fold CV\ndef rmse(model, X, y, cv):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=cv))\n    return rmse.mean()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:26.994359Z","iopub.execute_input":"2021-06-29T06:45:26.994898Z","iopub.status.idle":"2021-06-29T06:45:26.999707Z","shell.execute_reply.started":"2021-06-29T06:45:26.994863Z","shell.execute_reply":"2021-06-29T06:45:26.998824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lin = LinearRegression()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:27.944529Z","iopub.execute_input":"2021-06-29T06:45:27.944876Z","iopub.status.idle":"2021-06-29T06:45:27.949104Z","shell.execute_reply.started":"2021-06-29T06:45:27.944847Z","shell.execute_reply":"2021-06-29T06:45:27.947832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse_sc = rmse(lin, X, y, 5)\nrmse_sc","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:28.157417Z","iopub.execute_input":"2021-06-29T06:45:28.157803Z","iopub.status.idle":"2021-06-29T06:45:28.211178Z","shell.execute_reply.started":"2021-06-29T06:45:28.157767Z","shell.execute_reply":"2021-06-29T06:45:28.210122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create List to append dictionary of scores and model\nall_scores = []","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:28.325183Z","iopub.execute_input":"2021-06-29T06:45:28.325543Z","iopub.status.idle":"2021-06-29T06:45:28.330865Z","shell.execute_reply.started":"2021-06-29T06:45:28.325512Z","shell.execute_reply":"2021-06-29T06:45:28.329322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_scores.append(dict(model='OLD', score=rmse_sc))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:28.512777Z","iopub.execute_input":"2021-06-29T06:45:28.51316Z","iopub.status.idle":"2021-06-29T06:45:28.517871Z","shell.execute_reply.started":"2021-06-29T06:45:28.513125Z","shell.execute_reply":"2021-06-29T06:45:28.516656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ridge Regression\nIf you have studied about L1 and L2 Regularization, it's good thing to know them by different name: \n- L1 is also called Lasso\n- L2 is also called Ridge\n\nSome source I recommend to read on:\n- https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c\n- https://stats.stackexchange.com/questions/866/when-should-i-use-lasso-vs-ridge\n- https://stats.stackexchange.com/questions/200416/is-regression-with-l1-regularization-the-same-as-lasso-and-with-l2-regularizati","metadata":{}},{"cell_type":"code","source":"# let's try a default value alpha =1 \nridge = Ridge(alpha=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:29.153883Z","iopub.execute_input":"2021-06-29T06:45:29.15425Z","iopub.status.idle":"2021-06-29T06:45:29.158205Z","shell.execute_reply.started":"2021-06-29T06:45:29.154219Z","shell.execute_reply":"2021-06-29T06:45:29.157188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse(ridge, X, y, cv=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:29.326352Z","iopub.execute_input":"2021-06-29T06:45:29.326701Z","iopub.status.idle":"2021-06-29T06:45:29.370187Z","shell.execute_reply.started":"2021-06-29T06:45:29.326671Z","shell.execute_reply":"2021-06-29T06:45:29.369147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you ever heard of term `lambda` as regularization term, in this scikit module it's defined by the variable `alpha`, itgoverns how much we want to regularize the model, it's a hyperparameter, meaning we can set this value according to our will that gives the best metric we concern about, in this case `rmse`, let's create numbers for alpha","metadata":{}},{"cell_type":"code","source":"# Selection for alphas\nalphas = np.logspace(5,-5,50)\nalphas","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:30.265236Z","iopub.execute_input":"2021-06-29T06:45:30.265594Z","iopub.status.idle":"2021-06-29T06:45:30.273797Z","shell.execute_reply.started":"2021-06-29T06:45:30.265564Z","shell.execute_reply":"2021-06-29T06:45:30.272449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ridge CV is a ridge regression with built-in CV implementation\nridgecv = RidgeCV(alphas=alphas, scoring='neg_mean_squared_error')\nridgecv.fit(X, y)\n\n# RidgeCV gives us the model trained with best alpha values\nridgecv.alpha_","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:30.629422Z","iopub.execute_input":"2021-06-29T06:45:30.62984Z","iopub.status.idle":"2021-06-29T06:45:30.666143Z","shell.execute_reply.started":"2021-06-29T06:45:30.629804Z","shell.execute_reply":"2021-06-29T06:45:30.66499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mod = Ridge(alpha=ridgecv.alpha_)\nmod.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:31.368264Z","iopub.execute_input":"2021-06-29T06:45:31.368613Z","iopub.status.idle":"2021-06-29T06:45:31.380189Z","shell.execute_reply.started":"2021-06-29T06:45:31.368584Z","shell.execute_reply":"2021-06-29T06:45:31.379266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse(mod, X, y, cv=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:31.93369Z","iopub.execute_input":"2021-06-29T06:45:31.934061Z","iopub.status.idle":"2021-06-29T06:45:31.979366Z","shell.execute_reply.started":"2021-06-29T06:45:31.934006Z","shell.execute_reply":"2021-06-29T06:45:31.978283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Computing rmse\nrmse_sc = rmse(ridgecv, X, y, cv=5)\nrmse_sc","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:33.496835Z","iopub.execute_input":"2021-06-29T06:45:33.497209Z","iopub.status.idle":"2021-06-29T06:45:33.639447Z","shell.execute_reply.started":"2021-06-29T06:45:33.497178Z","shell.execute_reply":"2021-06-29T06:45:33.638373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Oops, it gives the same values as before with OLS, don't worry it happens. Notice that also in this case, using default alpha value or new alpha value doesn't really affect rmse_sc.\n\nAlso one thing to note is that, ridgecv returns the ridge model trained with best alpha, ","metadata":{}},{"cell_type":"code","source":"all_scores.append(dict(model='Ridge', score=rmse_sc))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:35.363042Z","iopub.execute_input":"2021-06-29T06:45:35.363424Z","iopub.status.idle":"2021-06-29T06:45:35.367742Z","shell.execute_reply.started":"2021-06-29T06:45:35.363391Z","shell.execute_reply":"2021-06-29T06:45:35.366589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lasso Regression\nThe same logic from before, applies to Lasso model","metadata":{}},{"cell_type":"code","source":"# Try lasso with 1 alpha values\nlasso = Lasso(alpha=1)\nrmse(lasso, X, y, cv=5)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:36.41838Z","iopub.execute_input":"2021-06-29T06:45:36.41872Z","iopub.status.idle":"2021-06-29T06:45:36.465035Z","shell.execute_reply.started":"2021-06-29T06:45:36.418692Z","shell.execute_reply":"2021-06-29T06:45:36.464087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find new alphas\nlassocv = LassoCV(alphas=alphas)\nlassocv.fit(X, y)\nlassocv.alpha_","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:36.796458Z","iopub.execute_input":"2021-06-29T06:45:36.796809Z","iopub.status.idle":"2021-06-29T06:45:36.847985Z","shell.execute_reply.started":"2021-06-29T06:45:36.796779Z","shell.execute_reply":"2021-06-29T06:45:36.847135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute rmse and add to list\nrmse_sc = rmse(lassocv, X, y, cv=5)\nrmse_sc","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:39.105375Z","iopub.execute_input":"2021-06-29T06:45:39.105706Z","iopub.status.idle":"2021-06-29T06:45:39.32469Z","shell.execute_reply.started":"2021-06-29T06:45:39.105678Z","shell.execute_reply":"2021-06-29T06:45:39.323677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that rmse with new alpha gives better (lower) rmse than default values.  \nBut still the rmse with new alpha is similar to previous two.","metadata":{}},{"cell_type":"code","source":"all_scores.append(dict(model='Lasso', score=rmse_sc))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:39.78456Z","iopub.execute_input":"2021-06-29T06:45:39.78492Z","iopub.status.idle":"2021-06-29T06:45:39.789898Z","shell.execute_reply.started":"2021-06-29T06:45:39.78489Z","shell.execute_reply":"2021-06-29T06:45:39.788631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Elastic Net\nElastic net is a combination of both L1 and L2 regularization.  \nWe will skip using the default alpha values, and immediately jump to using ElasticNetCV","metadata":{}},{"cell_type":"code","source":"elasticcv = ElasticNetCV(alphas=alphas)\nelasticcv.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:49.575246Z","iopub.execute_input":"2021-06-29T06:45:49.575741Z","iopub.status.idle":"2021-06-29T06:45:49.637215Z","shell.execute_reply.started":"2021-06-29T06:45:49.575696Z","shell.execute_reply":"2021-06-29T06:45:49.636426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute rmse and add to list\nrmse_sc = rmse(elasticcv, X, y, cv=5)\nrmse_sc","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:49.90013Z","iopub.execute_input":"2021-06-29T06:45:49.90241Z","iopub.status.idle":"2021-06-29T06:45:50.128531Z","shell.execute_reply.started":"2021-06-29T06:45:49.902355Z","shell.execute_reply":"2021-06-29T06:45:50.127442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_scores.append(dict(model='ElasticNet', score=rmse_sc))","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:51.77677Z","iopub.execute_input":"2021-06-29T06:45:51.777375Z","iopub.status.idle":"2021-06-29T06:45:51.781379Z","shell.execute_reply.started":"2021-06-29T06:45:51.777337Z","shell.execute_reply":"2021-06-29T06:45:51.780473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results summary ","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(all_scores).sort_values(by='score')","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:54.201101Z","iopub.execute_input":"2021-06-29T06:45:54.201449Z","iopub.status.idle":"2021-06-29T06:45:54.213406Z","shell.execute_reply.started":"2021-06-29T06:45:54.201418Z","shell.execute_reply":"2021-06-29T06:45:54.212324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Current results suggest Ridge Regression gives the lowest RSME score, let's use it to predict test data","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"markdown","source":"## Grab desired columns and impute missing values","metadata":{}},{"cell_type":"code","source":"# Desired columns\nwant_cols = X.columns\n\nsel_test = test[want_cols]","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:45:59.891859Z","iopub.execute_input":"2021-06-29T06:45:59.892243Z","iopub.status.idle":"2021-06-29T06:45:59.89897Z","shell.execute_reply.started":"2021-06-29T06:45:59.892209Z","shell.execute_reply":"2021-06-29T06:45:59.897688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imput missing values with an average, notice the column with missing values\nsel_test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:46:20.440539Z","iopub.execute_input":"2021-06-29T06:46:20.440937Z","iopub.status.idle":"2021-06-29T06:46:20.450439Z","shell.execute_reply.started":"2021-06-29T06:46:20.440905Z","shell.execute_reply":"2021-06-29T06:46:20.449472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Recall that `GarageCars` only contains integer values, so let's obey that. ","metadata":{}},{"cell_type":"code","source":"sel_test.GarageCars = sel_test.GarageCars.fillna(round(sel_test.GarageCars.mean()))\nsel_test.TotalBsmtSF = sel_test.TotalBsmtSF.fillna(sel_test.TotalBsmtSF.mean())","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:46:21.937498Z","iopub.execute_input":"2021-06-29T06:46:21.93784Z","iopub.status.idle":"2021-06-29T06:46:21.948156Z","shell.execute_reply.started":"2021-06-29T06:46:21.937811Z","shell.execute_reply":"2021-06-29T06:46:21.947171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After imputing missing values\nsel_test.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:46:23.171698Z","iopub.execute_input":"2021-06-29T06:46:23.172217Z","iopub.status.idle":"2021-06-29T06:46:23.179746Z","shell.execute_reply.started":"2021-06-29T06:46:23.172183Z","shell.execute_reply":"2021-06-29T06:46:23.178835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict using Ridge","metadata":{}},{"cell_type":"code","source":"# Predict\nypred = ridgecv.predict(sel_test)\n\n# Creating output csv\nresult = pd.DataFrame({sample.columns[0] : sample['Id'],\n                        sample.columns[1] : ypred\n})\n","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:46:25.040587Z","iopub.execute_input":"2021-06-29T06:46:25.041128Z","iopub.status.idle":"2021-06-29T06:46:25.049155Z","shell.execute_reply.started":"2021-06-29T06:46:25.041094Z","shell.execute_reply":"2021-06-29T06:46:25.04828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.to_csv('./20210629-housing-ridge.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-29T06:47:19.477489Z","iopub.execute_input":"2021-06-29T06:47:19.478094Z","iopub.status.idle":"2021-06-29T06:47:19.489467Z","shell.execute_reply.started":"2021-06-29T06:47:19.478057Z","shell.execute_reply":"2021-06-29T06:47:19.488511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}