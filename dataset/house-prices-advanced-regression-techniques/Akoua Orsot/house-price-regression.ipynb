{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## House Price Predictions\nDifferent variables come into play in appraising a house, such as the number of bedrooms, square footage, location, and much more. So, our task here is to build a machine learning model to make reasonably accurate predictions in terms of pricing houses. It would be an opportunity for those in real estate to gain more visibility on the market as a whole. In doing so, this notebook will offer a user-friendly explanation through every step using LIME (Local Interpretable Model-agnostic Explanations) principles.\n\n## Table of Contents\n1. Environment set-up\n    * Importing Libraries\n    * Loading the data\n2. Initial Diagnostics\n    * Glimpse\n    * Descriptive Statitics\n    * Target Variable Analysis\n    * Predictors Analysis\n3. Data Cleaning\n    * Missing Values\n    * Simple Imputation\n    * Grouped Imputation\n4. Inquiry Exploration\n    * Does bigger means pricier houses?\n    * Where is the real estate hotspot?\n    * Which miscellaneous feature add the most value?\n5. Feature Engineering\n    * Outliers - Feature Scaling\n    * Categorical Encoding\n    * Datetime Variables\n    \n6. Correlation Analysis\n\n7. Machine Learning set-up\n    * Train-test split\n    * Cross-validation\n    * Dimensionality Reduction\n\n8. Machine Learning - Simple Models\n\n9. Machine Learning - Ensemble Methods\n\n10. Hyperparameter Tuning\n\n11. Model Performance Evaluation\n \n12. Final Submission","metadata":{}},{"cell_type":"markdown","source":"# 1. Environment Set-up","metadata":{}},{"cell_type":"code","source":"## Importing Libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Set seed\nimport random\nrandom.seed(1234)\n\n# Manipulating & Visualizing Data\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(16,10)})\n\n# Feature Scaling\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n# Categorical Encoding\nimport category_encoders as ce\n\n# Model Selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, KFold\n\n# Dimensionality Reduction\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n# ML Models\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import linear_model \n\n# Ensemble Learning\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import StackingRegressor\n\n# Hyperparameter Tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Performance metrics\nimport sklearn.metrics as skm\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-14T12:32:55.673921Z","iopub.execute_input":"2022-04-14T12:32:55.674582Z","iopub.status.idle":"2022-04-14T12:32:58.01818Z","shell.execute_reply.started":"2022-04-14T12:32:55.674441Z","shell.execute_reply":"2022-04-14T12:32:58.016994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Loading the dataset\ndf = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:32:58.020543Z","iopub.execute_input":"2022-04-14T12:32:58.021291Z","iopub.status.idle":"2022-04-14T12:32:58.111706Z","shell.execute_reply.started":"2022-04-14T12:32:58.021235Z","shell.execute_reply":"2022-04-14T12:32:58.110926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Initial Diagnostics","metadata":{}},{"cell_type":"code","source":"## Glimpse of the data\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:32:58.112773Z","iopub.execute_input":"2022-04-14T12:32:58.114099Z","iopub.status.idle":"2022-04-14T12:32:58.15826Z","shell.execute_reply.started":"2022-04-14T12:32:58.11402Z","shell.execute_reply":"2022-04-14T12:32:58.156756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** From the glimpse above, we could already draw some observations. \n* Our dataset comprises 1460 rows and 80 columns, making it relatively small, so we would not expect the training process o to be as computationally intensive.\n* For null values, most columns appear to have no missing values, while null values make up 80% for some of those variables. It indicates that we shall proceed with data cleaning and tidying before doing any statistical analysis or machine learning. \n* In terms of variable type, we have mostly int64, float64, and object. Though 'object' can indicate text or categorical, we will need to investigate further in feature engineering.","metadata":{}},{"cell_type":"code","source":"## Descriptive Statistics\ndf.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:32:58.160475Z","iopub.execute_input":"2022-04-14T12:32:58.16076Z","iopub.status.idle":"2022-04-14T12:32:58.295273Z","shell.execute_reply.started":"2022-04-14T12:32:58.160718Z","shell.execute_reply":"2022-04-14T12:32:58.293966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** For all 80 variables, the table above captures the basic descriptive statistics showing things like mean, standard deviation, min, max, etc. Commenting on each variable would bring little value to our overall analysis, and so we will zoom on the target variable 'SalePrice'.","metadata":{}},{"cell_type":"code","source":"# Stats for the target variable\ndf['SalePrice'].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:32:58.296986Z","iopub.execute_input":"2022-04-14T12:32:58.297585Z","iopub.status.idle":"2022-04-14T12:32:58.311366Z","shell.execute_reply.started":"2022-04-14T12:32:58.297538Z","shell.execute_reply":"2022-04-14T12:32:58.310008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** The count indicates no null values in the column. The houses in the dataset vary from ~USD34.9k to ~USD755k, with a mean value of ~USD180k. With the standard deviation at ~USD79k, it appears that prices fluctuate pretty significantly, or we may potentially have houses with exorbitant prices (outliers) skewing the data. We will create a histogram to look at the distribution more closely.","metadata":{}},{"cell_type":"code","source":"## Feature Variable Analysis\nsns.histplot(data=df, x='SalePrice')\nplt.xlabel(\"Dollar Amount ($)\")\nplt.ylabel(\"Frequency (Count)\")\nplt.title(\"Distribution of House Sale Price\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:32:58.313339Z","iopub.execute_input":"2022-04-14T12:32:58.313624Z","iopub.status.idle":"2022-04-14T12:32:58.805363Z","shell.execute_reply.started":"2022-04-14T12:32:58.313591Z","shell.execute_reply":"2022-04-14T12:32:58.804424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** From the histogram above, we can deduct that house sale prices in this dataset have a right-skewed distribution with outliers on the upper end, indicating luxury houses with higher price points. However, most houses appear to fall between ~USD100k and ~USD300k, relatively consistent with real estate markets in the United States.","metadata":{}},{"cell_type":"markdown","source":"# 3. Data Cleaning","metadata":{}},{"cell_type":"code","source":"# Visualize missing data\nplt.figure(figsize=(10,6))\nsns.heatmap(df.isna().transpose(),\n            cmap=\"YlGnBu\",\n            cbar_kws={'label': 'Missing Data'})\nplt.xlabel(\"Features\")\nplt.ylabel(\"Observations\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:32:58.806763Z","iopub.execute_input":"2022-04-14T12:32:58.807764Z","iopub.status.idle":"2022-04-14T12:32:59.942492Z","shell.execute_reply.started":"2022-04-14T12:32:58.807706Z","shell.execute_reply":"2022-04-14T12:32:59.941279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** As the plot shows above, there are indeed null values confirming our observation in the initial diagnostics. Given that not all variables are of the same type or the same proportion of missing values, the cleaning process will attend to each column or group of similar columns.","metadata":{}},{"cell_type":"markdown","source":"**Definition:** When it comes to data science, we are constantly dealing with imperfect information, thus murking the waters on the quality of data overall. One of those issues is the recurrence of missing values and requires effective techniques to deal with them. Imputation methods present such an opportunity using strategies to replace null values with statistical measures like mean, mode, or median. More information [here](https://machinelearningmastery.com/statistical-imputation-for-missing-values-in-machine-learning/).","metadata":{}},{"cell_type":"code","source":"## No. of null values\nnull_vals = df.isna().sum().sum()\n\n# List of columns with missing values\nnull_cols = df.columns[df.isna().any()].tolist()\n\n# Reporting back\nprint(\"We are missing {:2d} values in our data at given percentages in the following columns:\" .format(null_vals))\nfor i in null_cols:\n    col_null = df[i].isnull().sum()\n    per_null = col_null / len(df[i])\n    print(\"  - {}: {} ({:.2%})\".format(i, col_null, per_null))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:32:59.944941Z","iopub.execute_input":"2022-04-14T12:32:59.948272Z","iopub.status.idle":"2022-04-14T12:32:59.987681Z","shell.execute_reply.started":"2022-04-14T12:32:59.948198Z","shell.execute_reply":"2022-04-14T12:32:59.98692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LotFrontage:** As per the data dictionary, it is the linear feet of street connected to property. It indicates the measurement of a piece of land (lot) often defined by frontage and depth respectively. For instance, an house can be 50 by 150, meaning 50 feet wide (frontage) and 150 feet long. Read more about it [here](https://www.gimme-shelter.com/frontage-50043/). Given that 'LotFrontage' is one of those characteristics all houses have, the null values indicate missing information that cannot just be equal to 0. Since we cannot get back and fetch more data, we will use imputation methods for this column and other ones which may require them.\n\n**Note:** Before proceeding to the imputation, we would like to investigate possible differences in distribution grouped by Lot shape.","metadata":{}},{"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2)\nfig.suptitle('Boxplots of LotFrontage')\nsns.boxplot(ax=ax1, data=df, y=\"LotFrontage\", orient = \"v\")\nsns.boxplot(ax=ax2, data=df, x=\"LotShape\", y=\"LotFrontage\", orient = \"v\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:32:59.988808Z","iopub.execute_input":"2022-04-14T12:32:59.989319Z","iopub.status.idle":"2022-04-14T12:33:00.364032Z","shell.execute_reply.started":"2022-04-14T12:32:59.989273Z","shell.execute_reply":"2022-04-14T12:33:00.363006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"For all houses' LotFrontage, the mean is {:.2f} and median is {:.2f}\".format(df['LotFrontage'].mean(),\n                                                               df['LotFrontage'].median()))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:00.367022Z","iopub.execute_input":"2022-04-14T12:33:00.367288Z","iopub.status.idle":"2022-04-14T12:33:00.373153Z","shell.execute_reply.started":"2022-04-14T12:33:00.367258Z","shell.execute_reply":"2022-04-14T12:33:00.372421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"For houses that are: \")\nfor i in df[\"LotShape\"].unique().tolist():\n    df_i = df[df[\"LotShape\"]==i]\n    mean_frontage = df_i['LotFrontage'].mean()\n    median_frontage = df_i['LotFrontage'].median()\n    print(\" -{}, mean LotFrontage = {:.2f} and median LotFrontage = {:.2f}\".format(i,\n                                                                            mean_frontage,\n                                                                            median_frontage))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:00.374303Z","iopub.execute_input":"2022-04-14T12:33:00.374656Z","iopub.status.idle":"2022-04-14T12:33:00.394723Z","shell.execute_reply.started":"2022-04-14T12:33:00.374626Z","shell.execute_reply":"2022-04-14T12:33:00.393834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** The boxplots indicate the presence of outliers in the data with massive and small houses by widths. When broken down by 'LotShape', we also observe a notable difference in those houses categorized as IR3, in other words, of very irregular shape. In light of both the outliers and category differences, we will use the median value grouped by LotShape for the imputation process to ensure consistency in the data.","metadata":{}},{"cell_type":"code","source":"# Imputation using group by\ndf['LotFrontage'] = df.groupby('LotShape').LotFrontage.transform(lambda x: x.fillna(x.median()))\ndf.LotFrontage = df.LotFrontage.round(2)\ndf['LotFrontage'].isnull().sum()\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:00.39638Z","iopub.execute_input":"2022-04-14T12:33:00.397322Z","iopub.status.idle":"2022-04-14T12:33:00.57864Z","shell.execute_reply.started":"2022-04-14T12:33:00.397276Z","shell.execute_reply":"2022-04-14T12:33:00.57775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Alley:** As per the data dictionary, it refers to the type of alley access to property. Given the real estate market in question, it may affect the price more or less and so, the null values are indeed significant with NA indicating that there isn't one. To ensure that it is taken into account, we will rename the NA into the full phrase 'No alley access' and then proceed in encoding this categorical variable.","metadata":{}},{"cell_type":"code","source":"# Replacing the null values with a significant term\ndf['Alley'].fillna(\"No alley access\", inplace = True)\ndf['Alley'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:00.580089Z","iopub.execute_input":"2022-04-14T12:33:00.580578Z","iopub.status.idle":"2022-04-14T12:33:00.590785Z","shell.execute_reply.started":"2022-04-14T12:33:00.580523Z","shell.execute_reply":"2022-04-14T12:33:00.590105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Variable Grouping:** It appears that the process in detecting missing valuies actually led to understanding those null values are actually categories significant or equal to 0 per the data dictionary. So, to be more efficient, we will make a list of those columns and the term/value we'll use to replace the na values.","metadata":{}},{"cell_type":"code","source":"for i in null_cols:\n    # Grouping of variables dependent on the presence of a basement\n    if 'Bsmt' in i:\n        df[i].fillna(\"No Basement\", inplace = True)\n        \n    # Grouping of variables dependent on the presence of a garage\n    elif 'Garage' in i:\n        if i == 'GarageYrBlt':\n            df[i].fillna(0, inplace = True)\n        else:\n            df[i].fillna(\"No Garage\", inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:00.592209Z","iopub.execute_input":"2022-04-14T12:33:00.59261Z","iopub.status.idle":"2022-04-14T12:33:00.610036Z","shell.execute_reply.started":"2022-04-14T12:33:00.592565Z","shell.execute_reply":"2022-04-14T12:33:00.608762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"other_cols_imp = {\n    'MasVnrType': 'No Veneer',\n    'MasVnrArea': 0, \n    'FireplaceQu': 'No Fireplace', \n    'PoolQC': 'No Pool', \n    'Fence': 'No Fence', \n    'MiscFeature': 'No Misc'\n   }\n\n# Grouping of variables dependent on the presence of other amenities\nfor i, j in other_cols_imp.items():\n    df[i].fillna(j, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:00.611173Z","iopub.execute_input":"2022-04-14T12:33:00.611878Z","iopub.status.idle":"2022-04-14T12:33:00.632626Z","shell.execute_reply.started":"2022-04-14T12:33:00.611839Z","shell.execute_reply":"2022-04-14T12:33:00.630822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** Assuming all houses have an electrical system, we will drop the obersvation having the eltrical system as a null values. ","metadata":{}},{"cell_type":"code","source":"# Deleting the Electrical \ndf.dropna(subset=['Electrical'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:00.634621Z","iopub.execute_input":"2022-04-14T12:33:00.635144Z","iopub.status.idle":"2022-04-14T12:33:00.646272Z","shell.execute_reply.started":"2022-04-14T12:33:00.63511Z","shell.execute_reply":"2022-04-14T12:33:00.645043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## No. of null values\nnull_vals = df.isna().sum().sum()\n\n# Reporting back\nprint(\"Afer imputation, we have missing {:d} values in our data.\".format(null_vals))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:00.647979Z","iopub.execute_input":"2022-04-14T12:33:00.648404Z","iopub.status.idle":"2022-04-14T12:33:00.667828Z","shell.execute_reply.started":"2022-04-14T12:33:00.648323Z","shell.execute_reply":"2022-04-14T12:33:00.666866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Inquiry Exploration\n\nIn this section, we will generate various questions to further consolidate our understanding of the problem at hand. It will allow us to guide the machine learning process more attuned to the particular subject matter.","metadata":{}},{"cell_type":"markdown","source":"**Question 1:** Do bigger houses always translate into higher prices?","metadata":{}},{"cell_type":"code","source":"## Scatterplot between lotArea and SalePrice\nsns.scatterplot(data=df, x='LotArea', y='SalePrice')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:00.669115Z","iopub.execute_input":"2022-04-14T12:33:00.669642Z","iopub.status.idle":"2022-04-14T12:33:00.983274Z","shell.execute_reply.started":"2022-04-14T12:33:00.669597Z","shell.execute_reply":"2022-04-14T12:33:00.981899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** From the scatterplot above, there is very little evidence indicating that bigger houses are ultimiately pricier. As noted in the diagnostics, the 80 initial variables show how the house valuation process is multi-dimensional.","metadata":{}},{"cell_type":"markdown","source":"**Question 2:** Where is the real estate hotspot?","metadata":{}},{"cell_type":"code","source":"# Which neighborhood registers the most sales?\ntotal = df['Neighborhood'].value_counts()[0]\nper = df['Neighborhood'].value_counts(normalize=True)[0]\nneigh_name = pd.DataFrame(df['Neighborhood'].value_counts()).index[0]\nprint(\"{} has the most houses sales with {} making up {:.2%} of all sales.\".format(neigh_name, \n                                                                                  total, per))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:00.985044Z","iopub.execute_input":"2022-04-14T12:33:00.985404Z","iopub.status.idle":"2022-04-14T12:33:00.997426Z","shell.execute_reply.started":"2022-04-14T12:33:00.985345Z","shell.execute_reply":"2022-04-14T12:33:00.996047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Which neighborhood registers the sales with the highest price tags?\ndf_grouped = pd.DataFrame(df.groupby('Neighborhood')['SalePrice'].sum())\ndf_sorted = df_grouped.sort_values('SalePrice', ascending=False)\ndf_sorted['per_total'] = (df_sorted['SalePrice'] / df_sorted['SalePrice'].sum())\n\nneigh_name = df_sorted.index[0]\ntotal = df_sorted['SalePrice'][0]\nper = df_sorted['per_total'][0]\nprint(\"{} has the highest cumulative sales amount of ${:,} making up {:.2%} of all transactions.\".format(\n                                                                                                    neigh_name, \n                                                                                                    total, per))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:00.999445Z","iopub.execute_input":"2022-04-14T12:33:00.999915Z","iopub.status.idle":"2022-04-14T12:33:01.016753Z","shell.execute_reply.started":"2022-04-14T12:33:00.999868Z","shell.execute_reply":"2022-04-14T12:33:01.015682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** As per the data dictionary, NAmes refers to Iowan city of North Ames. ","metadata":{}},{"cell_type":"markdown","source":"**Question 3:** What miscellaneous feature add the most value?","metadata":{}},{"cell_type":"code","source":"# Which miscellaneous feature is the most prevalent?\ntotal = df['MiscFeature'].value_counts()[1]\nmisc_name = pd.DataFrame(df['MiscFeature'].value_counts()).index[1]\nprint(\"For houses with miscellaneous features, {} is the most prevalent in {} houses.\".format(misc_name, total))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:01.017825Z","iopub.execute_input":"2022-04-14T12:33:01.018445Z","iopub.status.idle":"2022-04-14T12:33:01.035465Z","shell.execute_reply.started":"2022-04-14T12:33:01.018408Z","shell.execute_reply":"2022-04-14T12:33:01.034514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculating the value added\nmisc = df[df['MiscFeature'] == 'Shed']['MiscVal']\nsale = df[df['MiscFeature'] == 'Shed']['SalePrice']\navg_value_added = np.average(misc)\nper_sale = np.average(misc/sale)\nprint(\"{} brings ${:.2f} of value added making {:.2%} of the house sale price on average.\".format(\n                                                                                                misc_name, \n                                                                                                avg_value_added,\n                                                                                                per_sale))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:01.036831Z","iopub.execute_input":"2022-04-14T12:33:01.037257Z","iopub.status.idle":"2022-04-14T12:33:01.05186Z","shell.execute_reply.started":"2022-04-14T12:33:01.037216Z","shell.execute_reply":"2022-04-14T12:33:01.050622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"**Feature Scaling:** When dealing with data, we are working with different types of which required adpated pre-processing before applying any machine learning techniques. In our content, we perform feature scaling to standardize only the values in continuous numerical variables. Read more [here](https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35).","metadata":{}},{"cell_type":"code","source":"# Filter numeric columns\nnum_vars = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1',\n           'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n           'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n            'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\nscaler = StandardScaler().fit(df[num_vars].values)\ndf[num_vars] = scaler.transform(df[num_vars].values)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:01.053624Z","iopub.execute_input":"2022-04-14T12:33:01.05423Z","iopub.status.idle":"2022-04-14T12:33:01.092991Z","shell.execute_reply.started":"2022-04-14T12:33:01.05418Z","shell.execute_reply":"2022-04-14T12:33:01.092268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Categorical feature encoding** ensures that variables with categories/groupings are transformed into numerical inputs for the predictive modeling phase. The categorical variables are also subdivided as:\n- binary (two possible outcomes)\n- cardinal (no meaningful order) \n- ordinal (meaningful order) \n\nRead more [here](https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/).","metadata":{}},{"cell_type":"code","source":"# Encoding binary categorical variables\nbinary = ['CentralAir']\n\n# Applying binary encoder\nbinenc = ce.BinaryEncoder(cols = binary, return_df = True)\nbin_df = binenc.fit_transform(df)  \nbin_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:01.094053Z","iopub.execute_input":"2022-04-14T12:33:01.09476Z","iopub.status.idle":"2022-04-14T12:33:01.161666Z","shell.execute_reply.started":"2022-04-14T12:33:01.094708Z","shell.execute_reply":"2022-04-14T12:33:01.160841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of nominal categorical variables\ncardinal = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotConfig', \n            'Neighborhood', 'Condition1', 'Condition2', 'BldgType', \n            'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', \n            'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'Electrical', \n            'Functional', 'GarageType', 'MiscFeature', 'SaleType', \n            'SaleCondition']\n\n\n# Applying one-hot encoder \nohe = ce.OneHotEncoder(cols = cardinal, use_cat_names=True, return_df = True)\ndf_card_enc = ohe.fit_transform(bin_df)  \ndf_card_enc.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:01.162892Z","iopub.execute_input":"2022-04-14T12:33:01.163177Z","iopub.status.idle":"2022-04-14T12:33:01.526667Z","shell.execute_reply.started":"2022-04-14T12:33:01.163148Z","shell.execute_reply":"2022-04-14T12:33:01.525405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Encoding cardinal categorical variables\nordinal_cols_mapping = [ \n    {\"col\" : 'LotShape', \"mapping\": {'Reg':0, 'IR1': 1, 'IR2':2, 'IR3':3}},\n    {\"col\" : 'LandContour', \"mapping\": {'Low':0, 'Lvl':1, 'Bnk':2, 'HLS':3}},\n    {\"col\" : 'Utilities', \"mapping\": {'ELO':0, 'NoSeWa':1, 'NoSewr':2, 'AllPub':3}},\n    {\"col\" : 'LandSlope', \"mapping\": {'Gtl': 0, 'Mod': 1, 'Sev':2}},\n    {\"col\" : 'OverallQual', \"mapping\": {1: 0, 2: 1, 3:2, 4:3, 5:4, 6:5, 7:6, 8:7, 9:8, 10:9}},\n    {\"col\" : 'OverallCond', \"mapping\": {1: 0, 2: 1, 3:2, 4:3, 5:4, 6:5, 7:6, 8:7, 9:8, 10:9}},\n    {\"col\" : 'ExterQual', \"mapping\": {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}},\n    {\"col\" : 'ExterCond', \"mapping\": {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}},\n    {\"col\" : 'BsmtQual', \"mapping\": {'No Basement':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},\n    {\"col\" : 'BsmtCond', \"mapping\": {'No Basement':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},   \n    {\"col\" : 'BsmtExposure', \"mapping\": {'No Basement':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4}},\n    {\"col\" : 'BsmtFinType1', \"mapping\": {'No Basement':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}}, \n    {\"col\" : 'BsmtFinType2', \"mapping\": {'No Basement':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}},  \n    {\"col\" : 'HeatingQC', \"mapping\": {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}},\n    {\"col\" : 'KitchenQual', \"mapping\": {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}},\n    {\"col\" : 'FireplaceQu', \"mapping\": {'No Fireplace':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},\n    {\"col\" : 'GarageFinish', \"mapping\": {'No Garage':0, 'Unf':1, 'RFn':2, 'Fin':3}},\n    {\"col\" : 'GarageQual', \"mapping\": {'No Garage':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},\n    {\"col\" : 'GarageCond', \"mapping\": {'No Garage':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},\n    {\"col\" : 'PavedDrive', \"mapping\": {'N':0, 'P':1, 'Y':2}}, \n    {\"col\" : 'PoolQC', \"mapping\": {'No Pool':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},\n    {\"col\" : 'Fence', \"mapping\":{'No Fence':0, 'MnWw':1, 'GdWo':2, 'MnPrv':3, 'GdPrv':4}}\n]\n\n# Applying ordinal encoder\nordenc = ce.OrdinalEncoder(mapping = ordinal_cols_mapping, return_df = True)\ndf_ord_enc = ordenc.fit_transform(df_card_enc)  \ndf_ord_enc.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:01.528462Z","iopub.execute_input":"2022-04-14T12:33:01.528823Z","iopub.status.idle":"2022-04-14T12:33:01.675114Z","shell.execute_reply.started":"2022-04-14T12:33:01.528777Z","shell.execute_reply":"2022-04-14T12:33:01.674048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Datetime Variables:** There are variables denoting dates and thus, may hold significance and impact our target variable: the house's sale price. \n\nBased on research, we thought that the most sensible option would be to transform the datetime variables into ordinal categories in twofold:\n - Direct encoding of 'MoSold' and 'YrSold' having 12 and 5 pre-defined categories that are the 12 months and 5 years respectively during which the houses in the dataset were sold.\n - Binning of 'YearRemodAdd' and 'YearBuilt' into 6 categories of 10 and 20 years of interval respectively before proceding to ordinal encoding as well.","metadata":{}},{"cell_type":"code","source":"# Binning date variables in time intervals\ndf_ord_enc['YearRemodAdd'] = pd.cut(df_ord_enc['YearRemodAdd'], bins=6, precision=0).astype(str)\ndf_ord_enc['YearRemodAdd'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:01.676358Z","iopub.execute_input":"2022-04-14T12:33:01.676586Z","iopub.status.idle":"2022-04-14T12:33:01.697835Z","shell.execute_reply.started":"2022-04-14T12:33:01.676558Z","shell.execute_reply":"2022-04-14T12:33:01.696422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ord_enc['YearBuilt'] = pd.cut(df_ord_enc['YearBuilt'], bins=6, precision=0).astype(str)\ndf_ord_enc['YearBuilt'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:01.702446Z","iopub.execute_input":"2022-04-14T12:33:01.702762Z","iopub.status.idle":"2022-04-14T12:33:01.722446Z","shell.execute_reply.started":"2022-04-14T12:33:01.702725Z","shell.execute_reply":"2022-04-14T12:33:01.721433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_ord_enc['GarageYrBlt'] = pd.cut(df_ord_enc[df_ord_enc['GarageYrBlt'] != 0]['GarageYrBlt']\n                                   , bins=6, precision=0).astype(str)\ndf_ord_enc['GarageYrBlt'].fillna(\"No Garage\", inplace = True)\ndf_ord_enc['GarageYrBlt'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:01.72363Z","iopub.execute_input":"2022-04-14T12:33:01.723995Z","iopub.status.idle":"2022-04-14T12:33:01.754079Z","shell.execute_reply.started":"2022-04-14T12:33:01.723964Z","shell.execute_reply":"2022-04-14T12:33:01.753091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Datetime variable - ordinal encoding\ndate_cols_mapping = [ \n    {\"col\" : 'YrSold', \"mapping\": {2006:0, 2007: 1, 2008:2, \n                                   2009:3, 2010:4}},\n    \n    {\"col\" : 'MoSold', \"mapping\": {1: 0, 2: 1, 3:2, 4:3, 5:4, 6:5, 7:6, \n                                   8:7, 9:8, 10:9, 11:10, 12:11}},\n    \n    {\"col\" : 'YearRemodAdd', \"mapping\": {'(1950.0, 1960.0]':0, '(1960.0, 1970.0]':1,\n                                         '(1970.0, 1980.0]':2, '(1980.0, 1990.0]':3,\n                                         '(1990.0, 2000.0]':4, '(2000.0, 2010.0]':5}},\n    \n    {\"col\" : 'YearBuilt', \"mapping\": {'(1872.0, 1895.0]':0, '(1895.0, 1918.0]':1,\n                                         '(1918.0, 1941.0]':2, '(1941.0, 1964.0]':3,\n                                         '(1964.0, 1987.0]':4, '(1987.0, 2010.0]':5}},\n    \n    {\"col\" : 'GarageYrBlt', \"mapping\": {'No Garage':0, '(1900.0, 1918.0]':1,\n                                        '(1918.0, 1937.0]':2, '(1937.0, 1955.0]':3,\n                                        '(1955.0, 1973.0]':4, '(1973.0, 1992.0]':5,\n                                        '(1992.0, 2010.0]':6}},\n]\n\n# Applying label encoder\nordenc = ce.OrdinalEncoder(mapping = date_cols_mapping, return_df = True)\ndf_final = ordenc.fit_transform(df_ord_enc)  \ndf_final.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:01.755391Z","iopub.execute_input":"2022-04-14T12:33:01.755645Z","iopub.status.idle":"2022-04-14T12:33:01.809967Z","shell.execute_reply.started":"2022-04-14T12:33:01.755613Z","shell.execute_reply":"2022-04-14T12:33:01.809322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Correlation Analysis","metadata":{}},{"cell_type":"markdown","source":"**Note:** Given that we have 240 columns, it would be quite computationally intensive to display the entire correlation matrix and visualize it for user-friendly analysis. As a result, we will only filter out relatively and highly correlated relationship with coefficient between 0.7 and 1 (non-inclusive to avoid pairs of identical variables).","metadata":{}},{"cell_type":"code","source":"## Strongest Relationships\nmatrix_corr = df_final.corr()\nmatrix_corr = np.round(matrix_corr.unstack(), 2)\nstrong_rel = matrix_corr[(abs(matrix_corr) >= 0.7) & (abs(matrix_corr) != 1.00)]\nstrong_rel","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:01.811212Z","iopub.execute_input":"2022-04-14T12:33:01.81167Z","iopub.status.idle":"2022-04-14T12:33:02.069312Z","shell.execute_reply.started":"2022-04-14T12:33:01.811634Z","shell.execute_reply":"2022-04-14T12:33:02.068447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** We detected 98 relationships of which we assume 40 to be unique pairs having a correlation coefficients greather than 0.75. Given that our focus is on the sale of the house, we will filter out only relationships related to variables with 'Sale' as a prefix.","metadata":{}},{"cell_type":"code","source":"# Focus on variables directly related to the sale\nmatrix_corr = df_final.corr()\nmatrix_sale = matrix_corr.filter(regex='^Sale',axis=1)\nmatrix_sale = np.round(matrix_sale.unstack(), 2)\nstrong_rel_sale = matrix_sale[(abs(matrix_sale) >= 0.7) & (abs(matrix_sale) != 1.00)]\nstrong_rel_sale","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:02.070568Z","iopub.execute_input":"2022-04-14T12:33:02.070785Z","iopub.status.idle":"2022-04-14T12:33:02.324062Z","shell.execute_reply.started":"2022-04-14T12:33:02.070757Z","shell.execute_reply":"2022-04-14T12:33:02.323096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** Among the two detected, 'OverallQual' appears to be a key variable that is highly correlated to the sale price of the house. It probably falls inline with our expectations in terms of the valuation process of a house; indeed, it requires good to excellent quality to be desired on the real estate market.","metadata":{}},{"cell_type":"markdown","source":"# 7. Machine Learning Set-Up\n\nFirst off, we need to prepapre the data to feed the machine learning models. In doing so, we first separate the features and target variables and then proceed in creating train and testing set for model training and performance evaluation.","metadata":{}},{"cell_type":"code","source":"# Splitting features & target variable\nX = df_final.drop(['SalePrice'], axis=1).values\ny = df_final['SalePrice'].values\ny_log = np.log(y)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:02.325486Z","iopub.execute_input":"2022-04-14T12:33:02.325704Z","iopub.status.idle":"2022-04-14T12:33:02.339518Z","shell.execute_reply.started":"2022-04-14T12:33:02.325677Z","shell.execute_reply":"2022-04-14T12:33:02.338441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Training Testing Split\nX_train, X_test, y_train, y_test = train_test_split(X, y_log, \n                                                    test_size=1/3, \n                                                    random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:02.340845Z","iopub.execute_input":"2022-04-14T12:33:02.341518Z","iopub.status.idle":"2022-04-14T12:33:02.350899Z","shell.execute_reply.started":"2022-04-14T12:33:02.341471Z","shell.execute_reply":"2022-04-14T12:33:02.350102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Machine Learning - Simple Models\n\nThis section will leverage the powerful sci-kit-learn package to build multiple models with little to no parameter tuning for comparison. We will only use the cross-validation error on our training dataset to avoid any data leakage.","metadata":{}},{"cell_type":"code","source":"# Dictionary to store model structures\nmodels = {'MLR': linear_model.LinearRegression(),\n          'Ridge': linear_model.Ridge(),\n          'Lasso': linear_model.Lasso(),\n          'Elastic Net': linear_model.ElasticNet(),\n          'Decision tree': DecisionTreeRegressor()\n        }","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:02.352468Z","iopub.execute_input":"2022-04-14T12:33:02.352714Z","iopub.status.idle":"2022-04-14T12:33:02.363689Z","shell.execute_reply.started":"2022-04-14T12:33:02.352683Z","shell.execute_reply":"2022-04-14T12:33:02.36287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** Our goal is predict house prices which are all non-negative values; however, our machine learning will likely also results in some negative values. To mitiagte this issue, we performed logarithmic transformation on our target values and then obtain the error rate accordingly. Read more [here](https://stats.stackexchange.com/questions/360399/how-to-constrain-gradient-boosting-predictions-to-be-non-negative.).","metadata":{}},{"cell_type":"code","source":"# Model Building & performance evaluation\nkf = KFold(n_splits=5)\nkf.get_n_splits(X_train)\n\nfor name, model in models.items():\n    model_errs = []\n    for train_index, test_index in kf.split(X_train):\n        X_train_k = X_train[train_index] \n        y_train_k = y_train[train_index]\n        model.fit(X_train_k, y_train_k)\n        pred_log = model.predict(X_train_k)\n        # pred = np.exp(pred_log)\n        rmse = skm.mean_squared_error(y_train_k, pred_log, squared=False) \n        model_errs.append(rmse)\n        # report performance\n    print('{} - RMSLE: {:.5f} ({:.5f})' .format(name, np.mean(model_errs), \n                                               np.std(model_errs)))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:02.364932Z","iopub.execute_input":"2022-04-14T12:33:02.365237Z","iopub.status.idle":"2022-04-14T12:33:03.071541Z","shell.execute_reply.started":"2022-04-14T12:33:02.365202Z","shell.execute_reply":"2022-04-14T12:33:03.070476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 10. Machine Learning - Ensemble Methods\n\nThis section will extend our work in machine learning to incorporate ensemble methods. We generated simple models and compared the scores, which appear satisfactory; however, we may want more stability and minor variation in our predictive algorithm; it is where ensemble techniques come in. Most often, they act as a 'superposer' of multiple models throughout various ways and thus, bolster their predictive power. Further Information [here](https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/). ","metadata":{}},{"cell_type":"code","source":"# Stacking multiple models\nestimators = [\n    ('MLR', linear_model.LinearRegression()),\n    ('Ridge', linear_model.Ridge())\n]\n# Dictionary to store ensemble model structures\nensemble_models = {\n    'RF': RandomForestRegressor(),\n    'XGBoost': BaggingRegressor(base_estimator=xgb.XGBRegressor()),\n    'Stacking': StackingRegressor(estimators=estimators,\n                                   final_estimator=DecisionTreeRegressor())\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:03.072901Z","iopub.execute_input":"2022-04-14T12:33:03.073208Z","iopub.status.idle":"2022-04-14T12:33:03.078903Z","shell.execute_reply.started":"2022-04-14T12:33:03.073174Z","shell.execute_reply":"2022-04-14T12:33:03.078079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Building & performance evaluation\nkf = KFold(n_splits=5)\nkf.get_n_splits(X_train)\n\nfor name, model in ensemble_models.items():\n    model_errs = []\n    for train_index, test_index in kf.split(X_train):\n        X_train_k = X_train[train_index] \n        y_train_k = y_train[train_index]\n        model.fit(X_train_k, y_train_k)\n        pred_log = model.predict(X_train_k)\n        #pred = np.exp(pred_log)\n        rmsle = skm.mean_squared_error(y_train_k, pred_log, squared=False) \n        model_errs.append(rmsle)\n        # report performance\n    print('{} - RMSLE: {:.5f} ({:.5f})' .format(name, np.mean(model_errs), \n                                               np.std(model_errs)))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:03.080209Z","iopub.execute_input":"2022-04-14T12:33:03.080608Z","iopub.status.idle":"2022-04-14T12:33:58.602319Z","shell.execute_reply.started":"2022-04-14T12:33:03.080577Z","shell.execute_reply":"2022-04-14T12:33:58.601071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Takeaway:** In this instance, our best model is the XGBoost having the lowest RMSE for the log values of house prices. We will proceed in doing the hyperparameter tuning with the random Forest given that they offer more robust and reliable results than the simple ones. ","metadata":{}},{"cell_type":"markdown","source":"# 10. Hyperparameter Tuning\n\nThis section will walk through a process to find the best possible models given a set of parameters. In machine learning, we name it hyperparameter tuning during which the algorithm search for the set of optimal hyperarameters driving the metric as high (or low depending the case scenario) as possible. Further information [here](https://machinelearningmastery.com/hyperparameter-optimization-with-random-search-and-grid-search/). ","metadata":{}},{"cell_type":"code","source":"## Random Search\n# Define the model\nmodel = RandomForestRegressor()\n# define search space\nrf_space = {\n   'n_estimators': range(20, 100, 20),\n   'max_depth': range(3, 15, 3),\n   'min_samples_split': [2, 5, 10], \n   'min_samples_leaf': [1, 2, 4]\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:58.610379Z","iopub.execute_input":"2022-04-14T12:33:58.616257Z","iopub.status.idle":"2022-04-14T12:33:58.631086Z","shell.execute_reply.started":"2022-04-14T12:33:58.616179Z","shell.execute_reply":"2022-04-14T12:33:58.629949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search = GridSearchCV(estimator=model, param_grid=rf_space, \n                      cv=2, scoring='neg_root_mean_squared_error')\nsearch.fit(X_train, y_train)\nprint('RF - RMSE of log values: {:.5f}' .format(search.best_score_))\nprint('with best parameters: {}.\\n' .format(search.best_params_))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:33:58.633946Z","iopub.execute_input":"2022-04-14T12:33:58.635994Z","iopub.status.idle":"2022-04-14T12:35:34.342462Z","shell.execute_reply.started":"2022-04-14T12:33:58.635931Z","shell.execute_reply":"2022-04-14T12:35:34.341238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search = RandomizedSearchCV(estimator=model, param_distributions=rf_space, \n                            cv=2, scoring='neg_root_mean_squared_error')\nsearch.fit(X_train, y_train)\nprint('RF - RMSE of log values: {:.5f}' .format(search.best_score_))\nprint('with best parameters: {}.\\n' .format(search.best_params_))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:35:34.344458Z","iopub.execute_input":"2022-04-14T12:35:34.345485Z","iopub.status.idle":"2022-04-14T12:35:40.74566Z","shell.execute_reply.started":"2022-04-14T12:35:34.345429Z","shell.execute_reply":"2022-04-14T12:35:40.744656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 11. Model Performance Evaluation\n\nThis section will build on everything we've done throughout this notebook and evaluate the best model using RMSE of log values.","metadata":{}},{"cell_type":"code","source":"# Running on testing set\ntree = RandomForestRegressor(n_estimators=60, max_depth=12, \n                              min_samples_leaf=2, min_samples_split=5)\ntree.fit(X_train, y_train)\ny_pred_log = tree.predict(X_test)\nrmsle = skm.mean_squared_error(y_test, y_pred_log, squared=False) \n# report performance\nprint('RF - RMSE of log values: {:.5f}' .format(rmsle))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:35:40.747638Z","iopub.execute_input":"2022-04-14T12:35:40.747994Z","iopub.status.idle":"2022-04-14T12:35:41.628029Z","shell.execute_reply.started":"2022-04-14T12:35:40.747948Z","shell.execute_reply":"2022-04-14T12:35:41.627019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 12. Final Submission","metadata":{}},{"cell_type":"code","source":"# Loading the dataset\ntest_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:35:41.629458Z","iopub.execute_input":"2022-04-14T12:35:41.629773Z","iopub.status.idle":"2022-04-14T12:35:41.672321Z","shell.execute_reply.started":"2022-04-14T12:35:41.629731Z","shell.execute_reply":"2022-04-14T12:35:41.671356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of columns with missing values\nnull_cols = test_df.columns[test_df.isna().any()].tolist()\n\n# Imputation using group by\ntest_df['LotFrontage'] = test_df.groupby('LotShape').LotFrontage.transform(lambda x: x.fillna(x.median()))\ntest_df.LotFrontage = test_df.LotFrontage.round(2)\ntest_df['LotFrontage'].isnull().sum()\ntest_df.head()\n\n# Replacing the null values with a significant term\ntest_df['Alley'].fillna(\"No alley access\", inplace = True)\ntest_df['Alley'].value_counts()\n\n# Grouping of variables dependent on the presence of a basement\nfor i in null_cols:\n    if 'Bsmt' in i:\n        test_df[i].fillna(\"No Basement\", inplace = True)\n        \n# Grouping of variables dependent on the presence of a garage\nfor i in null_cols:\n    if 'Garage' in i and i != 'GarageYrBlt':\n        test_df[i].fillna(\"No Garage\", inplace = True)\n    elif i == 'GarageYrBlt':\n        test_df[i].fillna(0, inplace = True)\n        \nother_cols_imp = {\n    'MasVnrType': 'No Veneer',\n    'MasVnrArea': 0, \n    'FireplaceQu': 'No Fireplace', \n    'PoolQC': 'No Pool', \n    'Fence': 'No Fence', \n    'MiscFeature': 'No Misc'\n   }\n\n# Grouping of variables dependent on the presence of other amenities\nfor i, j in other_cols_imp.items():\n    test_df[i].fillna(j, inplace = True)\n\n# Deleting the Electrical \ntest_df.dropna(subset=['Electrical'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:35:41.674151Z","iopub.execute_input":"2022-04-14T12:35:41.675011Z","iopub.status.idle":"2022-04-14T12:35:41.722473Z","shell.execute_reply.started":"2022-04-14T12:35:41.674956Z","shell.execute_reply":"2022-04-14T12:35:41.721494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## No. of null values\nnull_vals = test_df.isna().sum().sum()\n\n# Reporting back\nprint(\"We are missing {:2d} values in our data at given percentages in the following columns:\" .format(null_vals))\nfor i in null_cols:\n    col_null = test_df[i].isnull().sum()\n    per_null = col_null / len(test_df[i])\n    print(\"  - {}: {} ({:.2%})\".format(i, col_null, per_null))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:35:41.723691Z","iopub.execute_input":"2022-04-14T12:35:41.723909Z","iopub.status.idle":"2022-04-14T12:35:41.764377Z","shell.execute_reply.started":"2022-04-14T12:35:41.723881Z","shell.execute_reply":"2022-04-14T12:35:41.763496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Given how few null values are left, we will drop those 12 rows.\ntest_df.dropna(subset=null_cols, inplace=True)\n\n## No. of null values\nnull_vals = test_df.isna().sum().sum()\n\n# Reporting back\nprint(\"We are missing {:2d} values in our data at given percentages in the following columns:\" .format(null_vals))","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:35:41.765572Z","iopub.execute_input":"2022-04-14T12:35:41.766051Z","iopub.status.idle":"2022-04-14T12:35:41.791553Z","shell.execute_reply.started":"2022-04-14T12:35:41.765994Z","shell.execute_reply":"2022-04-14T12:35:41.79091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Filter numeric columns\n# df = test_df\n# num_vars = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1',\n#            'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n#            'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n#             'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\n# scaler = StandardScaler().fit(df[num_vars].values)\n# df[num_vars] = scaler.transform(df[num_vars].values)\n# df.head()\n\n# # Encoding binary categorical variables\n# binary = ['CentralAir']\n\n# # Applying binary encoder\n# binenc = ce.BinaryEncoder(cols = binary, return_df = True)\n# bin_df = binenc.fit_transform(df)  \n# bin_df.head()\n\n# # List of nominal categorical/cardinal variables\n# cardinal = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotConfig', \n#             'Neighborhood', 'Condition1', 'Condition2', 'BldgType', \n#             'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', \n#             'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'Electrical', \n#             'Functional', 'GarageType', 'MiscFeature', 'SaleType', \n#             'SaleCondition']\n\n# # Applying one-hot encoder \n# ohe = ce.OneHotEncoder(cols = cardinal, use_cat_names=True, return_df = True)\n# df_card_enc = ohe.fit_transform(bin_df)  \n# df_card_enc.head()\n\n# # Encoding ordinal variables\n# ordinal_cols_mapping = [ \n#     {\"col\" : 'LotShape', \"mapping\": {'Reg':0, 'IR1': 1, 'IR2':2, 'IR3':3}},\n#     {\"col\" : 'LandContour', \"mapping\": {'Low':0, 'Lvl':1, 'Bnk':2, 'HLS':3}},\n#     {\"col\" : 'Utilities', \"mapping\": {'ELO':0, 'NoSeWa':1, 'NoSewr':2, 'AllPub':3}},\n#     {\"col\" : 'LandSlope', \"mapping\": {'Gtl': 0, 'Mod': 1, 'Sev':2}},\n#     {\"col\" : 'OverallQual', \"mapping\": {1: 0, 2: 1, 3:2, 4:3, 5:4, 6:5, 7:6, 8:7, 9:8, 10:9}},\n#     {\"col\" : 'OverallCond', \"mapping\": {1: 0, 2: 1, 3:2, 4:3, 5:4, 6:5, 7:6, 8:7, 9:8, 10:9}},\n#     {\"col\" : 'ExterQual', \"mapping\": {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}},\n#     {\"col\" : 'ExterCond', \"mapping\": {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}},\n#     {\"col\" : 'BsmtQual', \"mapping\": {'No Basement':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},\n#     {\"col\" : 'BsmtCond', \"mapping\": {'No Basement':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},   \n#     {\"col\" : 'BsmtExposure', \"mapping\": {'No Basement':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4}},\n#     {\"col\" : 'BsmtFinType1', \"mapping\": {'No Basement':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}}, \n#     {\"col\" : 'BsmtFinType2', \"mapping\": {'No Basement':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}},  \n#     {\"col\" : 'HeatingQC', \"mapping\": {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}},\n#     {\"col\" : 'KitchenQual', \"mapping\": {'Po':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}},\n#     {\"col\" : 'FireplaceQu', \"mapping\": {'No Fireplace':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},\n#     {\"col\" : 'GarageFinish', \"mapping\": {'No Garage':0, 'Unf':1, 'RFn':2, 'Fin':3}},\n#     {\"col\" : 'GarageQual', \"mapping\": {'No Garage':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},\n#     {\"col\" : 'GarageCond', \"mapping\": {'No Garage':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},\n#     {\"col\" : 'PavedDrive', \"mapping\": {'N':0, 'P':1, 'Y':2}}, \n#     {\"col\" : 'PoolQC', \"mapping\": {'No Pool':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}},\n#     {\"col\" : 'Fence', \"mapping\":{'No Fence':0, 'MnWw':1, 'GdWo':2, 'MnPrv':3, 'GdPrv':4}}\n# ]\n\n# # Applying ordinal encoder\n# ordenc = ce.OrdinalEncoder(mapping = ordinal_cols_mapping, return_df = True)\n# df_ord_enc = ordenc.fit_transform(df_card_enc)  \n# df_ord_enc.head()\n\n# # Binning date variables in time intervals\n# df_ord_enc['YearRemodAdd'] = pd.cut(df_ord_enc['YearRemodAdd'], bins=6, precision=0).astype(str)\n# df_ord_enc['YearRemodAdd'].value_counts()\n\n# df_ord_enc['YearBuilt'] = pd.cut(df_ord_enc['YearBuilt'], bins=6, precision=0).astype(str)\n# df_ord_enc['YearBuilt'].value_counts()\n\n# df_ord_enc['GarageYrBlt'] = pd.cut(df_ord_enc[df_ord_enc['GarageYrBlt'] != 0]['GarageYrBlt']\n#                                    , bins=6, precision=0).astype(str)\n# df_ord_enc['GarageYrBlt'].fillna(\"No Garage\", inplace = True)\n# df_ord_enc['GarageYrBlt'].value_counts()\n\n# # Datetime variable - ordinal encoding\n# date_cols_mapping = [ \n#     {\"col\" : 'YrSold', \"mapping\": {2006:0, 2007: 1, 2008:2, \n#                                    2009:3, 2010:4}},\n    \n#     {\"col\" : 'MoSold', \"mapping\": {1: 0, 2: 1, 3:2, 4:3, 5:4, 6:5, 7:6, \n#                                    8:7, 9:8, 10:9, 11:10, 12:11}},\n    \n#     {\"col\" : 'YearRemodAdd', \"mapping\": {'(1950.0, 1960.0]':0, '(1960.0, 1970.0]':1,\n#                                          '(1970.0, 1980.0]':2, '(1980.0, 1990.0]':3,\n#                                          '(1990.0, 2000.0]':4, '(2000.0, 2010.0]':5}},\n    \n#     {\"col\" : 'YearBuilt', \"mapping\": {'(1872.0, 1895.0]':0, '(1895.0, 1918.0]':1,\n#                                          '(1918.0, 1941.0]':2, '(1941.0, 1964.0]':3,\n#                                          '(1964.0, 1987.0]':4, '(1987.0, 2010.0]':5}},\n    \n#     {\"col\" : 'GarageYrBlt', \"mapping\": {'No Garage':0, '(1900.0, 1918.0]':1,\n#                                         '(1918.0, 1937.0]':2, '(1937.0, 1955.0]':3,\n#                                         '(1955.0, 1973.0]':4, '(1973.0, 1992.0]':5,\n#                                         '(1992.0, 2010.0]':6}},\n# ]\n\n# # Applying label encoder\n# ordenc = ce.OrdinalEncoder(mapping = date_cols_mapping, return_df = True)\n# test_df_final = ordenc.fit_transform(df_ord_enc)  \n# test_final.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:35:41.792751Z","iopub.execute_input":"2022-04-14T12:35:41.793564Z","iopub.status.idle":"2022-04-14T12:35:41.802052Z","shell.execute_reply.started":"2022-04-14T12:35:41.793523Z","shell.execute_reply":"2022-04-14T12:35:41.801351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining our orignal model\n# final_model = RandomForestRegressor(n_estimators=60, max_depth=12, \n#                               min_samples_leaf=2, min_samples_split=5)\n# final_model.fit(X, y)\n# pred_log = final_model.predict(test_df)\n# pred = np.exp(pred_log)\n# sub = pd.DataFrame({'ID': ids, 'target': pred})\n# sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T12:35:41.803206Z","iopub.execute_input":"2022-04-14T12:35:41.803981Z","iopub.status.idle":"2022-04-14T12:35:41.821343Z","shell.execute_reply.started":"2022-04-14T12:35:41.803942Z","shell.execute_reply":"2022-04-14T12:35:41.820092Z"},"trusted":true},"execution_count":null,"outputs":[]}]}