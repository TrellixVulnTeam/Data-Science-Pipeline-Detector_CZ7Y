{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 1) Load Data"},{"metadata":{},"cell_type":"markdown","source":"Firstly, we load train and test data into dataframe using pandas library because it makes handling data much easier and efficient."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd \n\ntrain_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2) Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"## Deleteing Missing Data"},{"metadata":{},"cell_type":"markdown","source":"Now we need to know how many missing values we have in each field and if they are considerable we should delete this field as we don't have it for many instances and it will not be helpful."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_na = (train_df.isnull().sum() / len(train_df)) * 100\ntrain_df_na = train_df_na.drop(train_df_na[train_df_na == 0].index).sort_values(ascending=False)[:20]\nmissing_data = pd.DataFrame({'Missing Ratio' :train_df_na})\nmissing_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see there is a high ratio of missing values in some features. I deleted the features having missing ratio greater that 80 percent. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1)\ntest_df = test_df.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding a New Feature"},{"metadata":{},"cell_type":"markdown","source":"Now we need some feature engineering. These new features are some new meaningful features which I added to train and test datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in [train_df, test_df]:\n    dataset['SF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF'] \n    dataset['TotalBath'] = dataset['BsmtFullBath'] + (1/2) * dataset['BsmtHalfBath'] + dataset['FullBath'] + (1/2) * dataset['HalfBath']\n    dataset['HasPool'] = dataset['PoolArea'].apply(lambda x : 1 if x>0 else 0)\n    dataset['Has2ndFloor'] = dataset['2ndFlrSF'].apply(lambda x : 1 if x>0 else 0)\n    dataset['HasGarage'] = dataset['GarageArea'].apply(lambda x : 1 if x>0 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then 'Id' feature can be deleted as it doesn't give us any information and it's not needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['Id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting Categorical Features to Numerical"},{"metadata":{},"cell_type":"markdown","source":"There are some categorical features which their categories are actually ordinal so it can be a good idea to convert them to numerical features. For instance \"ExerQual\" feature has values below :<br>\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Ex  &emsp; Excellent<br>\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Gd\t &emsp; Good<br>\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; TA\t &emsp; Average/Typical<br>\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Fa\t &emsp; Fair<br>\n&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; Po  &emsp; Poor<br>\nso they can be mapped to numbers from 1 to 5 to conserve their ordinal quality."},{"metadata":{"trusted":true},"cell_type":"code","source":"LotShape_mapping = {\"Reg\": 1, \"IR1\": 2, \"IR2\": 3, \"IR3\": 4}\nLandSlope_mapping = {\"Gtl\": 1, \"Mod\": 2, \"Sev\": 3}\nExterQual_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nExterCond_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nBsmtQual_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5, \"NA\": 6}\nBsmtCond_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5, \"NA\": 6}\nBsmtExposure_mapping = {\"Gd\": 1, \"Av\": 2, \"Mn\": 3, \"No\": 4, \"NA\": 5}\nBsmtFinType1_mapping = {\"GLQ\": 1, \"ALQ\": 2, \"BLQ\": 3, \"Rec\": 4, \"LwQ\": 5, \"Unf\": 6, \"NA\": 7}\nBsmtFinType2_mapping = {\"GLQ\": 1, \"ALQ\": 2, \"BLQ\": 3, \"Rec\": 4, \"LwQ\": 5, \"Unf\": 6, \"NA\": 7}\nHeatingQC_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nCentralAir_mapping = {\"N\": 0, \"Y\": 1}\nKitchenQual_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nFireplaceQu_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nGarageQual_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nGarageCond_mapping = {\"Ex\": 1, \"Gd\": 2, \"TA\": 3, \"Fa\": 4, \"Po\": 5}\nPavedDrive_mapping = {\"Y\": 1, \"P\": 2, \"N\": 3}\n\nfor dataset in [train_df, test_df]:\n    dataset['LotShape'] = dataset['LotShape'].map(LotShape_mapping)\n    dataset['LandSlope'] = dataset['LandSlope'].map(LandSlope_mapping)\n    dataset['ExterQual'] = dataset['ExterQual'].map(ExterQual_mapping)\n    dataset['ExterCond'] = dataset['ExterCond'].map(ExterCond_mapping)\n    dataset['BsmtQual'] = dataset['BsmtQual'].map(BsmtQual_mapping)    \n    dataset['BsmtCond'] = dataset['BsmtCond'].map(BsmtCond_mapping)    \n    dataset['BsmtExposure'] = dataset['BsmtExposure'].map(BsmtExposure_mapping)    \n    dataset['BsmtFinType1'] = dataset['BsmtFinType1'].map(BsmtFinType1_mapping)    \n    dataset['BsmtFinType2'] = dataset['BsmtFinType2'].map(BsmtFinType2_mapping)    \n    dataset['HeatingQC'] = dataset['HeatingQC'].map(HeatingQC_mapping) \n    dataset['CentralAir'] = dataset['CentralAir'].map(CentralAir_mapping)        \n    dataset['KitchenQual'] = dataset['KitchenQual'].map(KitchenQual_mapping)             \n    dataset['GarageQual'] = dataset['GarageQual'].map(GarageQual_mapping)        \n    dataset['GarageCond'] = dataset['GarageCond'].map(GarageCond_mapping)        \n    dataset['PavedDrive'] = dataset['PavedDrive'].map(PavedDrive_mapping)        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3) Feature Selection"},{"metadata":{},"cell_type":"markdown","source":"## Selecting Numerical Features Using Corrolation"},{"metadata":{},"cell_type":"markdown","source":"Below you can see the heatmap of corrolation matrix which shows how corrolated each pair of numerical features are. This can asist us in filtering some features for our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorr_matrix = train_df.corr()\nf, ax = plt.subplots(figsize=(25, 20))\nsns.heatmap(corr_matrix, vmax=.8, cmap=\"Blues\", square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to know how much each feature is useful for us to predict prices which means it should be corrolated with 'SalePrice'. After calculating corrolations we gain an insight of the most important features which we should use them in our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\nn_largest_top = 31\nn_smallest_top = 15\ntop_large_corr = corr_matrix.nlargest(n_largest_top, 'SalePrice')['SalePrice']\ntop_small_corr = corr_matrix.nsmallest(n_smallest_top, 'SalePrice')['SalePrice']\nprint(\"Top Largest Corrolations :\") \nprint(top_large_corr)\nprint(\"____________________________\")\nprint(\"Top Smallest Corrolations :\") \nprint(top_small_corr)\nnum_attrs = np.append(top_large_corr.index.values, top_small_corr.index.values, axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I chose the features which are strongly corrolated with 'SalePrice' as numerical attributed which will be used in future model."},{"metadata":{"trusted":true},"cell_type":"code","source":"cm = train_df[num_attrs].corr()\nf, ax = plt.subplots(figsize=(20, 15))\nhm = sns.heatmap(cm, cmap=\"Blues\", annot=True, square=True, fmt='.2f', yticklabels=num_attrs, annot_kws={'size': 8}, xticklabels=num_attrs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Selecting Categorical Features"},{"metadata":{},"cell_type":"markdown","source":"Now it's time to select useful categorical features for our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for attr in train_df.select_dtypes(include='object'):\n    print(train_df[[attr, \"SalePrice\"]].groupby([attr], as_index=False).mean().sort_values(by='SalePrice', ascending=False))\n    print(\"\\n ________________________ \\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_attrs =[\"MSZoning\",\"LandContour\", \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"HouseStyle\", \"RoofStyle\", \n            \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \"Electrical\", \"Functional\", \n            \"GarageType\", \"GarageFinish\", \"SaleType\", \"SaleCondition\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4) More Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"## Deleting Outliers"},{"metadata":{},"cell_type":"markdown","source":"Now it's time to delete outliers in our data as they can degrade our model and prediction. Below we plot the three most corrolated features with 'SalePrice' and if there exist any outlier we will remove them."},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in ['OverallQual', 'SF', 'GrLivArea']:\n    data = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\n    data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), label= 'Affect of ' + var + ' on SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_indexes = train_df.sort_values(by='SF', ascending=False)[:2].index\ntrain_df = train_df.drop(drop_indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for var in ['OverallQual', 'SF', 'GrLivArea']:\n    data = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\n    data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), label= 'Affect of ' + var + ' on SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Log Transform"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\n\nprint(train_df['SalePrice'].describe())\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nsns.distplot(train_df['SalePrice'].dropna() , fit=stats.norm);\nplt.subplot(1,2,2)\nprob=stats.probplot(train_df['SalePrice'].dropna(), plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can observe that there is long tail of outlying properties with high sale prices. This causes to biase the mean much higher than the median. For normalizing 'SalePrice' we use log transform."},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\n\ntrain_df['SalePrice'] = [np.log(x) for x in train_df['SalePrice']]\n\nprint(train_df['SalePrice'].describe())\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nsns.distplot(train_df['SalePrice'].dropna() , fit=stats.norm);\nplt.subplot(1,2,2)\nprob=stats.probplot(train_df['SalePrice'].dropna(), plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5) Data Transformation"},{"metadata":{},"cell_type":"markdown","source":"## Transformation Pipelines"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attr_names):\n        self.attr_names = attr_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attr_names].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\n\nclass LabelBinarizerPipelineFriendly(MultiLabelBinarizer):\n    def fit(self, X, y=None):\n        super(LabelBinarizerPipelineFriendly,self).fit(X)\n    def transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).transform(X)\n    def fit_transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).fit(X).transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pipeline for numerical features which aplies an imputer to put median of each feature for instances which doesn't have value for that feature and after that standardizing features using standard scaler."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\nnum_data_attrs = [x for x in num_attrs if not x=='SalePrice']\nnum_pipeline = Pipeline([\n    ('selector', DataFrameSelector(num_data_attrs)),\n    ('imputer',  SimpleImputer(strategy=\"median\")),\n    ('std_scaler', StandardScaler()),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another pipeline for categorical features which first use an imputer to replace missing values of features which the most frequent value and then applies a label binarizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_pipeline = Pipeline([\n    ('selector', DataFrameSelector(cat_attrs)),\n    ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n    ('binarizer', LabelBinarizerPipelineFriendly()),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And the full pipeline for transforming our data which consists of the two pipelines you see above."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import FeatureUnion\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('num_pipeline', num_pipeline),\n    ('cat_pipeline', cat_pipeline),\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Deleting more outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_outliers(model, X, y, sigma=3):\n    y_pred = pd.Series(model.predict(X), index=y.index)\n        \n    resid = y - y_pred\n    mean_resid = resid.mean()\n    std_resid = resid.std()\n\n    z = (resid - mean_resid)/std_resid    \n    outliers = z[abs(z)>sigma].index\n    return outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\ntrain_data = train_df.drop(['SalePrice'], axis=1)\ntrain_labels = train_df['SalePrice']\n\nmodel = Ridge()\ntrain_prepared = full_pipeline.fit_transform(train_data)\nmodel.fit(train_prepared, train_labels)\noutliers = find_outliers(model, train_prepared, train_labels)\ntrain_df = train_df.drop(np.asarray(outliers) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing Train Data for Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_df.drop(['SalePrice'], axis=1)\ntrain_labels = train_df['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_prepared = full_pipeline.fit_transform(train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6) Testing Models"},{"metadata":{},"cell_type":"markdown","source":"## Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\n\ndef rms_score_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, train_prepared, train_labels, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlinear_reg = LinearRegression()\nprint(\"Linear Regression : \", rms_score_cv(linear_reg) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nridge = Ridge(alpha=0.6, solver='cholesky')\nrms_score_cv(ridge)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lasso Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n# lasso = Lasso(alpha =0.0005, random_state=1)\nrms_score_cv(lasso)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Elastic Net Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\n\ne_net = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n# e_net = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\nrms_score_cv(e_net)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ransac"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RANSACRegressor\n\nransac = RANSACRegressor()\nrms_score_cv(ransac)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVM Regression"},{"metadata":{},"cell_type":"markdown","source":"Linear SVM Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVR\n\nsvm_reg = LinearSVR(epsilon=0.01)\nrms_score_cv(svm_reg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Non-linear SVM Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\n\nsvm_poly_reg = SVR(kernel='poly', degree=2, C=100, epsilon=0.1)\nrms_score_cv(svm_poly_reg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndecision_tree = DecisionTreeRegressor(max_depth=7, random_state=42)\nrms_score_cv(decision_tree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrand_forest = RandomForestRegressor(max_depth=15, random_state=42)\nrms_score_cv(rand_forest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Learning"},{"metadata":{},"cell_type":"markdown","source":"### Voting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\n\nvoting_reg = VotingRegressor(\n        estimators=[('e_net', e_net), ('ridge', ridge), ('svm_poly_reg', svm_poly_reg), ('rand_forest', rand_forest)])\nrms_score_cv(voting_reg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bagging"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingRegressor\n\nbagging_reg = BaggingRegressor(e_net, n_estimators=300, bootstrap=True, n_jobs=-1)\n# rms_score_cv(bagging_reg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pasting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingRegressor\n\nbagging_reg = BaggingRegressor(e_net, n_estimators=500, bootstrap=False, n_jobs=-1)\n# rms_score_cv(bagging_reg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\n\nextra_tree = ExtraTreesRegressor(random_state=42)\nrms_score_cv(extra_tree)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AdaBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\n\nada_boost = AdaBoostRegressor(e_net, n_estimators=500, learning_rate=0.5)\n# rms_score_cv(ada_boost)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr = GradientBoostingRegressor(max_depth=5, n_estimators=15, learning_rate=0.6)\nrms_score_cv(gbr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stacking"},{"metadata":{},"cell_type":"markdown","source":"1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"estimators = [ e_net, ridge, svm_poly_reg, rand_forest, extra_tree]\nslice = int((0.8)*(len(train_prepared)))\ntrain_d = train_prepared[:slice]\ntrain_l = train_labels[:slice]\nval_d = train_prepared[slice: ]\nval_l = train_labels[slice:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for estimator in estimators:\n  estimator.fit(train_d, train_l)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_preds = np.empty((len(val_d), len(estimators)), dtype=np.float32)\nfor index, estimator in enumerate(estimators):\n    val_preds[:, index] = model.predict(val_d)\nblender = RandomForestRegressor(n_estimators=200, oob_score=True, random_state=42)\nblender.fit(val_preds, val_l)\nblender.oob_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import RegressorMixin\nfrom sklearn.base import clone\nfrom sklearn.model_selection import KFold\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n    def fit(self, X, y=None):\n        \n        X = X\n        y = y.values\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    def get_metafeatures(self, X):\n        return np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(base_models = (e_net, ridge, svm_poly_reg, rand_forest), meta_model = lasso)\n\n# rms_score_cv(stacked_averaged_models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7)Predict Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"el_net = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n# stacked_averaged_models = StackingAveragedModels(base_models = (e_net, ridge, svm_poly_reg, rand_forest, extra_tree),meta_model = lasso)\n\nel_net.fit(train_prepared, train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_prepared = full_pipeline.transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = el_net.predict(test_prepared)\npredSalePrice = [np.exp(x) for x in pred]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\noutput = pd.DataFrame({\n    'Id' : test_df['Id'],\n    'SalePrice' : predSalePrice\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.head() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output.to_csv('prediction.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}