{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport optuna\nfrom sklearn import linear_model\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\nfrom optuna.samplers import RandomSampler, GridSampler, TPESampler\nimport sklearn\nimport xgboost as xgb\nfrom scipy.misc import derivative\nfrom sklearn.metrics import mean_squared_error\nimport pickle\nimport category_encoders as ce \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nimport random\nimport lightgbm as lgb\nimport random","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\nThis notebook will walk you through step by step how to tune lightGBM using Optuna. Missing values\nwill be directly handled by LightGBM\n\nIt will generate an out-of-sample file which can be used for further stacking.\n\nParameters will be tuned in sequence. Once a value for a parameter is found, it will be fixed for \nsubsequent tuning. \"cparams\" class will be used to store all parameters and track the tuning process.\n\nThe sequence of tuning is that:\n\nnum_leaves -> min_data_in_leaf/min_sum_hessian_in_leaf -> bagging_fraction/bagging_freq/feature_fraction/ \n-> l1 and l2 regularization -> lower down learning rate\n\nThe meaning of parameters will be explained later on. \n\nThe main purpose is to demonstrate the tuning process. You can further tune the model by increasing the number\nof trails.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seed_everything(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train =  pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ndf_test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ny_train = np.log(df_train['SalePrice'])\nX_train = df_train.drop(['SalePrice','Id'],axis=1)\nX_test = df_test.drop('Id',axis=1)\nprint('Train data: ',X_train.shape)\nprint('Test data: ',X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Seperate features into nominal, ordinal and categorical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_nominal_features = pickle.load(open('../input/features-housing/cat_nominal_features.p', \"rb\" ))\ncat_ordinal_features = pickle.load(open('../input/features-housing/cat_ordinal_features.p', \"rb\" ))\nnum_features = pickle.load(open('../input/features-housing/num_features.p', \"rb\" ))\ncat_features = cat_nominal_features + cat_ordinal_features\nprint('Number of numeric features: ',len(num_features))\nprint('Number of ordinal features: ',len(cat_ordinal_features))\nprint('Number of nominal featuures: ',len(cat_nominal_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing values will be automatically handled by LightGBM, sometimes\nthis may give you better results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[cat_ordinal_features] = X_train[cat_ordinal_features].fillna(np.nan)\nX_train[cat_nominal_features] = X_train[cat_nominal_features].fillna(np.nan)\nX_test[cat_ordinal_features] = X_test[cat_ordinal_features].fillna(np.nan)\nX_test[cat_nominal_features] = X_test[cat_nominal_features].fillna(np.nan)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Construct one-hot and ordinal encoder using category_encoder","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ord_mapping=[{'col': 'Street', 'mapping': {'Grvl': 1, 'Pave': 2}},\n        {'col': 'Alley', 'mapping': {np.nan:0,'Grvl': 1, 'Pave': 2}}, \n        {'col': 'Utilities', 'mapping': {'NoSeWa': 1, 'AllPub':2}},\n        {'col': 'ExterQual', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'ExterCond', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'BsmtCond', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'BsmtQual', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'BsmtExposure', 'mapping': {np.nan:0,'No':1,'Mn':2,'Av':3,\n                                        'Gd':4}},\n        {'col': 'BsmtFinType1', 'mapping': {np.nan:0,'Unf':1,'LwQ':2,'Rec':3,\n                                        'BLQ':4,'ALQ':5,'GLQ':6}},\n        {'col': 'BsmtFinType2', 'mapping': {np.nan:0,'Unf':1,'LwQ':2,'Rec':3,\n                                        'BLQ':4,'ALQ':5,'GLQ':6}},     \n        {'col': 'HeatingQC', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'CentralAir', 'mapping': {'Y':1,'N':0}},\n        {'col': 'KitchenQual', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}}, \n        {'col': 'Functional', 'mapping': {'Typ':8,'Min1':7,'Min2':6,\n                                        'Mod':5,'Maj1':4,'Maj2':3,\n                                         'Sev':2,\"Sal\":1}},                                    \n        {'col': 'FireplaceQu', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'GarageFinish', 'mapping': {np.nan:0,'Unf':1,'RFn':2,'Fin':3}},\n        {'col': 'GarageQual', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'GarageCond', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'PavedDrive', 'mapping': {'N':1,'P':2,'Y':3}},\n        {'col': 'PoolQC', 'mapping': {np.nan:0,'Fa':1,'TA':2,\n                                        'Gd':3,'Ex':4}},\n        {'col': 'Fence', 'mapping': {np.nan:0,'MnWw':1,'GdWo':2,'MnPrv':3,\n                                        'GdPrv':4}}]\nmapping_cols = ['Street','Alley','Utilities','ExterQual','ExterCond','BsmtCond','BsmtQual','BsmtExposure',\n               'BsmtFinType1','BsmtFinType2','HeatingQC','CentralAir','KitchenQual',\n               'Functional','FireplaceQu','GarageFinish','GarageQual','GarageCond','PavedDrive',\n               'PoolQC','Fence']\n\ndef construct_ord_nom(ordinal_features,nominal_features):\n    mapp = []\n    ord_features = []\n    for c in ordinal_features:\n        if c not in mapping_cols:\n            continue\n        idx = mapping_cols.index(c)\n        mapp.append(ord_mapping[idx])\n        ord_features.append(c)\n    ce_ord = ce.OrdinalEncoder(cols=ord_features,mapping=mapp,\n                               handle_unknown='return_nan',handle_missing='return_nan')\n    ce_nom = ce.OneHotEncoder(cols=nominal_features,handle_unknown='return_nan',handle_missing='return_nan')\n    return ce_ord,ce_nom \n\ndef get_CT(ord_features,nom_features,num_features_new,ce_ord,ce_nom):\n    numeric_transformer = Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='constant',fill_value=-1)),\n            ])\n    ce_ord, ce_nom = construct_ord_nom(ord_features,nom_features)\n    ct1 = ColumnTransformer(\n            transformers=[\n                ('nominal',ce_nom,nom_features),\n                ('ordinal',ce_ord,ord_features),\n                ('num',numeric_transformer,num_features_new)\n                ],remainder = 'passthrough')\n    return ct1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ce_ord, ce_nom = construct_ord_nom(cat_ordinal_features,cat_nominal_features)\nCT = get_CT(cat_ordinal_features,cat_nominal_features,num_features,ce_ord,ce_nom)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transform the dataset and get it ready for tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_new = CT.fit_transform(X_train)\nX_test_new = CT.transform(X_test)\nprint(X_train_new.shape)\nprint(X_test_new.shape)\ndtrain = lgb.Dataset(X_train_new,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create a class to track all parameters and tuning process","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class cparams():\n    def __init__(self): \n        self.seed = 0\n        self.num_iterations = 100 # Default = 100\n        self.learning_rate = 0.1 # Default = 0.1\n        self.num_leaves = 31 #Default = 31\n        self.min_child_samples = 20 #Default = 20\n        self.min_child_weight = 0.001 #Default = 0.001\n        self.bagging_fraction = 1.0\n        self.feature_fraction = 1.0\n        self.bagging_freq = 0\n        self.alpha = 0.25\n        self.gamma = 2.0\n        self.l1 = 0.0\n        self.l2 = 0.0\n        \n    def calibrate(self,num_round):\n        \n        param = {'boosting_type': 'gbdt', \n                'objective': 'regression',\n                'metric': 'rmse', \n                'learning_rate': self.learning_rate, \n                'num_leaves': self.num_leaves,     \n                'min_data_in_leaf': self.min_child_samples,   \n                'min_sum_hessian_in_leaf':self.min_child_weight, \n                'bagging_fraction': self.bagging_fraction, \n                'bagging_freq': self.bagging_freq,\n                'feature_fraction': self.feature_fraction, \n                'lambda_l1': self.l1,\n                'lambda_l2': self.l2,\n                'seed': self.seed\n        }\n        \n        # cv's seed used to generate folds passed to numpy.random.seed\n        bst = lgb.cv(param, dtrain,num_boost_round=num_round, stratified=False, \\\n                     shuffle=True,early_stopping_rounds=100,verbose_eval=10,seed=0)\n        return bst\n    \n    def get_param(self):\n        \n        param = {'boosting_type': 'gbdt', \n                 'objective': 'regression',\n                'metric': 'rmse', \n                'learning_rate': self.learning_rate, \n                'num_leaves': self.num_leaves,     \n                'min_data_in_leaf': self.min_child_samples,   \n                'min_sum_hessian_in_leaf':self.min_child_weight, \n                'bagging_fraction': self.bagging_fraction, \n                'bagging_freq': self.bagging_freq,\n                'feature_fraction': self.feature_fraction, \n                'lambda_l1': self.l1,\n                'lambda_l2': self.l2,\n                'seed': self.seed\n        }        \n        \n        return param","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_model = cparams()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Model Checking","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For this baselien model, we are going to\nuse the default parameters. This would give us a sense \nhow well our model is performing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use default parameters\n# num_iterations(num_boost_round) = 100\n# max_depth = -1\n# num_leaves = 31\n# min_data_in_leaf(min_child_samples) = 20\n# min_sum_hessian_in_leaf(min_child_weight) = 0.001\n# feature_fraction(colsample_bytree) = 1.0\n# bagging_fraction(subsample) = 1.0\n# bagging_freq(subsample_freq) = 0\n# learning_rate = 0.1\n\nparams = {\n    'objective': 'regression',\n    'metric': 'rmse', \n    \"verbosity\": 1,\n    \"boosting_type\": \"gbdt\",\n    'seed':0\n}\n\neval_history = lgb.cv(\n    params, dtrain, verbose_eval=20,\n    stratified=False, num_boost_round=1000, early_stopping_rounds=100,\n    nfold=5,seed=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best score: ', eval_history['rmse-mean'][-1])\nprint('Number of estimators: ', len(eval_history['rmse-mean']))\ncurrent_model.num_iterations = len(eval_history['rmse-mean'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tune leaves\nThe next step is tune num_leaves parameter which is important\nsince it controls the complexity of the model\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"study_name2 = 'lgb_leaves'\nstudy_leaves = optuna.create_study(study_name=study_name2,direction='maximize',sampler=TPESampler(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def opt_leaves(trial):\n    \n    params = {\n        'objective': 'regression',\n        'metric': 'rmse', \n        \"verbosity\": 1,\n        \"boosting_type\": \"gbdt\",\n        'seed':0,\n        'num_leaves':int(trial.suggest_loguniform(\"num_leaves\", 3,32))\n    }\n    \n    score = lgb.cv(\n        params, dtrain, verbose_eval=0, \n        stratified=False, num_boost_round=current_model.num_iterations,\n        nfold=5,seed=0)\n    return -score['rmse-mean'][-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study_leaves.optimize(opt_leaves, n_trials=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of trials: ',len(study_leaves.trials))\ntrial_leaves = study_leaves.best_trial\nprint('Best score : {}'.format(-trial_leaves.value))\nfor key, value in trial_leaves.params.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_model.num_leaves = int(trial_leaves.params['num_leaves'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Tune min child sample weight and min data in leaf\nmin_data_in_leaf and min_sum_hessian_in_leaf again these two parameters control the \ncomplexity of the model. If you have too few data in a leaf or min_sum_hessian_in_leaf is too small, \nthis may lead to overfitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"study_name3 = 'lgb_child_weight_sample'\nstudy_sample_weight = optuna.create_study(study_name=study_name3,direction='maximize',sampler=TPESampler(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def opt_sample_weight(trial):\n    \n    params = {\n        'objective': 'regression',\n        'metric': 'rmse', \n        \"verbosity\": 1,\n        \"boosting_type\": \"gbdt\",\n        'seed':0,\n        'num_leaves':current_model.num_leaves,\n        'min_data_in_leaf':int(trial.suggest_discrete_uniform('data_in_leaf',4,32,q=2)),\n        'min_sum_hessian_in_leaf':trial.suggest_discrete_uniform('min_hessian',0.001,0.003,q=0.0005)\n    }\n    \n    score = lgb.cv(\n        params, dtrain, verbose_eval=0, \n        stratified=False, num_boost_round=current_model.num_iterations,\n        nfold=5,seed=0)\n    return -score['rmse-mean'][-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study_sample_weight.optimize(opt_sample_weight, n_trials=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of trials: ',len(study_sample_weight.trials))\ntrial_sample_weight = study_sample_weight.best_trial\nprint('Best score : {}'.format(-trial_sample_weight.value))\nfor key, value in trial_sample_weight.params.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_model.min_child_samples = int(trial_sample_weight.params['data_in_leaf'])\ncurrent_model.min_child_weight = trial_sample_weight.params['min_hessian']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature fraction / Bagging fraction/ Bagging frequency\n\nFeature fraction:\nLightGBM will randomly select part of features on each iteration (tree) if feature_fraction smaller than 1.0. For example, if you set it to 0.8, LightGBM will select 80% of features before training each tree\n\ncan be used to speed up training\n\ncan be used to deal with over-fitting\n\nBagging fraction: like feature_fraction, but this will randomly select part of data without resampling\n\nBagging frequency: frequency for bagging","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"study_name4 = 'lgb_bagging'\nstudy_bagging = optuna.create_study(study_name=study_name4,direction='maximize',sampler=TPESampler(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def opt_bagging(trial):\n    \n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',  \n        \"verbosity\": 1,\n        \"boosting_type\": \"gbdt\",\n        'seed':0,\n        'num_leaves':current_model.num_leaves,\n        'min_data_in_leaf':current_model.min_child_samples,\n        'min_sum_hessian_in_leaf':current_model.min_child_weight,\n        'bagging_fraction': trial.suggest_discrete_uniform('bfrac',0.4,1.0,q=0.05),\n        'bagging_freq': int(trial.suggest_discrete_uniform('bfreq',1,7,q=1.0)),\n        'feature_fraction':trial.suggest_discrete_uniform('feature',0.4,1.0,q=0.05)\n    }\n    \n    score = lgb.cv(\n        params, dtrain, verbose_eval=0, \n        stratified=False, num_boost_round=current_model.num_iterations,\n        nfold=5,seed=0)\n    return -score['rmse-mean'][-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study_bagging.optimize(opt_bagging, n_trials=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of trials: ',len(study_bagging.trials))\ntrial_bagging = study_bagging.best_trial\nprint('Best score : {}'.format(trial_bagging.value))\nfor key, value in trial_bagging.params.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_model.bagging_fraction = trial_bagging.params['bfrac']\ncurrent_model.bagging_freq = int(trial_bagging.params['bfreq'])\ncurrent_model.feature_fraction = trial_bagging.params['feature']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# L1 and L2 regularization\nlambda_l1: l1 regularization\n\nlambda_l2: l2 regularization\n\nHigher values would make the model more conservative","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"study_name5 = 'l1_l2'\nstudy_reg = optuna.create_study(study_name=study_name5,direction='maximize',sampler=TPESampler(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def opt_reg(trial):\n    \n    params = {\n        'objective': 'regression',\n        'metric': 'rmse', \n        \"verbosity\": 1,\n        \"boosting_type\": \"gbdt\",\n        'seed':0,\n        'num_leaves':current_model.num_leaves,\n        'min_data_in_leaf':current_model.min_child_samples,\n        'min_sum_hessian_in_leaf':current_model.min_child_weight,\n        'bagging_fraction': current_model.bagging_fraction,\n        'bagging_freq': current_model.bagging_freq,\n        'feature_fraction':current_model.feature_fraction,\n        'lambda_l1': trial.suggest_loguniform(\"lambda_l1\", 1e-7, 10),\n        'lambda_l2': trial.suggest_loguniform(\"lambda_l2\", 1e-7, 10)\n    }\n    \n    score = lgb.cv(\n        params, dtrain, verbose_eval=0, \n        stratified=False, num_boost_round=current_model.num_iterations,\n        nfold=5,seed=0)\n    return -score['rmse-mean'][-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"study_reg.optimize(opt_reg, n_trials=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total number of trials: ',len(study_reg.trials))\ntrial_reg = study_reg.best_trial\nprint('Best score : {}'.format(trial_reg.value))\nfor key, value in trial_reg.params.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_model.l1 = trial_reg.params['lambda_l1']\ncurrent_model.l2 = trial_reg.params['lambda_l2']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lower down learning rate\nOnce we found all the parameters, we fix them and start lowering \ndown learning rate. Typically, as learning rate decreases, number of trees would\ngo up, we would want to find optimal number of trees with optimal learning rate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"current_model.learning_rate = 0.05\nlr1 = current_model.calibrate(10000) \nprint('Best score: ', lr1['rmse-mean'][-1])\nprint('Number of estimators: ', len(lr1['rmse-mean']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_model.learning_rate = 0.01\nlr2 = current_model.calibrate(10000) \nprint('Best score: ', lr2['rmse-mean'][-1])\nprint('Number of estimators: ', len(lr2['rmse-mean']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"current_model.learning_rate = 0.005\nlr3 = current_model.calibrate(10000) \nprint('Best score: ', lr3['rmse-mean'][-1])\nprint('Number of estimators: ', len(lr3['rmse-mean']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Best score we found is 0.121 with number of estimators to be 3920\nYou can experiment more learning rates if you want","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Finalize Model\nWe are going to get current parameters, do 5-fold cross validation \nto get our out-of-sample file which can be further used for stacking.\nWe are also going to train a model on the entire training set and make our predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get Current Parameters\ncurrent_model.learning_rate = 0.005 #Based on what we found above\ncurrent_param = current_model.get_param()\nfor key, value in current_param.items():\n    print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\ndef cv_training(train_data,y_train_data):\n    kFold = KFold(n_splits=5, random_state=0, shuffle=True)\n    models = []\n    eval_history = []\n    oof_pred = []\n    oof_target = []\n    scores = []\n    for fold, (trn_idx, val_idx) in enumerate(kFold.split(train_data)):\n        #print(trn_idx)\n        #print(val_idx)\n        X_train = train_data[trn_idx]\n        X_val = train_data[val_idx]\n        y_train = y_train_data[trn_idx]\n        y_val = y_train_data[val_idx]\n        dtrain =  lgb.Dataset(X_train,y_train)\n        dval =  lgb.Dataset(X_val,y_val)\n        evals_result = {}\n        model = lgb.train(current_param, dtrain,num_boost_round=5000,\n                          evals_result=evals_result,valid_sets=dval,verbose_eval=20,early_stopping_rounds=100)\n        models.append(model)\n        y_pred = model.predict(X_val)\n        score_temp = np.sqrt(mean_squared_error(y_pred,y_val))\n        scores.append(score_temp)\n        oof_pred.append(y_pred)\n        oof_target.append(y_val)\n        eval_history.append(evals_result)\n    oof_pred = np.concatenate((oof_pred[0],oof_pred[1],oof_pred[2],\n                               oof_pred[3],oof_pred[4]))\n    oof_target = np.concatenate((oof_target[0],oof_target[1],oof_target[2],\n                                 oof_target[3],oof_target[4]))\n    oof_df = pd.DataFrame({'predictions':oof_pred,'target':oof_target})\n    return models, eval_history, scores, oof_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models, eval_history,scores,oof_df = cv_training(X_train_new,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"oof_df.to_csv('oof_df.csv',index=False)\noof_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check average out of sample score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score = 0.0\nfor i in range(len(models)):\n    print(models[i].best_iteration)\n    print(eval_history[i]['valid_0']['rmse'][-1])\n    score = score + eval_history[i]['valid_0']['rmse'][-1]\nprint('Average score: ',score/5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Make Predictions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of boost_round will be based on what we found above\nmodel_final = lgb.train(current_param, dtrain,num_boost_round=3920)\nypred = model_final.predict(X_test_new)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a histo\nplt.hist(np.exp(ypred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\nsubmission  = pd.DataFrame({\n    'Id': sub['Id'],\n    'SalePrice': np.exp(ypred)\n})\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}