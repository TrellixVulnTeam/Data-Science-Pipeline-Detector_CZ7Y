{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split ,KFold,StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.feature_selection import mutual_info_regression\n\nfrom collections import Counter\n\nimport math \nfrom scipy import stats as ss\n#import scipy.stats as ss\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\ndef convert(data, to):\n    converted = None\n    if to == 'array':\n        if isinstance(data, np.ndarray):\n            converted = data\n        elif isinstance(data, pd.Series):\n            converted = data.values\n        elif isinstance(data, list):\n            converted = np.array(data)\n        elif isinstance(data, pd.DataFrame):\n            converted = data.as_matrix()\n    elif to == 'list':\n        if isinstance(data, list):\n            converted = data\n        elif isinstance(data, pd.Series):\n            converted = data.values.tolist()\n        elif isinstance(data, np.ndarray):\n            converted = data.tolist()\n    elif to == 'dataframe':\n        if isinstance(data, pd.DataFrame):\n            converted = data\n        elif isinstance(data, np.ndarray):\n            converted = pd.DataFrame(data)\n    else:\n        raise ValueError(\"Unknown data conversion: {}\".format(to))\n    if converted is None:\n        raise TypeError('cannot handle data conversion of type: {} to {}'.format(type(data),to))\n    else:\n        return converted\n    \ndef conditional_entropy(x, y):\n    \"\"\"\n    Calculates the conditional entropy of x given y: S(x|y)\n    Wikipedia: https://en.wikipedia.org/wiki/Conditional_entropy\n    :param x: list / NumPy ndarray / Pandas Series\n        A sequence of measurements\n    :param y: list / NumPy ndarray / Pandas Series\n        A sequence of measurements\n    :return: float\n    \"\"\"\n    # entropy of x given y\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x,y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0.0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] / total_occurrences\n        p_y = y_counter[xy[1]] / total_occurrences\n        entropy += p_xy * math.log(p_y/p_xy)\n    return entropy\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n    rcorr = r-((r-1)**2)/(n-1)\n    kcorr = k-((k-1)**2)/(n-1)\n    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))\n\ndef theils_u(x, y):\n    s_xy = conditional_entropy(x,y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n/total_occurrences, x_counter.values()))\n    s_x = ss.entropy(p_x)\n    if s_x == 0:\n        return 1\n    else:\n        return (s_x - s_xy) / s_x\n\ndef correlation_ratio(categories, measurements):\n    fcat, _ = pd.factorize(categories)\n    cat_num = np.max(fcat)+1\n    y_avg_array = np.zeros(cat_num)\n    n_array = np.zeros(cat_num)\n    for i in range(0,cat_num):\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n        n_array[i] = len(cat_measures)\n        y_avg_array[i] = np.average(cat_measures)\n    y_total_avg = np.sum(np.multiply(y_avg_array,n_array))/np.sum(n_array)\n    numerator = np.sum(np.multiply(n_array,np.power(np.subtract(y_avg_array,y_total_avg),2)))\n    denominator = np.sum(np.power(np.subtract(measurements,y_total_avg),2))\n    if numerator == 0:\n        eta = 0.0\n    else:\n        eta = numerator/denominator\n    return eta\n\ndef categoric_correlation(dataset, nominal_columns=None, mark_columns=False, theil_u=False, plot=True,\n                          return_results = False, **kwargs):\n    \"\"\"\n    Calculate the correlation/strength-of-association of features in data-set with both categorical (eda_tools) and\n    continuous features using:\n     - Pearson's R for continuous-continuous cases\n     - Correlation Ratio for categorical-continuous cases\n     - Cramer's V or Theil's U for categorical-categorical cases\n    :param dataset: NumPy ndarray / Pandas DataFrame\n        The data-set for which the features' correlation is computed\n    :param nominal_columns: string / list / NumPy ndarray\n        Names of columns of the data-set which hold categorical values. Can also be the string 'all' to state that all\n        columns are categorical, or None (default) to state none are categorical\n    :param mark_columns: Boolean (default: False)\n        if True, output's columns' names will have a suffix of '(nom)' or '(con)' based on there type (eda_tools or\n        continuous), as provided by nominal_columns\n    :param theil_u: Boolean (default: False)\n        In the case of categorical-categorical feaures, use Theil's U instead of Cramer's V\n    :param plot: Boolean (default: True)\n        If True, plot a heat-map of the correlation matrix\n    :param return_results: Boolean (default: False)\n        If True, the function will return a Pandas DataFrame of the computed associations\n    :param kwargs:\n        Arguments to be passed to used function and methods\n    :return: Pandas DataFrame\n        A DataFrame of the correlation/strength-of-association between all features\n    \"\"\"\n\n    dataset = convert(dataset, 'dataframe')\n    columns = dataset.columns\n    if nominal_columns is None:\n        nominal_columns = list()\n    elif nominal_columns == 'all':\n        nominal_columns = columns\n    corr = pd.DataFrame(index=columns, columns=columns)\n    for i in range(0,len(columns)):\n        for j in range(i,len(columns)):\n            if i == j:\n                corr[columns[i]][columns[j]] = 1.0\n            else:\n                if columns[i] in nominal_columns:\n                    if columns[j] in nominal_columns:\n                        if theil_u:\n                            corr[columns[j]][columns[i]] = theils_u(dataset[columns[i]],dataset[columns[j]])\n                            corr[columns[i]][columns[j]] = theils_u(dataset[columns[j]],dataset[columns[i]])\n                        else:\n                            cell = cramers_v(dataset[columns[i]],dataset[columns[j]])\n                            corr[columns[i]][columns[j]] = cell\n                            corr[columns[j]][columns[i]] = cell\n                    else:\n                        cell = correlation_ratio(dataset[columns[i]], dataset[columns[j]])\n                        corr[columns[i]][columns[j]] = cell\n                        corr[columns[j]][columns[i]] = cell\n                else:\n                    if columns[j] in nominal_columns:\n                        cell = correlation_ratio(dataset[columns[j]], dataset[columns[i]])\n                        corr[columns[i]][columns[j]] = cell\n                        corr[columns[j]][columns[i]] = cell\n                    else:\n                        cell, _ = ss.pearsonr(dataset[columns[i]], dataset[columns[j]])\n                        corr[columns[i]][columns[j]] = cell\n                        corr[columns[j]][columns[i]] = cell\n    corr.fillna(value=np.nan, inplace=True)\n    if mark_columns:\n        marked_columns = ['{} (nom)'.format(col) if col in nominal_columns else '{} (con)'.format(col) for col in columns]\n        corr.columns = marked_columns\n        corr.index = marked_columns\n    if plot:\n        plt.figure(figsize=(15,10))#kwargs.get('figsize',None))\n        sns.heatmap(corr, annot=kwargs.get('annot',True), fmt=kwargs.get('fmt','.2f'), cmap='coolwarm')\n        plt.show()\n    if return_results:\n        return corr\n    \ndef bar_plot(variable):\n\n    # get feature\n    var = train[variable]\n    # count number of categorical variable(value/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))\n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-10T13:58:18.408799Z","iopub.execute_input":"2022-04-10T13:58:18.409412Z","iopub.status.idle":"2022-04-10T13:58:18.47224Z","shell.execute_reply.started":"2022-04-10T13:58:18.409349Z","shell.execute_reply":"2022-04-10T13:58:18.471581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### READ DATASET","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:51:52.705638Z","iopub.execute_input":"2022-04-10T13:51:52.705981Z","iopub.status.idle":"2022-04-10T13:51:52.760267Z","shell.execute_reply.started":"2022-04-10T13:51:52.705947Z","shell.execute_reply":"2022-04-10T13:51:52.759387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:51:59.538539Z","iopub.execute_input":"2022-04-10T13:51:59.538822Z","iopub.status.idle":"2022-04-10T13:51:59.561199Z","shell.execute_reply.started":"2022-04-10T13:51:59.538793Z","shell.execute_reply":"2022-04-10T13:51:59.560121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:52:07.932497Z","iopub.execute_input":"2022-04-10T13:52:07.933027Z","iopub.status.idle":"2022-04-10T13:52:08.029558Z","shell.execute_reply.started":"2022-04-10T13:52:07.93297Z","shell.execute_reply":"2022-04-10T13:52:08.028639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:52:10.671806Z","iopub.execute_input":"2022-04-10T13:52:10.67243Z","iopub.status.idle":"2022-04-10T13:52:10.77293Z","shell.execute_reply.started":"2022-04-10T13:52:10.672385Z","shell.execute_reply":"2022-04-10T13:52:10.772214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA & PREPROCESSING","metadata":{}},{"cell_type":"markdown","source":"### Check duplicates and missing values.","metadata":{}},{"cell_type":"code","source":"train.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum().sort_values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(train.isnull().sum().sort_values()/train.shape[0])*100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(7,7))\nsns.boxplot(data=train[\"SalePrice\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Delete too much missing values columns","metadata":{}},{"cell_type":"code","source":"train_ID = train['Id']\ntest_ID = test['Id']\n\ntrain=train.drop(['FireplaceQu','LotFrontage', 'PoolQC','MiscFeature','Alley','Fence', 'Id'],axis=1)\ntest=test.drop(['FireplaceQu','LotFrontage', 'PoolQC','MiscFeature','Alley','Fence', 'Id'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:26:29.015637Z","iopub.execute_input":"2022-04-10T13:26:29.015981Z","iopub.status.idle":"2022-04-10T13:26:29.026119Z","shell.execute_reply.started":"2022-04-10T13:26:29.01594Z","shell.execute_reply":"2022-04-10T13:26:29.025386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"### Numeric Features","metadata":{}},{"cell_type":"code","source":"#num_cols = [col for col in train.columns if train[col].dtype in ['int64', 'float64']]\n#cat_cols = [col for col in train.columns if train[col].dtype in ['object','str']]\n\nnum_cols=[ feature  for feature in train.columns if  train[feature].dtypes!=\"object\" and feature!=\"SalePrice\"]\ncat_cols=[ feature  for feature in train.columns if  train[feature].dtypes==\"object\"]\n\ndiscrete_numeric_columns = ['OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath',\n                'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'MoSold', 'YrSold']\n\ncontinuous_numeric_columns=[]\nfor i in num_cols:\n    if i not in discrete_numeric_columns:\n        continuous_numeric_columns.append(i)\n        \ntrain[continuous_numeric_columns]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:26:40.687737Z","iopub.execute_input":"2022-04-10T13:26:40.68845Z","iopub.status.idle":"2022-04-10T13:26:40.742653Z","shell.execute_reply.started":"2022-04-10T13:26:40.688394Z","shell.execute_reply":"2022-04-10T13:26:40.741525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Determine best correlated features","metadata":{}},{"cell_type":"code","source":"c=pd.Series(abs(pd.get_dummies(train).corr()['SalePrice']).sort_values(ascending=False))\nc.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T14:02:56.921708Z","iopub.execute_input":"2022-04-10T14:02:56.922265Z","iopub.status.idle":"2022-04-10T14:02:57.308763Z","shell.execute_reply.started":"2022-04-10T14:02:56.922227Z","shell.execute_reply":"2022-04-10T14:02:57.307892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(20,30))\n\nfor index, col in enumerate(continuous_numeric_columns):\n    plt.subplot(6,4,index+1)\n    sns.distplot(train[col].dropna())\nfig.tight_layout(pad=2.0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:56:35.171015Z","iopub.execute_input":"2022-04-10T13:56:35.171505Z","iopub.status.idle":"2022-04-10T13:56:49.397425Z","shell.execute_reply.started":"2022-04-10T13:56:35.171452Z","shell.execute_reply":"2022-04-10T13:56:49.39657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style()\nfigure2=plt.figure(2,figsize=(20,20))\nplt.subplot(3,2,1)\nsns.violinplot(data=train,x=\"SalePrice\",color=\"skyblue\")\nplt.subplot(3,2,2)\nsns.violinplot(data=train,x=\"1stFlrSF\",color=\"skyblue\")\nplt.subplot(3,2,3)\nsns.violinplot(data =train, x=\"TotalBsmtSF\",color=\"skyblue\")\nplt.subplot(3,2,4)\nsns.violinplot(data =train, x=\"YearBuilt\",color=\"skyblue\")\nfigure2.tight_layout(pad=3.0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T14:11:02.933013Z","iopub.execute_input":"2022-04-10T14:11:02.933515Z","iopub.status.idle":"2022-04-10T14:11:04.719719Z","shell.execute_reply.started":"2022-04-10T14:11:02.933479Z","shell.execute_reply":"2022-04-10T14:11:04.718715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categoric Features","metadata":{}},{"cell_type":"code","source":"#Functions for plot correlation of categoric, numeric and target features in a heatmap\ngarage_cols=train[['SalePrice', 'GarageYrBlt', 'GarageCars', 'GarageArea']]\ncatcols=list(garage_cols.select_dtypes(['object']).columns)\ngarage_cols = garage_cols.fillna(0) # If there are null values, function doesn't work\nresults = categoric_correlation(garage_cols,nominal_columns=catcols, return_results=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T14:13:21.947458Z","iopub.execute_input":"2022-04-10T14:13:21.948361Z","iopub.status.idle":"2022-04-10T14:13:22.335486Z","shell.execute_reply.started":"2022-04-10T14:13:21.948307Z","shell.execute_reply":"2022-04-10T14:13:22.33428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(18,20))\nfor cols in cat_cols:\n    bar_plot(cols)\n\nfig.tight_layout(pad=1.0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:58:47.035753Z","iopub.execute_input":"2022-04-10T13:58:47.036679Z","iopub.status.idle":"2022-04-10T13:59:01.727426Z","shell.execute_reply.started":"2022-04-10T13:58:47.036607Z","shell.execute_reply":"2022-04-10T13:59:01.726488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mean price deviations by category","metadata":{}},{"cell_type":"code","source":"deviations=plt.figure(2,figsize=(20,20))\nmoy=train['SalePrice'].mean()\n\nplt.subplot(3,3,1)\nplt.title(\"OverallQual\")\n(train.groupby('OverallQual')[\"SalePrice\"].mean()-moy).plot(kind='bar',color='Purple',ls='dashed',edgecolor='Black')\nplt.axhline(y=0)\n\nplt.subplot(3,3,2)\nplt.title(\"GarageCars\")\n(train.groupby('GarageCars')[\"SalePrice\"].mean()-moy).plot(kind='bar',color='Purple',ls='dashed',edgecolor='Black')\nplt.axhline(y=0)\n\nplt.subplot(3,3,3)\nplt.title(\"TotRmsAbvGrd\")\n(train.groupby('TotRmsAbvGrd')[\"SalePrice\"].mean()-moy).plot(kind='bar',color='Purple',ls='dashed',edgecolor='Black')\nplt.axhline(y=0)\n\n\nplt.subplot(3,3,4)\nplt.title(\"FullBath\")\n(train.groupby('FullBath')[\"SalePrice\"].mean()-moy).plot(kind='bar',color='Purple',ls='dashed',edgecolor='Black')\nplt.axhline(y=0)\n\n\ndeviations.tight_layout(pad=2.0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T14:11:20.489222Z","iopub.execute_input":"2022-04-10T14:11:20.489555Z","iopub.status.idle":"2022-04-10T14:11:21.39266Z","shell.execute_reply.started":"2022-04-10T14:11:20.489524Z","shell.execute_reply":"2022-04-10T14:11:21.391798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fill null values","metadata":{}},{"cell_type":"code","source":"nan_count=100*train.isna().sum().sort_values(ascending=False)/train.shape[0]\nfig=px.bar(x=nan_count.index,y=nan_count.values, labels={\"y\": \"Nan ammount (%)\",\"x\": \"Feature\"})\nfig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#replacing train NaNs with modes\nnans=train.isna().sum()\nnans=nans[nans>0]\nfor c in nans.index:\n    train[c] = train[c].fillna(train[c].mode()[0])\n    \n#replacing test NaNs with modes\nnans=test.isna().sum()\nnans=nans[nans>0]\nfor c in nans.index:\n    test[c] = test[c].fillna(test[c].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:27:05.816235Z","iopub.execute_input":"2022-04-10T13:27:05.816579Z","iopub.status.idle":"2022-04-10T13:27:05.859671Z","shell.execute_reply.started":"2022-04-10T13:27:05.816544Z","shell.execute_reply":"2022-04-10T13:27:05.858607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in cat_cols:    \n    #some string values are present only in one of the dataset, so it is needed an unique list of both dataset to avoid conflicts\n    for num, value in enumerate(np.unique((list(train[feature].unique())+list(test[feature].unique())))):          \n        train[feature+\"_\"+str(num)]=pd.Series(train[feature]==value,dtype=\"int\")        \n        test[feature+\"_\"+str(num)]=pd.Series(test[feature]==value,dtype=\"int\")\n    train=train.drop(columns=feature)\n    test=test.drop(columns=feature)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:27:11.045407Z","iopub.execute_input":"2022-04-10T13:27:11.045731Z","iopub.status.idle":"2022-04-10T13:27:11.681937Z","shell.execute_reply.started":"2022-04-10T13:27:11.045696Z","shell.execute_reply":"2022-04-10T13:27:11.680903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler=StandardScaler()\ntrain[num_cols]=scaler.fit_transform(train[num_cols])\ntest[num_cols]=scaler.transform(test[num_cols])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:27:16.904458Z","iopub.execute_input":"2022-04-10T13:27:16.905482Z","iopub.status.idle":"2022-04-10T13:27:16.952689Z","shell.execute_reply.started":"2022-04-10T13:27:16.905436Z","shell.execute_reply":"2022-04-10T13:27:16.951917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Detect and Drop Outliers","metadata":{}},{"cell_type":"code","source":"outliers = IsolationForest(n_estimators=30,random_state=1234).fit_predict(train)\ntrain.drop(np.where(outliers==-1)[0],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:33:00.945423Z","iopub.execute_input":"2022-04-10T13:33:00.945761Z","iopub.status.idle":"2022-04-10T13:33:01.10413Z","shell.execute_reply.started":"2022-04-10T13:33:00.945714Z","shell.execute_reply":"2022-04-10T13:33:01.103341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train=train.drop(columns=\"SalePrice\")\ny_train=train['SalePrice']\npca = PCA(n_components=train.shape[1]-1)\nx_train=pca.fit_transform(x_train)\nfig=go.Figure()\nfig.add_traces(go.Bar(x=np.arange(train.shape[1]-1),y=np.cumsum(pca.explained_variance_ratio_),name=\"Cumulative Variance\"))\n#n_comp will be the number of components that explains the 99% of the data variance\nn_comp=np.where(np.cumsum(pca.explained_variance_ratio_)>0.99)[0][0]\nfig.add_traces(go.Scatter(x=np.arange(train.shape[1]-1),y=[0.99]*(train.shape[1]-1),name=\"Variance at 99%\"))\nfig.update_layout(title=\"How many components we need?\",xaxis_title=\"Components\",yaxis_title=\"Cumulative Variance\", font=dict(\n        family=\"Arial\",\n        size=18,\n    ))\nfig.show()\nprint(\"With n_components=\"+str(n_comp)+\" we have the 99% of the data variance, so we will choose this value.\")","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:34:49.999538Z","iopub.execute_input":"2022-04-10T13:34:50.000557Z","iopub.status.idle":"2022-04-10T13:34:50.306305Z","shell.execute_reply.started":"2022-04-10T13:34:50.000504Z","shell.execute_reply":"2022-04-10T13:34:50.304926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=n_comp)\nx_train=pca.fit_transform(train.drop(columns=[\"SalePrice\"]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODELING","metadata":{}},{"cell_type":"code","source":"test=pca.transform(test)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:36:36.227695Z","iopub.execute_input":"2022-04-10T13:36:36.228028Z","iopub.status.idle":"2022-04-10T13:36:36.245001Z","shell.execute_reply.started":"2022-04-10T13:36:36.227987Z","shell.execute_reply":"2022-04-10T13:36:36.24387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = [] \ntrain_p=[]\nscores = []\nkf =  StratifiedKFold(n_splits=10, shuffle=True, random_state=1234) \nfor fold, (idx_train, idx_valid) in enumerate(kf.split(train,y_train)):\n    X_tr, y_tr = x_train[idx_train], y_train.iloc[idx_train]\n    X_val, y_val = x_train[idx_valid], y_train.iloc[idx_valid]\n    gbr=GradientBoostingRegressor(random_state=1234,subsample=0.6)\n    lr= LinearRegression()\n    lr.fit(X_tr,y_tr)\n    gbr.fit(X_tr,y_tr)\n    val_pred_lr = lr.predict(X_val)\n    val_pred_gbr = gbr.predict(X_val)\n    score_lr = r2_score(y_val, val_pred_lr)\n    score_gbr = r2_score(y_val, val_pred_gbr)\n    scores.append((score_lr,score_gbr))\n    print(f\"Fold: {fold + 1} roc_auc Score is : {(score_lr,score_gbr)}\")\n    #ensembling the methods\n    test_preds = lr.predict(test)*0.4 + gbr.predict(test)*0.6\n    predictions.append(test_preds)\n    train_p.append(lr.predict(x_train)*0.4 + gbr.predict(x_train)*0.6)\nprint(f\" mean Validation R2 : {np.mean(scores)}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:39:46.426163Z","iopub.execute_input":"2022-04-10T13:39:46.426676Z","iopub.status.idle":"2022-04-10T13:40:13.488395Z","shell.execute_reply.started":"2022-04-10T13:39:46.42662Z","shell.execute_reply":"2022-04-10T13:40:13.487343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi=pd.Series(mutual_info_regression(train.drop(columns=\"SalePrice\"),np.mean(train_p,axis=0)))\nmi.index=train.columns[:-1]\n#ordering values to be plotted in descending order\nmi=mi.sort_values(ascending=False)\nfig = go.Figure()\nfig.add_trace(go.Bar(name=\"Mutual Information (MI)\", x=mi.index[:20], y=mi.values[:20]))\nfig.update_layout(title=\"Mutual Information-How relevant are the features  for the ensembled method?\")\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T13:41:03.169145Z","iopub.execute_input":"2022-04-10T13:41:03.169506Z","iopub.status.idle":"2022-04-10T13:41:05.70611Z","shell.execute_reply.started":"2022-04-10T13:41:03.169466Z","shell.execute_reply":"2022-04-10T13:41:05.705137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **This is my first notebook try. If you think this is useful, please give me upvote.**","metadata":{}}]}