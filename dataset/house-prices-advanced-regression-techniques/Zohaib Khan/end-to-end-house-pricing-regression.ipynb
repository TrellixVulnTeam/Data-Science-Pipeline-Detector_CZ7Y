{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Essentials\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n%matplotlib inline\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-14T13:18:39.808905Z","iopub.execute_input":"2021-09-14T13:18:39.809475Z","iopub.status.idle":"2021-09-14T13:18:40.752731Z","shell.execute_reply.started":"2021-09-14T13:18:39.809391Z","shell.execute_reply":"2021-09-14T13:18:40.752065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in the data\n# !ls ../input/house-prices-advanced-regression-techniques\ntrain_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n\n# Check the shapes\ntrain_data.shape, test_data.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:40.753829Z","iopub.execute_input":"2021-09-14T13:18:40.754215Z","iopub.status.idle":"2021-09-14T13:18:40.841635Z","shell.execute_reply.started":"2021-09-14T13:18:40.754177Z","shell.execute_reply":"2021-09-14T13:18:40.840616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"We can clearly see below that there is a good mix of Numerical and Categorical Features which could pose difficulties in exploring the data comprehensively.","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:40.843582Z","iopub.execute_input":"2021-09-14T13:18:40.843988Z","iopub.status.idle":"2021-09-14T13:18:40.892336Z","shell.execute_reply.started":"2021-09-14T13:18:40.843949Z","shell.execute_reply":"2021-09-14T13:18:40.891288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a rough overview of the training data\ntrain_data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:40.893837Z","iopub.execute_input":"2021-09-14T13:18:40.894112Z","iopub.status.idle":"2021-09-14T13:18:41.007675Z","shell.execute_reply.started":"2021-09-14T13:18:40.894086Z","shell.execute_reply":"2021-09-14T13:18:41.007009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a look at the column names and object types\ntrain_data.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:41.008635Z","iopub.execute_input":"2021-09-14T13:18:41.009033Z","iopub.status.idle":"2021-09-14T13:18:41.032164Z","shell.execute_reply.started":"2021-09-14T13:18:41.008996Z","shell.execute_reply":"2021-09-14T13:18:41.031523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We don't need the `Id` column for the Training Set so we drop it (it'd act as an unnecessary feature otherwise).","metadata":{}},{"cell_type":"code","source":"# Drop the ID column\ntrain_data.drop('Id', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:41.033092Z","iopub.execute_input":"2021-09-14T13:18:41.033467Z","iopub.status.idle":"2021-09-14T13:18:41.038947Z","shell.execute_reply.started":"2021-09-14T13:18:41.033436Z","shell.execute_reply":"2021-09-14T13:18:41.037924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our Pipelines will be easier to construct if we can gather the names of the Numerical and Categorical/Object Features.","metadata":{}},{"cell_type":"code","source":"# Collect the names of the Categorical and Numeric Variables seperately\nnum_columns = train_data.select_dtypes(include=np.number).columns.tolist()\nnum_columns.remove(\"SalePrice\") # Capturing feature names exclusively\ncat_columns = train_data.select_dtypes(exclude=np.number).columns.tolist()\n\n# Check if the number makes sense (+1 for the target variable that was dropped)\nlen(num_columns) + len(cat_columns) + 1 == len(train_data.columns)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:41.040371Z","iopub.execute_input":"2021-09-14T13:18:41.040785Z","iopub.status.idle":"2021-09-14T13:18:41.057525Z","shell.execute_reply.started":"2021-09-14T13:18:41.040736Z","shell.execute_reply":"2021-09-14T13:18:41.056412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Other than collecting the names of features based on what kind of data they store, we can also find a number of features with similar names. Specifically there are four keywords that are repeated a few times so they may require similar Engineering or special treatment.","metadata":{}},{"cell_type":"code","source":"# Explore Categorical Columns\n# cat_columns <- Find a number of variables related to each other by name\n\n# Explore overlapping variable names\nrepetitive = [\"Bsmt\", \"Garage\", \"Sale\", \"Kitchen\"]\nsimilar_cols = []\nprint(\"Looking for highly similar variable names\")\nprint('--'*30)\nfor col in (num_columns + cat_columns):\n    if any(x in col for x in repetitive):\n        print(col)\n        similar_cols.append(col)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:41.06048Z","iopub.execute_input":"2021-09-14T13:18:41.060859Z","iopub.status.idle":"2021-09-14T13:18:41.070842Z","shell.execute_reply.started":"2021-09-14T13:18:41.060828Z","shell.execute_reply":"2021-09-14T13:18:41.069812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Cardinality of a Feature refers to the number of Unique Values in that set. We should be on the lookout for:\n* High Cardinality Categorical Variables: for example, if Zipcodes were not stored as numbers, it could cause a Transformer like `OneHotEncoder` to explode\n* Low Cardinality Numeric Variables: for instance, if a feature had values `[1,2,3,4,5]`, we could create new useful features out of it","metadata":{}},{"cell_type":"code","source":"# Check the cardinality of each of these variables\nprint(\"Looking at Categorical Variable Cardinalities\")\nprint('--'*30)\nfor col in cat_columns:\n    uniques = train_data[col].unique()\n    if len(uniques) > 10:\n        print(f\"{len(uniques)} values in {col}\")\n    else:\n        print(f\"{len(uniques)} values in {col}: {uniques}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:41.073269Z","iopub.execute_input":"2021-09-14T13:18:41.073784Z","iopub.status.idle":"2021-09-14T13:18:41.107185Z","shell.execute_reply.started":"2021-09-14T13:18:41.073705Z","shell.execute_reply":"2021-09-14T13:18:41.106222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Are there any low cardinality numeric variables?\nprint(\"Checking for Low Cardinality Numeric Variables\")\nprint(\"--\"*30)\nfor col in num_columns:\n    uniques = train_data[col].unique()\n    if len(uniques) < 20:\n        print(f\"{len(uniques)} unique values in {col}: {sorted(uniques)}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:41.10862Z","iopub.execute_input":"2021-09-14T13:18:41.109017Z","iopub.status.idle":"2021-09-14T13:18:41.133428Z","shell.execute_reply.started":"2021-09-14T13:18:41.108978Z","shell.execute_reply":"2021-09-14T13:18:41.132497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Even though some features like `YrSold` are discrete enough to become Categorical Variables, we can leave them this way since their hierarchy being preserved makes sense.\n\nExploring the Linear Correlation between Features is helpful since it can \n* Highlight which features *look to be the* most useful for predicting the target variable \n* Shed some light on which features are highly correlated together in which case they could be *mutually redundant* (this helps in Feature Selection)","metadata":{}},{"cell_type":"code","source":"# Explore which numeric columns have high linear correlation\ncorr_matrix = train_data.corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(corr_matrix, cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:41.134774Z","iopub.execute_input":"2021-09-14T13:18:41.135134Z","iopub.status.idle":"2021-09-14T13:18:42.244387Z","shell.execute_reply.started":"2021-09-14T13:18:41.135105Z","shell.execute_reply":"2021-09-14T13:18:42.243669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort the highest linear correlations with target variable\ntarget_var = \"SalePrice\"\ncorr_matrix[target_var].apply(lambda x: abs(x)).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:42.245345Z","iopub.execute_input":"2021-09-14T13:18:42.245695Z","iopub.status.idle":"2021-09-14T13:18:42.254229Z","shell.execute_reply.started":"2021-09-14T13:18:42.245668Z","shell.execute_reply":"2021-09-14T13:18:42.253405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We find that the two features `OverallQual` and `GrLivArea` are the most *linearly* correlated with our target so we can choose to explore them a bit further.","metadata":{}},{"cell_type":"code","source":"# Explore the second variable (notice the significant Linear Correlation)\nsns.scatterplot(x=\"OverallQual\", y=\"SalePrice\", data=train_data)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:42.255535Z","iopub.execute_input":"2021-09-14T13:18:42.255957Z","iopub.status.idle":"2021-09-14T13:18:42.480468Z","shell.execute_reply.started":"2021-09-14T13:18:42.255912Z","shell.execute_reply":"2021-09-14T13:18:42.479529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cells explore the distribution of the Target Variable and it can be seen that it is **left-skewed** and has Outliers (as seen from the Percentiles and the critical values). Some ways to deal with such distributions are to:\n* Perform a logarithmic transformation on the values (Normalization and Standardization do not alter the skewness, just the scale)\n* Explore the Percentiles to come up with some upper/lower thresholds beyond which the values are set to something more common (Boxplots also help here)\n\nWe use a Log Transform here: it is simple and the resulting distribution is visually very similar to a Gaussian.","metadata":{}},{"cell_type":"code","source":"# Explore the Distribution of the Target Variable\nsns.distplot(train_data[target_var])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:42.481856Z","iopub.execute_input":"2021-09-14T13:18:42.482511Z","iopub.status.idle":"2021-09-14T13:18:42.928429Z","shell.execute_reply.started":"2021-09-14T13:18:42.48247Z","shell.execute_reply":"2021-09-14T13:18:42.9275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Where do most values lie under? Explore the Percentiles.\nfor i in range(95,100):\n    print(f\"{i}% of the target values lie under: {int(np.percentile(train_data[target_var], i))}\")\nprint(f\"Critical Values:\\n\\tMax:{train_data[target_var].max()}\\n\\tMin:{train_data[target_var].min()}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:42.929587Z","iopub.execute_input":"2021-09-14T13:18:42.92986Z","iopub.status.idle":"2021-09-14T13:18:42.938438Z","shell.execute_reply.started":"2021-09-14T13:18:42.929833Z","shell.execute_reply":"2021-09-14T13:18:42.937416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can get rid of Outliers by setting some thresholds\nupper_thresh = 38500\n# train_data[train_data[target_var] > upper_thresh][target_var] = upper_thresh","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:42.93986Z","iopub.execute_input":"2021-09-14T13:18:42.940148Z","iopub.status.idle":"2021-09-14T13:18:42.94649Z","shell.execute_reply.started":"2021-09-14T13:18:42.94012Z","shell.execute_reply":"2021-09-14T13:18:42.945713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Log Transform to reduce skewness of the Target Distribution\nprint(f\"Before Log Transform: Skewness {stats.skew(train_data.SalePrice)}\")\ntrain_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])\nprint(f\"After Log Transform: Skewness {stats.skew(train_data.SalePrice)}\")\n# y = np.expm1(y)\nprint(f\"Applying Inverse Transformation: Skewness {stats.skew(np.expm1(train_data.SalePrice))}\") # This is to demonstrate retaining our original targets\nprint(f\"Final Skewness: {stats.skew(train_data.SalePrice)}\")\nsns.distplot(train_data[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:42.947631Z","iopub.execute_input":"2021-09-14T13:18:42.948013Z","iopub.status.idle":"2021-09-14T13:18:43.193213Z","shell.execute_reply.started":"2021-09-14T13:18:42.947923Z","shell.execute_reply":"2021-09-14T13:18:43.192387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data before moving on\nX = train_data.drop(target_var, axis=1)\ny = train_data[target_var]\nX.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:43.194378Z","iopub.execute_input":"2021-09-14T13:18:43.194623Z","iopub.status.idle":"2021-09-14T13:18:43.206266Z","shell.execute_reply.started":"2021-09-14T13:18:43.194598Z","shell.execute_reply":"2021-09-14T13:18:43.205229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning: Dealing with Missing Values","metadata":{}},{"cell_type":"markdown","source":"The only two necessary steps for preparing Data for Modeling are:\n1. Dealing with Missing values (`nan`)\n2. Finding some numerical representation for Categorical/Non-numeric Variables\n\nIn Data Cleaning, the focus is on the first step.","metadata":{}},{"cell_type":"code","source":"# Get a visual of how many values are missing\nmissing_count = X.isnull().sum()\nmissing_count = missing_count[missing_count > 0]\nmissing_cols = pd.DataFrame(missing_count).index.tolist()\nplt.figure(figsize=(12,8))\nsns.heatmap(X[missing_cols].isnull(), cmap='viridis', cbar=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:43.207512Z","iopub.execute_input":"2021-09-14T13:18:43.207807Z","iopub.status.idle":"2021-09-14T13:18:44.051777Z","shell.execute_reply.started":"2021-09-14T13:18:43.20778Z","shell.execute_reply":"2021-09-14T13:18:44.050922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get actual numbers \nmissing_count.sort_values(ascending=False) / len(X) * 100","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:44.052914Z","iopub.execute_input":"2021-09-14T13:18:44.053172Z","iopub.status.idle":"2021-09-14T13:18:44.060852Z","shell.execute_reply.started":"2021-09-14T13:18:44.053147Z","shell.execute_reply":"2021-09-14T13:18:44.060073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The variable `PoolQC` has enough values that we could be lazy and drop the entire column, but it may be of benefit to us if we fill the missing values with `0` since the values may just *not exist*. \n\nWe can apply similar thinking to the features related to the Garage and Basements of these houses.","metadata":{}},{"cell_type":"code","source":"# Which missing value columns are numeric and which are categorical\nprint(X[missing_cols].dtypes)\nX[missing_cols].head(10)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:44.062172Z","iopub.execute_input":"2021-09-14T13:18:44.062456Z","iopub.status.idle":"2021-09-14T13:18:44.097841Z","shell.execute_reply.started":"2021-09-14T13:18:44.062429Z","shell.execute_reply":"2021-09-14T13:18:44.096927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function for Data Cleaning\ndef handle_missing(df):\n    # LotFrontage, MasVnrArea are generic numeric features so we can fill with the median\n    cols = ['LotFrontage', 'MasVnrArea']\n    for col in cols:\n        df[col] = df[col].fillna(df[col].median())\n    \n    # Some features have missing values because one does not exist for that instance\n    none_fill_cols = \"Alley BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinType2 Electrical FireplaceQu GarageType GarageFinish GarageQual GarageCond PoolQC Fence MiscFeature\".split()\n    df[none_fill_cols] = df[none_fill_cols].fillna('NONE')\n    \n    # Deal with Electrical, MasVnrType and GarageYrBlt\n    df['Electrical'] = df['Electrical'].fillna(\"SBrkr\") # This is the average\n    df['MasVnrType'] = df['MasVnrType'].fillna(df.MasVnrType.mode()) # The mode makes more sense based on feature description\n    df['GarageYrBlt'] = df['GarageYrBlt'].fillna(0) # This house instance has no garage\n    \n    # If the testing data has any surprises, we can apply a generic strategy\n    num_cols = df.select_dtypes(include=np.number).columns\n    cat_cols = df.select_dtypes(exclude=np.number).columns\n    for col in num_cols:\n        df[col] = df[col].fillna(df[col].median())\n    for col in cat_cols:\n        df[col] = df[col].fillna('NONE')\n    \n    return df\n    \n# Apply this to a copy of the DataFrame and check\ntmp = X.copy()\ntmp = handle_missing(tmp)\ntmp.isnull().sum()[tmp.isnull().sum() > 0]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:44.099172Z","iopub.execute_input":"2021-09-14T13:18:44.099424Z","iopub.status.idle":"2021-09-14T13:18:44.154243Z","shell.execute_reply.started":"2021-09-14T13:18:44.099398Z","shell.execute_reply":"2021-09-14T13:18:44.153569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply this to the actual data\nX = handle_missing(X)\nX.isnull().sum().max()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:44.156437Z","iopub.execute_input":"2021-09-14T13:18:44.156813Z","iopub.status.idle":"2021-09-14T13:18:44.201892Z","shell.execute_reply.started":"2021-09-14T13:18:44.156785Z","shell.execute_reply":"2021-09-14T13:18:44.201193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering and Feature Selection\n\nSome aspects of Feature Engineering include:\n* Create Aggregated Features (can result in more robust models, as will be seen later)\n* Dealing with heavily Skewed features (either dropping them, transforming them or doing nothing)\n* Scaling/Normalizing numeric variables (really only required for **non-Tree-based** algorithms like SVMs, Linear Regression, MLP etc.)\n* Encodings for Categorical Variables (the only thing that's necessary to carry out here)","metadata":{"execution":{"iopub.status.busy":"2021-09-01T09:38:20.647479Z","iopub.execute_input":"2021-09-01T09:38:20.647838Z","iopub.status.idle":"2021-09-01T09:38:20.688154Z","shell.execute_reply.started":"2021-09-01T09:38:20.647806Z","shell.execute_reply":"2021-09-01T09:38:20.687143Z"}}},{"cell_type":"code","source":"# Some interesting features we can create\ndef new_features(X):\n    X['HasWoodDeck'] = (X['WoodDeckSF'] == 0) * 1\n\n    X['HasOpenPorch'] = (X['OpenPorchSF'] == 0) * 1\n    X['HasEnclosedPorch'] = (X['EnclosedPorch'] == 0) * 1\n    X['Has3SsnPorch'] = (X['3SsnPorch'] == 0) * 1\n    X['HasScreenPorch'] = (X['ScreenPorch'] == 0) * 1\n\n    X['Total_Home_Quality'] = X['OverallQual'] + X['OverallCond']\n    X['TotalSF'] = X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF']\n    X['TotalSquareFootage'] = (X['BsmtFinSF1'] + X['BsmtFinSF2'] + X['1stFlrSF'] + X['2ndFlrSF'])\n\n    X['HasPool'] = X['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    X['Has2ndFloor'] = X['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    X['HasGarage'] = X['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    X['HasBsmt'] = X['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    X['HasFireplace'] = X['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    \n    return X\n\nX  = new_features(X)\nlen(X.columns)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:44.203115Z","iopub.execute_input":"2021-09-14T13:18:44.203496Z","iopub.status.idle":"2021-09-14T13:18:44.229557Z","shell.execute_reply.started":"2021-09-14T13:18:44.20346Z","shell.execute_reply":"2021-09-14T13:18:44.228714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find the heavily skewed features\nnum_columns = X.select_dtypes(include=np.number).columns\nskewed_features = X[num_columns].apply(lambda x: abs(stats.skew(x))).sort_values(ascending=False)\nhigh_skewed = skewed_features[skewed_features > 0.5]\nhigh_skewed ","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:44.230669Z","iopub.execute_input":"2021-09-14T13:18:44.230976Z","iopub.status.idle":"2021-09-14T13:18:44.265775Z","shell.execute_reply.started":"2021-09-14T13:18:44.230948Z","shell.execute_reply":"2021-09-14T13:18:44.265104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can set some threshold above which the filtered features will undergo a Logarithmic Transformation but a couple of the features justify their high skewness; e.g. only a few luxury homes have pools so there would naturally be outliers and a pulled distribution.","metadata":{}},{"cell_type":"code","source":"X.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:44.266635Z","iopub.execute_input":"2021-09-14T13:18:44.267027Z","iopub.status.idle":"2021-09-14T13:18:44.272023Z","shell.execute_reply.started":"2021-09-14T13:18:44.267Z","shell.execute_reply":"2021-09-14T13:18:44.271036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a final jab at EDA, we shall explore Feature Selection. There are many classes made specifically for this making use of tools like Chi-square tests and whatnot, but a simple and effective approach is to simply **fit a `RandomForest` on the data**.\n\nThe forest is made of many Decision Trees so by iterating through each Tree and seeing which splits contribute to better drops in impurity/entropy, the Ensemble can get an idea of which features are *more important* for predicting the target. Since the Ensemble is based on Trees, we only have to \n* Make sure there are no missing values\n* Numerically Encode the Categorical Variables (Label vs One Hot doesn't matter for tree-based models since the whole space is explored value-by-value)","metadata":{}},{"cell_type":"code","source":"# Label Encode a copy of the data\nfrom sklearn import preprocessing\ncat_columns = X.select_dtypes(exclude=np.number).columns\nfi_data = X.copy()\nfor feat in cat_columns:\n    fi_data[feat] = preprocessing.LabelEncoder().fit_transform(fi_data[feat])\n# Use a RandomForest model to look at the Feature Importances \nfrom sklearn.ensemble import RandomForestRegressor\nforest_fi = RandomForestRegressor(n_estimators=100,\n                                 min_samples_leaf=5,\n                                 min_samples_split=5,\n                                 n_jobs=-1).fit(fi_data, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:44.273322Z","iopub.execute_input":"2021-09-14T13:18:44.273653Z","iopub.status.idle":"2021-09-14T13:18:45.406153Z","shell.execute_reply.started":"2021-09-14T13:18:44.27358Z","shell.execute_reply":"2021-09-14T13:18:45.405397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract and visualize the importances\nimportances = forest_fi.feature_importances_\nfeat_imps = pd.Series(importances, index=fi_data.columns)\nfeat_imps","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:45.407004Z","iopub.execute_input":"2021-09-14T13:18:45.407238Z","iopub.status.idle":"2021-09-14T13:18:45.519856Z","shell.execute_reply.started":"2021-09-14T13:18:45.407215Z","shell.execute_reply":"2021-09-14T13:18:45.518942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the top 10 most relevant features to the target variable\nfeat_imps = feat_imps.sort_values(ascending=False)\nfeat_imps[:10]","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:45.520986Z","iopub.execute_input":"2021-09-14T13:18:45.521256Z","iopub.status.idle":"2021-09-14T13:18:45.52846Z","shell.execute_reply.started":"2021-09-14T13:18:45.521229Z","shell.execute_reply":"2021-09-14T13:18:45.527579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the importances\nplt.figure(figsize=(11,9))\nplt.title(\"Feature Importances after Engineering\")\nfeat_imps[:15].plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:45.529795Z","iopub.execute_input":"2021-09-14T13:18:45.530064Z","iopub.status.idle":"2021-09-14T13:18:45.787657Z","shell.execute_reply.started":"2021-09-14T13:18:45.530038Z","shell.execute_reply":"2021-09-14T13:18:45.787063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Incredible. Three of our aggregated features are among the best that can be used for predicting the Target Variable. \n\nThe `OverallQual` feature still reigns the highest and by a significant margin at that. Other than that we can also see the `YearBuilt` feature among the top predictors which implies that the later a house was built, the price tended to increase a lot more.\n\nAnother interesting note is that there are no Basement related features in this top-15 list, but there are a number of *Garage* related features. This could imply that in practice, Garages are more valuable than Basements which makes sense.","metadata":{}},{"cell_type":"markdown","source":"# Full Pipeline and Modeling","metadata":{}},{"cell_type":"markdown","source":"Now that we have gone through the whole process step-by-step, it would help to functionalize everything (create a Pipeline) for efficiency and code-reproducibility in case we want to run more experiments later.\n\nThe following cell applies every transformation/piece of engineering we did thus far.","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn import preprocessing\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\ntest_ids = test_data.Id # TEST IDS STORED HERE!\n\n# Scale and seperate Target Variable\ntarget_var = 'SalePrice'\ntrain_data[target_var] = np.log1p(train_data[target_var]) # TARGET SCALED HERE!\nX = train_data.drop(['Id', target_var], axis=1)\ny = train_data[target_var]\nX_test = test_data.drop('Id', axis=1)\n\n# Data Cleaning\nX = handle_missing(X)\nX_test = handle_missing(test_data)\n\n# Feature Engineering\nX = new_features(X)\nX_test = new_features(X_test)\n\n# Pipelines\nnum_columns = X.select_dtypes(include=np.number).columns\ncat_columns = X.select_dtypes(exclude=np.number).columns\npipeline = ColumnTransformer([\n    (\"one_hot_encoder\", preprocessing.OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_columns),\n    (\"standard_scaler\", preprocessing.StandardScaler(), num_columns)\n])\nX = pipeline.fit_transform(X)\nX_test = pipeline.transform(X_test) # Making sure to not fit to the testing set","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:45.788816Z","iopub.execute_input":"2021-09-14T13:18:45.789056Z","iopub.status.idle":"2021-09-14T13:18:46.015597Z","shell.execute_reply.started":"2021-09-14T13:18:45.789031Z","shell.execute_reply":"2021-09-14T13:18:46.014894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape, y.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:18:56.756618Z","iopub.execute_input":"2021-09-14T13:18:56.757097Z","iopub.status.idle":"2021-09-14T13:18:56.762035Z","shell.execute_reply.started":"2021-09-14T13:18:56.757068Z","shell.execute_reply":"2021-09-14T13:18:56.761346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now to finally begin modeling. \n\nIn keeping up with proper Cross Validation schemes, we define a function that returns the RMSE of a model when validated over 5 folds of the data. This is better than using `train_test_split` since that validates the model on only one fold and is vulnerable to randomness influencing the model's performance.","metadata":{}},{"cell_type":"code","source":"# Import dependencies and models\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Instantiate a dict (+function) for storing model scores\nscores = {}\ndef get_cv_score(estimator):\n    return np.sqrt(-1 * cross_val_score(estimator, X=X, y=y, cv=5, scoring='neg_mean_squared_error', n_jobs=-1))","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:19:28.632548Z","iopub.execute_input":"2021-09-14T13:19:28.632904Z","iopub.status.idle":"2021-09-14T13:19:29.809026Z","shell.execute_reply.started":"2021-09-14T13:19:28.632874Z","shell.execute_reply":"2021-09-14T13:19:29.807864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start with a Linear Regression model that performs *much much worse* than the other strong learners.","metadata":{}},{"cell_type":"code","source":"# Start with a simple Linear Model\nlin_reg = LinearRegression()\nscores['linear_regression'] = get_cv_score(lin_reg)\nscores['linear_regression']","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:19:29.810869Z","iopub.execute_input":"2021-09-14T13:19:29.811256Z","iopub.status.idle":"2021-09-14T13:19:32.106338Z","shell.execute_reply.started":"2021-09-14T13:19:29.811216Z","shell.execute_reply":"2021-09-14T13:19:32.105456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cell below takes ~5mins to finish evaluating the models","metadata":{}},{"cell_type":"code","source":"%%time\nimport time\n\n# Change the boolean to Cross Validate the Base Models\ncheck_cv = False\n\nif check_cv:\n    start = time.time()\n    # SVM with Linear Kernel\n    linear_svr = SVR(kernel='linear', \n                     C=10, \n                     epsilon=0.01, \n                     gamma=0.0005)\n    scores['linear_svr'] = get_cv_score(linear_svr)\n    print(f\"Finished Linear SVR: {time.time()-start:0.2f}sec\")\n    start = time.time()\n    # SVM with RBF kernel\n    svr = SVR(kernel='rbf', \n              C=10, \n              epsilon=0.01, \n              gamma=0.0005)\n    scores['svr'] = get_cv_score(svr)\n    print(f\"Finished Kernel SVR: {time.time()-start:0.2f}sec\")\n    start = time.time()\n    # Random Forest\n    rfr = RandomForestRegressor(n_estimators=250, \n                                max_depth=15, \n                                min_samples_leaf=5, \n                                min_samples_split=5, \n                                n_jobs=-1,\n                               random_state=42)\n    scores['rfr'] = get_cv_score(rfr)\n    print(f\"Finished Random Forest: {time.time()-start:0.2f}sec\")\n    start = time.time()\n    # Gradient Boosting\n    gbr = GradientBoostingRegressor(n_estimators=350, \n                                    learning_rate=0.1, \n                                    loss='huber',\n                                   random_state=42)\n    scores['gbr'] = get_cv_score(gbr)\n    print(f\"Finished Gradient Boosting: {time.time()-start:0.2f}sec\")\n    start = time.time()\n    # LGBM\n    lgbr = LGBMRegressor(objective='regression',\n                        n_estimators=300,\n                        learning_rate=0.1,\n                        random_state=42)\n    scores['lgbr'] = get_cv_score(lgbr)\n    print(f\"Finished LGBM: {time.time()-start:0.2f}sec\")\n    start = time.time()\n    # AdaBoost with DT Base Estimator\n    ada = AdaBoostRegressor(n_estimators=150, \n                            random_state=42)\n    scores['ada'] = get_cv_score(ada)\n    print(f\"Finished AdaBoost: {time.time()-start:0.2f}sec\")\n    start = time.time()\n    # Ending with XGBoost\n    xgb = XGBRegressor(n_estimators=300,\n                      max_depth=5, \n                      learning_rate=0.1,\n                      random_state=42)\n    scores['xgb'] = get_cv_score(xgb)\n    print(f\"Finished XGBoost: {time.time()-start:0.2f}sec\")\n\n    # Evaluate models before any serious Hyperparameter tuning\n    print(f\"AdaBoost: {scores['ada'].mean()}\")\n    print(f\"LGBM: {scores['lgbr'].mean()}\")\n    print(f\"GradientBoosting: {scores['gbr'].mean()}\")\n    print(f\"RandomForest: {scores['rfr'].mean()}\")\n    print(f\"Linear SVR: {scores['linear_svr'].mean()}\")\n    print(f\"Kernel SVR: {scores['svr'].mean()}\")\n    print(f\"XGBoost: {scores['xgb'].mean()}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:19:32.108212Z","iopub.execute_input":"2021-09-14T13:19:32.108612Z","iopub.status.idle":"2021-09-14T13:19:32.118042Z","shell.execute_reply.started":"2021-09-14T13:19:32.108577Z","shell.execute_reply":"2021-09-14T13:19:32.117143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```\nAdaBoost: 34566.742546702626\nLGBM: 29158.163160149958\nGradientBoosting: 26674.092765014524\nRandomForest: 30125.682357409016\nKernel SVR: 43880.25669866572\nLinear SVR: 81085.44385548346\nXGBoost: 26470.758028378285\n```\nWhat we can observe:\n* The SVMs performed the worst (though the RBF Kernel performed much better than the Linear Kernel)\n* AdaBoost got 5th place\n* RandomForests got 4th place\n* LGBM got 3rd\n* Gradient Boosting got 2nd\n* XGBoost performed the best, unsurprisingly, although the training time was nearly 19 times higher","metadata":{"execution":{"iopub.status.busy":"2021-09-08T10:49:11.417303Z","iopub.execute_input":"2021-09-08T10:49:11.417681Z","iopub.status.idle":"2021-09-08T10:49:11.424807Z","shell.execute_reply.started":"2021-09-08T10:49:11.41765Z","shell.execute_reply":"2021-09-08T10:49:11.423729Z"}}},{"cell_type":"markdown","source":"Following cell inspired from Notebook (Hyperparameter values noted):\nhttps://www.kaggle.com/lavanyashukla01/how-i-made-top-0-3-on-a-kaggle-competition#Train-a-model","metadata":{}},{"cell_type":"code","source":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Support Vector Regressor\nsvr = SVR(C= 20, epsilon= 0.008, gamma=0.0003)\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=2200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingRegressor(estimators=[\n                                ('xgboost',xgboost), \n                                ('lightgbm',lightgbm), \n                                ('svr',svr),  \n                                ('gbr',gbr), \n                                ('rf',rf)],\n                                final_estimator=xgboost,\n                                n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:19:32.119735Z","iopub.execute_input":"2021-09-14T13:19:32.120022Z","iopub.status.idle":"2021-09-14T13:19:32.13661Z","shell.execute_reply.started":"2021-09-14T13:19:32.119994Z","shell.execute_reply":"2021-09-14T13:19:32.135426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We make use of a `StackingRegressor` here which takes a collection of models and aggregates their predictions by having a *meta-learner* treat it as independent variables and the true values as the targets. This is another form of Ensembling.","metadata":{}},{"cell_type":"markdown","source":"## Fit the Models","metadata":{}},{"cell_type":"code","source":"%%time\n# Stacking Regressor\nstack_gen.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:19:32.138142Z","iopub.execute_input":"2021-09-14T13:19:32.138454Z","iopub.status.idle":"2021-09-14T13:26:30.369694Z","shell.execute_reply.started":"2021-09-14T13:19:32.138424Z","shell.execute_reply":"2021-09-14T13:26:30.368562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Random Forest\nrf.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:26:30.371988Z","iopub.execute_input":"2021-09-14T13:26:30.372663Z","iopub.status.idle":"2021-09-14T13:27:25.784502Z","shell.execute_reply.started":"2021-09-14T13:26:30.372618Z","shell.execute_reply":"2021-09-14T13:27:25.783653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# XGBoost\nxgboost.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:27:25.785707Z","iopub.execute_input":"2021-09-14T13:27:25.786003Z","iopub.status.idle":"2021-09-14T13:28:12.024156Z","shell.execute_reply.started":"2021-09-14T13:27:25.785975Z","shell.execute_reply":"2021-09-14T13:28:12.023172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Gradient Boosting\ngbr.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:28:12.027057Z","iopub.execute_input":"2021-09-14T13:28:12.027449Z","iopub.status.idle":"2021-09-14T13:28:33.337271Z","shell.execute_reply.started":"2021-09-14T13:28:12.027414Z","shell.execute_reply":"2021-09-14T13:28:33.336435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# SVR\nsvr.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:28:33.338603Z","iopub.execute_input":"2021-09-14T13:28:33.338873Z","iopub.status.idle":"2021-09-14T13:28:34.700903Z","shell.execute_reply.started":"2021-09-14T13:28:33.338847Z","shell.execute_reply":"2021-09-14T13:28:34.699829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# LGBM\nlightgbm.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:28:34.702198Z","iopub.execute_input":"2021-09-14T13:28:34.702568Z","iopub.status.idle":"2021-09-14T13:28:46.80885Z","shell.execute_reply.started":"2021-09-14T13:28:34.702533Z","shell.execute_reply":"2021-09-14T13:28:46.807816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With all the models trained on the data, we can get their predictions and *blend* them (the final predictions will be a linear combination/weighted sum of the individual models' predictions so that the stronger models aren't totally neglected). The weights should obviously add up to 1.","metadata":{}},{"cell_type":"code","source":"0.1 + 0.2 + 0.2 + 0.1 + 0.05 + 0.35","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:28:46.81033Z","iopub.execute_input":"2021-09-14T13:28:46.810732Z","iopub.status.idle":"2021-09-14T13:28:46.816859Z","shell.execute_reply.started":"2021-09-14T13:28:46.81068Z","shell.execute_reply":"2021-09-14T13:28:46.81588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Blend the predictions\ndef blended_predictions(X):\n    return ((0.1 * svr.predict(X)) + \\\n            (0.2 * gbr.predict(X)) + \\\n            (0.2 * xgboost.predict(X)) + \\\n            (0.1 * lightgbm.predict(X)) + \\\n            (0.05 * rf.predict(X)) + \\\n            (0.35 * stack_gen.predict(X)))\n\n# Get the submission file ready, REMEMBERING to invert the log transform we applied earlier\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blended_predictions(X_test)))\n\nsubmission.to_csv(\"submission_regression.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:28:46.817966Z","iopub.execute_input":"2021-09-14T13:28:46.818236Z","iopub.status.idle":"2021-09-14T13:28:50.783298Z","shell.execute_reply.started":"2021-09-14T13:28:46.818209Z","shell.execute_reply":"2021-09-14T13:28:50.782333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"FIN\")","metadata":{"execution":{"iopub.status.busy":"2021-09-14T13:28:50.784403Z","iopub.execute_input":"2021-09-14T13:28:50.784685Z","iopub.status.idle":"2021-09-14T13:28:50.789185Z","shell.execute_reply.started":"2021-09-14T13:28:50.784656Z","shell.execute_reply":"2021-09-14T13:28:50.788293Z"},"trusted":true},"execution_count":null,"outputs":[]}]}