{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"13442bb1-af4d-f230-00ba-f62f060aba21"},"source":"Predicting House Sale Prices with Python MICE + XGBoost + Regularized Linear Models\n--------------\n**Author**: Mizio Andersen\n\nMy notebook is on [github][1]\n\nThis notebook implements MICE (Multiple Imputation by Chained Equations) in fields with missing values.\n\n**Models used**: XGBoost, LassoCV, RidgeCV combined with feature selection.\n\n**Feature engineering**: one-hot encoding\n\n**Feature scaling**: standardization (centering and scaling) of dataset that removes mean and scales to unit variance. It is necessary for machine learning estimators.\n\n**Outlier identification**: based on LassoCV it compares rmse with residual of every point predicted vs. actual sale price.\n\n**Feature agglomeration**: clustering of highly correlated features\n\nValidation of models by split of training set and plotting predicted vs. actual sale price. Submission uses blending of xgboost and lasso result.\n\n  [1]: https://github.com/MizioAnd/HousePrices/blob/master/house_prices.ipynb"},{"cell_type":"markdown","metadata":{"_cell_guid":"a69eed8a-7c5a-13c9-94b3-11ff39c600e8"},"source":"Data Cleaning using Class (python OOP) \n----------\nWe will use a class to hold all our important methods, which is the easiest\nway to generalize our analysis to treat many cases with few code\nchanges."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f9cdf38-a062-7410-99e1-ebb49a6d8475"},"outputs":[],"source":"class HousePrices(object):\n    import numpy as np\n    import pandas as pd\n    # import matplotlib\n    # matplotlib.use('TkAgg')\n    import pylab as plt\n    from fancyimpute import MICE\n    # import sys\n    # sys.path.append('/custom/path/to/modules')\n    import random\n    # from sklearn.model_selection import cross_val_score\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.preprocessing import OneHotEncoder\n    from scipy.stats import skew\n    from sklearn.model_selection import cross_val_score\n    from sklearn.model_selection import KFold, train_test_split\n    from sklearn.linear_model import LassoCV\n    from sklearn.ensemble import IsolationForest\n    import xgboost as xgb\n    from sklearn.preprocessing import StandardScaler, LabelBinarizer\n    from sklearn_pandas import DataFrameMapper\n    import xgboost as xgb\n    from matplotlib.backends.backend_pdf import PdfPages\n    import datetime\n    from sklearn.cluster import FeatureAgglomeration\n    import seaborn as sns\n    # import math\n\n    def __init__(self):\n        self.df = HousePrices.df\n        self.df_test = HousePrices.df_test\n        self.df_all_feature_var_names = []\n        self.df_test_all_feature_var_names = []\n        self.timestamp = datetime.datetime.now().strftime('%Y%m%d_%Hh%Mm%Ss')\n        self.is_with_log1p_SalePrice = 0\n\n    # Private variables\n    _non_numerical_feature_names = []\n    _numerical_feature_names = []\n    _is_one_hot_encoder = 0\n    _feature_names_num = []\n    _save_path = '/home/mizio/Documents/Kaggle/HousePrices/prepared_data_train_and_test_saved/'\n    _is_not_import_data = 0\n    _is_dataframe_with_sale_price = 1\n\n    ''' Pandas Data Frame '''\n    df = pd.read_csv('../input/train.csv', header=0)\n    df_test = pd.read_csv('../input/test.csv', header=0)\n\n    @staticmethod\n    def square_feet_to_meters(area):\n        square_meter_per_square_feet = 0.3048**2\n        return area*square_meter_per_square_feet\n\n    @staticmethod\n    def extract_numerical_features(df):\n        df = df.copy()\n        # Identify numerical columns which are of type object\n        numerical_features = pd.Series(data=False, index=df.columns, dtype=bool)\n\n        for feature in df.columns:\n            if any(tuple(df[feature].apply(lambda x: type(x)) == int)) or \\\n                            any(tuple(df[feature].apply(lambda x: type(x)) == float)) & \\\n                            (not any(tuple(df[feature].apply(lambda x: type(x)) == str))):\n                numerical_features[feature] = 1\n        return numerical_features[numerical_features == 1].index\n\n    @staticmethod\n    def extract_non_numerical_features(df):\n        df = df.copy()\n        return df.select_dtypes(exclude=[np.number])\n\n    def clean_data(self, df):\n        df = df.copy()\n        is_with_MICE = 1\n        if df.isnull().sum().sum() > 0:\n            if is_with_MICE:\n                # Imputation using MICE\n                numerical_features_names = self.extract_numerical_features(df)\n                df.loc[:, tuple(numerical_features_names)] = self.estimate_by_mice(\n                    df[numerical_features_names])\n            else:\n                if any(tuple(df.columns == 'SalePrice')):\n                    df = df.dropna()\n                else:\n                    df = df.dropna(1)\n                    HousePrices._feature_names_num = pd.Series(data=np.intersect1d(\n                        HousePrices._feature_names_num.values, df.columns), dtype=object)\n        return df\n\n    @staticmethod\n    def encode_labels_in_numeric_format(df, estimated_var):\n        # Transform non-numeric labels into numerical values\n        # Cons.: gives additional unwanted structure to data, since some values are high and \n        # others low, despite labels\n        # where no such comparing measure exists.\n        # Alternative: use one-hot-encoding giving all labels their own column represented with \n        # only binary values.\n        feature_name_num = ''.join([estimated_var, 'Num'])\n        mask = ~df[estimated_var].isnull()\n        df[feature_name_num] = df[estimated_var]\n        df.loc[mask, tuple([feature_name_num])] = df[estimated_var].factorize()[0][\n            mask[mask == 1].index]\n\n    @staticmethod\n    def label_classes(df, estimated_var):\n        le = LabelEncoder()\n        le.fit(df[estimated_var].values)\n        return le.classes_\n\n    @staticmethod\n    def one_hot_encoder(df, estimated_var):\n        df_class = df.copy()\n        ohe = OneHotEncoder()\n        label_classes = df_class[estimated_var].factorize()[1]\n        new_one_hot_encoded_features = [''.join([estimated_var, '_', x]) for x in label_classes]\n        mask = ~df[estimated_var].isnull()\n        feature_var_values = ohe.fit_transform(np.reshape(np.array(\n            df[''.join([estimated_var, 'Num'])][mask].values), \n                                                          (df[mask].shape[0], \n                                                           1))).toarray().astype(int)\n        # Create new feature_var columns with one-hot encoded values\n        for ite in new_one_hot_encoded_features:\n            df[ite] = df[estimated_var]\n        df.loc[mask, tuple(new_one_hot_encoded_features)] = feature_var_values\n\n    @staticmethod\n    def add_feature_var_name_with_zeros(df, feature_var_name):\n        df[feature_var_name] = np.zeros((df.shape[0], 1), dtype=int)\n        pass\n\n    @staticmethod\n    def feature_var_names_in_training_set_not_in_test_set(feature_var_names_training, \n                                                          feature_var_names_test):\n        feature_var_name_addition_list = []\n        for feature_var_name in feature_var_names_training:\n            if not any(tuple(feature_var_name == feature_var_names_test)):\n                feature_var_name_addition_list.append(feature_var_name)\n        return np.array(feature_var_name_addition_list)\n\n    def feature_mapping_to_numerical_values(self, df):\n        HousePrices._is_one_hot_encoder = 0\n        mask = ~df.isnull()\n        # Assume that training set has all possible feature_var_names\n        # Although it may occur in real life that a training set may hold a feature_var_name. \n        # But it is probably\n        # avoided since such features cannot\n        # be part of the trained learning algo.\n        # Add missing feature_var_names of traning set not occuring in test set. Add these with \n        # zeros in columns.\n        if not any(tuple(df.columns == 'SalePrice')):\n            # All one-hot encoded feature var names occuring in test data is assigned the private \n            # public varaible\n            # df_test_all_feature_var_names.\n            self.df_test_all_feature_var_names = df.columns\n\n        _feature_names_num = np.zeros((HousePrices._non_numerical_feature_names.shape[0],), \n                                      dtype=object)\n        ith = 0\n        for feature_name in HousePrices._non_numerical_feature_names:\n            # Create a feature_nameNum list\n            feature_name_num = ''.join([feature_name, 'Num'])\n            _feature_names_num[ith] = feature_name_num\n            ith += 1\n            HousePrices.encode_labels_in_numeric_format(df, feature_name)\n\n            if HousePrices._is_one_hot_encoder:\n                is_with_label_binarizer = 0\n                if is_with_label_binarizer:\n                    if feature_name == 'MasVnrType':\n                        print('debug')\n                    # feature_var_values = mapper_df.fit_transform(df[feature_name][\n                    #     mask[feature_name]])\n                    mapper_df = DataFrameMapper([(feature_name, LabelBinarizer())], df_out=True)\n                    # Check if we need to merge our result into df\n                    feature_var_values = mapper_df.fit_transform(df.copy())\n                    print(df[feature_name].isnull().sum().sum())\n                    print(df[feature_name][mask[feature_name]].isnull().sum().sum())\n                    for ite in feature_var_values.columns:\n                        df[ite] = feature_var_values[ite]\n                else:\n                    HousePrices.one_hot_encoder(df, feature_name)\n        HousePrices._feature_names_num = pd.Series(data=_feature_names_num, dtype=object)\n\n    @staticmethod\n    def feature_agglomeration(df, number_of_clusters=int(df.shape[1] / 1.2)):\n        df = df.copy()\n        # Todo: find optimal number of clusters for the feature clustering\n        # number_of_clusters = int(df.shape[1]/2)\n\n        agglomerated_features = FeatureAgglomeration(n_clusters=number_of_clusters)\n        # mask = ~df[features].isnull()\n        # mask_index = mask[mask == 1].index\n        if any(tuple(df.columns == 'SalePrice')):\n            res = agglomerated_features.fit_transform(np.reshape(np.array(df.dropna().values), \n                                                                 df.dropna().shape), \n                                                      y=df.SalePrice.values)\n        else:\n            res = agglomerated_features.fit_transform(np.reshape(np.array(df.values), df.shape))\n\n        # Obs. case of adding values using df.loc[], remember mask is only possible for a \n        # single feature at a time.\n        # print(''.join(['labels:', str(agglomerated_features.labels_)]))\n        # print(''.join(['Children:', str(agglomerated_features.children_)]))\n        # print(''.join(['number of leaves in the hierarchical tree:', \n        #                str(agglomerated_features.n_leaves_)]))\n        # HousePrices.dendrogram(df)\n        df = pd.DataFrame(data=res)\n        return df\n\n    @staticmethod\n    def dendrogram(df, number_of_clusters=int(df.shape[1] / 1.2)):\n        # Create Dendrogram\n        agglomerated_features = FeatureAgglomeration(n_clusters=number_of_clusters)\n        used_networks = np.arange(0, number_of_clusters, dtype=int)\n        # used_networks = np.unique(agglomerated_features.labels_)\n\n        # In our case all columns are clustered, which means used_columns is true in every element\n        # used_columns = (df.columns.get_level_values(None)\n                        # .astype(int)\n                        # .isin(used_networks))\n        # used_columns = (agglomerated_feature_labels.astype(int).isin(used_networks))\n        # df = df.loc[:, used_columns]\n\n        # Create a custom palette to identify the networks\n        network_pal = sns.cubehelix_palette(len(used_networks),\n                                            light=.9, dark=.1, reverse=True,\n                                            start=1, rot=-2)\n        network_lut = dict(zip(map(str, df.columns), network_pal))\n\n        # Convert the palette to vectors that will be drawn on the side of the matrix\n        networks = df.columns.get_level_values(None)\n        network_colors = pd.Series(networks, index=df.columns).map(network_lut)\n        sns.set(font=\"monospace\")\n        # Create custom colormap\n        cmap = sns.diverging_palette(h_neg=210, h_pos=350, s=90, l=30, as_cmap=True)\n        cg = sns.clustermap(df.astype(float).corr(), cmap=cmap, linewidths=.5, \n                            row_colors=network_colors, col_colors=network_colors)\n        plt.setp(cg.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n        plt.setp(cg.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n        plt.show()\n\n\n    def feature_engineering(self, df):\n        # df['LotAreaSquareMeters'] = self.square_feet_to_meters(df.LotArea.values)\n\n        is_skewness_correction_for_all_features = 1\n        if is_skewness_correction_for_all_features:\n            # Correcting for skewness\n            # Treat all numerical variables that were not one-hot encoded\n            if any(tuple(df.columns == 'SalePrice')):\n                self.is_with_log1p_SalePrice = 1\n\n            numerical_feature_names_of_non_modified_df = HousePrices._numerical_feature_names\n\n            if HousePrices._is_one_hot_encoder:\n                numerical_feature_names_of_non_modified_df = numerical_feature_names_of_non_modified_df.values\n            else:\n                numerical_feature_names_of_non_modified_df = np.concatenate([\n                    HousePrices._feature_names_num.values, \n                    numerical_feature_names_of_non_modified_df.values])\n\n            relevant_features = df[numerical_feature_names_of_non_modified_df].columns[\n                (df[numerical_feature_names_of_non_modified_df].columns != 'Id')]\n            self.skew_correction(df, relevant_features)\n        else:\n            # Only scale down sale price, since all leave other numerical features standardized.\n            if any(tuple(df.columns == 'SalePrice')):\n                # self.skew_correction(df, 'SalePrice')  # dropna complaining since no nulls\n                self.is_with_log1p_SalePrice = 1\n                df.loc[:, tuple(['SalePrice'])] = np.log1p(df.SalePrice)\n\n    @staticmethod\n    def skew_correction(df, numerical_features):\n        # Skew correction\n        # compute skewness\n        skewed_feats = df[numerical_features].apply(lambda x: skew(x.dropna()))  \n        skewed_feats = skewed_feats[skewed_feats > 0.75]\n        skewed_feats = skewed_feats.index\n        df.loc[:, tuple(skewed_feats)] = np.log1p(np.asarray(df[skewed_feats], dtype=float))\n        # df[skewed_feats] = np.log1p(np.asarray(df[skewed_feats], dtype=float))\n\n    @staticmethod\n    def outlier_prediction(x_train, y_train):\n        # Use built-in isolation forest or use predicted vs. actual\n        # Compute squared residuals of every point\n        # Make a threshold criteria for inclusion\n\n        # The prediction returns 1 if sample point is inlier. If outlier prediction returns -1\n        rng = np.random.RandomState(42)\n        clf_all_features = IsolationForest(max_samples=100, random_state=rng)\n        clf_all_features.fit(x_train)\n\n        # Predict if a particular sample is an outlier using all features for higher dimensional \n        # data set.\n        y_pred_train = clf_all_features.predict(x_train)\n\n        # Exclude suggested outlier samples for improvement of prediction power/score\n        outlier_map_out_train = np.array(map(lambda x: x == 1, y_pred_train))\n        x_train_modified = x_train[outlier_map_out_train, ]\n        y_train_modified = y_train[outlier_map_out_train, ]\n\n        return x_train_modified, y_train_modified\n\n    def drop_variable_before_preparation(self, df):\n        # Acceptable limit of NaN in features\n        limit_of_nans = 0.3*df.shape[0]\n        # limit_of_nans = 800\n        for feature in self.features_with_missing_values_in_dataframe(df).index:\n            if df[feature].isnull().sum() > limit_of_nans:\n                df = df.drop([feature], axis=1)\n\n        # df = df.drop(['Alley'], axis=1)\n        # df = df.drop(['MasVnrType'], axis=1)\n        # df = df.drop([\"Utilities\", \"LotFrontage\", \"Alley\", \"MasVnrType\", \"MasVnrArea\", \n        #               \"BsmtQual\",\n        #               \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n        #               \"Electrical\", \"FireplaceQu\", \"GarageType\", \"GarageYrBlt\",\n        #               \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\",\n        #               \"Fence\", \"MiscFeature\"], axis=1)\n        return df\n\n    def drop_variable(self, df):\n        # df = df.drop(['Fireplaces'], axis=1)\n        df = df.drop(['Id'], axis=1)\n\n        if not any(tuple(df.columns == 'SalePrice')):\n            # All feature var names occuring in test data is assigned the public varaible \n            # df_test_all_feature_var_names.\n            self.df_test_all_feature_var_names = df.columns\n        return df\n\n    def save_dataframe(self, df):\n        if HousePrices._is_dataframe_with_sale_price:\n            df.to_csv(''.join([HousePrices._save_path, 'train_debug', self.timestamp, '.csv']), \n                      columns=df.columns, index=False)\n        else:\n            df.to_csv(''.join([HousePrices._save_path, 'test_debug', self.timestamp, '.csv']), \n                      columns=df.columns, index=False)\n\n    @staticmethod\n    def load_dataframe():\n        if HousePrices._is_dataframe_with_sale_price:\n            dataframe_name = 'train_debug'\n        else:\n            dataframe_name = 'test_debug'\n\n        # one-hot encoded\n        # date_time = '20170227_11h46m53s'  # has num col\n        # date_time = '20170227_12h19m16s'  # has num col\n        # date_time = '20170227_15h36m21s'  # has num col\n        # corrected below\n        # date_time = '20170227_16h50m45s'  # has bug in prediction\n        date_time = '20170228_10h50m23s'\n        # not one-hot\n        # date_time = '20170226_19h53m38s'\n        # date_time = '20170227_14h51m53s'\n        # date_time = '20170227_15h04m15s'\n        # date_time = '20170227_15h57m09s'\n        # date_time = '20170227_16h04m23s'\n        # corrected below\n        # date_time = '20170228_00h05m40s'\n        return pd.read_csv(''.join([HousePrices._save_path, dataframe_name, date_time, '.csv']), \n                           header=0)\n\n    @staticmethod\n    def drop_num_features(df):\n        # Drop all categorical feature helping columns ('Num')\n        # Todo: is it defined when importing data set? _feature_names_num\n        for feature_name in HousePrices._feature_names_num:\n            df = df.drop([feature_name], axis=1)\n        return df\n\n    def prepare_data_random_forest(self, df):\n        df = df.copy()\n        df = self.drop_variable_before_preparation(df)\n\n        # Todo: correct extract_non_numerical_features() and check if similar things are \n        # new in python 3.6\n        HousePrices._non_numerical_feature_names = HousePrices.extract_non_numerical_features(df)._get_axis(1)\n        HousePrices._numerical_feature_names = HousePrices.extract_numerical_features(df)\n        # HousePrices._non_numerical_feature_names = ['MSZoning', 'LotShape', 'Neighborhood', \n        # 'BldgType', 'HouseStyle',\n        # 'Foundation', 'Heating']\n\n        HousePrices._is_not_import_data = 1\n        if HousePrices._is_not_import_data:\n            self.feature_mapping_to_numerical_values(df)\n            if HousePrices._is_one_hot_encoder:\n                df = HousePrices.drop_num_features(df)\n            self.feature_engineering(df)\n            df = self.clean_data(df)\n            # df = self.feature_scaling(df)\n\n            is_save_dataframe = 0\n            if is_save_dataframe:\n                self.save_dataframe(df)\n                HousePrices._is_dataframe_with_sale_price = 0\n        else:\n            # Todo: create and save dataframe for debuggin in case of one-hot encoding\n            # if not HousePrices._is_not_import_data:\n            df = HousePrices.load_dataframe()\n            # HousePrices._non_numerical_feature_names = HousePrices.extract_non_numerical_features(df)._get_axis(1)\n            # HousePrices._numerical_feature_names = HousePrices.extract_numerical_features(df)\n            HousePrices._is_dataframe_with_sale_price = 0\n\n        df = self.drop_variable(df)\n        return df\n\n    @staticmethod\n    def features_with_null_logical(df, axis=1):\n        row_length = len(df._get_axis(0))\n        # Axis to count non null values in. aggregate_axis=0 implies counting for every feature\n        aggregate_axis = 1 - axis\n        features_non_null_series = df.count(axis=aggregate_axis)\n        # Whenever count() differs from row_length it implies a null value exists in feature \n        # column and a False in mask\n        mask = row_length == features_non_null_series\n        return mask\n\n    @staticmethod\n    def estimate_by_mice(df):\n        df_estimated_var = df.copy()\n        random.seed(129)\n        mice = MICE()  # model=RandomForestClassifier(n_estimators=100))\n        res = mice.complete(np.asarray(df.values, dtype=float))\n        df_estimated_var.loc[:, df.columns] = res[:][:]\n        return df_estimated_var\n\n    def feature_scaling(self, df):\n        df = df.copy()\n        # Standardization (centering and scaling) of dataset that removes mean and scales to \n        # unit variance\n        standard_scaler = StandardScaler()\n        numerical_feature_names_of_non_modified_df = HousePrices._numerical_feature_names\n        if any(tuple(df.columns == 'SalePrice')):\n            if not HousePrices._is_one_hot_encoder:\n                numerical_feature_names_of_non_modified_df = np.concatenate(\n                    [HousePrices._feature_names_num.values, \n                     numerical_feature_names_of_non_modified_df.values])\n            # Include scaling of SalePrice\n            y = df.SalePrice.values\n            relevant_features = df[numerical_feature_names_of_non_modified_df].columns[\n                (df[numerical_feature_names_of_non_modified_df].columns != 'SalePrice')\n                & (df[numerical_feature_names_of_non_modified_df].columns != 'Id')]\n            mask = ~df[relevant_features].isnull()\n            res = standard_scaler.fit_transform(X=df[relevant_features][mask].values, y=y)\n            if (~mask).sum().sum() > 0:\n                df = self.standardize_relevant_features(df, relevant_features, res)\n            else:\n                df.loc[:, tuple(relevant_features)] = res\n        else:\n            if not HousePrices._is_one_hot_encoder:\n                numerical_feature_names_of_non_modified_df = np.concatenate(\n                    [HousePrices._feature_names_num.values, \n                     numerical_feature_names_of_non_modified_df.values])\n\n            relevant_features = df[numerical_feature_names_of_non_modified_df].columns[\n                (df[numerical_feature_names_of_non_modified_df].columns != 'Id')]\n            mask = ~df[relevant_features].isnull()\n            res = standard_scaler.fit_transform(df[relevant_features][mask].values)\n            if mask.sum().sum() > 0:\n                df = self.standardize_relevant_features(df, relevant_features, res)\n            else:\n                df.loc[:, tuple(relevant_features)] = res\n        return df\n\n    @staticmethod\n    def standardize_relevant_features(df, relevant_features, res):\n        i_column = 0\n        for feature in relevant_features:\n            mask = ~df[feature].isnull()\n            mask_index = mask[mask == 1].index\n            df.loc[mask_index, tuple([feature])] = res[:, i_column]\n            i_column += 1\n        return df\n\n    def missing_values_in_dataframe(self, df):\n        mask = self.features_with_null_logical(df)\n        print(df[mask[mask == 0].index.values].isnull().sum())\n        print('\\n')\n\n    def features_with_missing_values_in_dataframe(self, df):\n        df = df.copy()\n        mask = self.features_with_null_logical(df)\n        return df[mask[mask == 0].index.values].isnull().sum()\n\n    @staticmethod\n    def rmse_cv(model, x_train, y_train):\n        rmse = np.sqrt(-cross_val_score(model, x_train, y_train, \n                                        scoring='neg_mean_squared_error', cv=5))\n        return rmse\n\n    @staticmethod\n    def rmse(y_pred, y_actual):\n        n_samples = np.shape(y_pred)[0]\n        squared_residuals_summed = 0.5*sum((y_pred - y_actual)**2)\n        return np.sqrt(2.0*squared_residuals_summed/n_samples)\n\n    def outlier_identification(self, model, x_train, y_train):\n        # Split the training data into an extra set of test\n        x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train,\n                                                                                    y_train)\n        print('\\nOutlier shapes')\n        print(np.shape(x_train_split), np.shape(x_test_split), np.shape(y_train_split), \n              np.shape(y_test_split))\n        model.fit(x_train_split, y_train_split)\n        y_predicted = model.predict(x_test_split)\n        residuals = np.absolute(y_predicted - y_test_split)\n        rmse_pred_vs_actual = self.rmse(y_predicted, y_test_split)\n        outliers_mask = residuals >= rmse_pred_vs_actual\n        # outliers_mask = np.insert(np.zeros((np.shape(y_train_split)[0],), dtype=np.int), \n        # np.shape(y_train_split)[0], outliers_mask)\n        outliers_mask = np.concatenate([np.zeros((np.shape(y_train_split)[0],), dtype=bool), \n                                        outliers_mask])\n        not_an_outlier = outliers_mask == 0\n        # Resample the training set from split, since the set was randomly split\n        x_out = np.insert(x_train_split, np.shape(x_train_split)[0], x_test_split, axis=0)\n        y_out = np.insert(y_train_split, np.shape(y_train_split)[0], y_test_split, axis=0)\n        return x_out[not_an_outlier, ], y_out[not_an_outlier, ]\n\n    def predicted_vs_actual_sale_price(self, x_train, y_train, title_name):\n        # Split the training data into an extra set of test\n        x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train, \n                                                                                    y_train)\n        print(np.shape(x_train_split), np.shape(x_test_split), np.shape(y_train_split), \n              np.shape(y_test_split))\n        lasso = LassoCV(alphas=[0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, \n                                0.1, 0.3, 0.6, 1],\n                        max_iter=50000, cv=10)\n        # lasso = RidgeCV(alphas=[0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, \n        #                          0.1, 0.3, 0.6, 1], cv=10)\n\n        lasso.fit(x_train_split, y_train_split)\n        y_predicted = lasso.predict(X=x_test_split)\n        plt.figure(figsize=(10, 5))\n        plt.scatter(y_test_split, y_predicted, s=20)\n        rmse_pred_vs_actual = self.rmse(y_predicted, y_test_split)\n        plt.title(''.join([title_name, ', Predicted vs. Actual.', ' rmse = ', \n                           str(rmse_pred_vs_actual)]))\n        plt.xlabel('Actual Sale Price')\n        plt.ylabel('Predicted Sale Price')\n        plt.plot([min(y_test_split), max(y_test_split)], [min(y_test_split), max(y_test_split)])\n        plt.tight_layout()\n\n    def predicted_vs_actual_sale_price_input_model(self, model, x_train, y_train, title_name):\n        # Split the training data into an extra set of test\n        x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train, \n                                                                                    y_train)\n        print(np.shape(x_train_split), np.shape(x_test_split), np.shape(y_train_split), \n              np.shape(y_test_split))\n        model.fit(x_train_split, y_train_split)\n        y_predicted = model.predict(x_test_split)\n        plt.figure(figsize=(10, 5))\n        plt.scatter(y_test_split, y_predicted, s=20)\n        rmse_pred_vs_actual = self.rmse(y_predicted, y_test_split)\n        plt.title(''.join([title_name, ', Predicted vs. Actual.', ' rmse = ', \n                           str(rmse_pred_vs_actual)]))\n        plt.xlabel('Actual Sale Price')\n        plt.ylabel('Predicted Sale Price')\n        plt.plot([min(y_test_split), max(y_test_split)], [min(y_test_split), max(y_test_split)])\n        plt.tight_layout()\n\n    def predicted_vs_actual_sale_price_xgb(self, xgb, best_nrounds, xgb_params, x_train, y_train, \n                                           title_name):\n        # Split the training data into an extra set of test\n        x_train_split, x_test_split, y_train_split, y_test_split = train_test_split(x_train, \n                                                                                    y_train)\n        dtrain_split = xgb.DMatrix(x_train_split, label=y_train_split)\n        dtest_split = xgb.DMatrix(x_test_split)\n\n        # res = xgb.cv(xgb_params, dtrain_split, num_boost_round=1000, nfold=4, seed=seed, \n        # stratified=False,\n        #              early_stopping_rounds=25, verbose_eval=10, show_stdv=True)\n        #\n        # best_nrounds = res.shape[0] - 1\n        print(np.shape(x_train_split), np.shape(x_test_split), np.shape(y_train_split), \n              np.shape(y_test_split))\n        gbdt = xgb.train(xgb_params, dtrain_split, best_nrounds)\n        y_predicted = gbdt.predict(dtest_split)\n        plt.figure(figsize=(10, 5))\n        plt.scatter(y_test_split, y_predicted, s=20)\n        rmse_pred_vs_actual = self.rmse(y_predicted, y_test_split)\n        plt.title(''.join([title_name, ', Predicted vs. Actual.', ' rmse = ', \n                           str(rmse_pred_vs_actual)]))\n        plt.xlabel('Actual Sale Price')\n        plt.ylabel('Predicted Sale Price')\n        plt.plot([min(y_test_split), max(y_test_split)], [min(y_test_split), max(y_test_split)])\n        plt.tight_layout()\n\n    @staticmethod\n    def multipage(filename, figs=None):\n        pp = PdfPages(filename)\n        if figs is None:\n            figs = [plt.figure(n) for n in plt.get_fignums()]\n        for fig in figs:\n            fig.savefig(pp, format='pdf')\n        pp.close()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"22ece59e-74e8-19fe-c47d-3e333b630609"},"outputs":[],"source":"if __name__ == '__main__':\n    import numpy as np\n    import pandas as pd\n    import pylab as plt\n    from fancyimpute import MICE\n    import random\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.preprocessing import OneHotEncoder\n    from scipy.stats import skew\n    from sklearn.model_selection import cross_val_score\n    from sklearn.model_selection import KFold, train_test_split\n    from sklearn.linear_model import LassoCV\n    from sklearn.ensemble import IsolationForest\n    import xgboost as xgb\n    from sklearn.preprocessing import StandardScaler, LabelBinarizer\n    from sklearn_pandas import DataFrameMapper\n    from matplotlib.backends.backend_pdf import PdfPages\n    from sklearn.cluster import FeatureAgglomeration\n    import datetime\n    \n    import xgboost as xgb\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import RandomForestRegressor\n    from sklearn.linear_model import SGDRegressor\n    from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n    # from sklearn.linear_model import LogisticRegression\n    from sklearn.feature_selection import SelectFromModel\n    # from sklearn.naive_bayes import GaussianNB\n    # from sklearn import svm\n    # from collections import OrderedDict\n    # from sklearn.ensemble import IsolationForest\n    import seaborn as sns\n    from sklearn.model_selection import StratifiedKFold\n    from sklearn.model_selection import GridSearchCV\n    # from sklearn.model_selection import KFold, train_test_split"},{"cell_type":"markdown","metadata":{"_cell_guid":"8d4cf7a1-b1c4-7e00-c885-2a53ef5681dd"},"source":"Prepare Data \n----------\nBefore we start to do any predictions using our machine learning estimators\nthere are a couple of important steps that needs to be done.\n\n - Transform categorical features to numerical values\n - Fill in missing values in our data using Multiple Impuations by Chained Equations (MICE).\n - Correct skewness and standardize our data by subtraction of mean and transforming \n    to unit variance\n - Later we will also subtract potential outliers"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1c4a418-5220-fce9-8d07-70b12e2e404d"},"outputs":[],"source":"    ''' Prepare data '''\n\n    house_prices = HousePrices()\n    df_publ = house_prices.df.copy()\n    df_test_publ = house_prices.df_test.copy()\n\n    df = house_prices.prepare_data_random_forest(df_publ)\n    house_prices.df_all_feature_var_names = df[df.columns[df.columns != 'SalePrice']].columns\n    print('\\n TRAINING DATA:----------------------------------------------- \\n')\n    print(df.head(3))\n    print('\\n')\n    print(df.info())\n    print('\\n')\n    print(df.describe())\n\n    # Test data\n    Id_df_test = house_prices.df_test['Id']  # Submission column\n    df_test = house_prices.prepare_data_random_forest(df_test_publ)\n    print('\\n TEST DATA:----------------------------------------------- \\n')\n    print(df_test.info())\n    print('\\n')\n    print(df_test.describe())\n    print('\\n')\n\n    # Check if feature_var_names of test exist that do not appear in training set\n    # feature_var_names_addition_to_training_set = \n    # house_prices.feature_var_names_in_training_set_not_in_test_set(\n    #     df_test.columns, df.columns)\n\n    df = df[house_prices.df_test_all_feature_var_names.insert(\n        np.shape(house_prices.df_test_all_feature_var_names)[0], 'SalePrice')]\n    df_test = df_test[house_prices.df_test_all_feature_var_names]\n    df_test_num_features = house_prices.extract_numerical_features(df_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"9cbe1bb7-1bc9-7aac-c91d-4e17c1da3043"},"source":"Agglomerated features in clusters"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f99a017-a0b4-b094-5df8-c919ade6bee5"},"outputs":[],"source":"    # Agglomeration of features\n    is_with_agglomeration = 1\n    if is_with_agglomeration:\n        df_merged_train_and_test = pd.DataFrame(data=np.concatenate(\n            (df[df.columns[df.columns != 'SalePrice']].values, df_test.values)), \n                                                columns=house_prices.df_test_all_feature_var_names)\n        df_merged_train_and_test = df_merged_train_and_test[df_test_num_features]\n        df_merged_train_and_test_agglom = HousePrices.feature_agglomeration(df_merged_train_and_test)\n        train_data = np.concatenate((df_merged_train_and_test_agglom.values[:df.shape[0], 0::],\n                                     np.reshape(df.SalePrice.values, \n                                                (df.SalePrice.values.shape[0], 1))), axis=1)\n        test_data = df_merged_train_and_test_agglom.values[df.shape[0]::, 0::]\n    else:\n        df[np.concatenate((df_test_num_features, ['SalePrice']))].values\n        test_data = df_test[df_test_num_features].values\n        # print(sum(np.isnan(train_data)).sum()) # 348 is nan"},{"cell_type":"markdown","metadata":{"_cell_guid":"6515a8df-970d-e485-4251-ea87e32a0359"},"source":"Explore Data and Missing Values\n---------\nIt is always a good idea to check if your data munging has worked.\nBelow we check if there are any forgotten missing values in our numerical\nfeatures concerning also transformed categorical features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36453399-cd4e-b256-521d-7d4bc947fdb7"},"outputs":[],"source":"    ''' Explore data '''\n    explore_data = 1\n    if explore_data:\n\n        is_missing_value_exploration = 1\n        if is_missing_value_exploration:\n            # Imputation for the 11 columns with none or nan values in the test data.\n            # Using only numerical feature columns as first approach.\n\n            # Train Data: numeric feature columns with none or nan in test data\n            print('\\nColumns in train data with none/nan values:\\n')\n            print('\\nTraining set numerical features\\' missing values')\n            df_publ_numerical_features = house_prices.extract_numerical_features(df_publ)\n            house_prices.missing_values_in_dataframe(df_publ[df_publ_numerical_features])\n\n            # Test Data: Print numeric feature columns with none/nan in test data\n            print('\\nColumns in test data with none/nan values:\\n')\n            print('\\nTest set numerical features\\' missing values')\n            df_test_publ_numerical_features = house_prices.extract_numerical_features(df_test_publ)\n            house_prices.missing_values_in_dataframe(df_test_publ[df_test_publ_numerical_features])\n\n            # Imputation method applied to numeric columns in test data with none/nan values\n            # print(\"Training set missing values after imputation\")\n            # df_imputed = house_prices.estimate_by_mice(df_publ_numerical_features)\n            # house_prices.missing_values_in_dataframe(df_imputed)\n            # print(\"Testing set missing values after imputation\")\n            # df_test_imputed = house_prices.estimate_by_mice(df_test_publ_numerical_features)\n            # house_prices.missing_values_in_dataframe(df_test_imputed)\n\n            print('\\nTotal Records for values: {}\\n'.format(house_prices.df.count().sum() + \n                                                            house_prices.df_test.count().sum()))\n            print('Total Records for missing values: {}\\n'\n                  .format(house_prices.df.isnull().sum().sum() + \n                          house_prices.df_test.isnull().sum().sum()))\n\n            print('All Training set missing values')\n            house_prices.missing_values_in_dataframe(house_prices.df)\n\n            print('All Test set missing values')\n            house_prices.missing_values_in_dataframe(house_prices.df_test)\n\n            print(\"\\n=== AFTER IMPUTERS ===\\n\")\n            print(\"=== Check for missing values in set ===\")\n            # Todo: fix the bug that \"Total Records for missing values\" stays unchanged while\n            # \"Total Records for values\" changes\n            print('\\nTotal Records for values: {}\\n'.format(df.count().sum() + \n                                                            df_test.count().sum()))\n            print('Total Records for missing values: {}\\n'.format(df.isnull().sum().sum() + \n                                                                  df_test.isnull().sum().sum()))\n\n            # Train Data: numeric feature columns with none or nan in test data\n            print('\\nColumns in train data with none/nan values:\\n')\n            print('\\nTraining set numerical features\\' missing values')\n            df_numerical_features = house_prices.extract_numerical_features(df)\n            house_prices.missing_values_in_dataframe(df[df_numerical_features])\n\n            # Test Data: Print numeric feature columns with none/nan in test data\n            print('\\nColumns in test data with none/nan values:\\n')\n            print('\\nTest set numerical features\\' missing values')\n            df_test_numerical_features = house_prices.extract_numerical_features(df_test)\n            house_prices.missing_values_in_dataframe(df_test[df_test_numerical_features])"},{"cell_type":"markdown","metadata":{"_cell_guid":"329683e4-d0d2-9af3-871f-2220b2fd545e"},"source":"Visualizing Data\n---------\nHistograms are quick ways to see how our data is distributed.\nWe notice that we need to correct for skewness and that the\nscale of sale price may be much larger than other features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"171df5bf-17c1-a5bb-edb4-7d08d2a7027c"},"outputs":[],"source":"            # Histogram of sale prices\n            plt.figure()\n            house_prices.df[['SalePrice']].hist(bins='auto', alpha=.5)\n            plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f38d8df1-ecc3-a216-311c-4ac667ccbd8e"},"outputs":[],"source":"            # Histogram of sale prices after data munging\n            plt.figure()\n            df[['SalePrice']].hist(bins='auto', alpha=.5)\n            plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"e45a6c4b-fe7c-5446-e85d-1c6022234822"},"source":"We could ask the question in what month or in what year are most\nhouses sold?"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3eeaa7fe-fe15-edf6-a6ad-1a28422d552f"},"outputs":[],"source":"            # We expect more houses to be sold in the summer. Which is also the \n            # case month MM, year YYYY.\n            # Sale tops in juli\n            plt.figure()\n            house_prices.df[['MoSold', 'YrSold']].hist(bins='auto', alpha=.5)\n            plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"683f67b5-7eef-026d-2bc9-ed7ec47e96bb"},"source":"Two heatmaps showing correlations between features before and after \ndata munging. In the second plot a lot more features occur, since\ncategorical features have been transformed to numerical.\nAlthough it may be a little difficult to read the names of the many features in the second plot,\nit can still give an overview. Notice that sale price is the last feature occurring\non both axes."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bf5da146-4aae-99d3-7477-db2dc6182e3b"},"outputs":[],"source":"        # Categorical plot with seaborn\n        is_categorical_plot = 1\n        if is_categorical_plot:\n            # sns.countplot(y='MSZoning', hue='MSSubClass', data=df, palette='Greens_d')\n            # plt.show()\n            # sns.stripplot(x='SalePrice', y='MSZoning', data=df, jitter=True, hue='LandContour')\n            # plt.show()\n            # sns.boxplot(x='SalePrice', y='MSZoning', data=df, hue='MSSubClass')\n            # plt.show()\n            \n            # Heatmap of feature correlations\n            plt.figure(figsize=(10, 8))\n            correlations = house_prices.df.corr()\n            sns.heatmap(correlations, vmax=0.8, square=True)\n            plt.show()\n\n            # plt.figure()\n            # sns.stripplot(x='SalePrice', y='OverallQual', data=house_prices.df, jitter=True)\n            # plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"19421a7a-d479-314e-a96e-eb767052ce05"},"outputs":[],"source":"            # Heatmap of feature correlations after data munging\n            plt.figure(figsize=(22, 14))\n            correlations = df[house_prices.extract_numerical_features(df)].corr()\n            sns.heatmap(correlations, vmax=0.8, square=True)\n            plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"281fc512-b78c-5f3e-e89e-273b05d594de"},"source":"Some features are highly correlated and it may therefore improve predictions if we apply feature agglomeration to our data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8c2427b-9e01-32e1-b37c-9455182071ba"},"outputs":[],"source":"            # Plot of agglomerated features in heatmap before data munging\n            num_features = house_prices.extract_numerical_features(house_prices.df_test)\n            \n            \n            df_merged_train_and_test_before_munging = pd.DataFrame(\n                data=np.concatenate((house_prices.df[house_prices.df.columns[\n                    house_prices.df.columns != 'SalePrice']].values, \n                                     house_prices.df_test.values)), columns=house_prices.df_test.columns)\n            HousePrices.dendrogram(df_merged_train_and_test_before_munging[num_features])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f7185dce-5f4c-c10b-21ab-b330233ef7b0"},"outputs":[],"source":"            # Plot of agglomerated features in heatmap after data munging\n            HousePrices.dendrogram(df_merged_train_and_test)"},{"cell_type":"markdown","metadata":{"_cell_guid":"edb5fca0-7d7b-1cd2-2d9e-78fc34ea3c4b"},"source":"With a heatmap we are able to discover how much some features depend on the sale price.\nSale price is indicated inside each small box and color scale is one the right."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f628b9e7-2dea-7eee-62d9-08042278bdcd"},"outputs":[],"source":"            # Produce a heatmap with coefficients\n            plt.figure()\n            heat_data = house_prices.df.pivot_table(values='SalePrice', index=['OverallQual'], \n                                                    columns=['GarageCars'])\n            htmp = sns.heatmap(heat_data, annot=True, cmap='YlGn')\n            plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"b663a979-94ed-d4da-5879-c9f1229b980e"},"source":"Another convenient way to use a heatmap is by showing correlation coefficients \nbetween features. This makes it easy to see what is very much correlated with \nsale price and what features are less correlated.\nTwo plots are shown for cases before and after data munging.\nNotice the white empty lines in the first plot occur due to missing values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2fd0254b-485e-2c5c-d61f-41e86e44a54b"},"outputs":[],"source":"            # Zoom of heatmap with coefficients\n            plt.figure(figsize=(20, 12))\n            top_features = 10\n            columns = correlations.nlargest(top_features, 'SalePrice')['SalePrice'].index\n            correlation_coeff = np.corrcoef(house_prices.df[columns].values.T)\n            sns.set(font_scale=1.20)\n            coeff_heatmap = sns.heatmap(correlation_coeff, annot=True, cmap='YlGn', cbar=True, \n                                        square=True, fmt='.2f', annot_kws={'size': 10}, \n                                        yticklabels=columns.values, xticklabels=columns.values)\n            plt.show()\n            plt.close()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"10fc353b-2ae8-9eea-0d77-e4b30687d588"},"outputs":[],"source":"            # Zoom of heatmap with coefficients after data munging\n            plt.figure(figsize=(20, 12))\n            top_features = 10\n            columns = correlations.nlargest(top_features, 'SalePrice')['SalePrice'].index\n            correlation_coeff = np.corrcoef(df[columns].values.T)\n            sns.set(font_scale=1.20)\n            coeff_heatmap = sns.heatmap(correlation_coeff, annot=True, cmap='YlGn', cbar=True, \n                                        square=True, fmt='.2f', annot_kws={'size': 10}, \n                                        yticklabels=columns.values, xticklabels=columns.values)\n            plt.show()\n            plt.close()"},{"cell_type":"markdown","metadata":{"_cell_guid":"bddc8104-129d-1486-0b48-7d28e7713921"},"source":"A boxplot is a good way to visualize how our data is positioned with \nrespect to sale price and to indicate the mean value, which is\nshown by a straight line inside each box."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c009c47b-5474-d3e6-a242-0e3e6a1ed837"},"outputs":[],"source":"            plt.figure()\n            sns.boxplot(y='SalePrice', x='OverallQual', data=house_prices.df)\n            plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f5740b3b-ef44-8e29-2453-32fdc7879b05"},"outputs":[],"source":"            plt.figure()\n            sns.boxplot(x='SalePrice', y='MSZoning', data=df)\n            plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"c8a7ce52-f5ee-6e3d-e242-2e8aacaec808"},"source":"We expect that house price may highly depend on neighborhood. In estate business location is a very important element."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e71d567c-5877-a129-705f-5cb3b8f68295"},"outputs":[],"source":"            plt.figure()\n            sns.boxplot(x='SalePrice', y='Neighborhood', data=df)\n            plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"66e0d3b2-47cf-01d1-da98-48103e236061"},"outputs":[],"source":"            \n            plt.figure()\n            sns.boxplot(x='SalePrice', y='HouseStyle', data=df)\n            plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5b0cd47d-70fc-9b0b-89d5-fb5da89790df"},"outputs":[],"source":"            \n            plt.figure()\n            sns.boxplot(x='SalePrice', y='SaleCondition', data=df)\n            plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6837816f-522b-89ef-839c-565626a6ee2b"},"outputs":[],"source":"            # sns.violinplot(x='SalePrice', y='MSZoning', data=df)\n            # plt.show()\n            # sns.violinplot(x='SalePrice', y='Neighborhood', data=df)\n            # plt.show()\n\n            # Arbitrary estimate, using the mean by default.\n            # It also uses bootstrapping to compute a confidence interval around the estimate \n            # and plots that using error bars\n            # sns.barplot(x='SalePrice', y='MSZoning', hue='LotShape', data=df)\n            # plt.show()\n            # sns.barplot(x='SalePrice', y='Neighborhood', data=df)#, hue='LotShape')\n            # plt.show()\n            # sns.barplot(x='SalePrice', y='SaleCondition', data=df)#, hue='LotShape')\n            # plt.show()\n            \n            plt.figure()\n            sns.barplot(x='SalePrice', y='HouseStyle', data=df)#, hue='LotShape')\n            plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"f38d6f13-7cd9-fd17-90db-f6b7818859ca"},"source":"Let us plot a few features and study them with respect to SalePrice and GrLivArea."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c67a90c5-0351-a3ee-2804-715c50aa8369"},"outputs":[],"source":"            sns.pointplot(x='SalePrice', y='MSZoning', hue='LotShape', data=df,\n                          palette={\"Reg\": \"g\", \"IR1\": \"m\", \"IR2\": \"b\", \"IR3\": \"r\"}, \n                          markers=[\"^\", \"o\", 'x', '<'], \n                          linestyles=[\"-\", \"--\", '-.', ':'])\n            plt.show()\n\n            g = sns.PairGrid(df, x_vars=['SalePrice', 'GrLivArea'], y_vars=['MSZoning', 'Utilities', \n                                                                          'LotShape'], \n                             aspect=.75, size=3.5)\n            g.map(sns.violinplot, palette='pastel')\n            plt.show()\n\n            # Quite slow\n            # sns.swarmplot(x='MSZoning', y='MSSubClass', data=df, hue='LandContour')\n            # plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"668093af-0f0b-1f47-2d02-91b48bd020ba"},"source":"It is important to study if our regularized linear models are able to find optimal \nregularization parameters alpha. A good parameter is able to prevent overfitting\nin case of high variance data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0915aa31-c200-eddf-2c0a-58ec225a526a"},"outputs":[],"source":"       is_choose_optimal_regularization_param = 1\n       if is_choose_optimal_regularization_param:\n            # Choose optimal value for alpha (regularization parameter) in Lasso and Ridge\n            x_train = train_data[0::, :-1]\n            y_train = train_data[0::, -1]\n            alphas = [0.05, 0.1, 0.3, 1, 3, 4, 10, 15, 30, 50, 75, 100, 110, 130]\n\n            ridge = RidgeCV(alphas=alphas)\n            ridge.fit(x_train, y_train)\n            alpha = ridge.alpha_\n            print(\"Best Ridge alpha:\", alpha)\n\n            alphas_lasso = [1e-6, 1e-5, 0.00005, 0.0001, 0.0005, 0.001, 0.01, 0.03, 0.06, 0.09, \n                            0.1, 0.15] \n            # [1, 0.1, 0.001, 0.0005]\n            lasso = LassoCV(alphas=alphas_lasso)\n            lasso.fit(x_train, y_train)\n            alpha = lasso.alpha_\n            print(\"Best Lasso alpha:\", alpha)\n\n            cv_ridge = [house_prices.rmse_cv(Ridge(alpha=alpha), x_train, y_train).mean() \n                        for alpha in alphas]\n            cv_ridge = pd.Series(np.expm1(cv_ridge), index=alphas)\n            cv_ridge = pd.Series(cv_ridge, index=alphas)\n            plt.figure()\n            cv_ridge.plot(title = \"Ridge, Validation\")\n            plt.xlabel('alpha')\n            plt.ylabel('rmse')\n            plt.show()\n            print(\"\\nRidge optimal regularization parameter alpha has rmse = \")\n            print(cv_ridge.min())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f8e95e05-b283-3928-2284-7a0d8dcab6e3"},"outputs":[],"source":"            # cv_lasso = [house_prices.rmse_cv(LassoCV(alphas=[alpha]), x_train, y_train).mean() \n            # for alpha in alphas_lasso]\n            cv_lasso = [house_prices.rmse_cv(Lasso(alpha=alpha), x_train, y_train).mean() \n                        for alpha in alphas_lasso]\n            # cv_lasso = pd.Series(np.expm1(cv_lasso), index=alphas_lasso)\n            cv_lasso = pd.Series(cv_lasso, index=alphas_lasso)\n            plt.figure()\n            cv_lasso.plot(title=\"Lasso, Validation\")\n            plt.xlabel('alpha')\n            plt.ylabel('rmse')\n            plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8fe8e50-997b-be3a-4314-9917dd05fa73"},"outputs":[],"source":"            print(\"\\nLasso optimal regularization parameter alpha has rmse = \")\n            print(cv_lasso.min())\n\n            print(\"\\nMean lasso rmse:\")\n            model_lasso = LassoCV(alphas=alphas_lasso).fit(x_train, y_train)\n            print(house_prices.rmse_cv(model_lasso, x_train, y_train).mean())\n            print(\"\\nbest lasso alpha:\", model_lasso.alpha_)"},{"cell_type":"markdown","metadata":{"_cell_guid":"40824f93-2c14-0f21-c85a-2c33e7f72dba"},"source":"We want to make sure that a potential machine learning estimator will form a model\nbased on some features that are highly correlated with sale price."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc653d40-307f-335c-b7cc-2ff0b50b6547"},"outputs":[],"source":"            if is_with_agglomeration:\n                coefficient_lasso = pd.Series(model_lasso.coef_, \n                                              index=df_merged_train_and_test_agglom\n                                              .columns).sort_values()\n            else:\n                coefficient_lasso = pd.Series(model_lasso.coef_, \n                                              index=df_test_num_features).sort_values()\n            \n            importance_coeff = pd.concat([coefficient_lasso.head(10), coefficient_lasso.tail(10)])\n            plt.figure()\n            importance_coeff.plot(kind='barh')\n            plt.title('Coefficients Lasso')\n            plt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1dcbf9e9-2860-c421-ec18-378902e7de6b"},"source":"Make Predictions using Machine Learning Estimators \n----------\nLet us finally make predictions on sale price using our machine learning estimators XGBoost, LassoCV and RidgeCV.\nTo see how well our model does, we have created a test using cross validation, where we\nsplit our training data set in two parts to compare predicted vs. actual sale price.\nThe points should be close to the line and we want lowest possible rmse (root mean square error).\nFurthermore, we also examine if feature selection (feature variance threshold) improves results with our regularized linear models."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b8ce9452-4a97-40b6-17cb-78bab719f4b0"},"outputs":[],"source":"    is_make_a_prediction = 1\n    if is_make_a_prediction:\n        ''' XGBoost and Regularized Linear Models and Random Forest '''\n        print(\"\\nPrediction Stats:\")\n        x_train = train_data[0::, :-1]\n        y_train = train_data[0::, -1]\n\n        # Regularized linear regression is needed to avoid overfitting even if you \n        # have lots of features\n        lasso = LassoCV(alphas=[0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, \n                                0.1, 0.3, 0.6, 1],\n                        max_iter=50000, cv=10)\n\n        # Exclude outliers\n        x_train, y_train = house_prices.outlier_identification(lasso, x_train, y_train)\n        # plt.show()\n        # Feature selection with Lasso\n        # Make comparison plot using only the train data.\n        # Predicted vs. Actual Sale price\n        title_name = 'LassoCV'\n        house_prices.predicted_vs_actual_sale_price_input_model(lasso, x_train, y_train, \n                                                                title_name)\n        plt.show()\n        # plt.close()\n        lasso.fit(x_train, y_train)\n        alpha = lasso.alpha_\n        print('best LassoCV alpha:', alpha)\n        score = lasso.score(x_train, y_train)\n        output_lasso = lasso.predict(test_data)\n        print('\\nSCORE Lasso linear model:---------------------------------------------------')\n        print(score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ec25ac12-33d3-a9d9-b2c9-b43271f6172d"},"outputs":[],"source":"        is_ridge_estimator = 1\n        if is_ridge_estimator:\n            ridge = RidgeCV(alphas=[0.06, 0.1, 0.3, 0.6, 1, 10, 100, 110], cv=10)\n            title_name = 'RidgeCV'\n            house_prices.predicted_vs_actual_sale_price_input_model(ridge, x_train, y_train, \n                                                                    title_name)\n            plt.show()\n            ridge.fit(x_train, y_train)\n            alpha = ridge.alpha_\n            print('best RidgeCV alpha:', alpha)\n            score = ridge.score(x_train, y_train)\n            output_ridge = ridge.predict(test_data)\n            print('\\nSCORE Ridge linear model:--------------------------------------------------')\n            print(score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e4220e30-149c-3e81-c8a1-1d77e7098424"},"outputs":[],"source":"            # Make comparison plot using only the train data.\n            # Predicted vs. Actual Sale price\n            add_name_of_regressor = 'RidgeCV'\n            forest_feature_selection = ridge\n\n            # Select most important features\n            feature_selection_model = SelectFromModel(forest_feature_selection, prefit=True)\n            x_train_new = feature_selection_model.transform(x_train)\n            print(x_train_new.shape)\n            test_data_new = feature_selection_model.transform(test_data)\n            print(test_data_new.shape)\n            # We get that 21 features are selected\n\n            title_name = ''.join([add_name_of_regressor, ' Feature Selection'])\n            house_prices.predicted_vs_actual_sale_price_input_model(forest_feature_selection, \n                                                                    x_train_new, y_train, \n                                                                    title_name)\n            plt.show()\n            forest_feature_selected = forest_feature_selection.fit(x_train_new, y_train)\n            score = forest_feature_selected.score(x_train_new, y_train)\n            output_feature_selection_ridge = forest_feature_selection.predict(test_data_new)\n            print('\\nSCORE {0} regressor (feature select):--------------------------------------'\n                  .format(add_name_of_regressor))\n            print(score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4304c8b5-4485-c58f-9c76-cfda393d43f9"},"outputs":[],"source":"        is_grid_search_RF_prediction = 0\n        if is_grid_search_RF_prediction:\n            # Fit the training data to the survived labels and create the decision trees\n\n            # Create the random forest object which will include all the parameters for the fit\n            forest = RandomForestRegressor()\n            # forest = SGDRegressor()\n            parameter_grid = {'max_depth': [4,5,6,7,8], 'n_estimators': [200,210,240,250]}  \n            # ,'criterion': ['gini', 'entropy']}\n            cross_validation = StratifiedKFold(random_state=None, shuffle=False)  # , n_folds=10)\n            grid_search = GridSearchCV(forest, param_grid=parameter_grid, cv=cross_validation, \n                                       n_jobs=24)\n            title_name = 'Random Forest with GridSearchCV'\n            house_prices.predicted_vs_actual_sale_price_input_model(grid_search, x_train, y_train, \n                                                                    title_name)\n            plt.show()\n            grid_search.fit(x_train, y_train)\n            # output = grid_search.predict(test_data)\n\n            print('Best score: {}'.format(grid_search.best_score_))\n            print('Best parameters: {}'.format(grid_search.best_params_))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5670fc30-9f36-f829-ac34-0d139276403c"},"outputs":[],"source":"        is_feature_selection_prediction = 1\n        if is_feature_selection_prediction:\n\n            is_feature_selection_with_lasso = 1\n            if is_feature_selection_with_lasso:\n                forest_feature_selection = lasso\n                add_name_of_regressor = 'Lasso'\n            else:\n                add_name_of_regressor = 'Random Forest'\n                # Random forest (rf) regressor for feature selection\n                forest_feature_selection = RandomForestRegressor(n_estimators=240, max_depth=8)\n                forest_feature_selection = forest_feature_selection.fit(x_train, y_train)\n\n                # Evaluate variable importance with no cross validation\n                importances = forest_feature_selection.feature_importances_\n                std = np.std([tree.feature_importances_ for tree \n                              in forest_feature_selection.estimators_], axis=0)\n                indices = np.argsort(importances)[::-1]\n\n                print('\\nFeatures:')\n                df_test_num_features = house_prices.extract_numerical_features(df_test)\n                print(np.reshape(\n                    np.append(np.array(list(df_test_num_features)), \n                              np.arange(0, len(list(df_test_num_features)))), \n                    (len(list(df_test_num_features)), 2), 'F'))  # , 2, len(list(df_test)))\n\n                print('\\nFeature ranking:')\n                for f in range(x_train.shape[1]):\n                    print('%d. feature %d (%f)' % (f + 1, indices[f], importances[indices[f]]))\n\n            # Select most important features\n            feature_selection_model = SelectFromModel(forest_feature_selection, prefit=True)\n            x_train_new = feature_selection_model.transform(x_train)\n            print(x_train_new.shape)\n            test_data_new = feature_selection_model.transform(test_data)\n            print(test_data_new.shape)\n            # We get that 21 features are selected\n\n            title_name = ''.join([add_name_of_regressor, ' Feature Selection'])\n            house_prices.predicted_vs_actual_sale_price_input_model(forest_feature_selection, \n                                                                    x_train_new, y_train, \n                                                                    title_name)\n            plt.show()\n            forest_feature_selected = forest_feature_selection.fit(x_train_new, y_train)\n            score = forest_feature_selected.score(x_train_new, y_train)\n            output_feature_selection_lasso = forest_feature_selection.predict(test_data_new)\n            print('\\nSCORE {0} regressor (feature select):-------------------------------------'\n                  .format(add_name_of_regressor))\n            print(score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ba0a43b5-5b58-4429-1216-1b64dffc9b7c"},"outputs":[],"source":"        ''' xgboost '''\n        is_xgb_cv = 1\n        if is_xgb_cv:\n            seed = 0\n            dtrain = xgb.DMatrix(x_train, label=y_train)\n            dtest = xgb.DMatrix(test_data)\n\n            xgb_params = {\n                'seed': 0,\n                'colsample_bytree': 0.8,\n                'silent': 1,\n                'subsample': 0.6,\n                'learning_rate': 0.01,\n                # 'booster': 'gblinear',  # default is gbtree\n                'objective': 'reg:linear',\n                'max_depth': 1,\n                'num_parallel_tree': 1,\n                'min_child_weight': 1,\n                'eval_metric': 'rmse',\n            }\n\n            res = xgb.cv(xgb_params, dtrain, num_boost_round=500, nfold=10, seed=seed, \n                         stratified=False, early_stopping_rounds=25, verbose_eval=10, \n                         show_stdv=True)\n\n            best_nrounds = res.shape[0] - 1\n            cv_mean = res.iloc[-1, 0]\n            cv_std = res.iloc[-1, 1]\n\n            print('Ensemble-CV: {0}+{1}'.format(cv_mean, cv_std))\n            title_name = 'xgb.cv'\n            # house_prices.predicted_vs_actual_sale_price_xgb(xgb, best_nrounds, xgb_params, \n            # x_train, y_train, title_name)\n            gbdt = xgb.train(xgb_params, dtrain, best_nrounds)\n            output_xgb_cv = gbdt.predict(dtest)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0256131f-9c5e-eab8-4aa3-afc4b200f54c"},"outputs":[],"source":"        use_xgb_regressor = 0\n        if use_xgb_regressor:\n            # Is a parallel job\n            xgb_model = xgb.XGBRegressor()\n            # xgb_model = xgb.XGBRegressor(n_estimators = 360, max_depth = 2, learning_rate = 0.1)\n            # XGBClassifier gives the best prediction\n            # xgb_model = xgb.XGBClassifier()\n            cross_validation = StratifiedKFold(n_splits=10, shuffle=False, random_state=None)  \n            # , n_folds=10)\n            parameter_grid = {'max_depth': [4, 5, 6, 7, 8], 'n_estimators': [200, 210, 240, 250]}\n            # parameter_grid = {'max_depth': [2, 4, 6], 'n_estimators': [50, 100, 200]}  \n            # , 'criterion': ['gini'\n            # , 'entropy']}\n            clf = GridSearchCV(xgb_model, param_grid=parameter_grid, cv=cross_validation)  \n            # verbose=1)\n            title_name = 'xgbRegressor'\n            house_prices.predicted_vs_actual_sale_price_input_model(clf, x_train, y_train, \n                                                                    title_name)\n            clf.fit(x_train, y_train)\n            output_xgb_regressor = clf.predict(test_data)\n            print('\\nSCORE XGBRegressor train data:-------------------------------------------')\n            print(clf.best_score_)\n            print(clf.best_params_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8834233d-dd42-fd83-1c39-6a81ef3303de"},"outputs":[],"source":"        # house_prices.timestamp = datetime.datetime.now().strftime('%Y%m%d_%Hh%Mm%Ss')\n        # save_path = '/home/user/predicted_vs_actual/'\n        # house_prices.multipage(''.join([save_path, 'Overview_estimators_rmse_', \n        #                                house_prices.timestamp, '.pdf']))\n        # plt.close()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3f852c0b-24d1-8e52-004e-e59c080ac079"},"outputs":[],"source":"        # Averaging the output using four different machine learning estimators\n        # output = (output_feature_selection_lasso + output_feature_selection_ridge + output_xgb_cv\n        # + output_xgb_regressor)/4.0\n        # output = (output_feature_selection_lasso + output_ridge + output_xgb_regressor) / 3.0\n        # output = (output_feature_selection_lasso + output_ridge) / 2.0\n        output = (output_feature_selection_lasso + output_xgb_cv) / 2.0\n        # print np.shape(output_ridge) == np.shape(output_lasso)"},{"cell_type":"markdown","metadata":{"_cell_guid":"5d91e2a0-b3bf-3972-5061-f1a12fe9f0ac"},"source":"Submission \n----------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6b782829-8d73-19b0-f55e-dc41c69a6d60"},"outputs":[],"source":"    if is_make_a_prediction:\n        ''' Submission '''\n        # Submission requires a csv file with Id and SalePrice columns.\n        # dfBestScore = pd.read_csv(''.join([save_path, 'submission_house_prices.csv']), header=0)\n\n        # We do not expect all to be equal since the learned model differs from time to time.\n        # print (dfBestScore.values[0::, 1::].ravel() == output.astype(int))\n        # print np.array_equal(dfBestScore.values[0::, 1::].ravel(), output.astype(int))  \n        # But they are almost never all equal\n\n        # Exp() is needed in order to get the correct sale price, since we took a log() earlier\n        # if not is_simple_model:\n        if house_prices.is_with_log1p_SalePrice:\n            output = np.expm1(output)\n            \n        submission = pd.DataFrame({'Id': Id_df_test, 'SalePrice': output})\n        submission.to_csv(''.join(['submission_house_prices_', house_prices.timestamp, \n                                   '.csv']), index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c713176e-95a0-f22f-6aba-cd8351bb7845"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}