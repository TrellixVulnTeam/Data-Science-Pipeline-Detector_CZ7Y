{"cells":[{"metadata":{"trusted":true,"_uuid":"8ed06ae09c24237acd657d32d8172adca062f453"},"cell_type":"code","source":"%%HTML\n<h1>Majdoubi's Guide to a beginner's first Kaggle Project (top 20%)</h1>\n<h3>Made by : Ahmed Amine MAJDOUBI</h3>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d4a696348ebc7adae125386edd1ed5388fa0298"},"cell_type":"markdown","source":"In this notebook, we will go through the basic steps of making predictions based on a given dataset. The steps that I followed in this notebook are as follows :\n- Importing Libraries and Datasets\n- Data Description\n- Finding Correlation Features\n- Removing Outliers\n- Imputation of missing values\n- Correcting Features\n- Adding Features\n- Skewness and Kurtosis\n- Label Encoding\n- Transformation and Scaling\n- Feature Selection\n- Principal Component Analysis\n- Testing Different Models\n- Hyper-parameter Tuning\n- Combining Models\n- Making Predictions"},{"metadata":{"trusted":true,"_uuid":"a82eea53f3fd64eda0f064f3dc36951a7f3ae1f9"},"cell_type":"code","source":"%%HTML\n<h1>Importing Libraries and Datasets</h1>","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ff9096edbc92c0ffca257ae73928951bc544de7"},"cell_type":"code","source":"#importing librairies\n\nimport pandas as pd # data processing\nimport numpy as np # numeric computation\nimport matplotlib.pyplot as plt # plot visualization\n%matplotlib inline\nimport seaborn as sns # data visualisation\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport scipy.stats as st # statistics\npd.options.display.max_columns = None # show all columns\nimport missingno as msno # missing data visualizations and utilities\nimport warnings # ignore file warnings\nwarnings.filterwarnings('ignore')\n\n# Importing the train and test datasets in  pandas dataframe\n\ntrainData = pd.read_csv('../input/train.csv')\ntestData = pd.read_csv('../input/test.csv')\ntrainData.drop(columns = 'Id', inplace =True)\ny_train = trainData['SalePrice']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed0632bede7e3a01dfe757b610599de83014fc34"},"cell_type":"code","source":"%%HTML\n<h1>Data Description</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"554a6508466497e75a2ee6658765f9e6cbae00b9"},"cell_type":"markdown","source":"First let's start by looking at the data and getting a general idea about it"},{"metadata":{"trusted":true,"_uuid":"8ea22d699eab4bcc9b7d181a2bf0164e61a512a0"},"cell_type":"code","source":"# shape of data\ntrainData.shape , testData.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c6d6b2cda812e570cf7270ec0f5467979153880"},"cell_type":"code","source":"# A basic description of the dataset\ntrainData.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5c7a2a4717cece181f481499818398202120ec6"},"cell_type":"code","source":"# display the first five rows of the train dataset.\ntrainData.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"839b7417f336eb4592cbd2277d68fab87d6b6381"},"cell_type":"code","source":"# display the last five rows of the train dataset.\ntrainData.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0034b04d8dd1f4e2ed5bf432cf0bf5543cc6820b"},"cell_type":"code","source":"# Numeric and categorical features in the dataset\ntrainData.select_dtypes(include=[np.number]).columns, trainData.select_dtypes(include=[np.object]).columns","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"5fc916932d2b9491a2358e3f06814b49cd94a50f"},"cell_type":"code","source":"%%HTML\n<h1>Correlation Features</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71baca20f1790c69a425f5caead51a407007650d"},"cell_type":"markdown","source":"Let's look at some correlation features between features."},{"metadata":{"trusted":true,"_uuid":"827d41628563f4c77743e0862b55d1e82d0184af"},"cell_type":"code","source":"# Showing the numerical varibales with the highest correlation with 'SalePrice', sorted from highest to lowest\ncorrelation = trainData.select_dtypes(include=[np.number]).corr()\nprint(correlation['SalePrice'].sort_values(ascending = False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d83ff8e4cad9e61520c477426270412998f67d30"},"cell_type":"code","source":"# Heatmap of correlation of numeric features\nf , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of numeric features',size=15)\nsns.heatmap(correlation,square = True,  vmax=0.8)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"733e6f6ebfe75a90046bcc219cc3c53c17398754"},"cell_type":"markdown","source":"- We observe two white squares (2,2 and 3,3) in the heatmap indicating high correlation. The first group of highly correlated variables is 'TotalBsmtSF' and '1stFlrSF'. The second group is 'GarageYrBlt', 'GarageCars' and 'GarageArea'. This indicates the presence of multicollinearity.\n- The other four white squares (1,1) just indicate an obvious correlation between 'GarageYrBlt' and 'YearBuilt' and between 'TotRmsAbvGrd' and 'GrLivArea'\n- We also observe  from the heatmap and the previous correlation list that'GrLivArea', 'TotalBsmtSF', 'OverallQual', 'FullBath', 'TotRmsAbvGrd' and 'YearBuilt' are highly correlated with 'SalePrice'"},{"metadata":{"trusted":true,"_uuid":"d0d86a4699cdb709983c08abc41fd87b62924016"},"cell_type":"code","source":"# Zoomed HeatMap of the most Correlayed variables\nzoomedCorrelation = correlation.loc[['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea'], ['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea']]\nf , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of numeric features',size=15)\nsns.heatmap(zoomedCorrelation, square = True, linewidths=0.01, vmax=0.8, annot=True,cmap='viridis',\n            linecolor=\"black\", annot_kws = {'size':12})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"662c66a3ab59f7b186073c174b7c740633017b52"},"cell_type":"markdown","source":"We conclude that :\n- 'TotalBsmtSF' and '1stFlrSF' are strongly correlated\n- 'TotRmsAbvGrd' and 'GrLivArea' are strongly correlated\n- 'GarageCars' and 'GarageArea' are strongly correlated\n- 'GarageYrBlt' and 'YearBuilt' are strongly correlated\n- 'TotRmsAbvGrd' and 'GrLivArea' are strongly correlated\n- 'OverallQual', 'GrLivArea' and 'TotRmsAbvGrd' are strongly correlated with 'SalePrice'"},{"metadata":{"trusted":true,"_uuid":"5ef1ee7de4088f1c747d275fb5a33ac5921ed49f"},"cell_type":"code","source":"# Pair plot\nsns.set()\ncols = ['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt','1stFlrSF','GarageYrBlt','GarageCars','GarageArea']\nsns.pairplot(trainData[cols],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8098d9919d828cdf84a656f9fb3af1802786ad23"},"cell_type":"markdown","source":"- We observe that 'SalePrice' increases almost quadratically with 'TotalBsmtSF', 'GrLivArea', '1stFlrSF'. So we conclude that the price of the houses increases quadratically with its surface area. We also observe that 'SalePrice' increases exponentially with 'OverallQual'.\n- We also observe from 'GrLivArea'-'1stFlSF' and '1stFlSF'-'TotalBsmSF' that all the points are above the identity fucntion line, which means that the ground living area has the biggest surface of all floors, and that the first floor area is generally bigger than the basement area.\n- We observe the same phenomenon for 'GarageYrBlt'-'YearBuilt'. which makes sense since we start building the garage after building the house, altough there are some exceptions in the data."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"88e50cafb9faf077b80568b95858af34ce8e60d9"},"cell_type":"code","source":"%%HTML\n<h1>Removing Outliers</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee5814221e5558f50204521048704955efc038d9"},"cell_type":"markdown","source":"From the previous pair plots, we can see that there are outliers for 'TotalBsmtSF', '1stFlrSF' and 'GrLivArea'. Let's use the scatterplot to observe these outliers more precisely"},{"metadata":{"trusted":true,"_uuid":"65930a0a46dfd13edd48abcf57a51ed7bdae32f3"},"cell_type":"code","source":"plt.figure(figsize=(7,5))\nplt.scatter(x = trainData.TotalBsmtSF,y = trainData.SalePrice)\nplt.title('TotalBsmtSF', size = 15)\nplt.figure(figsize=(7,5))\nplt.scatter(x = trainData['1stFlrSF'],y = trainData.SalePrice)\nplt.title('1stFlrSF', size = 15)\nplt.figure(figsize=(7,5))\nplt.scatter(x = trainData.GrLivArea,y = trainData.SalePrice)\nplt.title('GrLivArea', size = 15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f2cf7d0f80198739c32fcb52188400b6e9c2ea5"},"cell_type":"code","source":"# Removing the outliers\ntrainData.drop(trainData[trainData['TotalBsmtSF'] > 5000].index,inplace = True)\ntrainData.drop(trainData[trainData['1stFlrSF'] > 4000].index,inplace = True)\ntrainData.drop(trainData[(trainData['GrLivArea'] > 4000) & (trainData['SalePrice']<300000)].index,inplace = True)\ntrainData.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b07ef0cb20fb2bde69af7902614292542734a873"},"cell_type":"markdown","source":"Since only two outliers were dropped, it means that the three features shared the same outlier "},{"metadata":{"trusted":true,"_uuid":"09080f5b3fa471a124c35dba4409bd1404ab181a"},"cell_type":"code","source":"%%HTML\n<h1>Imputation of missing values</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21775dc34f04cdcdf20e100631cd96c6cc70e4bd"},"cell_type":"markdown","source":"Let's look at the missing valeus in our data. We will be using msno library. Msno provides a small toolset of flexible and easy-to-use missing data visualizations and utilities that allows you to get a quick visual summary of the completeness (or lack thereof) of your dataset"},{"metadata":{"trusted":true,"_uuid":"5bbdd27fbabb40f4e65ad414f73e5856dee76791"},"cell_type":"code","source":"# Visualising missing values of numeric features for sample of 200\nmsno.matrix(trainData.select_dtypes(include=[np.number]).sample(200))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b962cdac0c83c722d68076420720f97880a0811"},"cell_type":"code","source":"# Visualising percentage of missing values of the top 10 numeric variables\ntotal = trainData.select_dtypes(include=[np.number]).isnull().sum().sort_values(ascending=False)\npercent = (trainData.select_dtypes(include=[np.number]).isnull().sum()/trainData.select_dtypes(include=[np.number]).isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Missing Count', 'Missing Percentage'])\nmissing_data.index.name =' Numeric Feature'\nmissing_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e68257bbee51ef367869a62305fda0fb75fe949"},"cell_type":"markdown","source":"We observe that 'LotFrontage', 'GarageYrBlt' and 'MasVnrArea' are the only one who have missing values"},{"metadata":{"trusted":true,"_uuid":"4e4c57e9f1e220bb204d14deeaae7264740dfc02"},"cell_type":"code","source":"# Visualising missing values of categorical features for sample of 200\nmsno.matrix(trainData.select_dtypes(include=[np.object]).sample(200))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"729e87134c4f6884881eca0dfd2632a7fe50b542"},"cell_type":"code","source":"# Visualising percentage of missing values of the top 10 categorical variables\ntotal = trainData.select_dtypes(include=[np.object]).isnull().sum().sort_values(ascending=False)\npercent = (trainData.select_dtypes(include=[np.object]).isnull().sum()/trainData.select_dtypes(include=[np.object]).isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Missing Count', 'Missing Percentage'])\nmissing_data.index.name =' Numeric Feature'\nmissing_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e73c182fabf41d2c80ccc6ea1bdf8bc073fc869"},"cell_type":"markdown","source":"We observe that 'PoolQC', 'MiscFeature', 'Alley', 'Fence' and 'FireplaceQu' have a significant amount of missing values (at least more than half of observation)"},{"metadata":{"trusted":true,"_uuid":"1e1b61c4094acf40c399f1e9422ecd255ef0333c"},"cell_type":"code","source":"# Visualization of nullity by column\nmsno.bar(trainData.sample(1000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7bf3cabc506c99a6c3dfa8a62a9d673a78335eae"},"cell_type":"code","source":"# Nullity correlation heatmap : how strongly the presence or absence of one variable affects the presence of another\nmsno.heatmap(trainData)\n\n# -1 : if one variable appears the other definitely does not\n# 0 : variables appearing or not appearing have no effect on one another \n# 1 : if one variable appears the other definitely also does","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0003a9c144b48e5947564ade8e645b4e0aa6338c"},"cell_type":"code","source":"# Dendogram for variable completion, reveals trends deeper than the pairwise ones visible in the correlation heatmap\nmsno.dendrogram(trainData)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d66d4b26e71e56b513c61f431e665eb97cd2d6e8"},"cell_type":"markdown","source":"Cluster leaves which linked together at a distance of zero fully predict one another's presence : one variable might always be empty when another is filled, or they might always both be filled or both empty."},{"metadata":{"_uuid":"258bedf0317ae25d033c39c74955fc992be32476"},"cell_type":"markdown","source":"First of all, let's start by replacing the missing values in both the training and the test set. So we will be combining both datasets into one dataset"},{"metadata":{"trusted":true,"_uuid":"b8b70432addea1eedf18a4f53a56cdfa880b8314"},"cell_type":"code","source":"# Concatenate the training and test datasets into a single datafram\ndataFull = pd.concat([trainData,testData],ignore_index=True)\ndataFull.drop('Id',axis = 1,inplace = True)\ndataFull.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed11477dc4b68adf69e46ae247d17b95abcfe3f7"},"cell_type":"code","source":"# Sum of missing values by feature\nsumMissingValues = dataFull.isnull().sum()\nsumMissingValues[sumMissingValues>0].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8879dc08f43263c8a541b9b4fad8745353525257"},"cell_type":"code","source":"# Numeric features : replace with 0\nfor col in ['BsmtFullBath','BsmtHalfBath','BsmtUnfSF','TotalBsmtSF','GarageCars','BsmtFinSF2','BsmtFinSF1','GarageArea']:\n    dataFull[col].fillna(0,inplace= True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22a5dadab1dc80097fb28f13d9c668cdce89f32b"},"cell_type":"markdown","source":"Let's start by imputing features with less than five missing values"},{"metadata":{"trusted":true,"_uuid":"a1dc53d6e043d1c5421ae997589559c40694491b"},"cell_type":"code","source":"# Categorical features : replace with the mode (most frequently occured value)\nfor col in ['MSZoning','Functional','Utilities','KitchenQual','SaleType','Exterior2nd','Exterior1st','Electrical']:\n    dataFull[col].fillna(dataFull[col].mode()[0],inplace= True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49959c4f2b541d23f6b1f80638f1d88811afc523"},"cell_type":"code","source":"# Impute features with more than five missing values\n\n# Categorical features : replace all with 'None'\nfor col in ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageQual','GarageCond','GarageFinish','GarageType','BsmtExposure','BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1','MasVnrType']:\n    dataFull[col].fillna('None',inplace = True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"432c8b82e92c1a69cbd60b2a6b794d9e59fcd00c"},"cell_type":"markdown","source":"Since 'MasVnrArea' only have 23 missing values, we can replace them with the mean of the column"},{"metadata":{"trusted":true,"_uuid":"42df6bc540f462d5a953b1271ec4a1bb8ee54a87"},"cell_type":"code","source":"dataFull['MasVnrArea'].fillna(dataFull['MasVnrArea'].mean(), inplace=True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c8bce62652c1da6aa6162af793a27c30c9d29de"},"cell_type":"markdown","source":"Based on the previous correlation heatmap, 'GarageYrBlt' is highly correlated with 'YearBuilt', so let's replace the missing values by medians of 'YearBuilt'. To do that, we need to cut 'YearBuilt' into sections since it is a numeric variable"},{"metadata":{"trusted":true,"_uuid":"80ed1605561a318eaa11710470a98a25f495f72b"},"cell_type":"code","source":"# Cut 'YearBuilt' into 10 parts\ndataFull['YearBuiltCut'] = pd.qcut(dataFull.YearBuilt,10)\n# Impute the missing values of 'GarageYrBlt' based on the median of 'YearBuilt' \ndataFull['GarageYrBlt']= dataFull.groupby(['YearBuiltCut'])['GarageYrBlt'].transform(lambda x : x.fillna(x.median()))\n# convert the values to integers\ndataFull['GarageYrBlt'] = dataFull['GarageYrBlt'].astype(int)\n# Drop 'YearBuiltCut' column\ndataFull.drop('YearBuiltCut',axis=1,inplace=True)\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a458b6a14f7d5ad5efb4f05a88a50a1ad6bc3a6"},"cell_type":"markdown","source":"Based on the previous correlation heatmap, 'LotFrontage' is highly correlated with 'LotArea' and 'LotFrontage'. So let's use the same method to fill the missing values"},{"metadata":{"trusted":true,"_uuid":"7b410ffb9b83df27ba91cd48aa95bc31c9cd670e"},"cell_type":"code","source":"# Cut 'LotArea' into 10 parts\ndataFull['LotAreaCut'] = pd.qcut(dataFull.LotArea,10)\n# Impute the missing values of 'LotFrontage' based on the median of 'LotArea' and 'Neighborhood'\ndataFull['LotFrontage']= dataFull.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\ndataFull['LotFrontage']= dataFull.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\n# Drop 'LotAreaCut' column\ndataFull.drop('LotAreaCut',axis=1,inplace=True)\n\n# Check if missing values are imputed successfully\ndataFull.isnull().sum()[dataFull.isnull().sum()>0].sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa810252be324846d80c9176a3b8cc46ff089186"},"cell_type":"markdown","source":"The only missing values that are left are within SalePrice, which is exactly the number of lignes in the test data (the values that we need to predict)"},{"metadata":{"trusted":true,"_uuid":"209cf107cc71198f1a6eebb2ce31df38317b3c6b"},"cell_type":"code","source":"%%HTML\n<h1>Correcting Features</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b401eed6bdfab152ef798a8e3c7a9985ca13f91"},"cell_type":"markdown","source":"If we take a look at the numeric variables, we see that some of them obviously don't make a sense being numerical like year related features. Let's take a closer look at each one of them in the data description file and see which ones need to be converted to categorical type."},{"metadata":{"trusted":true,"_uuid":"fb6ff2bc808748a0e6139e0534beacddfb035b15"},"cell_type":"code","source":"dataFull.select_dtypes(include=[np.number]).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a92b0609c827750d6c5a9b234996a82773146c58"},"cell_type":"code","source":"# Converting numeric features to categorical features\nstrCols = ['YrSold','YearRemodAdd','YearBuilt','MoSold','MSSubClass','GarageYrBlt']\nfor i in strCols:\n    dataFull[i]=dataFull[i].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"116341d804f2fdbf53197261c1df252595829599"},"cell_type":"code","source":"%%HTML\n<h1>Adding Features</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d96f82c952e3da9f69fbcd9ce6714b10ae7751c"},"cell_type":"markdown","source":"First, we will map some categorical variable that represent some sort of rating to an integer score"},{"metadata":{"trusted":true,"_uuid":"2bcf072c4469f089d17f8ec2fb29ea1ad13923ae"},"cell_type":"code","source":"dataFull.select_dtypes(include=[np.object]).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de518dcf63db554aa5bc645b4df6123918f0ed9a"},"cell_type":"code","source":"dataFull[\"oExterQual\"] = dataFull.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\ndataFull[\"oBsmtQual\"] = dataFull.BsmtQual.map({'None':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndataFull[\"oBsmtExposure\"] = dataFull.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\ndataFull[\"oHeatingQC\"] = dataFull.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndataFull[\"oKitchenQual\"] = dataFull.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\ndataFull[\"oFireplaceQu\"] = dataFull.FireplaceQu.map({'None':1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\ndataFull[\"oGarageFinish\"] = dataFull.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\ndataFull[\"oPavedDrive\"] = dataFull.PavedDrive.map({'N':1, 'P':2, 'Y':3})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"641998bc7f20ff6d6d56ae7d88c232be45de4440"},"cell_type":"markdown","source":"Next, we will add up some numeric features with each other to create new features that make sense"},{"metadata":{"trusted":true,"_uuid":"7cc5a42e0dd33fbe3de7a422e63a0647dd5fbf51"},"cell_type":"code","source":"dataFull.select_dtypes(include=[np.number]).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce90331007d43c7c910fde509a01b114f65f9b34"},"cell_type":"code","source":"dataFull['HouseSF'] = dataFull['1stFlrSF'] + dataFull['2ndFlrSF'] + dataFull['TotalBsmtSF']\ndataFull['PorchSF'] = dataFull['3SsnPorch'] + dataFull['EnclosedPorch'] + dataFull['OpenPorchSF'] + dataFull['ScreenPorch']\ndataFull['TotalSF'] = dataFull['HouseSF'] + dataFull['PorchSF'] + dataFull['GarageArea']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"aa177559dba12cc10a373cf1bd7ad3b96b8ac737"},"cell_type":"code","source":"%%HTML\n<h1>Skewness and Kurtosis</h1>","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"833fc77ce186c9a88ecb6c9f89857c7f950a2919"},"cell_type":"code","source":"# Estimate Skewness and Kurtosis of the data\ntrainData.skew(), trainData.kurt()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a239c77acd942652777773e39c9ae53783c65ae8"},"cell_type":"code","source":"# Plot the Skewness of the data\nsns.distplot(trainData.skew(),axlabel ='Skewness')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f3717c5937a44da83805dd054c1e9beea929acf"},"cell_type":"code","source":"# Plot the Kurtosis of the data\nsns.distplot(trainData.kurt(),axlabel ='Kurtosis')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c68e3cfc8990f3f8108ac43f205b9fe4bc4ce8d"},"cell_type":"markdown","source":"There isn't much kurtosis in the data columns, but Skewness is very present, meaning that distribution is not symetrical"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"cccd5954fd954e8769d89a6a8fc83be8bab4d48f"},"cell_type":"code","source":"%%HTML\n<h1>Label Encoding</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"171ea855e52fd4637aa5fede4400f1e75568b012"},"cell_type":"markdown","source":"For this section we will use Pipelines which are a way to streamline a lot of the routine processes. It provides a way to take code, fit it to the training data, apply it to the test data without having to copy and paste everything.\nWe will create three classes : first for label encoding, second for skewness, and third for one hot label encoding."},{"metadata":{"trusted":true,"_uuid":"0035df3ce0837c4483f3ae938b3a7bc4392d9c62"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import LabelEncoder, Imputer\nfrom scipy.stats import skew\n\n# Label encoding class\nclass labenc(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        label = LabelEncoder()\n        X['YrSold']=label.fit_transform(X['YrSold'])\n        X['YearRemodAdd']=label.fit_transform(X['YearRemodAdd'])\n        X['YearBuilt']=label.fit_transform(X['YearBuilt'])\n        X['MoSold']=label.fit_transform(X['MoSold'])\n        X['GarageYrBlt']=label.fit_transform(X['GarageYrBlt'])\n        return X\n    \n# Skewness transform class\nclass skewness(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        skewness = X.select_dtypes(include=[np.number]).apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= 1].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        return X\n\n# One hot encoding class\nclass onehotenc(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,X,y=None):\n        return self\n    def transform(self,X):\n        X = pd.get_dummies(X)\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dc7638a3fa042e1f84a5aa59773c25ebd487794"},"cell_type":"code","source":"# Creating a copy of the full dataset\ndataFullCopy = dataFull.copy()\n\n# Creating a new fata with aplied transformations using sklearn Pipeline\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline([('labenc',labenc()),('skewness',skewness()),('onehotenc',onehotenc())])\ndataPipeline = pipeline.fit_transform(dataFullCopy)\ndataFull.shape, dataPipeline.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e00a3493f46d3f2c47d369e2ee88eb0c5b5516c6"},"cell_type":"markdown","source":"We can see now that the number of features increases from 88 to 328"},{"metadata":{"trusted":true,"_uuid":"c0b2918d974c03c24ac246e4881bd5764ef98747"},"cell_type":"code","source":"dataPipeline.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a008941b3541648ddc2b29010302a247d817b569"},"cell_type":"markdown","source":"Now we split the data to training and testing datasets"},{"metadata":{"trusted":true,"_uuid":"a290d531b0212f3f23ddd4b18072f984ad989282"},"cell_type":"code","source":"\nX_train = dataPipeline[:trainData.shape[0]]\ny_train = X_train['SalePrice']\nX_train.drop(columns = 'SalePrice', inplace=True)\nX_test = dataPipeline[trainData.shape[0]:]\nX_test.drop(columns = 'SalePrice', inplace=True)\nX_train.shape, y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"ab3fe8b236322172a6eccd1fce1a9361186a2677"},"cell_type":"code","source":"%%HTML\n<h1>Transformation and Scaling</h1>","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1d84d6dd76571128298d744512566a91dd9fa29"},"cell_type":"code","source":"# SalesPrices plot with three different fitted distributions\nplt.figure(2); plt.title('Normal')\nsns.distplot(y_train, kde=False, fit=st.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y_train, kde=False, fit=st.lognorm)\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y_train, kde=False, fit=st.johnsonsu)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fd07fe2f228ef06a852c95c566a7122bc02e0aa"},"cell_type":"markdown","source":"Normal distribution doesn't fit, so SalePrice need to be transformed before creating the model. Best fit is unbounded Johnson distribution, altough log normal distribution also fits well"},{"metadata":{"trusted":true,"_uuid":"75cab4022a761e59260aa7fbd593e30d17f6645f"},"cell_type":"code","source":"# transforming 'SalePrice' into normal distribution\ny_train_transformed = np.log(y_train)\ny_train_transformed.skew(), y_train_transformed.kurt()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e705f5af6fd6e7746b410d47548355bdea0c4ba2"},"cell_type":"code","source":"# plotting 'SalePrice' before and after the transformation\nplt.figure(1); plt.title('Before transformation')\nsns.distplot(y_train)\nplt.figure(2); plt.title('After transformation')\nsns.distplot(y_train_transformed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"36daa9272d2c35b42ecfc4fd71c05913691027b5"},"cell_type":"code","source":"# Using Robust Scaler to transform X_train\nfrom sklearn.preprocessing import RobustScaler\nrobust_scaler = RobustScaler()\nX_train_scaled = robust_scaler.fit(X_train).transform(X_train)\nX_test_scaled = robust_scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6507ef8380f6ea9611f0a308a04af06df8916b3"},"cell_type":"code","source":"# Shape of final data we will be working on\nX_train_scaled.shape, y_train_transformed.shape, X_test_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6da76829fe0fb3e52a7b1b2b20c4bf7fa20def48"},"cell_type":"code","source":"%%HTML\n<h1>Feature Selection</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14c016f3b72983c454a97f91cacc6ee34e4e3448"},"cell_type":"markdown","source":"We will use lasso regression (l1 regularization method). Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. We can also use it to find the most important features in our dataset."},{"metadata":{"trusted":true,"_uuid":"9d3f3bafb013cf0a4e3547b0fb90578b27afb8ac"},"cell_type":"code","source":"# Display features by their importance (lasso regression coefficient)\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha = 0.001)\nlasso.fit(X_train_scaled,y_train_transformed)\ny_pred_lasso = lasso.predict(X_test_scaled)\nlassoCoeff = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=dataPipeline.drop(columns = 'SalePrice').columns)\nlassoCoeff.sort_values(\"Feature Importance\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82c31d3b13a9b0e6de1b50fb85730240db9fd807"},"cell_type":"code","source":"# Plot features by importance (feature coefficient in the model)\nlassoCoeff[lassoCoeff[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(20,35))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69138124ef97484291ab98c0f0e3a436d8a1d7a7"},"cell_type":"markdown","source":"What's intersting here is that two of the variables that we have created 'HouseSF' and 'PorchSF' perform actually bad compared to their components. But when we sum all the surfaces as in 'TotalSF', which is just a combination of features that are significantly unimportant in this model, we suddently obtain the most important feature in the dataset."},{"metadata":{"trusted":true,"_uuid":"93ef042e15963edab0d819601e0be5c600efd67f"},"cell_type":"code","source":"%%HTML\n<h1>Principal Component Analysis</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"033ab832fd50c52b708361f58ee5e994fd48f31f"},"cell_type":"markdown","source":"PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance"},{"metadata":{"trusted":true,"_uuid":"a5d499e1e37f32a931b9343acb9668f7fffdb80d"},"cell_type":"code","source":"from sklearn.decomposition import PCA\n# Concatenate the training and test datasets into a single datafram\ndataFull2 = np.concatenate([X_train_scaled,X_test_scaled])\n# Choose the number of principle components such that 95% of the variance is retained\npca = PCA(0.95)\ndataFull2 = pca.fit_transform(dataFull2)\nvarPCA = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n# Principal Component Analysis of data\nprint(varPCA)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ca59999f82f7aa440c49949879463d37d5a9cf3"},"cell_type":"code","source":"# Principal Component Analysis plot of the data\nplt.figure(figsize=(16,12))\nplt.bar(x=range(1,len(varPCA)+1), height = varPCA)\nplt.ylabel(\"Explained Variance (%)\", size = 15)\nplt.xlabel(\"Principle Components\", size = 15)\nplt.title(\"Principle Component Analysis Plot : Training Data\", size = 15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52e1fd9f4b9356718a53b193b5f8363138322d3e"},"cell_type":"code","source":"# Shape of final data we will be working on\nX_train_scaled = dataFull2[:trainData.shape[0]]\nX_test_scaled = dataFull2[trainData.shape[0]:]\nX_train_scaled.shape, y_train_transformed.shape, X_test_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"42a5e5909c34efa60417701fb7385685176844e5"},"cell_type":"markdown","source":"We see that now we have 87 features instead of the 327 features that we had before using PCA."},{"metadata":{"trusted":true,"_uuid":"3f637aa50ab1d7a51d67de8909fb64d84e07a426"},"cell_type":"code","source":"%%HTML\n<h1>Testing Different Models</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddbb87ef82641537fcbc2ac1a4a053124f85938f"},"cell_type":"markdown","source":"Now that we have finished preparing our data, it's time to test different models to see which one performs the best.\nThe models we will be testing are : \n- Linear regression\n- Support vector regression\n- Stochastic gradient descent\n- Gradient boosting tree\n- Random forest\n- Lasso regression\n- Ridge regression\n- Elastic net regularization\n- Extra trees regression"},{"metadata":{"trusted":true,"_uuid":"1513ca82be1cad806c6c2f248f84eecca7f53883"},"cell_type":"code","source":"# importing the models\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, Lasso, SGDRegressor, Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.svm import LinearSVR,SVR\n# creating the models\nmodels = [\n             LinearRegression(),\n             SVR(),\n             SGDRegressor(),\n             SGDRegressor(max_iter=1000, tol = 1e-3),\n             GradientBoostingRegressor(),\n             RandomForestRegressor(),\n             Lasso(),\n             Lasso(alpha=0.01,max_iter=10000),\n             Ridge(),\n             BayesianRidge(),\n             KernelRidge(),\n             KernelRidge(alpha=0.6,kernel='polynomial',degree = 2,coef0=2.5),\n             ElasticNet(),\n             ElasticNet(alpha = 0.001,max_iter=10000),    \n             ExtraTreesRegressor(),\n             ]\n\nnames = ['Linear regression','Support vector regression','Stochastic gradient descent','Stochastic gradient descent 2','Gradient boosting tree','Random forest','Lasso regression','Lasso regression 2','Ridge regression','Bayesian ridge regression','Kernel ridge regression','Kernel ridge regression 2','Elastic net regularization','Elastic net regularization 2','Extra trees regression']\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15b3bf83e45dec3ced5f9a3146e644d18dcc9147"},"cell_type":"code","source":"# Define a root mean square error function\ndef rmse(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model,X,y,scoring=\"neg_mean_squared_error\",cv=5))\n    return rmse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7e04f721fa121d2b8ca4e4b02ae48119f640535"},"cell_type":"code","source":"from sklearn.model_selection import KFold,cross_val_score\nwarnings.filterwarnings('ignore')\n\n# Perform 5-folds cross-calidation to evaluate the models \nfor model, name in zip(models, names):\n    # Root mean square error\n    score = rmse(model,X_train_scaled,y_train_transformed)\n    print(\"- {} : mean : {:.6f}, std : {:4f}\".format(name, score.mean(),score.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9ff7ea4b03ac805315a7d907dca974c160483e6"},"cell_type":"markdown","source":"Surprisingly, the Random forest and Extra trees regression models are the ones who performed the worst, and the linear regression model performed actually pretty good relative to the other models.\nBy compiling the above code several times and observing the different scores each time, we can classify the models by accuracy :\n\n- 1st : Kernel ridge regression\n- 2nd : Elastic net regularization and Bayesian ridge regression\n- 3rd : Ridge regression and Linear regression\n- 4rth : Support vector regression\n- 5th : Gradient boosting tree\n- 6th : Stochastic gradient  and Lasso regression\n- 7th : Random forest and Extra trees regression\n\nI think we got a good score in Elastic net regularization, Lasso regression and Stochastic gradient descent because we chose some good parameters. We can see that their score above is very bad when not specifing parameter values. So if we really want to know to best model, we need to choose optimal parameters for all the models, and tha's what we will do in the next section."},{"metadata":{"trusted":true,"_uuid":"a7742ffec8d885390b847a3521e4356b8b0e8cc9"},"cell_type":"code","source":"%%HTML\n<h1>Hyper-parameter Tuning</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da2043ea79e12b40cebfe179c7d2c371137f7055"},"cell_type":"markdown","source":"For choosing the most optimal hyper-parameters, we will perform gird search. the class GridSearchCV exhaustively considers all parameter combinations and generates candidates from a grid of parameter values specified with the param_grid parameter.\nSince we will use the same procedure for all models, we will start by creating a function which takes specified parameter values as entry."},{"metadata":{"trusted":true,"_uuid":"d42512019e9b2d5478f02e03aa7141b88ebf029c"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nclass gridSearch():\n    def __init__(self,model):\n        self.model = model\n    def grid_get(self,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5,scoring='neg_mean_squared_error')\n        grid_search.fit(X_train_scaled,y_train_transformed)\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])\n        print('\\nBest parameters : {}, best score : {}'.format(grid_search.best_params_,np.sqrt(-grid_search.best_score_)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9093f68601ce5313c4267f7824bda67cf70f55e8"},"cell_type":"markdown","source":"1. Kernel ridge regression"},{"metadata":{"trusted":true,"_uuid":"989b616e7fa58fda179454a5391b0b697e927eb3"},"cell_type":"code","source":"gridSearch(KernelRidge()).grid_get(\n        {'alpha':[3.5,4,4.5,5,5.5,6,6.5], 'kernel':[\"polynomial\"], 'degree':[3],'coef0':[1,1.5,2,2.5,3,3.5]})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9a8affea5c018bfd21ac45527f28b530c2038e2"},"cell_type":"markdown","source":"2. Elastic net regularization"},{"metadata":{"trusted":true,"_uuid":"535c7abf5be2c5eaaf78c81bbe8df7c9381820fb"},"cell_type":"code","source":"gridSearch(ElasticNet()).grid_get(\n        {'alpha':[0.006,0.0065,0.007,0.0075,0.008],'l1_ratio':[0.070,0.075,0.080,0.085,0.09,0.095],'max_iter':[10000]})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94c031f97b9143bb6e5b0a6daf2be0393e5aa3f6"},"cell_type":"markdown","source":"3. Ridge regression"},{"metadata":{"trusted":true,"_uuid":"277200b6d2b72e201be8d6405eaa2cdbdf511f4b"},"cell_type":"code","source":"gridSearch(Ridge()).grid_get(\n        {'alpha':[10,20,25,30,35,40,45,50,55,57,60,65,70,75,80,100],'max_iter':[10000]})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bb3d6646185336a16e3cfa4d8a0e99824a74eb6"},"cell_type":"markdown","source":"4. Support vector regression"},{"metadata":{"trusted":true,"_uuid":"1db005416aba236323f9a876d630dc8abce22d84"},"cell_type":"code","source":"gridSearch(SVR()).grid_get(\n        {'C':[13,15,17,19,21],'kernel':[\"rbf\"],\"gamma\":[0.0005,0.001,0.002,0.01],\"epsilon\":[0.01,0.02,0.03,0.1]})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"002af4280fc79b8eec58bbda98a9021a19b533b2"},"cell_type":"markdown","source":"5. Lasso regression"},{"metadata":{"trusted":true,"_uuid":"4f7c1d031e1ac6b94c8ed3db514ea803a9a60096"},"cell_type":"code","source":"gridSearch(Lasso()).grid_get(\n       {'alpha':[0.01,0.001,0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0008,0.0009],'max_iter':[10000]})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a55e844549c580511703f2fb2288393b370b32e3"},"cell_type":"raw","source":"We see that the models perform almost the same way with a score of 0.116. Let's define these models with the their respective best hyper-parameters."},{"metadata":{"trusted":true,"_uuid":"786675eb3cff569064c36a7975b66d24e8628bae"},"cell_type":"code","source":"lasso = Lasso(alpha= 0.0006, max_iter= 10000)\nridge = Ridge(alpha=35, max_iter= 10000)\nsvr = SVR(C = 13, epsilon= 0.03, gamma = 0.001, kernel = 'rbf')\nker = KernelRidge(alpha=6.5 ,kernel='polynomial', degree=3 , coef0=2.5)\nela = ElasticNet(alpha=0.007,l1_ratio=0.07,max_iter=10000)\nbay = BayesianRidge()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2c6bdedd462190b54244c0c0c3e21e0f55657f5"},"cell_type":"code","source":"%%HTML\n<h1>Combining Models</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4edb16610d4752807dbf6a9da0d65340af2b1da"},"cell_type":"markdown","source":"In order to further improve our model accuracy. We will use an ensemble method. I chose to use stacking. Stacked generalization is an ensemble method where the models are combined using another machine learning algorithm. The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models. Then this new dataset is used as input for the combiner machine learning algorithm."},{"metadata":{"trusted":true,"_uuid":"e5e3986672777c294f136d811c3ad9f1575daab7"},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ae15dcf1c3473632a9b7f1ba86a0b1ca839859c"},"cell_type":"code","source":"# Creating the stacking function\nclass stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,mod,meta_model):\n        self.mod = mod\n        self.meta_model = meta_model\n        kff = KFold(n_splits=5, random_state=42, shuffle=True)\n        self.kf = kff\n        \n    def fit(self,X,y):\n        self.saved_model = [list() for i in self.mod]\n        oof_train = np.zeros((X.shape[0], len(self.mod)))\n        \n        for i,model in enumerate(self.mod):\n            for train_index, val_index in self.kf.split(X,y):\n                renew_model = clone(model)\n                renew_model.fit(X[train_index], y[train_index])\n                self.saved_model[i].append(renew_model)\n                oof_train[val_index,i] = renew_model.predict(X[val_index])\n        \n        self.meta_model.fit(oof_train,y)\n        return self\n    \n    def predict(self,X):\n        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n                                      for single_model in self.saved_model]) \n        return self.meta_model.predict(whole_test)\n    \n    def get_oof(self,X,y,test_X):\n        oof = np.zeros((X.shape[0],len(self.mod)))\n        test_single = np.zeros((test_X.shape[0],5))\n        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n        for i,model in enumerate(self.mod):\n            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n                clone_model = clone(model)\n                clone_model.fit(X[train_index],y[train_index])\n                oof[val_index,i] = clone_model.predict(X[val_index])\n                test_single[:,j] = clone_model.predict(test_X)\n            test_mean[:,i] = test_single.mean(axis=1)\n        return oof, test_mean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa1393caa130820975b3463fb1162830a918d67f"},"cell_type":"code","source":"# Impute the training dataset\nX_scaled_imputed = Imputer().fit_transform(X_train_scaled)\ny_log_imputed = Imputer().fit_transform(y_train_transformed.values.reshape(-1,1)).ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da09b1c25ebe4b28b28786ea8b135802bef77a5b"},"cell_type":"code","source":"X_scaled_imputed.shape,y_log_imputed.shape,X_test_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"90a8a6561a0b6d98a6f513b9eb9e3d41387bfd83"},"cell_type":"code","source":"# Calculating the score\nstack_model = stacking(mod=[lasso,ridge,svr,ker,ela,bay],meta_model=ker)\nscore = rmse(stack_model,X_scaled_imputed,y_log_imputed)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15c6a4a38515828cbedabc0a1e6013c59c45e26f"},"cell_type":"markdown","source":"We obtain a score of 0.113, which is slightly better than the average score of the other models"},{"metadata":{"trusted":true,"_uuid":"3dd418b18b865351c1ae5bb30d9eaf7ff5256bf0"},"cell_type":"code","source":"# Combining the extracted features generated from stacking whith original features\nX_train_stack,X_test_stack = stack_model.get_oof(X_scaled_imputed,y_log_imputed,X_test_scaled)\nX_train_add = np.hstack((X_scaled_imputed,X_train_stack))\nX_test_add = np.hstack((X_test_scaled,X_test_stack))\nX_train_add.shape,X_test_add.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eda34b8bf20c26efcdfa81cc58528880ad6ae0fe"},"cell_type":"code","source":"# Calculate the final score of the model\nscore = rmse(stack_model,X_train_add,y_log_imputed)\nprint(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7730936b15568d5db8afee81b4d351a926756a1d"},"cell_type":"markdown","source":"The last score we obtain is 0.1074, which is quite good"},{"metadata":{"trusted":true,"_uuid":"f94660c118ba10b01de2e0fef21e4d104683ba6d"},"cell_type":"code","source":"%%HTML\n<h1>Making Predictions</h1>","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcc367b10ed92fbbc8a333ffb8f3eedc85737f44"},"cell_type":"markdown","source":"Now it's time to make predictions and store them in a csv file with corresponding Ids. after we make prediction we need to transform them to their original shape with exponential function"},{"metadata":{"trusted":true,"_uuid":"c60e656aaa311726fd3a91ddbb8922d842590025"},"cell_type":"code","source":"# Fit the model to the dataset generated with stacking\nstack_model.fit(X_train_add,y_log_imputed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aab29c94adedc3d915f1a9299f46620a0cae2c34"},"cell_type":"code","source":"# Making prediction on the test set generated by stacking\npredicted_prices = np.exp(stack_model.predict(X_test_add))\n# Prepare the csv file\nmy_submission = pd.DataFrame({'Id': testData.Id, 'SalePrice': predicted_prices})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aee13d8a2c7145d23d3e6e60c98108b598dcf716"},"cell_type":"markdown","source":"The last time I submitted, I was the 859th (top 20%) with a score of 0.11934"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"333f01b65681799faa52a7cf8908f81f0492c748"},"cell_type":"code","source":"%%HTML\n<h2>Thanks for reading my notebook. If you liked my kernel please kindly UPVOTE for other people to see. If you have any remarks, please leave a comment bellow.<h2>","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8037a21d35dfce30628355c7bc2e77526e0ae63c"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}