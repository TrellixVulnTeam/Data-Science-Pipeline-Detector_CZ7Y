{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stacked Ensemble Models\nThis marks my first attempt to begin my journey on Kaggle.  \nAt the time of writing, it has a RMSLE of 0.11878 (Top 3% on Public Leaderboard).  \n  \nThis kernel will be a documentation of how I went through the processes and I sincerely hope that it will help the readers especially beginners.  \n**Feel free to comment if you have any suggestions or questions!**","metadata":{}},{"cell_type":"markdown","source":"# ***Initialization***","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom scipy.stats import skew, norm, probplot\nimport time\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.linear_model import Ridge, HuberRegressor, LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.cluster import KMeans\nimport catboost as cb\nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df['SalePrice']\ndf = df.drop(['SalePrice'],axis=1)\ndf = df.set_index('Id')\ntest = test.set_index('Id')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Missing-data Imputation***\nData are not always clean. We cannot just simply discard them either as that will cause a huge loss of information.  \n  \nImputation of missing data is therefore needed to preserves the data cases by replacing missing data with an estimated value based on other available information.","metadata":{}},{"cell_type":"code","source":"null_list = []\nfor col in df.columns:\n    null = df[col].isnull().sum()\n    test_null = test[col].isnull().sum()\n    if null != 0 or test_null != 0:\n        null_list.append([col,null,test_null])\nnull_df = pd.DataFrame(null_list,columns=['Feature','Null','Test Null'])\nnull_df.set_index('Feature')\nnull_df['Total Null'] = null_df['Null'] + null_df['Test Null']\nprint(\"-------------------------\")\nprint(\"Total columns with null:\")\nprint(len(null_df))\nprint(\"-------------------------\")\nprint(\"Total null values:\")\nprint(null_df['Total Null'].sum(axis=0))\nprint(\"-------------------------\")\nsns.set_palette(sns.color_palette(\"pastel\"))\nsns.barplot(data=null_df.sort_values(by='Total Null',ascending = False).head(10), x='Feature',y='Total Null')\nplt.xticks(rotation = 70)\nplt.title(\"Total Nulls in Feature\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will have to impute those missing values with the most sensable method by looking at them one at a time (a little overkill?).  \nBut it's nice to do some general EDA throughout the process too.\n\n___\n**MSZoning** : Identifies the general zoning classification of the sale.  \n       A\tAgriculture  \n       C\tCommercial  \n       FV\tFloating Village Residential  \n       I\tIndustrial  \n       RH\tResidential High Density  \n       RL\tResidential Low Density  \n       RP\tResidential Low Density Park   \n       RM\tResidential Medium Density  \n       \nWe join our given training set and test data set together while we go through the process","metadata":{}},{"cell_type":"code","source":"full = pd.concat([df,test],axis=0).reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null = test[test['MSZoning'].isnull()][[\"Neighborhood\",\"MSZoning\"]]\ndisplay(null)\nplot_data = pd.concat([full[full['Neighborhood'] == 'IDOTRR'],full[full['Neighborhood'] == 'Mitchel']],axis = 0)\nsns.histplot(data = plot_data, x ='MSZoning', hue ='Neighborhood',multiple=\"dodge\", shrink=.9)\nplt.title(\"Distribution of Zoning Classification\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the general zoning classification usually depends on the neighborhood, we will impute the missing value by the mode in the area.","metadata":{}},{"cell_type":"code","source":"test.loc[(test['Neighborhood'] == 'IDOTRR') & (test['MSZoning'].isnull()), 'MSZoning'] = 'RM'\ntest.loc[(test['Neighborhood'] == 'Mitchel') & (test['MSZoning'].isnull()), 'MSZoning'] = 'RL'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**LotFrontage** : Linear feet of street connected to property  \nWe expect LotFrontage to be somewhat correlated with LotArea. Hence we will use LinearRegression to impute the missing values.   \nWe also manually filter out the outliers from the data.","metadata":{}},{"cell_type":"code","source":"data = full[(~full['LotFrontage'].isnull()) & (full['LotFrontage'] <= 150) & (full['LotArea'] <= 20000)]\nsns.lmplot(data=data,x=\"LotArea\",y=\"LotFrontage\", line_kws={'color': 'black'})\nplt.ylabel(\"LotFrontage\")\nplt.xlabel(\"LotArea\")\nplt.title(\"LotArea vs LotFrontage\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"area_vs_frontage = LinearRegression()\narea_vs_frontage_X = data['LotArea'].values.reshape(-1, 1)\narea_vs_frontage_y = data['LotFrontage'].values\narea_vs_frontage.fit(area_vs_frontage_X,area_vs_frontage_y)\nfor table in [df,test]:\n    table['LotFrontage'].fillna(area_vs_frontage.intercept_ + table['LotArea'] * area_vs_frontage.coef_[0] , inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Alley** : data description says NA means no alley access","metadata":{}},{"cell_type":"code","source":"for table in [df,test]:\n    table['Alley'].fillna(\"None\",inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Utilities** : Type of utilities available","metadata":{}},{"cell_type":"code","source":"full['Utilities'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there is only 1 data that uses NoSeWa and, we will surely fill the missing value in test set with AllPub.  \nWe will just drop the NoSeWa row in our training dataset since it is not found in the test set and will contribute to overfitting if left alone.","metadata":{}},{"cell_type":"code","source":"test['Utilities'].fillna(\"AllPub\",inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.drop(df[df['Utilities'] == 'NoSeWa'].index, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exterior1st**: Exterior covering on house  \n**Exterior2nd**: Exterior covering on house (if more than one material)  \n\nThere are more than 10 types of materials used in both the metrics. However, we can notice from the barplot that most of them are made of Vinyl. Hence, we will just fill the null values with the mode (Vinyl).","metadata":{}},{"cell_type":"code","source":"for metrics in ['Exterior1st','Exterior2nd']:\n    table = full[metrics].value_counts(normalize=True).head()\n    sns.barplot(x=table.index,y=table.values)\n    plt.title(\"Distribution plot of \"+metrics)\n    plt.show()\n    print(\"\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Exterior1st'] = test['Exterior1st'].fillna(full['Exterior1st'].mode()[0])\ntest['Exterior2nd'] = test['Exterior2nd'].fillna(full['Exterior2nd'].mode()[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MasVnrType** : data description says NA means no Masonry veneer.  \nHowever we notice one data in test set with area but missing type.","metadata":{}},{"cell_type":"code","source":"test[(test['MasVnrType'].isnull()) & (test['MasVnrArea'].notnull())][['MasVnrType','MasVnrArea']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"table = full['MasVnrType'].value_counts(normalize=True).head()\nsns.barplot(x=table.index,y=table.values)\nplt.title(\"Distribution plot of MasVnrType\")\nplt.show()\nprint(\"\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since around 60% of our data do not have Masonry veneer. It will be used to fill the null value in row 2611 and also the other rows.","metadata":{}},{"cell_type":"code","source":"test['MasVnrType'][2611] = \"BrkFace\"\ntest['MasVnrType'] = test['MasVnrType'].fillna(full['MasVnrType'].mode()[0])\ntest['MasVnrArea'] = test['MasVnrArea'].fillna(0)\ndf['MasVnrType'] = df['MasVnrType'].fillna(full['MasVnrType'].mode()[0])\ndf['MasVnrArea'] = df['MasVnrArea'].fillna(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Basement Metrics** : data description says BsmtFinType1 measures the Type 1 finished square feet of basement.  \nHowever, we can see a few data in test data set having basement metrics but \"0\" squarefeets","metadata":{}},{"cell_type":"code","source":"for basement_metrics_cols in ['BsmtExposure','BsmtCond','BsmtQual']:\n    if len(full[(full[basement_metrics_cols].notnull()) & (full['BsmtFinType1'].isnull())]) > 0 :\n        print(\"Present with BsmtFinType1 but undetected\" + basement_metrics_cols)\n        display(full[(full[basement_metrics_cols].notnull()) & (full['BsmtFinType1'].isnull())])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for basement_metrics_cols in ['BsmtExposure','BsmtCond','BsmtQual']:\n    if len(full[(full[basement_metrics_cols].isnull()) & (full['BsmtFinType1'].notnull())]) > 0 :\n        print(\"\\nPresent with \"+ basement_metrics_cols+\" but BsmtFinType1 undetected\" )\n        display(full[(full[basement_metrics_cols].isnull()) & (full['BsmtFinType1'].notnull())])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We assume missing basement exposure of unfinished basement is \"No\".\ndf.loc[((df['BsmtExposure'].isnull()) & (df['BsmtFinType1'].notnull())), 'BsmtExposure'] = 'No'\ntest.loc[((test['BsmtExposure'].isnull()) & (test['BsmtFinType1'].notnull())), 'BsmtExposure'] = 'No'\n# We impute missing basement condition with \"mean\" value of Typical.\ntest.loc[((test['BsmtCond'].isnull()) & (test['BsmtFinType1'].notnull())), 'BsmtCond'] = 'TA'\n# We impute unfinished basement quality with \"mean\" value of Typical.\ntest.loc[((test['BsmtQual'].isnull()) & (test['BsmtFinType1'].notnull())), 'BsmtQual'] = 'TA'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is one test data with missing square feet values. Let's check it out too.","metadata":{}},{"cell_type":"code","source":"test[test['BsmtFinSF1'].isnull()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This test data do not have basement. Hence, those squarefeets metrics should be filled in with 0.","metadata":{}},{"cell_type":"code","source":"for square_feet_metrics in ['TotalBsmtSF','BsmtUnfSF','BsmtFinSF2','BsmtFinSF1']:\n    test[square_feet_metrics][2121] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is two test data with missing basement bathroom values. Let's check them out first too.","metadata":{}},{"cell_type":"code","source":"test[test['BsmtFullBath'].isnull()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The two test data do not have basement. Hence, those bathroom amount in basement should also be filled in with 0.","metadata":{}},{"cell_type":"code","source":"for bathroom_metrics in ['BsmtFullBath','BsmtHalfBath']:\n    test[bathroom_metrics][2121] = 0\n    test[bathroom_metrics][2189] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The other data are assumed to not have basements hence filling in None.","metadata":{}},{"cell_type":"code","source":"for table in [df,test]:\n    table[table.columns[table.columns.str.contains('Bsmt')]] = table[table.columns[table.columns.str.contains('Bsmt')]].fillna(\"None\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Electrical, Functional and Kitchen Quality** These three metrics will too be filled with their \"average\" values.","metadata":{}},{"cell_type":"code","source":"for metrics in ['Electrical','Functional','KitchenQual']:\n    table = full[metrics].value_counts(normalize=True)\n    sns.barplot(x=table.index,y=table.values)\n    plt.title(\"Distribution plot of \"+metrics)\n    plt.show()\n    print(\"\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These three metrics are safe to be filled with the mode values.","metadata":{}},{"cell_type":"code","source":"df['Electrical'].fillna('SBrkr',inplace=True)\ntest['Functional'].fillna('Typ',inplace=True)\ntest['KitchenQual'].fillna('TA',inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full[full['GarageCars'].isnull()]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Simililarly, this test data do not have a garage, filling GarageArea and GarageCars with 0.","metadata":{}},{"cell_type":"code","source":"test['GarageCars'].fillna(0,inplace=True)\ntest['GarageArea'].fillna(0,inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(full[full['SaleType'].isnull()])\ntable = full['SaleType'].value_counts(normalize=True)\nsns.barplot(x=table.index,y=table.values)\nplt.title(\"Distribution plot of SaleType\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the SaleType column, we will impute the missing data with the mode since the mode value is kinda high too.","metadata":{}},{"cell_type":"code","source":"test['SaleType'].fillna('WD',inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's now a good time to recheck all other remaining missing values.","metadata":{}},{"cell_type":"code","source":"null_list = []\nfor col in df.columns:\n    null = df[col].isnull().sum()\n    test_null = test[col].isnull().sum()\n    if null != 0 or test_null != 0:\n        null_list.append([col,null,test_null])\nnull_df = pd.DataFrame(null_list,columns=['Feature','Null','Test Null'])\nnull_df.set_index('Feature')\nnull_df['Total Null'] = null_df['Null'] + null_df['Test Null']\nprint(\"-------------------------\")\nprint(\"Total columns with null:\")\nprint(len(null_df))\nprint(\"-------------------------\")\nprint(\"Total null values:\")\nprint(null_df['Total Null'].sum(axis=0))\nprint(\"-------------------------\")\nsns.set_palette(sns.color_palette(\"pastel\"))\nsns.barplot(data=null_df.sort_values(by='Total Null',ascending = False).head(10), x='Feature',y='Total Null')\nplt.xticks(rotation = 70)\nplt.title(\"Total Nulls in Feature\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We do not have anything extra to infer these missing columns. Hence, we will treat them as \"None\" which is not having those items.","metadata":{}},{"cell_type":"code","source":"df['GarageYrBlt'].fillna(0,inplace=True)\ntest['GarageYrBlt'].fillna(0,inplace=True)\ndf.fillna(\"None\", inplace=True)\ntest.fillna(\"None\", inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the total null value again.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum().sum() + test.isnull().sum().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.index = df.index - 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Feature Engineering***\n## Log-transformation of skewed target variable\nLog-transformation is a technique used to perform Feature Transformation. It is one of the many techniques that can be used to transform the features so that they are treated equally.  \n\nWhy do we want models to treat them equally? It is because when we input these features to the model, there is a posibillity that an larger value in an imbalance feature will influence the result more and further affect the model performance. This is not something we will want as each and every row of data are equally important as a predictor. \n\nWe wouldn't want the model to prioritize predicting only data with higher sale prices. Hence, scaling and transforming is important for algorithms where distance between the data points is important.\n\nWe picked log-transformation here as it has the power to alter the skewness of a distribution towards normality. You can observe how log-transformation of a feature can transform its distribution and scale.","metadata":{}},{"cell_type":"code","source":"# Distribution plot\nsns.distplot(y , fit=norm);\n\n(mu, sigma) = norm.fit(y)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n# QQ-plot\nfig = plt.figure()\nres = probplot(y, plot=plt)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first plot is a distribution plot where we compare the distribution of our target variable with a normal distribution.  \nWe can easily see it is right-skewed.  \n  \nThe Q-Q plot below plots the quantiles of our target feature against the quantiles of a normal distribution.  \nWe can also easily see the skewness in the target feature.\n  \nNotice how it changes after we apply log transformation onto our feature.","metadata":{}},{"cell_type":"code","source":"y = np.log(y)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(y , fit=norm);\n(mu, sigma) = norm.fit(y)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = probplot(y, plot=plt)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now see the distribution plot are much closer to a normal distribution.  \n  \nThe Q-Q plot below also shows that the quantiles of our target feature and the quantiles of a normal distribution are much closer now.  ","metadata":{}},{"cell_type":"markdown","source":"## Feature creation\nIn this short section we will construct some new (important) features from existing data that can be fed into our model later on. There are many ways to increase our data, one of them is through creating combinations or ratio from the most relevant variables from the raw data.  \n  \nIn this competition, I decided to add only a few extra features related to square-feet as I think the size of a house will be the main factor of its price.  \n  \nWe also transformed some features that are supposingly categorical but labelled as numerical as they are consisting of numbers.","metadata":{}},{"cell_type":"code","source":"df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\ntest['TotalSF'] = test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for table in [df,test]:\n    table['MSSubClass'] = table['MSSubClass'].apply(str)\n    table['YrSold'] = table['YrSold'].astype(str)\n    table['MoSold'] = table['MoSold'].astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Encoding Round 1 (Ordinal)\nMany machine learning models prefer or can only work with numerical values. Hence, it is common practice to transform the categorical values of the relevant features into numerical ones.  \n  \nThere are many ways though, to transform the features, one of which is through ordinal encoding. We use this method whenever our features has order (A is better than B) so that we can retain the information regarding the order.","metadata":{}},{"cell_type":"code","source":"qual_dict = {'None': 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\nbsmt_fin_dict = {'None': 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n\nfor table in [df,test]:\n    table[\"ExterQual\"] = table[\"ExterQual\"].map(qual_dict)\n    table[\"ExterCond\"] = table[\"ExterCond\"].map(qual_dict)\n    table[\"BsmtQual\"] = table[\"BsmtQual\"].map(qual_dict)\n    table[\"BsmtCond\"] = table[\"BsmtCond\"].map(qual_dict)\n    table[\"PoolQC\"] = table[\"PoolQC\"].map(qual_dict)\n    table[\"HeatingQC\"] = table[\"HeatingQC\"].map(qual_dict)\n    table[\"KitchenQual\"] = table[\"KitchenQual\"].map(qual_dict)\n    table[\"FireplaceQu\"] = table[\"FireplaceQu\"].map(qual_dict)\n    table[\"GarageQual\"] = table[\"GarageQual\"].map(qual_dict)\n    table[\"GarageCond\"] = table[\"GarageCond\"].map(qual_dict)\n\n    table[\"BsmtExposure\"] = table[\"BsmtExposure\"].map(\n        {'None': 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}) \n    table[\"BsmtFinType1\"] = table[\"BsmtFinType1\"].map(bsmt_fin_dict)\n    table[\"BsmtFinType2\"] = table[\"BsmtFinType2\"].map(bsmt_fin_dict)\n\n    table[\"Functional\"] = table[\"Functional\"].map(\n        {'None': 0, \"Sal\": 1, \"Sev\": 2, \"Maj2\": 3, \"Maj1\": 4, \n         \"Mod\": 5, \"Min2\": 6, \"Min1\": 7, \"Typ\": 8})\n\n    table[\"GarageFinish\"] = table[\"GarageFinish\"].map(\n        {'None': 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3})\n\n    table[\"Fence\"] = table[\"Fence\"].map(\n        {'None': 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4})\n    \n    table[\"CentralAir\"] = table[\"CentralAir\"].map(\n        {'N': 0, \"Y\": 1})\n    \n    table[\"PavedDrive\"] = table[\"PavedDrive\"].map(\n        {'N': 0, \"P\": 1, \"Y\": 2})\n\n    \n    table[\"Street\"] = table[\"Street\"].map(\n        {'Grvl': 0, \"Pave\": 1})\n    \n    table[\"Alley\"] = table[\"Alley\"].map(\n        {'None': 0, \"Grvl\": 1, \"Pave\": 2})\n    \n    table[\"LandSlope\"] = table[\"LandSlope\"].map(\n        {'Gtl': 0, \"Mod\": 1, \"Sev\": 2})\n    \n    table[\"LotShape\"] = table[\"LotShape\"].map(\n        {'Reg': 0, \"IR1\": 1, \"IR2\": 2, \"IR3\": 3})\n    \nmodified_cols = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual' \\\n                    ,'FireplaceQu','GarageQual','GarageCond','BsmtExposure','BsmtFinType1' \\\n                   ,'BsmtFinType2', 'Functional','GarageFinish','Fence','Street','Alley','LandSlope'\\\n                    ,'PavedDrive' ,'CentralAir','PoolQC','OverallQual','OverallCond','LotShape']\n\n# Get list of categorical variables in holiday dataset\ns = (df.dtypes == 'object')\nobject_cols = list(s[s].index)\nobject_cols = [x for x in object_cols if x not in modified_cols]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After round 1 of encoding the obvious ordinal features. We can still go further to simplify our features. This is great when the feature is highly skewed, we can group some values into \"Others\" to reduce the number of columns when we use one-hot encoding later on.\n  \nSo, we will plot the distributions of the features and see how we should simplify them.","metadata":{}},{"cell_type":"markdown","source":"### **!! Long journey of charts and tables ahead, feel free to skip through**","metadata":{}},{"cell_type":"code","source":"full = pd.merge(left = df, right = y , left_index= True, right_index = True)\nfull['SalePrice'] = np.exp(full['SalePrice'])\n\nfor col in object_cols:\n    if full[col].nunique()> 1:\n        display(full.groupby(col)['SalePrice'].describe())\n        print(\"\\nSummary statistics and graph for \"+ col)\n        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n        sns.countplot(data = full, x=col, ax= ax[0])\n        ax[0].title.set_text(\"Count plot of \" + col)\n        sns.swarmplot(data=full,x=col,y='SalePrice', ax= ax[1])\n        ax[1].title.set_text(\"Swarm plot of \" + col +\" versus Sale Price\")\n        if (full[col].nunique()>=15):\n            ax[0].tick_params('x',labelrotation=70)\n            ax[1].tick_params('x',labelrotation=70)\n        fig.tight_layout()\n        plt.show()\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Encoding Round 2 (Simplification + Ordinal)\nWe can see that many of the features are highly skewed and some feature value counts are very low.  \nHence, we will just group them as \"Others\". For features that only have two value, we will also just do the manual one-hot encoding here.  \n  \nThose that have more than two unique values will be one-hot encoded below.","metadata":{}},{"cell_type":"code","source":"cond_1_keep = ['Norm','Feedr','Artery']\nroof_style_keep = ['Gable','Hip']\nfoundation_keep = ['PConc','CBlock','BrkTil']\ngarage_keep = ['Attchd','Detchd','BuiltIn']\nsale_keep = ['WD','New','COD']\nsale_cond_keep = ['Normal','Abnorml','Partial']\npeak_months = ['5','6','7']\nlot_config_keep = ['Inside','Corner','CulDSac']\nunfinished_style = ['1.5Unf','2.5Unf']\nexter_remove = ['AsphShn','BrkComm','CBlock','ImStucc','Stone']\nfor table in [df,test]:\n    table.loc[table['LandContour']!='Lvl','LandContour'] = 0\n    table.loc[table['LandContour']!=0,'LandContour'] = 1\n    \n    table.loc[~table['Condition1'].isin(cond_1_keep),'Condition1'] = \"Others\"\n    table.loc[table['Condition2']!=\"Norm\",'Condition2'] = 0\n    table.loc[table['Condition2']!= 0,'Condition2'] = 1\n    \n    table.loc[~table['RoofStyle'].isin(roof_style_keep),'RoofStyle'] = \"Others\"\n    table.loc[table['RoofMatl']!='CompShg','RoofMatl'] = 0\n    table.loc[table['RoofMatl']!=0,'RoofMatl'] = 1\n    \n    table.loc[~table['Foundation'].isin(foundation_keep),'Foundation'] = \"Others\"\n    table.loc[table['Heating']!='GasA','Heating'] = 0\n    table.loc[table['Heating']=='GasA','Heating'] = 1\n    table.loc[table['Electrical']!='SBrkr','Electrical'] = 0\n    table.loc[table['Electrical']!=0,'Electrical'] = 1\n    \n    table.loc[~table['GarageType'].isin(garage_keep),'GarageType'] = \"Others\"\n    \n    table.loc[~table['SaleType'].isin(sale_keep),'SaleType'] = \"Others\"\n    table.loc[~table['SaleCondition'].isin(sale_cond_keep),'SaleCondition'] = \"Others\"\n    table.loc[~table['SaleCondition'].isin(sale_cond_keep),'SaleCondition'] = \"Others\"\n    \n    table.loc[table['Exterior1st'].isin(exter_remove),'Exterior1st'] = \"Others\"\n    table.loc[table['Exterior2nd'].isin(exter_remove),'Exterior2nd'] = \"Others\"\n    \n    table.loc[table['MoSold'].isin(peak_months),'PeakMonths'] = 1\n    table.loc[table['PeakMonths']!=1,'PeakMonths'] = 0\n    \n    table.loc[~table['LotConfig'].isin(lot_config_keep),'LotConfig'] = \"Others\"\n    \n    table.loc[~table['HouseStyle'].isin(unfinished_style),'Unfinished'] = 1\n    table.loc[table['Unfinished']!= 1 ,'Unfinished'] = 0\n    table.loc[table['HouseStyle'].isin(['SFoyer','SLvl']),'IsSplit'] = 1\n    table.loc[table['IsSplit']!= 1 ,'IsSplit'] = 0   \n    table[\"HouseStyle\"] = table[\"HouseStyle\"].map(\n        {'SFoyer': 0, \"SLvl\": 0, \"1Story\": 1, \"1.5Fin\": 2, \"1.5Unf\": 2, \"2Story\": 3, \"2.5Fin\": 4, \"2.5Unf\": 4})\n    \n    table.drop('Utilities', axis = 1 , inplace = True)\n\n    \nmodified_cols_round_2 = ['HouseStyle','LandContour','Condition2','RoofMatl','Heating','Electrical','Utilities']\nobject_cols = [x for x in object_cols if x not in modified_cols_round_2]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Clustering\nBefore we go on to one-hot encode our categorical features. We can see that some of the features still have a lot of unique values.  \n  \nThis will cause our final training data to have a lot of columns as each and every of the unique values will be encoded into one extra columns. So we can go one step further to simplify the features using clusters.  \n  \nTo do that, we will use an unsupervised learning method which is K-Means to identify suitable clusters.\n  \nFor neighborhoods, I intend to group them into 5 clusters and subclasses I will group them into 4 clusters.\n  \nTo do that, we try to provide K-Means with as many information regarding the feature that we want to cluster as possible. We will use .describe() to include the various statistics regarding the feature and feed it into the model.","metadata":{}},{"cell_type":"code","source":"neighborhood = full.groupby(['Neighborhood'])['SalePrice'].describe()\ndisplay(neighborhood.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neighborhood_cluster = KMeans(n_clusters=5, random_state = 927)\nneighborhood_cluster.fit(neighborhood)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neigh_cluster_table = pd.DataFrame(zip(list(neighborhood.index),list(neighborhood.loc[:,'mean']),list(neighborhood_cluster.labels_)),columns = ['Neighborhood','MeanSalePrice','Neighborhood Cluster'])\nfor i  in range(len(neigh_cluster_table.groupby('Neighborhood Cluster')['Neighborhood'].unique())):\n    print(\"Cluster \" + str(i))\n    print(neigh_cluster_table.groupby('Neighborhood Cluster')['Neighborhood'].unique()[i])\nsns.scatterplot(data = neigh_cluster_table, x='Neighborhood',y = 'MeanSalePrice', hue='Neighborhood Cluster',palette=sns.color_palette(\"Set2\",5))\nplt.xticks(rotation=70)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subclass = full.groupby(['MSSubClass'])['SalePrice'].describe()\ndisplay(subclass.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subclass_cluster = KMeans(n_clusters=4, random_state = 927)\nsubclass_cluster.fit(subclass)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mssub_cluster_table = pd.DataFrame(zip(list(subclass.index),list(subclass.loc[:,'mean']),list(subclass_cluster.labels_)),columns = ['MSSubClass','MeanSalePrice','MSSubClass Cluster'])\nfor i  in range(len(mssub_cluster_table.groupby('MSSubClass Cluster')['MSSubClass'].unique())):\n    print(\"Cluster \" + str(i))\n    print(mssub_cluster_table.groupby('MSSubClass Cluster')['MSSubClass'].unique()[i])\nsns.scatterplot(data = mssub_cluster_table, x='MSSubClass',y = 'MeanSalePrice', hue='MSSubClass Cluster',palette=sns.color_palette(\"Set2\",4))\nplt.xticks(rotation=70)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mssub_cluster_table.drop('MeanSalePrice', axis = 1 ,inplace = True)\nneigh_cluster_table.drop('MeanSalePrice', axis = 1, inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.merge(left = df.reset_index(), right = mssub_cluster_table, how='left', on ='MSSubClass').set_index('Id')\ndf = pd.merge(left = df.reset_index(), right = neigh_cluster_table, how='left', on ='Neighborhood').set_index('Id')\ndf.drop('MSSubClass', axis = 1 ,inplace = True)\ndf.drop('Neighborhood', axis = 1 ,inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.merge(left = test.reset_index(), right = mssub_cluster_table, how='left', on ='MSSubClass').set_index('Id')\ntest = pd.merge(left = test.reset_index(), right = neigh_cluster_table, how='left', on ='Neighborhood').set_index('Id')\ntest.drop('MSSubClass', axis = 1 ,inplace = True)\ntest.drop('Neighborhood', axis = 1 ,inplace = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After merging the clusters into our training dataset, we also keep track of what are the remaining categorical variables that we want to one-hot encode below.","metadata":{}},{"cell_type":"code","source":"modified_cols.append('MSSubClass')\nmodified_cols.append('Neighborhood')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"object_cols.append('MSSubClass Cluster')\nobject_cols.append('Neighborhood Cluster')\nobject_cols.remove('MSSubClass')\nobject_cols.remove('Neighborhood')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Encoding Round 3 (Categorical)\nWe perform one-hot encoding to the remaining categorical variables","metadata":{}},{"cell_type":"code","source":"# One Hot Encoding for Other Columns\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols = pd.DataFrame(OH_encoder.fit_transform(df[object_cols]))\nOH_cols.index = df.index\nOH_cols.columns = OH_encoder.get_feature_names(object_cols)\ndf = df.drop(object_cols, axis=1)\ndf = pd.concat([df, OH_cols], axis=1)\n\nOH_cols = pd.DataFrame(OH_encoder.transform(test[object_cols]))\nOH_cols.index = test.index\nOH_cols.columns = OH_encoder.get_feature_names(object_cols)\ntest = test.drop(object_cols, axis=1)\ntest = pd.concat([test, OH_cols], axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Transformation (Skewed Features)\nWe should also take care of the skewness of the features in our dataset. We use skew() from the scipy.stats module to identify which columns are skewed.  \n  \nAny skewness greater than 0.5 is actually considered slightly skewed hence we will perform log-transformation for any values greather than that.","metadata":{}},{"cell_type":"code","source":"skewed = df[df.columns[~df.columns.isin(list(OH_cols.columns) + modified_cols + object_cols)]].apply(lambda x: skew(x.dropna().astype(float)))\nskewed = skewed[skewed > 0.5]\nskewed = skewed.index\n\ndf[skewed] = np.log1p(df[skewed])\ntest[skewed] = np.log1p(test[skewed])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling\nWhile log-transformation took care of the skewness in the features, we will also want to further scale the features to a standardize the range.  \n  \nOf the many scaling choices such as MinMaxScaler, StandardScaler, we picked RobustScaler.  \n  \nThe reasoning behind this is because we have seen that our data seems to be quite skewed and it will tend to have more outliers than a normal dataset. Using a RobustScaler can deal with that easily as it uses statistics that are insensitive to outliers to scale the data.\n  \nA robust scaler minuses the median and divides it by the interquatile range. Both of which are not affected by the outliers.","metadata":{}},{"cell_type":"code","source":"for col in df[df.columns]:\n    if col not in (list(OH_cols.columns) + modified_cols + object_cols):\n        scaler = RobustScaler()\n        df[col] = scaler.fit_transform(df[[col]])\n        test[col] = scaler.transform(test[[col]])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection\nFeature selection is a simple way to reduce redundant and irrelevant data from our dataset and some of them contribute close to nothing.  \nRemoving the irrelevant data actually improves learning accuracy and greatly reduces the computation time.\n  \nBy removing redundant data, we can reduce the chance of our model overfitting to the data too.\n  \nThere are some ways to perform features selection and some of which we surely studied before such as the Pearson’s Correlation and Analysis of Variance (ANOVA). In this notebook, we will utilize the mutual info regression to estimate the dependency of the variables with our target variable.  \n  \nMutual information is a non-negative value and it shows the dependency between the variables. Meaning a mutual information of 0 will be saying that both of the features are completely independent. Hence, it is a safe bet for us to remove them off. Note the other name of mutual information is information gain (you may have heard it before).\n  \nMutual information measures the amount of information one can obtain from one random variable given another. Source : Data Mining: Practical Machine Learning Tools and Techniques, 4th edition, 2016.","metadata":{}},{"cell_type":"code","source":"full = pd.merge(left = df, right = y , left_index= True, right_index = True)\nmi = mutual_info_regression(X = full.drop('SalePrice', axis = 1), y = full['SalePrice'])\nmi_df = pd.DataFrame(list(zip(full.columns,mi)), columns =['Feature','Mutual Info'])\nmi_df = mi_df.sort_values('Mutual Info',ascending=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"low_mi_df = mi_df[abs(mi_df['Mutual Info']) == 0]\nfilter_feature = sorted(list(low_mi_df['Feature']))\nprint(\"Number of low correlated features dropped: \" + str(len(filter_feature)))\ndf = df.drop(filter_feature,axis=1)\ntest = test.drop(filter_feature,axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Polynomial and Interaction Features\nAnother part of feature creation ! In this part, we create new polynomial and interaction features from the high mutual information features to derive new combinations that might be useful to our model later on.  \n  \nPolynomial features can allow our linear models to grasp on the non-linearity of the features and we can also see if there is some new interesting relationships between the features themselves by introducing interaction features.\n  \nWe can actually generate polynomial and interaction features from all of our features (quite large) and further cherry pick the good features. There may be hidden interesting relationship to be uncovered there but I am quite satisfied with only using the highly depended features.\n  \nTo read more about interaction features: https://stattrek.com/multiple-regression/interaction.aspx","metadata":{}},{"cell_type":"code","source":"top_mi_list = list(mi_df.head(20)['Feature'])\ntop_mi_subset = df[top_mi_list]\nindex_copy = top_mi_subset.index\n\npoly = PolynomialFeatures(2, interaction_only=True)\npoly_features = pd.DataFrame(poly.fit_transform(top_mi_subset),columns=poly.get_feature_names_out(top_mi_list))\npoly_features = poly_features.iloc[:,len(top_mi_list) + 1:]\npoly_features.set_index(index_copy, inplace = True)\npoly_and_price = pd.concat([y,poly_features],axis=1).dropna()\ntop_20_poly = abs(poly_and_price.corr()['SalePrice']).sort_values(ascending=False)[1:21]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df,poly_features[top_20_poly.index]],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_mi_subset = test[top_mi_list]\nindex_copy = top_mi_subset.index\npoly_features = pd.DataFrame(poly.transform(top_mi_subset),columns=poly.get_feature_names_out(top_mi_list))\npoly_features = poly_features.iloc[:,len(top_mi_list) + 1:]\npoly_features.set_index(index_copy, inplace = True)\ntest = pd.concat([test,poly_features[top_20_poly.index]],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_20_poly.index","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outlier Identification\nOutliers, the one thing that statistic text books like to assume they are normal.\n  \nToo bad they are usually not. A bad outlier case actually increases the variance in our model and further reduces the power of our model to grasp onto the data. Outliers cause regression model (especially linear ones) to learn a skewed understanding towards the outlier.  \n  \nIsolation Forest much like its' name, works to isolation a tree in a huge forest. It works by randomly sampling data based on randomly selected features and potray them in a binary decision tree structure. For an outlier, there are actually less splits needed in the forest to isolate them. Conversely, a datapoint that is not an outlier will require a lot more splits to be isloted. \n[Read more about Isolation Forest on my article.](https://medium.com/@limyenwee_19946/unsupervised-outlier-detection-with-isolation-forest-eab398c593b2)","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\niso_forest = IsolationForest(random_state=0)\ndf_without_outlier = pd.Series(iso_forest.fit_predict(df), index = full.index)\ndf = df.loc[df_without_outlier.index[df_without_outlier == 1],:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another way to categorize outliers is by using standardized residuals from linear models. Standardized residuals is can easily identify an abnormal residuals as they are standardized and we can observe the residuals in standard deviation units. Anything larger than 3 standard deviations are usually considered outliers.","metadata":{}},{"cell_type":"code","source":"full = pd.merge(left = df, right = y , left_index= True, right_index = True)\nlinear = LinearRegression()\nY = full['SalePrice']\nlinear.fit(full.drop(['SalePrice'],axis=1), Y)\nY_hat = linear.predict(full.drop(['SalePrice'],axis=1))\nresiduals = Y - Y_hat\ny_vs_yhat_df = pd.DataFrame(zip(Y.values,Y_hat,residuals),columns=['y','yhat','residuals'],index=full.index)\n\nr2 = r2_score(Y, Y_hat)\nprint(\"About \" + str(round(r2 * 100,2)) + \"% of variation in the Sale Price can be explained by the model.\")\n\nsns.scatterplot(Y, Y_hat)\nsns.lineplot(np.linspace(10.5,13.5),np.linspace(10.5,13.5), color='black', linewidth=2.5)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"standard_residuals = (residuals - residuals.mean()) / residuals.std()\noutliers = full[abs(standard_residuals) > 3]\ny_vs_yhat_df.loc[y_vs_yhat_df.index.isin(outliers.index),'Outlier'] = 1\ny_vs_yhat_df.loc[y_vs_yhat_df['Outlier'] != 1 ,'Outlier'] = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data = y_vs_yhat_df, x='y', y='yhat',hue ='Outlier', palette = ['blue','red'])\nsns.lineplot(np.linspace(10.5,13.5),np.linspace(10.5,13.5), color='black', linewidth=2.5)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.loc[y_vs_yhat_df[y_vs_yhat_df['Outlier'] == 0].index,:]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(list(test.columns[test.nunique()== 1 ]),axis=1)\ntest = test.drop(list(test.columns[test.nunique()== 1]),axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Modelling**\nFor this part, we will be using Ridge, XGB, Catboost, SVR, Huber and a Stacked regression.  \nKeep in mind that this notebook will not be showing the GridSearch part which is used to hypertune the parameters as that will take some time to finish running.\n  \nNote that you may choose not to select as much models as I used, I just did it to try out more models and decided to stick with them. The performance of the models will later be averaged out (ensemble model) and we will also implement a stacked regressor at the same time.  \n  \nStacked regressor is a type of Level 1 ensemble model that generalizes the predictions made by different models to get the final output. You can study more information regarding stacked models here  \nhttps://www.analyticsvidhya.com/blog/2020/12/improve-predictive-model-score-stacking-regressor/","metadata":{}},{"cell_type":"code","source":"full = pd.merge(left = df, right = y , left_index= True, right_index = True)\ntrain_y = full['SalePrice']\ntrain_X = full.drop(['SalePrice'],axis=1)\n\ndev_train, dev_test = train_test_split(full, test_size=0.2 ,shuffle=True)\ndev_train_y = dev_train['SalePrice']\ndev_train_X = dev_train.drop(['SalePrice'],axis=1)\ndev_test_y = dev_test['SalePrice']\ndev_test_X = dev_test.drop(['SalePrice'],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ridgemodel = Ridge(alpha=26)\n\nxgbmodel = XGBRegressor(alpha= 3, colsample_bytree=0.5, reg_lambda=3, learning_rate= 0.01,\\\n           max_depth=3, n_estimators=10000, subsample=0.65)\n\nsvrmodel = SVR(C=8, epsilon=0.00005, gamma=0.0008)\n\nhubermodel = HuberRegressor(alpha=30,epsilon=3,fit_intercept=True,max_iter=2000)\n\ncbmodel = cb.CatBoostRegressor(loss_function='RMSE',colsample_bylevel=0.3, depth=2, \\\n          l2_leaf_reg=20, learning_rate=0.005, n_estimators=15000, subsample=0.3,verbose=False)\n\nstackmodel = StackingCVRegressor(regressors=(ridgemodel, xgbmodel, svrmodel, hubermodel, cbmodel),\n             meta_regressor=cbmodel, use_features_in_secondary=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will fit the models onto development train and test data sets first to have a quick overview of the model performances.","metadata":{}},{"cell_type":"code","source":"start = time.time()\nprint(\"Recording Modelling Time\")\nfor i in [ridgemodel,hubermodel,cbmodel,svrmodel,xgbmodel,stackmodel]:\n    i.fit(train_X,train_y)\n    if i == stackmodel:\n        i.fit(np.array(dev_train_X), np.array(dev_train_y))\nend = time.time()\nprint(\"Time Elapsed: \" + str(round((end - start)/60,0)) +\"minutes.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"-----------------------------\")\nprint(\"Overview of model performance\")\nprint(\"-----------------------------\")\nfor i in [ridgemodel,hubermodel,cbmodel,svrmodel,xgbmodel,stackmodel]:\n    print(\"\\n\")\n    print(i)\n    print(\"RMSLE of Development train set: \")\n    print(mean_squared_error(dev_train_y,i.predict(dev_train_X), squared=False))\n    print(\"RMSLE of Development test set: \")\n    print(mean_squared_error(dev_test_y,i.predict(dev_test_X), squared=False))\n    print(\"\\n\")\nprint(\"-----------------------------\")\nprint(\"RMSLE of Development train set using ensemble model: \")\nfit = (svrmodel.predict(train_X) + xgbmodel.predict(train_X) +   stackmodel.predict(train_X) + ridgemodel.predict(train_X) + hubermodel.predict(train_X) + cbmodel.predict(train_X)) / 6\nprint(mean_squared_error(train_y,fit, squared=False))\nprint(\"-----------------------------\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This time we fit the models with all the data.","metadata":{}},{"cell_type":"code","source":"start = time.time()\nprint(\"Recording Modelling Time\")\nfor i in [ridgemodel,hubermodel,cbmodel,svrmodel,xgbmodel,stackmodel]:\n    i.fit(train_X,train_y)\n    if i == stackmodel:\n        i.fit(np.array(train_X), np.array(train_y))\nend = time.time()\nprint(\"Time Elapsed: \" + str(round((end - start)/60,0)) +\"minutes.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\nThe scores of the models are again averaged out.  \nThey are given different weight based on my confidence and experiences on using the models.  \nFeel free to modify the weights to obtain different results (maybe better).\n  \n---\nFinally, submit it onto Kaggle.  \n\nFinal score: **RMSLE of 0.11878** (Top 3% on Leaderboard)","metadata":{}},{"cell_type":"code","source":"final_prediction = (np.exp(ridgemodel.predict(test))+ 3 * np.exp(xgbmodel.predict(test)) \\\n+  5 * np.exp(stackmodel.predict(test)) + 4 * np.exp(svrmodel.predict(test)) \\\n+  np.exp(hubermodel.predict(test)) +  np.exp(cbmodel.predict(test))) / 15","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(final_prediction, index = test.index)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.reset_index(drop=False, inplace = True)\nsubmission = submission.rename(columns={0 : 'SalePrice', 'index' : 'Id'})\nsubmission.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Thanks!\nThanks for spending your precious time to read this kernel.  \nIf you have any questions, comments or suggestions feel free to comment. It would be much appreciated!","metadata":{}},{"cell_type":"markdown","source":"# References\nhttps://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard/notebook by **SERIGNE**  \nhttps://www.kaggle.com/humananalog/xgboost-lasso/script by **HUMAN ANALOG**","metadata":{}}]}