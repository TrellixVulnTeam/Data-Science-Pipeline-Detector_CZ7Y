{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Loading neccesary packages:\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n#\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler, QuantileTransformer, PowerTransformer, OrdinalEncoder, OneHotEncoder\n# experiment class\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n#\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\n#\n\nimport warnings\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-18T05:03:02.621931Z","iopub.execute_input":"2022-03-18T05:03:02.622356Z","iopub.status.idle":"2022-03-18T05:03:04.185344Z","shell.execute_reply.started":"2022-03-18T05:03:02.622248Z","shell.execute_reply":"2022-03-18T05:03:04.184333Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Meeting the data\nWe're going to start by loading the data and taking first look on it as usual. For the column names we have great dictionary file in our dataset location so we can get familiar with them in no time.","metadata":{}},{"cell_type":"code","source":"# Loading datasets.\n\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:04.187113Z","iopub.execute_input":"2022-03-18T05:03:04.187367Z","iopub.status.idle":"2022-03-18T05:03:04.263621Z","shell.execute_reply.started":"2022-03-18T05:03:04.187338Z","shell.execute_reply":"2022-03-18T05:03:04.262587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:04.265062Z","iopub.execute_input":"2022-03-18T05:03:04.265311Z","iopub.status.idle":"2022-03-18T05:03:04.35855Z","shell.execute_reply.started":"2022-03-18T05:03:04.265282Z","shell.execute_reply":"2022-03-18T05:03:04.357707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:04.455336Z","iopub.execute_input":"2022-03-18T05:03:04.456123Z","iopub.status.idle":"2022-03-18T05:03:04.578475Z","shell.execute_reply.started":"2022-03-18T05:03:04.456075Z","shell.execute_reply":"2022-03-18T05:03:04.577295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Id column looks useless we can safely drop it from both. I'm going to save our target (SalePrice) on different variable so we can use it in future.","metadata":{}},{"cell_type":"code","source":"# Dropping unnecessary Id column.\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:04.580023Z","iopub.execute_input":"2022-03-18T05:03:04.581045Z","iopub.status.idle":"2022-03-18T05:03:04.590292Z","shell.execute_reply.started":"2022-03-18T05:03:04.580999Z","shell.execute_reply":"2022-03-18T05:03:04.589085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Backing up target variables and dropping them from train data.\n\ny = train.SalePrice.reset_index(drop=True)\nX = train.drop('SalePrice', axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:04.591634Z","iopub.execute_input":"2022-03-18T05:03:04.592478Z","iopub.status.idle":"2022-03-18T05:03:04.608008Z","shell.execute_reply.started":"2022-03-18T05:03:04.592434Z","shell.execute_reply":"2022-03-18T05:03:04.607182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analysis Time!\nOk the short inspection at the beginning give us some hints how should we move from here. I'm going to play with the data we have while analysing the data at the same time. With this way I hope we can get the data in better shape while digging deeper into it.\n\nWe're going to start with basic correlation table here. I dropped the top part since it's just mirror of the other part below. With this table we can understand some linear relations between different features.\n\n### Observations:\n* There's strong relation between overall quality of the houses and their sale prices.\n* Again above grade living area seems strong indicator for sale price.\n* Garage features, number of baths and rooms, how old the building is etc. also having effect on the price on various levels too.\n* There are some obvious relations we gonna pass like total square feet affecting how many rooms there are or how many cars can fit into a garage vs. garage area etc.\n* Overall condition of the house seems less important on the pricing, it's interesting and worth digging.","metadata":{}},{"cell_type":"code","source":"# Display numerical correlations (pearson) between features on heatmap.\n\nsns.set(font_scale=1.1)\ncorrelation_train = train.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()\n\n# delete unnecesory varieble \ndel correlation_train, mask\n","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:04.609224Z","iopub.execute_input":"2022-03-18T05:03:04.609485Z","iopub.status.idle":"2022-03-18T05:03:07.944046Z","shell.execute_reply.started":"2022-03-18T05:03:04.609453Z","shell.execute_reply":"2022-03-18T05:03:07.943164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **I'm going to merge the datasets here before we start editing it so we don't have to do these operations twice. Let's call it features since it has features only. So our data has 2919 observations and 79 features to begin with...**","metadata":{}},{"cell_type":"code","source":"# Merging train test features for engineering.\n\nfeatures = pd.concat([X, test]).reset_index(drop=True)\nprint(features.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:07.945599Z","iopub.execute_input":"2022-03-18T05:03:07.946158Z","iopub.status.idle":"2022-03-18T05:03:07.977811Z","shell.execute_reply.started":"2022-03-18T05:03:07.946112Z","shell.execute_reply":"2022-03-18T05:03:07.977062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Data\nAlright, first of all we need detect missing values, then wee need to get rid of them for the next steps of our work. So let's list our missing values and visualize them:","metadata":{}},{"cell_type":"code","source":"def missing_percentage(df):\n    \n    \"\"\"A function for returning missing ratios.\"\"\"\n    total = df.isnull().sum().sort_values(ascending=False)\n    \n    return pd.concat([total, (total / len(df) * 100)], axis=1, keys=['Total', 'Percent'])[total!=0]","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:07.979008Z","iopub.execute_input":"2022-03-18T05:03:07.97939Z","iopub.status.idle":"2022-03-18T05:03:07.985284Z","shell.execute_reply.started":"2022-03-18T05:03:07.979357Z","shell.execute_reply":"2022-03-18T05:03:07.984184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **That's quite a lot! No need to panic though we got this. If you look at the data description given to us we can see that most of these missing data actually not missing, it's just means house doesn't have that specific feature, we can fix that easily...**","metadata":{}},{"cell_type":"code","source":"# Checking 'NaN' values.\n\nmissing = missing_percentage(features)\n\nfig, ax = plt.subplots(figsize=(20, 7))\nsns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\nplt.xticks(rotation=90)\n\ndisplay(missing.T.style.background_gradient(cmap='Reds', axis=1))\n\ndel missing","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-18T05:03:07.986564Z","iopub.execute_input":"2022-03-18T05:03:07.987291Z","iopub.status.idle":"2022-03-18T05:03:08.760275Z","shell.execute_reply.started":"2022-03-18T05:03:07.987253Z","shell.execute_reply":"2022-03-18T05:03:08.759283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ok this is how we gonna fix most of the missing data:\n1. First we fill the NaN's in the columns where they mean 'None' so we gonna replace them with that,\n2. Then we fill numerical columns where missing values indicating there is no parent feature to measure, so we replace them with 0's.\n3. Even with these there are some actual missing data, by checking general trends of these features we can fill them with most frequent value(with mode).\n4. MSZoning part is little bit tricky I choose to fill them with most common type of the related MSSubClass type. It's not perfect but at least we decrease randomness a little bit.\n4. Again we fill the Lot Frontage with similar approach.","metadata":{}},{"cell_type":"code","source":"# List of 'NaN' including columns where NaN's mean's none.\n\nnone_cols = [\n    'Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n    'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'\n]\n\n# List of 'NaN' including columns where NaN's mean's 0.\n\nzero_cols = [\n    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n    'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'\n]\n\n# List of 'NaN' including columns where NaN's actually missing gonna replaced with mode.\n\nmost_cols = [\n    'Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual',\n    'SaleType', 'Utilities', 'MSZoning'\n]\n\n# Multivariate feature imputation method\n# for ref => https://scikit-learn.org/stable/modules/impute.html#univariate-feature-imputation\n\nregg_cols = ['LotFrontage']#,'MSZoning']\n","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:08.761742Z","iopub.execute_input":"2022-03-18T05:03:08.762143Z","iopub.status.idle":"2022-03-18T05:03:08.770197Z","shell.execute_reply.started":"2022-03-18T05:03:08.76209Z","shell.execute_reply":"2022-03-18T05:03:08.769336Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# impute missing value with Column transformer\n\nmissing_value_preprocessor = ColumnTransformer(\n    transformers=[\n        # imputation\n        ('none_imputer', SimpleImputer(fill_value= 'none', strategy='constant'), none_cols),\n        ('zero_imputer', SimpleImputer(fill_value= 0, strategy='constant'), zero_cols),\n        ('most_imputer', SimpleImputer(strategy='most_frequent'), most_cols),\n        # experimental class imputation => Multivariate feature imputation\n        ('regg_features', IterativeImputer(max_iter=10, random_state=0), regg_cols),\n    ],\n    remainder = 'passthrough',\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:08.771619Z","iopub.execute_input":"2022-03-18T05:03:08.772141Z","iopub.status.idle":"2022-03-18T05:03:08.783761Z","shell.execute_reply.started":"2022-03-18T05:03:08.7721Z","shell.execute_reply":"2022-03-18T05:03:08.782853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_tr_col = none_cols + zero_cols + most_cols\ncol_tr_col = col_tr_col + [i for i in features.columns if i not in col_tr_col]\n\nfeatures = pd.DataFrame(\n    missing_value_preprocessor.fit_transform(features),\n    columns = col_tr_col)\nfeatures = features.convert_dtypes()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:08.787561Z","iopub.execute_input":"2022-03-18T05:03:08.78808Z","iopub.status.idle":"2022-03-18T05:03:08.96029Z","shell.execute_reply.started":"2022-03-18T05:03:08.788043Z","shell.execute_reply":"2022-03-18T05:03:08.959376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\nOk this is the part where we dig deeper into our completed dataset. There are no missing values so we're good to go! I'm going to start with grouping some values, these values are really rare and I'm thinking they do not add much, so if they appear less than 10 times in our observations they get into 'Other' group.","metadata":{}},{"cell_type":"code","source":"# Transforming rare values(less than 10) into one group.\n\nothers = [\n    'Condition1', 'Condition2', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n    'Heating', 'Electrical', 'Functional', 'SaleType'\n]\n\nfor col in others:\n    mask = features[col].isin(\n        features[col].value_counts()[features[col].value_counts() < 10].index)\n    features[col][mask] = 'Other'","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:08.961743Z","iopub.execute_input":"2022-03-18T05:03:08.962022Z","iopub.status.idle":"2022-03-18T05:03:09.022892Z","shell.execute_reply.started":"2022-03-18T05:03:08.961965Z","shell.execute_reply":"2022-03-18T05:03:09.022022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_box(y, df):\n    \n    '''A function for displaying categorical variables.'''\n    \n    fig, axes = plt.subplots(14, 3, figsize=(25, 80))\n    axes = axes.flatten()\n    \n    for i, j in zip(df.select_dtypes(include=['object']).columns, axes):\n        \n        sortd = df.groupby([i])[y].median().sort_values(ascending=False)\n        sns.boxplot(x=i,\n                    y=y,\n                    data=df,\n                    palette='plasma',\n                    order=sortd.index,\n                    ax=j)\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=18))\n\n        plt.tight_layout()\n\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-18T05:03:09.024362Z","iopub.execute_input":"2022-03-18T05:03:09.024803Z","iopub.status.idle":"2022-03-18T05:03:09.03462Z","shell.execute_reply.started":"2022-03-18T05:03:09.024759Z","shell.execute_reply":"2022-03-18T05:03:09.033877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Categorical Data\nWe already checked some of the numerical features with correlation heatmap but what about categorical values? We want to see relations between categorical data and sale price. Boxplots seems decent way to inspect this type of relation. We're also going to sort them by the median value of that group so we can see the importances in descending order.\n","metadata":{}},{"cell_type":"code","source":"# Displaying sale prices vs. categorical values:\n\nshow_box('SalePrice', train)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:03:09.035976Z","iopub.execute_input":"2022-03-18T05:03:09.036256Z","iopub.status.idle":"2022-03-18T05:04:09.272827Z","shell.execute_reply.started":"2022-03-18T05:03:09.036226Z","shell.execute_reply":"2022-03-18T05:04:09.271727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoding categorical features","metadata":{}},{"cell_type":"code","source":"# Converting some of the categorical values to numeric ones.\nordi_encode_cols = ['Neighborhood', 'ExterQual', 'ExterCond','BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageCond', 'GarageQual']\n\ncategorical_value_preprocessor = ColumnTransformer(\n    transformers=[('ordinal_encoder', OrdinalEncoder(), ordi_encode_cols)],\n    remainder = 'passthrough',\n)\n\nfeatures = pd.DataFrame(categorical_value_preprocessor.fit_transform(features),\n             columns = (ordi_encode_cols + [i for i in features.columns if i not in ordi_encode_cols]))\nfeatures = features.convert_dtypes()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:04:09.274184Z","iopub.execute_input":"2022-03-18T05:04:09.274461Z","iopub.status.idle":"2022-03-18T05:04:09.418618Z","shell.execute_reply.started":"2022-03-18T05:04:09.274427Z","shell.execute_reply":"2022-03-18T05:04:09.417615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Numeric Data\nThere are many numeric features the inspect, one of the best ways to see how they effect sale prices is scatter plots. We're also plotting polynomial regression lines to see general trend. With this way we can understand the numerical values and their importance on sale price, also it's really helpful to spot outliers.\n\n### Observations:\n* OverallQual; It's clearly visible that sale price of the house increases with overall quality. This confirms the correlation in first table we did at the beginning. (Pearson corr was 0.8)\n\n* OverallCondition; Looks like overall condition is left skewed where most of the houses are around 5/10 condition. But it doesn't effect the price like quality indicator...\n\n* YearBuilt; Again new buildings are generally expensive than the old ones.\n\n* Basement; General table shows bigger basements are increasing the price but I see some outliers there...\n\n* GrLivArea; This feature is pretty linear but we can spot two outliers effecting this trend. There are some huge area houses with pretty cheap prices, there might be some reason behind it but we better drop them.\n\n* SaleDates; They seem pretty unimportant on sale prices, we can drop them...","metadata":{}},{"cell_type":"code","source":"# Plotting numerical features with polynomial order to detect outliers by eye.\n\ndef show_reg(y, df):\n    fig, axes = plt.subplots(12, 3, figsize=(25, 80))\n    axes = axes.flatten()\n    \n    for i, j in zip(df.select_dtypes(include=['number']).columns, axes):\n\n        sns.regplot(x=i,\n                    y=y,\n                    data=df,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()\n\nshow_reg('SalePrice', train)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:04:09.419973Z","iopub.execute_input":"2022-03-18T05:04:09.420267Z","iopub.status.idle":"2022-03-18T05:04:54.019915Z","shell.execute_reply.started":"2022-03-18T05:04:09.420235Z","shell.execute_reply":"2022-03-18T05:04:54.01913Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outliers\nOk here we're going to drop some outliers we detected them just above, this part is kinda subjective and can try different approaches or implement some automatic outlier detection methods like isolation forests.","metadata":{}},{"cell_type":"code","source":"# Dropping outliers after detecting them by eye.\n\nfeatures = features.join(y)\n\nfeatures = features.drop(features[(features['GrLivArea'] > 4000)\n                                  & (features['SalePrice'] < 200000)].index)\nfeatures = features.drop(features[(features['GarageArea'] > 1200)\n                                  & (features['SalePrice'] < 250000)].index)\nfeatures = features.drop(features[(features['TotalBsmtSF'] > 3000)\n                                  & (features['SalePrice'] < 320000)].index)\nfeatures = features.drop(features[(features['1stFlrSF'] < 3000)\n                                  & (features['SalePrice'] > 600000)].index)\nfeatures = features.drop(features[(features['1stFlrSF'] > 3000)\n                                  & (features['SalePrice'] < 200000)].index)\n\ny = features['SalePrice']\ny.dropna(inplace=True)\nfeatures.drop(columns='SalePrice', inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:04:54.021043Z","iopub.execute_input":"2022-03-18T05:04:54.021853Z","iopub.status.idle":"2022-03-18T05:04:54.126162Z","shell.execute_reply.started":"2022-03-18T05:04:54.021804Z","shell.execute_reply":"2022-03-18T05:04:54.125228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating New Features\nOk in this part we going to create some features, these can improve our modelling. I went with basic approach by merging some important indicators and making them stronger.","metadata":{}},{"cell_type":"code","source":"# Creating new features  based on previous observations. There might be some highly correlated features now. You cab drop them if you want to...\n\nfeatures['TotalSF'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                       features['1stFlrSF'] + features['2ndFlrSF'])\nfeatures['TotalBathrooms'] = (features['FullBath'] +\n                              (0.5 * features['HalfBath']) +\n                              features['BsmtFullBath'] +\n                              (0.5 * features['BsmtHalfBath']))\n\nfeatures['TotalPorchSF'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                            features['EnclosedPorch'] +\n                            features['ScreenPorch'] + features['WoodDeckSF'])\n\nfeatures['YearBlRm'] = (features['YearBuilt'] + features['YearRemodAdd'])\n\n# Merging quality and conditions.\n\nfeatures['TotalExtQual'] = (features['ExterQual'] + features['ExterCond'])\nfeatures['TotalBsmQual'] = (features['BsmtQual'] + features['BsmtCond'] +\n                            features['BsmtFinType1'] +\n                            features['BsmtFinType2'])\nfeatures['TotalGrgQual'] = (features['GarageQual'] + features['GarageCond'])\nfeatures['TotalQual'] = features['OverallQual'] + features[\n    'TotalExtQual'] + features['TotalBsmQual'] + features[\n        'TotalGrgQual'] + features['KitchenQual'] + features['HeatingQC']\n\n# Creating new features by using new quality indicators.\n\nfeatures['QualGr'] = features['TotalQual'] * features['GrLivArea']\nfeatures['QualBsm'] = features['TotalBsmQual'] * (features['BsmtFinSF1'] +\n                                                  features['BsmtFinSF2'])\nfeatures['QualPorch'] = features['TotalExtQual'] * features['TotalPorchSF']\nfeatures['QualExt'] = features['TotalExtQual'] * features['MasVnrArea']\nfeatures['QualGrg'] = features['TotalGrgQual'] * features['GarageArea']\nfeatures['QlLivArea'] = (features['GrLivArea'] -\n                         features['LowQualFinSF']) * (features['TotalQual'])\nfeatures['QualSFNg'] = features['QualGr'] * features['Neighborhood']\n","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:04:54.127546Z","iopub.execute_input":"2022-03-18T05:04:54.12779Z","iopub.status.idle":"2022-03-18T05:04:54.16231Z","shell.execute_reply.started":"2022-03-18T05:04:54.127759Z","shell.execute_reply":"2022-03-18T05:04:54.161371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Observing the effects of newly created features on sale price.\n\ndef srt_reg():\n    fig, axes = plt.subplots(5, 3, figsize=(25, 40))\n    axes = axes.flatten()\n\n    new_features = [\n        'TotalSF', 'TotalBathrooms', 'TotalPorchSF', 'YearBlRm',\n        'TotalExtQual', 'TotalBsmQual', 'TotalGrgQual', 'TotalQual', 'QualGr',\n        'QualBsm', 'QualPorch', 'QualExt', 'QualGrg', 'QlLivArea', 'QualSFNg'\n    ]\n    merged = features.join(y)\n    merged = merged[new_features+['SalePrice']].astype('float')\n\n    for i, j in zip(new_features, axes):\n\n        sns.regplot(x=i,\n                    y='SalePrice',\n                    data=merged,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-18T05:04:54.164072Z","iopub.execute_input":"2022-03-18T05:04:54.164407Z","iopub.status.idle":"2022-03-18T05:04:54.174794Z","shell.execute_reply.started":"2022-03-18T05:04:54.164362Z","shell.execute_reply":"2022-03-18T05:04:54.173766Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking New Features\nWell... They look decent enough, I hope these can help us building strong models. I also wanted to add some more basic features for having specific feature or not. This approach was widely accepted by community so I see no harm to add them.","metadata":{}},{"cell_type":"code","source":"srt_reg()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:04:54.176517Z","iopub.execute_input":"2022-03-18T05:04:54.177479Z","iopub.status.idle":"2022-03-18T05:05:04.447108Z","shell.execute_reply.started":"2022-03-18T05:04:54.177437Z","shell.execute_reply":"2022-03-18T05:05:04.446396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating some simple features.\n\nfeatures['HasPool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['Has2ndFloor'] = features['2ndFlrSF'].apply(lambda x: 1\n                                                     if x > 0 else 0)\nfeatures['HasGarage'] = features['QualGrg'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasBsmt'] = features['QualBsm'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasFireplace'] = features['Fireplaces'].apply(lambda x: 1\n                                                        if x > 0 else 0)\nfeatures['HasPorch'] = features['QualPorch'].apply(lambda x: 1 if x > 0 else 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:04.448242Z","iopub.execute_input":"2022-03-18T05:05:04.448586Z","iopub.status.idle":"2022-03-18T05:05:04.476193Z","shell.execute_reply.started":"2022-03-18T05:05:04.448555Z","shell.execute_reply":"2022-03-18T05:05:04.475358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Here we dropping some unnecessary features had their use in feature engineering or not needed at all. Obviously it's subjective but I feel they don't add much to model. Then we one hot encode the categorical data left so everything will be prepared for the modelling.**","metadata":{}},{"cell_type":"code","source":"# Features to drop:\n\nto_drop = [\n    'Utilities',\n    'PoolQC',\n    'YrSold',\n    'MoSold',\n    'ExterQual',\n    'BsmtQual',\n    'GarageQual',\n    'KitchenQual',\n    'HeatingQC',\n]\n\n# Dropping features.\n\nfeatures.drop(columns=to_drop, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:04.477782Z","iopub.execute_input":"2022-03-18T05:05:04.478317Z","iopub.status.idle":"2022-03-18T05:05:04.486737Z","shell.execute_reply.started":"2022-03-18T05:05:04.478277Z","shell.execute_reply":"2022-03-18T05:05:04.485934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OneHotEncoding","metadata":{}},{"cell_type":"code","source":"features = pd.get_dummies(data=features,columns= features.select_dtypes('string').columns)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:04.488458Z","iopub.execute_input":"2022-03-18T05:05:04.489234Z","iopub.status.idle":"2022-03-18T05:05:04.561657Z","shell.execute_reply.started":"2022-03-18T05:05:04.489133Z","shell.execute_reply":"2022-03-18T05:05:04.560864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transforming the Data\nSome of the continious values are not distributed evenly and not fitting on normal distribution, we can fix them by using couple transformation approaches. We're going to use power tranformer here, again it's widely used by community and I want to thank them all for their great work.","metadata":{}},{"cell_type":"code","source":"# Numerical features we worked on which seems highly skewed but we filter again anyways...\n\nskewed = [\n    'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n    'ScreenPorch', 'PoolArea', 'LowQualFinSF', 'MiscVal'\n]","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:04.562927Z","iopub.execute_input":"2022-03-18T05:05:04.56334Z","iopub.status.idle":"2022-03-18T05:05:04.568308Z","shell.execute_reply.started":"2022-03-18T05:05:04.563306Z","shell.execute_reply":"2022-03-18T05:05:04.567409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npower_column_preprocessor = ColumnTransformer(\n    transformers=[('power_tranform', PowerTransformer(standardize=False), skewed)],\n    remainder = 'passthrough',\n)\n\nfeatures = pd.DataFrame(power_column_preprocessor.fit_transform(features),\n             columns = (skewed + [i for i in features.columns if i not in skewed]))","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:04.569488Z","iopub.execute_input":"2022-03-18T05:05:04.569902Z","iopub.status.idle":"2022-03-18T05:05:04.737931Z","shell.execute_reply.started":"2022-03-18T05:05:04.569857Z","shell.execute_reply":"2022-03-18T05:05:04.736907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.regplot(x='QualGr',y='SalePrice',data=features.join(y).astype('float'))\n#features.convert_dtypes()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:04.739351Z","iopub.execute_input":"2022-03-18T05:05:04.739639Z","iopub.status.idle":"2022-03-18T05:05:05.287799Z","shell.execute_reply.started":"2022-03-18T05:05:04.739599Z","shell.execute_reply":"2022-03-18T05:05:05.286997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Double Check\nBefore we move to modelling I want to take one last look to the data we processed. Everyting seems in order, not missing datas, values are numerical etc. Our feature engineered data is present...\n\nJust want to check how transformed data correlates with sale prices before we move on and it looks decent.\n\nAgain I wanted to check our target value distribution and it seems little skewed. We can fix this by applying log transformation so our models can perform better.","metadata":{}},{"cell_type":"code","source":"# Separating train and test set.\n\ntrain = features.iloc[:len(y), :]\ntest = features.iloc[len(train):, :]","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:05.289104Z","iopub.execute_input":"2022-03-18T05:05:05.289847Z","iopub.status.idle":"2022-03-18T05:05:05.295857Z","shell.execute_reply.started":"2022-03-18T05:05:05.289795Z","shell.execute_reply":"2022-03-18T05:05:05.294937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = train.join(y).astype('float')\ncorrelations = a.corrwith(a['SalePrice']).iloc[:-1].to_frame()\ncorrelations['Abs Corr'] = correlations[0].abs()\ncorrelations.sort_values('Abs Corr',ascending=False).head()\n","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:05.297198Z","iopub.execute_input":"2022-03-18T05:05:05.297773Z","iopub.status.idle":"2022-03-18T05:05:05.415438Z","shell.execute_reply.started":"2022-03-18T05:05:05.297726Z","shell.execute_reply":"2022-03-18T05:05:05.414508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling\nWell then, it's time to do some modelling! First of all I wanted to thank kaggle community for loads of examples inspired me. Especially Alex Lekov's great script and Serigne's stacked regressions approach were great guides for me!\n\nLet's start with loading packages needed and then we set our regressors. The regressors I'm going to use here are:\n\nRidge,\nLasso,\nElasticnet,\nSupport Vector Regression\nI'm going to apply robust scaler on these before we run them because they really get effected by outliers.\nGradient Boosting Regressor\nLightGBM Regressor\nXGBoost Regressor\nThese don't need scaling in my opinion so we just go as it is\nHist Gradient Boosting Regressor\nThis is just for experimenting, it's still experimental on sklearn anyways\nTweedie Regressor\nThis regressor added in latest version of sklearn and I wanted to try it. It's generalized linear model with a Tweedie distribution. We gonna use power of 0 because we expecting normal target distribution but you can try this or other generalized models like poisson regressor or gamma regressor.\nI tried to tune models by using Optuna package, that part is not added here.","metadata":{}},{"cell_type":"code","source":"# Loading neccesary packages for modelling.\n\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor # This is for stacking part, works well with sklearn and others...","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:05.416806Z","iopub.execute_input":"2022-03-18T05:05:05.41706Z","iopub.status.idle":"2022-03-18T05:05:06.946843Z","shell.execute_reply.started":"2022-03-18T05:05:05.417029Z","shell.execute_reply":"2022-03-18T05:05:06.945652Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.compose import TransformedTargetRegressor\n\ntt = TransformedTargetRegressor(regressor=SVR(),\n                                func=np.log, inverse_func=np.exp)\n\n\nkf = KFold(10, random_state=42)\n# Some parameters for ridge, lasso and elasticnet.\n\nalphas_alt = [0.01, 13, 15.5, 15.6, 15.7, 15.8, 15.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [\n    0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.001\n]\ne_alphas = [\n    0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007\n]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# ridge_cv\n\nridge = make_pipeline(RobustScaler(), RidgeCV(\n    alphas=alphas_alt,\n    cv=kf,\n))\n\n# lasso_cv:\n\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kf))\n\n# elasticnet_cv:\n\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7,\n                 alphas=e_alphas,\n                 cv=kf,\n                 random_state=42,\n                 l1_ratio=e_l1ratio))\n\n# svr\n\nsvr = make_pipeline(\n    RobustScaler(),\n    SVR(\n        C=20,\n        gamma=0.00017,\n))\n\n\ngbr = GradientBoostingRegressor(n_estimators=2900,\n                                learning_rate=0.0161,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=17,\n                                loss='huber',\n                                random_state=42)\n\n# lightgbm:\n\nlightgbm = LGBMRegressor(objective='regression',\n                         learning_rate=0.00721,\n                         )\n'''n_estimators=3500,\n                         num_leaves=5,\n                         max_bin=163,\n                         bagging_fraction=0.35711,\n                         n_jobs=-1,\n                         bagging_seed=42,\n                         feature_fraction_seed=42,\n                         bagging_freq=7,\n                         feature_fraction=0.1294,\n                         min_data_in_leaf=8)\n'''\n# xgboost:\n\nxgboost = XGBRegressor(\n    learning_rate=0.0139,\n    )\n'''n_estimators=4500,\n    max_depth=4,\n    min_child_weight=0,\n    subsample=0.7968,\n    colsample_bytree=0.4064,\n    nthread=-1,\n    scale_pos_weight=2,\n    seed=42,\n)'''\n\n\n\n# stacking regressor:\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr,\n                                            xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\n\nscores = cross_validate(\n    lasso,\n    train,\n    np.log1p(y),\n    cv=kf,\n    scoring=['r2','neg_root_mean_squared_error'],\n    return_train_score=True,\n    n_jobs=-1)\n\npd.DataFrame(scores).mean()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:06.948537Z","iopub.execute_input":"2022-03-18T05:05:06.9488Z","iopub.status.idle":"2022-03-18T05:05:11.370943Z","shell.execute_reply.started":"2022-03-18T05:05:06.948768Z","shell.execute_reply":"2022-03-18T05:05:11.370144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_check(X, y, estimators, cv):\n    \n    ''' A function for testing multiple estimators.'''\n    \n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        MLA_name = label\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test RMSE'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:11.372601Z","iopub.execute_input":"2022-03-18T05:05:11.373113Z","iopub.status.idle":"2022-03-18T05:05:11.38182Z","shell.execute_reply.started":"2022-03-18T05:05:11.373077Z","shell.execute_reply":"2022-03-18T05:05:11.381014Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting list of estimators and labels for them:\n\nestimators = [ridge, lasso, elasticnet, gbr, svr,\n             # xgboost, lightgbm\n             ]\nlabels = [\n    'Ridge', 'Lasso', 'Elasticnet', 'GradientBoostingRegressor',\n    'SVR', #'XGBRegressor', 'LGBMRegressor', \n]","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:11.383011Z","iopub.execute_input":"2022-03-18T05:05:11.383278Z","iopub.status.idle":"2022-03-18T05:05:11.397349Z","shell.execute_reply.started":"2022-03-18T05:05:11.383247Z","shell.execute_reply":"2022-03-18T05:05:11.396338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Results\nAllright, our results are here. Looks like our models did pretty close to each other, there might be some overfitting models and we can try to fix them by tuning but it was computationally expensive for me and since I'm going to stack and blend the models I think we can leave them as it is. We already added our models to stacking regression and set the XGBoost as meta regressor we can continue with stacking","metadata":{}},{"cell_type":"code","source":"# Executing cross validation.\n\nraw_models = model_check(train, np.log1p(y), estimators, kf)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","metadata":{"execution":{"iopub.status.busy":"2022-03-18T05:05:11.399349Z","iopub.execute_input":"2022-03-18T05:05:11.400037Z","iopub.status.idle":"2022-03-18T05:06:21.302804Z","shell.execute_reply.started":"2022-03-18T05:05:11.399953Z","shell.execute_reply":"2022-03-18T05:06:21.30213Z"},"trusted":true},"execution_count":null,"outputs":[]}]}