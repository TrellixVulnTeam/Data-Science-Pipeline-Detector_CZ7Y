{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"478db2ba-35d9-d698-469c-0820f22ddd91"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3c5eb3f-8b65-db5a-fab5-aa229039e6d3"},"outputs":[],"source":"# Next, load the train and test datasets available in the \"../input/\" directory\ntrain = pd.read_csv(\"../input/train.csv\") # the train dataset is now a Pandas DataFrame\ntest = pd.read_csv(\"../input/test.csv\") # the train dataset is now a Pandas DataFrame\n\n# Let's have a peek of the train data\ntrain.head()"},{"cell_type":"markdown","metadata":{"_cell_guid":"ccc28ba2-82c4-ecdd-518a-3135a16a2252"},"source":"**Data size**\n\nLet's start with the most basic exploration of the dataset: get the number of attributes (features) and instances (data points):\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5368f139-a4dc-c291-0809-012b966b19f3"},"outputs":[],"source":"instance_count, attr_count = train.shape\nprint('Number of instances: ', instance_count)\nprint('Number of features:', attr_count)"},{"cell_type":"markdown","metadata":{"_cell_guid":"96190335-94f6-7a5f-77e8-02380bfc618e"},"source":"The features can be split into input ones and target ones. In this case there's just one target (SalePrice) and several inputs."},{"cell_type":"markdown","metadata":{"_cell_guid":"53d2f24f-3ee6-9022-1449-38b42029e987"},"source":"## Distributions of each attribute ##\n\nWe'll start by exploring our dataset attributes:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6ebb8373-1229-9617-b23c-065c5b55e159"},"outputs":[],"source":"# View the columns\ntrain.columns"},{"cell_type":"markdown","metadata":{"_cell_guid":"434b00c6-7417-a331-7bb3-99bf2b5dd0a1"},"source":"Next we'd like to know how values of each attributes are distributed. We can readily use the basic statistics (`count, mean, min, max, quartiles`) via the pandas `df.describe()`"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"633f1ad2-dcf1-8bd7-b3d6-bee12b6aed26"},"outputs":[],"source":"# some statistical overview\n\ntrain.describe()"},{"cell_type":"markdown","metadata":{"_cell_guid":"9e49da1d-9c7a-edfe-4b5a-0633b01be410"},"source":"## Missing values ##"},{"cell_type":"markdown","metadata":{"_cell_guid":"695ded6f-b719-e3c3-8c26-3ebb17315c5d"},"source":"As we can see from the above cell we have some missing values in some of the columns. In pandas missing values are represented by `np.NaN`. The `pd.isnull(df).any()` command tells us whether each column contains any missing values, `pd.isnull(df).sum()` then counts the missing values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e372d774-e294-7e7e-fa1c-9ac7457c83ed"},"outputs":[],"source":"# Check for missing values\npd.isnull(train).any()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"90c12d7b-28fd-f8f3-86a7-08589d9b1f31"},"outputs":[],"source":"# Count missing values in training data set\npd.isnull(train).sum()"},{"cell_type":"markdown","metadata":{"_cell_guid":"cf14d494-45f0-08f0-65a6-f7af17044954"},"source":"**Filling in Missing Data**\n\nLet's call the `fillna` method to fill in the missing data with the column averages. \n\nFirst let's view the mean"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3e842c6d-e301-681e-2257-8f6641773b0a"},"outputs":[],"source":"train.mean()"},{"cell_type":"markdown","metadata":{"_cell_guid":"01ef1794-5387-3a53-d705-622426137d6a"},"source":"Let's fill in the \"holes\" with the means on numerical attributes"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1e8a1d0b-fe05-7799-6baa-a4e31985ca47"},"outputs":[],"source":"train.fillna(train.mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"757169f5-269b-ad30-cae9-c3ed259702e8"},"source":"## Corelations between attributes ##"},{"cell_type":"markdown","metadata":{"_cell_guid":"5c7828cc-59ea-f41a-20f2-9318ee58f55c"},"source":"By definition, a correlation is a statistical measure that indicates the extent to which two or more variables fluctuate together. A positive correlation indicates the extent to which those variables increase or decrease in parallel; a negative correlation indicates the extent to which one variable increases as the other decreases.\n\nIn our dataset, it is crucial to have a better understanding of the underlying structure and characteristics of the data and leads to better intuition in knowing whether some pairs of attributes are correlated and how much. For many ML algorithms correlated features might make some trouble, ideally we should try to get a set of independent features."},{"cell_type":"markdown","metadata":{"_cell_guid":"d7317c31-7653-7bb5-c567-9405dcf66390"},"source":"We can use Pandas `DataFrame.corr()` function to get the three various correlation coefficients: standard Pearson correlation coefficient, Spearman rank correlation, Kendall Tau correlation coefficient. "},{"cell_type":"markdown","metadata":{"_cell_guid":"4789b4ca-cc23-5b66-4fdf-c3c74364fdc6"},"source":"**Pearson correlation coefficient**\n\nOne of the simplest method for understanding a feature’s relation to the response variable is Pearson correlation coefficient, which measures linear correlation between two variables. The resulting value lies in [-1;1], with -1 meaning perfect negative correlation (as one variable increases, the other decreases), +1 meaning perfect positive correlation and 0 meaning no linear correlation between the two variables."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1aaf95f7-406f-bc21-6802-4810a43e842d"},"outputs":[],"source":"pearson = train.corr(method='pearson')\npearson"},{"cell_type":"markdown","metadata":{"_cell_guid":"5c2727d6-545c-20d0-b059-fe2692c64225"},"source":"We'd like to know how each input attribute is able to predict the target i.e. `SalePrice` and this is called predictivity i.e. the correlation between input attributes and the target one:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"76702a56-b655-74aa-8c5f-b9a127c5ce54"},"outputs":[],"source":"# Since the target attr is the last, remove corr with itself\ncorr_with_target = pearson.ix[-1][:-1]\n\ncorr_with_target_dict = corr_with_target.to_dict()\n\n# List the attributes sorted from the most predictive by their correlation with Sale Price\nprint(\"FEATURE \\tCORRELATION\")\nfor attr in sorted(corr_with_target_dict.items(), key = lambda x: -abs(x[1])):\n    print(\"{0}: \\t{1}\".format(*attr))"},{"cell_type":"markdown","metadata":{"_cell_guid":"72a81b2d-8cd1-965b-ee91-2bd5965cc449"},"source":"We might also be interested in strong negative correlations it would be better to sort the correlations by the absolute value:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"57be11bf-a595-f8d6-85c4-ce5af088eb63"},"outputs":[],"source":"corr_with_target[abs(corr_with_target).argsort()[::1]]"},{"cell_type":"markdown","metadata":{"_cell_guid":"249b27d2-2eb6-8704-3838-7c4a00e0b8e1"},"source":"It would also be interesting to understand strong correlations between attribute pairs."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7b69100d-295b-dc24-d517-abad7cb77f8f"},"outputs":[],"source":"attrs = pearson.iloc[:-1,:-1] # all except target\n# only important correlations and not auto-correlations\nthreshold = 0.5\n# {(YearBuilt, YearRemodAdd): 0.592855, (1stFlrSF, GrLivArea): 0.566024, ...\nimportant_corrs = (attrs[abs(attrs) > threshold][attrs != 1.0]) \\\n    .unstack().dropna().to_dict()\n#     attribute pair                   correlation\n# 0     (OverallQual, TotalBsmtSF)     0.537808\n# 1     (GarageArea, GarageCars)\t   0.882475\n# ...\nunique_important_corrs = pd.DataFrame(\n    list(set([(tuple(sorted(key)), important_corrs[key]) \\\n    for key in important_corrs])), columns=['Attribute Pair', 'Correlation'])\n# sorted by absolute value\nunique_important_corrs = unique_important_corrs.ix[\n    abs(unique_important_corrs['Correlation']).argsort()[::-1]]\n\nunique_important_corrs"},{"cell_type":"markdown","metadata":{"_cell_guid":"26fe9e28-b4ce-746c-5aa9-0953b1ad46fe"},"source":"## Visualisation ##"},{"cell_type":"markdown","metadata":{"_cell_guid":"f32ff1db-72eb-9268-304c-37ee5ed612d4"},"source":" Let's promote the above correlations with some visualisations. This also enables us to grasp difficult concepts or identify new patterns easily through some graphical representations of the different statistical inferences.\n\nSome good Python packages that we can use for plotting the above exist such as the standard matplotlib package and additionally seaborn for some extra statistical plots and for more elegant and comprehensible plot styles."},{"cell_type":"markdown","metadata":{"_cell_guid":"97f1481b-6011-b35d-1478-72907c8a84df"},"source":"Diagonal Correlation Matrix\n\nLet's start by visualising the value of correlation of pairs of attributes, ie. a 2D matrix:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4e2648d3-9867-417e-0f10-5ab5d029cfba"},"outputs":[],"source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(pearson, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 12))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(pearson, mask=mask, cmap=cmap, vmax=.3,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a454c940-8a8a-8c14-fc92-ffcd726263a7"},"source":"Now, let's visualize an estimation of the probability density function to get a better understanding of how values of each attribtue look like.\n\nWe can use a simple means as an intial pdf estimation and the standard `hist()` method from matplotlib will suffice:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5277da0-034c-2abd-784e-c22896b7c40d"},"outputs":[],"source":"target = train['SalePrice']\nplt.hist(target, bins=50)"},{"cell_type":"markdown","metadata":{"_cell_guid":"a741ab26-efc0-1994-59b7-785841d14b1e"},"source":"For a better distribution plot, we can use the Seaborn package's distplot() method, which offers  a smoothed histogram with a kernel density estimation (KDE) plot as a single plot:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"71820aca-ae66-506d-cac0-a2aea7a84cd1"},"outputs":[],"source":"sns.distplot(target)"},{"cell_type":"markdown","metadata":{"_cell_guid":"23fe3eee-0c69-5cc2-03aa-53995be6b0d8"},"source":"Going further with our plots, we would like to explore the attribute pair correlations by plotting a 2D plot with each axis representing the particular attribute range and the points on the plot representing the probability that both attributes have the particular values at once:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8d672817-7682-48c8-d6e1-7a626887abd1"},"outputs":[],"source":"# Scatter Plot\nx, y = train['YearBuilt'], train['SalePrice']\nplt.scatter(x, y, alpha=0.5)\n\n# or via jointplot (with histograms aside):\nsns.jointplot(x, y, kind='scatter', joint_kws={'alpha':0.5})"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f01d82a2-4f7d-70a2-b360-139072a8f6b1"},"outputs":[],"source":"# Hexagonal 2-D plot\nsns.jointplot(x, y, kind='hex')"},{"cell_type":"markdown","metadata":{"_cell_guid":"7bff3a76-84f5-2183-b6ea-6a8619ac7ed1"},"source":"We can also estimate the PDF smoothly by convolving each datapoint with a kernel function via the Seaborn `kdeplot()` method:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0684dfda-b048-0860-fc39-a371f38f94a2"},"outputs":[],"source":"sns.kdeplot(x, y, shade=True)\n# or \nsns.jointplot(x, y, kind='kde')"},{"cell_type":"markdown","metadata":{"_cell_guid":"923f0cd1-e2de-0bb2-41b5-de3c43682710"},"source":"Next, let's create a merged plot of the top 6 strong correlated features with the target (SalePrice). Recall from the start we saw that the following attributes have a strong positive correlation with the SalePrice: *OverallQual, GrLivArea(GarageCars), GargeArea, TotalBsmtSF, 1stFlrSF, FullBath, TotRmsAbvGrd, YearBuilt, YearRemodAdd, GargeYrBlt, MasVnrArea* and *Fireplaces*.\n\nLet's see how the pairwise matrix looks like for the top 6:"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"df9a0d5a-cb67-36f2-edbd-646bcf43b4f1"},"outputs":[],"source":"plt.figure(1)\nf, axarr = plt.subplots(3, 2, figsize=(10, 9))\ny = target.values\naxarr[0, 0].scatter(train['OverallQual'].values, y)\naxarr[0, 0].set_title('OverallQual')\naxarr[0, 1].scatter(train['TotRmsAbvGrd'].values, y)\naxarr[0, 1].set_title('TotRmsAbvGrd')\naxarr[1, 0].scatter(train['GarageCars'].values, y)\naxarr[1, 0].set_title('GarageCars')\naxarr[1, 1].scatter(train['GarageArea'].values, y)\naxarr[1, 1].set_title('GarageArea')\naxarr[2, 0].scatter(train['TotalBsmtSF'].values, y)\naxarr[2, 0].set_title('TotalBsmtSF')\naxarr[2, 1].scatter(train['1stFlrSF'].values, y)\naxarr[2, 1].set_title('1stFlrSF')\nf.text(-0.01, 0.5, 'Sale Price', va='center', rotation='vertical', fontsize = 12)\nplt.tight_layout()\nplt.show()"},{"cell_type":"markdown","metadata":{"_cell_guid":"1a866ee3-1690-c838-9c1a-d432f1963b10"},"source":"Source:\n\n 1. [Dataset exploration: Boston house pricing][1] by Bohumír Zámečník\n 2. [Plotting a diagonal correlation matrix][2] by Michael Waskom\n\n  [1]: http://www.neural.cz/dataset-exploration-boston-house-pricing.html\n  [2]: https://stanford.edu/~mwaskom/software/seaborn/examples/many_pairwise_correlations.html"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}