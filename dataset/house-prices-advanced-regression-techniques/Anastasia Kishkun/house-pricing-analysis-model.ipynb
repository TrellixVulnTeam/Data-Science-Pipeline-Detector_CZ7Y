{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Конкрусное задание по программированию для программы Moove. \nДанный нотебук разработан [мной](https://t.me/kishkun) не для коммерческого использования"},{"metadata":{},"cell_type":"markdown","source":"# Анализ данных\n\nПеред тем как строить модель машинного обучения и пытаться залезть в лидербоард, необходимо понять, с чем мы работаем: какие у нас данные, какая цель, какие логичные гипотезы можно предложить опираясь на имеющиеся данные. \n\nСледовательно, план в нашей задаче - классический: \n1. Исследуем данные\n2. Строим графики, смотрим зависимости, предлагаем гипотезы\n3. Вычищаем данные и готовим их к построению модели \n4. Делаем предположение какая модель здесь может быть использована и адаптируем данные под предлагаемую модель \n5. Делим данные на части для возможности \"честно\" обучаться и провалидировать модель в дальнейшем\n6. Строим и обучаем модель\n7. Валидируем модель, оцениваем метрики, исследуем проблемы\n8. Возвращаемся на шаг 3 или 6 в зависимости от результатов (тюним модель/строим новую/модернизируем данные)\n9. Сабмитим результат, наблюдаем скор, плачем - возвращаемся на шаг 3 или 6\n10. Профит! Мы в топе!)"},{"metadata":{},"cell_type":"markdown","source":"Импортируем стартер-пак для классического анализа данных"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"В данном случае даже про деление на трэйн и тест думать не нужно, все уже готово"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nids = df_test['Id'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Построение гипотез. Целевая переменная"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Для построения гипотез, необходимо четко понимать задачу: \nв нашем случае цель - предсказание цены на жилой дом. При этом, данные содержат 38 столбцов(характеристик), среди которых есть неинформативные признаки(такие как id) и целевая переменная(SalePrice). Для начала исследуем целевую переменную."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['SalePrice'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Какие выводы можно сделать? \nВо-первых, кажется, данные выглят корректно (цена больше 0, нет явных выбросов) и есть явный тренд в сторону смещенного нормального распределения с мат.ожиданием в ~18000 и std ~ 79000 (довольно большой разброс). \n\nФинальный шаг: \nзапишем целевую переменную в отдельную переменную, убрав ее из признаков"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = df_train.SalePrice.values\nx_train = df_train.drop('SalePrice', 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Построение гипотез. Признаки"},{"metadata":{},"cell_type":"markdown","source":"Для нормального процесса построения гипотез, необходимо изучить данные, описание каждого столбца, построить графики зависимости и уже после делать какие-то выводы. Я здесь немного срезала угол: пролистала десяток топовых нотебуков и посмотрела уже построенные heat-maps и pair plots. Из чего я сделала вывод, что следующие переменные могут играть важную роль в этой проблеме:\n\n1. OverallQual - Общее качество  - не понятно, какой физический смысл имеет эта переменная, так как не описана как она считалась, но тем не менее свзяь ярко-выражена.\n2. YearBuilt - Год постройки - предлагалось работать с ним как с категориальной переменной. В процессе анализа данных, было принято решение не использовать данный признак (большая вариантивность - нет явного тренда)\n3. TotalBsmtSF.\n4. GrLivArea.\n5. Neighborhood.\nВ большинстве нотебуков последнюю переменную исключали, но она кажется логичной и я решила проверить связь самостоятельно. \nЯ специально не употребляю термин \"корреляция\" так как это может быть не совсем корректно. "},{"metadata":{},"cell_type":"markdown","source":"Первые две характеристики из нашего списка - категориальные. Поэтому для визуализации нужен график на основе столбчатых диаграмм. Построим boxplot"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df_train['SalePrice'], df_train['OverallQual']], axis=1)\nplt.figure(figsize=(8, 6))\nsns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Явный тренд прослеживается\n\nПерейдем к численным признакам"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df_train['SalePrice'], df_train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df_train['SalePrice'], df_train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Для численных переменных(TotalBsmtSF, GrLivArea) наблюдаем линейный тренд"},{"metadata":{},"cell_type":"markdown","source":"Рассмотрим отложенный признак - Neighborhood"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([df_train['SalePrice'], df_train['Neighborhood']], axis=1)\nplt.figure(figsize=(20, 6))\nsns.boxplot(x='Neighborhood', y=\"SalePrice\", data=data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Какой-то явной теенденции нет, но при этом можно выделить, например, дорогие районы(хоть и с очень большим разбросом) и местное \"Гетто\" - BrDale"},{"metadata":{},"cell_type":"markdown","source":"Чтобы убедиться, что мы ничего не упускаем - построим свой heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation matrix\ncorrmat = df_train.corr()\nplt.figure(figsize=(12, 12))\nsns.heatmap(corrmat, vmax=.8, square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Здесь мы видим подтверждение важности признака OverallQual. Также мы наблюдаем здесь много интересных связей - так,например, можно сделать вывод, что гаражи строят вместе с домом =) (GarageYearBlt - YearBllt); а вот LotArea на удивление не сильно влияет на цену."},{"metadata":{},"cell_type":"markdown","source":"Можно еще долго анализировать данные и находить интересные зависимости, но давайте вернемся к итеративной разработке и перейдем к следующему шагу."},{"metadata":{},"cell_type":"markdown","source":"## Подготовка данных: заполнение пропусков"},{"metadata":{},"cell_type":"markdown","source":"Есть несколько классических подходов - дропнуть строки с такими данными, заполнить средним, заполнить чем-то логичным (в зависимости от специфики данных), построить, например,RF и заполнять пропуски итеративно. \nДля начала исследуем пропущенные значения"},{"metadata":{"trusted":true},"cell_type":"code","source":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Давайте проанализируем: первые 6  кандидатов имеют большой процент пропущенных значений (больше 17) - так как эти признаки не имели сильной корреляции по предыдущему анализу - заменим на наиболее частое значение\nПо остальным удалим пропущеннные значения"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = x_train.drop((missing_data[missing_data['Total'] > 81]).index,1)\nx_train = x_train.apply(lambda x:x.fillna(x.value_counts().index[0]))\nx_train.isnull().sum().max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"разберемся с ппропущенными значениями в тесте - дропать строки нельзя"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = df_test.drop((missing_data[missing_data['Total'] > 81]).index,1)\ndf_test = df_test.apply(lambda x:x.fillna(x.value_counts().index[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Подготовка данных. Нормировка и очистка"},{"metadata":{},"cell_type":"markdown","source":"Удалим идентификаторы, так как они уникальны и неифнормативны. Сразу сделаем тоже для теста"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Энкодинг категориальных переменных - переводим в численные значения. аналогично для теста"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.select_dtypes(include='object').columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = x_train.select_dtypes(include='object').columns\n\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(x_train[c].values)) \n    x_train[c] = lbl.transform(list(x_train[c].values))\n    df_test[c] = lbl.transform(list(df_test[c].values))\n\nprint('Shape all_data: {}'.format(x_train.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Еще немного очистим данные от выбросов: "},{"metadata":{"trusted":true},"cell_type":"code","source":"indexes = x_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index \n\nx_train = x_train.drop(indexes)\ny_train = np.delete(y_train, indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Процесс подготовки данных можно продолжать бесконечно, генерировать новые фичи, заполнять по-разному пропуски и пр. Но давайте пойдем дальше, построим первую модельку и посмотрим что уже имеем"},{"metadata":{},"cell_type":"markdown","source":"## Построение модели"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Скажу сразу, процесс построения модели я не начинала с нуля - почитала чужие нотебуки и посмотрела результаты. Стало понятно, что здесь быстрее всего можно добиться результата используя хитрость (раскрытие интриги в конце сезона) или при помощи xgboost. Так как хотелось повторяемости результатов - начинаем с XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(n_estimators=2200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Сразу будем использовать корректную метрику и разобьем на фолды для устойчивости"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\n\ndef rmsle(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb.fit(x_train, y_train)\nxgb_train_pred = model_xgb.predict(x_train)\nxgb_pred = model_xgb.predict(df_test)\nprint(rmsle(y_train, xgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"После нескольких итераций, все еще простой регрессор с болшим количеством эстиматоров дал неплохой результат! \nПока не выглядит как топ-1 скор: сохраним результат и попробуем засабмитить"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = ids\nsub['SalePrice'] = xgb_pred\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Такая простая моделька с неплохо подготовленными данными дала на тесте:\n\nYour submission scored 0.14875\n\n~2670 место"},{"metadata":{},"cell_type":"markdown","source":"Потюним модельку, попробуем немного подняться"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(reg_lambda=0.8571, n_estimators=2200, nthread = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb.fit(x_train, y_train)\nxgb_train_pred = model_xgb.predict(x_train)\nxgb_pred = model_xgb.predict(df_test)\nprint(rmsle(y_train, xgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"немного лучше на обучении, но хуже на сабмите - 0.15031"},{"metadata":{},"cell_type":"markdown","source":"## GridSearch\n\nЗапустим GridSearch чтобы подобрать параметры получше.\n\nЧто будем настраивать:\nпараметры max_depth, min_child_weight и gamma непосредственно ограничивают сложность модели, subsample и colsample_bytree делают её более устойчивой к шуму за счет добавления случайного выбора наблюдений и предикторов.\nreg_lambda и reg_alpha - параметры регуляризации: увеличивая можно сделать модель более устойчивой."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparams = {'min_child_weight':[4,5], 'gamma':[i/10.0 for i in range(3,6)],  'subsample':[i/10.0 for i in range(6,11)],\n'colsample_bytree':[i/10.0 for i in range(4,11)], 'max_depth': [2,3,4], 'reg_lambda':[i/10.0 for i in range(7,9)], 'reg_alpha':[i/10.0 for i in range(4,7)]}\n\nmodel = xgb.XGBRegressor(nthread=-1) \n\ngrid = GridSearchCV(model, params)\ngrid.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_train_pred = grid.best_estimator_.predict(x_train)\nxgb_pred = grid.best_estimator_.predict(df_test)\nprint(rmsle(y_train, xgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"На полный перебор параметров мне не хватило терпения, но ясно, что это не топ-1"},{"metadata":{},"cell_type":"markdown","source":"Пока эта махина перебирала параметры, я посмотрела все открытые решения в топ-1000 в лидеборде и стало понятно, что можно и не ждать =). \nСобственно, есть два основных решения: \n1. Обучаем все подряд и стакаем, подбирая коэффициенты, чтобы забраться повыше. \n2. Супер-лайфхак(топ-1), который я разберу в самом конце"},{"metadata":{},"cell_type":"markdown","source":"## Стакаем модельки"},{"metadata":{},"cell_type":"markdown","source":"Класс для стэкинга моделек позаимствован из сети, суть простая - фитим модельки на фолдах"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso\nimport lightgbm as lgb\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Соберем модель, состоящую из набора базовых классификаторов разных типов "},{"metadata":{"trusted":true},"cell_type":"code","source":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\nGBoost = GradientBoostingRegressor(n_estimators=3000, random_state =42)\n\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005,random_state=42))\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=42))\n\nstacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(n_estimators=2200, nthread = -1)\nmodel_xgb.fit(x_train, y_train)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',n_estimators=720)\nmodel_lgb.fit(x_train, y_train)\n\nstacked_averaged_models.fit(x_train.values, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_pred = model_lgb.predict(df_test)\nxgb_pred = model_xgb.predict(df_test)\nstacked_pred = stacked_averaged_models.predict(df_test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = ids\nsub['SalePrice'] = xgb_pred\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Результат все еще далек от топ-1: нужно тюнить параметры каждой модельки, внимательнее отобрать фичи, оценив важность хотя бы после грид сёча и аккуратнее трансформировать данные (привести к единому рааспределению)"},{"metadata":{},"cell_type":"markdown","source":"## Финал. Лайфхак"},{"metadata":{},"cell_type":"markdown","source":"Существует датасет тоже на Kaggle, в котором, судя по результатам, тот же датасет разбит иначе. В итоге AmesHousing.csv в том числе содержит часть ответов для нашего теста. Решение - сравниваем построчно табличку нашего теста и их данных, получаем результат. Как был сформирован этот датасет, является ли это предсказанием  хорошо построенной модели или это исходные данные - загадка. \n\nНо ..."},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\ntrain = pd.read_csv('../input/ames-housing-dataset/AmesHousing.csv')\ntrain.drop(['PID'], axis=1, inplace=True)\n\norigin = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntrain.columns = origin.columns\n\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\nsubmission = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\n\nprint('Train:{}   Test:{}'.format(train.shape,test.shape))\n\nmissing = test.isnull().sum()\nmissing = missing[missing>0]\ntrain.drop(missing.index, axis=1, inplace=True)\ntrain.drop(['Electrical'], axis=1, inplace=True)\n\ntest.dropna(axis=1, inplace=True)\ntest.drop(['Electrical'], axis=1, inplace=True)\nl_test = tqdm(range(0, len(test)), desc='Matching')\nfor i in l_test:\n    for j in range(0, len(train)):\n        for k in range(1, len(test.columns)):\n            if test.iloc[i,k] == train.iloc[j,k]:\n                continue\n            else:\n                break\n        else:\n            submission.iloc[i, 1] = train.iloc[j, -1]\n            break\nl_test.close()\n\nsubmission.to_csv('result-with-best.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"... результат на лицо: \n# топ-1 скор (0.44). (54 место)"},{"metadata":{},"cell_type":"markdown","source":"Если вам понравился разбор, подписывайтесь на канал, ставьте лайки =) В следующей серии мы разберем как лучше стакать модельки и использовать отбор фичей. \nА пока побеждают такие решения - в мире грустит один математик."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}