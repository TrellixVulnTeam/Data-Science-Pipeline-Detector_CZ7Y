{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-17T16:33:26.60184Z","iopub.execute_input":"2022-06-17T16:33:26.602331Z","iopub.status.idle":"2022-06-17T16:33:26.610465Z","shell.execute_reply.started":"2022-06-17T16:33:26.602298Z","shell.execute_reply":"2022-06-17T16:33:26.609734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# House Prices Data Science Project","metadata":{}},{"cell_type":"markdown","source":"## Intro\n\nThis project is addressing factors which can influence price negotiations. We need to use this data to predict the price of house as accurately as possible.\nIn this notebook, we will conduct an EDA, prepare dataset features to use it in model predictions and test some models to predict house prices.","metadata":{}},{"cell_type":"code","source":"# import python libraries\n\n# data analysis\nimport numpy as np\nimport pandas as pd\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# sklearn utilities\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n# prediction\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:26.741396Z","iopub.execute_input":"2022-06-17T16:33:26.74224Z","iopub.status.idle":"2022-06-17T16:33:26.75604Z","shell.execute_reply.started":"2022-06-17T16:33:26.742194Z","shell.execute_reply":"2022-06-17T16:33:26.755116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## Acquire data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntrain_test_data = [train_data, test_data]\nprint('Training data shape: ', train_data.shape)\nprint('Test data shape: ', test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:26.87278Z","iopub.execute_input":"2022-06-17T16:33:26.873317Z","iopub.status.idle":"2022-06-17T16:33:26.92383Z","shell.execute_reply.started":"2022-06-17T16:33:26.873285Z","shell.execute_reply":"2022-06-17T16:33:26.923118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Describing data","metadata":{}},{"cell_type":"markdown","source":"Now we need to research our data, look at data features and their types.","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:27.029468Z","iopub.execute_input":"2022-06-17T16:33:27.030015Z","iopub.status.idle":"2022-06-17T16:33:27.057284Z","shell.execute_reply.started":"2022-06-17T16:33:27.029983Z","shell.execute_reply":"2022-06-17T16:33:27.05625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:27.20353Z","iopub.execute_input":"2022-06-17T16:33:27.203975Z","iopub.status.idle":"2022-06-17T16:33:27.235042Z","shell.execute_reply.started":"2022-06-17T16:33:27.203941Z","shell.execute_reply":"2022-06-17T16:33:27.233785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:27.355547Z","iopub.execute_input":"2022-06-17T16:33:27.355929Z","iopub.status.idle":"2022-06-17T16:33:27.382523Z","shell.execute_reply.started":"2022-06-17T16:33:27.3559Z","shell.execute_reply":"2022-06-17T16:33:27.381454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:27.495754Z","iopub.execute_input":"2022-06-17T16:33:27.496134Z","iopub.status.idle":"2022-06-17T16:33:27.523156Z","shell.execute_reply.started":"2022-06-17T16:33:27.496105Z","shell.execute_reply":"2022-06-17T16:33:27.52235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us describe numerical and object data separately.","metadata":{}},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:27.642439Z","iopub.execute_input":"2022-06-17T16:33:27.64352Z","iopub.status.idle":"2022-06-17T16:33:27.74566Z","shell.execute_reply.started":"2022-06-17T16:33:27.64348Z","shell.execute_reply":"2022-06-17T16:33:27.74489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe(include=['O'])","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:27.793538Z","iopub.execute_input":"2022-06-17T16:33:27.796124Z","iopub.status.idle":"2022-06-17T16:33:27.88604Z","shell.execute_reply.started":"2022-06-17T16:33:27.796082Z","shell.execute_reply":"2022-06-17T16:33:27.885097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Summary:\n* Several features have a lot of empty values. Probably we will drop them in the future.\n* We have too many features (80) to use all of them in our predictions. We have to reduce feature's count next. \n* Several features (e.g. LandContour) basically filled the same values. It could be transform to a binary feature.","metadata":{}},{"cell_type":"markdown","source":"## Analize features","metadata":{}},{"cell_type":"markdown","source":"Firstly, we drop Id feature to reduce number of features to analize.","metadata":{}},{"cell_type":"code","source":"id_test = test_data['Id'].tolist()\n\nfor data in train_test_data:\n    data.drop(['Id'], axis=1, inplace=True)\nprint(train_data.shape, test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:27.932586Z","iopub.execute_input":"2022-06-17T16:33:27.93301Z","iopub.status.idle":"2022-06-17T16:33:27.943016Z","shell.execute_reply.started":"2022-06-17T16:33:27.932978Z","shell.execute_reply":"2022-06-17T16:33:27.942075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical data","metadata":{}},{"cell_type":"markdown","source":"#### feature extracting","metadata":{}},{"cell_type":"code","source":"train_data_num = train_data.select_dtypes(exclude=['object'])\ntest_data_num = test_data.select_dtypes(exclude=['object'])\ntrain_data_num.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:28.088004Z","iopub.execute_input":"2022-06-17T16:33:28.088446Z","iopub.status.idle":"2022-06-17T16:33:28.115766Z","shell.execute_reply.started":"2022-06-17T16:33:28.088415Z","shell.execute_reply":"2022-06-17T16:33:28.114725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at features values distribution.","metadata":{}},{"cell_type":"code","source":"train_data_num.hist(figsize=(25, 30), bins=30);","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:28.232017Z","iopub.execute_input":"2022-06-17T16:33:28.232444Z","iopub.status.idle":"2022-06-17T16:33:35.595724Z","shell.execute_reply.started":"2022-06-17T16:33:28.232388Z","shell.execute_reply":"2022-06-17T16:33:35.594571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that dataset contains few low variance features. We can drop features contains more than 95% similar values because they will have minor impact on our predictions.","metadata":{}},{"cell_type":"code","source":"selector = VarianceThreshold(threshold=0.05)\n\nselector.fit(train_data_num.iloc[:, :-1])\n\nsup = selector.get_support()\n\nprint('Number of retained features: ', sum(sup))\n\nprint('Number low-variance features: ', sum(~sup))\n\nlow_var_fet = train_data_num.drop(['SalePrice'], axis=1).loc[:, ~sup].columns.values\n\nprint('Low-variance features: ', low_var_fet)\n\nprint('Before: ',train_data_num.shape, test_data_num.shape)\ntrain_data_num.drop(low_var_fet, axis=1, inplace=True)\ntest_data_num.drop(low_var_fet, axis=1, inplace=True)\nprint('After: ', train_data_num.shape, test_data_num.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:35.597344Z","iopub.execute_input":"2022-06-17T16:33:35.598148Z","iopub.status.idle":"2022-06-17T16:33:35.614186Z","shell.execute_reply.started":"2022-06-17T16:33:35.598114Z","shell.execute_reply":"2022-06-17T16:33:35.613195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to reduce number of feature in our model. Take a look at the correlation table to select the most correlated with SalePrice feature.","metadata":{}},{"cell_type":"code","source":"pd.options.display.float_format = \"{:,.2f}\".format\n\ncorr_mat = train_data_num.corr('pearson')\n\n# replace very weak correlation\ncorr_mat[(corr_mat < 0.3) & (corr_mat > -0.3)] = 0\n\n# define triangular mask for better visibility\nmask = np.triu(np.ones_like(corr_mat, dtype=bool))\nplt.figure(figsize=(20, 20))\nsns.heatmap(corr_mat, mask=mask, vmax=1.0, vmin=-1.0, square=True, annot=True, annot_kws={\"size\": 9, \"color\": \"black\"}, linewidths=0.1, cmap='rocket');","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:35.615635Z","iopub.execute_input":"2022-06-17T16:33:35.616525Z","iopub.status.idle":"2022-06-17T16:33:38.325649Z","shell.execute_reply.started":"2022-06-17T16:33:35.61649Z","shell.execute_reply":"2022-06-17T16:33:38.324626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we extract SalePrice correlations:","metadata":{}},{"cell_type":"code","source":"corr_features = corr_mat['SalePrice'].drop(['SalePrice'])\ncorr_features.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:38.32785Z","iopub.execute_input":"2022-06-17T16:33:38.328197Z","iopub.status.idle":"2022-06-17T16:33:38.337971Z","shell.execute_reply.started":"2022-06-17T16:33:38.328166Z","shell.execute_reply":"2022-06-17T16:33:38.337188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are various strength of correlation:\n* < 0.3 - very weak correlation (on table it is replaced by 0)\n* \\> 0.3 & < 0.5 - weak correaltion\n* \\> 0.5 & < 0.7 - moderate correlation\n* \\> 0.7 - strong correlation\n\nIt also works for negative coefficients.","metadata":{}},{"cell_type":"markdown","source":"Now we can investigate the extracted features more in depth.\n\nLet's consider separately the signs belonging to each degree of correlation.","metadata":{}},{"cell_type":"code","source":"# strong correlation features (> 0.7)\n\nstrong_corr_fet_names = corr_features[abs(corr_features) >= 0.7].sort_values(ascending=False).index.tolist()\nprint('Strongly correlated features: ', strong_corr_fet_names)\n\nstrong_fet = train_data_num.loc[:, strong_corr_fet_names + ['SalePrice']]\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 10))\n\nfor i, ax in enumerate(ax):\n    if i < len(strong_corr_fet_names):\n        sns.regplot(x=strong_corr_fet_names[i], y='SalePrice', data=strong_fet, ax=ax, line_kws={'color': 'red'})","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:38.339274Z","iopub.execute_input":"2022-06-17T16:33:38.34019Z","iopub.status.idle":"2022-06-17T16:33:39.043359Z","shell.execute_reply.started":"2022-06-17T16:33:38.340151Z","shell.execute_reply":"2022-06-17T16:33:39.042139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# moderate correlation features ( > 0.5 & < 0.7)\n\nmoderate_fet_names = corr_features[(abs(corr_features) >= 0.5) & (abs(corr_features) < 0.7)].sort_values(ascending=False).index.tolist()\nprint('Moderate correlation features: ', moderate_fet_names)\n\nmoderate_fet = train_data_num.loc[:, moderate_fet_names + [\"SalePrice\"]]\n\nfig, ax = plt.subplots(3, 3, figsize=(30, 30))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(moderate_fet_names):\n        sns.regplot(x=moderate_fet_names[i], y='SalePrice', data=moderate_fet, ax=ax, line_kws={'color': 'red'})","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:39.044936Z","iopub.execute_input":"2022-06-17T16:33:39.045383Z","iopub.status.idle":"2022-06-17T16:33:42.031447Z","shell.execute_reply.started":"2022-06-17T16:33:39.045342Z","shell.execute_reply":"2022-06-17T16:33:42.030753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# weakly correlated features ( > 0.3 & < 0.5)\n\nweak_fet_names = corr_features[(abs(corr_features) >= 0.3) & (abs(corr_features) < 0.5)].sort_values(ascending=False).index.tolist()\nprint('Weakly correlated features: ', weak_fet_names)\n\nweak_fet = train_data_num.loc[:, weak_fet_names + [\"SalePrice\"]]\n\nfig, ax = plt.subplots(3, 3, figsize=(30, 30))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(weak_fet_names):\n        sns.regplot(x=weak_fet_names[i], y='SalePrice', data=weak_fet, ax=ax, line_kws={'color': 'red'})","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:42.032533Z","iopub.execute_input":"2022-06-17T16:33:42.033233Z","iopub.status.idle":"2022-06-17T16:33:44.913172Z","shell.execute_reply.started":"2022-06-17T16:33:42.033198Z","shell.execute_reply":"2022-06-17T16:33:44.912429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's merge all features and see how data looks like now.","metadata":{}},{"cell_type":"code","source":"numerical_features = strong_corr_fet_names + moderate_fet_names + weak_fet_names + ['SalePrice']\n\ntrain_data_num = train_data_num.loc[:, numerical_features]\n# excepting SalePrice\ntest_data_num = test_data_num.loc[:, numerical_features[:-1]]\ntrain_data_num.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:44.914243Z","iopub.execute_input":"2022-06-17T16:33:44.915133Z","iopub.status.idle":"2022-06-17T16:33:44.940686Z","shell.execute_reply.started":"2022-06-17T16:33:44.915095Z","shell.execute_reply":"2022-06-17T16:33:44.939617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We've extracted the most important features for predictions but 18 features is still too many. We can drop features which have strong correlation with each other.","metadata":{}},{"cell_type":"code","source":"from itertools import combinations\n\n# find pairs of strongly correalted features\ncols = train_data_num.columns.tolist()[:-1]\npairs = []\n\nfor pair in combinations(range(len(cols)), 2):\n    if corr_mat.loc[cols[pair[0]], cols[pair[1]]] >= 0.7:\n        pairs.append((cols[pair[0]], cols[pair[1]]))\n        \npairs","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:44.942223Z","iopub.execute_input":"2022-06-17T16:33:44.942738Z","iopub.status.idle":"2022-06-17T16:33:44.956436Z","shell.execute_reply.started":"2022-06-17T16:33:44.942693Z","shell.execute_reply":"2022-06-17T16:33:44.955691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we can drop one feature from each pair\n\nfor _, col in pairs:\n    train_data_num.drop(col, axis=1, inplace=True)\n    test_data_num.drop(col, axis=1, inplace=True)\n\ntrain_data_num.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:44.960179Z","iopub.execute_input":"2022-06-17T16:33:44.960978Z","iopub.status.idle":"2022-06-17T16:33:44.98836Z","shell.execute_reply.started":"2022-06-17T16:33:44.960937Z","shell.execute_reply":"2022-06-17T16:33:44.987348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### filling empty values","metadata":{}},{"cell_type":"markdown","source":"We need to fill empty values in data before using it our model. Take a look at data info:","metadata":{}},{"cell_type":"code","source":"train_data_num.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:44.989979Z","iopub.execute_input":"2022-06-17T16:33:44.990624Z","iopub.status.idle":"2022-06-17T16:33:45.00421Z","shell.execute_reply.started":"2022-06-17T16:33:44.990579Z","shell.execute_reply":"2022-06-17T16:33:45.003239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_num.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:45.005538Z","iopub.execute_input":"2022-06-17T16:33:45.006352Z","iopub.status.idle":"2022-06-17T16:33:45.018883Z","shell.execute_reply.started":"2022-06-17T16:33:45.00632Z","shell.execute_reply":"2022-06-17T16:33:45.017905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_nan_cols = train_data_num.columns[train_data_num.isnull().any()].tolist()\ntrain_nan = pd.DataFrame({ 'Column': train_nan_cols,\n                         'NaN_percent': [ train_data_num[col].isnull().sum() * 100 / len(train_data_num) \n                                         for col in train_nan_cols] })\nsns.barplot(data=train_nan, x='Column', y='NaN_percent');","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:45.020001Z","iopub.execute_input":"2022-06-17T16:33:45.020311Z","iopub.status.idle":"2022-06-17T16:33:45.201354Z","shell.execute_reply.started":"2022-06-17T16:33:45.020283Z","shell.execute_reply":"2022-06-17T16:33:45.200187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_nan_cols = test_data_num.columns[test_data_num.isnull().any()].tolist()\ntest_nan = pd.DataFrame({ 'Column': test_nan_cols,\n                         'NaN_percent': [ test_data_num[col].isnull().sum() * 100 / len(test_data_num) \n                                         for col in test_nan_cols] })\nsns.barplot(data=test_nan, x='Column', y='NaN_percent');","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:45.202909Z","iopub.execute_input":"2022-06-17T16:33:45.203299Z","iopub.status.idle":"2022-06-17T16:33:45.40688Z","shell.execute_reply.started":"2022-06-17T16:33:45.203266Z","shell.execute_reply":"2022-06-17T16:33:45.405868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like we have 2 features with empty values in training data and 6 features in test data. The number of empty values in the LotFrontage column is significantly greater than in other columns (about 16-17%). Filling empty values should not greatly affect the original distribution. We try to replace empty values by median. ","metadata":{}},{"cell_type":"code","source":"imp = SimpleImputer(strategy='median')\nimp_train_data = pd.DataFrame(imp.fit_transform(train_data_num))\nimp_train_data.columns = train_data_num.columns\n\nfig, ax = plt.subplots(2,2, figsize=(15, 15))\n\nfor i, col in enumerate(train_nan_cols):\n    \n    # before filling NaN\n    bfr = sns.histplot(data=train_data_num, x=col, ax=ax[i, 0], stat='density', bins=30)\n    sns.kdeplot(data=train_data_num, x=col, ax=ax[i, 0], color='red')\n    bfr.set_xlim(left=0)\n    bfr.set_ylabel('Before', fontsize=12)\n    \n    # after filling NaN\n    aftr = sns.histplot(data=imp_train_data, x=col, ax=ax[i, 1], stat='density', bins=30)\n    sns.kdeplot(data=train_data_num, x=col, ax=ax[i, 1], color='red')\n    aftr.set_xlim(left=0)\n    aftr.set_ylabel('After', fontsize=12)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:45.408078Z","iopub.execute_input":"2022-06-17T16:33:45.408385Z","iopub.status.idle":"2022-06-17T16:33:46.318495Z","shell.execute_reply.started":"2022-06-17T16:33:45.408357Z","shell.execute_reply":"2022-06-17T16:33:46.317448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The shape of LotFrontage distribution is affected by filling empty values so we can drop it. Other features not presented on plots have a very low percent of empty values (< 2%) so imputing it doesn't greatly affect the original distribution.","metadata":{}},{"cell_type":"code","source":"# drop LotFrontage\nimp_train_data.drop(['LotFrontage'], axis=1, inplace=True)\ntest_data_num.drop(['LotFrontage'], axis=1, inplace=True)\n\n# fill NaN in test data\nimp = SimpleImputer(strategy='median')\nimp_test_data = pd.DataFrame(imp.fit_transform(test_data_num))\nimp_test_data.columns = test_data_num.columns\n\ntrain_data_num = imp_train_data\ntest_data_num = imp_test_data","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:46.32028Z","iopub.execute_input":"2022-06-17T16:33:46.32067Z","iopub.status.idle":"2022-06-17T16:33:46.33592Z","shell.execute_reply.started":"2022-06-17T16:33:46.320636Z","shell.execute_reply":"2022-06-17T16:33:46.334994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_num.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:46.337516Z","iopub.execute_input":"2022-06-17T16:33:46.337885Z","iopub.status.idle":"2022-06-17T16:33:46.355367Z","shell.execute_reply.started":"2022-06-17T16:33:46.337853Z","shell.execute_reply":"2022-06-17T16:33:46.354382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_num.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:46.356786Z","iopub.execute_input":"2022-06-17T16:33:46.35728Z","iopub.status.idle":"2022-06-17T16:33:46.372118Z","shell.execute_reply.started":"2022-06-17T16:33:46.357236Z","shell.execute_reply":"2022-06-17T16:33:46.370888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### outliers","metadata":{}},{"cell_type":"markdown","source":"Now we need to get out of outliers.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(5, 3, figsize=(20, 20))\ntrain_num_cols = train_data_num.columns.tolist()[:-1]\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(train_num_cols):\n        sns.boxplot(data=train_data_num, y=train_num_cols[i], ax=ax)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:46.373353Z","iopub.execute_input":"2022-06-17T16:33:46.374295Z","iopub.status.idle":"2022-06-17T16:33:47.847151Z","shell.execute_reply.started":"2022-06-17T16:33:46.374256Z","shell.execute_reply":"2022-06-17T16:33:47.846448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To get out of outliers we will floor their values based on quantile. For outliers searching we will use Z-score from scipy library.","metadata":{}},{"cell_type":"code","source":"from scipy import stats\n\nfor col in train_num_cols:\n    z_upper_train = stats.zscore(train_data_num[col]) > 3\n    z_lower_train = stats.zscore(train_data_num[col]) < -3\n    q1, q3 = train_data_num[col].quantile([0.25, 0.75])\n    \n    train_data_num[z_upper_train] = q3\n    train_data_num[z_lower_train] = q1\n    \n    z_upper_test = stats.zscore(test_data_num[col]) > 3\n    z_lower_test = stats.zscore(test_data_num[col]) < -3\n    q1, q3 = test_data_num[col].quantile([0.25, 0.75])\n    \n    test_data_num[z_upper_test] = q3\n    test_data_num[z_lower_test] = q1\n","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:47.848308Z","iopub.execute_input":"2022-06-17T16:33:47.8488Z","iopub.status.idle":"2022-06-17T16:33:47.951513Z","shell.execute_reply.started":"2022-06-17T16:33:47.848768Z","shell.execute_reply":"2022-06-17T16:33:47.950765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's take a look at numerical data:","metadata":{}},{"cell_type":"code","source":"train_data_num.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:47.954869Z","iopub.execute_input":"2022-06-17T16:33:47.955224Z","iopub.status.idle":"2022-06-17T16:33:47.970481Z","shell.execute_reply.started":"2022-06-17T16:33:47.955194Z","shell.execute_reply":"2022-06-17T16:33:47.969467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical data","metadata":{}},{"cell_type":"markdown","source":"#### feature extracting","metadata":{}},{"cell_type":"code","source":"categorical_features = [col for col in train_data.columns if train_data.dtypes[col] == 'object']\n\n# training data\ntrain_data_cat = train_data[categorical_features + ['SalePrice']]\n\n# test data\ntest_data_cat = test_data[categorical_features]\n\ntrain_data_cat.shape, test_data_cat.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:47.971633Z","iopub.execute_input":"2022-06-17T16:33:47.971942Z","iopub.status.idle":"2022-06-17T16:33:47.996329Z","shell.execute_reply.started":"2022-06-17T16:33:47.971914Z","shell.execute_reply":"2022-06-17T16:33:47.995351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going look at countplots of each categorical feature to determine dominating categories for each feature in data.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(round(len(train_data_cat.columns) / 2), 2, figsize=(20, 40))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(train_data_cat.columns) - 1:\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=90)\n        sns.countplot(data=train_data_cat, x=train_data_cat.columns[i], ax=ax, palette='pastel')\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:47.99817Z","iopub.execute_input":"2022-06-17T16:33:47.998903Z","iopub.status.idle":"2022-06-17T16:33:53.651808Z","shell.execute_reply.started":"2022-06-17T16:33:47.998857Z","shell.execute_reply":"2022-06-17T16:33:53.650856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are we can notice the obvious domination of one of the categories in some features. Such features make a negligible contribution to predictions so we can drop them.","metadata":{}},{"cell_type":"code","source":"features_to_drop = ['Street', \n                    'LandContour', \n                    'Utilities', \n                    'LandSlope', \n                    'Condition2', \n                    'RoofMatl', \n                    'BsmtCond', \n                    'BsmtFinType2', \n                    'Heating', \n                    'CentralAir', \n                    'Electrical',\n                    'Functional',\n                    'GarageQual',\n                    'GarageCond',\n                    'PavedDrive'\n                   ]\n\ntrain_data_cat.drop(features_to_drop, axis=1, inplace=True)\ntest_data_cat.drop(features_to_drop, axis=1, inplace=True)\n\ntrain_data_cat.shape, test_data_cat.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:53.653235Z","iopub.execute_input":"2022-06-17T16:33:53.653631Z","iopub.status.idle":"2022-06-17T16:33:53.667982Z","shell.execute_reply.started":"2022-06-17T16:33:53.653598Z","shell.execute_reply":"2022-06-17T16:33:53.666966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are left now with 28 categorical features in both data sets.\n\nNext thing we will do is looking at variation of the target variable with respect to each categorical feature.","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(round(len(train_data_cat.columns) / 2), 2, figsize=(20, 30))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(train_data_cat.columns) - 1:\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.boxplot(data=train_data_cat, x=train_data_cat.columns[i], y='SalePrice', ax=ax, palette='Spectral_r')\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:33:53.669243Z","iopub.execute_input":"2022-06-17T16:33:53.669649Z","iopub.status.idle":"2022-06-17T16:34:00.932052Z","shell.execute_reply.started":"2022-06-17T16:33:53.669618Z","shell.execute_reply":"2022-06-17T16:34:00.930962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems a few features have similar distributions of SalePrice:\n* Exterior1st and Exterior2nd\n* ExterQual and MasVnrType\n* BsmtQual and BsmtExposure\n\nBased on the similarity of their distributions, we can say that these features are highly correlated with each other. Therefore, we can drop one feature from each pair.","metadata":{}},{"cell_type":"code","source":"train_data_cat.drop(['Exterior2nd', 'MasVnrType', 'BsmtExposure'], axis=1, inplace=True)\ntest_data_cat.drop(['Exterior2nd', 'MasVnrType', 'BsmtExposure'], axis=1, inplace=True)\n\ntrain_data_cat.shape, test_data_cat.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:00.933428Z","iopub.execute_input":"2022-06-17T16:34:00.933854Z","iopub.status.idle":"2022-06-17T16:34:00.947263Z","shell.execute_reply.started":"2022-06-17T16:34:00.933821Z","shell.execute_reply":"2022-06-17T16:34:00.945352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### filling empty values","metadata":{}},{"cell_type":"markdown","source":"Firstly, let's take a look how many empty values each feature has.","metadata":{}},{"cell_type":"code","source":"cols_null_train = train_data_cat.columns[train_data_cat.isnull().any()]\n\nnan_counts_train = pd.DataFrame({\n    'Column': cols_null_train,\n    'NaN_percent': [train_data_cat[col].isnull().sum()*100 / len(train_data_cat) \n                    for col in cols_null_train]\n})\n\nnan_counts_train.sort_values('NaN_percent', ascending=False, inplace=True, ignore_index=True)\nsns.barplot(data=nan_counts_train, y='Column', x='NaN_percent');","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:00.948684Z","iopub.execute_input":"2022-06-17T16:34:00.949642Z","iopub.status.idle":"2022-06-17T16:34:01.191306Z","shell.execute_reply.started":"2022-06-17T16:34:00.949603Z","shell.execute_reply":"2022-06-17T16:34:01.190174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the training data we can see that five features have more than 45% missing values. Filling empty values by feature's mode will significantly modify dustribution so we will drop them.","metadata":{}},{"cell_type":"code","source":"train_data_cat.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis=1, inplace=True)\ntest_data_cat.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.197046Z","iopub.execute_input":"2022-06-17T16:34:01.197478Z","iopub.status.idle":"2022-06-17T16:34:01.207965Z","shell.execute_reply.started":"2022-06-17T16:34:01.19744Z","shell.execute_reply":"2022-06-17T16:34:01.206766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will also fill empty values by feature's mode in other features.","metadata":{}},{"cell_type":"code","source":"for col in ['GarageType', 'GarageFinish', 'BsmtQual', 'BsmtFinType1']:\n    train_data_cat[col].fillna(train_data_cat[col].mode()[0], inplace=True)\n    test_data_cat[col].fillna(test_data_cat[col].mode()[0], inplace=True)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.20959Z","iopub.execute_input":"2022-06-17T16:34:01.210209Z","iopub.status.idle":"2022-06-17T16:34:01.230264Z","shell.execute_reply.started":"2022-06-17T16:34:01.210158Z","shell.execute_reply":"2022-06-17T16:34:01.229134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_cat.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.231645Z","iopub.execute_input":"2022-06-17T16:34:01.232008Z","iopub.status.idle":"2022-06-17T16:34:01.244279Z","shell.execute_reply.started":"2022-06-17T16:34:01.231977Z","shell.execute_reply":"2022-06-17T16:34:01.243508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data_cat.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.245536Z","iopub.execute_input":"2022-06-17T16:34:01.245917Z","iopub.status.idle":"2022-06-17T16:34:01.261766Z","shell.execute_reply.started":"2022-06-17T16:34:01.245886Z","shell.execute_reply":"2022-06-17T16:34:01.260927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems like we have a few empty values in test data. We will get rid of it with similar method.","metadata":{}},{"cell_type":"code","source":"cols_null_test = test_data_cat.columns[test_data_cat.isnull().any()]\n\nfor col in cols_null_test:\n    test_data_cat[col].fillna(test_data_cat[col].mode()[0], inplace=True)\n\ntest_data_cat.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.262964Z","iopub.execute_input":"2022-06-17T16:34:01.263695Z","iopub.status.idle":"2022-06-17T16:34:01.287061Z","shell.execute_reply.started":"2022-06-17T16:34:01.263661Z","shell.execute_reply":"2022-06-17T16:34:01.285965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_cat.shape, test_data_cat.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.28825Z","iopub.execute_input":"2022-06-17T16:34:01.288759Z","iopub.status.idle":"2022-06-17T16:34:01.302594Z","shell.execute_reply.started":"2022-06-17T16:34:01.288727Z","shell.execute_reply":"2022-06-17T16:34:01.301625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we have 20 categorical features in our data. However, before we go next we need to transform data from categories into binary.","metadata":{}},{"cell_type":"markdown","source":"#### transform categories","metadata":{}},{"cell_type":"code","source":"train_data_cat.drop(['SalePrice'], axis=1, inplace=True)\n\ntrain_cat_dummies = pd.get_dummies(train_data_cat)\ntrain_cat_dummies.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.303928Z","iopub.execute_input":"2022-06-17T16:34:01.304669Z","iopub.status.idle":"2022-06-17T16:34:01.352071Z","shell.execute_reply.started":"2022-06-17T16:34:01.30462Z","shell.execute_reply":"2022-06-17T16:34:01.350863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cat_dummies = pd.get_dummies(test_data_cat)\ntest_cat_dummies.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.353804Z","iopub.execute_input":"2022-06-17T16:34:01.354202Z","iopub.status.idle":"2022-06-17T16:34:01.393769Z","shell.execute_reply.started":"2022-06-17T16:34:01.354169Z","shell.execute_reply":"2022-06-17T16:34:01.392686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training data contains more columns than the test data. Training and test data need to have an equal columns count. Let's find differences in data.","metadata":{}},{"cell_type":"code","source":"dif = [col for col in train_cat_dummies.columns if col not in test_cat_dummies.columns]\nprint('Found differences in: ', dif)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.395283Z","iopub.execute_input":"2022-06-17T16:34:01.39622Z","iopub.status.idle":"2022-06-17T16:34:01.402104Z","shell.execute_reply.started":"2022-06-17T16:34:01.396177Z","shell.execute_reply":"2022-06-17T16:34:01.401135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can drop it.","metadata":{}},{"cell_type":"code","source":"train_cat_dummies.drop(dif, axis=1, inplace=True)\n\ntrain_cat_dummies.shape, test_cat_dummies.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.403356Z","iopub.execute_input":"2022-06-17T16:34:01.403716Z","iopub.status.idle":"2022-06-17T16:34:01.419229Z","shell.execute_reply.started":"2022-06-17T16:34:01.403685Z","shell.execute_reply":"2022-06-17T16:34:01.418471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have prepared categorical features to our model. Of course, 137 features it's too much so in future we will select the most important for predictions features. Now we can merge numerical and categorical data.","metadata":{}},{"cell_type":"code","source":"train_data_new = pd.concat([train_data_num, train_cat_dummies], axis=1)\ntest_data_new = pd.concat([test_data_num, test_cat_dummies], axis=1)\n\ntrain_data_new.shape, test_data_new.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.42056Z","iopub.execute_input":"2022-06-17T16:34:01.42096Z","iopub.status.idle":"2022-06-17T16:34:01.434266Z","shell.execute_reply.started":"2022-06-17T16:34:01.420927Z","shell.execute_reply":"2022-06-17T16:34:01.433097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engeenering","metadata":{}},{"cell_type":"markdown","source":"Now we can create new features that can help us improve out predictions. ","metadata":{}},{"cell_type":"code","source":"# Age of house from the year of construction\ntrain_data_new['Age'] = train_data_new['YearBuilt'].max() - train_data_new['YearBuilt']\ntest_data_new['Age'] = test_data_new['YearBuilt'].max() - test_data_new['YearBuilt']\n","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.435796Z","iopub.execute_input":"2022-06-17T16:34:01.436219Z","iopub.status.idle":"2022-06-17T16:34:01.446086Z","shell.execute_reply.started":"2022-06-17T16:34:01.43618Z","shell.execute_reply":"2022-06-17T16:34:01.444834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Age since renovating\ntrain_data_new['Renovate'] = train_data_new['YearRemodAdd'] - train_data_new['YearBuilt']\ntest_data_new['Renovate'] = test_data_new['YearRemodAdd'] - test_data_new['YearBuilt']\n\ntrain_data_new['Renovate'] = np.where(train_data_new['Renovate'] < 0, 0, train_data_new['Renovate'])\ntest_data_new['Renovate'] = np.where(test_data_new['Renovate'] < 0, 0, test_data_new['Renovate'])\n\n# Drop YearBuilt\ntrain_data_new.drop(['YearBuilt'], axis=1, inplace=True)\ntest_data_new.drop(['YearBuilt'], axis=1, inplace=True)\n# Drop YearRemodAdd\ntrain_data_new.drop(['YearRemodAdd'], axis=1, inplace=True)\ntest_data_new.drop(['YearRemodAdd'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.447557Z","iopub.execute_input":"2022-06-17T16:34:01.448098Z","iopub.status.idle":"2022-06-17T16:34:01.469843Z","shell.execute_reply.started":"2022-06-17T16:34:01.448064Z","shell.execute_reply":"2022-06-17T16:34:01.468723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Artificial feature combines OverallQual and GrLivArea\ntrain_data_new['Qual_Area'] = train_data_new['OverallQual'] * train_data_new['GrLivArea']\ntest_data_new['Qual_Area'] = test_data_new['OverallQual'] * test_data_new['GrLivArea']","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.47154Z","iopub.execute_input":"2022-06-17T16:34:01.472189Z","iopub.status.idle":"2022-06-17T16:34:01.479927Z","shell.execute_reply.started":"2022-06-17T16:34:01.472154Z","shell.execute_reply":"2022-06-17T16:34:01.478835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we look at continuous features. We try to mitigate the strong variance of some variables by using log transformation. It make predictions easier for our model because of normalizing data.","metadata":{}},{"cell_type":"code","source":"cont_features = train_data_new.select_dtypes(include=['int', 'float']).drop(['SalePrice'], axis=1).columns.tolist()\n\ncont_data = train_data_new.loc[:, cont_features]\ncont_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.481281Z","iopub.execute_input":"2022-06-17T16:34:01.48235Z","iopub.status.idle":"2022-06-17T16:34:01.507709Z","shell.execute_reply.started":"2022-06-17T16:34:01.482294Z","shell.execute_reply":"2022-06-17T16:34:01.506878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To extract the skewed features, we take out features with skew > 0.5.","metadata":{}},{"cell_type":"code","source":"skew_data = pd.DataFrame({\n    'Column': cont_features,\n    'Skew': abs(cont_data.skew())\n}).sort_values('Skew', ascending=False)\n\nskew_data","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.50908Z","iopub.execute_input":"2022-06-17T16:34:01.509442Z","iopub.status.idle":"2022-06-17T16:34:01.530859Z","shell.execute_reply.started":"2022-06-17T16:34:01.509386Z","shell.execute_reply":"2022-06-17T16:34:01.529389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_features = skew_data[skew_data['Skew'] > 0.5]['Column'].tolist()\nskew_features","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.532491Z","iopub.execute_input":"2022-06-17T16:34:01.533219Z","iopub.status.idle":"2022-06-17T16:34:01.542694Z","shell.execute_reply.started":"2022-06-17T16:34:01.533165Z","shell.execute_reply":"2022-06-17T16:34:01.541651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding 1 to avoid logarithm of 0\nfor col in skew_features:\n    train_data_new[col] = np.log(train_data_new[col] + 1)\n    test_data_new[col] = np.log(test_data_new[col] + 1)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.543977Z","iopub.execute_input":"2022-06-17T16:34:01.545093Z","iopub.status.idle":"2022-06-17T16:34:01.568911Z","shell.execute_reply.started":"2022-06-17T16:34:01.54503Z","shell.execute_reply":"2022-06-17T16:34:01.567863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at distribution from the beginning, we can see that SalePrice feature is skewed too so we use log transformation for it.  ","metadata":{}},{"cell_type":"code","source":"train_data_new['SalePriceLog'] = np.log(train_data_new['SalePrice'])\n\ntrain_data_new.drop('SalePrice', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.57011Z","iopub.execute_input":"2022-06-17T16:34:01.570479Z","iopub.status.idle":"2022-06-17T16:34:01.579082Z","shell.execute_reply.started":"2022-06-17T16:34:01.570439Z","shell.execute_reply":"2022-06-17T16:34:01.578028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at numerical data distribution now.","metadata":{}},{"cell_type":"code","source":"train_data_new_num = train_data_new.select_dtypes(include=['int', 'float'])\ntrain_data_new_num.hist(figsize=(20, 20), bins=30);","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:01.58218Z","iopub.execute_input":"2022-06-17T16:34:01.582878Z","iopub.status.idle":"2022-06-17T16:34:04.133525Z","shell.execute_reply.started":"2022-06-17T16:34:01.582842Z","shell.execute_reply":"2022-06-17T16:34:04.132341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"markdown","source":"## Preparing data","metadata":{}},{"cell_type":"markdown","source":"Before fitting and evaluating models we need to transform and split our data.","metadata":{}},{"cell_type":"code","source":"X = train_data_new.drop(['SalePriceLog'], axis=1)\ny = train_data_new['SalePriceLog']\n\nprint('X shape: ', X.shape)\nprint('y shape: ', y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:04.135336Z","iopub.execute_input":"2022-06-17T16:34:04.135922Z","iopub.status.idle":"2022-06-17T16:34:04.144689Z","shell.execute_reply.started":"2022-06-17T16:34:04.135872Z","shell.execute_reply":"2022-06-17T16:34:04.143749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardize data\nscaler = StandardScaler().fit(X)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:04.146141Z","iopub.execute_input":"2022-06-17T16:34:04.146747Z","iopub.status.idle":"2022-06-17T16:34:04.171746Z","shell.execute_reply.started":"2022-06-17T16:34:04.146711Z","shell.execute_reply":"2022-06-17T16:34:04.170706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we should select the most important features from all of we having now. We will use backward feature elimination for it.","metadata":{}},{"cell_type":"code","source":"import statsmodels.api as sm\n\ndef backward_elimination(X, y, threshold=0.05):\n    features = X.columns.tolist()\n    \n    while True:\n        changed = False\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[features]))).fit()\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max()\n        if worst_pval > threshold:\n            changed = True\n            worst_fet = pvalues.idxmax()\n            features.remove(worst_fet)\n        if not changed:\n            break\n            \n    return features\n","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:04.173052Z","iopub.execute_input":"2022-06-17T16:34:04.173729Z","iopub.status.idle":"2022-06-17T16:34:04.18318Z","shell.execute_reply.started":"2022-06-17T16:34:04.173685Z","shell.execute_reply":"2022-06-17T16:34:04.182263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"selected_features = backward_elimination(X, y)\nselected_features","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:04.184835Z","iopub.execute_input":"2022-06-17T16:34:04.185568Z","iopub.status.idle":"2022-06-17T16:34:08.649461Z","shell.execute_reply.started":"2022-06-17T16:34:04.185519Z","shell.execute_reply":"2022-06-17T16:34:08.648516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X.loc[:, selected_features]\ntest_data_new = test_data_new.loc[:, selected_features]","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:08.650797Z","iopub.execute_input":"2022-06-17T16:34:08.651521Z","iopub.status.idle":"2022-06-17T16:34:08.66114Z","shell.execute_reply.started":"2022-06-17T16:34:08.651475Z","shell.execute_reply":"2022-06-17T16:34:08.66007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can divide data into test and validation data.","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15)\nprint('Train size:', X_train.shape, y_train.shape)\nprint('Validation size:', X_val.shape, y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:08.663001Z","iopub.execute_input":"2022-06-17T16:34:08.663806Z","iopub.status.idle":"2022-06-17T16:34:08.676946Z","shell.execute_reply.started":"2022-06-17T16:34:08.663756Z","shell.execute_reply":"2022-06-17T16:34:08.675771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict and Solve","metadata":{}},{"cell_type":"markdown","source":"Now we are ready to use models for predicting houses prices. We will estimate quality of our predictions with 2 metrics: RMSE (main metric, checking in Kaggle submission) and $R^2$ score. Next we will use the following models:\n* Linear Regression\n* Ridge Regression (L2 penalty)\n* Lasso Regression (L1 penalty)\n* SVR\n* Decision Tree\n* Random Forest\n* XGBoost\n* Gradient Boosting\n* CatBoost","metadata":{}},{"cell_type":"code","source":"# Creating RMSE\n\ndef rmse_score(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# Creating estimating function\n\nr2_list = []\nrmse_list = []\n\ndef get_metrics(model):\n    r2 = model.score(X_val, y_val)\n    rmse = rmse_score(y_val, model.predict(X_val))\n    r2_list.append(r2)\n    rmse_list.append(rmse)\n    print('Cross validation score:', cross_val_score(model, X_train, y_train, cv=5))\n    print('R2 score:', r2)\n    print('RMSE:', rmse)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:08.679276Z","iopub.execute_input":"2022-06-17T16:34:08.680246Z","iopub.status.idle":"2022-06-17T16:34:08.691968Z","shell.execute_reply.started":"2022-06-17T16:34:08.680197Z","shell.execute_reply":"2022-06-17T16:34:08.690728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear Regression","metadata":{}},{"cell_type":"code","source":"linreg = LinearRegression()\nlinreg.fit(X_train, y_train)\n\nget_metrics(linreg)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:08.693911Z","iopub.execute_input":"2022-06-17T16:34:08.694745Z","iopub.status.idle":"2022-06-17T16:34:08.843683Z","shell.execute_reply.started":"2022-06-17T16:34:08.694699Z","shell.execute_reply":"2022-06-17T16:34:08.842676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ridge Regression","metadata":{}},{"cell_type":"code","source":"ridge_reg = Ridge(alpha=.001)\nridge_reg.fit(X_train, y_train)\n\nget_metrics(ridge_reg)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:08.845519Z","iopub.execute_input":"2022-06-17T16:34:08.846266Z","iopub.status.idle":"2022-06-17T16:34:08.984821Z","shell.execute_reply.started":"2022-06-17T16:34:08.846208Z","shell.execute_reply":"2022-06-17T16:34:08.983652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lasso Regression","metadata":{}},{"cell_type":"code","source":"lasso_reg = Lasso(alpha=.001)\nlasso_reg.fit(X_train, y_train)\n\nget_metrics(lasso_reg)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:08.986422Z","iopub.execute_input":"2022-06-17T16:34:08.987134Z","iopub.status.idle":"2022-06-17T16:34:09.326492Z","shell.execute_reply.started":"2022-06-17T16:34:08.987084Z","shell.execute_reply":"2022-06-17T16:34:09.325378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVR","metadata":{}},{"cell_type":"code","source":"svr = SVR()\nsvr.fit(X_train, y_train)\n\nget_metrics(svr)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:09.332184Z","iopub.execute_input":"2022-06-17T16:34:09.333029Z","iopub.status.idle":"2022-06-17T16:34:10.019473Z","shell.execute_reply.started":"2022-06-17T16:34:09.332976Z","shell.execute_reply":"2022-06-17T16:34:10.018473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decision Tree","metadata":{}},{"cell_type":"markdown","source":"Before using decision tree model we should select the most effective depth. Let's see how rmse depends from tree's depth.","metadata":{}},{"cell_type":"code","source":"depths = []\nscores = []\n\nfor d in range(3, 30, 3):\n    m = DecisionTreeRegressor(max_depth=d).fit(X_train, y_train)\n    depths.append(d)\n    scores.append(rmse_score(y_val, m.predict(X_val)))\n\ndt_scores = pd.DataFrame({\n    'Depth': depths,\n    'Score': scores\n})\nsns.lineplot(data=dt_scores, x='Depth', y='Score');","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:10.020594Z","iopub.execute_input":"2022-06-17T16:34:10.020915Z","iopub.status.idle":"2022-06-17T16:34:10.372541Z","shell.execute_reply.started":"2022-06-17T16:34:10.020887Z","shell.execute_reply":"2022-06-17T16:34:10.37159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_depth = int(dt_scores.loc[dt_scores['Score'] == dt_scores['Score'].min(), 'Depth'])\ntree_depth","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:10.373804Z","iopub.execute_input":"2022-06-17T16:34:10.374106Z","iopub.status.idle":"2022-06-17T16:34:10.382051Z","shell.execute_reply.started":"2022-06-17T16:34:10.374078Z","shell.execute_reply":"2022-06-17T16:34:10.380817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dt = DecisionTreeRegressor(max_depth=tree_depth)\ndt.fit(X_train, y_train)\n\nget_metrics(dt)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:10.383667Z","iopub.execute_input":"2022-06-17T16:34:10.384111Z","iopub.status.idle":"2022-06-17T16:34:10.47197Z","shell.execute_reply.started":"2022-06-17T16:34:10.384068Z","shell.execute_reply":"2022-06-17T16:34:10.470905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"markdown","source":"Now we will see how rmse depends from n_estimators. ","metadata":{}},{"cell_type":"code","source":"n_est = []\nrf_rmse = []\n\nfor n in range(50, 300, 25):\n    m = RandomForestRegressor(n_estimators=n).fit(X_train, y_train)\n    n_est.append(n)\n    rf_rmse.append(rmse_score(y_val, m.predict(X_val)))\n\nrf_scores = pd.DataFrame({\n    'N_estimators': n_est,\n    'Score': rf_rmse\n})\nsns.lineplot(data=rf_scores, x='N_estimators', y='Score');","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:10.473348Z","iopub.execute_input":"2022-06-17T16:34:10.473815Z","iopub.status.idle":"2022-06-17T16:34:29.26125Z","shell.execute_reply.started":"2022-06-17T16:34:10.473778Z","shell.execute_reply":"2022-06-17T16:34:29.260294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_estimators = int(rf_scores.loc[rf_scores['Score'] == rf_scores['Score'].min(), 'N_estimators'])\nn_estimators","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:29.262487Z","iopub.execute_input":"2022-06-17T16:34:29.262803Z","iopub.status.idle":"2022-06-17T16:34:29.273489Z","shell.execute_reply.started":"2022-06-17T16:34:29.262776Z","shell.execute_reply":"2022-06-17T16:34:29.272505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=n_estimators)\nrf.fit(X_train, y_train)\n\nget_metrics(rf)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:29.274699Z","iopub.execute_input":"2022-06-17T16:34:29.275418Z","iopub.status.idle":"2022-06-17T16:34:45.004665Z","shell.execute_reply.started":"2022-06-17T16:34:29.27536Z","shell.execute_reply":"2022-06-17T16:34:45.003599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"code","source":"xgb = XGBRegressor(n_estimators=n_estimators)\nxgb.fit(X_train, y_train)\n\nget_metrics(xgb)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:45.005835Z","iopub.execute_input":"2022-06-17T16:34:45.006164Z","iopub.status.idle":"2022-06-17T16:34:52.446238Z","shell.execute_reply.started":"2022-06-17T16:34:45.006134Z","shell.execute_reply":"2022-06-17T16:34:52.445466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Boosting","metadata":{}},{"cell_type":"code","source":"gbr = GradientBoostingRegressor()\ngbr.fit(X_train, y_train)\n\nget_metrics(gbr)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:52.447712Z","iopub.execute_input":"2022-06-17T16:34:52.448341Z","iopub.status.idle":"2022-06-17T16:34:54.291715Z","shell.execute_reply.started":"2022-06-17T16:34:52.448304Z","shell.execute_reply":"2022-06-17T16:34:54.290713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CatBoost","metadata":{}},{"cell_type":"code","source":"cbr = CatBoostRegressor()\ncbr.fit(X_train, y_train, verbose=0)\n\ncbr_pred = cbr.predict(X_val)\n\ncbr_r2 = r2_score(y_val, cbr_pred)\ncbr_rmse = rmse_score(y_val, cbr_pred)\nr2_list.append(cbr_r2)\nrmse_list.append(cbr_rmse)\n\nprint('R2 score:', cbr_r2)\nprint('RMSE score:', cbr_rmse)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:54.293285Z","iopub.execute_input":"2022-06-17T16:34:54.294244Z","iopub.status.idle":"2022-06-17T16:34:56.375875Z","shell.execute_reply.started":"2022-06-17T16:34:54.294199Z","shell.execute_reply":"2022-06-17T16:34:56.375116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's look at evaluating summary:","metadata":{}},{"cell_type":"markdown","source":" Linear Regression\n* Ridge Regression (L2 penalty)\n* Lasso Regression (L1 penalty)\n* SVR\n* Decision Tree\n* Random Forest\n* XGBoost\n* Gradient Boosting\n* CatBoost","metadata":{}},{"cell_type":"code","source":"model_list = ['linreg', 'ridge', 'lasso', 'svr', 'dt', 'rf', 'xgb', 'gbr', 'cbr']\n\nsummary = pd.DataFrame({\n    'Model': model_list,\n    'R2': r2_list,\n    'RMSE': rmse_list\n})\nsummary.sort_values('RMSE')","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:56.376914Z","iopub.execute_input":"2022-06-17T16:34:56.377384Z","iopub.status.idle":"2022-06-17T16:34:56.393032Z","shell.execute_reply.started":"2022-06-17T16:34:56.377354Z","shell.execute_reply":"2022-06-17T16:34:56.391829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the table above we can see that CatBoost Regressor is the best model for our predictions. Now we can make test data predictions.","metadata":{}},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"y_pred = np.exp(cbr.predict(test_data_new))\n\n\nsubmission = pd.DataFrame({\n    'Id': id_test,\n    'SalePrice': y_pred\n})\n\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:56.394661Z","iopub.execute_input":"2022-06-17T16:34:56.395025Z","iopub.status.idle":"2022-06-17T16:34:56.421174Z","shell.execute_reply.started":"2022-06-17T16:34:56.394997Z","shell.execute_reply":"2022-06-17T16:34:56.420211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-17T16:34:56.422447Z","iopub.execute_input":"2022-06-17T16:34:56.423485Z","iopub.status.idle":"2022-06-17T16:34:56.435627Z","shell.execute_reply.started":"2022-06-17T16:34:56.423443Z","shell.execute_reply":"2022-06-17T16:34:56.434435Z"},"trusted":true},"execution_count":null,"outputs":[]}]}