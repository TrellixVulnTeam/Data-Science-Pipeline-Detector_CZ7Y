{"cells":[{"metadata":{},"cell_type":"markdown","source":"---\n\n<h1 style=\"text-align: center;font-size: 30px;\">Exploratory Data Analysis</h1>\n\n---\n\n<center><img style=\"width: 700px;\" src=\"https://www.researchify.co.uk/generator/data.gif\"></center>\n\n---\n<i>image from Google</i>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)\n\n<p>There was an a quotes of Epictetus, something like that</p>\n<h3>Know, first, Who you are, and then adorn yourself accordingly.\n~ Epictetus\n </h3>\n<p>Exploratory Data Analysis (EDA) something like that, know your data first and do feature engineering accordingly. To knowing your data in data science, its called Exploratory Data Analysis in short EDA.</p>\n<p> So basically Exploratory Data Analysis is the examination of data and find out relationships among variables through both numerical and graphical methods.  EDA is a task of analyze data, investigate data to the way we find out patterns, relationship, outliers and distribution of data. It is one of the most important task for data scientist to do data science task. </p>\n<p> Doing EDA is good practice to know the data first. And find out inside information, relations as much as posible</p>\n<p> To starting data science task normally we start it with EDA. So solving \n<a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\">House Prices: Advanced Regression Techniques</a> we starting it with here. This is <b> part-1 </b> of this problem.</p>\n<p>Other part of this solution for Feature Engineering & Prediction, You find it here.</p>\n<a href=\"https://www.kaggle.com/snanilim/feature-engineering-prediction-house-prices\">Feature Engineering & Prediction - House Prices</a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Steps of EDA:\n<ol>\n    <li>Basic Analysis with Pandas\n        <ol>\n            <li> Descriptive analysis </li>\n        </ol>\n    </li>\n    <li>Missing Values</li>\n    <li>Features Classification\n        <ol>\n            <li>Numerical Features</li>\n            <li>Categorical Features</li>\n        </ol>\n    </li>\n    <li>Univariate Analysis\n        <ol>\n            <li>Analyze Date Time Columns</li>\n            <li>Univariate Analysis of Numerical Features</li>\n        </ol>\n    </li>\n    <li>Bivariate analysis\n        <ol>\n            <li>Bivariate analysis of Numerical Features</li>\n            <li>Bivariate analysis of Categorical Features</li>\n        </ol>\n     </li>\n    <li>Correlation coefficients</li>\n    <li>Outliers</li>\n    <li>Central tendency and distribution of target columns</li>\n</ol>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Goals of EDA\n<ol>\n    <li>Find out patterns</li>\n    <li>Find out Relationship among variables</li>\n    <li>Find out Anomalies</li>\n    <li>Check Assumptions</li>\n    <li>Frame Hypothesis</li>\n</ol>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"---\n\n<h3 style=\"text-align: center;font-size: 20px;\">In data science process, you can see where is EDA.</h3>\n\n---\n\n<center><img style=\"width: 700px;\" src=\"https://blog.camelot-group.com/wp-content/uploads/2019/03/Picture2.png\"></center>\n\n---\n<i>image from Google</i>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Import necessary libraries and files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.stats as st\npd.set_option('display.max_columns', None)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nimport missingno as msno\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To starts with,I import necessary libraries (which i done in above cell) and loaded the data set with pandas \"read_csv\" method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just load the train dataset. We have no plans for a test dataset here.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 1. Basic Analysis with Pandas","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With help of ‚Äú .head()‚Äù method of pandas library which returns first five observations of the data set.Similarly ‚Äú.tail()‚Äù returns last five observations of the data set.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"for total number of rows and columns we use pandas \".shape()\" method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So our Dataset contains 1460 rows and 81 columns ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"At first i want to drop \"Id\" column. which is unnecessary for me now","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = train_data.drop(columns=['Id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_rows = train_data.shape[0]\ntotal_columns = train_data.shape[1]\nprint('Total rows', total_rows)\nprint('Total columns', total_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now i have 1460 rows and 80 columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now let's try printing out column names using columns:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_columns = train_data.columns\nall_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see our all columns name with the respect of our target column \"SalePrice\". its help us to operate some loop and copy paste columns name üòÅ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p>Few key insights just by looking at dependent variable are as follows:</p>\n<p>Now we want to see top 5 and lowest 5 saleprices. basically its has no direct benefit but we want to see those to know whats are top and lowest prices on this place.</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"top 5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['SalePrice'].sort_values(ascending=False).head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"lowesst 5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['SalePrice'].sort_values(ascending=True).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_data['SalePrice'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data['SalePrice'].value_counts().head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Our target column or dependent column is continuous numerical in nature\n* top 20 repeated or most saling, \"salePrice\". if u have 130000 to 160000 u have most chance to buy a house","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We can use the info() method to output some general information about the dataframe:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation:\n\n* Alley, PoolQC, Fence, MiscFeature features contain a number of null values in that order for the training dataset. and all are categorical columns\n\n* 38 features are of float and int type and 43 features are type of object\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Descriptive analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The describe() function in pandas is very handy in getting various summary statistics.This function returns the count, mean, standard deviation, minimum and maximum values and the quantiles of the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Here you can notice the maximum case mean value is greater than the median value of each column which is represented by 50%(50th percentile) in the index column.\n* Some are weird also like \"MasVnrArea\" mean is 103.685262 but median is 0.00000\n* There is notably a large difference between 75th %tile and max values of predictors ‚ÄúMSSubClass‚Äù,‚ÄùLotArea‚Äù,‚ÄùMasVnrArea‚Äù so on.\n* Thus observations suggests that there are extreme values-Outliers in our data set.\n* One important thing is that in salePrice our minimum price is larger than zero. We don't have one of those personal traits that would destroy our model.\n* and other things is that all most all int columns have value","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe(include=['object', 'bool'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* there lots of columns have null value\n* we can see the unique row. from this we can get some basic idea about categorical columns ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data['SalePrice'].mean())\nprint('-' * 20)\nprint(train_data['SalePrice'].median())\nprint('-' * 20)\nprint('Difference between mean and median', train_data['SalePrice'].mean() - train_data['SalePrice'].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Estimate Skewness and Kurtosis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.skew()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* A skewness value of 0 in the output denotes a symmetrical distribution.\n* Normal skewness (mean = median = mode)\n* A negative skewness value in the output indicates an asymmetry and the tail will be larger towards the left hand side of the distribution.\n* Negative skewness (mean < median < mode)\n* A positive skewness value in the output indicates an asymmetry and the tail will be larger towards the right hand side of the distribution.\n* Positive skewness (mode < median < mean)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.kurt()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* A normal distribution is called mesokurtic and has kurtosis of or around zero\n* Mesokurtic [normal] k = 0\n* A platykurtic distribution has negative kurtosis and tails are very thin compared to the normal distribution.\n* platykurtic [negative] k < 0\n* Leptokurtic distributions have kurtosis greater than 3 and the fat tails mean that the distribution produces more extreme values and that it has a relatively small standard deviation\n* Leptokurtic [positive] k > 0\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 7))\nsns.distplot(train_data.skew(),color='green',axlabel ='Skewness')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 7))\nsns.distplot(train_data.kurt(),color='orange',axlabel ='Kurtosis',norm_hist= False, kde = True,rug = False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Missing value of each columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"total = train_data.isnull().sum().sort_values(ascending=False)\npercent = ((train_data.isnull().sum() / total_rows) * 100).sort_values(ascending=False)\nnull_data = pd.concat([total, percent], axis=1,join='outer', keys=['Null count', 'Percentage %'])\nnull_data.index.name ='Columns'\nnull_data = null_data[null_data['Null count'] > 0].reset_index()\nnull_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>here we can see our all null columns and their null amount and percentage. top 5 null columns are </p>\n<ul>\n<li> PoolQC </li>\n<li> MiscFeature </li>\n<li> Alley </li>\n<li> Fence </li>\n<li> FireplaceQu </li>\n</ul>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**We'll consider that when more than 20% to 30% of the data is missing, we should delete the corresponding variable and pretend it never existed. but we will do it in feature engineering part.**\n\n**Moreover, looking closer at the variables, we could say that variables like 'PoolQC', 'MiscFeature' and 'FireplaceQu' are strong candidates for outliers**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns = null_data['Columns']\nnull_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing value of each rows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"null_rows = train_data.isnull().sum(axis=1).sort_values(ascending=False).head(20)\n# null_rows = null_rows.head(20)\nnull_rows","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_rows.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Missing value of each rows percentage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"((null_rows *100) / 80)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"they shown us of top 20 missing values rows and their percentages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.loc[null_rows.index].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"almost 18% of all those rows don't have values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"now we can do some plotting with those null value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# msno.bar.__code__.co_varnames\nmsno.bar(train_data.sample(1460), labels=True, fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.heatmap(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"msno.dendrogram(train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"missingno is a awesome library for showing missing value. with the help of dendrogram, heatmap and bar chat you can plot missing value very well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_data = train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we see that we have lots of null values. we want to know that how much effect they have in our target column \"salePrice\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"we will convert or map our null value into 1 and have value into 0","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**This plot list is little bit longer if u want to see those please click \"Show\" button****","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for column in null_columns:\n    copy_data[column] = np.where(train_data[column].isnull(), 1, 0)\n    \n    plot_data = copy_data.groupby(by=[column])['SalePrice'].median()\n    plot_data = pd.DataFrame(plot_data)\n    plot_data = plot_data.reset_index()\n    sns.barplot(x=plot_data[column], y=plot_data['SalePrice'], data=plot_data, palette=\"Blues_d\")\n    \n    plt.xticks(plot_data[column], ('value(0)', 'Null(1)'))\n    plt.xlabel(column)\n    plt.ylabel('SalePrice')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"see some null value have effect on our target column sale prices","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"in code there was an a shortcut but want to use xtricks thats why i code like this. if u know shortcut way to present it let me know in comments","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 3. Features Classification","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3.1. Numerical Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_data = train_data.select_dtypes(include=[np.number])\nnumerical_columns = numerical_data.columns\nnumerical_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"though 'OverallQual' is not numeric feature but we keep it here to see some relation and plotting","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"discrete_column = []\ncontinious_column = []\nyear_column = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in numerical_columns:\n    if 'Year' in column or 'Yr' in column:\n#         print(column)\n        year_column.append(column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in numerical_columns:\n    if column != 'SalePrice' and column not in year_column:\n        if len(train_data[column].unique()) < 25:\n            discrete_column.append(column)\n        else:\n            continious_column.append(column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"discrete_column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"continious_column","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"its little bit hard to find out discrete and continuous columns when u don't know much about your all data. that's why i do this kind of things","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3.2. Categorical Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_data = train_data.select_dtypes(include=[np.object])\ncategorical_columns = categorical_data.columns\ncategorical_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Univariate analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Univariate visualization‚Ää‚Äî‚Ääprovides summary statistics for each field in the raw data set","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Analyze Date Time Columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"first we trying to find out date time variable and then trying to plotting them and analyzing them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"year_column = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in numerical_columns:\n    if 'Year' in column or 'Yr' in column:\n#         print(column)\n        year_column.append(column)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[year_column]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in year_column:\n    plt.figure(figsize=(10, 7))\n    train_data.groupby(by=[column])['SalePrice'].median().plot(color = ['c', 'y'])\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* YearBuilt YearRemodAdd GarageYrBlt sown us when year is increasing price is also increasing\n* but in YrSold we see something different we need to find out why","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"year_data = train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor column in year_column:\n    if column != 'YrSold':\n        plt.figure(figsize=(10, 7))\n        year_data[column] = year_data['YrSold']-year_data[column]\n        \n        sns.scatterplot(x=year_data[column], y=year_data['SalePrice'], data=year_data)\n        plt.xlabel(column)\n        plt.ylabel('SalePrice')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we can see its looks ok coz when a house is 140 years old that has less price and we see it in other features also","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Univariate Analysis of Numerical Columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**This plot list is little bit longer if u want to see those please click \"Show\" button****","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for column in continious_column:\n    plt.figure(figsize=(10, 7))\n    train_data[column].plot.hist(color = \"skyblue\")\n    plt.xlabel(column)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see our continuous variable is not well distributed. let's do something on this","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### For this Using logarithm transformation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_data = train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in continious_column:\n    if 0 in copy_data[column].unique():\n        pass\n    else:\n#         print(column)\n        plt.figure(figsize=(10, 7))\n        con_data = np.log(copy_data[column])\n        con_data.plot.hist(color = \"skyblue\")\n        plt.xlabel(column)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we can see have some distribution","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n# 5. Bivariate analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"we can also say it **Bivariate visualization**. Bivariate visualization‚Ää‚Äî‚Ääis performed to find the relationship between each variable in the dataset and the target variable of interest","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Bivariate analysis of Numerical Columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**This plot list is little bit longer if u want to see those please click \"Show\" button****","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"for column in discrete_column:\n    plt.figure(figsize=(10, 7))\n    train_data.groupby(by=[column])['SalePrice'].median().plot.bar(color = \"skyblue\")\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"its told us that **OverallQual** have good relation with sale price, when quality increases sale price exponentially increases. and also have some good relation in FullBath, TotRMSAdvGrd, GarageCars with sale price as well","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in continious_column:\n    if 0 in train_data[column].unique():\n        pass\n    else:\n        plt.figure(figsize=(10, 7))\n        sns.scatterplot(x=train_data[column], y=train_data['SalePrice'], data=train_data)\n        \n        plt.xlabel(column)\n        plt.ylabel('SalePrice')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two values with bigger 'GrLivArea' and 'LotFrontage' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_data = train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the value zero doesn't allow us to do log transformations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_data['SalePrice'] = np.log(copy_data['SalePrice'])\nfor column in continious_column:\n    if 0 in copy_data[column].unique():\n        pass\n    else:\n        plt.figure(figsize=(10, 7))\n        copy_data[column] = np.log(copy_data[column])\n        sns.scatterplot(x=copy_data[column], y=copy_data['SalePrice'], data=copy_data)\n        \n        plt.xlabel(column)\n        plt.ylabel('SalePrice')\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* npw its looks like something better. and shown have some relation with salePrice\n* It seems that 'SalePrice' and 'GrLivArea' are really old friends, with a linear relationship.\n* Moreover, it's clear that sometimes 'TotalBsmtSF' closes in itself and gives zero credit to 'SalePrice'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5.2 Bivariate analysis of Categorical Columns","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**This plot list is little bit longer if u want to see those please click \"Show\" button****","execution_count":null},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"for column in categorical_columns:\n    plt.figure(figsize=(10, 7))\n    copy_data.groupby(by=column)['SalePrice'].median().plot.bar(color = \"skyblue\")\n    \n    plt.xlabel(column)\n    plt.ylabel('SalePrice')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"it's shown us that some are categorical features have some correlation with 'salePrice'. but most of them are not correlated with 'saleprice'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n'OverallQual' is a ordinal categorical type. we can use boxplot to plot it, to see correlation with 'SalePrice'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'OverallQual'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Correlation coefficients","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_corr = numerical_data.corr()\nfeature_corr['SalePrice'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 12))\nsns.heatmap(feature_corr,square = True,  vmax=0.8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"heatmap is a good way to understand correlation. but actually here we see nothing. for that we can select first 11 strongly correlated columns to see heatmap correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 11\ncols = feature_corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncols\ncm = np.corrcoef(train_data[cols].values.T) # transformed data\ncm\nf , ax = plt.subplots(figsize = (14,12))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True, linecolor=\"white\",xticklabels = cols.values ,annot_kws = {'size':12},yticklabels = cols.values)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.\n* 'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.\n* its told us that '**OverallQual**', '**GrLivArea**' and '**TotalBsmtSF**' are strongly correlated with '**SalePrice**'.\n* '**GarageCars**' and '**GarageArea**' are strongly correlated.\n* '**TotRmsAdvGrd**' and '**GrLivArea**' are strongly correlated.\n* '**istFlrSF**' and '**TotalBsmtSF**' are strongly correlated. \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"we can shown them into pai plot also to see correlation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea','GarageArea','FullBath','YearBuilt','YearRemodAdd']\nsns.pairplot(train_data[columns],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Outliers is one of the most important task in EDA. Outliers have much influence in model. Here we get more important insights about our features and also can find out which point of data are not following other observation. While doing the EDA a quick visual way to check for the outliers for continuous data is via scatterplots and boxplots.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(train_data['SalePrice'][:,np.newaxis]);\n# saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10] # specifies that you want to slice out a 1D vector of length 97 from a 2D array.\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Low range values are similar and not too far from 0.\n* High range values are far from 0 and the 7.something values are really out of range.\n\nFor now, we'll not consider any of these values as an outlier but we should be careful with those two 7.something values.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# copy_data[column] = np.log(copy_data[column])\nplt.figure(figsize=(10, 7))\ntrain_data.boxplot(column='SalePrice', notch=True, vert=False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_data = train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_data['SalePrice'] = np.log(copy_data['SalePrice'])\nplt.figure(figsize=(10, 7))\ncopy_data.boxplot(column='SalePrice', notch=True, vert=False)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the value zero doesn't allow us to do log transformations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in continious_column:\n    if 0 in copy_data[column].unique():\n        pass\n    else:\n        plt.figure(figsize=(10, 7))\n        copy_data[column] = np.log(copy_data[column])\n        copy_data.boxplot(column=column , notch=True, vert=False)\n        \n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"with the help of boxplot we can see lots of outliers on those columns. we will taken care of in feature engineering section","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 8. Central tendency and distribution of target columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_data['SalePrice']\n\nplt.figure(figsize=(10, 7))\nplt.figure(1); plt.title('Johnson SU')\nsns.distplot(y, kde=True, fit=st.johnsonsu, color='#636efa')\n\nplt.figure(figsize=(10, 7))\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=True, fit=st.norm, color='#636efa')\n\nplt.figure(figsize=(10, 7))\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=True, fit=st.lognorm, color='#636efa')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* In normal \n* Deviate from the normal distribution.\n* Have appreciable positive skewness.\n* Show peakedness.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"It is apparent that SalePrice doesn't follow normal distribution, so before performing regression it has to be transformed. While log transformation does pretty good job, best fit is unbounded Johnson distribution.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 7))\nres = stats.probplot(train_data['SalePrice'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"copy_data = train_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#applying log transformation\nplt.figure(figsize=(10, 7))\ncopy_data['SalePrice'] = np.log(copy_data['SalePrice'])\nres = stats.probplot(copy_data['SalePrice'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness:\", train_data['SalePrice'].skew())\nprint('-' * 30)\nprint(\"Kurtosis:\", train_data['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(5, 3))\nplt.figure(figsize=(10, 7))\nplt.hist(train_data['SalePrice'],orientation = 'vertical',histtype = 'bar', color ='#21bf73')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(5, 3))\nplt.figure(figsize=(10, 7))\ntarget = np.log(train_data['SalePrice'])\ntarget.skew()\nplt.hist(target,color='#21bf73')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its looks much better now, after log well distributed.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Conclusion:\nThat's all. I think we do some good exploratory data analysis here.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h2>Note: If you think it's helpful please <i style=\"color: red;\">upvoted</i>. And inspire me to do more. Thank you</h2>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3>Acknowledgement:</h3>\n<ol>\n    <li><a href=\"https://towardsdatascience.com/intro-to-descriptive-statistics-252e9c464ac9\">https://towardsdatascience.com/intro-to-descriptive-statistics-252e9c464ac9</a></li>\n    <li><a href=\"https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15\">https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=FLuqwQgSBDw&list=PLupD_xFct8mFDeCqoUAWZpUddeqmT28_L\">https://www.youtube.com/watch?v=FLuqwQgSBDw&list=PLupD_xFct8mFDeCqoUAWZpUddeqmT28_L</a></li>\n    <li><a href=\"https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python\">https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python</a></li>\n    <li><a href=\"https://www.kaggle.com/pavansanagapati/a-simple-tutorial-on-exploratory-data-analysis\">https://www.kaggle.com/pavansanagapati/a-simple-tutorial-on-exploratory-data-analysis</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=ioN1jcWxbv8&list=PLZoTAELRMXVMcRQwR5_J8k9S7cffVFq_U\">https://www.youtube.com/watch?v=ioN1jcWxbv8&list=PLZoTAELRMXVMcRQwR5_J8k9S7cffVFq_U</a></li>\n</ol>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}