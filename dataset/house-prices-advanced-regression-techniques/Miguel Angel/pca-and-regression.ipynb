{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d680f230-0c97-3375-f47a-16cb2cb12e77"},"source":"**The idea is:**\n\n - Feature reduction with PCA\n - Data transformation (log, hot encoding, nan)\n - Test different regression models\n\n**Things found:**\n\n- Applying log transformation really increases the accuracy.\n- Using PCA with 36 components makes the learning and testing much (much much) faster.\n- Removing columns with more than 1000 NaNs gives better result than applying \"mean\" to them.\n- There are outliers. Instead of removing them, using Huber seems to provide a good result. Huber is a model robust to outliers."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c2d0b1a9-a6e3-7ace-ab73-6375fccb210a"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import KFold\nfrom sklearn import linear_model\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"},{"cell_type":"markdown","metadata":{"_cell_guid":"acc6da78-9491-216e-56c1-61d2f3e6da81"},"source":"## Data Load ##\n\nI mix data and test to manipulate all the data just once. SalePrice is extracted to its own variable \"labels\". Finally, SalesPrice is remove from data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6fad5e5f-097d-0916-8549-8ada7d29cdfe"},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')\nlabels=train[\"SalePrice\"]\ntest = pd.read_csv('../input/test.csv')\ndata = pd.concat([train,test],ignore_index=True)\ndata = data.drop(\"SalePrice\", 1)\nids = test[\"Id\"]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e769e045-f25a-a25d-047a-a03b931e43a4"},"outputs":[],"source":"train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"35a5b9cb-3383-ba7b-efb5-4662bdd46c47"},"outputs":[],"source":"# Count the number of rows in train\ntrain.shape[0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"041d9472-1f4a-3b15-d624-cfb9ea59710b"},"outputs":[],"source":"# Count the number of rows in total\ndata.shape[0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb27ffcc-3f7b-d0e1-3887-e955307c21b0"},"outputs":[],"source":"# Count the number of NaNs each column has.\nnans=pd.isnull(data).sum()\nnans[nans>0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"92ba1dbf-0b3b-7d19-ba9f-702c59991612"},"outputs":[],"source":"# Remove id and columns with more than a thousand missing values\ndata=data.drop(\"Id\", 1)\ndata=data.drop(\"Alley\", 1)\ndata=data.drop(\"Fence\", 1)\ndata=data.drop(\"MiscFeature\", 1)\ndata=data.drop(\"PoolQC\", 1)\ndata=data.drop(\"FireplaceQu\", 1)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1978e020-6ca4-124e-1063-3c8262fbc3e7"},"outputs":[],"source":"# Count the column types\ndata.dtypes.value_counts()"},{"cell_type":"markdown","metadata":{"_cell_guid":"82816c00-ff5e-247e-2a55-04f96e2ed313"},"source":"## Data Manipulation ##\n\n- Apply hot encoding, convert categorical variable into dummy/indicator variables.\n- Fill NaN with median for that column.\n- Log transformation.\n- Change -inf to 0."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd7155af-7e2e-12a0-5ef3-030bfc16a945"},"outputs":[],"source":"#Â One Hot Encoding and nan transformation\ndata = pd.get_dummies(data)\n\nimp = Imputer(missing_values='NaN', strategy='mean', axis=0)\ndata = imp.fit_transform(data)\n\n# Log transformation\ndata = np.log(data)\nlabels = np.log(labels)\n\n# Change -inf to 0 again\ndata[data==-np.inf]=0"},{"cell_type":"markdown","metadata":{"_cell_guid":"4affb7d5-578e-6862-243f-630f21a05934"},"source":"## Feature reduction ##\n\nThere are many features, so I am going to use PCA to reduce them. The idea is to start with n_components = number of columns. Then select the number of components that add up to 1 variance_ratio."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6954036d-1415-4bcf-d67b-ac7a410f8a65"},"outputs":[],"source":"pca = PCA(whiten=True)\npca.fit(data)\nvariance = pd.DataFrame(pca.explained_variance_ratio_)\nnp.cumsum(pca.explained_variance_ratio_)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e9a015a1-642e-0a50-2219-2e2f92db582a"},"outputs":[],"source":"pca = PCA(n_components=36,whiten=True)\npca = pca.fit(data)\ndataPCA = pca.transform(data)"},{"cell_type":"markdown","metadata":{"_cell_guid":"8901b708-1465-3e16-aca4-f3ea1bac97d8"},"source":"## Data Model Selection ##\n\nSimple test to run multiple models against our data. First, with raw features. No PCA.\n\nLabels need to be modified with log, same as we did we data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a2d1aafa-ea0a-5753-8886-581e88fe1cc0"},"outputs":[],"source":"# Split traing and test\ntrain = data[:1460]\ntest = data[1460:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a85a2fa0-59ec-c055-f088-ab0a29ea68bd"},"outputs":[],"source":"# R2 Score\n\ndef lets_try(train,labels):\n    results={}\n    def test_model(clf):\n        \n        cv = KFold(n_splits=5,shuffle=True,random_state=45)\n        r2 = make_scorer(r2_score)\n        r2_val_score = cross_val_score(clf, train, labels, cv=cv,scoring=r2)\n        scores=[r2_val_score.mean()]\n        return scores\n\n    clf = linear_model.LinearRegression()\n    results[\"Linear\"]=test_model(clf)\n    \n    clf = linear_model.Ridge()\n    results[\"Ridge\"]=test_model(clf)\n    \n    clf = linear_model.BayesianRidge()\n    results[\"Bayesian Ridge\"]=test_model(clf)\n    \n    clf = linear_model.HuberRegressor()\n    results[\"Hubber\"]=test_model(clf)\n    \n    clf = linear_model.Lasso(alpha=1e-4)\n    results[\"Lasso\"]=test_model(clf)\n    \n    clf = BaggingRegressor()\n    results[\"Bagging\"]=test_model(clf)\n    \n    clf = RandomForestRegressor()\n    results[\"RandomForest\"]=test_model(clf)\n    \n    clf = AdaBoostRegressor()\n    results[\"AdaBoost\"]=test_model(clf)\n    \n    clf = svm.SVR()\n    results[\"SVM RBF\"]=test_model(clf)\n    \n    clf = svm.SVR(kernel=\"linear\")\n    results[\"SVM Linear\"]=test_model(clf)\n    \n    results = pd.DataFrame.from_dict(results,orient='index')\n    results.columns=[\"R Square Score\"] \n    results=results.sort(columns=[\"R Square Score\"],ascending=False)\n    results.plot(kind=\"bar\",title=\"Model Scores\")\n    axes = plt.gca()\n    axes.set_ylim([0.5,1])\n    return results\n\nlets_try(train,labels)"},{"cell_type":"markdown","metadata":{"_cell_guid":"f8f5cc3a-09bf-629b-c8de-0c3ef68e226b"},"source":"Now, let's try the same but using data with PCA applied."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"af6df7bc-a49f-a5d8-7dc1-391aa0b9b2e3"},"outputs":[],"source":"# Split traing and test\ntrain = dataPCA[:1460]\ntest = dataPCA[1460:]\n\nlets_try(train,labels)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77b2423f-1b69-0d57-8cc7-ebf3b6d54b23"},"outputs":[],"source":"cv = KFold(n_splits=5,shuffle=True,random_state=45)\n\nparameters = {'alpha': [1000,100,10],\n              'epsilon' : [1.2,1.25,1.50],\n              'tol' : [1e-10]}\n\nclf = linear_model.HuberRegressor()\nr2 = make_scorer(r2_score)\ngrid_obj = GridSearchCV(clf, parameters, cv=cv,scoring=r2)\ngrid_fit = grid_obj.fit(train, labels)\nbest_clf = grid_fit.best_estimator_ \n\nbest_clf.fit(train,labels)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3c100e90-e9a9-c028-22a7-4c74ee31ab0c"},"outputs":[],"source":"# Make predictions\n\npredictions = best_clf.predict(test)\npredictions = np.exp(predictions)\nsub = pd.DataFrame({\n        \"Id\": ids,\n        \"SalePrice\": predictions\n    })\nsub.to_csv(\"prices_submission.csv\", index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}