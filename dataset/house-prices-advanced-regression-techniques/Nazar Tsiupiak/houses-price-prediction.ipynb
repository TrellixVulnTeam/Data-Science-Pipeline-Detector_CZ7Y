{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Prices Prediction"},{"metadata":{},"cell_type":"markdown","source":"<h3>Hello friend, today I'm going to make my spot on house prices prediction problems, so here we go)</h3>"},{"metadata":{},"cell_type":"markdown","source":"![](https://media.remax-dev.booj.io/91319a69-7a4b-3a88-83f0-e1a5be6c4d33/06_MiracleHomes.jpg)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingRegressor as GBR\nfrom sklearn.model_selection import train_test_split\n\n# Ignore all warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration\nLet's load and exemaine our data"},{"metadata":{},"cell_type":"markdown","source":"![](https://images.immediate.co.uk/production/volatile/sites/7/2018/04/BBC-WH9-final-red-2-3239864.jpg?quality=90&resize=620,413)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load train test dataframes\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntrain_size = int(len(train)*0.8)\n# Sort by time and drop target and id columns\ntrain = train.sort_values(['YrSold', 'MoSold'], axis=0).reset_index(drop=True)\ntrain_X = train.drop(['SalePrice', 'Id'], axis=1).loc[:train_size-1, :]\nvalid_X = train.drop(['SalePrice', 'Id'], axis=1).loc[train_size:, :]\ntrain_y = train['SalePrice'][:train_size]\nvalid_y = train['SalePrice'][train_size:]\ntest_X = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv', index_col='Id')\ntest_X = test_X.sort_values(['YrSold', 'MoSold'], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Na Values\n\n\n![](https://image.freepik.com/free-photo/fill-missing-parts-fragment-white-jigsaw-concept-puzzle-succeed_33807-777.jpg)"},{"metadata":{},"cell_type":"markdown","source":"As we see many columns has Na values. My way to deal with that is next: replace all NA in categorical columns with None or similar value and numerical features with 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomImputer:\n    def __init__(self):\n        pass\n    \n    def fit(self, data):\n        cat_cols = data.columns[data.dtypes == 'object']\n        self.impute_cols = cat_cols[data[cat_cols].isna().sum() > 0] \n    \n    def transform(self, data):\n        for column in self.impute_cols:\n            if data[column].isin(['None', 'No', 'Othr']).sum() > 0:\n                replace_value = data.loc[data[column].isin(['None', 'No', 'Othr']), column].unique()[0]\n                data[column] = data[column].fillna(replace_value)\n            else:\n                data[column] = data[column].fillna('None')\n        \n        return data\n                \n    def fit_transform(self, data):\n        self.fit(data)\n        return self.transform(data)\n    \n    def get_params(self):\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"na_cols = train_X.columns[(train_X.isna().sum() > 0).values]\nprint(f'Columns with NA: {na_cols}')\n# Replace NA values in categorical features with None and similar values        \ntrain_X.loc[:, na_cols] = CustomImputer().fit_transform(train_X.loc[:, na_cols])\ntest_X.loc[:, na_cols] = CustomImputer().fit_transform(test_X.loc[:, na_cols])\nvalid_X.loc[:, na_cols] = CustomImputer().fit_transform(valid_X.loc[:, na_cols])\n# Replace NA values in continuos features with 0 value\ntrain_X.loc[:, na_cols[train_X.loc[:, na_cols].dtypes == 'float64']] = train_X.loc[:, na_cols[train_X.loc[:, na_cols].dtypes == 'float64']].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.columns[train_X.isna().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Generation"},{"metadata":{},"cell_type":"markdown","source":"![](https://pmp-practitioners.com/wp-content/uploads/2019/02/Brainstorming.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Lets try make some features. Feature engineering is very important step in improving our model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sold_last_mnth(df):\n    \"\"\" Generates sold houses lats month feature \"\"\"\n    timeline = pd.to_datetime(df['YrSold'].astype('str') + '-' + df['MoSold'].astype('str'), format='%Y-%m')\n    tm_ln_indexed = pd.Series(data=timeline.index ,index=timeline.values)\n    tm_ln_sold_last_mnth = tm_ln_indexed.rolling('62d').count() - tm_ln_indexed.rolling('31d').count()\n    sold_lst_mnth = pd.Series(data=tm_ln_sold_last_mnth.values, index=tm_ln_indexed.values).reindex(df.index)\n    return sold_lst_mnth\n\ndef get_fireplaces_per_room(df):\n    \"\"\" Generates number of fireplaces per room feature \"\"\"\n    fp_per_house = df['Fireplaces'] / df['TotRmsAbvGrd']\n    return fp_per_house\n\ndef get_quality_per_room(df):\n    \"\"\" Generates quality per room feature \"\"\"\n    qual_per_room = df['OverallQual'] / df['TotRmsAbvGrd']\n    return qual_per_room","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add sold houses last month feature\ntrain_X = train_X.assign(Sold_Lst_Mnth=lambda df: get_sold_last_mnth(df))\nvalid_X = valid_X.assign(Sold_Lst_Mnth=lambda df: get_sold_last_mnth(df))\ntest_X = test_X.assign(Sold_Lst_Mnth=lambda df: get_sold_last_mnth(df))\n# Add number of fireplaces per room feature\ntrain_X = train_X.assign(FireplacesPerRm=lambda df: get_fireplaces_per_room(df))\nvalid_X = valid_X.assign(FireplacesPerRm=lambda df: get_fireplaces_per_room(df))\ntest_X = test_X.assign(FireplacesPerRm=lambda df: get_fireplaces_per_room(df))\n# Add quality per room feature\ntrain_X = train_X.assign(QualPerRm=lambda df: get_quality_per_room(df))\nvalid_X = valid_X.assign(QualPerRm=lambda df: get_quality_per_room(df))\ntest_X = test_X.assign(QualPerRm=lambda df: get_quality_per_room(df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Target leakage"},{"metadata":{},"cell_type":"markdown","source":"![](https://www.verisk.com/siteassets/media/images/verisk_commercial_premium_leakage_analysis.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Lets look at the data and check whether is there target leakage. It seems that **YrSold(Year when house is sold)** and **MoSold(Month when house is sold)** will cause target leakage, as we wil not have such values when predicting real world house prices."},{"metadata":{"trusted":true},"cell_type":"code","source":"ta_leakage_cols = ['YrSold', 'MoSold']\ntrain_X = train_X.drop(ta_leakage_cols, axis=1)\nvalid_X = valid_X.drop(ta_leakage_cols, axis=1)\ntest_X =test_X.drop(ta_leakage_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define catgerical and numerical features\ncat_features = train_X.columns[train_X.dtypes == 'object']\nnum_features = train_X.columns[(train_X.dtypes == 'int64') | (train_X.dtypes == 'float64')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Visualization"},{"metadata":{},"cell_type":"markdown","source":"First lets take a look of how our numerical features distributed"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X[num_features].hist(bins=15, figsize=(20, 20))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Countplots of categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(len(cat_features)//5 + 1, 5)\nfig.set_size_inches(20, 30)\nfor idx, feature in enumerate(cat_features):\n    sns.countplot(data=train_X, x=feature, ax=ax[idx//5, idx%5])\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heatmap!!!)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 18))\nsns.heatmap(pd.concat([train_X[num_features], train_y], axis=1).corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next lets exemine how our Sale Prices depends on different categorical columns "},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(len(cat_features)//3+1, 3, figsize=(20, len(cat_features)*2))\nfor idx, feature in enumerate(cat_features):\n    sns.violinplot(x=feature, y=train_y, data=train_X, ax=ax[idx//3, idx%3])\n    ax[idx//3, idx%3].set_title(f'Violin plot of {feature} x SalePrice')\n    ax[idx//3, idx%3].xaxis.set_tick_params(rotation=45)\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And numearical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn-darkgrid')\nfig, ax = plt.subplots(len(num_features)//3+1, 3, figsize=(36, len(cat_features)*3))\nfor idx, num_feature in enumerate(num_features):\n    sns.scatterplot(data=train_X, x=num_feature, y=train_y, ax=ax[idx//3, idx%3])\n    ax[idx//3, idx%3].set_title(f'Regplot of Sale Price x {num_feature}', size=24)\n    ax[idx//3, idx%3].set_xlabel(num_feature, size=20)\n    ax[idx//3, idx%3].set_ylabel('Sale Price', size=20)\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As we can see the highest influence on Sale Price have Quality features(Overall, Bath, Fireplace, etc), Conditions(Overall, Basement, etc.), Garage Area, 1stFlSF(Square metres on the 1st floor), Roof Materials, Garage Type and Neighbourhoods. \n#### So, if you want a home with the area like Disneyland, a roof that can protect you from the alien laser during the alien invasion, with best conditions and porcelain birds floating in an artificial pond, be next door to Keanu Reeves and have Big High-Quality Garage, where you and Keanu can put your cars and bikes after fascinating travel, you should pay a big amount of money for that( I hope such houses exist) "},{"metadata":{},"cell_type":"markdown","source":"# Data Transformation"},{"metadata":{},"cell_type":"markdown","source":"![](https://paulitaylor.files.wordpress.com/2016/10/lessons-in-rapid-experiments-and-learning-from-failure.png?w=960)"},{"metadata":{},"cell_type":"markdown","source":"For ordinal data I'll use OrdinalEncoder(as it much more apropriated for Tree-Based models than OHE) and CatBoosEncoder for nominal, and for numerical Standard Scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom category_encoders import CatBoostEncoder, OrdinalEncoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets divide or categorical features on ordinal and nominal\nordinal_features = [\n    'LandSlope',\n    'Condition1',\n    'Condition2',\n    'ExterQual',\n    'ExterCond',\n    'BsmtQual',\n    'BsmtCond',\n    'BsmtFinType1',\n    'BsmtFinType2',\n    'HeatingQC',\n    'KitchenQual',\n    'FireplaceQu',\n    'GarageQual',\n    'GarageCond',\n    'PoolQC'\n]\nnominal_features = cat_features.drop(ordinal_features)\n# Next define transformer of nominal features\nnominal_transformer = pipeline.Pipeline(steps=[\n    ('cat_boost', CatBoostEncoder()),\n    ('scaler', StandardScaler())\n])\n# Nest feature transformations of numerical ans ordinal cols\nnum_transformer = pipeline.Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=0, missing_values=np.nan)),\n    ('scaler', StandardScaler())\n])\nnum_ord_transformer = ColumnTransformer(transformers=[\n    ('ordinal_transformer', OrdinalEncoder(), ordinal_features),\n    ('num_transformer', num_transformer, num_features)\n])\n# First transform our nominal features\ntrain_X_nominal = nominal_transformer.fit_transform(train_X[nominal_features], train_y)\nvalid_X_nominal = nominal_transformer.transform(valid_X[nominal_features])\ntest_X_nominal = nominal_transformer.transform(test_X[nominal_features])\n# Next transform ordinal and numerical features\ntrain_X_num_ord = num_ord_transformer.fit_transform(train_X)\nvalid_X_num_ord = num_ord_transformer.transform(valid_X)\ntest_X_num_ord = num_ord_transformer.transform(test_X)\n# Finally we concatenate such arrays\ntrain_X_transformed = np.concatenate((train_X_nominal, train_X_num_ord), axis=1)\nvalid_X_transformed = np.concatenate((valid_X_nominal, valid_X_num_ord), axis=1)\ntest_X_transformed = np.concatenate((test_X_nominal, test_X_num_ord), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Hyper Parameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"![](https://techcrunch.com/wp-content/uploads/2017/03/5daa8a29b65f5d8422aaeece44ed0a2d_original.jpg?w=1280&h=2300)"},{"metadata":{},"cell_type":"markdown","source":"Ohh, it's time for tuning... And it's nice, but takes a very long time(and even more if you have no some kind of strategy). I used presented one by Aarshay Jain: https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n\nI've already made some hyperparameter tuning for XGB, so I just leave tuning of learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'learning_rate': np.arange(0.01, 0.11, 0.01)\n}\noptimizer = GridSearchCV(XGBRegressor(n_estimators=500, tree_method='gpu_hist', max_depth=4, min_child_weight=3, gamma=0, colsample_bytree=0.4, subsample=0.7, learning_rate=0.05), params, cv=TimeSeriesSplit(n_splits=3), n_jobs=-1)\noptimizer.fit(train_X_transformed, train_y)\nvalid_score = mean_absolute_error(valid_y, optimizer.predict(valid_X_transformed))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(optimizer.cv_results_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Mean validation score is {valid_score}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training Model"},{"metadata":{},"cell_type":"markdown","source":"![](https://familyproject.sfsu.edu/sites/default/files/Training%20Image_Medium.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_params = {\n    'colsample_bytree': 0.4,\n    'n_estimators': 1000,\n    'min_child_weight': 3,\n    'max_depth': 6,\n    'subsample': 0.4,\n    'learning_rate': 0.01,\n    'gamma': 0,\n    'reg_lambda': 0.02\n}\nmodel = XGBRegressor(tree_method='gpu_hist', **xgb_params).fit(train_X_transformed, train_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model evaluation"},{"metadata":{},"cell_type":"markdown","source":"![](https://eige.europa.eu/sites/default/files/styles/eige_original_optimised/public/images/evaluation.jpg?itok=DPuDMaP8)"},{"metadata":{},"cell_type":"markdown","source":"First we calculate MAE on train and validation datasets, next we'll plot learning curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_score = mean_absolute_error(train_y, model.predict(train_X_transformed))\nprint(f'Mean train score is {train_score}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_score = mean_absolute_error(valid_y, model.predict(valid_X_transformed))\nprint(f'Mean validation score is {valid_score}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets plot learning curve to define whether we have high biase or variance(underfitting and overfitting)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator, X_train, y_train, cv, train_sizes=np.linspace(0.1, 1, 10)):\n    plt.style.use('seaborn-darkgrid')\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X_train, y_train, cv=cv, n_jobs=-1, train_sizes=train_sizes)\n    train_mean_scores = np.mean(train_scores, axis=1)\n    test_mean_scores = np.mean(test_scores, axis=1)\n    plt.title('Learning curve')\n    plt.plot(train_sizes, train_mean_scores, 'y', label='Train Learning curve')\n    plt.plot(train_sizes, test_mean_scores, 'b', label='Test Learning curve')\n    plt.legend()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_learning_curve(model, train_X_transformed, train_y, TimeSeriesSplit(n_splits=3))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seeems that there is overfitting problem, but due to small dataset underfitting is more frightening than overfitting(I suppose so, if I'm not correct please write it in comment)"},{"metadata":{},"cell_type":"markdown","source":"Lets look at feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = nominal_features.values.tolist() + ordinal_features + num_features.values.tolist() \nplt.figure(figsize=(25, 20))\nsns.barplot(y=features, x=model.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Obviously if we had large dataset or more features I would do some feature elimination(RFE for example), but here I'll leave all of them"},{"metadata":{},"cell_type":"markdown","source":"# Making Predictions\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgflip.com/zcyxp.jpg)"},{"metadata":{},"cell_type":"markdown","source":"Pour champagne ladies and gentelmans, after long way we can make our predictions and submit our results. Sure It's not finall version, for example numerical feature imputation can be improved(we just replaced all Na values with 0), also we can generate lot more features than we did"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_test = model.predict(test_X_transformed)\noutput = pd.DataFrame({'Id': test_X.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}