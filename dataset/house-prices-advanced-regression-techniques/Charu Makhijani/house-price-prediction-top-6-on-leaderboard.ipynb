{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Description\n\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Goal\nThe goal is to predict the Sales Price for the test dataset with the given features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Steps-\n    1. Introduction\n    2. Read Data\n    3. Explore Data\n        a. Data size\n        b. Strtucture of Data\n    4. Exploratory Data Analysis & Visualization\n        a. Check Missing Values\n        b. Correlation between missing values and Sales Price\n        c. Check numerical varibale\n        d. Temporal variables and correlation with Sales Price\n        e. Discrete variables and correlation with Sales Price\n        f. Continuous variables, skeweness and outliers\n        g. Categorial variables and correlation with Sales Price\n    5. Missing value Imputation\n    6. Handle Rare Categorial Features\n    7. Label Encoding\n    8. Scaling\n    9. Train models\n        a. Lasso Regression\n        b. Elastic Net Regression\n        c. Kernel Ridge Regression\n        d. Support Vector Regression\n        e. Gradient Boosting Regression\n        f. XGBoost Regression\n        g. Light GBM Regression\n        h. Random Forest Regression\n    10. Stack models\n    11. Visualize model scores","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import packages","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.svm import SVR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Read Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Display all columns of a dataframe\npd.pandas.set_option(\"display.max_columns\", None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Dataset for House Price Prediction is from below URL:\n#### https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Explore Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Dataset: \", train.shape)\nprint(\"Test Dataset: \", test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sales Price Distribution\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(6,5))\nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Sales Price\")\nax.set(title=\"Home Sales Price Distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see here that the SalePrice is skewed towards right. \nAnd it is a problem because most of the ML models don't perform well with skewed/un-normally distributed data. \nSo we have ton apply a log(x) tranform to fix the skew.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Skew and kurt\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(data=train.corr(), cmap=\"Blues\", square=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratiory Data Analysis","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Check Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get features with missing values\nfeatures_with_na = [feature for feature in train.columns if train[feature].isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print missing features and its percentage in train dataset\nfor feature in features_with_na:\n    print(feature, np.round(train[feature].isnull().mean(), 4), \"% missing values\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### As we see there are many missing values in the train dataset, so lets check the relationship between missing values and the sales price.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in features_with_na:\n    data = train.copy()\n    #Create a variable that indicates 1 if the values is missing and 0 otherwise.\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n    \n    # Plot bar graph of median SalesPrice for values missing or present in train dataset\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel(\"SalePrice\")\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see here for most houses where features are missing the Sales Price is comparatevily low.\nWhy? We will know in a while.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Numerical Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features = [feature for feature in train.columns if train[feature].dtype != 'O']\nprint(\"Number of numerical features: \", len(numerical_features))\ntrain[numerical_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Temporal Variables (Date-time variables)\n<p> In the above train dataset we have 4 temporal variables</p>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temporal_features = [feature for feature in numerical_features if 'Year' in feature or 'Yr' in feature]\nprint(\"Number of temporal features: \", len(temporal_features))\ntrain[temporal_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p> Lets see the relation between temporal variables and SalesPrice.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in temporal_features:\n    data = train.copy()\n    \n    data.groupby(feature)['SalePrice'].median().plot()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>The first 3 plots here look fine as the recent the year house is built/remodling done/garage build, the higher the SalesPrice.\nBut in the 4th plot Sales Price is decreasing as the Year is increasing. Ideally SalesPrice should increase with every passing year.</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<p> So lets see the relation between the first 3 year variables and the Year Sold","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in temporal_features:\n    data = train.copy()\n    \n    if feature != 'YrSold':\n        data[feature] = data['YrSold'] - data[feature]\n        plt.scatter(data[feature], data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.title(feature)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p> So above scatter plot indicates: \n1. The lesser the difference between house YrSold and house year built/remodling done/garagebuilt, the higher the Sales Price.\n2. When Sales Price is less then it means the house is old with no/not recent alterations done.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"So now we also know that \"Houses where faeture values are missing have comparatively low price\", because no remodelling or feature enhancements are done recently.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Discrete Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"discrete_features = [feature for feature in numerical_features if len(train[feature].unique()) <=25 \n                     and feature not in temporal_features + ['Id']]\nprint(\"Length of discrete features: \", len(discrete_features))\ntrain[discrete_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in discrete_features:\n    data = train.copy()\n    \n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Continuous Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous_features = [feature for feature in numerical_features if feature not in discrete_features + temporal_features + ['Id']]\nprint(\"Length of continuous features: \", len(continuous_features))\ntrain[continuous_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in continuous_features:\n    data = train.copy()\n    \n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p> As the continuous variables are all skewed, we will use logarithmic transformation to visualize.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in continuous_features:\n    data = train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        plt.scatter(data[feature], data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('Sale Price')\n        plt.title(feature)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>While converting variables using logarithmic transformation we see there are only 5 skewed variables - LotFrontage, LotArea, \n1stFlrSF, GrLivArea and SalePrice which has non-zero values. </p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in continuous_features:\n    data = train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature] = np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.title(feature)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorial Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categorial_features = [feature for feature in train.columns if train[feature].dtypes == 'O']\nprint(categorial_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[categorial_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in categorial_features:\n    print(\"Feature {} has {} unique values\".format(feature, len(train[feature].unique())))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>Plot categorial features with target variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in categorial_features:\n    data = train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('Sale Price')\n    plt.title(feature)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Missing value imputation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### Categorical Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[categorial_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorial_with_nan = [feature for feature in categorial_features if train[feature].isnull().sum() > 0]\nprint(categorial_with_nan)\nfor feature in categorial_with_nan:\n    print(\"Feature {}, has {}% missing values in train dataset\", (feature, np.round(train[feature].isnull().mean(), 4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in categorial_with_nan:\n    train[feature].fillna('Missing', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorial_with_nan = [feature for feature in categorial_features if test[feature].isnull().sum() > 0]\nprint(categorial_with_nan)\nfor feature in categorial_with_nan:\n    print(\"Feature {}, has {}% missing values in test dataset\", (feature, np.round(test[feature].isnull().mean(), 4)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in categorial_with_nan:\n    test[feature].fillna('Missing', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[categorial_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[categorial_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Dataset Categorial Features:\",train[categorial_features].shape)\nprint(\"Test Dataset Categorial Features:\",test[categorial_features].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Numeric Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(numerical_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_with_nan = [feature for feature in numerical_features if train[feature].isnull().sum() > 0]\nprint(numerical_with_nan)\nfor feature in numerical_with_nan:\n    print(\"Feature {} has {}% missing values in train datset\", (feature,np.round(train[feature].isnull().mean(), 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replace the missing values in train set with median since there are outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in numerical_with_nan:\n    train[feature].fillna(train[feature].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_with_nan = [feature for feature in numerical_features if feature not in ['SalePrice'] and test[feature].isnull().sum() > 0]\nprint(numerical_with_nan)\nfor feature in numerical_with_nan:\n    print(\"Feature {} has {}% missing values in test datset\", (feature,np.round(test[feature].isnull().mean(), 4)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replace the missing values in test set with median since there are outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in numerical_with_nan:\n    test[feature].fillna(test[feature].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train Dataset\", train.shape)\nprint(\"Test Dataset\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Temporal Variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[temporal_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    train[feature] = train['YrSold'] - train[feature]\n    test[feature] = test['YrSold'] - test[feature]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[temporal_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[temporal_features].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Since the numeric variables are skewed, we will perform log normal distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_non_zero_skewed_features_train_set = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\ntrain[num_non_zero_skewed_features_train_set].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in num_non_zero_skewed_features_train_set:\n    train[feature] = np.log(train[feature])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We may assume the same numeric features will be skewed in test set as well.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_non_zero_skewed_features_test_set = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\ntest[num_non_zero_skewed_features_test_set].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in num_non_zero_skewed_features_test_set:\n    test[feature] = np.log(test[feature])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[num_non_zero_skewed_features_train_set].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[num_non_zero_skewed_features_test_set].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Handle rare categorical features - which are present in less than 1% of the observations.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[categorial_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(categorial_features))\nprint(len(numerical_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"remaining_features = [feature for feature in train.columns if feature not in categorial_features + numerical_features]\nprint(remaining_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Create Dummies","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = train.copy()\ntest1 = test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train1,test1], axis=0)\ntrain_rows = train1.shape[0]\n\nfor feature in categorial_features:\n    dummy = pd.get_dummies(data[feature])\n    for col_name in dummy.columns:\n        dummy.rename(columns={col_name: feature+\"_\"+col_name}, inplace=True)\n    data = pd.concat([data, dummy], axis = 1)\n    data.drop([feature], axis = 1, inplace=True)\n\ntrain1 = data.iloc[:train_rows, :]\ntest1 = data.iloc[train_rows:, :] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train\",train1.shape)\nprint(\"Test\",test1.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler, RobustScaler\n\nscaling_features = [feature for feature in train1.columns if feature not in ['Id', 'SalePrice']]\nscaling_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(scaling_features))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1[scaling_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = RobustScaler()\nscaler.fit(train1[scaling_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = scaler.transform(train1[scaling_features])\nX_test = scaler.transform(test1[scaling_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Train\", X_train.shape)\nprint(\"Test\", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train1['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([train1[['Id','SalePrice']].reset_index(drop=True), pd.DataFrame(X_train, columns = scaling_features)], axis =1)\nprint(X.shape)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Setup cross validation strategy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 12\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train)\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\ndef rmsle(y_train, y_pred):\n    return np.sqrt(mean_squared_error(y_train, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(alpha =0.0005, random_state=0)\nelasticNet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=0)\nkernelRidge = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nsvr = SVR(C= 20, epsilon= 0.008, gamma=0.0003)\ngradientBoosting = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =0)\nxgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =0, nthread = -1)\nlgbm = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11, random_state=0)\nrandomForest = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores ={}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(lasso)\nprint(\"Lasso:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['lasso'] = (score.mean(), score.std())\nlasso_model = lasso.fit(X_train, y_train)\ny_pred_lasso = lasso_model.predict(X_train)\nrmsle(y_train,y_pred_lasso)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(elasticNet)\nprint(\"ElasticNet:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['elasticNet'] = (score.mean(), score.std())\nelasticNet_model = elasticNet.fit(X_train, y_train)\ny_pred_elasticNet = elasticNet_model.predict(X_train)\nrmsle(y_train,y_pred_elasticNet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(kernelRidge)\nprint(\"KernelRidge:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['kernelRidge'] = (score.mean(), score.std())\nkernelRidge_model = kernelRidge.fit(X_train, y_train)\ny_pred_kernelRidge = kernelRidge_model.predict(X_train)\nrmsle(y_train,y_pred_kernelRidge)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(svr)\nprint(\"SVR:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['svr'] = (score.mean(), score.std())\nsvr_model = svr.fit(X_train, y_train)\ny_pred_svr = svr_model.predict(X_train)\nrmsle(y_train,y_pred_svr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(gradientBoosting)\nprint(\"GradientBoostingRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['gradientBoosting'] = (score.mean(), score.std())\ngradientBoosting_model = gradientBoosting.fit(X_train, y_train)\ny_pred_gradientBoosting = gradientBoosting_model.predict(X_train)\nrmsle(y_train,y_pred_gradientBoosting)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(xgb)\nprint(\"XGBRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['xgb'] = (score.mean(), score.std())\nxgb_model = xgb.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_train)\nrmsle(y_train,y_pred_xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(lgbm)\nprint(\"LGBMRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['lgbm'] = (score.mean(), score.std())\nlgbm_model = lgbm.fit(X_train, y_train)\ny_pred_lgbm = lgbm_model.predict(X_train)\nrmsle(y_train,y_pred_lgbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(randomForest)\nprint(\"RandomForestRegressor:: Mean:\",score.mean(), \" Std:\", score.std())\nscores['randomForest'] = (score.mean(), score.std())\nrandomForest_model = randomForest.fit(X_train, y_train)\ny_pred_randomForest = randomForest_model.predict(X_train)\nrmsle(y_train,y_pred_randomForest)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stack Models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def ensemble_models(X):\n    return ((0.1 * lasso_model.predict(X)) +\n            (0.1 * elasticNet_model.predict(X)) +\n            (0.1 * kernelRidge_model.predict(X)) +\n            (0.1 * svr_model.predict(X)) +\n            (0.2 * gradientBoosting_model.predict(X)) + \n            (0.1 * xgb_model.predict(X)) +\n            (0.2 * lgbm_model.predict(X)) +\n            (0.1 * randomForest_model.predict(X)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"averaged_score = rmsle(y_train, ensemble_models(X_train))\nscores['averaged'] = (averaged_score, 0)\nprint('RMSLE averaged score on train data:', averaged_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def stack_models(X):\n    return ((0.7 * ensemble_models(X)) +\n            (0.15 * lasso_model.predict(X)) +\n#             (0.1 * elasticNet_model.predict(X)) +\n#             (0.1 * gradientBoosting_model.predict(X)) + \n            (0.15 * xgb_model.predict(X))\n#             (0.15 * lgbm_model.predict(X))\n           )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_score = rmsle(y_train, stack_models(X_train))\nscores['stacked'] = (stacked_score, 0)\nprint('RMSLE stacked score on train data:', stacked_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize model scores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nfig = plt.figure(figsize=(20, 10))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.4f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score', size=20, labelpad=12.5)\nplt.xlabel('Regression Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\nplt.title('Regression Model Scores', size=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predict = np.exp(stack_models(X_test))\nprint(test_predict[:5])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = test['Id']\nsub['SalePrice'] = test_predict\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub1 = pd.read_csv('submission.csv')\nsub1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Don't forget to upvote if you like the kernel.**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}