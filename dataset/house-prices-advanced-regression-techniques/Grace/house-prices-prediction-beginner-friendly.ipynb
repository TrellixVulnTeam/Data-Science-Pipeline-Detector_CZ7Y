{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Prices Prediction"},{"metadata":{},"cell_type":"markdown","source":"## Summary\n\n1. Data preprocessing\n    * remove ID column: `df.drop('column', axis=1, inplace=True)`\n    * remove outliers: Visualize outliers by scatter plots\n    * analyze target variable: `sns.distplot(df['column'], fit=norm)` and QQ-plot `stats.probplot(df['column'], plot=plt)`\n    * log-transform target variable if it is not normal\n1. Features engineering\n    * combine train and test `pd.concat([train, test], sort=False).reset_index(drop=True)`\n    * remove highly correlated features\n    * deal with missing data: numerical -> 0 or mean/median, categorical -> none or mode\n    * convert numerical variables to categorical variables: `df['numerical_feature'].astype(str)`\n    * label encode categorical variables\n    * create new feature(s)\n    * transform skewed features\n    * convert categorical variables to dummy variables: `pd.get_dummies(df)`\n1. Modelling\n    * define a cross validation strategy\n    * set up base models with hyperparameter optimization using GridSearchCV\n    * evaluate base models\n    * stack averaged models with a meta model\n    * output predictions as csv:\n    ```\n    df = pd.DataFrame({ 'column': values, 'column2', values2 })\n    df.to_csv('name.csv', index=False)\n    ```"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data processing"},{"metadata":{},"cell_type":"markdown","source":"## remove ID column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the Id column for prediction dataframe\ntest_ID = test['Id']\n\n# drop the Id columns since Id's are not correlated with house prices\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize outliers in a scatter plot\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete outliers: large GrLivArea but low price\ntrain = train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 300000)].index)\n\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## analyze target variable: SalePrice"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# plot SalePrice distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n# QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target variable is right skewed. Since linear models require variables to be normally distributed, we can transform this variable and make it more normally distributed."},{"metadata":{},"cell_type":"markdown","source":"## log-transform target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"# use np.log1p which applies log(1+x) to all elements of the column\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\n# plot the two graphs again\nsns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution after log-transformation')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features engineering"},{"metadata":{},"cell_type":"markdown","source":"## combine train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat([train, test], sort=False).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint('all_data size is : {}'.format(all_data.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## remove highly correlated features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize correlation among variables in a heatmap\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 10\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.4)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"GarageCars and GarageArea are highly correlated variables, which makes sense since the number of cars that fit into the garage is proportional to the garage area. Therefore, we can remove GarageArea and keep GarageCars since GarageCars has a higher correlation with SalePrice (0.64 vs 0.62)."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.drop(['GarageArea'], axis=1, inplace=True)\nprint('Number of feature columns : {}'.format(len(all_data.columns)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## deal with missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index)\nprint('There are {} columns with missing values: {}'.format(len(all_data_na.index), all_data_na.index.values))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generally, if a variable is numerical: we replace null with 0 or mean/median. If it is categorical, we replace null with none or mode. We can use select_dtypes to return all numerical features."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[all_data_na.index].select_dtypes(include=[np.number]).columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, GarageCars, GarageYrBlt, LotFrontage, MasVnrArea, TotalBsmtSF are numerical. LotFrontage (Linear feet of street connected to property) is non-zero. We fill the rest with 0."},{"metadata":{"trusted":true},"cell_type":"code","source":"# numerical\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF',\n            'GarageCars', 'GarageYrBlt', 'MasVnrArea', 'TotalBsmtSF'):\n    all_data[col] = all_data[col].fillna(0)\n\n# group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[all_data_na.index].select_dtypes(include=[np.object]).columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# categorical\nfor col in ('Alley', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'BsmtQual', 'Fence', 'FireplaceQu', 'GarageCond', 'GarageFinish', 'GarageQual',\n       'GarageType', 'MasVnrType', 'MiscFeature', 'PoolQC'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('Electrical', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'MSZoning', 'SaleType'):\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n    \n# All records are \"AllPub\", except for one \"NoSeWa\" and 2 NA, Utilities won't help in predictive modelling.\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data['Functional'] = all_data['Functional'].fillna('Typ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if there are still missing values\nall_data_na = (all_data.isnull().sum() / len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nall_data_na","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## convert numerical variables to categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"# MSSubClass: The building class should be categorical\nall_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n\n# convert OverallCond to a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n# Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## label encode some categorical variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\nprint('Shape of all_data: {}'.format(all_data.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## create new feature(s)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# add total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## transform skewed features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# filter numerical features\nnumeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\n\n# check the skewness of numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness = skewness[abs(skewness) > 0.75]\nprint('There are {} skewed numerical features to Box Cox transform'.format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## convert categorical variables to dummy variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## define a cross validation strategy"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring='neg_mean_squared_error', cv = kf))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## set up base models with hyperparameter optimization (using GridSearchCV)"},{"metadata":{},"cell_type":"markdown","source":"We use multiple regression models [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso), [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html), [Kernel Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge), [Gradient Boosting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor), [XGBoost](https://xgboost.readthedocs.io/en/latest/parameter.html) and [LightGBM](https://lightgbm.readthedocs.io/en/latest/Parameters.html). To find out the best parameters for the models, we can use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nlasso = Lasso()\nlasso_parameters = {'alpha': [1e-4, 2e-4, 5e-4, 7e-4, 9e-4, 1e-3, 2e-3, 5e-3, 7e-3, 9e-3, 1e-2, 2e-2, 5e-2]}\nlasso_regressor = GridSearchCV(lasso, lasso_parameters, scoring='neg_mean_squared_error', cv=5)\nlasso_regressor.fit(train, y_train)\nprint('Best parameters for lasso regression: {}'.format(lasso_regressor.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"ENet = ElasticNet()\nENet_parameters = {'alpha': [1e-3, 2e-3, 5e-3, 7e-3, 9e3, 1e-2, 2e-2, 5e-2],\n                  'l1_ratio': [.1, .2, .3, .4, .5, .6, .7, .8, .9],\n                  'random_state': range(0, 10)}\nENet_regressor = GridSearchCV(ENet, ENet_parameters, scoring='neg_mean_squared_error', cv=5)\nENet_regressor.fit(train, y_train)\nprint('Best parameters for elastic net: {}'.format(ENet_regressor.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the other model parameters, I referred to this [kernel](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)."},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.001, l1_ratio=.4, random_state=0))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state=7, nthread=-1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin=55, bagging_fraction=0.8,\n                              bagging_freq=5, feature_fraction=0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf=6, min_sum_hessian_in_leaf=11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## evaluate base models"},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(lasso)\nprint('Lasso score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))\n\nscore = rmsle_cv(ENet)\nprint('ElasticNet score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))\n\nscore = rmsle_cv(KRR)\nprint('Kernel Ridge score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))\n\nscore = rmsle_cv(GBoost)\nprint('Gradient Boosting score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_xgb)\nprint('Xgboost score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_lgb)\nprint('LGBM score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## stack averaged models with a meta model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                # train cloned base models\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                # predict the untouched holdout with the trained first stage model\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # train the cloned meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    # do the predictions of all base models on the test data and use the averaged predictions as \n    # meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, model_xgb),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint('Stacking Averaged models score: {:.4f} ({:.4f})'.format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to apply `np.expm1` (applies exp(x)-1 to all elements) to the predictions because in the beginning we have `np.log1p(train['SalePrice'])`. $\\exp(\\log(1+x))-1 = x$ is the original result."},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))\n\nmodel_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train, stacked_train_pred*0.70 + xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## output predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('house_prices_predictions.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# References\n* [Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)\n* [Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}