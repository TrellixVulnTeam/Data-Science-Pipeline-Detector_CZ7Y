{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Prices Prediction - Standard regression problem\nOur task today is to predict prices of houses based on numerous features. Let's try!"},{"metadata":{"trusted":false},"cell_type":"code","source":"# load libraries we will be using\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import cross_val_score, KFold, GridSearchCV, RandomizedSearchCV, train_test_split\nfrom sklearn.feature_selection import SelectFromModel, RFECV\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,VotingRegressor,StackingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression,LogisticRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNetCV, SGDRegressor, ARDRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nimport xgboost\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error\n\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = 500","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# load our training dataset\ndf_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ndf_submission = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\n\ndf = pd.concat([df_train, df_test], axis=0)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"df.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\nLet's play with data now, explore some interesting facts and relations that could be really useful for predictions"},{"metadata":{"trusted":false},"cell_type":"code","source":"# what is size of our data?\nprint('Dataset has {} rows and {} columns'.format(df.shape[0], df.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# checking data types, number of rows, missing rows\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check summary statistics\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# drop ID as it's not beneficial\n#df.drop('Id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Handle missing values\nBased on analysis above we may see some fields have missing values. To make it clear, we will screen just features those has some missing values and how much it does"},{"metadata":{"trusted":false},"cell_type":"code","source":"null_counts = df.isnull().sum()\nfull_counts = df.isnull().count()\n\nmc = null_counts[null_counts > 0]\nnmc = null_counts[null_counts > 0]/full_counts[null_counts > 0]\n\nndf = pd.DataFrame([mc, (nmc*100).round(1)], index=['Null Count', 'Null %']).T.sort_values('Null Count', ascending=False)\nndf.style.background_gradient(cmap='PuBu_r', vmin=19, vmax=20, subset=['Null %'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclussion?** We've highlighted fields those has missing percentage under 20%. Generally everything under 15% should be dropped, let's check what values we have in fields with high missing values.\nRemember, missing value may not always mean it's missing ;)"},{"metadata":{"trusted":false},"cell_type":"code","source":"col_mis_high = ['FireplaceQu', 'Fence', 'Alley', 'MiscFeature', 'PoolQC']\ncol_mis_low  = ['LotFrontage', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'BsmtCond', 'BsmtQual', 'MasVnrArea', 'MasVnrType', 'Electrical','MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'KitchenQual', 'Functional', 'GarageCars', 'GarageArea', 'SaleType']","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# look on value counts for each field\nfor c in col_mis_high:\n    print('Unique values for column {}:'.format(c.upper()))\n    print(df[c].value_counts())\n    print('----------\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"checking descriptions for all these fields, missing value simply means that property does not have that feature. Dropping these fields could be mistake, as it can have impact on price predicted. Let's impute these values with None and check how it is correlated with SalePrice"},{"metadata":{"trusted":false},"cell_type":"code","source":"# impute missing\nfor c in col_mis_high:\n    df[c].fillna(value='None', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(18, 5))\nfor i,c in enumerate(col_mis_high):\n    plt.subplot(1,len(col_mis_high),i+1)\n    sns.barplot(x=c, y='SalePrice', data=df)\n    plt.yticks([])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks good, it seems some fields has impact on price, now impute fields with missing values less than 20%, will use mode for categorical features and mean for numerical features"},{"metadata":{"trusted":false},"cell_type":"code","source":"# split data for numerical and categorical, impute and then push back to original dataset\ndf_missing = df[col_mis_low].copy()\ndf_missing_cat = df_missing.select_dtypes(include='object')\ndf_missing_num = df_missing.select_dtypes(include='number')\n\nfor c in df_missing_cat.columns:\n    df[c] = df_missing_cat[c].fillna(df_missing_cat[c].mode()[0])\n\nfor c in df_missing_num.columns:\n    df[c] = df_missing_num.fillna(df_missing_num[c].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# final check, do we have any missing left?\ndf.isnull().sum()[df.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation\nIt's time to look how our features correlates to each other as well how they correlate to target variable. Our features should be independed, meaning correlation between features itself should be close to 0"},{"metadata":{"trusted":false},"cell_type":"code","source":"# take a look on co\ncorrmat = df.corr().style.background_gradient()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Try seaborn heatmap for more condensed view\nplt.figure(figsize=(12, 12))\nsns.heatmap(df.corr(), vmin = -0.8, vmax=0.8, annot=False, square=True, cmap='seismic_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Outcome?** It's obvious that significant part of our features are correlated, thus may not be independed. Also there is pretty strong correlation to target variable (last row), some of them are strongly positive, few have negative correlation"},{"metadata":{},"cell_type":"markdown","source":"### Find outliers\nWe wants to check following fields: MiscVal, GrLivArea, EnclosedPorch, BsmtFinSF1"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nplt.subplot(321)\nsns.scatterplot(df['MiscVal'], df['SalePrice'])\nplt.subplot(322)\nsns.scatterplot(df['GrLivArea'], df['SalePrice'])\nplt.subplot(323)\nsns.scatterplot(df['EnclosedPorch'], df['SalePrice'])\nplt.subplot(324)\nsns.scatterplot(df['BsmtFinSF1'], df['SalePrice'])\nplt.subplot(325)\nsns.scatterplot(df['LotFrontage'], df['SalePrice'])\nplt.subplot(326)\nsns.scatterplot(df['LotArea'], df['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"i1 = np.array(df[df['MiscVal']>3000]['Id'])\ni2 = np.array(df[df['GrLivArea']>4000]['Id'])\ni3 = np.array(df[df['EnclosedPorch']>350]['Id'])\ni4 = np.array(df[df['BsmtFinSF1']>3000]['Id'])\ni5 = np.array(df[df['LotArea']>70000]['Id'])\ni6 = np.array(df[df['LotFrontage']>200]['Id'])\n\noutlier_idx = np.concatenate((i1,i2,i3,i4,i5,i6))\noutlier_idx = [x for x in outlier_idx if x not in df_test['Id'].tolist()]\nprint('We have found {} outliers!'.format(len(outlier_idx)))\nprint(outlier_idx)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = df[~df['Id'].isin(outlier_idx)]\ndf_train = df_train[~df_train['Id'].isin(outlier_idx)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distributions\nWe have a lot of variables, it would take some time to draw pairplot on all of them, let's check just relation to SalePrice\nTo do this, we also split our variables to numerical and categorical and will continue checking them and doing analyses separately"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# some of our numerical variables are category IDs (like overall quality etc)\n\ndf_cat = df.select_dtypes(exclude='number').copy()\ndf_int = df[['YearBuilt', 'YearRemodAdd', 'MSSubClass', 'OverallQual', 'OverallCond', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'MoSold', 'YrSold']].copy()\ndf_num = df[['LotFrontage', 'LotArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']].copy()\n\nprint('Columns in categorical set: {}'.format(df_cat.shape))\nprint('Columns in ID set: {}'.format(df_int.shape))\nprint('Columns in numerical set: {}'.format(df_num.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Numerical / float values\nFirst work with float values"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# check correlations to SalePrice ... we need 20 charts\nfig = plt.figure(figsize=(15, 15))\n\nfor i,c in enumerate(df_num):\n    plt.subplot(5, 4, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    sns.regplot(x=df_num[c], y=df['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some of features has strong correlation with low p-value, some of them has strong correlation with high p-value and some of them has low correlation.\nNotice also outliers having in numerous dimensions, might be handled, but we will use RobustScaler later on that is pretty good agains outliers"},{"metadata":{"trusted":false},"cell_type":"code","source":"# check distributions\nfig = plt.figure(figsize=(15, 15))\n\nfor i,c in enumerate(df_num):\n    plt.subplot(5, 4, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    sns.distplot(df_num[c], kde=False, rug=True)    # kde disabled as there was issue to calculate it for some fields","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check skewness of our fields\n# screen only those that have higher or lower skew than 1, -1\nskews = df_num.skew().sort_values()\nskew_index = skews[abs(skews) > 0.5].index\nskews[abs(skews) > 0.5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# take a look on distributions of these fields separately\nfig = plt.figure(figsize=(15, 10))\n\nfor i,c in enumerate(df_num[skew_index]):\n    plt.subplot(4, 5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    sns.distplot(df_num[c], kde=False, rug=True)    # kde disabled as there was issue to calculate it for some fields\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# transform fields using log1p\ndf_num[skew_index] = df_num[skew_index].apply(np.log1p)\ndf['SalePrice'] = df['SalePrice'].apply(np.log1p)\n#df['SalePrice'] = RobustScaler().fit_transform(df['SalePrice'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# take a look on distributions of these fields after transformation\n# LotArea is definitely better\nfig = plt.figure(figsize=(15, 10))\n\nfor i,c in enumerate(df_num[skew_index]):\n    plt.subplot(4, 5, i+1)\n    plt.xticks([])\n    plt.yticks([])\n    sns.distplot(df_num[c], kde=False, rug=True)    # kde disabled as there was issue to calculate it for some fields\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check skewness of our problematic fields\n# notice these fields had skewness higher than 0.5\nskews = df_num[skew_index].skew().sort_values()\nskews","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Good!** We have fixed skewness at least a bit, it's time to scale our numerical features"},{"metadata":{},"cell_type":"markdown","source":"#### Features engineering"},{"metadata":{"trusted":false},"cell_type":"code","source":"# basement finished percentage\ndf_num['BsmtFinSF_P'] = (df_num['BsmtFinSF1'] + df_num['BsmtFinSF2'])/(df['TotalBsmtSF'] + 0.01)\n\n# fllor total size & low quality percentage\ndf_num['TotalFlrSF'] = (df_num['1stFlrSF'] + df_num['2ndFlrSF'])\ndf_num['FlrSF_P'] = df_num['LowQualFinSF']/(df_num['TotalFlrSF']+0.01)\n\n# porch\ndf_num['Porch'] = df_num['OpenPorchSF'] + df_num['EnclosedPorch'] + df_num['3SsnPorch'] + df_num['ScreenPorch']\n\ndf_num.drop(['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','1stFlrSF','2ndFlrSF','LowQualFinSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# bathrooms\ndf_int['BsmtBath'] = (df_int['BsmtFullBath'] + 0.5*df_int['BsmtHalfBath'])\ndf_int['Bath'] = (df_int['FullBath'] + 0.5*df_int['HalfBath'])\ndf_int['BsmtBath_P'] = df_int['Bath']/(df_int['BsmtBath'] + 0.01)\n\ndf_int.drop(['BsmtFullBath','BsmtHalfBath','FullBath','HalfBath'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rs = RobustScaler()\nfor c in df_num.columns:\n    df_num[c] = rs.fit_transform(df_num[c].values.reshape(-1,1))\n\n#df['SalePrice'] = rs.fit_transform(df['SalePrice'].values.reshape(-1,1))\ndf_num.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### PCA\nSome of our features has higher correlation, let's try PCA and check how much fields is needed to explain 0.95 of variance"},{"metadata":{"trusted":false},"cell_type":"code","source":"# original data shape\ndf_num.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Explain 0.95 of variance\npca = PCA(0.95)\ndf_num_pca = pca.fit_transform(df_num)\ndf_num_pca.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's pretty good! we've reduced our data by 5 columns and got 0.95 of variance"},{"metadata":{},"cell_type":"markdown","source":"#### Integer / ID values\nWe are now done with purely numerical values, let's go to integer values/ids"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_int.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Years**? Do we need years? maybe better would be to transform to something different?"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Do we have some clear correlation? Yes, slightly\nsns.regplot(df_int['YearBuilt'], df['SalePrice'],color='red')\nsns.regplot(df_int['YearRemodAdd'], df['SalePrice'],color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_int['YearBuilt'] = (2010 - df_int['YearBuilt'])//10*10\ndf_int['YearRemodAdd'] = (2010 - df_int['YearRemodAdd'])//10*10\ndf_int['YrSold'] = (2010 - df_int['YrSold'])//10*10","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# yes it's much better!\nsns.regplot(df_int['YearBuilt'], df['SalePrice'],color='red')\nsns.regplot(df_int['YearRemodAdd'], df['SalePrice'],color='blue')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_int['YearBuiltRemod'] = df_int['YearRemodAdd'] + df_int['YearBuilt']\ndf_int.drop(['YearRemodAdd','YearBuilt'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# for me it was hard to understand what GarageYrBlt\ndef garage_year(x):\n    y = 1900 + x\n    \n    if y > 2020:\n        y = y-100\n        \n    return round(y)\n\nhelp = df_int['GarageYrBlt'].map(garage_year)\ndf_int['GarageYrBlt'] = (2010 - df_int['GarageYrBlt'])//10*10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look on impact of month sold, it seems there is almost no correlation.. let's try to reshape month to winter, summer, ..."},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.regplot(df_int['GarageYrBlt'], df['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"sns.regplot(df_int['MoSold'], df['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def month_sold(x):\n    if x == 7:\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_int['SoldSeason'] = df_int['MoSold'].map(month_sold)\ndf_int.drop(['MoSold'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"sns.regplot(df_int['SoldSeason'], df['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sligtly better? Maybe just a little bit"},{"metadata":{"trusted":false},"cell_type":"code","source":"for c in df_int.columns:\n    df_int[c] = StandardScaler().fit_transform(df_int[c].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_int.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Categorical features\nNow work with categorical values and encode them"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Checking now unique values we have for each column, we will have to split our fields to candidates for one hot encoding and ordinal values\ndf_cat.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ordinal = ['LandSlope', 'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','KitchenQual', 'HeatingQC','FireplaceQu','GarageFinish','GarageQual','GarageCond','PoolQC','Fence']\nonehot = df_cat.columns.tolist()\nonehot = [x for x in onehot if x not in ordinal]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### One hot encoding"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_oe = OneHotEncoder(handle_unknown='ignore', sparse=False).fit_transform(df_cat[onehot])\ndf_cat_oe.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Label encoder\nMost of our fields are ordinal, would be mistake to simply encode them, let's do dirty job and encode them manually"},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le = df_cat[ordinal]\ndf_cat_le.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['LandSlope'] = df_cat_le['LandSlope'].map({'Gtl':0, 'Mod':1, 'Sev':2})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['BsmtQual'] = df_cat_le['BsmtQual'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['BsmtCond'] = df_cat_le['BsmtCond'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['BsmtExposure'] = df_cat_le['BsmtExposure'].map({'NA':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['BsmtFinType1'] = df_cat_le['BsmtFinType1'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['BsmtFinType2'] = df_cat_le['BsmtFinType2'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['BsmtFinType'] = df_cat_le['BsmtFinType1'] + df_cat_le['BsmtFinType2']\ndf_cat_le.drop(['BsmtFinType1','BsmtFinType2'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['KitchenQual'] = df_cat_le['KitchenQual'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['HeatingQC'] = df_cat_le['HeatingQC'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['FireplaceQu'] = df_cat_le['FireplaceQu'].map({'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['GarageFinish'] = df_cat_le['GarageFinish'].map({'NA':0, 'Unf':1, 'RFn':2, 'Fin':3})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['GarageQual'] = df_cat_le['GarageQual'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['GarageCond'] = df_cat_le['GarageCond'].map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['PoolQC'] = df_cat_le['PoolQC'].map({'None':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_cat_le['Fence'] = df_cat_le['Fence'].map({'None':0, 'MnWw':1, 'GdWo':2, 'MnPrv':3, 'GdPrv':4})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"for c in df_cat_le.columns:\n    df_cat_le[c] = StandardScaler().fit_transform(df_cat_le[c].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"df_cat_le.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(df_num_pca.shape)\nprint(df_int.shape)\nprint(df_cat_oe.shape)\nprint(df_cat_le.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_final = np.concatenate((df_num_pca, df_int, df_cat_oe, df_cat_le.values), axis=1)\ndf_final.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now exclude our testing dataset we wants to predict"},{"metadata":{"trusted":false},"cell_type":"code","source":"max_id = df_train.shape[0]\n\ndf_final_t = df_final[:max_id]\nprint('Training shape: ', df_final_t.shape)\ny = df['SalePrice'].iloc[:max_id].values.reshape(-1,1)\nprint('Target shape: ', y.shape)\n\n# # prepare feature values\ndf_predict = df_final[max_id:,]\nprint('Features shape: ', df_predict.shape)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling\nWe will use Lasso and Ridge"},{"metadata":{"trusted":false},"cell_type":"code","source":"# split data into train & test size\nX_train, X_test, y_train, y_test = train_test_split(df_final_t, y, test_size=0.25, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\nselector = SelectFromModel(estimator=LassoCV(n_jobs=-1, random_state=123)).fit(X_train, y_train)\ncoef = np.abs(selector.estimator_.coef_)\ncoef = coef>0.0001\n\nX_train = X_train[:, coef]\nX_test = X_test[:, coef]\ndf_predict = df_predict[:,coef]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# define function to get negative root mean squared error from model\ndef rmse_cv_train(model):\n    kf = KFold(5, random_state=123)\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n    return rmse.mean()\n\ndef rmse_cv_test(model):\n    kf = KFold(5, random_state=123)\n    rmse = np.sqrt(-cross_val_score(model, X_test, y_test, scoring=\"neg_mean_squared_error\", cv=kf))\n    return rmse.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rg = RidgeCV(alphas=np.linspace(1, 20, 60), cv=KFold(5,random_state=123)).fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(rg))\nprint('Score on test set:', rmse_cv_test(rg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"ls = LassoCV(n_alphas=200,random_state=123, cv=KFold(5,random_state=123)).fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(ls))\nprint('Score on test set:', rmse_cv_test(ls))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"el = ElasticNetCV(n_alphas=200,random_state=123, cv=KFold(5,random_state=123)).fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(el))\nprint('Score on test set:', rmse_cv_test(el))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gb = GradientBoostingRegressor().fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(gb))\nprint('Score on test set:', rmse_cv_test(gb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"rf = RandomForestRegressor(n_jobs=-1).fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(rf))\nprint('Score on test set:', rmse_cv_test(rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"xg = XGBRegressor(objective='reg:squarederror', n_jobs=-1).fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(xg))\nprint('Score on test set:', rmse_cv_test(xg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sv = SVR().fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(sv))\nprint('Score on test set:', rmse_cv_test(sv))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge results\ny_test_hat_rg = rg.predict(X_test)\ny_test_hat_ls = ls.predict(X_test)\ny_test_hat_el = el.predict(X_test)\ny_test_hat_gb = gb.predict(X_test)\ny_test_hat_rf = rf.predict(X_test)\ny_test_hat_xg = xg.predict(X_test)\n\ny_test_hat = 0.3*y_test_hat_rg.ravel() + 0.2*y_test_hat_ls.ravel() + 0.2*y_test_hat_el.ravel() + 0.1*y_test_hat_gb.ravel() +  0.1*y_test_hat_rf.ravel() + 0.1*y_test_hat_rg.ravel()\n\nrmse = np.sqrt(mean_squared_error(y_test_hat,  y_test))\nprint('RMSE: ', rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# do the same using voting\nvreg = VotingRegressor([\n    ('rg', rg), \n    ('ls', ls),\n    ('el', el),\n    ('gb', gb),\n    ('rf', rf),\n    ('xg', xg)\n])\n\nvr = vreg.fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(vr))\nprint('Score on test set:', rmse_cv_test(vr))\n\ny_test_hat_vr = vr.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test_hat_vr,  y_test))\nprint('RMSE: ', rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Those are pretty good scores! Will use manual merge later on after optimizatoin"},{"metadata":{},"cell_type":"markdown","source":"#### Hyperparameter tuning\nWe got nice scores from different models, let's try to tune them first before merging them together. Note that Ridge, Lasso and ElasticNet are already tuned. I will do several iterrations in each step to get best parameters. Alternatively I could use random grid search to speedup process."},{"metadata":{"trusted":false},"cell_type":"code","source":"# support vector machines\n# actually we've ended up not using SVM as it looked it's score is not so good as from the others\nparams = {\n    'degree':[1,2,3,4],\n    'C':[0.01,0.1,0.5,1,2,5,10],\n    'epsilon':[0.001,0.01,0.1,1,5]\n}\n\ngssvr = GridSearchCV(SVR(), param_grid=params, n_jobs=-1, scoring='neg_mean_squared_error').fit(X_train, y_train)\nprint('SVR best params: ', gssvr.best_params_)\nprint('SVR best score: ', gssvr.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# gradient boosting\nparams = {\n    'random_state': [123],\n    'max_depth': [1,2],\n    'max_features': [8,10],\n    'min_samples_leaf': [3,5],\n    'min_samples_split': [1,2,3],\n    'n_estimators': [1000,1200]\n}\n\ngsgb = GridSearchCV(GradientBoostingRegressor(), param_grid=params, n_jobs=-1, scoring='neg_mean_squared_error').fit(X_train, y_train)\nprint('Gradient Boosting best params: ', gsgb.best_params_)\nprint('Gradient Boosting best score: ', gsgb.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# random forest\nparams = {\n    'random_state': [123],\n    'n_jobs': [-1],\n    'bootstrap': [True],\n    'max_depth': [200, 300],\n    'max_features': [30,40],\n    'min_samples_leaf': [1,2],\n    'min_samples_split': [2,3],\n    'n_estimators': [1200]\n}\n\ngsrf = GridSearchCV(RandomForestRegressor(), param_grid=params, n_jobs=-1, scoring='neg_mean_squared_error').fit(X_train, y_train)\nprint('Random Forest best params: ', gsrf.best_params_)\nprint('Random Forest best score: ', gsrf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# xgb regressor\nparams = {\n    'nthread':[-1],\n    'objective':['reg:squarederror'],\n    'learning_rate': [0.02,0.03],\n    'max_depth': [2,3],\n    'min_child_weight': [1,2],\n    'subsample': [0.7],\n    'colsample_bytree': [0.5,0.6],\n    'n_estimators': [1500,2000]\n}\n\ngsxb = GridSearchCV(XGBRegressor(), param_grid=params, n_jobs=-1, scoring='neg_mean_squared_error').fit(X_train, y_train)\nprint('XGB Regressor best params: ', gsxb.best_params_)\nprint('XGB Regressor best score: ', gsxb.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hypertuning outcome\nWe have run parameter hypertuning on multiple models, we will now retrain our models to use best parameters and compare score of our prediction once again"},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"#gbT = GradientBoostingRegressor(max_depth=2, max_features=10, min_samples_leaf=5,min_samples_split=2, n_estimators=500, random_state=123).fit(X_train, y_train)\ngbT = gsgb.best_estimator_.fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(gbT))\nprint('Score on test set:', rmse_cv_test(gbT))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#rfT = RandomForestRegressor(bootstrap=True, max_depth=200, max_features=30, min_samples_leaf=1, min_samples_split=3, n_estimators=1200, n_jobs=-1, random_state=123).fit(X_train, y_train)\nrfT = gsrf.best_estimator_.fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(rfT))\nprint('Score on test set:', rmse_cv_test(rfT))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#xgT = XGBRegressor(colsample_bytree=0.5, learning_rate=0.03, max_depth=3, min_child_weight=1, n_estimators=1200, nthread=-1, objective='reg:squarederror', silent=None, subsample=0.7).fit(X_train, y_train)\nxgT = gsxb.best_estimator_.fit(X_train, y_train)\nprint('Score on train set:', rmse_cv_train(xgT))\nprint('Score on test set:', rmse_cv_test(xgT))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# merge results\ny_test_hat_rg = rg.predict(X_test)\ny_test_hat_ls = ls.predict(X_test)\ny_test_hat_el = el.predict(X_test)\ny_test_hat_gb = gbT.predict(X_test)\ny_test_hat_rf = rfT.predict(X_test)\ny_test_hat_xg = xgT.predict(X_test)\n\ny_test_hat = 0.2*y_test_hat_rg.ravel() + 0.2*y_test_hat_ls.ravel() + 0.2*y_test_hat_el.ravel() + 0.1*y_test_hat_gb.ravel() +  0.1*y_test_hat_rf.ravel() + 0.1*y_test_hat_rg.ravel()\n\nrmse = np.sqrt(mean_squared_error(y_test_hat,  y_test))\nprint('RMSE: ', rmse)\n\nrmsle = np.sqrt(mean_squared_log_error(y_test_hat,  y_test))\nprint('RMSLE: ', rmsle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predicted vs acutals\nsns.scatterplot(x=y_test.ravel(), y=y_test.ravel(), color='blue')\nsns.scatterplot(x=y_test.ravel(), y=y_test_hat.ravel(), color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Predicted vs acutals\nsns.scatterplot(x=y_test.ravel(), y=y_test.ravel(), color='blue')\nsns.scatterplot(x=y_test.ravel(), y=y_test_hat_vr.ravel(), color='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict our test data\nWe are done with modelling now, it's time to predict SalePrice for test set"},{"metadata":{"trusted":false},"cell_type":"code","source":"# predict price\ny_pred_rg = rg.predict(df_predict)\ny_pred_ls = ls.predict(df_predict)\ny_pred_el = el.predict(df_predict)\ny_pred_gb = gbT.predict(df_predict)\ny_pred_rf = rfT.predict(df_predict)\ny_pred_xg = xgT.predict(df_predict)\n\ny_pred = 0.2*y_pred_rg.ravel() + 0.2*y_pred_ls.ravel() + 0.2*y_pred_el.ravel() + 0.2*y_pred_gb.ravel() +  0.1*y_pred_rf.ravel() + 0.1*y_pred_rg.ravel()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# revert log transformation\ny_pred = np.expm1(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# save to csv\ndf_submission['SalePrice'] = y_pred\ndf_submission.to_csv('Submission.csv', index = False)\nprint('Submission saved!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_submission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}