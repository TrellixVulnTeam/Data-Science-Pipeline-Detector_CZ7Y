{"cells":[{"metadata":{},"cell_type":"markdown","source":"# AMES House Price Prediction\n\nFind extensive Data Exploration in this NOtebook with each and every step in detail analysis.\n\nHope you find this notebook useful : )"},{"metadata":{},"cell_type":"markdown","source":"# Data Understanding"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nimport warnings  \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Load the train and the test data and have a quick look at our dataset**"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/input/house-prices-advanced-regression-techniques/'\n\ndf_train = pd.read_csv(path + \"train.csv\")\ndf_test = pd.read_csv(path + \"test.csv\")\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Target Feature Analysis\n\nWe will plot **Histogram** and **QQPlot** to analyze the distribution and skewness type of the target feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"##### GET SKEWNESS #####\nprint(f\"Skewness Co-efficient: {round(df_train.SalePrice.skew(), 3)}\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), dpi=300)\n\n##### HISTOGRAM #####\nfrom scipy import stats\nsns.distplot(df_train['SalePrice'] , fit=stats.norm, ax=ax1)\nax1.set_title('Histogram')\n\n##### PROBABILITY / QQ PLOT #####\nstats.probplot(df_train['SalePrice'], plot=ax2)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:**\n\n* The Skewness co-efficient suggested the target feature is positive skewed.\n* We will apply log transformation to the feature to make the distribution close to gaussian.\n* We will apply `log(1+x)` transformation to avoid `0` values (if present).\n* After transforming the variable, we will see both the above plots again."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\n##### GET SKEWNESS #####\nprint(f\"Skewness Co-efficient: {round(df_train.SalePrice.skew(), 3)}\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), dpi=300)\n\n##### HISTOGRAM #####\nfrom scipy import stats\nsns.distplot(df_train['SalePrice'] , fit=stats.norm, ax=ax1)\nax1.set_title('Histogram')\n\n##### PROBABILITY / QQ PLOT #####\nstats.probplot(df_train['SalePrice'], plot=ax2)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After-Transformation Analysis:**\n* The skewness has reduced (from `1.883` to `0.121`) and the plot now looks close to the normal distribution.\n* Even the probability plot can confirm the same."},{"metadata":{},"cell_type":"markdown","source":"**We will be concatenating our `train` and `test` dataframes for two purpose**\n1. To avoid repeating all the operations (such as transformations, imputations, etc) done on the train set for the test set.\n2. To get more data for our analysis (`More the data, the BETTER it is.!!`)"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n    Store the number of rows or indexes for train and test dataset\n    to separate them while performing modeling and prediction.\n'''\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\n\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)\n\nnindex, nfeatures = all_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"## Missing Value Imputation"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get the dataframe with all the features and SUM of the NaN values present\ndf_null_count = all_data.isnull().sum().to_frame().rename({0:\"Sum\"}, axis=1)\n\n## Select only those features who have atleast 1 NaN value\ndf_null_count = df_null_count[df_null_count['Sum'] > 0]\n\n## Change the SUM to PERCENTAGE \ndf_null_count['Sum'] = df_null_count['Sum']*(100/nindex)\n\n## Plot a Horizontal Bar Graph\ndf_null_count.sort_values(by=\"Sum\", ascending=True).plot(\n    kind='barh', figsize=(12,10), fontsize=14, colormap=\"RdBu_r\", title=\"Percentage wise null values\"\n)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis:**\n* `PoolQC`, `MiscFeature`, and `Alley` have more than 50% values as `NaN`. Our first impression will be to drop them but instead in our analysis we will take these features and leave it to our `feature selection` algorithm to provide important set of features.\n* We will perform Missing Value Imputation for each and every feature mentioned in the bar graph. \n* `SalePrice` will be ignored since the missing  values belong to the test set. Out train set does not contain any missing values. We can confirm the same by the following code.\n\n```code\n>>> df.SalePrice.isnull().sum()\n... 0\n```"},{"metadata":{},"cell_type":"markdown","source":"### PoolQC :: Pool quality\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tAverage/Typical\n       Fa\tFair\n       NA\tNo Pool\n\n_Unique Values in PoolQC_\n\n```code\n>>> all_data.PoolQC.unique()\n... [nan, 'Ex', 'Fa', 'Gd']\n```\n\n`NaN` must be a blank field in the data corresponding to `NA` i.e. *No Pool*\n\nReplacing `NaN` with a string value `None`\n\nAlso, the values can be represented in an order ie ORDINAL Variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['PoolQC'] = all_data['PoolQC'].fillna('None')\n\nquality_map = {\n    \"None\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4\n}\n\nall_data['PoolQC'].replace(quality_map, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MiscFeature :: Miscellaneous feature not covered in other categories\n\t\t\n       Elev\tElevator\n       Gar2\t2nd Garage (if not described in garage section)\n       Othr\tOther\n       Shed\tShed (over 100 SF)\n       TenC\tTennis Court\n       NA\t  None\n\n_Unique Values in MiscFeature_\n\n```code\n>>> all_data.MiscFeature.unique()\n... [nan, 'Shed', 'Gar2', 'Othr', 'TenC']\n```\n`NaN` must be a blank field in the data corresponding to no miscellaneous feature.\n\nReplacing `NaN` with a string value `None`"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['MiscFeature'] = all_data['MiscFeature'].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Alley :: Type of alley access to property\n\n       Grvl\tGravel\n       Pave\tPaved\n       NA \tNo alley access\n       \n_Unique Values in Alley_\n```code\n>>> all_data.Alley.unique()\n... [nan, 'Grvl', 'Pave']\n```\n\n`NaN` must be a blank field in the data corresponding to no alley access.\n\nReplacing `NaN` with a strig value `None`"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['Alley'] = all_data['Alley'].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fence :: Fence Quality\n\n       MnPrv\tMinimum Privacy\n       GdWo\tGood Wood\n       MnWw\tMinimum Wood/Wire\n       NA\tNo Fence\n       \n_Unique Values in Fence_\n```code\n>>> all_data.Fence.unique()\n... [nan, 'MnPrv', 'GdWo', 'GdPrv', 'MnWw']\n```\n\n`NaN` must be a blank field in the data corresponding to no fence.\n\nReplacing `NaN` with a strig value `None`"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['Fence'] = all_data['Fence'].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### FireplaceQu :: Fireplace quality\n\n       Ex\tExcellent - Exceptional Masonry Fireplace\n       Gd\tGood - Masonry Fireplace in main level\n       TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n       Fa\tFair - Prefabricated Fireplace in basement\n       Po\tPoor - Ben Franklin Stove\n       NA\tNo Fireplace\n       \n_Unique Values in FireplaceQu_\n```code\n>>> all_data.FireplaceQu.unique()\n... [nan, 'TA', 'Gd', 'Fa', 'Ex', 'Po']\n```\n\n`NaN` must be a blank field in the data corresponding to no fireplace.\n\nReplacing `NaN` with a strig value `None`"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['FireplaceQu'] = all_data['FireplaceQu'].fillna('None')\n\nquality_map = {\n    \"None\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5\n}\n\nall_data['FireplaceQu'].replace(quality_map, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LotFrontage :: Linear feet of street connected to property\n\n```code\n>>> all_data['LotFrontage'].dtype\n... dtype('float64')\n```\n\nSince it is a numeric feature, let us try to impute mean or median values by refering to the distribution of the feature values."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm, skew\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120)\n\nsns.distplot(all_data['LotFrontage'].dropna(), fit=norm, ax=ax1)\nsns.boxplot(all_data['LotFrontage'].dropna(), ax=ax2, orient='v')\n\nprint(f\"Skewness value: %.2f\" %all_data['LotFrontage'].dropna().skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### The skewness value of `2.16` and the boxplot denote that the distribution is highly positive skewed with high number of outliers (extreme values). \n\n#### Therefore, we will make use of`median` imputation instead of `mean` imputation to avoid extreme values to have an impact."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data['LotFrontage'].fillna(value=all_data['LotFrontage'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis of Garage Feature\n\nThere are a total of 5 Garage Features having null values in them -\n\n*GarageType: Garage location*\n\t\t\n       2Types\tMore than one type of garage\n       Attchd\tAttached to home\n       Basment\tBasement Garage\n       BuiltIn\tBuilt-In (Garage part of house - typically has room above garage)\n       CarPort\tCar Port\n       Detchd\tDetached from home\n       NA\tNo Garage\n\t\t\n*GarageYrBlt: Year garage was built*\n\t\t\n*GarageFinish: Interior finish of the garage*\n\n       Fin\tFinished\n       RFn\tRough Finished\t\n       Unf\tUnfinished\n       NA\tNo Garage\n\n*GarageQual: Garage quality*\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage\n\t\t\n*GarageCond: Garage condition*\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical/Average\n       Fa\tFair\n       Po\tPoor\n       NA\tNo Garage\n       \n*GarageCars: Size of garage in car capacity*\n\n*GarageArea: Size of garage in square feet*\n\n---\n       \n```code\n>>> all_data['GarageType'].unique()\n... ['Attchd', 'Detchd', 'BuiltIn', 'CarPort', nan, 'Basment', '2Types']\n```\nThe above code shows the unique values present the GarageType feature. Our assumption is that `NaN` values correspond to houses with no garages.\n       \nLet us create a Garage Dataframe to further analyse the null values present."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_no_garage = all_data[[\n    'GarageYrBlt', 'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', 'GarageCars', 'GarageArea'\n]][\n    all_data['GarageType'].isnull()\n]\n\ndf_no_garage.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the above dataframe we can notice that when the feature `GarageTpe` is `NaN`, the corresponding categorical Garage features are also `NaN` and the numerical features are `0`. This clearly indicates absence of garage for the house."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Total number of entries with Null Garage Type are {len(df_no_garage.index)}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here we can safely impute the `NaN` values with string value of `None` for all the above features.**\n\n**NOTE**: The additional two features GarageCars and GarageArea have a value of `0` for the corresponding `NaN` values."},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in ['GarageCond', 'GarageFinish', 'GarageQual', 'GarageType']:\n    all_data[feature].fillna(value='None', inplace=True)\n    \nfor feature in ['GarageYrBlt', 'GarageCars', 'GarageArea']:\n    all_data[feature].fillna(value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Further analysis of Garage Features\n\n* `GarageFinish`, `GarageQual` and `GarageCond` can be converted to ordinal variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"#all_data['GarageYrBlt'] = all_data['GarageYrBlt'].astype(str)\n\nquality_map = {\n    \"None\": 0, \"Po\": 1,\"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5\n}\n\nall_data['GarageQual'].replace(quality_map, inplace=True)\nall_data['GarageCond'].replace(quality_map, inplace=True)\n\nquality_map = {\n    \"None\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3\n}\n\nall_data['GarageFinish'].replace(quality_map, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"--- \n#### We could see a similar kind of pattern being followed by the next 5 basement features. Let us analyse these features using a similar approach."},{"metadata":{},"cell_type":"markdown","source":"### Analysis of Basement Features\n\nThere are a total of 5 Garage Features having null values in them -\n\n*BsmtQual: Evaluates the height of the basement*\n\n       Ex\tExcellent (100+ inches)\t\n       Gd\tGood (90-99 inches)\n       TA\tTypical (80-89 inches)\n       Fa\tFair (70-79 inches)\n       Po\tPoor (<70 inches\n       NA\tNo Basement\n\t\t\n*BsmtCond: Evaluates the general condition of the basement*\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical - slight dampness allowed\n       Fa\tFair - dampness or some cracking or settling\n       Po\tPoor - Severe cracking, settling, or wetness\n       NA\tNo Basement\n\t\n*BsmtExposure: Refers to walkout or garden level walls*\n\n       Gd\tGood Exposure\n       Av\tAverage Exposure (split levels or foyers typically score average or above)\t\n       Mn\tMimimum Exposure\n       No\tNo Exposure\n       NA\tNo Basement\n\t\n*BsmtFinType1: Rating of basement finished area*\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement\n\n*BsmtFinType2: Rating of basement finished area (if multiple types)*\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement\n \n*BsmtFullBath: Basement full bathrooms*\n\n*BsmtHalfBath: Basement half bathrooms*\n\n*BsmtFinSF1: Type 1 finished square feet*\n\n*BsmtFinSF2: Type 2 finished square feet*\n\n*BsmtUnfSF: Unfinished square feet of basement area*\n\n*TotalBsmtSF: Total square feet of basement area*\n\n---\n       \n```code\n>>> all_data['BsmtQual'].unique()\n... ['Gd', 'TA', 'Ex', nan, 'Fa']\n```\nThe above code shows the unique values present the BsmtQual feature. Our assumption is that `NaN` values correspond to houses with no basement.\n       \nLet us create a Basement Dataframe to further analyse the null values present."},{"metadata":{"trusted":true},"cell_type":"code","source":"no_basement_df = all_data[[\n    'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'BsmtFinSF1', \n    'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'TotalBsmtSF'\n]][\n    all_data['BsmtQual'].isnull()\n]\n\nno_basement_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Total number of entries with Null Basement Cond are {len(no_basement_df.index)}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Safely imputing `NaN` value with string value of `None` for all the categorical features and `0` for numeric variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"for feature in ['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual']:\n    all_data[feature].fillna(value='None', inplace=True)\n    \nfor feature in ['BsmtFinSF1', 'BsmtFinSF2','BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'TotalBsmtSF']:\n    all_data[feature].fillna(value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Further Analysis of Basement Feature\n\n* All the basement features can be converted in to `ordinal` variables. (Label Encoding)"},{"metadata":{"trusted":true},"cell_type":"code","source":"''' BsmtCond and BsmtQual '''\n\nquality_map = {\n    \"None\": 0, \"Po\": 1,\"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5\n}\n\nall_data['BsmtCond'].replace(quality_map, inplace=True)\nall_data['BsmtQual'].replace(quality_map, inplace=True)\n\n''' BsmtExposure '''\n\nquality_map = {\n    \"None\": 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4\n}\n\nall_data['BsmtExposure'].replace(quality_map, inplace=True)\n\n''' BsmtFinType1 and BsmtFinType2 '''\n\nquality_map = {\n    \"None\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6\n}\n\nall_data['BsmtFinType1'].replace(quality_map, inplace=True)\nall_data['BsmtFinType2'].replace(quality_map, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis of Masonry Veneer Type and Area\n\n*MasVnrType: Masonry veneer type*\n\n       BrkCmn\t Brick Common\n       BrkFace\tBrick Face\n       CBlock\t Cinder Block\n       None\t   None\n       Stone\t  Stone\n\t\n*MasVnrArea: Masonry veneer area in square feet*\n\n```code\n>>> all_data['MasVnrType'].unique()\n... ['BrkFace', 'None', 'Stone', 'BrkCmn', nan]\n```\n\nWe can clearly see the MasVnrType feature has both `None` and `NaN` values. Therefore, the `nan` values might be true missing values. Let us impute the `MasVnrType` with the `mode` value and `MasVnrArea` with `median` value."},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_val = all_data.MasVnrType.value_counts().idxmax()    # 'None'\nall_data['MasVnrType'].fillna(mode_val, inplace=True) \n\nmedian_val = all_data.MasVnrArea.median()     # 0\nall_data['MasVnrArea'].fillna(median_val, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### MSZoning :: Identifies the general zoning classification of the sale.\n\t\t\n       A\tAgriculture\n       C\tCommercial\n       FV\tFloating Village Residential\n       I\tIndustrial\n       RH\tResidential High Density\n       RL\tResidential Low Density\n       RP\tResidential Low Density Park \n       RM\tResidential Medium Density\n\n```code\n>>> all_data['MSZoning'].unique()\n... ['RL', 'RM', 'C (all)', 'FV', 'RH', nan]\n```\n\n```code\n>>> all_data['MSZoning'].isnull().sum()\n... 4\n```\n\nThere are `4` missing values in `MSZoning` feature. Let us impute these `4 NaN` values with the `mode` value for the column. \n\nAlso, replacing the value `C (all)` in the `MSZoning` column with only `C`."},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_val = all_data.MSZoning.value_counts().idxmax()    # 'RL'\nall_data['MSZoning'].fillna(mode_val, inplace=True)\n\nall_data['MSZoning'].replace({'C (all)': 'C'}, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Utilities :: Type of utilities available\n\t\t\n       AllPub\tAll public Utilities (E,G,W,& S)\t\n       NoSewr\tElectricity, Gas, and Water (Septic Tank)\n       NoSeWa\tElectricity and Gas Only\n       ELO\t   Electricity only\t\n\n```code\n>>> all_data.Utilities.unique()\n... ['AllPub', 'NoSeWa', nan]\n```\n\n```code\n>>> all_data.Utilities.value_counts()\n... AllPub    2916\n... NoSeWa       1\n```\n\nBefore analyzing the `NaN` values, let us look at the above two blocks of code.\n\nThe first one tells us that only 2 unique values are present out of a set of 4 unique values available for the feature.\n\nThe second block tells us that except for 1 value all other values are `AllPub`. This feature does not provide any relevant information for our model so we will drop this feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.drop('Utilities', inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Functional :: Home functionality (Assume typical unless deductions are warranted)\n\n       Typ\t Typical Functionality\n       Min1\tMinor Deductions 1\n       Min2\tMinor Deductions 2\n       Mod\t Moderate Deductions\n       Maj1\tMajor Deductions 1\n       Maj2\tMajor Deductions 2\n       Sev\t Severely Damaged\n       Sal\t Salvage only\n\n```code\n>>> all_data.Functional.unique()\n... ['Typ', 'Min1', 'Maj1', 'Min2', 'Mod', 'Maj2', 'Sev', nan]\n```\n\n```code\n>>> all_data.Functional.isnull().sum()\n... 2\n```\n\nThere are 2 `NaN` values present in the feature so let us fill these 2 values with the `mode` value."},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_val = all_data.Functional.value_counts().idxmax()    #'Typ'\nall_data['Functional'].fillna(mode_val, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis of Exterior1st and Exterior2nd\n\nBoth the `Exterior1st` and `Exterior2nd` feature have 1 `NaN` value each. Instead of imputing `None` and introducing a new value for the feature, we will rather impute `Other` in place of `NaN` values since it is already present as one of the values for both the feature set."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.Exterior1st.fillna(value='Other', inplace=True)\nall_data.Exterior2nd.fillna(value='Other', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis of SaleType\n\n`SaleType` feature have 1 `NaN` value. Instead of imputing `None` and introducing a new value for the feature, we will rather impute `Other` in place of the `NaN` value since it is already present as one of the values in the feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.SaleType.fillna(value='Other', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Electrical :: Electrical system\n\n       SBrkr\tStandard Circuit Breakers & Romex\n       FuseA\tFuse Box over 60 AMP and all Romex wiring (Average)\t\n       FuseF\t60 AMP Fuse Box and mostly Romex wiring (Fair)\n       FuseP\t60 AMP Fuse Box and mostly knob & tube wiring (poor)\n       Mix\t  Mixed\n       \n```code\n>>> df['Electrical'].unique()\n... ['SBrkr', 'FuseF', 'FuseA', 'FuseP', 'Mix', nan]\n```\nAs we can see that the Electrical feature has 6 different unique values whereas the original data desription comprises of 5 different values. So, let us check the percentage of the missing values in this feature.\n\n```code\n>>> df['Electrical'].isnull().sum()\n... 1\n```\nWe have only one missing value in Electrical feature. Let us impute it with the `Mode` value."},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_val = all_data.Electrical.value_counts().idxmax()    # 'SBrkr'\nall_data['Electrical'].fillna(mode_val, inplace=True) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KitchenQual :: Kitchen Quality\n\n*KitchenQual: Kitchen quality*\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical/Average\n       Fa\tFair\n       Po\tPoor\n       \n```code\n>>> df['Electrical'].unique()\n... ['Gd', 'TA', 'Ex', 'Fa', nan]\n```\n\n```code\n>>> all_data.KitchenQual.isnull().sum()\n... 1\n```\nWe have only one missing value in `KitchenQual` feature. Let us impute it with the `Mode` value.\n\nAlso, we can Label encode the values since the feature values resemble ordinal variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_val = all_data.KitchenQual.value_counts().idxmax()    # 'TA'\nall_data['KitchenQual'].fillna(value=mode_val, inplace=True)\n\nquality_map = {\n    \"Po\": 0,\"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4\n}\nall_data['KitchenQual'].replace(quality_map, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** We did not impute `NaN` values in `KitchenQual` feature with `None` since the feature description does not contain an option for No Kitchen present. This is also understood by the fact that Houses will definately contain a kitchen."},{"metadata":{},"cell_type":"markdown","source":"### At this point, we have taken care of all the missing values in our data by imputing them with a more probable value.\n\nWe can cross-check the same using the following command - \n```code\n>>> sum(df_new.isnull().sum())\n... 0\n```"},{"metadata":{},"cell_type":"markdown","source":"### Drop the ID feature\nThe ID feature/variable is **not relevant** for our analysis so lets get rid of it. But before that, lets keep a copy of it. *Precaution is better than cure.!! ( ;*"},{"metadata":{"trusted":true},"cell_type":"code","source":"id_copy = all_data['Id']\nall_data.drop('Id', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Further analysis of features based on the feature description\n\n---\n**Street**\n\nFeature `Street` has only two possible values, namely, `Grvl` and `Pave`. Let us make this feature binary.\n\nReplacing `Grvl` with `1` and `Pave` with `0`.\n\n---\n\n**CentralAir**\n\nFeature `CentralAir` has two values `Y` for `Yes` or `N` for `No`. \n\nReplacing `Y` with `1` and `N` with `0`. \n\n---\n\n**HeatingQC | ExterQual | ExterCond | LandSlope | LotShape**\n\nAll the above features refers to a quality mapping.\n\nLet us treat it as ordinal variable and perform Label Encoding.\n\n---\n\n**MSSubClass**\n\nThe values of the feature is of `int` type, but it actually defines some kind of `class`.\n\nLet us treat this feature as a categorical one instead of numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"''' Street '''\nall_data['Street'].replace({'Grvl': 0, 'Pave': 1}, inplace=True)\n\n''' CentralAir '''\nall_data['CentralAir'].replace({'Y': 1, 'N': 0}, inplace=True)\n\n''' HeatingQC | ExterQual | ExterCond'''\nquality_map = {\n    \"Po\": 0,\"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4\n}\nall_data.HeatingQC.replace(quality_map, inplace=True)\nall_data.ExterQual.replace(quality_map, inplace=True)\nall_data.ExterCond.replace(quality_map, inplace=True)\n\n''' LandSlope '''\nquality_map = { 'Sev': 0, 'Mod': 1, 'Gtl': 2 }\nall_data.LandSlope.replace(quality_map, inplace=True)\n\n''' LotShape '''\nquality_map = { 'IR3': 0, 'IR2': 1, 'IR1': 2, 'Reg': 3 }\nall_data.LotShape.replace(quality_map, inplace=True)\n\n''' MSSubClass '''\nclass_map = {\n    20:'Class1', 30:'Class2', 40:'Class3', 45:'Class4', 50:'Class5', 60:'Class6', \n    70:'Class7', 75:'Class8', 80:'Class9', 85:'Class10', 90:'Class11', 120:'Class12',\n    150:'Class13', 160:'Class14', 180:'Class15', 190:'Class16'\n}\nall_data.MSSubClass.replace(class_map, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"**Creating new features that might be relevant for our Model.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\n    Usually we rely more on the total area of the house rather than talking about individual areas.\n    Create a new feature which sums up all the area parameters to give us the total area available in the house.\n'''\nall_data['TotalSF'] = all_data['1stFlrSF'] + all_data['2ndFlrSF'] + all_data['GrLivArea'] + all_data['TotalBsmtSF']\n\n'''\n    The Year features do not provide much information. We can rather work on getting more concrete information like\n    No. of Years ie Age instead of actual year.\n'''\n## Age of the house at the time of buying\nall_data['Age'] = all_data.YrSold - all_data.YearBuilt\n\n## No. of years since the house was remodeled at the time of buying\nall_data['AgeRemod'] = all_data.YrSold - all_data.YearRemodAdd\n\n## No. of years since the Garage was built at the time of buying\n'''\n    For the houses without garages we have already impted them with 0.0\n    So, before building the new feature  we will replace them with the corresponding values in YrSold.\n    This will ensure that the AgeGarage feature will be 0 for the houses that do not have Garage.\n    If we do not perform the above operation then we will get an Age which is ~ 2000.\n'''\nall_data['GarageYrBlt'].replace({0.0: np.nan}, inplace=True)\nall_data['GarageYrBlt'].fillna(all_data['YrSold'], inplace=True)\nall_data['AgeGarage'] = all_data.YrSold - all_data.GarageYrBlt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analysis: `all_data['Age']`**\n\nThere is one negative value (-1) in `all_data['Age']` which indicates that the Customer bought the house a year before it was built. We can confirm the same with the following code.\n\n```code\n>>> (all_data.YrSold - all_data.YearBuilt).unique()\n... array([5, 31, 7, ... -1, 101], dtype=int64)\n```\n\nWe will replace this value with `0` to avoid negative values.\n\n**Analysis: `all_data['AgeRemod']`**\n\nThere are two negative value (-1, -2) in `all_data['AgeRemod']` which indicates that the Customer has done the renovation after buying the house. We can confirm the same with the following code.\n\n```code\n>>> (all_data.YrSold - all_data.YearRemodAdd).unique()\n... array([5, 31, 6, ... -1, 55, 23, -2], dtype=int64)\n```\n\nWe will replace this value with `0` to avoid negative values.\n\n**NOTE:** We will drop the original time variables because we have already built new features from the information that these time variables are providing."},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data.Age       =  all_data.Age.map(lambda x: 0 if x < 0 else x)\nall_data.AgeRemod  =  all_data.AgeRemod.map(lambda x: 0 if x < 0 else x)\nall_data.AgeGarage =  all_data.AgeGarage.map(lambda x: 0 if x < 0 else x)\n\nall_data.drop(\n    ['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt', 'MoSold'], \n    axis=1, inplace=True\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analysis of Numeric Features"},{"metadata":{},"cell_type":"markdown","source":"Let us check the **correlation** of different **numeric features** with the dependent variable `SalesPrice`.\n\nWe know that the correlation value signifies the type of correlation between two variables.\n\nWe can say that the correlation is of -\n* High Degree     :: If the coefficient value lies between $\\pm0.50$ and $\\pm1$    (_Strong Correlation_)\n* Moderate Degree :: If the coefficient value lies between $\\pm0.30$ and $\\pm0.49$ (_Medium Correlation_)\n* Low Degree      :: If the coefficient value lies between $-0.29$ to $0.29$ (_Weak Correlation_)\n\nLet us identify all the numeric features having one of the above degree of correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_numeric_features(feature):\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5), dpi=110)\n\n    sns.distplot(all_data[feature], ax=ax1)\n    sns.scatterplot(all_data[feature], all_data[\"SalePrice\"], ax=ax2)\n    sns.boxplot(all_data[feature], ax=ax3, orient='v', width=0.2)\n\n    print(\"Skewness Coefficient of LotFrontage is %.2f\" %all_data[feature].skew())\n    ax1.set_yticks([])\n    \n    return plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outlier Detection"},{"metadata":{},"cell_type":"markdown","source":"* We will try to `detect outlliers` for the `numeric features` and then `remove` them from our analysis\n* But we will only remove those `outliers` which are a part of the `train data` i.e. having index within `ntrain` (defined earlier)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numeric_features(\"LotFrontage\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 Outliers present at `LotFrontage > 300`"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index = all_data['LotFrontage'][all_data['LotFrontage'] > 300].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numeric_features(\"LotArea\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"4 Outliers present at `LotArea > 100000`"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index = np.append(drop_index, all_data['LotArea'][all_data['LotArea'] > 100000].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numeric_features(\"BsmtFinSF1\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 outlier present at `BsmtFinSF1 > 5000`"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index = np.append(drop_index, all_data['BsmtFinSF1'][all_data['BsmtFinSF1'] > 5000].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numeric_features(\"TotalBsmtSF\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 outlier present at `TotalBsmtSF > 6000`"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index = np.append(drop_index, all_data['TotalBsmtSF'][all_data['TotalBsmtSF'] > 6000].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numeric_features(\"1stFlrSF\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1 outlier present at `1stFlrSF > 4000`"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index = np.append(drop_index, all_data['1stFlrSF'][all_data['1stFlrSF'] > 4000].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_numeric_features(\"GrLivArea\").show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2 outliers are present at `GrLivArea` greater than `4000` and `SalePrice` below `12.5`"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_index = np.append(drop_index, all_data['GrLivArea'][\n    (all_data['GrLivArea'] > 4000) & (all_data['SalePrice'] < 12.5)\n].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Making sure we only remove data from the train set\ndrop_index = drop_index[drop_index < ntrain]\n\nall_data = all_data.drop(drop_index).reset_index(drop=True)\n\n## Length of indexes dropped\ndropped_count = len(drop_index)\n\n## Modify our ntrain variable\nntrain -= dropped_count","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:** A total of 11 instances have been dropped. The training samples have reduced from `1460` to `1449`."},{"metadata":{},"cell_type":"markdown","source":"**Re-Analyzing the scatter plots after outlier removal**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(15, 10), dpi=150)\n\ndef create_scatter_plot(feature, axis):\n    sns.scatterplot(all_data[feature], all_data[\"SalePrice\"], ax=axis)\n    \ncreate_scatter_plot('LotFrontage', ax1)\ncreate_scatter_plot('LotArea', ax2)\ncreate_scatter_plot('BsmtFinSF1', ax3)\ncreate_scatter_plot('TotalBsmtSF', ax4)\ncreate_scatter_plot('1stFlrSF', ax5)\ncreate_scatter_plot('GrLivArea', ax6)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Log Transforming** all the **Highly Skewed Features**."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get all the numeric features in out dataset\nnumeric_features = all_data.skew().index\n\n## Getting all the skewed features (skew > 0.5 or skew < -0.5)\nskewed_features = all_data[numeric_features].skew()[np.abs(all_data[numeric_features].skew()) > 0.5].index\n\n## Performing log(1+x) transformation\nall_data[skewed_features] = np.log1p(all_data[skewed_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get all the categorical columns\ncat_cols = all_data.select_dtypes(\"object\").columns\n\n## One-Hot Encoding all the categorical variables but dropping one of the features among them.\ndrop_categ = []\nfor i in cat_cols:\n    drop_categ += [ i+'_'+str(all_data[i].unique()[-1]) ]\n\n## Create dummy variables (One-Hot Encoding)\nall_data = pd.get_dummies(all_data, columns=cat_cols) \n\n## Drop the last column generated from each categorical feature\nall_data.drop(drop_categ, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train =  all_data[:-ntest].drop(['SalePrice'], axis=1)\ny_train =  all_data[:-ntest]['SalePrice']\nX_test  =  all_data[-ntest:].drop(['SalePrice'], axis=1)\n\n## Remove SalePrice from numeric_features to avoid scaling it\nnumeric_features = numeric_features.drop('SalePrice')\n\n## Scaling all the numeric features using Robust Scaler\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\n\nX_train[numeric_features] = scaler.fit_transform(X_train[numeric_features])\nX_test[numeric_features]  = scaler.transform(X_test[numeric_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction"},{"metadata":{},"cell_type":"markdown","source":"We will be using **XGBRegressor** to find the importance of each feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nxgb = XGBRegressor()\n\nxgb.fit(X_train, y_train)\ndf_imp = pd.DataFrame(xgb.feature_importances_ , columns = ['Importance'], index=X_train.columns)\ndf_imp = df_imp.sort_values(['Importance'], ascending = False)\n\ndf_imp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find all the `highly important features` using **XGBRegressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Function to calculate RMSE\ndef rmse(y_true, y_pred):\n    return np.sqrt(np.mean((y_true-y_pred)**2))\n\n## Function to calculate negative RMSE\ndef nrmse(y_true, y_pred):\n    return -1.0*rmse(y_true, y_pred)\n\nfrom sklearn.metrics import make_scorer\nneg_rmse = make_scorer(nrmse)\n\nfrom xgboost import XGBRegressor\nestimator = XGBRegressor()\n\nfrom sklearn.feature_selection import RFECV\nestimates = RFECV(estimator, cv = 3, n_jobs = -1, scoring = neg_rmse)\nestimates = estimates.fit(X_train, y_train)\n\nprint(f\"No. of highly important features: {estimates.n_features_}\")\n\n## List of important features\nimp_features = X_train.columns.values[estimates.support_]\n\nX_train =  X_train[imp_features]\nX_test  =  X_test[imp_features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling and Evaluation"},{"metadata":{},"cell_type":"markdown","source":"Since we have already selected all the important features, it is wise to use the `Kernel Ridge` rather than `ElasticNet` or `Lasso`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.kernel_ridge import KernelRidge\nridge = KernelRidge()\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\nparameters = {\n    'alpha': uniform(0.05, 1.0), 'kernel': ['polynomial'], 'degree': [2], 'coef0': uniform(0.5, 3.5)\n}\n\nrandom_search = RandomizedSearchCV(\n    estimator = ridge, param_distributions = parameters, n_iter = 1000, \n    cv = 3, scoring = neg_rmse, n_jobs = -1, random_state=0\n)\n\nrandom_search = random_search.fit(X_train, y_train)\n\nmodel = random_search.best_estimator_\n\nprint(f\"Training RMSE: {round(rmse(y_train, model.predict(X_train)), 3)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Inverse Transforming log transformed SalePrice (y_pred)\ny_pred = np.exp(model.predict(X_test))\n\n## Final DataFrame consisting of the SalePrice\noutput = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': y_pred})\n\n#output.to_csv('prediction.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<center><h1>PLEASE UPVOTE</h1></center>\n\n<center><h3>Don't forget to UPVOTE if you find this Notebook useful. It will keep me motivated.</h3></center>"},{"metadata":{},"cell_type":"markdown","source":"<center><h1>--- END ---</h1></center>"},{"metadata":{},"cell_type":"markdown","source":"Subscribe to my Tech Blog: [Viking's Tech Blog](https://viking-pathak.blogspot.com)\n\nFollow my Data Science Page on instagram: [@gameofdata](https://www.instagram.com/gameofdata)\n\nConnect with me on LinkedIN: [Amit Pathak](https://www.linkedin.com/in/viking-pathak)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}