{"cells":[{"metadata":{},"cell_type":"markdown","source":"**If you are as new as me, why don't you start your House Pricing Model with me.  **  \n**Created by Raymond Wang **\n\n**If you are as new as me Series **     \nTitanic https://www.kaggle.com/yw6916/if-you-are-as-new-as-me-why-don-t-you-start-here1?scriptVersionId=15917018    \nHouse Pricing Advanced https://www.kaggle.com/yw6916/house-pricing-advance-if-you-are-as-new-as-me-3  (Part 2 of this model, please go and check it out if you want something to improve yours' performance)\n\nIn this Kernal, I will lead you through the journey of making a vanilla model with a straight and simple approach. I hope you will enjoy this very intuitive method."},{"metadata":{},"cell_type":"markdown","source":"**What you will learn from here: **   \n1. General Kaggle Data Science workflow\n2. Numerical and Non-numerical data analysis and handling\n3. Feature Extraction and engineering\n4. Heat map, correlation matrix for systematic analysis.\n5. Machine learning techniques and how to validate models.\n6. Missing data handling.\n"},{"metadata":{},"cell_type":"markdown","source":"Standard Import  \nWe always start a notebook with import. These are libraries that people usually use for data science."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#standard import\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read the dataset, and use head( ) to preview, this is a good way to understand data generally."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df    = pd.read_csv(\"../input/test.csv\")\n\n# preview the data\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df.info()\n#print(\"----------------------------\")\n#test_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In almost any situation, id is an irrelevant feature to prediction. Therefore, we drop it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#drop unnecessary data\ntrain_df=train_df.drop(['Id'],axis=1)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Until this point, we have almost the right dataset to sail. Then, in the following section, different methods are taken to further analyze the data."},{"metadata":{},"cell_type":"markdown","source":"**1.Target Analysis**"},{"metadata":{},"cell_type":"markdown","source":"For the first step, it is always a good idea to start with analyzing target, in this case, the SalePrice."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['SalePrice'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train_df['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The target here will not assist a lot to increase accuracy in this model. However, for advanced settings, skewness can be used to adjust outliers. Please refer to https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard."},{"metadata":{},"cell_type":"markdown","source":"**2. Features Extraction**"},{"metadata":{},"cell_type":"markdown","source":"In this section, I mannually pick 8 features which I think is critical to this analysis. Then, I will examine all of them to see if they are actually usefull and relevant for prediction. It is a very good exercise to build data scientist intuition."},{"metadata":{},"cell_type":"markdown","source":"In this sections, the 8 features extracted are:       \n1.LotArea   \n2.GrLivArea    \n3.OverallQual   \n4.Utilities   \n5.Neighborhood     \n6.CentralAir    \n7.GarageCars&GarageArea    \n8.YearBuilt    "},{"metadata":{},"cell_type":"markdown","source":"2.1 LotArea"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The intuition here is that the larger the area is, the higher the price should be.\n# Therefore, we are examing this proportionality in this regards.\ndata = pd.concat([train_df['SalePrice'], train_df['LotArea']], axis=1)\ndata.plot.scatter(x='LotArea', y='SalePrice', ylim=(0, 800000))\n\n#Well, not really see a proportionality in the chart, probably drop it.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.2.GrLivArea "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Similar intuition as LotArea\ndata = pd.concat([train_df['SalePrice'], train_df['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0, 800000))\n#Yeah, in this case, the linear proportionality is quite obvious. Keep it.\n#However, we can further clean the data by removing outliers.\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.3.OverallQual "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train_df['SalePrice'], train_df['OverallQual']], axis=1)\n#data.plot.scatter(x='OverallQual', y='SalePrice', ylim=(0, 800000))\n#better version of visualization\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\n#It is a quite good index. Definitely keep it.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.4.Utilities  "},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropped. All the same\npass","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.5.Neighborhood "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train_df['SalePrice'], train_df['Neighborhood']], axis=1)\nf, ax = plt.subplots(figsize=(26, 12))\nfig = sns.boxplot(x='Neighborhood', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\n# Different Neighborhoods have different range of price. Keep it.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Neighborhood is proven to be less relevent. However, for learning purposes, I will demonstrate how to deal with non-numerical data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Neighborhood is proven to be less effective, ignore.\n\n# Using Dummies to extract this feature\n#neighborhood_dummies_train  = pd.get_dummies(train_df['Neighborhood'])\n#neighborhood_dummies_test  = pd.get_dummies(test_df['Neighborhood'])\n#train_df = train_df.join(neighborhood_dummies_train)\n#test_df    = test_df.join(neighborhood_dummies_test)\n\n#train_df.drop(['Neighborhood'], axis=1,inplace=True)\n#test_df.drop(['Neighborhood'], axis=1,inplace=True)\n\n#train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.6.CentralAir "},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train_df['SalePrice'], train_df['CentralAir']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='CentralAir', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\n#With CentralAir, the sale price is higher. Keep it for now","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Another way to handle non-numerical data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert Y,N into 1,0\ntrain_df['CentralAir'].replace(to_replace=['N', 'Y'], value=[0, 1])\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.7.GarageCars&GarageArea"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This can be seen as a factor to represent the number of cars owning.\n# Typically, more cars, the house tends to be more expensive\n# We select GarageCars due to personal preference only\ndata = pd.concat([train_df['SalePrice'], train_df['GarageCars']], axis=1)\ndata.plot.scatter(x='GarageCars', y='SalePrice', ylim=(0, 800000))\ndata = pd.concat([train_df['SalePrice'], train_df['GarageArea']], axis=1)\ndata.plot.scatter(x='GarageArea', y='SalePrice', ylim=(0, 800000))\n# Fairly representative, keeping it","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2.8.YearBuilt  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# This may be a tricky one to see the correlation, since time series is involvd.\ndata = pd.concat([train_df['SalePrice'], train_df['YearBuilt']], axis=1)\ndata.plot.scatter(x='YearBuilt', y='SalePrice', ylim=(0, 800000))\n#for better visualization\nf, ax = plt.subplots(figsize=(26, 12))\nfig = sns.boxplot(x='YearBuilt', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\n#index is okay, but too many outliers, think about it later.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Overall Analysis**     \n"},{"metadata":{},"cell_type":"markdown","source":"After process 2, we gain an general idea about how indeces can affect our prediction. However, the above features are selected neither scientific nor systematic. Then, in this section, I will introduce heatmap, a systematic approach in finding right features."},{"metadata":{},"cell_type":"markdown","source":"In here, I introduce a new and general method to handle non-numerical data with sklearn."},{"metadata":{"trusted":true},"cell_type":"code","source":"#This sk method can process non-value datat= like Neighborhood\nfrom sklearn import preprocessing\nf_names = ['CentralAir', 'Neighborhood']\nfor x in f_names:\n    label = preprocessing.LabelEncoder()\n    train_df[x] = label.fit_transform(train_df[x])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To drwa heatmap for correlation. The fainter the colour, the higher the correlation. Now, by focusing the bottom row named SalePrice, we can find the correlation between our targets and features."},{"metadata":{"trusted":true},"cell_type":"code","source":"corrmat =train_df.corr()\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=0.8, square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the heat map, we extract missing potential useful index:     \n1.FullBath.    \n2.TotRmsAbvGrd.   \n3.TotalBsmtSF.   \n4.1stFlrSF.   \nAlso, Neighborhood and CentralAir seem to be less relevant. So drop them."},{"metadata":{},"cell_type":"markdown","source":"Then, use correlation matrix to examin the top 10 features to eliminate similar ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"k  = 10 \ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, \\\n                 square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this matrix, 1stFlrSF and TotalBsmtSF are similar. We take 1stFlrSF.\nGarageCars and GarageArea are similar. We takeGarageCars.\nTotRmsAbvGrd and GrLivArea are similar. We take GrLivArea."},{"metadata":{},"cell_type":"markdown","source":"Therefore, the model now only consists the following index, which we believed to be most correlated from the map: \n1. OveralQual\n2. GrLivArea\n3. GarageArea\n4. 1stFlrSF\n5. FullBath\n6. YearBuilt\n\n"},{"metadata":{},"cell_type":"markdown","source":"**4. ML to generate Models**"},{"metadata":{},"cell_type":"markdown","source":"For regression problems, we usually take staking approch (a very vanila one here, just to introduce the idea). "},{"metadata":{},"cell_type":"markdown","source":"ML import with different models."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\nfrom sklearn import linear_model, svm, gaussian_process\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Process the data with our 6 top chosen features.  \nThen using normalization to do feature scaling.\nAlso, we split data into training and validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['OverallQual','GrLivArea', 'GarageArea','1stFlrSF', 'FullBath', 'YearBuilt']\nx = train_df[cols].values\ny = train_df['SalePrice'].values\n#Normalization\nx_scaled = preprocessing.StandardScaler().fit_transform(x)\ny_scaled = preprocessing.StandardScaler().fit_transform(y.reshape(-1,1))\n#Train and validation\nX_train,X_vali, y_train, y_vali = train_test_split(x_scaled, y_scaled, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We inspect 3 example ML models: RandomForest, KNN and LGBoost (for the idea of dropping poorly performed one).  \nA very naive method of validation is used to test models.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['OverallQual','GrLivArea', 'GarageArea','1stFlrSF', 'FullBath', 'YearBuilt']\nX_train = train_df[cols].values\ny_train = train_df['SalePrice'].values\n\nclf_1 = RandomForestRegressor(n_estimators=400)\nclf_1.fit(X_train, y_train)\ny_pred = clf_1.predict(X_vali)\nprint(np.sum(abs(y_pred - y_vali))/len(y_pred))\n\nclf_2 = KNeighborsRegressor(n_neighbors=7)\nclf_2.fit(X_train, y_train)\ny_pred = clf_2.predict(X_vali)\nprint(np.sum(abs(y_pred - y_vali))/len(y_pred))\n\nclf_3 = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nclf_3.fit(X_train, y_train)\ny_pred = clf_3.predict(X_vali)\nprint(np.sum(abs(y_pred - y_vali))/len(y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see, the off-set of LGBoost is highest. Therefore, we discard LGBoost."},{"metadata":{},"cell_type":"markdown","source":"**5.Missing Data Handling**"},{"metadata":{},"cell_type":"markdown","source":"After training our model, we have to think about our input. Is the dataset complete?"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['OverallQual','GrLivArea', 'GarageArea','1stFlrSF', 'FullBath', 'YearBuilt']\ntest_df[cols].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No, there is one missing data in GarageArea we have to handle by filling with mean value."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Handling GarageArea Missing data\ntest_df['GarageArea'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['GarageArea']=test_df['GarageArea'].fillna(472.768861)\ntest_df['GarageArea'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6. Submission**"},{"metadata":{},"cell_type":"markdown","source":"Then, we are ready to go for submitting this vanila model!\nI will in the following days to summerize techniques of optimising this model, and I will upload a link here. If you like my idea, please stay with me. Thanks!!!"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = ['OverallQual','GrLivArea', 'GarageArea','1stFlrSF', 'FullBath', 'YearBuilt']\ntest_x = pd.concat( [test_df[cols]] ,axis=1)\n\nx = test_x.values\n\ny_pred_1 = clf_1.predict(x)\ny_pred_2 =clf_2.predict(x)\ny_pred_3 =clf_3.predict(x)\n\ny_pred=y_pred_1*0.5+y_pred_2*0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = pd.DataFrame(y_pred, columns=['SalePrice'])\nprint(prediction)\nresult = pd.concat([test_df['Id'], prediction], axis=1)\nresult.to_csv('./Predictions.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}