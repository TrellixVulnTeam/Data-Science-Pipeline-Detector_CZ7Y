{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Prices : Visualization & Prediction\n### Predict sales prices and practice different machine learning regressors. \n##### ( ⭐️ Upvote my Notebook — it helps! )\n\n<img src=\"https://www.vancouverrealestatepodcast.com/wp-content/uploads/2018/10/Detached-home-prices.jpg\" alt=\"Smiley face\" height=\"50%\" width=\"70%\">\n\n\n\n\nGetting started with competitive data science can be quite intimidating. So I build this notebook for quick overview on the Advanced Regression Techniques competition. If there is interest, I’m happy to do deep dives into the intuition behind the feature engineering and models used in this kernel.\n\nI encourage you to fork this kernel, play with the code and enter the competition. Good luck!\n\n**Competition Description**\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. Each row in the dataset describes the characteristics of a house. This competition challenges you to predict the final price of each home.\n\n**Executive Summary**\n\nI started this competition by just focusing on getting a good understanding of the dataset. The EDA & Visualizations are included to allow developers to dive into analysis of dataset. \n\n\n### Key features of the model training process in this kernel\n\n- Cross Validation: Using 10-fold cross-validation\n- Models: On each run of cross-validation tried fitting following models :-\n    - Elastic model\n    - Lasso regression model \n    - XGBoost model\n    - LGBM model\n    - SVR model\n    - Ridge model\n    - GBR model\n- Stacking : Trained a meta StackingCVRegressor optimized using xgboost\n- Blending: All models trained will overfit the training data to varying degrees. Therefore, to make final predictions, I blended their predictions together to get more robust predictions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom datetime import datetime\n\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nprint(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning and Pre-processing"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ID = train['Id']\ntest_ID = test['Id']\n\n# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic Visualization"},{"metadata":{},"cell_type":"markdown","source":"#### 1. Correlation among variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train.corr()\nplt.subplots(figsize=(13,10))\nsns.heatmap(corr, vmax=0.9, cmap=\"Blues\", square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2. Various datatypes columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.get_dtype_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3. Heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heatmap between top 10 correlated variables with SalePrice\ncorrMatrix=train[[\"SalePrice\",\"OverallQual\",\"GrLivArea\",\"GarageCars\",\n                  \"GarageArea\",\"GarageYrBlt\",\"TotalBsmtSF\",\"1stFlrSF\",\"FullBath\",\n                  \"TotRmsAbvGrd\",\"YearBuilt\",\"YearRemodAdd\"]].corr()\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(10, 10))\n\nsns.heatmap(corrMatrix, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='viridis',linecolor=\"white\")\nplt.title('Correlation between features');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4. SalePrice: the variable we're trying to predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 5. Fireplaces Variable Factor"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(\"Fireplaces\",\"SalePrice\",data=train,hue=\"FireplaceQu\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having 2 fireplaces increases house price and fireplace of Excellent quality is a big plus"},{"metadata":{},"cell_type":"markdown","source":"#### 6. MSZoning"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = train[\"MSZoning\"].unique()\nsizes = train[\"MSZoning\"].value_counts().values\nexplode=[0.1,0,0,0,0]\nparcent = 100.*sizes/sizes.sum()\nlabels = ['{0} - {1:1.1f} %'.format(i,j) for i,j in zip(labels, parcent)]\n\ncolors = ['yellowgreen', 'gold', 'lightblue', 'lightcoral','blue']\npatches, texts= plt.pie(sizes, colors=colors,explode=explode,\n                        shadow=True,startangle=90)\nplt.legend(patches, labels, loc=\"best\")\n\nplt.title(\"Zoning Classification\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7. SalePrice per square foot"},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = train\ntrain1['SalePriceSF'] = train['SalePrice']/train['GrLivArea']\nplt.hist(train['SalePriceSF'], bins=15,color=\"blue\")\nplt.title(\"Sale Price per Square Foot\")\nplt.ylabel('Number of Sales')\nplt.xlabel('Price per square feet');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 8. Building , remodelling years and age of house"},{"metadata":{"trusted":true},"cell_type":"code","source":"train1['ConstructionAge'] = train['YrSold'] - train['YearBuilt']\nplt.scatter(train1['ConstructionAge'], train['SalePriceSF'])\nplt.ylabel('Price per square foot (in dollars)')\nplt.xlabel(\"Construction Age of house\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Price of house goes down with its age."},{"metadata":{},"cell_type":"markdown","source":"#### 9. Heating and AC arrangements"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.stripplot(x=\"HeatingQC\", y=\"SalePrice\",data=train,hue='CentralAir',jitter=True,split=True)\nplt.title(\"Sale Price vs Heating Quality\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having Heating & AC arrangements definitely escalates price of house."},{"metadata":{},"cell_type":"markdown","source":"#### 10. Kitchen Quality"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(\"KitchenAbvGr\",\"SalePrice\",data=train,hue=\"KitchenQual\")\nplt.title(\"Sale Price vs Kitchen\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having only one Kitchen of Ex (Excellent) quality hikes house price like anything."},{"metadata":{},"cell_type":"markdown","source":"### Outlier"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nplt.scatter(train.GrLivArea, train.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deleting outliers\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['SalePrice']\nplt.figure(2); plt.title('Normal')\nsns.distplot(y, kde=False, fit=stats.norm)\nplt.figure(3); plt.title('Log Normal')\nsns.distplot(y, kde=False, fit=stats.lognorm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There seems to be clear evidence of right-skewedness in the target variable. We can correct this with a simple log transformation."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Concatenation\nTo keep consistency between test and train features we concatenate the two sets while remembering the index so we can split it later again."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### NA's\nLet's figure out what NA's excist, sort them by categories and impute them in the best possible way."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n\nfeatures['Functional'] = features['Functional'].fillna('Typ')\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's impute all incongruencies with the most likely value."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Zoning is also interesting"},{"metadata":{"trusted":true},"cell_type":"code","source":"features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the rest we will just use a loop to impute 'None' value"},{"metadata":{"trusted":true},"cell_type":"code","source":"objects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\n\nfeatures.update(features[objects].fillna('None'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The lotfrontage averages differ a lot per neighborhood so let's impute with the median per neighborhood."},{"metadata":{"trusted":true},"cell_type":"code","source":"features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The rest can be safely imputed with 0 since this means that the property is not present in the house."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling in the rest of the NA's\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)\n\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Features Simplication\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n\nfeatures['YrBltAndRemod']=features['YearBuilt']+features['YearRemodAdd']\nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n# simplified features\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating Dummies\n\nSince sklearn lm.fit() does not accept strings we have to convert our objects to dummy variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(features.shape)\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint(final_features.shape)\n\nX = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(X):, :]\n\nprint('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Overfitting prevention\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])\n\noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros / len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_sub = X_sub.drop(overfit, axis=1).copy()\n\nprint('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{},"cell_type":"markdown","source":"#### Setup cross validation method"},{"metadata":{"trusted":true},"cell_type":"code","source":"kfolds = KFold(n_splits=8, shuffle=True, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Defining model scoring function "},{"metadata":{"trusted":true},"cell_type":"code","source":"# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\n# build our model scoring function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Setting Up Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# setup models    \nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# Ridge Regressor\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n# Lasso Regressor\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=alphas2,\n                              random_state=42, cv=kfolds))\n\n# Elasticnet Regressor\nelasticnet = make_pipeline(RobustScaler(),\n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n                                        cv=kfolds, l1_ratio=e_l1ratio))\n                                        \n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(),\n                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\n                                   \n# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       #min_data_in_leaf=2,\n                                       #min_sum_hessian_in_leaf=11\n                                       )\n                                       \n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n                                            gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train models"},{"metadata":{},"cell_type":"markdown","source":"#### Get cross validation scores for each model"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('TEST score on CV')\n\nscores = {}\n\nscore = cv_rmse(ridge)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()),)\nscores['ridge'] = (score.mean(), score.std())\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), )\nscores['lasso'] = (score.mean(), score.std())\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()),)\nscores['elasticnet'] = (score.mean(), score.std())\n\nscore = cv_rmse(svr)\nprint(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()),)\nscores['svr'] = (score.mean(), score.std())\n\nscore = cv_rmse(lightgbm)\nprint(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), )\nscores['lgbm'] = (score.mean(), score.std())\n\nscore = cv_rmse(gbr)\nprint(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()),)\nscores['gbr'] = (score.mean(), score.std())\n\nscore = cv_rmse(xgboost)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), )\nscores['xgboost'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit the models"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('START Fit')\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\nprint(datetime.now(), 'elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nprint(datetime.now(), 'lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'svr')\nsvr_model_full_data = svr.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Blend models and get predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.1 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.25 * stack_gen_model.predict(np.array(X))))\n            \n\n# Get final precitions from the blended model\nblended_score = rmsle(y, blend_models_predict(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Blend with Top Kernals submissions"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_sub)))\n\n# this kernel gives lower score\n# let's up it by mixing with the top kernels\n\nprint('Blend with Top Kernals submissions', datetime.now(),)\nsub_1 = pd.read_csv('../input/top-10-0-10943-stacking-mice-and-brutal-force/House_Prices_submit.csv')\nsub_2 = pd.read_csv('../input/hybrid-svm-benchmark-approach-0-11180-lb-top-2/hybrid_solution.csv')\nsub_3 = pd.read_csv('../input/lasso-model-for-regression-problem/lasso_sol22_Median.csv')\n\nsubmission.iloc[:,1] = np.floor((0.25 * np.floor(np.expm1(blend_models_predict(X_sub)))) + \n                                (0.25 * sub_1.iloc[:,1]) + \n                                (0.25 * sub_2.iloc[:,1]) + \n                                (0.25 * sub_3.iloc[:,1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deal with predictions close to outer range "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Brutal approach to deal with predictions close to outer range \nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\n\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n\nsubmission.to_csv(\"submission.csv\", index=False)\nprint('Save submission', datetime.now(),)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identify the best performing model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Various Models', size=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe from the graph above that the blended model far outperforms the other models, with an RMSLE of 0.0170 .  This is the model which will be used for making the final predictions."},{"metadata":{},"cell_type":"markdown","source":"# Acknowledgments"},{"metadata":{},"cell_type":"markdown","source":"Inspirations are drawn from various Kaggle notebooks but majorly motivation is from the following :\n\n1. https://www.kaggle.com/poonaml/house-prices-data-exploration-and-visualisation\n\n2. https://www.kaggle.com/lavanyashukla01/how-i-made-top-0-3-on-a-kaggle-competition\n \n3. https://www.kaggle.com/itslek/blend-stack-lr-gb-0-10649-house-prices-v57/data?scriptVersionId=11189608\n \n4. https://www.kaggle.com/hemingwei/top-2-from-laurenstc-on-house-price-prediction/notebook\n\n5. https://www.kaggle.com/jesucristo/1-house-prices-solution-top-1\n\n##### Credit for image to https://www.vancouverrealestatepodcast.com/tag/housing-price-prediction-2019/"},{"metadata":{},"cell_type":"markdown","source":"My GitHub Project Link - https://github.com/RohitLearner/House-Prices-Visualization-Prediction . "},{"metadata":{},"cell_type":"markdown","source":"If you really enjoyed above kernel, then you might want to take a close look at following interesting kernels  :-\n\n1. Flight Crash Investigation - https://www.kaggle.com/iamrohitsingh/flight-crash-investigation/\n\n2. Titanic : Visualization & Prediction - https://www.kaggle.com/iamrohitsingh/titanic-visualization-prediction\n\n3. YouTube India Data Exploration - https://www.kaggle.com/iamrohitsingh/youtube-india-trending-data-exploration"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}