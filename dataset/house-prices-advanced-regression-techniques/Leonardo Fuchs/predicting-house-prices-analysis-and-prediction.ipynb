{"cells":[{"metadata":{"_uuid":"dcd7e76e-2589-4002-a011-c4cc12f2c311","_cell_guid":"83227eb4-d961-47d4-be30-8f08b16863d7","trusted":true},"cell_type":"markdown","source":"# Predicting House Prices \n\n![alt text](https://image.flaticon.com/icons/svg/70/70016.svg)\n\n[Leonardo Fuchs](https://www.kaggle.com/leofuchs) - May 2020\n\n---\n\nIn this notebook, we will go through the basic steps of making predictions based on a given dataset. The fifteen steps that I followed in this notebook are as follows:\n\n**01)** Importing Libraries and Datasets\n\n**02)** Data Description\n\n**03)** Finding Correlation Features\n\n**04)** Removing Outliers\n\n**05)** Imputation of Missing Values\n\n**06)** Correcting Features\n\n**07)** Adding Features\n\n**08)** Skewness and Kurtosis\n\n**09)** Label Encoding\n\n**10)** Transformation and Scaling\n\n**11)** Feature Selection\n\n**12)** Principal Component Analysis\n\n**13)** Testing Different Models\n\n**14)** Hyper-Parameter Tuning\n\n**15)** Making Predictions and Submission\n\n\n*Observation: This Notebook is based on the notebook presented by [Majdoubi Ahmed Amine](https://www.kaggle.com/mjidiba).*"},{"metadata":{"_uuid":"522bb830-b9ab-4b13-a6f1-a28f4395f98a","_cell_guid":"f2f82e57-4ccf-4383-b62b-d83053ffcc3e","trusted":true},"cell_type":"markdown","source":"## 01) Importing Libraries and Datasets"},{"metadata":{"_uuid":"6778bad4-a884-49d7-8d42-850ed353b5ef","_cell_guid":"631837d6-1fe8-444b-8e5c-63af6830dc7c","trusted":true},"cell_type":"code","source":"# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport scipy.stats as st\nimport seaborn as sns\nimport warnings\n\n%matplotlib inline\n\npd.options.display.max_columns = None\nwarnings.filterwarnings('ignore')\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\n# Importing the train and test datasets in pandas dataframes\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\n# Drop the 'Id' column from the train dataframe\ntrain_data.drop(columns='Id', inplace=True)\n\ny_train = train_data['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88ee3623-c1f1-4413-bd55-7fc2efd0c795","_cell_guid":"59fc0ef9-6747-44a9-b59f-569cb8689266","trusted":true},"cell_type":"markdown","source":"## 02) Data Description\n\nLet is start by taking a general look at the data we have to get an initial idea about it."},{"metadata":{"_uuid":"d050282c-fbaf-4f1c-8f68-56a089cfbcd1","_cell_guid":"95bb0f04-aa30-4129-b5e2-3f2b293ad17b","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# The shape of the data\ntrain_data.shape, test_data.shape, y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06d6c7b3-2447-48f3-af51-28352c817550","_cell_guid":"459d2df1-4d87-4beb-bdbb-c995d2079c14","trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"# Display the first five rows of the training dataset.\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f127fec9-f4d5-439f-8514-b11e0a24f173","_cell_guid":"74dda3fa-cb5e-4d2d-9458-d0c38655f9e4","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# The description of the train dataset\ntrain_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"55bffe58-c3a1-48bd-be5e-f5ed0e9419aa","_cell_guid":"1f46f114-95af-41e8-8e52-8caf582636e8","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Looking the type of the columns in the dataset\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"139be414-3e44-4f0d-a40f-60b6c69ec194","_cell_guid":"d4de2254-bbad-4ecb-ae94-9807f9427b0c","trusted":true},"cell_type":"markdown","source":"## 03) Finding Correlation Features\n\nLet's look at some correlation features between all the features."},{"metadata":{"_uuid":"60a0fc57-5332-4f55-8892-2b337a742ff7","_cell_guid":"20945337-d753-4632-8015-837971a2623a","trusted":true},"cell_type":"code","source":"# Showing the numerical varibales with the highest correlation with 'SalePrice', sorted from highest to lowest\ncorrelation = train_data.select_dtypes(include=[np.number]).corr()\n\nprint(correlation['SalePrice'].sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28ad8f27-1e26-4b35-8eab-105db8fd3d2c","_cell_guid":"0ae7ef9e-8c2b-4977-a3f8-ea3c2a2ee840","trusted":true},"cell_type":"code","source":"# Heatmap of correlation of numeric features\nfig, ax = plt.subplots(figsize = (14,14))\n\nplt.title('Correlation Between Numeric Features', size=15)\nsns.heatmap(correlation, square=True, vmax=0.8, cmap='coolwarm', linewidths=0.01);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16d29092-763e-41ea-95bd-a49f89717464","_cell_guid":"2d6f114d-ba3e-4122-bcf2-d8ac15273c60","trusted":true},"cell_type":"markdown","source":"- We observe two red squares **(2,2 and 3,3)** in the heatmap indicating high correlation. The first group of highly correlated variables is `TotalBsmtSF` and `1stFlrSF`. The second group is `GarageYrBlt`, `GarageCars` and `GarageArea`. This indicates the presence of multicollinearity.\n- The other four red squares **(1,1)** just indicate an obvious correlation between `GarageYrBlt` and `YearBuilt` and between `TotRmsAbvGrd` and `GrLivArea`"},{"metadata":{"_uuid":"69cb9623-ffb7-4ccf-b65e-ddba00527d7f","_cell_guid":"7de945d3-0f80-457c-a3a6-2ac360979c02","trusted":true},"cell_type":"code","source":"# Zoomed HeatMap of the most Correlayed variables\nzoomed_correlation = correlation.loc[['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt', 'YearRemodAdd', '1stFlrSF','GarageYrBlt','GarageCars','GarageArea'],\n                                     ['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt', 'YearRemodAdd', '1stFlrSF','GarageYrBlt','GarageCars','GarageArea']]\n\nfig , ax = plt.subplots(figsize = (14,14))\nplt.title('Zoomed Correlation Between Numeric Features', size=15)\nsns.heatmap(zoomed_correlation, square=True, vmax=0.8, annot=True, cmap='coolwarm', linewidths=0.01);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"710ef735-1f85-493a-b642-a7ae29add5fd","_cell_guid":"14908dee-b9f0-4d4f-a96c-0c5c3daf17ce","trusted":true},"cell_type":"markdown","source":"We conclude that :\n- `TotalBsmtSF` and `1stFlrSF` are strongly correlated (0.82)\n- `TotRmsAbvGrd` and `GrLivArea` are strongly correlated (0.83)\n- `YearBuilt` and `GarageYrBlt` are strongly correlated (0.83)\n- `GarageCars` and `GarageArea` are strongly correlated (0.88)\n- `OverallQual` and `GrLivArea` are correlated with `SalePrice` (0.79 and 0.71)"},{"metadata":{"_uuid":"17e4f8e3-c7a7-44e6-b215-f42fb6694638","_cell_guid":"725e3e98-21a5-47cc-9009-c8600daa4111","trusted":true},"cell_type":"code","source":"# Pair plot\ncols = ['SalePrice','GrLivArea','TotalBsmtSF','OverallQual','FullBath','TotRmsAbvGrd','YearBuilt', 'YearRemodAdd', '1stFlrSF','GarageYrBlt','GarageCars','GarageArea']\n\nsns.set()\nsns.pairplot(train_data[cols], size=2, kind='scatter', diag_kind='kde');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b22f4915-69df-4203-aaf5-b36bc53a7665","_cell_guid":"a987b5c3-2690-4285-b0e4-40ffe94b6c03","trusted":true},"cell_type":"markdown","source":"- We observe that `SalePrice` increases almost quadratically with `TotalBsmtSF`, `GrLivArea` and `1stFlrSF`. So we conclude that the price of the houses increases quadratically with its surface area. \n- We also observe that `SalePrice` increases exponentially with `OverallQual`.\n- We also observe from (`GrLivArea`-`1stFlSF`) and (`1stFlSF`-`TotalBsmSF`) that all the points are above the identity function line, which means that the ground living area has the biggest surface of all floors, and that the first floor area is generally bigger than the basement area.\n- We observe the same phenomenon for (`GarageYrBlt`-`YearBuilt`). which makes sense since we start building the garage after building the house, altough there are some exceptions in the data."},{"metadata":{"_uuid":"2ea17a52-fbd7-4895-80e9-f2d1489fabe0","_cell_guid":"472613de-cded-4c33-8e15-82a565a6a393","trusted":true},"cell_type":"markdown","source":"## 04) Removing Outliers\n\nFrom the previous pair plots, we can see that there are outliers for `TotalBsmtSF`, `1stFlrSF` and `GrLivArea`. Let's use the scatterplot to observe these outliers more precisely"},{"metadata":{"_uuid":"cf39a774-c607-4086-87b9-849a6f07c125","_cell_guid":"510e9d62-df51-40f9-aa8e-cf7da6261bdd","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\n\nax1 = plt.subplot(1, 3, 1)\nplt.scatter(x=train_data.TotalBsmtSF, y=train_data.SalePrice)\nplt.title('TotalBsmtSF x SalePrice', size=15)\n\nax2 = plt.subplot(1, 3, 2)\nplt.scatter(x=train_data['1stFlrSF'], y=train_data.SalePrice)\nplt.title('1stFlrSF x SalePrice', size=15)\n\nax3 = plt.subplot(1, 3, 3)\nplt.scatter(x = train_data.GrLivArea, y=train_data.SalePrice)\nplt.title('GrLivArea x SalePrice', size=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc0346a8-c524-4775-bf7d-32c39ee978d4","_cell_guid":"e30dc35e-4802-4293-a51f-62a381a14c28","trusted":true},"cell_type":"code","source":"print(train_data.shape)\n\n# Removing the four outliers found \ntrain_data.drop(train_data[train_data['TotalBsmtSF'] > 5000].index, inplace=True)\ntrain_data.drop(train_data[train_data['1stFlrSF'] > 4000].index,inplace=True)\ntrain_data.drop(train_data[(train_data['GrLivArea'] > 4000) & (train_data['SalePrice'] < 300000)].index, inplace = True)\n\nprint(train_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ffb7228-3007-4cb6-aa0f-f0b7ea745710","_cell_guid":"d6ff0c3e-0342-48d8-bee6-e009c4a224ed","trusted":true},"cell_type":"markdown","source":"Since only two outliers were dropped, it means that the three features shared the same outlier."},{"metadata":{"_uuid":"d305a496-22f9-4d52-9e4e-1372238c596e","_cell_guid":"ee670caf-6481-4ba0-8ff3-1446e19faa2a","trusted":true},"cell_type":"markdown","source":"## 05) Imputation of Missing Values\n\nLet's look at the missing valeus in our data. We will be using `msno` library. This library provides a small toolset of flexible and easy-to-use missing data visualizations and utilities that allows you to get a quick visual summary of the completeness (or lack thereof) of your dataset"},{"metadata":{"_uuid":"f1702163-373d-4b55-8ce4-10b48581ea75","_cell_guid":"886ed303-456f-415a-9fbd-ada516131b44","trusted":true},"cell_type":"code","source":"# Visualising missing values of numeric features\nmsno.matrix(train_data.select_dtypes(include=[np.number]));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01b3c172-0ac5-43d3-a2a7-c2172fca4559","_cell_guid":"ea993f01-7b75-4d48-a181-dfd0ba18bf7c","trusted":true},"cell_type":"code","source":"# Visualising percentage of missing values of the top 5 numeric variables\ntotal = train_data.select_dtypes(include=[np.number]).isnull().sum().sort_values(ascending=False)\npercent = (train_data.select_dtypes(include=[np.number]).isnull().sum() / train_data.select_dtypes(include=[np.number]).isnull().count()).sort_values(ascending=False)\n\nmissing_data = pd.concat([total, percent], axis=1, join='outer', keys=['Missing Count', 'Missing Percentage'])\nmissing_data.index.name=' Numeric Feature'\nmissing_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d974cf4-7724-4b32-ad6b-c99af276ce90","_cell_guid":"50602022-480d-45b1-a429-2dcf47c719d9","trusted":true},"cell_type":"markdown","source":"We observe that `LotFrontage`, `GarageYrBlt` and `MasVnrArea` are the only one who have missing values"},{"metadata":{"_uuid":"50327dbf-00d4-48e1-bab2-8072be558ae3","_cell_guid":"0b6b74a8-0828-49ab-9cff-fd0f0dbbbee4","trusted":true},"cell_type":"code","source":"# Visualising missing values of categorical features\nmsno.matrix(train_data.select_dtypes(include=[np.object]));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91a8d18a-c693-45cb-9306-f030996ebe2b","_cell_guid":"5c844df8-b064-4a97-b8a0-b27382566f86","trusted":true},"cell_type":"code","source":"# Visualising percentage of missing values of the top 10 categorical variables\ntotal = train_data.select_dtypes(include=[np.object]).isnull().sum().sort_values(ascending=False)\npercent = (train_data.select_dtypes(include=[np.object]).isnull().sum() / train_data.select_dtypes(include=[np.object]).isnull().count()).sort_values(ascending=False)\n\nmissing_data = pd.concat([total, percent], axis=1,join='outer', keys=['Missing Count', 'Missing Percentage'])\nmissing_data.index.name =' Object Feature'\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9c98f86-6e19-415a-acba-8e9cfdb7d927","_cell_guid":"c2ea4d6a-78c8-4b1b-b89d-7fb9c7c01efa","trusted":true},"cell_type":"markdown","source":"We observe that `PoolQC`, `MiscFeature`, `Alley`, `Fence` and `FireplaceQu` have a significant amount of missing values."},{"metadata":{"_uuid":"2227a59c-0582-48dd-bd96-cd166bd5913e","_cell_guid":"60942cf5-d646-44bc-93f8-722e27610dfc","trusted":true},"cell_type":"markdown","source":"First of all, let's start by replacing the missing values in both the training and the test set. So we will be combining both datasets into one dataset"},{"metadata":{"_uuid":"fc93fad0-ce66-4460-a040-b67130160bbb","_cell_guid":"1a2e3996-6458-437c-90b4-2ca866060b36","trusted":true},"cell_type":"code","source":"# Concatenate the training and test datasets into a single dataframe\ndata_full = pd.concat([train_data,test_data], ignore_index=True)\ndata_full.drop('Id', axis=1, inplace=True)\n\ndata_full.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1d50744-fb5c-4794-9d25-b9c049157f58","_cell_guid":"ea41f92f-e455-4c1a-9326-1d20d3469cd6","trusted":true},"cell_type":"code","source":"# Sum of missing values by numeric features\nsum_missing_values = data_full.select_dtypes(include=[np.number]).isnull().sum()\n\nsum_missing_values[sum_missing_values > 0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ffb8c10-3402-4c77-a89b-02bb916b039a","_cell_guid":"a3e849bd-d2c8-4a10-8b24-884a2be4f742","trusted":true},"cell_type":"code","source":"# Numeric features with small number of NaNs: replace with 0\nfor col in ['BsmtHalfBath', 'BsmtFullBath', 'GarageArea', 'GarageCars', 'TotalBsmtSF', 'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1']:\n    data_full[col].fillna(0, inplace=True)\n\n# Check if missing values are imputed successfully\nsum_missing_values = data_full.select_dtypes(include=[np.number]).isnull().sum()\nsum_missing_values[sum_missing_values > 0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bfa0cf7-9fd3-4868-8689-c4209baefe8c","_cell_guid":"03cef788-c701-4adb-8d86-44554162e402","trusted":true},"cell_type":"markdown","source":"Since 'MasVnrArea' only have 23 missing values, we can replace them with the mean of the column"},{"metadata":{"_uuid":"50ff055c-de49-4d45-85df-bc7cc8cee6e7","_cell_guid":"8cf172b9-9101-4109-ac6a-513f696b3ccd","trusted":true},"cell_type":"code","source":"# Numeric features with medium number of NaNs: replace with the mean\ndata_full['MasVnrArea'].fillna(data_full['MasVnrArea'].mean(), inplace=True)\n\n# Check if missing values are imputed successfully\nsum_missing_values = data_full.select_dtypes(include=[np.number]).isnull().sum()\nsum_missing_values[sum_missing_values > 0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0445fa43-3e77-4eb3-8d3c-d91d47955cef","_cell_guid":"e8a0b51f-4031-4aaf-9b41-65c3945f3b21","trusted":true},"cell_type":"markdown","source":"Based on the previous correlation heatmap, 'GarageYrBlt' is highly correlated with 'YearBuilt', so let's replace the missing values by medians of 'YearBuilt'. To do that, we need to cut 'YearBuilt' into sections since it is a numeric variable"},{"metadata":{"_uuid":"56de20dd-0238-4163-a3bf-58199bdea9d6","_cell_guid":"4f4f2347-1ebd-4f06-8c03-a426f03e5ea2","trusted":true},"cell_type":"code","source":"# Cut 'YearBuilt' into 10 parts\ndata_full['YearBuiltCut'] = pd.qcut(data_full['YearBuilt'], 10)\n\n# Impute the missing values of 'GarageYrBlt' based on the median of 'YearBuilt' \ndata_full['GarageYrBlt'] = data_full.groupby(['YearBuiltCut'])['GarageYrBlt'].transform(lambda x : x.fillna(x.median()))\n\n# Convert the values to integers\ndata_full['GarageYrBlt'] = data_full['GarageYrBlt'].astype(int)\n\n# Drop 'YearBuiltCut' column\ndata_full.drop('YearBuiltCut', axis=1, inplace=True)\n\n# Check if missing values are imputed successfully\nsum_missing_values = data_full.select_dtypes(include=[np.number]).isnull().sum()\nsum_missing_values[sum_missing_values > 0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d514e791-a0b8-46eb-ab63-e782b38ca2f5","_cell_guid":"3de6fa71-5591-41bc-90c6-977806cbed4f","trusted":true},"cell_type":"markdown","source":"Based on the previous correlation heatmap, 'LotFrontage' is highly correlated with 'LotArea' and 'Neighborhood'. So let's use the same method to fill the missing values"},{"metadata":{"_uuid":"fa3c6c53-d01c-4570-ac4a-dc788e091836","_cell_guid":"579eafc4-8b8e-48d7-87d8-9cd8ba07dacc","trusted":true},"cell_type":"code","source":"# Cut 'LotArea' into 10 parts\ndata_full['LotAreaCut'] = pd.qcut(data_full['LotArea'], 10)\n\n# Impute the missing values of 'LotFrontage' based on the median of 'LotArea' and 'Neighborhood'\ndata_full['LotFrontage'] = data_full.groupby(['LotAreaCut','Neighborhood'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\ndata_full['LotFrontage'] = data_full.groupby(['LotAreaCut'])['LotFrontage'].transform(lambda x : x.fillna(x.median()))\n\n# Drop 'LotAreaCut' column\ndata_full.drop('LotAreaCut',axis=1,inplace=True)\n\n# Check if missing values are imputed successfully\nsum_missing_values = data_full.select_dtypes(include=[np.number]).isnull().sum()\nsum_missing_values[sum_missing_values > 0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d4ad46c-70dc-4f9e-ae61-3f36d093199d","_cell_guid":"29204c47-3d88-43c0-88bf-c50a02811648","trusted":true},"cell_type":"markdown","source":"The only missing values that are left are within SalePrice, which is exactly the number of lignes in the test data (the values that we need to predict)."},{"metadata":{"_uuid":"2e709176-f402-4a8a-bf63-a9f6f3f191b5","_cell_guid":"1f74a96a-f63b-4fda-8802-45da339a0d0b","trusted":true},"cell_type":"code","source":"# Sum of missing values by feature (object)\nsum_missing_values = data_full.select_dtypes(include=[np.object]).isnull().sum()\nsum_missing_values[sum_missing_values > 0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e167a1e3-5c12-45fe-8a60-f15d861786f5","_cell_guid":"d39f513b-160e-4243-a848-5099956f4672","trusted":true},"cell_type":"code","source":"# Categorical features with less than 5 missing values: replace with the mode (most frequently occured value)\nfor col in ['MSZoning', 'Functional', 'Utilities', 'Exterior1st', 'SaleType', 'Exterior2nd', 'KitchenQual', 'Electrical']:\n    data_full[col].fillna(data_full[col].mode()[0], inplace=True)\n\n# Check if missing values are imputed successfully\nsum_missing_values = data_full.select_dtypes(include=[np.object]).isnull().sum()\nsum_missing_values[sum_missing_values > 0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"904aeae1-7351-4f40-98b9-024c06536e03","_cell_guid":"b37767f4-8c72-4126-83d0-fb3ae904c682","trusted":true},"cell_type":"code","source":"# Categorical features with more than 5 missing values: replace with 'None'\nfor col in ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageQual','GarageCond','GarageFinish','GarageType','BsmtExposure','BsmtCond','BsmtQual','BsmtFinType2','BsmtFinType1','MasVnrType']:\n    data_full[col].fillna('None', inplace=True)\n\n# Check if missing values are imputed successfully\nsum_missing_values = data_full.select_dtypes(include=[np.object]).isnull().sum()\nsum_missing_values[sum_missing_values > 0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23285c2a-5324-40d0-bc02-adebebeabe44","_cell_guid":"06b006b5-3489-45d9-bc3c-34e0cc93e675","trusted":true},"cell_type":"markdown","source":"## 06) Correcting Features\n\nIf we take a look at the numeric variables, we see that some of them obviously don't make a sense being numerical like year related features. Let's take a closer look at each one of them in the data description file and see which ones need to be converted to categorical type."},{"metadata":{"_uuid":"1c547ca5-6618-4a1a-ad01-94a65586e7d5","_cell_guid":"f469d690-56f0-417b-a2aa-30269a56b6e5","trusted":true},"cell_type":"code","source":"data_full.select_dtypes(include=[np.number]).columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15b5e5e3-211d-442c-938a-3244d83173ae","_cell_guid":"136cdffb-f049-4a36-818e-f78b3c3a00ec","trusted":true},"cell_type":"code","source":"# Converting numeric features to categorical features\nstr_cols = ['YrSold','YearRemodAdd','YearBuilt','MoSold','MSSubClass','GarageYrBlt']\n\nfor col in str_cols:\n    data_full[col] = data_full[col].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db1fd477-2f47-4601-a625-99a3fad7dac3","_cell_guid":"ab95ad84-eca9-41ce-8919-c799e54d9f49","trusted":true},"cell_type":"markdown","source":"## 07) Adding Features\n\nFirst, we will map some categorical variable that represent some sort of rating to an integer score."},{"metadata":{"_uuid":"6a6e4c98-9378-47d0-bb06-2b9b92ac13de","_cell_guid":"6b2d854c-1e0b-4479-b201-d4b998cbb807","trusted":true},"cell_type":"code","source":"data_full.select_dtypes(include=[np.object]).columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fd89b70-ec55-411a-9676-516900d06ac2","_cell_guid":"7d2804dd-8afa-45f8-bf60-e06d259380ae","trusted":true},"cell_type":"code","source":"data_full['GarageCond'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3fcfb62c-8342-465e-b32d-3249f507959d","_cell_guid":"f67a845c-431c-4fa1-84f1-ed3d87bd8916","trusted":true},"cell_type":"code","source":"# ExterQual = Evaluates the quality of the material on the exterior: Ex(Excellent), Gd(Good), TA(Typical), Fa(Fair), Po(Poor)\ndata_full[\"oExterQual\"] = data_full['ExterQual'].map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n\n# ExterCond = Evaluates the present condition of the material on the exterior: Ex(Excellent), Gd(Good), TA(Typical), Fa(Fair), Po(Poor)\ndata_full[\"oExterCond\"] = data_full['ExterCond'].map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n\n# BsmtQual = Evaluates the height of the basement: Ex(Excellent), Gd(Good), TA(Typical), Fa(Fair), Po(Poor), NA(No Basement)\ndata_full[\"oBsmtQual\"] = data_full['BsmtQual'].map({'None':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n\n# BsmtExposure = Refers to walkout or garden level walls: Gd(Good), Av(Average), Mn(Minimum), No(No Exposure), NA(No Basement)\ndata_full[\"oBsmtExposure\"] = data_full['BsmtExposure'].map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n\n# BsmtCond = Evaluates the general condition of the basement: Ex(Excellent), Gd(Good), TA(Typical), Fa(Fair), Po(Poor), NA(No Basement)\ndata_full[\"oBsmtCond\"] = data_full['BsmtCond'].map({'None':1, 'Po':2, 'Fa':3, 'TA':3, 'Gd':4})\n\n# HeatingQC = Heating quality and condition: Ex(Excellent), Gd(Good), TA(Average), Fa(Fair), Po(Poor)\ndata_full[\"oHeatingQC\"] = data_full['HeatingQC'].map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n\n# KitchenQual: Kitchen quality: Ex(Excellent), Gd(Good), TA(Typical), Fa(Fair), Po(Poor)\ndata_full[\"oKitchenQual\"] = data_full['KitchenQual'].map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n\n# FireplaceQu: Fireplace quality: Ex(Excellent), Gd(Good), TA(Average), Fa(Fair), Po(Poor), NA(No Fireplace)\ndata_full[\"oFireplaceQu\"] = data_full['FireplaceQu'].map({'None':1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\n\n# GarageFinish: Interior finish of the garage: Fin(Finished), RFn(Rough Finished), Unf(Unfinished), NA(No Garage)\ndata_full[\"oGarageFinish\"] = data_full['GarageFinish'].map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n\n# GarageQual: Garage quality: Ex(Excellent), Gd(Good), TA(Typical), Fa(Fair), Po(Poor), NA(No Garage)\ndata_full[\"oGarageQual\"] = data_full['GarageQual'].map({'None':1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\n\n# GarageCond: Garage condition: Ex(Excellent), Gd(Good), TA(Typical), Fa(Fair), Po(Poor), NA(No Garage)\ndata_full[\"oGarageCond\"] = data_full['GarageCond'].map({'None':1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6})\n\n# PavedDrive: Paved driveway: Y(Padev), P(Partial Pavement), N(Dirt)\ndata_full[\"oPavedDrive\"] = data_full['PavedDrive'].map({'N':1, 'P':2, 'Y':3})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7467bfe8-6f31-4e13-aab3-4a9bc949dc98","_cell_guid":"3b988fef-9fa5-4797-9fe8-ca6ee7420b92","trusted":true},"cell_type":"markdown","source":"Next, we will add up some numeric features with each other to create new features that make sense"},{"metadata":{"_uuid":"b5084aaa-3391-4e73-a6bb-263094cd7d9b","_cell_guid":"985e5405-2942-4e37-a45b-eac2805a446b","trusted":true},"cell_type":"code","source":"data_full.select_dtypes(include=[np.number]).columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b787459f-b6ee-4d54-95d1-79a6e02297a6","_cell_guid":"d22855cb-f6d9-4d21-a96d-c5f536d80b28","trusted":true},"cell_type":"code","source":"# House square feet = First floor square feet + Second floor square feet + Total square feet of basement area\ndata_full['HouseSF'] = data_full['1stFlrSF'] + data_full['2ndFlrSF'] + data_full['TotalBsmtSF']\n\n# Porch square feet = Three season porch area in square feet + Enclosed porch area in square feet + Screen porch area in square feet\ndata_full['PorchSF'] = data_full['3SsnPorch'] + data_full['EnclosedPorch'] + data_full['OpenPorchSF'] + data_full['ScreenPorch']\n\n# Total square feet = House square feet + Porch square feet + Garage area\ndata_full['TotalSF'] = data_full['HouseSF'] + data_full['PorchSF'] + data_full['GarageArea']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d432d17c-770e-4908-8928-e177f3af5990","_cell_guid":"d3e7086b-2255-4145-a2da-2a915ae0f8cd","trusted":true},"cell_type":"markdown","source":"## 08) Skewness and Kurtosis\n\n**Skewness** is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined. For a unimodal distribution, negative skew commonly indicates that the tail is on the left side of the distribution, and positive skew indicates that the tail is on the right. \n\n**Kurtosis** is a measure of the \"tailedness\" of the probability distribution of a real-valued random variable. Like skewness, kurtosis describes the shape of a probability distribution and there are different ways of quantifying it for a theoretical distribution and corresponding ways of estimating it from a sample from a population. Different measures of kurtosis may have different interpretations."},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# Estimate Skewness of the data\ntrain_data.skew()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c75bebc4-c696-40c1-ba2d-5f9c4153b457","_cell_guid":"e24aca6e-66e7-4e09-ab3e-c6100600d1ba","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Estimate Kurtosis of the data\ntrain_data.kurt()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01b458b7-6d34-496b-9662-cb50e074deb9","_cell_guid":"825e2fbd-de9b-4372-b26b-7f6b22edeea5","trusted":true},"cell_type":"code","source":"# Plot the Skewness and Kurtosis of the data\nplt.figure(figsize=(15,5))\n\nax1 = plt.subplot(1, 2, 1)\nsns.distplot(train_data.skew(), axlabel ='Skewness')\n\nax2 = plt.subplot(1, 2, 2)\nsns.distplot(train_data.kurt(), axlabel ='Kurtosis')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06a2ed19-cb7f-4b8b-9788-f40aaf0ffb0d","_cell_guid":"f7b53852-a89b-42a9-87be-10fb515ef0ab","trusted":true},"cell_type":"markdown","source":"There isn't much Kurtosis in the data columns, but Skewness is very present, meaning that distribution is not symetrical."},{"metadata":{"_uuid":"c3adf1ed-0203-47b0-9bed-3c01f7c5e234","_cell_guid":"2594dbd8-c8f2-483e-a28b-c35408100666","trusted":true},"cell_type":"markdown","source":"## 09) Label Encoding\n\nFor this section we will use Pipelines which are a way to streamline a lot of the routine processes. It provides a way to take code, fit it to the training data, apply it to the test data without having to copy and paste everything.\n\n- **Skewness**: Doing the transformation in the distribution to remove the positive skew. \n\n- **Label Encoder and One Hot Encoder:** These two encoders are parts of the SciKit Learn library in Python, and they are used to convert categorical data, or text data, into numbers, which our predictive models can better understand."},{"metadata":{"_uuid":"277cbf48-4b65-425b-b2d6-e5bbd2ee0f7a","_cell_guid":"4f7ac89a-b401-4f69-9407-64be79eff99a","trusted":true},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom scipy.stats import skew\n\n# Label encoding class\nclass labenc(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        label = LabelEncoder()\n        \n        X['YrSold'] = label.fit_transform(X['YrSold'])\n        X['YearRemodAdd'] = label.fit_transform(X['YearRemodAdd'])\n        X['YearBuilt'] = label.fit_transform(X['YearBuilt'])\n        X['MoSold'] = label.fit_transform(X['MoSold'])\n        X['GarageYrBlt'] = label.fit_transform(X['GarageYrBlt'])\n        \n        return X\n    \n# Skewness transform class\nclass skewness(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        skewness = X.select_dtypes(include=[np.number]).apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= 1].index\n        \n        X[skewness_features] = np.log1p(X[skewness_features])\n        \n        return X\n\n# One hot encoding class\nclass onehotenc(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X = pd.get_dummies(X)\n        \n        return X","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fec566f4-a7d1-4a40-a213-721d71d0cf2e","_cell_guid":"241b498f-6cd7-4cd0-be1f-494923a9e944","trusted":true},"cell_type":"code","source":"# Creating a copy of the full dataset\ndata_full_copy = data_full.copy()\n\n# Creating a new data with the applied transformations using a Pipeline\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([('labenc', labenc()), ('skewness', skewness()), ('onehotenc', onehotenc())])\n\ndata_pipeline = pipeline.fit_transform(data_full_copy)\n\ndata_full.shape, data_pipeline.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c448350-f782-4824-a185-4326bb8ef143","_cell_guid":"7dbdecb2-c5d9-4d26-aedd-210894ee16b9","trusted":true},"cell_type":"markdown","source":"We can see now that the number of features increases from 95 to 332 because of the feature transformations."},{"metadata":{"_uuid":"fc16ab71-442f-40de-83b7-7392596ad409","_cell_guid":"0a4686b9-f860-4ace-ac6b-5bb399fc474a","trusted":true},"cell_type":"code","source":"data_full.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27d232a7-307e-4699-99db-e64553efa4de","_cell_guid":"1c28006b-a9c0-40d1-a5f8-8eac6124adf9","trusted":true},"cell_type":"code","source":"data_pipeline.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07c367e3-f9c0-45fa-b314-67311d32875a","_cell_guid":"e8c35351-a66f-42fc-bf45-8bd33643df45","trusted":true},"cell_type":"markdown","source":"Now we split the data to training and testing datasets again."},{"metadata":{"_uuid":"278cbda7-1adc-4fed-b100-b701cb571e3e","_cell_guid":"dc093c0d-2482-476b-b5a4-0f0898da89cc","trusted":true},"cell_type":"code","source":"X_train = data_pipeline[:train_data.shape[0]]\ny_train = X_train['SalePrice']\nX_train.drop(columns='SalePrice', inplace=True)\n\nX_test = data_pipeline[train_data.shape[0]:]\nX_test.drop(columns='SalePrice', inplace=True)\n\nX_train.shape, y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a5aedba8-4463-4594-a3cd-df3729c84325","_cell_guid":"0551df40-f98a-476a-bf1c-59e2013a96d1","trusted":true},"cell_type":"markdown","source":"## 10) Transformation and Scaling"},{"metadata":{"_uuid":"baec03d5-f852-478e-8ba0-556b18e55505","_cell_guid":"84c13ce0-0810-472d-a550-d8961cb03929","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\n\nax1 = plt.subplot(1, 3, 1)\nsns.distplot(y_train, kde=False, fit=st.norm)\nplt.title('Normal', size = 15)\n\nax2 = plt.subplot(1, 3, 2)\nsns.distplot(y_train, kde=False, fit=st.lognorm)\nplt.title('Log Normal', size = 15)\n\nax3 = plt.subplot(1, 3, 3)\nsns.distplot(y_train, kde=False, fit=st.johnsonsu)\nplt.title('Johnson SU', size = 15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c282ebb3-b747-487a-9eab-e8f50e1fee9b","_cell_guid":"34673ae3-8910-4eca-baca-ffc35d71df1a","trusted":true},"cell_type":"markdown","source":"Normal distribution doesn't fit, so SalePrice need to be transformed before creating the model. Best fit is unbounded Johnson distribution, altough log normal distribution also fits well"},{"metadata":{"_uuid":"516693d0-dcfb-41c8-a48b-34a8fa7b258c","_cell_guid":"bc137d87-ccda-4d33-9157-f2f7e6ee3da4","trusted":true},"cell_type":"code","source":"# Transforming 'SalePrice' into normal distribution\ny_train_transformed = np.log(y_train)\n\ny_train_transformed.skew(), y_train_transformed.kurt()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2ae4020-c091-4900-9257-2181040cd99f","_cell_guid":"0ebf6e4f-d37e-4ac8-a782-102c31319d12","trusted":true},"cell_type":"code","source":"# Plotting 'SalePrice' before and after the transformation\nplt.figure(figsize=(15,5))\n\nax1 = plt.subplot(1, 2, 1)\nsns.distplot(y_train)\nplt.title('Before Transformation', size=15)\n\nax2 = plt.subplot(1, 2, 2)\nsns.distplot(y_train_transformed)\nplt.title('After Transformation', size=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8ae888f-3e30-42d6-b5c5-2e8d4c13e9fa","_cell_guid":"75f029b7-a783-4118-a2ac-1d9a61533553","trusted":true},"cell_type":"code","source":"# Using RobustScaler to transform X_train and X_test\nfrom sklearn.preprocessing import RobustScaler\nrobust_scaler = RobustScaler()\n\nX_train_scaled = robust_scaler.fit(X_train).transform(X_train)\nX_test_scaled = robust_scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cec437b9-8c37-4858-9cc9-cc08ff72bea6","_cell_guid":"e10f673d-a1e8-41e2-ab72-f77beb7c64a4","trusted":true},"cell_type":"code","source":"# Shape of final data we will be working on\nX_train_scaled.shape, y_train_transformed.shape, X_test_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff3f8179-a032-428b-9ddf-0a4de1e4eda1","_cell_guid":"d3a50167-ea8a-4009-99ae-87640c3258df","trusted":true},"cell_type":"markdown","source":"## 11) Feature Selection"},{"metadata":{"_uuid":"8c33e026-d01a-412a-8743-a66105621669","_cell_guid":"e43a139d-ad2b-4817-880c-2b08ef624bba","trusted":true},"cell_type":"markdown","source":"We will use lasso regression (l1 regularization method). Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. We can also use it to find the most important features in our dataset."},{"metadata":{"_uuid":"ea613660-56ae-476e-b8f9-e3a5abb16404","_cell_guid":"db96b3d4-e6f7-4c23-a089-5884b829e529","trusted":true},"cell_type":"code","source":"# Display features by their importance (lasso regression coefficient)\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha = 0.001)\n\nlasso.fit(X_train_scaled, y_train_transformed)\n\ny_pred_lasso = lasso.predict(X_test_scaled)\n\nlasso_coeff = pd.DataFrame({'Feature Importance':lasso.coef_}, index=data_pipeline.drop(columns='SalePrice').columns)\nlasso_coeff.sort_values('Feature Importance', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7225f84-4664-492b-887e-5f0cb0862f8a","_cell_guid":"d621b8a9-fc13-4286-95ce-343621aa39ba","trusted":true},"cell_type":"code","source":"# Plot features by importance (feature coefficient in the model)\nlasso_coeff[lasso_coeff['Feature Importance'] != 0].sort_values('Feature Importance').plot(kind='barh',figsize=(20,20))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5be8be30-18ec-4abb-943a-64ef87037551","_cell_guid":"ea85f80b-58ac-479f-b358-18d1494f2f83","trusted":true},"cell_type":"markdown","source":"What's intersting here is that two of the variables that we have created 'HouseSF' and 'PorchSF' perform actually bad compared to their components. But when we sum all the surfaces as in 'TotalSF', which is just a combination of features that are significantly unimportant in this model, we suddently obtain the most important feature in the dataset."},{"metadata":{"_uuid":"9ff686fe-a6f6-4471-a7c5-a60f7fb855db","_cell_guid":"7c8ccd7c-8e2f-49ad-8361-1fe50d032b41","trusted":true},"cell_type":"markdown","source":"## 12) Principal Components Analysis\n\nPrincipal Components Analysis (PCA) is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance."},{"metadata":{"_uuid":"ed1ab731-df7f-4d3f-a65c-f5dea2405089","_cell_guid":"e41a6572-865b-48b1-a046-9ab6dcbe75ac","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Concatenate the training and test datasets into a single dataframe\ndata_full_2 = np.concatenate([X_train_scaled, X_test_scaled])\n\n# Choose the number of principle components such that 95% of the variance is retained\npca = PCA(0.95)\ndata_full_2 = pca.fit_transform(data_full_2)\n\nvar_PCA = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n\n# Principal Component Analysis of data\nprint(var_PCA)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b195ea6b-1f77-4ea7-a9fc-35a6be45faab","_cell_guid":"c918550f-4963-444b-a3ea-48fbb0a9fbf6","trusted":true},"cell_type":"code","source":"# Principal Component Analysis plot of the data\nplt.figure(figsize=(15,5))\n\nplt.bar(x=range(1, len(var_PCA) + 1), height=var_PCA)\nplt.ylabel(\"Explained Variance (%)\", size=15)\nplt.xlabel(\"Principle Components\", size=15)\nplt.title(\"Principle Component Analysis Plot : Training Data\", size=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9452335e-125c-41c2-b4b6-321cfc02e99e","_cell_guid":"d9bd783e-fae3-4cf5-9147-799b7b39f2fe","trusted":true},"cell_type":"code","source":"# Shape of final data we will be working on\nX_train_scaled = data_full_2[:train_data.shape[0]]\n\nX_test_scaled = data_full_2[train_data.shape[0]:]\n\nX_train_scaled.shape, y_train_transformed.shape, X_test_scaled.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d411dc37-c978-444a-8d78-bf17b337c2e6","_cell_guid":"8c489fe0-b411-4ef1-bf07-6482551bffce","trusted":true},"cell_type":"markdown","source":"## 13) Testing Different Models\n\nNow that we have finished preparing our data, it's time to test different models to see which one performs the best.\nThe models we will be testing are : \n- Linear Regression\n- Support Vector Regression\n- Stochastic Gradient Descent\n- Gradient boosting tree\n- Random forest\n- Lasso regression\n- Ridge regression\n- Elastic net regularization\n- Extra trees regression"},{"metadata":{"_uuid":"aedc58b8-b540-45f4-81e0-fa1ead522cd5","_cell_guid":"b2d684ba-ad22-4861-a480-d8a2aedc6ce8","trusted":true},"cell_type":"code","source":"# Importing the models\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, Lasso, SGDRegressor, Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.svm import LinearSVR, SVR\n\n# kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n#alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n#alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n#e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n#e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n\n# Adicionar RidgeCV(alpha=alphas_alt, cv=kfolds)\n# Adicionar LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds)\n# Adicionar ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio)\n# Adicionar SVR(C= 20, epsilon= 0.008, gamma=0.0003,)\n# Adicionar GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)\n# Adicionar LGBMRegressor(objective='regression', num_leaves=4, learning_rate=0.01, n_estimators=5000, max_bin=200, bagging_fraction=0.75, bagging_freq=5, bagging_seed=7, feature_fraction=0.2,\n# feature_fraction_seed=7, verbose=-1,)\n\n# Adicionar XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                    # max_depth=3, min_child_weight=0,\n                                    # gamma=0, subsample=0.7,\n                                    # colsample_bytree=0.7,\n                                    # objective='reg:linear', nthread=-1,\n                                    # scale_pos_weight=1, seed=27,\n                                    # reg_alpha=0.00006)\n                        \n# StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                               # meta_regressor=xgboost,\n                               # use_features_in_secondary=True)\n                        \n# Creating the models\nmodels = [LinearRegression(), \n          SVR(),\n          SGDRegressor(),\n          SGDRegressor(max_iter=1000, tol=1e-3),\n          GradientBoostingRegressor(),\n          RandomForestRegressor(),\n          Lasso(),\n          Lasso(alpha=0.01, max_iter=10000),\n          Ridge(),\n          BayesianRidge(),\n          KernelRidge(),\n          KernelRidge(alpha=0.6, kernel='polynomial',degree=2, coef0=2.5),\n          ElasticNet(),\n          ElasticNet(alpha=0.001, max_iter=10000), ExtraTreesRegressor()\n         ]\n\nnames = ['Linear Regression',\n         'Support Vector Regression',\n         'Stochastic Gradient Descent',\n         'Stochastic Gradient Descent 2',\n         'Gradient Boosting Tree',\n         'Random Forest',\n         'Lasso Regression',\n         'Lasso Regression 2',\n         'Ridge Regression',\n         'Bayesian Ridge Regression',\n         'Kernel Ridge Regression',\n         'Kernel Ridge Regression 2',\n         'Elastic Net Regularization',\n         'Elastic Net Regularization 2',\n         'Extra Trees Regression'\n        ]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ecb8822-be8a-41a5-bd08-12e1651a2afc","_cell_guid":"dd1577e8-8713-43f3-b942-d53baea2632c","trusted":true},"cell_type":"code","source":"# Define a root mean square error function\ndef rmse(model, X, y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9ec2ab5-d926-4191-9557-a6937a7d83f9","_cell_guid":"0632bac5-4e30-4f33-a946-a56e60d6657a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\nwarnings.filterwarnings('ignore')\n\n# Perform 5-folds cross-validation to evaluate the models \nfor model, name in zip(models, names):\n    # Root mean square error\n    score = rmse(model, X_train_scaled, y_train_transformed)\n    print(\"- {}: Mean: {:.6f}, Std: {:4f}\".format(name, score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4484d4da-bbf5-4872-8f65-fd6b7e5eb4a9","_cell_guid":"adef7d9a-9d8f-42d0-a29b-387fff9f8333","trusted":true},"cell_type":"markdown","source":"Surprisingly, the Random forest and Extra trees regression models are the ones who performed the worst, and the linear regression model performed actually pretty good relative to the other models.\nBy compiling the above code several times and observing the different scores each time, we can classify the models by accuracy :\n\n- 1st : Kernel ridge regression\n- 2nd : Elastic net regularization and Bayesian ridge regression\n- 3rd : Ridge regression and Linear regression\n- 4rth : Support vector regression\n- 5th : Gradient boosting tree\n- 6th : Stochastic gradient  and Lasso regression\n- 7th : Random forest and Extra trees regression\n\nI think we got a good score in Elastic net regularization, Lasso regression and Stochastic gradient descent because we chose some good parameters. We can see that their score above is very bad when not specifing parameter values. So if we really want to know to best model, we need to choose optimal parameters for all the models, and tha's what we will do in the next section."},{"metadata":{"_uuid":"eb0a5b38-e2ce-4f3f-a5bd-69f9f82d5ba1","_cell_guid":"506af5c1-4555-497a-8866-b164f04ca44e","trusted":true},"cell_type":"markdown","source":"## 14) Hyper-parameter Tuning\n\nFor choosing the most optimal hyper-parameters, we will perform gird search. the class GridSearchCV exhaustively considers all parameter combinations and generates candidates from a grid of parameter values specified with the param_grid parameter.\nSince we will use the same procedure for all models, we will start by creating a function which takes specified parameter values as entry."},{"metadata":{"_uuid":"f4516f0e-c8a9-426d-8447-592ec606ee62","_cell_guid":"70d80f1a-d9ae-4c80-a695-d9516d0b4f15","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nclass gridSearch():\n    def __init__(self, model):\n        self.model = model\n    def grid_get(self, param_grid):\n        grid_search = GridSearchCV(self.model, param_grid, cv=5, scoring='neg_mean_squared_error')\n        grid_search.fit(X_train_scaled, y_train_transformed)\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        \n        #print(pd.DataFrame(grid_search.cv_results_)[['params', 'mean_test_score', 'std_test_score']])\n        print('Best Parameters: {}, \\nBest Score: {}'.format(grid_search.best_params_, np.sqrt(-grid_search.best_score_)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2403ec23-66e0-4d72-b7e1-a63e345ba7e9","_cell_guid":"c20ff359-f227-4839-b5f5-9e89cd7a281d","trusted":true},"cell_type":"markdown","source":"### 1. Kernel Ridge Regression"},{"metadata":{"_uuid":"3ccf6483-3f6a-4f47-aa3d-1125111162dd","_cell_guid":"430f2ba5-9958-4549-84c4-957a98f67988","trusted":true},"cell_type":"code","source":"gridSearch(KernelRidge()).grid_get({'alpha':[3.5, 4, 4.5, 5, 5.5, 6, 6.5], 'kernel':[\"polynomial\"], 'degree':[3], 'coef0':[1, 1.5, 2, 2.5, 3, 3.5]})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61f4c25e-721c-4305-9492-adbd589a0932","_cell_guid":"00e6d7e2-0762-4125-9487-fd07475976cc","trusted":true},"cell_type":"markdown","source":"### 2. Elastic Net Regularization"},{"metadata":{"_uuid":"d08463f3-8185-4083-9118-9f5f417637e6","_cell_guid":"ef3f5ead-b5ab-4943-b89b-8b7efd55aa25","trusted":true},"cell_type":"code","source":"gridSearch(ElasticNet()).grid_get({'alpha':[0.006, 0.0065, 0.007, 0.0075, 0.008], 'l1_ratio':[0.070, 0.075, 0.080, 0.085, 0.09, 0.095], 'max_iter':[10000]})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"35416d92-439b-4f61-b3a1-0a103a5e2435","_cell_guid":"0ce153d3-3af1-40ba-8987-9c1931fe8435","trusted":true},"cell_type":"markdown","source":"### 3. Ridge regression"},{"metadata":{"_uuid":"3d9e2822-1ecb-48a5-afb0-7e827697f807","_cell_guid":"26e73325-5abc-4646-99a8-88bc853ed41d","trusted":true},"cell_type":"code","source":"gridSearch(Ridge()).grid_get({'alpha':[10, 20, 25, 30, 35, 40, 45, 50, 55, 57, 60, 65, 70, 75, 80, 100], 'max_iter':[10000]})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6ef674c-318a-4e67-8e4c-dd5cbf9623de","_cell_guid":"8792d6e7-b2cf-41d5-9f2a-f1c3a7f4dcd2","trusted":true},"cell_type":"markdown","source":"### 4. Support vector regression"},{"metadata":{"_uuid":"90413987-5f93-4b3f-9ac0-228f18c8021c","_cell_guid":"7218e135-ebf4-4b87-a8a1-83df5490193f","trusted":true},"cell_type":"code","source":"gridSearch(SVR()).grid_get({'C':[13, 15, 17, 19, 21], 'kernel':[\"rbf\"], \"gamma\":[0.0005, 0.001, 0.002, 0.01], \"epsilon\":[0.01, 0.02, 0.03, 0.1]})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc3aee59-e6b1-4dac-914d-23b3fe28ebe2","_cell_guid":"a350d564-4f86-4ac0-8a68-a469e0ca04ef","trusted":true},"cell_type":"markdown","source":"### 5. Lasso regression"},{"metadata":{"_uuid":"ffad3eca-9445-416d-b1d5-014ed660df32","_cell_guid":"1ca9cb90-aaa7-4976-9224-ff30baa7cdfc","trusted":true},"cell_type":"code","source":"gridSearch(Lasso()).grid_get({'alpha':[0.01, 0.001, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009], 'max_iter':[10000]})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16915b0e-d297-4fbd-ba6a-fcf3ebc391cc","_cell_guid":"2e25ddd1-7f2d-4aff-b91b-b27af4743f0c","trusted":true},"cell_type":"markdown","source":"We see that the models perform almost the same way with a score of 0.116. Let's define these models with the their respective best hyper-parameters."},{"metadata":{"_uuid":"d9fb1c2c-f597-4ac2-b1fa-ff1f8a9dbbbe","_cell_guid":"01bcf0e4-c096-4293-85cc-e24da6bfe4a3","trusted":true},"cell_type":"code","source":"ker = KernelRidge(alpha=6.5, coef0=2.5, degree=3, kernel='polynomial')\nela = ElasticNet(alpha=0.007, l1_ratio=0.07, max_iter=10000)\nridge = Ridge(alpha=35, max_iter= 10000)\nsvr = SVR(C=13, epsilon=0.03, gamma=0.001, kernel='rbf')\nlasso = Lasso(alpha=0.0006, max_iter=10000)\nbay = BayesianRidge()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21764845-9623-4c4a-89f0-59e960f547a9","_cell_guid":"a69de748-6287-4a1f-bbb3-43ae1e4e6816","trusted":true},"cell_type":"markdown","source":"## 15) Making Predictions and Submission\nNow it's time to make predictions and store them in a csv file with corresponding Ids. after we make prediction we need to transform them to their original shape with exponential function"},{"metadata":{"_uuid":"d3d90ef9-3bc0-46cd-b5af-afad7f15d1f5","_cell_guid":"87382e90-0b3f-49f1-a856-cc2996b97397","trusted":true},"cell_type":"code","source":"# Create the model (Random Forest Classifier) and run with the train data\nmodel = SVR(C=13, epsilon=0.03, gamma=0.001, kernel='rbf')\nmodel.fit(X_train_scaled, y_train_transformed)\n\n# Generate the predictions running the model in the test data\npredictions = np.exp(model.predict(X_test_scaled))\n\n# Create the output file \noutput = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': predictions})\noutput.to_csv('submission.csv', index=False)\n\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}