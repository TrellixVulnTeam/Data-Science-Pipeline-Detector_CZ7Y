{"cells":[{"metadata":{},"cell_type":"markdown","source":"**EDA, Data Cleaning and Feature Engineering**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\nimport os\nfrom scipy import stats\nfrom scipy.stats import norm, skew\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\nfrom subprocess import check_output\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ID = train['Id']\ntest_ID = test['Id']\n#Dropping the 'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing the dataset during the EDA process can be very beneficial. For instance, histograms make analyzing the distribution of the data an easier task; boxplots are great for identifying outliers; scatterplots are very useful when it comes to checking the correlations between variables. Another helpful visualization tool to check the correlations between dependent and independent variables is heatmap. But first let's explore the target variable (SalePrice)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'], c='mediumorchid')\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing extreme outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n#Checking the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'], c='black')\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n#Checking the new distribution \nsns.distplot(train['SalePrice'], color='red', fit=norm);\n#Getting the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n#Plotting the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n#QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of the target variable was right-skewed, so we applied natural log to make it normally distributed"},{"metadata":{"trusted":true},"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\ndata = pd.concat((train, test)).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=[40,20])\nsns.heatmap(data.corr(), cmap='viridis', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ((ax1, ax2), (ax3, ax4),(ax5,ax6)) = plt.subplots(nrows=3, ncols=2, figsize=(14,10))\nsns.regplot(x='OverallQual', y = 'SalePrice', color='deepskyblue', data = data, scatter = True,\n            fit_reg=True, ax=ax1)\nsns.regplot(x='TotalBsmtSF', y = 'SalePrice', color='orchid', data = data, scatter= True,\n            fit_reg=True, ax=ax2)\nsns.regplot(x='GrLivArea', y = 'SalePrice', color='crimson', data = data, scatter= True,\n            fit_reg=True, ax=ax3)\nsns.regplot(x='GarageArea', y = 'SalePrice', color='gray', data = data, scatter= True,\n            fit_reg=True, ax=ax4)\nsns.regplot(x='FullBath', y = 'SalePrice', color='gold', data = data, scatter= True,\n            fit_reg=True, ax=ax5)\nsns.regplot(x='YearBuilt', y = 'SalePrice', color='yellowgreen', data = data, scatter= True,\n            fit_reg=True, ax=ax6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. 'GarageCars' and 'GarageArea' are strongly correlated variables. 'TotalBsmtSF' and '1stFlrSF' also seem to have high correlation between them just like 'TotRmsAbvGrd', 'GrLivArea' and '2ndFlrSF"},{"metadata":{},"cell_type":"markdown","source":"Exploratory Data Analysis (EDA) and Data Cleaning are two essential steps before we start to develop Machine Learning Models. One important part in the EDA is to inspect missing values, study if there are any patterns in the missing values, and make a decision about how to deal with them accordingly. The first thing here is to have a general idea of the total and percentage of missing data in each column:"},{"metadata":{"trusted":true},"cell_type":"code","source":"data_na = (data.isnull().sum() / len(data)) * 100\ndata_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :data_na})\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are lots of ways to deal with missing values. We can drop the columns with too many missing values or impute them instead. I don't really like to drop any columns since we loose data that way, so we'll impute missing values and maybe drop just one unnecessary column"},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"PoolQC\"] = data[\"PoolQC\"].fillna(\"None\")\ndata[\"MiscFeature\"] = data[\"MiscFeature\"].fillna(\"None\")\ndata[\"Alley\"] = data[\"Alley\"].fillna(\"None\")\ndata[\"Fence\"] = data[\"Fence\"].fillna(\"None\")\ndata[\"FireplaceQu\"] = data[\"FireplaceQu\"].fillna(\"None\")\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    data[col] = data[col].fillna('None')\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    data[col] = data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    data[col] = data[col].fillna('None')\ndata[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)\ndata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\ndata = data.drop(['Utilities'], axis=1)\ndata[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")\ndata['Electrical'] = data['Electrical'].fillna(data['Electrical'].mode()[0])\ndata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\ndata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])\ndata['MSSubClass'] = data['MSSubClass'].fillna(\"None\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming some features into categorical\ndata['MSSubClass'] = data['MSSubClass'].apply(str)\ndata['OverallCond'] = data['OverallCond'].astype(str)\ndata['YrSold'] = data['YrSold'].astype(str)\ndata['MoSold'] = data['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforming features into numeric\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(data[c].values)) \n    data[c] = lbl.transform(list(data[c].values))        \nprint('Shape data: {}'.format(data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = data.dtypes[data.dtypes != \"object\"].index\n#Checking the skew of all numerical features\nskewed_feats = data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.kurt()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['SalePrice','OverallQual','TotalBsmtSF','GrLivArea', 'FullBath',\n           'YearBuilt','YearRemodAdd']\nsns.pairplot(data[columns], size = 2, kind ='scatter', diag_kind='kde')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A skewness of zero or near zero indicates a symmetric distribution. A negative value for the skewness indicate a left skewness (tail to the left). A positive value for te skewness indicate a right skewness (tail to the right). Kurtosis is a measure of how extreme observations are in a dataset. The greater the kurtosis coefficient , the more peaked the distribution around the mean is. As we can see some features are highly skewed. To fix that we can use Box Cox Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #data[feat] += 1\n    data[feat] = boxcox1p(data[feat], lam)\n#data[skewed_features] = np.log1p(data[skewed_features])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing categorical features\ncategorical_features = data.select_dtypes(include=[np.object])\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y, palette='husl')\n    x=plt.xticks(rotation=90)\nf = pd.melt(data, id_vars=['SalePrice'], value_vars=categorical_features)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some outliers in this data as well, but we are not going to remove these ones too because we don't want our data biased and our models affected by that bias. So we'll keep them and use RobustScaler to make our models robust on them"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"data size is : {}\".format(data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Check remaining missing values if any \ndata_na = (data.isnull().sum() / len(data)) * 100\ndata_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :data_na})\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding total sqfootage feature \ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.get_dummies(data)\nprint(data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data[:ntrain]\ntest = data[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we're done with data cleaning and feature engineering "},{"metadata":{},"cell_type":"markdown","source":"**Models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to use regularized linear models (Lasso, ElasticNet and KernelRidge) to avoid the risk of overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using cross_val_score and shuffling the data\nn_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    #The score which needs to be minimized is negated\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KRR = KernelRidge(alpha=0.8, kernel='polynomial', degree=2, coef0=3.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add GradientBoosting, XGB abd LightGB models to see if the score can get improved"},{"metadata":{"trusted":true},"cell_type":"code","source":"GBoost = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.05,\n                                   max_depth=2, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.5604, gamma=0.0578, \n                             learning_rate=0.05, max_depth=2, \n                             min_child_weight=1.7817, n_estimators=900,\n                             reg_alpha=0.4765, reg_lambda=0.9173,\n                             subsample=0.4738, silent=1,\n                             random_state =42, nthread = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Averaging models\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n    #Defining clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        #Training cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n    #Doing the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The score has been improved"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adding a meta-model and stacking averaged models\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n    #Fitting the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        #Training cloned base models then creating out-of-fold predictions\n        #that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n        #Training the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n     #Doing the predictions of all base models on the test data and using the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we'll add XGB and LightGBM to the StackedRegressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Difining a rmsle evaluation function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''RMSE on the entire Train data when averaging'''\nprint('RMSLE score on train data:')\n#Weighted average\nprint(rmsle(y_train,stacked_train_pred*0.5 +\n               xgb_train_pred*0.1 + lgb_train_pred*0.4 ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = stacked_pred*0.5 + xgb_pred*0.1 + lgb_pred*0.4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**References:**\n\nhttps://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}