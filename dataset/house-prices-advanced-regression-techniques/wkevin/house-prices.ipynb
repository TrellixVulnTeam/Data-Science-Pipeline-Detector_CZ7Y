{"cells":[{"metadata":{"trusted":true,"_uuid":"17f9ffa3dc2b618973e0786bdf88c1d2d8c0b2b2"},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nfrom scipy import stats\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec8d740634cfcac471dd86f764c6314863137fff"},"cell_type":"code","source":"# 注意本机练习时最好与kaggle保持版本一致，以防水土不服。\nimport sys\nprint(sys.version)\nprint(np.__version__)\nprint(pd.__version__)\nprint(sp.__version__)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdb8b294f8dd18450fb498f9edd934c1ebb58676"},"cell_type":"markdown","source":"# **数据加载**"},{"metadata":{"trusted":true,"_uuid":"6798b8900a371649966194696f384c07e4f91909"},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")\ndf_allX = pd.concat([df_train.loc[:,'MSSubClass':'SaleCondition'],\n                   df_test.loc[:,'MSSubClass':'SaleCondition']])\ndf_allX = df_allX.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92191f47f835efe9c6a3ffe18150362c1b6b979c"},"cell_type":"markdown","source":"**Tips:**  \nread_csv() 中如何处理 NA：\n\n- `pd.read_csv(file,... na_values=None, keep_default_na=True, ...)`\n- na_values: 遇到该参数指定的字符时，即解析为 np.NaN(float型)，无论此列是数值型orobject型。\n    - 默认值：`'', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan','1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'nan'`.\n- keep_default_na\n    - True：将csv 文件中的数字or字符串与 na_values 的 default 值进行匹配，命中即解析为 np.NaN\n    - False：\n        - na_values=[...] ：与自定义的 na_values 匹配，命中即解析为 np.NaN\n        - na_values不赋值：不解析相关字符串，保留为原字符串，副作用：会把数值型的feature错误的认成 object 型 —— so，不可取"},{"metadata":{"trusted":true,"_uuid":"77aa66b9d99cecb404a927bf6bc02858b69fe2fa"},"cell_type":"code","source":"print(df_train.shape,df_test.shape,df_allX.shape) # df_allX 少了 Id 和 SalePrice 两列","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76e1ec831281247a9d2c07036b334396867383ee"},"cell_type":"markdown","source":"## 基础统计"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"f27e43bc8f6920131daaaebd67216f20384661d2"},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8162bd611d53cc0c4186f6d6ec7f3cc6e4563f5d"},"cell_type":"markdown","source":"## 特征分类"},{"metadata":{"_uuid":"ff25444d43c191803bb4854c75d891ea29e1f7d8"},"cell_type":"markdown","source":"**特征从2个维度进行分类：**\n\n| |**数值量（numeric）**|**字符量（object）**|\n|:---|:---|:---|\n|**连续型(Continuous)**|距离、面积……| 无 |\n|**离散型(discrete)<br>类型量(categorical)**|楼层、年份、<br>数值化后的字符量……|房型、材料种类……|\n\n\n* 通常需要将离散的字符量 --(转变为)--> 离散的数值量\n* pandas.read_csv() 得到的 dataframe 的column类型（df.dtypes）与特征的对应关系：\n    * int,float —— 数值量（连续和离散）\n    * object    —— 字符量（离散）\n        * 虽然 column 是 object的，但具体里面的值是 'str'、'float'(NA值是float）\n"},{"metadata":{"trusted":true,"_uuid":"90f33732f9535c51b761bb02bf573ad4881541dc"},"cell_type":"code","source":"# 数值量特征\nfeats_numeric  = df_allX.dtypes[df_allX.dtypes != \"object\"].index.values\n#feats_numeric = [attr for attr in df_allX.columns if df_allX.dtypes[attr] != 'object']\n\n# 字符量特征\nfeats_object = df_allX.dtypes[df_allX.dtypes == \"object\"].index.values\n#feats_object = [attr for attr in df_allX.columns if df_allX.dtypes[attr] == 'object']\n#feats_object = df_train.select_dtypes(include = [\"object\"]).columns\n\nprint(feats_numeric.shape,feats_object.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"033cb2daa00ad9744f4699a239a794fbbe3d04f6"},"cell_type":"markdown","source":"总共79个特征，pandas自动识别的 36个数值量，43个字符量。 —— 这是上表中第一个维度"},{"metadata":{"trusted":true,"_uuid":"2a7de4a0d2f9c65cb4024e69b0f6c2bd6d3ebc60"},"cell_type":"code","source":"# 离散的数值量，需要人工甄别\nfeats_numeric_discrete  = ['MSSubClass','OverallQual','OverallCond'] # 户型、整体质量打分、整体条件打分 —— 文档中明确定义的类型量\nfeats_numeric_discrete += ['TotRmsAbvGrd','KitchenAbvGr','BedroomAbvGr','GarageCars','Fireplaces'] # 房间数量\nfeats_numeric_discrete += ['FullBath','HalfBath','BsmtHalfBath','BsmtFullBath'] # 外国人这么爱洗澡？搞这么多浴室\nfeats_numeric_discrete += ['MoSold','YrSold'] # 年、月，这些不看成离散的应该也行\n\n# 连续型特征\nfeats_continu = feats_numeric.copy()\n# 离散型特征\nfeats_discrete = feats_object.copy()\n\nfor f in feats_numeric_discrete:\n    feats_continu = np.delete(feats_continu,np.where(feats_continu == f))\n    feats_discrete = np.append(feats_discrete,f)\n\nprint(feats_continu.shape,feats_discrete.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02fb3289f54a99142969360de29b9e241c7bd0b6"},"cell_type":"markdown","source":"经过处理，得到表中第2个维度：  22个连续型特征，57个离散型特征"},{"metadata":{"_uuid":"62d718f14a30f2dc530254336190a7939b1109fb"},"cell_type":"markdown","source":"## 基本分布(scatter)"},{"metadata":{"trusted":true,"_uuid":"b0f679d9c9356415b0e97aa0445e437e65179e3b"},"cell_type":"code","source":"def plotfeats(frame,feats,kind,cols=4):\n    \"\"\"批量绘图函数。\n    \n    Parameters\n    ----------\n    frame : pandas.DataFrame\n        待绘图的数据\n    \n    feats : list 或 numpy.array\n        待绘图的列名称\n        \n    kind : str\n        绘图格式：'hist'-直方图；'scatter'-散点图；'hs'-直方图和散点图隔行交替；'box'-箱线图，每个feat一幅图；'boxp'-Price做纵轴，feat做横轴的箱线图。\n        \n    cols : int\n        每行绘制几幅图\n    \n    Returns\n    -------\n    None\n    \"\"\"\n    rows = int(np.ceil((len(feats))/cols))\n    if rows==1 and len(feats)<cols:\n        cols = len(feats)\n    #print(\"输入%d个特征，分%d行、%d列绘图\" % (len(feats), rows, cols))\n    if kind == 'hs': #hs:hist and scatter\n        fig, axes = plt.subplots(nrows=rows*2,ncols=cols,figsize=(cols*5,rows*10))\n    else:\n        fig, axes = plt.subplots(nrows=rows,ncols=cols,figsize=(cols*5,rows*5))\n        if rows==1 and cols==1:\n            axes = np.array([axes])\n        axes = axes.reshape(rows,cols) # 当 rows=1 时，axes.shape:(cols,)，需要reshape一下\n    i=0\n    for f in feats:\n        #print(int(i/cols),i%cols)\n        if kind == 'hist':\n            #frame.hist(f,bins=100,ax=axes[int(i/cols),i%cols])\n            frame.plot.hist(y=f,bins=100,ax=axes[int(i/cols),i%cols])\n        elif kind == 'scatter':\n            frame.plot.scatter(x=f,y='SalePrice',ylim=(0,800000), ax=axes[int(i/cols),i%cols])\n        elif kind == 'hs':\n            frame.plot.hist(y=f,bins=100,ax=axes[int(i/cols)*2,i%cols])\n            frame.plot.scatter(x=f,y='SalePrice',ylim=(0,800000), ax=axes[int(i/cols)*2+1,i%cols])\n        elif kind == 'box':\n            frame.plot.box(y=f,ax=axes[int(i/cols),i%cols])\n        elif kind == 'boxp':\n            sns.boxplot(x=f,y='SalePrice', data=frame, ax=axes[int(i/cols),i%cols])\n        i += 1\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"e464c6abbf8d1b96bb5a676dbde2c8b7e9e14d96"},"cell_type":"code","source":"plotfeats(df_train,feats_continu,kind='scatter',cols=6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a19aadc4b8953ea3f0a801acaf076f0fc24f2c39"},"cell_type":"markdown","source":"分析上图：\n\n- LotFrontage、LotArea、GrLivArea、1stFlrSF、2stFlrSF、GarageArea、BsmtFinSF1、TotalBsmtSF： 这几个面积和距离和售价呈明显正相关趋势\n    - LotFrontage：房子到街道的距离，大多在50-100英尺（15-30米），距离远的是不是大多是豪宅？躲在山林深处……\n    - LotArea：占地面积（包括房屋、花园、前后院……），均值是10516平方英尺（900+平方米），向往啊……\n    - GrLivArea：地面以上整体面积\n    - 1stFlrSF、2stFlrSF： 第1、 2层建筑面积\n    - GarageArea：车库面积\n    - BsmtFinSF1、BsmtFinSF2、TotalBsmtSF：地下室面积，很多房子还有第2个地下室\n- YearBuilt、YearRemodAdd、GarageYrBlt：从图中可以看出，建造年限对售价虽正相关，但坡度较小，关联度没有上面几个因素大，早点、晚点售价差不多"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"061f623a380baaee41b2af7f6d5c469d0362f815"},"cell_type":"code","source":"plotfeats(df_train,feats_numeric_discrete,kind='scatter',cols=6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89fb3f9871b03d9d8189c1f02d8b71f6a763a909"},"cell_type":"markdown","source":"分析上图：\n\n- MSSubClass：户型，典型的离散型特征，只不过数据提供者已经帮我们数值化了，说明文件中的有详细解释\n    - 20/30/40: 单层（1-STORY）建筑，英语里楼层是 storey，美国简化为 story\n    - 45/50:1-1/2 story，应该是带阁楼吧，我猜\n    - ……\n- OverallQual、OverallCond：房屋材料、新旧度、condition等的整体打分 —— 这个因为是人为打分，有可能存在给售价高的打高分，所以需要持怀疑态度\n- TotRmsAbvGrd、GarageCars：房间数量、车库容量，可以看出和售价正相关\n- YrSold、MoSold：卖出的年、月，都是06年～10年的数据，只有卖出才有真实售价嘛，这两个特征和售价我觉得是没有关联的，可以考虑删除。\n- 其他的特征看不出啥明显趋势"},{"metadata":{"_uuid":"28e97855d4bc27a66dc6ca4e24507fe3570b08e5"},"cell_type":"markdown","source":"# **数据分析**\n\n**knowing your data is the most difficult thing in data science**\n\n下面开始分析数据，争取用一些图来挖掘数据中隐藏的信息，为数据处理提供依据。\n\n主要内容包括：\n\n- 正态性分析：使用 hist（直方图）和 scatter（散点图）展示\n- 分散度分析：使用 box（箱线图）展示\n- 方差齐次分析：\n- 方差分析： 使用 bar（柱状图）展示\n- 协方差分析：使用 heatmap（热图）展示\n\n非常感谢 matplotlib 和 seaborn，提供了简单、高效的绘图。"},{"metadata":{"_uuid":"159132b541cde9574c8e9ddab13b38219faf1664"},"cell_type":"markdown","source":"## 正态性检验\n\n常用的正态性检验方法有：\n\n* 正态概率纸法\n* 夏皮罗维尔克检验法(Shapiro-Wilktest)\n* 科尔莫戈罗夫检验法\n* 偏度-峰度检验法等 —— 下文使用"},{"metadata":{"_uuid":"4ffe514c0d11f6caaadc34be491e2c48f879964e"},"cell_type":"markdown","source":"### 偏离度分析(hist|scatter)\n\n**skewness:偏度、偏态、偏态系数**\n\n* 统计数据分布偏斜方向和程度的度量，是统计数据分布非对称程度的数字特征。\n* 表征概率分布密度曲线相对于平均值不对称程度的特征数\n* 偏离度是某一特征（即：某一列）自己的特性，不同于相关性（某两列之间）特性"},{"metadata":{"trusted":true,"_uuid":"8ccf0d5489f91bcb43a0861fc87617992fdaa521"},"cell_type":"code","source":"# SalePrice 的偏离度\ndf_train.skew()['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"fa3cf46190c7c680fd0c5d36fc68659b549f957c"},"cell_type":"code","source":"#df_train.plot(kind='hist',y='SalePrice',bins=100)\ndf_train['SalePrice'].plot(kind='hist',y='SalePrice',bins=100) # 为了和下面的图做对比才使用这行的\n#sns.distplot(df_train['SalePrice'], fit='norm');\n#plt.hist(df_train['SalePrice'],bins=100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c857fa3c884073af29e64fbb9cf5734381caa4b"},"cell_type":"markdown","source":"分析上图：\n\n直方图上看非正态，有可能是数据收集的不完整，也有可能本身就不是正态分布的，收集再多的数据也是非正态的，说不定它原本就是卡方分布呢？\n—— 至于是哪种，不知道，留给科学家吧。"},{"metadata":{"trusted":true,"_uuid":"b203b2b513b995dce09f6e44d21771c036cd8434"},"cell_type":"code","source":"stats.probplot(df_train['SalePrice'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f1da0ce64538e6778e742eb151e83ff2d9b0c91"},"cell_type":"markdown","source":"通过取 log 可以纠偏，但对于本预测任务似乎没什么帮助，难道要改为预测 log 值？最后再反计算一下？没必要吧。"},{"metadata":{"trusted":true,"_uuid":"768fbe09ab0575618dcabc6a1d84ae6fb7c110a6"},"cell_type":"code","source":"df_train['SalePrice'].apply(lambda x: np.log1p(x)).plot(kind='hist',y='SalePrice',bins=100)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"f3ecca0ecb1e4faaa107974e36a1644b04102c1a"},"cell_type":"code","source":"# 计算各列自己的偏离度\nskewed = df_allX[feats_numeric].apply(lambda x: stats.skew(x.dropna())).sort_values(ascending=False)\n#skewed = df_allX[feats_numeric].skew().sort_values(ascending=False)\nskewed[:10]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"7d964a851e73a6ff4344feeeac2dbfa145d76c16"},"cell_type":"code","source":"# 用直方图和散点图（SalePrice之间）对比展示偏离度\nplotfeats(df_train,skewed[:6].index,kind='hs',cols=6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d49a5c80737a188350592f285200410a21ba355a"},"cell_type":"markdown","source":"### 峰度分析(hist|scatter)\n\n* kurtosis,peakedness —— 峰度、峰态系数。\n* 表征概率密度分布曲线在平均值处峰值高低的特征数。\n* 峰度是和正态分布相比较而言统计量，反映了峰部的尖度。\n* 峰度大于三，峰的形状比较尖，比正态分布峰要陡峭。\n* 可用峰度来检验分布的正态性。\n* 在实际应用中，通常将峰度值做减3处理，使得正态分布的峰度0。"},{"metadata":{"trusted":true,"_uuid":"b3ef3e20baa78a6756585bd34d3fac9e7c9a8860"},"cell_type":"code","source":"df_train.kurt()['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"1e88a42fa00a4a947b60e4cf98d2fd10f14f5953"},"cell_type":"code","source":"# 计算各列自己的峰度\nkurted = df_allX[feats_numeric].kurt().sort_values(ascending=False)\nkurted[:10]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"1f15c1b1d0f0f6caeaa4ec9084e1fc33d967138c"},"cell_type":"code","source":"# 用直方图和散点图（SalePrice之间）对比展示峰度\nplotfeats(df_train,kurted[:6].index,kind='hs',cols=6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c54b4381407ce248a72f5b67b7181fdc0f48f3ae"},"cell_type":"markdown","source":"## 分散度分析(box)"},{"metadata":{"_uuid":"bd0243628e9bf61c6624256ab3c6c1123681c979"},"cell_type":"markdown","source":"### 特征本身分散度"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"ec758076a2504004411b8429c0f766011746d9f5"},"cell_type":"code","source":"plotfeats(df_train,feats_numeric,kind='box',cols=6)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"26a186d307511359afc04ed871e53bcf7e076361"},"cell_type":"code","source":"# 由于没有标准化，比例尺差异巨大，此处的绘图不具参考意义，待标准化后的数据才可以\n\nplt.figure(figsize=(16,10))\n\nplt.subplot(121)\nsns.boxplot(data=df_allX[feats_continu],orient=\"h\")\n\nplt.subplot(122)\nsns.boxplot(data=df_allX[feats_discrete],orient=\"h\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"479f003d03084a629c0bbb3ff49060c9dc885779"},"cell_type":"markdown","source":"### SalePrice 的分散度"},{"metadata":{"trusted":true,"_uuid":"08562484428ea2f0f8d4828d4c043735b8ddc253"},"cell_type":"code","source":"plotfeats(df_train, ['OverallQual'], kind='boxp', cols=6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0fea36d50d00da3474f9d7a13c23e69f086c439"},"cell_type":"code","source":"plotfeats(df_train, feats_numeric_discrete, kind='boxp', cols=6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"789571b69fa05c9086e72fca5da59c19f3e3a8c4"},"cell_type":"markdown","source":"分析上图：\n\n箱线图我基本看做散点图的加强版，最上面的散点图可以看出基本趋势，但箱线图可以一眼看出：均值、主范围之内值的趋势，可以看出异常值的多少。\n\n有个有趣的现象：几乎所有异常值都是向上异常，即超出上界（上面的那根横线），说明了所有不在随大流的售价都是高价，没有地板价、吐血价、大甩卖价的大量出现，博弈中明显卖方占优。"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"7efb9643c6a1ea5d906d463cee46ce04b91e752d"},"cell_type":"code","source":"plotfeats(df_train, feats_object, kind='boxp', cols=6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1e09b9c308dc99282e2f87df36c5d1905c53e33"},"cell_type":"markdown","source":"## 方差齐次检验\n\nHomoscedasticity，方差齐性，也就是方差相等\n\n常用方法有：\n\n* Hartley检验\n* Bartlett检验\n* 修正的Bartlett检验\n\n[如何理解线性回归中的方差齐性](https://www.sohu.com/a/197715883_655370)"},{"metadata":{"_uuid":"43963a1e15cb0fada66eb2e710361373f23200f9"},"cell_type":"markdown","source":"## 方差分析(bar)\n\nAnalysis of Variance，简称ANOVA,又称“变异数分析”或“F检验”.\n\n**学习笔记：**\n\n* 分类\n    * 按自变量（影响分析指标的因素）的个数\n        * 单因素方差分析\n        * 双因素方差分析\n        * 三因素方差分析\n    * 按因变量（分析指标、又称F统计量）的个数\n        * 一元方差分析（ANOVA - Analysis Of Variance）\n        * 协方差分析（ANCOVA - Analysis of Covariance）\n        * 多元方差分析（MANOVA - Multivariate Analysis Of Variance）\n    * 举例\n        * ANOVA：\n            * 单因：\n                * 不同施肥量是否给农作物产量带来显著影响\n                * 地区差异是否影响妇女的生育率\n                * 学历对工资收入的影响\n                * 房子的户型是否对房价有影响 —— 本例\n                    * 单因：指房型\n                    * 多水平：指房型有：1室2厅、2室4厅2卫……\n                    * 一元：值房价\n                    * 最初我们拿到的是房型-房价的对应数据，但需要我们根据根据多水平将因变量（房价）分解为多组数据，然后对这多组的房价进行方差分析，最终获得“组内”、“组间”……的特征\n                * 房子是否有砌墙是否对房价有影响 —— 本领\n            * 多因A：\n                * 不同品种、不同施肥量对农作物产量的影响\n* 指标\n    * SS（Sum of Squares，平方和，=∑(Yi-Ymean)^2 ）离散程度\n        * SSE: 组内样本与平均值的离散程度\n        * SSF：各组平均值与总平均值的离散程度\n        * SST = SSE + SSF\n    * df：自由度\n        * dfE：组内 n - m  // m组，每组n个样本\n        * dfF：组间 m - 1\n        * dfT：合计 n - 1\n    * Mean Square：均方差 = 变动/自由度\n        * 组内均方差: MSE = SSE/dfE = SSE/(n-m)\n        * 组间均方差：MSF = SSF/dfF = SSF/(m-1)\n        * 总体均方差：MST = SST/dfT = SST/(n-1)\n    * F：= MSF/MSE，组间差异显著性水平\n        * 越大，说明组间变异相对组内变异越大\n        * 到底多大才有意义，看 P 值\n    * P(Sig) = F/F表查询值：显著性水平、检验水平\n        * **P < 0.05 :（组间差异）显著**\n        * **P = 0 : 组间没有可比性**\n* 实现:\n    * Matlab：`anova1(x)` \n    * Scipy： `f,p = scipy.stats.f_oneway(sample1, sample2, ...)`"},{"metadata":{"_uuid":"aa901b6e467d56a99a124890c24f71fdd0167284"},"cell_type":"markdown","source":"**========= Test =========**"},{"metadata":{"trusted":true,"_uuid":"8b59360aebf9f7963d7a1609c81478c05c587388"},"cell_type":"code","source":"a = np.random.random(size=(1000,))\nb = np.random.random(size=1000,)\nf,p = stats.f_oneway(a, b)\nprint(f,p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7e33bbf30df24ea3dc7f15ec026cea579ee627c"},"cell_type":"code","source":"a = np.random.randn(1000,)\nb = np.random.randn(1000,)\nf,p = stats.f_oneway(a, b)\nprint(f,p)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"e34a8fefdc8f2100185a74003d375986e7aed3d7"},"cell_type":"code","source":"a = np.random.randint(1,10,size=1000,)\nb = np.random.randint(1,10,size=1000,)\nf,p = stats.f_oneway(a, b)\nprint(f,p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7b98ed113915ddff71bf49f90f612b0154c14e2"},"cell_type":"code","source":"a = np.random.randint(1,10,size=1000,)\nb = np.random.randint(5,15,size=1000,)\nf,p = stats.f_oneway(a, b)\nprint(f,p)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52385e4997c2530a2c3708195bfc4db27f877617"},"cell_type":"code","source":"a = np.random.binomial(5,0.2,size=1000)\nb = np.random.randn(1000,)\nf,p = stats.f_oneway(a, b)\nprint(f,p)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c434dc9b7db353cf59832b25694da6becff7d2a4"},"cell_type":"markdown","source":"**========= Test Over =========**"},{"metadata":{"_uuid":"86202bce3e4ce5d158cb6f4a3c8b921ad6d91bec"},"cell_type":"markdown","source":"### scipy.stats.f_oneway()"},{"metadata":{"trusted":true,"_uuid":"d7c685c8af35796b1eb5ad79b3ceba94d47fe6a9"},"cell_type":"code","source":"# stats.f_oneway() 的入参是分好组的多个array \n# 本例将2列数据(自变量X、因变量Y)的dataframe转换为分组数据\ndef anovaXY(data):\n    samples = []\n    X = data.columns[0]\n    Y = data.columns[1]\n    for level in data[X].unique():\n        if (type(level) == float): # np.NaN 的特殊处理\n            s = data[data[X].isnull()][Y].values\n        else:\n            s = data[data[X] == level][Y].values\n        samples.append(s)\n    f,p = stats.f_oneway(*samples) # 也能用指针？\n    return (f,p)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a151b1e55e4a480d64eeb527f36510cade973f61"},"cell_type":"markdown","source":"**使用方差分析某些feature和SalePrice之间关联的显著程度如何：**"},{"metadata":{"trusted":true,"_uuid":"799a9b2324dc522c541dae54aeb8b834d6c0d717"},"cell_type":"code","source":"df = pd.DataFrame(columns=('feature','f','p','logp'))\ndf['feature'] = feats_discrete\nfor fe in feats_discrete:\n    data = pd.concat([df_train[fe],df_train['SalePrice']],axis=1)\n    f,p = anovaXY(data)\n    df.loc[df[df.feature==fe].index,'f'] = f\n    df.loc[df[df.feature==fe].index,'p'] = p\n    df.loc[df[df.feature==fe].index,'logp'] = 1000 if (p==0) else np.log(1./p)\n\n# OverallQual 的 p=0，说明房价和整体评价紧密相关","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"4f67b2ece7797ce75bf29cee8358258a87707aa8"},"cell_type":"code","source":"plt.figure(figsize=(10,4))\nsns.barplot(data=df.sort_values('p'), x='feature', y='logp')\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21a4ab4c6a793d11558c655e8bfe98a512b67552"},"cell_type":"markdown","source":"### pandas.Series.corr()"},{"metadata":{"trusted":true,"_uuid":"93c0707961ae6d6017b12beb342da5a9add89ba1"},"cell_type":"code","source":"def spearman(frame, features):\n    '''\n    采用“斯皮尔曼等级相关”来计算变量与房价的相关性(可查阅百科)\n    '''\n    spr = pd.DataFrame()\n    spr['feature'] = features\n    spr['corr'] = [frame[f].corr(frame['SalePrice'], 'spearman') for f in features] # 此处用的是 Series.corr() \n    spr = spr.sort_values('corr')\n    plt.figure(figsize=(6, 0.2*len(features)))\n    sns.barplot(data=spr, y='feature', x='corr', orient='h')    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"d9b71f992614dee5a58fe113b56feafd7e9b2742"},"cell_type":"code","source":"spearman(df_train, np.delete(df_train.columns.values,-1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dac640ccd8fe3c53e5b518c060121a237938e8f8"},"cell_type":"markdown","source":"**观察和分析**\n\n* GarageCars 和 GarageArea：因为 GarageCars 与 SalePrice 相关系数更大，可以保留这个，删除 GarageArea\n* 同理，TotalBsmtSF 删除，保留 1stFlrSF"},{"metadata":{"_uuid":"dd81cd5f2d12eff5665561af89ea2c2a0ab55541"},"cell_type":"markdown","source":"## 协方差分析(-1~+1)\n\n**上面的方差分析，其实不过是下面协方差结果中的一列而已。**\n\n使用 DataFrame.corr(method='pearson', min_periods=1) 函数计算协方差，此函数返回值也是个 DataFrame，非常适合 heatmap 来绘图展示。\n\nmethod 有3种：  \n* pearson : standard correlation coefficient —— 标准相关系数\n* kendall : Kendall Tau correlation coefficient —— KT相关系统\n* spearman : Spearman rank correlation —— 斯皮尔曼相关系数\n\n返回值： 计算了任意两个feature之间的关联程度\n\n* 当 cov(X, Y)>0时，表明 X与Y 正相关；\n* 当 cov(X, Y)<0时，表明X与Y负相关；\n* 当 cov(X, Y)=0时，表明X与Y不相关。"},{"metadata":{"trusted":true,"_uuid":"50f7d1679fb8fc976ee88f11439869db2b732898"},"cell_type":"code","source":"corr_pearson = df_train.corr(method='pearson')\ncorr_spearman = df_train.corr(method='spearman')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7340d4968484bef46bb9563c9b5e5879a82c23d"},"cell_type":"code","source":"# corrmat 是 38*38的矩阵：所以只是 numeric 的 feature 才会参与计算\ncorr_pearson.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"598c94fd11b3158d4dc39a7f5e10f59cac445dc4"},"cell_type":"code","source":"corr_spearman.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69fdf12afbb701e2b624e100a5ff0e1a174360cd"},"cell_type":"markdown","source":"### 协方差热图(heatmap)\n\n* heatmap: \n    * matplotlib 画heatmap比较麻烦，没有简单接口\n    * sns.heatmap() 是个不错的选择\n* 颜色越浅，协方差越大，两个变量关联性越大；\n* 颜色越深，逆相关;"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"252f1c3a9e73dfd754fa756becaadcc1648e4746"},"cell_type":"code","source":"# 如果不设置figsize，会出现部分数据不显示的情况\n# 就是说要手工计算充分的空间给sns.heatmap() —— 这都是啥bug啊\nplt.figure(figsize=(20, 20))\nplt.subplot(211)\nsns.heatmap(corr_pearson, vmax=.8, square=True);\nplt.subplot(212)\nsns.heatmap(corr_spearman, vmax=.8, square=True);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f53fbbd05338605c11b9d91eb37f4b724728e23"},"cell_type":"markdown","source":"分析上图：  \n\n* 中间2个比较大的白色块：\n    * TotalBsmtSF--1stFlrSF：地下室面积和1楼面积正相关 —— 好像很废话哦，呵呵\n    * GarageCars--GarageArea：车库面积和停车容量正相关 —— 也很废话\n    * 上面4个在建模时可以去除2个，另外2个是无效feature\n    * 这叫：**多重共线性**\n"},{"metadata":{"_uuid":"058e3b080e93aa4c83d7db0bb95ba0831286cd91"},"cell_type":"markdown","source":"### 协方最大关联图(pairplot)"},{"metadata":{"trusted":true,"_uuid":"d371167eeb8937ab05d17f56c9a461e52c84691a"},"cell_type":"code","source":"feats_d = corr_pearson.nlargest(8,'SalePrice').index\nfeats_d","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67f8bc962099d9987e6f9c3270d49435c3b9e243"},"cell_type":"code","source":"sns.pairplot(df_train[feats_d],size=2.5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a99602afd698c42e5334d04244cb34b51a14a7e7"},"cell_type":"markdown","source":"图画的越来越花哨，我已无力分析，还是开始数据处理吧。"},{"metadata":{"_uuid":"643f98b2d7f31c4169140d35fbbda45c03d50f11"},"cell_type":"markdown","source":"# **数据处理**"},{"metadata":{"_uuid":"2308c477f739b1c3aec5c54e0989b2752934bb5c"},"cell_type":"markdown","source":"## 无效数据处理"},{"metadata":{"_uuid":"fe0d2889660ad164b28b45114baa77e377db57dd"},"cell_type":"markdown","source":"### 无效特征处理\n\n根据逻辑和理论，删除一些违反原则的特征：\n\n* YrSold、MoSold：我觉得和预测房价不相关\n\n还有一些根据个人观点随喜取舍：\n\n* OverallQual: 太人为化了，完全不理性，甚至说可能是售价的因变量（售价是自变量）\n* YearBuild: 房地产的口头禅：地段、地段、地段，房龄几乎无关"},{"metadata":{"trusted":true,"_uuid":"f52bb2d404786b6f888d00d4b4352e101c1b1482"},"cell_type":"code","source":"feats_del = ['YrSold','MoSold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"158acbb7375bc339262c8a38c688e0f16394676a"},"cell_type":"code","source":"df_allX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b74285f41eb49a1264a616f123a57131f24b70c"},"cell_type":"code","source":"df_allX.drop(feats_del, axis=1, inplace=True)  # 快意泯恩仇 ：）","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17ce63adb643d0345e7bdf7f80e8493b07ac5f43"},"cell_type":"code","source":"# 同步修正一下4个特征名称集\nfor f in feats_del:\n    feats_numeric  = np.delete(feats_numeric,  np.where(feats_numeric  == f))\n    feats_object   = np.delete(feats_object,   np.where(feats_object   == f))\n    feats_continu  = np.delete(feats_continu,  np.where(feats_continu  == f))\n    feats_discrete = np.delete(feats_discrete, np.where(feats_discrete == f))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f32a82e03ff47fd1b416034878f0ff143ead92d"},"cell_type":"code","source":"df_allX.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e6d0bea24c0aed1f02074863fb9b8f0dbd9cc59"},"cell_type":"markdown","source":"### 离群点处理\n\n离群点不同于分散度，离群点指偏离趋势的点，比如某特征和房价是正相关的，在趋势轴上即时很分散，但是不可以删除的，而不在趋势轴上的“离群点”是需要删除的。"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"92542101216201ea66f41bda0e8b8b00aa999df8"},"cell_type":"code","source":"# 经过前面偏离度分析，可以观察得出下面几个feature存在离群点\nfeats_away = ['LotFrontage','LotArea','BsmtFinSF1','BsmtFinSF2','1stFlrSF','GrLivArea','TotalBsmtSF']\nplotfeats(df_train,feats_away,kind='scatter')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14319b4911d47bfe9de1cc4d6405f712ed07f678"},"cell_type":"markdown","source":"**手工删除离群点** —— 这是个体力活啊！  \n并且离群点是以 SalePrice 为参照的，而 df_allX 中没有 SalePrice，只有 df_train 中有。  \ndf_allX 前面已经重新排序了Id，但前半部分的的 Id 应该是和 df_train 相同的，可以使用。"},{"metadata":{"trusted":true,"_uuid":"4f4e2148f5f29a721058996932253650a1a4420f"},"cell_type":"code","source":"ids = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b245ea8e7e61335a0a31e36354f0ca7ca20c1a75"},"cell_type":"code","source":"df_train.sort_values(by = 'GrLivArea', ascending = False)[:2][['Id','GrLivArea','SalePrice']]\n# 1299 是 df_train 中的 Id 列， 1298 对应 df_allX 中的 index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06297d6fd518029482dfb1c22cd78a865e7506f6"},"cell_type":"code","source":"ids.append(1299)\nids.append(524)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68e859d1cc7dd1594fac8a372489fe6a5efd561f"},"cell_type":"code","source":"df_train.sort_values(by = 'TotalBsmtSF', ascending = False)[:2][['Id','TotalBsmtSF','SalePrice']]\n# 1299 又出现了，看来提供这个数据的同学是不是恶作剧啊？","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6905f4d58c6c625d091b9293b1a167df230a5b65"},"cell_type":"code","source":"df_train.sort_values(by = '1stFlrSF', ascending = False)[:2][['Id','1stFlrSF','SalePrice']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88ab05b7bc5c33e46c4f81c31fca8c705357178f"},"cell_type":"code","source":"df_train.sort_values(by = 'BsmtFinSF1', ascending = False)[:2][['Id','BsmtFinSF1','SalePrice']]\n# 全部指向 1299，此行数据必须除之","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"adc58dd4dbc637318336ffb6cc255b1081bf7de5"},"cell_type":"code","source":"df_train.sort_values(by = 'LotArea', ascending = False)[:3][['Id','LotArea','SalePrice']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"821fd1f2e59341c7fb954c2d0ccce0b9acaeea2c"},"cell_type":"code","source":"ids.append(314)\nids.append(335)\nids.append(250)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0391cc2a03d5a61e63c369fce97946758af3158"},"cell_type":"code","source":"df_train.sort_values(by = 'LotFrontage', ascending = False)[:3][['Id','LotFrontage','SalePrice']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"157dcae2d82f0c0b45406143a272072bc3247a6b"},"cell_type":"code","source":"ids.append(1299)\nids.append(935)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69553f791b4f622ac4600c183fb9b4fa366cb1ff"},"cell_type":"code","source":"np.unique(ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49bd3fdc1456df1bbf592f18064c8fe487f38b54"},"cell_type":"code","source":"print(df_train.shape,df_test.shape,df_allX.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74877f195cd53abf9156157d48e09c01d786a049"},"cell_type":"code","source":"for id in np.unique(ids):\n    df_train = df_train.drop(df_train[df_train.Id==id].index)\n    df_allX = df_allX.drop(df_allX[df_allX.index==(id-1)].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f5d4979e0c35b6a85e505a76da2e89b95c9f031"},"cell_type":"code","source":"print(df_train.shape,df_test.shape,df_allX.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"074a965d6ec825fb300dbd47cf39177fc4665651"},"cell_type":"markdown","source":"## 缺失值处理\n\n* **缺失值不等于NA**\n    - 字符型特征中的 'NA' 分2类：\n        1. NA 代表某个含义，是离散量/类型量的一个有意义的值，需要请参考说明文件\n        2. NA 代表缺失\n    - 数值量特征中的 np.NaN 一般都是缺失值了\n* 缺失比例太大，或特征对任务有没有用 —— 直接删除\n* 缺失比例不大，需要补全\n    - 数值型特征：用min,max,mean,median或mod补全\n    - 字符型特别：——暂不处理"},{"metadata":{"_uuid":"411aac81f9cd8499d04e27247cb43c10f279dabb"},"cell_type":"markdown","source":"### NaN和NA的处理函数"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"b9fa53d99a384f49d7ea1ff426cd54d46a774b58"},"cell_type":"code","source":"# Python 中 NaN 的类型：\nprint(type(None),type(np.NaN))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19856111355fc59ac7cb699bc35dfc9a6da2782e"},"cell_type":"markdown","source":"先来实现几个函数，以便后面处理 np.NaN 和 'NA'"},{"metadata":{"trusted":true,"_uuid":"2e8384c90a3f4b73a7996152f79775ebd31c6a4a"},"cell_type":"code","source":"def NaNRatio(frame,feats):    \n    \"\"\"\n    查找并统计 numpy.NaN 的值, feats 可以是数值型 or 字符型特征\n    \"\"\"\n    na_count = frame[feats].isnull().sum().sort_values(ascending=False)\n    na_rate = na_count / len(frame)\n    na_data = pd.concat([na_count,na_rate],axis=1,keys=['count','ratio'])\n    return na_data[na_data['count']>0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69e017b01d1839b7cc694da917df789883e5ee48"},"cell_type":"code","source":"def NARatio(frame,feats):\n    \"\"\"\n    查找并统计字符串 NA 的值\n    \"\"\"\n    nadict={}\n    for c in feats:        \n        # 方法1：\n        # frame[f][frame[f]=='NA'] —— 问题是这种方法只能比较 object 列，numeric列会报错        \n        # 方法2：\n        for r in frame.index:\n            if 'NA'==frame.loc[r,c]:\n                if 0==nadict.get(c,0):\n                    nadict[c]=[]\n                nadict[c].append(r)\n    return nadict","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d671834d1ae59588f49a5242bbcfc9c98b9082ec"},"cell_type":"code","source":"def transNaNtoNumber(frame, column, method, val=0):\n    \"\"\"\n    将 numpy.NaN 转为指定的数字\n    \"\"\"\n    if method == 'mean':\n        frame[column] = frame[column].fillna(round(frame[column].mean()))\n    elif method == 'min':\n        frame[column] = frame[column].fillna(round(frame[column].min()))\n    elif method == 'max':\n        frame[column] = frame[column].fillna(round(frame[column].max()))\n    elif method == 'special':\n        frame[column] = frame[column].fillna(val).round()\n    else:\n        return\n    return frame","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a11940165a6108e57d8fd3ec20da840913b31d9e"},"cell_type":"code","source":"def transNaNtoNA(frame, feature):\n    \"\"\"\n    将 numpy.NaN 转为字符串 NA\n    \"\"\"\n    # frame[feature][df[feature].isnull()] = 'NA' # 这么写有warnning\n    frame.loc[frame[feature].isnull(),feature] = 'NA'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc4f822dc71da40c2ac9ff4a6db25150ec1bad36"},"cell_type":"code","source":"def transNAtoNumber(frame,feat,val=0):\n    \"\"\"\n    将字符串 NA 替换为指定数值（默认0）\n    \"\"\"\n    for r in frame[frame[feat]=='NA'].index:\n        frame.loc[r,feat] = val\n    return frame","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79018ddff6a34ba6ae4408c4b68ae76917fe5a62"},"cell_type":"markdown","source":"### 数值量:min,max,mean"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"9914410e9eff4fb05f849164cb2282f9a56cc99c"},"cell_type":"code","source":"# 查看数值型特征的 NA 值的数量和比例\npd.concat([NaNRatio(df_train,feats_numeric),NaNRatio(df_test,feats_numeric)],axis=1,sort=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8793be1f50beffc87d2114c31f8a5f96d21a48a5"},"cell_type":"markdown","source":"下面又是个体力活，可以像我这样逐个特征分析，然后使用不同的方式填补缺失值，也可以一刀切取个mean，不见得不好，我只是强迫症，想看看有啥影响。"},{"metadata":{"trusted":true,"_uuid":"542c9beed86b3ce725a358e1c0f1486b2c62a9a9"},"cell_type":"code","source":"#LotFrontage：到街道的距离：取平均值\ndf_allX = transNaNtoNumber(df_allX,'LotFrontage','mean') \n\n#GarageYrBlt：车库的建造年份：因为没有车库才没有年份，所以不能取平均值，暂取最小值\ndf_allX = transNaNtoNumber(df_allX,'GarageYrBlt','min')\n\n#MasVnrArea：砌墙面的面积，因为没有砌墙才导致为0，取最小值替代\ndf_allX = transNaNtoNumber(df_allX,'MasVnrArea','special',0)\n\n#其他：比例较小，统一用mean替代\ndf_allX = df_allX.fillna(df_allX.mean())","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"a31697ebc9c8a1adf85f15ae971f23060bece31d"},"cell_type":"code","source":"# df_allX 中的数值型特征中已没有 np.NaN\npd.concat([NaNRatio(df_allX,feats_numeric)],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2962b06b607b5b3baf624cd038143fac3ff85289"},"cell_type":"markdown","source":"### 字符量 -- 仅做类型转换\n\n将其从float类型的NaN转变为char的\"NA\"，因为这些'NA'都是有意义的，不是缺失，而是某个特定含义值。"},{"metadata":{"trusted":true,"_uuid":"2811900d52ab9c80cd6bee5ecff8359c45853023"},"cell_type":"code","source":"pd.concat([NaNRatio(df_train,feats_object),NaNRatio(df_test,feats_object)],axis=1,sort=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15d6222bd21f7e5d90e2103240bfbf01cda0bc9b"},"cell_type":"code","source":"for c in feats_object:\n    transNaNtoNA(df_allX,c)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"49fa3626d2e1e4c123be734ec6593584a8c62cfa"},"cell_type":"code","source":"# df_allX 中的字符型特征中已没有 np.NaN\nNaNRatio(df_allX,feats_object)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5eaa01c129fca31a9efca129ee35bbbc6a325843"},"cell_type":"markdown","source":"## 标准化(Normalization)处理\n\n标准化 (normalization)  \n将实际的值区间转换为标准的值区间(通常为 -1 到 +1 或 0 到 1)的过程。例如,假设某个特征的自然区间是 800 到 6000。通过减法和除\n法运算,您可以将这些值标准化为位于 -1 到 +1 区间内。  \n—— Google 机器学习术语-中文版(Machine Learning Glossary_Google Developers)\n\n常用标准化方法：\n\n1. z-score\n$$x_i = \\frac{x_i - \\mathbb{E} x_i}{\\text{std}(x_i)}。$$\n2. Min-Max Scaling\n$$x_i = \\frac{x_i - min}{max - min}$$\n3. Decimal scaling小数定标标准化"},{"metadata":{"trusted":true,"_uuid":"c180b2f1a91474124a615e56933261752d124288"},"cell_type":"code","source":"df_allX[feats_numeric] = df_allX[feats_numeric].apply(lambda x:(x-x.mean())/(x.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77cc89b4e143b4620b0b6c47d7b727b4c9ea704c"},"cell_type":"markdown","source":"标准化后的分散度"},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"4be908332dd5a13b0c914af684386bf6848ffae6"},"cell_type":"code","source":"plt.figure(figsize=(16,10))\n\nplt.subplot(121)\nsns.boxplot(data=df_allX[feats_continu],orient=\"h\")\n\nplt.subplot(122)\nsns.boxplot(data=df_allX[feats_discrete],orient=\"h\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73d71d560ebbc5c5b0d8d23d148924e6553d5ae5"},"cell_type":"markdown","source":"## 离散量编码\n\n* **离散/类型量都可以进行重新编码（encoding）：**\n    * **字符型-离散量的编码：即字符量的数值化。**\n    * **数值型-离散量的编码：本例需求不强烈**，因为数据提供者已经做了很好的排序，如OverallQual，10～1分别代表从VeryExcellent～VeryPoor\n\n离散量编码的常用方法有：\n\n1. One-Hot Encoding: 将离散量变为多个特征 —— 因为只考虑特征本身，so，可以用 df_allX 来计算\n2. 分组-均值-排序法：对特征的不同取值分组，并计算每组y的均值，然后排序，最后分别将分组数值化为1,2,3... \n\n具体实施2选1，下文我注释掉了1，选用的2，并不是1不好，而是1只需要1行语句，2我写了还多行，不用觉得可惜了我的工作量，哈哈。\n\n其实从结果对比来看，差别不明显，1占优的比例还高些，我是为了学习，拿2练练手。"},{"metadata":{"_uuid":"b39da1196ea54c24afd638b037edbd7f296d1fde"},"cell_type":"markdown","source":"### One-Hot Encoding\n\n又称：独热编码（这中文名也太low了），本质是按位编码，即使用1bit表示一个状态。\n\n目的：\n\n* 解决了分类器不好处理属性数据的问题，属性通常为序列值，而不是连续值，序列值没有连续值的大小、优劣……之分\n* 将序列型的属性对应到欧式空间\n* 在一定程度上也起到了扩充特征的作用\n\n实现：\n\n1. \n```\nfrom sklearn import preprocessing\npreprocessing.OneHotEncoder(sparse=False).fit_transform()\n```\n2. \n```\npandas.get_dummies() //dummy variables(虚拟变量、哑变量、离散特征编码)\n```"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"88cdee38f4058b387e66f5ea34d66ff0d119319d"},"cell_type":"code","source":"df_allX.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"881f4f6df0baa3d1b436ff2a2206becab3075181"},"cell_type":"code","source":"#df_allX = pd.get_dummies(df_allX[feats_object], dummy_na=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"660793052150333caf19acddb90daaa03eedeef1"},"cell_type":"code","source":"df_allX.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70854bfe46ed569a2ce880a2ddc6867afa5e11aa"},"cell_type":"markdown","source":"### 分组-均值-排序数值化\n\n* **以SalePrice为参考，将某个特征按每个离散值计算SalePrice对应的均值，排序后给出1、 2、 3……等数值**\n* **因为需要SalePrice参与计算，所以需要 df_train 计算，然后应用到 df_allX 上**\n* **时刻记住： np.NaN 和 'NA' 字符串的区别**\n* **问题：如果某个特征在 df_train 中没有NA，仅在 df_test 中有NA，则无法处理**"},{"metadata":{"_uuid":"1b38a2c2e851381fa93920e9794216a77643c7f6"},"cell_type":"markdown","source":"#### 以SalePrice为参考的数据"},{"metadata":{"trusted":true,"_uuid":"ff3ce1a45de96eb9be4254e1ec41ff3a4d1c7b73"},"cell_type":"code","source":"def encode(frame, feature, targetfeature='SalePrice'):\n    ordering = pd.DataFrame()\n    # 找出指定特征的水平值，并做临时df的索引\n    ordering['val'] = frame[feature].unique()\n    ordering.index = ordering.val\n    # 按各水平分组，并求每组房价的均值\n    ordering['price_mean'] = frame[[feature, targetfeature]].groupby(feature).mean()[targetfeature]\n    # 排序并为order列赋值1、2、3、……\n    ordering = ordering.sort_values('price_mean')\n    ordering['order'] = range(1, ordering.shape[0]+1)\n    ordering = ordering['order'].to_dict()\n    return ordering","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5804ecab8eb9b3c3f872ffbdd85f1ff461595c94"},"cell_type":"markdown","source":"**需要转为字符'NA'才能做 encode()，否则在计算 groupby() 时会漏掉 NaN 型 **\n\n测试："},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"ebc12efee0b394d340d334403a35f0a670b87ac5"},"cell_type":"code","source":"encode(df_train,'BsmtCond') # numpy.NaN 的处理不是所希望的","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08ef531698b97747a5fa8c61bba209371b3a3f1e"},"cell_type":"code","source":"dfc = df_train.copy()\ntransNaNtoNA(dfc,'BsmtCond') # 把 NaN 转为 'NA'\nencode(dfc,'BsmtCond') # 字符串 NA 能够正确处理","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea5329bf151652dff66dfd46dc4716977b03cec4"},"cell_type":"markdown","source":"测试完毕"},{"metadata":{"trusted":true,"_uuid":"34719bc03031112bb59d495dcdb5219627cfd694"},"cell_type":"code","source":"# 转前留证\ndf_allX.loc[20:30,'Alley'] ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"71d261b67ac8611498d0bbb2f559c8b5f78fe76e"},"cell_type":"code","source":"dfc = df_train.copy()\n\nfor fb in feats_object:\n    print(\"\\r\\n-----\\r\\n\",fb,end=':')\n    transNaNtoNA(dfc,fb)\n    for attr_v, score in encode(dfc,fb).items():\n        print(attr_v,score,end='\\t')\n        df_allX.loc[df_allX[fb] == attr_v, fb] = score        ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"fc2a5f4eae28b082fd4e2202914742f791647bda"},"cell_type":"code","source":"# 转后验证\ndf_allX.loc[20:30,'Alley'] ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47e961b85ede7105c52563ed068e75b2664b0295"},"cell_type":"markdown","source":"#### 没有房价可做基准的数据处理\n\n个别特征 train 中没有 NA，test 中有 NA，需要单独处理"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"80736bb08243b2bf2978b8f6e338da18cfa626e9"},"cell_type":"code","source":"# 检查一遍是否还有 numpy.NaN\nNaNRatio(df_allX,df_allX.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c5526d0c46cd29caf497420a7615fbae98c7d63","scrolled":true},"cell_type":"code","source":"# 检查一遍是否还有 'NA'\nstillNA = NARatio(df_allX,df_allX.columns.values)\nstillNA","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"c437697b8cb36960b70170870ce568c8ce6c5b0c"},"cell_type":"code","source":"df_allX.loc[1914:1916,'MSZoning'] #果然有","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21dac8ec0a9702fdc73d4617eea6b77674bb8869"},"cell_type":"markdown","source":"好吧，该处理他们了：\n\nMSZoning: 区域划分：农业、工业、商业、住宅高、中、低、花园密度区  \n查看了一下对应的 OverallQual（与房价方差分析最紧密关联的特征）为：  \n2、 1、 5、 1 —— 各种都有  \n\n**所以这里都取平均值吧**"},{"metadata":{"trusted":true,"_uuid":"cf23fbf04e64e72931975489c351eb338205a711"},"cell_type":"code","source":"df_allX[['MSZoning','OverallQual']][df_allX['MSZoning']=='NA']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ad79ac9215b723e3dd9d4e111e96e63e7ad5074","scrolled":false},"cell_type":"code","source":"dftemp = df_allX.copy()\nfor sn in stillNA.keys():\n    dftemp  = transNAtoNumber(dftemp,sn)\n    df_allX = transNAtoNumber(df_allX,sn,dftemp[sn].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"241f1d2b8aee6a425017603bb535a43f463c02b7"},"cell_type":"code","source":"df_allX.loc[1914:1916,'MSZoning']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"9306d186dc9c89be0f0a7154638a4aa61308e104"},"cell_type":"code","source":"NARatio(df_allX,df_allX.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0240709cbe29953e234ae118dc73db77fc6b314"},"cell_type":"markdown","source":"参考各路英豪的经验，能做的数据分析和预处理都差不多了，该上机器学习了。   \n\n上面这些数据处理，让我感觉大学学的概率统计都还给老师了，我那时候概率和统计还是一门课，听说现在已经分开2门课了，概率是从理论指导实践的科学，统计是实践发现规律的科学，毕竟还是有区别的，分开的好。  \n\n复习了课本，再加以《Python 科学计算》之类的书，终于拿下上面的数据处理之后，我发现：下面的机器学习才更重要。\n\n当然2项工作都重要，这里的重要是指：辛苦搞数据处理RMSE只能优化0.02上下浮动，优化ML模型，能在0.05~0.1之间，两者对最终结果的影响度上ML似乎更重要一点。\n\n数据处理是科学，机器学习是玄学 -- 我似乎有点信了。"},{"metadata":{"_uuid":"b4d88f52dee16380a41d028d34594defc44a59fc"},"cell_type":"markdown","source":"# **机器学习**"},{"metadata":{"_uuid":"84fcd4b2ae3185db977d8d37512381caacbdd497"},"cell_type":"markdown","source":"**RMSE**: Root-Mean-Squared-Error，根平均平方差，[wikipedia](https://en.wikipedia.org/wiki/Root-mean-square_deviation) ，这是本场比赛要求的评测标准。\n"},{"metadata":{"trusted":true,"_uuid":"11988a20bc5fd403cce92031d65c976f1e253f73"},"cell_type":"code","source":"def get_rmse_log(net, X_train, y_train):\n    num_train = X_train.shape[0]\n    clipped_preds = nd.clip(net(X_train),1,float('inf'))\n    return np.sqrt( 2 * nd.sum(square_loss(nd.log(clipped_preds), nd.log(y_train))).asscalar()/num_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fba01b315466a070611dfcdaed4391bd5f88c0db"},"cell_type":"markdown","source":"## **模型**"},{"metadata":{"_uuid":"00f8c40e4692e9c915b42f1e9adf42de12dbaf04"},"cell_type":"markdown","source":"### MXNet"},{"metadata":{"trusted":true,"_uuid":"47bd40a18a12ff292306cd4a398f904900fed977"},"cell_type":"code","source":"import mxnet\nprint(mxnet.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c5cc557f6a3b65350a49858351fee4f282e9a6b"},"cell_type":"code","source":"from mxnet import nd, autograd, gluon","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1168d16f5fc20a2ff2ae5aa947ab2e122cb49aab"},"cell_type":"markdown","source":"把数据用 mxnet 提供的 array 初始化一下，以便用 GPU 等大招。"},{"metadata":{"trusted":true,"_uuid":"3b7d121ea7434d6cd143e9e062ed9e42ea7151d4"},"cell_type":"code","source":"num_train = df_train.shape[0]\nX_train = nd.array(df_allX[:num_train])\nX_test  = nd.array(df_allX[num_train:].values)\ny_train = nd.array(df_train.SalePrice.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15c0320ab3a9109766d0eb73cce7e182c26b2be4"},"cell_type":"code","source":"square_loss = gluon.loss.L2Loss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ad54cf63dddc092c77f486670f998065f51589a"},"cell_type":"code","source":"def get_net(units=128, dropout=0.1):\n    net = gluon.nn.Sequential()\n    with net.name_scope():  \n        if units != 0:\n            net.add(gluon.nn.Dense(units, activation='relu'))\n        if dropout != 0:\n            net.add(gluon.nn.Dropout(dropout))\n        net.add(gluon.nn.Dense(1))\n    net.initialize()\n    return net","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db3acb8c2891cc5e974f08d64d251efd85f782fe"},"cell_type":"code","source":"def train(net, X_train, y_train, X_test, y_test, epochs, learning_rate, weight_decay):\n    train_loss = []\n    if X_test is not None:\n        test_loss = []\n    batch_size = 100\n    dataset_train = gluon.data.ArrayDataset(X_train, y_train)\n    data_iter_train = gluon.data.DataLoader(dataset_train, batch_size,shuffle=True)\n    trainer = gluon.Trainer(net.collect_params(), 'adam',\n                            {'learning_rate': learning_rate,\n                             'wd': weight_decay})\n    net.collect_params().initialize(force_reinit=True)\n    for epoch in range(epochs):\n        for data, label in data_iter_train:\n            with autograd.record():\n                output = net(data)\n                loss = square_loss(output, label) \n            loss.backward()\n            trainer.step(batch_size)\n\n        # 训练用 L2Loss，画图和返回用 RMSE Loss\n        train_loss.append(get_rmse_log(net, X_train, y_train))\n        if X_test is not None:\n            test_loss.append(get_rmse_log(net, X_test, y_test))\n\n        \n    # 返回的是 epochs 个过程 loss\n    if X_test is not None:\n        return train_loss, test_loss\n    else:\n        return train_loss","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"393f0cda8b9bbcc8d7ae0cf6b714a33d7e5e4a43"},"cell_type":"markdown","source":"### pytorch"},{"metadata":{"trusted":true,"_uuid":"085ab72491899c612e4bce2a7f35ae77a9fde9ef"},"cell_type":"code","source":"import torch\nprint(torch.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"970556a709e89bde336997a1f521e51b4d3f91da"},"cell_type":"code","source":"from torch import nn, autograd as ag, optim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e644ccd706addf480884ceefb843215d06289d7f"},"cell_type":"code","source":"# 待补充","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f611df7fd18666de6445aefee09738a037775a3b"},"cell_type":"markdown","source":"### TensorFlow"},{"metadata":{"trusted":true,"_uuid":"1725f5b1434f54168251e8d130f26cbaa3ebaab3"},"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33f3e27694ab18960acfa5f090b18d548e75ed95"},"cell_type":"code","source":"# 待补充","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a76c768fbf1f997b046157309f12394ebc25b5e2"},"cell_type":"markdown","source":"### PaddlePaddle"},{"metadata":{"trusted":true,"_uuid":"0b79b943e6acc998b6aaa8947d0131f342ddbdd4"},"cell_type":"code","source":"# 待补充","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"676be2478452a4f6a5e5eec8924453ef1eb82a1c"},"cell_type":"markdown","source":"## **训练**"},{"metadata":{"_uuid":"fb33a87f909754c357979a184fda7faf6dc114a7"},"cell_type":"markdown","source":"下面这个函数改编自 MXNet Demo 中，实现了 K-折交叉验证，是这么个意思：\n\n- Andrew Ng 说数据要分3份：训练、验证、测试。验证数据用来对训练数据得到的模型进行验证，而不是直接上测试集，毕竟挑战赛中我们手上也没有测试集的SalePrice，无法做评判\n- K-折交叉验证：把 训练集 分k份，取其中1份作为验证集，另外 (k-1) 份做训练集\n\n其中我注释掉了 savejpg 一行，是为了减少kaggle的负担，本机执行是我觉得挺有用的。"},{"metadata":{"trusted":true,"_uuid":"120f6eb3f0c0f0c7b926771b401c9868528115a0"},"cell_type":"code","source":"def k_fold_cross_valid(k, epochs, X_train, y_train, learning_rate, weight_decay, units=128, dropout=0.1, savejpg=False):\n    assert k > 1\n    fold_size = X_train.shape[0] // k\n    train_loss_sum = 0.0\n    test_loss_sum = 0.0\n    train_loss_std_sum = 0.0\n    test_loss_std_sum = 0.0\n\n    cols = k\n    rows = int(np.ceil(k/cols))\n    fig, axes = plt.subplots(nrows=rows,ncols=cols,figsize=(cols*5,rows*5))\n        \n    for test_i in range(k):\n        X_val_test = X_train[test_i * fold_size: (test_i + 1) * fold_size, :]\n        y_val_test = y_train[test_i * fold_size: (test_i + 1) * fold_size]\n\n        val_train_defined = False\n        for i in range(k):\n            if i != test_i:\n                X_cur_fold = X_train[i * fold_size: (i + 1) * fold_size, :]\n                y_cur_fold = y_train[i * fold_size: (i + 1) * fold_size]\n                if not val_train_defined:\n                    X_val_train = X_cur_fold\n                    y_val_train = y_cur_fold\n                    val_train_defined = True\n                else:\n                    X_val_train = nd.concat(X_val_train, X_cur_fold, dim=0)\n                    y_val_train = nd.concat(y_val_train, y_cur_fold, dim=0)\n        \n        net = get_net(units=units, dropout=dropout)\n        train_loss, test_loss = train(\n            net, X_val_train, y_val_train, X_val_test, y_val_test, \n            epochs, learning_rate, weight_decay)        \n        print(\"%d-fold \\tTrain loss:%f \\tTest loss: %f\" % (test_i+1, train_loss[-1], test_loss[-1]))\n        \n        axes[test_i%cols].plot(train_loss, label='train')\n        axes[test_i%cols].plot(test_loss, label='test')\n        \n        train_loss_sum += np.mean(train_loss[-10:])\n        test_loss_sum += np.mean(test_loss[-10:])\n        \n        train_loss_std_sum += np.std(train_loss[10:])\n        test_loss_std_sum  += np.std(test_loss[10:])\n    \n    print(\"%d-fold Avg: train loss: %f, Avg test loss: %f, Avg train lost std: %f, Avg test lost std: %f\" % \n          (k, train_loss_sum/k, test_loss_sum/k, train_loss_std_sum/k, test_loss_std_sum/k))\n\n    if savejpg:\n        #plt.savefig(\"~/house-prices/%d-%d-%.3f-%d-%d-%.3f.jpg\" %(k,epochs,learning_rate,weight_decay,units,dropout))\n        plt.close()\n    else:\n        plt.show()\n        \n    return train_loss_sum / k, test_loss_sum / k, train_loss_std_sum / k, test_loss_std_sum /k","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"22d1eba6777996e990a24982ff2b938aa2ffdde8"},"cell_type":"code","source":"# 下面先根据经验赋值一组数据，验证上面模型和算法的可行性\n\nk=5\nepochs=50\nlearning_rate=5\nweight_decay=0\nunits=0\ndropout=0\n\ntrain_avg_loss, test_avg_loss, train_avg_loss_std, test_avg_loss_std = k_fold_cross_valid(\n    k, epochs, X_train, y_train, learning_rate, weight_decay, units, dropout, savejpg=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4dff52b6929453331d8f550b9e85ed79ad86cb2"},"cell_type":"markdown","source":"## **炼丹**\n\n下面开始炼丹：目前的6个参数，可以有无数种组合，如何找到最佳的那个，开启格格巫模式。"},{"metadata":{"trusted":true,"_uuid":"2f671d62fdc3e3e9aedf3bc19a94c9464c65df76"},"cell_type":"code","source":"# 排列组合\ndef expand(mulcoldf, sigcoldf):\n    r = pd.DataFrame(columns=np.append(mulcoldf.columns.values, sigcoldf.columns.values))\n    for x in sigcoldf.values:\n        s = mulcoldf.copy()\n        s[sigcoldf.columns[0]] = x[0]\n        r = pd.concat([r,s])\n    return r\n\n# k,epochs,learning_rate,weight_decay,units,dropout\ndef get_params(k=[5],epochs=[50],learning_rate=[5,0.5],weight_decay=[0],units=[0,128],dropout=[0,0.01]):\n    p = pd.DataFrame()\n    p = expand(pd.DataFrame({'k':k}), pd.DataFrame({'epochs':epochs}))\n    p = expand(p, pd.DataFrame({'learning_rate':learning_rate}))\n    p = expand(p, pd.DataFrame({'weight_decay':weight_decay}))\n    p = expand(p, pd.DataFrame({'units':units}))\n    p = expand(p, pd.DataFrame({'dropout':dropout}))\n    return p.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21acdf5ba7632676b8914971bc1b05c427a9adfa"},"cell_type":"code","source":"params = get_params(learning_rate=[0.1,0.5,1,2,3,4,5], weight_decay=[1,10,100,130,150,500], units=[64,128,256])\nparams[-5:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e1b0c7e3a5f07e5dce641d9a3291bf75b563b06"},"cell_type":"markdown","source":"炼丹开始  \n嘀嗒嘀嗒嘀嗒嘀嗒  \n时针它不停在转动  \n嘀嗒嘀嗒嘀嗒嘀嗒  \n小雨她拍打着水花  "},{"metadata":{"trusted":true,"_uuid":"adf8c87c6e0e61482401caf21d89a9e8de97b121"},"cell_type":"code","source":"# 自己炼丹时注意删除，我这里主要为了提交 kaggle 方便\nparams = params[:8]","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"0270ad24de6d01e15e3612511ab4eecce34d9043"},"cell_type":"code","source":"dfrult = pd.DataFrame(columns=('k','epochs','learning_rate','weight_decay','units','dropout','train_avg_loss','test_avg_loss','train_avg_loss_std','test_avg_loss_std'))\ni = 0\nfor param in params.values:\n    print(\"%s %d\" % (\"=\"*80,i))\n    i += 1\n    k,epochs,learning_rate,weight_decay,units,dropout = param.tolist()    \n    print(\"k-fold=%d,epochs=%d,learning_rate=%f,weight_decay=%f,units=%d,dropout=%d\" % (k,epochs,learning_rate,weight_decay,units,dropout))\n    \n    train_avg_loss, test_avg_loss, train_avg_loss_std, test_avg_loss_std = k_fold_cross_valid(k, epochs, X_train, y_train, learning_rate, weight_decay, units, dropout, savejpg=True)\n    \n    temp = pd.DataFrame([[k,epochs,learning_rate,weight_decay,units,dropout,train_avg_loss,test_avg_loss,train_avg_loss_std,test_avg_loss_std]],\n                columns=['k','epochs','learning_rate','weight_decay','units','dropout','train_avg_loss','test_avg_loss','train_avg_loss_std','test_avg_loss_std'])\n    dfrult = pd.concat([dfrult, temp])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"495f799ea95898c19bf14e806064d7727b5e6081"},"cell_type":"markdown","source":"savejpg=True 会保存图片到指定位置，注意修改一下这个保存路径。如果正常的话，几百张图片已经在哪里等着了，每组超参一张图片，每张图片中k张图（k-折的k）。\n\n分析一下炼丹炉出来的丹药："},{"metadata":{"trusted":true,"_uuid":"ee6c5ab2ec3cb2c930b9668f3f22c5d413ea05e6"},"cell_type":"code","source":"df = dfrult.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fb5a4b5afb4407a20e40bd5905dd3a2c164be05"},"cell_type":"code","source":"df[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b88349ceb1287d0a712ef9aa56900a087d64d8b"},"cell_type":"code","source":"df['diff'] = df['test_avg_loss'] - df['train_avg_loss']\ndf['sum'] = df['test_avg_loss'] + df['train_avg_loss']\ndf = df.sort_values('sum').reset_index(drop=True)\ndf[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0107aaa38086d9affa018a74c8bf4050821e464"},"cell_type":"code","source":"# 标杆\ndf.loc[df['learning_rate']==0.1].loc[df['weight_decay']==130].loc[df['units']==128].loc[df['dropout']==0.01]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d4fae648804834218d8d2d99356c32736b2e0fb"},"cell_type":"code","source":"df[df['diff']<0.03][:5]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e0cf7e98dda5d1e9339372dbba6436eea4caa47"},"cell_type":"markdown","source":"## **预测**\n\n使用 test 数据集生成预测房价的时刻到了。"},{"metadata":{"trusted":true,"_uuid":"a493c94895aa08f3b016bf460acfd0f1fad905e3"},"cell_type":"code","source":"def learn(epochs, X_train, y_train, test, learning_rate, weight_decay, units, dropout):\n    net = get_net(units=units, dropout=dropout)\n    train_loss = train(net, X_train, y_train, None, None, epochs, learning_rate, weight_decay)\n    plt.plot(train_loss)\n    plt.show()\n    print(\"train loss last 10 data avg: %f\" % np.mean(train_loss[-10:]))\n    preds = net(X_test).asnumpy()\n    test['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n    submission = pd.concat([test['Id'], test['SalePrice']], axis=1)\n    submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"9e71234c3b0545709b20bb693996cf2bfecb8344"},"cell_type":"code","source":"# 选出那粒丹药\n#k,epochs,learning_rate,weight_decay,units,dropout = (5,50,0.1,130,128,0.01)\nk,epochs,learning_rate,weight_decay,units,dropout = df.iloc[0][0:6]\nprint(\"k-fold=%d,epochs=%d,learning_rate=%f,weight_decay=%f,units=%d,dropout=%d\" % (k,epochs,learning_rate,weight_decay,units,dropout))\n\nlearn(epochs, X_train, y_train, df_test, learning_rate, weight_decay, units, dropout)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a42dec163a2e2d988599b959e846a1dee6d772e2"},"cell_type":"markdown","source":"顺利的话，待提交 kaggle 的文件已经生成： submission.csv\n\n使用命令行提交： `kaggle c submit -c house-prices-advanced-regression-techniques -f submission.csv -m \"I love kaggle\"` \n\n祝你和我一样，也能进入前100名。\n\nGood luck！"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"nav_menu":{"height":"272px","width":"259px"},"number_sections":false,"sideBar":true,"skip_h1_title":false,"toc_cell":false,"toc_position":{"height":"850px","left":"0px","right":"1454px","top":"73px","width":"226px"},"toc_section_display":"block","toc_window_display":true}},"nbformat":4,"nbformat_minor":1}