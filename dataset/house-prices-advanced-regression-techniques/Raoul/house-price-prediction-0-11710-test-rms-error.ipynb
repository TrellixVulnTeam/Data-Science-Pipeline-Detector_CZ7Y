{"cells":[{"metadata":{"_cell_guid":"f5632012-2c52-4113-88a4-768564e62596","_uuid":"8b420da89ee6828721553e4533eb6cb4ee2db179"},"cell_type":"markdown","source":"**Author:** Raoul Malm\n\n**Abstract:** \n\nGiven are a training and test data set where each entry contains 80 features that are related to house properties. For the training set which has 4160 entries we know the actual sale prices for each house. The goal is to predict the unknown sale prices for the 4159 entries of the test set. This is a supervised regression problem. After analyzing the data we correct, convert, delete and create new features. We check the correlation of the features and their importance on the sale price. After the feature selection we model the data and try single and ensembles of models. Finally we predict and submit the test results. The precision of each model is evaluated by the root-mean-squared error (rmse) of the logarithm of the sale prices.\n\n**Results:** \n\n- When we use single (base) models trained with 10-fold cross-valdidation we can achieve an average root-mean-squared error of about 0.124 on the validation set. Lasso regression gives very good results.\n\n- We can stack different base models and train a meta-model on their out-of-fold predictions. Combining Lasso regression and gboost regression achieves a root-mean-squared error of 0.11710 on the test set.\n\n- We also implement a simple neural network but it underperforms with respect to the other base models.\n\n\n**Outline:**\n\n1. [Libraries and settings](#1-bullet)  \n2. [Analyze and manipulate data](#2-bullet)  \n2.1 [First look](#2.1-bullet)  \n2.2 [Target feature SalePrice](#2.2-bullet)  \n2.3 [Correct missing data](#2.3-bullet)  \n2.4 [Convert and create new features](#2.4-bullet)  \n2.5 [Check correlation of features](#2.5-bullet)  \n2.6 [Final feature selection](#2.6-bullet)  \n3. [Model and validate data](#3-bullet)    \n3.1 [Split training/validation/test data](#3.1-bullet)   \n3.2 [Neural network implementation](#3.2-bullet)  \n3.3 [Base models and model averaging](#3.3-bullet)     \n3.4 [Stacking of models and meta model prediction](#3.4-bullet)    \n4. [Predict and submit test results](#4-bullet)    \n\n**References:**\n\n[Stacked Regressions : Top 4% on LeaderBoard by Serigne](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)\n\n[Comprehensive data exploration with Python by Pedro Marcelino](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2a974f7d-1d37-451d-951f-c315636719cd","_uuid":"65c5f800201c7993c02b0fa0ace099229ef08132"},"cell_type":"markdown","source":"# 1. Libraries and settings <a class=\"anchor\" id=\"1-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b93107a8-4967-48df-aa9b-519906bcaecf","_uuid":"829b620b9a57ef33ea9fa543ee3b2ab9d82f951f","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np  \nimport pandas as pd  \nimport scipy.stats   \nimport scipy.special  \nimport subprocess\nimport sklearn.linear_model\nimport sklearn.model_selection\nimport sklearn.pipeline  \nimport sklearn.preprocessing  \nimport sklearn.ensemble  \nimport sklearn.kernel_ridge \nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\n# check the files available in the directory\nprint(subprocess.check_output([\"ls\", \"../input\"]).decode(\"utf8\")) ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"733e75b9-a594-4bfb-96a9-02e15b247e63","_uuid":"b5d8cdeada57349fe67521c04922b948d839a476"},"cell_type":"markdown","source":"# 2. Analyze and manipulate data <a class=\"anchor\" id=\"2-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a0c6c54e-71de-4454-bce0-f5e8444db8a3","_uuid":"6f41ee47b5065996606a6ea30f2b3c755720118a"},"cell_type":"markdown","source":"### 2.1 First look  <a class=\"anchor\" id=\"2.1-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"367472e6-d4da-4295-90ce-d46ae7591673","_uuid":"2018209ae12e205ec2e024fe4df0c8f42c75fb01","trusted":false,"collapsed":true},"cell_type":"code","source":"## import train and test data\n\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n\nprint('train_df.shape = ', train_df.shape)\nprint('test_df.shape = ', test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12c2fc16-e937-491f-9e0e-cb61ad03669f","_uuid":"13262d7130fc5e410d73212f8af8df455b74f614","trusted":false,"collapsed":true},"cell_type":"code","source":"## show all features\n\nnum_of_numerical_cols = train_df._get_numeric_data().columns.shape[0]\nprint(num_of_numerical_cols, 'numerical columns')\nprint(train_df.shape[1]-num_of_numerical_cols, 'categorial columns')\ntrain_df.columns.values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cc87a8d5-5233-4e04-9757-c93587ee5908","_uuid":"0aba5f9dafde8d87f6d118bb6e14b702f68ba5f5","collapsed":true,"trusted":false},"cell_type":"code","source":"#train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"483f189b-e5dd-4b59-9e78-12e9ba3c775e","_uuid":"42c9745d008f1d41d1785ac073f46be3026a8fc6","collapsed":true,"trusted":false},"cell_type":"code","source":"#test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9220ddc1-5fcd-402f-b56f-ba209fccdc37","_uuid":"d2388419a68478d4ca990f3b3afe7b17bbb61a49","trusted":false,"collapsed":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2b58e43-b097-4bc9-b548-0c4834ff5560","_uuid":"91fff85e473109f7b741fe5071f27fe0929ccee0","trusted":false,"collapsed":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1a357443-26b9-49f7-a5c6-d2d69454e0d1","_uuid":"6aaa560c4d2ed6e396081ee098cad56bad98d463"},"cell_type":"markdown","source":"### 2.2 Target feature SalePrice  <a class=\"anchor\" id=\"2.2-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b4a8f157-95d6-43a2-ae44-66f4972031c2","_uuid":"471a593e179aa1741b380b47f102aedb3f9454a9","trusted":false,"collapsed":true},"cell_type":"code","source":"## Analyze SalePrice\n\nprint(train_df['SalePrice'].describe())\n\nsns.distplot(train_df['SalePrice'], fit=scipy.stats.norm);\n\n# fit a gaussian function to SalePrice\n(mu, sigma) = scipy.stats.norm.fit(train_df['SalePrice'])\n\n# mean, std, skewness and kurtosis\nprint('mu = {:.2f} and sigma = {:.2f}'.format(mu, sigma))\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())\n\n# plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bd88e04c-3c20-4f30-861e-55ecca92dd20","_uuid":"0153ac91ddd674967eed45adc281977cfa246b3a","trusted":false,"collapsed":true},"cell_type":"code","source":"## create feature Log1pSalePrice = log(1+SalePrice)\n\n#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain_df[\"Log1pSalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train_df['Log1pSalePrice'] , fit=scipy.stats.norm);\n(mu, sigma) = scipy.stats.norm.fit(train_df['SalePrice'])\n\n# mean, std, skewness and kurtosis\nprint('mu = {:.2f} and sigma = {:.2f}'.format(mu, sigma))\nprint(\"Skewness: %f\" % train_df['Log1pSalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['Log1pSalePrice'].kurt())\n\n# plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9dce1d60-8fef-4e8e-8739-eb6104442eaa","_uuid":"084ef561324accded4fc8196f70f7267fd8c6217"},"cell_type":"markdown","source":"### 2.3 Correct missing data  <a class=\"anchor\" id=\"2.3-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ea13507d-7da0-44a8-92a7-0dfe1950cd7d","_uuid":"1d20c529dd2ee7348a6b42d9b4a1655d23f01f5c","trusted":false,"collapsed":true},"cell_type":"code","source":"## concatenate train and test data\n\ndata_df = pd.concat((train_df, test_df)).reset_index(drop=True)\ndata_df.drop(['SalePrice','Log1pSalePrice'], axis=1, inplace=True)\nprint(\"data_df.shape = \",data_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b384bdeb-bb0c-4cfb-a73b-b61321ec5f8e","_uuid":"863095c64c3f0b437d4ac35ef7ad8668b9352cf4","trusted":false,"collapsed":true},"cell_type":"code","source":"## check missing data\n\ndata_df_na = data_df.isnull().mean(axis=0)\ndata_df_na = data_df_na.drop(data_df_na[data_df_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Data Ratio': data_df_na})\nprint('data_df_na.shape = ', data_df_na.shape)\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2b338c19-b493-4c3b-bfde-ae2a40cc6395","_uuid":"0a4abea30dd80fe40c8a48b2da142c35ed1ab074","trusted":false,"collapsed":true},"cell_type":"code","source":"## visualize missing data\n\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x = data_df_na.index, y = 100*data_df_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percentage of missing values', fontsize=15)\nplt.title('Percentage of missing data of features', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9043689a-df98-45b8-8c9f-77ae66587a3e","_uuid":"c87692c9d22200257694fbafda3d7cde39121ac6","collapsed":true,"trusted":false},"cell_type":"code","source":"## treat missing values\n\n# set missing values to None (which means that the house does not have those features)\ndata_df[\"PoolQC\"] = data_df[\"PoolQC\"].fillna(\"None\") # has no pool\ndata_df[\"MiscFeature\"] = data_df[\"MiscFeature\"].fillna(\"None\") # has no special feature \ndata_df[\"Alley\"] = data_df[\"Alley\"].fillna(\"None\") # has no alley access\ndata_df[\"Fence\"] = data_df[\"Fence\"].fillna(\"None\") # has no fence \ndata_df[\"FireplaceQu\"] = data_df[\"FireplaceQu\"].fillna(\"None\") # has no fireplace\n\n# group by neighborhoods and set missing values to \n# the median of LotFrontage of the corresponding neighborhoods\ndata_df[\"LotFrontage\"] = data_df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n# no garage => set missing entries to None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    data_df[col] = data_df[col].fillna('None')\n\n# no garage => set missing entries to 0\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    data_df[col] = data_df[col].fillna(0)\n\n# having no basement => set missing entries to 0\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', \n            'BsmtHalfBath'):\n    data_df[col] = data_df[col].fillna(0)\n    \n# having no basement => set missing entries to None\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    data_df[col] = data_df[col].fillna('None')\n    \n# no masonry veneer => set Area = 0 and Type = None\ndata_df[\"MasVnrType\"] = data_df[\"MasVnrType\"].fillna(\"None\")\ndata_df[\"MasVnrArea\"] = data_df[\"MasVnrArea\"].fillna(0)\n\n# general zoning classification: set missing entry to most common value\ndata_df['MSZoning'] = data_df['MSZoning'].fillna(data_df['MSZoning'].mode()[0])\n\n# for this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA\n# => we can remove it\ndata_df = data_df.drop(['Utilities'], axis=1)\n\n# set missing value to typical\ndata_df[\"Functional\"] = data_df[\"Functional\"].fillna(\"Typ\")\n\n# set missing entries to most common values\ndata_df['Electrical'] = data_df['Electrical'].fillna(data_df['Electrical'].mode()[0])\ndata_df['KitchenQual'] = data_df['KitchenQual'].fillna(data_df['KitchenQual'].mode()[0])\ndata_df['Exterior1st'] = data_df['Exterior1st'].fillna(data_df['Exterior1st'].mode()[0])\ndata_df['Exterior2nd'] = data_df['Exterior2nd'].fillna(data_df['Exterior2nd'].mode()[0])\ndata_df['SaleType'] = data_df['SaleType'].fillna(data_df['SaleType'].mode()[0])\n\n# set missing value to None \ndata_df['MSSubClass'] = data_df['MSSubClass'].fillna(\"None\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"899c8eeb-c014-4bbc-9a28-a55e612d9b98","_uuid":"c4e4c47e998e26ba121876a160ae405cd98576f2","trusted":false,"collapsed":true},"cell_type":"code","source":"## check if missing values remain\n\ndata_df_na = data_df.isnull().median(axis = 0)\ndata_df_na = data_df_na.drop(data_df_na[data_df_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Data Ratio': data_df_na})\nprint('data_df_na.shape = ', data_df_na.shape)\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3b6cdfaa-04f9-42e3-b4d8-ff76a9d21742","_uuid":"2b47719e0736acf354c731850f7104f82b23ce9f"},"cell_type":"markdown","source":"### 2.4 Convert and create new features  <a class=\"anchor\" id=\"2.4-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ad1b246b-10e2-4fac-b162-205f4f135ec8","_uuid":"bb722d2a15f42ee28276e8475336a6f0a7c92b92","collapsed":true,"trusted":false},"cell_type":"code","source":"## Convert numerical type into string type for categorial features \n\n# MSSubClass = The building class\ndata_df['MSSubClass'] = data_df['MSSubClass'].apply(str)\n\n# changing OverallCond into a categorical variable\ndata_df['OverallCond'] = data_df['OverallCond'].astype(str)\n\n# year and month sold are transformed into categorical features\ndata_df['YrSold'] = data_df['YrSold'].astype(str)\ndata_df['MoSold'] = data_df['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"485371a2-87f0-447d-bc9f-10c473f1a8f6","_uuid":"9bc8d0db760cd894adcfc8c680c367c679a991be","trusted":false,"collapsed":true},"cell_type":"code","source":"## Encode some features with values between 0 and n_classes-1\n\n# categorial features\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n# apply sklearn.preprocessing.LabelEncoder to each categorical feature\nfor c in cols:\n    lbl = sklearn.preprocessing.LabelEncoder() \n    lbl.fit(list(data_df[c].values)) \n    data_df[c] = lbl.transform(list(data_df[c].values))\n\n# shape        \nprint('data_df.shape = ', data_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d3747df5-7570-4c4d-b38d-2bc4af23f6c7","_uuid":"8354c0772c600acd3377dba685207fd4583697f5","collapsed":true,"trusted":false},"cell_type":"code","source":"## Create new feature: total sqfootage \n\ndata_df['TotalSF'] = data_df['TotalBsmtSF'] + data_df['1stFlrSF'] + data_df['2ndFlrSF']\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e34a55cd-5d1b-474a-9ae7-c21d424a62a8","_uuid":"424c7aa0f181fad2e0ac0a5a81d4ad715c2e7b0d","trusted":false,"collapsed":true},"cell_type":"code","source":"## Check the skewness of all numerical features\n\nnumerical_features = data_df.dtypes[data_df.dtypes != \"object\"].index\nskewness_of_features = data_df[numerical_features].apply(lambda x: scipy.stats.skew(x.dropna())).sort_values(\n    ascending=False)\nskewness_df = pd.DataFrame({'Skewness': skewness_of_features})\nskewness_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0e062202-057c-428c-bce8-b1d99a1437c0","_uuid":"84d1ae14e68f8489eef16319aa102271529d4dba","trusted":false,"collapsed":true},"cell_type":"code","source":"## Compute the Box-Cox transformation of 1 + x for skewed features\n\nskewness_df = skewness_df[abs(skewness_df) > 0.75]\nprint(\"skewness_df.shape = \", skewness_df.shape)\nskewed_features = skewness_df.index\nlamb = 0.15\nfor feature in skewed_features:\n    data_df[feature] = scipy.special.boxcox1p(data_df[feature], lamb)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"92fe3009-a323-477d-963c-a20728a541d8","_uuid":"bd6ccad7d4d328bc77bf2474bf1f076e7864c5a6","trusted":false,"collapsed":true},"cell_type":"code","source":"## Check skewness again\n\nnumerical_features = data_df.dtypes[data_df.dtypes != \"object\"].index\nskewness_of_features = data_df[numerical_features].apply(lambda x: \n                            scipy.stats.skew(x.dropna())).sort_values(ascending=False)\nskewness_df = pd.DataFrame({'Skewness': skewness_of_features})\nskewness_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"19b5ec18-b60f-4598-bf78-d214f9f923a2","_uuid":"5cd0d38476685b957d82fb1d7d00b1688f867680"},"cell_type":"markdown","source":"### 2.5 Check correlation of features  <a class=\"anchor\" id=\"2.5-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e2942bde-ce6b-4809-adbf-2ca239c3cc11","_uuid":"2170995e2af7af14e7c00b1d87418d6444747f8e","trusted":false,"collapsed":true},"cell_type":"code","source":"## use LabelEncoding or dummy variables on all categorial features\n\ncols = data_df.select_dtypes(exclude = [np.number]).columns.values\nprint('numerical columns:', data_df.select_dtypes(include = [np.number]).columns.values.shape[0])\nprint('categorial columns:', cols.shape[0])\n\nif True:\n    # create dummy variables\n    data_df = pd.get_dummies(data_df).copy()\nelse:\n    # apply sklearn.preprocessing.LabelEncoder\n    for c in cols:\n        lbl = sklearn.preprocessing.LabelEncoder() \n        lbl.fit(list(data_df[c].values)) \n        data_df[c] = lbl.transform(list(data_df[c].values))\n\n# shape        \nprint('data_df.shape = ', data_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"868439d1-17e5-4db8-b7c9-2c90d0d40e60","_uuid":"a365321cae825664df3d978e737b8ea083f4ca06","trusted":false,"collapsed":true},"cell_type":"code","source":"# correlation map of all features\ndf = data_df[:train_df.shape[0]]\ndf['SalePrice'] = train_df['SalePrice'].values\ncorrmat = df.corr()\nplt.subplots(figsize=(30,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)\ncorr_values_df = pd.DataFrame(df.corr()['SalePrice'].abs().sort_values(ascending=True))\n\n# show features that have the smallest correlation with SalePrice\nprint(corr_values_df[0:10])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"611dedaf-027e-4f16-9df4-5735c6a35575","_uuid":"a4c79be2ac18b6ca12636f41c893d5c698b1f8b3","trusted":false,"collapsed":true},"cell_type":"code","source":"# box plot SalePrice versus OverallQual\nvar = 'OverallQual'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8738a873-48dc-4591-a753-34be70a774ef","_uuid":"bc7dc8459e65da35f3d20e1dce5bfa709917785c","trusted":false,"collapsed":true},"cell_type":"code","source":"## box-plot SalePrice versus YearBuilt\nvar = 'YearBuilt'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"139b3ac0-f3f2-48bc-8898-37b6ea94024d","_uuid":"c747d8a01d4aa1e812d53a2fb7be35ab6bba32a5"},"cell_type":"markdown","source":"### 2.6 Final feature selection <a class=\"anchor\" id=\"2.6-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"fd991f80-52ce-4df6-9e9e-a79aafd81cf0","_uuid":"0436662104b43e20a2673bfcadc107a0a3110119","trusted":false,"collapsed":true},"cell_type":"code","source":"## drop features\n\nif False:\n    drop_features = corr_values_df.index.values[0:100]\n    data_df = data_df.drop(drop_features, axis = 1)\n\nif False:\n    drop_features = ['Id','MoSold','Condition2','LandContour']\n    data_df = data_df.drop(drop_features, axis = 1)\n\nif False:\n    drop_features = ['Id','MoSold','Condition2','LandContour']\n    data_df = data_df.drop(drop_features, axis = 1)\n    \nprint('data_df.shape = ',data_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f18d0e1d-42ca-4698-9a45-574e31c2e4b9","_uuid":"e60b4e775237389b1758e5f2923ca38746a099f7"},"cell_type":"markdown","source":"# 3. Model data  <a class=\"anchor\" id=\"3-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0140de36-cad0-498c-be7b-a3d132379e7a","_uuid":"4b034de3f74d08b7c00378d4e0bf7d05fd867c70"},"cell_type":"markdown","source":"### 3.1 Split training/validation/test data <a class=\"anchor\" id=\"3.1-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e83c1500-c0d2-4de6-a987-0495836a8270","_uuid":"62345974e6ffa6add2d653c1b5798f8069cbfa7b","trusted":false,"collapsed":true},"cell_type":"code","source":"## split data into training/validation/test sets\n\n# function to compute the root mean squared error\ndef get_rmse(y_pred, y_target):\n    return np.sqrt(np.mean(np.square(y_pred.reshape(-1,) - y_target.reshape(-1,))))\n\n# function to normalize data\ndef normalize_data(data): \n    # scale features using statistics that are robust to outliers\n    # turns out that sklearn works very well \n    rs = sklearn.preprocessing.RobustScaler()\n    rs.fit(data)\n    data = rs.transform(data)\n    #div = np.percentile(data,0.75,axis=0) - np.percentile(data,0.25,axis=0)\n    #np.place(div, div < 1., 1.)\n    #print(div)\n    #data = (data-np.median(data,axis=0))/div\n    return data\n\n# normalize data to make it robust against outliers\ndata_df_norm = normalize_data(data_df.values)\n\n# store train+validation data\nx_train_valid = data_df_norm[:train_df.shape[0]]\ny_train_valid = train_df.Log1pSalePrice.values\nx_test = data_df_norm[train_df.shape[0]:]\n\n# dictionaries for storing results\ny_test_pred  = {}\nrmse_train = {}\nrmse_valid = {}\n\nprint('x_train_valid.shape = ', x_train_valid.shape)\nprint('y_train_valid.shape = ', y_train_valid.shape)\nprint('x_test.shape = ', x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aebb8513-ae3e-4b44-896f-9ada4bc311ab","_uuid":"8dd1db1d9a55c4981f9d00cbbe65437d1dae4028"},"cell_type":"markdown","source":"### 3.2 Neural network implementation <a class=\"anchor\" id=\"3.2-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"_uuid":"a0ca9ecf90b0a60fbed6478ed8f369e88ca0666e","_cell_guid":"9787118c-420f-4255-abc0-26b8562b14fe","trusted":false},"cell_type":"code","source":"## implementation of a neural network\n\n# global parameters needed for next_batch function\nperm_array_train = np.array([])\nindex_in_epoch = 0 \n\n# function to get the next mini batch\ndef next_batch(batch_size, x_train, y_train):\n    global index_in_epoch, perm_array_train\n    \n    start = index_in_epoch\n    index_in_epoch += batch_size\n    \n    if not len(perm_array_train) ==  len(x_train):\n        perm_array_train = np.arange(len(x_train))\n    \n    if index_in_epoch > len(x_train):\n        np.random.shuffle(perm_array_train) # shuffle data\n        start = 0 # start next epoch\n        index_in_epoch = batch_size # set index to batch size\n                \n    end = index_in_epoch\n    \n    x_tr  = x_train[perm_array_train[start:end]]\n    y_tr  = y_train[perm_array_train[start:end]].reshape(-1,1)\n     \n    return x_tr, y_tr\n\n# function to create the graph\ndef create_nn_graph(num_input_features = 10, num_output_features = 1):\n\n    # reset default graph\n    tf.reset_default_graph()\n\n    # parameters of nn architecture\n    x_size = num_input_features # number of features\n    y_size = num_output_features # output size\n    n_n_fc1 = 256; # number of neurons of first layer\n    n_n_fc2 = 32; # number of neurons of second layer\n\n    # variables for input and output \n    x_data = tf.placeholder('float', shape=[None, x_size])\n    y_data = tf.placeholder('float', shape=[None, y_size])\n\n    # 1.layer: fully connected\n    W_fc1 = tf.Variable(tf.truncated_normal(shape = [x_size, n_n_fc1], stddev = 0.1))\n    b_fc1 = tf.Variable(tf.constant(0.1, shape = [n_n_fc1]))  \n    h_fc1 = tf.nn.relu(tf.matmul(x_data, W_fc1) + b_fc1)\n\n    # dropout\n    tf_keep_prob = tf.placeholder('float')\n    h_fc1_drop = tf.nn.dropout(h_fc1, tf_keep_prob)\n\n    # 2.layer: fully connected\n    W_fc2 = tf.Variable(tf.truncated_normal(shape = [n_n_fc1, n_n_fc2], stddev = 0.1)) \n    b_fc2 = tf.Variable(tf.constant(0.1, shape = [n_n_fc2]))  \n    h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) \n\n    # dropout\n    h_fc2_drop = tf.nn.dropout(h_fc2, tf_keep_prob)\n\n    # 3.layer: fully connected\n    W_fc3 = tf.Variable(tf.truncated_normal(shape = [n_n_fc2, y_size], stddev = 0.1)) \n    b_fc3 = tf.Variable(tf.constant(0.1, shape = [y_size]))  \n    y_pred = tf.cast(tf.matmul(h_fc2_drop, W_fc3) + b_fc3, dtype = tf.float32)\n\n    # cost function huber_loss \n    #loss = tf.sqrt(tf.losses.mean_squared_error(labels=y_data, predictions=y_pred))\n    loss = tf.sqrt(tf.reduce_mean(tf.square(y_data - y_pred)))\n\n    # optimisation function\n    tf_learn_rate = tf.placeholder(dtype='float', name=\"tf_learn_rate\")\n    train_step = tf.train.AdamOptimizer(tf_learn_rate).minimize(loss)\n    \n    # tf tensors\n    tf_tensors = {'train_step': train_step, 'loss': loss, 'y_pred': y_pred,\n                  'tf_keep_prob': tf_keep_prob, 'tf_learn_rate': tf_learn_rate,\n                  'x_data': x_data, 'y_data': y_data}\n    \n    return tf_tensors\n\n# function to train the graph\ndef train_nn_graph(tf_tensors, x_train, y_train, x_valid, y_valid, verbose = False):\n    \n    # tf tensors\n    train_step = tf_tensors['train_step']\n    loss = tf_tensors['loss']\n    y_pred = tf_tensors['y_pred']\n    tf_keep_prob = tf_tensors['tf_keep_prob']\n    tf_learn_rate = tf_tensors['tf_learn_rate']\n    x_data = tf_tensors['x_data']\n    y_data = tf_tensors['y_data']\n    \n    # parameters\n    train_set_size = x_train.shape[0]\n    keep_prob = 0.5; # dropout regularization with keeping probability\n    learn_rate_range = [0.01, 0.0075, 0.005, 0.0025, 0.001, 0.00075, 0.00050, 0.00025,\n                        0.0001];\n    learn_rate_step = 10 # in terms of epochs \n    batch_size = 10\n    n_epoch = 100 # number of epochs\n\n    # start TensorFlow session and initialize global variables\n    sess = tf.InteractiveSession() \n    sess.run(tf.global_variables_initializer())  \n    n_learnrate_step = 0;\n\n    # iterate\n    for i in range(int(n_epoch*train_set_size/batch_size)):\n\n        # adapt learning rate\n        if (i%int(learn_rate_step*train_set_size/batch_size) == 0 and \n            n_learnrate_step < len(learn_rate_range)):\n\n            learn_rate = learn_rate_range[n_learnrate_step];\n            if verbose: \n                print('nn: set learnrate = ', learn_rate)\n            n_learnrate_step += 1;\n\n        # get next batch\n        x_batch, y_batch = next_batch(batch_size, x_train, y_train)\n\n        sess.run(train_step, feed_dict={x_data: x_batch, y_data: y_batch, \n                                        tf_keep_prob: keep_prob, tf_learn_rate: learn_rate})\n\n        # log the status\n        if (verbose and i%int(2.*train_set_size/batch_size) == 0):\n            train_loss = sess.run(loss,feed_dict={x_data: x_train, \n                                                  y_data: y_train, \n                                                  tf_keep_prob: 1.0})\n\n            valid_loss = sess.run(loss,feed_dict={x_data: x_valid, \n                                                  y_data: y_valid, \n                                                  tf_keep_prob: 1.0})\n\n            print('nn: %.2f epoch: train/val loss = %.4f/%.4f'%(\n                (i+1)*batch_size/train_set_size, train_loss, valid_loss))\n\n    # store rmse \n    y_train_pred = y_pred.eval(feed_dict={x_data: x_train, tf_keep_prob: 1.0}).flatten()\n    y_valid_pred = y_pred.eval(feed_dict={x_data: x_valid, tf_keep_prob: 1.0}).flatten()\n    y_test_pred = y_pred.eval(feed_dict={x_data: x_test, tf_keep_prob: 1.0}).flatten()\n\n    sess.close();\n    \n    return (y_train_pred, y_valid_pred, y_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"02aaf6299e62cf91768b12d51960633c41329394","_cell_guid":"0b4623b8-92c9-4bcf-9bfc-8b67ed0e43f0","trusted":false},"cell_type":"code","source":"## check training of neural network\n\nif False:\n    # create graph and receive tf tensors\n    tf_tensors = create_nn_graph(x_train_valid.shape[1], 1)\n\n    # cross validations\n    cv_num = 10\n    kfold = sklearn.model_selection.KFold(cv_num, shuffle=True)\n\n    for train_index, valid_index in kfold.split(x_train_valid):\n\n        x_train = x_train_valid[train_index]\n        y_train = y_train_valid[train_index].reshape(-1,1)\n        x_valid = x_train_valid[valid_index]\n        y_valid = y_train_valid[valid_index].reshape(-1,1)\n\n        # train nn\n        (y_train_pred['nn'], \n         y_valid_pred['nn'],\n         y_test_pred['nn']) = train_nn_graph(tf_tensors, x_train,  \n                                             y_train, x_valid,  y_valid, True)\n        \n        # loss\n        print('nn: train/val loss = %.4f/%.4f'%(get_rmse(y_train_pred['nn'],y_train),\n                                                get_rmse(y_valid_pred['nn'], y_valid)))\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a358e52d-e860-45b5-9302-5b9055cdbe08","_uuid":"a8c6554e814e385de5df0a71d98ca0d2529a5248"},"cell_type":"markdown","source":"### 3.3 Training base models  <a class=\"anchor\" id=\"3.3-bullet\"></a> ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"355756d5-6098-4fc7-b646-19781d94dd80","_uuid":"06304552492db7ad77f21316d825f44c34d6eabd","trusted":false,"collapsed":true},"cell_type":"code","source":"## Training base models\n\n# base models in sklearn\nlinreg = sklearn.linear_model.LinearRegression()\nlasso = sklearn.linear_model.Lasso(alpha = 0.0005, random_state=1) \nenet =  sklearn.linear_model.ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\nkrr = sklearn.kernel_ridge.KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\ngboost = sklearn.ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nrforest = sklearn.ensemble.RandomForestRegressor(n_estimators=3000,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   random_state =5)\n\n\n# store models in dictionary\nmodel_init = {'linreg': linreg, 'lasso': lasso, 'enet': enet, 'krr': krr, \n              'gboost': gboost, 'rforest': rforest}\n\n# choose the models to train on\ntake_base_models = ['linreg', 'lasso', 'enet', 'krr', 'gboost', 'rforest', 'nn']\n\n# dictionaries to store results\nrmse_valid = {}\nrmse_train = {}\ny_train_pred = {}\ny_valid_pred = {}\ny_test_pred = {}\n\n# initialize variables to zero\nfor mn in take_base_models:\n    rmse_train[mn] = 0\n    rmse_valid[mn] = 0\n    y_test_pred[mn] = 0\n\n# number of cross validations\ncv_num = 10\nkfold = sklearn.model_selection.KFold(cv_num, shuffle=True)\n\n# cross-validation\nfor i, (train_index, valid_index) in enumerate(kfold.split(x_train_valid)):\n    \n    print(i+1,'. training of models in progress')\n    x_train = x_train_valid[train_index]\n    y_train = y_train_valid[train_index]\n    x_valid = x_train_valid[valid_index]\n    y_valid = y_train_valid[valid_index]\n    \n    for mn in take_base_models:\n        \n        if mn == 'nn':\n            # create graph and receive tf tensors\n            tf_tensors = create_nn_graph(x_train_valid.shape[1], 1)\n    \n            # train neural network\n            params = train_nn_graph(tf_tensors, x_train, y_train.reshape(-1,1),\n                                    x_valid, y_valid.reshape(-1,1), False) \n    \n            rmse_train['nn'] += get_rmse(params[0],y_train)\n            rmse_valid['nn'] += get_rmse(params[1],y_valid)\n            y_test_pred['nn'] += params[2]\n\n        else:\n            # create cloned model from model_init\n            model = sklearn.base.clone(model_init[mn])\n            model.fit(x_train, y_train)\n            y_test_pred[mn] += model.predict(x_test)\n            rmse_train[mn] += get_rmse(model.predict(x_train), y_train)\n            rmse_valid[mn] += get_rmse(model.predict(x_valid), y_valid)\n          \nprint('')\n# store and print results\nfor mn in take_base_models:\n    \n    rmse_train[mn] /= cv_num\n    rmse_valid[mn] /= cv_num\n    y_test_pred[mn] /= cv_num\n        \n    print(mn,'train/valid RMSE = %.3f/%.3f'%(rmse_train[mn], rmse_valid[mn]))\n\n# average rmse over the following models\ntake_model_avg = ['linreg', 'lasso', 'enet']\n\nrmse_train['averaged'] = 0\nrmse_valid['averaged'] = 0\n\nfor mn in take_model_avg:\n    rmse_train['averaged'] += rmse_train[mn]**2\n    rmse_valid['averaged'] += rmse_valid[mn]**2\n\n# average rmse\nrmse_train['averaged'] = np.sqrt(rmse_train['averaged']/len(take_model_avg)) \nrmse_valid['averaged'] = np.sqrt(rmse_valid['averaged']/len(take_model_avg)) \n\nprint('')\nprint('Average the following models:', take_model_avg)\nprint('Averaged train/valid RMSE = %.3f/%.3f'%(rmse_train['averaged'],\n                                               rmse_valid['averaged']))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6ea77c01-79f8-4bf0-85f9-8a684a2e7c62","_uuid":"1965a1881c63352bff0b735f8f78ea506eceadf1","trusted":false,"collapsed":true},"cell_type":"code","source":"## correlation of test predictions\n\ny_test_pred_df = pd.DataFrame({})\nfor key in y_test_pred.keys():\n    y_test_pred_df[key] = np.expm1(y_test_pred[key])\n\ncorrmat = y_test_pred_df.corr()\ncorrmat\nplt.subplots(figsize=(10,5))\nplt.title('Correlation of Test Predictions')\nsns.heatmap(corrmat, vmax=1, square=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"522620a3-1ff3-4d1a-b745-f079c6527ff3","_uuid":"dc5d362238b5cf95ce083c1151b1ab5c500a36e8"},"cell_type":"markdown","source":"### 3.4 Stacking of base models <a class=\"anchor\" id=\"3.4-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4e71509d-67ca-492f-bf07-d295ab040398","_uuid":"6c2ed4c8a4322a552a465dda56f5d949e7938c63","trusted":false,"collapsed":true},"cell_type":"code","source":"# choose models for out-of-folds predictions\n\n#take_base_models = ['linreg','lasso','rforest', 'gboost', 'krr', 'enet', 'nn']\n\ntake_base_models = ['lasso', 'gboost','lasso', 'gboost', 'gboost',\n                    'lasso', 'gboost', 'lasso', 'gboost','gboost']\n\n# choose meta model\ntake_meta_model = 'lasso'\n\n# cross validations\nkfold = sklearn.model_selection.KFold(len(take_base_models), shuffle=True)\n\n# train data for meta model\nx_train_meta = np.array([])\ny_train_meta = np.array([])\nx_test_meta = np.zeros(x_test.shape[0])\n\n# make out-of-folds predictions from base models\nfor i,(train_index, valid_index) in enumerate(kfold.split(x_train_valid)):\n    \n    x_train = x_train_valid[train_index]\n    y_train = y_train_valid[train_index]\n    x_valid = x_train_valid[valid_index]\n    y_valid = y_train_valid[valid_index]\n    \n    if take_base_models[i] == 'nn':\n        # create graph and receive tf tensors\n        tf_tensors = create_nn_graph(x_train_valid.shape[1], 1)\n\n        # train neural network\n        params = train_nn_graph(tf_tensors, x_train, y_train.reshape(-1,1),\n                                x_valid, y_valid.reshape(-1,1), False) \n\n        y_train_pred['tmp'] = params[0]\n        y_valid_pred['tmp'] = params[1]\n        y_test_pred['tmp'] = params[2]\n\n    else:\n        # create cloned model from base models\n        model = sklearn.base.clone(model_init[take_base_models[i]])\n        model.fit(x_train, y_train)\n        y_train_pred['tmp'] = model.predict(x_train)\n        y_valid_pred['tmp'] = model.predict(x_valid)\n        y_test_pred['tmp'] = model.predict(x_test)\n\n    # collect train and test data for meta model \n    x_train_meta = np.concatenate([x_train_meta, y_valid_pred['tmp']])\n    y_train_meta = np.concatenate([y_train_meta, y_valid]) \n    x_test_meta += y_test_pred['tmp']\n    \n    print(take_base_models[i],': train/valid rmse = %.3f/%.3f'%(\n        get_rmse(y_train_pred['tmp'], y_train), get_rmse(y_valid_pred['tmp'], y_valid)))\n\nx_train_meta = x_train_meta.reshape(-1,1)\nx_test_meta = (x_test_meta/len(take_base_models)).reshape(-1,1)\ny_test_pred['stacked'] = x_test_meta\n\nprint('')\nprint('Stacked base models: valid rmse = ', get_rmse(x_train_meta, y_train_meta))\n\n# train meta model\nmodel = sklearn.base.clone(model_init[take_meta_model]) \nmodel.fit(x_train_meta, y_train_meta)\ny_train_pred['meta model'] = model.predict(x_train_meta)\ny_test_pred['meta model'] = model.predict(x_test_meta)\n\nprint('Meta model: train rmse = ', get_rmse(x_train_meta, y_train_pred['meta model']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"278a3a6d-abb7-45c1-85fd-0b328477ffa0","_uuid":"86e5ea23476538778a26bfbeebb04bbbe777f9b3"},"cell_type":"markdown","source":"# 4. Predict and submit test results  <a class=\"anchor\" id=\"4-bullet\"></a>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"cc2cf9fe-58f1-43ee-a2fb-24f7bc2bbb90","_uuid":"046f5e6d0177d6baa1a4a20ccb281786419e2824","trusted":false,"collapsed":true},"cell_type":"code","source":"## choose the test prediction\n\ny_test_submit = y_test_pred['meta model']\n\nsub_df = pd.DataFrame()\nsub_df['Id'] = test_df['Id'].values\nsub_df['SalePrice'] = np.expm1(y_test_submit)\nsub_df.to_csv('submission.csv',index=False)\nsub_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"ebaf3e9d35b8aad297de65a0b094e2835a4a6d9f","_cell_guid":"2b6e0ab4-c6fe-4bc0-83a0-0165d5c22e56","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}