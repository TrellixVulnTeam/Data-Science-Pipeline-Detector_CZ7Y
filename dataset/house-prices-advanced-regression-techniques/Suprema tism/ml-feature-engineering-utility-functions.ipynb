{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center> Table of contents </center></h1>","metadata":{}},{"cell_type":"markdown","source":"### I. Preparation\n\n* [1. Importing libraries](#I_1)\n\n\n* [2. Data preparation](#I_2)\n\n    \n* [3. Data preparation](#I_3)\n        \n        \n### II. Feature engineering\n\n* [1. K-means clustering](#II_1)\n\n\n* [2. K-bins discretisation](#II_2)\n\n\n* [3. Categorical variables interactions](#II_3)\n\n\n* [4. Featuretools](#II_4)","metadata":{}},{"cell_type":"markdown","source":"<h1><center> I. Preparation </center></h1>","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing basic libraries <a class=\"anchor\" id = \"I_1\"></a>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:12.250166Z","iopub.execute_input":"2021-12-01T08:14:12.25102Z","iopub.status.idle":"2021-12-01T08:14:12.271946Z","shell.execute_reply.started":"2021-12-01T08:14:12.250922Z","shell.execute_reply":"2021-12-01T08:14:12.271084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:12.273527Z","iopub.execute_input":"2021-12-01T08:14:12.27406Z","iopub.status.idle":"2021-12-01T08:14:13.07608Z","shell.execute_reply.started":"2021-12-01T08:14:12.274001Z","shell.execute_reply":"2021-12-01T08:14:13.07521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Setting global parameters for plots <a class=\"anchor\" id = \"I_2\"></a>","metadata":{}},{"cell_type":"code","source":"sns.set_theme(rc = {'grid.linewidth': 0.5,\n                    'axes.linewidth': 0.75, 'axes.facecolor': '#ECECEC', \n                    'axes.labelcolor': 'black',\n                    'figure.facecolor': 'white',\n                    'xtick.color': 'black', 'ytick.color': 'black'})","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:13.077168Z","iopub.execute_input":"2021-12-01T08:14:13.077485Z","iopub.status.idle":"2021-12-01T08:14:13.083202Z","shell.execute_reply.started":"2021-12-01T08:14:13.077458Z","shell.execute_reply":"2021-12-01T08:14:13.082277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Importing data <a class=\"anchor\" id = \"I_3\"></a>","metadata":{}},{"cell_type":"markdown","source":"<div style = \"color: #000000;\n             display: fill;\n             padding: 8px;\n             border-radius: 5px;\n             border-style: solid;\n             border-color: #a63700;\n             background-color: rgba(235, 125, 66, 0.3)\">\n    \n<span style = \"font-size: 20px; font-weight: bold\">Note:</span> \nThe purpose of this kernel is to familiarise you with some feature engineering techniques. I used cleaned training and test sets from one of my <a href=\"https://www.kaggle.com/suprematism/top-7-useful-graphs-and-encoding-techniques\">notebooks</a>. Thus, should you want to explore data cleaning and model building, refer to the link placed above.\n</div>","metadata":{}},{"cell_type":"code","source":"Set = pd.read_csv('../input/housing-prices-visual/Housing_prices_visual.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:13.084755Z","iopub.execute_input":"2021-12-01T08:14:13.08498Z","iopub.status.idle":"2021-12-01T08:14:13.147862Z","shell.execute_reply.started":"2021-12-01T08:14:13.084946Z","shell.execute_reply":"2021-12-01T08:14:13.147258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = Set.iloc[:1451]\ndf_test = Set.iloc[1451:]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:13.149177Z","iopub.execute_input":"2021-12-01T08:14:13.149461Z","iopub.status.idle":"2021-12-01T08:14:13.15411Z","shell.execute_reply.started":"2021-12-01T08:14:13.149425Z","shell.execute_reply":"2021-12-01T08:14:13.153244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><center> II. Feature engineering </center></h1>","metadata":{}},{"cell_type":"markdown","source":"## 1. K-means clustering <a class=\"anchor\" id = \"II_1\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:13.15579Z","iopub.execute_input":"2021-12-01T08:14:13.156418Z","iopub.status.idle":"2021-12-01T08:14:13.53692Z","shell.execute_reply.started":"2021-12-01T08:14:13.156381Z","shell.execute_reply":"2021-12-01T08:14:13.536113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Useful articles and documents:\n\n- <a href=\"https://medium.com/greyatom/using-clustering-for-feature-engineering-on-the-iris-dataset-f438366d0b4b\">Clustering for feature engineering</a>;\n- <a href=\"https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/\">A guide to k-means clustering</a>;\n- <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">Sklearn documentation</a>","metadata":{}},{"cell_type":"markdown","source":"<div style = \"text-align: justify\">We can use the output of <span style=\"color:#E85E40\">k-means clustering</span> as features for our models. On top of that, not only clusters themselves can be treated as variables but also the distance to the cluster centres can be used as potential predictors.</div>","metadata":{}},{"cell_type":"markdown","source":"First, we have to define numeric variables that we are going to use for clustering.","metadata":{}},{"cell_type":"code","source":"Num_vars = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', \n            'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n            '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n            'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'MiscVal', \n            '3SsnPorch' , 'PoolArea' , 'LowQualFinSF']","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:13.538167Z","iopub.execute_input":"2021-12-01T08:14:13.538443Z","iopub.status.idle":"2021-12-01T08:14:13.54294Z","shell.execute_reply.started":"2021-12-01T08:14:13.538408Z","shell.execute_reply":"2021-12-01T08:14:13.542197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style = \"text-align: justify\"> Before generating new variables, we have to determine the optimal number of clusters. One way to do it is to draw a so called <span style=\"color:#E85E40\">elbow curve</span>. We plot a number of clusters against <span style=\"color:#E85E40\"> inertia_ </span> – the sum of squared distances of samples to their closest cluster centre. Our task is to find the smallest number of clusters while keeping <span style=\"color:#E85E40\"> inertia_ </span> as low as possible. </div>","metadata":{}},{"cell_type":"markdown","source":"For that purpose, I created a simple utility function:","metadata":{}},{"cell_type":"code","source":"def K_means_claster_tuning(df_train, Vars_list, max_cluster = 15):\n    \n    ### Scaling data\n    \n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(df_train[Vars_list])\n    \n    ### Calculating inertia_ for each number of clusters\n    \n    SSE = []\n    \n    for cluster in range(1, max_cluster):\n        \n        kmeans = KMeans(n_clusters = cluster, init = 'k-means++', random_state = 999)\n        kmeans.fit(data_scaled)\n        SSE.append(kmeans.inertia_)\n        \n    df_plot = pd.DataFrame({'Cluster': range(1, max_cluster), 'SSE': SSE})\n    \n    return(df_plot)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:13.544112Z","iopub.execute_input":"2021-12-01T08:14:13.544325Z","iopub.status.idle":"2021-12-01T08:14:13.553765Z","shell.execute_reply.started":"2021-12-01T08:14:13.544301Z","shell.execute_reply":"2021-12-01T08:14:13.553083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Cluster_tuning = K_means_claster_tuning(df_train, Num_vars, max_cluster = 20)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-01T08:14:13.55566Z","iopub.execute_input":"2021-12-01T08:14:13.5563Z","iopub.status.idle":"2021-12-01T08:14:40.039156Z","shell.execute_reply.started":"2021-12-01T08:14:13.556269Z","shell.execute_reply":"2021-12-01T08:14:40.038187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}):\n    \n    fig_1, ax_1 = plt.subplots(1, 1, figsize = (5, 3.5))\n    \n    sns.lineplot(x = [1, 19], y = [27569, 10481], color = '#86b9cf',\n                 linewidth = 1)\n\n    sns.lineplot(x = Cluster_tuning['Cluster'].astype('int64'), \n                 y = Cluster_tuning['SSE'], color = '#146964', \n                 marker = 'o', linewidth = 1)\n    \n    plt.xticks(range(1, 20))\n          \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-01T08:17:59.417629Z","iopub.execute_input":"2021-12-01T08:17:59.417933Z","iopub.status.idle":"2021-12-01T08:17:59.747403Z","shell.execute_reply.started":"2021-12-01T08:17:59.417902Z","shell.execute_reply":"2021-12-01T08:17:59.746489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is evident that after 12 clusters a decrease in the SSE became insignificant.","metadata":{}},{"cell_type":"markdown","source":"Finally, I created a utility function for engineering features from <span style=\"color:#E85E40\">k-means clustering</span>:","metadata":{}},{"cell_type":"code","source":"def K_means_clastering(df_train, df_test, Vars_list, n_clusters = 10):\n    \n    ### Scaling data\n    \n    scaler = StandardScaler()\n    df_train_scaled = scaler.fit_transform(df_train[Vars_list])\n    df_test_scaled = scaler.transform(df_test[Vars_list])\n    \n    ### Initiating KMeans algorithm\n    \n    kmeans = KMeans(n_clusters = n_clusters, init = 'k-means++', random_state = 999)\n    \n    ### Getting clusters\n    \n    kmeans_train = kmeans.fit_predict(df_train_scaled)\n    kmeans_test = kmeans.predict(df_test_scaled)\n    \n    ### Getting the distance to the cluster centres\n    \n    Cluster_space = []\n    Cols = [f'Clust_space_{i}' for i in range(n_clusters)]\n    \n    Cluster_space.append(kmeans.fit_transform(df_train_scaled))\n    Cluster_space.append(kmeans.transform(df_test_scaled))\n    \n    ### Saving results\n    \n    df_clusters_train = pd.DataFrame(Cluster_space[0], columns = Cols)\n    df_clusters_train['Cluster'] = kmeans_train\n    \n    df_clusters_test = pd.DataFrame(Cluster_space[1], columns = Cols)\n    df_clusters_test['Cluster'] = kmeans_test\n    \n    return(df_clusters_train, df_clusters_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:40.532919Z","iopub.execute_input":"2021-12-01T08:14:40.533151Z","iopub.status.idle":"2021-12-01T08:14:40.541333Z","shell.execute_reply.started":"2021-12-01T08:14:40.533124Z","shell.execute_reply":"2021-12-01T08:14:40.540506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Features for only 5 clusters were calculated in order to keep the notebook less cluttered.","metadata":{}},{"cell_type":"code","source":"Result_1 = K_means_clastering(df_train, df_test, Num_vars[0:5], n_clusters = 5)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:40.542397Z","iopub.execute_input":"2021-12-01T08:14:40.542583Z","iopub.status.idle":"2021-12-01T08:14:40.748878Z","shell.execute_reply.started":"2021-12-01T08:14:40.542562Z","shell.execute_reply":"2021-12-01T08:14:40.748231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clusters_train = Result_1[0]\ndf_clusters_test = Result_1[1]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:40.750106Z","iopub.execute_input":"2021-12-01T08:14:40.750525Z","iopub.status.idle":"2021-12-01T08:14:40.754421Z","shell.execute_reply.started":"2021-12-01T08:14:40.750493Z","shell.execute_reply":"2021-12-01T08:14:40.753858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_clusters_train.round(2).head(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:40.755445Z","iopub.execute_input":"2021-12-01T08:14:40.756Z","iopub.status.idle":"2021-12-01T08:14:40.920948Z","shell.execute_reply.started":"2021-12-01T08:14:40.75597Z","shell.execute_reply":"2021-12-01T08:14:40.92016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. K-bins discretisation <a class=\"anchor\" id = \"II_2\"></a>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import KBinsDiscretizer","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:40.924758Z","iopub.execute_input":"2021-12-01T08:14:40.925268Z","iopub.status.idle":"2021-12-01T08:14:40.932839Z","shell.execute_reply.started":"2021-12-01T08:14:40.925231Z","shell.execute_reply":"2021-12-01T08:14:40.932115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Useful articles and documents:\n\n- <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html\">Sklearn documentation</a>","metadata":{}},{"cell_type":"markdown","source":"<div style = \"text-align: justify\"> Sometimes, binning continuous variables can be quite valuable. This technique allows you to deal with outliers and features that are not homogeneous, thus preventing overfitting. Nevertheless, binning always results in information loss. </div>","metadata":{}},{"cell_type":"markdown","source":"<div style = \"text-align: justify\"> If you want, you can change a strategy of binning. For instance, pass <code style = \"background-color: #faedde\">strategy = 'uniform'</code> instead of <code style = \"background-color: #faedde\">strategy = 'kmeans'</code>. Also, it is up to you to define how variables are going to be encoded. Use either <code style = \"background-color: #faedde\">encode = 'onehot-dense'</code> or <code style = \"background-color: #faedde\">encode = 'ordinal'</code>. To get more details, consult the official documentation. </div>","metadata":{}},{"cell_type":"code","source":"def discretizer(df_train, df_test, Vars_list, n_bins = 5, \n                encode = 'onehot-dense', strategy = 'kmeans'):\n    \n    ### Initiating KBinsDiscretizer algorithm\n    \n    KBins_d = KBinsDiscretizer(n_bins = n_bins, encode = encode, \n                               strategy = strategy)\n    \n    ### Getting binned variables\n    \n    df_train_binned = KBins_d.fit_transform(df_train[Vars_list])\n    df_test_binned = KBins_d.transform(df_test[Vars_list])\n    \n    ### NOT the best way of creating column names :)\n    \n    if encode == 'onehot-dense':\n        \n        Cols = df_train[Vars_list].shape[1] * n_bins\n        \n    else: Cols = df_train[Vars_list].shape[1]\n    \n    df_train_binned = pd.DataFrame(df_train_binned, \n                                   columns = [\"col\" + str(i) for i in range(0, Cols)])\n\n    df_test_binned = pd.DataFrame(df_test_binned, \n                                  columns = [\"col\" + str(i) for i in range(0, Cols)])\n    \n    return(df_train_binned, df_test_binned)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:40.937214Z","iopub.execute_input":"2021-12-01T08:14:40.937751Z","iopub.status.idle":"2021-12-01T08:14:40.95055Z","shell.execute_reply.started":"2021-12-01T08:14:40.937713Z","shell.execute_reply":"2021-12-01T08:14:40.949948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To make my example cleaner, I took only the first 5 numeric variables.","metadata":{}},{"cell_type":"code","source":"Result_2 = discretizer(df_train, df_test, Num_vars[0:5],\n                       n_bins = 3, encode = 'onehot-dense', strategy = 'kmeans')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:40.954591Z","iopub.execute_input":"2021-12-01T08:14:40.957164Z","iopub.status.idle":"2021-12-01T08:14:41.002536Z","shell.execute_reply.started":"2021-12-01T08:14:40.957124Z","shell.execute_reply":"2021-12-01T08:14:41.001852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_binned_train = Result_2[0]\ndf_binned_test = Result_2[1]","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:41.006809Z","iopub.execute_input":"2021-12-01T08:14:41.009204Z","iopub.status.idle":"2021-12-01T08:14:41.014513Z","shell.execute_reply.started":"2021-12-01T08:14:41.007236Z","shell.execute_reply":"2021-12-01T08:14:41.01392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_binned_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:41.018321Z","iopub.execute_input":"2021-12-01T08:14:41.020551Z","iopub.status.idle":"2021-12-01T08:14:41.045864Z","shell.execute_reply.started":"2021-12-01T08:14:41.018666Z","shell.execute_reply":"2021-12-01T08:14:41.045197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Categorical variables interactions <a class=\"anchor\" id = \"II_3\"></a>","metadata":{}},{"cell_type":"code","source":"from itertools import combinations","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:41.047358Z","iopub.execute_input":"2021-12-01T08:14:41.047785Z","iopub.status.idle":"2021-12-01T08:14:41.056841Z","shell.execute_reply.started":"2021-12-01T08:14:41.047748Z","shell.execute_reply":"2021-12-01T08:14:41.056123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Useful articles and documents:\n\n- <a href=\"https://www.coursera.org/lecture/competitive-data-science/feature-interactions-yt5t3\">Feature interactions</a>","metadata":{}},{"cell_type":"markdown","source":"First and foremost, you should create a list of categorical variables that you want to combine:","metadata":{}},{"cell_type":"code","source":"Cat_vars = ['ExterCond', 'MasVnrType', 'ExterQual']","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:41.122117Z","iopub.execute_input":"2021-12-01T08:14:41.124431Z","iopub.status.idle":"2021-12-01T08:14:41.130091Z","shell.execute_reply.started":"2021-12-01T08:14:41.12439Z","shell.execute_reply":"2021-12-01T08:14:41.129479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cat_var_combinations(df, Vars_list):\n\n    Comb_train = []\n    Cols = []\n    \n    for с_1, c_2 in combinations(df[Vars_list], 2):\n    \n        Comb_train.append(df[с_1].astype(str) + \" | \" + df[c_2].astype(str))\n    \n        Cols.append(str(с_1) + \" | \" + str(c_2))\n    \n        df_final = pd.DataFrame(Comb_train).T\n        df_final.columns = Cols\n        \n    return(df_final)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:41.134312Z","iopub.execute_input":"2021-12-01T08:14:41.137009Z","iopub.status.idle":"2021-12-01T08:14:41.146583Z","shell.execute_reply.started":"2021-12-01T08:14:41.136971Z","shell.execute_reply":"2021-12-01T08:14:41.145778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_comb_cat_train = cat_var_combinations(df_train, Cat_vars)\ndf_comb_cat_test = cat_var_combinations(df_test, Cat_vars)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:41.15371Z","iopub.execute_input":"2021-12-01T08:14:41.156577Z","iopub.status.idle":"2021-12-01T08:14:41.523923Z","shell.execute_reply.started":"2021-12-01T08:14:41.156523Z","shell.execute_reply":"2021-12-01T08:14:41.523219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_comb_cat_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:41.530678Z","iopub.execute_input":"2021-12-01T08:14:41.531221Z","iopub.status.idle":"2021-12-01T08:14:41.542561Z","shell.execute_reply.started":"2021-12-01T08:14:41.531179Z","shell.execute_reply":"2021-12-01T08:14:41.541786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, you can visualise new variables:","metadata":{}},{"cell_type":"code","source":"df_comb_cat_train['SalePrice'] = df_train['SalePrice']\n\nVisual_vars = df_comb_cat_train.columns.tolist()\nVisual_vars.remove('SalePrice')","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:41.543859Z","iopub.execute_input":"2021-12-01T08:14:41.544387Z","iopub.status.idle":"2021-12-01T08:14:41.550607Z","shell.execute_reply.started":"2021-12-01T08:14:41.544343Z","shell.execute_reply":"2021-12-01T08:14:41.549915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7.5, \n                          'xtick.labelsize': 5.5, 'ytick.labelsize': 5.5}):\n\n    fig_2, ax_2 = plt.subplots(3, 1, figsize = (8, 10))\n\n    for idx, (column, axes) in list(enumerate(zip(list(df_comb_cat_train.columns), ax_2.flatten()))):\n    \n        order = df_comb_cat_train.groupby(column)['SalePrice'].mean().sort_values(ascending = True).index\n    \n        sns.violinplot(ax = axes, x = df_comb_cat_train[column], \n                       y = np.log(df_comb_cat_train['SalePrice']),\n                       order = order, scale = 'width',\n                       linewidth = 0.5, palette = 'viridis',\n                       inner = None)\n    \n        plt.setp(axes.collections, alpha = 0.3)\n    \n        sns.stripplot(ax = axes, x = df_comb_cat_train[column], \n                      y = np.log(df_comb_cat_train['SalePrice']),\n                      palette = 'viridis', s = 1.5, alpha = 1,\n                      order = order, jitter = 0.2)\n        \n        sns.pointplot(ax = axes, x = df_comb_cat_train[column],\n                      y = np.log(df_comb_cat_train['SalePrice']),\n                      order = order,\n                      color = '#ff5736', scale = 0.2,\n                      estimator = np.mean, ci = 'sd',\n                      errwidth = 0.5, capsize = 0.15, join = True)\n    \n        plt.setp(axes.lines, zorder = 100)\n        plt.setp(axes.collections, zorder = 100)\n    \n        if df_comb_cat_train[column].nunique() > 5: \n        \n            plt.setp(axes.get_xticklabels(), rotation = 90)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax_2.flatten()[idx + 1:]]\n\nplt.tight_layout(pad = 1)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-01T08:31:58.274488Z","iopub.execute_input":"2021-12-01T08:31:58.275251Z","iopub.status.idle":"2021-12-01T08:32:01.40578Z","shell.execute_reply.started":"2021-12-01T08:31:58.275206Z","shell.execute_reply":"2021-12-01T08:32:01.404926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Featuretools <a class=\"anchor\" id = \"II_4\"></a>","metadata":{}},{"cell_type":"code","source":"import featuretools as ft","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:44.536221Z","iopub.execute_input":"2021-12-01T08:14:44.537268Z","iopub.status.idle":"2021-12-01T08:14:45.367504Z","shell.execute_reply.started":"2021-12-01T08:14:44.537227Z","shell.execute_reply":"2021-12-01T08:14:45.366631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Useful articles and documents:\n\n- <a href=\"https://featuretools.alteryx.com/en/stable/index.html\">Featuretools documentation</a>\n- <a href=\"https://www.kaggle.com/liananapalkova/automated-feature-engineering-for-titanic-dataset\">Automated feature engineering for Titanic dataset</a>","metadata":{}},{"cell_type":"markdown","source":"Well, <span style=\"color:#E85E40\"> featuretools </span> allows you to do a lot. In this notebook, I covered only feature engineering with the help of aggregations.","metadata":{}},{"cell_type":"markdown","source":"<div style = \"text-align: justify\"> First, I defined all variables that would participate in generating features <code style = \"background-color: #faedde\">['GrLivArea', 'LotArea', 'Neighborhood']</code>. Following that, I isolated a single categorical variable <code style = \"background-color: #faedde\">['Neighborhood']</code> that was used to aggregate the rest of the continuous variables.</div>","metadata":{}},{"cell_type":"code","source":"All_vars = ['GrLivArea', 'LotArea', 'Neighborhood']\n\nCat_vars_only = ['Neighborhood']","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:45.368413Z","iopub.execute_input":"2021-12-01T08:14:45.368624Z","iopub.status.idle":"2021-12-01T08:14:45.375271Z","shell.execute_reply.started":"2021-12-01T08:14:45.368601Z","shell.execute_reply":"2021-12-01T08:14:45.37357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def f_tools(df, Vars_list_0, Vars_list_1):\n    \n    df_ft = df[Vars_list_0]\n    df_ft['ID'] = list(range(0, df_ft.shape[0]))\n    \n    ### Loading data and creating an entity\n    \n    ES = ft.EntitySet(id = 'SalePrice_data')\n    ES = ES.entity_from_dataframe(entity_id = 'df_ft',                       \n                                  dataframe = df_ft, index = 'ID')\n    \n    ### Creating relationships\n    \n    for column in Vars_list_1:\n        \n        ES = ES.normalize_entity(base_entity_id = 'df_ft', \n                             new_entity_id = str(column), index = str(column))\n        \n    ### Creating features via aggregations\n    \n    features, feature_names = ft.dfs(entityset = ES,\n                                     target_entity = 'df_ft',\n                                     agg_primitives = ['max', 'min', 'mean'],\n                                     max_depth = 2)\n    \n    features = features.drop(Vars_list_0, axis = 1)\n    features = features.dropna(axis = 1)\n    \n    return(features)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:45.376609Z","iopub.execute_input":"2021-12-01T08:14:45.377102Z","iopub.status.idle":"2021-12-01T08:14:45.385562Z","shell.execute_reply.started":"2021-12-01T08:14:45.377049Z","shell.execute_reply":"2021-12-01T08:14:45.385048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style = \"text-align: justify\"> I used only 3 basic primitives <code style = \"background-color: #faedde\">agg_primitives = ['max', 'min', 'mean']</code>. If you want to learn more about other options, you should either type <code style = \"background-color: #faedde\">ft.primitives.list_primitives()</code> or consult this <a href=\"https://docs.featuretools.com/en/stable/api_reference.html#feature-primitives\">website</a>. </div>","metadata":{}},{"cell_type":"code","source":"df_ft_train = f_tools(df_train, All_vars, Cat_vars_only)\ndf_ft_test = f_tools(df_test, All_vars, Cat_vars_only)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:45.38691Z","iopub.execute_input":"2021-12-01T08:14:45.387403Z","iopub.status.idle":"2021-12-01T08:14:45.541473Z","shell.execute_reply.started":"2021-12-01T08:14:45.387366Z","shell.execute_reply":"2021-12-01T08:14:45.540714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's explore a particular variable:","metadata":{}},{"cell_type":"code","source":"df_ft_train[['Neighborhood.MAX(df_ft.LotArea)']].head(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:45.542399Z","iopub.execute_input":"2021-12-01T08:14:45.542593Z","iopub.status.idle":"2021-12-01T08:14:45.552847Z","shell.execute_reply.started":"2021-12-01T08:14:45.542571Z","shell.execute_reply":"2021-12-01T08:14:45.55211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style = \"text-align: justify\"> The name of this feature essentially speaks for itself. \"LotArea\" area was aggregated by \"Neighborhood\", and the aggregation function was the mean. We can get the same result with the help of <code style = \"background-color: #faedde\">.groupby()</code>:</div>","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({'Check': df_train.groupby('Neighborhood')['LotArea'].transform('max')}).head(3)","metadata":{"execution":{"iopub.status.busy":"2021-12-01T08:14:45.553847Z","iopub.execute_input":"2021-12-01T08:14:45.554261Z","iopub.status.idle":"2021-12-01T08:14:45.567009Z","shell.execute_reply.started":"2021-12-01T08:14:45.554224Z","shell.execute_reply":"2021-12-01T08:14:45.566126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can always plot new variables against the target and explore them more meticulously:","metadata":{}},{"cell_type":"code","source":"with plt.rc_context(rc = {'figure.dpi': 250, 'axes.labelsize': 6, \n                          'xtick.labelsize': 5, 'ytick.labelsize': 5}):\n\n    fig, ax = plt.subplots(2, 3, figsize = (6.5, 4.5), sharey = True)\n\n    for idx, (column, axes) in list(enumerate(zip(list(df_ft_train.columns), \n                                                  ax.flatten()))):\n    \n        sns.scatterplot(ax = axes, x = df_ft_train[column], \n                        y = np.log(df_train['SalePrice']),\n                        hue = np.log(df_train['SalePrice']),\n                        palette = 'viridis', alpha = 0.7, s = 7)\n    \n        axes.legend([], [], frameon = False)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n\n    plt.tight_layout(pad = 1)\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-01T08:17:08.467319Z","iopub.execute_input":"2021-12-01T08:17:08.467622Z","iopub.status.idle":"2021-12-01T08:17:10.642829Z","shell.execute_reply.started":"2021-12-01T08:17:08.467591Z","shell.execute_reply":"2021-12-01T08:17:10.642135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Thanks for reading!","metadata":{}}]}