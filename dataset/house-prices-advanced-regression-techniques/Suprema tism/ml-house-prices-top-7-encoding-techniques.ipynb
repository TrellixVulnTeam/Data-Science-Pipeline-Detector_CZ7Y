{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><center> Content </center></h1>","metadata":{}},{"cell_type":"markdown","source":"* [I. Getting started](#I)\n    * [1. Importing some basic libraries](#I_1)\n    * [2. Loading data](#I_2)\n    * [3. Preliminary cleaning](#I_2)\n\n\n* [II. Missing values](#II)\n    * [1. Visualising NaNs](#II_1)\n    * [2. Imputing NaN values. Training set](#II_2)\n    * [3. Imputing NaN values. Test set](#II_3)\n    \n   \n* [III. EDA](#III)\n    * [1. Visualising potential numeric variables](#III_1)\n    * [2. Visualising categorical variables](#III_2)\n\n\n* [IV. Feature engineering](#IV)\n    * [1. Dealing with outliers](#IV_1)\n    * [2. Adding some new variables](#IV_2)\n    * [3. Binning imbalanced features](#IV_3)\n    * [4. Transforming skewed variables](#IV_4)\n    * [5. Encoding variables](#IV_5)\n    * [6. Getting the final training and test sets](#IV_6)\n\n\n* [V. Building models](#V)\n    * [1. Tuning modelss](#V_1)\n    * [2. Stacking](#V_2)\n\n\n* [VI. Some techniques that could have been useful](#VI)\n    * [1. Feature interactions](#VI_1)","metadata":{}},{"cell_type":"markdown","source":"<h1><center> I. Getting started </center></h1> <a class=\"anchor\" id = \"I\"></a>","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing some basic libraries <a class=\"anchor\" id = \"I_1\"></a>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:46.148065Z","iopub.execute_input":"2021-12-06T19:31:46.14835Z","iopub.status.idle":"2021-12-06T19:31:46.15337Z","shell.execute_reply.started":"2021-12-06T19:31:46.14832Z","shell.execute_reply":"2021-12-06T19:31:46.152463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Loading data <a class=\"anchor\" id = \"I_2\"></a>","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:46.155358Z","iopub.execute_input":"2021-12-06T19:31:46.156028Z","iopub.status.idle":"2021-12-06T19:31:46.207176Z","shell.execute_reply.started":"2021-12-06T19:31:46.155972Z","shell.execute_reply":"2021-12-06T19:31:46.206308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Preliminary cleaning <a class=\"anchor\" id = \"I_3\"></a>","metadata":{}},{"cell_type":"markdown","source":"I want to delete \"Id\" columns, as they won't provide us with any useful information.","metadata":{}},{"cell_type":"code","source":"df_train = df_train.drop(columns = 'Id')\ndf_test = df_test.drop(columns = 'Id')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:46.208253Z","iopub.execute_input":"2021-12-06T19:31:46.208457Z","iopub.status.idle":"2021-12-06T19:31:46.215565Z","shell.execute_reply.started":"2021-12-06T19:31:46.208433Z","shell.execute_reply":"2021-12-06T19:31:46.215021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, let's make sure that columns that have a limited range of values don't have any obviously incorrect observations. To do so we can simply utilise the following function:","metadata":{}},{"cell_type":"code","source":"Check_years = df_train.columns[df_train.columns.str.contains(pat = 'Year|Yr')] ","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:46.216447Z","iopub.execute_input":"2021-12-06T19:31:46.21669Z","iopub.status.idle":"2021-12-06T19:31:46.231149Z","shell.execute_reply.started":"2021-12-06T19:31:46.216663Z","shell.execute_reply":"2021-12-06T19:31:46.230429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[Check_years.values].max().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:46.232998Z","iopub.execute_input":"2021-12-06T19:31:46.233385Z","iopub.status.idle":"2021-12-06T19:31:46.25074Z","shell.execute_reply.started":"2021-12-06T19:31:46.233343Z","shell.execute_reply":"2021-12-06T19:31:46.249754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[Check_years.values].max().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:46.252099Z","iopub.execute_input":"2021-12-06T19:31:46.252452Z","iopub.status.idle":"2021-12-06T19:31:46.269467Z","shell.execute_reply.started":"2021-12-06T19:31:46.252423Z","shell.execute_reply":"2021-12-06T19:31:46.268468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The year 2207 was replaced the mode of the column.","metadata":{}},{"cell_type":"code","source":"Replace_year = df_test.loc[(df_test['GarageYrBlt'] > 2050), 'GarageYrBlt'].index.tolist()\ndf_test.loc[Replace_year, 'GarageYrBlt'] = df_test['GarageYrBlt'].mode()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:46.271257Z","iopub.execute_input":"2021-12-06T19:31:46.271905Z","iopub.status.idle":"2021-12-06T19:31:46.284236Z","shell.execute_reply.started":"2021-12-06T19:31:46.271861Z","shell.execute_reply":"2021-12-06T19:31:46.283167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><center> II. Missing values </center></h1> <a class=\"anchor\" id = \"II\"></a>","metadata":{}},{"cell_type":"markdown","source":"## 1. Visualising NaNs <a class=\"anchor\" id = \"II_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"Before we can build any models or engineer some features, we have to deal with missing values. First of all, I created a visual representation of NaN values that helped me understand their structure.","metadata":{}},{"cell_type":"code","source":"train_missing = df_train.count().loc[df_train.count() < 1460].sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:46.285395Z","iopub.execute_input":"2021-12-06T19:31:46.285615Z","iopub.status.idle":"2021-12-06T19:31:46.310807Z","shell.execute_reply.started":"2021-12-06T19:31:46.285589Z","shell.execute_reply":"2021-12-06T19:31:46.310054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Setting some global parameters for all plots was done with the aid of <code style = \"background-color: #faedde\">sns.set_theme(rc = {})</code>.","metadata":{}},{"cell_type":"code","source":"sns.set_theme(rc = {'grid.linewidth': 0.6, 'grid.color': 'white',\n                    'axes.linewidth': 1, 'axes.facecolor': '#ECECEC', \n                    'axes.labelcolor': '#000000',\n                    'figure.facecolor': 'white',\n                    'xtick.color': '#000000', 'ytick.color': '#000000'})","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:46.312568Z","iopub.execute_input":"2021-12-06T19:31:46.31324Z","iopub.status.idle":"2021-12-06T19:31:46.320162Z","shell.execute_reply.started":"2021-12-06T19:31:46.313185Z","shell.execute_reply":"2021-12-06T19:31:46.318837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.rc_context(rc = {'figure.dpi': 120, 'axes.labelsize': 8.5, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}): \n\n    fig, ax = plt.subplots(1, 1, figsize = (6, 4))\n\n    sns.barplot(x = train_missing.values, y = train_missing.index, palette = 'viridis')\n\n    plt.xlabel('Non-Na values')\n\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T19:31:46.321103Z","iopub.execute_input":"2021-12-06T19:31:46.321633Z","iopub.status.idle":"2021-12-06T19:31:46.64853Z","shell.execute_reply.started":"2021-12-06T19:31:46.321603Z","shell.execute_reply":"2021-12-06T19:31:46.647571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_missing = df_test.count().loc[df_test.count() < 1459].sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:46.652039Z","iopub.execute_input":"2021-12-06T19:31:46.652381Z","iopub.status.idle":"2021-12-06T19:31:46.6712Z","shell.execute_reply.started":"2021-12-06T19:31:46.652339Z","shell.execute_reply":"2021-12-06T19:31:46.670043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.rc_context(rc = {'figure.dpi': 120, 'axes.labelsize': 8.5, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}):\n    \n    fig, ax = plt.subplots(1, 1, figsize = (7, 6))\n\n    sns.barplot(x = test_missing.values, y = test_missing.index, palette = 'viridis')\n\n    plt.xlabel('Non-Na values')\n\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T19:31:46.673033Z","iopub.execute_input":"2021-12-06T19:31:46.673474Z","iopub.status.idle":"2021-12-06T19:31:47.111891Z","shell.execute_reply.started":"2021-12-06T19:31:46.673433Z","shell.execute_reply":"2021-12-06T19:31:47.11078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on data description, we can conclude that NaN values in some columns are actually a category, namely \"Not present\". So, instead of dropping these columns, we can make them \"clean\".","metadata":{}},{"cell_type":"code","source":"None_category = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', \n                 'FireplaceQu', 'GarageCond', 'GarageQual', \n                 'GarageFinish', 'GarageType', 'BsmtCond', \n                 'BsmtExposure', 'BsmtQual', 'BsmtFinType1', \n                 'BsmtFinType2']","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.113094Z","iopub.execute_input":"2021-12-06T19:31:47.113375Z","iopub.status.idle":"2021-12-06T19:31:47.119034Z","shell.execute_reply.started":"2021-12-06T19:31:47.113345Z","shell.execute_reply":"2021-12-06T19:31:47.117693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in None_category:\n    \n    df_train.loc[df_train[column].isnull(), column] = 'None'\n    df_test.loc[df_test[column].isnull(), column] = 'None'","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.12098Z","iopub.execute_input":"2021-12-06T19:31:47.121405Z","iopub.status.idle":"2021-12-06T19:31:47.151664Z","shell.execute_reply.started":"2021-12-06T19:31:47.121363Z","shell.execute_reply":"2021-12-06T19:31:47.150811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Imputing NaN values. Training set <a class=\"anchor\" id = \"II_2\"></a>","metadata":{}},{"cell_type":"code","source":"df_train.loc[:, df_train.isna().sum() > 0].isna().sum().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.153288Z","iopub.execute_input":"2021-12-06T19:31:47.153578Z","iopub.status.idle":"2021-12-06T19:31:47.168037Z","shell.execute_reply.started":"2021-12-06T19:31:47.15354Z","shell.execute_reply":"2021-12-06T19:31:47.167169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used KNN imputer when the number of missing values was relatively large; however, when there were only few NaNs, I thought that replacing them with the mode or mean of a respective column was a reasonable choice.","metadata":{}},{"cell_type":"markdown","source":"### 2.1 \"LotFrontage\"","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.168975Z","iopub.execute_input":"2021-12-06T19:31:47.169598Z","iopub.status.idle":"2021-12-06T19:31:47.174551Z","shell.execute_reply.started":"2021-12-06T19:31:47.169566Z","shell.execute_reply":"2021-12-06T19:31:47.174056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Actually, separating variables by their type had been done before this step through analysing graphs; nevertheless, since EDA is never linear I decided to utilise some pieces of code that had been written beforehand.\n\nI also want to mention that I imputed NaN values in numeric columns using only numeric variables and categorical columns using only categorical variables.","metadata":{}},{"cell_type":"code","source":"cont_vars = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', \n             'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n             '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n             'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch']","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.175807Z","iopub.execute_input":"2021-12-06T19:31:47.176017Z","iopub.status.idle":"2021-12-06T19:31:47.186791Z","shell.execute_reply.started":"2021-12-06T19:31:47.175993Z","shell.execute_reply":"2021-12-06T19:31:47.186159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remember, it is important to scale your data before using KNN because this algorithm is distance-based. Robust scaler was used since outliers had not been dealt with yet.","metadata":{}},{"cell_type":"code","source":"knn_vars_train_cont = df_train[cont_vars].copy()\n\nScaler = RobustScaler()\n\nknn_vars_train_cont = pd.DataFrame(Scaler.fit_transform(knn_vars_train_cont), \n                                   columns = [\"col\" + str(i) for i in range(0, 15)])\n\ntrain_imp_cont = KNNImputer(n_neighbors = 5, weights = 'uniform', metric = 'nan_euclidean')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.187955Z","iopub.execute_input":"2021-12-06T19:31:47.188192Z","iopub.status.idle":"2021-12-06T19:31:47.209751Z","shell.execute_reply.started":"2021-12-06T19:31:47.188165Z","shell.execute_reply":"2021-12-06T19:31:47.208662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, don't forget to inverse transform your data.","metadata":{}},{"cell_type":"code","source":"train_imp_cont_results = train_imp_cont.fit_transform(knn_vars_train_cont)\n\ntrain_imp_cont_results = pd.DataFrame(Scaler.inverse_transform(train_imp_cont_results), \n                                      columns = [\"col\" + str(i) for i in range(0, 15)])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.210884Z","iopub.execute_input":"2021-12-06T19:31:47.211132Z","iopub.status.idle":"2021-12-06T19:31:47.262087Z","shell.execute_reply.started":"2021-12-06T19:31:47.211085Z","shell.execute_reply":"2021-12-06T19:31:47.261043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['LotFrontage'] = train_imp_cont_results['col0']\ndf_train['MasVnrArea'] = train_imp_cont_results['col2'].astype('float64')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.263586Z","iopub.execute_input":"2021-12-06T19:31:47.264213Z","iopub.status.idle":"2021-12-06T19:31:47.272207Z","shell.execute_reply.started":"2021-12-06T19:31:47.264161Z","shell.execute_reply":"2021-12-06T19:31:47.271211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Other variables (few missing values)","metadata":{}},{"cell_type":"code","source":"for column in ['MasVnrType', 'Electrical']:\n    \n    df_train.loc[df_train[column].isnull(), column] = df_train[column].mode()[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.273756Z","iopub.execute_input":"2021-12-06T19:31:47.274253Z","iopub.status.idle":"2021-12-06T19:31:47.291877Z","shell.execute_reply.started":"2021-12-06T19:31:47.274207Z","shell.execute_reply":"2021-12-06T19:31:47.290854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 \"GarageYrBlt\"","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.298488Z","iopub.execute_input":"2021-12-06T19:31:47.299523Z","iopub.status.idle":"2021-12-06T19:31:47.308719Z","shell.execute_reply.started":"2021-12-06T19:31:47.299464Z","shell.execute_reply":"2021-12-06T19:31:47.307768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Label encoder was used to transform categorical data before feeding it to KNN.","metadata":{}},{"cell_type":"code","source":"knn_vars_train_cat = df_train.drop(cont_vars, axis = 1)\nknn_vars_train_cat = knn_vars_train_cat.drop('SalePrice', axis = 1)\n\nobj_vars = knn_vars_train_cat.select_dtypes(include = ['object', 'category']).columns","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.310631Z","iopub.execute_input":"2021-12-06T19:31:47.313066Z","iopub.status.idle":"2021-12-06T19:31:47.345785Z","shell.execute_reply.started":"2021-12-06T19:31:47.313005Z","shell.execute_reply":"2021-12-06T19:31:47.344822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in obj_vars:\n    \n    knn_vars_train_cat[column] = LabelEncoder().fit_transform(knn_vars_train_cat[column])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.347794Z","iopub.execute_input":"2021-12-06T19:31:47.348474Z","iopub.status.idle":"2021-12-06T19:31:47.390934Z","shell.execute_reply.started":"2021-12-06T19:31:47.348424Z","shell.execute_reply":"2021-12-06T19:31:47.389915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_imp_cat = KNNImputer(n_neighbors = 5, weights = 'uniform', metric = 'nan_euclidean')\n\ntrain_imp_cat_results = train_imp_cat.fit_transform(knn_vars_train_cat)\n\ntrain_imp_cat_results = pd.DataFrame(train_imp_cat_results, \n                                     columns = [\"col\" + str(i) for i in range(0, 64)])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.392189Z","iopub.execute_input":"2021-12-06T19:31:47.392424Z","iopub.status.idle":"2021-12-06T19:31:47.434922Z","shell.execute_reply.started":"2021-12-06T19:31:47.392395Z","shell.execute_reply":"2021-12-06T19:31:47.433969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['GarageYrBlt'] = train_imp_cat_results['col48']\ndf_train['GarageYrBlt'] = df_train['GarageYrBlt'].astype('int64')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.437056Z","iopub.execute_input":"2021-12-06T19:31:47.437853Z","iopub.status.idle":"2021-12-06T19:31:47.446398Z","shell.execute_reply.started":"2021-12-06T19:31:47.4378Z","shell.execute_reply":"2021-12-06T19:31:47.445432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Imputing NaN values. Test set <a class=\"anchor\" id = \"II_3\"></a>","metadata":{}},{"cell_type":"markdown","source":"### 3.1 \"LotFrontage\"","metadata":{}},{"cell_type":"markdown","source":"The process was exactly the same for the test set.","metadata":{}},{"cell_type":"code","source":"knn_vars_test_cont = df_test[cont_vars].copy()\n\nScaler = StandardScaler()\n\nknn_vars_test_cont = pd.DataFrame(Scaler.fit_transform(knn_vars_test_cont), \n                                  columns = [\"col\" + str(i) for i in range(0, 15)])\n\ntest_imp_cont = KNNImputer(n_neighbors = 5, weights = 'uniform', metric = 'nan_euclidean')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.448681Z","iopub.execute_input":"2021-12-06T19:31:47.449694Z","iopub.status.idle":"2021-12-06T19:31:47.471363Z","shell.execute_reply.started":"2021-12-06T19:31:47.449476Z","shell.execute_reply":"2021-12-06T19:31:47.470301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imp_cont_results = test_imp_cont.fit_transform(knn_vars_test_cont)\n\ntest_imp_cont_results = pd.DataFrame(Scaler.inverse_transform(test_imp_cont_results), \n                                     columns = [\"col\" + str(i) for i in range(0, 15)])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.478331Z","iopub.execute_input":"2021-12-06T19:31:47.479171Z","iopub.status.idle":"2021-12-06T19:31:47.531982Z","shell.execute_reply.started":"2021-12-06T19:31:47.479114Z","shell.execute_reply":"2021-12-06T19:31:47.531003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['LotFrontage'] = test_imp_cont_results['col0']","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.534045Z","iopub.execute_input":"2021-12-06T19:31:47.534761Z","iopub.status.idle":"2021-12-06T19:31:47.541248Z","shell.execute_reply.started":"2021-12-06T19:31:47.534695Z","shell.execute_reply":"2021-12-06T19:31:47.540322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Other variables (few missing values)","metadata":{}},{"cell_type":"code","source":"for column in df_test.columns: \n    \n    if ((df_test[column].isnull().sum() <= 60) & (df_test[column].isnull().sum() > 0) & \n        ((df_test[column].dtypes == 'O') | (df_test[column].dtypes == 'float64')) & \n        (df_test[column].nunique() < 20)):\n        \n        df_test.loc[df_test[column].isnull(), column] = df_test[column].mode()[0]\n        \n    elif ((df_test[column].isnull().sum() <= 60) & (df_test[column].isnull().sum() > 0) & \n          (df_test[column].dtypes == 'float64') & (df_test[column].nunique() > 100)):\n        \n        df_test.loc[df_test[column].isnull(), column] = df_test[column].mean()\n        \n    else: pass","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.543221Z","iopub.execute_input":"2021-12-06T19:31:47.543894Z","iopub.status.idle":"2021-12-06T19:31:47.688566Z","shell.execute_reply.started":"2021-12-06T19:31:47.543847Z","shell.execute_reply":"2021-12-06T19:31:47.687963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 \"GarageYrBlt\"","metadata":{}},{"cell_type":"code","source":"knn_vars_test_cat = df_test.drop(cont_vars, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.689668Z","iopub.execute_input":"2021-12-06T19:31:47.68993Z","iopub.status.idle":"2021-12-06T19:31:47.69579Z","shell.execute_reply.started":"2021-12-06T19:31:47.689901Z","shell.execute_reply":"2021-12-06T19:31:47.694969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in knn_vars_test_cat:\n    \n    knn_vars_test_cat[column] = LabelEncoder().fit_transform(knn_vars_test_cat[column])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.696868Z","iopub.execute_input":"2021-12-06T19:31:47.697447Z","iopub.status.idle":"2021-12-06T19:31:47.755653Z","shell.execute_reply.started":"2021-12-06T19:31:47.697411Z","shell.execute_reply":"2021-12-06T19:31:47.754797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_imp_cat = KNNImputer(n_neighbors = 5, weights = 'uniform', metric = 'nan_euclidean')\n\ntest_imp_cat_results = test_imp_cat.fit_transform(knn_vars_test_cat)\n\ntest_imp_cat_results = pd.DataFrame(test_imp_cat_results, \n                                    columns = [\"col\" + str(i) for i in range(0, 64)])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.756913Z","iopub.execute_input":"2021-12-06T19:31:47.757213Z","iopub.status.idle":"2021-12-06T19:31:47.771356Z","shell.execute_reply.started":"2021-12-06T19:31:47.757176Z","shell.execute_reply":"2021-12-06T19:31:47.770647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['GarageYrBlt'] = test_imp_cat_results['col48']\ndf_test['GarageYrBlt'] = df_test['GarageYrBlt'].astype('int64')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.772496Z","iopub.execute_input":"2021-12-06T19:31:47.773298Z","iopub.status.idle":"2021-12-06T19:31:47.780765Z","shell.execute_reply.started":"2021-12-06T19:31:47.773268Z","shell.execute_reply":"2021-12-06T19:31:47.780061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we can check the number of NaN values left:","metadata":{}},{"cell_type":"code","source":"print(df_train.isna().sum().any(), df_test.isna().sum().any(), sep = '\\n')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.781807Z","iopub.execute_input":"2021-12-06T19:31:47.782167Z","iopub.status.idle":"2021-12-06T19:31:47.805356Z","shell.execute_reply.started":"2021-12-06T19:31:47.782127Z","shell.execute_reply":"2021-12-06T19:31:47.80468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><center> III. EDA </center></h1> <a class=\"anchor\" id = \"III\"></a>","metadata":{}},{"cell_type":"markdown","source":"For starters, we should separate variables by their type in order to figure out what columns are categorical and what are numeric, which is crucial for further analysis.","metadata":{}},{"cell_type":"code","source":"train_obj = df_train.select_dtypes(include = ['object', 'category']).columns\n\ntrain_int_float = df_train.select_dtypes(include = ['int64', 'float64'])\ncol_order = train_int_float.nunique().sort_values(ascending = False).index.tolist()\ntrain_int_float = train_int_float[col_order].columns","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:31:47.806259Z","iopub.execute_input":"2021-12-06T19:31:47.806818Z","iopub.status.idle":"2021-12-06T19:31:47.822653Z","shell.execute_reply.started":"2021-12-06T19:31:47.806782Z","shell.execute_reply":"2021-12-06T19:31:47.821997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Visualising potential numeric variables <a class=\"anchor\" id = \"III_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"<div style = \"color: #000000;\n             display: fill;\n             padding: 8px;\n             border-radius: 5px;\n             border-style: solid;\n             border-color: #a63700;\n             background-color: rgba(235, 125, 66, 0.3)\">\n    \n<span style = \"font-size: 20px; font-weight: bold\">Note:</span> \n<span style=\"font-size: 15px\">If you want to learn more about efficiently creating neat visualisations, please refer to this <a href=\"https://www.kaggle.com/suprematism/house-prices-advanced-visualisation\">notebook</a>.</span>\n</div>","metadata":{}},{"cell_type":"code","source":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7, \n                          'xtick.labelsize': 5, 'ytick.labelsize': 5}): \n\n    fig, ax = plt.subplots(5, 5, figsize = (8.5, 10), sharey = True)\n\n    for idx, (column, axes) in list(enumerate(zip(train_int_float[0:22], ax.flatten()))):\n    \n        sns.scatterplot(ax = axes, x = df_train[column], \n                        y = np.log(df_train['SalePrice']), \n                        hue =  np.log(df_train['SalePrice']), \n                        palette = 'viridis', alpha = 0.7, s = 8)\n    \n        axes.legend([], [], frameon = False)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n\n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T19:31:47.823847Z","iopub.execute_input":"2021-12-06T19:31:47.824723Z","iopub.status.idle":"2021-12-06T19:31:57.09445Z","shell.execute_reply.started":"2021-12-06T19:31:47.824667Z","shell.execute_reply":"2021-12-06T19:31:57.09379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7, \n                          'xtick.labelsize': 5, 'ytick.labelsize': 5}): \n\n    fig, ax = plt.subplots(5, 4, figsize = (8.5, 9), sharey = True)\n\n    for idx, (column, axes) in list(enumerate(zip(train_int_float[22:], ax.flatten()))):\n    \n        sns.scatterplot(ax = axes, x = df_train[column], \n                        y = np.log(df_train['SalePrice']), \n                        hue =  np.log(df_train['SalePrice']), \n                        palette = 'viridis', alpha = 0.7, s = 8)\n    \n        axes.legend([], [], frameon = False)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T19:31:57.095873Z","iopub.execute_input":"2021-12-06T19:31:57.096796Z","iopub.status.idle":"2021-12-06T19:32:03.427189Z","shell.execute_reply.started":"2021-12-06T19:31:57.096751Z","shell.execute_reply":"2021-12-06T19:32:03.425849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on graphs like the ones above, we can easily determine what variables are actually continous. In addition, I kept imbalanced predictors away from balanced ones.","metadata":{}},{"cell_type":"code","source":"train_cont_balanced = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', \n                       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n                       '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n                       'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch']\n\ntrain_cont_unbalanced = ['LowQualFinSF', '3SsnPorch' , 'PoolArea' , 'MiscVal']","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:03.428843Z","iopub.execute_input":"2021-12-06T19:32:03.429199Z","iopub.status.idle":"2021-12-06T19:32:03.434969Z","shell.execute_reply.started":"2021-12-06T19:32:03.429147Z","shell.execute_reply":"2021-12-06T19:32:03.43435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Visualising categorical variables <a class=\"anchor\" id = \"III_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"Let's get only categorical features:","metadata":{}},{"cell_type":"code","source":"train_cat = df_train.drop(train_cont_balanced, axis = 1).columns.tolist()\ntrain_cat.remove('SalePrice')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:03.435819Z","iopub.execute_input":"2021-12-06T19:32:03.436153Z","iopub.status.idle":"2021-12-06T19:32:03.447999Z","shell.execute_reply.started":"2021-12-06T19:32:03.436127Z","shell.execute_reply":"2021-12-06T19:32:03.447244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 High cardinality features","metadata":{}},{"cell_type":"markdown","source":"It is important to visualise variables with lots of categories, as you can observe whether they have some kind of relationship with the target. If yes, you should not drop them but encode properly.","metadata":{}},{"cell_type":"code","source":"df_train[train_cat].loc[:, df_train.nunique() > 25].nunique().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:03.449476Z","iopub.execute_input":"2021-12-06T19:32:03.449692Z","iopub.status.idle":"2021-12-06T19:32:03.477963Z","shell.execute_reply.started":"2021-12-06T19:32:03.449666Z","shell.execute_reply":"2021-12-06T19:32:03.477183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_high_cat = df_train[train_cat].loc[:, df_train.nunique() > 25].copy()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:03.479121Z","iopub.execute_input":"2021-12-06T19:32:03.479336Z","iopub.status.idle":"2021-12-06T19:32:03.500423Z","shell.execute_reply.started":"2021-12-06T19:32:03.47931Z","shell.execute_reply":"2021-12-06T19:32:03.499573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in train_high_cat.columns:\n    \n    train_high_cat[column] = train_high_cat[column].astype('category')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T19:32:03.501947Z","iopub.execute_input":"2021-12-06T19:32:03.502179Z","iopub.status.idle":"2021-12-06T19:32:03.509927Z","shell.execute_reply.started":"2021-12-06T19:32:03.502152Z","shell.execute_reply":"2021-12-06T19:32:03.509131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.rc_context(rc = {'figure.dpi': 450, 'axes.labelsize': 5, \n                          'xtick.labelsize': 4, 'ytick.labelsize': 4}): \n\n    fig, ax = plt.subplots(1, 3, figsize = (6, 7.5))\n\n    for idx, (column, axes) in list(enumerate(zip(train_high_cat.columns, ax.flatten()))): \n    \n        sns.stripplot(ax = axes, x = np.log(df_train['SalePrice']), \n                      y = train_high_cat[column], \n                      palette = 'viridis', alpha = 0.95, size = 1.5)\n\n        sns.boxplot(ax = axes, x = np.log(df_train['SalePrice']), \n                    y = train_high_cat[column],\n                    showmeans = True, meanline = True, zorder = 10,\n                    meanprops = {'color': 'r', 'linestyle': '-', 'lw': 0.8},\n                    medianprops = {'visible': False},\n                    whiskerprops = {'visible': False},\n                    showfliers = False, showbox = False, showcaps = False)\n        \n        sns.pointplot(ax = axes, x = np.log(df_train['SalePrice']), \n                      y = train_high_cat[column],\n                      ci = None, color = 'r', scale = 0.15)\n    \n    else: \n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T19:32:03.511334Z","iopub.execute_input":"2021-12-06T19:32:03.511531Z","iopub.status.idle":"2021-12-06T19:32:13.158628Z","shell.execute_reply.started":"2021-12-06T19:32:03.511508Z","shell.execute_reply":"2021-12-06T19:32:13.157721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Variables with manageable cardinality","metadata":{}},{"cell_type":"markdown","source":"For the sake of experimenting with <span style = \"color: #E85E40\"> matplotlib </span> and <span style = \"color: #E85E40\"> seaborn </span>, I plotted some categorical features. In my judgment, using stripplots with ordered by the target mean categories can be quite insightful. First and foremost, you can clearly see how many observations each category contains, which is vital if you want to isolate imbalanced features. On top of that, after ordering every category, a relationship (if present) of an independent variable with the target becomes evident.","metadata":{}},{"cell_type":"code","source":"train_norm_cat = df_train[train_cat].loc[:, df_train.nunique() <= 25].columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:13.159794Z","iopub.execute_input":"2021-12-06T19:32:13.160344Z","iopub.status.idle":"2021-12-06T19:32:13.183419Z","shell.execute_reply.started":"2021-12-06T19:32:13.160301Z","shell.execute_reply":"2021-12-06T19:32:13.182605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7, \n                          'xtick.labelsize': 5.5, 'ytick.labelsize': 5.5}): \n\n    fig, ax = plt.subplots(5, 3, figsize = (8, 13), sharey = True)\n\n    for idx, (column, axes) in list(enumerate(zip(train_norm_cat[: 15], ax.flatten()))):\n    \n        order = df_train.groupby(column)['SalePrice'].mean().sort_values(ascending = True).index\n    \n        sns.violinplot(ax = axes, x = df_train[column], \n                       y = np.log(df_train['SalePrice']),\n                       order = order, scale = 'width',\n                       linewidth = 0.3, palette = 'viridis',\n                       saturation = 0.5, inner = None)\n    \n        plt.setp(axes.collections, alpha = 0.3)\n    \n        sns.stripplot(ax = axes, x = df_train[column], \n                      y = np.log(df_train['SalePrice']),\n                      palette = 'viridis', s = 1.3, alpha = 0.9,\n                      order = order)\n    \n        sns.boxplot(ax = axes, x = df_train[column], order = order,\n                    y = np.log(df_train['SalePrice']),\n                    showmeans = True, meanline = True, zorder = 10,\n                    meanprops = {'color': 'r', 'linestyle': '--', 'lw': 0.6},\n                    medianprops = {'visible': False},\n                    whiskerprops = {'visible': False},\n                    showfliers = False, showbox = False, showcaps = False)\n        \n        if df_train[column].nunique() > 5: \n        \n            plt.setp(axes.get_xticklabels(), rotation = 90)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n\n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T19:32:13.184891Z","iopub.execute_input":"2021-12-06T19:32:13.185119Z","iopub.status.idle":"2021-12-06T19:32:20.096834Z","shell.execute_reply.started":"2021-12-06T19:32:13.185092Z","shell.execute_reply":"2021-12-06T19:32:20.096006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><center> IV. Feature engineering </center></h1> <a class=\"anchor\" id = \"IV\"></a>","metadata":{}},{"cell_type":"markdown","source":"## 1. Dealing with outliers <a class=\"anchor\" id = \"IV_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"How did I determine what observations were outliers? I simply run a Lasso model and collected the largest residuals. I didn't do it in this segment because all columns needed to be properly encoded (remember actual analysis is not linear while writing a notebook is).","metadata":{}},{"cell_type":"code","source":"indx_final = [30, 462, 495, 523, 588, 632, 968, 1298, 1324]","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:20.09836Z","iopub.execute_input":"2021-12-06T19:32:20.098598Z","iopub.status.idle":"2021-12-06T19:32:20.10744Z","shell.execute_reply.started":"2021-12-06T19:32:20.098569Z","shell.execute_reply":"2021-12-06T19:32:20.106482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.drop(indx_final, axis = 0).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:20.108675Z","iopub.execute_input":"2021-12-06T19:32:20.109479Z","iopub.status.idle":"2021-12-06T19:32:20.122797Z","shell.execute_reply.started":"2021-12-06T19:32:20.109438Z","shell.execute_reply":"2021-12-06T19:32:20.12183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here is an axample of how you can do it yourself:","metadata":{}},{"cell_type":"code","source":"##### Training a model #####\n\n# Lasso_outliers = linear_model.Lasso(alpha = 0.0005)\n\n# Lasso_fit = Lasso_outliers.fit(X_train, y)\n\n##### Getting outliers #####\n\n# rows_to_drop = (Lasso_fit.predict(X_train) - df_train['SalePrice'])**2\n# rows_to_drop[rows_to_drop > 0.2].index","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:20.124498Z","iopub.execute_input":"2021-12-06T19:32:20.124826Z","iopub.status.idle":"2021-12-06T19:32:20.132977Z","shell.execute_reply.started":"2021-12-06T19:32:20.124777Z","shell.execute_reply":"2021-12-06T19:32:20.132011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before setting a threshold, try plotting residuals. It can help a lot.","metadata":{}},{"cell_type":"markdown","source":"## 2. Adding some new variables <a class=\"anchor\" id = \"IV_2\"></a>","metadata":{}},{"cell_type":"markdown","source":"I added some variables that made sense to me. For instance, I calculated the total number of rooms (kitchens, bathrooms and other rooms).","metadata":{}},{"cell_type":"code","source":"df_train['TotalPorch'] = (df_train['ScreenPorch'] + df_train['EnclosedPorch'] + \n                          df_train['3SsnPorch'] + df_train['ScreenPorch'])\n\ndf_train['Rooms_kitchens'] = (df_train['TotRmsAbvGrd'] + df_train['BsmtFullBath'] + \n                              df_train['BsmtHalfBath'] + df_train['FullBath'] + \n                              df_train['HalfBath'])\n\ndf_train['Sqr_feet_per_room'] = ((df_train['1stFlrSF'] + \n                                  df_train['2ndFlrSF']) / df_train['TotRmsAbvGrd'])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:20.134487Z","iopub.execute_input":"2021-12-06T19:32:20.135024Z","iopub.status.idle":"2021-12-06T19:32:20.154354Z","shell.execute_reply.started":"2021-12-06T19:32:20.134981Z","shell.execute_reply":"2021-12-06T19:32:20.153344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cont_balanced.append('TotalPorch')\ntrain_cont_balanced.append('Sqr_feet_per_room')","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:20.155605Z","iopub.execute_input":"2021-12-06T19:32:20.155994Z","iopub.status.idle":"2021-12-06T19:32:20.167883Z","shell.execute_reply.started":"2021-12-06T19:32:20.155962Z","shell.execute_reply":"2021-12-06T19:32:20.16684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test['TotalPorch'] = (df_test['ScreenPorch'] + df_test['EnclosedPorch'] + \n                         df_test['3SsnPorch'] + df_test['ScreenPorch'])\n\ndf_test['Rooms_kitchens'] = (df_test['TotRmsAbvGrd'] + df_test['BsmtFullBath'] + \n                             df_test['BsmtHalfBath'] + df_test['FullBath'] + \n                             df_test['HalfBath'])\n\ndf_test['Sqr_feet_per_room'] = ((df_test['1stFlrSF'] + \n                                 df_test['2ndFlrSF']) / df_test['TotRmsAbvGrd'])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:20.169404Z","iopub.execute_input":"2021-12-06T19:32:20.169843Z","iopub.status.idle":"2021-12-06T19:32:20.185879Z","shell.execute_reply.started":"2021-12-06T19:32:20.169807Z","shell.execute_reply":"2021-12-06T19:32:20.184992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Binning imbalanced features <a class=\"anchor\" id = \"IV_3\"></a>","metadata":{}},{"cell_type":"markdown","source":"Imbalanced numeric variables were determined at the very beginning.","metadata":{}},{"cell_type":"code","source":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6,\n                          'legend.fontsize': 6, 'legend.title_fontsize': 6}): \n\n    fig, ax = plt.subplots(1, 4, figsize = (8, 3), sharey = True)\n\n    for idx, (column, axes) in list(enumerate(zip(train_cont_unbalanced, ax.flatten()))):\n    \n        sns.scatterplot(ax = axes, x = df_train[column], \n                        y = np.log(df_train['SalePrice']), \n                        hue = np.log(df_train['SalePrice']), \n                        palette = 'viridis', alpha = 0.8, s = 9)\n\n    axes_legend = ax.flatten()\n\n    axes_legend[0].legend(title = 'SalePrice', loc = 'lower right')\n    axes_legend[1].legend(title = 'SalePrice', loc = 'lower right')\n    axes_legend[3].legend(title = 'SalePrice', loc = 'lower right')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T19:32:20.187458Z","iopub.execute_input":"2021-12-06T19:32:20.187979Z","iopub.status.idle":"2021-12-06T19:32:22.161949Z","shell.execute_reply.started":"2021-12-06T19:32:20.187933Z","shell.execute_reply":"2021-12-06T19:32:22.161052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in train_cont_unbalanced:\n    \n    df_train.loc[(df_train[column] == 0), column] = 'None' \n    \n    df_train.loc[(df_train[column] != 0) & (df_train[column] != 'None'), column] = 'Present'","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:22.163406Z","iopub.execute_input":"2021-12-06T19:32:22.163672Z","iopub.status.idle":"2021-12-06T19:32:22.176471Z","shell.execute_reply.started":"2021-12-06T19:32:22.163645Z","shell.execute_reply":"2021-12-06T19:32:22.175832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in train_cont_unbalanced:\n    \n    df_test.loc[(df_test[column] == 0), column] = 'None' \n    \n    df_test.loc[(df_test[column] != 0) & (df_test[column] != 'None'), column] = 'Present'","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:22.178133Z","iopub.execute_input":"2021-12-06T19:32:22.178383Z","iopub.status.idle":"2021-12-06T19:32:22.193603Z","shell.execute_reply.started":"2021-12-06T19:32:22.178354Z","shell.execute_reply":"2021-12-06T19:32:22.192629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Transforming skewed variables <a class=\"anchor\" id = \"IV_4\"></a>","metadata":{}},{"cell_type":"code","source":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 7, \n                          'xtick.labelsize': 5, 'ytick.labelsize': 5}):\n    \n    fig, ax = plt.subplots(5, 4, figsize = (8.5, 9))\n\n    for idx, (column, axes) in list(enumerate(zip(train_cont_balanced, ax.flatten()))):\n    \n        sns.kdeplot(ax = axes, x = df_train[column], \n                    fill = True, alpha = 0.2, color = '#006e7a',\n                    linewidth = 0.8)\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax.flatten()[idx + 1:]]\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-06T19:32:22.194998Z","iopub.execute_input":"2021-12-06T19:32:22.195467Z","iopub.status.idle":"2021-12-06T19:32:27.191743Z","shell.execute_reply.started":"2021-12-06T19:32:22.195424Z","shell.execute_reply":"2021-12-06T19:32:27.190871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As far as I can tell, making features look more \"normal\" is not the number one priority, especially if we take into account that we are not doing statistical analysis. We care about how accurate our models are. Thus, I simply used log transformation.","metadata":{}},{"cell_type":"code","source":"df_train[train_cont_balanced] = np.log(df_train[train_cont_balanced] + 1)\ndf_test[train_cont_balanced] = np.log(df_test[train_cont_balanced] + 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:32:27.192973Z","iopub.execute_input":"2021-12-06T19:32:27.193364Z","iopub.status.idle":"2021-12-06T19:32:27.218372Z","shell.execute_reply.started":"2021-12-06T19:32:27.193321Z","shell.execute_reply":"2021-12-06T19:32:27.217731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Encoding variables <a class=\"anchor\" id = \"IV_5\"></a>","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Mean encoding","metadata":{}},{"cell_type":"markdown","source":"High cardinality features were encoded via mean encoding with cross validation and regularisation, which are a must if you want to prevent overfitting.\n\nI used a piece of code from this excellent notebook:\n\nhttps://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n\nI can also recommend you two great videos that cover the idea of mean encoding and various regularisation techniques: \n\nhttps://www.coursera.org/lecture/competitive-data-science/concept-of-mean-encoding-b5Gxv\n\nhttps://www.coursera.org/lecture/competitive-data-science/regularization-LGYQ2","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.058288Z","iopub.execute_input":"2021-12-06T19:28:20.059057Z","iopub.status.idle":"2021-12-06T19:28:20.062338Z","shell.execute_reply.started":"2021-12-06T19:28:20.059024Z","shell.execute_reply":"2021-12-06T19:28:20.061755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mean_encode(train_data, test_data, columns, target_col, alpha = 0, folds = 1):\n    encoded_cols = []\n    target_mean_global = train_data[target_col].mean()\n    for col in columns:\n        # Getting means for test data\n        nrows_cat = train_data.groupby(col)[target_col].count()\n        target_means_cats = train_data.groupby(col)[target_col].mean()\n        target_means_cats_adj = (target_means_cats*nrows_cat + \n                                 target_mean_global*alpha)/(nrows_cat+alpha)\n        # Mapping means to test data\n        encoded_col_test = test_data[col].map(target_means_cats_adj)\n        # Getting a train encodings\n        kfold = KFold(folds, shuffle=True, random_state=1).split(train_data[target_col].values)\n        parts = []\n        \n        for tr_in, val_ind in kfold:\n            # divide data\n            df_for_estimation, df_estimated = train_data.iloc[tr_in], train_data.iloc[val_ind]\n            # getting means on data for estimation (all folds except estimated)\n            nrows_cat = df_for_estimation.groupby(col)[target_col].count()\n            target_means_cats = df_for_estimation.groupby(col)[target_col].mean()\n            target_means_cats_adj = (target_means_cats*nrows_cat + \n                                         target_mean_global*alpha)/(nrows_cat+alpha)\n            # Mapping means to estimated fold\n            encoded_col_train_part = df_estimated[col].map(target_means_cats_adj)\n \n            # Saving estimated encodings for a fold\n            parts.append(encoded_col_train_part)\n            encoded_col_train = pd.concat(parts, axis = 0)\n            encoded_col_train.fillna(target_mean_global, inplace = True)\n\n        # Saving the column with means\n        encoded_col = pd.concat([encoded_col_train, encoded_col_test], axis = 0)\n        encoded_col[encoded_col.isnull()] = target_mean_global\n        encoded_cols.append(pd.DataFrame({'mean_'+ target_col + '_' + col:encoded_col}))\n    all_encoded = pd.concat(encoded_cols, axis = 1)\n    return (all_encoded.loc[train_data.index,:], \n            all_encoded.loc[test_data.index,:])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.063439Z","iopub.execute_input":"2021-12-06T19:28:20.063899Z","iopub.status.idle":"2021-12-06T19:28:20.076411Z","shell.execute_reply.started":"2021-12-06T19:28:20.063871Z","shell.execute_reply":"2021-12-06T19:28:20.075333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style = \"color: #000000;\n             display: fill;\n             padding: 8px;\n             border-radius: 5px;\n             border-style: solid;\n             border-color: #a63700;\n             background-color: rgba(235, 125, 66, 0.3)\">\n    \n<span style = \"font-size: 20px; font-weight: bold\">Note:</span> \n<span style=\"font-size: 15px\">This function works properly only if training and test sets have different indices.</span>\n</div>","metadata":{}},{"cell_type":"code","source":"train_mean_encoding = df_train[list(train_high_cat.columns)].copy()\ntrain_mean_encoding['SalePrice'] = df_train['SalePrice']\n\ntarget_col = 'SalePrice'\ncolumns = train_mean_encoding.columns.tolist()\n\ncolumns_test = columns\ncolumns_test.remove('SalePrice')\ntest_mean_encoding = df_test[columns_test]\n\nindex_0 = list(range(0, 1459))\nindex_1 = list(range(1451, 2910))\n\ntest_mean_encoding = test_mean_encoding.rename(index = dict(zip(index_0, index_1)))","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.077855Z","iopub.execute_input":"2021-12-06T19:28:20.078751Z","iopub.status.idle":"2021-12-06T19:28:20.101479Z","shell.execute_reply.started":"2021-12-06T19:28:20.078686Z","shell.execute_reply":"2021-12-06T19:28:20.100455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Mean_encoding = mean_encode(train_mean_encoding, test_mean_encoding, \n                            columns, target_col, alpha = 5, folds = 10)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.102998Z","iopub.execute_input":"2021-12-06T19:28:20.103794Z","iopub.status.idle":"2021-12-06T19:28:20.21435Z","shell.execute_reply.started":"2021-12-06T19:28:20.103753Z","shell.execute_reply":"2021-12-06T19:28:20.213387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_high_cat_encoded = np.log(Mean_encoding[0].reset_index(drop = True))\ntest_high_cat_encoded = np.log(Mean_encoding[1].reset_index(drop = True))","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.215542Z","iopub.execute_input":"2021-12-06T19:28:20.215911Z","iopub.status.idle":"2021-12-06T19:28:20.222033Z","shell.execute_reply.started":"2021-12-06T19:28:20.215868Z","shell.execute_reply":"2021-12-06T19:28:20.221029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 One-hot encoding","metadata":{}},{"cell_type":"markdown","source":"The rest of categorical variables were encoded with the help of one-hot encoding.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.223329Z","iopub.execute_input":"2021-12-06T19:28:20.223569Z","iopub.status.idle":"2021-12-06T19:28:20.235037Z","shell.execute_reply.started":"2021-12-06T19:28:20.223538Z","shell.execute_reply":"2021-12-06T19:28:20.234191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_norm_cat = pd.concat([df_train[train_norm_cat], \n                                 df_test[train_norm_cat]], \n                                 axis = 0, join = 'outer', \n                                 ignore_index = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.236391Z","iopub.execute_input":"2021-12-06T19:28:20.236906Z","iopub.status.idle":"2021-12-06T19:28:20.2568Z","shell.execute_reply.started":"2021-12-06T19:28:20.236863Z","shell.execute_reply":"2021-12-06T19:28:20.256081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OHE =  OneHotEncoder(sparse = False, handle_unknown = 'ignore')\n\ntrain_test_norm_cat_OHE = pd.DataFrame(pd.DataFrame(OHE.fit_transform(train_test_norm_cat)))\ntrain_test_norm_cat_OHE.columns = OHE.get_feature_names(train_test_norm_cat.columns.tolist())","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.258482Z","iopub.execute_input":"2021-12-06T19:28:20.258925Z","iopub.status.idle":"2021-12-06T19:28:20.340351Z","shell.execute_reply.started":"2021-12-06T19:28:20.258885Z","shell.execute_reply":"2021-12-06T19:28:20.339344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this stage, you can drop columns that almost entirely consist of a single class.","metadata":{}},{"cell_type":"code","source":"NULLS = pd.DataFrame({'%_nulls': train_test_norm_cat_OHE.isin([0]).mean()})\nNULLS = NULLS.reset_index().sort_values(ascending = False, by = '%_nulls')\nNULLS = NULLS.rename(columns = {'index': 'Variable'})\n\nDROP = NULLS.loc[((NULLS['%_nulls'] >= 0.99) | (NULLS['%_nulls'] <= 0.005)), 'Variable'].values","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.341492Z","iopub.execute_input":"2021-12-06T19:28:20.342262Z","iopub.status.idle":"2021-12-06T19:28:20.358473Z","shell.execute_reply.started":"2021-12-06T19:28:20.342216Z","shell.execute_reply":"2021-12-06T19:28:20.357208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_test_norm_cat_OHE = train_test_norm_cat_OHE.drop(DROP, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.35965Z","iopub.execute_input":"2021-12-06T19:28:20.359927Z","iopub.status.idle":"2021-12-06T19:28:20.383307Z","shell.execute_reply.started":"2021-12-06T19:28:20.359898Z","shell.execute_reply":"2021-12-06T19:28:20.382393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_norm_cat_OHE = train_test_norm_cat_OHE.iloc[:1451, ]\ntest_norm_cat_OHE = (train_test_norm_cat_OHE.iloc[1451:, ]).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.38469Z","iopub.execute_input":"2021-12-06T19:28:20.384967Z","iopub.status.idle":"2021-12-06T19:28:20.400001Z","shell.execute_reply.started":"2021-12-06T19:28:20.384936Z","shell.execute_reply":"2021-12-06T19:28:20.399195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Ordinal encoding","metadata":{}},{"cell_type":"markdown","source":"Playing around with different encoding techniques, I found out that \"OverallQual\" and \"OverallCond\" significantly boosted CV scores when they were encoded ordinally. But I decided to do both: keep them ordinal and one-hot encode them, allowing models to make all difficult choices for themselves.","metadata":{}},{"cell_type":"code","source":"train_ordinal = pd.DataFrame()\ntest_ordinal = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.401441Z","iopub.execute_input":"2021-12-06T19:28:20.401733Z","iopub.status.idle":"2021-12-06T19:28:20.409603Z","shell.execute_reply.started":"2021-12-06T19:28:20.401681Z","shell.execute_reply":"2021-12-06T19:28:20.408977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ordinal['OverallQual'] = df_train['OverallQual']\ntrain_ordinal['OverallCond'] = df_train['OverallCond']","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.410634Z","iopub.execute_input":"2021-12-06T19:28:20.411394Z","iopub.status.idle":"2021-12-06T19:28:20.425375Z","shell.execute_reply.started":"2021-12-06T19:28:20.411353Z","shell.execute_reply":"2021-12-06T19:28:20.424687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ordinal['OverallQual'] = df_test['OverallQual']\ntest_ordinal['OverallCond'] = df_test['OverallCond']","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.426326Z","iopub.execute_input":"2021-12-06T19:28:20.426872Z","iopub.status.idle":"2021-12-06T19:28:20.438203Z","shell.execute_reply.started":"2021-12-06T19:28:20.42684Z","shell.execute_reply":"2021-12-06T19:28:20.437603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Getting the final training and test sets <a class=\"anchor\" id = \"IV_6\"></a>","metadata":{}},{"cell_type":"code","source":"train_cont_balanced_default = df_train[train_cont_balanced].copy()\ntest_cont_balanced_default = df_test[train_cont_balanced].copy()","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.439115Z","iopub.execute_input":"2021-12-06T19:28:20.439765Z","iopub.status.idle":"2021-12-06T19:28:20.454405Z","shell.execute_reply.started":"2021-12-06T19:28:20.439722Z","shell.execute_reply":"2021-12-06T19:28:20.453431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_list = [train_high_cat_encoded, train_norm_cat_OHE,\n              train_cont_balanced_default, train_ordinal]","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.455507Z","iopub.execute_input":"2021-12-06T19:28:20.456301Z","iopub.status.idle":"2021-12-06T19:28:20.464836Z","shell.execute_reply.started":"2021-12-06T19:28:20.456267Z","shell.execute_reply":"2021-12-06T19:28:20.463991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.concat(train_list, axis = 1)\ny = np.log(df_train['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.466254Z","iopub.execute_input":"2021-12-06T19:28:20.466714Z","iopub.status.idle":"2021-12-06T19:28:20.480855Z","shell.execute_reply.started":"2021-12-06T19:28:20.466668Z","shell.execute_reply":"2021-12-06T19:28:20.47991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_list = [test_high_cat_encoded, test_norm_cat_OHE,\n             test_cont_balanced_default, test_ordinal]","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.482399Z","iopub.execute_input":"2021-12-06T19:28:20.482639Z","iopub.status.idle":"2021-12-06T19:28:20.487338Z","shell.execute_reply.started":"2021-12-06T19:28:20.482611Z","shell.execute_reply":"2021-12-06T19:28:20.486733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = pd.concat(test_list, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-12-06T19:28:20.488266Z","iopub.execute_input":"2021-12-06T19:28:20.488927Z","iopub.status.idle":"2021-12-06T19:28:20.50167Z","shell.execute_reply.started":"2021-12-06T19:28:20.488894Z","shell.execute_reply":"2021-12-06T19:28:20.501024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><center> V. Building models </center></h1> <a class=\"anchor\" id = \"V\"></a>","metadata":{}},{"cell_type":"markdown","source":"I built 6 models: **Lasso**, **ElasticNet**, **XGB**, **LGBM**, **SVR** and **KNN**, and then stacked them using <span style = \"color: #E85E40\"> StackingRegressor </span>. Since tuning hyperparameters took me about 2 hours (XGB was quite slow), I run only the final (tuned) models in this notebook, but you can still see how they were tuned. I mostly relied on <span style = \"color: #E85E40\"> RandomizedSearchCV </span>.","metadata":{}},{"cell_type":"markdown","source":"## 1. Tuning models <a class=\"anchor\" id = \"V_1\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold\n\nCV = KFold(n_splits = 10, random_state = 999, shuffle = True)\nCV_rep = RepeatedKFold(n_splits = 10, n_repeats = 3, random_state = 999)\n\n# 1.1 Lasso\n\nfrom sklearn import linear_model\n\n###### Training a model ######\n\n# %%time\n\n# Lasso_model = linear_model.Lasso()\n\n# alpha = {'alpha': [x / 25000 for x in range(1, 50, 1)],\n#          'tol': [0.0000001], \n#          'max_iter': [3000]}\n\n# Lasso_grid = GridSearchCV(Lasso_model, alpha, verbose = True, \n#                           scoring = 'neg_root_mean_squared_error', \n#                           n_jobs = 7, cv = CV)\n\n# Lasso_fit = Lasso_grid.fit(X_train, y)\n\n###### Getting scores and parameters ######\n\n# round(-1*Lasso_fit.best_score_, 5)\n# Lasso_fit.best_params_\n\n###### Getting feature importance ######\n\n# FI_lasso = list(zip(abs(Lasso_fit.best_estimator_.coef_), X_train.columns))\n# FI_lasso = pd.DataFrame(FI_lasso, columns = ['Imp', 'Variable'])\n# FI_lasso = FI_lasso.sort_values(ascending = False, by = 'Imp')\n\n########################################\n\n# 1.2 Elastic Net\n\n###### Training a model ######\n\n# %%time\n\n# ElasticNet_model = linear_model.ElasticNet()\n\n# alpha_l1 = {'alpha': [x / 25000 for x in range(1, 25, 1)],\n#             'l1_ratio': [x / 100 for x in range(10, 100, 1)],\n#             'tol': [0.000001], \n#             'max_iter': [4000]}\n\n# ElasticNet_random = RandomizedSearchCV(ElasticNet_model, alpha_l1, verbose = True, \n#                                        scoring = 'neg_root_mean_squared_error', \n#                                        n_jobs = 7, cv = CV, n_iter = 50)\n\n# ElasticNet_fit = ElasticNet_random.fit(X_train, y)\n\n###### Getting scores and parameters ######\n\n# round(-1*ElasticNet_fit.best_score_, 5)\n# ElasticNet_fit.best_params_\n\n########################################\n\n# 1.3 XGBoost\n\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X_train, y, \n                                   test_size = 0.1, random_state = 999, \n                                   shuffle = True)\n\n###### Training a model ######\n\n# %%time\n\n# XGB_model = xgb.XGBRegressor(use_label_encoder = False, \n#                              eval_metric = 'rmse', \n#                              n_estimators = 10000)\n\n# XGB_param_Random = {'reg_alpha': [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 3],\n#                     'reg_lambda': [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 3],\n#                     'learning_rate': [x / 400 for x in range(1, 10, 1)],\n#                     'max_depth': list(range(2, 15, 1)),\n#                     'min_child_weight': list(range(2, 35, 1)),\n#                     'gamma': [x / 200 for x in range(0, 50, 1)],\n#                     'subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n#                     'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9]}\n\n# XGB_random_grid = RandomizedSearchCV(XGB_model, XGB_param_Random, cv = CV, \n#                                      verbose = False, n_jobs = 7, \n#                                      scoring = 'neg_root_mean_squared_error', \n#                                      n_iter = 65)\n\n# XGB_fit = XGB_random_grid.fit(x_train, y_train, \n#                               early_stopping_rounds = 200, \n#                               eval_set = [[x_test, y_test]], \n#                               eval_metric = 'rmse', verbose = False)\n\n###### Getting scores and parameters ######\n\n# round(-1*XGB_fit.best_score_, 5)\n# XGB_fit.best_params_\n\n########################################\n\n# 1.4 LGBM\n\nimport lightgbm as lgb\n\nfrom scipy.stats import randint\nfrom scipy.stats import uniform\n\n###### Training a model ######\n\n# %%time\n\n# LGBM_model = lgb.LGBMRegressor(n_estimators = 10000)\n\n# LGBM_param_Random = {'reg_lambda': [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2],\n#                      'reg_alpha': [0.01, 0.05, 0.1, 0.2, 0.5, 1, 2],\n#                      'min_child_samples': randint(1, 100),\n#                      'subsample': [x / 10 for x in range(1, 10, 1)], # bagging_fraction\n#                      'subsample_freq': randint(1, 200), # bagging_freq\n#                      'num_leaves': randint(1, 200),\n#                      'max_depth': list(range(1, 15, 1)),\n#                      'max_bin': randint(1, 700),\n#                      'learning_rate': [x / 200 for x in range(1, 10, 1)],\n#                      'colsample_bytree': [x / 10 for x in range(1, 11, 1)]} # feature_fraction \n                        \n                    \n# LGBM_random_grid = RandomizedSearchCV(LGBM_model, LGBM_param_Random, cv = CV, \n#                                       verbose = False, n_jobs = 7, \n#                                       scoring = 'neg_root_mean_squared_error', n_iter = 100)\n\n# LGBM_fit = LGBM_random_grid.fit(x_train, y_train, early_stopping_rounds = 100, \n#                                 eval_set = [[x_test, y_test]], \n#                                 eval_metric = 'rmse', verbose = False)\n\n###### Getting scores and parameters ######\n\n# round(-1*LGBM_fit.best_score_, 5)\n# LGBM_fit.best_params_\n\n########################################\n\n# 1.5 SVR\n\n# Before using KNN or SVR, we have to scale data!\n\nfrom sklearn.preprocessing import RobustScaler\n\nvars_for_scaling = (train_high_cat_encoded.columns.tolist() + \n                   train_cont_balanced_default.columns.tolist())\n\nScaler = RobustScaler()\n\nX_train_scaled = X_train.copy()\nX_test_scaled = X_test.copy()\n\nfor column in vars_for_scaling:\n    \n    X_train_scaled[column] = Scaler.fit_transform(X_train[[column]])\n    X_test_scaled[column] = Scaler.fit_transform(X_test[[column]])\n    \nfrom sklearn.svm import SVR\n\n###### Training a model ######\n\n# %%time\n\n# SVR_model = SVR()\n\n# parameters = {'kernel' : ['rbf'],\n#               'C' : list(range(1, 100, 1)),\n#               'epsilon' : [x / 2000 for x in range(1, 50, 1)],\n#               'gamma' : [x / 10000 for x in range(1, 50, 1)]}\n\n# SVR_random_grid = RandomizedSearchCV(SVR_model, parameters, cv = CV, \n#                                      verbose = False, n_jobs = 7, \n#                                      scoring = 'neg_root_mean_squared_error', \n#                                      n_iter = 60)\n\n# SVR_fit = SVR_random_grid.fit(X_train_scaled, y)\n\n###### Getting scores and parameters ######\n\n# round(-1*SVR_fit.best_score_, 3)\n# SVR_fit.best_params_\n\n########################################\n\n# 1.6 KNN\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\n###### Training a model ######\n\n# %%time\n\n# KNN_model = KNeighborsRegressor()\n\n# KNN_param_Random = {'leaf_size': list(range(1, 50, 1)),\n#                     'n_neighbors': list(range(1, 50, 1)),\n#                     'p' : [1, 2], \n#                     'weights': ('uniform', 'distance'),\n#                     'metric': ('minkowski', 'chebyshev'), \n#                     'algorithm': ('ball_tree', 'kd_tree')}\n\n# KNN_random_grid = RandomizedSearchCV(KNN_model, KNN_param_Random, cv = CV_rep,  \n#                                      scoring = 'neg_root_mean_squared_error', \n#                                      verbose = True, n_jobs = 7, n_iter = 100)\n\n# KNN_fit = KNN_random_grid.fit(X_train_scaled, y)\n\n###### Getting scores and parameters ######\n\n# round(-1*KNN_fit.best_score_, 5)\n# KNN_fit.best_params_","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Stacking <a class=\"anchor\" id = \"V_2\"></a>","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.pipeline import make_pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_learners = [\n                 ('Lasso', linear_model.Lasso(tol = 1e-7, \n                           alpha = 0.00028, max_iter = 3000)),\n    \n                 ('El_Net', linear_model.ElasticNet(tol = 1e-6, \n                            alpha = 0.00044, l1_ratio = 0.61, max_iter = 4000)),\n    \n                 ('XGB', xgb.XGBRegressor(use_label_encoder = False, \n                         eval_metric = 'rmse',                   \n                         n_estimators = 5000,\n                         reg_alpha = 0.1,\n                         reg_lambda = 0.005,\n                         learning_rate = 0.0125,\n                         max_depth = 13,\n                         min_child_weight = 4,\n                         gamma = 0.04,\n                         subsample = 0.7,\n                         colsample_bytree = 0.6)),\n    \n                 ('LGBM', lgb.LGBMRegressor(\n                          n_estimators = 9000,\n                          reg_lambda = 1.8,\n                          reg_alpha = 0.01,\n                          min_child_samples = 13,\n                          subsample = 0.8,\n                          subsample_freq = 11,\n                          num_leaves = 101,\n                          max_depth = 3,\n                          max_bin = 160,\n                          learning_rate = 0.005,\n                          colsample_bytree = 0.1)),\n    \n                 ('KNN', make_pipeline(RobustScaler(), \n                         KNeighborsRegressor(\n                         leaf_size = 25,\n                         n_neighbors = 9,\n                         p = 1,\n                         weights = 'distance',\n                         metric = 'minkowski',\n                         algorithm = 'ball_tree'))),\n    \n                 ('SVR', make_pipeline(RobustScaler(), \n                         SVR(\n                         kernel = 'rbf',\n                         C =  10, \n                         epsilon =  0.017,\n                         gamma =  0.0007)))\n                ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Final_stack = StackingRegressor(estimators = base_learners, \n                                final_estimator = linear_model.Lasso(tol = 1e-7, \n                                alpha = 0.00028, max_iter = 3000), \n                                passthrough = True, verbose = False, \n                                cv = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Final_fit = Final_stack.fit(X_train, y)\n\ny_pred = Final_fit.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'Id': list(range(1461, 2920)), 'SalePrice': np.exp(y_pred)})\nsubmission.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1><center> VI. Some techniques that could have been useful </center></h1> <a class=\"anchor\" id = \"VI\"></a>","metadata":{}},{"cell_type":"markdown","source":"In this segment I included some feature engineering options that didn't really work in this case (based on CV scores) but might be valuable in other situations.","metadata":{}},{"cell_type":"markdown","source":"<div style = \"color: #000000;\n             display: fill;\n             padding: 8px;\n             border-radius: 5px;\n             border-style: solid;\n             border-color: #a63700;\n             background-color: rgba(235, 125, 66, 0.3)\">\n    \n<span style = \"font-size: 20px; font-weight: bold\">Note:</span> \n<span style=\"font-size: 15px\">If you want to explore other feature engineering techniques and get utility functions, please refer to this <a href=\"https://www.kaggle.com/suprematism/advanced-feature-engineering-utility-functions\">notebook</a>.</span>\n</div>","metadata":{}},{"cell_type":"markdown","source":"## 1. Feature interactions <a class=\"anchor\" id = \"VI_1\"></a>","metadata":{}},{"cell_type":"markdown","source":"Sometimes interactions between variables may prove to be valuable. Instead of picking pairs of predictors by hand and crossing them via various mathematical operations, you can automate this process to some extent.\n\nAt first, define what variables you want to use. For instance, you can collect features that are highly correlated with your target:","metadata":{}},{"cell_type":"code","source":"Corr_vars = abs(df_train.corr()['SalePrice']).sort_values(ascending = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"High_corr_vars = Corr_vars.loc[Corr_vars > 0.6].index.tolist()\nHigh_corr_vars.remove('SalePrice')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"High_corr_vars","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Following that, you should create combinations of the previously picked variables and multiply them, for example.","metadata":{}},{"cell_type":"code","source":"from itertools import combinations","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cont_comb = pd.DataFrame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c_1, c_2 in combinations(df_train[High_corr_vars], 2):\n    \n    train_cont_comb['{0}*{1}'.format(c_1, c_2)] = df_train[c_1] * df_train[c_2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cont_comb.head(3).round(1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, you can train a model that has a built-in regularisation (Lasso, for instance) and see what variables are actually important.","metadata":{}},{"cell_type":"markdown","source":"## Thanks for reading!","metadata":{}}]}