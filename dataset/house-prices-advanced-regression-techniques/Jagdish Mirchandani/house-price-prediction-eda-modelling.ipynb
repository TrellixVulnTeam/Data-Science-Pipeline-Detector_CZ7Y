{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Price Prediction","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import xticks\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import RFE\npd.pandas.set_option('display.max_columns', None)\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Analysis & EDA","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# load the dataset\nhouse = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\nhouse.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"house.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"######  Wohooo!!! 81 features!! This dataset would surely require lots of data analysis!!!","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the dataset\nhouse.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see there are lots of null values present in the dataset, we will check them in detail while performing Data Analysis and EDA.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### \"SalePrice\" is the target variable, lets see how it looks like!!","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets check the target variable \"SalePrice\"\nhouse[\"SalePrice\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Great!! There are no negative values in the dataset for sale price which is good**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets check the distribution of saleprice\nsns.distplot(house.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Saleprice seems to be skewed, This need to be handled else this will adversly impact our model**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets drop Id because its of no use to us\nhouse.drop(\"Id\",1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's display the variables with more than 0 null values\nnull_cols = []\nfor col in house.columns:\n    if house[col].isnull().sum() > 0 :\n        print(\"Column\",col, \"has\", house[col].isnull().sum(),\"null values\")    \n        null_cols.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets visualize the null vaues\nplt.figure(figsize=(12,10))\nsns.barplot(x=house[null_cols].isnull().sum().index, y=house[null_cols].isnull().sum().values)\nxticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**So many Null Values!!!**\n\n**Let's take a look at the data dictionary, these are not actually the null values, rather these are the features which are not present in the house.**\n\n**For example, let check the field Alley, Value \"NA: here means house has \"No Alley Access\"**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets check if these null values actually have any relation with the target variable\n\nhouse_eda = house.copy()\n\nfor col in null_cols:\n    house_eda[col] = np.where(house_eda[col].isnull(), 1, 0)  \n\n# lets see if these null values have to do anything with the sales price\nplt.figure(figsize = (16,48))\nfor idx,col in enumerate(null_cols):\n    plt.subplot(10,2,idx+1)\n    sns.barplot(x = house_eda.groupby(col)[\"SalePrice\"].median(),y =house_eda[\"SalePrice\"])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above graphs, we can clearly see that the null values have strong relation with the SalePrice, hence we can niether drop the columns with null values, nor we can drop the rows with null values.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Null Values Treatment","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# all missing values for the categorical columns will be replaced by \"None\"\n# all missing values for the numeric columns will be replaced by median of that field\n\nfor col in house.columns:\n    if house[col].dtypes == 'O':\n        house[col] = house[col].replace(np.nan,\"None\")\n    else:\n        house[col] = house[col].replace(np.nan,house[col].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There are some Date Variables in the dataset when we performed df.head(),Let's check again**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# making list of date variables\nyr_vars = []\nfor col in house.columns:\n    if \"Yr\" in col or \"Year\" in col:\n        yr_vars.append(col)\n\nyr_vars = set(yr_vars)\nyr_vars","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's check relation of these fields with the target variable**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize = (15,12))\nfor idx,col in enumerate(yr_vars):\n    plt.subplot(2,2,idx+1)\n    plt.plot(house.groupby(col)[\"SalePrice\"].median())\n    plt.xlabel(col)\n    plt.ylabel(\"SalePrice\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Make a note of the trend of sale price with the field \"YrSold\", it shows a decreasing trend which seems unreal in real state scenario, price is expected to increase as the time passes by, but here it shows opposite. Let's handle this by creating \"Age\" variables from these variables**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# creating age variables\nhouse['HouseAge'] =  house['YrSold'] - house['YearBuilt']\n# age of master after remodelling\nhouse['RemodAddAge'] = house['YrSold'] - house['YearRemodAdd']\n# creating age of the garage from year built of the garage to the sale of the master\nhouse['GarageAge'] = house['YrSold'] - house['GarageYrBlt'] \n\n# lets drop original variables\nhouse.drop([\"YearBuilt\",\"YearRemodAdd\",\"GarageYrBlt\"],1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check variation in the feature values","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets firs create seperate lists of categorical and numeric columns\ncat_vars = []\nnum_vars = []\nfor col in house.columns.drop(\"SalePrice\"):\n    if house[col].dtypes == 'O':\n        cat_vars.append(col)\n    else:\n        num_vars.append(col)\n\n#lets check the lists created.\nprint(\"List of Numeric Columns:\",num_vars)\nprint(\"\\n\")\nprint(\"List of Categorical Columns:\",cat_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Let's further seperate the numeric features into continous and discrete numeric features\nnum_cont = []\nnum_disc = []\nfor col in num_vars:\n    if house[col].nunique() > 25: # if variable has more than 25 different values, we consider it as continous variable\n        num_cont.append(col)\n    else:\n        num_disc.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets check for the variance in the different continous numeric columns present in the dataset\nhouse.hist(num_cont,bins=50, figsize=(20,15))\nplt.tight_layout(pad=0.4)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets check the variance in numbers\nfor col in num_cont:\n    print(house[col].value_counts())\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Following variables seems to have low variance:**\n* MasVnArea\n* BsmtFinSF2\n* 2ndFlrSF\n* EnclosedPorch\n* ScreenPorch","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets check for the variance in the different discrete numeric columns present in the dataset\nplt.figure(figsize = (16,96))\nfor idx,col in enumerate(num_disc):\n    plt.subplot(9,2,idx+1)\n    ax=sns.countplot(house[col])\n    #for p in ax.patches:\n    #    ax.annotate(p.get_height(), (p.get_x()+0.1, p.get_height()+10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**following variables seems to have low variance:**\n* **LowQualFinSF**\n* **BsmtHalfBath**\n* **KitchenAbvGr**\n* **3SsnPorch**\n* **PoolArea**\n* **MiscVal**\n","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets check for the variance in the categorical columns present in the dataset\nplt.figure(figsize = (20,200))\nfor idx,col in enumerate(cat_vars):\n    plt.subplot(22,2,idx+1)\n    ax=sns.countplot(house[col])\n    xticks(rotation=45)\n    #for p in ax.patches:\n    #    ax.annotate(p.get_height(), (p.get_x()+0.25, p.get_height()+5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets check the variance in numbers\nfor col in cat_vars:\n    print(house[col].value_counts())\n    print(\"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Following variables seems to have low variance:**\n\n* MSZoning\n* Street,\n* Alley\n* LandContour,\n* Utilities,\n* LotConfig\n* Condition1\n* LandSlope\n* Condition2,\n* BldgType\n* RoofStyle\n* RoofMatl\n* ExterCond\n* BsmtCond\n* BsmtFinType2\n* Heating\n* CentralAir\n* Electrical\n* Functional\n* GarageQual\n* GarageCond\n* PavedDrive\n* PoolQC\n* Fence\n* MiscFeature\n* SaleType\n* SaleCondition","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets drop the variables identified above as they have low variance\nlow_var_num_cont = ['MasVnrArea','BsmtFinSF2','2ndFlrSF','EnclosedPorch','ScreenPorch']\n\nlow_var_num_disc = ['LowQualFinSF','BsmtHalfBath','KitchenAbvGr','3SsnPorch','PoolArea','MiscVal']\n\nlow_var_cat_vars = ['MSZoning','Alley','LandContour','Utilities','LotConfig','Condition1','LandSlope','Condition2','BldgType','RoofStyle','RoofMatl','ExterCond','BsmtCond','BsmtFinType2','Heating','CentralAir','Electrical','Functional','GarageQual','GarageCond','PavedDrive','PoolQC','SaleType','SaleCondition','Street','Fence','MiscFeature']\n\nhouse.drop(low_var_num_cont,1,inplace= True)\nhouse.drop(low_var_num_disc,1,inplace= True)\nhouse.drop(low_var_cat_vars,1,inplace= True)\n\nnum_cont = list(set(num_cont)-set(low_var_num_cont))\nnum_disc = list(set(num_disc)-set(low_var_num_disc))\ncat_vars = list(set(cat_vars)-set(low_var_cat_vars))\n       \nnum_vars = num_cont + num_disc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets handle Skewness before moving to Bi-Variate Analysis","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets handle skewness in saleprice, lets take log to get normal distribution\nhouse.SalePrice = np.log(house.SalePrice)\n \n# lets check the distribution of saleprice again\nsns.distplot(house.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SalePrice looks good now, lets handle other numeric variables","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# taking the log of numeric variables to hanlde skewness\nnum_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\nfor col in num_features:\n    house[col] = np.log(house[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bi-Variate analysis with \"SalePrice\"","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Now we will see how SalePrice varies with respect to \"Continous numeric variables\" in the dataset**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# now lets plot the graphs for continous variables\nplt.figure(figsize=(16,48))\nfor idx,col in enumerate(num_cont):\n    plt.subplot(7,2,idx+1)\n    plt.scatter(x = house[col],y=house[\"SalePrice\"])\n    plt.ylabel(\"SalePrice\")\n    plt.xlabel(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Most of the features above seems to have a good relation with SalePrice**\n\n**There are some outliars present which need to be treated**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Now we will see how SalePrice varies with respect to \"Discrete numeric variables\" in the dataset**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# now lets plot the graphs for discrete variables\nplt.figure(figsize=(16,48))\nfor idx,col in enumerate(num_disc):\n    plt.subplot(10,2,idx+1)\n    sns.boxplot(x = house[col],y=house[\"SalePrice\"])\n    plt.ylabel(\"SalePrice\")\n    plt.xlabel(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can drop MSSubClass, YrSold & MoSold as they have no impact on SalePrice","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# dropping the variables\nhouse.drop(['MSSubClass','YrSold','MoSold'],1,inplace= True)\n\nnum_disc = list(set(num_disc)-set(['MSSubClass','YrSold','MoSold']))\nnum_vars = list(set(num_vars)-set(['MSSubClass','YrSold','MoSold']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets check relation of sale price with categorical variables\nplt.figure(figsize=(16,48))\nfor idx,col in enumerate(cat_vars):\n    plt.subplot(10,2,idx+1)\n    sns.boxplot(x = house[col],y=house[\"SalePrice\"])\n    xticks(rotation=45)\n    plt.ylabel(\"SalePrice\")\n    plt.xlabel(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outliars Detection","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets create boxplots to detect outliars detection \nplt.figure(figsize=(16,48))\nfor idx,col in enumerate(num_vars):\n    plt.subplot(11,2,idx+1)\n    plt.boxplot(house[col])\n    plt.xlabel(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are outliers in the dataset, these will be treated in the data engineering section","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#train_set[\"SalePrice\"] = train_sp\n# lets check the variables\n#num_vars = []\n#for col in train_set.columns:\n#    if train_set[col].dtypes != 'O':\n#        num_vars.append(col)\n\nfor col in num_vars:\n    print(house[col].describe(percentiles = [0.05,0.10,0.25,0.50,0.75,0.90,0.95,0.99]))\n    print(\"\\n\")\n    \n# lets handle the outliers\nq3 = house['OpenPorchSF'].quantile(0.99)\nhouse = house[house.OpenPorchSF <= q3]\n    \nq3 = house['GarageArea'].quantile(0.99)\nhouse = house[house.GarageArea <= q3]\n\nq3 = house['TotalBsmtSF'].quantile(0.99)\nhouse = house[house.TotalBsmtSF <= q3]\n\nq3 = house['BsmtUnfSF'].quantile(0.99)\nhouse = house[house.BsmtUnfSF <= q3]\n\nq3 = house['WoodDeckSF'].quantile(0.99)\nhouse = house[house.WoodDeckSF <= q3]\n\nq3 = house['BsmtFinSF1'].quantile(0.99)\nhouse = house[house.BsmtFinSF1 <= q3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"house.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering on Test Data Set","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets read the test dataset, we will apply all the feature engineering operations on test set as well\ntest_set = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\n# save \"Id\" in a variable and drop the column (as we have already dropped from train dataset)\ntest_set_id = test_set.Id\ntest_set.drop(\"Id\",1,inplace = True)\n\n# save SalePrice to a variable and drop it from training dataset as test dataset does not have this column\ntrain_sp = house.SalePrice\nhouse.drop(\"SalePrice\",1,inplace=True)\n\n# all missing values for the categorical columns will be replaced by \"None\"\n# all missing values for the numeric columns will be replaced by median of that field\nfor col in test_set.columns:\n    if test_set[col].dtypes == 'O':\n        test_set[col] = test_set[col].replace(np.nan,\"None\")\n    else:\n        test_set[col] = test_set[col].replace(np.nan,test_set[col].median())\n\n\n# creating age of the master from year built to the sale of the master\ntest_set['HouseAge'] =  test_set['YrSold'] - test_set['YearBuilt']\n# age of master after remodelling\ntest_set['RemodAddAge'] = test_set['YrSold'] - test_set['YearRemodAdd']\n# creating age of the garage from year built of the garage to the sale of the master\ntest_set['GarageAge'] = test_set['YrSold'] - test_set['GarageYrBlt'] \n\n# lets drop original variables\ntest_set.drop([\"YearBuilt\",\"YearRemodAdd\",\"GarageYrBlt\"],1,inplace = True)\n        \n        \n# skewness in test set\n# taking the log of numeric variables to hanlde skewness\nnum_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\nfor col in num_features:\n    test_set[col] = np.log(test_set[col])\n\n            \ntest_set.drop(low_var_num_cont,1,inplace= True)\ntest_set.drop(low_var_num_disc,1,inplace= True)\ntest_set.drop(low_var_cat_vars,1,inplace= True)\n\ntest_set.drop(['MSSubClass','YrSold','MoSold'],1,inplace= True)        \n        \n\n# merge the two datasets\nmaster=pd.concat((house,test_set)).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"master.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# In order to perform linear regression, we need to convert categorical variables to numeric variables.\n\n# We have ordinal variables present in the dataest, lets treat them first:\nmaster['ExterQual'] = master['ExterQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\nmaster['BsmtQual'] = master['BsmtQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\nmaster['BsmtExposure'] = master['BsmtExposure'].map({'Gd':4,'Av':3,'Mn':2,'No':1,'None':0})\nmaster['BsmtFinType1'] = master['BsmtFinType1'].map({'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'None':0})\nmaster['HeatingQC'] = master['HeatingQC'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\nmaster['KitchenQual'] = master['KitchenQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})\nmaster['GarageFinish'] = master['GarageFinish'].map({'Fin':3,'RFn':2,'Unf':1,'None':0})\nmaster['FireplaceQu'] = master['FireplaceQu'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# now lets create dummy variables for the remaining cateogorical variables\ncat_vars = []\nfor col in master.columns:\n    if master[col].dtypes == 'O':\n        cat_vars.append(col)\n\n# convert into dummies\nmaster_dummies = pd.get_dummies(master[cat_vars], drop_first=True)\n\n# drop categorical variables \nmaster.drop(cat_vars,1,inplace = True)\n\n# concat dummy variables with X\nmaster = pd.concat([master, master_dummies], axis=1)\n\n# lets check the shape of the final dataset\nmaster.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# we have perfomed all the necessary operations on the train and test datasets, time to sperate the two sets again\ntrain_set = master[:1372]\n\ntest_set = master[1372:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Scaling","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\n\ny = train_sp.reset_index(drop=True)\n\nscaler.fit(train_set)\nX = scaler.transform(train_set)\n\n# transform the train and test set, and add on the Id and SalePrice variables\nX = pd.DataFrame(X,columns = train_set.columns).reset_index(drop=True)\nX.head()\n\nscaler.fit(test_set)\ntest_set = scaler.transform(test_set)\ntest_set = pd.DataFrame(test_set,columns = train_set.columns).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA  (Dimensionality reduction)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X.isnull().sum().sort_values(ascending = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#let's apply PCA\npca.fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#List of PCA components.It would be the same as the number of variables\npca.components_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Let's check the variance ratios\npca.explained_variance_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Plotting the scree plot\n#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From Scree plot we can conclude that we 60 PCs can explain around 90% variation of the dataset**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"#Using incremental PCA for efficiency - saves a lot of time on larger datasets\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_pca = pd.DataFrame(pca_final.fit_transform(X))\ndf_pca.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df_pca.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear Regression Model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Add a constant to get an intercept\nX_train_sm = sm.add_constant(df_pca)\n\n# train the model\nlr = sm.OLS(y, X_train_sm).fit()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Performing a summary operation lists out all the different parameters of the regression line fitted\nprint(lr.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# prediction on training dataset\ny_train_pred = lr.predict(X_train_sm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"r_squared = r2_score(y_train_pred, y)\nr_squared","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We have got 88% accuracy on the training dataset**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**lets check RMSE as this is used by the kaggle competition for to evaluate model's predictive power**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms = sqrt(mean_squared_error(y, y_train_pred))\nrms","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets make predictions on the test dataset\n\ntest_pca = pd.DataFrame(pca_final.fit_transform(test_set))\n\ntest_pca_sm = sm.add_constant(test_pca)\ny_test_pred = lr.predict(test_pca_sm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear Regression produced good resuls, but, lets try Random Forest as well","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# training the model\nregr = RandomForestRegressor(n_estimators=50,random_state=0,n_jobs=1)\nregr.fit(df_pca,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets make prediction on training dataset\ny_train_pred = regr.predict(df_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"r_squared = r2_score(y_train_pred, y)\nr_squared","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms = sqrt(mean_squared_error(y, y_train_pred))\nrms","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Clearly, performance is better with Random Forest, lets make final submission with RF model**","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets make prediction on test dataset\ny_test_pred = regr.predict(test_pca)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_set_id\nsub['SalePrice'] = np.exp(y_test_pred)\nsub.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}