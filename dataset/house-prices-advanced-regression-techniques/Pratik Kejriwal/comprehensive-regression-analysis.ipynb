{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Preprocessing and EDA:\n* Handling Missing Data\n* Swarm Plot, Violin Plot for:\n    - OverallQual vs SalePrice with variations based on OverallCond\n    - Neighborhood vs SalePrice with variations based on MSZoning\n    - Neighborhood vs SalePrice with variations based on MSSubClass\n    - Neighborhood vs SalePrice with variations based on HouseStyle\n    - Neighborhood vs SalePrice with variations based on Foundation\n    - ExterQual vs SalePrice with variations based on ExterCond\n    - BasementQual vs SalePrice with variations based on BsmtCond\n* Regression Plot for:\n    - GrLivArea vs Sale Price\n    - LotArea vs SalePrice\n* Label Encoding Categorical Ordinal Data\n* One-Hot Encoding Categorical Nominal Data\n* Visualizing Feature Correlations","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## In this Version:\n- Basic Sequential Neural Network Model for Regression Analysis in PyTorch\n\n### Regression Techniques in Version 2:\n* Bagging Regressor\n* Random Subspaces Model\n* Random Pathces Model\n* Random Forest Regression\n* Extra Trees\n* Gradient Boosted Regressor\n* Gradient Boosted Regressor with GridSearch for hyperparameter tuning\n* Stacking Random Forest, Extra Trees and Gradient Boosted Regressors\n* XGBoost Regressor\n\nSubmission was based on XGBoost with tiny percent of Extra Trees, Gradient Boosted Regressor and Random Forest Regressor\n\n### Regression Techniques in Version 1:\n* Kitchen-Sink Regression\n* Selecting K-Best features\n    - Using Mutual Info Regression Method\n    - Using RFE - Recursive Feature Elimination\n    - Using Sequential Feature Selector\n* Gradient Boosted Regressor\n\nSubmission was based on Gradient Boosted Regressor alone","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the training and testing data\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\ntrain_data.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# general information about the columns\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for Missing Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for missing data in training data\ntrain_data.isna().sum().sort_values(ascending=False)[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage of missing values per column\n(train_data.isna().sum() / train_data.shape[0]).sort_values(ascending=False)[:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"PoolQC,MiscFeature,Alley,Fence,FireplaceQu all have more than 40% missing data so drop these","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing unwanted columns\ntrain_data = train_data.drop(['PoolQC','MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis=1)\ntest_data = test_data.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Handling Missing Values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data_col = ['LotFrontage','GarageCond','GarageType','GarageYrBlt','GarageFinish','GarageQual','BsmtExposure','BsmtFinType2','BsmtFinType1','BsmtCond','BsmtQual','MasVnrArea','MasVnrType','Electrical']\ntrain_data[missing_data_col].dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Using Imputer instead of fillna","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# instead of fillna using Imputer\n# replacing float data with mean and string with mode of corresponding columns\nfrom sklearn.impute import SimpleImputer\nfloat_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\ntrain_data[['LotFrontage', 'GarageYrBlt', 'MasVnrArea']] = float_imputer.fit_transform(train_data[['LotFrontage', 'GarageYrBlt', 'MasVnrArea']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ntemp_var = ['GarageCond','GarageType','GarageFinish','GarageQual','BsmtExposure','BsmtFinType2','BsmtFinType1','BsmtCond','BsmtQual','MasVnrType','Electrical']\ntrain_data[temp_var] = string_imputer.fit_transform(train_data[temp_var])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# no null data now\ntrain_data.isnull().sum().sort_values(ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing values in test data\ntest_data.isnull().sum().sort_values(ascending=False)[:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_missing = ['LotFrontage','GarageCond','GarageQual','GarageYrBlt','GarageFinish','GarageType','BsmtCond','BsmtQual','BsmtExposure','BsmtFinType1','BsmtFinType2','MasVnrType','MasVnrArea','MSZoning','BsmtHalfBath','Utilities','Functional','BsmtFullBath','BsmtUnfSF','SaleType','BsmtFinSF2','BsmtFinSF1','Exterior2nd','Exterior1st','TotalBsmtSF','GarageCars','KitchenQual','GarageArea']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training data and testing data columns having NaN values differ \nmissing_data_col == test_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data[test_missing].dtypes.sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling nan values for float dtype features\nfloat_missing = ['LotFrontage','GarageCars','TotalBsmtSF','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','BsmtFullBath','BsmtHalfBath','MasVnrArea','GarageArea','GarageYrBlt']\ntest_float_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\ntest_data[float_missing] = test_float_imputer.fit_transform(test_data[float_missing])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# filling nan values for str dtype features\nstr_missing = ['BsmtFinType2','GarageCond','GarageQual','Exterior1st','Exterior2nd','GarageFinish','SaleType','GarageType','BsmtCond','Functional','Utilities','BsmtQual','KitchenQual','BsmtExposure','MasVnrType','BsmtFinType1','MSZoning']\ntest_str_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ntest_data[str_missing] = test_str_imputer.fit_transform(test_data[str_missing])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum().sort_values(ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Some Plots to Visualize the effects of features on driving the Sale Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Swarm Plot of OverallQual vs SalePrice with variations based on OverallCond\nplt.figure(figsize=(23,12))\nsns.swarmplot(train_data['OverallQual'], train_data['SalePrice'], hue=train_data['OverallCond'], palette='husl')\nplt.title(\"Swarm Plot of OverallQual vs SalePrice with variations based on OverallCond\")\nplt.xlabel(\"OverallQual\")\nplt.ylabel(\"Sale Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Swarm Plot of Neighborhood vs SalePrice with variations based on MSZoning\nplt.figure(figsize=(23,12))\nsns.swarmplot(train_data['Neighborhood'], train_data['SalePrice'], hue=train_data['MSZoning'])\nplt.title(\"Swarm Plot of Neighborhood vs SalePrice with variations based on MSZoning\")\nplt.xlabel(\"Neighborhood\")\nplt.ylabel(\"Sale Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Swarm Plot of Neighborhood vs SalePrice with variations based on MSSubClass\nplt.figure(figsize=(23,12))\nsns.swarmplot(train_data['Neighborhood'], train_data['SalePrice'], hue=train_data['MSSubClass'], palette='winter')\nplt.title(\"Swarm Plot of Neighborhood vs SalePrice with variations based on MSSubClass\")\nplt.xlabel(\"Neighborhood\")\nplt.ylabel(\"Sale Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Swarm Plot of Neighborhood vs SalePrice with variations based on HouseStyle\nplt.figure(figsize=(23,12))\nsns.swarmplot(train_data['Neighborhood'], train_data['SalePrice'], hue=train_data['HouseStyle'])\nplt.title(\"Swarm Plot of Neighborhood vs SalePrice with variations based on HouseStyle\")\nplt.xlabel(\"Neighborhood\")\nplt.ylabel(\"Sale Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Swarm Plot of Neighborhood vs SalePrice with variations based on Foundation\nplt.figure(figsize=(23,12))\nsns.swarmplot(train_data['Neighborhood'], train_data['SalePrice'], hue=train_data['Foundation'])\nplt.title(\"Swarm Plot of Neighborhood vs SalePrice with variations based on Foundation\")\nplt.xlabel(\"Neighborhood\")\nplt.ylabel(\"Sale Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Violin Plot of ExterQual vs SalePrice with variations based on ExterCond\nplt.figure(figsize=(23,12))\nsns.violinplot(train_data['ExterQual'], train_data['SalePrice'], hue=train_data['ExterCond'], inner='quartile', palette='RdBu')\nplt.title(\"Violin Plot of ExterQual vs SalePrice with variations based on ExterCond\")\nplt.xlabel(\"External Quality\")\nplt.ylabel(\"Sale Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Violin Plot of BasementQual vs SalePrice with variations based on BsmtCond\nplt.figure(figsize=(23,12))\nsns.violinplot(train_data['BsmtQual'], train_data['SalePrice'], hue=train_data['BsmtCond'], inner='quartile', palette='RdBu')\nplt.title(\"Violin Plot of BasementQual vs SalePrice with variations based on BsmtCond\")\nplt.xlabel(\"Basement Quality\")\nplt.ylabel(\"Sale Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# KDE Plot\nplt.figure(figsize=(15,8))\nsns.kdeplot(train_data['SalePrice'])\nplt.title('KDE Plot for Sales Price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regression Plot GrLivArea vs Sale Price\nplt.figure(figsize=(15,8))\nsns.regplot(train_data['GrLivArea'], train_data['SalePrice'])\nplt.title(\"Regression Plot of Living Area vs SalePrice\")\nplt.xlabel(\"Living Area\")\nplt.ylabel(\"Sale Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Regression Plot LotArea vs SalePrice\nplt.figure(figsize=(15,8))\nsns.regplot(train_data['LotArea'], train_data['SalePrice'])\nplt.title(\"Regression Plot of Lot Area vs SalePrice\")\nplt.xlabel(\"Lot Area\")\nplt.ylabel(\"Sale Price\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Label Encoding Categorical Data\nLabel Encoding works best with Categorical Data which have some inherent ordering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# these features appear to have ordering in them\nordinal_columns = ['Street','LotShape','LandContour','Utilities','LandSlope','BldgType','HouseStyle','ExterQual','ExterCond','BsmtQual','BsmtCond',\n                   'BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC','CentralAir','KitchenQual','Functional','GarageFinish','GarageQual','GarageCond',\n                   'PavedDrive']\ntrain_le = {}\nfor col in ordinal_columns:\n    train_le[col] = LabelEncoder()\n    \n    train_data[col] = train_le[col].fit_transform(train_data[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_le = {}\nfor col in ordinal_columns:\n    test_le[col] = LabelEncoder()\n    \n    test_data[col] = test_le[col].fit_transform(test_data[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## One-Hot Encoding rest of the features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# need to combine both the dataframes or else encoding creates dataframes of different sizes\n\ntrain_data['train'] = 1\ntest_data['test'] = 1\n\ncombined_df = pd.concat([train_data, test_data])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use Pandas get_dummies for One-hot Encoding\ncombined_df = pd.get_dummies(combined_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = combined_df[combined_df['train'] == 1]\ntest_data = combined_df[combined_df['test'] == 1]\ntrain_data = train_data.drop(['train', 'test'], axis=1)\ntest_data = test_data.drop(['train', 'test', 'SalePrice'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shape is same for both the datasets\nprint(train_data.shape, test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scaling the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\n\nscale = RobustScaler()\n\nfeatures = train_data.drop('SalePrice', axis=1)\nfeatures = pd.DataFrame(data=scale.fit_transform(features), columns=features.columns)\ntarget = train_data['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_scale = RobustScaler()\n\ntest_df = pd.DataFrame(data=test_scale.fit_transform(test_data), columns=test_data.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Feature Correlations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pearson Correlation\nfrom yellowbrick.target import FeatureCorrelation\nplt.figure(figsize=(25,35))\nviz = FeatureCorrelation(labels = features.columns, method='pearson', sort=True)\nviz.fit(features, target)\nviz.poof()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation as per Mutual Info Regression\nplt.figure(figsize=(25,35))\nviz = FeatureCorrelation(labels=features.columns, method='mutual_info-regression', sort=True)\nviz.fit(features, target)\nviz.poof()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Heatmap Visualization of Pearson's Coefficient\ndatacor = train_data.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(datacor, dtype=np.bool))\n\nplt.figure(figsize=(25,20))\nsns.heatmap(datacor, mask=mask, annot=False, square=True, cmap='RdBu')\nplt.title(\"Heatmap Visualization of Pearson's Coefficient\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### define some helper functions, which would be used repeatedly","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a function that would split data into training and testing\ndef split_data(features, target):\n    \n    from sklearn.model_selection import train_test_split\n    \n    x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=1)\n    \n    return x_train, x_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper function to evaluate different scores\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\ndef get_score(y_test, y_pred):\n    r2 = r2_score(y_test, y_pred)\n    mse = mean_squared_error(y_test, y_pred)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    rmsle = mean_squared_log_error(y_test, y_pred) \n    return (r2, mse, rmse, rmsle)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to Plot the Actual vs Predicted Sale Prices\ndef plot_data(y_test, y_pred):\n    plt.figure(figsize=(18,12))\n    plt.plot(y_test.values, label='Actual', c='r')\n    plt.plot(y_pred, label='Predicted', c='b')\n    plt.title('Actual vs Predicted Sale Price of the House')\n    plt.ylabel('Sale Price')\n    plt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Basic PyTorch NN model for Regression Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# might have to download this package to visualize the PyTorch model\n!pip install hiddenlayer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport graphviz\nimport hiddenlayer as hl","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if cuda enabled gpu is present, train on that else on cpu\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = split_data(features, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert this data to tensors and transfer them to device\nx_train = torch.tensor(X_train.values, device=device, dtype=torch.float)\ny_train = torch.from_numpy(Y_train.values).view(1, -1)[0]\n\nx_test = torch.tensor(X_test.values, device=device, dtype=torch.float)\ny_test = torch.from_numpy(Y_test.values).view(1, -1)[0]\n\n# y is converted from [dim, 1] to [dim] as this is what is expected by the NN model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.type(torch.FloatTensor)\ny_test = y_test.type(torch.FloatTensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train.to(device)\ny_test = y_test.to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### While, working with GPUs, I/O operations become bottleneck as GPUs can perform operations so instantly that it has to wait till the next data point comes in, which is why we need to feed in data in batches\n#### we'll work with DataLoaders to process data in batches\n#### Datasets work with PyTorch data loaders and these loaders can load multiple samples/batches of data in parallel ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# utils.data contains data loaders\nimport torch.utils.data as data_utils","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we first need to convert the training set as a Tensor Dataset\ntrain_data = data_utils.TensorDataset(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# specify batch size for this train_loader, if cuda device is present use larger batch_size else a smaller one\n# shuffle the dataset before it is fed into the model, to remove any chances to NN picking up irrelevant patterns\n\ntrain_loader = data_utils.DataLoader(train_data, batch_size=100, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(x_train.shape, y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training points will be divided into batches\nlen(train_loader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterate through the batches from the loader\nx_batch, y_batch = iter(train_loader).next()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NN architecture -> size of each layer\n\ninput_size = x_batch.shape[1]\noutput_size = 1 # as prediction would be Sale Price\nhidden_size = 25","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining the Sequential Model\nwith 2 Fully Connected Layers, which have ReLU activation and a Dropout Layer between them\n\nprobability that a neuron will be turned off is 0.2","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# using a basic Sequential NN model\n# input layer which has size = # of features in x_train\n# introduced a dropout layer to avoid overfitting\nmodel = nn.Sequential(nn.Linear(input_size, hidden_size),\n                      nn.ReLU(),\n                      nn.Dropout(p=0.2),\n                      nn.Linear(hidden_size, output_size))\n\n# transfer the model to device\nmodel.to(device)\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize the model\nhl_graph = hl.build_graph(model, torch.zeros([1, input_size], device=device))\nhl_graph.theme = hl.graph.THEMES[\"blue\"].copy() \nhl_graph","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# use objective loss function as the MSE Loss\ncriterion = nn.MSELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# learning rate for the model\nlearning_rate = 1e-3","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# using Adam optimizer - Momentum based Optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\noptimizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training the model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# set the model to be in training mode as it behaves differently in training and validation phases\nmodel.train()\n\ntotal_batch = len(train_loader)\n\n# set the number of epochs you want to train for\nepochs = 3500\n\nfor epoch in range(epochs+1):\n    for i, (features, target) in enumerate(train_loader):\n        \n        # prediction based solely on features\n        predict = model(features)\n        \n        predict = predict.view(1, -1)[0]\n        \n        # get the loss function which will guide the optimizer to reduce the loss as per the model performance\n        loss = criterion(predict, target)\n        \n        # zero-out the previously calculated gradients\n        optimizer.zero_grad()\n        \n        # perform back propagation\n        loss.backward()\n        \n        # update the model parameters\n        optimizer.step()\n        \n        # print out the training score after few epochs\n        if epoch % 500 == 0:\n            print(f'| Epoch: {epoch+1:04} | Batch: {i+1:02} | Training Loss: {loss.item():.3f} |')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluating the model","execution_count":null},{"metadata":{"trusted":false},"cell_type":"code","source":"# before we can use NN for prediction, switch to evaluation mode\n# as there are layers in NN that perform differently during training and prediction phases\n\nmodel.eval()\n\n# as we don't want to calculate gradients while evaluating, turn-off the grad function\nwith torch.no_grad():\n    y_pred_tensor = model(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# convert the pytorch cuda tensor to cpu\ny_pred = y_pred_tensor.to('cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"r2, mse, rmse, rmsle = get_score(Y_test, y_pred)\n\nprint(f'R2 score of the model is {r2:.3f}')\nprint(f'MSE score of the model is {mse:.3f}')\nprint(f'RMSE score of the model is {rmse:.3f}')\nprint(f'RMSLE score of the model is {rmsle:.5f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plot_data(Y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# convert the test_data to cuda tensor\ntest_tensor = torch.tensor(test_df.values, device=device, dtype=torch.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# prediction from the model\npred = model(test_tensor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# convert the size of the model from [size, 1] to [size]\npred = pred.view(1, -1)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# copy tensor from gpu to cpu and then convert it to numpy\npred = pred.to('cpu').detach().numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub_df = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': pred})\nsub_df.to_csv('submit.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"sub_df.sample(5)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}