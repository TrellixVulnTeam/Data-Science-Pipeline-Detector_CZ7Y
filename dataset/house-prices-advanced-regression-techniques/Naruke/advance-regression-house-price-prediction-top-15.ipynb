{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom tqdm import tqdm\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy import stats\nplt.style.use(\"fivethirtyeight\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\nsample,features = train.shape[0],train.shape[1]\nprint(f\"Train data contains {sample} rows and {features} columns\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\nsample,features = test.shape[0],test.shape[1]\nprint(f\"Test data contains {sample} rows and {features} columns\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,8))\nsns.distplot(train['SalePrice']\n                ,color = 'r')\n#plt.xlabel(\"Sale Price distribution across train dataset\")\nplt.ylabel(\"Frequency\")\nskewness = np.round(train['SalePrice'].skew(),2)\nkurtosis = np.round(train['SalePrice'].kurt(),2)\nplt.axvline(np.percentile(train['SalePrice'],80),color = 'blue',label = \"80% percentile\")\nplt.suptitle(f\"Sale Price distribution across train dataset\")\nplt.legend()\nplt.show()\n\nprint(f\"Above Distribution has {skewness} skewness values\")\nprint(f\"Above Distribution has {kurtosis} kurtosis values\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So.... What do we analyze from above distribution and data\n\n1. SalePrice features is not normally distributed, and can be clearly interpreted as right skewed as there is long tail in the right of the plot.\n\n2. To check if above features is log normally distributed or not we are going to plot log values of SalePrice feature. If resulted distribution is normally distributed then we can take log(SalePrice) as target variable.\n\n3. Positive Skewness values interpret that it's a right skewed distribution which is easilly proved looking above plot.\n\n### Extra Info: Negative value determined it's a left skewed and zero value implies it's normally distributed.\n\n4. Kurtosis just like skewness tells us about the shape of distribution and kurtosis value above 3 implies that tails of those distribution contains lot more information then normal distribution.\n\n### Extra Info: If value is less than 3 it's states that information at the tails of the distribution is less than normal distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,8))\nplt.subplot(1,2,1)\nsns.distplot(np.log(train['SalePrice'])\n                ,color = 'r',label = \"Actual Distribution\",fit = norm)\n#plt.xlabel(\"Sale Price distribution across train dataset\")\nplt.ylabel(\"Frequency\")\nplt.suptitle(f\"log(SalePrice) distribution across train dataset\")\nplt.legend()\n\nplt.subplot(1,2,2)\nstats.probplot(np.log(train['SalePrice']),plot = plt)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well....... Nothing can be more satisfying then above result. \n\nOur target variable Price started to form a normal distribution. This implies that SalePrice is a power law distribution as stated in previous section(2nd Point).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['log(Price)'] = np.log(train['SalePrice']) # added one new feature which is going to be very useful in future sections ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's dive into more features...........\n\n#### We are going to see 3 more main numerical features and how related this features is with SalePrice:\n\n1. 1stFlrSF : First Floor square feet\n2. 2ndFlrSF : Second floor square feet\n3. GrLivArea : Above grade (ground) living area square feet\n\n#### Swoop into the indepth analysis of above features..........","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (24,8))\nplt.subplot(1,3,1)\nsns.distplot(train['1stFlrSF']\n                ,color = 'r',label = \"Actual Distribution\")\n\nplt.axvline(np.percentile(train['1stFlrSF'],80),color = 'blue',label = \"80% percentile\")\n\nskewness = np.round(train['1stFlrSF'].skew(),2)\nkurtosis = np.round(train['1stFlrSF'].kurt(),2)\n\nplt.ylabel(\"Frequency\")\nplt.suptitle(f\"log(1stFlrSF) distribution across train dataset\")\nplt.legend()\n\nplt.subplot(1,3,2)\nsns.distplot(np.log(train['1stFlrSF'])\n                ,color = 'r',label = \"Normal Distribution\",fit = norm)\n# stats.probplot(np.log(train['1stFlrSF']),plot = plt)\n\nplt.subplot(1,3,3)\n\nsns.scatterplot(train['log(Price)'],np.log(train['1stFlrSF']))\nplt.xlabel(\"log(SalePrice)\")\nplt.ylabel(\"log(1stFlrSF)\")\ncor = stats.pearsonr(train['log(Price)'],np.log(train['1stFlrSF']))\nspear_cor = stats.spearmanr(train['log(Price)'],np.log(train['1stFlrSF']))\nplt.title(\"Price vs 1stFlr Area\")\n\nplt.show()\n\n\nprint(f\"Above Distribution has {skewness} skewness values\")\nprint(f\"Above Distribution has {kurtosis} kurtosis values\")\nprint(f\"Pearson Correlation between Price and 1stFlr Area is {cor[0]}\")\nprint(f\"Spearman Correlation between Price and 1stFlr Area is {spear_cor[0]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Above set of plots do tell us some story right !!!!\n\nWe can definitely concludes that 1stFlr Area feature has good relationship with SalePrice. The correlation of log of Price and 1stFlr Area features is pretty good.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (24,8))\nplt.subplot(1,3,1)\nsns.distplot(train['2ndFlrSF']\n                ,color = 'r',label = \"Actual Distribution\")\n\nskewness = np.round(train['2ndFlrSF'].skew(),2)\nkurtosis = np.round(train['2ndFlrSF'].kurt(),2)\n\nplt.ylabel(\"Frequency\")\nplt.suptitle(f\"log(2ndFlrSF) distribution across train dataset\")\nplt.legend()\n\nplt.subplot(1,3,2)\nsns.distplot(np.log1p(train['2ndFlrSF'])\n                ,color = 'r',label = \"Normal Distribution\",fit = norm)\n# stats.probplot(np.log(train['1stFlrSF']),plot = plt)\n\nplt.subplot(1,3,3)\n\nsns.scatterplot(train['log(Price)'],np.log(train['2ndFlrSF']))\nplt.xlabel(\"log(SalePrice)\")\nplt.ylabel(\"log(2ndFlrSF)\")\ncor = stats.pearsonr(train['log(Price)'],np.log1p(train['2ndFlrSF']))\nspear_cor = stats.spearmanr(train['log(Price)'],np.log1p(train['2ndFlrSF']))\nplt.title(\"Price vs 2ndFlrSF Area\")\n\nplt.show()\n\n\nprint(f\"Above Distribution has {skewness} skewness values\")\nprint(f\"Above Distribution has {kurtosis} kurtosis values\")\nprint(f\"Pearson Correlation between Price and 2ndFlr Area is {cor[0]}\")\nprint(f\"Spearman Correlation between Price and 2ndFlr Area is {spear_cor[0]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. Well some of the values in 2ndFlr Area features is 0, this might be because there is no 2nd floor in that house.\n\n2. Relationship between Price and 2ndFlr area also not good as compare to 1stFlr area.\n\n3. Looking into above plots we can say that it's a bimodal distribution i.e it is having two modes one at very lower end close to zero because there are many samples having zero as values (no 2nd floor). If we remove those 0 values samples ( just for exploration) let's see how the distribution comes out to be.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## This going to be so........... interesting to see how this 2ndFlr Area feature behave when we remove samples with zero values.\n\n## Spolier Alert !!! It's going to be very intutive ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = train[train['2ndFlrSF']!=0]\n\nplt.figure(figsize = (24,8))\nplt.subplot(1,3,1)\nsns.distplot(sample['2ndFlrSF']\n                ,color = 'r',label = \"Actual Distribution\")\n\nskewness = np.round(sample['2ndFlrSF'].skew(),2)\nkurtosis = np.round(sample['2ndFlrSF'].kurt(),2)\n\nplt.ylabel(\"Frequency\")\nplt.suptitle(f\"log(2ndFlrSF) distribution across train dataset\")\nplt.legend()\n\nplt.subplot(1,3,2)\nsns.distplot(np.log1p(sample['2ndFlrSF'])\n                ,color = 'r',label = \"Normal Distribution\",fit = norm)\n# stats.probplot(np.log(train['1stFlrSF']),plot = plt)\n\nplt.subplot(1,3,3)\n\nsns.scatterplot(sample['log(Price)'],np.log(sample['2ndFlrSF']))\nplt.xlabel(\"log(SalePrice)\")\nplt.ylabel(\"log(2ndFlrSF)\")\ncor = stats.pearsonr(sample['log(Price)'],np.log1p(sample['2ndFlrSF']))\nspear_cor = stats.spearmanr(sample['log(Price)'],np.log1p(sample['2ndFlrSF']))\nplt.title(\"Price vs 2ndFlrSF Area\")\n\nplt.show()\n\n\nprint(f\"Above Distribution has {skewness} skewness values\")\nprint(f\"Above Distribution has {kurtosis} kurtosis values\")\nprint(f\"Pearson Correlation between Price and 2ndFlr Area is {cor[0]}\")\nprint(f\"Spearman Correlation between Price and 2ndFlr Area is {spear_cor[0]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Isn't it intriguing the way our resultant plot transformed to be a insight data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (24,8))\nplt.subplot(1,3,1)\nsns.distplot(train['GrLivArea']\n                ,color = 'r',label = \"Actual Distribution\")\nplt.axvline(np.percentile(train['GrLivArea'],80),color = 'blue',label = \"80% percentile\")\n\nskewness = np.round(train['GrLivArea'].skew(),2)\nkurtosis = np.round(train['GrLivArea'].kurt(),2)\n\nplt.ylabel(\"Frequency\")\nplt.suptitle(f\"log(GrLivArea) distribution across train dataset\")\nplt.legend()\n\nplt.subplot(1,3,2)\nsns.distplot(np.log(train['GrLivArea'])\n                ,color = 'r',label = \"Normal Distribution\",fit = norm)\n# stats.probplot(np.log(train['1stFlrSF']),plot = plt)\n\nplt.subplot(1,3,3)\n\nsns.scatterplot(train['log(Price)'],np.log(train['GrLivArea']))\nplt.xlabel(\"log(SalePrice)\")\nplt.ylabel(\"log(GrLivArea)\")\ncor = stats.pearsonr(train['log(Price)'],np.log(train['GrLivArea']))\nspear_cor = stats.spearmanr(train['log(Price)'],np.log(train['GrLivArea']))\nplt.title(\"Price vs GrLivArea Area\")\n\nplt.show()\n\n\n\nprint(f\"Above Distribution has {skewness} skewness values\")\nprint(f\"Above Distribution has {kurtosis} kurtosis values\")\nprint(f\"Pearson Correlation between Price and GrLivArea is {cor[0]}\")\nprint(f\"Spearman Correlation between Price and GrLivArea is {spear_cor[0]}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Great it turnsout that GrliveArea is the most important then above all features for predicting SalePrice. Nice Going ........","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### In my perspective, heatmap is the best way to have a look at different correlation without going through all the troubles.......\n\n### Below, I have visualization which shows top 10 (including SalePrice) features having highest correlation.\n\n# Alert!! \n\n## Before finding top 10 correlation drop newly created features...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#saleprice correlation matrix\n\ncol_drop = [\"log(Price)\"]\nplt.figure(figsize = (24,12))\nk = 10 #number of variables for heatmap\n\ncols = train.drop(col_drop,axis=1).corr().nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train.drop(col_drop,axis=1)[cols].values.T)\nsns.set(font_scale=1.25)\n\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Have a look at some categorical features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Year Built Features should have nice correlation with SalePrice\n\n### We generally prefer to buy house which has been recently contructed and that's why as Year increases SalePrice of House Increases.\n\n### But we might see some earlier constructed building to be more costly then present. Because some of use definitely likes antique pieces.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (24,8))\n\ntemp = train.groupby('YearBuilt')['SalePrice'].median()\nsns.boxenplot(x = train['YearBuilt'],y = train['SalePrice'],ax = ax)\n#sns.lineplot(y = temp.values,x = temp.index,ax = ax)\nax.plot(temp.values)\nplt.xticks(rotation = 90)\nplt.suptitle(\"Year Built vs SalePrice\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Next two categorical features we are going to exxplore is OverallQual and OverallCond....\n\n## I don't think anyone will have any doubt regarding these features correlation with SalePrice......\n\n## well definetly we are going to see positive relation is it???? .........Can't Wait..... then let's dive in ................","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (24,8))\n\ntemp = train.groupby('OverallQual')['SalePrice'].median()\nsns.boxenplot(x = train['OverallQual'],y = train['SalePrice'],ax = ax)\n#sns.lineplot(y = temp.values,x = temp.index,ax = ax)\nax.plot(temp.values)\nplt.xticks(rotation = 90)\nplt.suptitle(\"Overall Quality vs SalePrice\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Well as expected Quality of House Increases Sale Price Increases..........  No Doubt right.........Let See if you are clear with the below result...\n\n## Get Ready for Shocking Result..........................\n\n<img src = \"https://tenor.com/view/friends-oh-my-eyes-my-eyes-gif-10973109.gif\" width = \"500px\" height = \"200px\">","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (24,8))\n\ntemp = train.groupby('OverallCond')['SalePrice'].median()\nsns.boxenplot(x = train['OverallCond'],y = train['SalePrice'],ax = ax)\n#sns.lineplot(y = temp.values,x = temp.index,ax = ax)\nax.plot(temp.values)\nplt.xticks(rotation = 90)\nplt.suptitle(\"Overall Condition vs SalePrice\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Well to be frank I couldn't understand why OverallCond value 5 has high median then all of it......... Certainly Quality and Condition may be different things in this dataset (but how???? isn't it the synonym of each other!!! certainly not here......).\n\n## What I can conclude here is that there are certains features Condition1 and Condition2 in the dataset which might be contributing towards the calculation of OverallCond rating of each House............\n\n\n## What else might be the reason...... I can't think of another reason if you know feel free to shoot in the **comment section**.........","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,ax = plt.subplots(figsize = (24,8))\n\ntemp = train.groupby('FullBath')['SalePrice'].median()\nsns.boxenplot(x = train['FullBath'],y = train['SalePrice'],ax = ax)\n#sns.lineplot(y = temp.values,x = temp.index,ax = ax)\nax.plot(temp.values)\nplt.xticks(rotation = 90)\nplt.suptitle(\"FullBath vs SalePrice\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confused!!! How can fullbath features be so important contributor for SalePrice.....Really!!!!!!!!!!!!!!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Let's fill in missing values and Embark a journey of creating a Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### First Let's find out % of missing values in each features...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(train)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nkeys,value = [],[]\nfor i in range(len(df_miss[:10])):\n    keys.append(df_miss[i][0])\n    value.append(df_miss[i][1])\n# let's see the distribution of missing values\n\nplt.figure(figsize = (16,12))\nax = sns.barplot(x = keys,y = value)\n\nrects = ax.patches\n\nfor rect in rects:\n    height = rect.get_height()\n    ax.text(rect.get_x() + 0.1,height + 2, str (np.round(height,0)), ha = 'left',va = 'top')\n\n\n\nplt.xlabel(\"Top 10 Null Features\",fontsize = 20)\nplt.ylabel(\"Frequency\",fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## There is certain features having dtype as int but it should be object or string type. Let's convert them.........","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_dtype(data):\n    data['MSSubClass'] = data['MSSubClass'].apply(str)\n    data['YrSold'] = data['YrSold'].astype(str)\n    data['MoSold'] = data['MoSold'].astype(str)\n    return(data)\ntrain = convert_dtype(train)\ntest = convert_dtype(test)\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's fill in the missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def handle_missing(data):\n    \n    # the data description states that NA refers to typical ('Typ') values\n    data['Functional'] = data['Functional'].fillna('Typ')\n    \n    # Replace the missing values in each of the columns below with their mode\n    data['Electrical'] = data['Electrical'].fillna(\"SBrkr\")\n    data['KitchenQual'] = data['KitchenQual'].fillna(\"TA\")\n    data['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\n    data['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\n    data['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])\n    data['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    \n    # the data description stats that NA refers to \"No Pool\"\n    data[\"PoolQC\"] = data[\"PoolQC\"].fillna(\"None\")\n    \n    \n    # Replacing the missing values with 0, since no garage = no cars in garage\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        data[col] = data[col].fillna(0)\n    \n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        data[col] = data[col].fillna('None')\n    \n    # NaN values for these categorical basement features, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        data[col] = data[col].fillna('None')\n        \n    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n    data['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n    # We have no particular intuition around how to fill in the rest of the categorical features\n    \n    # So we replace their missing values with None\n    objects = []\n    for i in data.columns:\n        if data[i].dtype == object:\n            objects.append(i)\n    data.update(data[objects].fillna('None'))\n        \n    # And we do the same thing for numerical features, but this time with 0s\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in data.columns:\n        if data[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    data.update(data[numeric].fillna(0))    \n    return data\n\ntrain = handle_missing(train)\ntest = handle_missing(test)\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## so we have no missing values now...cooool...But we are still not done we have some numerical features that has to be handled and some feature engineering needs to be done....","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in train.drop(['Id','SalePrice','log(Price)'],axis=1).columns:\n    if train[i].dtype in numeric_dtypes:\n        numeric.append(i)\n        \ndef find_skewness(data):\n    skew = {}\n    for col in data.columns:\n        skewness = np.round(data[col].skew(),3)\n        skew[col] = skewness\n    return(skew)\n\nskew_data = find_skewness(train[numeric]) \nskew_data = dict(sorted(skew_data.items(), key=lambda x: x[1], reverse=True))\n\nf, ax = plt.subplots(figsize=(16, 8))\nax.set_yscale(\"log\")\nax = sns.boxenplot(data = train[numeric], orient=\"v\", palette=\"Set2\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nplt.xticks(rotation = 90)\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Below we have applied BoxCox transformation for each numerical features...when we input the feature values to boxcox function it return lambda values which we will use to transform each non-gaussian distribution to gaussian distribution......\n\n## Point to Remember: Not all numerical features is converted to gaussian distribution.....it tries to convert features such that it approximate normal distribution.........","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in list(skew_data.keys())[:5]:\n    train[i] = stats.boxcox(train[i]+1\n                            , stats.boxcox_normmax(train[i] + 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skew_data = find_skewness(train[numeric]) \nskew_data = dict(sorted(skew_data.items(), key=lambda x: x[1], reverse=True))\n\nf, ax = plt.subplots(figsize=(16, 8))\nax.set_yscale(\"log\")\nax = sns.boxenplot(data = train[numeric], orient=\"v\", palette=\"Set2\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features (After BoxCox Transformation)\")\nplt.xticks(rotation = 90)\nsns.despine(trim=True, left=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric = []\nfor i in test.drop(['Id'],axis=1).columns:\n    if test[i].dtype in numeric_dtypes:\n        numeric.append(i)\nskew_data = find_skewness(test[numeric]) \nskew_data = dict(sorted(skew_data.items(), key=lambda x: x[1], reverse=True))\nfor i in list(skew_data.keys())[:5]:\n    test[i] = stats.boxcox(test[i]+1\n                            , stats.boxcox_normmax(test[i] + 1))\n    \nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now let's Create some intresting features which might help our model to understand some complex patterns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_features(data):\n    \n    data['BsmtFinType1_Unf'] = 1*(data['BsmtFinType1'] == 'Unf')\n    data['HasWoodDeck'] = (data['WoodDeckSF'] == 0) * 1\n    data['HasOpenPorch'] = (data['OpenPorchSF'] == 0) * 1\n    data['HasEnclosedPorch'] = (data['EnclosedPorch'] == 0) * 1\n    data['Has3SsnPorch'] = (data['3SsnPorch'] == 0) * 1\n    data['HasScreenPorch'] = (data['ScreenPorch'] == 0) * 1\n    data['YearsSinceRemodel'] = data['YrSold'].astype(int) - data['YearRemodAdd'].astype(int)\n    data['Total_Home_Quality'] = data['OverallQual'] + data['OverallCond']\n    data = data.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n    data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n    data['YrBltAndRemod'] = data['YearBuilt'] + data['YearRemodAdd']\n\n    data['Total_sqr_footage'] = (data['BsmtFinSF1'] + data['BsmtFinSF2'] +\n                                     data['1stFlrSF'] + data['2ndFlrSF'])\n    data['Total_Bathrooms'] = (data['FullBath'] + (0.5 * data['HalfBath']) +\n                                   data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath']))\n    data['Total_porch_sf'] = (data['OpenPorchSF'] + data['3SsnPorch'] +\n                                  data['EnclosedPorch'] + data['ScreenPorch'] +\n                                  data['WoodDeckSF'])\n    \n    \n    # below transformation is for each bimodal distribution \n    \n    data['TotalBsmtSF'] = data['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n    data['2ndFlrSF'] = data['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n    data['GarageArea'] = data['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n    data['GarageCars'] = data['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\n    data['LotFrontage'] = data['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\n    data['MasVnrArea'] = data['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\n    data['BsmtFinSF1'] = data['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n    \n    # boolean features\n\n    data['haspool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    data['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasgarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasbsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasfireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    return(data)\ntrain = new_features(train)\ntest = new_features(test)\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def log_square_features(data):\n    log_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                     'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                     'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                     'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                     'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n\n    for col in log_features:\n        data['log('+col+')'] = np.log(1.01+data[col])\n\n    squared_features = ['log(LotFrontage)', \n                  'log(TotalBsmtSF)', 'log(1stFlrSF)', 'log(2ndFlrSF)', 'log(GrLivArea)',\n                  'log(GarageCars)', 'log(GarageArea)']\n\n    for col in squared_features:\n        data['Square_'+col] = data[col]*data[col]\n    return(data)\n\ntrain = log_square_features(train)\ntest = log_square_features(test)\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical Features Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['Id'],axis=1,inplace = True)\ntest.drop(['Id'],axis=1,inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(train.drop(['SalePrice','log(Price)'],axis=1),train[['SalePrice','log(Price)']],test_size = 0.05,random_state = 42)\nprint(f\"Train data has shape {X_train.shape}\")\nprint(f\"Test data has shape {X_test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The reason why we are not doing applying Dummy function here because when we apply this function individually on train and test data this may lead to different feature vector dimension becuase of different number of unqiue varaible in features..........Confusing ... let's narrow it down one simple example\n\n## Let's consider we have applied Dummy Variable for feature 'A' which contains 5 unqiue variables (here we are talking about train data). This will create 5 new features (each feature containing boolean values) and now let's say if apply Dummy function on test data for same feature 'A' but this time it contains 6 variable i.e One extra variable, which will create 6 different features.\n\n## So here now problem arise our Model has been trained for 5 feature vector space and will be expecting 5 dimension vector on test data but here it contains 6 vector dimension....then boooom...... Error will showed up........\n\n## I hope you get it why it's not safe to use dummy variable...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom tqdm import tqdm\nobj_col = []\nnum_col = []\nfor col in X_train.columns:\n    if X_train[col].dtype=='O':\n        obj_col.append(col)\n    else:\n        num_col.append(col)\n\ntemp1 = pd.DataFrame()\ntemp2 = pd.DataFrame()\ntemp3 = pd.DataFrame()\nerror = []       \nfor i in tqdm(range(len(obj_col))):\n    try:\n        vec = CountVectorizer(stop_words=[])\n        vec.fit(X_train[obj_col[i]])\n        X1 = vec.transform(X_train[obj_col[i]])\n        X2 = vec.transform(X_test[obj_col[i]])\n        X3 = vec.transform(test[obj_col[i]])\n    except: \n        error.append(obj_col[i])\n        continue\n    \n    feature_name = []\n    for f in vec.get_feature_names():\n        feature_name.append('f'+str(i)+'_'+f)\n        \n    X1 = pd.DataFrame(X1.toarray(),columns = feature_name)\n    X2 = pd.DataFrame(X2.toarray(),columns = feature_name)\n    X3 = pd.DataFrame(X3.toarray(),columns = feature_name)\n    temp1 = pd.concat([temp1,X1],axis=1)\n    temp2 = pd.concat([temp2,X2],axis=1)\n    temp3 = pd.concat([temp3,X3],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train[num_col]\nX_test = X_test[num_col]\ntest = test[num_col]\nprint(X_train.shape)\nprint(X_test.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.concat([X_train,temp1],axis=1)\nX_test = pd.concat([X_test,temp2],axis=1)\ntest = pd.concat([test,temp3],axis=1)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's Define Performance Metric","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y_true,y_pred):\n    return(np.sqrt(mean_squared_error(y_true,y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scaling \n\n## Let's just bee clear here, Scaling is done most importantly for the algorithms which use distance metric for segregating points but In other case Tree based algorithms used splitting techniques.\n\n## If we scaled the input for Tree based algorithms , the result won't change........ ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"robust_scaler = RobustScaler()\nrobust_scaler.fit(X_train)\nX_tr = robust_scaler.transform(X_train)\nX_ts = robust_scaler.transform(X_test)\ntest = robust_scaler.transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Below we are going to create 5 models including stacked model.........\n\n1. Ridge Regression\n2. LightGBM\n3. Random Forest\n4. Gradient Boosting DT\n5. Stacked (of all above)\n\n## Then we will have weighted prediction based on which model above perform well...................\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Ridge Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nridge = Ridge()\nparam_dist = {'alpha':[1e-5,9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5,10,12,15,20,30,50],\n              'solver':['auto','svd','lsqr','cholesky','saga']}\n\nclf = GridSearchCV(ridge,\n                   param_grid=param_dist,\n                   cv=5,\n                   return_train_score=True,\n                   scoring = 'neg_mean_squared_error')\n\nclf.fit(X_tr,y_train['log(Price)'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = clf.best_estimator_ #Ridge(alpha = 12,solver = 'svd')\nridge.fit(X_tr,y_train['log(Price)'].values)\npred = ridge.predict(X_ts)\n\nprint(\"RMSE score for test data is :\",rmse(y_test['log(Price)'].values,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RandomForest Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rfg = RandomForestRegressor(n_estimators=500,\n                          max_depth=30,\n                          min_samples_split=2,\n                          min_samples_leaf=2,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\nrfg.fit(X_tr,y_train['log(Price)'].values)\npred = rfg.predict(X_ts)\nprint(\"RMSE score for test data is :\",rmse(y_test['log(Price)'].values,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Light GBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.002, \n                       n_estimators=10000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\nlightgbm.fit(X_tr,y_train['log(Price)'].values)\npred = lightgbm.predict(X_ts)\nprint(\"RMSE score for test data is :\",rmse(y_test['log(Price)'].values,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GradientBoostingRegressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor(n_estimators=2000,\n                                learning_rate=0.005,\n                                max_depth=5,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)\n\ngbr.fit(X_tr,y_train['log(Price)'].values)\npred = gbr.predict(X_ts)\nprint(\"RMSE score for test data is :\",rmse(y_test['log(Price)'].values,pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacked Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_gen = StackingRegressor(regressors=(lightgbm, gbr, rfg),\n                                meta_regressor=gbr)\nstack_gen.fit(X_tr,y_train['log(Price)'].values)\npred = stack_gen.predict(X_ts)\nprint(\"RMSE score for test data is :\",rmse(y_test['log(Price)'].values,pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ensemble(data):\n    return( 0.3*gbr.predict(data) + \n          0.2*lightgbm.predict(data) + \n         0.1*stack_gen.predict(data) + 0.1*rfg.predict(data)+ 0.3*ridge.predict(data))\n\nrmse(y_test['log(Price)'].values,ensemble(X_ts))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n# submission['SalePrice'] = (np.exp(gbr.predict(test)) + np.exp(lightgbm.predict(test)) + np.exp(stack_gen.predict(test)) + np.exp(rfg.predict(test))+ np.exp(ridge.predict(test)))/5\nsubmission['SalePrice'] = 0.20*np.exp(gbr.predict(test)+0.0) +  0.25*np.exp(lightgbm.predict(test)) +  0.35*np.exp(stack_gen.predict(test)) + 0.2*np.exp(rfg.predict(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Avoid doing data leakage............ while creating a model it's pretty obvious for someone accidently share information of test data with model................ This really mislead our model.\n\n## So, This concludes my work. Thank you for reading it............ I hope you enjoyed it.......... pls do upvote....\n","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}