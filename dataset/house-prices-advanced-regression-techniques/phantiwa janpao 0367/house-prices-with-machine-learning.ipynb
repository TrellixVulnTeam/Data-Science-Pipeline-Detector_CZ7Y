{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Predict House Price**"},{"metadata":{},"cell_type":"markdown","source":"**1. Importing Packages**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn import metrics\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom collections import Counter\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom scipy.stats import skew\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/house-prices-advanced-regression-techniques/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n# Check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n# Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n# Now drop the 'Id' column since it's unnecessary for the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n# Check data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###importing necesary libraries...\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns=train.columns[train.isnull().any()]\ntrain[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[null_columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columnst=test.columns[test.isnull().any()]\ntest[null_columnst].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[null_columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Categorical Data\ntrain.select_dtypes(include=['object']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Categorical Data\ntest.select_dtypes(include=['object']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Numerical Data\ntrain.select_dtypes(include=['int64','float64']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = len(train.select_dtypes(include=['object']).columns)\nnum = len(train.select_dtypes(include=['int64','float64']).columns)\nprint('Total Features: ', cat, 'categorical', '+',\n      num, 'numerical', '=', cat+num, 'features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking Numerical Data\ntest.select_dtypes(include=['int64','float64']).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = len(test.select_dtypes(include=['object']).columns)\nnum = len(test.select_dtypes(include=['int64','float64']).columns)\nprint('Total Features: ', cat, 'categorical', '+',\n      num, 'numerical', '=', cat+num, 'features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2.Correlation data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Matrix Heatmap\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Top 10 Heatmap\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_corr = pd.DataFrame(cols)\nmost_corr.columns = ['Most Correlated Features']\nmost_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Overall Quality vs Sale Price\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Living Area vs Sale Price\nsns.jointplot(x=train['GrLivArea'], y=train['SalePrice'], kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing outliers manually (Two points in the bottom right)\ntrain = train.drop(train[(train['GrLivArea']>4000) \n                         & (train['SalePrice']<300000)].index).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Living Area vs Sale Price\nsns.jointplot(x=train['GrLivArea'], y=train['SalePrice'], kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Garage Cars Area vs Sale Price\nsns.boxplot(x=train['GarageCars'], y=train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing outliers manually (More than 4-cars, less than $300k)\ntrain = train.drop(train[(train['GarageCars']>3) \n                         & (train['SalePrice']<300000)].index).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Garage Area vs Sale Price\nsns.boxplot(x=train['GarageCars'], y=train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Garage Area vs Sale Price\nsns.jointplot(x=train['GarageArea'], y=train['SalePrice'], kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing outliers manually (More than 1000 sqft, less than $300k)\ntrain = train.drop(train[(train['GarageArea']>1000) \n                         & (train['SalePrice']<300000)].index).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Garage Area vs Sale Price\nsns.jointplot(x=train['GarageArea'], y=train['SalePrice'], kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Basement Area vs Sale Price\nsns.jointplot(x=train['TotalBsmtSF'], y=train['SalePrice'], kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First Floor Area vs Sale Price\nsns.jointplot(x=train['1stFlrSF'], y=train['SalePrice'], kind='reg')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Rooms vs Sale Price\nsns.boxplot(x=train['TotRmsAbvGrd'], y=train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Total Rooms vs Sale Price\nvar = 'YearBuilt'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3.Imputing Null values and places where Null means something**"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_null = pd.isnull(train).sum()\ntesting_null = pd.isnull(test).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Train\", \"Test\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #not as much missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_many","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_few","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_has_meaning = [\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in null_has_meaning:\n    train[i].fillna(\"None\", inplace=True)\n    test[i].fillna(\"None\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns=train.columns[train.isnull().any()]\ntrain[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Imputing \"Real\" Null Values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom sklearn.impute import SimpleImputer\n\nSimpleImputer = SimpleImputer(strategy=\"median\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_null = pd.isnull(train).sum()\ntesting_null = pd.isnull(test).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Train\", \"Test\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_many = null[null.sum(axis=1) > 200]  #a lot of missing values\nnull_few = null[(null.sum(axis=1) > 0) & (null.sum(axis=1) < 200)]  #few missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_many","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(\"LotFrontage\", axis=1, inplace=True)\ntest.drop(\"LotFrontage\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_columns=train.columns[train.isnull().any()]\ntrain[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_few","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"GarageYrBlt\"].fillna(train[\"GarageYrBlt\"].median(), inplace=True)\ntest[\"GarageYrBlt\"].fillna(test[\"GarageYrBlt\"].median(), inplace=True)\ntrain[\"MasVnrArea\"].fillna(train[\"MasVnrArea\"].median(), inplace=True)\ntest[\"MasVnrArea\"].fillna(test[\"MasVnrArea\"].median(), inplace=True)\ntrain[\"MasVnrType\"].fillna(\"None\", inplace=True)\ntest[\"MasVnrType\"].fillna(\"None\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, the features with a lot of missing values have been taken care of! Let's move on to the features with fewer missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"types_train = train.dtypes #type of each feature in data: int, float, object\nnum_train = types_train[(types_train == int) | (types_train == float)] #numerical values are either type int or float\ncat_train = types_train[types_train == object] #categorical values are type object\n\n#we do the same for the test set\ntypes_test = test.dtypes\nnum_test = types_test[(types_test == int) | (types_test == float)]\ncat_test = types_test[types_test == object]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Numerical Imputing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#we should convert num_train and num_test to a list to make it easier to work with\nnumerical_values_train = list(num_train.index)\nnumerical_values_test = list(num_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(numerical_values_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are all the numerical features in our data."},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_num = []\n\nfor i in numerical_values_train:\n    if i in list(null_few.index):\n        fill_num.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fill_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in fill_num:\n    train[i].fillna(train[i].median(), inplace=True)\n    test[i].fillna(test[i].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorical Imputing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(categorical_values_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are all the categorical features in our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_cat = []\n\nfor i in categorical_values_train:\n    if i in list(null_few.index):\n        fill_cat.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(fill_cat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the categorical features in the data that have missing values in them. We'll impute with the most common term below."},{"metadata":{"trusted":true},"cell_type":"code","source":"def most_common_term(lst):\n    lst = list(lst)\n    return max(set(lst), key=lst.count)\n#most_common_term finds the most common term in a series\n\nmost_common = [\"Electrical\", \"Exterior1st\", \"Exterior2nd\", \"Functional\", \"KitchenQual\", \"MSZoning\", \"SaleType\", \"Utilities\", \"MasVnrType\"]\n\ncounter = 0\nfor i in fill_cat:\n    most_common[counter] = most_common_term(train[i])\n    counter += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_common_dictionary = {fill_cat[0]: [most_common[0]], fill_cat[1]: [most_common[1]], fill_cat[2]: [most_common[2]], fill_cat[3]: [most_common[3]],\n                          fill_cat[4]: [most_common[4]], fill_cat[5]: [most_common[5]], fill_cat[6]: [most_common[6]], fill_cat[7]: [most_common[7]],\n                          fill_cat[8]: [most_common[8]]}\nmost_common_dictionary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"counter = 0\nfor i in fill_cat:  \n    train[i].fillna(most_common[counter], inplace=True)\n    test[i].fillna(most_common[counter], inplace=True)\n    counter += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_null = pd.isnull(train).sum()\ntesting_null = pd.isnull(test).sum()\n\nnull = pd.concat([training_null, testing_null], axis=1, keys=[\"Training\", \"Testing\"])\nnull[null.sum(axis=1) > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**4.Feature Engineering**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(np.log(train[\"SalePrice\"]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"TransformedPrice\"] = np.log(train[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_values_train = list(cat_train.index)\ncategorical_values_test = list(cat_test.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(categorical_values_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in categorical_values_train:\n    feature_set = set(train[i])\n    for j in feature_set:\n        feature_list = list(feature_set)\n        train.loc[train[i] == j, i] = feature_list.index(j)\n\nfor i in categorical_values_test:\n    feature_set2 = set(test[i])\n    for j in feature_set2:\n        feature_list2 = list(feature_set2)\n        test.loc[test[i] == j, i] = feature_list2.index(j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! It seems like we have changed all the categorical strings into a representative number. We are ready to build our models!"},{"metadata":{},"cell_type":"markdown","source":"**5.Creating, Training, Evaluating, Validating, and Testing ML Models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Defining Training/Test Sets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_training = train.drop([\"Id\", \"SalePrice\", \"TransformedPrice\"], axis=1).values\ny_training = train[\"TransformedPrice\"].values\nX_test = test.drop(\"Id\", axis=1).values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Splitting into Validation**\n\nIt is always good to split our training data again into validation sets. This will help us evaluate our model performance as well as avoid overfitting our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_training, y_training, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear Regression Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"linreg = LinearRegression()\nlinreg.fit(X_train, y_train)\nlin_pred = linreg.predict(X_valid)\nr2_lin = r2_score(y_valid, lin_pred)\nrmse_lin = np.sqrt(mean_squared_error(y_valid, lin_pred))\nprint(\"R^2 Score: \" + str(r2_lin))\nprint(\"RMSE Score: \" + str(rmse_lin))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_lin = cross_val_score(linreg, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lin)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree Regressor Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtr = DecisionTreeRegressor()\ndtr.fit(X_train, y_train)\ndtr_pred = dtr.predict(X_valid)\nr2_dtr = r2_score(y_valid, dtr_pred)\nrmse_dtr = np.sqrt(mean_squared_error(y_valid, dtr_pred))\nprint(\"R^2 Score: \" + str(r2_dtr))\nprint(\"RMSE Score: \" + str(rmse_dtr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_dtr = cross_val_score(dtr, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_dtr)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Regressor Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor()\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_valid)\nr2_rf = r2_score(y_valid, rf_pred)\nrmse_rf = np.sqrt(mean_squared_error(y_valid, rf_pred))\nprint(\"R^2 Score: \" + str(r2_rf))\nprint(\"RMSE Score: \" + str(rmse_rf))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_rf = cross_val_score(rf, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_rf)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lasso Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso()\nlasso.fit(X_train, y_train)\nlasso_pred = lasso.predict(X_valid)\nr2_lasso = r2_score(y_valid, lasso_pred)\nrmse_lasso = np.sqrt(mean_squared_error(y_valid, lasso_pred))\nprint(\"R^2 Score: \" + str(r2_lasso))\nprint(\"RMSE Score: \" + str(rmse_lasso))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_lasso = cross_val_score(lasso, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_lasso)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ridge**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge()\nridge.fit(X_train, y_train)\nridge_pred = ridge.predict(X_valid)\nr2_ridge = r2_score(y_valid, ridge_pred)\nrmse_ridge = np.sqrt(mean_squared_error(y_valid, ridge_pred))\nprint(\"R^2 Score: \" + str(r2_ridge))\nprint(\"RMSE Score: \" + str(rmse_ridge))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores_ridge = cross_val_score(ridge, X_train, y_train, cv=10, scoring=\"r2\")\nprint(\"Cross Validation Score: \" + str(np.mean(scores_ridge)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**6.Valuation Models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_performances = pd.DataFrame({\n    \"Model\" : [\"Linear Regression\", \"Decision Tree Regressor\", \"Random Forest Regressor\",\"Ridge\", \"Lasso\"],\n    \"R Squared\" : [str(r2_lin)[0:5], str(r2_dtr)[0:5], str(r2_rf)[0:5], str(r2_ridge)[0:5], str(r2_lasso)[0:5]],\n    \"RMSE\" : [str(rmse_lin)[0:8], str(rmse_dtr)[0:8], str(rmse_rf)[0:8], str(rmse_ridge)[0:8], str(rmse_lasso)[0:8]]\n})\nmodel_performances.round(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Sorted by R Squared:\")\nmodel_performances.sort_values(by=\"R Squared\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Sorted by RMSE:\")\nmodel_performances.sort_values(by=\"RMSE\", ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linreg.fit(X_training, y_training)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**7.Submission**"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_predictions = np.exp(linreg.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": submission_predictions\n    })\n\nsubmission.to_csv(\"saleprice_3_group6.csv\", index=False)\nprint(submission.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}