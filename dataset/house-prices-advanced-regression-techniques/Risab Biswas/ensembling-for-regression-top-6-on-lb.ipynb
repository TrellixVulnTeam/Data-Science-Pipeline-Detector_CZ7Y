{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, let us try and explore the data given for **House Prices: Advanced Regression Techniques**. Before we dive deep into the data, let us know a little more about the competition.\n\n**What decides a House Price?**\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But the ral world experiments proves that price negotiations are much more dependent on other **Factors** rather than the number of bedrooms or a white-picket fence.\n\n**Objective:**\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home."},{"metadata":{},"cell_type":"markdown","source":"# Contents\n1. [Importing Packages](#p1)\n2. [Loading Data](#p2)\n3. [Imputing Null Values](#p3)\n4. [Feature Engineering](#p4)\n5. [Creating, Training, Evaluating, Validating, and Testing ML Models](#p5)\n6. [Submission](#p6)"},{"metadata":{},"cell_type":"markdown","source":"**Let's Do Some Real \"Work\"**\n\nWe are creating this Notebook to illustrate that how you can \"Approach almost any Regression Problem\" \n\n**A Comprehensive Checklist for Solving Any Regression Problem:**\n\n* Data Fetching\n* Understanding the Data\n* Checking the Skwewness of the Output Variable\n* Performing Log Transformation (if required)\n* Exploratory Data Analysis\n* Analysing Correlation\n* Finding out Important Predictors\n* Feature Engineering: -\n     1. Missing Values\n     2. Outliers\n     3. Categorical Feature Encoding     \n* Creating Folds and Defining Fold Map\n* Defining Models\n* Fitting the Model and Running Cross Validation\n* Stacking and Ensembling\n* Hyperparameter Optimization\n\nThe above mentioned checklist is very importnant for solving any regression based problem. The similar kind of checklist can also be prepared for other Machine Learning based problems. I will cover them in my upcoming kernals.\n\nLet's begin!\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#import some of the necessary libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import norm\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(15,12)})\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us first import the training and the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's do a little exploration on the training file."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['SalePrice'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Thus,**\n\n* Total Observations in the Training Data : - 1460\n* Total Features in the Training Data : - 79, Excluding Id Column and Dependent Variable i.e. SalePrice\n    \n**Description of the Training Data: -**\n\n1. Mean Value      -->  180921.195890\n2. Std. Deviation  -->  79442.502883\n3. Min Value       -->  34900.00\n4. Max Value       -->  755000.00"},{"metadata":{},"cell_type":"markdown","source":"Now let's have the Training and Testing ID's  aved in a dataframe for future references. As you know for any machine learning based problem the Id doesn't make a feature, so we are going to drop the ID column from our train and test dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ID = train_df['Id']\ntest_ID = test_df['Id']\ntrain_df.drop(\"Id\", axis = 1, inplace = True)\ntest_df.drop(\"Id\", axis = 1, inplace = True)\n#Deleting outliers\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, **SalePrice** is what we have to Predict. So we will first start with Checking the Distribution of the Variable and Let's see how much Skewness it has got."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(18,8)})\nsns.distplot(train_df['SalePrice'],fit=norm)\n\n(mu, sig) = norm.fit(train_df['SalePrice'])\n#Now plot the distribution\nplt.legend(['Normal Distribution Curve ($\\mu=$ {:.2f} & $\\sigma=$ {:.2f} )'.format(mu, sig)])\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By Analyzing the Graph we can see the following:**\n\n* Deviation from the Normal Distribution.\n* Have Positive Skewness.\n* Show Peakedness\n\nAlso let's see the measurement of Skewness: "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness of Sale Price is: \",train_df['SalePrice'].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Looks like our Data is Skewed Towards Right.**\n\n* We are normalising the data by simply takithe Natural Log and then adding 1.\n\n**Why do we need to make the Data Normal?**\n\nSince Machine Learning or Data Science is nothing but Glorified Statistics at the end of the day and most of the algorithms assumes that the data is that the data is normal and it calculates various stats assuming this. So the more the data is close to normal, the better it fits the assumption.\n\n**Log Transformation: -**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['SalePrice'] = np.log(train_df['SalePrice']+1)\nsns.distplot(train_df['SalePrice'],fit=norm)\n\n(mu, sig) = norm.fit(train_df['SalePrice'])\n#Now plot the distribution\nplt.legend(['Normal Distribution Curve ($\\mu=$ {:.2f} & $\\sigma=$ {:.2f} )'.format(mu, sig)])\nplt.ylabel('Frequency')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This is the Normalised Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness of Sale Price is: \",train_df['SalePrice'].skew())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now, Let's do Some More Feature Analysis**\n\nThe First and Foremost Important thing is to See that What are the Relevant Features. Not all features might be useful for our prediction and Having all the unnecessary features is going to make our model complex and we don't want the dimensionality to be huge!"},{"metadata":{},"cell_type":"markdown","source":"Let's Generate the Correlation Matrix [](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"corremap = train_df.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corremap, vmax=0.9, square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the Correlation Map, We Got: **\n\n* OverallQual, GrLivArea, TotalBsmtSF, GarrageCars, GarrageArea, 1stFlrSF, YearBuilt, FullBath are the most important Predictors.\n* We can see from the above graph that how significantly they are related to our output variable \"SalePrice\""},{"metadata":{},"cell_type":"markdown","source":"**Let's have an Eagle's Eye View! **"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set()\ncolumns = ['OverallQual', 'GrLivArea', 'TotalBsmtSF','GarageCars', 'GarageArea','1stFlrSF', 'FullBath', 'YearBuilt','SalePrice']\nsns.pairplot(train_df[columns], size = 2)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**According to our tarot card, these are the variables most correlated with 'SalePrice'**\n\n* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. \n* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point,    the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. \n* 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. \n* 'FullBath'?? Really?\n* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again.\n* It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start     feeling that we should do a little bit of time-series analysis to get this right."},{"metadata":{},"cell_type":"markdown","source":"Now Let's See the Relationship Between the Predictors and the \"SalePrice\"\n\n**Let's See who makes the Best Couple?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'GrLivArea'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(var,'SalePrice');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems that 'SalePrice' and 'GrLivArea' are really in love with each other, with a **linear relationship**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot grlivarea/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(var,'SalePrice');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'TotalBsmtSF' also have a great bond with 'SalePrice' but this seems a much more emotional relationship! Everything is ok and suddenly, in a strong linear (exponential?) reaction, everything changes. We can call it a **Mood Swing!** Moreover, it's clear that sometimes 'TotalBsmtSF' closes in itself and gives no credit to 'SalePrice'."},{"metadata":{},"cell_type":"markdown","source":"**Let's Analyze others as well!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot grlivarea/saleprice\nvar = 'GarageArea'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(var,'SalePrice');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot grlivarea/saleprice\nvar = '1stFlrSF'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\ndata.plot.scatter(var, 'SalePrice');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analyzing the Categorical Predictors**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#box plot overallqual/saleprice\nvar = 'OverallQual'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nsns.boxplot(x=var,y='SalePrice',hue=var,data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#box plot overallqual/saleprice\nf, ax = plt.subplots(figsize=(20, 16))\nplt.xticks(rotation='90')\nvar = 'YearBuilt'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nsns.boxplot(x=var,y='SalePrice',data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#box plot overallqual/saleprice\nvar = 'GarageCars'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nsns.violinplot(x=var,y='SalePrice',data=data,palette='rainbow', hue = 'GarageCars')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'FullBath'\ndata = pd.concat([train_df['SalePrice'], train_df[var]], axis=1)\nsns.violinplot(x=var,y='SalePrice',data=data,palette='rainbow', hue = 'FullBath')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now that we are done with most of the Feature Analysis, Let's Beging with the Feature Engineering!**"},{"metadata":{},"cell_type":"markdown","source":"Let's Concat the Training and the Test Data to a Complete Dataframe. This has to be done because both our training and testing data might contain missing values, outliers and may require categorical features to be handelled. \n\nTherefore rather than doing them seperatly we can do it together to save our time, so that we can waste the time somewhere else!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ntrain = train_df.shape[0]\nntest = test_df.shape[0]\ny_train = train_df.SalePrice.values\ncomp_data = pd.concat((train_df, test_df)).reset_index(drop=True)\ncomp_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"Comp_data size is : {}\".format(comp_data.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Our Feature Engineering begins with Handling Missing Data!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val = comp_data.isnull().sum().sort_values(ascending=False)\nmissing_val_df = pd.DataFrame({'Feature':missing_val.index, 'Count':missing_val.values})\nmissing_val_df = missing_val_df.drop(missing_val_df[missing_val_df.Count == 0].index)\nmissing_val_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above dataframe illustrates the count of Missing values corresponding to each observation."},{"metadata":{},"cell_type":"markdown","source":"**Numbers our not my Stuff! Let's See some Graphs!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='60')\nplt.title('Count of Missing Data Per Feature', fontsize=15)\nsns.barplot(x = 'Feature', y = 'Count', data = missing_val_df,\n            palette = 'cool', edgecolor = 'b')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now it looks Beautiful!**"},{"metadata":{},"cell_type":"markdown","source":"So, there are 2920 observations in are Complete dataframe, i,.e 1460 in both train and test dataframe. Now if we see the top three predictors from our missing value dataframe we see that most of them are close to 2920 that means,most of the observations from those predictors are not present."},{"metadata":{},"cell_type":"markdown","source":"**Let's Handle the Missing Values!**"},{"metadata":{},"cell_type":"markdown","source":"**2. Handling Missing Values in Numerical Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_data[\"LotFrontage\"] = comp_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    comp_data[col] = comp_data[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    comp_data[col] = comp_data[col].fillna(0)\ncomp_data['MSZoning'] = comp_data['MSZoning'].fillna(comp_data['MSZoning'].mode()[0])\ncomp_data[\"MasVnrArea\"] = comp_data[\"MasVnrArea\"].fillna(0)\ncomp_data['Electrical'] = comp_data['Electrical'].fillna(comp_data['Electrical'].mode()[0])\ncomp_data['SaleType'] = comp_data['SaleType'].fillna(comp_data['SaleType'].mode()[0])\ncomp_data['KitchenQual'] = comp_data['KitchenQual'].fillna(comp_data['KitchenQual'].mode()[0])\ncomp_data['Exterior1st'] = comp_data['Exterior1st'].fillna(comp_data['Exterior1st'].mode()[0])\ncomp_data['Exterior2nd'] = comp_data['Exterior2nd'].fillna(comp_data['Exterior2nd'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**3. Handling Missing Values in Categorical Data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('PoolQC', 'MiscFeature', 'Alley'):\n    comp_data[col] = comp_data[col].fillna('None')\ncomp_data[\"MasVnrType\"] = comp_data[\"MasVnrType\"].fillna(\"None\")\ncomp_data[\"Fence\"] = comp_data[\"Fence\"].fillna(\"None\")\ncomp_data[\"FireplaceQu\"] = comp_data[\"FireplaceQu\"].fillna(\"None\")\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    comp_data[col] = comp_data[col].fillna('None')\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    comp_data[col] = comp_data[col].fillna('None')\ncomp_data['MSSubClass'] = comp_data['MSSubClass'].fillna(\"None\")\ncomp_data['SaleType'] = comp_data['SaleType'].fillna(comp_data['SaleType'].mode()[0])\ncomp_data = comp_data.drop(['Utilities'], axis=1)\ncomp_data['OverallCond'] = comp_data['OverallCond'].astype(str)\ncomp_data[\"Functional\"] = comp_data[\"Functional\"].fillna(\"Typ\")  \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's Handle the Categorical Features!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_data['MSSubClass'] = comp_data['MSSubClass'].apply(str)\ncomp_data['YrSold'] = comp_data['YrSold'].astype(str)\ncomp_data['MoSold'] = comp_data['MoSold'].astype(str)\nfrom sklearn.preprocessing import LabelEncoder\ncolumns = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\nfor col in columns:\n    labl = LabelEncoder() \n    labl.fit(list(comp_data[col].values)) \n    comp_data[col] = labl.transform(list(comp_data[col].values))     \nprint('Shape all_data: {}'.format(comp_data.shape))\ncomp_data['TotalSF'] = comp_data['TotalBsmtSF'] + comp_data['1stFlrSF'] + comp_data['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Looking at Skewed Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm, skew\nnumeric_feats = comp_data.dtypes[comp_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = comp_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Box Cox Transformation on Skewed Features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    comp_data[feat] = boxcox1p(comp_data[feat], lam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating Dummy Columns for Label Encoding the Categorical Features."},{"metadata":{"trusted":true},"cell_type":"code","source":"comp_data = pd.get_dummies(comp_data)\nprint(comp_data.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we are done with our Feature Engineering,We can now seperate thhe Training and Test Data from the complete data."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = comp_data[:ntrain]\ntest_df = comp_data[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's Create the Folds for Our Cross Validation Model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nn_folds = 5\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_df.values)\n    rmse= np.sqrt(-cross_val_score(model, train_df.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's Define the Models and Do a Scoring!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge,ElasticNet,Lasso\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.kernel_ridge import KernelRidge\nimport xgboost as xgb\nimport lightgbm as lgb\n\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nscore = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Analyze the Above Results Well!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Final Training and Prediction**"},{"metadata":{},"cell_type":"markdown","source":"* XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_xgb.fit(train_df, y_train)\nxgb_train_pred = model_xgb.predict(train_df)\nxgb_pred = np.expm1(model_xgb.predict(test_df))\nprint(rmsle(y_train, xgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_lgb.fit(train_df, y_train)\nlgb_train_pred = model_lgb.predict(train_df)\nlgb_pred = np.expm1(model_lgb.predict(test_df.values))\nprint(rmsle(y_train, lgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**And Finally! Let's Make the Submission Dataframe!**\n**You have Done it!**"},{"metadata":{},"cell_type":"markdown","source":"We are going to generate our final submission by Ensembling the XGBoost and LightGBM Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = xgb_pred*0.5 + lgb_pred*0.5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Id'] = test_ID\nsubmission['SalePrice'] = ensemble\n\nprint(\"Creating Submission File\")\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Conclusion**\n\nThat's it! We reached the end of our exercise. We Saw how a Simple Model gave us Great Results!\n\nIf you liked the Kernal then don't forget to hit the Upvote! :) Also, Suggestion are always Welcomed! Post your Doubts and Suggestions on the Comment Section."},{"metadata":{},"cell_type":"markdown","source":"**References**\n\n* [COMPREHENSIVE DATA EXPLORATION WITH PYTHON](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}