{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install -q dabl","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport dabl\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\npd.options.display.max_columns = 8000","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the columns: `Alley`, `PoolQC`, `Fence` and `MiscFeature` have more than 1000 NULL Values, when the data itself has 1460 total samples. For the sake of simplicity, I will just drop them and not include in the main dataset.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also remove outliers from traning data before joining","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove outliers from OverallQual, GrLivArea and SalesPrice\ntrain_data.drop(train_data[(train_data['OverallQual']<5) & (train_data['SalePrice']>200000)].index, inplace=True)\ntrain_data.drop(train_data[(train_data['GrLivArea']>4500) & (train_data['SalePrice']<300000)].index, inplace=True)\ntrain_data.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I will join both train and test data so that we can process all the data at once.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train_data['SalePrice']\ntrain_features = train_data.drop(['SalePrice'], axis=1)\n\ndata = pd.concat([train_features, test_data]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# These columns have a lot of Null values, so we drop them\ndata = data.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA\nLet's start with EDA and keep the note of things along the way","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## MSSubClass\nThis feature Identifies the type of dwelling involved in the sale.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"fivethirtyeight\")\nplt.figure(figsize=(16, 9))\nsns.distplot(data['MSSubClass'])\nplt.xlabel(\"Type of Dwelling\")\nplt.ylabel(\"Count\")\nplt.title(\"Dwelling Type Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## MSZoning\nThis feature identifies general zoning classification of the sale.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"ggplot\")\nplt.figure(figsize=(16, 9))\nsns.countplot(data['MSZoning'])\nplt.xlabel(\"Type of Zoning of the property\")\nplt.ylabel(\"Count\")\nplt.title(\"Zone Type Count\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lot Frontage\nThis feature tells about the Linear feet of Street Connected to the Property","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"classic\")\nplt.figure(figsize=(16, 9))\nsns.distplot(data['LotFrontage'])\nplt.xlabel(\"Lot Frontage (in ft)\")\nplt.ylabel(\"Count\")\nplt.title(\"Lot Frontage Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sales Price\nLet's jump directly to sales price, since it will take a lot of time to visualize every single feature all by itself!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"classic\")\nplt.figure(figsize=(16, 9))\nsns.distplot(train_labels, color='red')\nplt.xlabel(\"Price (in $)\")\nplt.ylabel(\"Count\")\nplt.title(\"Sales Price Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see one very important thing from above which is that our target column (SalesPrice) is shifted to the left. In other word, it's **skewed to the left**.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Pairplot\nLet's see the correlation pair plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data.corr())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Heatmap\nAlso see the heatmap of correlation of different features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16, 9))\nsns.heatmap(data.corr())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DABL\nLet's also look at DABL Plot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dabl.plot(train_data, target_col='SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Sales price\nTo get rid of the data skewness, we have to log shift the data.\nWe can apply `log(1+x)` to out data to shift it at center.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train_labels.apply(lambda x: np.log(1+x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's replot Sales price, to see if the skewness is gone","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"classic\")\nplt.figure(figsize=(16, 9))\nsns.distplot(train_labels, color='red')\nplt.xlabel(\"Price (in $)\")\nplt.ylabel(\"Count\")\nplt.title(\"Sales Price Distribution\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the data is now centered in the middle and the skewness is gone.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Dealing with NuLL Values\nSince the data still had a lot of Null values, we are dealing with them here.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['MSSubClass'] = data['MSSubClass'].apply(str)\ndata['YrSold'] = data['YrSold'].astype(str)\ndata['MoSold'] = data['MoSold'].astype(str)\n\n# the data description states that NA refers to typical ('Typ') values\ndata['Functional'] = data['Functional'].fillna('Typ')\n# Replace the missing values in each of the columns below with their mode\ndata['Electrical'] = data['Electrical'].fillna(\"SBrkr\")\ndata['KitchenQual'] = data['KitchenQual'].fillna(\"TA\")\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\ndata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])\ndata['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n# Replacing the missing values with 0, since no garage = no cars in garage\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)\n# Replacing the missing values with None\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    data[col] = data[col].fillna('None')\n# NaN values for these categorical basement features, means there's no basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    data[col] = data[col].fillna('None')\n\n# Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\ndata['LotFrontage'] = data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# We have no particular intuition around how to fill in the rest of the categorical features\n# So we replace their missing values with None\nobjects = []\nfor i in data.columns:\n    if data[i].dtype == object:\n        objects.append(i)\ndata.update(data[objects].fillna('None'))\n\n# And we do the same thing for numerical features, but this time with 0s\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in data.columns:\n    if data[i].dtype in numeric_dtypes:\n        numeric.append(i)\ndata.update(data[numeric].fillna(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(['Id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, no other columns now have null values in them.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Encode Categorical Features\nLet's encode categorical features in our data to make them suitable for our models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make a list of all categorical columns\ncat_cols = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']\n\n# Get the dummy variables from them\ndata = pd.get_dummies(data, columns=cat_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are now `286` features in this dataset!","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Recheck the shape of the data\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split the Data\nLet's now finally split the data back into their respective sets","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove any repeated columns\ndata = data.iloc[:, ~data.columns.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Identify the split percent and split the data\ntrain = data[:len(train_labels)]\ntest = data[len(train_labels):]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling\nLet's now get to modelling our data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define some metrics\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, train=train, train_labels=train_labels):\n    rmse = np.sqrt(-cross_val_score(model, train.values, train_labels.values, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# First Make 5-Folds for cross validation\nkf = KFold(n_splits=10, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start with Cross Validation Scores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of all regressors\nregs = [(lightgbm, \"Light Gradient Boosting Regressor\"), (xgboost, \"X-Gradient Boosting Regressor\"), (ridge, \"Ridge Regressor\"), (svr, \"Support Vector Regressor\"), (gbr, \"Gradient Boosting Regressor\"), (rf, \"Random Forest Regressor\"), (stack_gen, \"All Model Stacked\")]\n\n# We will store all the scores in here\ncv_scores = {}\n\n# Calculate CV-RMSE Scores for all regressors\nfor reg, reg_name in regs:\n    sc = cv_rmse(reg)\n    cv_scores[reg_name] = (sc.mean(), sc.std())\n    print(f\"Calculating CV-RMSE for {reg_name} ==> Score Mean: {sc.mean():.2f} | Score Std: {sc.std():.2f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we fit all the above models and then get the final model which we will use to blend our predictions\nfor model, model_name in regs:\n    print('='*40)\n    print(f\"Fitting {model_name}...\")\n    model.fit(train.values, train_labels.values)\n    val_score_temp = model.score(train.values, train_labels.values)\n    print(f\"val_acc: {val_score_temp:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's blend the predictions of all of our models and make blended prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def blended_predictions(X):\n    return ((0.1 * regs[2][0].predict(X)) + \\\n            (0.2 * regs[3][0].predict(X)) + \\\n            (0.1 * regs[4][0].predict(X)) + \\\n            (0.1 * regs[1][0].predict(X)) + \\\n            (0.1 * regs[0][0].predict(X)) + \\\n            (0.05 * regs[5][0].predict(X)) + \\\n            (0.35 * regs[6][0].predict(np.array(X))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"blended_score = rmsle(train_labels.values, blended_predictions(train.values))\ncv_scores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(cv_scores.keys()), y=[score for score, _ in cv_scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(cv_scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the Blended Model Prediction has the lowest of all losses. So we will use it to predict.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"New let's do the predictions on this data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# First we load the submission file\nsub = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we make predictions on the test data\npreds = blended_predictions(test.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['SalePrice'] = np.floor(np.expm1(preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv(\"submission_fixed.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}