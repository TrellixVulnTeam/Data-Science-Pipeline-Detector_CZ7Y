{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', None) \nsns.set_palette(\"Set2\")\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\nsample = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\")\ntext_file = open(\"/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\", \"r\")\ndescription = text_file.read()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data file contains three types: int64, object, float types. \nWe can firstly check each type of attribute, then we want to deal with NaN value. If the type is numeric attribute, we fill it with 0. If the type is categorical value, we fill it with \"missing\". If there are significant missing value (90% are null) in a feature, we drop that feature. \n## Categorical attribute\n Firstly, working with categorical attributes, lets see what information the data provides."},{"metadata":{"trusted":true},"cell_type":"code","source":"objectscol = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n       'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature',\n       'SaleType', 'SaleCondition']  #<---train.describe(include =np.object).columns\n\ninfotable = train.describe(include =np.object)\ninfotable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"For each attributes in the info table, we firstly focusing on the missing value. \n- step 1: remove the 90% null value attributes. "},{"metadata":{"trusted":true},"cell_type":"code","source":"for attribute in objectscol:\n    x = infotable.loc['count'][attribute]/1460\n    if x < 0.1:print(attribute)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- step 2: fill the rest of object's null value with \"missing\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop([\"Alley\",\"PoolQC\",\"MiscFeature\"], axis = 1)\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"objectcol = train.describe(include = np.object).columns\ntrain[objectcol] = train[objectcol].fillna(\"missing\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](http://) Take an example of *Fence* attribute, we noticed the majority is filled with missing value."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train[\"Fence\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(objectcol)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take an overview of all the catogrical attributes."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,50))\nfig.subplots_adjust(hspace=0.8, wspace=0.8)\n\nfor i in range(1, 41):\n    ax = fig.add_subplot(20, 2, i)\n    g = sns.countplot(train[objectcol[i-1]])\n    plt.xticks(rotation=45)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"> ## Numeric Attributes\n- Float64\n- Int64\n#### Now, move to the numeric attributes:\n- step 1: find all the numeric columns.\n- step 2: check the nan values percentatge, if it is over 90% are missing, then we delete this attributes\n- step 3: fill the rest attributes naa with 0 if nothing indicated .\n\nFloat64 is already using 0 represnet NANs, we simply check the 0 situation, same applt for Int64."},{"metadata":{"trusted":true},"cell_type":"code","source":"flotcol = train.describe(include = np.float64).columns\ninfortable2 = train.describe(include = np.float64)\nfor attribute in flotcol:\n    x = infortable2.loc['count'][attribute]/1460\n    if x < 0.1:\n        print(\"found 90% null attribute:{}\",attribute)\n    else:\n        print(\"no attribute found with over 90% empty\")\n        pass\n        #nothing happend\ntrain[flotcol]=train[flotcol].fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(15,5))\nfig.subplots_adjust(hspace=0.8, wspace=0.8)\nfor i in range(1, 4):\n    ax = fig.add_subplot(1, 3, i)\n    sns.distplot(train[flotcol[i-1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# INT64 attribiutes\nintcol = train.describe(include = np.int64).columns\ninfotable3 = train.describe(include = np.int64)\ninfotable3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for attribute in intcol:\n    x = infotable3.loc['count'][attribute]/1460\n    if x < 0.1:\n        print(\"found 90% null attribute:{}\",attribute)\n    else:\n        pass\n        #nothing happend\nprint(\"finished\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(intcol)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #### Plot the overview of int64 attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.distplot(train[\"MSSubClass\"])\nfig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(hspace=0.8, wspace=0.8)\nfor i in range(1, 36):\n    ax = fig.add_subplot(7, 5, i)\n    sns.distplot(train[intcol[i-1]])\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Relation exploration\nTake a closer look at the relationship between year sold and housing price: "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.violinplot(y = \"SalePrice\", x = \"YrSold\", data = train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The range of the price are not changing much alone five years"},{"metadata":{},"cell_type":"markdown","source":"> #### There are many attributes (80)! I will frist make a feature selection then draw the correlation graphs to seek correlations.\n## Feature Importance check[](http://)\n> #### Many ways are possible for feature selection, hereby I choose to use   *Extra Trees Ckaasifier*, which is an ensemble method could plot feature importance. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\ntarget = train['SalePrice']\nfeatures = train.drop(columns = ['SalePrice','Id'])\n#only possible for numeric features, so we here only use numeric\nnon_obj_features = features.describe(exclude = np.object).columns\nfeatures = features[non_obj_features]\nbeforefs = features.columns\nprint(\"before feature selection:{} number of features.\".format(len(non_obj_features)))\n#create a classifer for feature selection\nforest  = ExtraTreesRegressor(n_estimators = 50)\nforest = forest.fit(features, target)\nimportance_table = pd.Series(forest.feature_importances_, index=features.columns)\nimportance_table = importance_table.sort_values(ascending = False)\n\nplt.figure(figsize=(16,8))\nax = sns.barplot(x = importance_table.index[:10], y = importance_table.values[:10], palette=\"BuGn_r\")\nax.set_title('Feature Importance')\n\nselect_feature = importance_table.index[:10]\n#importance_table","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"select_feature ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the forest regressor we have select 10 most important features, they are  * 'OverallQual', 'GarageCars', 'GrLivArea', 'FullBath', 'TotalBsmtSF','GarageArea', 'YearBuilt', 'Fireplaces', '1stFlrSF', 'BsmtFinSF1' *\n\n- Apparently \"OverallQual\" is the most relevant attribute in terms of the sale price. This is indicateing the Overall material and finish quality of a house. \n- \"GarageCars\" is the size of garage in car capacity, ranging from 0 - 4 in this case. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = sns.PairGrid(train[select_feature[0:5]])\nfig.map_offdiag(plt.scatter)\nfig.map_diag(sns.distplot, bins=30)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We noticed from selected features part I:\n- Most of GrLivArea (Above ground living area square feet) is between 0-4000. \n- The lower the GrliveArea corresponse with less number of fullBath (0,1,2,3)\n- The OverallQual( Rates the overall material and finish of the house, which is range from 1-10) shows houese build from 1950 are mostly above or equal to level 5- average. \n\nLets move to the second five other attributes have selected.[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = sns.PairGrid(train[select_feature[5:]])\nfig.map_offdiag(plt.scatter)\nfig.map_diag(sns.distplot, bins=30);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"We noticed from selected features part II:\n- TotalBsmtSF (Total square feet of basement area) is strongly increasing as well as the 1stFLrSF(First Floor square feet). And nomrally 1st floor sf is just bigger than basement. \n- BsmtFinSF1 (Basement finihsed area type1 in square feet) is correlated with TotalBsmtSF and 1stFLrSF\n\nAfter Checked the ten most correlated features according to ExtraTreesRegressor, we noticed there are few correlations exist between basement data. Next, we will check the correlation from features with target ->sale price.\n\n## More pattern discovery\n#### 1.The correlation between sale price and selected 10 features"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,20))\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n\nfor i in range(1, 11):\n    ax = fig.add_subplot(5, 2, i)\n    g = sns.scatterplot(x = train[select_feature[i-1]], y=\"SalePrice\", data=train)\n    g.set_xlabel(g.get_xlabel(),fontsize= 20)\n    g.set_ylabel(g.get_ylabel(),fontsize= 15)\n    plt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### 2.Sale price with all the categorical features, if we noticed there some attributes exist patterns,we can encode it later. \nThe way I present is Violin Graph, there is a blog nicely describes how we can interpret: https://mode.com/blog/violin-plot-examples"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,150))\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\n\nfor i in range(1, 41):\n    ax = fig.add_subplot(20, 2, i)\n    g = sns.violinplot(x = train[objectcol[i-1]], y=\"SalePrice\", data=train)\n    g.set_xlabel(g.get_xlabel(),fontsize= 20)\n    g.set_ylabel(g.get_ylabel(),fontsize= 15)\n    plt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(objectcol)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now I am going to fit my selected feature with a machine leanring algroithm and born a predictive model. However, I just realize I still have null vlue exsit on my test data! Lets take a look.."},{"metadata":{"trusted":true},"cell_type":"code","source":"test[select_feature].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking misisng valus\nmissing_val_count_by_column = (test[select_feature].isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test[select_feature].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val_count_by_column = (test[select_feature].isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- A simple way to replace the missing numeric value is replace with the average number. "},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_cols =missing_val_count_by_column[test[select_feature].isnull().sum()>0].index\nfor i in range(0,len(missing_cols)):   \n    fillwith = test[missing_cols[i]].mean()\n    test[missing_cols[i]].fillna(value=fillwith, inplace  = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> #### Prepare categorical value data before running machine leanring model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop(columns =['Id','SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"There are {} features in used. \".format(len(X.columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.get_dummies(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"After dummies applied, There are {} features in used. \".format(len(X.columns)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test.drop(columns =['Id'])\nX_test = pd.get_dummies(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val_count_by_column = (X_test.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])\n#missing_cols =missing_val_count_by_column[X_test.isnull().sum()>0].index\nfor i in range(0,len(missing_cols)):   \n    fillwith = X_test[missing_cols[i]].mean()\n    X_test[missing_cols[i]].fillna(value=fillwith, inplace  = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Apparently in the test file there contains more columns than in the train file. So I made them the same."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test =X_test.drop(columns =['PoolQC_Gd', 'Alley_Pave', 'Alley_Grvl', 'MiscFeature_Shed', 'MiscFeature_Othr', 'PoolQC_Ex', 'MiscFeature_Gar2'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X[X_test.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_val_count_by_column = (X_test.isnull().sum())\nmissing = missing_val_count_by_column[missing_val_count_by_column > 0]\nmissing_cols =missing_val_count_by_column[X_test.isnull().sum()>0].index\nfor i in range(0,len(missing)):   \n    fillwith = X_test[missing_cols[i]].mean()\n    print(fillwith)\n    X_test[missing_cols[i]] = X_test[missing_cols[i]].fillna(value=fillwith)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ML 1: Randomforest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.ensemble import RandomForestRegressor\n# forest = RandomForestRegressor(n_estimators=100)\n# forest.fit(X,target)\n# result = forest.predict(X_test)\n# handin = pd.DataFrame({'Id': test.Id,'SalePrice': result})\n# handin.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Grid search on the forest classifer, try to improve the performance. \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\nparameters  = { \n    'n_estimators': [50,100,200,300,700],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\nforest = RandomForestRegressor()\n#apply grid search cv method on the forest and parameters.\ngs_forest = GridSearchCV(forest, parameters, cv = 5)\ngs_forest.fit(X, target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best score: {}, \\nBest params: {}\".format(gs_forest.best_score_,gs_forest.best_estimator_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Call predict on the estimator with the best found parameters.\nresult = gs_forest.predict(X_test)\nhandin = pd.DataFrame({'Id': test.Id,'SalePrice': result})\nhandin.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ML 2: PCA + SVR\n1. Apply standard scalar on both train/test data.\n2. Apply PCA for dimesion reduction.\n3. Applly Support Vector regressor."},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.preprocessing import StandardScaler\n# from sklearn.decomposition import PCA\n# from sklearn.svm import SVR\n\n# scaler = StandardScaler()\n# scaler.fit(X)\n# X = scaler.transform(X)\n# X_test = scaler.transform(X_test)\n# pca = PCA(n_components=10)\n# pca.fit(X)\n# X = pca.transform(X)\n# X_test = pca.transform(X_test)\n# clf = SVR(gamma='scale', C=1.0, epsilon=0.2)\n# clf.fit(X,target)\n# result = clf.predict(X_test)\n# handin = pd.DataFrame({'Id': test.Id,'SalePrice': result})\n# handin.to_csv('submission.csv', index=False)\n# print(\"score:\",0.4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}