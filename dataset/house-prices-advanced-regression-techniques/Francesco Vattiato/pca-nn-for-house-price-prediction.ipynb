{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\nfrom keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import r2_score\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-03T23:10:32.017127Z","iopub.execute_input":"2022-02-03T23:10:32.0174Z","iopub.status.idle":"2022-02-03T23:10:32.023379Z","shell.execute_reply.started":"2022-02-03T23:10:32.017371Z","shell.execute_reply":"2022-02-03T23:10:32.0227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\nsubs = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:32.025262Z","iopub.execute_input":"2022-02-03T23:10:32.025819Z","iopub.status.idle":"2022-02-03T23:10:32.079823Z","shell.execute_reply.started":"2022-02-03T23:10:32.025782Z","shell.execute_reply":"2022-02-03T23:10:32.079187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:32.08165Z","iopub.execute_input":"2022-02-03T23:10:32.082111Z","iopub.status.idle":"2022-02-03T23:10:32.107168Z","shell.execute_reply.started":"2022-02-03T23:10:32.08207Z","shell.execute_reply":"2022-02-03T23:10:32.106399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:32.110087Z","iopub.execute_input":"2022-02-03T23:10:32.110556Z","iopub.status.idle":"2022-02-03T23:10:32.141478Z","shell.execute_reply.started":"2022-02-03T23:10:32.110528Z","shell.execute_reply":"2022-02-03T23:10:32.140696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:32.142939Z","iopub.execute_input":"2022-02-03T23:10:32.143368Z","iopub.status.idle":"2022-02-03T23:10:32.233991Z","shell.execute_reply.started":"2022-02-03T23:10:32.143334Z","shell.execute_reply":"2022-02-03T23:10:32.233098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset Preprocessing\n","metadata":{}},{"cell_type":"code","source":"train=train.drop(columns=\"Id\")\ntest=test.drop(columns=\"Id\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:32.235143Z","iopub.execute_input":"2022-02-03T23:10:32.235441Z","iopub.status.idle":"2022-02-03T23:10:32.243463Z","shell.execute_reply.started":"2022-02-03T23:10:32.235406Z","shell.execute_reply":"2022-02-03T23:10:32.242754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how many NaNs we have for each feature.","metadata":{}},{"cell_type":"code","source":"nan_count=100*train.isna().sum().sort_values(ascending=False)/train.shape[0]\nfig=px.bar(x=nan_count.index,y=nan_count.values, labels={\"y\": \"Nan ammount (%)\",\"x\": \"Feature\"})\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:32.246283Z","iopub.execute_input":"2022-02-03T23:10:32.246492Z","iopub.status.idle":"2022-02-03T23:10:32.313343Z","shell.execute_reply.started":"2022-02-03T23:10:32.246466Z","shell.execute_reply":"2022-02-03T23:10:32.312728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can remove the features with NaN>40%, while the others will be handled replacing NaN with the respective median value.","metadata":{}},{"cell_type":"code","source":"train=train.drop(columns=['PoolQC', 'MiscFeature', 'Alley', 'Fence',\"FireplaceQu\"])\ntest=test.drop(columns=['PoolQC', 'MiscFeature', 'Alley', 'Fence',\"FireplaceQu\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:32.314553Z","iopub.execute_input":"2022-02-03T23:10:32.314985Z","iopub.status.idle":"2022-02-03T23:10:32.322844Z","shell.execute_reply.started":"2022-02-03T23:10:32.314951Z","shell.execute_reply":"2022-02-03T23:10:32.322015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_features=[ feature  for feature in train.columns if  train[feature].dtypes!=\"object\" and feature!=\"SalePrice\"]\ncategorical_features=[ feature  for feature in train.columns if  train[feature].dtypes==\"object\"]","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:32.326087Z","iopub.execute_input":"2022-02-03T23:10:32.326366Z","iopub.status.idle":"2022-02-03T23:10:32.333841Z","shell.execute_reply.started":"2022-02-03T23:10:32.326333Z","shell.execute_reply":"2022-02-03T23:10:32.333185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are going to remove the last NaN values with the median value of each feature.","metadata":{}},{"cell_type":"code","source":"#replacing train NaNs with modes\nnans=train.isna().sum()\nnans=nans[nans>0]\nfor feature in nans.index:\n    train[feature] = train[feature].fillna(train[feature].mode()[0])\n#replacing test NaNs with modes\nnans=test.isna().sum()\nnans=nans[nans>0]\nfor feature in nans.index:\n    test[feature] = test[feature].fillna(test[feature].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:32.335245Z","iopub.execute_input":"2022-02-03T23:10:32.335673Z","iopub.status.idle":"2022-02-03T23:10:32.388776Z","shell.execute_reply.started":"2022-02-03T23:10:32.335639Z","shell.execute_reply":"2022-02-03T23:10:32.388184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One hot encoding the categorical feature of train and test set.","metadata":{}},{"cell_type":"code","source":"for feature in categorical_features:    \n    #some string values are present only in one of the dataset, so it is needed an unique list of both dataset to avoid conflicts\n    for num, value in enumerate(np.unique((list(train[feature].unique())+list(test[feature].unique())))):          \n        train[feature+\"_\"+str(num)]=pd.Series(train[feature]==value,dtype=\"int\")        \n        test[feature+\"_\"+str(num)]=pd.Series(test[feature]==value,dtype=\"int\")\n    train=train.drop(columns=feature)\n    test=test.drop(columns=feature)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:32.390045Z","iopub.execute_input":"2022-02-03T23:10:32.390296Z","iopub.status.idle":"2022-02-03T23:10:32.907882Z","shell.execute_reply.started":"2022-02-03T23:10:32.390263Z","shell.execute_reply":"2022-02-03T23:10:32.907138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:32.909351Z","iopub.execute_input":"2022-02-03T23:10:32.909629Z","iopub.status.idle":"2022-02-03T23:10:32.93754Z","shell.execute_reply.started":"2022-02-03T23:10:32.909592Z","shell.execute_reply":"2022-02-03T23:10:32.936761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Standard transformation\nof the train and test test (only numeric features).","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\ntrain[numeric_features]=scaler.fit_transform(train[numeric_features])\ntest[numeric_features]=scaler.transform(test[numeric_features])","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:33.549747Z","iopub.execute_input":"2022-02-03T23:10:33.550218Z","iopub.status.idle":"2022-02-03T23:10:33.591652Z","shell.execute_reply.started":"2022-02-03T23:10:33.550182Z","shell.execute_reply":"2022-02-03T23:10:33.59091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before feeding the data to the Neural Network, we peforme an PCA dimensionality reduction to reduce the noise of the data and to ease the calculation of the neural net.","metadata":{}},{"cell_type":"code","source":"x_train=train.drop(columns=\"SalePrice\")\ny_train=train['SalePrice']\npca = PCA(n_components=train.shape[1]-1)\nx_train=pca.fit_transform(x_train)\nfig=go.Figure()\nfig.add_traces(go.Bar(x=np.arange(train.shape[1]-1),y=np.cumsum(pca.explained_variance_ratio_),name=\"Cumulative Variance\"))\n#n_comp will be the number of components that explains the 95% of the data variance\nn_comp=np.where(np.cumsum(pca.explained_variance_ratio_)>0.95)[0][0]\nfig.add_traces(go.Scatter(x=np.arange(train.shape[1]-1),y=[0.95]*(train.shape[1]-1),name=\"Variance at 95%\"))\nfig.update_layout(title=\"How many components we need?\",xaxis_title=\"Components\",yaxis_title=\"Cumulative Variance\", font=dict(\n        family=\"Arial\",\n        size=18,\n    ))\nfig.show()\nprint(\"With n_components=\"+str(n_comp)+\" we have the 95% of the data variance, so we will choose this value.\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:33.593101Z","iopub.execute_input":"2022-02-03T23:10:33.593361Z","iopub.status.idle":"2022-02-03T23:10:33.684711Z","shell.execute_reply.started":"2022-02-03T23:10:33.593326Z","shell.execute_reply":"2022-02-03T23:10:33.683894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components=n_comp+50)\nx_train=pca.fit_transform(train.drop(columns=[\"SalePrice\"]))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:10:33.686134Z","iopub.execute_input":"2022-02-03T23:10:33.686422Z","iopub.status.idle":"2022-02-03T23:10:33.76961Z","shell.execute_reply.started":"2022-02-03T23:10:33.686383Z","shell.execute_reply":"2022-02-03T23:10:33.768797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model definition and training\n","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n      layers.Dense(2048, activation='relu'),\n      layers.Dropout(0.5),\n      layers.Dense(2048, activation='relu'),\n      layers.Dropout(0.5),\n      layers.Dense(1)\n  ])\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adamax(1e-3))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:11:09.195217Z","iopub.execute_input":"2022-02-03T23:11:09.195741Z","iopub.status.idle":"2022-02-03T23:11:09.21374Z","shell.execute_reply.started":"2022-02-03T23:11:09.195701Z","shell.execute_reply":"2022-02-03T23:11:09.212901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train,y_train,validation_split=0.1,verbose=0, epochs=300)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:11:09.419339Z","iopub.execute_input":"2022-02-03T23:11:09.419537Z","iopub.status.idle":"2022-02-03T23:11:50.727835Z","shell.execute_reply.started":"2022-02-03T23:11:09.419514Z","shell.execute_reply":"2022-02-03T23:11:50.727034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=go.Figure()\nfig.add_trace(go.Scatter(x=np.arange(300), y=history.history['loss'],mode='lines', name='Train Loss'))\nfig.add_trace(go.Scatter(x=np.arange(300), y=history.history['val_loss'],mode='lines', name='Validation Loss',))\nfig.update_layout(title=\"MAE loss on train and validation set\",xaxis_title=\"Epoch\", yaxis_title=\"Loss\", font=dict(\n        family=\"Arial\",\n        size=18,\n    ))\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:11:50.730225Z","iopub.execute_input":"2022-02-03T23:11:50.730502Z","iopub.status.idle":"2022-02-03T23:11:50.752066Z","shell.execute_reply.started":"2022-02-03T23:11:50.730465Z","shell.execute_reply":"2022-02-03T23:11:50.751392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model evaluation and submission\n","metadata":{}},{"cell_type":"code","source":"print(\"Validation loss:\",history.history['val_loss'][-1])\nprint(\"Training loss:\",history.history['loss'][-1])\nprint(\"Loss on entire train set:\",mean_absolute_error(model.predict(x_train),y_train))\nprint(\"R2 score(Train):\",r2_score(model.predict(x_train),y_train))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:11:50.753358Z","iopub.execute_input":"2022-02-03T23:11:50.753788Z","iopub.status.idle":"2022-02-03T23:11:50.955207Z","shell.execute_reply.started":"2022-02-03T23:11:50.753754Z","shell.execute_reply":"2022-02-03T23:11:50.954361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_preds = model.predict(pca.transform(test))\nsubs[\"SalePrice\"] = sub_preds\nsubs.to_csv(\"submission.csv\", index = False)\nprint(\"Submission done!\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T23:11:03.917039Z","iopub.execute_input":"2022-02-03T23:11:03.917349Z","iopub.status.idle":"2022-02-03T23:11:04.062017Z","shell.execute_reply.started":"2022-02-03T23:11:03.917317Z","shell.execute_reply":"2022-02-03T23:11:04.061086Z"},"trusted":true},"execution_count":null,"outputs":[]}]}