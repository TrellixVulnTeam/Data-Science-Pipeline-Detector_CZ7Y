{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\"> HOUSE PRICES ADVANCED REGRESSION TECHNIQUES </h1>"},{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\">![](https://storage.googleapis.com/kaggle-competitions/kaggle/5407/media/housesbanner.png) <h1/>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import RobustScaler,MinMaxScaler\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nimport matplotlib.style as style\nstyle.use('seaborn-colorblind')\n\nimport warnings  \nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>Before we Begin:  </h2>\nIn models with a lot of variables, as in this dataset, you may not always have time to understand the meaning of all variables and what they are. For example, I usually try to make more than 300 variables meaningful. Sometimes this figure exceeds thousands. In such cases, someone has to tell us which variable is important and what are the meanings of them. A corelation matrix is usually can tell, but this alone is not enough. It is also necessary to use the techniques that regression models offer us.<br><br>"},{"metadata":{},"cell_type":"markdown","source":"<h2> Introduction </h2>\nIn this notebook we will use various predictive models to see how accurate they  are.<br>\nThen combine them and try to improve model performance and analyzing best parameters<br>\n<font color='red'>\n### Our goal is create a Auto Machine Learning model for any dataset.<br>\n<font color='black'>\nLet's Begin\n\n<font color='red'>\n\n<h2> Road Map: </h2>\n1. [Parameters](#1)  \n1. [Understand the Target (SalePrice) distribution](#2)  \n1. [Drop high null ratio features](#3)\n1. [Fill null values with Mode/Median (for categorical features -Mode and for numbers-Median)](#4)\n1. [Auto Detect Outliers](#5)\n1. [Check Skewness and fit transormations if needed](#6)\n1. [Check Correlation between features and remove features with high correlations](#7)\n1. [Create regression models and compare the accuracy to our best regressor.](#8)\n1. [Find best model and make a submission](#9)\n1. [Improving models performance with StackingCVRegressor](#10)\n1. [Create stacked model and make a new submission](#11)\n1. [Use Shap and find features importances](#12)\n\n"},{"metadata":{},"cell_type":"markdown","source":"First Import Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_l=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest_l=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\ndesc=pd.read_fwf(\"/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\")\ndataset =  pd.concat(objs=[train_l, test_l], axis=0,sort=False).reset_index(drop=True)\nsample=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='1'></a><br>\n## 1. Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_r=0.6                # Remove Null value ratio more than n_r. For example 0.6 means if column null ratio more than %60 then remove column\ns_r=0.50               # If skewness more than %75 transform column to get normal distribution\nc_r=1                  # Remove correlated columns\nn_f= dataset.shape[1]  # n_f number of features. dataset.shape[1] means all columns. If you change it to 10, it will select 10 most correlated feature\nr_s=42                  # random seed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='2'></a><br>\n## 2. Understand the Target (SalePrice) distribution"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plotting_3_chart(df, feature): \n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    ## crea,ting a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n \n\nprint('Skewness: '+ str(dataset['SalePrice'].skew())) \nprint(\"Kurtosis: \" + str(dataset['SalePrice'].kurt()))\nplotting_3_chart(dataset, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have skewed target so we need to transofmation. I ll use log but you try other transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"#log transform the target:\ndataset[\"SalePrice\"] = np.log1p(dataset[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Skewness: '+ str(dataset['SalePrice'].skew()))   \nprint(\"Kurtosis: \" + str(dataset['SalePrice'].kurt()))\nplotting_3_chart(dataset, 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our Target normalized. \n<font color='red'>\nNote: When making submission , this transofmation need to be undone."},{"metadata":{},"cell_type":"markdown","source":"<a id='3'></a><br>\n## 3. Drop high null ratio features"},{"metadata":{},"cell_type":"markdown","source":"You can change null ratio (n_r) on parameters section"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ndataset_isna=dataset.isna()\ndataset_isna_sum=dataset_isna.sum()\ndataset_isna_ratio=dataset_isna_sum/len(dataset)\nif \"SalePrice\" in dataset_isna_ratio:\n    dataset_isna_ratio.drop(\"SalePrice\",inplace=True)\nremove_columns=dataset_isna_ratio[dataset_isna_ratio>n_r]\ncolumns=pd.DataFrame(remove_columns)\nprint(\"This Columns will be remove because of null ratio higher than %\"+str(n_r*100)+\": \")\nprint(remove_columns)\ndataset=dataset.drop(columns.index,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4'></a><br>\n## 4. Fill null values with Mode/Median (for categorical features -Mode and for numbers-Median)"},{"metadata":{},"cell_type":"markdown","source":"I use mode for cats and for median for numeric features but you can change it whatever you want."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat=dataset.select_dtypes(\"object\")\nfor column in cat:\n    dataset[column].fillna(dataset[column].mode()[0], inplace=True)\n    #dataset[column].fillna(\"NA\", inplace=True)\n\n\nfl=dataset.select_dtypes([\"float64\",\"int64\"]).drop(\"SalePrice\",axis=1)\nfor column in fl:\n    dataset[column].fillna(dataset[column].median(), inplace=True)\n    #dataset[column].fillna(0, inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5'></a><br>\n## 5. Auto Detect Outliers"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"train_o=dataset[dataset[\"SalePrice\"].notnull()]\nfrom sklearn.neighbors import LocalOutlierFactor\ndef detect_outliers(x, y, top=5, plot=True):\n    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.1)\n    x_ =np.array(x).reshape(-1,1)\n    preds = lof.fit_predict(x_)\n    lof_scr = lof.negative_outlier_factor_\n    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n    if plot:\n        f, ax = plt.subplots(figsize=(9, 6))\n        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n    return out_idx\n\nouts = detect_outliers(train_o['GrLivArea'], train_o['SalePrice'],top=5)\nouts\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Detect and Remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\noutliers=outs\nall_outliers=[]\nnumeric_features = train_o.dtypes[train_o.dtypes != 'object'].index\nfor feature in numeric_features:\n    try:\n        outs = detect_outliers(train_o[feature], train_o['SalePrice'],top=5, plot=False)\n    except:\n        continue\n    all_outliers.extend(outs)\n\nprint(Counter(all_outliers).most_common())\nfor i in outliers:\n    if i in all_outliers:\n        print(i)\ntrain_o = train_o.drop(train_o.index[outliers])\ntest_o=dataset[dataset[\"SalePrice\"].isna()]\ndataset =  pd.concat(objs=[train_o, test_o], axis=0,sort=False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5'></a><br>\n## 5. Check Skewness and fit transormations if needed"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.special import boxcox1p\nfrom scipy.stats import boxcox\nlam = 0.15\n\n#log transform skewed numeric features:\nnumeric_feats = dataset.dtypes[dataset.dtypes != \"object\"].index\n\nskewed_feats = dataset[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > s_r]\nskewed_feats = skewed_feats.index\n\ndataset[skewed_feats] = boxcox1p(dataset[skewed_feats],lam)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Now we don't have any missing value"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.columns[dataset.isnull().any()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6'></a><br>\n## 6. Check Correlation between features and remove features with high correlations"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train_heat=dataset[dataset[\"SalePrice\"].notnull()]\ntrain_heat=train_heat.drop([\"Id\"],axis=1)\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train_heat.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train_heat.corr(), \n            cmap=sns.diverging_palette(255, 133, l=60, n=7), \n            mask = mask, \n            annot=True, \n            center = 0, \n           );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove correlated features"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"feature_corr = train_heat.corr().abs()\ntarget_corr=dataset.corr()[\"SalePrice\"].abs()\ntarget_corr=pd.DataFrame(target_corr)\ntarget_corr=target_corr.reset_index()\nfeature_corr_unstack= feature_corr.unstack()\ndf_fc=pd.DataFrame(feature_corr_unstack,columns=[\"corr\"])\ndf_fc=df_fc[(df_fc[\"corr\"]>=.80)&(df_fc[\"corr\"]<1)].sort_values(by=\"corr\",ascending=False)\ndf_dc=df_fc.reset_index()\n\n#df_dc=pd.melt(df_dc, id_vars=['corr'], var_name='Name')\ntarget_corr=df_dc.merge(target_corr, left_on='level_1', right_on='index',\n          suffixes=('_left', '_right'))\n\ncols=target_corr[\"level_0\"].values\n\ntarget_corr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=dataset.drop([\"GarageArea\",\"TotRmsAbvGrd\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Converting categorical features to numerical (some models doesn't need this conversion)"},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=pd.get_dummies(dataset,columns=cat.columns)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Remove low features with low variances"},{"metadata":{"trusted":true},"cell_type":"code","source":"all_features = dataset.keys()\n# Removing features.\ndataset = dataset.drop(dataset.loc[:,(dataset==0).sum()>=(dataset.shape[0]*0.9994)],axis=1)\ndataset = dataset.drop(dataset.loc[:,(dataset==1).sum()>=(dataset.shape[0]*0.9994)],axis=1) \n# Getting and printing the remaining features.\nremain_features = dataset.keys()\nremov_features = [st for st in all_features if st not in remain_features]\nprint(len(remov_features), 'features were removed:', remov_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='7'></a><br>\n## 7. Create regression models and compare the accuracy to our best regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"train=dataset[dataset[\"SalePrice\"].notnull()]\ntest=dataset[dataset[\"SalePrice\"].isna()]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = n_f # if you change it 10 model uses most 10 correlated features\ncorrmat=abs(dataset.corr())\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ntrain_x=train[cols].drop(\"SalePrice\",axis=1)\ntrain_y=train[\"SalePrice\"]\nX_test=test[cols].drop(\"SalePrice\",axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Train Test Split - Classic"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.20, random_state=r_s)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='8'></a><br>\n## 8. Find best model and make a submission"},{"metadata":{},"cell_type":"markdown","source":"Do you know all models names in sckitlearn?\n"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\nfrom sklearn.utils.testing import all_estimators\nfrom sklearn import base\n\nestimators = all_estimators()\n\nfor name, class_ in estimators:\n    if issubclass(class_, base.RegressorMixin):\n       print(name+\"()\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(seed=r_s)\n\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor,ExtraTreesRegressor,HistGradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Ridge,RidgeCV,BayesianRidge,LinearRegression,Lasso,LassoCV,ElasticNet,RANSACRegressor,HuberRegressor,PassiveAggressiveRegressor,ElasticNetCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.cross_decomposition import CCA\nfrom sklearn.neural_network import MLPRegressor\n\n\n\nmy_regressors=[ \n               ElasticNet(alpha=0.001,l1_ratio=0.70,max_iter=100,tol=0.01, random_state=r_s),\n               ElasticNetCV(l1_ratio=0.9,max_iter=100,tol=0.01,random_state=r_s),\n               CatBoostRegressor(logging_level='Silent',random_state=r_s),\n               GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber',random_state =r_s),\n               LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       random_state=r_s\n                                       ),\n               RandomForestRegressor(random_state=r_s),\n               AdaBoostRegressor(random_state=r_s),\n               ExtraTreesRegressor(random_state=r_s),\n               SVR(C= 20, epsilon= 0.008, gamma=0.0003),\n               Ridge(alpha=6),\n               RidgeCV(),\n               BayesianRidge(),\n               DecisionTreeRegressor(),\n               LinearRegression(),\n               KNeighborsRegressor(),\n               Lasso(alpha=0.00047,random_state=r_s),\n               LassoCV(),\n               KernelRidge(),\n               CCA(),\n               MLPRegressor(random_state=r_s),\n               HistGradientBoostingRegressor(random_state=r_s),\n               HuberRegressor(),\n               RANSACRegressor(random_state=r_s),\n               PassiveAggressiveRegressor(random_state=r_s)\n               #XGBRegressor(random_state=r_s)\n              ]\n\nregressors=[]\n\nfor my_regressor in my_regressors:\n    regressors.append(my_regressor)\n\n\nscores_val=[]\nscores_train=[]\nMAE=[]\nMSE=[]\nRMSE=[]\n\n\nfor regressor in regressors:\n    scores_val.append(regressor.fit(X_train,y_train).score(X_val,y_val))\n    scores_train.append(regressor.fit(X_train,y_train).score(X_train,y_train))\n    y_pred=regressor.predict(X_val)\n    MAE.append(mean_absolute_error(y_val,y_pred))\n    MSE.append(mean_squared_error(y_val,y_pred))\n    RMSE.append(np.sqrt(mean_squared_error(y_val,y_pred)))\n\n    \nresults=zip(scores_val,scores_train,MAE,MSE,RMSE)\nresults=list(results)\nresults_score_val=[item[0] for item in results]\nresults_score_train=[item[1] for item in results]\nresults_MAE=[item[2] for item in results]\nresults_MSE=[item[3] for item in results]\nresults_RMSE=[item[4] for item in results]\n\n\ndf_results=pd.DataFrame({\"Algorithms\":my_regressors,\"Training Score\":results_score_train,\"Validation Score\":results_score_val,\"MAE\":results_MAE,\"MSE\":results_MSE,\"RMSE\":results_RMSE})\ndf_results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sort models results according to RMSE and select best model for submission."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"best_models=df_results.sort_values(by=\"RMSE\")\nbest_model=best_models.iloc[0][0]\nbest_stack=best_models[\"Algorithms\"].values\nbest_models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nbest_model.fit(X_train,y_train)\ny_test=best_model.predict(X_test)\ntest_Id=test['Id']\nmy_submission = pd.DataFrame({'Id': test_Id, 'SalePrice': np.expm1(y_test)})\nmy_submission.to_csv('submission_bm.csv', index=False)\nprint(\"Model Name: \"+str(best_model))\nprint(best_model.score(X_val,y_val))\ny_pred=best_model.predict(X_val)\nprint(\"RMSE: \"+str(np.sqrt(mean_squared_error(y_val,y_pred))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How is look like our predictions. Are they close to real target values? <br>\nLet's look at the graph"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\ny_pred=best_model.predict(X_val)\nsns.regplot(x=y_val,y=y_pred,truncate=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='9'></a><br>\n## 9. Improving models performance with StackingCVRegressor"},{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\">![](http://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor_files/stacking_cv_regressor_overview.png) <h1/>"},{"metadata":{},"cell_type":"markdown","source":"This code block finds best combinations for you. It's taking time but worth it."},{"metadata":{"trusted":true},"cell_type":"code","source":"i_num=[]\nj_num=[]\nscore=[]\nRMSE=[]\nfor i in range(1,8):\n    stack_models=i\n    for j in range(1,4):\n        base_model=j\n        best_n_models=best_models.head(stack_models).index\n        regressors_top_n=list( regressors[i] for i in best_n_models)\n\n        from mlxtend.regressor import StackingCVRegressor\n        stack = StackingCVRegressor(regressors=regressors_top_n,meta_regressor= best_stack[base_model], use_features_in_secondary=True)\n        comb=stack.fit(X_train,y_train)\n        y_pred=comb.predict(X_val)\n        score.append(comb.score(X_val,y_val))\n        RMSE.append(np.sqrt(mean_squared_error(y_val,y_pred)))\n        i_num.append(i)\n        j_num.append(j)\n        \nopt_resr=zip(score,RMSE,i_num,j_num)\nopt_resr=set(opt_resr)\nopt_resr_score=[item[0] for item in opt_resr]\nopt_resr_RMSE=[item[1] for item in opt_resr]\nopt_resr_i_num=[item[2] for item in opt_resr]\nopt_resr_j_num=[item[3] for item in opt_resr]\n\n\n\ndf_opt_resr=pd.DataFrame({\"Score\":opt_resr_score,\"RMSE\":opt_resr_RMSE,\"i_num\":opt_resr_i_num,\"j_num\":opt_resr_j_num})\ndf_opt_resr=df_opt_resr.sort_values(by=\"RMSE\")\nopt_best_model_i_num=df_opt_resr.iloc[0][2].astype(int)\nopt_best_model_j_num=df_opt_resr.iloc[0][3].astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='10'></a><br>\n## 10. Create stacked model and make a new submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_n_models=best_models.head(opt_best_model_i_num).index\nregressors_top_n=list( regressors[i] for i in best_n_models)\nfrom mlxtend.regressor import StackingCVRegressor\nstack = StackingCVRegressor(regressors=regressors_top_n,meta_regressor= regressors[opt_best_model_j_num], use_features_in_secondary=True)\ncomb=stack.fit(X_train,y_train)\nprint(comb.score(X_val,y_val))\ny_pred=comb.predict(X_val)\nprint(\"RMSE: \"+str(np.sqrt(mean_squared_error(y_val,y_pred))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can see your models improvement"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(10,7))\ny_pred=comb.predict(X_val)\nsns.regplot(x=y_val,y=y_pred,truncate=False)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test=comb.predict(X_test)\ntest_Id=test['Id']\nmy_submission = pd.DataFrame({'Id': test_Id, 'SalePrice': np.expm1(y_test)})\n#my_submission = pd.DataFrame({'Id': test_Id, 'SalePrice': np.expm1(y_test)})\nmy_submission.to_csv('submission_stack.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's try Blending our Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"def blended_predictions(X):\n    return ((0.40 * comb.predict(X)) + \\\n            (0.40 * best_models.iloc[0][0].predict(X)) + \\\n            (0.10 * best_models.iloc[1][0].predict(X)) + \\\n            (0.10 * best_models.iloc[2][0].predict(X)))\n\ny_pred=blended_predictions(X_val)\n\n\n#y_pred=comb.predict(X_val)\nprint(\"RMSE: \"+str(np.sqrt(mean_squared_error(y_val,y_pred))))\n\ny_test=blended_predictions(X_test)\ntest_Id=test['Id']\nmy_submission = pd.DataFrame({'Id': test_Id, 'SalePrice': np.expm1(y_test)})\nmy_submission.to_csv('submission_blend3.csv', index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='11'></a><br>\n## 11. Use Shap and find features importances"},{"metadata":{},"cell_type":"markdown","source":"Note: SHAP doesnt support all models that we have. So if model which we used is unsupported then we will get error. In my case no problem."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import shap  # package used to calculate Shap values\n\nxgb=XGBRegressor(random_state=2)\nxgb=xgb.fit(X_train,y_train)\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(xgb)\n\n# calculate shap values. This is what we will plot.\n# Calculate shap_values for all of val_X rather than a single row, to have more data for plot.\nshap_values = explainer.shap_values(X_val)\n\n# Make plot. Index of [1] is explained in text below.\nshap.summary_plot(shap_values, X_val)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.force_plot(explainer.expected_value, shap_values[0,:], X_val.iloc[0,:],matplotlib=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"most_corr=dataset.corr()[\"SalePrice\"].abs().sort_values(ascending=False).index[1]\nshap.dependence_plot(most_corr, shap_values, X_val, interaction_index=\"SaleCondition_Partial\")\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}