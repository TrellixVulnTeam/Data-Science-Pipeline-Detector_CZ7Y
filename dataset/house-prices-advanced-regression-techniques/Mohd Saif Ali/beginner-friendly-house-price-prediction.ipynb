{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# House Price Prediction with Linear Regression\n\n![](https://i.imgur.com/3sw1fY9.jpg)\n\nWe predict the price of house based on various features leveraging the understanding of corelation amongst the features using linear regression model ie Ridge\n\nSteps Include:\n\n1. Downloading and exploring the data\n2. Preparing the dataset for training\n3. Training a linear regression model\n4. Make predictions and evaluating the model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"Loading the data from the file train.csv into a Pandas data frame.","metadata":{}},{"cell_type":"code","source":"!pip install numpy pandas matplotlib seaborn plotly opendatasets jovian --quiet","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:15.599658Z","iopub.execute_input":"2021-07-04T06:49:15.600193Z","iopub.status.idle":"2021-07-04T06:49:22.191934Z","shell.execute_reply.started":"2021-07-04T06:49:15.600158Z","shell.execute_reply":"2021-07-04T06:49:22.190519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nprices_df = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.194354Z","iopub.execute_input":"2021-07-04T06:49:22.194702Z","iopub.status.idle":"2021-07-04T06:49:22.227738Z","shell.execute_reply.started":"2021-07-04T06:49:22.194656Z","shell.execute_reply":"2021-07-04T06:49:22.226804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prices_df","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.228879Z","iopub.execute_input":"2021-07-04T06:49:22.229178Z","iopub.status.idle":"2021-07-04T06:49:22.278171Z","shell.execute_reply.started":"2021-07-04T06:49:22.22915Z","shell.execute_reply":"2021-07-04T06:49:22.276894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prices_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.279626Z","iopub.execute_input":"2021-07-04T06:49:22.280047Z","iopub.status.idle":"2021-07-04T06:49:22.313843Z","shell.execute_reply.started":"2021-07-04T06:49:22.280002Z","shell.execute_reply":"2021-07-04T06:49:22.312275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Describing number of rows and columns does the dataset contain?","metadata":{}},{"cell_type":"code","source":"n_rows = prices_df.shape[0]\nn_cols = len(prices_df.columns)\nprint('The dataset contains {} rows and {} columns.'.format(n_rows, n_cols))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.318007Z","iopub.execute_input":"2021-07-04T06:49:22.31836Z","iopub.status.idle":"2021-07-04T06:49:22.324456Z","shell.execute_reply.started":"2021-07-04T06:49:22.318329Z","shell.execute_reply":"2021-07-04T06:49:22.323151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"understand the corelation between the features by visualizing the dataset","metadata":{}},{"cell_type":"code","source":"prices_df.corr()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.327692Z","iopub.execute_input":"2021-07-04T06:49:22.328086Z","iopub.status.idle":"2021-07-04T06:49:22.399639Z","shell.execute_reply.started":"2021-07-04T06:49:22.328043Z","shell.execute_reply":"2021-07-04T06:49:22.398534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finding top correlations with absolute values and both direct and indirection relations are important","metadata":{}},{"cell_type":"code","source":"c = prices_df.corr().abs()\ne = c['SalePrice']\nd= c['SalePrice']>0.65\nf = e[d]\nf = pd.DataFrame(data=f)\nf","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.40102Z","iopub.execute_input":"2021-07-04T06:49:22.401341Z","iopub.status.idle":"2021-07-04T06:49:22.421877Z","shell.execute_reply.started":"2021-07-04T06:49:22.401312Z","shell.execute_reply":"2021-07-04T06:49:22.420846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nx=prices_df['OverallQual']\ny = prices_df['SalePrice']\nplt.scatter(prices_df['GrLivArea'],y)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.423207Z","iopub.execute_input":"2021-07-04T06:49:22.423474Z","iopub.status.idle":"2021-07-04T06:49:22.58596Z","shell.execute_reply.started":"2021-07-04T06:49:22.423448Z","shell.execute_reply":"2021-07-04T06:49:22.585089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x,y)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.58692Z","iopub.execute_input":"2021-07-04T06:49:22.587213Z","iopub.status.idle":"2021-07-04T06:49:22.897212Z","shell.execute_reply.started":"2021-07-04T06:49:22.587186Z","shell.execute_reply":"2021-07-04T06:49:22.896285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2 - Preparing the Dataset for Training\n\nBefore we can train the model, we need to prepare the dataset. Here are the steps we'll follow:\n\n1. Identify the input and target column(s) for training the model.\n2. Identify numeric and categorical input columns.\n3. [Impute](https://scikit-learn.org/stable/modules/impute.html) (fill) missing values in numeric columns\n4. [Scale](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) values in numeric columns to a $(0,1)$ range.\n5. [Encode](https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features) categorical data into one-hot vectors.\n6. Split the dataset into training and validation sets.","metadata":{}},{"cell_type":"markdown","source":"### Identify Inputs and Targets\n\nWhile the dataset contains 81 columns, not all of them are useful for modeling. Note the following:\n\n- The first column `Id` is a unique ID for each house and isn't useful for training the model.\n- The last column `SalePrice` contains the value we need to predict i.e. it's the target column.\n- Data from all the other columns (except the first and the last column) can be used as inputs to the model.","metadata":{}},{"cell_type":"code","source":"prices_df","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.898529Z","iopub.execute_input":"2021-07-04T06:49:22.898799Z","iopub.status.idle":"2021-07-04T06:49:22.947378Z","shell.execute_reply.started":"2021-07-04T06:49:22.898773Z","shell.execute_reply":"2021-07-04T06:49:22.946237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify the input columns (a list of column names)\ninput_cols = list(prices_df.columns[1:-1])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.948982Z","iopub.execute_input":"2021-07-04T06:49:22.949398Z","iopub.status.idle":"2021-07-04T06:49:22.954421Z","shell.execute_reply.started":"2021-07-04T06:49:22.949362Z","shell.execute_reply":"2021-07-04T06:49:22.953204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify the name of the target column (a single string, not a list)\ntarget_col = prices_df.columns[-1]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.95572Z","iopub.execute_input":"2021-07-04T06:49:22.956015Z","iopub.status.idle":"2021-07-04T06:49:22.966731Z","shell.execute_reply.started":"2021-07-04T06:49:22.955986Z","shell.execute_reply":"2021-07-04T06:49:22.965677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(input_cols))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.967725Z","iopub.execute_input":"2021-07-04T06:49:22.967982Z","iopub.status.idle":"2021-07-04T06:49:22.981914Z","shell.execute_reply.started":"2021-07-04T06:49:22.967957Z","shell.execute_reply":"2021-07-04T06:49:22.980898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(input_cols)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.983178Z","iopub.execute_input":"2021-07-04T06:49:22.983484Z","iopub.status.idle":"2021-07-04T06:49:22.991885Z","shell.execute_reply.started":"2021-07-04T06:49:22.983455Z","shell.execute_reply":"2021-07-04T06:49:22.990999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(target_col)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:22.993025Z","iopub.execute_input":"2021-07-04T06:49:22.993324Z","iopub.status.idle":"2021-07-04T06:49:23.004754Z","shell.execute_reply.started":"2021-07-04T06:49:22.993297Z","shell.execute_reply":"2021-07-04T06:49:23.003629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs_df = prices_df[input_cols].copy()\ntargets = prices_df[target_col]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.00623Z","iopub.execute_input":"2021-07-04T06:49:23.006511Z","iopub.status.idle":"2021-07-04T06:49:23.018831Z","shell.execute_reply.started":"2021-07-04T06:49:23.006484Z","shell.execute_reply":"2021-07-04T06:49:23.017861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs_df","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.022762Z","iopub.execute_input":"2021-07-04T06:49:23.02332Z","iopub.status.idle":"2021-07-04T06:49:23.072704Z","shell.execute_reply.started":"2021-07-04T06:49:23.023279Z","shell.execute_reply":"2021-07-04T06:49:23.071949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.073771Z","iopub.execute_input":"2021-07-04T06:49:23.074221Z","iopub.status.idle":"2021-07-04T06:49:23.081096Z","shell.execute_reply.started":"2021-07-04T06:49:23.074179Z","shell.execute_reply":"2021-07-04T06:49:23.080247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Identify Numeric and Categorical Data\n\nThe next step in data preparation is to identify numeric and categorical columns. We can do this by looking at the data type of each column.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nnumeric_cols = inputs_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\ncategorical_cols = inputs_df.select_dtypes(include='object').columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.082434Z","iopub.execute_input":"2021-07-04T06:49:23.082982Z","iopub.status.idle":"2021-07-04T06:49:23.097274Z","shell.execute_reply.started":"2021-07-04T06:49:23.082945Z","shell.execute_reply":"2021-07-04T06:49:23.09613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(numeric_cols))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.098527Z","iopub.execute_input":"2021-07-04T06:49:23.098817Z","iopub.status.idle":"2021-07-04T06:49:23.110619Z","shell.execute_reply.started":"2021-07-04T06:49:23.09879Z","shell.execute_reply":"2021-07-04T06:49:23.109317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(list(categorical_cols))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.113977Z","iopub.execute_input":"2021-07-04T06:49:23.114386Z","iopub.status.idle":"2021-07-04T06:49:23.123203Z","shell.execute_reply.started":"2021-07-04T06:49:23.114345Z","shell.execute_reply":"2021-07-04T06:49:23.122292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Impute Numerical Data\n\nSome of the numeric columns in our dataset contain missing values (`nan`).","metadata":{}},{"cell_type":"code","source":"missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\nmissing_counts[missing_counts > 0]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.124544Z","iopub.execute_input":"2021-07-04T06:49:23.125109Z","iopub.status.idle":"2021-07-04T06:49:23.140821Z","shell.execute_reply.started":"2021-07-04T06:49:23.125075Z","shell.execute_reply":"2021-07-04T06:49:23.139806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n# 1. Create the imputer\nimputer = SimpleImputer(strategy='median')\n# 2. Fit the imputer to the numeric colums\nimputer.fit(prices_df[numeric_cols])\nlist(imputer.statistics_)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.142185Z","iopub.execute_input":"2021-07-04T06:49:23.14265Z","iopub.status.idle":"2021-07-04T06:49:23.160629Z","shell.execute_reply.started":"2021-07-04T06:49:23.142613Z","shell.execute_reply":"2021-07-04T06:49:23.15981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs_df[numeric_cols] = imputer.transform(inputs_df[numeric_cols])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.161758Z","iopub.execute_input":"2021-07-04T06:49:23.162235Z","iopub.status.idle":"2021-07-04T06:49:23.199114Z","shell.execute_reply.started":"2021-07-04T06:49:23.162192Z","shell.execute_reply":"2021-07-04T06:49:23.198128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_counts = inputs_df[numeric_cols].isna().sum().sort_values(ascending=False)\nmissing_counts[missing_counts > 0]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.200479Z","iopub.execute_input":"2021-07-04T06:49:23.200798Z","iopub.status.idle":"2021-07-04T06:49:23.214437Z","shell.execute_reply.started":"2021-07-04T06:49:23.200766Z","shell.execute_reply":"2021-07-04T06:49:23.213109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Scale Numerical Values\n\nThe numeric columns in our dataset have varying ranges. ","metadata":{"execution":{"iopub.status.busy":"2021-07-04T05:57:10.035803Z","iopub.execute_input":"2021-07-04T05:57:10.036167Z","iopub.status.idle":"2021-07-04T05:57:10.041943Z","shell.execute_reply.started":"2021-07-04T05:57:10.036133Z","shell.execute_reply":"2021-07-04T05:57:10.04077Z"}}},{"cell_type":"code","source":"inputs_df[numeric_cols].describe().loc[['min', 'max']]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.216006Z","iopub.execute_input":"2021-07-04T06:49:23.216632Z","iopub.status.idle":"2021-07-04T06:49:23.315728Z","shell.execute_reply.started":"2021-07-04T06:49:23.216584Z","shell.execute_reply":"2021-07-04T06:49:23.314962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A good practice is to [scale numeric features](https://scikit-learn.org/stable/modules/preprocessing.html#scaling-features-to-a-range) to a small range of values e.g. $(0,1)$. Scaling numeric features ensures that no particular feature has a disproportionate impact on the model's loss. Optimization algorithms also work better in practice with smaller numbers.\n","metadata":{}},{"cell_type":"markdown","source":"Scaling the numeric values to the  (0,1)  range using MinMaxScaler from sklearn.preprocessing.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n# Create the scaler\nscaler = MinMaxScaler()\nscaler.fit(inputs_df[numeric_cols])\ninputs_df[numeric_cols] = scaler.transform(inputs_df[numeric_cols])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.316853Z","iopub.execute_input":"2021-07-04T06:49:23.317323Z","iopub.status.idle":"2021-07-04T06:49:23.353665Z","shell.execute_reply.started":"2021-07-04T06:49:23.317286Z","shell.execute_reply":"2021-07-04T06:49:23.352642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs_df[numeric_cols].describe().loc[['min', 'max']]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.355042Z","iopub.execute_input":"2021-07-04T06:49:23.355374Z","iopub.status.idle":"2021-07-04T06:49:23.455494Z","shell.execute_reply.started":"2021-07-04T06:49:23.355345Z","shell.execute_reply":"2021-07-04T06:49:23.454365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Encode Categorical Columns\nOur dataset contains several categorical columns, each with a different number of categories.","metadata":{}},{"cell_type":"code","source":"inputs_df[categorical_cols].nunique().sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.456894Z","iopub.execute_input":"2021-07-04T06:49:23.457518Z","iopub.status.idle":"2021-07-04T06:49:23.49155Z","shell.execute_reply.started":"2021-07-04T06:49:23.457471Z","shell.execute_reply":"2021-07-04T06:49:23.490692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since machine learning models can only be trained with numeric data, we need to convert categorical data to numbers. A common technique is to use one-hot encoding for categorical columns.\n\n<img src=\"https://i.imgur.com/n8GuiOO.png\" width=\"640\">\n\nOne hot encoding involves adding a new binary (0/1) column for each unique category of a categorical column.","metadata":{}},{"cell_type":"markdown","source":"encoding categorical columns in the dataset as one-hot vectors using OneHotEncoder from sklearn.preprocessing. Add a new binary (0/1) column for each category","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n# 1. Create the encoder\nencoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n# 2. Fit the encoder to the categorical colums\nencoder.fit(inputs_df[categorical_cols])","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.492695Z","iopub.execute_input":"2021-07-04T06:49:23.493158Z","iopub.status.idle":"2021-07-04T06:49:23.510317Z","shell.execute_reply.started":"2021-07-04T06:49:23.493114Z","shell.execute_reply":"2021-07-04T06:49:23.509372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. Generate column names for each category\nencoded_cols = list(encoder.get_feature_names(categorical_cols))\nencoded_cols","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.511567Z","iopub.execute_input":"2021-07-04T06:49:23.511865Z","iopub.status.idle":"2021-07-04T06:49:23.529159Z","shell.execute_reply.started":"2021-07-04T06:49:23.511834Z","shell.execute_reply":"2021-07-04T06:49:23.527714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs_df[encoded_cols] = encoder.transform(inputs_df[categorical_cols])\ninputs_df","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.536326Z","iopub.execute_input":"2021-07-04T06:49:23.536982Z","iopub.status.idle":"2021-07-04T06:49:23.811684Z","shell.execute_reply.started":"2021-07-04T06:49:23.536941Z","shell.execute_reply":"2021-07-04T06:49:23.810774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training and Validation Set\n\nFinally, let's split the dataset into a training and validation set. We'll use a randomly select 25% subset of the data for validation. Also, we'll use just the numeric and encoded columns, since the inputs to our model must be numbers. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_inputs, val_inputs, train_targets, val_targets = train_test_split(inputs_df[numeric_cols + encoded_cols], \n                                                                        targets, \n                                                                        test_size=0.25, \n                                                                        random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.8134Z","iopub.execute_input":"2021-07-04T06:49:23.813706Z","iopub.status.idle":"2021-07-04T06:49:23.828485Z","shell.execute_reply.started":"2021-07-04T06:49:23.813675Z","shell.execute_reply":"2021-07-04T06:49:23.827316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inputs","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.829699Z","iopub.execute_input":"2021-07-04T06:49:23.829972Z","iopub.status.idle":"2021-07-04T06:49:23.875854Z","shell.execute_reply.started":"2021-07-04T06:49:23.829945Z","shell.execute_reply":"2021-07-04T06:49:23.875056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_targets","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.876807Z","iopub.execute_input":"2021-07-04T06:49:23.877095Z","iopub.status.idle":"2021-07-04T06:49:23.884711Z","shell.execute_reply.started":"2021-07-04T06:49:23.877043Z","shell.execute_reply":"2021-07-04T06:49:23.883737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_inputs","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.886779Z","iopub.execute_input":"2021-07-04T06:49:23.88739Z","iopub.status.idle":"2021-07-04T06:49:23.935917Z","shell.execute_reply.started":"2021-07-04T06:49:23.887345Z","shell.execute_reply":"2021-07-04T06:49:23.934814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_targets","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.937482Z","iopub.execute_input":"2021-07-04T06:49:23.937903Z","iopub.status.idle":"2021-07-04T06:49:23.947148Z","shell.execute_reply.started":"2021-07-04T06:49:23.937858Z","shell.execute_reply":"2021-07-04T06:49:23.946095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3 - Train a Linear Regression Model\n\nWe're now ready to train the model. Linear regression is a commonly used technique for solving [regression problems](https://jovian.ai/aakashns/python-sklearn-logistic-regression/v/66#C6). In a linear regression model, the target is modeled as a linear combination (or weighted sum) of input features. The predictions from the model are evaluated using a loss function like the Root Mean Squared Error (RMSE).\n\n\nHere's a visual summary of how a linear regression model is structured:\n\n<img src=\"https://i.imgur.com/iTM2s5k.png\" width=\"480\">\n\nHowever, linear regression doesn't generalize very well when we have a large number of input columns with co-linearity i.e. when the values one column are highly correlated with values in other column(s). This is because it tries to fit the training data perfectly. \n\nInstead, we'll use Ridge Regression, a variant of linear regression that uses a technique called L2 regularization to introduce another loss term that forces the model to generalize better. Learn more about ridge regression here: https://www.youtube.com/watch?v=Q81RR3yKn30","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:02:09.24529Z","iopub.execute_input":"2021-07-04T06:02:09.245615Z","iopub.status.idle":"2021-07-04T06:02:09.256387Z","shell.execute_reply.started":"2021-07-04T06:02:09.245586Z","shell.execute_reply":"2021-07-04T06:02:09.254752Z"}}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\n# Create the model\nmodel = Ridge()\n# Fit the model using inputs and targets\nmodel.fit(train_inputs[numeric_cols + encoded_cols], train_targets)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.948438Z","iopub.execute_input":"2021-07-04T06:49:23.948727Z","iopub.status.idle":"2021-07-04T06:49:23.99107Z","shell.execute_reply.started":"2021-07-04T06:49:23.948697Z","shell.execute_reply":"2021-07-04T06:49:23.988837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4 - Make Predictions and Evaluate Your Model\n\nThe model is now trained, and we can use it to generate predictions for the training and validation inputs. We can evaluate the model's performance using the RMSE (root mean squared error) loss function.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\ntrain_preds = model.predict(train_inputs[numeric_cols + encoded_cols])\ntrain_preds","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:23.993445Z","iopub.execute_input":"2021-07-04T06:49:23.993881Z","iopub.status.idle":"2021-07-04T06:49:24.048144Z","shell.execute_reply.started":"2021-07-04T06:49:23.993838Z","shell.execute_reply":"2021-07-04T06:49:24.045895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_rmse = mean_squared_error(train_targets,train_preds,squared=False )","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:24.050516Z","iopub.execute_input":"2021-07-04T06:49:24.050972Z","iopub.status.idle":"2021-07-04T06:49:24.06722Z","shell.execute_reply.started":"2021-07-04T06:49:24.050929Z","shell.execute_reply":"2021-07-04T06:49:24.064581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('The RMSE loss for the training set is $ {}.'.format(train_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:24.069603Z","iopub.execute_input":"2021-07-04T06:49:24.073188Z","iopub.status.idle":"2021-07-04T06:49:24.083579Z","shell.execute_reply.started":"2021-07-04T06:49:24.073127Z","shell.execute_reply":"2021-07-04T06:49:24.081847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_preds = model.predict(val_inputs)\nval_preds","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:24.084961Z","iopub.execute_input":"2021-07-04T06:49:24.088254Z","iopub.status.idle":"2021-07-04T06:49:24.123798Z","shell.execute_reply.started":"2021-07-04T06:49:24.0882Z","shell.execute_reply":"2021-07-04T06:49:24.122639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_rmse = mean_squared_error(val_targets,val_preds,squared=False )\nprint('The RMSE loss for the validation set is $ {}.'.format(val_rmse))","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:24.1294Z","iopub.execute_input":"2021-07-04T06:49:24.132449Z","iopub.status.idle":"2021-07-04T06:49:24.144935Z","shell.execute_reply.started":"2021-07-04T06:49:24.132378Z","shell.execute_reply":"2021-07-04T06:49:24.143666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance\n\nLet's look at the weights assigned to different columns, to figure out which columns in the dataset are the most important.","metadata":{}},{"cell_type":"code","source":"weights = model.coef_\nweights_df = pd.DataFrame({\n    'columns': train_inputs.columns,\n    'weight': weights\n}).sort_values('weight', ascending=False)\nweights_df","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:24.151123Z","iopub.execute_input":"2021-07-04T06:49:24.154318Z","iopub.status.idle":"2021-07-04T06:49:24.18329Z","shell.execute_reply.started":"2021-07-04T06:49:24.154247Z","shell.execute_reply":"2021-07-04T06:49:24.181861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving the model\n\nLet's save the model (along with other useful objects) to disk, so that we use it for making predictions without retraining.","metadata":{}},{"cell_type":"code","source":"import joblib\nhouse_price_predictor = {\n    'model': model,\n    'imputer': imputer,\n    'scaler': scaler,\n    'encoder': encoder,\n    'input_cols': input_cols,\n    'target_col': target_col,\n    'numeric_cols': numeric_cols,\n    'categorical_cols': categorical_cols,\n    'encoded_cols': encoded_cols\n}\njoblib.dump(house_price_predictor, 'house_price_predictor.joblib')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:24.190158Z","iopub.execute_input":"2021-07-04T06:49:24.193525Z","iopub.status.idle":"2021-07-04T06:49:24.225347Z","shell.execute_reply.started":"2021-07-04T06:49:24.193453Z","shell.execute_reply":"2021-07-04T06:49:24.224121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### As we have done training,fitting, optimizing our model, lets try testing it with test.csv dataset","metadata":{}},{"cell_type":"code","source":"testing_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:24.228892Z","iopub.execute_input":"2021-07-04T06:49:24.229253Z","iopub.status.idle":"2021-07-04T06:49:24.25855Z","shell.execute_reply.started":"2021-07-04T06:49:24.229223Z","shell.execute_reply":"2021-07-04T06:49:24.257732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_data","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:24.25966Z","iopub.execute_input":"2021-07-04T06:49:24.260135Z","iopub.status.idle":"2021-07-04T06:49:24.300716Z","shell.execute_reply.started":"2021-07-04T06:49:24.260091Z","shell.execute_reply":"2021-07-04T06:49:24.299919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now have to transform the data similar to what we did with our train data","metadata":{}},{"cell_type":"code","source":"testing_data[numeric_cols] = imputer.transform(testing_data[numeric_cols])\ntesting_data[numeric_cols] = scaler.transform(testing_data[numeric_cols])\ntesting_data[encoded_cols] = encoder.transform(testing_data[categorical_cols].values)\nX_input = testing_data[numeric_cols + encoded_cols]","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:24.301846Z","iopub.execute_input":"2021-07-04T06:49:24.302291Z","iopub.status.idle":"2021-07-04T06:49:24.587961Z","shell.execute_reply.started":"2021-07-04T06:49:24.302247Z","shell.execute_reply":"2021-07-04T06:49:24.587044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_data","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:24.589307Z","iopub.execute_input":"2021-07-04T06:49:24.589612Z","iopub.status.idle":"2021-07-04T06:49:24.637373Z","shell.execute_reply.started":"2021-07-04T06:49:24.589581Z","shell.execute_reply":"2021-07-04T06:49:24.636251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Followin our final output for testing dataset.","metadata":{}},{"cell_type":"code","source":"test_preds = model.predict(X_input)\noutp = {'id': testing_data['Id'],\n        'SalePrice': test_preds}","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:56:34.780738Z","iopub.execute_input":"2021-07-04T06:56:34.781721Z","iopub.status.idle":"2021-07-04T06:56:34.800175Z","shell.execute_reply.started":"2021-07-04T06:56:34.781674Z","shell.execute_reply":"2021-07-04T06:56:34.798115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_df = pd.DataFrame(outp)\nout_df","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:56:45.048478Z","iopub.execute_input":"2021-07-04T06:56:45.048882Z","iopub.status.idle":"2021-07-04T06:56:45.06564Z","shell.execute_reply.started":"2021-07-04T06:56:45.048845Z","shell.execute_reply":"2021-07-04T06:56:45.064854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out_df.to_csv('output.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:58:01.105648Z","iopub.execute_input":"2021-07-04T06:58:01.106083Z","iopub.status.idle":"2021-07-04T06:58:01.118182Z","shell.execute_reply.started":"2021-07-04T06:58:01.106034Z","shell.execute_reply":"2021-07-04T06:58:01.117143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-07-04T06:49:24.673661Z","iopub.execute_input":"2021-07-04T06:49:24.674167Z","iopub.status.idle":"2021-07-04T06:49:24.701469Z","shell.execute_reply.started":"2021-07-04T06:49:24.674116Z","shell.execute_reply":"2021-07-04T06:49:24.700067Z"},"trusted":true},"execution_count":null,"outputs":[]}]}