{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Table of Content\n\n* [1. Introduction](#1.-Introduction)\n    * [1.1. Goals](#1.1.-Goals)\n    * [1.2. Libraries & Tools](#1.2.-Libraries-&-Tools)\n* [2. Exploratory Data Analysis](#2.-Exploratory-Data-Analysis)\n    * [2.1. Overview of the data](#2.1.-Overview-of-the-data)\n    * [2.2. House Building Variables](#2.2.-House-Building-Variables)\n    * [2.3. Continuous Variables](#2.3.-Continuous-Variables)\n        * [2.3.1. Continuous Variables (EDA)](#2.3.1.-Continuous-Variables-(EDA))\n        * [2.3.2. Correlations Among Continuous Variables](#2.3.2.-Correlations-Among-Continuous-Variables)\n        * [2.3.3. Continuous Variables Pipeline](#2.3.3.-Continuous-Variables-Pipeline)\n    * [2.4. Categorical Variables](#2.4.-Categorical-Variables)\n        * [2.4.1. Categorical Variables (EDA)](#2.4.1.-Categorical-Variables-(EDA))\n        * [2.4.2. Discardable Categorical Variables](#2.4.2.-Discardable-Categorical-Variables)\n        * [2.4.3. Categorical Variables Pipeline](#2.4.3.-Categorical-Variables-Pipeline)\n    * [2.5. General House Pricing Pipeline](#2.5.-General-House-Pricing-Pipeline)\n* [3. House Pricing Model](#3.-House-Pricing-Model)\n    * [3.1. Model Selection](#3.1.-Model-Selection)\n    * [3.2. Fine Tune Model](#3.2.-Fine-Tune-Model)\n    * [3.3. System Evaluation on the Test Set](#3.3.-System-Evaluation-on-the-Test-Set)\n* [4. Conclusion](#4.-Conclusion)","metadata":{}},{"cell_type":"markdown","source":"# 1. Introduction\n\nI started out this book [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) 2nd Edition by [Aurélien Géron](https://www.oreilly.com/people/aurelien-geron/). Just through with the second chapter on <b style=\"color: #DE3163\">End-to-End Machine Learning Project</b> where we worked on a housing dataset to predict sales prices. This was a regression problem and at the end of this chapter i was inspired to practice what I learnt. So I went out in search of a good dataset to practice with because i didn't want to make use of the same dataset that was used in the book. I came across this dataset in my search and its quiet an amazing dataset exceeding my expectations by far. So here we are my first kaggle competition after reading two chapters of a text book.\n\n**_Description and context:_**\n_Ask a home buyer to describe the home of their dreams and he probably won't start the description with \"basement ceiling height\" or \"proximity to an east-west railroad\". However, the data set of this competition proves that there are influences in the negotiation of houses in addition to the number of bedrooms or bathrooms. With approximately 80 explanatory variables describing virtually any residential aspect of homes in Ames, Iowa, this competition challenges the user to predict the final price of homes._\n\n\n## 1.1. Goals\n\nMy goals in this **notebook** are to:\n\n1. Discover and visualize the data to gain insights.\n2. Prepare the data for Machine Learning algorithms.\n3. Select a model and train it.\n4. Fine-tune the model.\n5. Present my solution.\n\n### If you find this notebook inciteful, please don't forget to upvote :)\n\nSo let's get started.\n\n## 1.2. Libraries & Tools","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\n\n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pie_plot(column, data=None, title=None, ax=None, fontsize=15, explode=0,\n             autopct='%1.1f%%', shadow=None, figsize=(10, 6), colors=None, color_with_label=None):\n    \"\"\"\n    returns: a pie plot of the quantities of unique valiues from a column.\n    \"\"\"\n    \n    if type(column) == str:\n        target = data[column].value_counts()\n    else:\n        target = column.value_counts()\n    \n    explode = [explode for _ in range(len(target))]\n    \n    if color_with_label:\n        colors = [color_with_label[key] for key in target.index]\n        \n    if ax is not None:\n        if title is not None:\n            ax.set_title(title, fontsize=fontsize)\n\n        ax.pie(target, labels=target.index, autopct=autopct, shadow=shadow, explode=explode, colors=colors)\n    else:\n        fig = plt.figure(figsize=figsize)\n\n        if title is not None:\n            plt.title(title, fontsize=fontsize)\n\n        plt.pie(target, labels=target.index, autopct=autopct, shadow=shadow, explode=explode, colors=colors)\n        \n\ndef bar_plot(column, data=None, title=None, ax=None, fontsize=15, figsize=(10, 6), color='b'):\n    \"\"\"\n    returns: a bar plot of the quantities of unique valiues from a column.\n    \"\"\"\n    \n    if type(column) == str:\n        target = data[column].value_counts()\n    else:\n        target = column.value_counts()\n    \n    if ax is not None:\n        if title is not None:\n            ax.set_title(title, fontsize=fontsize)\n\n        ax.bar(target.index, target, color=color)\n    else:\n        fig = plt.figure(figsize=figsize)\n\n        if title is not None:\n            plt.title(title, fontsize=fontsize)\n\n        plt.bar(target.index, target, color=color)\n    \n\ndef compare_plots(shape, columns, titles=None, data=None, kind='pie', explode=0, color='b',\n                  fontsize=15, autopct='%1.1f%%', figsize=(20, 10), shadow=None, color_with_label=True):\n    \"\"\"\n    returns: a matplotlib.subplot consisting of several features as described by\n            the 'columns' using the 'data' DataFrame using a preferred kind.\n    \"\"\"\n    \n    fig, axes = plt.subplots(*shape, figsize=figsize)\n    \n    for i, ax in enumerate(axes.ravel()):\n        title = titles[i] if titles is not None else None\n        \n        if kind == 'pie':\n            pie_plot(columns[i], data=data, title=title, ax=ax, fontsize=fontsize, colors=color,\n                     autopct=autopct, figsize=figsize, explode=explode, shadow=shadow, \n                     color_with_label=color_with_label)\n        elif kind == 'bar':\n            bar_plot(columns[i], data=data, title=title, ax=ax, fontsize=fontsize, figsize=figsize, color=color)\n        else:\n            raise TypeError\n\ndef known_vs_unknown(columns, data, unknown=np.nan):\n    \"\"\"\n    returns: pd.DataFrame object - consisting of the 'columns' features \n            distinguishing between known and unknown variables.\n    \"\"\"\n    \n    _data = data.copy()\n\n    for col in columns:\n        _data[col] = _data[col].replace(unknown, 'Unknown')\n        _data[col][_data[col] != 'Unknown'] = 'Known'\n    \n    return _data.loc[:, columns]\n\ndef compare_models(data, labels, models, scoring=\"neg_mean_squared_error\", cv=10):\n    \"\"\"\n    returns: a dictionary comparing a series of models on a whole set and \n            a cross validation set\n    \"\"\"\n    \n    record = {}\n    \n    for name, model in models:\n        m = model\n        model.fit(data, labels)\n        predictions = model.predict(data)\n        model_mse = mean_squared_error(labels, predictions)\n        model_rmse = np.sqrt(model_mse)\n        scores = cross_val_score(model, data, labels, scoring=scoring, cv=cv)\n        scores = np.sqrt(-scores)\n        \n        model_record = {\n            \"model\": m,\n            \"mean_squared_error\": model_mse,\n            \"root_mean_squared_error\": model_rmse,\n            \"scores\": scores,\n            \"scores_mean\": scores.mean(),\n            \"scores_std\": scores.std()\n        }\n        \n        record[name] = model_record\n    \n    return record","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis\n\nThe housing data-set has already been divided into two distinctive set - the train and test set. We'll start by loading and performing data analysis on the training-set.\n\n\n## 2.1. Overview of the data\n\nThis dataset consists of 80 features with their descriptions below:\n\n* **_1stFlrSF_**: Total area of ​​the first floor of the house\n* **_2ndFlrSF_**: Total area of ​​the second floor of the house\n* **_3SsnPorch_**: Varande area of ​​three seasons (?)\n* **_Alley_**: Characteristic of the alley that gives access to housing\n* **_BedroomAbvGr_**: Number of beds in the house (above the basement)\n* **_BldgType_**: Type of housing\n* **_BsmtCond_**: Classifies the basement's general condition\n* **_BsmtExposure_**: Exposition of the basement of the dwelling\n* **_BsmtFinSF1_**: Area covered by type 1 finish (BsmtFinType1 attribute)\n* **_BsmtFinSF2_**: Area covered by type 2 finish (BsmtFinType2 attribute)\n* **_BsmtFinType1_**: Score of the finish of the basement-related region\n* **_BsmtFinType2_**: Score of the finish of the basement-related region (if more than one exists)\n* **_BsmtFullBath_**: Full bathrooms in the basement-related area\n* **_BsmtHalfBath_**: Incomplete bathrooms (half) of the basement-related area\n* **_BsmtQual_**: Classifies the house according to the size of the basement\n* **_BsmtUnfSF_**: Basement area without finishing\n* **_CentralAir_**: Defines whether or not there is a central air conditioner (Boolean attribute)\n* **_Condition1_**: Proximity to important points in the city\n* **_Condition2_**: Proximity to important points in the city (if there are more than one)\n* **_Electrical_**: Type of home electrical system\n* **_EnclosedPorch_**: Closed balcony area in the house\n* **_ExterCond_**: Condition of the external material on the observation date\n* **_Exterior1st_**: External coverage of the house\n* **_Exterior2nd_**: External roof of the house (if there is more than one roof)\n* **_ExterQual_**: Quality of the material used abroad\n* **_Fence_**: Quality of the enclosure present in the house\n* **_FireplaceQu_**: Quality of fireplaces\n* **_Fireplaces_**: Number of fireplaces in the house\n* **_Foundation_**: Type of foundation used in construction\n* **_FullBath_**: Number of full bathrooms in the house (above the basement)\n* **_Functional_**: Describes features of the house under warranty\n* **_GarageArea_**: Garage area in square meters\n* **_GarageCars_**: Size of the garage related to the number of possible cars\n* **_GarageCond_**: Score that defines the conditions of the garage\n* **_GarageFinish_**: Internal garage finish\n* **_GarageQual_**: Quality of the garage\n* **_GarageType_**: Type of garage in the house\n* **_GarageYrBlt_**: Year of construction of the garage\n* **_GrLivArea_**: Total living room area\n* **_HalfBath_**: Number of incomplete bathrooms (half) in the house (above the basement)\n* **_Heating_**: Type of house heating\n* **_HeatingQC_**: Heating quality\n* **_HouseStyle_**: Housing style\n* **_KitchenAbvGr_**: Number of kitchens in the house (above the basement)\n* **_KitchenQual_**: Quality of the kitchens\n* **_LandContour_**: Housing leveling\n* **_LandSlope_**: Property slope\n* **_LotArea_**: Allotment area\n* **_LotConfig_**: Allotment configuration\n* **_LotFrontage_**: Dimension of the front perimeter of the house\n* **_LotShape_**: General housing format\n* **_LowQualFinSF_**: Total area of ​​low quality finishes throughout the house\n* **_MasVnrArea_**: Area covered by masonry\n* **_MasVnrType_**: Type of masonry used\n* **_MiscFeature_**: Some features not included in other categories\n* **_MiscVal_**: Value of features not included in quantity criteria\n* **_MoSold_**: Month in which the sale of the house was made\n* **_MSSubClass_**: Identifies the type of residence\n* **_MSZoning_**: Classifies the property by zone\n* **_Neighborhood_**: Locality related to city boundaries\n* **_variable_name_**: description\n* **_OpenPorchSF_**: Open balcony area in the house\n* **_OverallCond_**: Score of the general condition of the house\n* **_OverallQual_**: Score of the material and finish of the house\n* **_PavedDrive_**: Attribute that defines the paving of the street (inside the house)\n* **_PoolArea_**: Pool area in the house\n* **_PoolQC_**: Quality of the pool\n* **_RoodMatl_**: Material used in the roof (roof)\n* **_RoofStyle_**: Type of roof of the house (roof)\n* **_SaleCondition_**: Conditions of sale\n* **_SaleType_**: Type of sale\n* **_ScreenPorch_**: Screen area on the balcony of the house\n* **_Street_**: Characteristic of the street that gives access to housing\n* **_TotalBsmtSF_**: Total hold area\n* **_TotRmsAbvGrd_**: Total number of rooms in the house (above the basement)\n* **_Utilities_**: Utilities\n* **_WoodDeckSF_**: Wooden deck area present in the house\n* **_YearBuilt_**: Year of construction of the house\n* **_YearRemodAdd_**: Year of remodeling of the house (same as YearBuilt if the house has not been remodeled)\n* **_YrSold_**: Year in which the sale of the house was made","metadata":{}},{"cell_type":"code","source":"house_prices_train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\", index_col='Id')\nhouse_prices_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"house_prices_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 1460 training data in this set with 80 features. This is alot to handle, but the huge amount of features makes up for the reduced number of training data, and so we definitely want to keep as many feature as necessary. ","metadata":{}},{"cell_type":"code","source":"house_prices_train.duplicated().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thankfully there are no duplicates, so we'll keep all the data.","metadata":{}},{"cell_type":"code","source":"house_prices_train.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `describe` method here describes all the continuous data in the data-set which comes to a total of 37 continuous variables.","metadata":{}},{"cell_type":"code","source":"house_prices_train.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2. House Building Variables\n\nThe house building variables can generally be divided in two major categories which are the **continuous variables** consisting of floating point and integer data types and the **categorical variables** consisting of the numpy.object data type.","metadata":{}},{"cell_type":"code","source":"house_prices_train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's properly distinguish between continuous and categorical variables","metadata":{}},{"cell_type":"code","source":"continuous_col= list(house_prices_train.describe().columns)\ncategorical_col = [_d for _d in house_prices_train.columns if _d not in continuous_col]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That settled, we should also seperate continuous and categorical variables for easier and effetive data analysis","metadata":{}},{"cell_type":"code","source":"continuous_data = house_prices_train.loc[:, continuous_col]\ncategorical_data = house_prices_train.loc[:, categorical_col]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3. Continuous Variables\n\nLet's look through the continuous variables to identify features containing null values. ","metadata":{}},{"cell_type":"code","source":"continuous_data.columns, f\"Length: {len(continuous_col)}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"continuous_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.1. Continuous Variables (EDA)","metadata":{}},{"cell_type":"code","source":"cont_d = continuous_data.isnull().any()\nmissing_continuous = list(cont_d[cont_d == True].index)\nmissing_continuous","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have identified three (3) features containing `null` values. \n\n1. `LotFrontage`: Linear feet of street connected to property\n2. `MasVnrArea`: Masonry veneer area in square feet\n3. `GarageYrBlt`: Year garage was built\n\nWe now have to compare quantity of known to the unknown values. This will help us determine useful an imputer will be to missing columns. It should also tell us if any of this features should be discarded. ","metadata":{}},{"cell_type":"code","source":"data = known_vs_unknown(missing_continuous, continuous_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colors = {\n    \"Unknown\": \"#FF5733\",\n    \"Known\": \"#2471A3\",\n}\n\ncompare_plots((1, 3), columns=missing_continuous, titles=missing_continuous, data=data, color_with_label=colors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the amount of unknown for `LotFrontage`, `MasVnrArea` & `GarageYrBlt` are relatively small. We can thus fill all the unknown with the median of each feature.\n\nBefore going into that, we need to extract the labels (`SalePrice`) from the continuous variables.","metadata":{}},{"cell_type":"code","source":"labels = continuous_data.SalePrice\ncontinuous_data.drop(\"SalePrice\", axis=1, inplace=True)\ncontinuous_col = list(continuous_data.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's plot an histogram of all the features in the continuous variables to get a feel of the type of data we are dealing with.","metadata":{}},{"cell_type":"code","source":"plt.style.use(\"ggplot\")\ncontinuous_data.hist(bins=50, figsize=(20,20))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plots we notice these attributes have very different scales.\n\nAnother thing to notice is many histograms are tail-heavy -that is either having negative skew or positive skew (they extend much farther to the right of the median than to the left). This might make it a bit harder for some Machine Learning algorithms to detect patterns. We will have to transform some of these attributes to have more bell-shaped distributions.\n\nWe should also look for **correlations** among continuous variables. Say the factors that relates best with\n\n1. `YearBuilt`: Original construction date\n2. `1stFlrSF`: First Floor square feet\n3. `GarageArea`: Size of garage in square feet\n\n### 2.3.2. Correlations Among Continuous Variables\n\nCorrelation coefficients ranges from –1 to 1. When it is close to 1, it means that there is a strong positive correlation and vice versa.","metadata":{}},{"cell_type":"code","source":"Image(\"../input/correlation/correlation.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix = continuous_data.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix[\"YearBuilt\"].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix[\"1stFlrSF\"].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix[\"GarageArea\"].sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can tell most of the features are correlated. Now let's see correlations between just a few features.","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"ticks\")\nattributes = [\"TotRmsAbvGrd\", \"GrLivArea\", \"TotalBsmtSF\", \"2ndFlrSF\", \"BedroomAbvGr\", \"OverallQual\", \"1stFlrSF\", \n              \"GarageArea\"]\nsns.pairplot(continuous_data[attributes])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have two means we wish to apply in handling the continuous variables.\n\n1. Replacing missing values with the median for each column. To achieve this we will be making use of the `sklearn.impute.SimpleImputer` which will calculate the median for all features and automatically replace missing values with the median.\n2. Scaling the data to all have a bell-shaped distribution. To achieve this we will apply standardization using the `sklearn.preprocessing.StandardScaler`.\n\n### 2.3.3. Continuous Variables Pipeline\n\nTo effectively carry this out - even to new datasets, we should create a pipeline for continuous variables.","metadata":{}},{"cell_type":"code","source":"continuous_data_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"median\")),\n    ('num_scaler', StandardScaler()),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"continuous_data_tr = continuous_data_pipeline.fit_transform(continuous_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have succesfully analysed and created a pipeline for the continuous variables, we can switch focus to the categorical variables.\n\n## 2.4. Categorical Variables\n\nLet's look through the categorical variables to identify features containing null values. ","metadata":{}},{"cell_type":"code","source":"categorical_data.columns, f\"Length: {len(categorical_col)}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.1. Categorical Variables (EDA)","metadata":{}},{"cell_type":"code","source":"cat_d = categorical_data.isnull().any()\nmissing_categorical = list(cat_d[cat_d == True].index)\nlen(missing_categorical), missing_categorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just like we did with the continuous variables. We have identified sixteen (16) features containing `null` values. \n\n1. `Alley`: Type of alley access to property\n2. `MasVnrType`: Masonry veneer type\n3. `BsmtQual`: Evaluates the height of the basement\n4. `BsmtCond`: Evaluates the general condition of the basement\n5. `BsmtExposure`: Refers to walkout or garden level walls\n6. `BsmtFinType1`: Rating of basement finished area\n7. `BsmtFinType2`: Rating of basement finished area (if multiple types)\n8. `Electrical`: Electrical system\n9. `FireplaceQu`: Fireplace quality\n10. `GarageType`: Garage location\n11. `GarageFinish`: Interior finish of the garage\n12. `GarageQual`: Garage quality\n13. `GarageCond`: Garage condition\n14. `PoolQC`: Pool quality\n15. `Fence`: Fence quality\n16. `MiscFeature`: Miscellaneous feature not covered in other categories\n\nWe now have to compare quantity of known to the unknown values. This will help us determine useful columns and columns that should be discarded. ","metadata":{}},{"cell_type":"code","source":"data = known_vs_unknown(missing_categorical, categorical_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"compare_plots((4, 4), columns=missing_categorical, titles=missing_categorical, data=data, \n              figsize=(20, 20), color_with_label=colors)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4.2. Discardable Categorical Variables\n\nRight from the info method on code line 26, we could tell that the features `Alley`, `FireplaceQu`, `PoolQC`, `Fence` & `MiscFeature` are really missing alot of values, but visualizing them left us see the impart of this missing values on this features.\n\nIn situations like this where majority or close to have of the dataset values is unknown. It will only make sense to discard of this values. Thus discarding `[\"Alley\", \"FireplaceQu\", \"PoolQC\", \"Fence\", \"MiscFeature\"]`","metadata":{}},{"cell_type":"code","source":"drop_categorical = [\"Alley\", \"FireplaceQu\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\ncategorical_data.drop(drop_categorical, axis=1, inplace=True)\ncategorical_col = list(categorical_data.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The proportions of unknown in other features are significantly small enough. Let's preview the category of each feature in a bar plot to understand them better.","metadata":{}},{"cell_type":"code","source":"f, axes = plt.subplots(9, 3, figsize=(20, 35))\n\nfor ax, col in zip(axes.ravel(), categorical_data.columns):\n    target = categorical_data[col].value_counts()\n    ax.bar(target.index, target)\n    ax.set_title(col)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's a positive skewness across every categorical feature in terms of frequencies. So one way we can input missing values is by imputing the most frequent values.\n\nWe also have to encode every column in the categorical variable. From the [data description](../input/house-prices-advanced-regression-techniques/data_description.txt), we can tell most of the categorical features of the dataset are organized (ordered), therefore an OrdinalEncoder should work prefectly for encoding each column.\n\n### 2.4.3. Categorical Variables Pipeline\n\nwe would now create a pipeline for categorical variables.","metadata":{}},{"cell_type":"code","source":"categorical_data_pipeline = Pipeline([\n    ('freq_imputer', SimpleImputer(strategy='most_frequent')),\n    ('cat_encoder', OrdinalEncoder())\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"categorical_data_tr = categorical_data_pipeline.fit_transform(categorical_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.5. General House Pricing Pipeline\n\nWith pipelines for both continuous and categorical variables created, we can create a general pipeline for the housing price dataset and then a function that will autonomously prepare the dataset for a Machine Learning model.","metadata":{}},{"cell_type":"code","source":"housing_price_pipeline = ColumnTransformer([\n    (\"continous\", continuous_data_pipeline, continuous_col),\n    (\"categorical\", categorical_data_pipeline, categorical_col),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(data, drop_cols, fit_trasform=True):\n    data.drop(drop_cols, axis=1, inplace=True)\n    if fit_trasform:\n        return housing_price_pipeline.fit_transform(data)\n    else:\n        return housing_price_pipeline.transform(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = house_prices_train.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = prepare_data(data, drop_categorical + [\"SalePrice\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we're all set to train models\n\n# 3. House Pricing Model\n\nSince we are dealing with a regression problem, let's hand four (4) regression models and compare them to each other. We'll be making use of `Linear Regression`, `Decision Tree Regression`, `Random Forest Regression`, `Support Vector Regression`. We well use the `mean_squared_error` metrics to evaluate the model.\n\n## 3.1. Model Selection\n\nLet's compare the models.","metadata":{}},{"cell_type":"code","source":"lin_reg = LinearRegression()\ndt_reg = DecisionTreeRegressor()\nrf_reg = RandomForestRegressor()\nsvm_reg = SVR()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = zip(\n    [\"Linear Regression\", \"Decision Tree Regression\", \"Random Forest Regression\", \"Support Vector Regression\"],\n    [lin_reg, dt_reg, rf_reg, svm_reg]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `compare_models` function performs the following operation on each model.\n\n1. Train a model with the whole training set the gets both the `mean_squared_error` & the `root_mean_squared_error` to evaluate the model performance on the whole dataset.\n2. Then perform cross validation (with cross validation folds set to 10) to see how well the model scores to new instance after being trained.","metadata":{}},{"cell_type":"code","source":"records = compare_models(data, labels, models)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"records[\"Linear Regression\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the `Linear Regression` model, we have a rmse of 31785.37 after the model trained on the whole dataset and an overall mean of 44628698410.08 after training on a cross validation set, which tells us that the `Linear Regression` model is underfitting.","metadata":{}},{"cell_type":"code","source":"records[\"Decision Tree Regression\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the `Decision Tree Regression` model, we have a rmse of 0.0 after the model trained on the whole dataset. This is actually a perfect score but after training on a cross validation set we get an overall mean of 39577.57, which tells us that the `Decision Tree Regression` model is overfitting.","metadata":{}},{"cell_type":"code","source":"records[\"Random Forest Regression\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the `Random Forest Regression` model, we have a rmse of 10654.21 after the model trained on the whole dataset. This is better than the `Linear Regression` model and after training on a cross validation set we get an overall mean of 28638.76, which tells us that the `Random Forest Regression` model is not overfitting like the `Decision Tree Regression`.","metadata":{}},{"cell_type":"code","source":"records[\"Support Vector Regression\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `Support Vector Regression` model on the other is clearly underfitting on both whole training set and the cross validation set.\n\n## 3.2. Fine Tune Model\n\nWe'll now make use of the Scikit-Learn’s GridSearchCV to manually fiddle the best hyperparameters for our model. ","metadata":{}},{"cell_type":"code","source":"param_grid = [\n    {'bootstrap': [True, False], 'n_estimators': [10, 30, 40], 'max_features': [6, 8, 10, 12]}\n]\n\ngrid_search = GridSearchCV(records[\"Random Forest Regression\"][\"model\"], param_grid, cv=5, \n                           scoring='neg_mean_squared_error', return_train_score=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.fit(data, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After running the grid search model we get the parameters to be `{'bootstrap': False, 'max_features': 12, 'n_estimators': 40}`\n\nNow that we have successfully fine-tuned our model, let's set our final model.","metadata":{}},{"cell_type":"code","source":"model = grid_search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances = grid_search.best_estimator_.feature_importances_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"attributes = continuous_col + categorical_col","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now check the importance of each feature to the model and exclude features below the threshold of 0.01.","metadata":{}},{"cell_type":"code","source":"sorted(zip(feature_importances, attributes), reverse=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_important_features = ['OverallQual', 'GarageCars', 'ExterQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', \n                          'BsmtQual', '1stFlrSF', 'FullBath', 'YearBuilt', '2ndFlrSF', 'BsmtFinSF1', \n                          'YearRemodAdd', 'KitchenQual', 'LotArea', 'TotRmsAbvGrd', 'GarageFinish', 'MasVnrArea', \n                          'Fireplaces']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"continuous_col = [_c for _c in continuous_col if _c in most_important_features]\ncategorical_col = [_c for _c in categorical_col if _c in most_important_features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"housing_price_pipeline = ColumnTransformer([\n    (\"continous\", continuous_data_pipeline, continuous_col),\n    (\"categorical\", categorical_data_pipeline, categorical_col),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3. System Evaluation on the Test Set\n\nNow we evaluate the system on the test set","metadata":{}},{"cell_type":"code","source":"data = house_prices_train.copy()\ndata = prepare_data(data, drop_categorical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(data, labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"house_prices_test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\", index_col=\"Id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = house_prices_test.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = prepare_data(test_data, drop_categorical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = model.predict(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sales_price = pd.Series(predictions, name=\"SalePrice\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.DataFrame({\n    \"Id\": house_prices_test.index, \n    \"SalePrice\": sales_price\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = result.set_index(\"Id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Conclusion\n\nIn this notebook I try making use of techniques I learnt from the book [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) to create a model to try and predict housing prices in line with the competition challenges to predict the final price of each home.\n\nPractice Skills Gained\n\n1. Creative feature engineering \n2. Advanced regression techniques (Random Forest) \n\nI wish to end this note by thanking Dean De Cock and all who compiled The Ames Housing dataset to be use in data science education.\n\n### Next Notebook\n\n[EDA on the various categorical data (0.80464)](https://www.kaggle.com/ganiyuolalekan/eda-on-the-various-categorical-data-0-80464)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}