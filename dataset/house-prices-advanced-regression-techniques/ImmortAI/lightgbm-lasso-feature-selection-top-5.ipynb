{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import and load data","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import RobustScaler\nimport sklearn.metrics as skm\nimport lightgbm as lgb\nfrom lightgbm import early_stopping\nfrom sklearn.linear_model import Lasso\n\nrandom_seed = 12345\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nsns.set_style('dark')\n\nTRAIN_PATH = \"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\"\nTEST_PATH = \"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\"\n\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-10T12:34:36.915956Z","iopub.execute_input":"2022-04-10T12:34:36.916462Z","iopub.status.idle":"2022-04-10T12:34:36.938814Z","shell.execute_reply.started":"2022-04-10T12:34:36.916425Z","shell.execute_reply":"2022-04-10T12:34:36.937933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(TRAIN_PATH) # load data\ndf_test = pd.read_csv(TEST_PATH) # load data\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:37.025582Z","iopub.execute_input":"2022-04-10T12:34:37.026048Z","iopub.status.idle":"2022-04-10T12:34:37.157188Z","shell.execute_reply.started":"2022-04-10T12:34:37.026016Z","shell.execute_reply":"2022-04-10T12:34:37.155968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Target Engineering","metadata":{}},{"cell_type":"markdown","source":"Let's take a look at the target","metadata":{}},{"cell_type":"code","source":"TARGET = 'SalePrice'\n\nplt.figure()\ndf[TARGET].hist(bins=20)\nplt.title(TARGET + ' before transformation')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:37.15893Z","iopub.execute_input":"2022-04-10T12:34:37.159232Z","iopub.status.idle":"2022-04-10T12:34:37.418455Z","shell.execute_reply.started":"2022-04-10T12:34:37.15919Z","shell.execute_reply":"2022-04-10T12:34:37.417591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, because of the high skewness and outliers, we need to log transform the target and delete some outliers by z-score.","metadata":{}},{"cell_type":"code","source":"df[TARGET] = np.log(df[TARGET])\ndf['z_score_target'] = np.abs(stats.zscore(df[TARGET]))\ndf = df.loc[df['z_score_target'] < 3].reset_index(drop=True)\ndel df['z_score_target']\n\nplt.figure()\ndf[TARGET].hist(bins=20)\nplt.title(TARGET + ' after transformation')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:37.420139Z","iopub.execute_input":"2022-04-10T12:34:37.420414Z","iopub.status.idle":"2022-04-10T12:34:37.707742Z","shell.execute_reply.started":"2022-04-10T12:34:37.420382Z","shell.execute_reply":"2022-04-10T12:34:37.707072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"As a first step, we categorize features into three groups:\n* Categorical: these features are disceret and contain limited number of categories and there is no order among categories.\n* Nominal: these features are similar to categorical features but there is order between them. for instance **GarageCars** feature, which is the number of cars that the garage supports, is nominal.\n* Numerical: these are features that are continous.\n\nAmong all the features, **GarageYrBlt** is excluded from the list because it has too many nan values that cannot be handled by assigning a meaningful number and there is not much information in it, since it is usually equal to YearBuilt.","metadata":{}},{"cell_type":"code","source":"\nNOMINAL_FEATURES = [\"BedroomAbvGr\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\",\n                    \"BsmtFullBath\", \"BsmtHalfBath\", \"BsmtQual\", \"ExterCond\", \"ExterQual\",\n                    \"Fireplaces\", \"FireplaceQu\", \"Functional\", \"FullBath\", \"GarageCars\",\n                    \"GarageCond\", \"GarageQual\", \"HalfBath\", \"HeatingQC\", \"KitchenAbvGr\",\n                    \"KitchenQual\", \"LandSlope\", \"LotShape\", \"PavedDrive\", \"PoolQC\",\n                    \"Street\", \"Utilities\", \"OverallCond\", \"OverallQual\", \"TotRmsAbvGrd\"]\n\n\nCATEGORICAL_FEATURES = [\"Alley\", 'MSSubClass', 'MoSold', 'MSZoning', 'LandContour',\n                        'LotConfig', 'Neighborhood', 'Condition1', 'Condition2',\n                        'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n                        'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n                        'Heating', 'CentralAir', 'Electrical', 'GarageType',\n                        'GarageFinish', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n\nNUMERICAL_FEATURES = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n                      'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF','1stFlrSF',\n                      '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF',\n                      'OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n                      'MiscVal', 'YrSold'] # GarageYrBlt is deleted because of too many nan values\n\n\nNUM_FEATURES = NOMINAL_FEATURES + NUMERICAL_FEATURES\nALL_FEATURES = NOMINAL_FEATURES + NUMERICAL_FEATURES + CATEGORICAL_FEATURES\n\ndf = df[ALL_FEATURES + [TARGET]].copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:37.709026Z","iopub.execute_input":"2022-04-10T12:34:37.709991Z","iopub.status.idle":"2022-04-10T12:34:37.725004Z","shell.execute_reply.started":"2022-04-10T12:34:37.709938Z","shell.execute_reply":"2022-04-10T12:34:37.723901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fix missing values","metadata":{}},{"cell_type":"code","source":"def fix_missing(df_data):\n        # Handle missing values for features where median/mean or most common value doesn't make sense\n\n    # Alley : data description says NA means \"no alley access\"\n    df_data.loc[:, \"Alley\"] = df_data.loc[:, \"Alley\"].fillna(\"None\")\n    # BedroomAbvGr : NA most likely means 0\n    df_data.loc[:, \"BedroomAbvGr\"] = df_data.loc[:, \"BedroomAbvGr\"].fillna(0)\n    # BsmtQual etc : data description says NA for basement features is \"no basement\"\n    df_data.loc[:, \"BsmtQual\"] = df_data.loc[:, \"BsmtQual\"].fillna(\"No\")\n    df_data.loc[:, \"BsmtCond\"] = df_data.loc[:, \"BsmtCond\"].fillna(\"No\")\n    df_data.loc[:, \"BsmtExposure\"] = df_data.loc[:, \"BsmtExposure\"].fillna(\"No\")\n    df_data.loc[:, \"BsmtFinType1\"] = df_data.loc[:, \"BsmtFinType1\"].fillna(\"No\")\n    df_data.loc[:, \"BsmtFinType2\"] = df_data.loc[:, \"BsmtFinType2\"].fillna(\"No\")\n    df_data.loc[:, \"BsmtFullBath\"] = df_data.loc[:, \"BsmtFullBath\"].fillna(0)\n    df_data.loc[:, \"BsmtHalfBath\"] = df_data.loc[:, \"BsmtHalfBath\"].fillna(0)\n    df_data.loc[:, \"BsmtUnfSF\"] = df_data.loc[:, \"BsmtUnfSF\"].fillna(0)\n    df_data.loc[:, \"TotalBsmtSF\"] = df_data.loc[:, \"TotalBsmtSF\"].fillna(0)\n    df_data.loc[:, \"BsmtFinSF1\"] = df_data.loc[:, \"BsmtFinSF1\"].fillna(0)\n    df_data.loc[:, \"BsmtFinSF2\"] = df_data.loc[:, \"BsmtFinSF2\"].fillna(0)\n    # CentralAir : NA most likely means No\n    df_data.loc[:, \"CentralAir\"] = df_data.loc[:, \"CentralAir\"].fillna(\"N\")\n    # Condition : NA most likely means Normal\n    df_data.loc[:, \"Condition1\"] = df_data.loc[:, \"Condition1\"].fillna(\"Norm\")\n    df_data.loc[:, \"Condition2\"] = df_data.loc[:, \"Condition2\"].fillna(\"Norm\")\n    # EnclosedPorch : NA most likely means no enclosed porch\n    df_data.loc[:, \"EnclosedPorch\"] = df_data.loc[:, \"EnclosedPorch\"].fillna(0)\n    # External stuff : NA most likely means average\n    df_data.loc[:, \"ExterCond\"] = df_data.loc[:, \"ExterCond\"].fillna(\"TA\")\n    df_data.loc[:, \"ExterQual\"] = df_data.loc[:, \"ExterQual\"].fillna(\"TA\")\n    # Fence : data description says NA means \"no fence\"\n    df_data.loc[:, \"Fence\"] = df_data.loc[:, \"Fence\"].fillna(\"No\")\n    # FireplaceQu : data description says NA means \"no fireplace\"\n    df_data.loc[:, \"FireplaceQu\"] = df_data.loc[:, \"FireplaceQu\"].fillna(\"No\")\n    df_data.loc[:, \"Fireplaces\"] = df_data.loc[:, \"Fireplaces\"].fillna(0)\n    # Functional : data description says NA means typical\n    df_data.loc[:, \"Functional\"] = df_data.loc[:, \"Functional\"].fillna(\"Typ\")\n    # GarageType etc : data description says NA for garage features is \"no garage\"\n    df_data.loc[:, \"GarageType\"] = df_data.loc[:, \"GarageType\"].fillna(\"No\")\n    df_data.loc[:, \"GarageFinish\"] = df_data.loc[:, \"GarageFinish\"].fillna(\"No\")\n    df_data.loc[:, \"GarageQual\"] = df_data.loc[:, \"GarageQual\"].fillna(\"No\")\n    df_data.loc[:, \"GarageCond\"] = df_data.loc[:, \"GarageCond\"].fillna(\"No\")\n    df_data.loc[:, \"GarageArea\"] = df_data.loc[:, \"GarageArea\"].fillna(0)\n    df_data.loc[:, \"GarageCars\"] = df_data.loc[:, \"GarageCars\"].fillna(0)\n    # HalfBath : NA most likely means no half baths above grade\n    df_data.loc[:, \"HalfBath\"] = df_data.loc[:, \"HalfBath\"].fillna(0)\n    # HeatingQC : NA most likely means typical\n    df_data.loc[:, \"HeatingQC\"] = df_data.loc[:, \"HeatingQC\"].fillna(\"TA\")\n    # KitchenAbvGr : NA most likely means 0\n    df_data.loc[:, \"KitchenAbvGr\"] = df_data.loc[:, \"KitchenAbvGr\"].fillna(0)\n    # KitchenQual : NA most likely means typical\n    df_data.loc[:, \"KitchenQual\"] = df_data.loc[:, \"KitchenQual\"].fillna(\"TA\")\n    # LotFrontage : NA most likely means no lot frontage\n    df_data.loc[:, \"LotFrontage\"] = df_data.loc[:, \"LotFrontage\"].fillna(0)\n    # LotShape : NA most likely means regular\n    df_data.loc[:, \"LotShape\"] = df_data.loc[:, \"LotShape\"].fillna(\"Reg\")\n    # MasVnrType : NA most likely means no veneer\n    df_data.loc[:, \"MasVnrType\"] = df_data.loc[:, \"MasVnrType\"].fillna(\"None\")\n    # KitchenAbvGr : NA most likely means 0\n    df_data.loc[:, \"MasVnrArea\"] = df_data.loc[:, \"MasVnrArea\"].fillna(0)\n    # MiscFeature : data description says NA means \"no misc feature\"\n    df_data.loc[:, \"MiscFeature\"] = df_data.loc[:, \"MiscFeature\"].fillna(\"No\")\n    df_data.loc[:, \"MiscVal\"] = df_data.loc[:, \"MiscVal\"].fillna(0)\n    # OpenPorchSF : NA most likely means no open porch\n    df_data.loc[:, \"OpenPorchSF\"] = df_data.loc[:, \"OpenPorchSF\"].fillna(0)\n    # PavedDrive : NA most likely means not paved\n    df_data.loc[:, \"PavedDrive\"] = df_data.loc[:, \"PavedDrive\"].fillna(\"N\")\n    # PoolQC : data description says NA means \"no pool\"\n    df_data.loc[:, \"PoolQC\"] = df_data.loc[:, \"PoolQC\"].fillna(\"No\")\n    df_data.loc[:, \"PoolArea\"] = df_data.loc[:, \"PoolArea\"].fillna(0)\n    # SaleCondition : NA most likely means normal sale\n    df_data.loc[:, \"SaleCondition\"] = df_data.loc[:, \"SaleCondition\"].fillna(\"Normal\")\n    # ScreenPorch : NA most likely means no screen porch\n    df_data.loc[:, \"ScreenPorch\"] = df_data.loc[:, \"ScreenPorch\"].fillna(0)\n    # TotRmsAbvGrd : NA most likely means 0\n    df_data.loc[:, \"TotRmsAbvGrd\"] = df_data.loc[:, \"TotRmsAbvGrd\"].fillna(0)\n    # Utilities : NA most likely means all public utilities\n    df_data.loc[:, \"Utilities\"] = df_data.loc[:, \"Utilities\"].fillna(\"AllPub\")\n    # WoodDeckSF : NA most likely means no wood deck\n    df_data.loc[:, \"WoodDeckSF\"] = df_data.loc[:, \"WoodDeckSF\"].fillna(0)\n    return df_data\n\ndf = fix_missing(df.copy())\ndf_test = fix_missing(df_test.copy()) # apply to test data as well","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:37.730163Z","iopub.execute_input":"2022-04-10T12:34:37.730736Z","iopub.status.idle":"2022-04-10T12:34:37.83046Z","shell.execute_reply.started":"2022-04-10T12:34:37.730702Z","shell.execute_reply":"2022-04-10T12:34:37.829541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical features","metadata":{}},{"cell_type":"code","source":"def fix_categories(df_data):\n\n    # Some numerical features are actually really categories\n    df_data = df_data.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\",\n                                               45 : \"SC45\", 50 : \"SC50\", 60 : \"SC60\",\n                                               70 : \"SC70\", 75 : \"SC75\", 80 : \"SC80\",\n                                               85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\",\n                                               150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\",\n                                               190 : \"SC190\"},\n                               \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\",\n                                           5 : \"May\", 6 : \"Jun\",7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\",\n                                           10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}})\n    \n    \n    # Encode some categorical features as ordered numbers when there is information in the order\n    df_data = df_data.replace({\"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3,\n                                             \"Gd\" : 4, \"Ex\" : 5},\n                               \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                               \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\n                                                 \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6},\n                               \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3,\n                                                 \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6},\n                               \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4,\n                                             \"Ex\" : 5},\n                               \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                               \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                               \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3,\n                                                \"Gd\" : 4, \"Ex\" : 5},\n                               \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4,\n                                               \"Mod\": 5, \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                               \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3,\n                                               \"Gd\" : 4, \"Ex\" : 5},\n                               \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3,\n                                               \"Gd\" : 4, \"Ex\" : 5},\n                               \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4,\n                                              \"Ex\" : 5},\n                               \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4,\n                                                \"Ex\" : 5},\n                               \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                               \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                               \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                               \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3,\n                                           \"Ex\" : 4},\n                               \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                               \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3,\n                                              \"AllPub\" : 4}}\n                             )\n    \n    return df_data\n\n\ndf = fix_categories(df.copy())\ndf_test = fix_categories(df_test.copy()) # apply to test data as well","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:37.831699Z","iopub.execute_input":"2022-04-10T12:34:37.831916Z","iopub.status.idle":"2022-04-10T12:34:37.967919Z","shell.execute_reply.started":"2022-04-10T12:34:37.83189Z","shell.execute_reply":"2022-04-10T12:34:37.967267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this step, when a category is rare, which means it is present in fewer than 2% of the data, we combine it with other rare ones. Therefore, data has fewer rare categories, which makes learning process easier.","metadata":{}},{"cell_type":"code","source":"# combine rare categorical features\nfor col in CATEGORICAL_FEATURES:\n    temp = df.groupby(col)[TARGET].count() / len(df)\n    temp = temp[temp<0.02].index\n    \n    df[col] = np.where(df[col].isin(temp),'rare',df[col])\n    df_test[col] = np.where(df_test[col].isin(temp),'rare',df_test[col]) # apply to test data as well","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:37.969468Z","iopub.execute_input":"2022-04-10T12:34:37.970389Z","iopub.status.idle":"2022-04-10T12:34:38.0333Z","shell.execute_reply.started":"2022-04-10T12:34:37.970343Z","shell.execute_reply":"2022-04-10T12:34:38.032419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert category names to numbers so that algorithms can use them. we also have to support missing for all categorical features, because test data has missing values that never occured in some features in train data.","metadata":{}},{"cell_type":"code","source":"for feature in CATEGORICAL_FEATURES:\n    labels_ordered = list(df.groupby([feature])['SalePrice'].mean().sort_values().index)\n    if 'missing' not in labels_ordered:\n        labels_ordered.append('missing')\n    labels_ordered = {k:i for i,k in enumerate(labels_ordered,0)}\n    df[feature] = df[feature].map(labels_ordered)\n    \n    df_test[feature] = df_test[feature].map(labels_ordered)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:38.034404Z","iopub.execute_input":"2022-04-10T12:34:38.034625Z","iopub.status.idle":"2022-04-10T12:34:38.124988Z","shell.execute_reply.started":"2022-04-10T12:34:38.034592Z","shell.execute_reply":"2022-04-10T12:34:38.124093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Numerical Features","metadata":{}},{"cell_type":"code","source":"df.plot.scatter(x='GrLivArea', y=TARGET)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:38.126228Z","iopub.execute_input":"2022-04-10T12:34:38.126674Z","iopub.status.idle":"2022-04-10T12:34:38.407388Z","shell.execute_reply.started":"2022-04-10T12:34:38.126635Z","shell.execute_reply":"2022-04-10T12:34:38.406469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Samples with GrLivArea bigger than 4000 are obviously outliers, so we should delete them from our training dataset","metadata":{}},{"cell_type":"code","source":"df = df[df['GrLivArea'] < 4000].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:38.409463Z","iopub.execute_input":"2022-04-10T12:34:38.410128Z","iopub.status.idle":"2022-04-10T12:34:38.417791Z","shell.execute_reply.started":"2022-04-10T12:34:38.410089Z","shell.execute_reply":"2022-04-10T12:34:38.41708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(int(np.ceil(len(NUMERICAL_FEATURES)/5)), 5, figsize=(25, len(NUMERICAL_FEATURES)))\nfor i, col in enumerate(NUMERICAL_FEATURES):\n    df[col].hist(bins=20, ax= ax[i // 5, i % 5])\n    ax[i // 5, i % 5].title.set_text(col)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:38.418966Z","iopub.execute_input":"2022-04-10T12:34:38.419344Z","iopub.status.idle":"2022-04-10T12:34:44.650737Z","shell.execute_reply.started":"2022-04-10T12:34:38.419302Z","shell.execute_reply":"2022-04-10T12:34:44.649687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As can bee seen from the distributions, there are features with high skewness. Therefore, We should apply log transformation to those features. \n\nBut some features have zero values. For these features, , we add another column called has_zero_{feature} to the data and log non-zero values","metadata":{}},{"cell_type":"code","source":"for col in NUMERICAL_FEATURES:\n\n    if col in [\"YearBuilt\", \"YearRemodAdd\", \"YrSold\"]:\n        continue\n    skew_before = df[col].skew()\n    \n    \n    # if we had zero or negative value in the data, we add another column called has_zero_{feature} to the data and log non-zero values\n    if (df[col]<=0).sum()>0:\n        # skip applying if we had not significant skewness\n        if df.loc[df[col]>0, col].skew() < 0.5:\n            continue\n            \n        \n        df['has_zero_' + col] = 0\n        df.loc[df[col] > 0,'has_zero_' + col] = 1\n        df.loc[df[col] > 0, col] = np.log(df.loc[df[col]>0, col])\n        \n        print('{} skewness before: {:.2f} skewness after: {:.2f}'.format(col, skew_before, df.loc[df[col]>0, col].skew()))\n        \n        \n        df_test['has_zero_' + col] = 0\n        df_test.loc[df_test[col] > 0,'has_zero_' + col] = 1\n        df_test.loc[df_test[col] > 0, col] = np.log(df_test.loc[df_test[col]>0, col])\n        \n        ALL_FEATURES.append('has_zero_' + col)\n        \n\n    # else we just apply log transformation\n    else:\n        # skip applying if we had not significant skewness\n        if df[col].skew() < 0.5:\n            continue\n        df[col] = np.log(df[col])\n        \n        # apply to test data as well\n        df_test[col] = np.log(df_test[col])\n        print('{} skewness before: {:.2f} skewness after: {:.2f}'.format(col, skew_before, df[col].skew()))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:44.652333Z","iopub.execute_input":"2022-04-10T12:34:44.652897Z","iopub.status.idle":"2022-04-10T12:34:44.788994Z","shell.execute_reply.started":"2022-04-10T12:34:44.652838Z","shell.execute_reply":"2022-04-10T12:34:44.78801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scaling data is necessary in machine learning. fortunately, robust scalar is an easy way to scale data without being worried about the impact of outlires.","metadata":{}},{"cell_type":"code","source":"scalar = RobustScaler()\n\ndf[NUM_FEATURES] = scalar.fit_transform(df[NUM_FEATURES])\ndf_test[NUM_FEATURES] = scalar.transform(df_test[NUM_FEATURES])","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:44.790296Z","iopub.execute_input":"2022-04-10T12:34:44.790539Z","iopub.status.idle":"2022-04-10T12:34:44.841889Z","shell.execute_reply.started":"2022-04-10T12:34:44.790511Z","shell.execute_reply":"2022-04-10T12:34:44.841095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# calculate metrics and print them\ndef calc_metrics(y, y_pred):\n    mae = skm.mean_absolute_error(y, y_pred)\n    r2 = skm.r2_score(y, y_pred)\n    rmse = np.sqrt(skm.mean_squared_error(y, y_pred))\n    corr = np.corrcoef(y, y_pred)[0,1]\n    print_str = 'MAE: {:.2f} R2: {:.2f} RMSE: {:.2f} Corr: {:.2f}'.format(mae, r2, rmse, corr)\n    return {'mae':mae, 'r2':r2, 'rmse':rmse, 'corr':corr}, print_str\n\n# a unique cross validaton for different models\ndef cv_train_and_evaluate(model, param_grid, x_train, y_train, x_val, y_val, model_name, n_folds=5, scoring='neg_median_absolute_error', fit_params={}):\n\n    clf = GridSearchCV(model, param_grid, cv=n_folds, scoring= scoring, refit=True, verbose=0)\n    clf.fit(x_train, y_train, **fit_params)\n    y_train_pred = clf.predict(x_train)\n    y_val_pred = clf.predict(x_val)\n\n    metrics_train, metric_str = calc_metrics(y_train ,y_train_pred)\n    print ('{} Train = {}'.format(model_name, metric_str))\n    metrics_val, metric_str = calc_metrics(y_val ,y_val_pred)\n    print ('{} Validation = {}'.format(model_name, metric_str))\n    return metrics_train, metrics_val, clf","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:44.843159Z","iopub.execute_input":"2022-04-10T12:34:44.843433Z","iopub.status.idle":"2022-04-10T12:34:44.853971Z","shell.execute_reply.started":"2022-04-10T12:34:44.843403Z","shell.execute_reply":"2022-04-10T12:34:44.853164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection model","metadata":{}},{"cell_type":"markdown","source":"Lasso regression has a very powerful built-in feature selection capability that can be used in several situation. The L1 penalty eliminates useless features.","metadata":{}},{"cell_type":"code","source":"# prepare data for lasso\ndf_all = pd.concat([df[ALL_FEATURES], df_test[['Id'] + ALL_FEATURES]]) # concat train and test to get dummy variable for all\ndf_all = pd.get_dummies(df_all, columns=CATEGORICAL_FEATURES)\n\ndf_lasso_train = df_all[pd.isna(df_all['Id'])] # get train data from df_All with id column since we didn't pass id for them\ndf_lasso_test = df_all[~pd.isna(df_all['Id'])] # get test data from df_All where the id is not nan\n\ndel df_lasso_test['Id']\ndel df_lasso_train['Id']\ndf_lass_test = df_lasso_train[df_lasso_train.columns]\n\nx_train_lasso, x_val_lasso, y_train_lasso, y_val_lasso = train_test_split(df_lasso_train, df[TARGET], test_size=0.2, random_state=random_seed)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:44.855553Z","iopub.execute_input":"2022-04-10T12:34:44.855818Z","iopub.status.idle":"2022-04-10T12:34:44.920865Z","shell.execute_reply.started":"2022-04-10T12:34:44.855787Z","shell.execute_reply":"2022-04-10T12:34:44.920194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lasso = Lasso(random_state=random_seed, max_iter=100000)\nalphas = np.logspace(-4, -0.01, 50)\nparam_grid = [{'alpha': alphas}]\n\n_, _, lasso_clf = cv_train_and_evaluate(lasso, param_grid, x_train_lasso, y_train_lasso, x_val_lasso, y_val_lasso, 'Lasso')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:34:44.923798Z","iopub.execute_input":"2022-04-10T12:34:44.924676Z","iopub.status.idle":"2022-04-10T12:35:01.934497Z","shell.execute_reply.started":"2022-04-10T12:34:44.924638Z","shell.execute_reply":"2022-04-10T12:35:01.933479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Coefficients that have non-zero values are removed from selected features. we use these features for lightgbm","metadata":{}},{"cell_type":"code","source":"coef = pd.Series(lasso_clf.best_estimator_.coef_, index = x_train_lasso.columns)\nSELECTED_FEATURES = coef[coef != 0]\nSELECTED_FEATURES = list(SELECTED_FEATURES.index)\nSELECTED_FEATURES = [a.split('_')[0] if 'has_zero_' not in a else a for a in SELECTED_FEATURES]\nSELECTED_FEATURES = list(set(SELECTED_FEATURES))\nprint(SELECTED_FEATURES)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:35:01.937152Z","iopub.execute_input":"2022-04-10T12:35:01.938948Z","iopub.status.idle":"2022-04-10T12:35:01.957745Z","shell.execute_reply.started":"2022-04-10T12:35:01.938891Z","shell.execute_reply":"2022-04-10T12:35:01.956651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(df[SELECTED_FEATURES], df[TARGET], test_size=0.2, random_state=random_seed)\n\nSELECTED_CATEGORICAL_FEATURES = [a for a in CATEGORICAL_FEATURES if a in SELECTED_FEATURES]","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:35:01.961675Z","iopub.execute_input":"2022-04-10T12:35:01.963219Z","iopub.status.idle":"2022-04-10T12:35:02.005666Z","shell.execute_reply.started":"2022-04-10T12:35:01.963144Z","shell.execute_reply":"2022-04-10T12:35:02.003737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_md = lgb.LGBMRegressor()\n\n# to prevent too many warnings being printed in the output I just use the one that was the best so far in my experiments\nparam_grid = {'learning_rate':[0.01], 'num_iterations': [30000], 'n_estimators': [40], 'num_leaves': [40],\n              'colsample_bytree':[0.4], 'subsample': [0.4], 'max_depth': [6]} \n\n\n_, _, model_clf = cv_train_and_evaluate(lgbm_md, param_grid, x_train, y_train, x_val, y_val, 'LGBM', fit_params={'categorical_feature':SELECTED_CATEGORICAL_FEATURES, \n                                                                                                                 'eval_set':[(x_val, y_val)], 'eval_metric':'l1', \n                                                                                                                 'callbacks':[early_stopping(500)],\n                                                                                                                 'verbose':-1})\nmodel_clf.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:35:02.007837Z","iopub.execute_input":"2022-04-10T12:35:02.008601Z","iopub.status.idle":"2022-04-10T12:35:15.676299Z","shell.execute_reply.started":"2022-04-10T12:35:02.008531Z","shell.execute_reply":"2022-04-10T12:35:15.675611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"markdown","source":"For better result the average value of two trained models on test data is submited.","metadata":{}},{"cell_type":"code","source":"test_predictions = 0.6 * lasso_clf.predict(df_lasso_test) + 0.4 * model_clf.predict(df_test[SELECTED_FEATURES])\ny_test = np.exp(test_predictions)\n\nmy_submission = pd.DataFrame({'Id': df_test.Id, 'SalePrice': y_test})\nmy_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T12:35:15.679597Z","iopub.execute_input":"2022-04-10T12:35:15.680272Z","iopub.status.idle":"2022-04-10T12:35:15.87458Z","shell.execute_reply.started":"2022-04-10T12:35:15.680235Z","shell.execute_reply":"2022-04-10T12:35:15.873722Z"},"trusted":true},"execution_count":null,"outputs":[]}]}