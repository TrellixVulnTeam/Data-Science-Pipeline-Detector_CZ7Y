{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Prices: Advanced Regression Techniques"},{"metadata":{},"cell_type":"markdown","source":"Hello everyone!\n\nThe key objective is to use feature engineering to improve performance for tree-based and non-tree models."},{"metadata":{},"cell_type":"markdown","source":"## Load packages and data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\npd.pandas.set_option('display.max_columns',None)\nimport numpy as np\nimport seaborn as sns\n\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\nX_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv', index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"correlation_train=X_train.corr()\nsns.set(font_scale=1.2)\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize = (20,20))\nax = sns.heatmap(correlation_train, annot=True,annot_kws={\"size\": 11},fmt='.1f', linewidths=.5, square=True, mask=mask)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Target log-transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = X_train.SalePrice.reset_index(drop=True)\nX_train.drop(['SalePrice'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.distplot(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.probplot(y, plot=plt)\nprint(f\"Skewness: {y.skew():.3f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.log1p(y)            \nplt.figure(figsize=(12,6))\nsns.distplot(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.probplot(y, plot=plt)\nprint(f\"Skewness: {y.skew():.3f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Detecting missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test = pd.concat([X_train, X_test], axis=0).reset_index(drop=True)\ntrain_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find Missing Ratio of Dataset\nmissing = (train_test.isnull().sum() / len(train_test)) * 100\nmissing = missing.drop(missing[missing == 0].index).sort_values(ascending=False)[:35]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 10))\nplt.xticks(rotation='90')\nsns.barplot(x=missing.index, y=missing)\nplt.xlabel('Features')\nplt.ylabel('%')\nplt.title('Percentage of missing values');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputing nominal categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test['MSSubClass'] = train_test['MSSubClass'].astype(str)\ntrain_test['MoSold'] = train_test['MoSold'].astype(str)\ntrain_test['YrSold'] = train_test['YrSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"none = ['Alley', 'PoolQC', 'MiscFeature', 'Fence', 'GarageType','MasVnrType']\nfor col in none:\n    train_test[col].replace(np.nan, 'None', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test['MSZoning'] = train_test.groupby('MSSubClass')['MSZoning'].transform(\n    lambda x: x.fillna(x.mode()[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freq_cols = [\n    'Electrical', 'Exterior1st', 'Exterior2nd',\n    'SaleType', 'Utilities'\n]\nfor col in freq_cols:\n    train_test[col].replace(np.nan, train_test[col].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding ordinal categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"qualcond = ['GarageQual', 'GarageCond', 'FireplaceQu', 'KitchenQual', 'HeatingQC', 'BsmtCond', 'BsmtQual', 'ExterCond', 'ExterQual']\nfor f in qualcond:\n    train_test[f] = train_test[f].replace({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0})\ntrain_test['BsmtExposure'] = train_test['BsmtExposure'].replace({'Gd':4, 'Av':3, 'Mn':2, 'No':1, 'NA':0})\ntrain_test['GarageFinish'] = train_test['GarageFinish'].replace({'Fin':3, 'RFn':2, 'Unf':1, 'NA':0})\nbasement = ['BsmtFinType1', 'BsmtFinType2']\nfor f in basement:\n    train_test[f] = train_test[f].replace({'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"functional = {'Typ': 3, 'Min1': 2.5, 'Min2': 2, 'Mod': 1.5, 'Maj1': 1, 'Maj2': 0.5, 'Sev': 0, 'Sal': 0}\ntrain_test['Functional'] = train_test['Functional'].replace(functional)\ntrain_test['CentralAir'] = train_test['CentralAir'].replace({'Y':1, 'N':0})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test.isnull().sum().sort_values(ascending=False)[:22]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dropping features with one value - Utilities, Street, PoolQC"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test = train_test.drop(['Utilities', 'Street', 'PoolQC', ], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers = [ 30, 462, 523, 588, 632, 1298, 1324]\ntrain_test = train_test.drop(train_test.index[outliers])\nlinear_train_test = train_test.copy()\ny = y.drop(y.index[outliers])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding nominal categorical features and Imputing missing values (Tree-based)"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = [cname for cname in train_test.columns if  train_test[cname].dtype == \"object\"]\ncat_cols\ntrain = train_test.iloc[:1453]\ntest = train_test.iloc[1453:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CatBoostEncoder replaces a categorical value with the average value of the target from the rows before it. It works well with XGBoost and LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from category_encoders import CatBoostEncoder\ncbe = CatBoostEncoder()\ntrain[cat_cols] = cbe.fit_transform(train[cat_cols], y)\ntest[cat_cols] = cbe.transform(test[cat_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test = pd.concat([train, test]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each sampleâ€™s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import KNNImputer\nimp = KNNImputer(n_neighbors=7, weights='distance', missing_values=np.nan)\nimp_train_test = imp.fit_transform(train_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test = pd.DataFrame(imp_train_test, columns=train_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing = ['GarageCars', 'BsmtFinSF1', 'GarageArea', 'BsmtUnfSF', 'KitchenQual',\n       'BsmtFinSF2', 'TotalBsmtSF', 'Functional', 'BsmtHalfBath',\n       'BsmtFullBath', 'MasVnrArea', 'BsmtFinType1', 'BsmtFinType2',\n       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'GarageQual', 'GarageFinish',\n       'GarageYrBlt', 'GarageCond', 'LotFrontage', 'FireplaceQu']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test[missing] = train_test[missing].apply(lambda x: np.round(x))\ntrain_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Generation"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test['YearsSinceBuilt'] = train_test['YrSold'].astype(int) - train_test['YearBuilt']\ntrain_test['YearsSinceRemod'] = train_test['YrSold'].astype(int) - train_test['YearRemodAdd']\ntrain_test['TotalSF'] = train_test['TotalBsmtSF'] + train_test['1stFlrSF'] + train_test['2ndFlrSF']\n\ntrain_test['Total_Bathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) +\n                               train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n\ntrain_test['TotalPorchArea'] = (train_test['OpenPorchSF'] + train_test['3SsnPorch'] +\n                              train_test['EnclosedPorch'] + train_test['ScreenPorch'] +\n                              train_test['WoodDeckSF'])\ntrain_test['TotalOccupiedArea'] = train_test['TotalSF'] + train_test['TotalPorchArea']\ntrain_test['OtherRooms'] = train_test['TotRmsAbvGrd'] - train_test['BedroomAbvGr'] - train_test['KitchenAbvGr']\ntrain_test['haspool'] = train_test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['has2ndfloor'] = train_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['hasgarage'] = train_test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['hasbsmt'] = train_test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['hasfireplace'] = train_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_test.shape)\ntrain = train_test.iloc[:1453]\ntest = train_test.iloc[1453:]\nprint(train.shape, test.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding nominal categorical features, Imputing missing values and Scaling (Non-Tree)"},{"metadata":{"trusted":true},"cell_type":"code","source":"ohe = pd.get_dummies(linear_train_test).reset_index(drop=True)\n\nimp = KNNImputer(n_neighbors=7, weights='distance', missing_values=np.nan)\nimp_linear_train_test = imp.fit_transform(ohe)\n\nlinear_train_test = pd.DataFrame(imp_linear_train_test, columns=ohe.columns)\n\nlinear_train_test[missing] = linear_train_test[missing].apply(lambda x: np.round(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_train_test['TotalSF'] = linear_train_test['TotalBsmtSF'] + linear_train_test['1stFlrSF'] + linear_train_test['2ndFlrSF']\n\nlinear_train_test['Total_Bathrooms'] = (linear_train_test['FullBath'] + (0.5 * linear_train_test['HalfBath']) +\n                               linear_train_test['BsmtFullBath'] + (0.5 * linear_train_test['BsmtHalfBath']))\n\nlinear_train_test['TotalPorchArea'] = (linear_train_test['OpenPorchSF'] + linear_train_test['3SsnPorch'] +\n                              linear_train_test['EnclosedPorch'] + linear_train_test['ScreenPorch'] +\n                              linear_train_test['WoodDeckSF'])\nlinear_train_test['TotalOccupiedArea'] = linear_train_test['TotalSF'] + linear_train_test['TotalPorchArea']\nlinear_train_test['OtherRooms'] = linear_train_test['TotRmsAbvGrd'] - linear_train_test['BedroomAbvGr'] - linear_train_test['KitchenAbvGr']\nlinear_train_test['haspool'] = linear_train_test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nlinear_train_test['has2ndfloor'] = linear_train_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nlinear_train_test['hasgarage'] = linear_train_test['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nlinear_train_test['hasbsmt'] = linear_train_test['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nlinear_train_test['hasfireplace'] = linear_train_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\nlinear_train_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nlin_train = linear_train_test.iloc[:1453]\nlin_test = linear_train_test.iloc[1453:]\n\n\nScaler = StandardScaler()\nscaled_train = Scaler.fit_transform(lin_train)\nscaled_test = Scaler.transform(lin_test)\n\nscaled_train = pd.DataFrame(scaled_train, columns=linear_train_test.columns)\nscaled_test = pd.DataFrame(scaled_test, columns=linear_train_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(scaled_train.shape, scaled_test.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection (Tree-based)"},{"metadata":{},"cell_type":"markdown","source":"We'll build a CatBoost model and find best features with SHAP Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor, Pool\nmodel = CatBoostRegressor(iterations=2500,\n                            learning_rate=0.03,\n                            depth=6,\n                            loss_function='RMSE',\n                            random_seed = 10,\n                            bootstrap_type='Bernoulli',\n                            subsample=0.66,\n                            rsm=0.7\n                         )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(train, y, verbose=False, plot=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nshap.initjs()\n\nshap_values = model.get_feature_importance(Pool(train, y), type='ShapValues')\n\nexpected_value = shap_values[0,-1]\nshap_values = shap_values[:,:-1]\n\nshap.force_plot(expected_value, shap_values[0,:], train.iloc[0,:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SHAP importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, train, max_display=88,  plot_type='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_sum = np.abs(shap_values).mean(axis=0)\nimportance_df = pd.DataFrame([train.columns.tolist(), shap_sum.tolist()]).T\nimportance_df.columns = ['column_name', 'shap_importance']\nimportance_df = importance_df.sort_values('shap_importance', ascending=False)\nimportance_df.tail(35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll drop features with less than **1.5e-3** importance(you can change this threshold)"},{"metadata":{"trusted":true},"cell_type":"code","source":"drop = importance_df[importance_df['shap_importance'] < 1.5e-3].iloc[:,0].tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_drop = train.drop(drop, axis=1)\ntest_drop = test.drop(drop, axis=1)\ntrain_drop","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Selection (Non-Tree-based)"},{"metadata":{},"cell_type":"markdown","source":"We'll use L1 Regularization with alpha 0.001"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\ndef selected_features_l1(X,y,alpha):\n    logistic = Lasso(random_state=20, alpha=alpha).fit(X,y)\n    model = SelectFromModel(logistic, prefit=True)\n    X_new = model.transform(X)\n    selected_features = pd.DataFrame(model.inverse_transform(X_new),\n                                    index=X.index,\n                                    columns=X.columns)\n    features = selected_features.columns[selected_features.var() != 0]\n    drop = selected_features.columns[selected_features.var() == 0]\n    return features, drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features, drop = selected_features_l1(scaled_train, y, 0.001)\nprint(drop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selected_train =  scaled_train.drop(drop, axis=1)\nselected_test = scaled_test.drop(drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Hyperparameter optimization"},{"metadata":{},"cell_type":"markdown","source":"We'll use Tree-structured Parzen Estimater (TPE), which is a form of Bayesian Optimization.\n\n![](https://miro.medium.com/max/700/1*tYWqO5BwNDVaM3kP3w1IAg.png)\n\nWe'll define hyperparameters and ranges with `trial`, perform 10-fold CV, and set direction with number of trials for optimization `n_trial` to 200. We'll use tree_method `gpu_hist` for faster computation."},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nimport optuna\n\n\ndef objective(trial):\n    dtrain = xgb.DMatrix(train_drop, label=y)\n\n    param = {\n        'seed': 20,\n        'tree_method': 'gpu_hist',\n        'max_depth': trial.suggest_int(\"max_depth\", 3, 8),\n        'eta' : trial.suggest_uniform(\"eta\", 1e-3, 5e-2),\n        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n        \"gamma\": trial.suggest_uniform(\"gamma\", 1e-8, 1e-4),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 1.0),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 1.0),        \n    }\n    if param['grow_policy']==\"lossguide\":\n        param['max_leaves'] =  trial.suggest_int('max_leaves',2, 32)\n    bst = xgb.cv(param, dtrain, num_boost_round=5000, nfold=10, early_stopping_rounds=50,  metrics='rmse', seed=20)\n    score = bst['test-rmse-mean'].tail(1).values[0]\n    return score\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=200)\nprint(study.best_trial)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After optimization, our best result will look like this:\n\nFinished trial#110 with value: **0.1036024** with parameters:\n{'max_depth': 3, 'eta': 0.01423912926193527, 'grow_policy': 'lossguide', 'gamma': 2.804584764149306e-05, \n'colsample_bytree': 0.2403604834036041, 'subsample': 0.38141269740154965, 'max_leaves': 6}.\n\nWe can see our hyperparameter's history"},{"metadata":{"trusted":true},"cell_type":"code","source":"from optuna.visualization import plot_parallel_coordinate\n\nplot_parallel_coordinate(study)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot hyperparameter importance"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import optuna\n\nimportances = optuna.importance.get_param_importances(study)\nimportance_values = list(importances.values())\nparam_names = list(importances.keys())\nparams = pd.DataFrame([param_names, importance_values]).T\nparams.columns = ['param_name', 'importance']\nparams = params.sort_values('importance', ascending=False)\nsns.catplot(x='param_name', y='importance', data=params, kind='bar')\nplt.xticks(rotation='45');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at the charts above, we can conclude that we can change boundaries for hyperparameters.\n\nFor example: `eta: 7e-3, 2.2e-2`, `max_leaves: 2,16`, `gamma: 1e-6, 7e-5`, `colsample_bytree: 0.1, 0.4`, `subsample: 0.3, 0.6`\n\nAfter another optimization, our best result improved to **0.1031473**"},{"metadata":{},"cell_type":"markdown","source":"It was an example of one model optimization, and we can apply this steps to any model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}