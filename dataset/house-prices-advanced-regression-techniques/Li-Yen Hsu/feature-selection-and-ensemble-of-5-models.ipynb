{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"celltoolbar":"Raw Cell Format","language_info":{"name":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.6.3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1,"cells":[{"metadata":{"_uuid":"2a19a4f60092fa997678247d03ab0c14c6ae7700","_cell_guid":"a32f7a2f-56be-4c09-9f05-6643073a08eb","_execution_state":"idle"},"cell_type":"markdown","source":"# Feature Selection and Ensemble of 5 Models\n\n<br>\n** leaderboard score = 0.11920 (September 30, 2017)\n<br>\nComments and suggestions are welcome!**\n<br>\n<br>\n**I used XGBoost Regressor along with [RFECV](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV) to rank the importance of the features and eliminate the redundant features. I built five base regressors, XGBoost, Lasso, Elastic Net, Kernel Ridge, and Neural Network. Then I used Ridge, a linear model with L2 regularization, to weight and combine the predictions from the five base models.**\n\n**I've learned a lot from some other kernals and borrow some of their ideas:**\n<br>\n[A study on Regression applied to the Ames dataset](https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset)\n<br>\n[XGBoost + Lasso](https://www.kaggle.com/humananalog/xgboost-lasso/code)\n<br>\n[Ensemble of 4 models with CV (LB: 0.11489)](https://www.kaggle.com/opanichev/ensemble-of-4-models-with-cv-lb-0-11489/code)\n<br>\n[Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)"},{"metadata":{"_uuid":"a5125b5a75edce78ad11ccbe6c74033697f44bb5","_cell_guid":"9884a06b-31aa-4958-897b-52faa8249254","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.stats import boxcox\nfrom scipy.stats import skew\nfrom scipy.stats import randint\nfrom scipy.stats import uniform\n\nfrom xgboost import XGBRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso, ElasticNet, Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import make_scorer \nfrom sklearn.base import BaseEstimator, RegressorMixin\n\n# neural networks\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import regularizers\n\n# ignore Deprecation Warning\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n# load the data\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\ndf = df_train.append(df_test , ignore_index = True)\n\n# basic inspection\ndf_train.shape, df_test.shape, df_train.columns.values"},{"metadata":{"_uuid":"ae31d01d9e415e338f971ef1a1c4be3f3681efc8","_cell_guid":"0f7a8ed2-da2d-413d-9d11-6cfae68bacf7","_execution_state":"idle"},"cell_type":"markdown","source":"## Data Preprocessing ##"},{"metadata":{"_uuid":"5f6b8a88c2590f8524700cc7ed170ae0d6de8f2f","_cell_guid":"b79b5e77-386a-406e-9504-58cb66b882e2","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# divide the data into numerical (\"quan\") and categorical (\"qual\") features\nquan = list( df_test.loc[:,df_test.dtypes != 'object'].drop('Id',axis=1).columns.values )\nqual = list( df_test.loc[:,df_test.dtypes == 'object'].columns.values )"},{"metadata":{"_uuid":"ecee11cbb2f00edad69409055d666b62fe95f474","_cell_guid":"67b7753c-6959-44cf-8852-d8c3de5e591e","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Find out the missing values for quantitative and categorical features\n\nhasNAN = df[quan].isnull().sum()\nhasNAN = hasNAN[hasNAN > 0]\nhasNAN = hasNAN.sort_values(ascending=False)\nprint(hasNAN)\n\nprint('**'*40)\n\nhasNAN = df[qual].isnull().sum()\nhasNAN = hasNAN[hasNAN > 0]\nhasNAN = hasNAN.sort_values(ascending=False)\nprint(hasNAN)"},{"metadata":{"_uuid":"89365d112534900e569d0662af574da454a48dd3","_cell_guid":"664dd5ec-6c1c-4c7b-ace9-89100dd5bd5b","_execution_state":"idle"},"cell_type":"markdown","source":"Since the final result is evaluated using the Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price, it would be a good idea to log-transform the SalePrice. At the end of the code, I will transform my final prediction back to real house price values for the csv file. "},{"metadata":{"_uuid":"0d7cee2b729767ab7610a7766e55b9159640e70d","_cell_guid":"a0a2443d-db25-4d94-869c-1989e6c1fc55","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"df.SalePrice = np.log(df.SalePrice)"},{"metadata":{"_uuid":"5a711614d7ff87a5b42bf1b674f95d5b3feafcab","_cell_guid":"30b3e1e2-149a-4aa3-8a72-d3069e9727a2","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Filling missing values for numerical features\n# Most of the NAN should mean that the corresponding facillity/structure doesn't \n# exist, so I use zero for most cases\ndf.LotFrontage.fillna(df.LotFrontage.median(), inplace=True)\n\n# NAN should mean no garage. I temporarily use yr =0 here. Will come back to this later. \ndf.GarageYrBlt.fillna(0, inplace=True)\n\ndf.MasVnrArea.fillna(0, inplace=True)    \ndf.BsmtHalfBath.fillna(0, inplace=True)\ndf.BsmtFullBath.fillna(0, inplace=True)\ndf.GarageArea.fillna(0, inplace=True)\ndf.GarageCars.fillna(0, inplace=True)    \ndf.TotalBsmtSF.fillna(0, inplace=True)   \ndf.BsmtUnfSF.fillna(0, inplace=True)     \ndf.BsmtFinSF2.fillna(0, inplace=True)    \ndf.BsmtFinSF1.fillna(0, inplace=True)    \n\n# categorical features\ndf.PoolQC.fillna('NA', inplace=True)\ndf.MiscFeature.fillna('NA', inplace=True)    \ndf.Alley.fillna('NA', inplace=True)          \ndf.Fence.fillna('NA', inplace=True)         \ndf.FireplaceQu.fillna('NA', inplace=True)    \ndf.GarageCond.fillna('NA', inplace=True)    \ndf.GarageQual.fillna('NA', inplace=True)     \ndf.GarageFinish.fillna('NA', inplace=True)   \ndf.GarageType.fillna('NA', inplace=True)     \ndf.BsmtExposure.fillna('NA', inplace=True)     \ndf.BsmtCond.fillna('NA', inplace=True)        \ndf.BsmtQual.fillna('NA', inplace=True)        \ndf.BsmtFinType2.fillna('NA', inplace=True)     \ndf.BsmtFinType1.fillna('NA', inplace=True)     \ndf.MasVnrType.fillna('None', inplace=True)   \ndf.Exterior2nd.fillna('None', inplace=True) \n\n# These are general properties that all houses should have, so NAN probably \n# just means the value was not recorded. I therefore use \"mode\", the most \n# common value to fill in\ndf.Functional.fillna(df.Functional.mode()[0], inplace=True)       \ndf.Utilities.fillna(df.Utilities.mode()[0], inplace=True)          \ndf.Exterior1st.fillna(df.Exterior1st.mode()[0], inplace=True)        \ndf.SaleType.fillna(df.SaleType.mode()[0], inplace=True)                \ndf.KitchenQual.fillna(df.KitchenQual.mode()[0], inplace=True)        \ndf.Electrical.fillna(df.Electrical.mode()[0], inplace=True)    \n\n# MSZoning should highly correlate with the location, so I use the mode values of individual \n# Neighborhoods\nfor i in df.Neighborhood.unique():\n    if df.MSZoning[df.Neighborhood == i].isnull().sum() > 0:\n        df.loc[df.Neighborhood == i,'MSZoning'] = \\\n        df.loc[df.Neighborhood == i,'MSZoning'].fillna(df.loc[df.Neighborhood == i,'MSZoning'].mode()[0]) \n\n# These categorical features are \"rank\", so they can be transformed to \n# numerical features\ndf.Alley = df.Alley.map({'NA':0, 'Grvl':1, 'Pave':2})\ndf.BsmtCond =  df.BsmtCond.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.BsmtExposure = df.BsmtExposure.map({'NA':0, 'No':1, 'Mn':2, 'Av':3, 'Gd':4})\ndf['BsmtFinType1'] = df['BsmtFinType1'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})\ndf['BsmtFinType2'] = df['BsmtFinType2'].map({'NA':0, 'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6})\ndf.BsmtQual = df.BsmtQual.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.ExterCond = df.ExterCond.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.ExterQual = df.ExterQual.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.FireplaceQu = df.FireplaceQu.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.Functional = df.Functional.map({'Sal':1, 'Sev':2, 'Maj2':3, 'Maj1':4, 'Mod':5, 'Min2':6, 'Min1':7, 'Typ':8})\ndf.GarageCond = df.GarageCond.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.GarageQual = df.GarageQual.map({'NA':0, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.HeatingQC = df.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.KitchenQual = df.KitchenQual.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\ndf.LandSlope = df.LandSlope.map({'Sev':1, 'Mod':2, 'Gtl':3}) \ndf.PavedDrive = df.PavedDrive.map({'N':1, 'P':2, 'Y':3})\ndf.PoolQC = df.PoolQC.map({'NA':0, 'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\ndf.Street = df.Street.map({'Grvl':1, 'Pave':2})\ndf.Utilities = df.Utilities.map({'ELO':1, 'NoSeWa':2, 'NoSewr':3, 'AllPub':4})\n\n# Update my lists of numerical and categorical features\nnewquan = ['Alley','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','BsmtQual',\n           'ExterCond','ExterQual','FireplaceQu','Functional','GarageCond',\n           'GarageQual','HeatingQC','KitchenQual','LandSlope','PavedDrive','PoolQC',\n           'Street','Utilities']\nquan = quan + newquan \nfor i in newquan: qual.remove(i)\n\n\n# This is actually a categorical feature...\ndf.MSSubClass = df.MSSubClass.map({20:'class1', 30:'class2', 40:'class3', 45:'class4',\n                                   50:'class5', 60:'class6', 70:'class7', 75:'class8',\n                                   80:'class9', 85:'class10', 90:'class11', 120:'class12',\n                                   150:'class13', 160:'class14', 180:'class15', 190:'class16'})\n\n# Keeping \"YrSold\" is enough.\ndf=df.drop('MoSold',axis=1)\n\n# Update my lists of numerical and categorical features\nquan.remove('MoSold')\nquan.remove('MSSubClass')\nqual.append('MSSubClass')"},{"metadata":{"_uuid":"c125428e3b8911cb51e83905c932029e8c4e419c","_cell_guid":"0e1212e8-adb2-44cf-b77e-3c954a044245","_execution_state":"idle"},"cell_type":"markdown","source":"For all the time variables, what matters should be the time duration. So I create three features: the age of the house (Age), the time duration since the remodel date (AgeRemod), and the age of the Garage (AgeGarage)"},{"metadata":{"_uuid":"674b1752343a261deba52e512e778c23defb5c7a","_cell_guid":"8a2b3401-da4d-4288-add4-9965d719c484","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"df['Age'] = df.YrSold - df.YearBuilt\ndf['AgeRemod'] = df.YrSold - df.YearRemodAdd\ndf['AgeGarage'] = df.YrSold - df.GarageYrBlt\n\n# For the houses without a Garage, I filled the NANs with zeros, which makes AgeGarage ~ 2000\n# Here I replace their AgeGarage with the maximum value among the houses with Garages\nmax_AgeGarage = np.max(df.AgeGarage[df.AgeGarage < 1000])\ndf['AgeGarage'] = df['AgeGarage'].map(lambda x: max_AgeGarage if x > 1000 else x)\n\n# Some of the values are negative because the work was done after the house \n# was sold. In these cases, I change them to zero to avoid negative ages.\ndf.Age = df.Age.map(lambda x: 0 if x < 0 else x)\ndf.AgeRemod = df.AgeRemod.map(lambda x: 0 if x < 0 else x)\ndf.AgeGarage = df.AgeGarage.map(lambda x: 0 if x < 0 else x)\n\n# drop the original time variables \ndf=df.drop(['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt'],axis=1)\n\n# update my list of numerical feature\nquan.remove('YrSold')\nquan.remove('YearBuilt')\nquan.remove('YearRemodAdd')\nquan.remove('GarageYrBlt')\nquan = quan + ['Age','AgeRemod','AgeGarage']"},{"metadata":{"_uuid":"e58e64274cf4f8473292e14c6ee7b5fc473bf444","_cell_guid":"921f1b9b-daa1-448f-96d8-17491586e342","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# visualize the distribution of each numerical feature\ntemp = pd.melt(df.drop('SalePrice',axis=1), value_vars=quan)\ngrid = sns.FacetGrid(temp, col=\"variable\",  col_wrap=6 , size=3.0, \n                     aspect=0.8,sharex=False, sharey=False)\ngrid.map(sns.distplot, \"value\")\nplt.show()\n\n# scatter plots\ntemp = pd.melt(df, id_vars=['SalePrice'],value_vars=quan)\ngrid = sns.FacetGrid(temp, col=\"variable\",  col_wrap=4 , size=3.0, \n                     aspect=1.2,sharex=False, sharey=False)\ngrid.map(plt.scatter, \"value\",'SalePrice',s=1.5)\nplt.show()"},{"metadata":{"_uuid":"dd7c9863257bdd607a4e88c9c0fc41f9996b09c7","_cell_guid":"679f9f45-d31c-4e96-a7d0-99e8dc8bc974","_execution_state":"idle"},"cell_type":"markdown","source":"Being inspired by other Kernels and this blog http://shahramabyari.com/, it is necessary to transform the numerical features that are skewed. This is because lots of regression models building assume that the features are distributed normally and have a symmetrical shape."},{"metadata":{"_uuid":"999cf6ae73741a5d10faeec53077a23a43c57c80","_cell_guid":"ed991c28-f031-44e5-808c-8cf6182b7758","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# print the skewness of each numerical feature\nfor i in quan:\n    print(i+':', round(skew(df[i]),2) ) "},{"metadata":{"_uuid":"2c08791bdf87891a68c8382a064d8ae1a1fdb298","_cell_guid":"da104dcb-602a-4df3-92ef-5e1f648f1761","_execution_state":"idle"},"cell_type":"markdown","source":"I've tried various transformations (log, boxcox, sqrt...etc) and found that log-transform somehow works better for me."},{"metadata":{"_uuid":"95b3ecd4999a72c46fe4d071eb1c1f3e0125d1ad","_cell_guid":"ebe0e5ef-641a-4e1b-9df4-0a63dd3c7ffd","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# transform those with skewness > 0.5\nskewed_features = np.array(quan)[np.abs(skew(df[quan])) > 0.5]\ndf[skewed_features] = np.log1p(df[skewed_features])"},{"metadata":{"_uuid":"03d7a07f105ae3cbb7376449ac7055210133ceb6","_cell_guid":"4578f7c4-10d5-49f4-85eb-2dd824dbdbcc","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"## visualize the distribution again\ntemp = pd.melt(df, value_vars=quan)\ngrid = sns.FacetGrid(temp, col=\"variable\",  col_wrap=6 , size=3.0, \n                     aspect=0.8,sharex=False, sharey=False)\ngrid.map(sns.distplot, \"value\")\nplt.show()\n\n# scatter plots\ntemp = pd.melt(df, id_vars=['SalePrice'],value_vars=quan)\ngrid = sns.FacetGrid(temp, col=\"variable\",  col_wrap=4 , size=3.0, \n                     aspect=1.2,sharex=False, sharey=False)\ngrid.map(plt.scatter, \"value\",'SalePrice',s=1.5)\nplt.show()"},{"metadata":{"_uuid":"af91cb4d31459c35a0de41af3aa65ec696decf4a","_cell_guid":"0720b065-6eba-4192-9983-a3a7daf1cdc4","_execution_state":"idle"},"cell_type":"markdown","source":"For the categorical features, I will transform them to dummy variables, but I'll drop one column from each of them to avoid \"dummy variable trap\" (http://www.algosome.com/articles/dummy-variable-trap-regression.html). "},{"metadata":{"_uuid":"197bd8a1efe3adbe0266e2946eb6ee53c992d1e2","_cell_guid":"ddee92ef-c307-4dc1-9a2e-a3eeb4f3f081","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# create of list of dummy variables that I will drop, which will be the last\n# column generated from each categorical feature\ndummy_drop = []\nfor i in qual:\n    dummy_drop += [ i+'_'+str(df[i].unique()[-1]) ]\n\n# create dummy variables\ndf = pd.get_dummies(df,columns=qual) \n# drop the last column generated from each categorical feature\ndf = df.drop(dummy_drop,axis=1)"},{"metadata":{"_uuid":"68febf88e5f2b73141c61b61cd1d857d31f9f6b1","_cell_guid":"efd18aa7-faa6-4b73-92bc-d094a45d04e7","_execution_state":"idle"},"cell_type":"markdown","source":"Now I can split the data into training set and test set. And then I'll perform Standardization on the numerical features (those that are not dummy variables). Being inspired by [Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard), here I use RobustScaler instead, which is more robust to outliers in the data."},{"metadata":{"_uuid":"35e965e25ef728b3dd6935c808e56f32e25b4558","_cell_guid":"fadd76d8-86ed-408d-bf6c-5d82c821f0e4","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"X_train  = df[:1460].drop(['SalePrice','Id'], axis=1)\ny_train  = df[:1460]['SalePrice']\nX_test  = df[1460:].drop(['SalePrice','Id'], axis=1)\n\n# fit the training set only, then transform both the training and test sets\nscaler = RobustScaler()\nX_train[quan]= scaler.fit_transform(X_train[quan])\nX_test[quan]= scaler.transform(X_test[quan])\n\nX_train.shape # now we have 221 features!"},{"metadata":{"_uuid":"95773b0d6417d88115395adb24b82b6f6aea38b5","_cell_guid":"02836ba4-0b94-4e7e-8a2b-fae26bb8a442","_execution_state":"idle"},"cell_type":"markdown","source":"## Feature Selection ##"},{"metadata":{"_uuid":"e695a3f03d6d92c6a51f7323e841fc05018abed1","_cell_guid":"b7bef2a2-afcc-4241-8fb5-146bb8fc0810","_execution_state":"idle"},"cell_type":"markdown","source":"Now there are 221 features due to a large amount of the dummy variable. Overfitting can easily occur when there are redundant features. Therefore I use XGBoost regressor to generate the rank of \"feature importance\""},{"metadata":{"_uuid":"683766f4a43d19d296242ca771146530c549aa60","_cell_guid":"f85d957b-1af0-4c32-908a-761ca3273b91","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"xgb = XGBRegressor()\nxgb.fit(X_train, y_train)\nimp = pd.DataFrame(xgb.feature_importances_ ,columns = ['Importance'],index = X_train.columns)\nimp = imp.sort_values(['Importance'], ascending = False)\n\nprint(imp)"},{"metadata":{"_uuid":"d5c94615e10f8183f5aef034ee0bd01460bd56b5","_cell_guid":"8e53441b-24f2-45f8-9a3d-3ffbe9dc5940"},"cell_type":"markdown","source":"Now we can use [RFECV](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV) to eliminate the redundant features."},{"metadata":{"_uuid":"4efabcee27a03157ead6b55d23fedec8caf8f34e","_cell_guid":"1692cce0-39e4-4889-8a70-49fcd8a2f1b5","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# Define a function to calculate RMSE\ndef rmse(y_true, y_pred):\n    return np.sqrt(np.mean((y_true-y_pred)**2))\n\n# Define a function to calculate negative RMSE (as a score)\ndef nrmse(y_true, y_pred):\n    return -1.0*rmse(y_true, y_pred)\n\nneg_rmse = make_scorer(nrmse)\n\nestimator = XGBRegressor()\nselector = RFECV(estimator, cv = 3, n_jobs = -1, scoring = neg_rmse)\nselector = selector.fit(X_train, y_train)\n\nprint(\"The number of selected features is: {}\".format(selector.n_features_))\n\nfeatures_kept = X_train.columns.values[selector.support_] \n\nX_train = selector.transform(X_train)  \nX_test = selector.transform(X_test)\n\n# transform it to a numpy array so later we can feed it to a neural network\ny_train = y_train.values "},{"metadata":{"_uuid":"574fdd318fa5378fec83068978d81e5d6439f978","_cell_guid":"53e7a5dd-2411-4c8a-83a8-49249a5d42d1","_execution_state":"idle"},"cell_type":"markdown","source":"The following are what I did originally to select features. I ran a loop and use XGBoost regressor with cross validation to see the improvement of Root-Mean-Squared-Error as a function of the number of features included. I got 87 features using this method (which is essentially the RFECV algorithm)."},{"metadata":{"_uuid":"70a3c07e05c549c6e99af0668d01f621d91fb1cb","_kg_hide-output":true,"_cell_guid":"02ea3b48-0cb6-43dd-b5f1-2c95d528c069","_execution_state":"idle","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"\"\"\"\nxgb = XGBRegressor()\nscoring_fnc = make_scorer(rmse)\n\nbest_rmse = 100 # initialize the best rmse, which will be updated in the loop\nn_feat = 0  # initialize the number of features we choose, which will be updated in the loop\n\nrmse_cv = [] # for recording RMSE\n\n# shuffle the data first\nfrom sklearn.utils import shuffle\nX_shuffled, y_shuffled = shuffle(X_train, y_train, random_state=0) \n\n# start from the top 10 features, then add more less important ones\nfor i in range(10,len(imp)+1):\n\n    keep = imp.iloc[0:i].index.values\n    X_temp = X_shuffled[keep] \n\n    scores = cross_val_score(xgb, X_temp, y_shuffled, scoring=scoring_fnc)\n    rms = scores.mean() # mean rmse of the three k-folds\n    rmse_cv += [rms]\n\n    # include more features only if RMES improves by more than 0.01% \n    if (rms - best_rmse)/best_rmse < -1e-4:\n        best_rmse = rms\n        n_feat = len(keep)\n        feat = keep\n\n# plot RMES v.s. number of features\nfig = plt.figure()\nplt.plot(range(n_min,len(imp)+1),rmse_cv)\nplt.xlabel('# of Features')\nplt.ylabel('RMSE')\nplt.show(block=False)\n        \n# final number of features we will use = 87\nprint(n_feat) \nX_train = X_train[feat]        \nX_test = X_test[feat]     \n\n\"\"\""},{"metadata":{"_uuid":"b178b3e88bde80ea59c51b1a7f2d32d421c4ca90","_cell_guid":"670e46f5-36b0-4962-a578-aabfb0338eba","_execution_state":"idle"},"cell_type":"markdown","source":"## Modeling and Prediction ##"},{"metadata":{"_uuid":"706642ed04d75f11a6d4aec3443e0f0398b7af89","_cell_guid":"488f0ca8-709c-497c-b899-be67af0c0cf2","_execution_state":"idle"},"cell_type":"markdown","source":"I will now build the five base models. The hyperparameters of these regressors were tuned using GridSearchCV or RandomizedSearchCV, which I skip here. However, these are definitely not the best values. More trial and error should improve the performance of individual models as well as the final stacked model."},{"metadata":{"_uuid":"4e3096d5c1515dd3b115f1b6b2b01abdac87fb2a","_cell_guid":"2faea44a-2b92-4e83-b1b5-7497bf2d7c37","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"# XGBoost: LB score = 0.12431\nxgb = XGBRegressor(n_estimators=500, learning_rate=0.05, subsample=0.5, colsample_bytree=0.5, \n                   max_depth=3, gamma=0, reg_alpha=0, reg_lambda=2, min_child_weight=1)\n\n# Lasso: LB score = 0.12568\nlas = Lasso(alpha=0.00049, max_iter=50000) \n\n# Elastic Net: LB score = 0.12651\nelast = ElasticNet(alpha=0.0003, max_iter=50000, l1_ratio=0.83) \n\n# Kernel Ridge: LB score = 0.12570\nridge = KernelRidge(alpha=0.15, coef0=3.7, degree=2, kernel='polynomial')\n\n# Gradient Boosting: LB score > 0.13 --> decided not to use it\n# boost = GradientBoostingRegressor(n_estimators=500, learning_rate=0.042, subsample=0.5, \n#       random_state=0, min_samples_split=4, max_depth=4)\n\n# Neural Network: LB score = 0.12064\nnn = Sequential()\n\n# layers\nnn.add(Dense(units = 40, kernel_initializer = 'uniform', activation = 'relu',\n             input_dim = X_train.shape[1], kernel_regularizer=regularizers.l2(0.003)))\nnn.add(Dense(units = 20, kernel_initializer = 'uniform', activation = 'relu',\n             kernel_regularizer=regularizers.l2(0.003)))\nnn.add(Dense(units = 20, kernel_initializer = 'uniform', activation = 'relu',\n             kernel_regularizer=regularizers.l2(0.003)))\nnn.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'relu',\n             kernel_regularizer=regularizers.l2(0.003)))\n\n# Compile the NN\nnn.compile(loss='mean_squared_error', optimizer='sgd')"},{"metadata":{"_uuid":"4dd679e92470754a56cfa7dfa3f6a7b32edd50a6","_cell_guid":"cba62f40-f0ae-4916-b3a0-fc93fcf542db"},"cell_type":"markdown","source":"The simplest way to stack the five base models is to average their predictions. This method essentially gives each base model the same wight (0.2 for each) and then combine them. Here I use a linear model with L2 regularization (Ridge) to find the optimal linear combination of the five base models. In doing so, the predictions from the five base models are fed to Ridge as five new features. "},{"metadata":{"_uuid":"a01d780bf8aa49269b84b2a6259187c98b3dc18d","_cell_guid":"69739afa-620d-43d0-8422-792f7fefe7ae","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"class Ensemble(BaseEstimator, RegressorMixin):\n    def __init__(self, regressors=None):\n        self.regressors = regressors\n        \n    def level0_to_level1(self, X):\n        self.predictions_ = []\n\n        for regressor in self.regressors:\n            self.predictions_.append(regressor.predict(X).reshape(X.shape[0],1))\n\n        return np.concatenate(self.predictions_, axis=1)\n    \n    def fit(self, X, y):\n        for regressor in self.regressors:\n            if regressor != nn:\n                regressor.fit(X, y)\n            else: regressor.fit(X, y, batch_size=64, epochs=1000, verbose=0) # Neural Network\n            \n        self.new_features = self.level0_to_level1(X)\n        \n        # using a large L2 regularization to prevent the ensemble from biasing toward \n        # one particular base model\n        self.combine = Ridge(alpha=10, max_iter=50000)   \n        self.combine.fit(self.new_features, y)\n\n        self.coef_ = self.combine.coef_\n\n    def predict(self, X):\n        self.new_features = self.level0_to_level1(X)\n            \n        return self.combine.predict(self.new_features).reshape(X.shape[0])\n"},{"metadata":{"_uuid":"8092bba2610c482b88ddc781d5dfe2321c47da61","_cell_guid":"ac4b73f3-6859-4f64-9d8e-753374b1abba","collapsed":true},"cell_type":"code","execution_count":null,"outputs":[],"source":"model = Ensemble(regressors=[xgb, las, elast, ridge, nn])\nmodel.fit(X_train, y_train)\ny_pred = np.exp(model.predict(X_test))\n\nprint(\"\\nThe weights of the five base models are: {}\".format(model.coef_))\n\noutput = pd.DataFrame({'Id': df_test['Id'], 'SalePrice': y_pred})\noutput.to_csv('prediction-ensemble.csv', index=False)"},{"metadata":{"_uuid":"dc3296c7e0bc3bd58dbec77c9bb33c820d501e25","_cell_guid":"ac1d4c6b-8e59-47fb-b5f7-6c1e00151725"},"cell_type":"markdown","source":"More work needs to be done to improve my feature engineering process and the performance of individual base models."}]}