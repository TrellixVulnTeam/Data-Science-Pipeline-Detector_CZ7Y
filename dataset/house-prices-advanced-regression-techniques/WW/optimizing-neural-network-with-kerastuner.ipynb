{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nHi fellow Kagglers!\n\nHere is an attempt to perform a clear and simple implementation of Neural network using Tensorflow 2.0 to solve House Prices Regression algorithm.\nI have then chosen the optimum configuration of the neural network using KerasTuner module.\nHope to make it helpful for those new to Deep Learning!<br>\n<br>\nHope you enjoy! Let's dive in!\n<br>\n<img src=\"https://media.giphy.com/media/dWy2WwcB3wvX8QA1Iu/giphy.gif\">\n","metadata":{}},{"cell_type":"code","source":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport logging\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom time import time\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\npd.options.mode.chained_assignment= None #avoid unnecessary warnings","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-22T20:45:26.368598Z","iopub.execute_input":"2022-02-22T20:45:26.36907Z","iopub.status.idle":"2022-02-22T20:45:26.378281Z","shell.execute_reply.started":"2022-02-22T20:45:26.369025Z","shell.execute_reply":"2022-02-22T20:45:26.377212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logger = logging.getLogger()\nlogger.setLevel(logging.INFO)\nlogging.info(\"test\")\n\n# To visualize all columns in the dataframe\npd.pandas.set_option('display.max_columns', None)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-22T20:45:26.380525Z","iopub.execute_input":"2022-02-22T20:45:26.385726Z","iopub.status.idle":"2022-02-22T20:45:26.400605Z","shell.execute_reply.started":"2022-02-22T20:45:26.385685Z","shell.execute_reply":"2022-02-22T20:45:26.399599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Importing data\n\ndf= pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\nprint(df.shape)\ndf.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-02-22T20:45:26.404672Z","iopub.execute_input":"2022-02-22T20:45:26.408204Z","iopub.status.idle":"2022-02-22T20:45:26.64289Z","shell.execute_reply.started":"2022-02-22T20:45:26.408161Z","shell.execute_reply":"2022-02-22T20:45:26.641573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# def reduce_mem_usage(df, verbose=True):\n#     numerics = ['int16','int32','int64','float16','float32','float64']\n#     start_mem = df.memory_usage().sum() / (1024**2)\n    \n#     for col in df.columns:\n#         col_type = df[col].dtypes\n#         if col_type in numerics:\n#             c_min = df[col].min()\n#             c_max = df[col].max()\n#             if str(col_type)[:3] == 'int':\n#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n#                     df[col] = df[col].astype(np.int8)\n#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n#                     df[col] = df[col].astype(np.int16)\n#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n#                     df[col] = df[col].astype(np.int32)\n#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n#                     df[col] = df[col].astype(np.int64)  \n#             else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n#                 elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n#                     df[col] = df[col].astype(np.float32)\n#                 else:\n#                     df[col] = df[col].astype(np.float64)    \n#     end_mem = df.memory_usage().sum() / 1024**2\n#     if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n#     return df\n\n# df= reduce_mem_usage(df)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-22T20:45:26.652441Z","iopub.execute_input":"2022-02-22T20:45:26.654706Z","iopub.status.idle":"2022-02-22T20:45:26.665371Z","shell.execute_reply.started":"2022-02-22T20:45:26.654664Z","shell.execute_reply":"2022-02-22T20:45:26.664484Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Excluding columns which have majority as null\ndf= df.loc[:, df.isnull().sum()/len(df)<0.80]\n\nx= df.iloc[:, 1:-1] # Dropping 'Id' and the Y feature\ny= df.iloc[:,-1]\n\ntrain_cols= x.columns\nprint(df.shape, x.shape, y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:26.671375Z","iopub.execute_input":"2022-02-22T20:45:26.674702Z","iopub.status.idle":"2022-02-22T20:45:26.733539Z","shell.execute_reply.started":"2022-02-22T20:45:26.674659Z","shell.execute_reply":"2022-02-22T20:45:26.732737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Looking at the Overall statistics of variables and correlation among all variables\ntrain_stats= x.describe().transpose()\ntrain_stats","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:26.734798Z","iopub.execute_input":"2022-02-22T20:45:26.736519Z","iopub.status.idle":"2022-02-22T20:45:27.009263Z","shell.execute_reply.started":"2022-02-22T20:45:26.736479Z","shell.execute_reply":"2022-02-22T20:45:27.008469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation matrix\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\n# sns.heatmap(corrmat, vmax=.8, square=True)\n\n#saleprice correlation matrix\nk=10\ncols= corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm= np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm= sns.heatmap(cm, cbar=True, annot=True, square=True,\n                fmt='.2f', annot_kws={'size':10},\n                yticklabels=cols.values,\n                xticklabels=cols.values)\nplt.show()\n# Conclusion: OverallQual is highly correlated with the target variable.","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:27.013084Z","iopub.execute_input":"2022-02-22T20:45:27.015178Z","iopub.status.idle":"2022-02-22T20:45:27.677388Z","shell.execute_reply.started":"2022-02-22T20:45:27.01514Z","shell.execute_reply":"2022-02-22T20:45:27.676552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. Division of different column type for different preprocessing and division between the train and test datasets","metadata":{}},{"cell_type":"code","source":"ordinal_cols= list(x.columns[x.columns.str.contains('Yr|Year')])\nprint('ordinal/temporal columns:\\n',ordinal_cols)\nnominal_cols= list(set(x.select_dtypes(include=['object']).columns)- set(ordinal_cols))\nprint('nominal columns:\\n', nominal_cols)\nnumeric_cols= list(set(x.select_dtypes(exclude=['object']).columns)- set(ordinal_cols))\nprint('numeric columns:\\n',numeric_cols)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:27.678653Z","iopub.execute_input":"2022-02-22T20:45:27.679137Z","iopub.status.idle":"2022-02-22T20:45:27.703654Z","shell.execute_reply.started":"2022-02-22T20:45:27.6791Z","shell.execute_reply":"2022-02-22T20:45:27.70281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking unique values\nx[nominal_cols].describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:27.704858Z","iopub.execute_input":"2022-02-22T20:45:27.705339Z","iopub.status.idle":"2022-02-22T20:45:27.974415Z","shell.execute_reply.started":"2022-02-22T20:45:27.705301Z","shell.execute_reply":"2022-02-22T20:45:27.973588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test= train_test_split(x,y, test_size=0.20, random_state=0)\n\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:27.975744Z","iopub.execute_input":"2022-02-22T20:45:27.976064Z","iopub.status.idle":"2022-02-22T20:45:27.987625Z","shell.execute_reply.started":"2022-02-22T20:45:27.976028Z","shell.execute_reply":"2022-02-22T20:45:27.986624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Missing Value Imputation\n\nPerforming simple imputation as the goal of this exercise is to focus on improvement using KerasTuner.","metadata":{}},{"cell_type":"code","source":"def missing_val_imputation(x, ordinal_cols,nominal_cols,numeric_cols):\n    \n    for col in ordinal_cols:\n        x.loc[:,col]= x.loc[:,col].fillna(x.loc[:,col].mean())\n\n    x.loc[:,nominal_cols]= x.loc[:,nominal_cols].fillna(\"?\")\n    \n    for col in numeric_cols:\n        x.loc[:,col]= x.loc[:,col].fillna(x.loc[:,col].mean())\n#         x.loc[:,col]= x.groupby(\"OverallQual\")[col].transform(lambda grp:grp.fillna(np.mean(grp)))\n\n    print(\"All missing values are now imputed!\\n\",x.isnull().sum().sort_values(ascending=False))\n    \n    return x","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:27.989289Z","iopub.execute_input":"2022-02-22T20:45:27.989706Z","iopub.status.idle":"2022-02-22T20:45:27.999303Z","shell.execute_reply.started":"2022-02-22T20:45:27.989647Z","shell.execute_reply":"2022-02-22T20:45:27.998584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train= missing_val_imputation(x_train,ordinal_cols,nominal_cols,numeric_cols)\nx_test= missing_val_imputation(x_test,ordinal_cols,nominal_cols,numeric_cols)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-22T20:45:28.005059Z","iopub.execute_input":"2022-02-22T20:45:28.005345Z","iopub.status.idle":"2022-02-22T20:45:28.114036Z","shell.execute_reply.started":"2022-02-22T20:45:28.005308Z","shell.execute_reply":"2022-02-22T20:45:28.113366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"5. Analysis and treatment of temporal variables\n\nStrategy- capture time difference from YearSold","metadata":{}},{"cell_type":"code","source":"# Fitting OHE object\nohe= OneHotEncoder(handle_unknown='ignore', sparse=False).fit(x_train[nominal_cols]) \n\n#Feature Encoding for nominal columns\n\ndef ohe_transform(x, ohe, nominal_cols):\n    x_ohe= pd.DataFrame(ohe.transform(x[nominal_cols]))\n    x_ohe.columns=ohe.get_feature_names(nominal_cols)\n\n    # prepping x\n    x=x.drop(nominal_cols, axis=1)\n    x.reset_index(inplace=True, drop=True)\n    x= x.merge(x_ohe, left_index=True, right_index=True)\n    \n    return x\n\nx_train= ohe_transform(x_train, ohe, nominal_cols)\nx_test= ohe_transform(x_test, ohe, nominal_cols)\nx_train.shape, x_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:28.134194Z","iopub.execute_input":"2022-02-22T20:45:28.134625Z","iopub.status.idle":"2022-02-22T20:45:28.195064Z","shell.execute_reply.started":"2022-02-22T20:45:28.134595Z","shell.execute_reply":"2022-02-22T20:45:28.194231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standard Scaling\nss= StandardScaler()\nx_train_ss= pd.DataFrame(ss.fit_transform(x_train))\nx_test_ss= pd.DataFrame(ss.transform(x_test))\nx_train.shape, x_train_ss.shape, x_test_ss.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:28.196309Z","iopub.execute_input":"2022-02-22T20:45:28.196686Z","iopub.status.idle":"2022-02-22T20:45:28.226798Z","shell.execute_reply.started":"2022-02-22T20:45:28.19665Z","shell.execute_reply":"2022-02-22T20:45:28.225962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel\n\nsel= SelectFromModel(Lasso(alpha=0.5, max_iter=3000, tol=0.005, random_state=0, warm_start= False)) # warm_start= True\n\n# train the lasso model and select features\nsel.fit(x_train_ss, y_train)\n\nsel.get_support()\n\nselected_feats= x_train_ss.columns[(sel.get_support())]\n\n# print the stats\nprint(\"# of total features: \",x_train.shape[1])\nprint(\"# of selected features: \",len(selected_feats))\n# print(\"# of rejected features: \",np.sum(sel.estimator_.coef_==0))\n# print('Selected features:', selected_feats)\n\nx_train_ss= x_train_ss[selected_feats]\nx_test_ss= x_test_ss[selected_feats]\n\nx_train_ss.shape, x_test_ss.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:28.228391Z","iopub.execute_input":"2022-02-22T20:45:28.228976Z","iopub.status.idle":"2022-02-22T20:45:28.433699Z","shell.execute_reply.started":"2022-02-22T20:45:28.228938Z","shell.execute_reply":"2022-02-22T20:45:28.432835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting up a baseline model\n\nlogreg= LogisticRegression(random_state=3, max_iter=150, warm_start=True, n_jobs=-1)\n\nlogreg.fit(x_train_ss, y_train)\n\ny_pred_logreg= logreg.predict(x_test_ss)\ny_pred_logreg_train= logreg.predict(x_train_ss)\n\nprint(\"Training score:\",r2_score(y_train, y_pred_logreg_train))\n# Model Accuracy\n\nprint(\"Test score:\",r2_score(y_test,y_pred_logreg))\nprint(\"Mean squared error: \",np.sqrt(mean_squared_error(y_test, y_pred_logreg))) \n\n# Big time overfitting however our ","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:28.434906Z","iopub.execute_input":"2022-02-22T20:45:28.43539Z","iopub.status.idle":"2022-02-22T20:45:38.672967Z","shell.execute_reply.started":"2022-02-22T20:45:28.435353Z","shell.execute_reply":"2022-02-22T20:45:38.672026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implemention of Neural network","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models\n\n# import tensorflow_docs as tfdocs\n# import tensorflow_docs.plots\n# import tensorflow_docs.modeling\nprint(tf.__version__)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-22T20:45:38.674147Z","iopub.execute_input":"2022-02-22T20:45:38.674573Z","iopub.status.idle":"2022-02-22T20:45:38.682484Z","shell.execute_reply.started":"2022-02-22T20:45:38.674529Z","shell.execute_reply":"2022-02-22T20:45:38.681534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building a simple neural network\n\ndef build_model():\n    model=keras.Sequential([\n        layers.Dense(128, activation='relu', input_shape=[len(x_train_ss.keys())]),\n        layers.Dense(64, activation='relu'),\n        layers.Dense(32, activation='relu'),\n        layers.Dense(1)\n    ])\n    #No activation is used in the last layer as this is regression\n    optimizer= tf.keras.optimizers.Adam(0.001)\n    \n    model.compile(loss='mse',\n                 optimizer= optimizer,\n                 metrics= ['mae', 'mse'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:38.683555Z","iopub.execute_input":"2022-02-22T20:45:38.683889Z","iopub.status.idle":"2022-02-22T20:45:38.693365Z","shell.execute_reply.started":"2022-02-22T20:45:38.683856Z","shell.execute_reply":"2022-02-22T20:45:38.692309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build and inspect the model\n\nmodel= build_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:38.694797Z","iopub.execute_input":"2022-02-22T20:45:38.695528Z","iopub.status.idle":"2022-02-22T20:45:38.773608Z","shell.execute_reply.started":"2022-02-22T20:45:38.695487Z","shell.execute_reply":"2022-02-22T20:45:38.772845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the model\n\nTrain the model for 500 epochs, and record the training and validation accuracy in the history object","metadata":{}},{"cell_type":"code","source":"# Setting 'restore_best_weights' as True helps restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.\nearly_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min', restore_best_weights=False)\n\nhistory= model.fit(\n    x_train_ss, y_train,\n    epochs=200,\n    validation_data=(x_test_ss, y_test),\n    verbose=0, #set verbose=1 for full details at every epoch\n    callbacks= [early_stopping_cb])\n\nloss, mae, mse= model.evaluate(x_test_ss, y_test, verbose=2)\n\nprint(\"Test-set Mean absolute error: {:5.2f}\".format(mae)) # test mae- 36286","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:38.774838Z","iopub.execute_input":"2022-02-22T20:45:38.775221Z","iopub.status.idle":"2022-02-22T20:45:44.197184Z","shell.execute_reply.started":"2022-02-22T20:45:38.775182Z","shell.execute_reply":"2022-02-22T20:45:44.196487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\n\nplt.subplot(2,2,1)\nplt.plot(history.history['loss'], label='loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.legend()\nplt.title('Training - Loss Function')\n\nplt.subplot(2, 2, 2)\nplt.plot(history.history['mae'], label='Mean absolute error')\nplt.plot(history.history['val_mae'], label='Validation mean absolute error')\nplt.legend()\nplt.title('Train - MAE')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:44.20004Z","iopub.execute_input":"2022-02-22T20:45:44.200301Z","iopub.status.idle":"2022-02-22T20:45:44.682414Z","shell.execute_reply.started":"2022-02-22T20:45:44.200276Z","shell.execute_reply":"2022-02-22T20:45:44.681523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize the model's training progress using the stats stored in the history object","metadata":{}},{"cell_type":"code","source":"y_pred_test= model.predict(x_test_ss).flatten()\n\n# a = plt.axes(aspect='equal')\nplt.scatter(y_test, y_pred_test)\nplt.xlabel('True Values [SalesPrice]')\nplt.ylabel('Predictions [SalesPrice]')\n\nlims=[0, max(y_test)]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:44.683768Z","iopub.execute_input":"2022-02-22T20:45:44.684258Z","iopub.status.idle":"2022-02-22T20:45:45.08879Z","shell.execute_reply.started":"2022-02-22T20:45:44.684218Z","shell.execute_reply":"2022-02-22T20:45:45.087798Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It looks like our model predicts reasonably well. Let's take a look at the error distribution.","metadata":{}},{"cell_type":"code","source":"error= y_pred_test-y_test\nplt.hist(error, bins=25)\nplt.xlabel('Prediction Error [SalesPrice]')\n_=plt.ylabel('Count')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-02-22T20:45:45.090257Z","iopub.execute_input":"2022-02-22T20:45:45.090749Z","iopub.status.idle":"2022-02-22T20:45:45.347553Z","shell.execute_reply.started":"2022-02-22T20:45:45.090709Z","shell.execute_reply":"2022-02-22T20:45:45.346732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### It's not quite gaussian, but we might expect that because the number of samples is very small.","metadata":{}},{"cell_type":"code","source":"# Accuracy metrics :\n\ny_pred_train= model.predict(x_train_ss).flatten()\n\nprint(\"Accuracy obtained using x_train and x_val sets from the original x!\")\n\nprint(\"Training accuracy: \",r2_score(y_train, y_pred_train))\n\nprint(\"Test accuracy: \",r2_score(y_test, y_pred_test))\n\nprint(\"Test mean-squared error: \",np.sqrt(mean_squared_error(y_test, y_pred_test)))\n\n#Hence the current test accuracy is 0.48385 and Test MSE is 59703.04\n# Note- adding a droput layer decreases the accuracy to 0.46","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:45.34916Z","iopub.execute_input":"2022-02-22T20:45:45.349514Z","iopub.status.idle":"2022-02-22T20:45:45.516934Z","shell.execute_reply.started":"2022-02-22T20:45:45.349477Z","shell.execute_reply":"2022-02-22T20:45:45.516016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KerasTuner\n\nLibraries such as keras tuner make it dead simple to implement hyperparameter optimization into our training scripts in an organic manner:<br>\n\nAs we implement our model architecture, we define what ranges we want to search over for a given parameter (e.g., # of filters in our first CONV layer, # of filters in the second CONV layer, etc.)<br>\nWe then define an instance of either Hyperband, RandomSearch, or BayesianOptimization <br>\nThe keras tuner package takes care of the rest, running multiple trials until we converge on the best set of hyperparameters. <br>\n\nIt implements novel hyperparameter tuning algorithms including **Bayesian hyperparameter optimization and Hyperband**. It is an amazing tool to boost accuracy with minimal effort on your part!","metadata":{}},{"cell_type":"markdown","source":"### Optimizing Neural networks through KerasTuner\n","metadata":{}},{"cell_type":"code","source":"from kerastuner import HyperModel\nfrom kerastuner.tuners import RandomSearch, Hyperband\nimport IPython","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-22T20:45:45.518054Z","iopub.execute_input":"2022-02-22T20:45:45.518425Z","iopub.status.idle":"2022-02-22T20:45:45.523039Z","shell.execute_reply.started":"2022-02-22T20:45:45.518386Z","shell.execute_reply":"2022-02-22T20:45:45.522101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Define the model for hypertuning","metadata":{}},{"cell_type":"code","source":"class ANNhypermodel(HyperModel):\n    \n    def __init__(self, input_shape):\n        self.input_shape= input_shape\n        \n    def build(self, hp):\n        model= keras.Sequential()\n        \n        # Tune the number of units in the first Dense layer\n        # Defining dense units as a close approx to the original neural network to perform a fair comparision!\n        \n        \n        hp_units_1= hp.Int('units_1', min_value=128, max_value= 160, step=32)\n        hp_units_2= hp.Int('units_2', min_value=64, max_value= 128, step=32)\n        hp_units_3= hp.Int('units_3', min_value=32, max_value= 64, step=16)\n\n        model.add(keras.layers.Dense(units=hp_units_1, activation='relu', input_shape= self.input_shape))\n        model.add(keras.layers.Dense(units=hp_units_2, activation='relu'))\n        model.add(keras.layers.Dense(units=hp_units_3, activation='relu'))\n        model.add(keras.layers.Dense(1))\n\n        # Tune the learning rate for the optimizer \n        hp_learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default= 0.0005)\n\n        model.compile(loss='mse',\n                    optimizer= keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                    metrics= ['mae','mse']\n                     )\n\n        return model\n\nhypermodel= ANNhypermodel(input_shape= [len(x_train_ss.keys())])","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:45:45.5241Z","iopub.execute_input":"2022-02-22T20:45:45.52447Z","iopub.status.idle":"2022-02-22T20:45:45.541272Z","shell.execute_reply.started":"2022-02-22T20:45:45.524435Z","shell.execute_reply":"2022-02-22T20:45:45.540158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the same model we built earlier, except that for every hyperparameter, we defined a search space. You may have noticed hp.Int, hp.Float, and hp.Choice, these are used to define a search space for a hyperparameter that accepts an integer, float and a category respectively. ‘hp’ is an alias for Keras Tuner’s HyperParameters class.","metadata":{}},{"cell_type":"markdown","source":"#### 2. Instantiate the tuner to perform hypertuning <br>\nThe Keras Tuner has four tuners available - RandomSearch, Hyperband, BayesianOptimization, and Sklearn.","metadata":{}},{"cell_type":"markdown","source":"The most intuitive way to perform hyperparameter tuning is to randomly sample hyperparameter combinations and test them out. This is exactly what the RandomSearch tuner does! The objective is the function to optimize. The tuner infers if it is a maximization or a minimization problem based on its value.\n\n<pre>\nMAX_TRIALS = 20\ntuner= RandomSearch(hypermodel,\n               objective= 'val_mse',\n               max_trials= MAX_TRIALS,\n               executions_per_trial= EXECUTION_PER_TRIAL,\n               directory= 'random_search',\n               project_name='houseprices',\n               overwrite=True)\n</pre>\n'Max_trials' variable represents the number of hyperparameter combinations that will be tested by the tuner, while the 'execution_per_trial' variable is the number of models that should be built and fit for each trial for robustness purposes.","metadata":{}},{"cell_type":"markdown","source":"**In this tutorial, we use the Hyperband tuner.** <br> Hyperband is an optimized version of random search which uses early-stopping to speed up the hyperparameter tuning process. The main idea is to fit a large number of models for a small number of epochs and to only continue training for the models achieving the highest accuracy on the validation set. The max_epochs variable is the max number of epochs that a model can be trained for.","metadata":{}},{"cell_type":"code","source":"HYPERBAND_MAX_EPOCHS = 150\nEXECUTION_PER_TRIAL = 2\n\ntuner= Hyperband(hypermodel,\n                   objective= 'val_mse',\n                   max_epochs=HYPERBAND_MAX_EPOCHS, #Set 100+ for good results\n                   executions_per_trial=EXECUTION_PER_TRIAL,\n                   directory= 'hyperband',\n                   project_name='houseprices',\n                   overwrite=True)\n\n# tuner.search_space_summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:48:30.45947Z","iopub.execute_input":"2022-02-22T20:48:30.459875Z","iopub.status.idle":"2022-02-22T20:48:30.652993Z","shell.execute_reply.started":"2022-02-22T20:48:30.459842Z","shell.execute_reply":"2022-02-22T20:48:30.652103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + logfactor(max_epochs) and rounding it up to the nearest integer.","metadata":{}},{"cell_type":"markdown","source":"3. Run the hyperparameter search. <br>\nThe arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above.","metadata":{}},{"cell_type":"code","source":"print('searching for the best params!')\n\nt0= time()\ntuner.search(x= x_train_ss,\n             y= y_train,\n             epochs=100,\n             batch_size= 64,\n             validation_data= (x_test_ss, y_test),\n             verbose=0,\n             callbacks= []\n            )\nprint(time()- t0,\" secs\")\n\n# Retreive the optimal hyperparameters\nbest_hps= tuner.get_best_hyperparameters(num_trials=1)[0]\n\n# Retrieve the best model\nbest_model = tuner.get_best_models(num_models=1)[0]","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-22T20:48:32.712841Z","iopub.execute_input":"2022-02-22T20:48:32.713152Z","iopub.status.idle":"2022-02-22T20:48:55.170182Z","shell.execute_reply.started":"2022-02-22T20:48:32.713124Z","shell.execute_reply":"2022-02-22T20:48:55.169375Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"\"\"\nThe hyperparameter search is complete. The optimal number of units in the \nfirst densely-connected layer is {best_hps.get('units_1')},\nsecond layer is {best_hps.get('units_2')} \nthird layer is {best_hps.get('units_3')}  \n\nand the optimal learning rate for the optimizer\nis {best_hps.get('learning_rate')}.\n\"\"\")\n\n# Evaluate the best model.\nprint(best_model.metrics_names)\nloss, mae, mse = best_model.evaluate(x_test_ss, y_test)\nprint(f'loss:{loss} mae: {mae} mse: {mse}')","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:48:55.171947Z","iopub.execute_input":"2022-02-22T20:48:55.172268Z","iopub.status.idle":"2022-02-22T20:48:55.628198Z","shell.execute_reply.started":"2022-02-22T20:48:55.172241Z","shell.execute_reply":"2022-02-22T20:48:55.627318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"4. Retrain the model with the optimal hyperparameters from the search","metadata":{}},{"cell_type":"code","source":"# Build the model with the optimal hyperparameters and train it on the data\ntuned_model = tuner.hypermodel.build(best_hps)\n\n# Check result using best model\nt00= time()\nhistory_tuned= tuned_model.fit(x_train_ss, y_train, \n          epochs = 200, \n          validation_data = (x_test_ss, y_test),\n          verbose=0,\n          callbacks= early_stopping_cb)\n\n# print(time()- t00,\" secs\")\n\nprint(\"\\n Using Early stopping, needed only \",len(history_tuned.history['val_mse']),\"epochs to converge!\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-22T20:48:55.629528Z","iopub.execute_input":"2022-02-22T20:48:55.630056Z","iopub.status.idle":"2022-02-22T20:48:59.579664Z","shell.execute_reply.started":"2022-02-22T20:48:55.630015Z","shell.execute_reply":"2022-02-22T20:48:59.578718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train_tuned= tuned_model.predict(x_train_ss).flatten()\ny_pred_test_tuned= tuned_model.predict(x_test_ss).flatten()\n\nprint(\"Training accuracy: \",r2_score(y_train, y_pred_train_tuned))\n\nprint(\"Test accuracy: \",r2_score(y_test, y_pred_test_tuned))\n\nprint(\"Test mean-squared error: \",np.sqrt(mean_squared_error(y_test, y_pred_test_tuned)))","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:48:59.580807Z","iopub.execute_input":"2022-02-22T20:48:59.581143Z","iopub.status.idle":"2022-02-22T20:48:59.901525Z","shell.execute_reply.started":"2022-02-22T20:48:59.581105Z","shell.execute_reply":"2022-02-22T20:48:59.900738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking model performance on Test data","metadata":{}},{"cell_type":"code","source":"# Importing Test data\ntest_df= pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\n# Considering same columns which are used for training\ntest_df= test_df[train_cols]\n\ntest_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:49:23.375367Z","iopub.execute_input":"2022-02-22T20:49:23.375715Z","iopub.status.idle":"2022-02-22T20:49:23.414164Z","shell.execute_reply.started":"2022-02-22T20:49:23.375684Z","shell.execute_reply":"2022-02-22T20:49:23.413263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing the test dataset\ntest_df= missing_val_imputation(test_df,ordinal_cols,nominal_cols,numeric_cols)\n        \ntest_df= ohe_transform(test_df, ohe, nominal_cols)\n\ntest_df_ss= pd.DataFrame(ss.transform(test_df))\n\ntest_df_ss= test_df_ss[selected_feats]\n\ntest_df.shape, test_df_ss.shape","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-22T20:49:40.145414Z","iopub.execute_input":"2022-02-22T20:49:40.145765Z","iopub.status.idle":"2022-02-22T20:49:40.250801Z","shell.execute_reply.started":"2022-02-22T20:49:40.145732Z","shell.execute_reply":"2022-02-22T20:49:40.24993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make predictions\n\nFinally predict the SalesPrice values using the testing dataset:","metadata":{}},{"cell_type":"code","source":"# tuned_model_final.load_weights(checkpoint_path)\n\ntest_predictions= tuned_model.predict(test_df_ss).flatten()","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:50:56.141788Z","iopub.execute_input":"2022-02-22T20:50:56.142221Z","iopub.status.idle":"2022-02-22T20:50:56.413136Z","shell.execute_reply.started":"2022-02-22T20:50:56.142181Z","shell.execute_reply":"2022-02-22T20:50:56.412049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"subm = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\nsubm.iloc[:,1]= np.array(test_predictions)\nsnapshot_date= datetime.datetime.today().strftime('%m_%d_%Y')\n\n# subm.to_csv(str(snapshot_date)+'_tunedNN.csv',index=False)\nsubm.to_csv('tunedNN.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:51:07.966089Z","iopub.execute_input":"2022-02-22T20:51:07.966409Z","iopub.status.idle":"2022-02-22T20:51:08.304519Z","shell.execute_reply.started":"2022-02-22T20:51:07.966379Z","shell.execute_reply":"2022-02-22T20:51:08.303756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fetch the saved file and print results here\n\n# mysubm= pd.read_csv(f'{str(snapshot_date)}_tunedNN.csv')\nmysubm= pd.read_csv('tunedNN.csv')\n\nprint(mysubm.head())","metadata":{"execution":{"iopub.status.busy":"2022-02-22T20:51:08.526834Z","iopub.execute_input":"2022-02-22T20:51:08.527101Z","iopub.status.idle":"2022-02-22T20:51:08.543299Z","shell.execute_reply.started":"2022-02-22T20:51:08.527075Z","shell.execute_reply":"2022-02-22T20:51:08.542551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Well, Any Conclusions?\n\n<img src=\"https://media.giphy.com/media/l4FGC3sZppT0imaty/giphy.gif\">\n<br><br>\nThis notebook first introduced a simple neural network to handle a regression problem.<br>\nWe then went on to solve the painful and grey hyperparamater search using Keras Tuner, which is a rather easy-to-use, distributable hyperparameter optimization framework.<br>\n<br>\n\n**Here are a few learnings**: <br><br>\nMean Squared Error (MSE) is a common loss function used for regression problems (different loss functions are used for classification problems).<br>\nSimilarly, evaluation metrics used for regression differ from classification. <br>\nWhen numeric input data features have values with different ranges, each feature should be scaled independently to the same range.<br>\nIf there is not much training data, one technique is to prefer a small network with few hidden layers to avoid overfitting.<br>\nEarly stopping and Dropout is a useful technique to prevent overfitting.<br>\nIt is must to scale all input values before feeding to a Neural network.<br>\nFor simple datasets, even simple algorithms such as logistic regression can give close to state-of-the-art results without having to dive into the complexity of a neural network.<br>\nUsing Keras Tuner makes it easy to define a search space and leverage included algorithms to find the best hyperparameter values. <br>","metadata":{}},{"cell_type":"markdown","source":"<pre>\nReferences:\nYou can use these links to dive deeper into KerasTuner:\nhttp:////www.tensorflow.org/tutorials/keras/regression\nhttps://www.tensorflow.org/tutorials/keras/keras_tuner\nhttps://www.sicara.ai/blog/hyperparameter-tuning-keras-tuner\nhttps://pyimagesearch.com/2021/06/07/easy-hyperparameter-tuning-with-keras-tuner-and-tensorflow/\nhttps://neptune.ai/blog/keras-tuner-tuning-hyperparameters-deep-learning-model\n\nDo let me know if you have any suggestions! \nPlease upvote if your found my kernel helpful!<br>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}