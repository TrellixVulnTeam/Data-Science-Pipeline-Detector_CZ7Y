{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Introduction \n![featured image](https://storage.googleapis.com/idx-acnt-gs.ihouseprd.com/AR658975/file_manager/increase.jpg)\nThis Kernel is meant to solve the Housing Prices Challenge in a simple way, very much understandable for beginners at the same time being able to produce a great LB score.  \n\nPredicting Housing Prices is one of the very first things you must be able to do as at the start of your Data Science career. Our goal here is to build a machine learning model that can predict housing prices with good accuracy provided a great set of features which we'll extract from the training data for this competition. The performance of our model is evaluated based on a metric called Root-Mean-Squared-Error(RMSE). If you haven't yet read the competition desciption I strongly suggest you to go out and read it first up [here](http://https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/description). One of primary objectives of this kernel is to make things look simple enough code-wise for beginners so that they can understand, replicate and reproduce better models in the future. Without further ado, let's jump right in. \n\n**And by the way if you like this Kernel please give it an upvote.**"},{"metadata":{},"cell_type":"markdown","source":"## Import all the necessary packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\npd.set_option('display.width', 1000)\n\n# Plotting Tools\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\n\n# Import Sci-Kit Learn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold, learning_curve, KFold\n\n# Ensemble Models\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Package for stacking models\nfrom vecstack import stacking\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom IPython.display import display, HTML\ndisplay(HTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n</style>\n\"\"\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_all(df):\n    #This fuction lets us view the full dataframe\n    with pd.option_context('display.max_rows', 100, 'display.max_columns', 100):\n        display(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Bring training data into the environment\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\n\n# Bring test data into the environment\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv', index_col='Id')\n\nshow_all(train.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Oh looks like there are a ton of columns(features)."},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration, Engineering and Cleaning "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are a lot of missing values in a number of columns. I'll come on to fixing that soon. The target variable for us is the SalePrice. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lets make a plot to intuitively understand what's missing in our data and where it is missing."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot missing values \ndef plot_missing(df):\n    # Find columns having missing values and count\n    missing = df.isnull().sum()\n    missing = missing[missing > 0]\n    #missing.sort_values(inplace=True)\n    \n    # Plot missing values by count \n    missing.plot.bar(figsize=(12,8))\n    plt.xlabel('Columns with missing values')\n    plt.ylabel('Count')\n    \n    # search for missing data\n    import missingno as msno\n    msno.matrix(df=df, figsize=(16,8), color=(0,0.2,1))\n    \nplot_missing(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fixing the missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # IMPUTING MISSING VALUES\ndef fill_missing_values(df):\n    ''' This function imputes missing values with median for numeric columns \n        and most frequent value for categorical columns'''\n    missing = df.isnull().sum()\n    missing = missing[missing > 0]\n    for column in list(missing.index):\n        if df[column].dtype == 'object':\n            df[column].fillna(df[column].value_counts().index[0], inplace=True)\n        elif df[column].dtype == 'int64' or 'float64' or 'int16' or 'float16':\n            df[column].fillna(df[column].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_missing_values(train)\ntrain.isnull().sum().max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Now the test set also needs attention"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using the function written above to visualize missing values\nplot_missing(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Fixing missing data in test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"fill_missing_values(test)\ntest.isnull().sum().max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Imputing Categorical variables\nAs we have quite a lot of columns with textual and categorical data we have to impute them with numerics because ML models can only work with numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_cats(df):\n    '''This function converts categorical and non-numeric \n       columns into numeric columns to feed into a ML algorithm'''\n    # Find the columns of object type along with their column index\n    object_cols = list(df.select_dtypes(exclude=[np.number]).columns)\n    object_cols_ind = []\n    for col in object_cols:\n        object_cols_ind.append(df.columns.get_loc(col))\n\n    # Encode the categorical columns with numbers    \n    label_enc = LabelEncoder()\n    for i in object_cols_ind:\n        df.iloc[:,i] = label_enc.fit_transform(df.iloc[:,i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute the missing values\nimpute_cats(train)\nimpute_cats(test)\nprint(\"Train Dtype counts: \\n{}\".format(train.dtypes.value_counts()))\nprint(\"Test Dtype counts: \\n{}\".format(test.dtypes.value_counts()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Understanding our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_mat = train[[\"SalePrice\",\"MSSubClass\",\"MSZoning\",\"LotFrontage\",\"LotArea\", \"BldgType\",\n                       \"OverallQual\", \"OverallCond\",\"YearBuilt\", \"BedroomAbvGr\", \"PoolArea\", \"GarageArea\",\n                       \"SaleType\", \"MoSold\"]].corr()\n# corr_mat = train.corr()\nf, ax = plt.subplots(figsize=(16, 8))\nsns.heatmap(corr_mat, vmax=1 , square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the features OverallQual, GarageArea and YearBuilt are closely correlated with the sale price. That means these features play an important role in determining the SalePrice of a house. \nSimilarly a lot of inferences can be made just by looking at the heatmap but we're not going to go through each one of them. If you don't know what a heatmap is and wish to learn, I strongly recommend checking out [this video](https://www.youtube.com/watch?v=oMtDyOn2TCc)."},{"metadata":{},"cell_type":"markdown","source":"#### Visulaizing the relationship between SalePrice and YearBuilt"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(16, 8))\nsns.lineplot(x='YearBuilt', y='SalePrice', data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You might have noticed a significant increase in SalePrices just after the start of the 21st century, which is pretty interesting. What's even more surprising is the late 1800s saw phenomenal increase in SalePrices but dropping way below even before the end of that century."},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing the relationship between Sale prices and Overall Quality "},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x='OverallQual', y='SalePrice', color='green',data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the SalePrices increase rapidly with houses with better overall quality which is pretty reasonable."},{"metadata":{},"cell_type":"markdown","source":"#### Visulizing the Distribution of SalePrice"},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 6))\nsns.distplot(train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's clear that most of the houses, about 80% are sold for a price in the region on 100,000 and 200,000 dollars. Looks like there are a lot of houses sold in the mid 100,000s which makes up most of the data."},{"metadata":{},"cell_type":"markdown","source":"#### Visualizing the relationship between SalePrice and SaleType"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x='SaleType', y='SalePrice', data=train, kind='bar', palette='muted')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is clear that the selling prices for Sale types 2 and 6 are significantly higher than the others."},{"metadata":{},"cell_type":"markdown","source":"### Preparing the Data to do Machine Learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('SalePrice', axis=1)\ny = np.ravel(np.array(train[['SalePrice']]))\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use train_test_split from sci-kit learn to segment our data into train and a local testset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Evaluation Metric"},{"metadata":{},"cell_type":"markdown","source":"Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n\n![RMSE formula](https://gisgeography.com/wp-content/uploads/2014/07/rmse-formula1-300x96.png)\n \nWe will write a function named rmse to perform this task."},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(np.log(y), np.log(y_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{},"cell_type":"markdown","source":"We'll start by building standalone models, validating their performance and picking the right ones. Later we will stack all our models into an ensemble for better accuracy.\n\nI just played with a number of models and ended up picking the following models which gave me best results personally. If you find a better way please let me know :)\n\nI tuned the hyperparameters by manually experimenting a lot based on previous experiences, saving you a bunch of time hopefully. Anyways if you find a set of parameters that can work even better let me know in the comments down below. "},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize the model\nrandom_forest = RandomForestRegressor(n_estimators=1200,\n                                      max_depth=15,\n                                      min_samples_split=5,\n                                      min_samples_leaf=5,\n                                      max_features=None,\n                                      random_state=42,\n                                      oob_score=True\n                                     )\n\n# Perform cross-validation to see how well our model does \nkf = KFold(n_splits=5)\ny_pred = cross_val_score(random_forest, X, y, cv=kf, n_jobs=-1)\ny_pred.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model to our data\nrandom_forest.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on test data\nrf_pred = random_forest.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XG Boost"},{"metadata":{},"cell_type":"markdown","source":"Note: I found that our standalone XGBoost model in itself gives a good score. I suggest you to check it out."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize our model\nxg_boost = XGBRegressor( learning_rate=0.01,\n                         n_estimators=6000,\n                         max_depth=4, min_child_weight=1,\n                         gamma=0.6, subsample=0.7,\n                         colsample_bytree=0.2,\n                         objective='reg:linear', nthread=-1,\n                         scale_pos_weight=1, seed=27,\n                         reg_alpha=0.00006\n                       )\n\n# Perform cross-validation to see how well our model does \nkf = KFold(n_splits=5)\ny_pred = cross_val_score(xg_boost, X, y, cv=kf, n_jobs=-1)\ny_pred.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit our model to the training data\nxg_boost.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on the test data\nxgb_pred = xg_boost.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boost Regressor(GBM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize our model\ng_boost = GradientBoostingRegressor( n_estimators=6000, learning_rate=0.01,\n                                     max_depth=5, max_features='sqrt',\n                                     min_samples_leaf=15, min_samples_split=10,\n                                     loss='ls', random_state =42\n                                   )\n\n# Perform cross-validation to see how well our model does \nkf = KFold(n_splits=5)\ny_pred = cross_val_score(g_boost, X, y, cv=kf, n_jobs=-1)\ny_pred.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit our model to the training data\ng_boost.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on test data\ngbm_pred = g_boost.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ### LightGBM"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize our model\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=6,\n                                       learning_rate=0.01, \n                                       n_estimators=6400,\n                                       verbose=-1,\n                                       bagging_fraction=0.80,\n                                       bagging_freq=4, \n                                       bagging_seed=6,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                    )\n\n# Perform cross-validation to see how well our model does\nkf = KFold(n_splits=5)\ny_pred = cross_val_score(lightgbm, X, y, cv=kf)\nprint(y_pred.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit our model to the training data\nlightgbm.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on test data\nlgb_pred = lightgbm.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Stacking"},{"metadata":{},"cell_type":"markdown","source":"Stacking (also called meta ensembling) is a model ensembling technique used to combine information from multiple predictive models to generate a new model which usually performs better.\nIn this project we use python package called **vecstack** that helps us stack our models which we have imported earlier. It's actually very easy to use, you can have a look at the [documentation](https://github.com/vecxoz/vecstack) for more information.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of the models to be stacked\nmodels = [g_boost, xg_boost, lightgbm, random_forest]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform Stacking\nS_train, S_test = stacking(models,\n                           X_train, y_train, X_test,\n                           regression=True,\n                           mode='oof_pred_bag',\n                           metric=rmse,\n                           n_folds=5,\n                           random_state=25,\n                           verbose=2\n                          )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize 2nd level model\nxgb_lev2 = XGBRegressor(learning_rate=0.1, \n                        n_estimators=500,\n                        max_depth=3,\n                        n_jobs=-1,\n                        random_state=17\n                       )\n\n# Fit the 2nd level model on the output of level 1\nxgb_lev2.fit(S_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make predictions on the localized test set\nstacked_pred = xgb_lev2.predict(S_test)\nprint(\"RMSE of Stacked Model: {}\".format(rmse(y_test,stacked_pred)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{},"cell_type":"markdown","source":"Now it is finally time to make predictions on the real world test data. The approach here might look strange to you. You can visit this [link](https://github.com/vecxoz/vecstack/issues/4) to understand how it is done. "},{"metadata":{},"cell_type":"markdown","source":"#### Prediction on Stacked Layer 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"y1_pred_L1 = models[0].predict(test)\ny2_pred_L1 = models[1].predict(test)\ny3_pred_L1 = models[2].predict(test)\ny4_pred_L1 = models[3].predict(test)\nS_test_L1 = np.c_[y1_pred_L1, y2_pred_L1, y3_pred_L1, y4_pred_L1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Final Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stacked_pred = xgb_lev2.predict(S_test_L1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save the predictions in form of a dataframe\nsubmission = pd.DataFrame()\n\nsubmission['Id'] = np.array(test.index)\nsubmission['SalePrice'] = test_stacked_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Blending with top kernels"},{"metadata":{"trusted":true},"cell_type":"code","source":"top_public = pd.read_csv('../input/top-submissions/submission (9).csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_blend = (0.8*top_public.SalePrice.values + 0.2*test_stacked_pred)\n\nblended_submission = pd.DataFrame()\n\nblended_submission['Id'] = np.array(test.index)\nblended_submission['SalePrice'] = final_blend","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Submission Files"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)\nblended_submission.to_csv('blended_submission.csv', index=False) # Best LB Score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"i hope that was helpful. Thanks for being here, if you like this kernel **please upvote**. If you have any suggestions drop them down below. \n\nUntil next time, Happy Kaggling! :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}