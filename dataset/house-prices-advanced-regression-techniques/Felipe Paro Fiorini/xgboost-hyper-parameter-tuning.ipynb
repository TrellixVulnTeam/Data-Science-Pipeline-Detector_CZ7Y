{"cells":[{"metadata":{},"cell_type":"markdown","source":"# XGBoost & Hyperparameter tuning\n\n\n* [1. Loading and Inspecting Data](#2.-Loading-and-Inspecting-Data)\n* [2. Data preprocessing](#3.-Data-preprocessing)\n* [2.1 Fill NaN values](#3.1-Fill-NaN-values)\n* [2.2 Encoding ordinal features](#3.2-Encoding-ordinal-features)\n* [2.3 Encode nominal features](#3.3-Encode-nominal-features)\n* [3. Feature Engineering](#4.-Feature-Engineering)\n* [4. Normalize](#5.-Normalize)\n* [5. Fit Models](#6.-Fit-Models)\n* [5.1 Base line model](#6.1-Base-line-model)\n* [5.2 XGBoost](#6.2-XGBoost)\n    * [Parameters](#Parameters)\n    * [Tuning the hyper-parameters](#Tuning-the-hyper-parameters)\n    * [Best Fit](#Best-Fit)\n* [6. Compare Models](#7.-Compare-Models)\n* [7. Plot Results](#8.-Plot-Results)\n* [8. Predic Test & Submission](#9.-Predic-Test-&-Submission)\n\n\n\n\n\n<br>Reference:</br>\n<br>https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n<br>https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n<br>https://scikit-learn.org/stable/modules/grid_search.html#multimetric-grid-search\n<br>https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"},{"metadata":{},"cell_type":"markdown","source":"# 1. Loading and Inspecting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing packages\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Load dataset\ntrain = pd.read_csv(\"../input/train.csv\")\ntest  = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"#Dataset shape\nprint('Train %s\\nTest %s' % (train.shape, test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature to predict\nft_pred = list(set(train.columns) - set(test.columns))\nft_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[ft_pred].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot GrLivArea vs SalePrice\nplt.scatter(train['GrLivArea'], train['SalePrice'], color='blue', alpha=0.5)\nplt.title(\"LotArea vs SalePrice\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data preprocessing\n\n<ul>\n    <li>First I'll replace the numeric missing values (NaN's) with 0 and non numeric with none.\n    <li>Create Dummy variables for the categorical features.\n    <li>transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal.\n</ul>"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Fill NaN values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#save and drop id\ntrain_id = train[\"Id\"]\ntrain.drop(columns='Id',inplace=True)\n\ntest_id = test[\"Id\"]\ntest.drop(columns='Id',inplace=True)\n\n#select object columns\nobj_col = train.columns[train.dtypes == 'object'].values\n\n#select non object columns\nnum_col = train.columns[train.dtypes != 'object'].values\nnum_col_test = test.columns[test.dtypes != 'object'].values\n\n#replace null value in obj columns with None\ntrain[obj_col] = train[obj_col].fillna('None')\ntest[obj_col] = test[obj_col].fillna('None')\n\n#replace null value in numeric columns with 0\ntrain[num_col] = train[num_col].fillna(0)\ntest[num_col_test] = test[num_col_test].fillna(0)\n\ntrain_001 = train\ntest_001 = test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Encoding ordinal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"import category_encoders as ce\n\n#Ordinal features\nordinal_features = [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\", \"BsmtFinType1\",\"BsmtFinType2\",\n                    \"HeatingQC\",\"Electrical\",\"KitchenQual\", \"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"PoolQC\"]\n\n#Split X,y\ntrain_002_X = train_001.drop(ft_pred, axis=1)\ntrain_002_y = train_001[ft_pred]\n\nce_one_hot = ce.OrdinalEncoder(cols = ordinal_features)\n\ntrain_003 = pd.concat([ce_one_hot.fit_transform(train_002_X), train_002_y], axis=1, sort=False)\ntest_003  = ce_one_hot.transform(test_001)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Encode nominal features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Nominal features\nnominal_features = [x for x in obj_col if x not in ordinal_features]\n\n#Transfer object to int\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\n\n#for loop nominal feature column\nfor i in train_003[nominal_features].columns:\n    #fit and transform each column and assign to itself\n    train_003[i] = labelencoder.fit_transform(train_003[i])\n    \n#for loop nominal feature column\nfor i in test_003[nominal_features].columns:\n    #fit and transform each column and assign to itself\n    test_003[i] = labelencoder.fit_transform(test_003[i])\n    \n#Get dummy variable for nominal features\ntrain_005 = pd.get_dummies(train_003,columns=nominal_features,drop_first=True)\ntest_005 = pd.get_dummies(test_003,columns=nominal_features,drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"#Only for test set\n#Check if any null values\nprint(train_005.isnull().any().sum())\nprint(test_005.isnull().any().sum())\n\n#Get missing columns in the training test\nmissing_cols = set(train_005.drop(columns=\"SalePrice\").columns) - set(test_005.columns)\n\n#Add a missing column in test set with default value equal to 0\nfor cols in missing_cols:\n    test_005[cols] = 0\n    \n#Ensure the order of column in the test set is in the same order than in train set\ntest_005 = test_005[train_005.drop(columns=\"SalePrice\").columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#TotalBath\ntrain_005['TotalBath'] = (train_005['FullBath'] + train_005['HalfBath'] + train_005['BsmtFullBath'] + train_005['BsmtHalfBath'])\ntest_005['TotalBath']  = (test_005['FullBath']  + test_005['HalfBath']  + test_005['BsmtFullBath']  + test_005['BsmtHalfBath'])\n\n#TotalPorch\ntrain_005['TotalPorch'] = (train_005['OpenPorchSF'] + train_005['3SsnPorch'] + train_005['EnclosedPorch'] + train_005['ScreenPorch'] + train_005['WoodDeckSF'])\ntest_005['TotalPorch']  = (test_005['OpenPorchSF']  + test_005['3SsnPorch']  + test_005['EnclosedPorch']  + test_005['ScreenPorch']    + test_005['WoodDeckSF'])\n\n#Modeling happen during the sale year\ntrain_005[\"RecentRemodel\"] = (train_005[\"YearRemodAdd\"] == train_005[\"YrSold\"]) * 1\ntest_005[\"RecentRemodel\"]  = (test_005[\"YearRemodAdd\"]  == test_005[\"YrSold\"]) * 1\n\n#House sold in the year it was built\ntrain_005[\"NewHouse\"] = (train_005[\"YearBuilt\"] == train_005[\"YrSold\"]) * 1\ntest_005[\"NewHouse\"]  = (test_005[\"YearBuilt\"]  == test_005[\"YrSold\"]) * 1\n\n#YrBltAndRemod\ntrain_005[\"YrBltAndRemod\"] = train_005[\"YearBuilt\"] + train_005[\"YearRemodAdd\"]\ntest_005[\"YrBltAndRemod\"]  = test_005[\"YearBuilt\"]  + test_005[\"YearRemodAdd\"]\n\n#Total_sqr_footage\ntrain_005[\"Total_sqr_footage\"] = train_005[\"BsmtFinSF1\"] + train_005[\"BsmtFinSF2\"] + train_005[\"1stFlrSF\"] + train_005[\"2ndFlrSF\"]\ntest_005[\"Total_sqr_footage\"]  = test_005[\"BsmtFinSF1\"]  + test_005[\"BsmtFinSF2\"]  + test_005[\"1stFlrSF\"]  + test_005[\"2ndFlrSF\"]\n\n#HasPool\ntrain_005['HasPool'] = train_005['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasPool']  = test_005['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\n#HasFireplaces\ntrain_005['HasFirePlace'] = train_005['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasFirePlace']  = test_005['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n#Has2ndFloor\ntrain_005['Has2ndFloor'] = train_005['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['Has2ndFloor']  = test_005['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\n#HasGarage\ntrain_005['HasGarage'] = train_005['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasGarage']  = test_005['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n\n#HasBsmnt\ntrain_005['HasBsmnt'] = train_005['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasBsmnt']  = test_005['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Split dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing packages\nfrom sklearn.model_selection import train_test_split\n\nX = train_005.drop(columns=\"SalePrice\")\ny = train_005[\"SalePrice\"]\n\n#Particiona o data set originalmente Train em Train(Treino) e Val(validação)\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.25,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_val.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Outlier Detection\n\nPerhaps the most important hyperparameter in the model is the “contamination” argument, which is used to help estimate the number of outliers in the dataset. This is a value between 0.0 and 0.5 and by default is set to 0.1."},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\n\n#Isolation Forest\n\n# identify outliers in the training dataset\n#iso = IsolationForest(contamination=0.01)\n#yhat = iso.fit_predict(X_train)\n\n#Minimum Covariance Determinant\n\n# identify outliers in the training dataset\nee = EllipticEnvelope(contamination=0.01)\nyhat = ee.fit_predict(X_train)\n\n#Local Outlier Factor\n\n# identify outliers in the training dataset\n#lof = LocalOutlierFactor()\n#yhat = lof.fit_predict(X_train)\n\n#One-Class SVM\n\n# identify outliers in the training dataset\n#ee = OneClassSVM(nu=0.01)\n#yhat = ee.fit_predict(X_train)\n\n# select all rows that are not outliers\nmask = yhat != -1\nX_train_001, y_train_001 = X_train[mask], y_train[mask]\n\n# select all rows that are outliers\nmasko = yhat == -1\nX_train_o, y_train_o = X_train[masko], y_train[masko]\n\n# summarize the shape of the updated training dataset\nprint(X_train_001.shape, y_train_001.shape)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"#Plot GrLivArea vs SalePrice\nplt.scatter(X_train_001['GrLivArea'], y_train_001, color='blue', alpha=0.5)\nplt.scatter(X_train_o['GrLivArea'], y_train_o, color='red', alpha=0.5, label='outlier')\nplt.legend(loc=\"upper left\")\nplt.title(\"GrLivArea vs SalePrice\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Fit Models"},{"metadata":{},"cell_type":"markdown","source":"## 5.2 XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Importing Packages\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBRFRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n#from sklearn.preprocessing import Imputer#","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Parameters"},{"metadata":{},"cell_type":"markdown","source":"<b>Default parameters</b>\n<br>max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:squarederror', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain'"},{"metadata":{},"cell_type":"markdown","source":"**GridSearchCV params:**\n* **estimator:** estimator object\n* **param_grid :** dict or list of dictionaries\n* **scoring:** A single string or a callable to evaluate the predictions on the test set. If None, the estimator’s score method is used.\n    * https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n* **n_jobs:** Number of jobs to run in parallel. None means. -1 means using all processors.\n* **cv:** cross-validation, None, to use the default 3-fold cross validation. Integer, to specify the number of folds in a (Stratified)KFold."},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoost hyper-parameter tuning\ndef hyperParameterTuning(X_train_001, y_train_001):\n    param_tuning = {\n        'objective': ['reg:squarederror'],\n        'colsample_bytree': [0.2, 0.5, 1],\n        'subsample': [0.7, 1],\n        'learning_rate': [0.05, 0.1, 0.3],\n        'max_depth': [3, 6, 8],\n        'min_child_weight': [0, 1, 10],\n        'n_estimators' : [700, 1000, 5000]\n    }\n\n    xgb_model = XGBRegressor(tree_method='gpu_hist')\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,                        \n                           #scoring = 'neg_mean_absolute_error', #MAE\n                           #scoring = 'neg_mean_squared_error',  #MSE\n                           cv = 3,\n                           n_jobs = -1,\n                           verbose = 10)\n\n    gsearch.fit(X_train_001,y_train_001)\n\n    return gsearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"#Run only in the first run of the kernel.\n#hyperParameterTuning(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Best Params\n{'colsample_bytree': 0.7,\n 'learning_rate': 0.01,\n 'max_depth': 10,\n 'min_child_weight': 5,\n 'n_estimators': 500,\n 'subsample': 0.5}\n <br>\n {'colsample_bytree': 1,\n 'learning_rate': 0.05,\n 'max_depth': 8,\n 'min_child_weight': 0,\n 'n_estimators': 700,\n 'objective': 'reg:squarederror',\n 'subsample': 0.7}\n <br>\n {'colsample_bytree':0.01, 'n_estimators':3460,\n                                     'max_depth':3, 'min_child_weight':0,\n                                     'gamma':0, 'subsample':0.7,\n                                     'colsample_bytree':0.7,\n                                     'objective':'reg:linear', 'nthread':-1,\n                                     'scale_pos_weight':1, 'seed':27,\n                                     'reg_alpha':0.00006)"},{"metadata":{},"cell_type":"markdown","source":"### Best Fit"},{"metadata":{"trusted":true},"cell_type":"code","source":"XGBReg_def = XGBRegressor(objective = 'reg:squarederror', \n                        tree_method='gpu_hist')\n\nXGBReg_t01 =  XGBRegressor(objective = 'reg:squarederror', \n                        colsample_bytree = 0.7, \n                        learning_rate = 0.01, \n                        max_depth = 10, \n                        min_child_weight = 5, \n                        n_estimators = 500, \n                        subsample = 0.5,\n                        seed=27,\n                        tree_method='gpu_hist')\n\nXGBReg_t02 =  XGBRegressor(learning_rate=0.01,\n                           n_estimators=3000,\n                           max_depth=5, \n                           min_child_weight=0,\n                           gamma=0, \n                           subsample=0.7,                                \n                           colsample_bytree=0.7,                                     \n                           objective='reg:squarederror',                                \n                           scale_pos_weight=1, \n                           seed=27,                                     \n                           reg_alpha=0.00006,\n                           tree_method='gpu_hist')","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"xgb_model_t01 = XGBReg_t01\nxgb_model_t02 = XGBReg_t02\n\n%time xgb_model_t01.fit(X_train_001, y_train_001, early_stopping_rounds=10, eval_set=[(X_val, y_val)], verbose=False)\n%time xgb_model_t02.fit(X_train_001, y_train_001, early_stopping_rounds=10, eval_set=[(X_val, y_val)], verbose=False)\n\ny_pred_xgb_t01 = xgb_model_t01.predict(X_val)\ny_pred_xgb_t02 = xgb_model_t02.predict(X_val)\n\nmae_xgb_t01 = mean_absolute_error(y_val, y_pred_xgb_t01)\nmae_xgb_t02 = mean_absolute_error(y_val, y_pred_xgb_t02)\n\nprint(\"MAE t01: \", mae_xgb_t01)\nprint(\"MAE t02: \", mae_xgb_t02)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Join models"},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"y_pred = 0.5*y_pred_xgb_t01 + 0.5*y_pred_xgb_t02\n\nmae_xgb = mean_absolute_error(y_val, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"tags":[],"trusted":true},"cell_type":"code","source":"print(mae_xgb)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Plot Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Plot Real vs Predict\nplt.scatter(X_val['GrLivArea'], y_val,          color='blue', label='Real',    alpha=0.5)\nplt.scatter(X_val['GrLivArea'], y_pred,  color='red' , label='Predict', alpha=0.5)\nplt.title(\"Real vs Predict\")\nplt.legend(loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Feature importance \nfor model in [xgb_model_t01, xgb_model_t02]:\n    xgb.plot_importance(model, max_num_features=20)\n    plt.title(\"xgboost.plot_importance(model)\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Predic Test & Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test_005\n\n# Use the model to make predictions\ny_pred_test_t01 = xgb_model_t01.predict(X_test)\ny_pred_test_t02 = xgb_model_t02.predict(X_test)\n\ny_pred_test = 0.4*y_pred_test_t01 + 0.6*y_pred_test_t02\n\nsubmission = pd.DataFrame({'Id':test_id,'SalePrice':y_pred_test})\n\n# Save results\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}