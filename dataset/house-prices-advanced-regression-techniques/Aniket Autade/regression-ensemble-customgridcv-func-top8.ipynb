{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#33ccff;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<p style=\"padding: 12px; color:white; text-align:center\"><b>Highlights</b></p>\n</div>\n<div>\n<p style=\"text-align:center\">1. Custom scoring function for parameter tuning based on RMSE values.</p>\n<p style=\"text-align:center\">2. Extensive missing data analysis with detailed reasoning.</p>\n<p style=\"text-align:center\">3. Comparison amongst different regression algorithms.</p>\n<p style=\"text-align:center\">4. Novice attempt at the Stacking method.</p>\n<p style=\"text-align:center\">5. Prediction using ensemble method.</p>\n</div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntrain=pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest=pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T06:32:36.199587Z","iopub.execute_input":"2022-04-22T06:32:36.200216Z","iopub.status.idle":"2022-04-22T06:32:37.254398Z","shell.execute_reply.started":"2022-04-22T06:32:36.200111Z","shell.execute_reply":"2022-04-22T06:32:37.253691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"train.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-04-22T06:32:37.255728Z","iopub.execute_input":"2022-04-22T06:32:37.256062Z","iopub.status.idle":"2022-04-22T06:32:37.37382Z","shell.execute_reply.started":"2022-04-22T06:32:37.256032Z","shell.execute_reply":"2022-04-22T06:32:37.372939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.describe(include='O').T","metadata":{"execution":{"iopub.status.busy":"2022-04-22T06:32:37.374826Z","iopub.execute_input":"2022-04-22T06:32:37.37502Z","iopub.status.idle":"2022-04-22T06:32:37.446037Z","shell.execute_reply.started":"2022-04-22T06:32:37.374997Z","shell.execute_reply":"2022-04-22T06:32:37.445471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Bivariate Analysis** ","metadata":{}},{"cell_type":"markdown","source":"**Let's look at house price variable against different numerical variables as per their sale conditions.**","metadata":{}},{"cell_type":"code","source":"sns.set_theme()\nfeatures=[i for i in (train.iloc[:,1:-1]).columns if train[i].nunique() > 25 ]\n\nplt.style.use(plt.style.available[19])\ni = 1\nplt.figure(figsize=(20,25))\nfor j in features:\n    plt.subplot(6, 3, i)\n    sns.scatterplot(x=j,data=train, y=train['SalePrice'], hue='SaleCondition',palette='deep')\n    plt.xlabel(j)\n    i += 1\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-22T06:32:37.447282Z","iopub.execute_input":"2022-04-22T06:32:37.447927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of Houses sold in each neighborhood**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(22,6))\nsns.countplot(data=train, x='Neighborhood')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's understand the distribution of houses sold by the built year for each neighborhood separately.**","metadata":{}},{"cell_type":"code","source":"sns.set_theme()\nj=1\nplt.figure(figsize=(24,40))\nfor i in train['Neighborhood'].unique():\n    plt.subplot(9,3, j)\n    sns.scatterplot(x=train[train['Neighborhood']==i]['YearBuilt'],data=train, y=train[train['Neighborhood']==i]['SalePrice'],hue='YrSold',palette='deep')\n    plt.xlabel(i)\n    j+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_theme()\nfig=plt.figure(figsize=(20,40))\nfor i in range(len((train.select_dtypes(include='object')).columns)):\n    fig.add_subplot(11,4,i+1)\n    train.select_dtypes(include='object').iloc[:,i].value_counts().plot(kind=\"pie\", subplots=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outlier Analysis","metadata":{}},{"cell_type":"code","source":"sns.set_theme()\nfor i in (train[['BsmtFinSF2', 'BsmtFinSF1', 'MasVnrArea', 'LotArea', 'LotFrontage']].columns):\n    plt.figure(figsize=(15,6))\n    sns.scatterplot(x=i,data=train,y=train['SalePrice'])\n    plt.title('SalePrice against {}'.format(i))\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Extreme outlier observations** are neglected from the train dataset. Hence, the following code. ","metadata":{}},{"cell_type":"code","source":"train= train[~((train['BsmtFinSF2']>1200) |(train['ScreenPorch']>350)|(train['GrLivArea']>4000)|(train['OpenPorchSF']>350)|(train['EnclosedPorch']>350)| (train['BsmtFinSF1']>3000) |(train['MasVnrArea']>1200) |(train['LotArea']>100000) | (train['LotFrontage']>200))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's concatenate train and test csv files to only address missing values issue.**","metadata":{}},{"cell_type":"markdown","source":"This concatenation also helps in unifying structure for train and test datasets. ","metadata":{}},{"cell_type":"code","source":"data=pd.concat([train, test], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Missing Data Analysis","metadata":{}},{"cell_type":"code","source":"df=pd.DataFrame({'Type': data.dtypes,\n                  'Missing': data.isna().sum(),\n                  'Size':data.shape[0],\n                  'Unique': data.nunique()})\ndf['Missing_%']= (df.Missing/df.Size)*100\ndf[df['Missing']>0].sort_values(by=['Missing_%'], ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Categorical Variables","metadata":{}},{"cell_type":"markdown","source":"**PoolQC**: Missing value indicates No pool as few dwellings are likely to have pool.\n\n**MiscFeature**: Data Description text file indicates NA means None.\n\n**Alley**: Data Description text file indicates NA means 'No alley access' regarding Alley column. It is likely that missing values are NA.\n\n**Fence**: Similar reasoning as previous code cell.\n\n**FireplaceQC**:It is likely that certain share of dwellings will not have any fireplace.\n\n**GarageType**:Similar reasoning as previous code cell.\n\n**GarageFinish**: If the GarageType is 'NA', then 'GarageFinish' missing values should be denoted as 'NA'.\n\n**GarageQual**: Similarly, All the missing values in GarageQual column have 'NA' as a garage type.\n\n**GarageCond**: Similar reasoning for GarageCond column as mentioned in the previous cell.","metadata":{}},{"cell_type":"markdown","source":"**Basement Condition**:Unique values with missing values do not indicate a single dwelling that has no basement. Similar assumption as Garagetype is taken into considertaion. Certain share of dwellings likely to have no basement.\n\n**BasementExposure**: All the 37 missing values in BsmtExposure column have NO basement as the basement type.\n\n**BasementQuality** and **BasementFintype1**: Similar reasoning as the above.\n\n**BasementFinType2**: All the missing values are assumed 'NA'.","metadata":{}},{"cell_type":"markdown","source":"**Masonry veneer type**: Observations with missing values in the MasVnrType column have missing values in MasVnrArea as well.\n\nLet's replace the MasVnrType with the mode(Mode is 'None').","metadata":{}},{"cell_type":"markdown","source":"**Electrical**: The missing observation is replaced by the most frequent category.","metadata":{}},{"cell_type":"markdown","source":"Regarding **MsZoning**, **Functional**, **Utilities**, **KitchenQual**, **Exterior2nd**,**Exterior1st** and **Saletype**: \n    missing values are replaced with most frequent observation for the respective columns.","metadata":{}},{"cell_type":"code","source":"data['PoolQC']=data['PoolQC'].fillna('NA')\ndata['MiscFeature']=data['MiscFeature'].fillna('NA')\ndata['Alley']=data['Alley'].fillna('NA')\ndata['Fence']=data['Fence'].fillna('NA')\ndata['FireplaceQu']=data['FireplaceQu'].fillna('NA')\ndata['GarageType']=data['GarageType'].fillna('NA')\ndata['GarageFinish']=data['GarageFinish'].fillna('NA')\ndata['BsmtCond']=data['BsmtCond'].fillna('NA')\ndata['BsmtExposure']=data['BsmtExposure'].fillna('NA')\ndata['BsmtQual']=data['BsmtQual'].fillna('NA')\ndata['BsmtFinType2']=data['BsmtFinType2'].fillna('NA')\ndata['Electrical']=data['Electrical'].fillna(data['Electrical'].mode()[0])\ndata['GarageCond']=data['GarageCond'].fillna('NA')\ndata['GarageQual']=data['GarageQual'].fillna('NA')\ndata['BsmtFinType1']=data['BsmtFinType1'].fillna('NA')\ndata['MasVnrType']=data['MasVnrType'].fillna('None')\ndata['MSZoning']=data['MSZoning'].fillna('RL')\ndata['Functional']=data['Functional'].fillna('Typ')\ndata['Utilities']=data['Utilities'].fillna('AllPub')\ndata['KitchenQual']=data['KitchenQual'].fillna('TA')\ndata['Exterior2nd']=data['Exterior2nd'].fillna('VinylSd')\ndata['Exterior1st']=data['Exterior1st'].fillna('VinylSd')\ndata['SaleType']=data['SaleType'].fillna('WD')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MSSubClass**: Let's make MSSubclass a categorical variable as numbers represent the belonging class. **MoSold** is assumed \ncategorical variable as well. ","metadata":{}},{"cell_type":"code","source":"data['MSSubClass']=data['MSSubClass'].astype(object)\ndata['MoSold']=data['MoSold'].astype(object)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing values in Numerical Variables","metadata":{}},{"cell_type":"code","source":"df=pd.DataFrame({'Type': data.dtypes,\n                  'Missing': data.isna().sum(),\n                  'Size':data.shape[0],\n                  'Unique': data.nunique()})\ndf['Missing_%']= (df.Missing/df.Size)*100\ndf[df['Missing']>0].sort_values(by=['Missing_%'], ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SalePrice**: The idea behind this competition is predict saleprice values for the test dataset. These missing values hence \n    represent empty cells. While addressing the missing values issue using median value, it is important to ignore this column.","metadata":{}},{"cell_type":"code","source":"for i in df[df['Missing']>0].index:\n    if i=='SalePrice':\n        continue\n    else:\n        data[i]=data[i].fillna(data[i].median())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make sure that **SalePrice** remains unaffected.","metadata":{}},{"cell_type":"code","source":"df=pd.DataFrame({'Type': data.dtypes,\n                  'Missing': data.isna().sum(),\n                  'Size':  data.shape[0],\n                  'Unique': data.nunique()})\ndf['Missing_%']= (df.Missing/df.Size)*100\ndf[df['Missing']>0].sort_values(by=['Missing_%'], ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now the missing values issue has been addressed.**","metadata":{}},{"cell_type":"markdown","source":"# Variable Categorization","metadata":{}},{"cell_type":"markdown","source":"This competition provides a text file which explains the variables. All the variables are **categorized manually** using\nthe information provided in the text file.","metadata":{}},{"cell_type":"code","source":"categorical=['MSSubClass', 'MSZoning', 'LotConfig', 'Neighborhood', 'LandSlope', 'LandContour',\n             'Condition1','Condition2','BldgType', 'HouseStyle', 'YearBuilt','YearBuilt',\n             'YearRemodAdd', 'RoofStyle', 'Exterior1st', 'Exterior2nd','RoofMatl',\n            'MasVnrType', 'Foundation', 'Heating', 'Electrical', 'GarageType', 'Fence',\n            'MiscFeature', 'MoSold' ,'YrSold', 'SaleType', 'PavedDrive','Alley','SaleCondition' ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal=['LotShape', 'ExterQual','ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n         'BsmtFinType1', 'BsmtFinType2','HeatingQC','KitchenQual', 'Functional', \n        'FireplaceQu', 'GarageFinish','GarageQual', 'GarageCond', ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical=['LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'MasVnrArea',\n          'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n          '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath','FullBath',\n          'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', \n           'GarageCars', 'GarageArea', 'WoodDeckSF','OpenPorchSF', 'EnclosedPorch',\n          'ScreenPorch', 'MiscVal', 'GarageYrBlt' ]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the **value_counts** code, categories in few ordinal columns are clubbed together and applied same rating. This is done in order to reduce **the number of rating levels.** \n\nUnfortunately, the code is not provided. The notebook becomes unnecessarily longer with value_counts result.","metadata":{}},{"cell_type":"code","source":"ex_qu= { 'Po':0, 'Fa': 0, 'TA': 1, 'Gd': 2, 'Ex': 3 }\nex_cond={ 'Po':0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4 }\nBsmt_Qual={\"NA\": 0, 'Po':1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5 }\nBsmtFinType1={ \"NA\": 0, 'Unf':0, 'LwQ': 1, 'Rec': 2, 'BLQ': 3, 'ALQ': 4, 'GLQ':5 }\nBsmt_Exposure={ \"NA\":0, \"No\": 0, \"Mn\": 1, \"Av\": 2, \"Gd\": 3}\ngarage_fin={'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\ngarage_qu= { \"NA\": 0, 'Po':0, 'Fa':1, 'TA': 2, 'Gd': 3, 'Ex': 4  }\nLotShape={\"IR3\": 0, 'IR2':0, 'IR1': 1, 'Reg': 2}\nFunctional={\"Sal\": 0, 'Sev':1, 'Maj2': 2, 'Maj1': 3, 'Mod': 4, 'Min2':5, 'Min1':6, 'Typ':7}\n\ndata=data.replace({\"LotShape\": LotShape,\n                    \"ExterQual\": ex_qu,\n                   \"ExterCond\": ex_cond,\n                   \"BsmtQual\": Bsmt_Qual,\n                   \"BsmtCond\": Bsmt_Qual,\n                   \"BsmtExposure\": Bsmt_Exposure, \n                   \"BsmtFinType1\": BsmtFinType1, \n                   \"BsmtFinType2\": BsmtFinType1,\n                   \"HeatingQC\": ex_qu,\n                   \"KitchenQual\": ex_qu,\n                   \"Functional\": Functional,\n                    \"GarageFinish\": garage_fin,\n                    \"GarageQual\": garage_qu,\n                    \"GarageCond\": garage_qu,\n                    \"FireplaceQu\": garage_qu})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X1=data[ordinal]\nX2=pd.get_dummies(data[categorical], drop_first=True)\nX3=data[numerical]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In order to avoid data leakage from the observations in the test csv file into train csv file, I will convert X3 into X3_train and X3_test file and then apply box-cox transformation.**","metadata":{}},{"cell_type":"code","source":"X3_train=X3.iloc[:len(train),:]\nX3_test=X3.iloc[len(train):,:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Box-cox Transformation for the data in the Train csv file","metadata":{}},{"cell_type":"code","source":"skewed_columns=[]\nfor i in X3_train.columns:\n    if abs(X3_train[i].skew())> 0.5:\n        skewed_columns.append(i)\n\nfrom scipy.special import boxcox1p\nlam=0.15\nfor i in skewed_columns:\n    X3_train[i]= boxcox1p(X3_train[i],lam)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Box-cox Transformation for the data in the Test csv file","metadata":{}},{"cell_type":"code","source":"from scipy.special import boxcox1p\nlam=0.15\nfor i in skewed_columns:\n    X3_test[i]= boxcox1p(X3_test[i],lam)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's merge these two X3_train and X3_test files to revert back to the same dimension as X2 and X1.**","metadata":{}},{"cell_type":"code","source":"X3=pd.concat([X3_train,X3_test], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset=(pd.concat([X2, X1, X3], axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=dataset.iloc[:len(train),:].values\nY=train.iloc[:,-1:].values\nY=np.log1p(Y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's come back to this test dataset after finding the hypertuned model**","metadata":{}},{"cell_type":"code","source":"test_dataset=dataset.iloc[len(train):,:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the dataset- Train and Test","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.2, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nsc= RobustScaler()\nX_train[:,(len(X1.columns)+len(X2.columns)):]= sc.fit_transform(X_train[:, (len(X1.columns)+len(X2.columns)):])\nX_test[:,(len(X1.columns)+len(X2.columns)):]= sc.transform(X_test[:, (len(X1.columns)+len(X2.columns)):])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Important Libraries","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Regression Algorithms","metadata":{}},{"cell_type":"markdown","source":"## Lasso Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nreg = Lasso(alpha=0.0008)\nreg.fit(X_train, Y_train)\nY_pred = reg.predict(X_test)\nprint(reg.score(X_train,Y_train))\nprint(\"RMSE: \",round(mean_squared_error((Y_test),(Y_pred), squared=False), 4))\nprint(\"MSE: \", round(mean_absolute_error(np.exp(Y_test), np.exp(Y_pred)), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom GridSearch CV","metadata":{}},{"cell_type":"markdown","source":"I plan to use **custom scoring technique** based on RMSE values. More details on this technique are mentioned in the following link.\n\nlink: 'https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter' ","metadata":{}},{"cell_type":"code","source":"def custom_function(Y_train, Y_pred):\n    RMSE=(round(mean_squared_error((Y_train),(Y_pred), squared=False), 4))\n    return RMSE\n\nscorer=make_scorer(custom_function,greater_is_better=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = [{ 'alpha': [0.0005,0.0006,0.0007,0.0008,0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.009,0.01,0.011,0.012,0.1,0.2,0.3,0.4]\n              }]\ngrid_search = GridSearchCV(estimator = Lasso(),\n                           param_grid = parameters,\n                           scoring = scorer,\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ridge Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nreg = Ridge(alpha=1)\nreg.fit(X_train, Y_train)\nY_pred = reg.predict(X_test)\nprint(reg.score(X_train,Y_train))\nprint(\"RMSE: \",round(mean_squared_error((Y_test),(Y_pred), squared=False), 4))\nprint(\"MSE: \", round(mean_absolute_error(np.exp(Y_test), np.exp(Y_pred)), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"parameters = [{ 'alpha': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n              }]\ngrid_search = GridSearchCV(estimator = Ridge(),\n                           param_grid = parameters,\n                           scoring = scorer,\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grid_search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RidgeCV Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\nreg = RidgeCV(alphas=(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0))\nreg.fit(X_train, Y_train)\nY_pred = reg.predict(X_test)\nprint(reg.score(X_train,Y_train))\nprint(\"RMSE: \",round(mean_squared_error((Y_test),(Y_pred), squared=False), 4))\nprint(\"MSE: \", round(mean_absolute_error(np.exp(Y_test), np.exp(Y_pred)), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random_Forest Regressor","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nreg = RandomForestRegressor()\nreg.fit(X_train, Y_train)\nY_pred = reg.predict(X_test)\nprint(reg.score(X_train,Y_train))\nprint(\"RMSE: \",round(mean_squared_error((Y_test),(Y_pred), squared=False), 4))\nprint(\"MSE: \", round(mean_absolute_error(np.exp(Y_test), np.exp(Y_pred)), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XG Boost Regressor","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nreg = xgb.XGBRegressor()\nreg.fit(X_train, Y_train)\nY_pred = reg.predict(X_test)\nprint(reg.score(X_train,Y_train))\nprint(\"RMSE: \",round(mean_squared_error((Y_test),(Y_pred), squared=False), 4))\nprint(\"MSE: \", round(mean_absolute_error(np.exp(Y_test), np.exp(Y_pred)), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LGBM Regressor","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\nreg = LGBMRegressor()\nreg.fit(X_train, Y_train)\nY_pred = reg.predict(X_test)\nprint(reg.score(X_train,Y_train))\nprint(\"RMSE: \",round(mean_squared_error((Y_test),(Y_pred), squared=False), 4))\nprint(\"MSE: \", round(mean_absolute_error(np.exp(Y_test), np.exp(Y_pred)), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**I ran grid search CV technique to find best parameters. Best parameters are used in following results.**","metadata":{}},{"cell_type":"markdown","source":"<h4>Let's tabulate the results from different ML algorithms and compare the results using RMSE and MAE values.</h4>","metadata":{}},{"cell_type":"code","source":"models=['RidgeCV_Regression', 'Random_Forest_Regression', 'XG-Boost_Regression', 'Ridge_Regression', 'Lasso_Regression', 'LGBM_Regression']\n\nregressor = [\n             RidgeCV(alphas=(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0)), \n             RandomForestRegressor(), \n             xgb.XGBRegressor(learning_rate=0.07, max_depth= 2, n_estimators= 900, reg_alpha= 0.5, reg_lambda=0.3), \n             Ridge(alpha=0.006),\n             Lasso(alpha=0.0008),\n             LGBMRegressor(learning_rate=0.07, max_depth= 2, n_estimators= 1100, reg_alpha= 0.4, reg_lambda=0.4)\n            ]\nscores=[]\nRMSE=[]\nMAE=[]\n\nfor i in regressor:\n    i.fit(X_train, Y_train)\n    Y_pred = i.predict(X_test)\n    scores.append(i.score(X_train,Y_train))\n    RMSE.append(round(mean_squared_error((Y_test),(Y_pred), squared=False), 4))\n    MAE.append(round(mean_absolute_error(np.exp(Y_test), np.exp(Y_pred)), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result=pd.DataFrame({'Model': models, 'Score': scores, 'RMSE': RMSE, 'MAE': MAE})\nresult.sort_values(by=['RMSE'], ascending=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble prediction using base models","metadata":{}},{"cell_type":"code","source":"models=['Model-1', 'Model-2', 'Model-3', 'Model-4', 'Model-5']\n\nregressor = [LGBMRegressor(learning_rate=0.07, max_depth= 2, n_estimators= 900, reg_alpha= 0.5, reg_lambda=0.3), \n             xgb.XGBRegressor(learning_rate=0.05, n_estimators=900, reg_alpha=0.3, reg_lambda=0.2, max_depth=4),\n             Lasso(alpha=0.0008)]\n\nweights1=[0.25,0.45,0.30]\nweights2=[0.40,0.40,0.20]\nweights3=[0.05,0.75,0.2]\nweights4=[0.45,0.45,0.10]\nweights5=[0.35,0.35,0.30]\n\nRMSE=[]\nMAE=[]\nY_tot1=0\nY_tot2=0\nY_tot3=0\nY_tot4=0\nY_tot5=0\n\n\nfor i in range(len(regressor)):\n    regressor[i].fit(X_train, Y_train)\n    Y_pred = regressor[i].predict(X_test)\n    Y_tot1+=weights1[i]*Y_pred\n    Y_tot2+=weights2[i]*Y_pred\n    Y_tot3+=weights3[i]*Y_pred\n    Y_tot4+=weights4[i]*Y_pred\n    Y_tot5+=weights5[i]*Y_pred\n\nRMSE.append(round(mean_squared_error((Y_test),(Y_tot1), squared=False), 4))\nRMSE.append(round(mean_squared_error((Y_test),(Y_tot2), squared=False), 4))\nRMSE.append(round(mean_squared_error((Y_test),(Y_tot3), squared=False), 4))\nRMSE.append(round(mean_squared_error((Y_test),(Y_tot4), squared=False), 4))\nRMSE.append(round(mean_squared_error((Y_test),(Y_tot5), squared=False), 4))\n\nMAE.append(round(mean_absolute_error(np.exp(Y_test), np.exp(Y_tot1)), 3))\nMAE.append(round(mean_absolute_error(np.exp(Y_test), np.exp(Y_tot2)), 3))\nMAE.append(round(mean_absolute_error(np.exp(Y_test), np.exp(Y_tot3)), 3))\nMAE.append(round(mean_absolute_error(np.exp(Y_test), np.exp(Y_tot4)), 3))\nMAE.append(round(mean_absolute_error(np.exp(Y_test), np.exp(Y_tot5)), 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result=pd.DataFrame({'Model': models ,'RMSE': RMSE, 'MAE': MAE})\nresult.sort_values(by=['RMSE'], ascending=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**It is evident from the results that ensemble predictions have lower RMSE values compared to base model predictions.**","metadata":{}},{"cell_type":"markdown","source":"I am introduced to the following technique from different notebooks under the same competition codes. \n\nI thank the community for your codes. ","metadata":{}},{"cell_type":"markdown","source":"I used following link to generate my first stacking regressor predictions.\n\n**Link**: https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/) ","metadata":{}},{"cell_type":"markdown","source":"# Stacking Regressor","metadata":{}},{"cell_type":"markdown","source":"**This is first time I am creating a stacking regressor.** \n\n**Please comment if you see any mistakes in my structural approach.** ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"level_0 = list()\nlevel_0.append(('Lasso',Lasso(alpha=0.0008)))\nlevel_0.append(('LGBMRegressor', LGBMRegressor(learning_rate=0.07, max_depth= 2, n_estimators= 900, reg_alpha= 0.5, reg_lambda=0.3)))\nlevel_0.append(('XGBoost', xgb.XGBRegressor(learning_rate=0.05, n_estimators=900, reg_alpha=0.3, reg_lambda=0.2, max_depth=4)))\n\nlevel1 =RidgeCV()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = StackingRegressor(estimators=level_0, final_estimator=level1, cv=10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred = model.predict(X_test)\nprint(model.score(X_train,Y_train))\nprint(\"RMSE: \",round(mean_squared_error((Y_test),(Y_pred), squared=False), 4))\nprint(\"MSE: \", round(mean_absolute_error(np.exp(Y_test), np.exp(Y_pred)), 4))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble model with a stacking regressor","metadata":{}},{"cell_type":"code","source":"models=['Model-1', 'Model-2', 'Model-3', 'Model-4', 'Model-5']\n\nregressor = [LGBMRegressor(learning_rate=0.07, max_depth= 2, n_estimators= 900, reg_alpha= 0.5, reg_lambda=0.3), \n             xgb.XGBRegressor(learning_rate=0.05, n_estimators=900,reg_alpha=0.4, reg_lambda=0.2,max_depth=3), \n             Lasso(alpha=0.0008), model]\n\nweights1=[0.2,0.2,0.3,0.3]\nweights2=[0.1,0.1,0.4,0.4]\nweights3=[0.1,0.2,0.2, 0.5]\nweights4=[0.05,0.05,0.25, 0.65]\nweights5=[0.1,0.1,0.05,0.75]\n\nRMSE=[]\nMAE=[]\nY_tot1=0\nY_tot2=0\nY_tot3=0\nY_tot4=0\nY_tot5=0\n\nfor i in range(len(regressor)):\n    regressor[i].fit(X_train, Y_train)\n    Y_pred = regressor[i].predict(X_test)\n    Y_tot1+=weights1[i]*Y_pred\n    Y_tot2+=weights2[i]*Y_pred\n    Y_tot3+=weights3[i]*Y_pred\n    Y_tot4+=weights4[i]*Y_pred\n    Y_tot5+=weights5[i]*Y_pred\n\nRMSE.append(round(mean_squared_error((Y_test),(Y_tot1), squared=False), 4))\nRMSE.append(round(mean_squared_error((Y_test),(Y_tot2), squared=False), 4))\nRMSE.append(round(mean_squared_error((Y_test),(Y_tot3), squared=False), 4))\nRMSE.append(round(mean_squared_error((Y_test),(Y_tot4), squared=False), 4))\nRMSE.append(round(mean_squared_error((Y_test),(Y_tot5), squared=False), 4))\n\nMAE.append(round(mean_absolute_error(np.exp(Y_test), np.exp(Y_tot1)), 3))\nMAE.append(round(mean_absolute_error(np.exp(Y_test), np.exp(Y_tot2)), 3))\nMAE.append(round(mean_absolute_error(np.exp(Y_test), np.exp(Y_tot3)), 3))\nMAE.append(round(mean_absolute_error(np.exp(Y_test), np.exp(Y_tot4)), 3))\nMAE.append(round(mean_absolute_error(np.exp(Y_test), np.exp(Y_tot5)), 3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result=pd.DataFrame({'Model': models, 'RMSE': RMSE, 'MAE': MAE})\nresult","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions for the Test dataset","metadata":{}},{"cell_type":"code","source":"test=test_dataset.values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor = [LGBMRegressor(learning_rate=0.07, max_depth= 2, n_estimators= 900, reg_alpha= 0.5, reg_lambda=0.3), \n             xgb.XGBRegressor(learning_rate=0.05, n_estimators=900,reg_alpha=0.4, reg_lambda=0.2,max_depth=3), \n             Lasso(alpha=0.0008), model]\n\nweights=[0.1,0.1,0.4,0.4]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[:,(len(X1.columns)+len(X2.columns)):]= sc.transform(test[:, (len(X1.columns)+len(X2.columns)):])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_tot=0\nfor i in range(0,len(regressor),1):\n               regressor[i].fit(X_train, Y_train)\n               Y_pred = regressor[i].predict(test)\n               Y_tot+=weights[i]*Y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_tot","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred=pd.DataFrame(np.exp(Y_tot))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ID=(pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')).iloc[:,0:1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result=pd.concat([ID, Y_pred], axis=1)\nresult.columns=['ID', 'SalePrice']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.to_csv('prediction.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Possible improvements:** \n   1. Feature engineering (removing the least correlated features)\n   2. Hypertuning Stacking regressor parameters. ","metadata":{}},{"cell_type":"markdown","source":"**Please upvote if you like this kernel.**","metadata":{}}]}