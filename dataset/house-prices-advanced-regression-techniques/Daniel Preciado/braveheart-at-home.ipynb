{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from ml import simple #add custom Package the1owl\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn import linear_model, metrics, decomposition\n\ntrain = pd.read_csv('../input/train.csv')\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\ntest = pd.read_csv('../input/test.csv')\nsub = pd.read_csv('../input/sample_submission.csv')\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f13409d1af9fbf67148070def4e7bd0871369987"},"cell_type":"code","source":"col = [c for c in train.columns if c not in ['Id', 'SalePrice']]\n\ntrain['null_count'] = np.sum((train[col]==np.nan).values, axis=1)\ntest['null_count'] = np.sum((test[col]==np.nan).values, axis=1)\nfor c in col:\n    train[c + '_null'] = (train[c]==np.nan).astype(int)\n    test[c + '_null'] = (test[c]==np.nan).astype(int)\n    if train[c].dtype == 'O':\n        lbl0 = {k: v for k, v in train.groupby(by=[c], as_index=False)['SalePrice'].mean().values} \n        lbl1 = {k: v for k, v in train.groupby(by=[c], as_index=False)['SalePrice'].max().values} \n        lbl2 = {k: v for k, v in train.groupby(by=[c], as_index=False)['SalePrice'].min().values} \n        lbl3 = {k:v for v, k in enumerate(train[c].value_counts().index)}\n        lbl4 = {k: np.log1p(v) for k, v in train[c].value_counts().reset_index().values}\n        lbl5 = {k:v for v, k in enumerate(pd.concat((train[c], test[c])).value_counts().index)}\n        lbl6 = {k: np.log1p(v) for k, v in pd.concat((train[c], test[c])).value_counts().reset_index().values}\n        if len(lbl1)>100:\n            for k in lbl1:\n                train[c+'_'+str(k)] = (train[c] == k).astype(int)\n                test[c+'_'+str(k)] = (test[c] == k).astype(int)\n        for i in range(6):\n            train[c+'_key_'+str(i)] = train[c].map(eval('lbl'+str(i)))\n            test[c+'_key_'+str(i)] = test[c].map(eval('lbl'+str(i)))\n        train[c] = train[c].map(lbl6)\n        test[c] = test[c].map(lbl6)\n    else:\n        slope, intercept, r_value, p_value, std_err = stats.linregress(train[c],train['SalePrice'])\n        train[c+'_slope'] = train[c] * slope\n        test[c+'_slope'] = test[c] * slope\n        \n        train[c+'_log1p'] = np.log1p(train[c])\n        test[c+'_log1p'] = np.log1p(test[c])\n\ntrain.fillna(-1, inplace=True)\ntest.fillna(-1, inplace=True)\n\nfeature_cnt = 25\ncol = [c for c in train.columns if c not in ['Id', 'SalePrice']]\ndim_reduction_pca = decomposition.PCA(n_components=feature_cnt, random_state=10)\ndim_reduction_svd = decomposition.TruncatedSVD(n_components=feature_cnt, random_state=11)\ndim_reduction_pca_train = dim_reduction_pca.fit_transform(train[col])\ndim_reduction_svd_train = dim_reduction_svd.fit_transform(train[col])\ndim_reduction_pca_test = dim_reduction_pca.transform(test[col])\ndim_reduction_svd_test = dim_reduction_svd.transform(test[col])\nfor i in range(feature_cnt):\n    train['dim_reduction_pca_'+str(i)] = dim_reduction_pca_train[:,i]\n    test['dim_reduction_pca_'+str(i)] = dim_reduction_pca_test[:,i]\n    train['dim_reduction_svd_'+str(i)] = dim_reduction_svd_train[:,i]\n    test['dim_reduction_svd_'+str(i)] = dim_reduction_svd_test[:,i]\n\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"789bc4cbc7b4542c2f37e62c88cfbfec495a815e"},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import train_test_split\n\ncol = [c for c in train.columns if c not in ['Id', 'SalePrice']]\ncol = [c for c in col if train[c].dtype != 'O']\n\nx1, x2, y1, y2 = train_test_split(train[col].fillna(-1), train['SalePrice'], test_size=0.2, random_state=3)\netr = ExtraTreesRegressor(n_jobs=-1, random_state=3)\netr.fit(x1, y1)\nprint(np.sqrt(metrics.mean_squared_error(y2, etr.predict(x2))))\nfeature_importances = pd.DataFrame({'features':col, 'importance': etr.feature_importances_}).sort_values(by=['importance'], ascending=False)\nfeature_importances.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42977af423e885450b9d3b0d6a3de0509c86afa4"},"cell_type":"code","source":"#Testing Outlier Exclusions - need similar approach for categorical and dates\nd_imp = {k:v for k, v in feature_importances.values}\n\ndef f_hl_test(a): #test more options with the target values\n    q1 = a.quantile(0.25)\n    q3 = a.quantile(0.75)\n    iqr = q3 - q1\n    iqr_h = q3 + iqr*1.5\n    iqr_l = q1 - iqr*1.5\n    #a.min(), sum(a < iqr_l), iqr_l, q1, q3, iqr_h, sum(a > iqr_h), a.max()\n    return iqr_l, iqr_h\n\ntrain['outlier'] = 0.0\ntest['outlier'] = 0.0\nfor c in feature_importances.features:\n    l, h = f_hl_test(train[c])\n    #print(c, len(train[(train[c] < l) | (train[c] > h)]), l, h)\n    #train = train[(train[c] >= l) & (train[c] <= h)].reset_index(drop=True)\n    train['outlier'] += (((train[c] < l).astype(int) + (train[c] > h).astype(int)) > 0).astype(int) * d_imp[c]\n    test['outlier'] += (((test[c] < l).astype(int) + (test[c] > h).astype(int)) > 0).astype(int) * d_imp[c]\n\n#train = train.sort_values(by=['outlier'], ascending=False).reset_index(drop=True)\n#ocut = -30\n#outliers = train[ocut:].copy().reset_index(drop=True)\n#train = train[:ocut].copy().reset_index(drop=True)\n#train.drop(columns=['outlier'], inplace=True)\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2204a96796f80221b0f653fd3ce2811a40b94703"},"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\n\nsk_if = IsolationForest(n_jobs=-1, random_state=3)\nsk_lof = LocalOutlierFactor(novelty=True, n_jobs=-1)\ntrain['outlier_sk_if'] = sk_if.fit_predict(train[col].fillna(-1), train['SalePrice'])\nsk_lof.fit(train[col].fillna(-1), train['SalePrice'])\ntrain['outlier_sk_lof'] = sk_lof.predict(train[col].fillna(-1))\ntest['outlier_sk_if'] = sk_if.predict(test[col].fillna(-1))\ntest['outlier_sk_lof'] = sk_lof.predict(test[col].fillna(-1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ebba210530fc897a5657d7c6f7fd4b378ce464a"},"cell_type":"code","source":"col = [c for c in train.columns if c not in ['Id', 'SalePrice']]\n\n#using ml.simple shortcuts here\ndata = simple.Data(train, test, 'Id', 'SalePrice')\nparams = {'learning_rate': 0.005, 'max_depth': -1, 'boosting': 'gbdt', 'objective': 'regression', 'metric':'rmse', 'seed': 3, 'num_iterations': 5000, 'early_stopping_round': 200, 'verbose_eval': 300, 'num_leaves': 64, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}\nsub = simple.Model(data, 'LGB', params, 0.2, 7).PRED\nsub['SalePrice'] = np.expm1(sub['SalePrice'])\nsub.to_csv('submission_lgb.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"497673bccea2350b8a3d6f07ce345bb9a61084a1"},"cell_type":"code","source":"lr = linear_model.Ridge(random_state=3)\nlr.fit(train[col], train['SalePrice'])\nprint('Ridge', np.sqrt(metrics.mean_squared_error(train['SalePrice'], lr.predict(train[col]))))\n#print('Ridge on Outliers', np.sqrt(metrics.mean_squared_error(outliers['SalePrice'], lr.predict(outliers[col]))))\ntest['SalePrice'] = np.expm1(lr.predict(test[col]))\ntest[['Id','SalePrice']].to_csv('submission_ridge.csv', index=False)\n\nsub = simple.Blend(['submission_lgb.csv', 'submission_ridge.csv'], 'Id', 'SalePrice').BLEND.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03964fbeb757cf5fe802692eda6567ed13da6ac9"},"cell_type":"code","source":"xtrain = train.copy()\nfor i in range(3):\n    test.drop(columns=['SalePrice'], inplace=True)\n    sub['SalePrice'] = np.log1p(sub['SalePrice'])\n    test = pd.merge(test, sub[['Id','SalePrice']], how='left', on='Id')\n    train = pd.concat((xtrain, test), ignore_index=True, sort=False)\n\n    for c in feature_importances.features[:5]:\n        l, h = f_hl_test(train[c])\n        print(c, len(train[(train[c] < l) | (train[c] > h)]))\n        train = train[(train[c] >= l) & (train[c] <= h)].reset_index(drop=True)\n    \n    data = simple.Data(train, test, 'Id', 'SalePrice')\n    params = {'learning_rate': 0.02, 'max_depth': -1, 'boosting': 'gbdt', 'objective': 'regression', 'metric':'rmse', 'seed': 3, 'num_iterations': 1000, 'early_stopping_round': 200, 'verbose_eval': 300, 'num_leaves': 64, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}\n    sub = simple.Model(data, 'LGB', params, 0.2, 7).PRED\n    sub['SalePrice'] = np.expm1(sub['SalePrice'])\n    sub.to_csv('submission_lgb.csv', index=False)\n    \n    lr.fit(train[col], train['SalePrice'])\n    print('Ridge', np.sqrt(metrics.mean_squared_error(train['SalePrice'], lr.predict(train[col]))))\n    #print('Ridge on Outliers', np.sqrt(metrics.mean_squared_error(outliers['SalePrice'], lr.predict(outliers[col]))))\n    test['SalePrice'] = np.expm1(lr.predict(test[col]))\n    test[['Id','SalePrice']].to_csv('submission_ridge.csv', index=False)\n\n    sub = simple.Blend(['submission_lgb.csv', 'submission_ridge.csv'], 'Id', 'SalePrice', 'blend' + str(i+2).zfill(2) + '.csv').BLEND.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4628faa91b57624d8f12724f1f3a935abe1a02b6"},"cell_type":"code","source":"testing_linear = \"\"\"\nimport signal\ndef handler(signum, frame):\n    print ('Time out...', signum)\n    raise 'Time out...'\nsignal.signal(signal.SIGALRM, handler)\n\ncol = [c for c in train.columns if c not in ['Id', 'SalePrice']]\nx1, x2, y1, y2 = train_test_split(train[col], train['SalePrice'], test_size=0.2, random_state=3)\n\nmodels=[]\nfor m in linear_model.__all__:\n    try:\n        signal.alarm(10)\n        model = eval('linear_model.' + m + '(random_state=3)')\n        model.fit(x1, y1)\n        signal.alarm(0) \n        score = np.sqrt(metrics.mean_squared_error(y2, model.predict(x2)))\n        models.append([score, m, model])\n        print(m, score)\n    except:\n        print('\\t'*2, m)\n        \nmodels = sorted(models)\nprint([[s, m] for s, m, model in models])\n\"\"\"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}