{"cells":[{"metadata":{},"cell_type":"markdown","source":"## House Prices Dataset Analysis\n\n### Table of Contents\n<ol>\n<li><a href=\"#intro\">Introduction</a></li>\n<li><a href=\"#wrangling\">Data Wrangling</a></li>\n<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n<li><a href=\"#conclusions\">Conclusions</a></li>\n</ol>\n\n<a id='intro'></a>\n## 1. Introduction\n\n[The Ames Housing dataset](http://jse.amstat.org/v19n3/decock.pdf) was compiled by Dean De Cock for use in data science education and it's a great a;ternative to the Boston Housing dataset. It describes the sale of individual residential property in Ames, Iowa from 2006 to 2010. This dataset contains only residential sales within the date set, only the most recent sales data on any property. \n\nThe dataset contains 2919 observations separated in:\n* training set, with 1460 observations\n* testing set, with 1459 observations.\n\nThere are 80 features involved in assessing home values, with the target variable included. They focus on the quality and quantity of many physical attributes of the property. \n\nThe features have the following structure:\n\n#### #1 Categorical Variables:\n\nThey range from 2 to 28 classes. We should use label encoding for these categorical variables.\n\n* 23 **nominal**: typically identify various types of dwellings, garages, materials, and environmental conditions \n* 23 **ordinal**: ordinal variables typically rate various items within the property. \n\n**PID**  and **NEIGHBORHOOD** are two features of special interest.\n\nPID (Parcel Identification Number assigned to each property within the Ames Assessor’s system) \n* This number can be used in conjunction with the [Assessor’s Office](http://www.cityofames.org/assessor/) or [Beacon](http://beacon.schneidercorp.com/) websites to directly view the records of a particular observation.\n* The typical record will indicate the values for characteristics commonly quoted on most home flyers and will include a picture of the property.\n* I must say that PID number was especially useful when trying to fill the missing values.\n\n#### #2 Numeric variables:\n* 14 discrete: quantify the number of items occurring within the house:\n    * the number of kitchens, bedrooms, and bathrooms (full and half) located in the basement and above grade (ground) living areas of the home, the garage capacity, construction/remodeling dates.\n* 19 continuous (area dimensions)\n    * typical lot size and total dwelling square footage and other more specific variables are quantified in the data set like: area measurements on the basement, main living area, porches are broken down into individual categories based on quality and type.\n    \nI have compiled a spreadsheet (features.ods) with all the features with the description for each type of variable: \n\n* numeric (continuous, discrete) \n* categorical (nominal, ordinal)\n\n### Goal\n\nThe goal of this notebook is to understand the Ames Dataset in order to uncover meaningful patterns and insights and model the data to make accurate sale price predictions.\n1. First, I will assess and clean the data by: \n    * Categorize features \n    * Fill in missing values\n    * Remove outliers \n2. Perform **Exploratory Data Analysis** to visualise how our variables are distributed and how they correlate to each other.\n3. Fit the clean data to a simple **Linear Regression Model** in order to make a baseline model for further improvements. Using only two variables I was able to make a simple model with a **Coefficient of Determination  (R Squared)** of about 0.80. I first applied a log transformation on our target variable to make it normally distributed and then I fitted my input variables to the linear model. The two variables used in the regression are the Total Square Footage (`TotalBsmtS`F + `GrLivArea`) and the `Neighborhood`. On the second variable I used one-hot-encoding. The model was evaluated with `Root Mean Squared Error (RMSE)` with a value of about 0.17444 on the training set and 0.19363 on the testing set on the Kaggle House Prices Competition. \nThis is just a baseline model which has great room for improvement and creativity on feature engineering. This model used only two features and in the dataset there are 79. Also, there are other models that should be used like XGBoost, CatBoost, LightGBM, ElasticNet and others. Stacking the results of these models and hyperparameter tuning are the next steps for a second more complex model with better predictions."},{"metadata":{},"cell_type":"markdown","source":"<a id='wrangling'></a>\n## 2 Data Wrangling\n\nGetting the data I need in three steps:\n1. Gather \n2. Assess\n3. Cleaning"},{"metadata":{},"cell_type":"markdown","source":"### #2.1. Gather the Data\n\nThe data set can be found on Kaggle, the classic [\"House Prices: Advanced Regression Techniques\"](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview) competition."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \nimport os\nprint(os.listdir(\"../input\"))\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Graphics in SVG format are more sharp and legible\n%config InlineBackend.figure_format = 'svg' \n\n# Increase the default plot size and set the color scheme\nplt.rcParams['figure.figsize'] = 8, 5\nplt.rcParams['image.cmap'] = 'viridis'\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_rows', 5000)\npd.set_option('display.max_columns', 5000)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# load the dataset\nPATH_TO_DATA = '../input'\n\ndf_train = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                             'train.csv'), index_col='Id')\ndf_test = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                             'test.csv'), index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### #2 Assesing Data\n\nGeneral properties of the training and testing sets:\n* Number of samples in train/test dataset: 1460, 1459\n* Number of columns in train/test dataset: 80, 79\n* Duplicate rows in each dataset: 0\n* Datatypes: float64(3), int64(35), object(43). So, at first look we have 37 numeric variables and 43 categorical variables. Actually, there are 19 continuous features (without the target variable), 14 discrete features, 23 nominal features and 23 ordinal features\n* Features with missing values: there are 19 columns in the training set with missing values and 33 in the test set\n* Number of non-null unique values for features in training set\n* Use the `describe` function for the statistics of the dataset:\n    * the count, mean, standard deviation and the 5 number summary for each variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look at the first 5 rows of the dataset\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look at the first 5 rows of the dataset\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# number of samples and columns\ndf_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for duplicates\nsum(df_train.duplicated()), sum(df_test.duplicated())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the datatypes\ndf_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, at first look we have 37 numeric variables and 43 categorical variables. I'll pay some attention to this, as the variable types are important when modelling our data and making predictions."},{"metadata":{},"cell_type":"markdown","source":"#### Features\n\nLet's categorize our features to identify them easier for each of the variable type (numeric and categorical): continuous, discrete, nominal and ordinal."},{"metadata":{"trusted":true},"cell_type":"code","source":"# column names\ndf_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### #1.a Continuous Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# continuous variables\ncontinuous_features = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', \n                       '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n                       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\ndf_train[continuous_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the number of continuous features\ndf_test[continuous_features].shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### #1.b Discrete Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# discrete variables\ndiscrete_features = ['YearBuilt', 'YearRemodAdd', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n                     'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'MoSold', 'YrSold']\n# check the filter\ndf_train[discrete_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the number of discrete features\ndf_test[discrete_features].shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### #2.a Nominal Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nominal variables\nnominal_features = ['MSSubClass', 'MSZoning', 'Street', 'Alley', 'Utilities', 'LotConfig', 'Neighborhood', 'Condition1', \n                    'Condition2', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n                    'Foundation', 'Heating', 'CentralAir', 'Electrical', 'GarageType', 'PavedDrive', 'MiscFeature',\n                    'SaleType', 'SaleCondition']\n\n# check the filter\ndf_train[nominal_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the number of continuous features\ndf_test[nominal_features].shape[1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### #2.b Ordinal Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ordinal variables\nordinal_features = ['LotShape', 'LandContour', 'LandSlope', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', \n                    'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n                    'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', \n                    'PoolQC', 'Fence']\n\n# check the filter\ndf_train[ordinal_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the number of continuous features\ndf_test[ordinal_features].shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test to check if all the columns are included\nlist(continuous_features + discrete_features + nominal_features + ordinal_features).sort() == list(df_test.columns).sort() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Nulls in training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for missing values\ndf_train.isnull().sum()[df_train.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the number \nnuls_columns = list(df_train.isnull().sum()[df_train.isnull().sum() > 0].index)\nlen(nuls_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 19 columns with nuls in the training set. Let's identify the variable types with nuls because we have to figure out the reason why this variables are left blank in order to fill them with appropriate values.\n\nWe can see that there are some particular columns with really high number of nulls. For example, the `Alley` feature has 1369 nuls and only 91 non-nulls value. Same goes for `PoolQC`, `Fence`, `MiscFeature` and possibly `FireplaceQu`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nuls in continuous features\ncontinuous_nuls = [nul_columns for nul_columns in nuls_columns if nul_columns in continuous_features]\ncontinuous_nuls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nuls in discrete features\ndiscrete_nuls = [nul_columns for nul_columns in nuls_columns if nul_columns in discrete_features]\ndiscrete_nuls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nuls in nominal features\nnominal_nuls = [nul_columns for nul_columns in nuls_columns if nul_columns in nominal_features]\nnominal_nuls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nuls in ordinal features\nordinal_nuls = [nul_columns for nul_columns in nuls_columns if nul_columns in ordinal_features]\nordinal_nuls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the training set, we have nulls in 19 columns: 2 continuous, 1 discrete, 5 nominal and 11 ordinal columns. "},{"metadata":{},"cell_type":"markdown","source":"#### Nulls in testing set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for missing values\ndf_test.isnull().sum()[df_test.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Same for the testing set, we can see the features: `Alley`, `PoolQC`, `Fence`, `MiscFeature` and `FireplaceQu` have lots of nulls. I'll look at these variable to figure out the reason for this."},{"metadata":{"trusted":true},"cell_type":"code","source":"# the number of variables with nulls\nnuls_columns = list(df_test.isnull().sum()[df_test.isnull().sum() > 0].index)\nlen(nuls_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 14 more variables with nuls in the testing set. Let's identify them."},{"metadata":{"trusted":true},"cell_type":"code","source":"# nuls in continuous features\ncontinuous_nuls = [nul_columns for nul_columns in nuls_columns if nul_columns in continuous_features]\ncontinuous_nuls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nuls in discrete features\ndiscrete_nuls = [nul_columns for nul_columns in nuls_columns if nul_columns in discrete_features]\ndiscrete_nuls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nuls in nominal features\nnominal_nuls = [nul_columns for nul_columns in nuls_columns if nul_columns in nominal_features]\nnominal_nuls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nuls in ordinal features\nordinal_nuls = [nul_columns for nul_columns in nuls_columns if nul_columns in ordinal_features]\nordinal_nuls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the testing set, we have nuls in 33 columns: 7 continuous features, 4 discrete column, 9 nominal columns and 13 ordinal columns. "},{"metadata":{},"cell_type":"markdown","source":"#### Nulls in Categorical variables \n\nLooking at the description for each variable I found that 'NA' stands for:\n\n* `Alley`: No alley access\n* `GarageType`: No Garage\n* `MiscFeature`: None\n* `BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`: No Basement\n* `FireplaceQu`: No Fireplace\n* `Pool quality`, `PoolQC`: No pool\n* `GarageFinish`, `GarageQual`, `GarageCond`: No Garage\n* `Fence`: No Fence\n\nThese should be replaced by other variables in order to account for them when encoding our features.\n\n\nSome other features with Nulls that require further investigations: `MSZoning`, `Utilities`, `Exterior1st`, `Exterior2nd`, `MasVnrType`, `SaleType`, `Functional`."},{"metadata":{},"cell_type":"markdown","source":"#### Non-null unique values"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's filter only for discrete, nominal and ordinal features\nunique_filter =  discrete_features + nominal_features + ordinal_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# non-null unique values for ordinal features\ndf_train[ordinal_features].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# non-null unique values for nominal features\ndf_train[nominal_features].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# non-null unique values for nominal features\ndf_train[discrete_features].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[discrete_features].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# non-null unique values differences between training and testing set\ndf_diff_features = df_train[unique_filter].nunique() - df_test[unique_filter].nunique()\ndf_diff_features = df_diff_features[df_diff_features != 0]\ndf_diff_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_diff_features.plot(kind='barh', figsize=(10, 10));\nplt.title('Categorical Features Differences Training/Testing Set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see there are some differences between the unique values from training set to testing set. It's important to assess the differences since we want our model to make predictions on similar data. It is tough to predict values if we don't have training examples."},{"metadata":{},"cell_type":"markdown","source":"#### Dataset Statistics"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"# describe the dataset\ndf_train[continuous_features + ['SalePrice']].describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### #2 Cleaning the Data\n\n* The dataset is pretty clean but we do need to fill in the missing values and remove potential outliers that can affect our Linear Model."},{"metadata":{},"cell_type":"markdown","source":"#### Fillna for categorical variables\n\nFrom the dataset description, the missing values in these categorical features: 'Alley', 'GarageType', 'MiscFeature', 'FireplaceQu', 'Fence', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'PoolQC', 'GarageFinish', 'GarageQual', 'GarageCond', means `None`"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feat in ['Alley', 'GarageType', 'MiscFeature', 'FireplaceQu', 'Fence']:\n    # fill NaNs\n    df_train[feat].fillna(f'No{feat}', inplace=True)\n    df_test[feat].fillna(f'No{feat}', inplace=True)\n    print(f'{feat}...done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test for training set\ndf_train[['Alley', 'GarageType', 'MiscFeature', 'FireplaceQu', 'Fence']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for testing set \ndf_test[['Alley', 'GarageType', 'MiscFeature', 'FireplaceQu', 'Fence']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill for no basement\nfor feat in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n    # fill NaNs\n    df_train[feat].fillna(f'NoBasement', inplace=True)\n    df_test[feat].fillna(f'NoBasement', inplace=True)\n    print(f'{feat}...done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test for train\ndf_train[['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and testing set\ndf_test[['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill for no pool\ndf_train['PoolQC'].fillna(f'NoPool', inplace=True)\ndf_test['PoolQC'].fillna(f'NoPool', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['PoolQC'].isnull().sum(), df_test['PoolQC'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill for no garage\nfor feat in ['GarageFinish', 'GarageQual', 'GarageCond']:\n    # fill NaNs\n    df_train[feat].fillna(f'NoGarage', inplace=True)\n    df_test[feat].fillna(f'NoGarage', inplace=True)\n    print(f'{feat}...done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test for train\ndf_train[['GarageFinish', 'GarageQual', 'GarageCond']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# and testing set\ndf_test[['GarageFinish', 'GarageQual', 'GarageCond']].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check again for missing values\ndf_test[nominal_features+ordinal_features].isnull().sum()[df_test.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for missing values\ndf_train[nominal_features+ordinal_features].isnull().sum()[df_train.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are left now with the above variables to fill in missing values for categorical features. We can see there are more missing values in the testing set than in the training set. Let's see if we can figure out why they are not filled."},{"metadata":{},"cell_type":"markdown","source":"After looking online, I found [here]( http://www.amstat.org/publications/jse/v19n3/decock/AmesHousing.xls ) the original version of the dataset and I could easily extract the information for the `MSZoning` and other features.\nLet's make the changes manually."},{"metadata":{},"cell_type":"markdown","source":"#### MsZoning"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see the entries\ndf_test[df_test['MSZoning'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.loc[df_test[df_test['MSZoning'].isnull()].index, 'MSZoning']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# index variable\nMSZoning_null_index = list(df_test[df_test['MSZoning'].isnull()].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.loc[MSZoning_null_index[0], 'MSZoning'] = 'I'\ndf_test.loc[MSZoning_null_index[1], 'MSZoning'] = 'A'\ndf_test.loc[MSZoning_null_index[2], 'MSZoning'] = 'A'\ndf_test.loc[MSZoning_null_index[3], 'MSZoning'] = 'I'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test the changes\ndf_test['MSZoning'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['MSZoning'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['MSZoning'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Differences \nBecause there are no Industrial and Agricultural examples in our testing set, I will reassign these to \n* I -> RH (Residential High Density)\n* A -> RL (Residential Low Density)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# reassign values\ndf_test.loc[MSZoning_null_index[0], 'MSZoning'] = 'RH'\ndf_test.loc[MSZoning_null_index[1], 'MSZoning'] = 'RL'\ndf_test.loc[MSZoning_null_index[2], 'MSZoning'] = 'RL'\ndf_test.loc[MSZoning_null_index[3], 'MSZoning'] = 'RH'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot categorical feature differences between training and testing set\ndef plot_bar(feature):\n    width = 0.35\n    ind = np.arange(df_test[feature].value_counts().shape[0])\n    locations = ind + width / 2 # ytick locations\n    labels = list(df_test[feature].value_counts().index) # ytick labels\n\n    heights_test = list(df_test[feature].value_counts().values)\n    heights_train = list(df_train[feature].value_counts().values)\n    plot_test = plt.bar(ind, heights_test, width, label='Test')\n    plot_train = plt.bar(ind + width, heights_train, width, label='Train')\n\n    plt.title('{} Bar Chart'.format(feature))\n    plt.xlabel('{}'.format(feature))\n    plt.ylabel('')\n    plt.xticks(locations, labels)\n\n    plt.legend()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('MSZoning')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above bar chart we can see the two distribution for zone classification are similar."},{"metadata":{},"cell_type":"markdown","source":"#### Utilities"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test['Utilities'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# index variable\nUtilities_null_index = list(df_test[df_test['Utilities'].isnull()].index)\nUtilities_null_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign the new values\ndf_test.loc[Utilities_null_index, 'Utilities'] = 'NoSewr'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test the changes\ndf_test['Utilities'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('Utilities')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['Utilities'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Utilities'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here there are some differences between the two features, with two `NoSewr` values in the testing set and one `NoSeWa` in the training set."},{"metadata":{},"cell_type":"markdown","source":"#### Exterior1st"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test['Exterior1st'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# index variable\nExterior1st_null_index = df_test[df_test['Exterior1st'].isnull()].index[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This observation has more missing values. Let's fill them all."},{"metadata":{"trusted":true},"cell_type":"code","source":"# reassign values\ndf_test.loc[Exterior1st_null_index, 'Exterior1st'] ='PreCast'\ndf_test.loc[Exterior1st_null_index, 'Exterior2nd'] ='PreCast'\ndf_test.loc[Exterior1st_null_index, 'GarageYrBlt'] ='NoGarage'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test['Exterior2nd'].isnull()].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### SaleType"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test['SaleType'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# index variable\nSaleType_null_index = df_test[df_test['SaleType'].isnull()].index[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.loc[SaleType_null_index, 'SaleType'] ='VWD'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test['SaleType'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['SaleType'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['SaleType'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# I'll put it in the Oth category in order to have similar structure\ndf_test.loc[SaleType_null_index, 'SaleType'] ='Oth'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('SaleType')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### KitchenQual"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test['KitchenQual'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# index variable\nKitchenQual_null_index = df_test[df_test['KitchenQual'].isnull()].index[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_test.loc[KitchenQual_null_index, 'KitchenQual'] = 'Po'\n# reassign value to match distributions\ndf_test.loc[KitchenQual_null_index, 'KitchenQual'] = 'Fa'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['KitchenQual'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['KitchenQual'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Functional"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test['Functional'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# index variable\nFunctional_null_index = list(df_test[df_test['Functional'].isnull()].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.loc[Functional_null_index[0], 'Functional'] = 'Sev'\ndf_test.loc[Functional_null_index[1], 'Functional'] = 'Sev'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These where Sal (Salvage Only) but in order to have the same structure I put them in Sev (Severely Damaged)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Functional'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['Functional'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('Functional')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Electrical"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['Electrical'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# index variable\nElectrical_null_index = df_train[df_train['Electrical'].isnull()].index[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[Electrical_null_index, 'Electrical'] = 'SBrkr'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Electrical'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['Electrical'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Nulls in Numeric variables\n\nThere are also Nuls in continuous variables:\n* `LotFrontage`, `MasVnrArea`, `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF`, `TotalBsmtSF`, `GarageArea`: maybe fill them with the mean values to not affect the distributions of values.\n\nDiscrete Null values:\n* `BsmtFullBath`, `BsmtHalfBath`, `GarageYrBlt`, `GarageCars`"},{"metadata":{},"cell_type":"markdown","source":"#### MasVnrType  and MasVnrArea\n\n* MasVnrArea - None\n* MasVnrType - 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['MasVnrType'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test['MasVnrType'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# index variable\nMasVnrType_null_index = list(df_train[df_train['MasVnrType'].isnull()].index)\nMasVnrType_null_index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The properties with `None`, `MasVnrType` have `0`, `MasVnrArea`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign values\ndf_train.loc[MasVnrType_null_index, 'MasVnrArea'] = 0\ndf_train.loc[MasVnrType_null_index, 'MasVnrType'] = 'None'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MasVnrType_null_index = list(df_test[df_test['MasVnrType'].isnull()].index)\n\ndf_test.loc[MasVnrType_null_index, 'MasVnrArea'] = 0\ndf_test.loc[MasVnrType_null_index, 'MasVnrType'] = 'None'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_bar('MasVnrType')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Continuous Features Nulls"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[continuous_features + discrete_features].isnull().sum()[df_test[continuous_features + discrete_features].isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[continuous_features + discrete_features].isnull().sum()[df_train[continuous_features + discrete_features].isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LotFrontage"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill LotFrontage with mean\ndf_train['LotFrontage'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_train['LotFrontage'], bins=30, alpha=0.5, label='Train set')\nplt.hist(df_test['LotFrontage'], bins=30, alpha=0.5, label='Test set')\n\nplt.title(\"LotFrontage Histogram Train/Test\")\nplt.xlabel('LotFrontage ($ft$)')\nplt.ylabel('Frequency')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# median value LotFrontage\nLotFrontage_null_fill = df_train['LotFrontage'].mode()[0]\n# fill nans for training and testing set\ndf_train['LotFrontage'].fillna(LotFrontage_null_fill, inplace=True)\ndf_test['LotFrontage'].fillna(LotFrontage_null_fill, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['LotFrontage'].isnull().sum(), df_test['LotFrontage'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GarageYrBlt"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['GarageYrBlt'].isnull()][:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GarageYrBlt_null_vals_train = list(df_train[df_train['GarageYrBlt'].isnull()].index)\nGarageYrBlt_null_vals_test = list(df_test[df_test['GarageYrBlt'].isnull()].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.loc[GarageYrBlt_null_vals_train, 'GarageYrBlt'] = 'NoGarage'\ndf_test.loc[GarageYrBlt_null_vals_test, 'GarageYrBlt'] = 'NoGarage'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['GarageYrBlt'].isnull().sum(), df_test['GarageYrBlt'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[continuous_features + discrete_features].isnull().sum()[df_test[continuous_features + discrete_features].isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GarageArea & GarageCars"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test['GarageArea'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GarageArea_null_index = df_test[df_test['GarageArea'].isnull()].index[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.loc[GarageArea_null_index, 'GarageCars'] = 0\ndf_test.loc[GarageArea_null_index, 'GarageArea'] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### BsmtFinSF1, BsmtFinSF2, BsmtUnfSF,  TotalBsmtSF, BsmtFullBath, BsmtHalfBath"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[df_test['BsmtFullBath'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BsmtFullBath_nulls_index = list(df_test[df_test['BsmtFullBath'].isnull()].index)\nmask_bsm = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',  'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']\ndf_test.loc[BsmtFullBath_nulls_index, mask_bsm] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test[mask_bsm].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check to see if we cleaned for nulls\ndf_train.isnull().sum()[df_train.isnull().sum() > 0], df_test.isnull().sum()[df_train.isnull().sum() > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Remove Outliers\n\n* Remove from the training set and testing set the observations with a `GrLivArea` > 4,000. It is important to delete these values as Linear Regression is sensitive to outliers. \n* We have to be aware that there is an extreme data point in the testing set as well which we cannot remove.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see the four observations with GrLivArea bigger the 4000 \n# from the training set\ndf_test[df_test['GrLivArea'] > 4000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see the four observations with GrLivArea bigger the 4000 \n# from the training set\ndf_train[df_train['GrLivArea'] > 4000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the rows with extreme values\ndf_train.drop(df_train[df_train['GrLivArea'] > 4000].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test the change\ndf_train[df_train['GrLivArea'] > 4000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reset index\ndf_train = df_train.reset_index(drop=True)\n# check the shape of our dataframe\ndf_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rename the index column\ndf_train.index.names = ['Id'] ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save this for later\ndf_train.to_csv(os.path.join('train_clean.csv'), sep=',', index_label='Id')\ndf_test.to_csv(os.path.join('test_clean.csv'), sep=',', index_label='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(os.path.join( \n                                             'train_clean.csv'), index_col='Id')\ndf_test = pd.read_csv(os.path.join( \n                                             'test_clean.csv'), index_col='Id')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='eda'></a>\n## Exploratory Data Analysis\n\nLet’s visualize the information in our dataset by finding correlation between our variables and see how the data is distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_train['SalePrice'], bins=30)\nplt.title(\"Sale Price Histogram\")\nplt.xlabel('SalePrice ($USD$)')\nplt.ylabel('Frequency')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Sale Price Histogram is right skewed, ranging from 34,900 USD to 755,000 USD. The median sale price is 163,000 which might indicate there are some potential outliers or extreme values that can cause bias to our model if we don't eliminate them."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(np.log(df_train['SalePrice']), bins=30)\nplt.title(\"Sale Price (Log Transformation) Histogram\")\nplt.xlabel('$log(SalePrice)$ ($USD$)')\nplt.ylabel('Frequency')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Applying a log transformation on the Sale Price we can now see that our distribution is now more normal."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_train['GrLivArea'], bins=30)\nplt.title(\"Above Ground Living Area Square Feet Histogram\")\nplt.xlabel('GrLivArea ($ft^{2}$)')\nplt.ylabel('Frequency')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`GrLivArea` (above ground living area square feet) ranges from 334 to a maximum value of 3627 $ft^{2}$. From the above histogram of Above Ground Living Area we can see that most properties have a living area between 1,128 and 1,775 $ft^{2}$, with a median 1,458 $ft^{2}$.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df_train.GrLivArea, df_train.SalePrice)\nplt.title(\"Above Ground Living Area Square Feet vs Sale Price (training set)\")\nplt.xlabel('GrLivArea')\nplt.ylabel('Sale Price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing the outliers now we can better see the strong linear relation between Sale Price and Above Ground Living Area."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(df_train['GrLivArea'], bins=30, alpha=0.5, label='Train set')\nplt.hist(df_test['GrLivArea'], bins=30, alpha=0.5, label='Test set')\n\nplt.title(\"Above Ground Living Area Square Feet Histogram Train/Test\")\nplt.xlabel('GrLivArea ($ft^{2}$)')\nplt.ylabel('Frequency')\n\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above histogram, we can see that the training and testing set distributions of Above Ground Living Area are similar with the difference that in the testing set there is one extreme value."},{"metadata":{},"cell_type":"markdown","source":"#### Sale Price Over Time Period"},{"metadata":{"trusted":true},"cell_type":"code","source":"# group data by year and month\ndf_time_price = df_train.groupby(['YrSold', 'MoSold'], as_index=False)['SalePrice'].mean()\n# see the first rows\ndf_time_price.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We don't have 2010 data for all year round. Therefore, we need to create some 0 data for vizualization purpose and append it to our dataframe.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_2010 = df_time_price[df_time_price['YrSold'] == 2010]['SalePrice'].mean()\nmean_2010","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create data with median value\nnew_dummy_df2010 = pd.DataFrame(np.array([[2010, 8, mean_2010], [2010, 9, mean_2010], [2010, 10, mean_2010], \n                       [2010, 11, mean_2010], [2010, 12, mean_2010]]),\n                   columns=['YrSold', 'MoSold', 'SalePrice'], index=[55, 56, 57, 58, 59])\nnew_dummy_df2010","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# append the new data\ndf_time_price = df_time_price.append(new_dummy_df2010)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"years = list(range(2006,2011))\nlabels = list(range(1, 13))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8, 6))\n\nfor year in years:\n    plt.plot(labels, df_time_price[df_time_price['YrSold'] == year]['SalePrice'], label=year)\n\nplt.title('Sale Price During Time Period')\nplt.xticks(labels)\nplt.xlabel('Months')\nplt.ylabel('Sale Price')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above line plot we can see monthly average `SalePrice` from 2006 to 2010. We don't have data for 8-12 months of 2010, so they are filled with the mean value. The biggest sales with a mean value of more then 220,000 USD were recorded in September of 2006 while the worst month was in July of 2010. We can see that, typically, the last four months of the year are more profitable."},{"metadata":{},"cell_type":"markdown","source":"#### Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate correlation matrix\ncorr = df_train[continuous_features + ['SalePrice']].corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(12, 12))\n#Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n#Generate Heat Map, allow annotations and place floats in map\ng = sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n#Apply xticks\n# plt.xticks(range(len(corr.columns)), corr.columns);\n# #Apply yticks\n# plt.yticks(range(len(corr.columns)), corr.columns)\n# #show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above carrelation matrix, we can see that our target variable, `SalePrice`, has a strong positive linear relationship with `GrLivArea`. ALso, a moderate positive linear relationship with:  \n* `TotalBsmtSF`(0.65)\n* `GarageArea`(0.64)\n* `1stFlrSF`(0.63) + `2ndFlrSF`(0.3) + `LowQualFinSF`= GrLivArea (that's why they correlate with each)\n* `MasVnrArea`(0.47)\n* `BsmtFinSF1` (0.4)\n* `LotFrontage` (0.34)\n* `OpenPorchSF` (0.33)\n* `WoodDeckSF` (0.32)\n\nBsmtFinSF2 + BsmtUnfSF = TotalBsmtSF"},{"metadata":{"trusted":true},"cell_type":"code","source":"imp_cont_features = ['GrLivArea', 'TotalBsmtSF', 'GarageArea', 'MasVnrArea', 'LotFrontage', \n                     'OpenPorchSF', 'WoodDeckSF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%config InlineBackend.figure_format = 'png' \nsm = pd.plotting.scatter_matrix(df_train[imp_cont_features + ['SalePrice']], figsize=(30, 30), diagonal='kde');\n\nfor ax in sm.ravel():\n    ax.set_xlabel(ax.get_xlabel(), fontsize = 20, rotation = 45)\n    ax.set_ylabel(ax.get_ylabel(), fontsize = 20, rotation = 0)\n\n#May need to offset label when rotating to prevent overlap of figure\n[s.get_yaxis().set_label_coords(-0.5,0.5) for s in sm.reshape(-1)]\n\n#Hide all ticks\n[s.set_xticks(()) for s in sm.reshape(-1)]\n[s.set_yticks(()) for s in sm.reshape(-1)]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the the above scatter matrix we can see the distributions of the features correlated to our target variable as well as  the density plot for each variable."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"for feat in imp_cont_features:\n    plt.scatter(df_train[feat], np.log(df_train.SalePrice))\n    plt.xlabel(feat)\n    plt.ylabel('$log(Sale Price)$')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Baseline Model\n\nLet's try to fit a Linear Regression by simply taking into consideration the neighborhood and total square footage:\n* Neighborhood\n* TotalBsmtSF + GrLivArea"},{"metadata":{},"cell_type":"markdown","source":"#### TotalSquareFootage"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['TotalSquareFootage'] = df_train['GrLivArea'] + df_train['TotalBsmtSF']\ndf_test['TotalSquareFootage'] = df_test['GrLivArea'] + df_test['TotalBsmtSF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df_train['TotalSquareFootage'], np.log(df_train.SalePrice))\nplt.xlabel('TotalSquareFootage')\nplt.ylabel('$log(Sale Price)$')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['logSalePrice'] = np.log(df_train.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate correlation matrix\ncorr = df_train[['TotalSquareFootage', 'SalePrice']].corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(3,3))\n#Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n#Generate Heat Map, allow annotations and place floats in map\ng = sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n#Apply xticks\n# plt.xticks(range(len(corr.columns)), corr.columns);\n# #Apply yticks\n# plt.yticks(range(len(corr.columns)), corr.columns)\n# #show plot\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see here that SalePrice has a strong positive linear relation with TotalSquareFootage, with a Correlation Coefficient of 0.82. I will use this continuous variable in my first simple model."},{"metadata":{},"cell_type":"markdown","source":"### Fit a Simple Linear Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### #1 Create an intercept"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['intercept'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_train[['intercept', 'TotalSquareFootage']]\ny = df_train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predicting the price and add all of our var that are quantitative\nlm = sm.OLS(y, X)\nresults = lm.fit()\nresults.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based only on the total square footage we get a R-squared of 0.674. This means that 67,4 % of the variability in SalePrice is explained by `TotalSquareFootage`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# these are our cofficients for our function\nnp.dot(np.dot(np.linalg.inv(np.dot(X.transpose(), X)) , X.transpose()), y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"$yhat = 82.80489695x - 31594.19591877$"},{"metadata":{},"cell_type":"markdown","source":"#### #Add Neighborhood Dummies"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Neighborhood'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create neighborhood dummies\nneighborhood_dummies = pd.get_dummies(df_train['Neighborhood'])\nneighborhood_dummies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fit a regression model with Bloomington Heights as Baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"# select all the columns but the first\nneighborhood_columns = list(neighborhood_dummies.columns[1:])\nneighborhood_dummies[neighborhood_columns].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.join(neighborhood_dummies)\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = df_train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lm2 = sm.OLS(y, X[['intercept', 'TotalSquareFootage'] + neighborhood_columns])\nresults2 = lm2.fit()\nresults2.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'{0:.10f}'.format(-4.119e+04)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Conclusions for Neighborhood Blmngtn\n\n1. 79.6% of the variability in price can be explained by the linear model built using total square footage and neighborhood.\n2. For each additional unit increase in TotalSquareFootage, the price is expected to increase by 62 dollars as long as all the other variables stay the same.\n3. We expect that a house in NridgHt will cost 75310 more than a house in Blmngtn, all else being equal.\n4.  We expect that a house in SWISU will cost 41190 less than a house in Blmngtn, all else being equal."},{"metadata":{},"cell_type":"markdown","source":"#### sklearn\n\n* let's use the sklearn Ordinary Least Squares Linear Regression\n* fit the logarithm of SalePrice: `logSalePrice`"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.drop(columns=['intercept'], inplace=True)\ny = df_train['logSalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LinearRegression()\n# fit training data\nreg.fit(X, y)\n# get the R^2\nreg.score(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the coefficients\nreg.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the intercept\nreg.intercept_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions\npred = reg.predict(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Evaluate our base model"},{"metadata":{},"cell_type":"markdown","source":"Let's calculate [Root-Mean-Squared-Error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) \n* between the logarithm of the predicted value and the logarithm of the observed sales price. \n\n**Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate RMSE\ndef rmse(y, pred):\n    return np.sqrt(mean_squared_error(y, pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# error\nrmse(y, pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate RMSE\n# np.sqrt(mean_squared_error(y, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submit Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the dataset\nPATH_TO_DATA = '../input'\n\nsub = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                             'sample_submission.csv'), index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create neighborhood dummies\nneighborhood_dummies_test = pd.get_dummies(df_test['Neighborhood'])\nneighborhood_dummies_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = df_test[['TotalSquareFootage']]\nX_test = X_test.join(neighborhood_dummies_test)\nX_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions\npred_test = reg.predict(X_test)\n# exponentiate the results\npred_test = np.exp(pred_test)\npred_test[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['SalePrice'] = pred_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(pred_test, bins=40);\nplt.title('Distribution of SalePrice predictions');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('model1.csv')\n# load the dataset\n\nmodel1_sub = pd.read_csv(os.path.join( \n                                             'model1.csv'), index_col='Id')\nmodel1_sub.head() # 0.19363","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"conclusions\"></a>\n## Conclusions\n\nFitting the clean data to a simple **Linear Regression Model** in order to make a baseline model for further improvements. Using only two variables I was able to make a simple model with a **Coefficient of Determination  (R Squared)** of about 0.80. I first applied a log transformation on our target variable to make it normally distributed and then I fitted my input variables to the linear model. The two variables used in the regression are the Total Square Footage (`TotalBsmtS`F + `GrLivArea`) and the `Neighborhood`. On the second variable I used one-hot-encoding. The model was evaluated with `Root Mean Squared Error (RMSE)` with a value of about 0.17444 on the training set and 0.19363 on the testing set on the Kaggle House Prices Competition. \nThis is just a baseline model which has great room for improvement and creativity on feature engineering. This model used only two features and in the dataset there are 79. Also, there are other models that should be used like XGBoost, CatBoost, LightGBM, ElasticNet and others. Stacking the results of these models and hyperparameter tuning are the next steps for a second more complex model with better predictions."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}