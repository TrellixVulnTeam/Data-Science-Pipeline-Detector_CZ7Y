{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Price Kaggle Competition"},{"metadata":{},"cell_type":"markdown","source":"Importing some libraries and reading train and test datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, StackingRegressor\nfrom sklearn.linear_model import LinearRegression, Lasso, LassoCV, Ridge, RidgeCV, ElasticNet, ElasticNetCV\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.svm import SVR\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\ntotal = [train, test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This dataset has 81 columns with 38 numeric variables and 43 text variables. I won't explain every variable in this notebook, but I'll give some intuitions behind every move I did in order to clean the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Int64 columns are: ' + str(len(train.loc[:,train.dtypes == np.int64].columns)))\nprint('Str columns are: ' + str(len(train.loc[:,train.dtypes == np.object].columns)))\nprint('Float64 columns are: ' + str(len(train.loc[:,train.dtypes == np.float64].columns)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that columns like PoolQC, MiscFeature, Alley and Fence have an high rate of missing values. We won't delete this columns because we can keep some information and we can use them for creating other features."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum().sort_values(ascending=False)[train.isnull().sum().sort_values(ascending=False) > 0] / train.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum().sort_values(ascending=False)[test.isnull().sum().sort_values(ascending=False) > 0] / test.shape[0] * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the SalePrice, our target variable, we can see that it's highly positive skewed and leptokurtic. So this distribution is far from a normal distribution. Let's log(1+x) transform the dependent variable and solve normality issues."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train.SalePrice)\nsns.distplot(np.random.normal(train.SalePrice.mean(), train.SalePrice.std(), 1000))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Skewness: ', train.SalePrice.skew())\nprint('Kurtosis: ', train.SalePrice.kurt())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['SalePrice'] = np.log1p(train['SalePrice'])\nsns.distplot(train.SalePrice)\nsns.distplot(np.random.normal(train.SalePrice.mean(), train.SalePrice.std(), 1000), color='green')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Skewness: ', train.SalePrice.skew())\nprint('Kurtosis: ', train.SalePrice.kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These numerical variable are categorical so we can transform them in object type."},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in total:\n    dataset['MSSubClass'] = dataset['MSSubClass'].astype(np.object)\n    dataset['MoSold'] = dataset['MoSold'].astype(np.object)\n    dataset['YrSold'] = dataset['YrSold'].astype(np.object)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Evaluating model performances, highly skewed variables affected results negatively, so I applied the same transformation made on SalePrice."},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = train.loc[:,train.dtypes == np.int64].columns.append(train.loc[:,train.dtypes == np.float64].columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skew_feats = []\nfor feat in numeric_features:\n    if train[feat].skew() > 0.75:\n        skew_feats.append(feat)\n        \nfor dataset in total:\n    for feat in skew_feats:\n        dataset[feat] = dataset[feat].apply(np.log1p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I mapped all the object type variables that had ordinality, for example variables that have an evaluation scale. I filled other missing data with the mode of the variable or 0. This cell is a summary of all the work done to clean the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"mapping = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, np.nan:0}\nmapping_1 = {'Gd': 4, 'Av': 3, 'Mn':2, 'No':1, np.nan: 0}\nmapping_2 = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, np.nan: 0}\nmapping_3 = {'Typ': 7, 'Min1': 6, 'Min2': 5, 'Mod': 4, 'Maj1': 3, 'Maj2': 2, 'Sev': 1, 'Sal': 0}\nmapping_4 = {'Fin': 3, 'RFn': 2, 'Unf': 1, np.nan: 0}\nmapping_5 = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, np.nan: 0}\n\nfor dataset in total:\n    for column in ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']:\n        dataset[column] = dataset[column].map(mapping)\n    dataset['BsmtExposure'] = dataset['BsmtExposure'].map(mapping_1)\n    dataset['BsmtFinType1'] = dataset['BsmtFinType1'].map(mapping_2)\n    dataset['BsmtFinType2'] = dataset['BsmtFinType2'].map(mapping_2)\n    dataset['Functional'] = dataset['Functional'].map(mapping_3)\n    dataset['GarageFinish'] = dataset['GarageFinish'].map(mapping_4)\n    dataset['Fence'] = dataset['Fence'].map(mapping_5)\n    dataset[['LotFrontage','GarageYrBlt','MasVnrArea','BsmtFullBath','BsmtHalfBath']] = dataset[['LotFrontage','GarageYrBlt','MasVnrArea','BsmtFullBath','BsmtHalfBath']].fillna(0)\n    dataset[['MiscFeature','Alley','GarageType']] = dataset[['MiscFeature','Alley','GarageType']].fillna('No')\n    dataset['MasVnrType'] = dataset['MasVnrType'].fillna('None')\n\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\ntest.at[1150, 'MasVnrType'] = 'BrkFace'\ntest.at[1116, 'GarageCars'] = 0\ntest.at[1116, 'GarageArea'] = 0\ntest.at[1116, 'GarageType'] = 0\ntest['BsmtFinSF1'] = test['BsmtFinSF1'].fillna(0)\ntest['BsmtUnfSF'] = test['BsmtUnfSF'].fillna(0)\ntest['BsmtFinSF2'] = test['BsmtFinSF2'].fillna(0)\ntest['TotalBsmtSF'] = test['TotalBsmtSF'].fillna(0)\ntest['MSZoning'] = test['MSZoning'].fillna(train['MSZoning'].mode()[0])\ntest['Utilities'] = test['Utilities'].fillna(train['Utilities'].mode()[0])\ntest['Functional'] = test['Functional'].fillna(train['Functional'].mode()[0])\ntest['SaleType'] = test['SaleType'].fillna(train['SaleType'].mode()[0])\ntest['Exterior1st'] = test['Exterior1st'].fillna(train['Exterior1st'].mode()[0])\ntest['Exterior2nd'] = test['Exterior2nd'].fillna(train['Exterior2nd'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I added some new variables:\n- HasPool: 1 if house has pool, 0 otherwise\n- Has2ndFloor: 1 if house has 2nd floor, 0 otherwise\n- HasGarage: 1 if house has garage, 0 otherwise\n- HasBsmt: 1 if house has basement, 0 otherwise\n- HasFireplace: 1 if house has fireplace, 0 otherwise\n- BltSoldYrDiff: time passed between year of built and last sale year in years\n- TotalSF: house total surface\n- TotalBathr: total bathrooms\n- TotalPorchSF: porch total surface"},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in total:\n    dataset['HasPool'] = dataset['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['Has2ndFloor'] = dataset['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['HasGarage'] = dataset['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['HasBsmt'] = dataset['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['HasFireplace'] = dataset['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['BltSoldYrDiff'] = dataset['YrSold'].astype(np.int64) - dataset['YearBuilt']\n    dataset['TotalSF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']\n    dataset['TotalBathr'] = dataset['FullBath'] + 0.5 * dataset['HalfBath'] + dataset['BsmtFullBath'] + 0.5 * dataset['BsmtHalfBath']\n    dataset['TotalPorchSF'] = dataset['OpenPorchSF'] + dataset['3SsnPorch'] + dataset['EnclosedPorch'] + dataset['ScreenPorch'] + dataset['WoodDeckSF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = ['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n       'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n       '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath',\n       'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr',\n       'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n       'MiscVal', 'LotFrontage', 'MasVnrArea', 'GarageYrBlt', 'SalePrice', 'BltSoldYrDiff',\n        'TotalSF', 'TotalBathr', 'TotalPorchSF']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plotting all variables vs target variable to check for some outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[:5])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[5:10])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[10:15])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[15:20])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[20:25])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[25:30])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[30:35])\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=numeric_features[35:40])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removing some outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data=train, y_vars=['SalePrice'], x_vars=['OverallQual','OverallCond','OpenPorchSF','TotalPorchSF'])\ntrain = train.drop(train[(train['OverallQual'] == 10) & (train['SalePrice'] < 12.5)].index)\ntrain = train.drop(train[(train['OverallCond'] == 2) & (train['SalePrice'] > 12)].index)\ntrain = train.drop(train[(train['OpenPorchSF'] > 3.5) & (train['SalePrice'] < 11)].index)\ntrain = train.drop(train[(train['TotalPorchSF'] > 6) & (train['SalePrice'] < 11)].index)\nsns.pairplot(data=train, y_vars=['SalePrice'], x_vars=['OverallQual','OverallCond','OpenPorchSF','TotalPorchSF'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum().sort_values(ascending=False)[train.isnull().sum().sort_values(ascending=False) > 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum().sort_values(ascending=False)[test.isnull().sum().sort_values(ascending=False) > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting dummy variables for all categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = train.loc[:,train.dtypes == np.object].columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummies = pd.get_dummies(data=pd.concat([train, test]), columns=columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummies.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = df_dummies.iloc[:train.shape[0]]\ntest = df_dummies.iloc[train.shape[0]:].drop('SalePrice', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Standard scaling data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.DataFrame(StandardScaler().fit_transform(train), columns=train.columns).drop(['Id', 'SalePrice'], axis=1)\ny = train['SalePrice']\nscaled_test = pd.DataFrame(StandardScaler().fit_transform(test),columns=test.columns).drop('Id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transforming categorical variables into dummies, increased the numeber of columns, so we need to do some feature selection. In this case I did it in three ways, using Ridge, Lasso and ElasticNet models."},{"metadata":{},"cell_type":"markdown","source":"Ridge keeps 270 of 272 variables, so we won't use it."},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = RidgeCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in RidgeCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in RidgeCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)\nprint(\"Ridge picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ElasticNet keeps 98 variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = ElasticNetCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in ElasticNetCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in ElasticNetCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)\nprint(\"ElasticNet picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\nridge_coef = coef[coef != 0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enet_coef = coef[coef != 0]\nimp_coef = enet_coef.sort_values()\nimport matplotlib\nplt.figure(figsize=(8,18))\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using ElasticNet Model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lasso keeps 96 variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = LassoCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso_coef = coef[coef != 0]\nimp_coef = lasso_coef.sort_values()\nimport matplotlib\nplt.figure(figsize=(8,18))\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test all these models using the Ridge, ElasticNet and Lasso feature selection."},{"metadata":{"trusted":true},"cell_type":"code","source":"models = [('DTR', DecisionTreeRegressor()),\n          ('RFR', RandomForestRegressor()),\n          ('KNR', KNeighborsRegressor()),\n          ('GBR', GradientBoostingRegressor()),\n          ('LR', LinearRegression()),\n          ('XGB', XGBRegressor()),\n          ('LGBM', LGBMRegressor()),\n          ('SVR', SVR()),\n          ('Ridge', Ridge(alpha=10)),\n          ('Lasso', Lasso(alpha=0.003487)),\n          ('ENet', ElasticNet(alpha=0.006974))]\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that with Lasso feature selection we generally meet better performances by negative mean squared error. Using features picked by Lasso model, we can exclude some models: Decision Tree, Linear Regression, K-nearest neighbour, Support Vector Regression and Random Forest. XGBoost, LightGBM and Gradient Boosting can be good but they need hyperparameter tuning to reach better performances. I also excluded Ridge because better performances are reached at higher value of the alpha hyperparameter: high values of alpha (like 10 or more) highly reduce the complexity of the model fit, so it's almost averaging the points, giving bad performances on the test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\n\nfor coef in [ridge_coef.index, enet_coef.index, lasso_coef.index]:\n    for name, model in models:\n        kfold = KFold(n_splits=10, random_state=21)\n        cv_results = cross_val_score(model, X[coef], y, cv=kfold, scoring='neg_mean_squared_error')\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I searched the best alpha from 1e-10 and 1 resulting to be 0.001"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'alpha': [0.001]\n    }\n\nreg = Lasso()\nrs = GridSearchCV(estimator = reg, param_grid = params, \n                               cv = 10, verbose= 5, n_jobs = -1, scoring='neg_mean_squared_error')\nrs.fit(X[lasso_coef.index],y)\nprint(rs.best_score_)\nprint(rs.best_estimator_)\nlasso = rs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Gradient Boosting hyperparameter tuning, I started finding the best n_estimator for 0.1 learning rate in order to reduce the numerical computation of high n_estimators, then i tuned in a few steps:\n- max_depth and min_samples_split together\n- min_samples_split and min_sample_leaf together\n- max_features\n- subsample\n\nAfter all I reduced learning rate to 0.01 increasing the n_estimetors, to reach better performances.\n\nHyperparameters detailed explanation can be found here:\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n\nI found really helpful for hyperparameter tuning detailed explanation this page:\nhttps://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'learning_rate': [0.01],\n    'n_estimators': [2000],\n    'max_depth': [11],\n    'min_samples_split': [200],\n    'min_samples_leaf': [10],\n    'max_features': ['sqrt'],\n    'subsample': [0.85]\n    }\n\nreg = GradientBoostingRegressor()\nrs = GridSearchCV(estimator = reg, param_grid = params, \n                               cv = 10, verbose= 5, n_jobs = -1, scoring='neg_mean_squared_error')\nrs.fit(X[lasso_coef.index],y)\nprint(rs.best_score_)\nprint(rs.best_estimator_)\ngbr = rs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For Elastic Net, I used the same technique used for Lasso."},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'alpha': [0.001]\n    }\n\nreg = ElasticNet()\nrs = GridSearchCV(estimator = reg, param_grid = params, \n                               cv = 10, verbose= 5, n_jobs = -1, scoring='neg_mean_squared_error')\nrs.fit(X[lasso_coef.index],y)\nprint(rs.best_score_)\nprint(rs.best_estimator_)\nelasticnet = rs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For XGBoost and LightGBM I proceeded in the same way of Gradient Boosting for the hyperparameter optimization, some hyperparameters are different.\n\nI found really helpful this page for the tuning procedure:\nhttps://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n\nHere you can find a deep explanation of XGBoost parameters:\nhttps://xgboost.readthedocs.io/en/latest/parameter.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'learning_rate': [0.01],\n    'n_estimators': [3000],\n    'max_depth': [3],\n    'min_child_weight': [5],\n    'gamma': [0],\n    'colsample_bytree': [0.65],\n    'subsample': [0.6],\n    'reg_alpha':[1e-6]\n    }\n\nreg = XGBRegressor()\nrs = GridSearchCV(estimator = reg, param_grid = params, \n                               cv = 10, verbose= 5, n_jobs = -1, scoring='neg_mean_squared_error')\nrs.fit(X[lasso_coef.index],y)\nprint(rs.best_score_)\nprint(rs.best_estimator_)\nxgboost = rs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LightGBM is not giving good performances so I won't use it for final predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'learning_rate': [0.01],\n    'n_estimators': [3000],\n    'max_depth': [3],\n    'min_child_weight': [1],\n    'gamma': [0],\n    'colsample_bytree': [0.8],\n    'subsample': [0.6],\n    }\n\nreg = LGBMRegressor()\nrs = GridSearchCV(estimator = reg, param_grid = params, \n                               cv = 10, verbose= 5, n_jobs = -1, scoring='neg_mean_squared_error')\nrs.fit(X[lasso_coef.index],y)\nprint(rs.best_score_)\nprint(rs.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I submitted the results of every model I tuned and the results were good for the Gradient Boosting and XGBoost, but the best result for me (0.12173) in the Kaggle competition (top 13% of the leaderboard) was given by stacking three models: this is an ensemble technique, ElasticNet and Gradient Boost are trained individually on the training set, then their predictions are stacked to fit a final estimator, that in this case is Lasso."},{"metadata":{"trusted":true},"cell_type":"code","source":"level_0 = [('ENet',elasticnet),('GBR', gbr)]\nlevel_1 = lasso\n\nmodel = StackingRegressor(estimators=level_0, final_estimator=level_1, cv=10)\n\ncv = KFold(n_splits=10, random_state=21)\nscores = cross_val_score(model, X[lasso_coef.index],y,cv=cv,scoring='neg_mean_absolute_error')\nprint(scores.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X[lasso_coef.index],y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_stacked = model.predict(scaled_test[lasso_coef.index])\npred = np.expm1(pred_stacked)\nsub = test[['Id']]\nsub['SalePrice'] = pred\nsub[['Id', 'SalePrice']].to_csv('pred_submission.csv', index=False, encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}