{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nfrom sklearn.exceptions import DataConversionWarning, ConvergenceWarning\nwarnings.filterwarnings(action='ignore')\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# House Prices \n<img src=\"http://bridgingandcommercial.co.uk/cms/upload/image/dab1369294471_213_126864066.jpg\">\nData set is [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nTRAIN_PATH = '../input/house-prices-advanced-regression-techniques'\nTEST_PATH = '../input/house-prices-advanced-regression-techniques'\ndef load_houses_data(TRAIN_PATH=TRAIN_PATH, TEST_PATH=TEST_PATH):\n    train_csv = os.path.join(TRAIN_PATH, 'train.csv')\n    test_csv = os.path.join(TEST_PATH, 'test.csv')\n    return pd.read_csv(train_csv), pd.read_csv(test_csv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test = load_houses_data()\ny_train = X_train['SalePrice']\ny_train_first = y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train.info()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train.std()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train.hist(figsize=(20, 20), bins=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['3SsnPorch'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(X_train['BedroomAbvGr'].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.groupby('BedroomAbvGr').count()['Id']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train.groupby('BsmtFullBath').count()['Id']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train[['TotalBsmtSF', 'BsmtFinSF2', 'BsmtFinSF1', 'BsmtUnfSF']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[['GarageType']].info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.groupby('GarageFinish').count()['Id'] # Let it be in our dataset","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"X_train.corr()['GarageArea']['GarageCars']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(X_train['LotArea'], bins=100)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train.corr()['SalePrice'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train[['MasVnrArea']].hist(bins=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* should drop id.\n* almost all 3SsnPorch are zero and we can delete it.\n* we can group rooms 5 or more into one single column.\n* can merge full and half bathrooms into one filed name bathroom\n* we can drop two type of finished and have one finished basement feet\n* we can change basement unfinished into a fraction of finished/unfinished (It got bad correlation)\n* can merge porchs into one field.\n* garage year built can be droped and have a single year built for house.\n* we could have just garage area between cars and area.\n* we cad drop kitchen also. becasue a bunch of them are one.\n* can filter Lot Area above 50000 to 50001\n* we **can** drop MSSubClass, OverallCond, YrSold, LowQualFinSF, Id, MiscVal, BsmtHalfBath, BsmtFinSF2, 3SsnPorch, MoSold, PoolArea because of their corrolation\n* It's not important when it was sold. So we drop MoSold and YrSold.\n* Overall Qual is important but Overall cond not!\n* We can add a luxury style field to show having pool or not and other fantasy features.\n* we can have just one of YearRemodAdd or YearBuilt.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train['YrSold'] = X_train['YrSold'].astype(str)\nX_train['MoSold'] = X_train['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"label_attrs = X_train.select_dtypes([object]).columns.values\nnum_attrs = X_train.select_dtypes([np.int64, np.float64]).columns.values\nnum_attrs = num_attrs[~(num_attrs == 'SalePrice')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalize Sale Price\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.base import TransformerMixin, BaseEstimator\nimport seaborn as sns\nclass Normalize(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        dataset = y_train.copy()\n        dataset = np.log1p(dataset)\n        return dataset\ny_train = Normalize().transform(y_train)\nsns.distplot(y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_label = X_train[label_attrs]\nX_train_num = X_train[num_attrs]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX_train_num_std = pd.DataFrame(StandardScaler().fit_transform(X_train_num), columns=X_train_num.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Colinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = X_train_num_std.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = X_train_num_std.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, vmax=1, center=0,vmin=-1 , \n            square=True, linewidths=.005)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Its hard to select from them by eye. so we filter it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = corr.iloc[1:, 1:]\ncorr = corr.applymap(lambda x : 1 if x > 0.75 else -1 if x < -0.75 else 0)\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, vmax=1, center=0,vmin=-1 , \n            square=True, linewidths=.005)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train.corr()['SalePrice'].sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We should drop from orange blocks\n* `GarageCars` should remain, and `GarageArea` should be deleted.\n* `GrLivArea` should remain, and `BedroomAbvGr` and `TotRmsAbvGrd` should be deleted.\n* `TotalBsmtSF` should remain, and `1stFlrSF` should be deleted.\n* `YearBuilt` should remain, and `GarageYrBlt` should be deleted."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_colinear_drop_attrs = ['GarageArea', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageYrBlt', '1stFlrSF']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Work With Numbers"},{"metadata":{},"cell_type":"markdown","source":"### Merge FullBath and HalfBath"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"class MergeBath(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self;\n    def transform(self, X, y=None):\n        X = X.copy()\n        X['Bath'] = X['HalfBath'] * X['FullBath']\n        X['HalfBath2'] = X['HalfBath'] ** 2\n        X['FullBath2'] = X['FullBath'] ** 2\n        X['BsmtBath'] = X['BsmtHalfBath'] * X['BsmtFullBath']\n        X['BsmtHalfBath2'] = X['BsmtHalfBath'] ** 2\n        X['BsmtFullBath2'] = X['BsmtFullBath'] ** 2\n        return X\nX_num_merged = MergeBath().transform(X_train_num)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merge BsmntFS and add Unfinished Fraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MergeBsmntFs(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self;\n    def transform(self, X, y=None):\n        X = X.copy()\n        X['BsmtFinSF'] = X['BsmtFinSF1'] * X['BsmtFinSF2']\n        X['BsmtFinSF12'] = X['BsmtFinSF1'] ** 2\n        X['BsmtFinSF22'] = X['BsmtFinSF2'] ** 2\n        return X\nX_num_bsmnt_proved = MergeBsmntFs().transform(X_num_merged)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merge Porchs "},{"metadata":{"trusted":true},"cell_type":"code","source":"# PolynomialFeatures?\nclass MergePorches(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self;\n    def transform(self, X, y=None):\n        X = X.copy()\n        X['Porch'] = X['OpenPorchSF'] + X['EnclosedPorch'] + X['3SsnPorch'] + X['ScreenPorch']\n        X.drop(['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch'], axis=1, inplace=True)\n        return X\nX_num_porch_merged = MergePorches().transform(X_num_bsmnt_proved)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Filter Lot Area above 50000 and Room above 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FilterLotAreaAndRooms(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self;\n    def transform(self, X, y=None):\n        X = X.copy()\n        X['LotArea'] = X['LotArea'].apply(lambda l: 50001 if l > 50000 else l)\n        X['BedroomAbvGr'] = X['BedroomAbvGr'].apply(lambda l: 5 if l > 5 else l)\n        return X\nX_num_lot_filtered = FilterLotAreaAndRooms().transform(X_num_porch_merged)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merge Lots\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MergeLots(BaseEstimator, TransformerMixin) :\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X = X.copy()\n        X['Lot'] = X['LotArea'] + X['LotFrontage']\n        X.drop(['LotArea', 'LotFrontage'], axis=1, inplace=True)\n        X['Lot'] = X['Lot'].apply(lambda l: 30000 if l > 30000 else l)\n        return X\nX_num_lots_merged = MergeLots().transform(X_num_lot_filtered)\nsns.regplot(x='Lot', y=y_train, data = X_num_lots_merged)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linearing And Removing Outliers"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"i = 1;\nplt.figure(figsize=(20, 35))\nfor col in X_num_lots_merged.drop(num_colinear_drop_attrs, axis=1):\n    if col is not 'Id' and col is not 'SalePrice':\n        plt.subplot(10, 4, i)\n        sns.regplot(x=col, y=y_train, data=X_num_lots_merged)\n        i = i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_scatter_drop = ['MSSubClass', 'LowQualFinSF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_delete_outlires = ['GrLivArea', 'OverallCond', 'BsmtFinSF1', 'GarageCars',\n                      '2ndFlrSF', 'YearBuilt', 'YearRemodAdd'] #Think about garage cars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nX_train_num_std_imputed = pd.DataFrame(SimpleImputer().fit_transform(X_train_num_std), \n                                       columns= X_train_num_std.columns)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"i = 1;\nplt.figure(figsize=(20, 35))\nfor col in X_train_num.drop(num_colinear_drop_attrs, axis=1):\n    if col is not 'Id' and col is not 'SalePrice':\n        plt.subplot(10, 4, i)\n        sns.boxplot(x=X_train[col])\n        i = i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nimport numpy as np\nz = pd.DataFrame(np.abs(stats.zscore(X_train_num)), columns=X_train_num.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_without_outlier = X_train[(z[to_delete_outlires] < 3).all(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_without_outlier = y_train[(z[to_delete_outlires] < 3).all(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_without_outlier.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sns.distplot(X_train_num_std_imputed.LotFrontage)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping "},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataFrameDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, drop_attrs=[]):\n        self.drop_attrs = drop_attrs\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X = X.copy()\n        X.drop(self.drop_attrs, axis=1, inplace=True, errors='ignore')\n        return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_drop_attrs = num_scatter_drop + num_colinear_drop_attrs\nX_num_dropped = DataFrameDropper(num_drop_attrs).transform(X_num_lot_filtered)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Work With Labels"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"X_train_label.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Dropping very null attributes"},{"metadata":{"trusted":true},"cell_type":"code","source":"label_drop_attrs = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'] # Think about FireplaceQu\nX_label_dropped = DataFrameDropper(label_drop_attrs).transform(X_train_label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Impute"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nX_label_imputed = pd.DataFrame(SimpleImputer(strategy=\"most_frequent\").fit_transform(X_label_dropped.values),\n                               columns=X_label_dropped.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Encoding Label"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nclass OneHotGoodEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.encoder = OneHotEncoder()\n    def fit(self, X, y=None): \n        self.encoder.fit(X)\n    def transform(self, X, y=None):\n        columns = X.columns\n        X_transformed = self.encoder.transform(X).toarray()\n        cats = self.encoder.categories_\n        i = 0\n        labels = []\n        for cat in cats:\n            for c in cat:\n                labels.append(columns[i] + ' : ' + c)\n            i = i+1\n        return pd.DataFrame(X_transformed, columns=labels)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder = OneHotGoodEncoder()\nencoder.fit(X_label_imputed)\nX_label_encoded = encoder.transform(X_label_imputed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analys labels using p-value\n"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import f_regression\nF, p_value = f_regression(X_label_encoded, y_train)\nnp.array(X_label_encoded.columns) + \" = \" + (p_value < 0.05).astype(str) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If all classes of a category was false we will delete it."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, OneHotEncoder, OrdinalEncoder\nencoder = OrdinalEncoder()\nX_label_encoded = pd.DataFrame(OrdinalEncoder().fit_transform(X_label_imputed), columns=X_label_imputed.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Analys Labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_label_analys = X_label_encoded.copy()\nX_label_analys['PriceSale'] = y_train.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlabel_new_drop_attrs = ['Utilities', 'LandSlope', 'YrSold', 'MoSold']\nX_label_new_analys = DataFrameDropper(label_new_drop_attrs).transform(X_label_analys)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create Pipeline "},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attrs):\n        self.attrs = attrs\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return X.loc[:, self.attrs]\nclass LabelBinarizerPipelineFriendly(OneHotEncoder):\n    def fit(self, X, y=None):\n        \"\"\"this would allow us to fit the model based on the X input.\"\"\"\n        super(LabelBinarizerPipelineFriendly,self).fit(X)\n    def transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).transform(X).toarray()\n    def fit_transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).fit(X).transform(X)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n\nnum_pipeline = Pipeline([\n    ('selection', DataFrameSelector(num_attrs)),\n    ('merge_bath', MergeBath()),\n    ('merge_bsmnt', MergeBsmntFs()),\n    ('merge_porch', MergePorches()),\n    ('filter', FilterLotAreaAndRooms()),\n    ('drop', DataFrameDropper(num_drop_attrs)),\n    ('impute', SimpleImputer()),\n    ('std_scale', StandardScaler()),\n])\n\nlabel_pipeline = Pipeline([\n    ('selection', DataFrameSelector(label_attrs)),\n    ('drop', DataFrameDropper(label_new_drop_attrs)),\n    ('impute', SimpleImputer(strategy=\"most_frequent\")),\n#     ('encode', OrdinalEncoder()), # one hot is  better \n    ('encode', OneHotEncoder(sparse=False, handle_unknown='ignore')),\n    ('std_scale', StandardScaler()),\n])\n\nfull_pipeline = FeatureUnion([\n    ('num_pipline', num_pipeline),\n    ('label_pipeline', label_pipeline),\n])\n\n\nX_train_cleaned = pd.DataFrame(full_pipeline.fit_transform(X_train_without_outlier))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = y_train_without_outlier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_cleaned.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Linear Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression()\nlinear_model.fit(X_train_cleaned, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_validate\n\ndef analys_model(model):\n    some_data = X_train.iloc[:5]\n    some_label = y_train.iloc[:5]\n    some_data_prepared = full_pipeline.transform(some_data)\n    print(f\"\\x1b[31mPredictions are \\033[92m{model.predict(some_data_prepared)}\")\n    print(f\"\\x1b[31mLables are \\033[92m{list(some_label)}\")\n    housing_prediction = model.predict(X_train_cleaned)\n    scores = cross_validate(model, X_train_cleaned, y_train, scoring=\"neg_mean_squared_error\", cv=3)\n    rmse_scores = np.sqrt(-scores['test_score'])\n    print(f\"\\x1b[31mScores : \\033[92m{rmse_scores}\")\n    print(f\"\\x1b[31mMean : \\033[92m{rmse_scores.mean()}\")\n    print(f\"\\x1b[31mStandard Deviation : \\033[92m{rmse_scores.std()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"analys_model(linear_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train SGD Regressor"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nsgd_grid = {\n    'n_iter_no_change': [10, 20, 30, 40, 50, 60, 80, 100, 130, 140],\n    'eta0': [0.4, 0.2, 0.1, 0.05, 0.03, 0.01, 0.009, 0.004],\n}\nsgd_model = SGDRegressor()\nsgd_best = RandomizedSearchCV(sgd_model, sgd_grid, verbose=2, cv=3,n_jobs=-1, \n                              scoring=\"neg_mean_squared_error\").fit(X_train_cleaned, y_train).best_estimator_\n\nsgd_best\nsgd_best.fit(X_train_cleaned, y_train)\nanalys_model(sgd_best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because sgd is also a linear model but with selection of theta using stochastic gradient it should not go very better than linear regression."},{"metadata":{},"cell_type":"markdown","source":"## Train Polynomial Regression\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_model = Pipeline([\n    ('poly_feature', PolynomialFeatures(degree=2, include_bias=False)),\n    ('std_scale', StandardScaler()),\n    ('lin_reg', LinearRegression())\n])\npoly_model.fit(X_train_cleaned, y_train)\nanalys_model(poly_model)\nplt.plot(y_train[:100])\nplt.plot(poly_model.predict(X_train_cleaned[:100]), 'r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems polynomial regression overfitted. With more degrees also it doesnt get better and get very slow. "},{"metadata":{},"cell_type":"markdown","source":"# Regularized Linear Models\n## TrainRidge Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Ridge \nridge_grid = {\n    'alpha': np.linspace(0, 1, num=500),\n    'solver' : ['cholesky'],\n}\nridge_model = Ridge()\nridge_best =  RandomizedSearchCV(ridge_model, ridge_grid, verbose=2, cv=3,n_jobs=-1, \n                              scoring=\"neg_mean_squared_error\").fit(X_train_cleaned, y_train).best_estimator_\nridge_best.fit(X_train_cleaned, y_train)\nanalys_model(ridge_best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Lasso Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso_grid = {\n    'alpha': np.linspace(0, 1e-3, num=1000),\n}\nlasso_model =  Lasso()\nlasso_best =  RandomizedSearchCV(lasso_model, lasso_grid, verbose=2, cv=3,n_jobs=-1, \n                              scoring=\"neg_mean_squared_error\").fit(X_train_cleaned, y_train).best_estimator_\nlasso_best.fit(X_train_cleaned, y_train)\nanalys_model(lasso_best)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import LassoLars\nlasso_lars_grid = {\n    'alpha': np.linspace(0, 1e-3, num=10000),\n    'max_iter' : [int(x) for x in np.linspace(1, 110, num = 100)]\n}\nlasso_lars_model = LassoLars()\nlasso_lars_best =  RandomizedSearchCV(lasso_lars_model, lasso_lars_grid, verbose=2, cv=3,n_jobs=-1, \n                              scoring=\"neg_mean_squared_error\").fit(X_train_cleaned, y_train).best_estimator_\nlasso_lars_best.fit(X_train_cleaned, y_train)\nanalys_model(lasso_lars_best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Try to boost with gradient method"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.base import clone\nclass GradientBoostingOtherRegressor(TransformerMixin, BaseEstimator):\n    def __init__(self, estimator, n_estimates = 3):\n        self.estimator = estimator\n        self.estimators = []\n        self.n_estimates = n_estimates\n    def fit(self, X, y_train=None):\n        last_estimator = self.estimator\n        last_estimator.fit(X, y_train)\n        y = y_train.values\n        self.estimators.append(last_estimator)\n        for i in range(self.n_estimates):\n            y = y - last_estimator.predict(X)\n            new_estimator = clone(self.estimator)\n            new_estimator.fit(X, y)\n            last_estimator = new_estimator\n            self.estimators.append(last_estimator)\n        return self\n    def predict(self, X_test):\n        y_pred = sum(tree.predict(X_test) for tree in self.estimators)\n        return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"gbor = GradientBoostingOtherRegressor(ridge_best, n_estimates=4)\ngbor.fit(X_train_cleaned, y_train)\nanalys_model(gbor)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Elastic Net"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.linear_model import ElasticNet\nelastic_grid = {\n    'alpha': np.linspace(0, 1e-2, num=10000),\n    'l1_ratio' : np.linspace(0, 1, num=10)\n}\nelastic_model = ElasticNet()\nelastic_best =  RandomizedSearchCV(elastic_model, elastic_grid, verbose=2, cv=3,n_jobs=-1, \n                              scoring=\"neg_mean_squared_error\").fit(X_train_cleaned, y_train).best_estimator_\nelastic_best.fit(X_train_cleaned, y_train)\nanalys_model(elastic_best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train SVM Regression"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.svm import SVR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM - Linear"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nsvm_linear_grid = {\n    'epsilon' : np.linspace(0, 0.5, num=200),\n}\nsvm_linear_model = SVR(kernel='linear')\nsvm_linear_best = RandomizedSearchCV(svm_linear_model, svm_linear_grid, verbose=2, cv=3, n_jobs=-1, \n                              scoring='neg_mean_squared_error').fit(X_train_cleaned, y_train).best_estimator_\nsvm_linear_best.fit(X_train_cleaned, y_train)\nanalys_model(svm_linear_best)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### SVM - Poly"},{"metadata":{"trusted":false},"cell_type":"code","source":"svm_poly_grid = {\n    'epsilon' : np.linspace(0, 0.5, num=200),\n}\nsvm_poly_model = SVR(kernel='poly')\nsvm_poly_best = RandomizedSearchCV(svm_poly_model, svm_poly_grid, verbose=2, cv=3, n_jobs=-1, \n                              scoring='neg_mean_squared_error').fit(X_train_cleaned, y_train).best_estimator_\nsvm_poly_best.fit(X_train_cleaned, y_train)\nanalys_model(svm_poly_best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Decision Tree Regression"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\ndt_model = DecisionTreeRegressor()\ndt_model.fit(X_train_cleaned, y_train)\n\nmax_features = [int(x) for x in np.linspace(1, 270, num = 30)]\nmax_depth = [1, 2, 4, 5, 6, 9, 10, 12 , None]\nmin_samples_leaf = [int(x) for x in np.linspace(1, 10, num = 5)]\nrandom_grid = {'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_leaf': min_samples_leaf}\n\nrandom_search =  RandomizedSearchCV(estimator = dt_model, param_distributions = random_grid,\n                                    n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring=\"neg_mean_squared_error\")\nrandom_search.fit(X_train_cleaned, y_train) \ndt_best = random_search.best_estimator_\ndt_best.fit(X_train_cleaned, y_train)\nanalys_model(dt_best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boost Decison Tree\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\ndt_ada_model = AdaBoostRegressor(dt_best, n_estimators=200, learning_rate=0.5)\ndt_ada_model.fit(X_train_cleaned, y_train)\nanalys_model(dt_ada_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Random Forest Regression\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 5)]\nmax_features = ['auto', 10, 20, 40, 90, 140, 200, 250]\nmax_depth = [int(x) for x in np.linspace(1, 1000, num = 20)]\nmax_depth.append(None)\nmin_samples_leaf = [5, 10, 15]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrf = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"random_search =  RandomizedSearchCV(estimator = rf, param_distributions = random_grid,\n                                    n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring=\"neg_mean_squared_error\")\nrandom_search.fit(X_train_cleaned, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"rf_best = random_search.best_estimator_\nrf_best.fit(X_train_cleaned, y_train)\nanalys_model(rf_best)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boost Random Forest"},{"metadata":{},"cell_type":"markdown","source":"* First we use ada boost with our random"},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\ndt_ada_model = AdaBoostRegressor(rf_best, n_estimators=10, learning_rate=0.5)\ndt_ada_model.fit(X_train_cleaned, y_train)\nanalys_model(dt_ada_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Then try a gradient boost with a weak random forest ( max_depth = 5 ); also, we try to find number of estimator with early stoping. "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nX_t, X_v, y_t, y_v = train_test_split(X_train_cleaned, y_train, random_state=42, test_size=0.2)\n\ngradient_reg = GradientBoostingRegressor(\n    n_estimators=1000, \n    random_state=42, \n    learning_rate=0.1, \n    min_samples_split=10,\n    max_features='sqrt',\n    max_depth=5\n)\ngradient_reg.fit(X_t, y_t)\n\nerrors = [mean_squared_error(y_v, y_pred) for y_pred in gradient_reg.staged_predict(X_v)]\nbest_n_estimators = np.argmin(errors)\n\nplt.plot(errors)\n\ngradient_best = GradientBoostingRegressor(\n    n_estimators=best_n_estimators, \n    random_state=42, \n    learning_rate=0.1, \n    min_samples_split=10,\n    max_features='sqrt',\n    max_depth=5\n)\ngradient_best.fit(X_train_cleaned, y_train)\n\nanalys_model(gradient_best)\n\nprint(f'min estimator {best_n_estimators}')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble Methods\n\n## Voting "},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\nvoting_model = VotingRegressor(\n    estimators=[('ridge', ridge_best), ('lasso', lasso_best), ('elastic', elastic_best), ('svm', svm_linear_best),\n               ('rf', gradient_best), ('dt', dt_ada_model)],\n    n_jobs=-1\n)\nvoting_model.fit(X_train_cleaned, y_train)\nanalys_model(voting_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Stacking Our Model\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfrom sklearn.base import clone, RegressorMixin\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n    def fit(self, X, y=None):\n        X = X.values\n        y = y.values\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    def get_metafeatures(self, X):\n        return np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"stacked_averaged_models = StackingAveragedModels(base_models = [gradient_best, dt_ada_model, elastic_best, lasso_lars_best, svm_linear_best],\n                                                 meta_model = LinearRegression())\nstacked_averaged_models.fit(X_train_cleaned, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"meta_features = pd.DataFrame(stacked_averaged_models.get_metafeatures(X_train_cleaned))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"i = 0;\nplt.figure(figsize=(20, 15))\nfor col in meta_features:\n    plt.subplot(3, 3, i+1)\n    sns.regplot(x=meta_features[i], y=y_train)\n    i = i+1","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":false},"cell_type":"code","source":"analys_model(stacked_averaged_models)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Sumbit Test Set"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test_clean = full_pipeline.transform(X_test)\npredictions = stacked_averaged_models.predict(X_test_clean)\nfinal_prediction = pd.DataFrame({'Id': X_test['Id'],\n                                'SalePrice': np.expm1(predictions)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_prediction.to_csv('prediction.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":1}