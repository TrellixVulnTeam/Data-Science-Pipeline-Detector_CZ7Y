{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Abstraction\nOverfitting is one of a common problem we need to deal with when working on a machine learning model. Whenever it is happened, your model works **perfectly on the training data**, but **badly on the real situation**. As it can be seen that, overfitting is an essential problem that we must know how to detect and avoid it.","metadata":{}},{"cell_type":"markdown","source":"## What is overfitting\nAs I already mentioned above, overfitting is a situation that your model work very well on the training data but not for the other (unseen) data. The reason why this happened is that your model is too complex which make it learn the \"noise\" (outliers) instead of being general. Looking at the example below, we can see that the model work perfectly on the training sample (red), but badly on the test/valid samples. In this example, I used a linear regression model with $degree=16$:\n$$y=\\sum_{i=0}^{16}(w_i.x^i)$$\n\n![overfit-example](https://i.imgur.com/bx589P7.png)","metadata":{}},{"cell_type":"markdown","source":"## Detect overfitting\nIt can be seen that the key point to recognize the overfitting is the difference of error between training and testing/validation set. With a naive idea, we can split into 2 dataset train and validation set. For instance, 80% train set and 20% validation set. (This is call `percentage split` strategy).\n\n![train-test-loss](https://i.imgur.com/aiemPZC.png)\n\nFrom the image above, we can see that at the first stage, both train loss and validation loss are decrease. However, if we continue to train the model the validation loss will increase while the train loss is decrease.\n\nFurthermore, the `percentage split` strategy seem underestimate the model. In practice, we usually use `k-fold cross validation` to evaluate the overfitting. With `k-fold`, we split our dataset into `k` equal-size folds. And then we do the validation `k` times, each time we pick a fold to be test set, and the others to be the train set. Finally, we calculate the mean value of loss:\n\n![k-fold](https://i.imgur.com/QzfsD8P.jpg)","metadata":{}},{"cell_type":"markdown","source":"## How to reduce overfitting\n- Enlarge the training set. Some examples:\n    - Collect more data (but not practical)\n    - Data augumentation: Widely use with image problem (shift, scale, etc)\n    - Using GAN model to generate image\n- Using regularization to penalize the weight (weight decay)\n- Using drop-out (mostly in deep learning)\n- Using prune tree (in decision tree)\n- Using VC dimension\n- Early stopping\n- Doing feature selection\n- Using ensemble method\n\n**In this document, I use `linear regression` model and reduce overfitting with `feature selection`**\n\nUsing feature selection, we can reduce the complexity of our model (which also means dimensions)","metadata":{}},{"cell_type":"markdown","source":"# Feature selection using Pearson correlation coefficent","metadata":{}},{"cell_type":"markdown","source":"While using `linear regression` model, we can use `Pearson correlation coefficent` to do the feature selection. The `PCC` is a measure of **linear correlation** between **two sets of data**. That is the ratio between the `covariance` and `product of their standard deviation`:\n$$\\rho_{X,Y}=\\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}$$\nIt means that, if our two sets:\n- Form an upward line, then $\\rho=1$\n- Form an downward line, then $\\rho=-1$\n- Scatter data randomly, then $\\rho=0$\n\nTo understand it clearly, look at the image below:\n\n![pearson-example](https://i.imgur.com/Ga36VPp.png)\n\nThe pearson correlation only consider about the `linear` property. If the two sets form a strange shape, PCC also equal $0$:\n\n![pearson-strange](https://i.imgur.com/uWZjK5i.png)\n\n> Note: The PCC only works for `numeric` type\n\nWe can see that the pearson correlation help us on selecting features while using linear regression model. We only select attributes that have the correlation with our target 50% (in term of both negative and positive).","metadata":{}},{"cell_type":"markdown","source":"# Housing price problem\n**Problem state**\n\nGiven a dataset contains many attributes about a house which is selling. The problem is to predict the price of that house.\n- Input: House's attributes\n- Output: House's price\n\n**Dataset**\n\nI use the dataset for contest [House prices - Advanced regression techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).\n","metadata":{}},{"cell_type":"markdown","source":"## Load and explore data","metadata":{}},{"cell_type":"markdown","source":"**Import libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:25.346908Z","iopub.execute_input":"2021-12-31T15:16:25.347461Z","iopub.status.idle":"2021-12-31T15:16:26.938812Z","shell.execute_reply.started":"2021-12-31T15:16:25.347338Z","shell.execute_reply":"2021-12-31T15:16:26.937425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load data**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:26.940721Z","iopub.execute_input":"2021-12-31T15:16:26.940989Z","iopub.status.idle":"2021-12-31T15:16:27.003667Z","shell.execute_reply.started":"2021-12-31T15:16:26.940958Z","shell.execute_reply":"2021-12-31T15:16:27.002204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data analysis**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.00556Z","iopub.execute_input":"2021-12-31T15:16:27.005962Z","iopub.status.idle":"2021-12-31T15:16:27.059468Z","shell.execute_reply.started":"2021-12-31T15:16:27.005915Z","shell.execute_reply":"2021-12-31T15:16:27.058384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.06245Z","iopub.execute_input":"2021-12-31T15:16:27.062852Z","iopub.status.idle":"2021-12-31T15:16:27.072829Z","shell.execute_reply.started":"2021-12-31T15:16:27.062786Z","shell.execute_reply":"2021-12-31T15:16:27.07203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.07449Z","iopub.execute_input":"2021-12-31T15:16:27.075286Z","iopub.status.idle":"2021-12-31T15:16:27.125606Z","shell.execute_reply.started":"2021-12-31T15:16:27.075234Z","shell.execute_reply":"2021-12-31T15:16:27.124535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.127788Z","iopub.execute_input":"2021-12-31T15:16:27.128205Z","iopub.status.idle":"2021-12-31T15:16:27.243974Z","shell.execute_reply.started":"2021-12-31T15:16:27.128157Z","shell.execute_reply":"2021-12-31T15:16:27.242936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number of missing cell**\n","metadata":{}},{"cell_type":"code","source":"print(\"Missing value by column\")\nprint(\"-\"*20)\nprint(df.isna().sum())\nprint(\"-\"*20)\nprint(\"Total:\",df.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.245679Z","iopub.execute_input":"2021-12-31T15:16:27.246025Z","iopub.status.idle":"2021-12-31T15:16:27.274388Z","shell.execute_reply.started":"2021-12-31T15:16:27.245977Z","shell.execute_reply":"2021-12-31T15:16:27.273401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Drop missing column**\n\nBecause the data contains many missing column, I must drop the `NaN` value in order to use `linear regression`. We have two options here:\n- Drop the column that have `NaN`\n- Drop the row that have `NaN`\n\nIf we look in detail, we can see that we cannot drop by row since our data will be empty. So, I choose to drop the column.","metadata":{}},{"cell_type":"code","source":"df = df.dropna(axis=1)\n\nprint(\"Missing value by column\")\nprint(\"-\"*20)\nprint(df.isna().sum())\nprint(\"-\"*20)\nprint(\"Total:\",df.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.276703Z","iopub.execute_input":"2021-12-31T15:16:27.277252Z","iopub.status.idle":"2021-12-31T15:16:27.310576Z","shell.execute_reply.started":"2021-12-31T15:16:27.277205Z","shell.execute_reply":"2021-12-31T15:16:27.309548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Without using feature selection","metadata":{}},{"cell_type":"markdown","source":"**Create data**","metadata":{}},{"cell_type":"code","source":"X = df.drop([\"Id\",\"SalePrice\"], axis=1)\ny = df[\"SalePrice\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.311991Z","iopub.execute_input":"2021-12-31T15:16:27.312309Z","iopub.status.idle":"2021-12-31T15:16:27.322036Z","shell.execute_reply.started":"2021-12-31T15:16:27.312276Z","shell.execute_reply":"2021-12-31T15:16:27.321067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalize**\n\nAs you may already know that we need to normalize the data in order to mantain their magnitude between each other. In normalization, we also have many choices, two common normalizations are:\n- `Z-score`\n- `Min-max`\n\nIn this project, I use `Z-score` normalization in order to handle the outliers better.","metadata":{}},{"cell_type":"code","source":"col_types = X.dtypes\nnumeric_col = col_types[col_types!='object'].index\n\nscaler = StandardScaler()\nX[numeric_col] = scaler.fit_transform(X[numeric_col])\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.324747Z","iopub.execute_input":"2021-12-31T15:16:27.325583Z","iopub.status.idle":"2021-12-31T15:16:27.374671Z","shell.execute_reply.started":"2021-12-31T15:16:27.325548Z","shell.execute_reply":"2021-12-31T15:16:27.373963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**One-hot encoding**\n\nIn our data attributes, there are both `numeric` and `nomial` type. To use the `linear regression`, all the data must be `numeric`, so I use the one-hot encoding to convert all the `nominal` into `numeric`.","metadata":{}},{"cell_type":"code","source":"X = pd.get_dummies(X)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.375648Z","iopub.execute_input":"2021-12-31T15:16:27.376314Z","iopub.status.idle":"2021-12-31T15:16:27.432041Z","shell.execute_reply.started":"2021-12-31T15:16:27.376276Z","shell.execute_reply":"2021-12-31T15:16:27.4309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split data**\n\nAs mentioned above, I will split the data into two sets to detect the overfitting:\n- Train set (80%)\n- Test set (20%)","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.433629Z","iopub.execute_input":"2021-12-31T15:16:27.434329Z","iopub.status.idle":"2021-12-31T15:16:27.445129Z","shell.execute_reply.started":"2021-12-31T15:16:27.434282Z","shell.execute_reply":"2021-12-31T15:16:27.44414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train model**","metadata":{}},{"cell_type":"code","source":"model1 = LinearRegression()\nmodel1.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.446922Z","iopub.execute_input":"2021-12-31T15:16:27.447306Z","iopub.status.idle":"2021-12-31T15:16:27.521644Z","shell.execute_reply.started":"2021-12-31T15:16:27.447259Z","shell.execute_reply":"2021-12-31T15:16:27.520654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluation**\n\nTo measure the performance of our model, I use the `root mean squared error (RMSE)` which is a common metric to evaluate the `linear regression` model.","metadata":{}},{"cell_type":"code","source":"def RMSE(target,pred):\n    return np.sqrt(mean_squared_error(target,pred))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.523275Z","iopub.execute_input":"2021-12-31T15:16:27.523763Z","iopub.status.idle":"2021-12-31T15:16:27.528937Z","shell.execute_reply.started":"2021-12-31T15:16:27.52372Z","shell.execute_reply":"2021-12-31T15:16:27.528099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_train = model1.predict(X_train)\npred_test = model1.predict(X_test)\n\nprint(\"Train RMSE:\",RMSE(y_train, pred_train))\nprint(\"Test RMSE:\",RMSE(y_test, pred_test))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.530605Z","iopub.execute_input":"2021-12-31T15:16:27.531108Z","iopub.status.idle":"2021-12-31T15:16:27.564003Z","shell.execute_reply.started":"2021-12-31T15:16:27.531065Z","shell.execute_reply":"2021-12-31T15:16:27.563159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Discussion**\n\nFrom the result, we can see that the overfitting problem happened since the `MSE` on:\n- `Training set`: is quite acceptable \n- `Test set`: is considerably huge\n\nThis is a evidence to show that with $215$ features, the `linear regression` model is overfitting.","metadata":{}},{"cell_type":"markdown","source":"## Using feature selection","metadata":{}},{"cell_type":"markdown","source":"**Calculate pearson correlation**","metadata":{}},{"cell_type":"code","source":"correlation = df.corr()\ncorrelation_price = correlation[\"SalePrice\"]","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.565575Z","iopub.execute_input":"2021-12-31T15:16:27.566082Z","iopub.status.idle":"2021-12-31T15:16:27.584109Z","shell.execute_reply.started":"2021-12-31T15:16:27.566039Z","shell.execute_reply":"2021-12-31T15:16:27.582896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot with heat map**","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(13,10))\nsns.heatmap(correlation, cmap=\"rainbow\")\nplt.title(\"Correlations Between Variables\", size=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:27.585784Z","iopub.execute_input":"2021-12-31T15:16:27.586303Z","iopub.status.idle":"2021-12-31T15:16:28.603538Z","shell.execute_reply.started":"2021-12-31T15:16:27.586259Z","shell.execute_reply":"2021-12-31T15:16:28.60288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature selection**\n\nIn the `Pearson correlation coeffecient`, I have said that we only choose the negative attribute and positive attribute respective to the target (`SalePrice`). Furthermore, the PCC only works for the `numeric` attribute, so we can select the `nominal` attribute with our prior knowledge.\n\nLook at the rightmost column of the heatmap, we can see that some typical selected attributes:\n- `OverallQual`\n- `YearBuilt`\n- `YearRemoteAdd`\n\nFor my knowledge, I also pick `nominal` attributes that is `Utilities`, `Heating`, etc. For instance, it is obvious that the utilities (e.g. electronics, gas, water) will also affact the price of that house. (This called domain knowledge)","metadata":{}},{"cell_type":"code","source":"positive_attributes = (correlation_price > 0.50)\nnegative_attributes = (correlation_price < -0.50)\nnumeric_col = list(correlation_price[positive_attributes | negative_attributes].index)\ncategory_col = [\"Utilities\",\"Heating\",\"KitchenQual\",\"SaleCondition\",\"LandSlope\"]\n\nimportant_cols = numeric_col + category_col\n\ntry:\n    important_cols.remove(\"Id\")\nexcept:\n    print(\"Column [Id] not in `important_cols`\")","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:28.605027Z","iopub.execute_input":"2021-12-31T15:16:28.605489Z","iopub.status.idle":"2021-12-31T15:16:28.613239Z","shell.execute_reply.started":"2021-12-31T15:16:28.605439Z","shell.execute_reply":"2021-12-31T15:16:28.612319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create data**","metadata":{}},{"cell_type":"code","source":"X = df[important_cols]\nX = X.drop(columns=\"SalePrice\",axis=1)\ny = df[\"SalePrice\"]\nnumeric_col.remove(\"SalePrice\")","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:28.614489Z","iopub.execute_input":"2021-12-31T15:16:28.614756Z","iopub.status.idle":"2021-12-31T15:16:28.639298Z","shell.execute_reply.started":"2021-12-31T15:16:28.614725Z","shell.execute_reply":"2021-12-31T15:16:28.63835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalize**","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\nX[numeric_col] = scaler.fit_transform(X[numeric_col])\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:28.641061Z","iopub.execute_input":"2021-12-31T15:16:28.641597Z","iopub.status.idle":"2021-12-31T15:16:28.673718Z","shell.execute_reply.started":"2021-12-31T15:16:28.641552Z","shell.execute_reply":"2021-12-31T15:16:28.673105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**One-hot encoding**","metadata":{}},{"cell_type":"code","source":"X = pd.get_dummies(X)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:28.67492Z","iopub.execute_input":"2021-12-31T15:16:28.675186Z","iopub.status.idle":"2021-12-31T15:16:28.707619Z","shell.execute_reply.started":"2021-12-31T15:16:28.675157Z","shell.execute_reply":"2021-12-31T15:16:28.706739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Split data**","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:28.708799Z","iopub.execute_input":"2021-12-31T15:16:28.709162Z","iopub.status.idle":"2021-12-31T15:16:28.716414Z","shell.execute_reply.started":"2021-12-31T15:16:28.709126Z","shell.execute_reply":"2021-12-31T15:16:28.715554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train model**","metadata":{}},{"cell_type":"code","source":"model2 = LinearRegression()\nmodel2.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:28.717642Z","iopub.execute_input":"2021-12-31T15:16:28.717988Z","iopub.status.idle":"2021-12-31T15:16:28.739618Z","shell.execute_reply.started":"2021-12-31T15:16:28.717957Z","shell.execute_reply":"2021-12-31T15:16:28.738417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluation**","metadata":{}},{"cell_type":"code","source":"pred_train = model2.predict(X_train)\npred_test = model2.predict(X_test)\n\nprint(\"Train RMSE:\",RMSE(y_train, pred_train))\nprint(\"Test RMSE:\",RMSE(y_test, pred_test))","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:28.741321Z","iopub.execute_input":"2021-12-31T15:16:28.743023Z","iopub.status.idle":"2021-12-31T15:16:28.765313Z","shell.execute_reply.started":"2021-12-31T15:16:28.742958Z","shell.execute_reply":"2021-12-31T15:16:28.764021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Discuss**\n\nIt can be seen that after doing the feature selection, the result on the test set is much better. Instead of using $215$ features, we only select and pick out $31$ features. There are many different feature selection algorithm that we can apply to make our model being better, such as PCA, ICA, IDA, etc.","metadata":{}},{"cell_type":"markdown","source":"## Reference\n- Các phương pháp tránh Overfitting - Trần Trung Trực - Viblo - $6^{th}$ Nov, 2020\n- Overfitting - Vũ Hữu Tiệp - Machine learning cơ bản - $4^{th}$ Mar, 2017\n- Overfitting - IBM Cloud Education - $3^{rd}$ Mar, 2021\n- Different methods for mitigating overfitting on Neural Networks - Pablo Sánchez - quantdare - $26^{th}$ May, 2021\n- House Price Prediction - Advanced regression techniques - Kaggle contest - GettingStarted Prediction Competition\n- House Price Prediction submission - Emre Arslan - Kaggle contest - $29^{th}$ Dec, 2021\n- Correlation Coefficient - The organic chemistry tutor - $25^{th}$ Jun, 2020 \n- Pearson correlation coefficient - Wikipedia - $27^{th}$ Dec, 2021","metadata":{}},{"cell_type":"markdown","source":"# For submission","metadata":{}},{"cell_type":"markdown","source":"**Import imputer and load data**","metadata":{}},{"cell_type":"code","source":"from  sklearn.impute import SimpleImputer\n\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:28.771799Z","iopub.execute_input":"2021-12-31T15:16:28.772333Z","iopub.status.idle":"2021-12-31T15:16:28.906596Z","shell.execute_reply.started":"2021-12-31T15:16:28.772281Z","shell.execute_reply":"2021-12-31T15:16:28.905913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Take feature selection**","metadata":{}},{"cell_type":"code","source":"feature_cols = important_cols.copy()\nfeature_cols.remove(\"SalePrice\")\nX_final = test[feature_cols]\nX_final.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:28.90868Z","iopub.execute_input":"2021-12-31T15:16:28.909507Z","iopub.status.idle":"2021-12-31T15:16:28.925524Z","shell.execute_reply.started":"2021-12-31T15:16:28.909451Z","shell.execute_reply":"2021-12-31T15:16:28.924917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Impute missing data**","metadata":{}},{"cell_type":"code","source":"imp = SimpleImputer(missing_values=np.nan,strategy=\"most_frequent\")\nX_final[:] = imp.fit_transform(X_final)\nX_final.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:28.926552Z","iopub.execute_input":"2021-12-31T15:16:28.927208Z","iopub.status.idle":"2021-12-31T15:16:28.983892Z","shell.execute_reply.started":"2021-12-31T15:16:28.927171Z","shell.execute_reply":"2021-12-31T15:16:28.982902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalize**","metadata":{}},{"cell_type":"code","source":"col_types = X_final.dtypes\nnumeric_col = col_types[col_types!='object'].index\n\nscaler = StandardScaler()\nX_final[numeric_col] = scaler.fit_transform(X_final[numeric_col])\nX_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:28.987482Z","iopub.execute_input":"2021-12-31T15:16:28.987759Z","iopub.status.idle":"2021-12-31T15:16:29.016405Z","shell.execute_reply.started":"2021-12-31T15:16:28.987727Z","shell.execute_reply":"2021-12-31T15:16:29.015785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**One-hot encoding and re-index with model**","metadata":{}},{"cell_type":"code","source":"X_final = pd.get_dummies(X_final)\nX_final = X_final.reindex(columns=X_train.columns,fill_value=0)\nX_final.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:29.017322Z","iopub.execute_input":"2021-12-31T15:16:29.018096Z","iopub.status.idle":"2021-12-31T15:16:29.052251Z","shell.execute_reply.started":"2021-12-31T15:16:29.018054Z","shell.execute_reply":"2021-12-31T15:16:29.051347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predictions**","metadata":{}},{"cell_type":"code","source":"pred = model2.predict(X_final)\ntest['SalePrice'] = pred\nresult = test[[\"Id\",\"SalePrice\"]]\nprint(result)\n\nresult.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-31T15:16:29.05359Z","iopub.execute_input":"2021-12-31T15:16:29.053866Z","iopub.status.idle":"2021-12-31T15:16:29.090288Z","shell.execute_reply.started":"2021-12-31T15:16:29.053809Z","shell.execute_reply":"2021-12-31T15:16:29.089397Z"},"trusted":true},"execution_count":null,"outputs":[]}]}