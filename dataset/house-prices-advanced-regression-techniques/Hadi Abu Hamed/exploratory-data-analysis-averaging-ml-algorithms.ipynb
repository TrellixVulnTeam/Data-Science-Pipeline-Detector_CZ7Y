{"cells":[{"metadata":{"_uuid":"da3dd2976a6caece17d910c420de6e666be487bf"},"cell_type":"markdown","source":"This kernal was made for all the dreamers out there that took the path of bravey to become the greatest Data Scientiest they could be!\n\n**I hope you will find value in the content that I'm about to share, don't forget to share the love by upvoting this project of mine.** it would be a great sign of support, love and apprecation. A great shoutout for kaggle as well from making all of this possible. \n\nHurrah!!\n\n**Importing and Exploring**\n\nFirst thing's first. let's do some importing of the libraries that we'll use (i assure you the project will get more entertaining as you progress)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"89cb3b36c4b02add47efa0e6450b6dfff6441b8c"},"cell_type":"markdown","source":"Let's import more libraries that we'll be using and try to take a peak at the some of the training data:"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# importing some libraries for visulizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# importing sklearn to select the model that will be fitting out data into\n# we will train_test_split to divide the data\n# we will use cross_val_score to determine best accuracy \nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# import the data into dataframes using pandas library\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5bc14de5fcdb4b9c3a6088b96600ed83db9f35cf"},"cell_type":"markdown","source":"Now, let's explore the data a bit before we dig into the bigger stuff:\n\nalways, **ALWAYS**!! check the shape of your data before you start!!"},{"metadata":{"trusted":true,"_uuid":"6888be7e19216e832048ba952d28c39f8c6264c7"},"cell_type":"code","source":"train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce4c81cb7470daf860bb92d44e8cb26ff6ffde6f"},"cell_type":"markdown","source":"hmm.. as predicted it seems that we have 1 more row and 1 more column for the training data.\nThank God we checked before it was too late ;)\n\nokay, let's figure out which column it is:"},{"metadata":{"trusted":true,"_uuid":"50c94b9c757d4d5d4fd7c5688ed3607c71bfd830"},"cell_type":"code","source":"cols = {}\nuniqueCols =[]\nfor col in test.columns:\n    if col not in cols:\n        cols[col]=1\n    else:\n        cols+=1\nfor col in train.columns:\n    if col not in cols:\n        uniqueCols.append(col)\n\nprint( uniqueCols)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"328ffc45983c6eb48b82470048421c6b72261dc5"},"cell_type":"markdown","source":"yeah, that makes sense. since we need to predict the 'SalePrice' for the test data it can't be a column there :)\n\nokay, moving on..\n\nlet's drop the 'Id' columns since the are unnecessary for the prediction process:\n"},{"metadata":{"trusted":true,"_uuid":"63d1322fb1500cc73be767d5464169dfc6efd6cc"},"cell_type":"code","source":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9c89351a23a4e8839f555e0336914dfbc8be376"},"cell_type":"markdown","source":"\nA good practice when trying to understand dependent variables (in this case it's 'SalePrice') is to look at it's normal distribution and try to understand it's nature.\n\nwe will look also at it's skewness (if it's equal to 0 it means that this variable is evenly distributed) and kurtosis (the standard value should be 3)"},{"metadata":{"trusted":true,"_uuid":"256c2a02579bee455123f69666a578c9ea4f3d55"},"cell_type":"code","source":"sns.distplot(train['SalePrice'], bins=20, rug=True)\n\nprint(\"Skewness: %0.2f\" %train['SalePrice'].skew())\nprint(\"Kurtosis: %0.2f\" %train['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"715553f4ac223db5e47d3419cb407a64ec418b06"},"cell_type":"markdown","source":"Another good practice (and also the best first move) when doing DS projects is to look for all sorts of correlations between all features."},{"metadata":{"trusted":true,"_uuid":"799ae93086982cac86d5813feeec9ef98c6fe2f2"},"cell_type":"code","source":"corrmat = train.corr()\nplt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=.8, annot=True);\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7c2dcacf032dd4e79ec0e410257a9cbea0729f6"},"cell_type":"markdown","source":"Whoa.. now that's a messy graph!\n\nWhen we have TOO MANY features, it is best to filter correlations first. in this case we will settle for 0.5 correlation or above.\n"},{"metadata":{"trusted":true,"_uuid":"4bd88223bd84d83ddb9ab29b19275e14afda7fa4"},"cell_type":"code","source":"corrmat = train.corr()\n# extracting the relevant features\nfilteredCorrMat_features = corrmat.index[abs(corrmat['SalePrice'])>=0.5]\nplt.figure(figsize=(12,12))\n# performing corr on the chosen features and presenting it on the heatmap\nsns.heatmap(train[filteredCorrMat_features].corr(),annot=True,cmap='winter')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5602bd0a2c3883df8af00f21b1cb247fd6faa18e"},"cell_type":"markdown","source":"In this way, we selected only the most important features that will serve us as the best predictors for SalePrice.\n\nFurthermore, we find that the columns ** 'OverallQaul', 'GrLivArea' ** have the highest corrlations with SalePrice.\n\nIt is also very important to notice correlations amongst other features like:\n* ** 'GrLivArea' ** and 'TotalRmsAbvGrd' (corr= 0.83)\n*  'GarageCars' and 'GarageArea' (corr= 0.88)\n* 'lstFlrSF' and 'TotalBsmtSF' (corr= 0.82)\n\nIt seems like OverallQaul serves as the most reliable feature for predicting SalePrice, but don't believe me, let's just see it visually:\n"},{"metadata":{"trusted":true,"_uuid":"ba4b989f9ba1e003db451b1f3435628a2a779ebe"},"cell_type":"code","source":"sns.barplot(train.OverallQual,train.SalePrice)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bae08542d87d2e990799a0e50f9591e8376e2df0"},"cell_type":"markdown","source":"Before we dive into feature engineering, let's join our training data and test data so that we won't get lost later and stay consistent with changes across the data."},{"metadata":{"trusted":true,"_uuid":"3793ace04529b877c3083c18d2d5fd4f85f1250b"},"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ca66b6f0d2c1c9ab0141a1973ce15d8fb00bbb9"},"cell_type":"markdown","source":"**Pre-processing** and **Feature Engineering**\n\n**Stage 1: Handling Missing Data**\n\nNow we approach one of the most important sub sections of DS project. \n\nWhat do we do about missing data?\nDo we ommit it?\nDo we replace it with the mean? the median?\n\nThere are many considerations when we are dealing with missing data. \nAs a first step, let's see which data is missing and it's weight in percentage."},{"metadata":{"trusted":true,"_uuid":"a0be0dd20bcc324171589a6a03d54d0246816b0f"},"cell_type":"code","source":"totalMissing = all_data.isnull().sum().sort_values(ascending=False)\npercentage = ((all_data.isnull().sum()/all_data.isnull().count())*100).sort_values(ascending=False)\n\nmissingData = pd.concat([totalMissing,percentage],axis=1,keys=['Total','Percentage'])\nmissingData.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da6f73a4140aa0d460754cad53c09a7a0383998b"},"cell_type":"markdown","source":"as always, it's best to see things visually before moving forward"},{"metadata":{"trusted":true,"_uuid":"bc8679bc776939d3039390fe1a33e13ce6fb705d"},"cell_type":"code","source":"plt.subplots(figsize=(15,20))\nplt.xticks(rotation='90')\nsns.barplot(x=totalMissing.index[:24],y=percentage[:24])\nplt.xlabel('features')\nplt.ylabel('percentage of missing data')\nplt.title('percent of missing data by feature')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e2e6494cb4d693601a5fe7929eab4833de094de"},"cell_type":"markdown","source":"Now let's drop any feature that has more that 50% missing data. In this case, the features 'PoolQC', 'MiscFeature', 'Alley' and 'Fence' don't seem to add much either way. perhaps that why this data is mostly missing in the first place. \n\nSince most of this data is missing and since such data does not seem to be if high correlation with our dependent variable. let's go ahead and drop them!"},{"metadata":{"trusted":true,"_uuid":"852cf159154e184ae0f09b26992f855c887b40ad"},"cell_type":"code","source":"# columns to be dropped\ncolumnsToDrop = missingData[missingData['Percentage']>50].index\n\nall_data = all_data.drop(columnsToDrop, axis=1)\n# test = test.drop(columnsToDrop, axis=1)\nprint(all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4035f6851f1d4aeb39d19c41d03654efa07af55d"},"cell_type":"markdown","source":"** Handling categorcial missing data**\n\nWe will replace missing data for the catigorical features with None\n\nBsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these **categorical** basement-related features, NaN means that there is no basement.\n\nFireplaceQu, GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None"},{"metadata":{"trusted":true,"_uuid":"bea1136a4c8b4222e11a65803846296b337678d0"},"cell_type":"code","source":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', \n            'BsmtFinType1', 'BsmtFinType2','BsmtFullBath', 'BsmtHalfBath',\n            'GarageType', 'GarageFinish', 'GarageQual', 'BsmtUnfSF','BsmtFinSF1','BsmtFinSF2',\n            'GarageCond', 'FireplaceQu', 'MasVnrType', 'Exterior2nd'):\n    if col in all_data.columns:\n        all_data[col] = all_data[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bc7a3b04dd216d550c6b65444ab44950f76b3b5"},"cell_type":"markdown","source":"**Handling numerical missing data**\n\nNow, lets take care of missing data for the numerical features.\n\n"},{"metadata":{"trusted":true,"_uuid":"859baba9a7644e836219d5e32de7fd8c17dc2bb4"},"cell_type":"code","source":"#GarageYrBlt replacing missing data with 0\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(0)\n\n# NA most likely means no masonry veneer for these houses. We can fill in 0\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\n# let's drop YrSold since it's also not correlated with 'SalePrice'\nall_data = all_data.drop('YrSold', axis=1)\n\n# Electrical has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n\n# 'RL' is by far the most common value. So we can fill in missing values with 'RL'\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\n# For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . \n# Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling.\nall_data = all_data.drop(['Utilities'], axis=1)\n\n# data description says NA means typical\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\n# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n#  Replacing missing data with 0 (Since missing in this case would imply 0.)\nfor col in ('TotalBsmtSF', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n    \n#  Replacing missing data with the most common\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bea79f23796ce21fc83be00f18d2cdb7da04183a"},"cell_type":"markdown","source":"Now let's check if there are any missing values left"},{"metadata":{"trusted":true,"_uuid":"f54f8756802b65dc299d1b08a7297272fb0b030b"},"cell_type":"code","source":"all_data.isnull().sum().sort_values(ascending=False) #check\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"193af29892e5063e22dd99ed27e9f592499cfdb2"},"cell_type":"markdown","source":"Would you look at that.\nAin't that beautiful :)"},{"metadata":{"_uuid":"df4772fdd6aae58346799ef788bbc3802ffe35c6"},"cell_type":"markdown","source":"**Stage 2: Outliers! **\n\nIn statistics, an outlier is an observation point that is distant from other observations. usually the distance is measured by standard deviations. such points are usually produced by some sort of error or simply do not represent any real data and just get in the way to make our predictions less accurate. \n\nThe approach we're going to go with is simply remove data that's below the 0.05 percentile or above the 0.9 percentile (check out this link to better understand quantiles and percentiles: http://www.statisticshowto.com/quantile-definition-find-easy-steps/). "},{"metadata":{"trusted":true,"_uuid":"69e49b5835033b199447a71b3ff9a5ba576fe524"},"cell_type":"code","source":"from pandas.api.types import is_numeric_dtype\ndef remove_outliers(df):\n    low = .05\n    high = .9\n    quant_df = df.quantile([low, high])\n    for name in list(df.columns):\n        if is_numeric_dtype(df[name]):\n            df = df[(df[name] > quant_df.loc[low, name]) & (df[name] < quant_df.loc[high, name])]\n    return df\n\nremove_outliers(all_data).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"924f6950c10043251fb7ea4877291eb1e9e6ba4e"},"cell_type":"markdown","source":"huh.. doesn't see like we have any outliers in the chosen quantile.\n\nNonetheless, i would like to explore the feature 'GrLivArea' and see if i could visually spot outliers."},{"metadata":{"trusted":true,"_uuid":"a3ea6da8c2e5c97e8ee33a76cc88213bad2eb414"},"cell_type":"code","source":"plt.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdb5bc031dd8ee1a04f2c756f95e8ef769772355"},"cell_type":"markdown","source":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers (those bastards). Therefore, we can safely delete them."},{"metadata":{"trusted":true,"_uuid":"272803bac67ac45138127d7fd52c19a0dc2d8a83"},"cell_type":"code","source":"#Deleting outliers\ntempTrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\nplt.scatter(x = tempTrain['GrLivArea'], y = tempTrain['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ab7b53a0a2e8626555882cc62c259c7cd05a41f"},"cell_type":"markdown","source":"Outliers removal is not always safe. We decided to delete these two as they are very huge and really bad ( extremely large areas for very low prices). There are probably other outliers in the training data that we need to handle. but removing outliers always comes with a price. For the time being we will settle for the above explination and demonstration of outliers.\n\n**however, notice that this is just a demonstration and we did not in fact change our data (all_data). we will however deal with outliers later using StandardScaler/RobustScaler**\n \n\n**Stage 3: Target Variable**\n\nSalePrice is the variable we need to predict. So let's do some analysis on this variable first (remember the skewness?).\n"},{"metadata":{"trusted":true,"_uuid":"5b998d28134c36dfa9266e2986c42129f5d0fe60"},"cell_type":"code","source":"from scipy import stats\nfrom scipy.stats import norm\n\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function \n# mu is the mean across the population (more accurately, given data)\n# and sigma is the standard deviation across the population\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot (probability plot)\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a76dce25b2773ab45f87bf1bf9029fa7feb18b9"},"cell_type":"markdown","source":"We can clearly see that the 'SalePrice' distrution plot is right skewed (also indicative from the probability plot). \n\nSince we will be working with linear models, our next step would be to transform our distrubtion to look more normally distributed. We will do that by appling log(1+x) to all elements of 'SalePrice'.\n\nlet's begin :)\n"},{"metadata":{"trusted":true,"_uuid":"6ffb5dc5aff784909cd745852911430cbaab1a42"},"cell_type":"code","source":"#Appling log(1+x) to all elements of 'SalePrice'\ny_train = train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd11b2ea7d6f4800a70dfd3e9b8a07c14d74e6a1"},"cell_type":"markdown","source":"Ain't that beautiful :)\n\nit seems that we have come a long way with our data.\n\nBefore we continue with feature processing, there's this one thing that i would like to be aware of. \nIt's important to realize that till now, our data (all the features that we have been working with) is simply gathered and put into a table. thinking outside the box leads us to one important understanding. Can we combine features in order to create a new one that's potentially more correlatioed with our target variable?\n\nof course we can! take for example the ground area of the whole house, includeing the basement, the first floor, second floor..\nthe total square foor area of the house is a dominant feature for the prediction of the price of the house. \n\nlet's take care of it then ;)"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"fbde31aea0406c5dc1f6a53b5db8c04d23d52f1a"},"cell_type":"code","source":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a117cb9b1f5cabd3e248739e6c2176e761409438"},"cell_type":"markdown","source":"Up next, let's see what's up with skewed features( hmm.. screwed features..)\n\nwe'll focus on the numerical features and how skewed they are.\n"},{"metadata":{"trusted":true,"_uuid":"02c58277a41eaf3eef1b7d73ba5d07c197a68e9e"},"cell_type":"code","source":"from scipy.stats import skew \n\n# extracting numerical features\nnumeric_features = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nnumeric_features = all_data[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :numeric_features})\nskewness.head(10)\n\nprint(all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18666b97ee1623017a526431a04a8852a5cf6542"},"cell_type":"markdown","source":"Now we will use the Box-Cox transformation to deal with the highly skewed values (any features with skewness value > 3)."},{"metadata":{"trusted":true,"_uuid":"030afe0d97b654314db793d4fab5c5afcf9c3905"},"cell_type":"code","source":"highly_skewed = ['PoolArea','LotArea','KitchenAbvGr','ScreenPorch']\n\nfrom scipy.special import boxcox1p\nlam = 0.15\nfor feat in highly_skewed:\n    all_data[feat] = boxcox1p(all_data[feat], lam)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"032afcddd07b13f0a68f34e489e8c988ff37eb87"},"cell_type":"markdown","source":"Now, let's get all the dummies :)\n\n(this is basically converting categorical variables into dummy/indicator variables)"},{"metadata":{"trusted":true,"_uuid":"ede853685fb95a440ff192a1f7e059f1ed3ada9a"},"cell_type":"code","source":"all_data = pd.get_dummies(all_data)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"142e9138b35d183e991593be61815d7a5564f244"},"cell_type":"markdown","source":"Remember when we combined the training data and the test data to make one big all_data dataframe. well it's time to split them once more."},{"metadata":{"trusted":true,"_uuid":"df7bace605e5499e526bd9ba0b6d9d1bf073e6ce"},"cell_type":"code","source":"train = all_data[:ntrain]\ntest = all_data[ntrain:]\n\n# train.drop('SalePrice',axis=1,inplace=True)\n# train['SalePrice']\nprint(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f05a0fb8a6e7850819d5bd63023dc269d3051320"},"cell_type":"markdown","source":"phew!! that was a long jounrey. however, the interesting part is just about to start ;)\n\n\n**Modeling**\nFirst step's first. let's import all the classes that we will be working with:"},{"metadata":{"trusted":true,"_uuid":"4d7a3a8031153e35a35eb48c3d47b23ab9591f8c"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression, LassoCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer \nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d57188c44d54368865a099311046ec7cb23abcb"},"cell_type":"markdown","source":"Now let's split our data into training data and test data. We do this so we can validate our results later on."},{"metadata":{"trusted":true,"_uuid":"d37332189ae0a35ea41562697d7ee980309f0001"},"cell_type":"code","source":"# train= train.drop(train.index[[0,1]],axis=0)\nprint(y_train.shape, train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a69b95929aecf8bc385429c65a96b6582539f62"},"cell_type":"code","source":"\nX_train,X_test,y_train2,y_test = train_test_split(train.values,y_train,test_size = 0.3,random_state= 0)\nX_train.shape,X_test.shape,y_train2.shape,y_test.shape\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8c6a38f24bbba4f43d0ee05cb1914c2fc759629"},"cell_type":"markdown","source":"What i would like to do next is define the root mean squared error function for calculating the accuracy of the different modeling algorithms we are going to use.\n\n**Notice** that we are using cross validation technique with 5 folds."},{"metadata":{"trusted":true,"_uuid":"4fa812d1908270e606e186f0444be5819b7c00a8"},"cell_type":"code","source":"\n# Scoring - Root Mean Squared Error\ndef rmse_CVscore(model,X,y):\n    return np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a175118ff8e7e96091ecf01143bc8165cbc31a5c"},"cell_type":"markdown","source":"There are many modeling algorithms we can use, let's check sevearl and see their accuracy scores.\n\nwe'll start with **LASSO Regression**\n(since this model is senstive to outliers, we can use the RobustScaler to deal with them. the RobustScaler is a standardization technique that allows us to standardize our data using the mean and the standard deviation to be able to compare between data points)"},{"metadata":{"trusted":true,"_uuid":"bc2bdffff717241ffe920eec6e4e16944d377eef"},"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.05, random_state=1))\n\nscore = rmse_CVscore(lasso,X_train,y_train2)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c93041f805c1a5efbc7a45904689e8aeb6d38a5"},"cell_type":"markdown","source":"**ElasticNet** regression model (which basically combines Ridge regression and Lasso regression):"},{"metadata":{"trusted":true,"_uuid":"94e4907a7cfd53e8b3b3725f028ed818d6f62f35"},"cell_type":"code","source":"\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.05, l1_ratio=.9, random_state=3))\n\nscore = rmse_CVscore(ENet,X_train,y_train2)\nprint(\"\\nElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fae3a155992d10f3b976f8f737cf20aef3e902c5"},"cell_type":"markdown","source":"**Gradient Boosting Regression**\nIn order to take care of outliers for this model we can use the huber loss function."},{"metadata":{"trusted":true,"_uuid":"c9c6cb9e91c137df9f3094499587b00ccebcaf03"},"cell_type":"code","source":"\nGBoost = GradientBoostingRegressor(n_estimators=1000,max_depth=4,\n                                   learning_rate=0.05,\n                                   max_features='sqrt',\n                                   loss=\"huber\",random_state =5)\nscore = rmse_CVscore(GBoost,X_train,y_train2)\nprint(\"\\nGradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eac9bb05a92f53a38d7f6993121fddf43ce932e1"},"cell_type":"markdown","source":" **XGBRegressor** "},{"metadata":{"trusted":true,"_uuid":"8a69e76c8c7bd965818614425913a890e04764ca"},"cell_type":"code","source":"# create pipeline\n# my_pipeline = make_pipeline(\n#     SimpleImputer(),\n# XGBR = XGBRegressor(n_estimators=1000, learning_rate=0.05, random_state =7,max_depth=3)\n# )\n\n# score = rmse_CVscore(XGBR,X_train,y_train2)\n# print(\"\\nXGBRegressor score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4668e814d2c3009908347fc10f9316e547b2223f"},"cell_type":"markdown","source":"**LightGBM **"},{"metadata":{"trusted":true,"_uuid":"6be9e863709b5f29157b6c35f2e7fe8a814dc514"},"cell_type":"code","source":"\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=1000)\n\nscore = rmse_CVscore(model_lgb, X_train, y_train2)\nprint(\"\\nLightGBM Regressor score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57f8b6b947fe897b977d2ce2d150f704415d6ee9"},"cell_type":"markdown","source":"As you can see, sometimes the more complicated the model, the poorer the results."},{"metadata":{"_uuid":"dd161cf6edd7c3653e6b5a4dab9610d6c1c3821a"},"cell_type":"markdown","source":"What we can do now is a strategy called averaging base models. we combine some of the used models and average their results to get a more accurate result.\n\nlet's do this in a class:\n"},{"metadata":{"trusted":true,"_uuid":"825344fef690d836979a9eb9b36db0f962de88a6"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # creating clones of the original models\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # fitting our data to the models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #making predictions on our fitted models and averaging them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2138b0c4d87780df9297e6a376963809831e299c"},"cell_type":"code","source":"averaged_models = AveragingModels(models = (ENet,GBoost, lasso, model_lgb))\n\nscore = rmse_CVscore(averaged_models,X_train, y_train2)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b84ddecdb76f9bf6a39004f908136a04daa1cfcb"},"cell_type":"markdown","source":"Awesome!! it seems that we have a great estimate of accuracy for our model.\n\n**Final Training and Prediction**\n"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"9c56bf91dd8e3e6ce1d5958350fe1940006028ff"},"cell_type":"code","source":"def rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\nprint(\"test shape: {}, train shape: {}\".format(test.shape, y_train.shape))\n\n# train=train.drop(train.index[[0,4]],axis=0)\n\naveraged_models.fit(train, y_train)\ntrain_pred = averaged_models.predict(train)\navg_pred = np.expm1(averaged_models.predict(test))\n\nprint(rmse(y_train, train_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32f2e796339d7896ea7210d7a132175b472c0928"},"cell_type":"markdown","source":"Now that's some HOT results!!!\n\nlet's go ahead and submit our hard work :)"},{"metadata":{"trusted":true,"_uuid":"8f76cd650dca9cf853e52897a1a916619c37d912"},"cell_type":"code","source":"# test['Id'].shape\n# avg_pred.shape\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = avg_pred\nsub.to_csv('submission.csv',index=False)\n#train.shape\n#test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05a5fd5dd2bd2347a1917bf7e76f2085c7c06d7b"},"cell_type":"markdown","source":"Thank you for your interest in this project. I hope it brought value to your Data science endouvers and i hope you had some fun along the way!\n\nThis project was possible with the help of the following kernals:\nhttps://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\nhttps://www.kaggle.com/tecknomart/basic-data-science-skillset-we-must-have\nhttps://www.kaggle.com/bsivavenu/house-price-calculation-methods-for-beginners\n\nCheers!!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}