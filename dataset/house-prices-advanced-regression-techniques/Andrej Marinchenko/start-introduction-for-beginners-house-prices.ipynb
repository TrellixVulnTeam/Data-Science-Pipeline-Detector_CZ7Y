{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction: House Prices - Advanced Regression Techniques\n\nThis notebook is intended for those who are new to machine learning competitions or want a gentle introduction to the problem. I purposely avoid jumping into complicated models or joining together lots of data in order to show the basics of how to get started in machine learning! Any comments or suggestions are much appreciated.\n\nIn this notebook, we will take an initial look at the House Prices machine learning competition currently hosted on Kaggle. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\nGoal: to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\n\nI hope you liked this code, I also prepared more interesting laptops for this competition and I will be glad to share them with you:\n\n1. [COMPREHENSIVE DATA EXPLORATION WITH PYTHON](https://www.kaggle.com/andrej0marinchenko/comprehensive-data-exploration-with-python-upd)\n2. [Data ScienceTutorial for Beginners ](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-house-prices)\n3. [House Price Calculation methods for beginnners](https://www.kaggle.com/andrej0marinchenko/house-price-calculation-methods-for-beginnners)\n4. [Start: Introduction for beginners ](https://www.kaggle.com/andrej0marinchenko/start-introduction-for-beginners-house-prices)\n5. [EDA + Data Analytics For beginners](https://www.kaggle.com/andrej0marinchenko/eda-data-analytics-for-beginners-house-prices)\n6. [1 step for beginners linear model](https://www.kaggle.com/andrej0marinchenko/1-step-for-beginners-linear-model-house-prices)\n7. [Universal notebook 4 data analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-4-data-analysis)\n\n\n# Data\n\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. \n\nFile descriptions\n- train.csv - the training set\n- test.csv - the test set\n\nMoreover, we are provided with the definitions of all the columns (in `data_description.txt`) and an example of the expected submission file `sample_submission.csv`. \n\nIn this notebook, we will stick to using only the main application training and testing data. Although if we want to have any hope of seriously competing, we need to use all the data, for now we will stick to one file which should be more manageable. This will let us establish a baseline that we can then improve upon. With these projects, it's best to build up an understanding of the problem a little at a time rather than diving all the way in and getting completely lost! \n\n## Metric: ROC AUC\n\nOnce we have a grasp of the data, we need to understand the metric by which our submission is judged. In this case, it is a common classification metric known as the [Receiver Operating Characteristic Area Under the Curve (ROC AUC, also sometimes called AUROC)](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it).\n\nThe ROC AUC may sound intimidating, but it is relatively straightforward once you can get your head around the two individual concepts. The [Reciever Operating Characteristic (ROC) curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) graphs the true positive rate versus the false positive rate:\n\n![image](http://www.statisticshowto.com/wp-content/uploads/2016/08/ROC-curve.png)\n\nA single line on the graph indicates the curve for a single model, and movement along a line indicates changing the threshold used for classifying a positive instance. The threshold starts at 0 in the upper right to and goes to 1 in the lower left. A curve that is to the left and above another curve indicates a better model. For example, the blue model is better than the red model, which is better than the black diagonal line which indicates a naive random guessing model. \n\nThe [Area Under the Curve (AUC)](http://gim.unmc.edu/dxtests/roc3.htm) explains itself by its name! It is simply the area under the ROC curve. (This is the integral of the curve.) This metric is between 0 and 1 with a better model scoring higher. A model that simply guesses at random will have an ROC AUC of 0.5.\n\nWhen we measure a classifier according to the ROC AUC, we do not generation 0 or 1 predictions, but rather a probability between 0 and 1. This may be confusing because we usually like to think in terms of accuracy, but when we get into problems with inbalanced classes (we will see this is the case), accuracy is not the best metric. For example, if I wanted to build a model that could detect terrorists with 99.9999% accuracy, I would simply make a model that predicted every single person was not a terrorist. Clearly, this would not be effective (the recall would be zero) and we use more advanced metrics such as ROC AUC or the [F1 score](https://en.wikipedia.org/wiki/F1_score) to more accurately reflect the performance of a classifier. A model with a high ROC AUC will also have a high accuracy, but the [ROC AUC is a better representation of model performance.](https://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy)\n\nNot that we know the background of the data we are using and the metric to maximize, let's get into exploring the data. In this notebook, as mentioned previously, we will stick to the main data sources and simple models which we can build upon in future work. ","metadata":{"_uuid":"66406036d8dd7a0071295d1aee64f13bffc44e3a","_cell_guid":"551ce207-0976-48c3-9242-fac3e6bdf527"}},{"cell_type":"markdown","source":"## Imports\n\nWe are using a typical data science stack: `numpy`, `pandas`, `sklearn`, `matplotlib`. ","metadata":{"_uuid":"eb13bf76d4e1e60d0703856ec391cdc2c5bdf1fb","_cell_guid":"d632b08c-d252-4238-b496-e2c6edebec4b"}},{"cell_type":"code","source":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Read in Data \n\nFirst, we can list all the available data files. There are a total of 9 files: 1 main file for training (with target) 1 main file for testing (without the target), 1 example submission file, and 6 other files containing additional information about each loan. ","metadata":{"_uuid":"ded520f73b9e94ed47ac2e994a5fb1bcb9093d0f","_cell_guid":"a5e67831-4751-4f11-8e07-527e3e092671"}},{"cell_type":"code","source":"# List files available\nprint(os.listdir(\"../input/house-prices-advanced-regression-techniques/\"))","metadata":{"_uuid":"c54e1559611512ebd447ac24f2226c2fffd61dcd","_cell_guid":"2cdca894-e637-43a9-8f80-5791c2bb9041","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training data\napp_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training data has 1460 observations (each one a separate hause) and 79 features (variables) including the `SalePrice` (the label we want to predict). why 79 if we see that there are 81 columns in the composition? Because we took away two columns:\nnumber of the sold house in the data table and the desired value (sales value).","metadata":{"_uuid":"4695541966d3d29e8a7a8975b072d01caff1631d"}},{"cell_type":"code","source":"# Testing data features\napp_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","metadata":{"_uuid":"cbd1c4111df6f07bc0d479b51f50895e728b717a","_cell_guid":"d077aee0-5271-440e-bc07-6087eab40b74","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The test set is considerably smaller and lacks a `TARGET` column. ","metadata":{"_uuid":"e351f02c8a5886756507a2d4f1ddba4791220f12"}},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.","metadata":{"_uuid":"0b1a02afd367d1c4ee3a3a936382ca42fb921b9d"}},{"cell_type":"markdown","source":"## Examine the Distribution of the Target Column\n\nThe target is what we are asked to predict: `SalePrice`.","metadata":{"_uuid":"7c006a09627df1333c557dc11a09f372bde34dda","_cell_guid":"23b20e53-3484-4c4b-bec9-2d8ac2ac918d"}},{"cell_type":"code","source":"app_train['SalePrice'].value_counts()","metadata":{"_uuid":"2163ca09678b53dbe88388ccbc7d0e0f7d6c6230","_cell_guid":"5fb6ab16-1b38-4ecf-8123-e48c7c061773","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train['SalePrice'].astype(int).plot.hist();","metadata":{"_uuid":"1b2611fb3cf392023c3f40fd2f7b96f56f5dee7d","_cell_guid":"0e93c1e2-f6b8-4a0b-82b6-7dad8df56048","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this information, we see this is an [_imbalanced class problem_](http://www.chioka.in/class-imbalance-problem/). The number of houses sold under $250,000 is much higher than the rest. Once we get into more sophisticated machine learning models, we can [weight the classes](http://xgboost.readthedocs.io/en/latest/parameter.html) by their representation in the data to reflect this imbalance. ","metadata":{"_uuid":"119106000875202a0030109f14b73245fc4285e1","_cell_guid":"48f008ff-d81e-46b2-80a3-e58f2a6627ca"}},{"cell_type":"markdown","source":"## Examine Missing Values\n\nNext we can look at the number and percentage of missing values in each column. ","metadata":{"_uuid":"58851dfef481f32b3026e89b086534ea3683440d","_cell_guid":"507ec6b1-99d0-4324-a3ed-bdea2f916227"}},{"cell_type":"code","source":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","metadata":{"_uuid":"7a2f5c72c45fa04d9fa95e8051ae595be806e9a2","_cell_guid":"fc4c675f-e4a1-4e4f-9ece-3c59e5c8f7fd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","metadata":{"_uuid":"98b0a82a3009b8f6d0bc718a2e1eaba779b4ace9","_cell_guid":"786881f0-235e-441c-8319-f715a3b7d920","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When it comes time to build our machine learning models, we will have to fill in these missing values (known as imputation). In later work, we will use models such as XGBoost that can [handle missing values with no need for imputation](https://stats.stackexchange.com/questions/235489/xgboost-can-handle-missing-data-in-the-forecasting-phase). Another option would be to drop columns with a high percentage of missing values, although it is impossible to know ahead of time if these columns will be helpful to our model. Therefore, we will keep all of the columns for now.","metadata":{"_uuid":"0b1be19103910ce83ebf54eeed99e42829643578","_cell_guid":"df3c6a1d-b3ff-4565-bb32-5cba43c52729"}},{"cell_type":"markdown","source":"## Column Types\n\nLet's look at the number of columns of each data type. `int64` and `float64` are numeric variables ([which can be either discrete or continuous](https://stats.stackexchange.com/questions/206/what-is-the-difference-between-discrete-data-and-continuous-data)). `object` columns contain strings and are  [categorical features.](http://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/what-are-categorical-discrete-and-continuous-variables/) . ","metadata":{"_uuid":"0672e40c3ab75a7901c0de35d248b322a227dc7f"}},{"cell_type":"code","source":"# Number of each type of column\napp_train.dtypes.value_counts()","metadata":{"_uuid":"a03caadd76fa32f4b193e52467d4f39f2145d7b6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now look at the number of unique entries in each of the `object` (categorical) columns.","metadata":{"_uuid":"5859303c9acc63f7ff7acce063a9cd022a6d38cd"}},{"cell_type":"code","source":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","metadata":{"_uuid":"2d021eda10939a19b141292d34491b357acd201a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the categorical variables have a relatively small number of unique entries. We will need to find a way to deal with these categorical variables! ","metadata":{"_uuid":"10ceaf3ba31e26c822b242b1278d93ebfbefcc0a"}},{"cell_type":"markdown","source":"## Encoding Categorical Variables\n\nBefore we go any further, we need to deal with pesky categorical variables.  A machine learning model unfortunately cannot deal with categorical variables (except for some models such as [LightGBM](http://lightgbm.readthedocs.io/en/latest/Features.html)). Therefore, we have to find a way to encode (represent) these variables as numbers before handing them off to the model. There are two main ways to carry out this process:\n\n* Label encoding: assign each unique category in a categorical variable with an integer. No new columns are created. An example is shown below\n\n![image](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/label_encoding.png)\n\n* One-hot encoding: create a new column for each unique category in a categorical variable. Each observation recieves a 1 in the column for its corresponding category and a 0 in all other new columns. \n\n![image](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/one_hot_encoding.png)\n\nThe problem with label encoding is that it gives the categories an arbitrary ordering. The value assigned to each of the categories is random and does not reflect any inherent aspect of the category. In the example above, programmer recieves a 4 and data scientist a 1, but if we did the same process again, the labels could be reversed or completely different. The actual assignment of the integers is arbitrary. Therefore, when we perform label encoding, the model might use the relative value of the feature (for example programmer = 4 and data scientist = 1) to assign weights which is not what we want. If we only have two unique values for a categorical variable (such as Male/Female), then label encoding is fine, but for more than 2 unique categories, one-hot encoding is the safe option.\n\nThere is some debate about the relative merits of these approaches, and some models can deal with label encoded categorical variables with no issues. [Here is a good Stack Overflow discussion](https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor). I think (and this is just a personal opinion) for categorical variables with many classes, one-hot encoding is the safest approach because it does not impose arbitrary values to categories. The only downside to one-hot encoding is that the number of features (dimensions of the data) can explode with categorical variables with many categories. To deal with this, we can perform one-hot encoding followed by [PCA](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) or other [dimensionality reduction methods](https://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/) to reduce the number of dimensions (while still trying to preserve information). \n\nIn this notebook, we will use Label Encoding for any categorical variables with only 2 categories and One-Hot Encoding for any categorical variables with more than 2 categories. This process may need to change as we get further into the project, but for now, we will see where this gets us. (We will also not use any dimensionality reduction in this notebook but will explore in future iterations).","metadata":{"_uuid":"1b49e667293daabffd8a4b2b6d02cf44bf6a3ba8","_cell_guid":"86d1b309-5524-4298-b873-2c1c09eddec6"}},{"cell_type":"markdown","source":"### Label Encoding and One-Hot Encoding\n\nLet's implement the policy described above: for any categorical variable (`dtype == object`) with 2 unique categories, we will use label encoding, and for any categorical variable with more than 2 unique categories, we will use one-hot encoding. \n\nFor label encoding, we use the Scikit-Learn `LabelEncoder` and for one-hot encoding, the pandas `get_dummies(df)` function.","metadata":{"_uuid":"46f5bf9a6de52e270aa911ffd895e704da5426ec","_cell_guid":"95627792-157e-457a-88a8-3b3875c7e1d5"}},{"cell_type":"code","source":"# Create a label encoder object\nle = dict()\nle_count = 0\n\ndef encode_transform(app, col, le):\n    # Transform both training and testing data\n    app[col] = le[col].transform(app[col])\n    return app, col, le\n\n    \n\n\n# Iterate through the columns\nfor col in app_train:\n#     print(col)\n    if app_train[col].dtype == 'object':        \n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            le[col] = LabelEncoder()\n            # Train on the training data\n            le[col].fit(app_train[col])\n            encode_transform(app_train, col, le)\n#             encode_transform(app_test, col, le)\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","metadata":{"_uuid":"ddfaae5c3dcc7ec6bb47a2dffc10d364e8d25355","_cell_guid":"70641d4d-1075-4837-8972-e58d70d8f242","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Iterate through the columns\nfor col in app_train:\n#     print(col)\n    if app_train[col].dtype == 'object':        \n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            le[col] = LabelEncoder()\n            # Train on the training data\n            le[col].fit(app_train[col])\n#             encode_transform(app_train, col, le)\n            encode_transform(app_test, col, le)\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"_uuid":"6796c6dc793a08e162b6e20c6f185ef37bdf51f3","_cell_guid":"0851773b-39fd-4cf0-9a66-e30adeef3e57","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aligning Training and Testing Data\n\nThere need to be the same features (columns) in both the training and testing data. One-hot encoding has created more columns in the training data because there were some categorical variables with categories not represented in the testing data. To remove the columns in the training data that are not in the testing data, we need to `align` the dataframes. First we extract the target column from the training data (because this is not in the testing data but we need to keep this information). When we do the align, we must make sure to set `axis = 1` to align the dataframes based on the columns and not on the rows!","metadata":{"_uuid":"1b2c4198638ec8e5155097d112249de8754eb5c0","_cell_guid":"61d910b5-84f5-4655-bd8a-d29672c13741"}},{"cell_type":"code","source":"train_labels = app_train['SalePrice']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['SalePrice'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","metadata":{"_uuid":"e0d12a13cb95521c19b10d8829e8abe2b1118396","_cell_guid":"d99ca215-e893-490c-a6a4-83f3e8a067b3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training and testing datasets now have the same features which is required for machine learning. The number of features has grown significantly due to one-hot encoding. At some point we probably will want to try [dimensionality reduction (removing features that are not relevant)](https://en.wikipedia.org/wiki/Dimensionality_reduction) to reduce the size of the datasets.","metadata":{"_uuid":"802ffdbae02e43a9ca4e256ffcd6bd40ae15f3e9"}},{"cell_type":"markdown","source":"## Back to Exploratory Data Analysis\n\n### Anomalies\n\nOne problem we always want to be on the lookout for when doing EDA is anomalies within the data. These may be due to mis-typed numbers, errors in measuring equipment, or they could be valid but extreme measurements. One way to support anomalies quantitatively is by looking at the statistics of a column using the `describe` method. The numbers in the `YrSold` column are negative because they are recorded relative to the current loan application. To see these stats in years, we can mutliple by -1 and divide by the number of days in a year:\n\n","metadata":{"_uuid":"4d7c8dd1d5bb5a0ef84cb78e6bff927249e62145","_cell_guid":"13918211-0e6b-4d72-955b-f997db19eea2"}},{"cell_type":"code","source":"# (app_train['YrSold'] / -1).describe()\napp_train['YrSold'].describe()","metadata":{"_uuid":"a60be93c2d7d63855e6d65c1109f408ad85da134","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Those ages look reasonable. There are no outliers for the age on either the high or low end.","metadata":{"_uuid":"acb37a3e3f2e0b2fd581259788b9255398314157"}},{"cell_type":"markdown","source":"### Correlations\n\nNow that we have dealt with the categorical variables and the outliers, let's continue with the EDA. One way to try and understand the data is by looking for correlations between the features and the target. We can calculate the Pearson correlation coefficient between every variable and the target using the `.corr` dataframe method.\n\nThe correlation coefficient is not the greatest method to represent \"relevance\" of a feature, but it does give us an idea of possible relationships within the data. Some [general interpretations of the absolute value of the correlation coefficent](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf) are:\n\n\n* .00-.19 “very weak”\n*  .20-.39 “weak”\n*  .40-.59 “moderate”\n*  .60-.79 “strong”\n* .80-1.0 “very strong”\n","metadata":{"_uuid":"fd656b392faad3b34ecfa448b55ad03e75449e0a"}},{"cell_type":"code","source":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['SalePrice'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","metadata":{"_uuid":"d39d15d64db1f2c9015c6f542911ef9a9cac119e","_cell_guid":"02acdb8d-d95f-41b9-8ad1-e2b6cb26f398","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at some of more significant correlations: the `OverallQual` is the most positive correlation. (except for `SalePrice` because the correlation of a variable with itself is always 1!) Looking at the documentation, `OverallQual` is the rates the overall material and finish of the house. \n\nOverallQual: Rates the overall material and finish of the house\n\n       10\tVery Excellent\n       9\tExcellent\n       8\tVery Good\n       7\tGood\n       6\tAbove Average\n       5\tAverage\n       4\tBelow Average\n       3\tFair\n       2\tPoor\n       1\tVery Poor","metadata":{"_uuid":"67e1f0f22ec8e26c38827c24ca1e9409d73c9c64","_cell_guid":"8cfa409c-ec74-4fa4-8093-e7d00596c9c5"}},{"cell_type":"code","source":"# Find the correlation of the positive days since birth and target\napp_train['OverallQual'] = abs(app_train['OverallQual'])\napp_train['OverallQual'].corr(app_train['SalePrice'])","metadata":{"_uuid":"f705c7aa49486ec3bf119c4edc4e4af58861b88d","_cell_guid":"b0ab583c-dfbb-4ff7-80e5-d747fc408499","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the style of plots\nplt.style.use('fivethirtyeight')\n\n# Plot the General quality\nplt.hist(app_train['OverallQual'], edgecolor = 'k', bins = 25)\nplt.title('General quality'); plt.xlabel('OverallQual'); plt.ylabel('Count');","metadata":{"_uuid":"739226c4594130d6aabeb25ffb8742c37657d7a4","_cell_guid":"35e36393-e388-488e-ba7a-7473169d3e6f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By itself, the distribution of 'OverallQual' does not tell us much. To visualize the effect of the 'OverallQual' on the 'SalePrice', we will next make a [kernel density estimation plot](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE) colored by the value of the target. A [kernel density estimate plot shows the distribution of a single variable](https://chemicalstatistician.wordpress.com/2013/06/09/exploratory-data-analysis-kernel-density-estimation-in-r-on-ozone-pollution-data-in-new-york-and-ozonopolis/) and can be thought of as a smoothed histogram (it is created by computing a kernel, usually a Gaussian, at each data point and then averaging all the individual kernels to develop a single smooth curve). We will use the seaborn `kdeplot` for this graph.","metadata":{"_uuid":"340680b4a4ecf310a6369808157b17cac7c13461","_cell_guid":"02f5d3c5-e527-430b-a38d-531aeb8f3dd1"}},{"cell_type":"code","source":"# KDE plot \nplt.figure(figsize = (20, 15))\nsns.kdeplot(app_train['OverallQual'], label = 'OverallQual')\nsns.kdeplot(app_train['SalePrice']/25000, label = 'levelized selling price')\nplt.xlabel('OverallQual \\ levelized selling price'); plt.ylabel('Density'); plt.title('Distribution of General quality');\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (25, 15))\n\nsns.kdeplot(data=app_train, x=\"OverallQual\", y=\"SalePrice\", fill=True,)","metadata":{"_uuid":"2e045e65f048789b577477356df4337c9e5e2087","_cell_guid":"3982a18f-2731-4bb2-80c9-831b2377421f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is a clear trend: the most sold housing is of average quality - this is understandable! The cost of housing at the average level still has 3 local maximums and minimums, respectively, in order to find the cause of these differences, it is necessary to investigate the remaining parameters of housing data.","metadata":{"_uuid":"eb2bd6392ed6d6f7e002bc8dbea6aab0f30487d9","_cell_guid":"2dad060f-bcab-4fe3-aa19-29fbf3e6fdab"}},{"cell_type":"markdown","source":"### Exterior Sources\n\nThe 1 variable with the strongest negative correlations with the `SalePrice` is `ExterQual_TA`, and 6 variables with the strongest positive correlations `1stFlrSF`, `TotalBsmtSF`, `GarageArea`,`GarageCars`,`GrLivArea`,`OverallQual`.\nAccording to the documentation, these features represent a \n- 1stFlrSF: First Floor square feet            \n- TotalBsmtSF: Total square feet of basement area         \n- GarageArea: Size of garage in square feet          \n- GarageCars: Size of garage in car capacity          \n- GrLivArea: Above grade (ground) living area square feet          \n- OverallQual: Rates the overall material and finish of the house        \n\n\n\nIn the initial data, such a feature ExterQual_TA did not exist, but there was a categorical feature ExterQual, which was later coded by us and corresponds to the value TA - Average/Typical.\n\nExterQual: Evaluates the quality of the material on the exterior \n\t\t\n       Ex\tExcellent\n       Gd\tGood\n       TA\tAverage/Typical\n       Fa\tFair\n       Po\tPoor\n\nLet's take a look at these variables.\n\nFirst, we can show the correlations of the  features with the SalePrice and with each other.","metadata":{"_uuid":"43a3bb87bdaa65509e9dc887492239ae06cd1c77","_cell_guid":"4749204f-ec63-4eeb-8d25-9c80967348f1"}},{"cell_type":"code","source":"# 1stFlrSF            0.605852\n# TotalBsmtSF         0.613581\n# GarageArea          0.623431\n# GarageCars          0.640409\n# GrLivArea           0.708624\n# OverallQual         0.790982\n# \n# ExterQual_TA        -0.589044\n\n# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['SalePrice', '1stFlrSF', 'TotalBsmtSF', 'GarageArea', 'GarageCars', 'GrLivArea', 'OverallQual', 'ExterQual_TA']]\next_data_corrs = ext_data.corr()\next_data_corrs","metadata":{"_uuid":"6197819149feaff75176e64e54c65ea6be3864fe","_cell_guid":"e2ab3b7f-3a53-4495-a1de-31ad287f032a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.6, annot = True, vmax = 0.8)\nplt.title('Correlation Heatmap');","metadata":{"_uuid":"20b21a6b4e15a726c29596abeb01346dc416729c","_cell_guid":"0479863d-cfa9-47ab-83e6-7d7877e3e939","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we can look at the distribution of each of these features colored by the value of the target. This will let us visualize the effect of this variable on the target.","metadata":{"_uuid":"6a592aa7c01858b268489ccb8fd00690cd26cd58","_cell_guid":"78bd5acc-003d-4795-a57a-a6c4fc9c8c5f"}},{"cell_type":"code","source":"\n    \nplt.figure(figsize = (20, 15))\nsns.kdeplot(app_train['ExterQual_TA'], label = 'ExterQual_TA')\nsns.kdeplot(app_train['SalePrice']/200000, label = 'levelized selling price')\nplt.xlabel('ExterQual_TA \\ levelized selling price'); plt.ylabel('Density'); plt.title('Distribution of ExterQual_TA');   \n\n","metadata":{"_uuid":"49afab6b3790abcc2dea04c483f462f39e536503","_cell_guid":"5e2b6507-96d1-4f96-964f-d8241e321f09","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20, 15))\n\n\n\nplt.xlabel('ExterQual_TA \\ levelized selling price'); plt.ylabel('Density'); plt.title('Distribution of ExterQual_TA'); \n\n# iterate through the sources\nfor i, source in enumerate(['1stFlrSF', 'TotalBsmtSF', 'GarageArea', 'GarageCars', 'GrLivArea']):\n    \n    # create a new subplot for each source\n    plt.subplot(5, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train[source], label = source)\n\n    # plot loans that were not repaid\n    sns.kdeplot(app_train['SalePrice']/100, label = 'levelized selling price')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (20, 15))\n\n\n\nplt.xlabel('ExterQual_TA \\ levelized selling price'); plt.ylabel('Density'); plt.title('Distribution of ExterQual_TA'); \n\n# iterate through the sources\nfor i, source in enumerate(['1stFlrSF', 'TotalBsmtSF', 'GarageArea', 'GarageCars', 'GrLivArea']):\n    \n    # create a new subplot for each source\n    plt.subplot(5, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(data=app_train, x=source, y=\"SalePrice\", fill=True,)\n       \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GrLivArea displays the next largest determinant of the cost of housing. We can clearly see that this characteristic is most directly related to the cost of housing. The relationship is not very strong (in fact, they are all considered very weak, but these variables will still be useful for a machine learning model to predict the cost of housing).","metadata":{"_uuid":"71ce5855665256dacfd7c52bceb11c68f5c58759","_cell_guid":"0ee531e8-f131-4ae3-b542-d4bf550d9bd5"}},{"cell_type":"markdown","source":"# Feature Engineering\n\nKaggle competitions are won by feature engineering: those win are those who can create the most useful features out of the data. (This is true for the most part as the winning models, at least for structured data, all tend to be variants on [gradient boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)). This represents one of the patterns in machine learning: feature engineering has a greater return on investment than model building and hyperparameter tuning. [This is a great article on the subject)](https://www.featurelabs.com/blog/secret-to-data-science-success/). As Andrew Ng is fond of saying: \"applied machine learning is basically feature engineering.\" \n\nWhile choosing the right model and optimal settings are important, the model can only learn from the data it is given. Making sure this data is as relevant to the task as possible is the job of the data scientist (and maybe some [automated tools](https://docs.featuretools.com/getting_started/install.html) to help us out).\n\nFeature engineering refers to a geneal process and can involve both feature construction: adding new features from the existing data, and feature selection: choosing only the most important features or other methods of dimensionality reduction. There are many techniques we can use to both create features and select features.\n\nWe will do a lot of feature engineering when we start using the other data sources, but in this notebook we will try only two simple feature construction methods: \n\n* Polynomial features\n* Domain knowledge features\n","metadata":{"_uuid":"d5506d0483af10dbf71e8ed11c99b2d5253680fb","_cell_guid":"bd49d18b-e35f-4122-a005-dd06d8f2f7ca"}},{"cell_type":"markdown","source":"## Polynomial Features\n\nOne simple feature construction method is called [polynomial features](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). In this method, we make features that are powers of existing features as well as interaction terms between existing features. For example, we can create variables `SOURCE_1^2` and `SOURCE_2^2` and also variables such as `SOURCE_1` x `SOURCE_2`, `SOURCE_1` x `SOURCE_2^2`, `SOURCE_1^2` x   `SOURCE_2^2`, and so on. These features that are a combination of multiple individual variables are called [interaction terms](https://en.wikipedia.org/wiki/Interaction_(statistics) because they  capture the interactions between variables. In other words, while two variables by themselves  may not have a strong influence on the target, combining them together into a single interaction variable might show a relationship with the target. [Interaction terms are commonly used in statistical models](https://www.theanalysisfactor.com/interpreting-interactions-in-regression/) to capture the effects of multiple variables, but I do not see them used as often in machine learning. Nonetheless, we can try out a few to see if they might help our model to predict housing costs. \n\nJake VanderPlas writes about [polynomial features in his excellent book Python for Data Science](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html) for those who want more information.\n\n\nIn the following code, we create polynomial features using the `GrLivArea` variables and the `OverallQual` variable. [Scikit-Learn has a useful class called `PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) that creates the polynomials and the interaction terms up to a specified degree. We can use a degree of 3 to see the results (when we are creating polynomial features, we want to avoid using too high of a degree, both because the number of features scales exponentially with the degree, and because we can run into [problems with overfitting](http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html#sphx-glr-auto-examples-model-selection-plot-underfitting-overfitting-py)). ","metadata":{"_uuid":"70322dd11709dcaaf879a56103fde8fc787b7d4c","_cell_guid":"464705a1-7ecf-47ba-a1fe-9f870102eb85"}},{"cell_type":"code","source":"# Make a new dataframe for polynomial features\npoly_features = app_train[['GrLivArea', 'OverallQual', 'SalePrice']]\npoly_features_test = app_test[['GrLivArea', 'OverallQual']]\n\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\n\n\npoly_target = poly_features['SalePrice']\n\npoly_features = poly_features.drop(columns = ['SalePrice'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","metadata":{"_uuid":"a63d53dcac14c4ac2e31ea9c5e16b5d161c2415b","_cell_guid":"e5b0efd9-67ac-4aa0-91e9-2141a87a6a8a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","metadata":{"_uuid":"72c5ecaae9c6ff038d16cbd9208f1abb69912631","_cell_guid":"2be7c1ab-d1e5-40f2-b8e7-e2b2ce1e2f9a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This creates a considerable number of new features. To get the names we have to use the polynomial features `get_feature_names` method.","metadata":{"_uuid":"4d837e47bada5411ffce06266605f043c6ffe19e","_cell_guid":"a7833b1e-714c-4988-8cbf-757d01290d8f"}},{"cell_type":"code","source":"poly_transformer.get_feature_names(input_features = ['GrLivArea', 'OverallQual'])[:15]","metadata":{"_uuid":"121f98d2ec9c81c5dabb911dc68562d0b2b6d737","_cell_guid":"7465d1e6-d360-4029-afa7-67cb34f60249","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 10 features with individual features raised to powers up to degree 3 and interaction terms. Now, we can see whether any of these new features are correlated with the target.","metadata":{"_uuid":"4e68b80b2738ef46863b53b7b781f299d602d316","_cell_guid":"7eaeb645-bb25-4ac5-884f-d1231ab6d88f"}},{"cell_type":"code","source":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['GrLivArea', 'OverallQual']))\n\n# Add in the target\npoly_features['SalePrice'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['SalePrice'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","metadata":{"_uuid":"e712923de757457bb87a35ecaccd27007b351e6c","_cell_guid":"95725a63-f8f2-4680-8f7a-4252f04e7f7f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Several of the new variables have a greater (in terms of absolute magnitude) correlation with the target than the original features. When we build machine learning models, we can try with and without these features to determine if they actually help the model learn. \n\nWe will add these features to a copy of the training and testing data and then evaluate models with and without the features. Many times in machine learning, the only way to know if an approach will work is to try it out! ","metadata":{"_uuid":"082ac97a068afed6758ed191acf5ab485e39230c","_cell_guid":"971de432-c65e-4c9a-a3c0-c923ed27ddcb"}},{"cell_type":"code","source":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['GrLivArea', 'OverallQual']))\n\n# Merge polynomial features into training dataframe\npoly_features['Id'] = app_train['Id']\napp_train_poly = app_train.merge(poly_features, on = 'Id', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['Id'] = app_test['Id']\napp_test_poly = app_test.merge(poly_features_test, on = 'Id', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","metadata":{"_uuid":"ed758ed436a86f92a8ee574999aa91089242ca7a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics app_train_poly\nmissing_values = missing_values_table(app_train_poly)\nmissing_values.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill missing values based on probability of occurrence\nfor column in app_train_poly.columns:\n    null_vals = app_train_poly.isnull().values\n    a, b = np.unique(app_train_poly.values[~null_vals], return_counts = 1)\n    app_train_poly.loc[app_train_poly[column].isna(), column] = np.random.choice(a, app_train_poly[column].isnull().sum(), p = b / b.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(app_train_poly)\nmissing_values.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics app_test_poly\nmissing_values = missing_values_table(app_test_poly)\nmissing_values.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill missing values based on probability of occurrence\nfor column in app_test_poly.columns:\n    null_vals = app_test_poly.isnull().values\n    a, b = np.unique(app_test_poly.values[~null_vals], return_counts = 1)\n    app_test_poly.loc[app_test_poly[column].isna(), column] = np.random.choice(a, app_test_poly[column].isnull().sum(), p = b / b.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(app_train_poly)\nmissing_values.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"app_train_poly.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for column in app_train.columns:\n    null_vals = app_train.isnull().values\n    a, b = np.unique(app_train.values[~null_vals], return_counts = 1)\n    app_train.loc[app_train[column].isna(), column] = np.random.choice(a, app_train[column].isnull().sum(), p = b / b.sum())\n    \nfor column in app_test.columns:\n    null_vals = app_test.isnull().values\n    a, b = np.unique(app_test.values[~null_vals], return_counts = 1)\n    app_test.loc[app_test[column].isna(), column] = np.random.choice(a, app_test[column].isnull().sum(), p = b / b.sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modelling\nI will perform a simple linear regression on the dataset to predict house prices. In order to train out the regression model, we need to first split up the data into an X list that contains the features to train on, and a y list with the target variable, in this case, the Price column.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\napp_train_poly['SalePrice']=app_train['SalePrice']\nX = app_train_poly.drop(['SalePrice'], axis = 1)\ny = app_train_poly['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''for comparison, I also left the code with the original dataset in this laptop, without additional functions '''\n# from sklearn.model_selection import train_test_split\n\n# app_train_poly['SalePrice']=app_train['SalePrice']\n\n# X = app_train.drop(['SalePrice'], axis = 1)  \n# y = app_train['SalePrice']\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the data into training and testing set using scikit-learn train_test_split function. We are using 80% of the data for training and 20% for testing, train_test_split() returns four objects:\n\n- X_train: the subset of our features used for training\n- X_test: the subset which will be our ‘hold-out’ set – what we’ll use to test the model\n- y_train: the target variable SalePrice which corresponds to X_train\n- y_test: the target variable SalePrice which corresponds to X_test\nNow we will import the linear regression class, create an object of that class, which is the linear regression model.","metadata":{}},{"cell_type":"code","source":"from sklearn import linear_model\n\nlr = linear_model.LinearRegression()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then using the fit method to \"fit\" the model to the dataset. What this does is nothing but make the regressor \"study\" the data and \"learn\" from it.","metadata":{}},{"cell_type":"code","source":"model = lr.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"R-squared is the measure of how close the data are to the fitted regression line, in other words it measures the strength of the relationship between the model and the SalePrice on a convenient 0 – 100% scale.","metadata":{}},{"cell_type":"code","source":"# make predictions based on model\npredictions = model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are three primary metrics used to evaluate linear models. These are:\n\n- Mean absolute error (MAE)\n- Mean squared error (MSE)\n- Root mean squared error (RMSE)\n    * MAE: The easiest to understand. Represents average error.\n    * MSE: Similar to MAE but noise is exaggerated and larger errors are \"punished\". It is harder to interpret than MAE as it's not in base units, however, it is generally more popular.\n    * RMSE: Most popular metric, similar to MSE, however, the result is square rooted to make it more interpretable as it's in base units. It is recommended that RMSE be used as the primary metric to interpret your model.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom math import sqrt\n\nprint ('MAE is:', mean_absolute_error(y_test, predictions))\nprint ('MSE is:', mean_squared_error(y_test, predictions))\nprint ('RMSE is:', sqrt(mean_squared_error(y_test, predictions)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# alpha helps to show overlapping data\nplt.scatter(predictions, y_test, alpha = 0.7, color = 'b')\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Id'] = app_test_poly['Id'].astype(int)\ntemp = app_test_poly.select_dtypes(include = [np.number]).drop(['Id'], axis = 1).interpolate()\npredictions = model.predict(app_test_poly)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# submission = pd.DataFrame()\n# submission['Id'] = app_test['Id'].astype(int)\n# temp = app_test.select_dtypes(include = [np.number]).drop(['Id'], axis = 1).interpolate()\n# predictions = model.predict(app_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['SalePrice'] = predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The submission has now been saved to the virtual environment in which our notebook is running. To access the submission, at the end of the notebook, we will hit the blue Commit & Run button at the upper right of the kernel. This runs the entire notebook and then lets us download any files that are created during the run. \n\nOnce we run the notebook, the files created are available in the Versions tab under the Output sub-tab. From here, the submission files can be submitted to the competition or downloaded. Since there are several models in this notebook, there will be multiple output files. \n\n__Our script hits  score around 0.34661 when submitted.__\n\nIf we use the original dataset with missing values filled in but no additional features, our final score is - 0.38272","metadata":{"_uuid":"11e18bd5c4e75b931f90a22bc6ff84441a13570c"}},{"cell_type":"markdown","source":"# Conclusions\n\nIn this notebook, we saw how to get started with a Kaggle machine learning competition. We first made sure to understand the data, our task, and the metric by which our submissions will be judged. Then, we performed a fairly simple EDA to try and identify relationships, trends, or anomalies that may help our modeling. Along the way, we performed necessary preprocessing steps such as encoding categorical variables, imputing missing values, and scaling features to a range. Then, we constructed new features out of the existing data to see if doing so could help our model. \n\nOnce the data exploration, data preparation, and feature engineering was complete, we implemented a baseline model upon which we hope to improve. Then we built a second slightly more complicated model to beat our first score. We also carried out an experiment to determine the effect of adding the engineering variables. \n\nWe followed the general outline of a [machine learning project](https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420): \n\n1.  Understand the problem and the data\n2. Data cleaning and formatting (this was mostly done for us)\n3. Exploratory Data Analysis\n4. Baseline model\n5.  Improved model\n\n\nMachine learning competitions do differ slightly from typical data science problems in that we are concerned only with achieving the best performance on a single metric and do not care about the interpretation. However, by attempting to understand how our models make decisions, we can try to improve them or examine the mistakes in order to correct the errors. In future notebooks we will look at incorporating more sources of data, building more complex models (by following the code of others), and improving our scores. \n\nI hope this notebook was able to get you up and running in this machine learning competition and that you are now ready to go out on your own - with help from the community - and start working on some great problems! \n\n__Running the notebook__: now that we are at the end of the notebook, you can hit the blue Commit & Run button to execute all the code at once. After the run is complete (this should take about 2 minutes), you can then access the files that were created by going to the versions tab and then the output sub-tab. The submission files can be directly submitted to the competition from this tab or they can be downloaded to a local machine and saved. ","metadata":{"_uuid":"fd407ca0f7c5c50ee71fe5c8532eabeb92c15c50"}},{"cell_type":"markdown","source":"I hope you liked this code, I also prepared more interesting laptops for this competition and I will be glad to share them with you:\n\n1. [COMPREHENSIVE DATA EXPLORATION WITH PYTHON](https://www.kaggle.com/andrej0marinchenko/comprehensive-data-exploration-with-python-upd)\n2. [Data ScienceTutorial for Beginners ](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-house-prices)\n3. [House Price Calculation methods for beginnners](https://www.kaggle.com/andrej0marinchenko/house-price-calculation-methods-for-beginnners)\n4. [Start: Introduction for beginners ](https://www.kaggle.com/andrej0marinchenko/start-introduction-for-beginners-house-prices)\n5. [EDA + Data Analytics For beginners](https://www.kaggle.com/andrej0marinchenko/eda-data-analytics-for-beginners-house-prices)\n6. [1 step for beginners linear model](https://www.kaggle.com/andrej0marinchenko/1-step-for-beginners-linear-model-house-prices)\n7. [Universal notebook 4 data analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-4-data-analysis)","metadata":{}}]}