{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"'''Universal data analysis laptop. It only takes one line to replace - reading your data file'''\n# This is a sample Python script.\nimport pandas as pd\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#we will split the train set into train and test data in future sections\ndata_raw = pd.read_csv('../input/human-trafficking/HT_2013-2020.csv')\n\n\n#to play with our data we'll create a copy\n#remember python assignment or equal passes by reference vs values, so we use the copy function: https://stackoverflow.com/questions/46327494/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs\ndata1 = data_raw.copy(deep = True)\n\n#preview data\n\nprint(\"\\n ----------Top-5- Record----------\")\nprint(data_raw.head(5))  #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\n# print(data_raw.tail(5)) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.tail.html\n# print(data_raw.sample(10)) #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sample.html\nprint(\"\\n -----------Information-----------\")\nprint(data_raw.info())  #https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html\nprint(\"\\n -----------Data Types-----------\")\nprint(data_raw.dtypes)\nprint(\"\\n ----------Missing value-----------\")\nprint(data_raw.isnull().sum())\nprint(\"\\n ----------Null value-----------\")\nprint(data_raw.isna().sum())\nprint(\"\\n ----------Shape of Data----------\")\nprint(data_raw.shape)\nprint(\"\\n ----------Number of duplicates----------\")\nprint('Number of duplicates:', len(data_raw[data_raw.duplicated()]))\n\n# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n\nmissing_values_data = missing_values_table(data1)\nprint(\"\\n ----------Missing values----------\")\nprint(missing_values_data.head(30))\n\nprint(\"\\n ----------Number of types----------\")\n# Number of each type of column\nprint(data1.dtypes.value_counts())\n\nprint(\"\\n ----------Number of uniques----------\")\n# Let's now look at the number of unique entries in each of the object (categorical) columns.\nprint(data1.select_dtypes('object').apply(pd.Series.nunique, axis = 0))\n\nprint(\"\\n ----------Describe of tables----------\")\nprint(data_raw.describe(include = 'all'))\n\n\n#preview data again\nprint(data1.corr())","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.382298,"end_time":"2022-02-06T05:35:32.339013","exception":false,"start_time":"2022-02-06T05:35:30.956715","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-07T10:40:37.078443Z","iopub.execute_input":"2022-02-07T10:40:37.0803Z","iopub.status.idle":"2022-02-07T10:40:38.665431Z","shell.execute_reply.started":"2022-02-07T10:40:37.080136Z","shell.execute_reply":"2022-02-07T10:40:38.664422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(data1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()\n\ndata1.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8); # ; avoid having the matplotlib verbose informations","metadata":{"papermill":{"duration":5.832615,"end_time":"2022-02-06T05:35:38.17749","exception":false,"start_time":"2022-02-06T05:35:32.344875","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-07T10:40:38.667321Z","iopub.execute_input":"2022-02-07T10:40:38.667588Z","iopub.status.idle":"2022-02-07T10:40:40.779889Z","shell.execute_reply.started":"2022-02-07T10:40:38.667554Z","shell.execute_reply":"2022-02-07T10:40:40.77868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set()\nsns.pairplot(data1, size = 2.5)\nplt.show();","metadata":{"papermill":{"duration":50.264057,"end_time":"2022-02-06T05:36:28.453052","exception":false,"start_time":"2022-02-06T05:35:38.188995","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-07T10:40:40.781824Z","iopub.execute_input":"2022-02-07T10:40:40.782304Z","iopub.status.idle":"2022-02-07T10:40:49.312469Z","shell.execute_reply.started":"2022-02-07T10:40:40.782255Z","shell.execute_reply":"2022-02-07T10:40:49.311615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I hope you liked this code, I also prepared more interesting laptops for this competition and I will be glad to share them with you:\n\n1. [COMPREHENSIVE DATA EXPLORATION WITH PYTHON](https://www.kaggle.com/andrej0marinchenko/comprehensive-data-exploration-with-python-upd)\n2. [Data ScienceTutorial for Beginners ](https://www.kaggle.com/andrej0marinchenko/data-sciencetutorial-for-beginners-house-prices)\n3. [House Price Calculation methods for beginnners](https://www.kaggle.com/andrej0marinchenko/house-price-calculation-methods-for-beginnners)\n4. [Start: Introduction for beginners ](https://www.kaggle.com/andrej0marinchenko/start-introduction-for-beginners-house-prices)\n5. [EDA + Data Analytics For beginners](https://www.kaggle.com/andrej0marinchenko/eda-data-analytics-for-beginners-house-prices)\n6. [1 step for beginners linear model](https://www.kaggle.com/andrej0marinchenko/1-step-for-beginners-linear-model-house-prices)\n7. [Universal notebook 4 data analysis](https://www.kaggle.com/andrej0marinchenko/universal-notebook-4-data-analysis)","metadata":{}}]}