{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py","version":"3.6.1","codemirror_mode":{"name":"ipython","version":3}}},"nbformat":4,"nbformat_minor":1,"cells":[{"cell_type":"markdown","metadata":{"_uuid":"fedb804569c9119c7f25470dc27bec3b2e9df488","_cell_guid":"66130e60-e247-41b4-8b85-a693957e299f"},"source":"Hey, I was predicting house prices for some time now - thought I'd share some of my major lessons."},{"cell_type":"code","metadata":{"_uuid":"f8f2553ef10608c04e544031f8eb266e2f57eeb0","collapsed":true,"_cell_guid":"7af97c72-c731-471e-90d0-05b766646b1f"},"execution_count":null,"outputs":[],"source":"import pandas as pd\nimport numpy\nfrom sklearn import linear_model\n\n# init\ntrain_2d = pd.read_csv(\"../input/train.csv\")\npred_2d = pd.read_csv(\"../input/test.csv\")\nx_train_2d, y_train_1d, x_pred_2d = train_2d.loc[:, \"MSSubClass\":\"SaleCondition\"], train_2d[\"SalePrice\"], pred_2d.loc[:, \"MSSubClass\":\"SaleCondition\"]\nx_2d = pd.concat((x_train_2d, x_pred_2d)); x_2d = x_2d.reset_index(); "},{"cell_type":"markdown","metadata":{"_uuid":"8f30ff3b19b32408c1345a1581f740e99eee177a","_cell_guid":"1a56268c-8d58-48b9-99d3-9f06579b2d18"},"source":"* logging/normalizing gave a major boost to linear regression (0.135-&gt;0.120~). Though this is a subtle thing - make sure you only log the correct features, because some features really hurt the score. Also note XGB sometime does not like logged features."},{"cell_type":"code","metadata":{"_uuid":"397178a84d6225e27766083cd54d4eb260a3f339","collapsed":true,"_cell_guid":"10807c94-fe0b-4eab-97cc-99c2c4015847"},"execution_count":null,"outputs":[],"source":"# example\ncur_x_train_2d = x_train_2d.copy()\ncur_x_train_2d['GrLivArea'] = numpy.log1p(cur_x_train_2d['GrLivArea'])"},{"cell_type":"markdown","metadata":{"_uuid":"780fa9435e33b173a32174f36d1a89d5f66845e5","_cell_guid":"0b08f414-7318-4e33-8fb5-53aada6b9d4e"},"source":"* engineer new features - make many and trust your model to pick the best. This improved my score drastically (at the time it was 0.119 -&gt; 0.116 for my lasso regression model)"},{"cell_type":"code","metadata":{"_uuid":"0fbb439abed8a348b543d4f65d861f2baa3289ba","collapsed":true,"_cell_guid":"29a8d2f3-43a5-4684-b32c-cde17d71ee9d"},"execution_count":null,"outputs":[],"source":"# example\nx_2d[\"YrSold\"] = 2010 - x_2d[\"YrSold\"]"},{"cell_type":"markdown","metadata":{"_uuid":"aa2b85c7add98c27df440e7ecaeacc3fdbbae2aa","_cell_guid":"530c509a-9daf-4969-8408-48ec97b8da9a"},"source":"* clean data according to what makes sense. Worry less about CV score if the change makes sense - though still have a look it doesn't hurt results too much. For me, if the change made sense and didn't hurt results more than 0.0003 then I let it stay. Common fix types: fill nans according to what makes sense (mostly just some string), convert ordered classes to numeric and vice versa, removing the index feature, dummy variables, etc"},{"cell_type":"code","metadata":{"_uuid":"042a96ceec982ce979d28d5b3a65fbc1b7ebdd65","collapsed":true,"_cell_guid":"df047ba7-cb8a-44a7-b630-ac1d614881c1"},"execution_count":null,"outputs":[],"source":"# example\nx_2d['LotShape'] = x_2d['LotShape'].replace({'Reg': 4, 'IR1': 3, 'IR2': 2, 'IR3': 1})"},{"cell_type":"markdown","metadata":{"_uuid":"9699cc008c5aa3d3ec560331daeb8d109a0b0a69","_cell_guid":"976f8225-b4af-4439-9b4f-225c1795a70f"},"source":"* remove outliers. Another major boost was due to proper outliers removal (I removed like 20 observations overall)."},{"cell_type":"code","metadata":{"_uuid":"86e85f4b7dd1d9a8043c875eb1fe4aa8e07f71de","collapsed":true,"_cell_guid":"100fd0b6-8976-4398-9996-f024d1dec549"},"execution_count":null,"outputs":[],"source":"# example\nx_2d, y_train_1d = x_2d.drop([1298]), y_train_1d.drop([1298])"},{"cell_type":"markdown","metadata":{"_uuid":"a8aa6cab001fde357c5eff8e14ec4fce206584e2","_cell_guid":"a631417b-9376-43ad-87a3-37414452f208"},"source":"* ensemble. My ensemble was just an avg between xgb (score 0.11366) and lasso regression (score 0.11594). Actually I had in mind that this avg is just temporary and had several plans to much improve it, but decided I'd move on to the Zillow's competition instead."},{"cell_type":"code","metadata":{"_uuid":"fc18e52859089fa5fab548bd839553a32af0b823","collapsed":true,"_cell_guid":"fe3eff98-a19e-4445-ab30-1e6e0a2ee7b4"},"execution_count":null,"outputs":[],"source":"# add some filler values for the sake of the example\nlinear_y_pred_1d = numpy.ones(100); xgb_y_pred_1d = numpy.ones(100)\n# note that sometime it's better to do 0.6, 0.4 or 0.7 0.3. depends on relative strength of models.\navg_pred_1d = (linear_y_pred_1d * 0.5 + xgb_y_pred_1d * 0.5)"},{"cell_type":"markdown","metadata":{"_uuid":"d1cca45b01ac7cbf845a7de1c717554e14435226","_cell_guid":"3ec31ab6-50f5-4adf-a8e0-2493da0853d4"},"source":"* make sure to optimize parameters once in the start (so that your CV score is somewhat relevant) and then once again in the end after all your data tweaks (this improved my score from 10th place to 3rd). But not too much in the middle of the competition in my opinion (spend this making progress with data or so)."},{"cell_type":"code","metadata":{"_uuid":"245651f51830d38f09d205b54f35fb91ffb1e332","collapsed":true,"_cell_guid":"989a9386-3497-4671-bad3-d9cc1987444f"},"execution_count":null,"outputs":[],"source":"# my eventual lasso parametrs\nregr = linear_model.Lasso(max_iter=1e6, alpha=5e-4)"}]}