{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The task is in this kaggle competition: Predict sales prices","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None\n\nimport warnings\nwarnings.simplefilter(action =\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nimport math\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nimport sklearn.model_selection as GridSearchCV\nimport sklearn.model_selection as ms\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyse statically insight of train data\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyse statically insight of test data\ntest.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The train data size: {train.shape}\")\nprint(f\"The test data size: {test.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets check that test dataset has all the columns in train dataset except SalePrice","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"diff_train_test = set(train.columns) - set(test.columns)\ndiff_train_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The principal of this challenge is, predict the final sale price. So get the info about the column of \"SalePrice\":\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"SalePrice\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Checking skewness for \"SalePrice\" is therefore an important process because it can give us clearer idea of the orientation\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Skewness of SalePrice: {train['SalePrice'].skew()}\")\nprint(f\"Kurtosis of SalePrice: {train['SalePrice'].kurt()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot a histogram and kernel density estimate for SalePrice target\nsns.distplot(train[\"SalePrice\"], color = \"#330033\");\nplt.xlabel(\"Sale price\", fontsize = 14, color = \"#330033\" );","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The SalePrice is skewed to the right. solving this problem by using g(1+x) tranform to fix the skew.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(np.log1p(train[\"SalePrice\"]));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Get SalePrice with normally distributed, by using log transformation and it may give us better picture of the data.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.distplot(train[\"SalePrice\"], bins=20, kde=False, fit=stats.norm);\nplt.title(\"Distribution of SalePrice\")\n\n# Get the fitted parameters used by sns\n(mu, sigma) = stats.norm.fit(train[\"SalePrice\"])\nprint(\"mu={:.2f}, sigma={:.2f}\".format(mu, sigma))\n\n# Legend and labels \nplt.legend([\"Normal dist. fit ($\\mu=${:.2f}, $\\sigma=${:.2f})\".format(mu, sigma)])\nplt.ylabel(\"Frequency\")\n\n# Cross-check this is indeed the case - should be overlaid over black curve\nx_dummy = np.linspace(stats.norm.ppf(0.01), stats.norm.ppf(0.99), 100)\nax.plot(x_dummy, stats.norm.pdf(x_dummy, mu, sigma))\nplt.legend([\"Normal dist. fit ($\\mu=${:.2f}, $\\sigma=${:.2f})\".format(mu, sigma), \"cross-check\"]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define numerical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = train.select_dtypes(include=[np.number])\ncorr_numeric_features = numeric_features.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation Numeric featurs with output variable(SalePrice)\ncorr_target = abs(corr_numeric_features[\"SalePrice\"])\nprint(f\"Correlation between numeric featurs and SalePrice:\\n{corr_target.sort_values()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Selecting highly correlated features with SalePrice\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"relevant_features = corr_target[corr_target > 0.6]\nrelevant_features\nprint(f\"Selecting highly correlated numeric features with SalePrice:\\n{relevant_features}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"highly_correlated_visualization = sns.pairplot(train[[\"OverallQual\",  \"TotalBsmtSF\", \"1stFlrSF\", \"GrLivArea\", \"GarageCars\", \"GarageArea\", \"SalePrice\"]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train[\"SalePrice\"].to_frame()\n\n#Combine train and test sets\nconcat_data = pd.concat((train, test), sort=False).reset_index(drop=True)\n#Drop the target \"SalePrice\" and Id columns\nconcat_data.drop([\"SalePrice\"], axis=1, inplace=True)\nconcat_data.drop([\"Id\"], axis=1, inplace=True)\nprint(\"Total size is :\",concat_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concat_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"concat_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the null columns\nnull_columns = concat_data.columns[concat_data.isnull().any()]\nconcat_data[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Droping columns \"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"FireplaceQu\" because they have less impact on the result.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = concat_data.drop(columns=[\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"FireplaceQu\"], axis = 1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The full data size: {df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data claening","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the null columns\nnull_columns = df.columns[df.isnull().any()]\ndf[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Working with Numeric Features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_features = df.select_dtypes(include=[np.number])\nnumeric_features.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Numerical features: {numeric_features.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_list_numeric_features = [(item, np.count_nonzero(df[item].unique())) for item in numeric_features]\nprint(f\"Unique numeric features:\\n{unique_list_numeric_features}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Corralation between Numeric features \ncorr_numeric_features = numeric_features.corr()\n\n#Using Pearson Correlation\nplt.figure(figsize=(30, 30))\n\nsns.heatmap(corr_numeric_features, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap=\"Blues\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Count the null columns' numeric_features in data set\nnull_columns_numeric_features = numeric_features.columns[numeric_features.isnull().any()]\nprint(f\"Missing values in numerical features: \\n{numeric_features[null_columns_numeric_features].isnull().sum()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing missing values for columns: \"LotFrontage\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\" With \"0\" because \"Na\" value in those columns represent the absence of what is being measured:\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_absence_zero = [\"LotFrontage\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\"]\ndf[cols_absence_zero] = df[cols_absence_zero].replace(to_replace = np.nan, value = 0) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing missing values for \"GarageYrBlt\" column with the value of the year as the house was built.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"GarageYrBlt\"] = df.apply(\n    lambda row: row[\"YearBuilt\"] if np.isnan(row[\"GarageYrBlt\"]) else row[\"GarageYrBlt\"],\n    axis=1\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing missing values for columns:  \"GarageCars\", \"GarageArea\" with Median\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def ReplaceNanWithMedian(df, featureName):\n    median = df.loc[:,featureName].median()\n\n    df[featureName] = df.apply(lambda row: median if np.isnan(row[featureName]) else row[featureName], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ReplaceNanWithMedian(df, \"GarageCars\")\nReplaceNanWithMedian(df, \"GarageArea\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Working with non-numeric Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categoricals = df.select_dtypes(exclude=[np.number])\ncategoricals.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Categorical features: {categoricals.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # Count the null columns\nnull_columns = categoricals.columns[categoricals.isnull().any()]\nprint(f\"Missing values in categorical features: \\n{categoricals[null_columns].isnull().sum()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing missing values for columns: \"MasVnrType\", \"BsmtQual\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"BsmtCond\" With \"None\" because \"Na\" value in those columns represent the absence of what is being measured:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_absence_none = [\"MasVnrType\", \"BsmtQual\", \"BsmtExposure\",\"BsmtFinType1\", \"BsmtFinType2\",\"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"BsmtCond\"]\ndf[cols_absence_none] = df[cols_absence_none].replace(to_replace = np.nan, value = \"None\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Replacing missing values by the most common value for columns: \"MSZoning\", \"Utilities\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Electrical\", \"KitchenQual\", \"Functional\", \"SaleType\"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def ReplaceNanWithMostFrequent(df, featureName):\n    df[featureName] = df.apply(lambda x:x.fillna(x.value_counts().index[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ReplaceNanWithMostFrequent(df, \"MSZoning\")\nReplaceNanWithMostFrequent(df, \"Utilities\")\nReplaceNanWithMostFrequent(df, \"Exterior1st\")\nReplaceNanWithMostFrequent(df, \"Exterior2nd\")\nReplaceNanWithMostFrequent(df, \"MasVnrType\")\nReplaceNanWithMostFrequent(df, \"Electrical\")\nReplaceNanWithMostFrequent(df, \"KitchenQual\")\nReplaceNanWithMostFrequent(df, \"Functional\")\nReplaceNanWithMostFrequent(df, \"SaleType\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # Check the null columns in data set\nnull_columns = df.columns[df.isnull().any()]\ndf[null_columns].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if the all of the columns have 0 null values.\nsum(df.isnull().sum() != 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"The Shape of all data: {df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming some numerical variables that are really categorical\ndf[\"MSSubClass\"] = df[\"MSSubClass\"].apply(str)\n\n#Changing OverallCond into a categorical variable\ndf[\"OverallCond\"] = df[\"OverallCond\"].astype(str)\n\n#Year and month sold are transformed into categorical features.\ndf[\"YrSold\"] = df[\"YrSold\"].astype(str)\ndf[\"MoSold\"] = df[\"MoSold\"].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### One Hot Encoding: Convert categorical variables into dummy variable\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = pd.get_dummies(df).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Original dataset shape: {df.shape}\")\nprint(f\"Encoded dataset shape: {final_df.shape}\")\nprint(f\"We have: {final_df.shape[1] - df.shape[1]} new encoded features\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainData = final_df[:ntrain] \nTestData = final_df[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainData.shape, TestData.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Working with outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Plotting the scatter diagram between sale price(dependent) and highly correlated numeric features but two featurs \"GarageArea\" and \"GarageCars\" are not useful for the model. We replace them with \"LotArea\" and \"YearBuilt\":","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax_arr = plt.subplots(3, 2, figsize=(14, 14))\n\nax_arr[0, 0].scatter(x = train[\"OverallQual\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[0, 0].set_title(\"House price Vs Overall material and finish quality\", fontsize=14, color=\"#330033\")\n\nax_arr[0, 1].scatter(x = train[\"TotalBsmtSF\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[0, 1].set_title(\"House price Vs Total square feet of basement area\", fontsize=14, color=\"#330033\")\n\nax_arr[1, 0].scatter(x = train[\"1stFlrSF\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[1, 0].set_title(\"House price Vs First Floor square feet\", fontsize=14, color=\"#330033\")\n\nax_arr[1, 1].scatter(x = train[\"GrLivArea\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[1, 1].set_title(\"House price Vs Above grade (ground) living area square feet\", fontsize=14, color=\"#330033\")\n\nax_arr[2, 0].scatter(x = train[\"LotArea\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[2, 0].set_title(\"House price Vs Lot size in square feet\", fontsize=14, color=\"#330033\")\n\nax_arr[2, 1].scatter(x = train[\"YearBuilt\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[2, 1].set_title(\"House price Vs Original construction date\", fontsize=14, color=\"#330033\")\n\nplt.tight_layout()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OverallQual_visualization = sns.swarmplot(y = \"SalePrice\", x = \"OverallQual\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nOverallQual_visualization.figure.set_size_inches(14,10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TotalBsmtSF_visualization = sns.swarmplot(y = \"SalePrice\", x = \"TotalBsmtSF\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nTotalBsmtSF_visualization.figure.set_size_inches(14,10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"StFlrSF_visualization = sns.swarmplot(y = \"SalePrice\", x = \"1stFlrSF\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nStFlrSF_visualization.figure.set_size_inches(14,10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LivArea_visualization = sns.swarmplot(y = \"SalePrice\", x = \"GrLivArea\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nLivArea_visualization.figure.set_size_inches(14,10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LotArea_visualization = sns.swarmplot(y = \"SalePrice\", x = \"LotArea\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nLotArea_visualization.figure.set_size_inches(14,10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"YearBuilt_visualization = sns.swarmplot(y = \"SalePrice\", x = \"YearBuilt\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nYearBuilt_visualization.figure.set_size_inches(14,10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MasVnrArea_visualization = sns.swarmplot(y = \"SalePrice\", x = \"MasVnrArea\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nMasVnrArea_visualization.figure.set_size_inches(14,10)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = TrainData[(TrainData[\"GrLivArea\"] < 4600) & (TrainData[\"MasVnrArea\"] < 1500)]\nprint(f\"We removed: {TrainData.shape[0]- train_df.shape[0]} outliers\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Encoded dataset shape: {final_df.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target = train[[\"SalePrice\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos = [1298,523, 297]\ntarget.drop(target.index[pos], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"We make sure that both train and target sets have the same row number after removing the outliers:\")\nprint(f\"Train: {train_df.shape[0]} rows\")\nprint(f\"Target: {target.shape[0]} rows\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Skewness before log transform: {target.SalePrice.skew()}\")\nprint(f\"Kurtosis before log transform: {target.SalePrice.kurt()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target[\"SalePrice\"] = np.log1p(target[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Skewness after log transform: {target.SalePrice.skew()}\")\nprint(f\"Kurtosis after log transform: {target.SalePrice.kurt()}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Remove any duplicated column names\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = final_df.loc[:,~final_df.columns.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train_df\ny = np.array(target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data set into train and test sets \nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Regularization reduces the magnitude of the features’ coefficients to improve the accuracy of a model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Scince we have outliers for scaling data using the mean and variance of the data is likely to not work very well. In this case, we can use robust_scale and RobustScaler as drop-in replacements instead.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = RobustScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform \"x_train\"\nx_train = scaler.fit_transform(x_train)\n# transform \"x_test\"\nx_test = scaler.transform(x_test)\n#Transform the test set\nX_test= scaler.transform(TestData)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ridge regression model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge()\nparameters = {\"alpha\":[x for x in range(1,101)]}\nridge_regressor = ms.GridSearchCV(ridge, param_grid = parameters, scoring = \"neg_mean_squared_error\", cv = 15)\nridge_regressor_mod = ridge_regressor.fit(x_train, y_train)\nprint(f\"Best parameter for Ridge regression: {ridge_regressor_mod.best_params_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge(alpha = 13)\nridge_mod = ridge.fit(x_train, y_train)\npred_ridge = ridge_mod.predict(x_test) \nmse_ridge = mean_squared_error(y_test,pred_ridge)\nrmse_ridge = np.sqrt(mean_squared_error(y_test, pred_ridge))\nscore_ridge_train = ridge_mod.score(x_train, y_train)\nscore_ridge_test = ridge_mod.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean Square Error for Ridge regression = {mse_ridge}\")\nprint(f\"Root Mean Square Error for Ridge regression = {rmse_ridge}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_ridge_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_ridge_test}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print coefficients\nprint(f\"Ridge coefficient:\\n {ridge.coef_}\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lasso regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"parameters= {\"alpha\":[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n\nlasso = Lasso()\nlasso_reg = ms.GridSearchCV(lasso, param_grid = parameters, scoring = \"neg_mean_squared_error\", cv = 15)\nlasso_reg.fit(x_train,y_train)\n\nprint(\"The best value of Alpha for Lasso regression is: \",lasso_reg.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(alpha = 0.0009)\nlasso_mod = lasso.fit(x_train, y_train)\npred_lasso = lasso_mod.predict(x_test) \n\nmse_lasso = mean_squared_error(y_test,pred_lasso)\nrmse_lasso = np.sqrt(mean_squared_error(y_test, pred_lasso))\nscore_lasso_train = lasso_mod.score(x_train, y_train)\nscore_lasso_test = lasso_mod.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean Square Error for Lasso regression = {mse_lasso}\")\nprint(f\"Root Mean Square Error for Lasso regression = {rmse_lasso}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_lasso_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_lasso_test}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print coefficients\nprint(f\"Lasso coefficient:\\n {lasso.coef_}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coefs = pd.Series(lasso_mod.coef_, index = x.columns)\nplt.figure(figsize=(20, 20))\n\nimp_coefs = pd.concat([coefs.sort_values().head(10), coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\", color = \"#800080\")\nplt.xlabel(\"Lasso coefficient\", weight = \"bold\")\nplt.title(\"Feature importance in the Lasso Model\", weight = \"bold\", color = \"#800080\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Lasso kept \",sum(coefs != 0), \"important features and dropped the other \", sum(coefs == 0),\" features\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Elastic Net CV Regression ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"alphas = [0.0005]\nl1ratio = [0.9]\n\nelastic_net_cv = ElasticNetCV(cv = 5, max_iter = 1e7, alphas = alphas,  l1_ratio = l1ratio)\nelastic_mod = elastic_net_cv.fit(x_train, y_train.ravel())\npred_elastic = elastic_mod.predict(x_test) \n\nmse_elastic = mean_squared_error(y_test, pred_elastic)\nrmse_elastic = np.sqrt(mean_squared_error(y_test, pred_elastic))\nscore_elastic_train = elastic_mod.score(x_train, y_train)\nscore_elastic_test = elastic_mod.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean Square Error for Elastic Net CV regression = {mse_elastic}\")\nprint(f\"Root Mean Square Error for Elastic Net CV regression = {rmse_elastic}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_elastic_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_elastic_test}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print coefficients\nprint(f\"Elastic Net CV coefficient:\\n {lasso.coef_}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVR","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"svr = SVR(C = 20, epsilon = 0.008, gamma = 0.0003,)\nsvr_mod = svr.fit(x_train, y_train.ravel())\npred_svr = svr_mod.predict(x_test) \n\nmse_svr = mean_squared_error(y_test, pred_svr)\nrmse_svr = np.sqrt(mean_squared_error(y_test, pred_svr))\nscore_svr_train = svr_mod.score(x_train, y_train)\nscore_svr_test = svr_mod.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean Square Error for SVR = {mse_svr}\")\nprint(f\"Root Mean Square Error for SVR = {rmse_svr}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_svr_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_svr_test}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Gradient Boosting regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr = GradientBoostingRegressor(n_estimators = 3000, learning_rate = 0.05, max_depth = 4, max_features = \"sqrt\", min_samples_leaf = 15, min_samples_split = 10, loss = \"huber\", random_state = 42)\ngbr_mod = gbr.fit(x_train, y_train.ravel())\npred_gbr = gbr_mod.predict(x_test) \n\nmse_gbr = mean_squared_error(y_test, pred_gbr)\nrmse_gbr = np.sqrt(mean_squared_error(y_test, pred_gbr))\nscore_gbr_train = gbr_mod.score(x_train, y_train)\nscore_gbr_test = gbr_mod.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean Square Error for Gradient Boosting regression = {mse_gbr}\")\nprint(f\"Root Mean Square Error for Gradient Boosting regression = {rmse_gbr}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_gbr_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_gbr_test}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lightgbm.LGBMRegressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lightgbm = LGBMRegressor(objective = \"regression\", \n                                       num_leaves = 4,\n                                       learning_rate = 0.01, \n                                       n_estimators = 5000,\n                                       max_bin = 200, \n                                       bagging_fraction = 0.75,\n                                       bagging_freq = 5, \n                                       bagging_seed = 7,\n                                       feature_fraction = 0.2,\n                                       feature_fraction_seed = 7,\n                                       verbose = -1,\n                                       )\nlightgbm_mod = lightgbm.fit(x_train, y_train.ravel())\npred_lightgbm = lightgbm_mod.predict(x_test) \n\nmse_lightgbm = mean_squared_error(y_test, pred_lightgbm)\nrmse_lightgbm = np.sqrt(mean_squared_error(y_test, pred_lightgbm))\nscore_lightgbm_train = lightgbm_mod.score(x_train, y_train)\nscore_lightgbm_test = lightgbm_mod.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean Square Error for LGBMRegressor = {mse_lightgbm}\")\nprint(f\"Root Mean Square Error for LGBMRegressor = {rmse_lightgbm}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_lightgbm_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_lightgbm_test}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost = XGBRegressor(learning_rate = 0.01, n_estimators = 3460,\n                                     max_depth = 3, min_child_weight = 0,\n                                     gamma = 0, subsample = 0.7,\n                                     colsample_bytree = 0.7,\n                                     objective = \"reg:squarederror\", nthread=-1,\n                                     scale_pos_weight = 1, seed = 27,\n                                     reg_alpha = 0.00006)\nxgboost_mod = xgboost.fit(x_train, y_train)\npred_xgboost = xgboost_mod.predict(x_test) \n\nmse_xgboost = mean_squared_error(y_test, pred_xgboost)\nrmse_xgboost = np.sqrt(mean_squared_error(y_test, pred_xgboost))\nscore_xgboost_train = xgboost_mod.score(x_train, y_train)\nscore_xgboost_test = xgboost_mod.score(x_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Mean Square Error for xgboost regression = {mse_lightgbm}\")\nprint(f\"Root Mean Square Error for xgboost regression = {rmse_lightgbm}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_lightgbm_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_lightgbm_test}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ENSEMBLE METHODS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"vote = VotingRegressor([(\"Ridge\", ridge_mod), (\"Lasso\", ridge_mod), (\"Elastic Net CV\", elastic_net_cv), \n                        (\"SVR\", svr), (\"GradientBoostingRegressor\", gbr), (\"LGBMRegressor\", lightgbm), (\"XGBRegressor\", xgboost)])\nvote_mod = vote.fit(x_train, y_train.ravel())\nvote_pred = vote_mod.predict(x_test)\n\nprint(f\"Root Mean Square Error test for ENSEMBLE METHODS: {np.sqrt(mean_squared_error(y_test, vote_pred))}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stacking: to avoid fitting on the same data twice , and is effective in reducing overfitting.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_gen = StackingCVRegressor(regressors = [ridge_mod, ridge_mod, elastic_net_cv, svr, gbr, lightgbm, xgboost, vote],\n                                meta_regressor = xgboost,\n                                use_features_in_secondary = True)\nstack_mod = stack_gen.fit(x_train, y_train.ravel())\nstack_pred = stack_mod.predict(x_test)\n\nprint(f\"Root Mean Square Error test for STACKING REGRESSOR: {np.sqrt(mean_squared_error(y_test, vote_pred))}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Averaging Approach","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"averaged_preds = (vote_pred*0.3 + stack_pred*0.5 + pred_lasso*0.2)\n\nprint(f\"Root Mean Square Error test for STACKING REGRESSOR: {np.sqrt(mean_squared_error(y_test, averaged_preds))}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the result in a plot with averaging predict.\n\nplt.figure(figsize=(20,8))\n\nx_ax = range(len(x_test))\nplt.scatter(x_ax, y_test, s = 5, color=\"#422d42\", label = \"Original\")\nplt.plot(x_ax, averaged_preds, lw = 0.8, color = \"#9c6d9c\", label = \"Predicted\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#VotingRegressor to predict the final Test\nvote_test = vote.predict(X_test)\nfinal1 = np.expm1(vote_test)\n\n#StackingRegressor to predict the final Test\nstack_test = stack_gen.predict(X_test)\nfinal2 = np.expm1(stack_test)\n\n#LassoRegressor to predict the final Test\nlasso_test = lasso.predict(X_test)\nfinal3 = np.expm1(lasso_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test[\"Id\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final = (0.2*final1 + 0.6*final2 + 0.2*final3)\n\nfinal_submission = pd.DataFrame({\"Id\": test[\"Id\"], \"SalePrice\": final})\nfinal_submission.to_csv(\"final_submission.csv\", index=False)\nfinal_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Thank you for your consideration sincerely.*","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}