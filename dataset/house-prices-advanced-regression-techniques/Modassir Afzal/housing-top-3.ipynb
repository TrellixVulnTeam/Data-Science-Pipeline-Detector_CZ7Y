{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('max_colwidth', None)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/ames-housing-dataset/AmesHousing.csv')\ntest=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntrain2=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns = train.columns.str.replace(' ', '')\ntrain=train.rename(columns={\"YearRemod/Add\": \"YearRemodAdd\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Size of the Ames Dataset\",len(train))\nprint(\"Size of the Housing Dataset\",len(train2))\nprint(\"Size of the Housing Test Dataset\",len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding duplicates in data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data=pd.concat([train,train2,test], axis=0, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Size of the Housing Dataset\",len(data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"useless = ['Id','PID','Order','SalePrice'] \ndata = data.drop(useless, axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate = data[data.duplicated(keep='last')].index\nlen(duplicate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate[382:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate[390:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we want to delete duplicates till index 2902 which means only from the ames dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate=duplicate[0:390]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicate[386:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(duplicate, axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Length of the Ames Dataset now',len(train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training=pd.concat([train,train2], axis=0, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"useless = ['Id','PID','Order'] \ntraining = training.drop(useless, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\n(mu, sigma) = norm.fit(training['SalePrice'])\nplt.figure(figsize = (12,6))\nsns.distplot(training['SalePrice'], kde = True, hist=True, fit = norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"House's sale Price in $\", fontsize = 12)\nplt.legend(['Normal dist ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma),'actual price dist'],loc='best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In literature, acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis. Looking at the plot, we can clearly see how the distribution does not seem to be normal, but highly right-skewed. The non-normality of our distribution is also supported by the Shapiro test for normality (p-value really small that allows us to reject the hypotesis of normality). Despite that, let's leave it like that for now, we'll deal with that later in the notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\nshap = stats.shapiro(training['SalePrice'])\nprint('Skewness : %f' % abs(training['SalePrice']).skew())\nprint('Kurtosis : %f' % abs(training['SalePrice']).kurt())\nprint('Shapiro_Test_statistic : %f' % shap.statistic )\nprint('Shapiro_Test_pvalue : %f' % shap.pvalue )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(50, 35))\nmat = training.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OverallQuall - SalePrice [Pearson = 0.8]\nfig,ax=plt.subplots(1,3,figsize=(20,10))\nsns.stripplot(data=training,x='OverallQual',y='SalePrice',ax=ax[1])\nsns.violinplot(data=training,x='OverallQual',y='SalePrice',ax=ax[2])\nsns.boxplot(data=training,x='OverallQual',y='SalePrice',ax=ax[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# OverallQuall - SalePrice [Pearson = -0.011\nfig,ax=plt.subplots(1,3,figsize=(20,10))\nsns.stripplot(data=train,x='BsmtFinSF2',y='SalePrice',ax=ax[1])\nsns.violinplot(data=train,x='BsmtFinSF2',y='SalePrice',ax=ax[2])\nsns.boxplot(data=train,x='BsmtFinSF2',y='SalePrice',ax=ax[0])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# GrLivArea vs SalePrice [corr = 0.71]\n\nPearson_GrLiv = 0.71\nplt.figure(figsize = (12,6))\nsns.regplot(data=train, x = 'GrLivArea', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('GrLivArea vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_GrLiv)], loc = 'best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# YearBuilt vs SalePrice\n\nPearson_YrBlt = 0.56\nplt.figure(figsize = (12,6))\nsns.regplot(data=train, x = 'YearBuilt', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('YearBuilt vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_YrBlt)], loc = 'best')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\nsns.barplot(x='YrSold',y='SalePrice',data=train,estimator=np.median)\nplt.title('Median of Sale Price by Year')\nplt.xlabel('Year of Selling')\nplt.ylabel('Median of Price')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Separating Target and Features\n\ntarget = training['SalePrice']\ntest_id = test['Id']\ntest = test.drop(['Id'],axis = 1)\ntraining2 = training.drop(['SalePrice'], axis = 1)\n\n\n# Concatenating train & test set\n\ntrain_test = pd.concat([training2,test], axis=0, sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nan=pd.DataFrame(train_test.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']/1460)*100\nnan['Percentage of total data']=(nan['Nan_sum']/5459)*100\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.barplot(x=nan['feat'],y=nan['Percentage'])\nplt.xticks(rotation=40)\nplt.title('Features Containing Nan')\nplt.xlabel('Features')\nplt.ylabel('% of Missing Data')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting non-numeric predictors stored as numbers into string\n\ntrain_test['MSSubClass'] = train_test['MSSubClass'].apply(str)\ntrain_test['YrSold'] = train_test['YrSold'].apply(str)\ntrain_test['MoSold'] = train_test['MoSold'].apply(str)\ntrain_test['OverallQual'] = train_test['OverallQual'].apply(str)\ntrain_test['OverallCond'] = train_test['OverallCond'].apply(str)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling Categorical NaN (That we know how to fill due to the description file )\n\ntrain_test['Functional'] = train_test['Functional'].fillna('Typ')\ntrain_test['Electrical'] = train_test['Electrical'].fillna(\"SBrkr\")\ntrain_test['KitchenQual'] = train_test['KitchenQual'].fillna(\"TA\")\ntrain_test['Exterior1st'] = train_test['Exterior1st'].fillna(train_test['Exterior1st'].mode()[0])\ntrain_test['Exterior2nd'] = train_test['Exterior2nd'].fillna(train_test['Exterior2nd'].mode()[0])\ntrain_test['SaleType'] = train_test['SaleType'].fillna(train_test['SaleType'].mode()[0])\ntrain_test[\"PoolQC\"] = train_test[\"PoolQC\"].fillna(\"None\")\ntrain_test[\"Alley\"] = train_test[\"Alley\"].fillna(\"None\")\ntrain_test['FireplaceQu'] = train_test['FireplaceQu'].fillna(\"None\")\ntrain_test['Fence'] = train_test['Fence'].fillna(\"None\")\ntrain_test['MiscFeature'] = train_test['MiscFeature'].fillna(\"None\")\nfor col in ('GarageArea', 'GarageCars'):\n    train_test[col] = train_test[col].fillna(0)\n        \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    train_test[col] = train_test[col].fillna('None')\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train_test[col] = train_test[col].fillna('None')\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea','BsmtUnfSF', 'TotalBsmtSF'):\n    train_test[col] = train_test[col].fillna(0)\n\ntrain_test['LotFrontage'] = train_test['LotFrontage'].fillna(train['LotFrontage'].median())\n    \n    # Checking the features with NaN remained out\n\nfor col in train_test:\n    if train_test[col].isna().sum() > 0:\n        print(train_test[col][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test[\"SqFtPerRoom\"] = train_test[\"GrLivArea\"] / (train_test[\"TotRmsAbvGrd\"] +\n                                                       train_test[\"FullBath\"] +\n                                                       train_test[\"HalfBath\"] +\n                                                       train_test[\"KitchenAbvGr\"])\n\ntrain_test['Total_Home_Quality'] = train_test['OverallQual'] + train_test['OverallCond']\n\ntrain_test['Total_Bathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) +\n                               train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n\ntrain_test[\"HighQualSF\"] = train_test[\"1stFlrSF\"] + train_test[\"2ndFlrSF\"]\ntrain_test['renovated']=train_test['YearRemodAdd']+train_test['YearBuilt']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Removing the useless variables\n\nuseless = ['GarageYrBlt','YearRemodAdd'] \ntrain_test = train_test.drop(useless, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating dummy variables from categorical features\n\ntrain_test_dummy = pd.get_dummies(train_test)\nfrom scipy.stats import skew\nnumeric_features = train_test_dummy.dtypes[train_test_dummy.dtypes != object].index\nskewed_features = train_test_dummy[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    train_test_dummy[i] = np.log1p(train_test_dummy[i] )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking for Nan values after dummy"},{"metadata":{"trusted":true},"cell_type":"code","source":"nan=pd.DataFrame(train_test_dummy.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']/1460)*100\nnan['Perc']=(nan['Nan_sum']/5459)*100\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"checking if the values are in infinity or not after log transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"inf=pd.DataFrame(np.isinf(train_test_dummy).sum() ,columns=['Inf_sum'])\ninf['feat']=inf.index\ninf=inf[inf['Inf_sum']>0]\ninf=inf.sort_values(by=['Inf_sum'])\ninf.insert(0,'Serial No.',range(1,len(inf)+1))\ninf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\n# SalePrice before transformation\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\" qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\n#research sm \nsns.distplot(target, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"transforming the sale price "},{"metadata":{"trusted":true},"cell_type":"code","source":"# SalePrice after transformation\n\ntarget_log = np.log1p(target)\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\"qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target_log, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\nsns.distplot(target_log, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nfrom xgboost import XGBRegressor\nfrom catboost import Pool\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test.iloc[3999:4005]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train-Test separation\n\nX_train = train_test_dummy[0:4000]\nX_test = train_test_dummy[4000:]\n\n# Creation of the RMSE metric:\n    \ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train, target_log, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"checking for nan values in training set"},{"metadata":{"trusted":true},"cell_type":"code","source":"nan=pd.DataFrame(X_train.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']/1460)*100\nnan['Perc']=(nan['Nan_sum']/4000)*100\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"checking for nan values in test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"nan=pd.DataFrame(X_test.isna().sum(),columns=['Nan_sum'])\nnan['feat']=nan.index\nnan=nan[nan['Nan_sum']>0]\nnan['Percentage']=(nan['Nan_sum']/1460)*100\nnan['Perc']=(nan['Nan_sum']/2919)*100\nnan=nan.sort_values(by=['Nan_sum'])\nnan.insert(0,'Serial No.',range(1,len(nan)+1))\nnan","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10 Fold Cross validation\n\nkf = KFold(n_splits=11, random_state=42, shuffle=True)\n\ncv_scores = []\ncv_std = []\n\nbaseline_models = ['Linear_Reg.','Bayesian_Ridge_Reg.','LGBM_Reg.','SVR',\n                   'Dec_Tree_Reg.','Random_Forest_Reg.', 'XGB_Reg.',\n                   'Grad_Boost_Reg.','Cat_Boost_Reg.','Stacked_Reg.','Stacked_Reg2']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Linear Regression\n\nlreg = LinearRegression()\nscore_lreg = cv_rmse(lreg)\ncv_scores.append(score_lreg.mean())\ncv_std.append(score_lreg.std())\n\n# Bayesian Ridge Regression\n\nbrr = BayesianRidge(compute_score=True)\nscore_brr = cv_rmse(brr)\ncv_scores.append(score_brr.mean())\ncv_std.append(score_brr.std())\n\n# Light Gradient Boost Regressor\n\nl_gbm = LGBMRegressor(objective='regression')\nscore_l_gbm = cv_rmse(l_gbm)\ncv_scores.append(score_l_gbm.mean())\ncv_std.append(score_l_gbm.std())\n\n# Support Vector Regression\n\nsvr = SVR()\nscore_svr = cv_rmse(svr)\ncv_scores.append(score_svr.mean())\ncv_std.append(score_svr.std())\n\n# Decision Tree Regressor\n\ndtr = DecisionTreeRegressor()\nscore_dtr = cv_rmse(dtr)\ncv_scores.append(score_dtr.mean())\ncv_std.append(score_dtr.std())\n\n# Random Forest Regressor\n\nrfr = RandomForestRegressor()\nscore_rfr = cv_rmse(rfr)\ncv_scores.append(score_rfr.mean())\ncv_std.append(score_rfr.std())\n\n# XGB Regressor\n\nxgb = XGBRegressor()\nscore_xgb = cv_rmse(xgb)\ncv_scores.append(score_xgb.mean())\ncv_std.append(score_xgb.std())\n\n# Gradient Boost Regressor\n\ngbr = GradientBoostingRegressor()\nscore_gbr = cv_rmse(gbr)\ncv_scores.append(score_gbr.mean())\ncv_std.append(score_gbr.std())\n\n# Cat Boost Regressor\n\ncatb = CatBoostRegressor()\nscore_catb = cv_rmse(catb)\ncv_scores.append(score_catb.mean())\ncv_std.append(score_catb.std())\n\n# Stacked Regressor\n\nstack_gen = StackingRegressor(regressors=(CatBoostRegressor(),\n                                          BayesianRidge()),\n                              meta_regressor = CatBoostRegressor(),\n                              use_features_in_secondary = True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"score_stack_gen = cv_rmse(stack_gen)\ncv_scores.append(score_stack_gen.mean())\ncv_std.append(score_stack_gen.std())\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"# Stacked Regressor\n\nstack_gen2 = StackingRegressor(regressors=(CatBoostRegressor(),\n                                          XGBRegressor()),\n                              meta_regressor = CatBoostRegressor(),\n                              use_features_in_secondary = True)\n\nscore_stack_gen2 = cv_rmse(stack_gen2)\ncv_scores.append(score_stack_gen2.mean())\ncv_std.append(score_stack_gen2.std())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_cv_score = pd.DataFrame(baseline_models, columns = ['Regressors'])\nfinal_cv_score['RMSE_mean'] = cv_scores\nfinal_cv_score['RMSE_std'] = cv_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_cv_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,8))\nsns.barplot(final_cv_score['Regressors'],final_cv_score['RMSE_mean'])\nplt.xlabel('Regressors', fontsize = 12)\nplt.ylabel('CV_Mean_RMSE', fontsize = 12)\nplt.xticks(rotation=40)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat = CatBoostRegressor()\ncat_model = cat.fit(X_train,target_log,\n                     plot=True,\n                     verbose = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_imp = cat_model.get_feature_importance(prettified=True)\nfeat_imp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting top 30 features' importance\n\nplt.figure(figsize = (12,8))\nsns.barplot(feat_imp['Importances'][:30],feat_imp['Feature Id'][:30], orient = 'h')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {'iterations': 6000,\n          'learning_rate': 0.005,\n          'depth': 4,\n          'l2_leaf_reg': 1,\n          'eval_metric':'RMSE',\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': 42}\n         \ncat_f = CatBoostRegressor(**params)\ncat_model_f = cat_f.fit(X_train,target_log,\n                     plot=True,\n                     verbose = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = cat_f.predict(X_test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pred = np.expm1(test_pred)\nsubmission['SalePrice'] = test_pred \nsubmission.head()\nsubmission.to_csv(\"cat.csv\", index = False, header = True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"stack_f=stack_gen.fit(X_train,target_log)\ntest_stack = stack_gen.predict(X_test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pre = np.expm1(test_stack)\nsubmission['SalePrice'] = test_pre\n\nsubmission.to_csv(\"stack.csv\", index = False, header = True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"stack_f2=stack_gen2.fit(X_train,target_log)\ntest_stack = stack_gen2.predict(X_test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pre = np.expm1(test_stack)\nsubmission['SalePrice'] = test_pre\n\nsubmission.to_csv(\"stack2.csv\", index = False, header = True)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}