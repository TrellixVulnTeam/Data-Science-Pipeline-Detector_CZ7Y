{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<font color='red' size='5'> A detailed exploratory data analysis and regression models guide with House Price<font>\n\n<font color='darkblue' size='4'>This core is a step by step analysis from data mining to feature selection and modeling in order to solve House Pricing with Advanced Regression Analysis.\nHere are the different parts of this core:\n    \nI.\tExplarotory data analysis\n    \n    I.1. General exploration\n\n    I.2. Numerical features\n        I.2.1. Explore and clean Numerical features\n        I.2.2. Missing data of Numerical features\n\n    I.3. Categorical features\n        I.3.1. Explore and clean Categorical features\n        I.3.2. Missing data of Categorical features\n        I.3.3. Transform Categorical features into Binary features (get_dummies)\n\n    I.4. Merge numerical and binary features into one data set\n\n    I.5. Drop outliers from the train set\n    \n\nII.\tFeature engineering\n\nIII.\tPreparing data for modeling\n    \n    III.1. Target variable 'SalePrice'\n    \n    III.2. Split data into train and test and Standardization\n\n    III.3. Backward Stepwise Regression\n\n    III.4. Variance Inflation Factor\n\n    III.5. Cook distance\n\nIV.\tModeling\n\n    IV.1. Models and metrics selection\n\n    IV.2. Hyperparameters tuning and model optimization\n        IV.2.1. Ridge regression\n        IV.2.2. Lasso regression\n        IV.2.3. XGBoost regression\n        IV.2.4. LightGBM regression\n\n    IV.3. Choosing the best model\n\n    IV.4. Prediction on 'House Prices-Advanced Regression Techniques' test data set","metadata":{}},{"cell_type":"markdown","source":"<font color='darkred' size='4'>Data set: <font color='darkblue' size='4'>the training and test datasets were imported from the kaggle competition \"House Prices - Advanced Regression Techniques\".","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue' size='4'>This work was inspired from some of the kernels in the same competition as well as other sources and machine learning tutorials. All the steps from data mining to modeling are in this same notebook.\n    \n<font color='darkred' size='4'>Goal: <font color='darkblue' size='4'>the goal from this kernel is choose the best model that best fits our problem. Thus, in this kernel we will start by cleaning the data, processing the missing data, selecting the relevant variables, deducing some features, run statistical tests, defining the regression models and finally choosing the best price prediction model for the test set.","metadata":{}},{"cell_type":"markdown","source":"# <font color='darkblue'>I. Explarotory data analysis</font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue' size='3'>This exploratory analysis and feature engineering are mainly based on these Kaggle cores:\n    \n- [Comprehensive data exploration with python](https://www.kaggle.com/code/pmarcelino/comprehensive-data-exploration-with-python)\n    \n- [Detailed exploratory data analysis with python](https://www.kaggle.com/code/ekami66/detailed-exploratory-data-analysis-with-python)\n    \n- [House price just with categorical features](https://www.kaggle.com/code/hosseinbehjat/house-price-just-with-categorical-features)\n    \n- [Handling missing values](https://www.kaggle.com/code/dansbecker/handling-missing-values)","metadata":{}},{"cell_type":"markdown","source":"## <font color='darkblue'>I.1. General exploration</font>","metadata":{}},{"cell_type":"code","source":"# Load librarie\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nimport sklearn\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom scipy import stats\nfrom scipy.stats import chi2_contingency\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\nsns.set(rc={\"figure.figsize\": (20, 15)})\nsns.set_style(\"whitegrid\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:33.75074Z","iopub.execute_input":"2022-04-19T18:27:33.75108Z","iopub.status.idle":"2022-04-19T18:27:33.766942Z","shell.execute_reply.started":"2022-04-19T18:27:33.751044Z","shell.execute_reply":"2022-04-19T18:27:33.765641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Train set\ndf_train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\nprint(f\"Train set shape:\\n{df_train.shape}\\n\")\n\n# Load Test set\ndf_test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\nprint(f\"Test set shape:\\n{df_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:34.020983Z","iopub.execute_input":"2022-04-19T18:27:34.021779Z","iopub.status.idle":"2022-04-19T18:27:34.079504Z","shell.execute_reply.started":"2022-04-19T18:27:34.021733Z","shell.execute_reply":"2022-04-19T18:27:34.078614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# info of each of the variables in our train set\ndf_train.info()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-19T18:27:34.319914Z","iopub.execute_input":"2022-04-19T18:27:34.32042Z","iopub.status.idle":"2022-04-19T18:27:34.348039Z","shell.execute_reply.started":"2022-04-19T18:27:34.320388Z","shell.execute_reply":"2022-04-19T18:27:34.347417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>We can notice that many columns have missing data. I will handlle these missing values later in this notebook</font>","metadata":{}},{"cell_type":"code","source":"# Checking if column headings are the same in both data set\ndif_1 = [x for x in df_train.columns if x not in df_test.columns]\nprint(f\"Columns present in df_train and absent in df_test: {dif_1}\\n\")\n\ndif_2 = [x for x in df_test.columns if x not in df_train.columns]\nprint(f\"Columns present in df_test set and absent in df_train: {dif_2}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:34.7755Z","iopub.execute_input":"2022-04-19T18:27:34.776015Z","iopub.status.idle":"2022-04-19T18:27:34.784729Z","shell.execute_reply.started":"2022-04-19T18:27:34.775979Z","shell.execute_reply":"2022-04-19T18:27:34.783836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color = 'darkblue'> The only column present in the train set and absent in the test set is 'SalePrice' which is the target variable to be predicted. So it's OK !","metadata":{}},{"cell_type":"markdown","source":"<font color = 'red'>PS: <font color ='darkblue'>throughout our analysis, the df_test will receive the same data processing as df_train in order to have the same features in both datasets for modeling and prediction.<font>","metadata":{}},{"cell_type":"code","source":"# Drop the 'Id' column from the train set\ndf_train.drop([\"Id\"], axis=1, inplace=True)\n\n# Save the list of 'Id' before dropping it from the test set\nId_test_list = df_test[\"Id\"].tolist()\ndf_test.drop([\"Id\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:35.492095Z","iopub.execute_input":"2022-04-19T18:27:35.492369Z","iopub.status.idle":"2022-04-19T18:27:35.502033Z","shell.execute_reply.started":"2022-04-19T18:27:35.492339Z","shell.execute_reply":"2022-04-19T18:27:35.501039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='darkblue'>I.2. Numerical features</font>","metadata":{}},{"cell_type":"markdown","source":"### <font color='darkblue'>I.2.1. Explore and clean Numerical features</font>","metadata":{}},{"cell_type":"code","source":"# Let's select the columns of the train set with numerical data\ndf_train_num = df_train.select_dtypes(exclude=[\"object\"])\ndf_train_num.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:36.178027Z","iopub.execute_input":"2022-04-19T18:27:36.17861Z","iopub.status.idle":"2022-04-19T18:27:36.200751Z","shell.execute_reply.started":"2022-04-19T18:27:36.178541Z","shell.execute_reply":"2022-04-19T18:27:36.199909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's drop quasi-constant features where 95% of the values are similar or constant\nsel = VarianceThreshold(threshold=0.05) # 0.05: drop column where 95% of the values are constant\n\n# fit finds the features with constant variance\nsel.fit(df_train_num.iloc[:, :-1])\n\n\n# Get the number of features that are not constant\nprint(f\"Number of retained features: {sum(sel.get_support())}\")\nprint(\n    f\"\\nNumber of quasi_constant features: {len(df_train_num.iloc[:, :-1].columns) - sum(sel.get_support())}\")\n\nquasi_constant_features_list = [x for x in df_train_num.iloc[:, :-\n                                                             1].columns if x not in df_train_num.iloc[:, :-1].columns[sel.get_support()]]\n\nprint(\n    f\"\\nQuasi-constant features to be dropped: {quasi_constant_features_list}\")\n\n\n# Let's drop these columns from df_train_num\ndf_train_num.drop(quasi_constant_features_list, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:36.406747Z","iopub.execute_input":"2022-04-19T18:27:36.407309Z","iopub.status.idle":"2022-04-19T18:27:36.435674Z","shell.execute_reply.started":"2022-04-19T18:27:36.407258Z","shell.execute_reply":"2022-04-19T18:27:36.43457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the distribution of all the numerical data\nfig_ = df_train_num.hist(figsize=(16, 20), bins=50, color=\"deepskyblue\",\n                         edgecolor=\"black\", xlabelsize=8, ylabelsize=8)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:36.65056Z","iopub.execute_input":"2022-04-19T18:27:36.651182Z","iopub.status.idle":"2022-04-19T18:27:45.709026Z","shell.execute_reply.started":"2022-04-19T18:27:36.65113Z","shell.execute_reply":"2022-04-19T18:27:45.708141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Heatmap for all the remaining numerical data including the taget 'SalePrice'\n\n# Define the heatmap parameters\npd.options.display.float_format = \"{:,.2f}\".format\n\n# Define correlation matrix\ncorr_matrix = df_train_num.corr()\n\n# Replace correlation < |0.3| by 0 for a better visibility\ncorr_matrix[(corr_matrix < 0.3) & (corr_matrix > -0.3)] = 0\n\n# Mask the upper part of the heatmap\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n\n# Choose the color map\ncmap = \"viridis\"\n\n# plot the heatmap\nsns.heatmap(corr_matrix, mask=mask, vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot_kws={\"size\": 9, \"color\": \"black\"}, square=True, cmap=cmap, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:45.710682Z","iopub.execute_input":"2022-04-19T18:27:45.711025Z","iopub.status.idle":"2022-04-19T18:27:48.933395Z","shell.execute_reply.started":"2022-04-19T18:27:45.710994Z","shell.execute_reply":"2022-04-19T18:27:48.932563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>From the distribution of each numerical variables as well as the heatmap we can notice 18 features that are important and correlated (correlation higher than absolute 0.3) with our target variable 'SalePrice'.\n    \n<font color='darkblue'>We can also notice that a lot of features are correlated with each other. I will handle these correlation while selecting the features for our models.<font>","metadata":{}},{"cell_type":"code","source":"# Let's select features where the correlation with 'SalePrice' is higher than |0.3|\n# -1 because the latest row is SalePrice\ndf_num_corr = df_train_num.corr()[\"SalePrice\"][:-1]\n\n# Correlated features (r2 > 0.5)\nhigh_features_list = df_num_corr[abs(\n    df_num_corr) >= 0.5].sort_values(ascending=False)\nprint(\n    f\"{len(high_features_list)} strongly correlated values with SalePrice:\\n{high_features_list}\\n\")\n\n# Correlated features (0.3 < r2 < 0.5)\nlow_features_list = df_num_corr[(abs(df_num_corr) < 0.5) & (\n    abs(df_num_corr) >= 0.3)].sort_values(ascending=False)\nprint(\n    f\"{len(low_features_list)} slightly correlated values with SalePrice:\\n{low_features_list}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:48.934469Z","iopub.execute_input":"2022-04-19T18:27:48.93469Z","iopub.status.idle":"2022-04-19T18:27:48.953015Z","shell.execute_reply.started":"2022-04-19T18:27:48.934662Z","shell.execute_reply":"2022-04-19T18:27:48.952047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features with high correlation (higher than 0.5)\nstrong_features = df_num_corr[abs(df_num_corr) >= 0.5].index.tolist()\nstrong_features.append(\"SalePrice\")\n\ndf_strong_features = df_train_num.loc[:, strong_features]\n\nplt.style.use(\"seaborn-whitegrid\")  # define figures style\nfig, ax = plt.subplots(round(len(strong_features) / 3), 3)\n\nfor i, ax in enumerate(fig.axes):\n    # plot the correlation of each feature with SalePrice\n    if i < len(strong_features)-1:\n        sns.regplot(x=strong_features[i], y=\"SalePrice\", data=df_strong_features, ax=ax, scatter_kws={\n                    \"color\": \"deepskyblue\"}, line_kws={\"color\": \"black\"})","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:48.954904Z","iopub.execute_input":"2022-04-19T18:27:48.955153Z","iopub.status.idle":"2022-04-19T18:27:53.300462Z","shell.execute_reply.started":"2022-04-19T18:27:48.955123Z","shell.execute_reply":"2022-04-19T18:27:53.29986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features with low correlation (between 0.3 and 0.5)\nlow_features = df_num_corr[(abs(df_num_corr) >= 0.3) & (\n    abs(df_num_corr) < 0.5)].index.tolist()\nlow_features.append(\"SalePrice\")\n\ndf_low_features = df_train_num.loc[:, low_features]\n\nplt.style.use(\"seaborn-whitegrid\")  # define figures style\nfig, ax = plt.subplots(round(len(low_features) / 3), 3)\n\nfor i, ax in enumerate(fig.axes):\n    # plot the correlation of each feature with SalePrice\n    if i < len(low_features) - 1:\n        sns.regplot(x=low_features[i], y=\"SalePrice\", data=df_low_features, ax=ax, scatter_kws={\n                    \"color\": \"deepskyblue\"}, line_kws={\"color\": \"black\"},)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:53.301964Z","iopub.execute_input":"2022-04-19T18:27:53.302174Z","iopub.status.idle":"2022-04-19T18:27:56.934245Z","shell.execute_reply.started":"2022-04-19T18:27:53.302147Z","shell.execute_reply":"2022-04-19T18:27:56.933257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'> Here we can notice several houses where the price is not expensive (less than 250 000 dollars) for the corresponding surface ('TotalBsmtSF', '1stFlrSF', 'GrLivaArea' etc.). It is better to remove these observations to avoid influencing the prediction model. <font>\n    We can also note that for some features that designate a given area there is too much price variation for an area equal to 0, e.g. 'WoodDeckSF', 'OpenPorchSF' etc. \n    \n<font color='darkblue'>I will drop these outliers at the end of the data mining. ","metadata":{}},{"cell_type":"code","source":"# Define the list of numerical fetaures to keep\nlist_of_numerical_features = strong_features[:-1] + low_features\n\n# Let's select these features form our train set\ndf_train_num = df_train_num.loc[:, list_of_numerical_features]\n\n# The same features are selected from the test set (-1 -> except 'SalePrice')\ndf_test_num = df_test.loc[:, list_of_numerical_features[:-1]]","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:56.935853Z","iopub.execute_input":"2022-04-19T18:27:56.936519Z","iopub.status.idle":"2022-04-19T18:27:56.943236Z","shell.execute_reply.started":"2022-04-19T18:27:56.936475Z","shell.execute_reply":"2022-04-19T18:27:56.942522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>Here I kept the 18 most correlated numerical features with 'SalePrice'","metadata":{}},{"cell_type":"markdown","source":"### <font color='darkblue'>I.2.2. Missing data of Numerical features</font>","metadata":{}},{"cell_type":"markdown","source":"### <font color='darkblue'>Train set</font>","metadata":{}},{"cell_type":"code","source":"# Check the NaN of the train set by ploting percent of missing values per column\ncolumn_with_nan = df_train_num.columns[df_train_num.isnull().any()]\ncolumn_name = []\npercent_nan = []\n\nfor i in column_with_nan:\n    column_name.append(i)\n    percent_nan.append(\n        round(df_train_num[i].isnull().sum()*100/len(df_train_num), 2))\n\ntab = pd.DataFrame(column_name, columns=[\"Column\"])\ntab[\"Percent_NaN\"] = percent_nan\ntab.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (10, 7)})\nsns.set_style(\"whitegrid\")\n\n# Plot results\np = sns.barplot(x=\"Percent_NaN\", y=\"Column\", data=tab,\n                edgecolor=\"black\", color=\"deepskyblue\")\n\np.set_title(\"Percent of NaN per column of the train set\\n\", fontsize=20)\np.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\np.set_ylabel(\"Column Name\\n\", fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:56.944488Z","iopub.execute_input":"2022-04-19T18:27:56.945145Z","iopub.status.idle":"2022-04-19T18:27:57.245739Z","shell.execute_reply.started":"2022-04-19T18:27:56.945098Z","shell.execute_reply":"2022-04-19T18:27:57.244238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputation of missing values (NaNs) with SimpleImputer\nmy_imputer = SimpleImputer(strategy=\"median\")\ndf_train_imputed = pd.DataFrame(my_imputer.fit_transform(df_train_num))\ndf_train_imputed.columns = df_train_num.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:57.247177Z","iopub.execute_input":"2022-04-19T18:27:57.247511Z","iopub.status.idle":"2022-04-19T18:27:57.262367Z","shell.execute_reply.started":"2022-04-19T18:27:57.247465Z","shell.execute_reply":"2022-04-19T18:27:57.26136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the distribution of each imputed feature before and after imputation\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (14, 12)})\nsns.set_style(\"whitegrid\")\nfig, axes = plt.subplots(3, 2)\n\n# Plot the results\nfor feature, fig_pos in zip([\"LotFrontage\", \"GarageYrBlt\", \"MasVnrArea\"], [0, 1, 2]):\n\n    \"\"\"Features distribution before and after imputation\"\"\"\n\n    # before imputation\n    p = sns.histplot(ax=axes[fig_pos, 0], x=df_train_num[feature],\n                     kde=True, bins=30, color=\"deepskyblue\", edgecolor=\"black\")\n    p.set_ylabel(f\"Before imputation\", fontsize=14)\n\n    # after imputation\n    q = sns.histplot(ax=axes[fig_pos, 1], x=df_train_imputed[feature],\n                     kde=True, bins=30, color=\"darkorange\", edgecolor=\"black\")\n    q.set_ylabel(f\"After imputation\", fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:57.263736Z","iopub.execute_input":"2022-04-19T18:27:57.264125Z","iopub.status.idle":"2022-04-19T18:27:59.529057Z","shell.execute_reply.started":"2022-04-19T18:27:57.264093Z","shell.execute_reply":"2022-04-19T18:27:59.528401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>For \"LotFrontage\" and \"GarageYrBlt\" the distributions have changed after imputations. There is an over-representation of the median class compare ot the original distribution. However, the distribution remains the same for \"MasVnrArea\". Thus, to avoid any error related to the imputation I keep only the feature \"MasVnrArea\" for my analyses.<font>\n","metadata":{}},{"cell_type":"code","source":"# Drop 'LotFrontage' and 'GarageYrBlt'\ndf_train_imputed.drop([\"LotFrontage\", \"GarageYrBlt\"], axis=1, inplace=True)\ndf_train_imputed.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:59.532699Z","iopub.execute_input":"2022-04-19T18:27:59.533195Z","iopub.status.idle":"2022-04-19T18:27:59.552692Z","shell.execute_reply.started":"2022-04-19T18:27:59.533146Z","shell.execute_reply":"2022-04-19T18:27:59.552055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color='darkblue'>Test set</font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'>The columns that have been deleted in the train set must also be deleted in the test set so that the two data sets remain identical for the modeling and prediction.<font>","metadata":{}},{"cell_type":"code","source":"# Drop the same features from test set as for the train set\ndf_test_num.drop([\"LotFrontage\", \"GarageYrBlt\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:59.55389Z","iopub.execute_input":"2022-04-19T18:27:59.554207Z","iopub.status.idle":"2022-04-19T18:27:59.559158Z","shell.execute_reply.started":"2022-04-19T18:27:59.554178Z","shell.execute_reply":"2022-04-19T18:27:59.558436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the NaN of the test set by ploting percent of missing values per column\ncolumn_with_nan = df_test_num.columns[df_test_num.isnull().any()]\ncolumn_name = []\npercent_nan = []\n\nfor i in column_with_nan:\n    column_name.append(i)\n    percent_nan.append(\n        round(df_test_num[i].isnull().sum()*100/len(df_test_num), 2))\n\ntab = pd.DataFrame(column_name, columns=[\"Column\"])\ntab[\"Percent_NaN\"] = percent_nan\ntab.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (10, 7)})\nsns.set_style(\"whitegrid\")\n\n# Plot results\np = sns.barplot(x=\"Percent_NaN\", y=\"Column\", data=tab,\n                edgecolor=\"black\", color=\"deepskyblue\")\n\np.set_title(\"Percent of NaN per column of the test set\\n\", fontsize=20)\np.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\np.set_ylabel(\"Column Name\\n\", fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:59.560302Z","iopub.execute_input":"2022-04-19T18:27:59.56065Z","iopub.status.idle":"2022-04-19T18:27:59.85071Z","shell.execute_reply.started":"2022-04-19T18:27:59.560618Z","shell.execute_reply":"2022-04-19T18:27:59.85009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imputation of missing values (NaNs) with SimpleImputer\nmy_imputer = SimpleImputer(strategy=\"median\")\ndf_test_imputed = pd.DataFrame(my_imputer.fit_transform(df_test_num))\ndf_test_imputed.columns = df_test_num.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:59.851681Z","iopub.execute_input":"2022-04-19T18:27:59.852224Z","iopub.status.idle":"2022-04-19T18:27:59.863772Z","shell.execute_reply.started":"2022-04-19T18:27:59.852193Z","shell.execute_reply":"2022-04-19T18:27:59.862938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's check the distribution of each imputed feature before and after imputation\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (20, 18)})\nsns.set_style(\"whitegrid\")\nfig, axes = plt.subplots(5, 2)\n\n# Plot the results\nfor feature, fig_pos in zip(tab[\"Column\"].tolist(), range(0, 6)):\n\n    \"\"\"Features distribution before and after imputation\"\"\"\n\n    # before imputation\n    p = sns.histplot(ax=axes[fig_pos, 0], x=df_test_num[feature],\n                     kde=True, bins=30, color=\"deepskyblue\", edgecolor=\"black\")\n    p.set_ylabel(f\"Before imputation\", fontsize=14)\n\n    # after imputation\n    q = sns.histplot(ax=axes[fig_pos, 1], x=df_test_imputed[feature],\n                     kde=True, bins=30, color=\"darkorange\", edgecolor=\"black\",)\n    q.set_ylabel(f\"After imputation\", fontsize=14)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:27:59.864807Z","iopub.execute_input":"2022-04-19T18:27:59.865452Z","iopub.status.idle":"2022-04-19T18:28:02.613169Z","shell.execute_reply.started":"2022-04-19T18:27:59.865404Z","shell.execute_reply":"2022-04-19T18:28:02.612335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>The percentage of NaN in each of these fetaures did not exceed 1.5%. Thus, by imputing these missing data, few errors were introduced and the distributions are similar before and after imputation.<font>","metadata":{}},{"cell_type":"markdown","source":"## <font color='darkblue'>I.3. Categorical features</font>","metadata":{}},{"cell_type":"markdown","source":"### <font color='darkblue'>I.3.1. Explore and clean Categorical features</font>","metadata":{}},{"cell_type":"code","source":"# Categorical to Quantitative relationship\ncategorical_features = [\n    i for i in df_train.columns if df_train.dtypes[i] == \"object\"]\ncategorical_features.append(\"SalePrice\")\n\n# Train set\ndf_train_categ = df_train[categorical_features]\n\n# Test set (-1 because test set don't have 'Sale Price')\ndf_test_categ = df_test[categorical_features[:-1]]","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:02.614714Z","iopub.execute_input":"2022-04-19T18:28:02.615058Z","iopub.status.idle":"2022-04-19T18:28:02.628776Z","shell.execute_reply.started":"2022-04-19T18:28:02.615027Z","shell.execute_reply":"2022-04-19T18:28:02.628106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Countplot for each of the categorical features in the train set\nfig, axes = plt.subplots(\n    round(len(df_train_categ.columns) / 3), 3, figsize=(12, 30))\n\nfor i, ax in enumerate(fig.axes):\n    # plot barplot of each feature\n    if i < len(df_train_categ.columns) - 1:\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.countplot(\n            x=df_train_categ.columns[i], alpha=0.7, data=df_train_categ, ax=ax)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:02.62996Z","iopub.execute_input":"2022-04-19T18:28:02.630353Z","iopub.status.idle":"2022-04-19T18:28:10.142869Z","shell.execute_reply.started":"2022-04-19T18:28:02.630309Z","shell.execute_reply":"2022-04-19T18:28:10.141925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>By looking closely at each of the count plots we can notice that for some categorical feature the observation are concentrated in a single level of the category. These features are less informative for our model, so it would be better to remove them.<font>","metadata":{}},{"cell_type":"code","source":"# Drop some categorical 'non-informative' features from train set\ncolumns_to_drop = [\n    \"Street\",\n    \"Alley\",\n    \"LandContour\",\n    \"Utilities\",\n    \"LandSlope\",\n    \"Condition2\",\n    \"RoofMatl\",\n    \"CentralAir\",\n    \"GarageQual\",\n    \"GarageCond\",\n    \"SaleType\",\n    \"PavedDrive\",\n    \"LandContour\",\n    \"ExterCond\",\n    \"GarageCond\",\n    \"Heating\",\n    \"MiscFeature\",\n    \"BsmtFinType2\",\n    \"Functional\",\n    \"GarageQual\",\n    \"GarageCond\",\n]\n\n# Train set\ndf_train_categ.drop(columns_to_drop, axis=1, inplace=True)\n\n# Test set\ndf_test_categ.drop(columns_to_drop, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:10.144476Z","iopub.execute_input":"2022-04-19T18:28:10.144794Z","iopub.status.idle":"2022-04-19T18:28:10.154876Z","shell.execute_reply.started":"2022-04-19T18:28:10.144759Z","shell.execute_reply":"2022-04-19T18:28:10.153908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# With the boxplot we can see the variation of the target 'SalePrice' in each of the categorical features\nfig, axes = plt.subplots(\n    round(len(df_train_categ.columns)/3), 3, figsize=(15, 30))\n\nfor i, ax in enumerate(fig.axes):\n    # plot the variation of SalePrice in each feature\n    if i < len(df_train_categ.columns) - 1:\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.boxplot(\n            x=df_train_categ.columns[i], y=\"SalePrice\", data=df_train_categ, ax=ax)\n\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:10.156332Z","iopub.execute_input":"2022-04-19T18:28:10.156552Z","iopub.status.idle":"2022-04-19T18:28:17.580247Z","shell.execute_reply.started":"2022-04-19T18:28:10.156525Z","shell.execute_reply":"2022-04-19T18:28:17.579269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>Some of these features seem to be codependent such as 'Exterior1st' &  'Exterior2nd', 'BsmtQual' & 'BsmtCond', 'MasVnrType' & 'ExterQual' etc.\nSo let's plot the contingency table and perform the Chi square test in order to identify these codependency <font>","metadata":{}},{"cell_type":"code","source":"# Plot contingency table\n\nsns.set(rc={\"figure.figsize\": (10, 7)})\n\nX = [\"Exterior1st\", \"ExterQual\", \"BsmtQual\", \"BsmtQual\", \"BsmtQual\"]\nY = [\"Exterior2nd\", \"MasVnrType\", \"BsmtCond\", \"BsmtExposure\"]\n\nfor i, j in zip(X, Y):\n\n    # Contingency table\n    cont = df_train_categ[[i, j]].pivot_table(\n        index=i, columns=j, aggfunc=len, margins=True, margins_name=\"Total\")\n    tx = cont.loc[:, [\"Total\"]]\n    ty = cont.loc[[\"Total\"], :]\n    n = len(df_train_categ)\n    indep = tx.dot(ty) / n\n    c = cont.fillna(0)  # Replace NaN with 0 in the contingency table\n    measure = (c - indep) ** 2 / indep\n    xi_n = measure.sum().sum()\n    table = measure / xi_n\n\n    # Plot contingency table\n    p = sns.heatmap(table.iloc[:-1, :-1],\n                    annot=c.iloc[:-1, :-1], fmt=\".0f\", cmap=\"Oranges\")\n    p.set_xlabel(j, fontsize=18)\n    p.set_ylabel(i, fontsize=18)\n    p.set_title(f\"\\nχ² test between groups {i} and groups {j}\\n\", size=18)\n    plt.show()\n\n    # Performing Chi-sq test\n    CrosstabResult = pd.crosstab(\n        index=df_train_categ[i], columns=df_train_categ[j])\n    ChiSqResult = chi2_contingency(CrosstabResult)\n    # P-Value is the Probability of H0 being True\n    print(\n        f\"P-Value of the ChiSq Test bewteen {i} and {j} is: {ChiSqResult[1]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:17.581615Z","iopub.execute_input":"2022-04-19T18:28:17.58187Z","iopub.status.idle":"2022-04-19T18:28:19.689204Z","shell.execute_reply.started":"2022-04-19T18:28:17.581839Z","shell.execute_reply":"2022-04-19T18:28:19.688364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'> p-value is significant for all tests so there is some co-dependence between these variables. For this I will drop 'Exterior2nd', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1' and 'MasVnrType'<font>","metadata":{}},{"cell_type":"code","source":"# Let's drop the one of each co-dependent variables\n# Train set\ndf_train_categ.drop(Y, axis=1, inplace=True)\n\n# Test set\ndf_test_categ.drop(Y, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:19.690459Z","iopub.execute_input":"2022-04-19T18:28:19.690702Z","iopub.status.idle":"2022-04-19T18:28:19.698666Z","shell.execute_reply.started":"2022-04-19T18:28:19.690672Z","shell.execute_reply":"2022-04-19T18:28:19.697641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color='darkblue'>I.3.2. Missing data of Categorical features</font>","metadata":{}},{"cell_type":"markdown","source":"### <font color='darkblue'>Train set</font>","metadata":{}},{"cell_type":"code","source":"# Check the NaN of the test set by ploting percent of missing values per column\ncolumn_with_nan = df_train_categ.columns[df_train_categ.isnull().any()]\ncolumn_name = []\npercent_nan = []\n\nfor i in column_with_nan:\n    column_name.append(i)\n    percent_nan.append(\n        round(df_train_categ[i].isnull().sum() * 100 / len(df_train_categ), 2))\n\ntab = pd.DataFrame(column_name, columns=[\"Column\"])\ntab[\"Percent_NaN\"] = percent_nan\ntab.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (10, 7)})\nsns.set_style(\"whitegrid\")\n\n# Plot results\np = sns.barplot(x=\"Percent_NaN\", y=\"Column\", data=tab,\n                edgecolor=\"black\", color=\"deepskyblue\")\np.set_title(\"Percent of NaN per column of the test set\\n\", fontsize=20)\np.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\np.set_ylabel(\"Column Name\\n\", fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:19.699974Z","iopub.execute_input":"2022-04-19T18:28:19.7002Z","iopub.status.idle":"2022-04-19T18:28:20.01991Z","shell.execute_reply.started":"2022-04-19T18:28:19.700171Z","shell.execute_reply":"2022-04-19T18:28:20.019107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>Drop the features where the percentage of NaN is higher than 5% to avoid introducing any error. Than impute the NaN of 'BsmtQual' and 'Electrical' by the corresponding modal class<font>","metadata":{}},{"cell_type":"code","source":"# Drop the features where the percentage of NaN is higher than 5%\ndf_train_categ.drop([\"PoolQC\", \"Fence\", \"FireplaceQu\",\n                     \"GarageType\", \"GarageFinish\"], axis=1, inplace=True,)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.021055Z","iopub.execute_input":"2022-04-19T18:28:20.021343Z","iopub.status.idle":"2022-04-19T18:28:20.0273Z","shell.execute_reply.started":"2022-04-19T18:28:20.021312Z","shell.execute_reply":"2022-04-19T18:28:20.026547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill the NaN of each feature by the corresponding modal class\ncateg_fill_null = {\"BsmtQual\": df_train_categ[\"BsmtQual\"].mode().iloc[0],\n                   \"BsmtFinType1\": df_train_categ[\"BsmtFinType1\"].mode().iloc[0],\n                   \"Electrical\": df_train_categ[\"Electrical\"].mode().iloc[0]}\n\ndf_train_categ = df_train_categ.fillna(value=categ_fill_null)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.028156Z","iopub.execute_input":"2022-04-19T18:28:20.028837Z","iopub.status.idle":"2022-04-19T18:28:20.045208Z","shell.execute_reply.started":"2022-04-19T18:28:20.028802Z","shell.execute_reply":"2022-04-19T18:28:20.043927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color='darkblue'>Test set</font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'>The columns that have been deleted in the train set must also be deleted in the test set so that the two data sets remain identical for the modeling and prediction.<font>","metadata":{}},{"cell_type":"code","source":"# Drop the same features from test set as for the train set\ndf_test_categ.drop([\"PoolQC\", \"Fence\", \"FireplaceQu\",\n                    \"GarageType\", \"GarageFinish\"], axis=1, inplace=True,)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.046853Z","iopub.execute_input":"2022-04-19T18:28:20.047279Z","iopub.status.idle":"2022-04-19T18:28:20.057899Z","shell.execute_reply.started":"2022-04-19T18:28:20.047238Z","shell.execute_reply":"2022-04-19T18:28:20.057308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the NaN of the test set by ploting percent of missing values per column\ncolumn_with_nan = df_test_categ.columns[df_test_categ.isnull().any()]\ncolumn_name = []\npercent_nan = []\n\nfor i in column_with_nan:\n    column_name.append(i)\n    percent_nan.append(\n        round(df_test_categ[i].isnull().sum() * 100 / len(df_test_categ), 2))\n\ntab = pd.DataFrame(column_name, columns=[\"Column\"])\ntab[\"Percent_NaN\"] = percent_nan\ntab.sort_values(by=[\"Percent_NaN\"], ascending=False, inplace=True)\n\n\n# Define figure parameters\nsns.set(rc={\"figure.figsize\": (10, 7)})\nsns.set_style(\"whitegrid\")\n\n# Plot results\np = sns.barplot(x=\"Percent_NaN\", y=\"Column\", data=tab,\n                edgecolor=\"black\", color=\"deepskyblue\")\np.set_title(\"Percent of NaN per column of the test set\\n\", fontsize=20)\np.set_xlabel(\"\\nPercent of NaN (%)\", fontsize=20)\np.set_ylabel(\"Column Name\\n\", fontsize=20)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.059072Z","iopub.execute_input":"2022-04-19T18:28:20.059614Z","iopub.status.idle":"2022-04-19T18:28:20.364219Z","shell.execute_reply.started":"2022-04-19T18:28:20.059557Z","shell.execute_reply":"2022-04-19T18:28:20.3636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill the NaN of each feature by the corresponding modal class\ncateg_fill_null = {\"BsmtQual\": df_test_categ[\"BsmtQual\"].mode().iloc[0],\n                   \"BsmtFinType1\": df_test_categ[\"BsmtFinType1\"].mode().iloc[0],\n                   \"MSZoning\": df_test_categ[\"MSZoning\"].mode().iloc[0],\n                   \"Exterior1st\": df_test_categ[\"Exterior1st\"].mode().iloc[0],\n                   \"KitchenQual\": df_test_categ[\"KitchenQual\"].mode().iloc[0]}\n\ndf_test_categ = df_test_categ.fillna(value=categ_fill_null)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.365525Z","iopub.execute_input":"2022-04-19T18:28:20.366022Z","iopub.status.idle":"2022-04-19T18:28:20.377823Z","shell.execute_reply.started":"2022-04-19T18:28:20.365989Z","shell.execute_reply":"2022-04-19T18:28:20.376828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color='darkblue'>I.3.3. Transform Categorical features into Binary features (get_dummies)</font>","metadata":{}},{"cell_type":"code","source":"# Train set\nfor i in df_train_categ.columns.tolist()[:-1]:\n    df_dummies = pd.get_dummies(df_train_categ[i], prefix=i)\n\n    # merge both tables\n    df_train_categ = df_train_categ.join(df_dummies)\n\n# Select the binary features only\ndf_train_binary = df_train_categ.iloc[:, 18:]\ndf_train_binary.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.378897Z","iopub.execute_input":"2022-04-19T18:28:20.379104Z","iopub.status.idle":"2022-04-19T18:28:20.433906Z","shell.execute_reply.started":"2022-04-19T18:28:20.379078Z","shell.execute_reply":"2022-04-19T18:28:20.43295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test set\nfor i in df_test_categ.columns.tolist():\n    df_dummies = pd.get_dummies(df_test_categ[i], prefix=i)\n\n    # merge both tables\n    df_test_categ = df_test_categ.join(df_dummies)\n\n# Select the binary features only\ndf_test_binary = df_test_categ.iloc[:, 17:]\ndf_test_binary.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.439088Z","iopub.execute_input":"2022-04-19T18:28:20.439498Z","iopub.status.idle":"2022-04-19T18:28:20.490815Z","shell.execute_reply.started":"2022-04-19T18:28:20.439448Z","shell.execute_reply":"2022-04-19T18:28:20.489836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>We can notice that in df_test_binary there is 118 columns while in df_train_binary there is 122. Let's see which columns are missing from df_test_binary","metadata":{}},{"cell_type":"code","source":"# Let's check if the column headings are the same in both data set, df_train and df_test\ndif_1 = [x for x in df_train_binary.columns if x not in df_test_binary.columns]\nprint(\n    f\"Features present in df_train_categ and absent in df_test_categ: {dif_1}\\n\")\n\ndif_2 = [x for x in df_test_binary.columns if x not in df_train_binary.columns]\nprint(\n    f\"Features present in df_test_categ set and absent in df_train_categ: {dif_2}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.492201Z","iopub.execute_input":"2022-04-19T18:28:20.492443Z","iopub.status.idle":"2022-04-19T18:28:20.498853Z","shell.execute_reply.started":"2022-04-19T18:28:20.492412Z","shell.execute_reply":"2022-04-19T18:28:20.498051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>Four of the binary features are absent from the test set. Thus, these features will be dropped from the train in order to have the same columns in both data set<font>","metadata":{}},{"cell_type":"code","source":"# Let's drop these columns from df_train_binary\ndf_train_binary.drop(dif_1, axis=1, inplace=True)\n\n# Check again if the column headings are the same in both data set\ndif_1 = [x for x in df_train_binary.columns if x not in df_test_binary.columns]\nprint(\n    f\"Features present in df_train_categ and absent in df_test_categ: {dif_1}\\n\")\n\ndif_2 = [x for x in df_test_binary.columns if x not in df_train_binary.columns]\nprint(\n    f\"Features present in df_test_categ set and absent in df_train_categ: {dif_2}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.50001Z","iopub.execute_input":"2022-04-19T18:28:20.500423Z","iopub.status.idle":"2022-04-19T18:28:20.519973Z","shell.execute_reply.started":"2022-04-19T18:28:20.500391Z","shell.execute_reply":"2022-04-19T18:28:20.519369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>Both data set have the same featues now<font>","metadata":{}},{"cell_type":"markdown","source":"## <font color='darkblue'>I.4. Merge numerical and binary features into one data set</font>","metadata":{}},{"cell_type":"code","source":"# Add binary features to numreical features\n# Train set\ndf_train_new = df_train_imputed.join(df_train_binary)\nprint(f\"Train set: {df_train_new.shape}\")\n\n# Test set\ndf_test_new = df_test_imputed.join(df_test_binary)\nprint(f\"Test set: {df_test_new.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.521217Z","iopub.execute_input":"2022-04-19T18:28:20.521666Z","iopub.status.idle":"2022-04-19T18:28:20.533835Z","shell.execute_reply.started":"2022-04-19T18:28:20.521625Z","shell.execute_reply":"2022-04-19T18:28:20.533097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='darkblue'>I.5. Drop outliers from the train set</font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'>Previoulsy in the part 'I.2. Numerical Features' of this notebook I noticed some houses with large surface (\"GrLivArea\", \"TotalBsmtSF\" and \"GarageArea\") and with a very low Price. It is better for our models to drop theese outliers.\n    \n<font color='darkblue'>I also noticed for both features \"WoodDeckSF\" and \"OpenPorchSF\" a high number of 0 values with a correspondingly high price variation. These outliers should be deleted.\nHowever, since the number of these outliers is very important, the best thing to do is to drop these columns.<font>","metadata":{}},{"cell_type":"code","source":"# Drop \"WoodDeckSF\" and \"OpenPorchSF\"\ndf_train_new.drop([\"WoodDeckSF\", \"OpenPorchSF\"], axis=1, inplace=True)\ndf_test_new.drop([\"WoodDeckSF\", \"OpenPorchSF\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.535065Z","iopub.execute_input":"2022-04-19T18:28:20.535536Z","iopub.status.idle":"2022-04-19T18:28:20.54729Z","shell.execute_reply.started":"2022-04-19T18:28:20.5355Z","shell.execute_reply":"2022-04-19T18:28:20.546339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's handle the outliers in \"GrLivArea\", \"TotalBsmtSF\" and \"GarageArea\"\n# Outliers in \"GrLivArea\"\noutliers1 = df_train_new[(df_train_new[\"GrLivArea\"] > 4000) & (\n    df_train_new[\"SalePrice\"] <= 200000)].index.tolist()\n\n# Outliers in \"TotalBsmtSF\"\noutliers2 = df_train_new[(df_train_new[\"TotalBsmtSF\"] > 3000) & (\n    df_train_new[\"SalePrice\"] <= 400000)].index.tolist()\n\n# Outliers in \"GarageArea\"\noutliers3 = df_train_new[(df_train_new[\"GarageArea\"] > 1200) & (\n    df_train_new[\"SalePrice\"] <= 300000)].index.tolist()\n\n# List of all the outliers\noutliers = outliers1 + outliers2 + outliers3\noutliers = list(set(outliers))\nprint(outliers)\n\n# Drop these outlier\ndf_train_new = df_train_new.drop(df_train_new.index[outliers])\n\n# Reset index\ndf_train_new = df_train_new.reset_index().drop(\"index\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.548809Z","iopub.execute_input":"2022-04-19T18:28:20.549034Z","iopub.status.idle":"2022-04-19T18:28:20.56785Z","shell.execute_reply.started":"2022-04-19T18:28:20.549007Z","shell.execute_reply":"2022-04-19T18:28:20.566667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color='darkblue'>II. Feature engineering</font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'>While looking closley at the remaining features, we can notice that several of them designate a given surface of the property. Thus, I will try to combine some of these surfaces into indicators without losing the information they provide.\n    \n<font color='darkblue'>In addition, I will turn years into age, e.g. year of construction will be transformed into age of the house since the construction. <font>","metadata":{}},{"cell_type":"code","source":"# Define a function to calculate the occupancy rate of the first floor of the total living area\n\n\ndef floor_occupation(x):\n    \"\"\"First floor occupation of the total live area\n\n    floor_occupation equation has the following form:\n    (1st Floor Area * 100) / (Ground Live Area)\n\n    Args:\n        x -- the corresponding feature\n\n    Returns:\n        0 -- if Ground Live Area = 0\n        equation -- if Ground Live Area > 0\n    \"\"\"\n    if x[\"GrLivArea\"] == 0:\n        return 0\n    else:\n        return x[\"1stFlrSF\"] * 100 / x[\"GrLivArea\"]\n\n\n# Apply the function on train and test set\ndf_train_new[\"1stFlrPercent\"] = df_train_new.apply(\n    lambda x: floor_occupation(x), axis=1)\n\ndf_test_new[\"1stFlrPercent\"] = df_test_new.apply(\n    lambda x: floor_occupation(x), axis=1)\n\n# Drop \"1stFlrSF\" and \"2ndFlrSF\"\ndf_train_new.drop([\"1stFlrSF\", \"2ndFlrSF\"], axis=1, inplace=True)\ndf_test_new.drop([\"1stFlrSF\", \"2ndFlrSF\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.569487Z","iopub.execute_input":"2022-04-19T18:28:20.570359Z","iopub.status.idle":"2022-04-19T18:28:20.661056Z","shell.execute_reply.started":"2022-04-19T18:28:20.57031Z","shell.execute_reply":"2022-04-19T18:28:20.660199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to calculate the occupancy rate of the finished basement area\n\n\ndef bsmt_finish(x):\n    \"\"\"Propotion of finished area in basement \n\n    bsmt_finish equation has the following form:\n    (Finished Basement Area * 100) / (Total Basement Area)\n\n    Args:\n        x -- the corresponding feature\n\n    Returns:\n        0 -- if Total Basement Area = 0\n        equation -- if Total Basement Area > 0\n    \"\"\"\n    if x[\"TotalBsmtSF\"] == 0:\n        return 0\n    else:\n        return x[\"BsmtFinSF1\"] * 100 / x[\"TotalBsmtSF\"]\n\n\n# Apply the function on train and test set\ndf_train_new[\"BsmtFinPercent\"] = df_train_new.apply(\n    lambda x: bsmt_finish(x), axis=1)\n\ndf_test_new[\"BsmtFinPercent\"] = df_test_new.apply(\n    lambda x: bsmt_finish(x), axis=1)\n\n# Drop \"BsmtFinSF1\"\ndf_train_new.drop([\"BsmtFinSF1\"], axis=1, inplace=True)\ndf_test_new.drop([\"BsmtFinSF1\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.662813Z","iopub.execute_input":"2022-04-19T18:28:20.663127Z","iopub.status.idle":"2022-04-19T18:28:20.753552Z","shell.execute_reply.started":"2022-04-19T18:28:20.663085Z","shell.execute_reply":"2022-04-19T18:28:20.752704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert Year of construction to Age of the house since the construction\ndf_train_new[\"AgeSinceConst\"] = (\n    df_train_new[\"YearBuilt\"].max() - df_train_new[\"YearBuilt\"])\n\ndf_test_new[\"AgeSinceConst\"] = df_test_new[\"YearBuilt\"].max() - \\\n    df_test_new[\"YearBuilt\"]\n\n# Drop \"YearBuilt\"\ndf_train_new.drop([\"YearBuilt\"], axis=1, inplace=True)\ndf_test_new.drop([\"YearBuilt\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.754785Z","iopub.execute_input":"2022-04-19T18:28:20.755205Z","iopub.status.idle":"2022-04-19T18:28:20.76746Z","shell.execute_reply.started":"2022-04-19T18:28:20.755127Z","shell.execute_reply":"2022-04-19T18:28:20.7667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert Year of remodeling to Age of the house since the remodeling\ndf_train_new[\"AgeSinceRemod\"] = (\n    df_train_new[\"YearRemodAdd\"].max() - df_train_new[\"YearRemodAdd\"])\n\ndf_test_new[\"AgeSinceRemod\"] = (\n    df_test_new[\"YearRemodAdd\"].max() - df_test_new[\"YearRemodAdd\"])\n\n# Drop \"YearRemodAdd\"\ndf_train_new.drop([\"YearRemodAdd\"], axis=1, inplace=True)\ndf_test_new.drop([\"YearRemodAdd\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.768503Z","iopub.execute_input":"2022-04-19T18:28:20.769064Z","iopub.status.idle":"2022-04-19T18:28:20.78156Z","shell.execute_reply.started":"2022-04-19T18:28:20.769021Z","shell.execute_reply":"2022-04-19T18:28:20.780686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>To avoid redundancy and to mitigate the strong variations of some features according to the SalePrice. I will use a Log transformation for skewed features. Generally in real estate, as the area of the property increases for example, the price per square feet decreases, hence the transformation. <font>","metadata":{}},{"cell_type":"code","source":"# Define the features on which I will calculate the Skeweness\ncontinuous_features = [\"OverallQual\", \"TotalBsmtSF\", \"GrLivArea\", \"FullBath\",\n                       \"TotRmsAbvGrd\", \"GarageCars\", \"GarageArea\", \"MasVnrArea\",\n                       \"Fireplaces\", \"1stFlrPercent\",\"BsmtFinPercent\", \n                       \"AgeSinceConst\", \"AgeSinceRemod\"]\ndf_skew_verify = df_train_new.loc[:, continuous_features]\n\n\n# Select features with absolute Skeweness higher than 0.5\nskew_ft = []\n\nfor i in continuous_features:\n    # calculate skew for each corresponding feature\n    skew_ft.append(abs(df_skew_verify[i].skew()))\n\ndf_skewed = pd.DataFrame({\"Columns\": continuous_features,\n                          \"Abs_Skew\": skew_ft})\n\nsk_features = df_skewed[df_skewed[\"Abs_Skew\"] > 0.5][\"Columns\"].tolist()\nprint(f\"List of skewed features: {sk_features}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.783426Z","iopub.execute_input":"2022-04-19T18:28:20.783934Z","iopub.status.idle":"2022-04-19T18:28:20.802657Z","shell.execute_reply.started":"2022-04-19T18:28:20.783888Z","shell.execute_reply":"2022-04-19T18:28:20.801675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Log transformation of the Skewed features\nfor i in sk_features:\n    # loop over i (features) to calculate Log of surfaces\n    # Train set\n    df_train_new[i] = np.log((df_train_new[i]) + 1)\n    \n    # Test set\n    df_test_new[i] = np.log((df_test_new[i]) + 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:20.80472Z","iopub.execute_input":"2022-04-19T18:28:20.805221Z","iopub.status.idle":"2022-04-19T18:28:20.816135Z","shell.execute_reply.started":"2022-04-19T18:28:20.805177Z","shell.execute_reply":"2022-04-19T18:28:20.815172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color='darkblue'>III.  Preparing data for modeling</font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue' size='3'>This modeling part is mainly based on theese Kaggle cores:\n- [House pricing explained step by step](https://www.kaggle.com/code/binarymachine/house-pricing-explained-step-by-step)\n- [Houseprice step by step](https://www.kaggle.com/code/abdelrahmantarek13/houseprice-step-by-step)","metadata":{}},{"cell_type":"markdown","source":"## <font color='darkblue'>III.1.  Target variable 'SalePrice'</font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'>log transformation of the independent variable ('SalePrice') to have a distribution that approaches the normal distribution <font>","metadata":{}},{"cell_type":"code","source":"# Log transformation of the target variable \"SalePrice\"\ndf_train_new[\"SalePriceLog\"] = np.log(df_train_new.SalePrice)\n\n# Plot the distribution before and after transformation\nfig, axes = plt.subplots(1, 2)\nfig.suptitle(\"Distribution of 'SalePrice' before and after log-transformation\")\n\n# before log transformation\np = sns.histplot(ax=axes[0], x=df_train_new[\"SalePrice\"],\n                 kde=True, bins=100, color=\"deepskyblue\")\np.set_xlabel(\"SalePrice\", fontsize=16)\np.set_ylabel(\"Effectif\", fontsize=16)\n\n# after log transformation\nq = sns.histplot(ax=axes[1], x=df_train_new[\"SalePriceLog\"],\n                 kde=True, bins=100, color=\"darkorange\")\nq.set_xlabel(\"SalePriceLog\", fontsize=16)\nq.set_ylabel(\"\", fontsize=16)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:39.094353Z","iopub.execute_input":"2022-04-19T18:28:39.094636Z","iopub.status.idle":"2022-04-19T18:28:39.966474Z","shell.execute_reply.started":"2022-04-19T18:28:39.094606Z","shell.execute_reply":"2022-04-19T18:28:39.965672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop the original SalePrice\ndf_train_new.drop([\"SalePrice\"], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:39.968385Z","iopub.execute_input":"2022-04-19T18:28:39.968882Z","iopub.status.idle":"2022-04-19T18:28:39.97577Z","shell.execute_reply.started":"2022-04-19T18:28:39.968827Z","shell.execute_reply":"2022-04-19T18:28:39.974738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='darkblue'>III.2. Split data into train and test set and Standardization","metadata":{}},{"cell_type":"code","source":"# Extract the features (X) and the target (y)\n# Features (X)\nX = df_train_new[[i for i in list(\n    df_train_new.columns) if i != \"SalePriceLog\"]]\nprint(X.shape)\n\n# Target (y)\ny = df_train_new.loc[:, \"SalePriceLog\"]\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:40.674193Z","iopub.execute_input":"2022-04-19T18:28:40.674472Z","iopub.status.idle":"2022-04-19T18:28:40.68324Z","shell.execute_reply.started":"2022-04-19T18:28:40.674442Z","shell.execute_reply":"2022-04-19T18:28:40.682181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into X_train and X_test (by stratifying on y)\n# Stratify on a continuous variable by splitting it in bins\n# Create the bins.\nbins = np.linspace(0, len(y), 150)\ny_binned = np.digitize(y, bins)\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n                                                    stratify=y_binned, shuffle=True)\nprint(f\"X_train:{X_train.shape}\\ny_train:{y_train.shape}\")\nprint(f\"\\nX_test:{X_test.shape}\\ny_test:{y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:41.195599Z","iopub.execute_input":"2022-04-19T18:28:41.195926Z","iopub.status.idle":"2022-04-19T18:28:41.209138Z","shell.execute_reply.started":"2022-04-19T18:28:41.195892Z","shell.execute_reply":"2022-04-19T18:28:41.208159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardize the data\nstd_scale = preprocessing.StandardScaler().fit(X_train)\nX_train = std_scale.transform(X_train)\nX_test = std_scale.transform(X_test)\n# The same standardization is applied for df_test_new\ndf_test_new = std_scale.transform(df_test_new)\n\n\n# The output of standardization is a vector. Let's turn it into a table\n# Convert X, y and test data into dataframe\nX_train = pd.DataFrame(X_train, columns=X.columns)\nX_test = pd.DataFrame(X_test, columns=X.columns)\ndf_test_new = pd.DataFrame(df_test_new, columns=X.columns)\n\ny_train = pd.DataFrame(y_train)\ny_train = y_train.reset_index().drop(\"index\", axis=1)\n\ny_test = pd.DataFrame(y_test)\ny_test = y_test.reset_index().drop(\"index\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:42.360344Z","iopub.execute_input":"2022-04-19T18:28:42.360809Z","iopub.status.idle":"2022-04-19T18:28:42.390549Z","shell.execute_reply.started":"2022-04-19T18:28:42.360763Z","shell.execute_reply":"2022-04-19T18:28:42.389716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='darkblue'>III.3. Backward Stepwise Regression<font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'>Let's lighten the model vith a Backward Stepwise Regression\n    \n<font color='darkblue'>Backward Stepwise Regression is a stepwise regression approach that begins with a full\n(saturated) model and at each step gradually eliminates variables from the regression model to find a\nreduced model that best explains the data. Also known as Backward Elimination regression.\n[Click here for more information](https://www.analystsoft.com/en/products/statplus/content/help/pdf/analysis_regression_backward_stepwise_elimination_regression_model.pdf)<font>\n","metadata":{}},{"cell_type":"code","source":"Selected_Features = []\n\n\ndef backward_regression(X, y, initial_list=[], threshold_in=0.01, threshold_out=0.05, verbose=True):\n    \"\"\"To select feature with Backward Stepwise Regression \n\n    Args:\n        X -- features values\n        y -- target variable\n        initial_list -- features header\n        threshold_in -- pvalue threshold of features to keep\n        threshold_out -- pvalue threshold of features to drop\n        verbose -- true to produce lots of logging output\n\n    Returns:\n        list of selected features for modeling \n    \"\"\"\n    included = list(X.columns)\n    while True:\n        changed = False\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max()  # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed = True\n            worst_feature = pvalues.idxmax()\n            included.remove(worst_feature)\n            if verbose:\n                print(f\"worst_feature : {worst_feature}, {worst_pval} \")\n        if not changed:\n            break\n    Selected_Features.append(included)\n    print(f\"\\nSelected Features:\\n{Selected_Features[0]}\")\n\n\n# Application of the backward regression function on our training data\nbackward_regression(X_train, y_train)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-19T18:28:43.726704Z","iopub.execute_input":"2022-04-19T18:28:43.72738Z","iopub.status.idle":"2022-04-19T18:28:47.219666Z","shell.execute_reply.started":"2022-04-19T18:28:43.727336Z","shell.execute_reply":"2022-04-19T18:28:47.218764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keep the selected features only\nX_train = X_train.loc[:, Selected_Features[0]]\nX_test = X_test.loc[:, Selected_Features[0]]\ndf_test_new = df_test_new.loc[:, Selected_Features[0]]","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:47.221608Z","iopub.execute_input":"2022-04-19T18:28:47.222114Z","iopub.status.idle":"2022-04-19T18:28:47.233013Z","shell.execute_reply.started":"2022-04-19T18:28:47.222065Z","shell.execute_reply":"2022-04-19T18:28:47.232104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='darkblue'>III.4. Variance Inflation Factor<font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'>     \nVariance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables. Mathematically, the VIF for a regression model variable is equal to the ratio of the overall model variance to the variance of a model that includes only that single independent variable.\n    \n<font color='darkblue'>A feature with a VIF higher than 10 (5 is also common) implies that there is a multi-collinearity with the latter.   \n[Click here for more information](https://www.investopedia.com/terms/v/variance-inflation-factor.asp#:~:text=Variance%20inflation%20factor%20(VIF)%20is,only%20that%20single%20independent%20variable).<font>","metadata":{}},{"cell_type":"code","source":"# Here I calculate VIF for each feature\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(\n    X_train.values, i) for i in range(X_train.shape[1])]\n\n# VIF results in a table\nvif[\"features\"] = X_train.columns\nvif.round(1)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:47.234628Z","iopub.execute_input":"2022-04-19T18:28:47.235156Z","iopub.status.idle":"2022-04-19T18:28:47.649086Z","shell.execute_reply.started":"2022-04-19T18:28:47.235111Z","shell.execute_reply":"2022-04-19T18:28:47.648278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select features with high VIF\nhigh_vif_list = vif[vif[\"VIF Factor\"] > 10][\"features\"].tolist()\n\nif len(high_vif_list) == 0:\n    # print empty list if low multicolinearity\n    print(f\"None of the features have a high multicollinearity\")\nelse:\n    # print list of features with high multicolinearity\n    print(f\"List of features with high multicollinearity: {high_vif_list}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:47.651279Z","iopub.execute_input":"2022-04-19T18:28:47.651781Z","iopub.status.idle":"2022-04-19T18:28:47.667616Z","shell.execute_reply.started":"2022-04-19T18:28:47.651735Z","shell.execute_reply":"2022-04-19T18:28:47.666664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>The strong muticollinearity is probably due to the presence of a lot 0's in these binary features. These features must be dropped<font>","metadata":{}},{"cell_type":"code","source":"# Drop features with high multicollinearity from X_train, X_test and df_test_new\nX_train.drop(high_vif_list, axis=1, inplace=True)\n\nX_test.drop(high_vif_list, axis=1, inplace=True)\n\ndf_test_new.drop(high_vif_list, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:47.672784Z","iopub.execute_input":"2022-04-19T18:28:47.677636Z","iopub.status.idle":"2022-04-19T18:28:47.69182Z","shell.execute_reply.started":"2022-04-19T18:28:47.677546Z","shell.execute_reply":"2022-04-19T18:28:47.690785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color='darkblue'>III.5. Cook distance<font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'>By calculating Cook distance we can detect influential observations in a regression model. Cook distance detects data with large residuals (outliers) that can distort the prediction and the accuracy of a regression.<font>","metadata":{}},{"cell_type":"code","source":"X_constant = sm.add_constant(X_train)\n\nmodel = sm.OLS(y_train, X_constant)\nlr = model.fit()\n\n# Cook distance\nnp.set_printoptions(suppress=True)\n\n# Create an instance of influence\ninfluence = lr.get_influence()\n\n# Get Cook's distance for each observation\ncooks = influence.cooks_distance\n\n# Result as a dataframe\ncook_df = pd.DataFrame({\"Cook_Distance\": cooks[0], \"p_value\": cooks[1]})\ncook_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:47.69865Z","iopub.execute_input":"2022-04-19T18:28:47.701009Z","iopub.status.idle":"2022-04-19T18:28:47.751276Z","shell.execute_reply.started":"2022-04-19T18:28:47.700938Z","shell.execute_reply":"2022-04-19T18:28:47.750337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove the influential observation from X_train and y_train\ninfluent_observation = cook_df[cook_df[\"p_value\"] < 0.05].index.tolist()\nprint(f\"Influential observations dropped: {influent_observation}\")\n\n# Drop these obsrevations\nX_train = X_train.drop(X_train.index[influent_observation])\ny_train = y_train.drop(y_train.index[influent_observation])","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:47.757944Z","iopub.execute_input":"2022-04-19T18:28:47.761965Z","iopub.status.idle":"2022-04-19T18:28:47.784102Z","shell.execute_reply.started":"2022-04-19T18:28:47.761897Z","shell.execute_reply":"2022-04-19T18:28:47.782806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <font color='darkblue'>IV. Modeling</font>","metadata":{}},{"cell_type":"markdown","source":"## <font color = 'darkblue'> IV.1. Models and metrics selection<font>","metadata":{}},{"cell_type":"markdown","source":"<font color = 'darkblue'> Here I am going to use RMSE and R² metrics in order to measure the performance of the selected models and their predictions.\n    \n<font color = 'darkblue'>Then I will test the models that best meet the estimation of house prices. It's clearly a regression so here are the following models I will use:\n- Ridge regression\n- Lasso regression\n- Elastic Net regression\n- Support Vector regression (SVR)\n- Random Forest regression\n- XGBoost\n- LigthGBM\n    <font>","metadata":{}},{"cell_type":"code","source":"import sklearn\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:48.823717Z","iopub.execute_input":"2022-04-19T18:28:48.823979Z","iopub.status.idle":"2022-04-19T18:28:48.829825Z","shell.execute_reply.started":"2022-04-19T18:28:48.823951Z","shell.execute_reply":"2022-04-19T18:28:48.829043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's define a function for each metrics\n# R²\ndef rsqr_score(test, pred):\n    \"\"\"Calculate R squared score \n\n    Args:\n        test -- test data\n        pred -- predicted data\n\n    Returns:\n        R squared score \n    \"\"\"\n    r2_ = r2_score(test, pred)\n    return r2_\n\n\n# RMSE\ndef rmse_score(test, pred):\n    \"\"\"Calculate Root Mean Square Error score \n\n    Args:\n        test -- test data\n        pred -- predicted data\n\n    Returns:\n        Root Mean Square Error score\n    \"\"\"\n    rmse_ = np.sqrt(mean_squared_error(test, pred))\n    return rmse_\n\n\n# Print the scores\ndef print_score(test, pred):\n    \"\"\"Print calculated score \n\n    Args:\n        test -- test data\n        pred -- predicted data\n\n    Returns:\n        print the regressor name\n        print the R squared score\n        print Root Mean Square Error score\n    \"\"\"\n\n    print(f\"- Regressor: {regr.__class__.__name__}\")\n    print(f\"R²: {rsqr_score(test, pred)}\")\n    print(f\"RMSE: {rmse_score(test, pred)}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:49.232765Z","iopub.execute_input":"2022-04-19T18:28:49.23354Z","iopub.status.idle":"2022-04-19T18:28:49.241051Z","shell.execute_reply.started":"2022-04-19T18:28:49.233497Z","shell.execute_reply":"2022-04-19T18:28:49.240049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define regression models\nridge = Ridge()\nlasso = Lasso(alpha=0.001)\nelastic = ElasticNet(alpha=0.001)\nsvr = SVR()\nrdf = RandomForestRegressor()\nxgboost = XGBRegressor()\nlgbm = LGBMRegressor()\n\n# Train models on X_train and y_train\nfor regr in [ridge, lasso, elastic, svr, rdf, xgboost, lgbm]:\n    # fit the corresponding model\n    regr.fit(X_train, y_train)\n    y_pred = regr.predict(X_test)\n    # Print the defined metrics above for each classifier\n    print_score(y_test, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:28:49.672156Z","iopub.execute_input":"2022-04-19T18:28:49.672496Z","iopub.status.idle":"2022-04-19T18:28:51.500149Z","shell.execute_reply.started":"2022-04-19T18:28:49.672433Z","shell.execute_reply":"2022-04-19T18:28:51.499352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color = 'darkblue'>According to the result of R² and Root Mean Squared of these 7 models, we can conclude that the relationship between the features and the target variable is clearly linear. Thus, in the next part I will optimize the hyperparameters of the model with highest R² and lowest RMSE: Ridge, Lasso, XGBRegressor and LGBMRegressor. <font>","metadata":{}},{"cell_type":"markdown","source":"## <font color = 'darkblue'> IV.2. Hyperparameters tuning and model optimization<font>","metadata":{}},{"cell_type":"markdown","source":"### <font color='darkblue'> IV.2.1. Ridge regression<font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'> Ridge will reduce the impact of features that are not important in predicting the target values.\n    \n[Click here for more information about Ridge regression](https://medium.com/@vijay.swamy1/lasso-versus-ridge-versus-elastic-net-1d57cfc64b58)<font>\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Define hyperparameters\nalphas = np.logspace(-5, 5, 50).tolist()\n\ntuned_parameters = {\"alpha\": alphas}\n\n# GridSearch\nridge_cv = GridSearchCV(Ridge(), tuned_parameters, cv=10, n_jobs=-1, verbose=1)\n\n# fit the GridSearch on train set\nridge_cv.fit(X_train, y_train)\n\n# print best params and the corresponding R²\nprint(f\"Best hyperparameters: {ridge_cv.best_params_}\")\nprint(f\"Best R² (train): {ridge_cv.best_score_}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:41:54.941064Z","iopub.execute_input":"2022-04-19T17:41:54.942078Z","iopub.status.idle":"2022-04-19T17:41:58.566859Z","shell.execute_reply.started":"2022-04-19T17:41:54.942029Z","shell.execute_reply":"2022-04-19T17:41:58.565709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ridge Regressor with the best hyperparameters\nridge_mod = Ridge(alpha=ridge_cv.best_params_[\"alpha\"])\n\n# Fit the model on train set\nridge_mod.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = ridge_mod.predict(X_test)\n\nprint(f\"- {ridge_mod.__class__.__name__}\")\nprint(f\"R²: {rsqr_score(y_test, y_pred)}\")\nprint(f\"RMSE: {rmse_score(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:41:58.573784Z","iopub.execute_input":"2022-04-19T17:41:58.576761Z","iopub.status.idle":"2022-04-19T17:41:58.609562Z","shell.execute_reply.started":"2022-04-19T17:41:58.576687Z","shell.execute_reply":"2022-04-19T17:41:58.608606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model results into lists\nmodel_list = []\nr2_list = []\nrmse_list = []\n\nmodel_list.append(ridge_mod.__class__.__name__)\nr2_list.append(round(rsqr_score(y_test, y_pred), 4))\nrmse_list.append(round(rmse_score(y_test, y_pred), 4))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:41:58.615357Z","iopub.execute_input":"2022-04-19T17:41:58.618215Z","iopub.status.idle":"2022-04-19T17:41:58.634176Z","shell.execute_reply.started":"2022-04-19T17:41:58.61815Z","shell.execute_reply":"2022-04-19T17:41:58.632891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Actual vs. Predicted house prices\nactual_price = np.exp(y_test[\"SalePriceLog\"])\npredicted_price = np.exp(y_pred)\n\nplt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (Ridge)\", fontsize=20)\nplt.scatter(actual_price, predicted_price,\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:41:58.694411Z","iopub.execute_input":"2022-04-19T17:41:58.697648Z","iopub.status.idle":"2022-04-19T17:41:58.958656Z","shell.execute_reply.started":"2022-04-19T17:41:58.697564Z","shell.execute_reply":"2022-04-19T17:41:58.957763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'> The model is able to predict house prices. However, it underestimates the prices of houses above 400,000 dollars <font>","metadata":{}},{"cell_type":"markdown","source":"### <font color='darkblue'> IV.2.2. Lasso regression<font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'> Lasso will eliminate many features, and reduce overfitting in the linear model.\n\n[Click here for more information about Lasso regression](https://medium.com/@vijay.swamy1/lasso-versus-ridge-versus-elastic-net-1d57cfc64b58)<font>","metadata":{}},{"cell_type":"code","source":"# Define hyperparameters\nalphas = np.logspace(-5, 5, 50).tolist()\n\ntuned_parameters = {\"alpha\": alphas}\n\n# GridSearch\nlasso_cv = GridSearchCV(Lasso(), tuned_parameters, cv=10, n_jobs=-1, verbose=1)\n\n# fit the GridSearch on train set\nlasso_cv.fit(X_train, y_train)\n\n# print best params and the corresponding R²\nprint(f\"Best hyperparameters: {lasso_cv.best_params_}\")\nprint(f\"Best R² (train): {lasso_cv.best_score_}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:43:37.940815Z","iopub.execute_input":"2022-04-18T08:43:37.941132Z","iopub.status.idle":"2022-04-18T08:43:40.376381Z","shell.execute_reply.started":"2022-04-18T08:43:37.941101Z","shell.execute_reply":"2022-04-18T08:43:40.374702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lasso Regressor with the best hyperparameters\nlasso_mod = Lasso(alpha=lasso_cv.best_params_[\"alpha\"])\n\n# Fit the model on train set\nlasso_mod.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = lasso_mod.predict(X_test)\n\nprint(f\"- {lasso_mod.__class__.__name__}\")\nprint(f\"R²: {rsqr_score(y_test, y_pred)}\")\nprint(f\"RMSE: {rmse_score(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:43:40.379766Z","iopub.execute_input":"2022-04-18T08:43:40.380524Z","iopub.status.idle":"2022-04-18T08:43:40.423286Z","shell.execute_reply.started":"2022-04-18T08:43:40.380468Z","shell.execute_reply":"2022-04-18T08:43:40.42192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model results into lists\nmodel_list.append(lasso_mod.__class__.__name__)\nr2_list.append(round(rsqr_score(y_test, y_pred), 4))\nrmse_list.append(round(rmse_score(y_test, y_pred), 4))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:43:40.42563Z","iopub.execute_input":"2022-04-18T08:43:40.427266Z","iopub.status.idle":"2022-04-18T08:43:40.448717Z","shell.execute_reply.started":"2022-04-18T08:43:40.427205Z","shell.execute_reply":"2022-04-18T08:43:40.446718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Actual vs. Predicted house prices\nactual_price = np.exp(y_test[\"SalePriceLog\"])\npredicted_price = np.exp(y_pred)\n\nplt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (Lasso)\", fontsize=20)\nplt.scatter(actual_price, predicted_price,\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:43:40.4555Z","iopub.execute_input":"2022-04-18T08:43:40.45779Z","iopub.status.idle":"2022-04-18T08:43:40.821517Z","shell.execute_reply.started":"2022-04-18T08:43:40.45772Z","shell.execute_reply":"2022-04-18T08:43:40.8204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color = 'darkblue'> IV.2.3. XGBoost regression <font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'>XGBoost is one of the most popular algorithms that are based on Gradient Boosted Machines. Gradient Boosting refers to a methodology where an ensemble of weak learners is used to improve the model performance in terms of efficiency, accuracy, and interpretability. Gradient Boosting can be applied to a regression by taking the average of the outputs by the weak learners. <font>\n\n[Click here for more information about XGBoost](https://neptune.ai/blog/xgboost-vs-lightgbm)","metadata":{}},{"cell_type":"code","source":"# Define hyperparameters\ntuned_parameters = {\"max_depth\": [3],\n                    \"colsample_bytree\": [0.3, 0.7],\n                    \"learning_rate\": [0.01, 0.05, 0.1],\n                    \"n_estimators\": [100, 500]}\n\n# GridSearch\nxgbr_cv = GridSearchCV(estimator=XGBRegressor(),\n                       param_grid=tuned_parameters,\n                       cv=5,\n                       n_jobs=-1,\n                       verbose=1)\n\n# fit the GridSearch on train set\nxgbr_cv.fit(X_train, y_train)\n\n# print best params and the corresponding R²\nprint(f\"Best hyperparameters: {xgbr_cv.best_params_}\\n\")\nprint(f\"Best R²: {xgbr_cv.best_score_}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:44:35.088503Z","iopub.execute_input":"2022-04-18T08:44:35.089627Z","iopub.status.idle":"2022-04-18T08:58:51.397755Z","shell.execute_reply.started":"2022-04-18T08:44:35.089566Z","shell.execute_reply":"2022-04-18T08:58:51.396465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>It's always better to set more hyperparameters with more cross validation but it's very time consuming with XGBRegressor in kaggle kerner. Thus, I limited the tuning to 3 hyperparameters with 5 cross validation only.","metadata":{}},{"cell_type":"code","source":"# XGB Regressor with the best hyperparameters\nxgbr_mod = XGBRegressor(seed=20,\n                        colsample_bytree=xgbr_cv.best_params_[\"colsample_bytree\"],\n                        learning_rate=xgbr_cv.best_params_[\"learning_rate\"],\n                        max_depth=xgbr_cv.best_params_[\"max_depth\"],\n                        n_estimators=xgbr_cv.best_params_[\"n_estimators\"])\n\n# Fit the model on train set\nxgbr_mod.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = xgbr_mod.predict(X_test)\n\nprint(f\"- {xgbr_mod.__class__.__name__}\")\nprint(f\"R²: {rsqr_score(y_test, y_pred)}\")\nprint(f\"RMSE: {rmse_score(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:58:57.972312Z","iopub.execute_input":"2022-04-18T08:58:57.972904Z","iopub.status.idle":"2022-04-18T08:58:59.780663Z","shell.execute_reply.started":"2022-04-18T08:58:57.972836Z","shell.execute_reply":"2022-04-18T08:58:59.779683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model results into lists\nmodel_list.append(xgbr_mod.__class__.__name__)\nr2_list.append(round(rsqr_score(y_test, y_pred), 4))\nrmse_list.append(round(rmse_score(y_test, y_pred), 4))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:59:02.423345Z","iopub.execute_input":"2022-04-18T08:59:02.424012Z","iopub.status.idle":"2022-04-18T08:59:02.435368Z","shell.execute_reply.started":"2022-04-18T08:59:02.423955Z","shell.execute_reply":"2022-04-18T08:59:02.43396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Actual vs. Predicted house prices\nactual_price = np.exp(y_test[\"SalePriceLog\"])\npredicted_price = np.exp(y_pred)\n\nplt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (XGBoost)\", fontsize=20)\nplt.scatter(actual_price, predicted_price,\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:35:37.058432Z","iopub.execute_input":"2022-04-19T14:35:37.058719Z","iopub.status.idle":"2022-04-19T14:35:37.404978Z","shell.execute_reply.started":"2022-04-19T14:35:37.058688Z","shell.execute_reply":"2022-04-19T14:35:37.403892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <font color = 'darkblue'> IV.2.4. LightGBM regression <font>","metadata":{}},{"cell_type":"markdown","source":"<font color='darkblue'> LightGBM is also one of the most popular algorithms that are based on Gradient Boosted Machines.<font>\n\n[Click here for more information about LightGBM](https://neptune.ai/blog/xgboost-vs-lightgbm)","metadata":{}},{"cell_type":"code","source":"# Define hyperparameters\ntuned_parameters = {\"max_depth\": [3, 6, 10], \"learning_rate\": [\n    0.01, 0.05, 0.1], \"n_estimators\": [100, 500, 1000], }\n\n# GridSearch\nlgbm_cv = GridSearchCV(estimator=LGBMRegressor(\n), param_grid=tuned_parameters, cv=10, n_jobs=-1, verbose=1)\n\n# fit the GridSearch on train set\nlgbm_cv.fit(X_train, y_train)\n\n# print best params and the corresponding R²\nprint(f\"Best hyperparameters: {lgbm_cv.best_params_}\\n\")\nprint(f\"Best R²: {lgbm_cv.best_score_}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:59:23.322011Z","iopub.execute_input":"2022-04-18T08:59:23.322523Z","iopub.status.idle":"2022-04-18T09:00:03.604906Z","shell.execute_reply.started":"2022-04-18T08:59:23.322485Z","shell.execute_reply":"2022-04-18T09:00:03.604147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LGBM Regressor with the best hyperparameters\nlgbm_mod = LGBMRegressor(learning_rate=lgbm_cv.best_params_[\"learning_rate\"],\n                         max_depth=lgbm_cv.best_params_[\"max_depth\"],\n                         n_estimators=lgbm_cv.best_params_[\"n_estimators\"])\n\n# Fit the model on train set\nlgbm_mod.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = lgbm_mod.predict(X_test)\n\nprint(f\"- {lgbm_mod.__class__.__name__}\")\nprint(f\"R²: {rsqr_score(y_test, y_pred)}\")\nprint(f\"RMSE: {rmse_score(y_test, y_pred)}\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T09:00:05.469379Z","iopub.execute_input":"2022-04-18T09:00:05.469725Z","iopub.status.idle":"2022-04-18T09:00:05.795786Z","shell.execute_reply.started":"2022-04-18T09:00:05.469692Z","shell.execute_reply":"2022-04-18T09:00:05.795055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model results into lists\nmodel_list.append(lgbm_mod.__class__.__name__)\nr2_list.append(round(rsqr_score(y_test, y_pred), 4))\nrmse_list.append(round(rmse_score(y_test, y_pred), 4))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T09:00:09.173058Z","iopub.execute_input":"2022-04-18T09:00:09.173429Z","iopub.status.idle":"2022-04-18T09:00:09.184434Z","shell.execute_reply.started":"2022-04-18T09:00:09.173393Z","shell.execute_reply":"2022-04-18T09:00:09.182966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot Actual vs. Predicted house prices\nactual_price = np.exp(y_test[\"SalePriceLog\"])\npredicted_price = np.exp(y_pred)\n\nplt.figure()\nplt.title(\"Actual vs. Predicted house prices\\n (LGBM)\", fontsize=20)\nplt.scatter(actual_price, predicted_price,\n            color=\"deepskyblue\", marker=\"o\", facecolors=\"none\")\nplt.plot([0, 800000], [0, 800000], \"darkorange\", lw=2)\nplt.xlim(0, 800000)\nplt.ylim(0, 800000)\nplt.xlabel(\"\\nActual Price\", fontsize=16)\nplt.ylabel(\"Predicted Price\\n\", fontsize=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T09:00:10.957546Z","iopub.execute_input":"2022-04-18T09:00:10.958305Z","iopub.status.idle":"2022-04-18T09:00:11.274518Z","shell.execute_reply.started":"2022-04-18T09:00:10.958258Z","shell.execute_reply":"2022-04-18T09:00:11.273615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <font color ='darkblue'> IV.3. Choosing the best model<font>","metadata":{}},{"cell_type":"code","source":"# Create a table with pd.DataFrame\nmodel_results = pd.DataFrame({\"Model\": model_list,\n                              \"R²\": r2_list,\n                              \"RMSE\": rmse_list})\n\nmodel_results","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:28:49.327606Z","iopub.execute_input":"2022-04-18T11:28:49.327972Z","iopub.status.idle":"2022-04-18T11:28:49.345017Z","shell.execute_reply.started":"2022-04-18T11:28:49.327934Z","shell.execute_reply":"2022-04-18T11:28:49.343418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<font color='darkblue'>The results show that that the best performances in terms of R squared (R²) and Root Mean Square Error (RMSE) correspond to the Ridge, Lasso, ElasticNet and XGB Regressor.\n\n<font color='darkblue'>Indeed, our goal is to maximize R² and minimize RMSE.\n     \n<font color='darkblue'>However, based on these results and the results of the figures that show the Actual prices vs. Predicted prices, we can conclude that the XGB Regressor model has the best performance, especially with homes priced above 350,000 dollars. <font>\n    \n<font color='red'>Thus, XGB Regressor model will be chosen to predict house prices of the Test set of this Kaggle competition.","metadata":{}},{"cell_type":"markdown","source":"## <font color ='darkblue'> IV.4. Prediction on 'House Prices-Advanced Regression Techniques' test data set<font>","metadata":{}},{"cell_type":"code","source":"# Predictions from Ridge model\npredictions_list = xgbr_mod.predict(df_test_new)\n\n# Conversion of logarithmic predictions to logical data Sale Price\nsaleprice_preds = np.exp(predictions_list)\n\n# DataFrame of test ID and their corresponding predictions\noutput = pd.DataFrame({\"Id\": Id_test_list,\n                       \"SalePrice\": saleprice_preds})\noutput.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T14:36:12.755186Z","iopub.execute_input":"2022-04-19T14:36:12.755465Z","iopub.status.idle":"2022-04-19T14:36:12.781243Z","shell.execute_reply.started":"2022-04-19T14:36:12.755434Z","shell.execute_reply":"2022-04-19T14:36:12.780301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the output\noutput.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:28:49.446943Z","iopub.status.idle":"2022-04-18T11:28:49.448222Z","shell.execute_reply.started":"2022-04-18T11:28:49.447788Z","shell.execute_reply":"2022-04-18T11:28:49.447833Z"},"trusted":true},"execution_count":null,"outputs":[]}]}