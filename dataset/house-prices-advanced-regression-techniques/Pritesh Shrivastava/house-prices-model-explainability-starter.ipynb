{"cells":[{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#!pip install --upgrade pip\n!pip install fastai==0.7.0 ## Based on Fast.ai ML course\n\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom IPython.display import display\nfrom fastai.imports import *\nfrom fastai.structured import *\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom pandas_summary import DataFrameSummary\nfrom matplotlib import pyplot as plt\nimport math\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\nimport re\n\nimport shap\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp, get_dataset, info_plots\n\nimport IPython\nfrom IPython.display import display\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The evaluation criteria is RMSE of log of Sales Price. So first, let's change the target variable to log"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['SalePrice'] = np.log(train_df['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll just use a Random Forest Regressor. For that, we need to convert all columns to numeric type. But there are some categorical variables too."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cats(train_df)\napply_cats(test_df, train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We'll replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable. Fastai to the rescue again !!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_trn, y_trn, nas = proc_df(train_df, 'SalePrice')\ndf_test, _, _ = proc_df(test_df, na_dict=nas)\ndf_trn.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.drop(['LotFrontage_na', 'MasVnrArea_na', 'BsmtFinSF1_na', 'BsmtFinSF2_na', 'BsmtUnfSF_na', \n              'TotalBsmtSF_na', 'BsmtFullBath_na', 'BsmtHalfBath_na', 'GarageYrBlt_na', 'GarageCars_na',\n              'GarageArea_na'], axis =1, inplace = True)\ndf_trn.drop(['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining function to calculate the evaluation metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(train_X), train_y), rmse(m.predict(val_X), val_y),     ## RMSE of log of prices\n                m.score(train_X, train_y), m.score(val_X, val_y)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split the data into training and validation sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(df_trn, y_trn, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can now pass this processed data frame to Random Forest Regressor"},{"metadata":{},"cell_type":"markdown","source":"Initially, let's just fit a single decision tree to visualize it properly"},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nm = RandomForestRegressor(n_estimators=1, min_samples_leaf=3, n_jobs=-1, max_depth = 3, oob_score=True) ## Use all CPUs available\nm.fit(train_X, train_y)\n\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"draw_tree(m.estimators_[0], train_X, precision=3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A single decision tree did not perform so badly. Now, let's bag a collection of trees to create a random forest."},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nm = RandomForestRegressor(n_estimators=20, min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True) ## Use all CPUs available\nm.fit(train_X, train_y)\n\nprint_score(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"## Permuation importance of features"},{"metadata":{"trusted":true},"cell_type":"code","source":"perm = PermutationImportance(m, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Partial Dependence Plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"for feat_name in val_X.columns:\n#for feat_name in base_features:\n    #pdp_dist = pdp.pdp_isolate(model=m, dataset=val_X, model_features=base_features, feature=feat_name)\n    pdp_dist = pdp.pdp_isolate(model = m, dataset=val_X, model_features=val_X.columns, feature=feat_name)\n\n    pdp.pdp_plot(pdp_dist, feat_name)\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SHAP values for selected rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"explainer = shap.TreeExplainer(m)\nshap_values = explainer.shap_values(val_X)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value, shap_values[1,:], val_X.iloc[1,:], matplotlib=True) ## change shap and val_X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, val_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap.summary_plot(shap_values, val_X, plot_type=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submitting Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = m.predict(df_test)\nsubmission = pd.read_csv('../input/sample_submission.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['SalePrice'] = np.exp(pred)   ## Convert log back \nsubmission.to_csv('rf_submission_v1.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"nbformat":4,"nbformat_minor":1}