{"cells":[{"metadata":{"_cell_guid":"8abd549e-ba60-4660-af1e-352274a3c0eb","_uuid":"8ce51cf43fcfa03815d9cda7e2fcad6ab08d224d"},"cell_type":"markdown","source":"### This Docs is Speical for me. It's my first long doc in kernel and train my writing skills! Please enjoy it.\n[Section]  \n\n00. Basic Preprocessing\n01. Normality Test & Indepdence Test\n - Normality with Kurtosis and Skewness & Correlation Analysis  \n - Chi-square Indepdence Test of Categorical and Ordinal Variables  \n02. Imputation & Outliers with Statistical Analysis\n - Imputation: Diagonse the Type of Missing, then Cure by a fitted way\n - Outlier Dectection: Bivariate Access to let know the Outlier, then deletion  \n03. Feature Engineering  \n04. Cross Validation & Parameter Tuning  \n---\n### 1. Basic Preprocessing: Based on Feature Descriptions, Figuring Out Variables \n    - Convert NA into 'No' categorical values\n    - Guess the Relationship with scheming variables name\n    - Discover a summation relationship    \n    - Unveil the possibility of transformation of time variables\n    - Grouping Num/Cat/Ord variables"},{"metadata":{"_cell_guid":"163df5e6-bf4f-4821-8d00-83af6a8c4f46","_uuid":"6f0496730d58f717d46da5c37db763b58de7e314","collapsed":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy.stats import skew, kurtosis\n\ndf_train = pd.read_csv('../input/train.csv')\n#df_train.sample(2)\n\n#0. Delete MiscVal\ndf_train.drop('MiscVal', axis = 1, inplace = True)\n#1. Convert NA into 'No' Categorical values\nmeaningfulNA = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', \n 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\ndf_train[meaningfulNA] = df_train[meaningfulNA].fillna('No')\n#df_train[meaningfulNA].isnull().sum()\n#print('Step1. Convert NA value as No cateogrical values From Description')\n\n#2. Guess the Relationship with Scheming variables name\n#if (df_train.loc[:,'TotalBsmtSF'] == df_train.loc[:,'BsmtFinSF1'] + df_train.loc[:,'BsmtFinSF2'] + df_train.loc[:,'BsmtUnfSF']).sum() == df_train.shape[0]:\n    #print('Step2. TotalBsmtSF = BsmtFinSf1 + BsmtFinSf2 + BsmtUnfSF')\n\n#3. Discover Summation Relationship\n##df_train['totalFlrSF'] = df_train.loc[:,'1stFlrSF'] + df_train.loc[:,'2ndFlrSF']\n#print('Step3. Find the feature \"totalFlr\", 1stFlrSF + 2ndFlrSF')\n\n#4. Unveil the possibility of transformation of time variables\n##df_train['SeasonSold'] = (df_train['YrSold'] % 2000) * 100 + df_train['MoSold']\n#print(\"Step4. Find the time variable 'SeasonSold', YrSold + MoSold\")\n\n#5. Complex Categorical/Ordinal/Numerical Variables?\n#qualCon = ['ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC', 'BsmtQual', 'BsmtCond', 'OverallQual', 'OverallCond']\n#compl = ['MSSubClass']\n#print(\"Step5. Extract variables that we don't know Categorical or Ordinal now\")\n# print(qualCon + compl)\n\nqualCon = ['ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC', 'BsmtQual', 'BsmtCond', 'OverallQual', 'OverallCond']\ncompl = ['MSSubClass']\ncat = df_train.select_dtypes(include=['object']).columns.tolist()\ncat = set(cat).difference(qualCon + compl)\nnum = df_train.select_dtypes(exclude=['object']).columns\nnum = set(num).difference(qualCon + compl)\n\n#Numerical\ntotal = ['LowQualFinSF'] #OverallQual, OverallCond in qualCon\nfloor = ['TotalBsmtSF', 'BsmtUnfSF', '1stFlrSF', '2ndFlrSF']\nroom = ['BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd']\nrest = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath','HalfBath']\nbsmt = ['BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']\narea = ['GrLivArea', 'LotArea', 'LotFrontage']\ngarage = ['GarageArea', 'GarageCars', 'GarageYrBlt']\noutRoom = ['Fireplaces', 'WoodDeckSF','OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\nseason = ['YearRemodAdd', 'GarageYrBlt', 'YearBuilt']\nseason2 = ['MoSold', 'YrSold']\n#Catgorical\ntotal2 = ['MSSubClass','MSZoning','BldgType','Foundation','HouseStyle']\nmaterial = ['RoofStyle', 'RoofMatl',  'MasVnrType', 'Electrical','Exterior1st', 'Exterior2nd']\nbsmt2 = ['BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\narea2 = ['LotShape','LotConfig']\ngarage2 = ['GarageType', 'GarageFinish'] \noutRoom2 = ['Heating', 'CentralAir','Fence', 'MiscFeature']\ntown2 = ['Neighborhood','LandContour','LandSlope','Condition1', 'Condition2']\nsystem2 = ['Street', 'Alley','PavedDrive',  'Utilities','Functional', 'SaleType', 'SaleCondition']\n#Ordinal\ntotal3 = ['OverallQual', 'OverallCond', 'MSSubClass']\nmaterial3 = [ 'ExterQual', 'ExterCond']\nbsmt3 = ['BsmtQual', 'BsmtCond']\nroom3 = ['KitchenQual']\ngarage3 = ['GarageQual', 'GarageCond']\noutRoom3 = ['FireplaceQu','HeatingQC', 'PoolQC']\nordVar = total3+material3+bsmt3 + room3 + garage3 + outRoom3","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"458aa47f-b292-4996-8b34-dcd71558c607","_uuid":"e3bf6288f9820730d73bb04401aa74e5ceba03ce"},"cell_type":"markdown","source":"### 2. Independence Test with Visualization & Statistical Measurement\n2.1. Indep_Dep Numerical Test\n- Separte Zero Varialbes and None Zero Variables\n- Find Distorted Variables regard of Normality by Kurtosis & Skewness\n- Correlation Test  **-> Find a direct and strong relationship with Indep **\n- VIF Test  **-> Deep insight of multicolinearity**\n---\n\n- Separe Zero variables and None zero variables  \nSince 0 means that there doesn't exsit in that instance. We have to see partially correlation matrix of them.  \n- Find Distorted Variables of Normality "},{"metadata":{"_cell_guid":"6b97cd4e-18c9-48c9-9c9b-6884903ad8e3","_uuid":"96519f5bc3fc789ff85697e84d151b7bfe90feca","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"overSkew = df_train.loc[:,num].columns[df_train.loc[:,num].skew() > 2.5].values\noverKurt = df_train.loc[:,num].columns[df_train.loc[:,num].kurt() > 2.5].values\nlowKurt = df_train.loc[:,num].columns[df_train.loc[:,num].kurt() < -2.5].values\nif not overSkew.size: print('overSkew: ' ,overSkew) \nelse:print('overSkew var exist')\nif not overKurt.size: print('overKurt: ' ,overKurt) \nelse: print('overKurt var exist')\nif not lowKurt.size: print('lowKurt is None + No uniform distribution in Data Sets')\nelse: print('lowKurt var exist')\n\noverDist = set(overSkew).union(set(overKurt)).union(set(lowKurt))\nmodNum = set(num) - overDist\nzeroNum = df_train.loc[:, overDist].columns[(df_train.loc[:, overDist] == 0).sum() > 100]\noverDistN0 = overDist - set(zeroNum)\noverDist0 = set(zeroNum)\n#print(list(zeroNum))\nprint('Distorted Variables None Zero ', overDistN0)\nprint('Distorted Varialbes Lot Zero ',overDist0)","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"eb148280-9cf1-4212-8162-c81839f2ab61","_uuid":"1538501b9ef0286af9916802c646ccc13c213f7f","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tmp_train = df_train.loc[:,overDistN0].dropna()\ndef reject_outliers(data, m=4):\n    return data[abs(data - np.mean(data)) < m * np.std(data)]\nf, axes = plt.subplots(2, 4, figsize = (12, 6), sharey = False)\ncolors = sns.color_palette(\"hls\", 8)\nfor ix, var in enumerate(tmp_train.columns):\n    row, col = divmod(ix, 4)\n    tmpVar = reject_outliers(tmp_train[var])\n    var_label = 'skew: ' + str(round(tmpVar.skew(),2)) + ' kurt: ' + str(round(tmpVar.skew(),2))\n    axes[row,col] = sns.distplot(tmpVar,kde = True, ax = axes[row, col], color =colors[ix])\n    axes[row, col].set_title(var)\n    axes[row, col].set_xlabel(var_label)\nplt.suptitle('The Distored Non Zero Variables Distribution +- 4 SD', fontsize = 12)\nplt.tight_layout()\nplt.subplots_adjust(wspace = 0.5, hspace = 0.5, top = 0.8)\nplt.show()\n#print(df_train.loc[:,overDistN0].apply(reject_outliers).dropna().apply(lambda x: pd.Series({'kurt': kurtosis(x), 'skew' : skew(x)})).T)\ndel tmp_train\n\nlogLot = np.log1p(df_train['LotArea']).kurt()\nrootLot = np.sqrt(df_train['LotArea']).kurt()\ncubicLot = np.power(df_train['LotArea'],2).kurt()\nminVal = min([logLot, rootLot, cubicLot])\nif logLot == minVal:\n    best = 'log'\n    df_train['LotArea_log'] = np.log1p(df_train['LotArea'])\nelif rootLot == minVal:\n    best = 'root'\n    df_train['LotArea_root'] = np.sqrt(df_train['LotArea'])\nelif cubicLot == minVal:\n    best = 'cubic'\n    df_train['LotArea_cubic'] = np.power(df_train['LotArea'],2)\nprint('The Most distorted variables is LotArea')\nprint('For LotArea, the Best TF is ' + best)","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"731ee230-5b47-4e48-bdbb-7d9ca1dcff22","_uuid":"e5fb93a680f2a0778223bf21edc5c20bd7da1e31"},"cell_type":"markdown","source":"Before Seeing the Analysis, I draw the distribution and got the kurt & skew for the range +- 4 standard deviation. Since if we look all of the values, such a values was so spiled by outliers. I hope to see a major impacts of them.  \n**Find**\n- The Shape of All looked a right skewed. But in statistical measurement, only LotArea has severe kurtosis.\n- the datatype of KitchenABvGr is 'int' and most of the values focused on 1.  \n\n**Treat**\n- LotArea transformed by the best one among (Log, Root, Cubic)"},{"metadata":{"_cell_guid":"cf9c77f4-7e30-4f55-8080-31f54af0c661","_uuid":"063d28d210e952b2a83f148de5796301dbeeff33","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tmp_train = df_train.loc[:,overDist0].dropna()\ndef reject_outliers2(data, m=4):\n    data = data[abs(data - np.mean(data)) < m * np.std(data)]\n    return data.loc[data != 0]\nf, axes = plt.subplots(3, 4, figsize = (12, 9), sharey = False)\ncolors = sns.color_palette(\"hls\", 12)\ndataF = dict()\nfor ix, var in enumerate(tmp_train.columns):\n    row, col = divmod(ix, 4)\n    tmpVar = reject_outliers2(tmp_train[var])\n    var_label = 'skew:'+ str(round(tmpVar.skew(),2)) + ' kurt: ' + str(round(tmpVar.kurt(),2))\n    sns.distplot(tmpVar,kde = True, ax = axes[row, col], color =colors[ix])\n    axes[row,col].set_title(var)\n    axes[row,col].set_xlabel(var_label)\nplt.tight_layout()\nplt.subplots_adjust(wspace = 0.5, hspace = 0.5, top = 0.85)\nplt.suptitle('The Distored Zero Variables Distribution +- 4 SD', fontsize = 12)\nplt.show()\n\ndel tmp_train","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"002ba333-95c8-47b8-aa71-54743f68c15c","_uuid":"dafadbe2442a76bdf986ad26660d12d3e8cfdf76"},"cell_type":"markdown","source":"**Find**\n- There are no severe distortion on Zero variables\n\n---\n#### Correlation Test  \n- Non-zero Case\n- zero Case"},{"metadata":{"_cell_guid":"e6a96461-b7d4-4ef9-9067-3f370945648f","_uuid":"13c32f52b29ef4097db799a707f1d450a66515af","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"num_none_zero = list(overDistN0) + list(modNum)\nfor ix, x in enumerate(num_none_zero):\n    if x == 'SalePrice':\n        salePos = ix\n        break\nnum_none_zero[salePos], num_none_zero[len(num_none_zero) - 1] = num_none_zero[len(num_none_zero) - 1], num_none_zero[salePos]\n\nnumCorr = df_train.loc[:,num_none_zero].corr().round(1)\n#numCorr = np.tril(numCorr.values, k = -1)\nprint('numVariables Shape : ', numCorr.shape)\n\nf, ax = plt.subplots(1,1, figsize = (12, 12))\nmask = np.zeros_like(numCorr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    sns.heatmap(numCorr, annot = True, mask=mask, vmin =0, square=True, linewidths = .5, cmap=\"YlGnBu\", ax = ax,\n               cbar_kws = {'shrink' : 0.7})\n    ax.set_yticks([])\nplt.title(\"Correlation Matrix\", fontsize = 15)\nplt.tight_layout()\nplt.show()","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"56573739-bae1-4b08-896c-25f9b3e0e3ca","_uuid":"122869294cc59ce8ee483b4df1ee91e64430cc49"},"cell_type":"markdown","source":"**Find**\n- Where we are using is important to SalePrice  \n  (Cor)TotalBsmtSF, 1stFlrSF, GrLivArea, FullBath, GargeCars, GarageArea > 0.5  \n  (Cor)LotArea = 0.3  \n- Kitchen, Beedroom, Bath(except FullBath) was trivial. "},{"metadata":{"_cell_guid":"0f2122f1-2aa5-49b6-8d08-bcc4b3ac8788","_uuid":"0584926aee704e67d6919455540afa38b6e11c57","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"num_zero = list(overDist0) + ['SalePrice']\npart_zero = df_train.loc[:, num_zero]\ndef zero_Corr(data, yVar):\n    indexF = data.columns\n    a = None\n    for col in indexF:\n        tmpVar = data.loc[data[col] != 0, :]\n        if a is None:\n            a = tmpVar.corr().loc[col,:]\n        else:\n            a = pd.concat([a, tmpVar.corr().loc[col,:]], axis = 1)\n    return a\nzeroCorr = zero_Corr(part_zero,  'SalePrice').round(1)\nprint('numVariables Shape : ', zeroCorr.shape)\n\nf, ax = plt.subplots(1,1, figsize = (12, 12))\nmask = np.zeros_like(zeroCorr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    sns.heatmap(zeroCorr, annot = True, mask=mask, vmin =0, square=True, linewidths = .5, cmap=\"Greens\", ax = ax,\n               cbar_kws = {'shrink' : 0.7})\n    ax.set_yticks([])\nplt.title(\"Correlation Matrix\", fontsize = 15)\nplt.tight_layout()\nplt.show()","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"0ffd66fc-9b9d-4f40-a097-3d148a73425f","_uuid":"8423921e407d323a43d2ba2d6e0e18d9da4193e6"},"cell_type":"markdown","source":"**Find**\n- MasVnrArea, BsmtFinSF1 is important.\n- The other variables couldn't give an impact to SalePrice\n---\n \n#### VIF Test\n- to Get Deep insight of multicolinearity"},{"metadata":{"_cell_guid":"11baa7bc-b778-4f6f-bd4d-77057c28fcbe","_kg_hide-output":false,"_uuid":"5a5aed696b91e7d202dc5afc147952ee5a93024b","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor \n#https://stats.stackexchange.com/questions/155028/how-to-systematically-remove-collinear-variables-in-python\ndef calculate_vif_(X, thresh=5.0):\n    variables = list(range(X.shape[1]))\n    droppedLst = []\n    dropped=True\n    while dropped:\n        dropped=False\n        vif = [variance_inflation_factor(X.iloc[:,variables].values, ix) for ix in range(X.iloc[:,variables].shape[1])]\n\n        maxloc = vif.index(max(vif))\n        if max(vif) > thresh:\n            #print('dropping \\'' + X.iloc[:, variables].iloc[:, maxloc].name)\n            droppedLst += variables[maxloc],\n            del variables[maxloc]\n            dropped=True\n    #print('Dropped variables: ')\n    #print(X.columns[droppedLst].tolist())\n    print('Remaining variables:')\n    print(X.columns[variables].tolist())\n    return X.iloc[:,variables]\n\nmissingVar = df_train.columns[df_train.isnull().sum() != 0].tolist()\n#print(missingVar)\n#I hope these are a resonable reason to exclude them\n#- GarageYrBlt has 0.8 correlation with YearBuilt\n#- LotFrontage has 0.7-0.8 correlation with LotArea\n#- MasVnrArea has lots of zero\n#- Except them, they are categorical\npart_VIF = set(num) - set(missingVar + ['SalePrice'])\ndf_VIF = df_train.loc[:, part_VIF].copy()\ndf_VIF_f = calculate_vif_(df_VIF)\ndf_VIF_f['SalePrice'] = df_train.loc[:, 'SalePrice']\ncorr_T = df_VIF_f.corr().round(1)\nmask = np.zeros_like(corr_T)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize = (12,12))\nsns.heatmap(corr_T, mask = mask, annot = True, cmap = 'BuGn', linewidth = .7, cbar_kws = {'shrink' : 0.7})\nplt.show()","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"d6024fb4-cdb5-4276-ae2d-f60f0a5f67d2","_uuid":"6afe03dc6c630f19391613fafc05ce762449396f"},"cell_type":"markdown","source":"**Find**\n- (Good)The correlation went to almost zero except the relationship with SalePrice.\n- (Bad) Since the code burtually delete the variable having max VIF value, the popularly known variables such as GrivArea was deleted!\n---\n  \n### 2.2. Indep_Dep Categorical Test\n- Boxplot with Indepdent Variables  **-> Find a Meaningful Categorical Variables**\n- Chi-Square with each others   **-> Discover the relationship between variables**\n"},{"metadata":{"collapsed":true,"_cell_guid":"c84e8ad4-3313-424b-a907-e5c6439680fb","_kg_hide-output":false,"_uuid":"ae4736f12fe9afa5ab15364bf46afd34b502582b","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def boxplotSet(sample, title, col = 3):\n    height, left = divmod(len(sample), col)\n    if left: height += 1\n    f, axes = plt.subplots(height, col, figsize = (12,3*height))\n    for ix, var in enumerate(sample):\n        r, c = divmod(ix, col)\n        sns.boxplot(x = var, y = 'SalePrice', data = df_train, ax = axes[r,c])\n        plt.xticks(rotation = 90)\n    plt.subplots_adjust(0, 0, 1, 0.9)\n    plt.suptitle(title, fontsize = 14)\n    plt.show()\nimport scipy.stats as stats\nfrom collections import defaultdict\ndef chiTest(dfCat):\n    varList = dfCat.columns.tolist()\n    posDict = defaultdict(list)\n    for ix in range(len(varList)):\n        var1 = varList[ix]\n        for ix2 in range(ix+1, len(varList)):\n            var2 = varList[ix2]\n            obsv1 = pd.crosstab(dfCat.loc[:,var1], dfCat.loc[:, var2])\n            obsv2 = pd.crosstab(dfCat.loc[:,var1], dfCat.loc[:, var2])\n            #obsv1 = pd.crosstab(dfCat.loc[:,var1], dfCat.loc[:, var2], normalize= 'index')#'columns'\n            #obsv2 = pd.crosstab(dfCat.loc[:,var1], dfCat.loc[:, var2], normalize= 'columns')#\n            _, p1, *_ = stats.chi2_contingency(observed= obsv1)\n            _, p2, *_ = stats.chi2_contingency(observed= obsv2)\n            \n            if p1 < 0.05:\n                posDict[var1] += var2,\n                #print(var1 + 'is important to ' + var2)\n            if p2 < 0.05:\n                posDict[var2] += var1,\n                #print(var2 + 'is important to ' + var1)\n    \n    varList = set(varList)\n    sd = True\n    for x, y in posDict.items():\n        if len(y) < len(varList) - 1:\n            print(x, 'is independent on ',set(varList).difference(set(y)).difference(set([x])))\n            sd = False\n    \n    if sd: print('Every variables are depedent on each others')\n    return posDict","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"bc5a834e-b123-4d93-8906-ae4ceecc21c7","_uuid":"4b0fef6a4251888bcc1bc7bd96223ada4317e2ce","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sample = ['BldgType', 'MSSubClass', 'MasVnrType', 'BsmtExposure', 'Electrical', 'RoofMatl']\nboxplotSet(sample, 'Representative Variables of Categorical Var')\n\n#####\nuseCat = ['MSZoning', 'MasVnrType', 'LotShape', 'GarageFinish', 'CentralAir', 'Neighborhood', 'BsmtExposure', 'LotConfig', 'GarageType']\nresDict = chiTest(df_train.loc[:, useCat])","execution_count":9,"outputs":[]},{"metadata":{"_cell_guid":"75b40808-e823-450a-a39a-fafded34a0c3","_uuid":"da91c37a7d625b01440487d8f913708eaeb06907"},"cell_type":"markdown","source":"**Find**  \n  \n1)Boxplots  \n- One_Biased Variables : BldgType  \n- Many_Baised Variables: MSSubClass, HouseStyle, RoofStyle  \n- Orderd Variables: MSZoning, MasVnrType, LotShape, GarageFinish, CentralAir, Neighborhood  \n- Uniform: BsmtExposure, LotConfig, GarageType  \n- One_Small Variables: Eletrical, BsmtFinType2, Heating, Fence, MiscFeature, Street, Alley, PavedDrive, Utilties, Functional, SaleType, SaleCondition  \n- Many_Small Variables: RoofMatl, Exterior1st, Exterior2nd, BsmtFinType1  \n*X_Biased And Y_Small is simultaneously shown*  \n_Neighborhood include [LandContour, LandSlope, Condition1, Condition2]_  \n_Even though some are ordered, it is hard to say 'Ordinal'._  \n**I think that 'x Biased Variables and y small Variables' cause severe biased prediction. So now I delete it!**  \n  \n2) Chi Square:  \n  http://hamelg.blogspot.kr/2015/11/python-for-data-analysis-part-25-chi.htmlY  \n - Based on Chi Independence Test, we can choose a categorical set ['MasVnrTypee', 'LotConfig'] or ['GarageFinsih', 'LotConfig']\n\n---\n3) Indep_Dep Oridnal variables\n- Boxplot with Indepdent Variables  **-> Find a Meaningful Ordinal Variables and a proper order of variables**\n- Chi-Square with each others  **-> DIscover the relationship between varaibles**"},{"metadata":{"_cell_guid":"622d7b56-ff63-4d56-b1af-6add780fdc55","_uuid":"cf742c57fe149484c8028409f804c5ea0159eb87","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sample = ['OverallQual', 'ExterQual', 'BsmtQual','OverallCond', 'ExterCond', 'BsmtCond']\nboxplotSet(sample, 'Representative Variables of Ordinal Var')\nuseOrd = ['OverallQual', 'ExterQual', 'BsmtQual', 'KitchenQual', 'FireplaceQu']\nresDict1 = chiTest(df_train.loc[:, useOrd])","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"ad84451c-64be-4df1-a258-daa9a019043c","_uuid":"f6ee577b92cae9deffbbac2c23bc82fc4631fa60"},"cell_type":"markdown","source":"**Find**  \n  \n1) Boxplot  \nLinear :OverallQual, ExterQual, BsmtQual, KitchenQual, FireplaceQu  \nQuestionMark: OverallCond, ExterCond, BsmtCond, GarageQual, GarageCond, PoolQC, MSSubClass -> Categorical  \n2) Chi Square  \nEvery varialbes are depdent on each others  \n---  \netc1) Converted Ordinal variables  into Categorical  \nOverallCond, GarageQual, GarageCond, PoolQC, MSSubClass  \nAll of them include in One_Biased Variables  \netc2) circleGraph to visualize the dependence "},{"metadata":{"_cell_guid":"e49874e8-80eb-4c0a-b59f-29372fc03c6d","_uuid":"b4a38a8f4b4109bb851e8e18021d04d3d225c26d"},"cell_type":"markdown","source":"## 3. Imputation and Outliers\n1) Imputaiton\n   - Find Null Variables\n   - Diagonse the type of Null value (MCAR/MAR) and type of the remedy based on Rules of Thums\n   - Cure the Null Variable, deletion, linear regression model(f), bayesian EM algorithm(not yet)  "},{"metadata":{"_cell_guid":"5736ce2e-de65-4e51-9928-b47ff6af7258","_kg_hide-output":false,"_uuid":"7376bc551dd7aaac5358823f1f9da16380b5cfe3","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"print('Step1. Find Null Varialbes')\nprint('Null Variables :' ,list(df_train.isnull().sum()[df_train.isnull().sum() != 0].index))\nprint('Step2. Diagonse the type of Error MCAR/MAR')\n(df_train.isnull().sum()[df_train.isnull().sum() != 0]) / df_train.shape[0]","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"760cac27-acd3-4ce8-b3be-cae8bf7d1b97","_uuid":"848b2e7a860baea0731f46c404b44b4e2a3ca000"},"cell_type":"markdown","source":"I delete 'MasVnrType', 'MasVnrARea', 'Eletrical'. By Rules of Thumb2 \"Deletions Based on Missing Data\", (1)  \n - Variables with as little as 15 percent missing data are candidates for deletion, but higher levels of missing data (20 -30%) can often be remedies"},{"metadata":{"_cell_guid":"16aca133-6be2-4a97-a3ed-33a1551bd97d","_uuid":"735e5e019d316d5189df89f22fe90a451c0c5a49","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_train.loc[:, ['GarageYrBlt', 'YearBuilt']].corr()['GarageYrBlt']","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"98264d71-02b9-4b14-a0df-49be69ad6cf9","_uuid":"7add1a226305792e972bf10cccc94ff8a55c0a1a"},"cell_type":"markdown","source":"I delete since GarageYrBlt has a high correlation 0.82. By Rules of Thumb2, \"Deletions Based on Missing Data\", (2)  \n- When deleting a variable, ensure that alternative variables, hopefully highly correlated, are available to represent the intent of the orignal variable"},{"metadata":{"_cell_guid":"991aea2c-5a93-43db-a99b-ac43c932aba0","_uuid":"0bcfa3740d8127b0f9c1cb0ee48128010d8633ee","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tmp = abs(numCorr.corr()['LotFrontage']).sort_values()\nprint(tmp.index.tolist()[-4:])\nprint(tmp.tolist()[-4:])","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"0297a3f8-aba3-4dc5-952e-464073ea65fa","_uuid":"c956ba5fab2de68a5c1bced8906993cc144f4927"},"cell_type":"markdown","source":"I impute LotFrontage by mean imputation & regression method. By Rules of Thumbs3, \"Imputation of Missing Data\", (2)\n- 10% to 20%, The increased presence of missing data makes the all-available, hot deck case substituion, and regression methods preferred for MCAR data, whereas mdodel-based mehtods are necesary with MAR missing data process  \n\n---\n** Linear Imputation **"},{"metadata":{"_cell_guid":"6776ff6b-394a-4ba5-a34e-ecece8ae8757","_uuid":"d529d0bfca03e9773667945d74df6ddb7ef6bbf1","collapsed":true,"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"#mean Impute\n#df_train.loc[:, 'LotFrontage'] = df_train.loc[:, 'LotFrontage'].fillna(df_train.loc[:, 'LotFrontage'].mean())\n#Linear Regression Imputation\nfrom sklearn import linear_model\ndfImpute = df_train.loc[:, ['1stFlrSF', 'LotArea', 'LotArea_log', 'LotFrontage']].copy()\ntestIx = dfImpute.loc[:,'LotFrontage'].isnull()\ndfTrain = dfImpute.loc[~testIx, :]\ndfTest = dfImpute.loc[testIx, :]\nlr = linear_model.LinearRegression()\nlr.fit(dfTrain.loc[:, ['1stFlrSF', 'LotArea', 'LotArea_log']], dfTrain.loc[:, 'LotFrontage'])\nlrImpute = lr.predict(dfTest.loc[:, ['1stFlrSF', 'LotArea', 'LotArea_log']])\n#df_train.loc[testIx:, 'LotFrontage] = lrImpute","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"2cc9d658-d052-438e-8194-199481d37c83","_uuid":"f148f1ed499940878ceed2c824f1c6d4755d5abc"},"cell_type":"markdown","source":"However I can't make sure 100% they are MCAR type. So that I tried to make the model based method, even though the number of houses on the town, having highest probability of missing NA value for LotFrontage, is small 29(15 missing)  \n\n\n### Rules Of Thums in Imputation\n\n- Rules of Thumb2 \"Deletions Based on Missing Data\"  \n    - Variables with as little as 15 percent missing data are candidates for deletion, but higher levels of missing data (20 -30%) can often be remedies\n    - When deleting a variable, ensure that alternative variables, hopefully highly correlated, are available to represent the intent of the orignal variable\n    - Always consider performing the anaylsis with and without the deleted cases or variables to identify any marked differneces  \n- Rules of Thumb3 \"Imputation of Missing Data\"\n    - Under 10%, Any of the imputation methods can be applied when missing data are this low, although the complete case method has been shown to be the least preferred.\n    - 10% to 20%, The increased presence of missing data makes the all-available, hot deck case substituion, and regression methods preferred for MCAR data, whereas mdodel-based mehtods are necesary with MAR missing data process\n    - OVer 20%, If it deemed necessary to impute missing data when the level is over 20 percent, the preferred mehthods are:\n        - The regressiom method for MCAR situations\n        - Model based methods when MAR missing data occur\n  \n*etc1, complete case - delete the sample having NA value on one of the variables  \netc2, MCAR - Missing completely ar random, that is, the cases with missing data are indistinguishable from cases with complete data  \netc3, MAR - the missing values of Y depend on X, but not on X, for example, we know'the gender of respondents(X var) and are asking about household income(Y var). We find that the missing data are random for both but occur at a much higher frequency for males than females. We have to remedey the missing data by different way according to the gender.*\n\n---\n\n2) Outlier\n   - Univariate Methods, (+- 4 sigma)\n   - Bivariate Methods, (Bivaraite Normal Distribution 0.9994% ~ +- 4sigma)\n   - Multivariate Detect with Mahalanobis D Meausre(not yet)"},{"metadata":{"_cell_guid":"0f324ec9-6e22-4ab7-8d7b-b4bf1556ffdf","_uuid":"fbd926bfe385ffe5dd10207c296d15c7df72fbec","collapsed":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def checkOutlier(df, m = 4):\n    uniOutlier = dict().fromkeys(df.columns, None)\n    outSample = abs(df - df.mean()) > 4 * df.std()\n    outSum = (abs(df - df.mean()) > 4 * df.std()).sum()\n    for key in uniOutlier.keys():\n        uniOutlier[key] = set(outSample.index[outSample.loc[:, key]])\n    outportion = outSum / df.shape[0]\n    print(\"No outlier: \" ,outSum.index[outportion == 0].tolist())\n    #print(\"Outlier Portion\")\n    #print(outportion[outportion != 0].index.tolist())\n    #print(outportion[outportion != 0].values.tolist())\n    outportion = outportion[outportion != 0].sort_values()\n    outlierLst = outportion.index.tolist()\n    return uniOutlier, outlierLst\n\nfrom collections import Counter\ndef outlierCounter(outlierDict, exceptionLst = ['SalePrice']):\n    inter = Counter()\n    name = defaultdict(list)\n    coreKey = set(outlierDict.keys()).difference(exceptionLst)\n    for key in coreKey:\n        value = outlierDict[key]\n        for val in value:\n            inter[val] += 1\n            name[val].append(key)\n    res = pd.DataFrame([inter, name], index = ['count', 'variable']).T\n    res = res.sort_values('count', ascending = False)\n    return res","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"1b7f5f7e-192c-441e-b5e8-e99d8c4dc176","_uuid":"a49ece906cb5c637d66b5d17e02ab6968f388c7e","trusted":true},"cell_type":"code","source":"uniOutlier, outlierList = checkOutlier(df_train.loc[:, num_none_zero])","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"d7fa4cde-039b-4a27-8850-172fd5d5add0","_uuid":"14128ece500199c453b6d806e67ab8f7b53b1ed0"},"cell_type":"markdown","source":"Would you remember KitchenAbvGr just be dense on '1'. It's hard to say normality."},{"metadata":{"_cell_guid":"2e67e4d7-142f-420f-84b1-b7daff7ab886","collapsed":true,"_uuid":"e5637fb4402d5a370499006fd093a124fe255a2f","trusted":true},"cell_type":"code","source":"uniOut = outlierCounter(uniOutlier, ['KitchenAbvGr','SalePrice'])","execution_count":17,"outputs":[]},{"metadata":{"_cell_guid":"73274f40-1e0b-4286-aea8-fee61850dcec","_uuid":"685033094926b94b2b98ea84b508cec55acd470a"},"cell_type":"markdown","source":"- Bivariate Methods, (Bivaraite Normal Distribution 0.9994% ~ +- 4sigma)"},{"metadata":{"_cell_guid":"fe135632-a701-4ded-8524-539ee31d309c","_uuid":"2a2b63866821b28fdf3681c0bdabbfee37b22766","collapsed":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"from scipy.stats import multivariate_normal\ndef bivarCI(dfNum, y = 'SalePrice', outer = 10, z_score = 0.00006, cols = 2):\n    \n    colNum = dfNum.shape[1]\n    row, col = divmod(colNum-1, cols)\n    if row == 1 and col == 0: row += 1\n    if col != 0: row += 1\n    \n    \n    z_under = z_score * 0.98\n    z_upper = z_score * 1.02\n    \n    biOutlier = dict().fromkeys(dfNum.columns, None)\n    f, axes = plt.subplots(row, cols, figsize = (4*cols, 4*row))\n    f.suptitle('Bivaraite CI', fontsize = 12)\n    for ix, var1 in enumerate(dfNum.columns):\n        if var1 == y: break\n        r,c = divmod(ix, cols)\n        dfPart = dfNum.loc[:, [var1,y]]\n        dfPart = dfPart[~dfPart.isnull()].copy()\n        dfPart = dfPart.loc[dfPart.loc[:, var1] != 0,:]\n        dfPart = (dfPart - dfPart.mean()) / dfPart.std()\n        F, X, Y, posProb = bivarConverter(dfPart, outer, z_under, z_upper, N = 700)\n        axes[r,c].contourf(X, Y, posProb)\n        axes[r,c].scatter(dfPart.loc[:, var1], dfPart.loc[:, y], alpha = 1)\n        axes[r,c].set_title('Bivaraite CI ' + var1)\n        dfPartProb = F.pdf(dfPart.values)\n        outIndex = dfPart.index[dfPartProb < z_score]\n        biOutlier[var1] = set(outIndex.tolist())\n    f.tight_layout(rect = [0, 0.03, 1, 0.95])\n    plt.show()\n    \n    return biOutlier\n\ndef bivarConverter(df, outer, z_under, z_upper, N = 500):\n    x_init, y_init = df.min() - outer\n    x_end, y_end = df.max() + outer\n    X = np.linspace(x_init, x_end, N)\n    Y = np.linspace(y_init, y_end, N)\n    X, Y = np.meshgrid(X, Y)\n    pos = np.empty(X.shape + (2,))\n    pos[:,:,0] = X\n    pos[:,:,1] = Y\n    F = multivariate_normal(mean=df.mean().values, cov=df.corr().values)\n    posProb = F.pdf(pos)\n    posProb[(z_under < posProb) & (posProb < z_upper)] = 1\n    posProb[(z_under > posProb) | (posProb < z_upper)] = 0\n    \n    \n    return F , X, Y, posProb\n","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"9dcebc8d-6d71-4461-a514-d5c424876f85","_uuid":"d38945f0d96ca721392502b77e96b4ea8cc1b3b9","trusted":true},"cell_type":"code","source":"biOutlier = bivarCI(df_train.loc[:, num_none_zero], outer = 2, z_score = 0.00006,  cols = 4)","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"2834a07f-9677-42f4-8390-eb8b5f28fa1a","_uuid":"09bab53da0f1ef3ab7907e74404a26764651e98e"},"cell_type":"markdown","source":"'YrSold', 'SalePrice', 'KitchenAbvGr', 'BsmtUnfSF', 'BsmtFullBath', 'Id', 'Fireplace', 'HalfBath' are 100% hard to say bivaraite normal distribution"},{"metadata":{"_cell_guid":"8fbf66f3-8e60-428d-a484-ff65e00a7266","_uuid":"1b052d0b425e420483e339b6e6aad3197a293b69","trusted":true,"collapsed":true},"cell_type":"code","source":"biOut = outlierCounter(biOutlier, ['YrSold', 'SalePrice', 'KitchenAbvGr', 'BsmtUnfSF', 'BsmtFullBath', 'Id', 'Fireplace', 'HalfBath'])","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"fa548771-5730-4361-81e9-2c9afe9e93fb","_uuid":"10df0700ace264c8fcdbcb0a299bfc2cd59fc929"},"cell_type":"markdown","source":"- Intersection of biOut and uniOut"},{"metadata":{"_cell_guid":"d45ff32a-8727-4672-8e65-c9056772e137","_uuid":"792300d49167ed460fd96ae214e9c7d377aa417e","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def mergeTwoOut(uni, bi, cutoff = 1):\n    uni = uni.loc[uni.loc[:,'count'] != cutoff,:].copy()\n    bi = bi.loc[bi.loc[:,'count'] != cutoff,:].copy()\n    interIx = set(uni.index).intersection(bi.index)\n    totCnt = uni.loc[interIx,'count'] + bi.loc[interIx,'count']\n    totVar = (uni.loc[interIx,'variable'] + bi.loc[interIx, 'variable']).map(set)\n    res = pd.concat([totCnt, totVar], axis = 1).sort_values('count', ascending = False)\n    return res\n\ninterOut = mergeTwoOut(uniOut, biOut)\nprint(interOut)\nprint(\"See the Case Outlier with Red Dot\")\n\nf, axes = plt.subplots(1,3, figsize = (12, 4))\nf.suptitle('Multivaraite Outlier Dot Graph')\naxes[0].scatter(df_train.loc[:,'GrLivArea'], df_train.loc[:, 'SalePrice'])\naxes[0].scatter(df_train.ix[1298,'GrLivArea'], df_train.ix[1298, 'SalePrice'], c = 'r', s = 30)\naxes[0].set_title('GrLivArea + SalePrice')\naxes[1].scatter(df_train.loc[:,'GrLivArea'], df_train.loc[:, 'SalePrice'])\naxes[1].scatter(df_train.ix[523,'GrLivArea'], df_train.ix[523, 'SalePrice'], c = 'r', s = 30)\naxes[1].set_title('GrLivArea + SalePrice')\naxes[2].scatter(df_train.loc[:,'LotArea'], df_train.loc[:, 'SalePrice'])\naxes[2].scatter(df_train.ix[706,'LotArea'], df_train.ix[706, 'SalePrice'], c = 'r', s = 30)\naxes[2].set_title('LotArea + SalePrice')\nf.tight_layout(rect = [0, 0.03, 1, 0.95])\nplt.show()","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"da1fc314-7744-4da7-b510-6763912efd92","_uuid":"891d98ea3fa036c945f3b9678205151db5b5d05c"},"cell_type":"markdown","source":"Undoubtely Delete the RED Point, InterOut!"},{"metadata":{"_cell_guid":"c069d9cb-8838-4e94-bdf0-5cbf885110e8","collapsed":true,"_uuid":"1a2dfb4c1bd551c59b46e578498ed3216938aa7a","trusted":true},"cell_type":"code","source":"df_train = df_train.drop(interOut.index)","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"a331d93e-7660-4d39-8c34-6c0d55d69ba8","_uuid":"f75ed6d1dec6115de78753f714b97c2b86eecd49"},"cell_type":"markdown","source":"- Multivariate Detect with Mahalanobis D Meausre(will do it)\n\n\n### Rules of Tums in Outlier\n- Rules Of Thumb4 \"Outlier Dectection\"\n    - Univariate methods : Examine all metric variables to identify unique or extreme obsevations\n        - For small samples (<= 80), outliers typically are defined as cases with standard scores of 2.5 or greater\n        - For larger sample sizes, increa the threshold value of standard scores up to 4\n    - Bivaraite Methods: Focus on the relationships such as the independent vs dependent variables\n        - Use scatterplots with confidence intervals at a specified alpha level\n    - Multivariate methods: Best suited for examing a complete variate, such as the independent variables in regression or the variables in factor analysis\n        - Threshold levels for the D^2/measure should be conservative (.005 or .001), resulting in values of 2.5(small samples) vs 3 or 4 in larger samples\n---\n\n## 4. Feature Enginering\n   1) Numerical Variables \n   - Cerate new ratios and proportions\n   - Apply standard transformations\n   - Check variables for seasonality and create the model for right period  \n*https://www.analyticsvidhya.com/blog/2013/11/simple-manipulations-extract-data/*"},{"metadata":{"_cell_guid":"44c7f307-5ed7-4c09-96e9-0fb642f6806d","_uuid":"11ec2829d5b3fc0d1c88660ff2ae4435f2d7536b","collapsed":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"total = ['LowQualFinSF']\nfloor = ['1stFlrSF', '2ndFlrSF']\nroom = ['BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd']\nrest = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath','HalfBath']\nbsmt = ['BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']\narea = ['GrLivArea', 'LotArea', 'LotFrontage']\ngarage = ['GarageArea', 'GarageCars']\nseason = ['YearRemodAdd', 'GarageYrBlt', 'YearBuilt']\nseason2 = ['MoSold', 'YrSold']\noutRoom = ['Fireplaces', 'WoodDeckSF','OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal']\n\n#New ideas\n#0. Mean = OverallQual + OverallCOn\n#1. Bath / Area  of Floor and Bsmt\n#2. area, I can't think in now\n#3. season, Check Remod or Not / Check the GarageYrBlt == YearBuilt\n#4. outRoom, exist or Not\n\n#total\ndf_total = df_train.loc[:, total].copy()\n#Floor\ndf_floor = df_train.loc[:,floor].copy()\ndf_floor['totalFlrSF'] = df_floor.sum(axis = 1)\ndf_floor['1stFlrRatio'] = df_floor.loc[:,'1stFlrSF'] / df_floor.loc[:,'totalFlrSF']\ndf_floor['2ndFlrRatio'] = df_floor.loc[:,'2ndFlrSF'] / df_floor.loc[:,'totalFlrSF']\n#rest\n## Be careful where no have BsmBath, FullBath -> Dividng by 0\ndf_rest = df_train.loc[:, rest].copy()\ndf_rest['BsmtBath'] = df_rest.loc[:,'BsmtFullBath'] + df_rest.loc[:, 'BsmtHalfBath'] * 0.5\ndf_rest['BsmtFullBathRatio'] = df_rest.loc[:,'BsmtFullBath'] / df_rest['BsmtBath']\ndf_rest['BsmtHalfBathRatio'] = df_rest.loc[:,'BsmtHalfBath'] / df_rest['BsmtBath']\ndf_rest['FloorBath'] = df_rest.loc[:,'FullBath'] + df_rest.loc[:, 'HalfBath'] * 0.5\ndf_rest['FloorFullBathRatio'] = df_rest.loc[:,'FullBath'] / df_rest['FloorBath']\ndf_rest['FloorHalfBathRatio'] = df_rest.loc[:,'HalfBath'] / df_rest['FloorBath']\ndf_rest[np.isinf(df_rest)] = 0\n#var_ = ['BsmtFullBathRatio', 'BsmtHalfBathRatio']\n#var__ = ['FloorFullBathRatio', 'FloorHalfBathRatio']\n#for var in var_:\n#    df_rest.loc[:, var] = df_rest.loc[:, var].fillna(0)\n#for var in var__:\n#    df_rest.loc[:, var] = df_rest.loc[:, var].fillna(df_rest.loc[:, var].mean())\n#Room\n## Be careful where no have BsmBath, FullBath -> Dividng by 0\ndf_room = df_train.loc[:, room].copy()\ndf_room['AbvGrSum'] = df_room.sum(axis = 1)\ndf_room['BedroomAbvGrRatio'] = df_room.loc[:,'BedroomAbvGr'] / df_room.loc[:,'AbvGrSum']\ndf_room['KitchenAbvGrRatio'] = df_room.loc[:,'KitchenAbvGr'] / df_room.loc[:,'AbvGrSum']\ndf_room['TotRmsAbvGrdRatio'] = df_room.loc[:,'TotRmsAbvGrd'] / df_room.loc[:,'AbvGrSum']\ndf_room[np.isinf(df_room)] = 0\n#Bsmt\n## Be careful where no have BsmBath, FullBath -> Dividng by 0\ndf_bsmt = df_train.loc[:,bsmt].copy()\ndf_bsmt.loc[:,'BsmtFinSF1Ratio'] = df_bsmt.loc[:,'BsmtFinSF1'] / df_bsmt.loc[:,'TotalBsmtSF']\ndf_bsmt.loc[:,'BsmtFinSF2Ratio'] = df_bsmt.loc[:,'BsmtFinSF2'] / df_bsmt.loc[:,'TotalBsmtSF']\ndf_bsmt.loc[:,'BsmtUnfSFRatio']= df_bsmt.loc[:,'BsmtUnfSF'] / df_bsmt.loc[:,'TotalBsmtSF']\ndf_bsmt[np.isinf(df_bsmt)] = 0\ndf_bsmt = df_bsmt.fillna(0)\n#area\ndf_area = df_train.loc[:, area].copy()\ndf_area.loc[:,'LotFrontage'] = df_area.loc[:,'LotFrontage'].fillna(df_area.loc[:,'LotFrontage'].mean())\n#garage\ndf_garage = df_train.loc[:, garage].copy()\n#outRoom\ndf_outRoom = df_train.loc[:, outRoom].copy()\n\n#Adding\ndf_add = df_bsmt.loc[:,'TotalBsmtSF'] + df_floor.loc[:, 'totalFlrSF']\ndf_add.name = 'fullArea'\n\n#Regard of Area, skewed or kurtosis\n\n#Season\n#I want to dropoff GarageYrBlt since it has not big difference with YrBuilt\n#deletion = 'GarageYrBlt' since it has lots of NA and high correlation with YearBuilt\n#New Cat : SeasonRemod, SeasonGarYr\ndf_season = df_train.loc[:, season].copy()\n#df_season = df_season.drop('GarageYrBlt', axis = 1)\ndf_season['SeasonRemod'] = (df_season.loc[:,'YearRemodAdd'] - df_season.loc[:, 'YearBuilt'])\ndf_season.loc[df_season['SeasonRemod'] != 0, 'SeasonRemod'] = 1\n#Season2\ndf_season2 = df_train.loc[:, season2].copy()\ndf_season2['SeasonSold'] = (df_season2['YrSold'] % 2000) * 100 + df_season2['MoSold']\n\ndf_Num = pd.concat([df_total, df_floor, df_rest, df_room, df_bsmt, df_area, df_garage,df_outRoom, df_season, df_season2, df_add], axis = 1)\ndel df_total, df_floor, df_rest, df_room, df_bsmt, df_area, df_garage,df_outRoom, df_season, df_season2, df_add","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"8f9bbc60-2e3b-424f-ab99-b37bc7f31d3e","_uuid":"de468858796dd5fbbaa9b533144898fd7bc7b79e"},"cell_type":"markdown","source":"2) Categorical Variables, Get dummies!\n"},{"metadata":{"_cell_guid":"b56e3635-5980-4b2e-8516-7883efd48ed0","_uuid":"56ec0f8676dcb0b94e963af901c703102f433c14","collapsed":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"useCat = ['MSZoning','BsmtExposure', 'GarageFinish', 'GarageType', 'MasVnrType', 'LotShape', 'LotConfig', 'CentralAir', 'Neighborhood']\ndf_Cat = df_train.loc[:, useCat].copy()\ndf_Cat = pd.get_dummies(df_Cat, prefix = useCat)","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"75930a5e-4a00-4118-97eb-19c08379535e","_uuid":"a5b9e8e544a0e18fc9822d4d8ba3a36c0905effd"},"cell_type":"markdown","source":"3) Ordinal Variables, Mapping values according to a correct order\n"},{"metadata":{"_cell_guid":"cb6e4397-72af-4ddf-9f43-d56d49736350","_uuid":"ad3b78bcb9860c9aa374289e6bad4531c1196abf","collapsed":true,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"useOrd = ['OverallQual', 'ExterQual', 'BsmtQual', 'KitchenQual', 'FireplaceQu']\ndf_Ord = df_train.loc[:, useOrd].copy()\ndf_Ord.loc[:,'ExterQual'] = df_Ord.loc[:,'ExterQual'].map({'Fa':1, 'TA':2, 'Gd' : 3, 'Ex': 4})\ndf_Ord.loc[:,'BsmtQual'] = df_Ord.loc[:,'BsmtQual'].map({'No':1, 'Fa':1, 'TA':2, 'Gd' : 3, 'Ex': 4})\ndf_Ord.loc[:,'KitchenQual'] = df_Ord.loc[:,'KitchenQual'].map({'Fa':1, 'TA':2, 'Gd' : 3, 'Ex': 4})\ndf_Ord.loc[:,'FireplaceQu'] = df_Ord.loc[:,'FireplaceQu'].map({'No':1, 'Po': 1,'Fa':1, 'TA':2, 'Gd' : 3, 'Ex': 4})","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"30485c39-8345-43dc-bed0-e1b9f81d19ea","_uuid":"989ce51306e70cf460eed60629a46e39ee887e49"},"cell_type":"markdown","source":"Further Engineering is possible excpecially between Oridnal and Numerical Variables! And If you find any meaningful relationship, Do it! for example: TotalBsmtSF / BsmtQual"},{"metadata":{"_cell_guid":"e3005fff-44f3-450a-8706-0c0a7de251ba","_uuid":"21196434b25ed592e9ef6df61ab56cc552a17468","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df_train2 = pd.concat([df_Num, df_Cat,df_Ord], axis = 1)\nprint(\"Merging Finished\")\nif len(df_train2.columns) == len(df_train2.select_dtypes(exclude= ['object']).columns):\n    print(\"All of them converted into numerical!\")\nelse:\n    print(\"Where is still categorical?\")\n    print(df_train2.select_dtypes(include = ['object']).columns)","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"f91cd3be-c1d8-4a91-97ad-b9fdbade8ff8","_uuid":"50114dd5d424c75bd920e65f41ad911c6b71c297"},"cell_type":"markdown","source":"Feature Engineering Success!\n\n---\n## 5. Tune Parameter \n\n   ** This is done before Chapter 02. Imputation and Outliers**\n   1) Parameter Tuning of Linear Regression\n    - 5 Fold Cross Validation & alpha\n        -> Ridge Regressoin, alpha = (5, RMSE: 32902), (10, MAE : 19793) in [1, 5, 10, 100]\n        -> Lasso Regression, alpha = (1, RMSE : 33516), (1, MAE : 20296) in [0.1, 0.3, 0.5, 0.7, 0.9, 1]\n\n    \n    02. Parameter Tuning in GBM\n    - 5 Fold Cross Validation & (n_estimatprs & Tree-specific parameters & subsample & learing rate)\n        -> n_estimators, 5808 **This value gets easily after repeatation of the following procedures**\n        -> max_depth(and num_samples_split), 8\n        -> Tune min_samples_leaf,10\n        -> Tune max_features, 0.5\n        -> Tune subsample, 0.45\n        -> Tune learning rate, 0.3\n"},{"metadata":{"_cell_guid":"85b1a993-032b-4934-8f39-eb8f7405e948","collapsed":true,"_uuid":"f77081899eb4859eed10e71b6aa0c293454b9d33","trusted":false},"cell_type":"code","source":"from sklearn import linear_model\nfrom math import sqrt\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom collections import defaultdict","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"39be7a28-e5c4-4e6f-be73-1d82e07e2663","collapsed":true,"_uuid":"bdefe2e7516d87d2204a0669b7b15c52a190449c","trusted":false},"cell_type":"code","source":"def regressionCV(df, dfY, alpha, n_splits, sort):\n    kf = KFold(n_splits = n_splits, random_state = 2)\n    kf.get_n_splits(df)\n    lstRMSE = []\n    lstMAE = []\n    for ixx, (train_index, test_index) in enumerate(kf.split(df)):\n        #print(str(ixx) + 'th Fold Prediction')\n        X_train, X_cross = df.iloc[train_index,:], df.iloc[test_index,:]\n        y_train, y_cross = dfY.iloc[train_index], dfY.iloc[test_index]\n        if sort == 'ridge':\n            reg = linear_model.Ridge(alpha = alpha)\n        elif sort == 'lasso':\n            reg = linear_model.Lasso(alpha = alpha)\n        reg.fit(X_train, y_train)\n        y_pred = reg.predict(X_cross)\n        rmse = sqrt(mean_squared_error(y_pred, y_cross))\n        mae = mean_absolute_error(y_pred, y_cross)\n        lstRMSE += rmse,\n        lstMAE += mae,\n            \n        del reg, X_train, X_cross, y_train, y_cross\n    \n    meanRMSE = sum(lstRMSE) / n_splits\n    meanMAE = sum(lstMAE) / n_splits\n    print(sort, ' ', alpha, 'rmse: ', meanRMSE, 'mae: ',meanMAE)\n    return [sort, alpha, meanRMSE, meanMAE]\n\ndef findAlpha(df, dfY, alphaLst, n_splits = 5, sort = 'ridge'):\n    resultLst = []\n    for alpha in alphaLst:\n        resultLst += regressionCV(df, dfY, alpha, n_splits, sort),\n    resultLst = sorted(resultLst, key=lambda x: (x[2], x[3]))\n    return resultLst\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2e4febe9-e2ee-4df1-80ba-f75b183d2ee4","collapsed":true,"_uuid":"0b13da96843175d45ab1d681f677c8efb2d0df80","trusted":false},"cell_type":"code","source":"def regressionResult(df, dfY):\n    df_trainY = df_train.loc[:,'SalePrice'].copy()\n    df_trainX = df_train2.copy()\n    if df_trainX.isnull().sum().sum() == 0:\n        print(\"Not Null in data\")\n    else:\n        nanIndex = df_trainX.isnull().sum()[df_trainX.isnull().sum() != 0].index\n        df_trainX = df_trainX.drop(nanIndex, axis = 1)\n        print(\"Null in \", nanIndex)\n    ridgeLst = findAlpha(df_trainX, df_trainY, [1, 5, 10, 100])\n    lassoLst = findAlpha(df_trainX, df_trainY, [0.1, 0.3, 0.5, 0.7, 0.9, 1], sort = 'lasso')\n    return ridgeLst, lassoLSt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"66fe6c48-9585-4858-9850-751af2252517","collapsed":true,"_uuid":"14fdb58f26b1af8a2bc173a0c927d50f425201c2","trusted":false},"cell_type":"code","source":"#print('Ridge RMSE Min Alpha',sorted([(lst[1], lst[2]) for lst in ridgeLst], key = lambda x: x[1])[0])\n#print('Ridge MAE Min Alpha', sorted([(lst[1], lst[3]) for lst in ridgeLst], key = lambda x: x[1])[0])\n#print('Lasso RMSE Min Alpha',sorted([(lst[1], lst[2]) for lst in lassoLst], key = lambda x: x[1])[0])\n#print('Lasso MAE Min Alpha', sorted([(lst[1], lst[3]) for lst in lassoLst], key = lambda x: x[1])[0])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a3037f73-c4b0-486f-82b7-08e18087477f","_uuid":"ee887f734718368ddc572b8d67f8e153f4db7491"},"cell_type":"markdown","source":"Ridge RMSE Min Alpha (5, 32902.72307681233)  \nRidge MAE Min Alpha (10, 19793.760076860788)  \nLasso RMSE Min Alpha (1, 33516.78681588221)  \nLasso MAE Min Alpha (1, 20296.151435088632)  "},{"metadata":{"_cell_guid":"0fcb67df-7e35-48e2-b2d4-6db1700039cd","collapsed":true,"_uuid":"077d79dc2df903bdc123af12fd6d28fa3a2b3e15","trusted":false},"cell_type":"code","source":"def lightGBM(df, dfY, sort, val, params, num_round = 1000, n_splits = 5):\n    #Paramrs\n    early_stopping_rounds = 100\n    params[sort] = val\n    \n    kf = KFold(n_splits = n_splits, random_state = 2)\n    kf.get_n_splits(df)\n    lstRound = []\n    lstRMSE = []\n    lstMAE = []\n    for ixx, (train_index, test_index) in enumerate(kf.split(df)):\n        #print(str(ixx) + 'th Fold Prediction')\n        X_train, X_cross = df.iloc[train_index,:], df.iloc[test_index,:]\n        y_train, y_cross = dfY.iloc[train_index], dfY.iloc[test_index]\n    \n        dtrain = lgb.Dataset(X_train, label = y_train, silent = True)\n        dvalid = lgb.Dataset(X_cross, label = y_cross, silent = True)\n        \n        mdl = lgb.train(params, train_set = dtrain, num_boost_round = num_round, valid_sets = dvalid,\n                early_stopping_rounds = early_stopping_rounds, verbose_eval = None)\n        y_pred = mdl.predict(X_cross, num_iteration = mdl.best_iteration)\n        rmse = sqrt(mean_squared_error(y_pred, y_cross))\n        mae = mean_absolute_error(y_pred, y_cross)\n        lstRound.append(mdl.best_iteration)\n        lstRMSE += rmse,\n        lstMAE += mae,\n        del X_train, X_cross, y_train, y_cross, dtrain, dvalid, mdl\n    tRound = max(lstRound)\n    tRMSE = sum(lstRMSE) / n_splits\n    tMAE = sum(lstMAE) / n_splits\n    ans = [sort, val, tRMSE, tMAE, tRound]\n    \n    return ans\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d32e4b89-8d0d-4fe1-aeea-be4327ee38f8","collapsed":true,"_uuid":"e5093668cbf6eb28baefc9c8ae895f008fe3ae43","trusted":false},"cell_type":"code","source":"def lightgbmCV(df, dfY, sort, valLst, params, num_round = 1000, n_splits = 5):\n    resLst = []\n    for val in valLst:\n        resLst += lightGBM(df, dfY, sort, val, nparams, num_round, n_splits),\n    resLst = sorted(resLst, key = lambda x: x[2])\n    print(resLst[0][0], '_', 'val: ', resLst[0][1], 'rmse: ', resLst[0][2], ' mae: ', resLst[0][3], ' ', resLst[0][4])\n    return resLst\n\ndef getBoostParams(df, dfY, varArray, params, num_round = 10000, n_splits = 5):\n    nparams = params.copy()\n    depthCV = lightgbmCV(df, dfY, 'max_depth', varArray[0], nparams, num_round, n_splits)\n    nparams['max_depth'] = depthCV[0][1]\n    leafCV = lightgbmCV(df, dfY, 'min_data_in_leaf', varArray[1], nparams, num_round, n_splits)\n    nparams['min_data_in_leaf'] = leafCV[0][1]\n    featuremaxCV = lightgbmCV(df, dfY, 'colsample_bytree', varArray[2], nparams, num_round, n_splits)\n    nparams['colsample_bytree'] = featuremaxCV[0][1]\n    baggingCV = lightgbmCV(df, dfY, 'bagging_fraction', varArray[3], nparams, num_round, n_splits)\n    nparams['bagging_fraction'] = baggingCV[0][1]\n    learningCV = lightgbmCV(df, dfY, 'learning_rate', varArray[4], nparams, num_round, n_splits)\n    nparams['learning_rate'] = learningCV[0][1]\n    return nparams","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fdf8c612-90e8-4e08-931c-05441e0b3312","collapsed":true,"_uuid":"75ad15a22b983c28805cc0f0911201eb07c6c79d","trusted":false},"cell_type":"code","source":"#df_trainY = df_train.loc[:,'SalePrice'].copy()\n#df_trainX = df_train2.copy()\n#params_l2 = {'application' : 'regression','metric' : 'l2', 'boosting' : 'gbdt', 'reg_alpha' : 1, \n#'learning_rate' : 0.5  ,'max_depth' : 7, 'min_data_in_leaf' : 20, 'colsample_bytree': 0.7, 'bagging_fraction' :0.7 }\n#params_l1 = {'application' : 'regression_l1','metric' : 'l1', 'boosting' : 'gbdt', 'reg_lambda' : 1, \n# 'learning_rate' : 0.5 ,'max_depth' : 7, 'min_data_in_leaf' : 20 , 'colsample_bytree': 0.7, 'bagging_fraction' :0.7 }\n#params_huber = {'application' : 'huber','metric' : 'huber', 'boosting' : 'gbdt', 'reg_alpha' : 1, \n#'learning_rate' : 0.5 ,'max_depth' : 7, 'min_data_in_leaf' : 20 , 'colsample_bytree': 0.7, 'bagging_fraction' : 0.7}\n\n#params_l2 = getBoostParams(df_trainX, df_trainY, varArray = [[3,5,7,9], [10, 20, 50, 100], [0.5, 0.7, 0.9], [0.5, 0.7, 0.9], [0.1, 0.3, 0.5]], params = params_l2)\n#params_l2 = getBoostParams(df_trainX, df_trainY, varArray = [[6,7,8], [9, 10, 15], [0.45, 0.5, 0.55], [0.45, 0.5, 0.55], [0.28, 0.3, 0.32]], params = params_l2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"15df7372-67e2-4474-b6f3-850ab5caa941","_uuid":"513111d23613fb10858aca6fcc606ab1d799006c"},"cell_type":"markdown","source":"**1st getBoostParams**  \nmax_depth _ val:  7 rmse:  29323.298722332423  mae:  17944.8637161   4517  \nmin_data_in_leaf _ val:  10 rmse:  27867.13170054141  mae:  17357.4795697   5209  \ncolsample_bytree _ val:  0.5 rmse:  27663.93373367426  mae:  17273.9522368   5460  \nbagging_fraction _ val:  0.5 rmse:  27663.93373367426  mae:  17273.9522368   5460  \nlearning_rate _ val:  0.3 rmse:  27609.021642548592  mae:  17162.3710014   5790  \n**2nd getBoostParams**  \nmax_depth _ val:  8 rmse:  27474.760390151474  mae:  17178.4440785   5808  \nmin_data_in_leaf _ val:  10 rmse:  27474.760390151474  mae:  17178.4440785   5808  \ncolsample_bytree _ val:  0.5 rmse:  27474.760390151474  mae:  17178.4440785   5808  \nbagging_fraction _ val:  0.45 rmse:  27474.760390151474  mae:  17178.4440785   5808  \nlearning_rate _ val:  0.3 rmse:  27474.760390151474  mae:  17178.4440785   5808  "},{"metadata":{"_cell_guid":"911365e4-b5d3-4439-a3dd-5b88605becee","_uuid":"e2bf1cac6f3b562333189a82401568e291ede18b"},"cell_type":"markdown","source":"Finish!! Thank you for reading this doc!! I wish that you feel nice!"},{"metadata":{"_cell_guid":"3edba4b0-d7eb-4f90-b078-371e19eaaf27","collapsed":true,"_uuid":"4fe466b8859f686e5df763258c5eb05f9503fc97","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}