{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-09T21:41:01.593768Z","iopub.execute_input":"2022-02-09T21:41:01.59405Z","iopub.status.idle":"2022-02-09T21:41:01.602889Z","shell.execute_reply.started":"2022-02-09T21:41:01.594022Z","shell.execute_reply":"2022-02-09T21:41:01.601962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call functions and load data\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:01.604434Z","iopub.execute_input":"2022-02-09T21:41:01.604734Z","iopub.status.idle":"2022-02-09T21:41:01.673498Z","shell.execute_reply.started":"2022-02-09T21:41:01.604704Z","shell.execute_reply":"2022-02-09T21:41:01.672743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we create dummies for these, either to small number of categories or some speciality e.g. \"MSSubClass\"\nnumeric_but_string=[\"MSSubClass\",\"YrSold\",\"BsmtFullBath\",\"BsmtHalfBath\",\"BedroomAbvGr\",\n                    \"FullBath\",\"HalfBath\",\"Fireplaces\",\"GarageCars\",\"KitchenAbvGr\"]\n\n\n# We check the skewness of numeric cols and we fill miss these as well\nnumerical_cols = [ col for col in train.columns[1:-2] if ((train.dtypes[col]==\"int64\"\n                                                           or  train.dtypes[col]==\"float64\") and col not in numeric_but_string)]\n\n\ntarget_col=\"SalePrice_log\"\ntrain[\"SalePrice_log\"]=np.log(train[\"SalePrice\"])\ntrain_tf=train[[\"Id\"]].copy()\ntest_tf=test[[\"Id\"]].copy()\nstd_map={}","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:01.674919Z","iopub.execute_input":"2022-02-09T21:41:01.675478Z","iopub.status.idle":"2022-02-09T21:41:01.697266Z","shell.execute_reply.started":"2022-02-09T21:41:01.675424Z","shell.execute_reply":"2022-02-09T21:41:01.696345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I create a new data set, so I can later rescale everything...\nnumerical_variables_revisited=list()\nfor col in train.columns:\n    if col not in [\"Id\"]:\n        # we copy the numerical or close to numerical variables\n        if (train.dtypes[col]==\"int64\" or  train.dtypes[col]==\"float64\") and col not in numeric_but_string:\n            train_tf[col]=train[col].copy()\n            if train.dtypes[col]==\"int64\":\n                print(col+\" \"+str(len(train[col].unique())))\n            if col not in [\"SalePrice_log\",\"SalePrice\"]:\n                test_tf[col]=test[col].copy()\n                numerical_variables_revisited.append(col)\n        else:\n            train_tf=pd.concat([train_tf, pd.get_dummies(train[col],prefix=col, prefix_sep='_',)], axis=1)\n            test_tf=pd.concat([test_tf, pd.get_dummies(test[col],prefix=col, prefix_sep='_',)], axis=1)\n\n    if col==\"PoolArea\":\n        # My Hungarian thinking suggests that we should only check whether this exists...\n        train_tf[\"Pool_Exists\"]=np.where(train[\"PoolArea\"]>0,1,0)\n        test_tf[\"Pool_Exists\"]=np.where(test[\"PoolArea\"]>0,1,0)\n        \n# I drop the id, as we try to keep the ordering.\ntrain_tf.drop(columns=[\"Id\"],inplace=True)\ntest_tf.drop(columns=[\"Id\"],inplace=True)\n\nfor col in train_tf.columns:\n    if col not in test_tf.columns:\n        test_tf[col]=0\n        \nfor col in test_tf.columns:\n    if col not in train_tf.columns:\n        train_tf[col]=0\n        \ntrain_target=train_tf[[target_col]].copy()\ntrain_tf=train_tf[test_tf.columns].copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:01.699493Z","iopub.execute_input":"2022-02-09T21:41:01.699735Z","iopub.status.idle":"2022-02-09T21:41:02.007949Z","shell.execute_reply.started":"2022-02-09T21:41:01.699707Z","shell.execute_reply":"2022-02-09T21:41:02.007086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we apply a scaler to scale everything to 0-1\nsaved_cols = train_tf.columns\nscaler = StandardScaler()\nscaler=scaler.fit(train_tf)\ntrain_scaled=pd.DataFrame(scaler.transform(train_tf))\ntrain_scaled.columns=saved_cols\ntest_scaled=pd.DataFrame(scaler.transform(test_tf))\ntest_scaled.columns=saved_cols\n\nsaved_cols2 = train_target.columns\ntarget_scaler= StandardScaler()\ntarget_scaled=pd.DataFrame(target_scaler.fit_transform(train_target))\ntarget_scaled.columns=saved_cols2","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:02.009339Z","iopub.execute_input":"2022-02-09T21:41:02.009553Z","iopub.status.idle":"2022-02-09T21:41:02.054758Z","shell.execute_reply.started":"2022-02-09T21:41:02.009527Z","shell.execute_reply":"2022-02-09T21:41:02.053815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now we apply a scaler to scale everything to 0-1\nsaved_cols = train_tf.columns\nscaler = StandardScaler()\nscaler=scaler.fit(train_tf)\ntrain_scaled=pd.DataFrame(scaler.transform(train_tf))\ntrain_scaled.columns=saved_cols\ntest_scaled=pd.DataFrame(scaler.transform(test_tf))\ntest_scaled.columns=saved_cols\n\ntarget_scaler= StandardScaler()\ntarget_scaled=pd.DataFrame(target_scaler.fit_transform(train_target))\ntarget_scaled.columns=[target_col]","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:02.056364Z","iopub.execute_input":"2022-02-09T21:41:02.0567Z","iopub.status.idle":"2022-02-09T21:41:02.095106Z","shell.execute_reply.started":"2022-02-09T21:41:02.056656Z","shell.execute_reply":"2022-02-09T21:41:02.094075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing value treatment and skew correction\nhouse_df_tf=train_scaled.append(test_scaled).copy()\nimputer = KNNImputer(n_neighbors=10, weights='distance')\nhouse_df_tf.loc[:,numerical_variables_revisited] = imputer.fit_transform(house_df_tf.loc[:,numerical_variables_revisited])\n\nfor col in numerical_variables_revisited:\n    if house_df_tf[col].skew()>3:\n        print(col+\" Init skew: \"+str(round(house_df_tf[col].skew(),2)))\n        house_df_tf[col]=np.log(house_df_tf[col]+1)\n        print(\"After skew: \"+str(round(house_df_tf[col].skew(),2)))\n\ntrain_scaled=house_df_tf.head(len(train_scaled)).copy()\ntest_scaled=house_df_tf.tail(len(test_scaled)).copy()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:02.096587Z","iopub.execute_input":"2022-02-09T21:41:02.096978Z","iopub.status.idle":"2022-02-09T21:41:02.431149Z","shell.execute_reply.started":"2022-02-09T21:41:02.096933Z","shell.execute_reply":"2022-02-09T21:41:02.430467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# before feature generation\ncorrelation_matrix=train_scaled[[target_col]+numerical_variables_revisited].corr()\nwith_targe_variable=correlation_matrix[[target_col]].reset_index()\nwith_targe_variable.sort_values(by=target_col, ascending=False, inplace=True)\n\nsns.set(rc = {'figure.figsize':(18,8)})\nsns.set_theme(style=\"whitegrid\")\ntips = sns.load_dataset(\"tips\")\nax = sns.barplot(x=\"index\", y=target_col, \n                 data=with_targe_variable[with_targe_variable[\"index\"]!=target_col])\nax.set_xticklabels(ax.get_xticklabels(),rotation = 90);\nplt.title(\"Correlation with the logarithm of the sales price\", fontsize=20);","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:02.432642Z","iopub.execute_input":"2022-02-09T21:41:02.433577Z","iopub.status.idle":"2022-02-09T21:41:03.008113Z","shell.execute_reply.started":"2022-02-09T21:41:02.433528Z","shell.execute_reply":"2022-02-09T21:41:03.007326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We try to generate new numerical columns: \ndef identity(x):\n    return x\nfunction_forms={\n\"sqrt\":np.sqrt,\n\"log\":np.log1p,\n\"id\":identity\n}\nnumerical_variables_revisited_update=numerical_variables_revisited.copy()\nfor v in numerical_variables_revisited:\n    for w in numerical_variables_revisited:\n        for key in function_forms.keys():\n            combined_v=abs(np.corrcoef(train_scaled[target_col], function_forms[key](train_scaled[v]*train_scaled[w]))[0,1])\n            corr_1=abs(np.corrcoef(train_scaled[target_col], train_scaled[w])[0,1])\n            corr_2=abs(np.corrcoef(train_scaled[target_col], train_scaled[v])[0,1])\n            if combined_v>max(corr_1,corr_2):\n                print(\"New variable generate: \")\n                print(\"Var1: \"+w+ \" Var2: \"+v+\" function: \"+key)\n                print(\"With correlation: \"+str(round(combined_v,3)))\n                print(\"Var 1 correlation: \"+str(round(corr_1,3)))\n                print(\"Var 1 correlation: \"+str(round(corr_2,3)))\n                train_scaled[w+\"_\"+v+\"_\"+key]=function_forms[key](train_scaled[v]*train_scaled[w])\n                test_scaled[w+\"_\"+v+\"_\"+key]=function_forms[key](test_scaled[v]*test_scaled[w])\n                train_scaled[w+\"_\"+v+\"_\"+key].fillna(0,inplace=True)\n                test_scaled[w+\"_\"+v+\"_\"+key].fillna(0,inplace=True)\n                numerical_variables_revisited_update.append(w+\"_\"+v+\"_\"+key)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:03.010764Z","iopub.execute_input":"2022-02-09T21:41:03.011025Z","iopub.status.idle":"2022-02-09T21:41:04.720888Z","shell.execute_reply.started":"2022-02-09T21:41:03.010976Z","shell.execute_reply":"2022-02-09T21:41:04.720189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the variables with highest correlation, lets plot them!\nselected_vars=list(\n    with_targe_variable[with_targe_variable[\"index\"]!=target_col].head(5)[\"index\"])\n\nfor var in selected_vars:\n    sns.scatterplot(data=train_scaled, x=target_col, y=var)\n    plt.title(target_col+\" and variable: \"+var, fontsize=20)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:04.721833Z","iopub.execute_input":"2022-02-09T21:41:04.722059Z","iopub.status.idle":"2022-02-09T21:41:06.286579Z","shell.execute_reply.started":"2022-02-09T21:41:04.722029Z","shell.execute_reply":"2022-02-09T21:41:06.285421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test , train data\nregressor_cols=list(set(train_scaled.columns)-set([\"SalePrice\",\"SalePrice_log\"]))\n\nX_train, y_train = train_scaled[regressor_cols], target_scaled[[target_col]]\nX_test = test_scaled[regressor_cols]","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:06.287814Z","iopub.execute_input":"2022-02-09T21:41:06.288044Z","iopub.status.idle":"2022-02-09T21:41:06.305726Z","shell.execute_reply.started":"2022-02-09T21:41:06.288017Z","shell.execute_reply":"2022-02-09T21:41:06.304647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we search using the test version....\nmodel = ElasticNet()\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['alpha'] = [0.0001, 0.0002,0.0004, 0.0006, 0.0008, 0.001, 0.003, 0.006, 0.009,0.012]\ngrid['l1_ratio'] = np.arange(0.79, 1.00, 0.01) # closer to lasso\ngrid['max_iter']=[50000]\n\n# define search\nsearch = GridSearchCV(model, grid, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=10, verbose=1)\nresults = search.fit(X_train, y_train,sample_weight=target_scaler.inverse_transform(y_train[\"SalePrice_log\"]))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:06.306843Z","iopub.execute_input":"2022-02-09T21:41:06.307168Z","iopub.status.idle":"2022-02-09T21:41:06.311521Z","shell.execute_reply.started":"2022-02-09T21:41:06.30714Z","shell.execute_reply":"2022-02-09T21:41:06.310437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('MRSE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:06.343432Z","iopub.status.idle":"2022-02-09T21:41:06.343802Z","shell.execute_reply.started":"2022-02-09T21:41:06.34362Z","shell.execute_reply":"2022-02-09T21:41:06.34364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ElasticNet(alpha=results.best_params_['alpha'],  l1_ratio=results.best_params_['l1_ratio'], max_iter=50000 )\nmodel_rec = ElasticNet(alpha=results.best_params_['alpha'],  l1_ratio=results.best_params_['l1_ratio'] )\n# model = ElasticNet(alpha=0.012,  l1_ratio=0.79, max_iter=50000 )\n# model_rec = ElasticNet(alpha=0.012,  l1_ratio=0.79 )","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:16.204539Z","iopub.execute_input":"2022-02-09T21:41:16.204985Z","iopub.status.idle":"2022-02-09T21:41:16.209744Z","shell.execute_reply.started":"2022-02-09T21:41:16.20494Z","shell.execute_reply":"2022-02-09T21:41:16.208963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we check how this settings look like in other samples and in a somewhat out of sample setting\nn=100\navg_insample_score=0\navg_out_of_sample_score=0\navg_insample_r2=list()\navg_outofsample_r2=list()\nexperiment_data=pd.concat([train_scaled[regressor_cols],target_scaled],axis=1).copy()\nfor i in range(0,n):\n    t1, t2 = train_test_split(experiment_data, test_size=0.33)\n    X1, y1 = t1[regressor_cols], t1[[target_col]]\n    X_t2, y_t2 = t2[regressor_cols], t2[[target_col]]\n    model_rec.fit(X1, y1,sample_weight=target_scaler.inverse_transform(y1[\"SalePrice_log\"]))\n    avg_insample_score+=(model_rec.score(X1,y1))/n\n    avg_out_of_sample_score+=(model_rec.score(X_t2,y_t2))/n\n    \n    y_in_pred=np.matrix(target_scaler.inverse_transform(model_rec.predict(X1))).T\n    y_in_real=target_scaler.inverse_transform(y1)\n    y_ou_pred=np.matrix(target_scaler.inverse_transform(model_rec.predict(X_t2))).T\n    y_ou_real=target_scaler.inverse_transform(y_t2)\n    \n    avg_insample_r2.append(1-np.nansum(np.power(y_in_real-y_in_pred,2))/np.nansum((y_in_real-np.nanmean(y_in_real))**2))\n    avg_outofsample_r2.append(1-np.nansum(np.power(y_ou_pred-y_ou_real,2))/np.nansum((y_ou_real-np.nanmean(y_ou_real))**2))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:18.077112Z","iopub.execute_input":"2022-02-09T21:41:18.077423Z","iopub.status.idle":"2022-02-09T21:41:34.145093Z","shell.execute_reply.started":"2022-02-09T21:41:18.07739Z","shell.execute_reply":"2022-02-09T21:41:34.144112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Average in sample score: \"+str(avg_insample_score))\nprint(\"Average out of sample score: \"+str(avg_out_of_sample_score))\nprint(\"Average in sample r2 score: \"+str(np.nanmean(avg_insample_r2)))\nprint(\"Average out of sample r2 score: \"+str(np.nanmean(avg_outofsample_r2)))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:34.151502Z","iopub.execute_input":"2022-02-09T21:41:34.152077Z","iopub.status.idle":"2022-02-09T21:41:34.163497Z","shell.execute_reply.started":"2022-02-09T21:41:34.152023Z","shell.execute_reply":"2022-02-09T21:41:34.162506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(avg_outofsample_r2, color=\"purple\");\nplt.title(\"Out of sample R2 distribution\", fontsize=20);","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:34.164916Z","iopub.execute_input":"2022-02-09T21:41:34.16634Z","iopub.status.idle":"2022-02-09T21:41:34.486123Z","shell.execute_reply.started":"2022-02-09T21:41:34.16629Z","shell.execute_reply":"2022-02-09T21:41:34.485237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(avg_insample_r2, color=\"purple\");\nplt.title(\"In sample R2 distribution\", fontsize=20);","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:34.487968Z","iopub.execute_input":"2022-02-09T21:41:34.488216Z","iopub.status.idle":"2022-02-09T21:41:34.728619Z","shell.execute_reply.started":"2022-02-09T21:41:34.488185Z","shell.execute_reply":"2022-02-09T21:41:34.727794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model=model.fit( \n    X_train,\n    y_train,sample_weight=target_scaler.inverse_transform(y_train[target_col]))\n\nfeature_importance = pd.Series(index = \n                               X_train.columns, \n                               data = np.abs(model.coef_))\n\nn_selected_features = (feature_importance>0).sum()\nprint('{0:d} features, reduction of {1:2.2f}%'.format(\n    n_selected_features,(1-n_selected_features/len(feature_importance))*100))\n\nfeature_importance.sort_values().tail(30).plot(kind = 'bar', figsize = (18,6), color=\"purple\");\nplt.title(\"Feature importance\", fontsize=20);","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:34.730059Z","iopub.execute_input":"2022-02-09T21:41:34.730436Z","iopub.status.idle":"2022-02-09T21:41:35.441556Z","shell.execute_reply.started":"2022-02-09T21:41:34.73039Z","shell.execute_reply":"2022-02-09T21:41:35.440508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We generate the predictions.\nif target_col==\"SalePrice\":\n    yhat = target_scaler.inverse_transform(model.predict(X_test))\n    guess=pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": yhat\n       })\nelse:\n    yhat = target_scaler.inverse_transform(model.predict(X_test))\n    guess=pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": np.exp(yhat)\n       })        \nguess.to_csv(\"submission_noscale.csv\", index=False)\n\n# now we scale our estimates a little bit, we apply Jensen inequality\nyhat_train = np.exp(target_scaler.inverse_transform(model.predict(X_train)))\ny_orig=np.exp(target_scaler.inverse_transform(target_scaled[[target_col]]))\n\n\nrescale=pd.concat([pd.DataFrame(y_orig),pd.DataFrame(yhat_train)],axis=1)\nrescale.columns=[\"SalePrice\",\"SalePrice_predict\"]\nrescale[\"diff\"]=rescale[\"SalePrice\"]/rescale[\"SalePrice_predict\"]\n\nguess2=guess.copy()\nguess2[\"SalePrice\"]=guess2[\"SalePrice\"]*np.mean(rescale[\"diff\"])\nguess2.to_csv(\"submission.csv\", index=False)\nprint(\"No scaling: \")\ndisplay(guess2.head())\nprint(\" With Scaling: \")\ndisplay(guess.head())\nprint(\"Scaling scalar\")\nprint(\"Value: \"+str(round(np.mean(rescale[\"diff\"]),3)))","metadata":{"execution":{"iopub.status.busy":"2022-02-09T21:41:37.149263Z","iopub.execute_input":"2022-02-09T21:41:37.149554Z","iopub.status.idle":"2022-02-09T21:41:37.233498Z","shell.execute_reply.started":"2022-02-09T21:41:37.149524Z","shell.execute_reply":"2022-02-09T21:41:37.23267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}