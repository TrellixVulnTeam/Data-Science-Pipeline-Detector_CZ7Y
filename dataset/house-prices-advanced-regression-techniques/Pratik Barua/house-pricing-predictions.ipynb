{"cells":[{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3.7\" color=\"green\"><b><center>House Pricing Predictions Guide</center></b></font><br>\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n# Introduction\nHi,\nI created a notebook as a guide for this Advanced House Pricing Competition. This competition is relatively similar to its predecessor except for a different output and slightly different procedure. I have a notebook for the regular House Pricing Competition if you want to check it out. This notebook will continuously be updated as I still have a lot to learn from this competition and in general. \n\nThis kernel was very helpful to me:\nhttps://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n\nIf you have any questions or comments please leave them in the comment section! Also if you found this helpful please leave an UPVOTE, thank you!\n\n<font size=\"+3\" color=\"green\"><b>Useful Pandas CheatSheet</b></font><br>\n### -> https://www.kaggle.com/getting-started/146910\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of Contents:</h1>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">1. Importing Libraries<span class=\"badge badge-primary badge-pill\">1</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\">2. Reading and Inspecting Data<span class=\"badge badge-primary badge-pill\">2</span></a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"settings\">3. SalePrice: Skew and Kurtosis Analysis<span class=\"badge badge-primary badge-pill\">3</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"settings\">4. Outliers<span class=\"badge badge-primary badge-pill\">4</span></a> \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"settings\">5. Missing Values<span class=\"badge badge-primary badge-pill\">5</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"settings\">6. Feature Engineering<span class=\"badge badge-primary badge-pill\">6</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#7\" role=\"tab\" aria-controls=\"settings\">7. High Skew Features<span class=\"badge badge-primary badge-pill\">7</span></a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#8\" role=\"tab\" aria-controls=\"settings\">8. Modelling<span class=\"badge badge-primary badge-pill\">8</span></a>  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#9\" role=\"tab\" aria-controls=\"settings\">9. Submission<span class=\"badge badge-primary badge-pill\">9</span></a>  \n</div>\n\n# Version Logs:\n* Version 1: RELEASED\n\n* Version 2:\n    * ['TotalSF'] feature now includes '1stFlrSF' in Feature Engineering section\n    * Dataframe with Percentage of Missing values is multiplied by 100\n    * Removed accidently created duplicate of 'HasBsmt'\n* Version 3 **[CURRENT]**:\n    * Dropped pre-used outliers from other notebooks\n    * Added more visualization for outliers","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a> <br>\n<font size=\"+3\" color=\"green\"><b>1 - Importing Libraries</b></font><br><a id=\"1\"></a>\n<br>\nImporting Necessary Libraries ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn import metrics \nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"green\"><b>2 - Reading and Inspecting Data</b></font><br><a id=\"2\"></a>\n### Section Contents: \n\n- 2.1.  Reading Data\n\n- 2.2.  Inspecting Data\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Reading Data\nReading in train and test data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"home = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 Inspecting Data\nChecking our Data, Saving then dropping Id column, Checking Dataset's Dimensions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"home.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save Id for submission\ntest_Id = test['Id']\n\nhome.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Home Data Dimensions: {}\".format(home.shape))\nprint(\"Test Data Dimensions: {}\".format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"green\"><b>3 - SalePrice: Skew and Kurtosis Analysis</b></font><br><a id=\"3\"></a>\n### Section Contents:\n- 3.1 Original SalePrice Visualization\n- 3.2 Log-Transforming SalePrice\n- 3.3 Log-Transformed SalePrice Visualization\n\n### Additional Information about Statistics:\n### - https://www.kaggle.com/getting-started/149618","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Skew Formula:\n![](https://i0.wp.com/datalabbd.com/wp-content/uploads/2019/05/3b.png?w=520&ssl=1)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Kurtosis Formula:\n![](https://rula-tech.com/uploads/images/Kurtosis/kurtosis_formula.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.1 Original SalePrice Visualization\nPrint out skew and Kurtosis of SalePrice, Visualize SalePrice with distplot and probplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness: %f\" % home['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % home['SalePrice'].kurt())\n\nfigure = plt.figure(figsize=(18,10))\nplt.subplot(1,2,1)\nsns.distplot(home['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(home['SalePrice'])\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\n\nplt.subplot(1,2,2)\nstats.probplot(home['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://miro.medium.com/max/1110/1*mshlbZAnP4uZubMNSuWWOA.jpeg)\n\n### Reasons a right-skewed predictive variable is bad:\n- Mean greater than mode\n- Median greater than mode\n- Mean is greater than median\n\n### This eventually can affect the performance of our modelling process, so we will decide to log-transform it in the next step.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 3.2 Log-Transforming SalePrice\nLog-Transforming SalePrice with np.log1p","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"![](https://i.stack.imgur.com/giq8G.png)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Random Example:\nThis shows the distribution of a dataset before and after log-transformation\n\n![](https://miro.medium.com/max/810/1*4fagRPrzuHquTAKe35OhdA.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"home['SalePrice'] = np.log1p(home['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 Log-Transformed SalePrice Visualization\nPrint out skew and Kurtosis after log-transformation, Visualize SalePrice after Log-transformation\n\n![](https://cdn-images-1.medium.com/max/800/1*hxVvqttoCSkUT2_R1zA0Tg.gif)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print out Skewness and Kurtosis\nprint(\"Skewness: %f\" % home['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % home['SalePrice'].kurt())\n\nfigure = plt.figure(figsize=(18,10))\nplt.subplot(1,2,1)\nsns.distplot(home['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(home['SalePrice'])\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\n\nplt.subplot(1,2,2)\nstats.probplot(home['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"green\"><b>4 - Outliers</b></font><br><a id=\"4\"></a>\n\n### Section Contents:\n- 4.1 Univariate Analysis\n- 4.2 Bivariate Analysis\n- 4.3 Removing Outliers\n\n### Additional Information about Outliers: \n### - https://www.kaggle.com/getting-started/147428\n\n### - https://www.kaggle.com/getting-started/150064","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 4.1 Univariate Analysis\nIn the name 'Univariate Analysis' 'Uni' means one, and 'variate' means variable, meaning that Univariate analysis is analysis of one feature. This procedure basically tells us the distribution of each feature and information about its mean, median, and mode. This is our first step to detecting outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_features = home.select_dtypes(exclude='object').drop(['SalePrice'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(16,20))\n\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(9, 4, i+1)\n    sns.boxplot(y=numerical_features.iloc[:,i])\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2 Bivariate Analysis\nBivariate Analysis is a procedure but with two variables (hence the name 'Bi'), not one. This is our second step to detecting outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,18))\nfor i in range(len(numerical_features.columns)):\n    fig.add_subplot(9, 4, i+1)\n    sns.scatterplot(numerical_features.iloc[:, i],home['SalePrice'])\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Credit to https://www.kaggle.com/amiiiney/price-prediction-regularization-stacking\n\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=home['GrLivArea'], y=home['SalePrice'], color=('yellowgreen'), alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Ground living Area- Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x=home['TotalBsmtSF'], y=home['SalePrice'], color=('red'),alpha=0.5)\nplt.axvline(x=5900, color='r', linestyle='-')\nplt.title('Basement Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,0))\nplt.scatter(x=home['1stFlrSF'], y=home['SalePrice'], color=('deepskyblue'),alpha=0.5)\nplt.axvline(x=4000, color='r', linestyle='-')\nplt.title('First floor Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,1))\nplt.scatter(x=home['MasVnrArea'], y=home['SalePrice'], color=('gold'),alpha=0.9)\nplt.axvline(x=1500, color='r', linestyle='-')\nplt.title('Masonry veneer Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,0))\nplt.scatter(x=home['GarageArea'], y=home['SalePrice'], color=('orchid'),alpha=0.5)\nplt.axvline(x=1230, color='r', linestyle='-')\nplt.title('Garage Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,1))\nplt.scatter(x=home['TotRmsAbvGrd'], y=home['SalePrice'], color=('tan'),alpha=0.9)\nplt.axvline(x=13, color='r', linestyle='-')\nplt.title('TotRmsAbvGrd - Price scatter plot', fontsize=15, weight='bold' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3 Removing Outliers\nAfter looking at the Visualizations of each numerical feature, we use the 1.5 IQR Rule in order to detect and remove outliers ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Using the 1.5 IQR Rule: \n![](https://cdn.kastatic.org/googleusercontent/8bSRVB7q_zWxFliXcZVQSBDtip3sMGRkkHGLVzvflS3goQZZhmhrSD9u1cSduXh-9DJ9sSjCqVyozwQ_FwJNkptC)\n\n### It states that a data point is an outlier if:\n- It is below the First Quadrant (Q1) subtracted by (1.5 x IQR)\n- It is above the Third Quadrant (Q3) added by (1.5 x IQR)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gathered these positions and outliers from previous notebooks\n\npos = [1298,523, 297]\nhome.drop(home.index[pos], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"home.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that our dataset shrunk because some of the rows with outliers were removed","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"home.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"green\"><b>5 - Missing Values</b></font><br><a id=\"5\"></a>\n\n### Section Contents:\n- 5.1 Checking for Missing Values\n- 5.2 Imputing Certain features with 'None'\n- 5.3 Imputing Certain features with their Mode\n- 5.4 Imputing Garage-related features\n- 5.5 Imputing Basement-related features\n- 5.6 Imputing MasVnr-Related Features\n- 5.7 Imputing LotFrontage\n- 5.8 Checking for any Extra Missing Values","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 5.1 Checking for Missing Values\nCombining train and test data in order to make imputing missing values easier, locating missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save target value for later\ny = home.SalePrice.values\n\n# In order to make imputing easier, we combine train and test data\nhome.drop(['SalePrice'], axis=1, inplace=True)\ndataset = pd.concat((home, test)).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Creating a Dataframe listing every feature with missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"na_percent = (dataset.isnull().sum()/len(dataset))[(dataset.isnull().sum()/len(dataset))>0].sort_values(ascending=False)\n\nmissing_data = pd.DataFrame({'Missing Percentage':na_percent*100})\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Creating a Visualization of every feature with missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"na = (dataset.isnull().sum() / len(dataset)) * 100\nna = na.drop(na[na == 0].index).sort_values(ascending=False)\n\nf, ax = plt.subplots(figsize=(15,12))\nsns.barplot(x=na.index, y=na)\nplt.xticks(rotation='90')\nplt.xlabel('Features', fontsize=15)\nplt.title('Percentage Missing', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Checking each feature's type to know which value to impute later","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[na.index].dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.2 Imputing Certain features with 'None'\nImputing some features with 'None'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- Any of the below features with a missing value likely indicates that it doesn't exist\n\nEx: FireplaceQu doesn't exist, therefore it probably means that there was no Fireplace for that house","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('FireplaceQu', 'Fence', 'Alley', 'MiscFeature', 'PoolQC', 'MSSubClass'):\n    dataset[col] = dataset[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.3 Imputing Certain features with their Mode\nImputing some features with their mode","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- The reason we fill in these features with their mode (most common value) is because these features are mandatory in a house, meaning that if these values are missing it has to be because of the data, not because of the fact that the house is missing the feature.\n\nEx: For the 'Electrical' feature we fill in the most common value because every house has electricity, therefore it wouldn't make sense for there to be any missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'Functional', 'MSZoning', 'SaleType', 'Utilities'):\n    dataset[col] = dataset[col].fillna(dataset[col].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.4 Imputing Garage-related features\nImputing Garage-Related Features which are numerical with 0 and 'None' for categorical","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- A Garage-Related Feature with a missing value usually indicates there is no Garage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    dataset[col] = dataset[col].fillna(0)\n    \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    dataset[col] = dataset[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.5 Imputing Basement-related features\nImputing Basement-Related Features which are numerical with 0 and 'None' for categorical","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- A Basement-Related Feature with a missing value usually indicates there is no Basement","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    dataset[col] = dataset[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    dataset[col] = dataset[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()[dataset.isnull().sum() > 0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.6 Imputing MasVnr-Related Features\nImputing 'MasVnrType' and 'MasVnrArea'","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"- There are two features related to MasVnr that have missing values: 'MasVnrType' and 'MasVnrArea'. \n- 'MasVnrType' is a categorical feature that we need to impute with 'None'\n- 'MasVnrArea' is a categorical feature that we need to impute with 'MasVnrType'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['MasVnrType'] = dataset['MasVnrType'].fillna('None')\ndataset['MasVnrArea'] = dataset['MasVnrArea'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.7 Imputing LotFrontage\nImputing LotFrontage with median based on 'Neighborhood'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# LotFrontage is correlated to the 'Neighborhood' feature because the LotFrontage for nearby houses will be really similar, so we fill in missing values by the median based off of Neighborhood\ndataset[\"LotFrontage\"] = dataset.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5.8 Checking for any Extra Missing Values\nChecking for any more missing values (Ignore SalePrice because that's our target variable)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"na_percent = (dataset.isnull().sum()/len(dataset))[(dataset.isnull().sum()/len(dataset))>0].sort_values(ascending=False)\n\nmissing_data = pd.DataFrame({'Missing Percentage':na_percent})\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"green\"><b>6 - Feature Engineering</b></font><br><a id=\"6\"></a>\n\n### Section Contents:\n- 6.1 Changing Types\n- 6.2 Create 'TotalSF' Feature\n- 6.3 Create 'TotalBath' Feature\n- 6.4 Create 'YrBuiltAndRemod' Feature\n- 6.5 Create 'PorchSF' Feature\n- 6.6 Creating Extra Features\n- 6.7 Label Encoding","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"list(dataset.select_dtypes(exclude='object').columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1 Changing Types\nChanging the types of some of the features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features 'MSSubClass', 'YrSold', 'OverallCond' and 'MoSold' are supposed to be categorical features so change their type\ndataset['MSSubClass'] = dataset['MSSubClass'].apply(str)\ndataset['YrSold'] = dataset['YrSold'].apply(str)\ndataset['MoSold'] = dataset['MoSold'].apply(str)\ndataset['OverallCond'] = dataset['OverallCond'].astype(str)\n\n# Features 'LotArea' and 'MasVnrArea' are supposed to be numerical features so change their type\ndataset['LotArea'] = dataset['LotArea'].astype(np.int64)\ndataset['MasVnrArea'] = dataset['MasVnrArea'].astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2 Create 'TotalSF' Feature\nVisualize 'TotalBsmtSF' and '2ndFlrSF' and them together, Create 'TotalSF' for total surface area with those features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"figure, (ax1, ax2, ax3,ax4) = plt.subplots(nrows=1, ncols=4)\nfigure.set_size_inches(20,10)\n_ = sns.regplot(home['TotalBsmtSF'], y, ax=ax1)\n_ = sns.regplot(home['1stFlrSF'], y, ax=ax2)\n_ = sns.regplot(home['2ndFlrSF'], y, ax=ax3)\n_ = sns.regplot(home['TotalBsmtSF'] + home['2ndFlrSF']+home['1stFlrSF'], y, ax=ax4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['TotalSF']=dataset['TotalBsmtSF']  + dataset['1stFlrSF'] + dataset['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.3 Create 'TotalBath' Feature\nVisualize 'BsmtFullBath', 'FullBath', 'BsmtHalfBath' and them together, Create 'TotalBath' for total number of bathrooms using those features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"figure, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\nfigure.set_size_inches(14,10)\n_ = sns.barplot(home['BsmtFullBath'], y, ax=ax1)\n_ = sns.barplot(home['FullBath'], y, ax=ax2)\n_ = sns.barplot(home['BsmtHalfBath'], y, ax=ax3)\n_ = sns.barplot(home['BsmtFullBath'] + home['FullBath'] + home['BsmtHalfBath'] + home['HalfBath'], y, ax=ax4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['TotalBath']=dataset['BsmtFullBath'] + dataset['FullBath'] + (0.5*dataset['BsmtHalfBath']) + (0.5*dataset['HalfBath'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.4 Create 'YrBuiltAndRemod' Feature\nVisualize 'YearBuilt', 'YearRemodAdd', and them together, Create 'YrBltAndRemod' for Year Built and Year remodel combined using those features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"figure, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3)\nfigure.set_size_inches(18,8)\n_ = sns.regplot(home['YearBuilt'], y, ax=ax1)\n_ = sns.regplot(home['YearRemodAdd'],y, ax=ax2)\n_ = sns.regplot((home['YearBuilt']+home['YearRemodAdd'])/2, y, ax=ax3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['YrBltAndRemod']=dataset['YearBuilt']+dataset['YearRemodAdd']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.5 Create 'PorchSF' Feature\nVisualize 'OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch', 'WoodDeckSF' and them together, Create 'Porch_SF' for total porch surface using those features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"figure, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3)\nfigure.set_size_inches(20,10)\n_ = sns.regplot(home['OpenPorchSF'], y, ax=ax1)\n_ = sns.regplot(home['3SsnPorch'], y, ax=ax2)\n_ = sns.regplot(home['EnclosedPorch'], y, ax=ax3)\n_ = sns.regplot(home['ScreenPorch'], y, ax=ax4)\n_ = sns.regplot(home['WoodDeckSF'], y, ax=ax5)\n_ = sns.regplot((home['OpenPorchSF']+home['3SsnPorch']+home['EnclosedPorch']+home['ScreenPorch']+home['WoodDeckSF']), y, ax=ax6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Porch_SF'] = (dataset['OpenPorchSF'] + dataset['3SsnPorch'] + dataset['EnclosedPorch'] + dataset['ScreenPorch'] + dataset['WoodDeckSF'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.6 Creating Extra Features\nCreating useful extra features in order to strongly distinct data\n\nEx: 'Has2ndfloor' feature below indicates whether there is a 2ndfloor or not","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['Has2ndfloor'] = dataset['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasBsmt'] = dataset['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasFirePlace'] =dataset['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ndataset['Has2ndFlr']=dataset['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndataset['HasPool']=dataset['PoolArea'].apply(lambda x: 1 if x > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.7 Label Encoding\nOur dataset cannot run with categorical columns so we must Label Encode these columns in order to make them numerical","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncategorical_col = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor col in categorical_col:\n    label = LabelEncoder() \n    label.fit(list(dataset[col].values)) \n    dataset[col] = label.transform(list(dataset[col].values))\n\nprint('Shape all_data: {}'.format(dataset.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"green\"><b>7 - High Skew Features</b></font><br><a id=\"7\"></a>\n\n\n### Section Contents:\n- 7.1 Checking Skew\n- 7.2 Skew Visualization\n- 7.3 Box-Cox Transformation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 7.1 Checking Skew\nCreate a new variable containing the dataset of only numerical features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"num_features = dataset.dtypes[dataset.dtypes != 'object'].index\nskewed_features = dataset[num_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_features})\nskewness.head(15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.2 Skew Visualization\nVisualize each numerical feature with distplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_dataset = dataset.select_dtypes(exclude='object')\n\nfor i in range(len(numerical_dataset.columns)):\n    f, ax = plt.subplots(figsize=(7, 4))\n    fig = sns.distplot(numerical_dataset.iloc[:,i].dropna(), rug=True, hist=False, label='UW', kde_kws={'bw':0.1})\n    plt.xlabel(numerical_dataset.columns[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.3 Box-Cox Transformation\nPerform Box-Cox Transformation on Skewed Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"skewness = skewed_features[abs(skewed_features) > 0.75]\nskewed_features = skewness.index\n\nlam = 0.15\nfor i in skewed_features:\n    dataset[i] = boxcox1p(dataset[i], lam)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"green\"><b>8 - Modelling</b></font><br><a id=\"8\"></a>\n### Section Contents:\n- 8.1 Importing Libraries for Modelling\n- 8.2 Preparing Data for Modelling\n- 8.3 Models\n- 8.4 Viewing Model Performance\n- 8.5 Stacking","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 8.1 Importing Libraries for Modelling\nFor modelling we must import these libraries","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error , make_scorer\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.2 Preparing Data for Modelling\nPreparing data so we can use it for modelling, Splitting data using train_test_split","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hot-Encode Categorical features\ndataset = pd.get_dummies(dataset) \n\n# Splitting dataset back into X and test data\nX = dataset[:len(home)]\ntest = dataset[len(home):]\n\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Splitting data into train and validation data using variables X and y","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.3 Models\nSetting up Models, Printing out the RMSE and STD (Standard Deviation) of each model, Training each model.\n\n### Models Used:\n- LassoCV\n- Ridge\n- ElasticNet\n- LightGBM\n- XGBOOST\n- CATBOOST\n- STACKED (Combined)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Standard Deviation:\n![](https://cdn.kastatic.org/googleusercontent/N8xzWFc6eo0XBHEXZjz1SwvLSnPezvhTRF1P17kdjUG_tnJivGKkyCxbwVe4MZ0-USOxIZBohgcjdi8e7Z4Hswcqfw)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Parameters\n'kfolds' is used for cross validation, the list type variables are for the models","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Indicate number of folds for cross validation\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Parameters for models\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lasso","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lasso Model\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas = alphas2, random_state = 42, cv=kfolds))\n\n# Printing Lasso Score with Cross-Validation\nlasso_score = cross_val_score(lasso, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nlasso_rmse = np.sqrt(-lasso_score.mean())\nprint(\"LASSO RMSE: \", lasso_rmse)\nprint(\"LASSO STD: \", lasso_score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Model for later\nlasso.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ridge","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas = alphas_alt, cv=kfolds))\nridge_score = cross_val_score(ridge, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nridge_rmse =  np.sqrt(-ridge_score.mean())\n# Printing out Ridge Score and STD\nprint(\"RIDGE RMSE: \", ridge_rmse)\nprint(\"RIDGE STD: \", ridge_score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Model for later\nridge.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ElasticNet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\nelastic_score = cross_val_score(elasticnet, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nelastic_rmse =  np.sqrt(-elastic_score.mean())\n\n# Printing out ElasticNet Score and STD\nprint(\"ELASTICNET RMSE: \", elastic_rmse)\nprint(\"ELASTICNET STD: \", elastic_score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Model for later\nelasticnet.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lightgbm = make_pipeline(RobustScaler(),\n                        LGBMRegressor(objective='regression',num_leaves=5,\n                                      learning_rate=0.05, n_estimators=720,\n                                      max_bin = 55, bagging_fraction = 0.8,\n                                      bagging_freq = 5, feature_fraction = 0.2319,\n                                      feature_fraction_seed=9, bagging_seed=9,\n                                      min_data_in_leaf =6, \n                                      min_sum_hessian_in_leaf = 11))\n\n# Printing out LightGBM Score and STD\nlightgbm_score = cross_val_score(lightgbm, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nlightgbm_rmse = np.sqrt(-lightgbm_score.mean())\nprint(\"LIGHTGBM RMSE: \", lightgbm_rmse)\nprint(\"LIGHTGBM STD: \", lightgbm_score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Model for later\nlightgbm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBOOST","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost = make_pipeline(RobustScaler(),\n                        XGBRegressor(learning_rate =0.01, n_estimators=3460, \n                                     max_depth=3,min_child_weight=0 ,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,nthread=4,\n                                     scale_pos_weight=1,seed=27, \n                                     reg_alpha=0.00006))\n\n# Printing out XGBOOST Score and STD\nxgboost_score = cross_val_score(xgboost, X, y, cv=kfolds, scoring='neg_mean_squared_error')\nxgboost_rmse = np.sqrt(-xgboost_score.mean())\nprint(\"XGBOOST RMSE: \", xgboost_rmse)\nprint(\"XGBOOST STD: \", xgboost_score.std())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training Model for later\nxgboost.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.4 Viewing Model Performance\nView Model Performance through a DataFrame and a barplot","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame({\n    'Model':['Lasso',\n            'Ridge',\n            'ElasticNet',\n            'LightGBM',\n            'XGBOOST',\n            ],\n    'Score':[lasso_rmse,\n             ridge_rmse,\n             elastic_rmse,\n             lightgbm_rmse,\n             xgboost_rmse,\n             \n            ]})\n\nsorted_result = results.sort_values(by='Score', ascending=True).reset_index(drop=True)\nsorted_result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(14,8))\nplt.xticks(rotation='90')\nsns.barplot(x=sorted_result['Model'], y=sorted_result['Score'])\nplt.xlabel('Model', fontsize=15)\nplt.ylabel('Performance', fontsize=15)\nplt.ylim(0.10, 0.12)\nplt.title('RMSE', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.5 Stacking\nPredict every model, then combine every prediction into a final predictions used for submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict every model\nlasso_pred = lasso.predict(test)\nridge_pred = ridge.predict(test)\nelasticnet_pred = elasticnet.predict(test)\nlightgbm_pred = lightgbm.predict(test)\nxgboost_pred = xgboost.predict(test)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Stacking:** At this point we basically trained and predicted each model so we can combine its predictions into a 'final_predictions' variable for submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combine predictions into final predictions\nfinal_predictions = np.expm1((0.3*elasticnet_pred) + (0.3*lasso_pred) + (0.2*ridge_pred) + \n               (0.1*xgboost_pred) + (0.1*lightgbm_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"+3\" color=\"green\"><b>9 - Submission</b></font><br><a id=\"9\"></a>\n<br>\nSubmitting to Competition using test_Id which stores the test data's Id and the final predictions ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Id'] = test_Id\nsubmission['SalePrice'] = final_predictions\nsubmission.to_csv('house_pricing_submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## To show your support PLEASE UPVOTE 👍, Thank You! Also if you have any Suggestions/Questions please comment them down below!","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}