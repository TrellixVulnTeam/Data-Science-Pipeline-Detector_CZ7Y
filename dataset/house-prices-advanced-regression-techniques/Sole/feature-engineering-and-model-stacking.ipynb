{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feature Engineering and Model Stacking for House Price Modelling\n\nIn this notebook, I use the open source Python library [Feature-engine](https://feature-engine.readthedocs.io/en/latest/) to create 3 different pipelines of variable transformation. Then, I train several machine learning models utilizing the transformed datasets, tuning their parameters with a grid search with cross-validation. And finally, I will combine the models through stacking.\n\nI analysed the house prices data set in a [different notebook](https://www.kaggle.com/solegalli/predict-house-price-with-feature-engine) in case you are interested in getting more familiar with the variables. There are also a number of notebooks in Kaggle with good data exploration.\n\n### This notebook is based on the following resources:\n\n- [Feature-engine](https://feature-engine.readthedocs.io/en/latest/), Python open source library\n- [Feature Engineering for Machine Learning](https://www.courses.trainindata.com/p/feature-engineering-for-machine-learning), online course.\n- [Packt Feature Engineering Cookbook](https://www.packtpub.com/data/python-feature-engineering-cookbook)\n- [Kaggle ensembling guide](https://mlwave.com/kaggle-ensembling-guide/)","metadata":{}},{"cell_type":"code","source":"# let's install Feature-engine\n\n!pip install feature_engine","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import uniform, randint\n\n# Scikit-learn metrics and handling\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import (\n    train_test_split,\n    GridSearchCV,\n    RandomizedSearchCV,\n    cross_validate\n)\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Scikit-learn models\nfrom sklearn.ensemble import (\n    RandomForestRegressor,\n    GradientBoostingRegressor,\n    StackingRegressor,\n)\n\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\n\n# Other models\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\n\n# feature engineering\nfrom feature_engine import creation\nfrom feature_engine import discretisation as disc\nfrom feature_engine import encoding as enc\nfrom feature_engine import imputation as imp\nfrom feature_engine import selection as sel\nfrom feature_engine import transformation as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"# load training data\ndata = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n\n# load data for competition submission\n# this data does not have the target\nsubmission = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split training data into train and test\n\nX_train, X_test, y_train, y_test = train_test_split(data.drop(\n    ['Id', 'SalePrice'], axis=1),\n    data['SalePrice'],\n    test_size=0.05,\n    random_state=0)\n\nX_train.shape, X_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop id\nid_ = submission['Id']\n\nsubmission.drop('Id', axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's transform the target with the log\n\ny_train = np.log(y_train)\n\ny_test = np.log(y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quick setting\n\nI will create lists with the variable names for which I will carry common pre-processing and transformations.","metadata":{}},{"cell_type":"code","source":"# let's identify the categorical variables\n\ncategorical = [var for var in X_train.columns if data[var].dtype == 'O']\n\n# MSSubClass is also categorical by definition, despite its numeric values\ncategorical = categorical + ['MSSubClass']\n\n# number of categorical variables\nlen(categorical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# cast all variables as categorical, for automatic use with Feature-engine\n\nX_train[categorical] = X_train[categorical].astype('O')\nX_test[categorical] = X_test[categorical].astype('O')\nsubmission[categorical] = submission[categorical].astype('O')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"master_data = pd.concat([data, submission], axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# variables to impute with the most frequent category\ncategorical_mode = [var for var in categorical \n                    if master_data[var].isnull().sum()>0 \n                    and master_data[var].isnull().mean()<0.1]\n\n# variables to impute with the string missing\ncategorical_missing = [var for var in categorical \n                       if master_data[var].isnull().sum()>0 \n                       and master_data[var].isnull().mean()>=0.1]\n\nlen(categorical_mode), len(categorical_missing)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# some variables refer to years, we are better off if we combine them into new features\n\nyear_vars = [var for var in X_train.columns if 'Yr' in var or 'Year' in var]\n\nyear_vars","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# when I create new features automatically using feature engine, these\n# 2 new variables will contain missing data, as they come from garageYrBlt, which\n# shows na.\n\nnew_vars = ['YrSold_sub_GarageYrBlt', 'GarageYrBlt_sub_YearBuilt']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's find the numerical variables\n\nnumerical = [var for var in X_train.columns if var not in categorical+year_vars]\n\nlen(numerical)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# variables to impute with the most frequent category\nnumerical_median = [var for var in numerical \n                    if master_data[var].isnull().sum()>0 \n                    and master_data[var].isnull().mean()<0.1]\n\n# variables to impute with the string missing\nnumerical_arbitrary = [var for var in numerical \n                       if master_data[var].isnull().sum()>0 \n                       and master_data[var].isnull().mean()>=0.1]\n\nlen(numerical_median), len(numerical_arbitrary)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's find non-discrete variables\n\ndiscretize = [\n    var for var in numerical if len(X_train[var].unique()) >= 20\n]\n\n# number of discrete variables\nlen(discretize)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering Pipelines","metadata":{}},{"cell_type":"code","source":"linear_pipe = Pipeline([\n    \n    # === feature creation ===\n    \n    # this transformer substracts the reference variables from YrSold, \n    # one at a time, to create 3 new variables with the elapsed time between the 2\n    \n    ('elapsed_time', creation.CombineWithReferenceFeature(\n        variables_to_combine = ['YrSold'],\n        reference_variables = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'],\n        operations=['sub'],\n    )),\n    \n    # this transformer substracts YearBuilt from the 2 variables to combine\n    # to capture the time passed between when the house was built and then \n    # subsequently remodelled, or when the garage was built\n    \n    ('elapsed_time2', creation.CombineWithReferenceFeature(\n    variables_to_combine = ['YearRemodAdd', 'GarageYrBlt'],\n    reference_variables = ['YearBuilt'],\n    operations=['sub'],\n     )),\n    \n    # the following 2 steps are inspired of the engineering in this notebook:\n    # https://www.kaggle.com/marto24/beginners-prediction-top3\n    \n    # this transformer summs the 4 variables to combine in a new variable with the\n    # indicated name in new_variables_name\n    ('total_surface', creation.MathematicalCombination(\n        variables_to_combine=['TotRmsAbvGrd', 'FullBath','HalfBath', 'KitchenAbvGr'],\n        math_operations=['sum'],\n        new_variables_names=['Total_Surface']\n    )),\n    \n    # this transformer takes the ratio of the variable to combine and the reference\n    # into a new variable\n    \n    ('surface_room', creation.CombineWithReferenceFeature(\n        variables_to_combine = ['GrLivArea'],\n        reference_variables = ['Total_Surface'],\n        operations=['div'],\n    )),\n    \n    \n    # this transformer summs the 2 variables to combine in a new variable with the\n    # indicated name in new_variables_name\n    # idea also taken from https://www.kaggle.com/marto24/beginners-prediction-top3\n    ('qual_sf', creation.MathematicalCombination(\n        variables_to_combine=['1stFlrSF', '2ndFlrSF'],\n        math_operations=['sum'],\n        new_variables_names=['HighQualSF']\n    )),\n    \n    # === drop year vars ===\n    \n    # now that I have used these variables to derive the above features, I\n    # drop them from the data\n    \n    ('drop_features', sel.DropFeatures(\n        features_to_drop =['YearBuilt', 'YearRemodAdd', 'GarageYrBlt','YrSold']\n    )),\n    \n    # === missing data imputation ====\n    \n    # adds binary variables when data is missing for the indicated variables\n    \n    ('missing_ind', imp.AddMissingIndicator(\n        missing_only=True, variables=numerical_arbitrary+categorical_mode+new_vars\n    )),\n    \n    # replaces NA by a value placed at the 75th quantile + 3 * IQR of the variable\n    \n    ('arbitrary_number', imp.EndTailImputer(\n        imputation_method='iqr', tail='right', fold=3, variables=numerical_arbitrary\n    )),\n    \n    # replaces NA with the median value of the variable\n    \n    ('median', imp.MeanMedianImputer(\n        imputation_method='median', variables=numerical_median+new_vars\n    )),\n    \n    # replaces NA with the most frequent category\n    \n    ('frequent', imp.CategoricalImputer(\n        imputation_method='frequent', variables=categorical_mode, return_object=True\n    )),\n    \n    # replaces NA with the string 'Missing'\n    \n    ('missing', imp.CategoricalImputer(\n        imputation_method='missing', variables=categorical_missing, return_object=True\n    )),\n    \n    # === transformation ==\n    \n    # applies Yeo-Johnson transformation to the indicated variables\n    \n    ('transformation', tf.YeoJohnsonTransformer(variables=discretize)),\n     \n     # === categorical encoding \n\n    # one hot encoding of the 10 most frequent categories of each categorical\n    # variable\n    # (Feature-engine recognises categorical variables automatically if they \n    # are casted as object)\n    \n    ('encoder', enc.OneHotEncoder(top_categories=10)),\n     \n    # === feature Scaling ===\n    \n    ('scaler', StandardScaler()),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit pipeline, learns all necessary parameters\nlinear_pipe.fit(X_train, y_train)\n\n# transform the data\nX_train_linear = linear_pipe.transform(X_train)\nX_test_linear = linear_pipe.transform(X_test)\nsubmission_linear = linear_pipe.transform(submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monotonic_pipe = Pipeline([\n    \n    # === feature creation ===\n    \n    ('elapsed_time', creation.CombineWithReferenceFeature(\n        variables_to_combine = ['YrSold'],\n        reference_variables = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'],\n        operations=['sub'],\n    )),\n    \n    ('elapsed_time2', creation.CombineWithReferenceFeature(\n    variables_to_combine = ['YearRemodAdd', 'GarageYrBlt'],\n    reference_variables = ['YearBuilt'],\n    operations=['sub'],\n     )),\n    \n    # the following 2 steps are inspired of the engineering in this notebook:\n    # https://www.kaggle.com/marto24/beginners-prediction-top3\n    \n    # this transformer summs the 4 variables to combine in a new variable with the\n    # indicated name in new_variables_name\n    ('total_surface', creation.MathematicalCombination(\n        variables_to_combine=['TotRmsAbvGrd', 'FullBath','HalfBath', 'KitchenAbvGr'],\n        math_operations=['sum'],\n        new_variables_names=['Total_Surface']\n    )),\n    \n    # this transformer takes the ratio of the variable to combine and the reference\n    # into a new variable\n    \n    ('surface_room', creation.CombineWithReferenceFeature(\n        variables_to_combine = ['GrLivArea'],\n        reference_variables = ['Total_Surface'],\n        operations=['div'],\n    )),\n    \n    \n    # this transformer summs the 2 variables to combine in a new variable with the\n    # indicated name in new_variables_name\n    # idea also taken from https://www.kaggle.com/marto24/beginners-prediction-top3\n    ('qual_sf', creation.MathematicalCombination(\n        variables_to_combine=['1stFlrSF', '2ndFlrSF'],\n        math_operations=['sum'],\n        new_variables_names=['HighQualSF']\n    )),\n    \n    # === drop year vars ===\n    \n    ('drop_features', sel.DropFeatures(\n        features_to_drop =['YearBuilt', 'YearRemodAdd', 'GarageYrBlt','YrSold']\n    )),\n    \n    # === missing data imputation ====\n    \n    ('missing_ind', imp.AddMissingIndicator(\n        missing_only=True, variables=numerical_arbitrary+categorical_mode+new_vars\n    )),\n    \n    ('arbitrary_number', imp.EndTailImputer(\n        imputation_method='iqr', tail='right', fold=3, variables=numerical_arbitrary\n    )),\n       \n    ('median', imp.MeanMedianImputer(\n        imputation_method='median', variables=numerical_median+new_vars\n    )),\n    \n    ('frequent', imp.CategoricalImputer(\n        imputation_method='frequent', variables=categorical_mode, return_object=True\n    )),\n     \n    ('missing', imp.CategoricalImputer(\n        imputation_method='missing', variables=categorical_missing, return_object=True\n    )),\n    \n    \n    # == rare category grouping ==\n    \n    # we group categories that appear in less than 10% of the observations\n    # into a new label called 'Rare'\n    \n    ('rare_grouping', enc.RareLabelEncoder(\n        tol = 0.1,n_categories=1,\n    )),\n    \n    # === discretization ==\n    \n    # sort continuous variables into discrete bins of equal number of observations\n    # returns variables cast as objects, that will be automatically captured \n    # by the encoder later on\n    ('discretizer', disc.EqualWidthDiscretiser(\n        bins=3, variables=discretize,return_object=True\n    )),\n     \n     # === categorical encoding\n    \n    # transform the categories of categorical variables and the bins of the disctetized\n    # variables into integers, that go from 0 to the number of unique values, in the order\n    # of the target mean per category or per bin\n    \n    ('encoder', enc.OrdinalEncoder(encoding_method='ordered')),\n     \n    # === feature Scaling ===\n    \n    ('scaler', StandardScaler()),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monotonic_pipe.fit(X_train, y_train)\n\nX_train_monotonic = monotonic_pipe.transform(X_train)\nX_test_monotonic = monotonic_pipe.transform(X_test)\nsubmission_monotonic = monotonic_pipe.transform(submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_pipe = Pipeline([\n    \n    # === feature creation ===\n    \n    ('elapsed_time', creation.CombineWithReferenceFeature(\n        variables_to_combine = ['YrSold'],\n        reference_variables = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'],\n        operations=['sub'],\n    )),\n    \n    ('elapsed_time2', creation.CombineWithReferenceFeature(\n    variables_to_combine = ['YearRemodAdd', 'GarageYrBlt'],\n    reference_variables = ['YearBuilt'],\n    operations=['sub'],\n     )),\n    \n    # the following 2 steps are inspired of the engineering in this notebook:\n    # https://www.kaggle.com/marto24/beginners-prediction-top3\n    \n    # this transformer summs the 4 variables to combine in a new variable with the\n    # indicated name in new_variables_name\n    ('total_surface', creation.MathematicalCombination(\n        variables_to_combine=['TotRmsAbvGrd', 'FullBath','HalfBath', 'KitchenAbvGr'],\n        math_operations=['sum'],\n        new_variables_names=['Total_Surface']\n    )),\n    \n    # this transformer takes the ratio of the variable to combine and the reference\n    # into a new variable\n    \n    ('surface_room', creation.CombineWithReferenceFeature(\n        variables_to_combine = ['GrLivArea'],\n        reference_variables = ['Total_Surface'],\n        operations=['div'],\n    )),\n    \n    \n    # this transformer summs the 2 variables to combine in a new variable with the\n    # indicated name in new_variables_name\n    # idea also taken from https://www.kaggle.com/marto24/beginners-prediction-top3\n    ('qual_sf', creation.MathematicalCombination(\n        variables_to_combine=['1stFlrSF', '2ndFlrSF'],\n        math_operations=['sum'],\n        new_variables_names=['HighQualSF']\n    )),\n    \n    # === drop year vars ===\n    \n    ('drop_features', sel.DropFeatures(\n        features_to_drop =['YearBuilt', 'YearRemodAdd', 'GarageYrBlt','YrSold']\n    )),\n    \n    # === missing data imputation ====\n    \n    ('missing_ind', imp.AddMissingIndicator(\n        missing_only=True, variables=numerical_arbitrary+categorical_mode+new_vars\n    )),\n    \n    ('arbitrary_number', imp.EndTailImputer(\n        imputation_method='iqr', tail='right', fold=3, variables=numerical_arbitrary\n    )),\n       \n    ('median', imp.MeanMedianImputer(\n        imputation_method='median', variables=numerical_median+new_vars\n    )),\n    \n    ('frequent', imp.CategoricalImputer(\n        imputation_method='frequent', variables=categorical_mode, return_object=True\n    )),\n     \n    ('missing', imp.CategoricalImputer(\n        imputation_method='missing', variables=categorical_missing, return_object=True\n    )),\n    \n    \n    # == rare category grouping ==\n    \n    ('rare_grouping', enc.RareLabelEncoder(\n        tol = 0.1,n_categories=1,\n    )),\n    \n    # === discretization ==\n    \n    # we replace the values of continuous variables by the predictions made by a \n    # decision tree\n    ('discretizer', disc.DecisionTreeDiscretiser(\n        cv=3, scoring='neg_mean_squared_error', variables=discretize,\n        regression=True, random_state=0\n    )),\n     \n     # === categorical encoding\n    \n    # we replace the categories of categorical variables by the predictions made by a \n    # decision tree\n    ('encoder', enc.DecisionTreeEncoder(\n        encoding_method='arbitrary', cv=3, scoring='neg_mean_squared_error',\n        regression=True, random_state=0\n    )),\n     \n    # === feature Scaling ===\n    ('scaler', StandardScaler()),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree_pipe.fit(X_train, y_train)\n\nX_train_tree = tree_pipe.transform(X_train)\nX_test_tree = tree_pipe.transform(X_test)\nsubmission_tree = tree_pipe.transform(submission)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Machine Learning Model Library","metadata":{}},{"cell_type":"code","source":"# gradient boosting regressor\n\ngbm_param = dict(\n    loss=['ls', 'huber'],\n    n_estimators=[10, 20, 50, 100, 200],\n    min_samples_split=[0.01, 0.1, 0.3],\n    max_depth=[1,2,3,None],\n    )\n\ngbm = GradientBoostingRegressor(\n    loss='ls',\n    n_estimators=100,\n    criterion='friedman_mse',\n    min_samples_split=2,\n    max_depth=3,\n    random_state=0,\n    n_iter_no_change=2,\n    tol=0.0001,\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbm_grid = RandomizedSearchCV(gbm, gbm_param, scoring='neg_mean_squared_error', n_iter=100, random_state=1)\n\n# gbm_grid = GridSearchCV(gbm, gbm_param, scoring='neg_mean_squared_error')\n\ngbm_linear = gbm_grid.fit(X_train_linear, y_train)\ngbm_monotonic = gbm_grid.fit(X_train_monotonic, y_train)\ngbm_tree = gbm_grid.fit(X_train_tree, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbm_linear.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to plot the results of the grid search\n\ndef plot_grid(grid, title):\n    \n    # make df with results\n    results = pd.DataFrame(grid.cv_results_)\n    results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n    results.reset_index(drop=True, inplace=True)\n    \n    # plot results\n    results['mean_test_score'].plot(yerr=[results['std_test_score'], results['std_test_score']], subplots=True)\n    plt.ylabel('Mean test score')\n    plt.title(title)\n    plt.show()\n    \n    return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot results\n\nplot_grid(gbm_linear, 'gbm search - linear data')\nplot_grid(gbm_monotonic, 'gbm search - monotonic data')\nplot_grid(gbm_tree, 'gbm search - tree data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Elastic Net - Linear Model\n\nelastic_param = dict(\n    max_iter=[50000, 100000],\n    alpha=[0.001, 0.01],\n    l1_ratio=[0, 0.2, 0.5, 0.7, 1]\n    )\n\nelastic = ElasticNet(\n    alpha=1.0,\n    l1_ratio=0.5,\n    max_iter=100000,\n    random_state=0\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# elastic_grid = RandomizedSearchCV(elastic, elastic_param, scoring='neg_mean_squared_error', n_iter=5, random_state=1)\n\nelastic_grid = GridSearchCV(elastic, elastic_param, scoring='neg_mean_squared_error')\n\nelastic_linear = elastic_grid.fit(X_train_linear, y_train)\nelastic_monotonic = elastic_grid.fit(X_train_monotonic, y_train)\nelastic_tree = elastic_grid.fit(X_train_tree, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elastic_linear.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_grid(elastic_linear, 'elastic search - linear data')\nplot_grid(elastic_monotonic, 'elastic search - monotonic data')\nplot_grid(elastic_tree, 'elastic search - tree data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Nearest Neighbours\n\nknn_param = dict(\n    n_neighbors=[3,5,10], \n    algorithm=['ball_tree', 'kd_tree', 'brute'],\n    p=[1,2],\n    )\n\nknn = KNeighborsRegressor(\n    n_neighbors=5, \n    algorithm='auto',\n    leaf_size=30,\n    p=2,\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# knn_grid = RandomizedSearchCV(knn, knn_param, scoring='neg_mean_squared_error', n_iter=5, random_state=1)\n\nknn_grid = GridSearchCV(knn, knn_param, scoring='neg_mean_squared_error')\n\nknn_linear = knn_grid.fit(X_train_linear, y_train)\nknn_monotonic = knn_grid.fit(X_train_monotonic, y_train)\nknn_tree = knn_grid.fit(X_train_tree, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_linear.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_grid(knn_linear, 'knn search - linear data')\nplot_grid(knn_monotonic, 'knn search - monotonic data')\nplot_grid(knn_tree, 'knn search - tree data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Light GBM\n\nlgbm_param = {\n    \"num_leaves\": [20,30,40],\n    \"max_depth\": [4, 6, 10, 20],\n    \"n_estimators\": [20, 64, 100],\n}\n\nlgbm = LGBMRegressor(\n    learning_rate = 0.16060612646519587, \n    min_child_weight = 0.4453842422224686,\n    objective='regression', \n    random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_grid = GridSearchCV(lgbm, lgbm_param, scoring='neg_mean_squared_error')\n\nlgbm_linear = lgbm_grid.fit(X_train_linear, y_train)\nlgbm_monotonic = lgbm_grid.fit(X_train_monotonic, y_train)\nlgbm_tree = lgbm_grid.fit(X_train_tree, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_linear.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_grid(lgbm_linear, 'lgbm search - linear data')\nplot_grid(lgbm_monotonic, 'lgbm search - monotonic data')\nplot_grid(lgbm_tree, 'lgbm search - tree data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Regressor\n\nsvr_param = {\n    \"kernel\": [\"poly\",'rbf'],\n    \"C\": [0.053677105521141605, 0.1],\n    \"epsilon\": [0.03925943476562099, 0.1],\n    \"coef0\": [0.9486751042886584, 0.5],\n}\n\nsvr = SVR(\n    kernel='rbf',\n    degree=3,\n    C=1.0,\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# svr_grid = RandomizedSearchCV(svr, svr_param, scoring='neg_mean_squared_error', n_iter=5, random_state=1)\n\nsvr_grid = GridSearchCV(svr, svr_param, scoring='neg_mean_squared_error')\n\nsvr_linear = svr_grid.fit(X_train_linear, y_train)\nsvr_monotonic = svr_grid.fit(X_train_monotonic, y_train)\nsvr_tree = svr_grid.fit(X_train_tree, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"svr_linear.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_grid(svr_linear, 'svr search - linear data')\nplot_grid(svr_monotonic, 'svr search - monotonic data')\nplot_grid(svr_tree, 'svr search - tree data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gradient boosting regressor\n\nrf_param = dict(\n    n_estimators=[100, 200, 500, 1000],\n    min_samples_split=[0.1, 0.3, 0.5, 1.0],\n    max_depth=[1,2,3,None],\n    )\n\nrf = RandomForestRegressor(\n    n_estimators=100,\n    min_samples_split=2,\n    max_depth=3,\n    random_state=0,\n    n_jobs=-1,\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_grid = RandomizedSearchCV(rf, rf_param, scoring='neg_mean_squared_error', n_iter=10, random_state=0)\n\n# rf_grid = GridSearchCV(rf, rf_param, scoring='neg_mean_squared_error')\n\nrf_linear = rf_grid.fit(X_train_linear, y_train)\nrf_monotonic = rf_grid.fit(X_train_monotonic, y_train)\nrf_tree = rf_grid.fit(X_train_tree, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_linear.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_grid(rf_linear, 'svr search - linear data')\nplot_grid(rf_monotonic, 'svr search - monotonic data')\nplot_grid(rf_tree, 'svr search - tree data')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def select_best_score(grid):\n    \n    results = pd.DataFrame(grid.cv_results_)\n    \n    results.sort_values(by='mean_test_score', ascending=False, inplace=True)\n    \n    results = results[['mean_test_score', 'std_test_score']]\n    \n    return results.head(1)\n\n# test function\nselect_best_score(rf_linear)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = pd.concat([\n    select_best_score(gbm_linear),\n    select_best_score(gbm_monotonic),\n    select_best_score(gbm_tree),\n    \n    select_best_score(elastic_linear),\n    select_best_score(elastic_monotonic),\n    select_best_score(elastic_tree),\n    \n    select_best_score(knn_linear),\n    select_best_score(knn_monotonic),\n    select_best_score(knn_tree),\n    \n    select_best_score(svr_linear),\n    select_best_score(svr_monotonic),\n    select_best_score(svr_tree),\n    \n    select_best_score(lgbm_linear),\n    select_best_score(lgbm_monotonic),\n    select_best_score(lgbm_tree),\n    \n    select_best_score(rf_linear),\n    select_best_score(rf_monotonic),\n    select_best_score(rf_tree),\n    ], axis=0)\n\nresults.index = [\n    'gbm_linear','gbm_monotonic', 'gbm_tree',\n    'elastic_linear','elastic_monotonic', 'elastic_tree',\n    'knn_linear','knn_monotonic', 'knn_tree',\n    'svr_linear','svr_monotonic', 'svr_tree',\n    'lgbm_linear','lgbm_monotonic', 'lgbm_tree',\n    'rf_linear','rf_monotonic', 'rf_tree',\n]\n\nresults.sort_values(by='mean_test_score', ascending=False, inplace=True)\n\nresults.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results['mean_test_score'].plot.bar(\n    yerr=[results['std_test_score'], results['std_test_score']],\n    subplots=True, figsize=(10,5))\n\nplt.ylabel('Score')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compare performance","metadata":{}},{"cell_type":"code","source":"# let's get the predictions from the elastic net\nX_train_preds = elastic_monotonic.predict(X_train_monotonic)\nX_test_preds = elastic_monotonic.predict(X_test_monotonic)\nsubmission_preds = elastic_monotonic.predict(submission_monotonic)\n\nprint('Train rmse: ', mean_squared_error(y_train, X_train_preds,squared=False))\nprint('Test rmse: ', mean_squared_error(y_test, X_test_preds,squared=False))\nprint()\nprint('Train r2: ', r2_score(y_train, X_train_preds))\nprint('Test r2: ', r2_score(y_test, X_test_preds))\n\nmy_submission = pd.DataFrame({'Id': id_, 'SalePrice': np.exp(submission_preds)})\n\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_elastic.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's get the predictions from the SVR\nX_train_preds = svr_monotonic.predict(X_train_monotonic)\nX_test_preds = svr_monotonic.predict(X_test_monotonic)\nsubmission_preds = svr_monotonic.predict(submission_monotonic)\n\nprint('Train rmse: ', mean_squared_error(y_train, X_train_preds,squared=False))\nprint('Test rmse: ', mean_squared_error(y_test, X_test_preds,squared=False))\nprint()\nprint('Train r2: ', r2_score(y_train, X_train_preds))\nprint('Test r2: ', r2_score(y_test, X_test_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's get the predictions from the light GBM\nX_train_preds = lgbm_tree.predict(X_train_tree)\nX_test_preds = lgbm_tree.predict(X_test_tree)\nsubmission_preds = lgbm_tree.predict(submission_tree)\n\nprint('Train rmse: ', mean_squared_error(y_train, X_train_preds,squared=False))\nprint('Test rmse: ', mean_squared_error(y_test, X_test_preds,squared=False))\nprint()\nprint('Train r2: ', r2_score(y_train, X_train_preds))\nprint('Test r2: ', r2_score(y_test, X_test_preds))\n\nmy_submission = pd.DataFrame({'Id': id_, 'SalePrice': np.exp(submission_preds)})\n\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_lgbm.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's get the predictions from the gradient boosting regressor\nX_train_preds = gbm_tree.predict(X_train_tree)\nX_test_preds = gbm_tree.predict(X_test_tree)\nsubmission_preds = gbm_tree.predict(submission_tree)\n\nprint('Train rmse: ', mean_squared_error(y_train, X_train_preds,squared=False))\nprint('Test rmse: ', mean_squared_error(y_test, X_test_preds,squared=False))\nprint()\nprint('Train r2: ', r2_score(y_train, X_train_preds))\nprint('Test r2: ', r2_score(y_test, X_test_preds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Stacking","metadata":{}},{"cell_type":"code","source":"estimators = [\n    ('gbm_linear',gbm_linear.best_estimator_),\n    ('gbm_monotonic',gbm_monotonic.best_estimator_),\n    ('gbm_tree',gbm_tree.best_estimator_),\n    ('elastic_linear',elastic_linear.best_estimator_),\n    ('elastic_monotonic',elastic_monotonic.best_estimator_),\n    ('elastic_tree',elastic_tree.best_estimator_),\n    ('knn_linear',knn_linear.best_estimator_),\n    ('knn_monotonic',knn_monotonic.best_estimator_),\n    ('knn_tree',knn_tree.best_estimator_),\n    ('svr_linear',svr_linear.best_estimator_),\n    ('svr_monotonic',svr_monotonic.best_estimator_),\n    ('svr_tree',svr_tree.best_estimator_),\n    ('lgbm_linear',lgbm_linear.best_estimator_),\n    ('lgbm_monotonic',lgbm_monotonic.best_estimator_),\n    ('lgbm_tree',lgbm_tree.best_estimator_),\n    ('rf_linear',rf_linear.best_estimator_),\n    ('rf_monotonic',rf_monotonic.best_estimator_),\n    ('rf_tree', rf_tree.best_estimator_),\n]\n\nstacked = StackingRegressor(\n    estimators=estimators,\n    final_estimator=LGBMRegressor(random_state=1)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacking = cross_validate(\n    stacked, X_train_linear, y_train, cv=5,\n    scoring='neg_mean_squared_error', return_estimator=True)\n\nstacking['test_score'].mean(), stacking['test_score'].std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacking = cross_validate(\n    stacked, X_train_monotonic, y_train, cv=5,\n    scoring='neg_mean_squared_error', return_estimator=True)\n\nstacking['test_score'].mean(), stacking['test_score'].std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacking = cross_validate(\n    stacked, X_train_tree, y_train, cv=5,\n    scoring='neg_mean_squared_error', return_estimator=True)\n\nstacking['test_score'].mean(), stacking['test_score'].std()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacked.fit(X_train_tree, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's get the predictions from the stacked models\n\nX_train_preds = stacked.predict(X_train_tree)\nX_test_preds = stacked.predict(X_test_tree)\nsubmission_preds = stacked.predict(submission_tree)\n\nprint('Train rmse: ', mean_squared_error(y_train, X_train_preds,squared=False))\nprint('Test rmse: ', mean_squared_error(y_test, X_test_preds,squared=False))\nprint()\nprint('Train r2: ', r2_score(y_train, X_train_preds))\nprint('Test r2: ', r2_score(y_test, X_test_preds))\n\nmy_submission = pd.DataFrame({'Id': id_, 'SalePrice': np.exp(submission_preds)})\n\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission_stacke.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}