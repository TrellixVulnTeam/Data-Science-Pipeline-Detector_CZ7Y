{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# magic word for producing visualizations in notebook\n%matplotlib inline\nplt.style.use('seaborn')\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-13T10:43:26.681717Z","iopub.execute_input":"2022-06-13T10:43:26.682076Z","iopub.status.idle":"2022-06-13T10:43:27.319685Z","shell.execute_reply.started":"2022-06-13T10:43:26.681976Z","shell.execute_reply":"2022-06-13T10:43:27.317872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HOUSE PRICES PREDICTION\n![Main pic](https://www.yourmoney.com/wp-content/uploads/sites/3/2022/02/house-prices-scaled.jpg)","metadata":{}},{"cell_type":"markdown","source":"### The Machine Learning Project Checklist\n1. Frame the problem\n2. Get the data\n3. Explore the data\n4. Prepare the data\n5. Model the data\n6. Fine-tune the models\n7. Present the solution\n8. Launch the ML system\n\n\n\n## 1. Frame the problem\n#### Goal\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\n#### Metric\nSubmissions are evaluated on RMSLE (Root Mean Squared Log Error), Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.\n \n \n## 2. Get the data\n   Kaggle already done this step for us, and splitted the date for train & test dataframe\n    \n    \n## 3. Explore the data (Train data)\n   we must create a test set, put it aside, and never look at it.","metadata":{}},{"cell_type":"code","source":"# read the data\ntrain_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n\n# inspect first 5 rows of the train dataset\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:28.925136Z","iopub.execute_input":"2022-06-13T10:43:28.925422Z","iopub.status.idle":"2022-06-13T10:43:29.003909Z","shell.execute_reply.started":"2022-06-13T10:43:28.925393Z","shell.execute_reply":"2022-06-13T10:43:29.003058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print a concise summary of the tain DataFrame.\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:29.446957Z","iopub.execute_input":"2022-06-13T10:43:29.44785Z","iopub.status.idle":"2022-06-13T10:43:29.490109Z","shell.execute_reply.started":"2022-06-13T10:43:29.447805Z","shell.execute_reply":"2022-06-13T10:43:29.489469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the total number of NaNs in the train df\ntrain_df.isna().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:30.175095Z","iopub.execute_input":"2022-06-13T10:43:30.175956Z","iopub.status.idle":"2022-06-13T10:43:30.191041Z","shell.execute_reply.started":"2022-06-13T10:43:30.1759Z","shell.execute_reply":"2022-06-13T10:43:30.190207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the total number of NaNs in each column in the train df\ntrain_df[\n    train_df.columns[\n        train_df.isna().any()\n    ]\n].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:31.119063Z","iopub.execute_input":"2022-06-13T10:43:31.119362Z","iopub.status.idle":"2022-06-13T10:43:31.139732Z","shell.execute_reply.started":"2022-06-13T10:43:31.119329Z","shell.execute_reply":"2022-06-13T10:43:31.139172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the NaNs distribution in the df\ntrain_df.isna().sum().plot(kind='hist')\nplt.xlabel(\"Number of NaNs\")\nplt.title('NaNs distribution', fontsize=18);","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:31.774825Z","iopub.execute_input":"2022-06-13T10:43:31.775441Z","iopub.status.idle":"2022-06-13T10:43:32.126026Z","shell.execute_reply.started":"2022-06-13T10:43:31.775393Z","shell.execute_reply":"2022-06-13T10:43:32.125213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"find-out the outliers percent in terms of the proportion of values that are missing.","metadata":{}},{"cell_type":"code","source":"# we will remove columns with grater than 800 NaNs\nthreshold = 800\nprint(f\"Columns with grater than {threshold} NaNs, {round((threshold/train_df.shape[0]) * 100)}% of it's values are NaNs.\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:33.229095Z","iopub.execute_input":"2022-06-13T10:43:33.229947Z","iopub.status.idle":"2022-06-13T10:43:33.235749Z","shell.execute_reply.started":"2022-06-13T10:43:33.229891Z","shell.execute_reply":"2022-06-13T10:43:33.234897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Which columns we need to remove from the dataset?\noutliers = train_df.isna().sum()[train_df.isna().sum() > threshold]\noutliers","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:33.917992Z","iopub.execute_input":"2022-06-13T10:43:33.918324Z","iopub.status.idle":"2022-06-13T10:43:33.942416Z","shell.execute_reply.started":"2022-06-13T10:43:33.918292Z","shell.execute_reply":"2022-06-13T10:43:33.941795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if there is any duplicates in the df\ntrain_df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:34.49409Z","iopub.execute_input":"2022-06-13T10:43:34.494395Z","iopub.status.idle":"2022-06-13T10:43:34.519818Z","shell.execute_reply.started":"2022-06-13T10:43:34.494365Z","shell.execute_reply":"2022-06-13T10:43:34.51878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inspect the descriptive statistics of the numeric features\ntrain_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:35.248281Z","iopub.execute_input":"2022-06-13T10:43:35.248546Z","iopub.status.idle":"2022-06-13T10:43:35.355959Z","shell.execute_reply.started":"2022-06-13T10:43:35.248518Z","shell.execute_reply":"2022-06-13T10:43:35.355331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We had lots of Nans, and the values have very large scale.","metadata":{}},{"cell_type":"code","source":"# plot the distribution of the numeric features\ntrain_df.hist(bins=50, figsize=(20,16));","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:36.902268Z","iopub.execute_input":"2022-06-13T10:43:36.902778Z","iopub.status.idle":"2022-06-13T10:43:44.897534Z","shell.execute_reply.started":"2022-06-13T10:43:36.902744Z","shell.execute_reply":"2022-06-13T10:43:44.896831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the data are skewed, and if a feature has a skewed distribution, applying a logarithm can help normalize it.  \nWe should try taking the log of the skewed numeric features.","metadata":{}},{"cell_type":"code","source":"# plot the correlation between the numeric features\n\n# make a bigger plot\n# plt.figure(figsize=(19,15))\n\n# Mask the upper part of the heatmap\n# mask = np.triu(train_df.corr())\n\n# plot the heatmap using Seaborn\nsns.heatmap(train_df.corr(), annot=False, cmap='icefire');","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:44.899057Z","iopub.execute_input":"2022-06-13T10:43:44.899306Z","iopub.status.idle":"2022-06-13T10:43:45.457074Z","shell.execute_reply.started":"2022-06-13T10:43:44.89927Z","shell.execute_reply":"2022-06-13T10:43:45.45618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find out the top correlated features \ntrain_df.corr().unstack().sort_values(ascending=False).drop_duplicates()[:24]","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:45.458203Z","iopub.execute_input":"2022-06-13T10:43:45.458436Z","iopub.status.idle":"2022-06-13T10:43:45.476551Z","shell.execute_reply.started":"2022-06-13T10:43:45.458407Z","shell.execute_reply":"2022-06-13T10:43:45.475777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we should try removing Multicollinearity  features (Remove strongly correlated columns)","metadata":{}},{"cell_type":"code","source":"# find out the correlation between the features and the Target\ntrain_df.corrwith(train_df['SalePrice']).sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:45.478628Z","iopub.execute_input":"2022-06-13T10:43:45.479375Z","iopub.status.idle":"2022-06-13T10:43:45.50168Z","shell.execute_reply.started":"2022-06-13T10:43:45.47933Z","shell.execute_reply":"2022-06-13T10:43:45.500817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the correlation with the target\ntrain_df.corrwith(train_df['SalePrice']).sort_values(ascending=False)[1:].plot(kind='bar')\n# plot a vertical line on where we want to remove the non correlated columns\nplt.vlines(26.5, -0.1, 0.8, colors='red')\nplt.title(\"Correlations with the Target 'SalePrice'\");","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:45.503213Z","iopub.execute_input":"2022-06-13T10:43:45.503611Z","iopub.status.idle":"2022-06-13T10:43:45.940581Z","shell.execute_reply.started":"2022-06-13T10:43:45.503565Z","shell.execute_reply":"2022-06-13T10:43:45.939707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should try removing non correlated columns","metadata":{}},{"cell_type":"markdown","source":"## 4. Prepare the data\n\nYou should always create a test set and set it aside before\ninspecting the data closely.","metadata":{}},{"cell_type":"code","source":"# split the features X and the target y\nX = train_df.drop(columns=['SalePrice'])\ny = train_df.SalePrice","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:45.941917Z","iopub.execute_input":"2022-06-13T10:43:45.9422Z","iopub.status.idle":"2022-06-13T10:43:45.948465Z","shell.execute_reply.started":"2022-06-13T10:43:45.942169Z","shell.execute_reply":"2022-06-13T10:43:45.947541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the columns we want too remove to use this variable later in the pipeline\nless_than_0_corr = train_df.corrwith(train_df['SalePrice'])[train_df.corrwith(train_df['SalePrice']) < 0].index.to_list()\ncols_to_remove = list(outliers.index) + less_than_0_corr","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:45.949623Z","iopub.execute_input":"2022-06-13T10:43:45.949864Z","iopub.status.idle":"2022-06-13T10:43:45.990756Z","shell.execute_reply.started":"2022-06-13T10:43:45.949833Z","shell.execute_reply":"2022-06-13T10:43:45.989847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### the numeric values","metadata":{}},{"cell_type":"code","source":"# select the numeric features\nnum_df = X.select_dtypes(include='number')\n\n# inspect first 5 rows of the numeric features\nnum_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:43:46.508023Z","iopub.execute_input":"2022-06-13T10:43:46.508331Z","iopub.status.idle":"2022-06-13T10:43:46.531667Z","shell.execute_reply.started":"2022-06-13T10:43:46.508298Z","shell.execute_reply":"2022-06-13T10:43:46.530836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's build a pipeline for preprocessing the numerical features/attributes:","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\nnum_pipeline = Pipeline([\n        # When the data is skewed, it is good to consider using\n        # the median value for replacing the missing values. \n        ('imputer', SimpleImputer(strategy=\"median\", add_indicator=True)),\n        # MinMaxScaler is useful when the distribution isn't Normal or Gaussian.\n        ('scaler', MinMaxScaler()),\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:02.620098Z","iopub.execute_input":"2022-06-13T10:45:02.620406Z","iopub.status.idle":"2022-06-13T10:45:02.626355Z","shell.execute_reply.started":"2022-06-13T10:45:02.620371Z","shell.execute_reply":"2022-06-13T10:45:02.625548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical values","metadata":{}},{"cell_type":"code","source":"# select the categorical features\ncat_df = X.select_dtypes(include='object')\n\n# inspect first 5 rows of the categorical features\ncat_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:44:04.590692Z","iopub.execute_input":"2022-06-13T10:44:04.590971Z","iopub.status.idle":"2022-06-13T10:44:04.616969Z","shell.execute_reply.started":"2022-06-13T10:44:04.590942Z","shell.execute_reply":"2022-06-13T10:44:04.616367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's build a pipeline for preprocessing the categorical features/attributes:","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\ncat_pipeline = Pipeline([\n        # we will fill the NaNs with the mode\n        ('imputer', SimpleImputer(strategy=\"most_frequent\", add_indicator=True)),\n        # the features has order meaning\n        ('encoder', OrdinalEncoder()),\n    ])","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:05.064844Z","iopub.execute_input":"2022-06-13T10:45:05.065142Z","iopub.status.idle":"2022-06-13T10:45:05.070386Z","shell.execute_reply.started":"2022-06-13T10:45:05.065112Z","shell.execute_reply":"2022-06-13T10:45:05.069427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now let's build a pipeline for preprocessing all the attributes:","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\n# keep only the wanted columns\nnum_attribs = list(set(num_df) - set(cols_to_remove))\ncat_attribs = list(set(cat_df) - set(cols_to_remove))\n\n# combine the numeric & categorical pipelines\npreprocessor = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", cat_pipeline, cat_attribs),\n    ], \n    remainder='drop' # the remainder features will be dropped [default]\n)\n\n# prepare the df form the ML models by calling the preprocessor\nX_prepared = preprocessor.fit_transform(X)\n# inspect the number of rows & columns of the prepared df\nX_prepared.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:07.422759Z","iopub.execute_input":"2022-06-13T10:45:07.423041Z","iopub.status.idle":"2022-06-13T10:45:07.495873Z","shell.execute_reply.started":"2022-06-13T10:45:07.422992Z","shell.execute_reply":"2022-06-13T10:45:07.49498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Model the data\n\nTraining and Evaluating on the Training Set","metadata":{}},{"cell_type":"code","source":"# for reproducability\nrandom_state = 10    # 10 for MESSI","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:21.181057Z","iopub.execute_input":"2022-06-13T10:45:21.181348Z","iopub.status.idle":"2022-06-13T10:45:21.186054Z","shell.execute_reply.started":"2022-06-13T10:45:21.181318Z","shell.execute_reply":"2022-06-13T10:45:21.185056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### LinearRegression model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\n\n# Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\nlog_y = np.log(y)\nlin_reg.fit(X_prepared, log_y)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:23.048395Z","iopub.execute_input":"2022-06-13T10:45:23.048683Z","iopub.status.idle":"2022-06-13T10:45:23.080403Z","shell.execute_reply.started":"2022-06-13T10:45:23.048657Z","shell.execute_reply":"2022-06-13T10:45:23.079491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\n# create a finction that scores the model for later usage\ndef score_model(model):\n    y_pred = model.predict(X_prepared)\n    model_mse = mean_squared_error(log_y, y_pred)\n    return np.sqrt(model_mse)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:24.025452Z","iopub.execute_input":"2022-06-13T10:45:24.025745Z","iopub.status.idle":"2022-06-13T10:45:24.031575Z","shell.execute_reply.started":"2022-06-13T10:45:24.025717Z","shell.execute_reply":"2022-06-13T10:45:24.030654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# score the LinearRegression model\nscore_model(lin_reg)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:26.394957Z","iopub.execute_input":"2022-06-13T10:45:26.395557Z","iopub.status.idle":"2022-06-13T10:45:26.41087Z","shell.execute_reply.started":"2022-06-13T10:45:26.39552Z","shell.execute_reply":"2022-06-13T10:45:26.409534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### DecisionTreeRegressor model","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=random_state)\ntree_reg.fit(X_prepared, log_y)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:32.509726Z","iopub.execute_input":"2022-06-13T10:45:32.509987Z","iopub.status.idle":"2022-06-13T10:45:32.578097Z","shell.execute_reply.started":"2022-06-13T10:45:32.509961Z","shell.execute_reply":"2022-06-13T10:45:32.577178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# score the DecisionTreeRegressor model\nscore_model(tree_reg)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:33.62952Z","iopub.execute_input":"2022-06-13T10:45:33.629797Z","iopub.status.idle":"2022-06-13T10:45:33.637733Z","shell.execute_reply.started":"2022-06-13T10:45:33.62977Z","shell.execute_reply":"2022-06-13T10:45:33.637079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nThe `DecisionTreeRegressor` model badly Overfit the data, we will skip it.\n","metadata":{}},{"cell_type":"markdown","source":"#### RandomForestRegressor model","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=100, random_state=random_state)\nrf.fit(X_prepared, log_y)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:36.595041Z","iopub.execute_input":"2022-06-13T10:45:36.595581Z","iopub.status.idle":"2022-06-13T10:45:38.700307Z","shell.execute_reply.started":"2022-06-13T10:45:36.595542Z","shell.execute_reply":"2022-06-13T10:45:38.699466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# score the RandomForestRegressor model\nscore_model(rf)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:38.702048Z","iopub.execute_input":"2022-06-13T10:45:38.702392Z","iopub.status.idle":"2022-06-13T10:45:38.749595Z","shell.execute_reply.started":"2022-06-13T10:45:38.702348Z","shell.execute_reply":"2022-06-13T10:45:38.748683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### XGBRegressor model","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\n\nxgb_reg = xgb.XGBRegressor(seed=random_state)\nxgb_reg.fit(X_prepared, log_y)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:43.983955Z","iopub.execute_input":"2022-06-13T10:45:43.984278Z","iopub.status.idle":"2022-06-13T10:45:44.89305Z","shell.execute_reply.started":"2022-06-13T10:45:43.984247Z","shell.execute_reply":"2022-06-13T10:45:44.89232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# score the XGBRegressor model\nscore_model(xgb_reg)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:45.075539Z","iopub.execute_input":"2022-06-13T10:45:45.076341Z","iopub.status.idle":"2022-06-13T10:45:45.088792Z","shell.execute_reply.started":"2022-06-13T10:45:45.076299Z","shell.execute_reply":"2022-06-13T10:45:45.087917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Better Evaluation Using Cross-Validation\n\n**cross-validation** allows you to get not only an estimate of the performance of your\nmodel, but also a measure of how precise this estimate is (i.e., its standard\ndeviation). ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(xgb_reg, X_prepared, log_y,\n                         scoring=\"neg_mean_squared_error\", cv=10)\n# Scikit-Learn’s cross-validation features expect a utility function \n# (greater is better) rather than a cost function (lower is better)\nxgb_scores = np.sqrt(-scores)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:50.389742Z","iopub.execute_input":"2022-06-13T10:45:50.390092Z","iopub.status.idle":"2022-06-13T10:45:58.544702Z","shell.execute_reply.started":"2022-06-13T10:45:50.390055Z","shell.execute_reply":"2022-06-13T10:45:58.544047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(xgb_scores)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:58.548735Z","iopub.execute_input":"2022-06-13T10:45:58.551055Z","iopub.status.idle":"2022-06-13T10:45:58.559519Z","shell.execute_reply.started":"2022-06-13T10:45:58.549488Z","shell.execute_reply":"2022-06-13T10:45:58.558927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(rf, X_prepared, log_y,\n                         scoring=\"neg_mean_squared_error\", cv=10)\n# Scikit-Learn’s cross-validation features expect a utility function \n# (greater is better) rather than a cost function (lower is better)\nrf_scores = np.sqrt(-scores)\ndisplay_scores(rf_scores)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:45:58.560388Z","iopub.execute_input":"2022-06-13T10:45:58.56093Z","iopub.status.idle":"2022-06-13T10:46:16.950157Z","shell.execute_reply.started":"2022-06-13T10:45:58.560898Z","shell.execute_reply":"2022-06-13T10:46:16.948951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`RandomForestRegressor` has much better scores, so we will stick with it.","metadata":{}},{"cell_type":"markdown","source":"### Fine-Tune Your Model\n\n#### Random Search","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import RandomizedSearchCV\n# from scipy.stats import randint\n\n# random_grid = {'bootstrap': [True, False],\n#               'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n#               'max_features': ['auto', 'sqrt'],\n#               'min_samples_leaf': [1, 2, 4],\n#               'min_samples_split': [2, 5, 10],\n#               'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n\n# rnd_search = RandomizedSearchCV(rf, param_distributions=random_grid,\n#                                 n_iter=100, cv=5, scoring='neg_mean_squared_error', \n#                                 random_state=random_state, verbose=1, n_jobs=-1)\n\n\n\n# rnd_search.fit(X_prepared, log_y)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T07:40:42.735087Z","iopub.execute_input":"2022-06-13T07:40:42.735567Z","iopub.status.idle":"2022-06-13T07:40:42.739589Z","shell.execute_reply.started":"2022-06-13T07:40:42.735515Z","shell.execute_reply":"2022-06-13T07:40:42.738933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the score of each hyperparameter combination tested during the random search:","metadata":{}},{"cell_type":"code","source":"# cvres = rnd_search.cv_results_\n# for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n#     print(np.sqrt(-mean_score), params)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T07:40:42.741052Z","iopub.execute_input":"2022-06-13T07:40:42.74148Z","iopub.status.idle":"2022-06-13T07:40:42.757827Z","shell.execute_reply.started":"2022-06-13T07:40:42.74143Z","shell.execute_reply":"2022-06-13T07:40:42.756981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rnd_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2022-06-13T07:40:42.75954Z","iopub.execute_input":"2022-06-13T07:40:42.76016Z","iopub.status.idle":"2022-06-13T07:40:42.769096Z","shell.execute_reply.started":"2022-06-13T07:40:42.760116Z","shell.execute_reply":"2022-06-13T07:40:42.76806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rnd_search.best_score_","metadata":{"execution":{"iopub.status.busy":"2022-06-13T07:40:42.77239Z","iopub.execute_input":"2022-06-13T07:40:42.774196Z","iopub.status.idle":"2022-06-13T07:40:42.782319Z","shell.execute_reply.started":"2022-06-13T07:40:42.774123Z","shell.execute_reply":"2022-06-13T07:40:42.78135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Grid Search\n\nWe will try to play arround the best `RandomSearchCV` values","metadata":{}},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# param_grid = {'n_estimators': np.arange(1500, 1700, 50),\n#              'min_samples_split': [1, 2, 3],\n#              'min_samples_leaf': [1, 2],\n#              'max_features': ['sqrt'],\n#              'max_depth': [18, 20, 22],\n#              'bootstrap': [False]}\n\n# # train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n# grid_search = GridSearchCV(rf, param_grid, cv=5,\n#                            scoring='neg_mean_squared_error',\n#                            return_train_score=True\n#                           ,n_jobs = -1, verbose = 1)\n# grid_search.fit(X_prepared, log_y)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T07:40:42.783754Z","iopub.execute_input":"2022-06-13T07:40:42.784624Z","iopub.status.idle":"2022-06-13T07:40:42.794759Z","shell.execute_reply.started":"2022-06-13T07:40:42.784576Z","shell.execute_reply":"2022-06-13T07:40:42.79371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at the score of each hyperparameter combination tested during the grid search:","metadata":{}},{"cell_type":"code","source":"# cvres = grid_search.cv_results_\n# for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n#     print(np.sqrt(-mean_score), params)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T07:40:42.797143Z","iopub.execute_input":"2022-06-13T07:40:42.798122Z","iopub.status.idle":"2022-06-13T07:40:42.806592Z","shell.execute_reply.started":"2022-06-13T07:40:42.79807Z","shell.execute_reply":"2022-06-13T07:40:42.805313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final_model = grid_search.best_estimator_\n# final_model","metadata":{"execution":{"iopub.status.busy":"2022-06-13T07:40:42.808189Z","iopub.execute_input":"2022-06-13T07:40:42.80918Z","iopub.status.idle":"2022-06-13T07:40:42.819886Z","shell.execute_reply.started":"2022-06-13T07:40:42.809122Z","shell.execute_reply":"2022-06-13T07:40:42.818788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model = RandomForestRegressor(bootstrap=False, max_depth=18, max_features='sqrt',\n                                    n_estimators=1650, random_state=10)\nfinal_model.fit(X_prepared, log_y)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:46:16.951808Z","iopub.execute_input":"2022-06-13T10:46:16.952076Z","iopub.status.idle":"2022-06-13T10:46:26.138802Z","shell.execute_reply.started":"2022-06-13T10:46:16.952032Z","shell.execute_reply":"2022-06-13T10:46:26.137779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# score the best model\nscore_model(final_model)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:46:26.139948Z","iopub.execute_input":"2022-06-13T10:46:26.140214Z","iopub.status.idle":"2022-06-13T10:46:26.87272Z","shell.execute_reply.started":"2022-06-13T10:46:26.140183Z","shell.execute_reply":"2022-06-13T10:46:26.871952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### A full pipeline with both preparation and prediction","metadata":{}},{"cell_type":"code","source":"full_pipeline = Pipeline([\n    (\"preparation\", preprocessor),\n    (\"model\", final_model)\n    ])\n\nfull_pipeline.fit(X, log_y)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:46:26.874124Z","iopub.execute_input":"2022-06-13T10:46:26.874606Z","iopub.status.idle":"2022-06-13T10:46:35.961299Z","shell.execute_reply.started":"2022-06-13T10:46:26.874559Z","shell.execute_reply":"2022-06-13T10:46:35.960193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Evaluate Your System on the Test Set\nSubmitting the data","metadata":{}},{"cell_type":"code","source":"# read the df\nX_test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n\n# save the model's predictions\n# the competition loss is RMSLE So, we need to log-transform y to train \n# and exp-transform the predictions\nfinal_predictions = np.exp(full_pipeline.predict(X_test))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:47:00.294501Z","iopub.execute_input":"2022-06-13T10:47:00.294814Z","iopub.status.idle":"2022-06-13T10:47:01.081838Z","shell.execute_reply.started":"2022-06-13T10:47:00.294773Z","shell.execute_reply":"2022-06-13T10:47:01.080815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The lines below shows how to save predictions in format used for competition scoring\n# Just uncomment them.\n\noutput = pd.DataFrame({'Id': X_test.Id, 'SalePrice': final_predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-06-13T10:47:02.722346Z","iopub.execute_input":"2022-06-13T10:47:02.72262Z","iopub.status.idle":"2022-06-13T10:47:02.737711Z","shell.execute_reply.started":"2022-06-13T10:47:02.722592Z","shell.execute_reply":"2022-06-13T10:47:02.736983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For any suggestions, please let me know in the comments! Thanks.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}