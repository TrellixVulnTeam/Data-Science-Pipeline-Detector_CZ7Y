{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prediction of House Prices Using Advanced Regression Methods","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://miro.medium.com/max/1400/0*YMZOAO8QE4bZ4_Rk.jpg\"  Width=\"800\">","metadata":{}},{"cell_type":"markdown","source":"Prediction of a house price can be a quite difficult task to achieve, as there are numerous amount of features that may affect the price of a house. Even though some of the house characteristics may have a greater effect on the price of a house, ones that have a weaker effect still needs to be considered as the determination of the price is a crucial process, especially for real estate agents and customers. \n\nIn this notebook a detailed exploratory analysis of the house price data is going to be made. After that, the raw data is going to be cleaned and an advanced regression model is going to be constructed using the provided train dataset. Lastly, the constructed model is going to be applied on the test dataset to predict the prices of given houses.\n\nAny feedback for this kernel is appreciated and please feel free to <b>upvote</b> or leave a <b>comment</b> if you liked the work or if you want to criticize.","metadata":{}},{"cell_type":"markdown","source":"## 1. Importing Libraries and Datasets","metadata":{}},{"cell_type":"code","source":"#Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import r2_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Lasso\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import norm, skew\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import RegressorMixin\nfrom sklearn.base import TransformerMixin\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:35.0304Z","iopub.execute_input":"2022-06-15T06:45:35.032723Z","iopub.status.idle":"2022-06-15T06:45:37.111462Z","shell.execute_reply.started":"2022-06-15T06:45:35.030782Z","shell.execute_reply":"2022-06-15T06:45:37.110472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing the datasets (1 for training and 1 for predicting)\ntrain = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.113362Z","iopub.execute_input":"2022-06-15T06:45:37.113821Z","iopub.status.idle":"2022-06-15T06:45:37.187733Z","shell.execute_reply.started":"2022-06-15T06:45:37.113779Z","shell.execute_reply":"2022-06-15T06:45:37.18672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is split into two parts, one of them is training data and the other one is testing data. We are going to use the train data to train our models and the test data to predict the house prices.","metadata":{}},{"cell_type":"markdown","source":"## 2. Glimpse of the Dataset","metadata":{}},{"cell_type":"code","source":"#Train data overview (first five rows)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.189826Z","iopub.execute_input":"2022-06-15T06:45:37.190329Z","iopub.status.idle":"2022-06-15T06:45:37.226303Z","shell.execute_reply.started":"2022-06-15T06:45:37.190294Z","shell.execute_reply":"2022-06-15T06:45:37.225342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test data overview (first five rows)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.2279Z","iopub.execute_input":"2022-06-15T06:45:37.228378Z","iopub.status.idle":"2022-06-15T06:45:37.25351Z","shell.execute_reply.started":"2022-06-15T06:45:37.228346Z","shell.execute_reply":"2022-06-15T06:45:37.252374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first difference that catches our eye is that the test data doesn't have a SalePrice column, as it is the dataset that we are going to apply the model that we will construct. Also, we can see that there are numerous amount of columns (doesn't even fit the cell), which we will inspect deeper in the upcoming section.","metadata":{}},{"cell_type":"markdown","source":"Now, we will check the size of the datasets before we inspect deeper.","metadata":{}},{"cell_type":"code","source":"print (\"The shape of the train data is (row, column):\"+ str(train.shape))\nprint (\"The shape of the test data is (row, column):\"+ str(test.shape))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.254691Z","iopub.execute_input":"2022-06-15T06:45:37.255256Z","iopub.status.idle":"2022-06-15T06:45:37.264145Z","shell.execute_reply.started":"2022-06-15T06:45:37.255218Z","shell.execute_reply":"2022-06-15T06:45:37.263117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The sizes of the datasets shows us that we have 1460 houses to train the model and 1459 houses to predict their prices. As we have mentioned, train dataset have one extra column which is the the SalePrice of the houses.","metadata":{}},{"cell_type":"markdown","source":"Next, we will check what these 81 features of houses are and whether they have missing values.","metadata":{}},{"cell_type":"code","source":"#More information on train data\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.265425Z","iopub.execute_input":"2022-06-15T06:45:37.266317Z","iopub.status.idle":"2022-06-15T06:45:37.299327Z","shell.execute_reply.started":"2022-06-15T06:45:37.266279Z","shell.execute_reply":"2022-06-15T06:45:37.298325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#More information on test data\ntest.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.300576Z","iopub.execute_input":"2022-06-15T06:45:37.301061Z","iopub.status.idle":"2022-06-15T06:45:37.321623Z","shell.execute_reply.started":"2022-06-15T06:45:37.301015Z","shell.execute_reply":"2022-06-15T06:45:37.320628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Column Labels: (taken from Data fields of the competition page)\n- **SalePrice**: The property's sale price in dollars. This is the target variable that you're trying to predict.\n- **MSSubClass**: The building class\n- **MSZoning**: The general zoning classification\n- **LotFrontage**: Linear feet of street connected to property\n- **LotArea**: Lot size in square feet\n- **Street**: Type of road access\n- **Alley**: Type of alley access\n- **LotShape**: General shape of property\n- **LandContour**: Flatness of the property\n- **Utilities**: Type of utilities available\n- **LotConfig**: Lot configuration\n- **LandSlope**: Slope of property\n- **Neighborhood**: Physical locations within Ames city limits\n- **Condition1**: Proximity to main road or railroad\n- **Condition2**: Proximity to main road or railroad (if a second is present)\n- **BldgType**: Type of dwelling\n- **HouseStyle**: Style of dwelling\n- **OverallQual**: Overall material and finish quality\n- **OverallCond**: Overall condition rating\n- **YearBuilt**: Original construction date\n- **YearRemodAdd**: Remodel date\n- **RoofStyle**: Type of roof\n- **RoofMatl**: Roof material\n- **Exterior1st**: Exterior covering on house\n- **Exterior2nd**: Exterior covering on house (if more than one material)\n- **MasVnrType**: Masonry veneer type\n- **MasVnrArea**: Masonry veneer area in square feet\n- **ExterQual**: Exterior material quality\n- **ExterCond**: Present condition of the material on the exterior\n- **Foundation**: Type of foundation\n- **BsmtQual**: Height of the basement\n- **BsmtCond**: General condition of the basement\n- **BsmtExposure**: Walkout or garden level basement walls\n- **BsmtFinType1**: Quality of basement finished area\n- **BsmtFinSF1**: Type 1 finished square feet\n- **BsmtFinType2**: Quality of second finished area (if present)\n- **BsmtFinSF2**: Type 2 finished square feet\n- **BsmtUnfSF**: Unfinished square feet of basement area\n- **TotalBsmtSF**: Total square feet of basement area\n- **Heating**: Type of heating\n- **HeatingQC**: Heating quality and condition\n- **CentralAir**: Central air conditioning\n- **Electrical**: Electrical system\n- **1stFlrSF**: First Floor square feet\n- **2ndFlrSF**: Second floor square feet\n- **LowQualFinSF**: Low quality finished square feet (all floors)\n- **GrLivArea**: Above grade (ground) living area square feet\n- **BsmtFullBath**: Basement full bathrooms\n- **BsmtHalfBath**: Basement half bathrooms\n- **FullBath**: Full bathrooms above grade\n- **HalfBath**: Half baths above grade\n- **Bedroom**: Number of bedrooms above basement level\n- **Kitchen**: Number of kitchens\n- **KitchenQual**: Kitchen quality\n- **TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)\n- **Functional**: Home functionality rating\n- **Fireplaces**: Number of fireplaces\n- **FireplaceQu**: Fireplace quality\n- **GarageType**: Garage location\n- **GarageYrBlt**: Year garage was built\n- **GarageFinish**: Interior finish of the garage\n- **GarageCars**: Size of garage in car capacity\n- **GarageArea**: Size of garage in square feet\n- **GarageQual**: Garage quality\n- **GarageCond**: Garage condition\n- **PavedDrive**: Paved driveway\n- **WoodDeckSF**: Wood deck area in square feet\n- **OpenPorchSF**: Open porch area in square feet\n- **EnclosedPorch**: Enclosed porch area in square feet\n- **3SsnPorch**: Three season porch area in square feet\n- **ScreenPorch**: Screen porch area in square feet\n- **PoolArea**: Pool area in square feet\n- **PoolQC**: Pool quality\n- **Fence**: Fence quality\n- **MiscFeature**: Miscellaneous feature not covered in other categories\n- **MiscVal**: $Value of miscellaneous feature\n- **MoSold**: Month Sold\n- **YrSold**: Year Sold\n- **SaleType**: Type of sale\n- **SaleCondition**: Condition of sale\n","metadata":{}},{"cell_type":"markdown","source":"For more detailed explanation of the columns please see data_description.txt.","metadata":{}},{"cell_type":"markdown","source":"Some basics stats of the numerical values of train data.","metadata":{}},{"cell_type":"code","source":"train.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.323003Z","iopub.execute_input":"2022-06-15T06:45:37.323326Z","iopub.status.idle":"2022-06-15T06:45:37.434866Z","shell.execute_reply.started":"2022-06-15T06:45:37.323297Z","shell.execute_reply":"2022-06-15T06:45:37.434134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Counts of different types\ntrain.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.435885Z","iopub.execute_input":"2022-06-15T06:45:37.436548Z","iopub.status.idle":"2022-06-15T06:45:37.445511Z","shell.execute_reply.started":"2022-06-15T06:45:37.436511Z","shell.execute_reply":"2022-06-15T06:45:37.444147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above informations tells us that there are 43 features of type object (probably categorical values), and 38 numerical values (35 integer types and 3 float types of features.)","metadata":{}},{"cell_type":"markdown","source":"## 3. Cleaning of Data","metadata":{}},{"cell_type":"markdown","source":"From the above information, we can see that we have unequal amount of data point in our datasets. Which implies that we probably have some missing values in our datasets. We have to replace or get rid of these missing values so that they dont result in any discrepancies in our calculations. Let's check how many NaN values are present in each data set.","metadata":{}},{"cell_type":"code","source":"#A function that calculates the percentage of missing data\ndef missing_percentage(df):\n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.449582Z","iopub.execute_input":"2022-06-15T06:45:37.450033Z","iopub.status.idle":"2022-06-15T06:45:37.456679Z","shell.execute_reply.started":"2022-06-15T06:45:37.449996Z","shell.execute_reply":"2022-06-15T06:45:37.455844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train data\nmissing_percentage(train)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.457902Z","iopub.execute_input":"2022-06-15T06:45:37.45833Z","iopub.status.idle":"2022-06-15T06:45:37.500195Z","shell.execute_reply.started":"2022-06-15T06:45:37.458265Z","shell.execute_reply":"2022-06-15T06:45:37.499054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test data\nmissing_percentage(test)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.501905Z","iopub.execute_input":"2022-06-15T06:45:37.502656Z","iopub.status.idle":"2022-06-15T06:45:37.536899Z","shell.execute_reply.started":"2022-06-15T06:45:37.502608Z","shell.execute_reply":"2022-06-15T06:45:37.536053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see while the train dataset have 19 fatures that has minimum 1 missing value, test dataset has 33 features with minimum 1 missing values. While some of the features have a considerable amount of missing values, some of them have negligible amounts. Therefore we will use different methods to handle different features that has missing values.","metadata":{}},{"cell_type":"markdown","source":"Now we will combine test and train datasets to do all the cleaning at once. We will drop the SalePrice as there isn't a SalePrice column in the test dataset.","metadata":{}},{"cell_type":"code","source":"salesprice = train['SalePrice']\nall_data = pd.concat((train, test)).reset_index(drop = True)\nall_data.drop(['SalePrice'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.53827Z","iopub.execute_input":"2022-06-15T06:45:37.538836Z","iopub.status.idle":"2022-06-15T06:45:37.567906Z","shell.execute_reply.started":"2022-06-15T06:45:37.538773Z","shell.execute_reply":"2022-06-15T06:45:37.567094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After reading the descripiton of the columns, we can figure out that some of the missing values are not meant to be missing values but rather a 'None' value for categorical features and '0' value for numerical features. Let's fix those columns first.","metadata":{}},{"cell_type":"code","source":"missing_value_0 = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath', 'BsmtHalfBath', 'GarageYrBlt','GarageArea','GarageCars','MasVnrArea']\n\nfor i in missing_value_0:\n    all_data[i] = all_data[i].fillna(0)\n    \nmissing_value_none = ['Alley','PoolQC','MiscFeature','Fence','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','MasVnrType']\n\nfor i in missing_value_none:\n    all_data[i] = all_data[i].fillna('None')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.569124Z","iopub.execute_input":"2022-06-15T06:45:37.569604Z","iopub.status.idle":"2022-06-15T06:45:37.58731Z","shell.execute_reply.started":"2022-06-15T06:45:37.569573Z","shell.execute_reply":"2022-06-15T06:45:37.586364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_percentage(all_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.58841Z","iopub.execute_input":"2022-06-15T06:45:37.588875Z","iopub.status.idle":"2022-06-15T06:45:37.62972Z","shell.execute_reply.started":"2022-06-15T06:45:37.588843Z","shell.execute_reply":"2022-06-15T06:45:37.628964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the fixing of 'None' and '0' values the number of features with missing values reduced to only 9 features. And there is only one column that has a considerable amount of missing values, which is LotFrontage. We are going to estimate LotFrontage according to LotArea and we are goingto use the modes of remaining values for the other missing values.","metadata":{}},{"cell_type":"markdown","source":"What we will do with the LotFrontage values is that we will compare the square root of the lot are to the existing lot frontage values to see the relationsip between them.","metadata":{}},{"cell_type":"code","source":"lot_frontage_data = all_data['LotFrontage']\nsqr_lot_area_data = np.sqrt(all_data['LotArea'])","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.631018Z","iopub.execute_input":"2022-06-15T06:45:37.631482Z","iopub.status.idle":"2022-06-15T06:45:37.636699Z","shell.execute_reply.started":"2022-06-15T06:45:37.631446Z","shell.execute_reply":"2022-06-15T06:45:37.635321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = sns.regplot(sqr_lot_area_data, lot_frontage_data)\nax.set_ylabel('LotFrontage')\nax.set_xlabel('LotAreaUnsq')\nax.set_title('Lot Area Squarerooted vs Lot Frontage')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:37.63811Z","iopub.execute_input":"2022-06-15T06:45:37.639145Z","iopub.status.idle":"2022-06-15T06:45:38.081708Z","shell.execute_reply.started":"2022-06-15T06:45:37.6391Z","shell.execute_reply":"2022-06-15T06:45:38.080732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The relationship between square root of lot area and the lot frontage looks like a pretty linear relationship. Therefore, we can estimate that the each lot has a shape of square and we can estimate that each lot frontage is equal to the square root of the lot area of each house.","metadata":{}},{"cell_type":"code","source":"all_data['LotFrontage'] = sqr_lot_area_data.round(2) ","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:38.083276Z","iopub.execute_input":"2022-06-15T06:45:38.083923Z","iopub.status.idle":"2022-06-15T06:45:38.090154Z","shell.execute_reply.started":"2022-06-15T06:45:38.083864Z","shell.execute_reply":"2022-06-15T06:45:38.088995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the remaining missing values we will use the most frequent values of the each column.","metadata":{}},{"cell_type":"code","source":"all_data['MSZoning'] = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nall_data['Utilities'] = all_data['Utilities'].fillna(all_data['Utilities'].mode()[0]) \nall_data['Functional'] = all_data['Functional'].fillna(all_data['Functional'].mode()[0]) \nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0]) \nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0]) \nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0]) \nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:38.091746Z","iopub.execute_input":"2022-06-15T06:45:38.092245Z","iopub.status.idle":"2022-06-15T06:45:38.126293Z","shell.execute_reply.started":"2022-06-15T06:45:38.092204Z","shell.execute_reply":"2022-06-15T06:45:38.125312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_percentage(all_data)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:38.127409Z","iopub.execute_input":"2022-06-15T06:45:38.127758Z","iopub.status.idle":"2022-06-15T06:45:38.172453Z","shell.execute_reply.started":"2022-06-15T06:45:38.127726Z","shell.execute_reply":"2022-06-15T06:45:38.171754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no missing values left and the dataset is cleaned now.","metadata":{}},{"cell_type":"markdown","source":"Lastly, we will separate the train and test datasets.","metadata":{}},{"cell_type":"code","source":"train = all_data[:1460]\ntest = all_data[1460:]\ntrain['SalePrice'] = salesprice","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:38.173734Z","iopub.execute_input":"2022-06-15T06:45:38.174852Z","iopub.status.idle":"2022-06-15T06:45:38.180131Z","shell.execute_reply.started":"2022-06-15T06:45:38.174815Z","shell.execute_reply":"2022-06-15T06:45:38.179392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Correlation and Visualization of Data","metadata":{}},{"cell_type":"markdown","source":"Before we visualize some of the house features first we need to check the correlation between each other and between our target value salesprice.","metadata":{}},{"cell_type":"markdown","source":"Let's check the correlation between each feature.","metadata":{}},{"cell_type":"code","source":"train.corr()","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:38.181299Z","iopub.execute_input":"2022-06-15T06:45:38.181929Z","iopub.status.idle":"2022-06-15T06:45:38.238082Z","shell.execute_reply.started":"2022-06-15T06:45:38.181868Z","shell.execute_reply":"2022-06-15T06:45:38.2371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As our dataset is huge we are not able to see the correlation between features clearly. A better wat to do that is creating a heatmap of all the features.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30, 20))\n# define the mask to set the values in the upper triangle to True\nmask = np.triu(np.ones_like(train.corr(), dtype=np.bool))\nheatmap = sns.heatmap(train.corr(), mask=mask, vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Heatmap of Train data', fontdict={'fontsize':18}, pad=16);","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:38.239293Z","iopub.execute_input":"2022-06-15T06:45:38.239655Z","iopub.status.idle":"2022-06-15T06:45:41.482942Z","shell.execute_reply.started":"2022-06-15T06:45:38.239624Z","shell.execute_reply":"2022-06-15T06:45:41.481962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's inspect the features that have high correlations. As we have implied before, LotArea and LotFrontage has a considerable amount of correlation, 0.91. We have used the correlation between these features to estimate the missing LotFrontage values with considering the lot as a square and squarerooting the LotArea. Another highly correlated pair of features are GarageArea and GarageCars. As we could guess they are almost directly effected by each other therefore the high correlation is expected. Some of other highly correlated paris are TotRmsAbvGrd and GrLivArea, 1stFlrSF and TotalBsmtSF, and SalePrice and OverallQual. Let's visualize them!","metadata":{}},{"cell_type":"code","source":"#TotRmsAbvGrd and GrLivArea regression plot\nax = sns.regplot(train['TotRmsAbvGrd'],train['GrLivArea'])\nax.set_ylabel('GrLivArea')\nax.set_xlabel('TotRmsAbvGrd')\nax.set_title('TotRmsAbvGrd vs GrLivArea')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:41.484279Z","iopub.execute_input":"2022-06-15T06:45:41.484676Z","iopub.status.idle":"2022-06-15T06:45:41.836596Z","shell.execute_reply.started":"2022-06-15T06:45:41.484637Z","shell.execute_reply":"2022-06-15T06:45:41.835715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GarageArea and GarageCars regression plot\nax = sns.regplot(train['GarageArea'],train['GarageCars'])\nax.set_ylabel('GarageCars')\nax.set_xlabel('GarageArea')\nax.set_title('GarageArea vs GarageCars')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:41.838215Z","iopub.execute_input":"2022-06-15T06:45:41.838814Z","iopub.status.idle":"2022-06-15T06:45:42.193709Z","shell.execute_reply.started":"2022-06-15T06:45:41.838769Z","shell.execute_reply":"2022-06-15T06:45:42.192849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#1stFlrSF and TotalBsmtSF regression plot\nax = sns.regplot(train['1stFlrSF'],train['TotalBsmtSF'])\nax.set_ylabel('TotalBsmtSF')\nax.set_xlabel('1stFlrSF')\nax.set_title('1stFlrSF vs TotalBsmtSF')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:42.194973Z","iopub.execute_input":"2022-06-15T06:45:42.195302Z","iopub.status.idle":"2022-06-15T06:45:42.532113Z","shell.execute_reply.started":"2022-06-15T06:45:42.195274Z","shell.execute_reply":"2022-06-15T06:45:42.53104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#OverallQual vs SalePrice regression plot\nax = sns.regplot(train['OverallQual'],train['SalePrice'])\nax.set_xlabel('OverallQual')\nax.set_ylabel('SalePrice')\nax.set_title('OverallQual vs SalePrice')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:42.533448Z","iopub.execute_input":"2022-06-15T06:45:42.534133Z","iopub.status.idle":"2022-06-15T06:45:42.862754Z","shell.execute_reply.started":"2022-06-15T06:45:42.534087Z","shell.execute_reply":"2022-06-15T06:45:42.861719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These features are also highly dependent on each other therefore the correlation is expected. We notice that in those highly correlated examples one of them have SalePrice, which is our target value. Now let's focus on our target value and try to find the features that effect the SalePrice the most.","metadata":{}},{"cell_type":"code","source":"#Correlation between SalePrice and other features\npd.DataFrame(train.corr()['SalePrice'].sort_values(ascending = False))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:42.86714Z","iopub.execute_input":"2022-06-15T06:45:42.867476Z","iopub.status.idle":"2022-06-15T06:45:42.889625Z","shell.execute_reply.started":"2022-06-15T06:45:42.867448Z","shell.execute_reply":"2022-06-15T06:45:42.888934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features that have a bigger correlation between SalePrice will have a bigger effect on the model we will create. We want those data to be as clean as possible to get the best outcome from our model. So, let's visualize the features with most correlation (>0.6) and check if they have any discontinuity or outliers.","metadata":{}},{"cell_type":"code","source":"#OverallQual vs SalePrice regression plot\nax = sns.regplot(train['OverallQual'],train['SalePrice'])\nax.set_xlabel('OverallQual')\nax.set_ylabel('SalePrice')\nax.set_title('OverallQual vs SalePrice')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:42.890613Z","iopub.execute_input":"2022-06-15T06:45:42.891458Z","iopub.status.idle":"2022-06-15T06:45:43.221993Z","shell.execute_reply.started":"2022-06-15T06:45:42.89142Z","shell.execute_reply":"2022-06-15T06:45:43.221001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a categorical variable and therefore dropping outliers wouldn't effect our result too much. Also, threre aren't any crucial outliers.","metadata":{}},{"cell_type":"code","source":"#GrLivArea vs SalePrice regression plot\nax = sns.regplot(train['GrLivArea'],train['SalePrice'])\nax.set_xlabel('GrLivArea')\nax.set_ylabel('SalePrice')\nax.set_title('GrLivArea vs SalePrice')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:43.22309Z","iopub.execute_input":"2022-06-15T06:45:43.223405Z","iopub.status.idle":"2022-06-15T06:45:43.567472Z","shell.execute_reply.started":"2022-06-15T06:45:43.223377Z","shell.execute_reply":"2022-06-15T06:45:43.566687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This one has 2 obvious outliers at the right bottom corner. We will get rid of them soon.","metadata":{}},{"cell_type":"code","source":"#GarageCars vs SalePrice regression plot\nax = sns.regplot(train['GarageCars'],train['SalePrice'])\nax.set_xlabel('GarageCars')\nax.set_ylabel('SalePrice')\nax.set_title('GarageCars vs SalePrice')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:43.568407Z","iopub.execute_input":"2022-06-15T06:45:43.569069Z","iopub.status.idle":"2022-06-15T06:45:43.935241Z","shell.execute_reply.started":"2022-06-15T06:45:43.569036Z","shell.execute_reply":"2022-06-15T06:45:43.934229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GarageArea vs SalePrice regression plot\nax = sns.regplot(train['GarageArea'],train['SalePrice'])\nax.set_xlabel('GarageArea')\nax.set_ylabel('SalePrice')\nax.set_title('GarageArea vs SalePrice')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:43.936396Z","iopub.execute_input":"2022-06-15T06:45:43.936799Z","iopub.status.idle":"2022-06-15T06:45:44.292527Z","shell.execute_reply.started":"2022-06-15T06:45:43.936769Z","shell.execute_reply":"2022-06-15T06:45:44.291452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This one seems to have 4 outliers on the right bottom corner.","metadata":{}},{"cell_type":"code","source":"#TotalBsmtSF vs SalePrice regression plot\nax = sns.regplot(train['TotalBsmtSF'],train['SalePrice'])\nax.set_xlabel('TotalBsmtSF')\nax.set_ylabel('SalePrice')\nax.set_title('TotalBsmtSF vs SalePrice')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:44.293639Z","iopub.execute_input":"2022-06-15T06:45:44.293997Z","iopub.status.idle":"2022-06-15T06:45:44.644701Z","shell.execute_reply.started":"2022-06-15T06:45:44.293964Z","shell.execute_reply":"2022-06-15T06:45:44.643655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is also an obvious outlier in this one.","metadata":{}},{"cell_type":"code","source":"#1stFlrSF vs SalePrice regression plot\nax = sns.regplot(train['1stFlrSF'],train['SalePrice'])\nax.set_xlabel('1stFlrSF')\nax.set_ylabel('SalePrice')\nax.set_title('1stFlrSF vs SalePrice')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:44.646049Z","iopub.execute_input":"2022-06-15T06:45:44.646776Z","iopub.status.idle":"2022-06-15T06:45:45.03016Z","shell.execute_reply.started":"2022-06-15T06:45:44.646709Z","shell.execute_reply":"2022-06-15T06:45:45.027772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is also an obvious outlier in this one. Let's get rid of them all together.","metadata":{}},{"cell_type":"code","source":"train = train[train.GrLivArea < 4500]\ntrain = train[train.GarageArea < 1250]\ntrain = train[train.TotalBsmtSF < 3500]\ntrain = train[train['1stFlrSF'] < 3500]\ntrain.reset_index(drop = True, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.031224Z","iopub.execute_input":"2022-06-15T06:45:45.031649Z","iopub.status.idle":"2022-06-15T06:45:45.045945Z","shell.execute_reply.started":"2022-06-15T06:45:45.031615Z","shell.execute_reply":"2022-06-15T06:45:45.044964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This way we got rid of some outliers that we don't want in our dataset.","metadata":{}},{"cell_type":"markdown","source":"## 5. Machine Learning Preprocessing","metadata":{}},{"cell_type":"markdown","source":"We have to do some preprocessing before we apply the machine learning methods. We need numbers instead of categorical values, as machine learning algorithms can not understand categorical values. Also we will have to split our train dataset for doing consistency checks and after that we will standardize the data.","metadata":{}},{"cell_type":"markdown","source":"Let's start with dropping unnecessary variables.","metadata":{}},{"cell_type":"code","source":"#Save old train dataframe in case we need it again \nold_train = train.copy()\nold_test= test.copy()\nsalesprice = train['SalePrice']\n\n#Dropping unnecessary features\ntrain.drop(['Id'],axis=1, inplace=True)\ntest.drop(['Id'],axis=1, inplace=True)\ntrain.drop(['SalePrice'],axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.046949Z","iopub.execute_input":"2022-06-15T06:45:45.047267Z","iopub.status.idle":"2022-06-15T06:45:45.060884Z","shell.execute_reply.started":"2022-06-15T06:45:45.047239Z","shell.execute_reply":"2022-06-15T06:45:45.059972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, to get rid of categorical values we will assign dummy values to categorical variables. But, to get the same number of columns for the train and test data we will merge these two datasets and get dummies for the whole dataset.","metadata":{}},{"cell_type":"code","source":"#Merge train and test data\nall_data = pd.concat((train, test)).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.062563Z","iopub.execute_input":"2022-06-15T06:45:45.063205Z","iopub.status.idle":"2022-06-15T06:45:45.087824Z","shell.execute_reply.started":"2022-06-15T06:45:45.063161Z","shell.execute_reply":"2022-06-15T06:45:45.086901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Assign dummy values\nall_data_dummies = pd.get_dummies(all_data).reset_index(drop=True)\nall_data_dummies.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.089031Z","iopub.execute_input":"2022-06-15T06:45:45.089354Z","iopub.status.idle":"2022-06-15T06:45:45.297932Z","shell.execute_reply.started":"2022-06-15T06:45:45.089327Z","shell.execute_reply":"2022-06-15T06:45:45.297197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split the all_data dataset\ntrain = all_data_dummies[:1456]\ntest = all_data_dummies[1456:]","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.298941Z","iopub.execute_input":"2022-06-15T06:45:45.299292Z","iopub.status.idle":"2022-06-15T06:45:45.304729Z","shell.execute_reply.started":"2022-06-15T06:45:45.299263Z","shell.execute_reply":"2022-06-15T06:45:45.303589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will split our train dataset for future validation and then standardize each dataset.","metadata":{}},{"cell_type":"code","source":"X = train\ny = salesprice\n\n#Splitting the train data \nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.306464Z","iopub.execute_input":"2022-06-15T06:45:45.30703Z","iopub.status.idle":"2022-06-15T06:45:45.321778Z","shell.execute_reply.started":"2022-06-15T06:45:45.306985Z","shell.execute_reply":"2022-06-15T06:45:45.3208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lastly, we will scale our datasets.","metadata":{}},{"cell_type":"code","source":"#X_train = preprocessing.StandardScaler().fit(X_train).transform(X_train)\n#X_test = preprocessing.StandardScaler().fit(X_test).transform(X_test)\n#test_scaled = preprocessing.StandardScaler().fit(test).transform(test)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.323049Z","iopub.execute_input":"2022-06-15T06:45:45.323411Z","iopub.status.idle":"2022-06-15T06:45:45.327721Z","shell.execute_reply.started":"2022-06-15T06:45:45.32337Z","shell.execute_reply":"2022-06-15T06:45:45.326893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Modelling the Data","metadata":{}},{"cell_type":"markdown","source":"### Simple Approach","metadata":{}},{"cell_type":"markdown","source":"For a simple approach we will fit a linear regression model to the dataset, using the vairable with most correlation. We will try the most correlated data GrLivArea. We won't use OverallQual as it is basically a categorical value and its hard to create regression for it.","metadata":{}},{"cell_type":"code","source":"regr = linear_model.LinearRegression()\ntrain_x = np.asanyarray(X_train['GrLivArea'])\ntrain_y = np.asanyarray(y_train)\nregr.fit(train_x.reshape(-1, 1), train_y)\n# The coefficients\nprint ('Coefficients: ', regr.coef_)\nprint ('Intercept: ',regr.intercept_)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.328815Z","iopub.execute_input":"2022-06-15T06:45:45.329564Z","iopub.status.idle":"2022-06-15T06:45:45.34896Z","shell.execute_reply.started":"2022-06-15T06:45:45.329531Z","shell.execute_reply":"2022-06-15T06:45:45.347939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GrLivArea vs SalePrice regression plot\nax = sns.regplot(train_x,train_y)\nax.set_xlabel('GrLivArea')\nax.set_ylabel('SalePrice')\nax.set_title('GrLivArea vs SalePrice')","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.350341Z","iopub.execute_input":"2022-06-15T06:45:45.350775Z","iopub.status.idle":"2022-06-15T06:45:45.622603Z","shell.execute_reply.started":"2022-06-15T06:45:45.350734Z","shell.execute_reply":"2022-06-15T06:45:45.621469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"On plot the linear regression looks like a good approximation. Let's evaluate the model with comparing to the test values.","metadata":{}},{"cell_type":"code","source":"test_x = np.asanyarray(X_test['GrLivArea'])\ntest_y = np.asanyarray(y_test)\ntest_y_ = regr.predict(test_x.reshape(-1, 1))\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y , test_y_) )","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.62455Z","iopub.execute_input":"2022-06-15T06:45:45.62531Z","iopub.status.idle":"2022-06-15T06:45:45.633774Z","shell.execute_reply.started":"2022-06-15T06:45:45.625261Z","shell.execute_reply":"2022-06-15T06:45:45.632703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"An R2-score of 0.53 shows us that linear regression with GrLivArea is not a good approxiamtion method to calculate SalePrice values. Therefore we need a better approach to approximate our target value.","metadata":{}},{"cell_type":"markdown","source":"### Multiple Linear Regression","metadata":{}},{"cell_type":"markdown","source":"Let's use each and every feature in our train dataset to construct a linear model and to predict the saleprice out our houses.","metadata":{}},{"cell_type":"code","source":"regr.fit(X_train, y_train)\n# The coefficients\nprint ('Coefficients: ', regr.coef_)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.635193Z","iopub.execute_input":"2022-06-15T06:45:45.635807Z","iopub.status.idle":"2022-06-15T06:45:45.708724Z","shell.execute_reply.started":"2022-06-15T06:45:45.635761Z","shell.execute_reply":"2022-06-15T06:45:45.707782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_x = np.asanyarray(X_test)\ntest_y = np.asanyarray(y_test)\ntest_y_ = regr.predict(X_test)\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y , test_y_) )","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.709981Z","iopub.execute_input":"2022-06-15T06:45:45.7106Z","iopub.status.idle":"2022-06-15T06:45:45.732303Z","shell.execute_reply.started":"2022-06-15T06:45:45.710557Z","shell.execute_reply":"2022-06-15T06:45:45.731315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These scores shows us that these models perform very poorly therefore some improvements need to be done both to the datasets and the model we're going to use.","metadata":{}},{"cell_type":"markdown","source":"## Advanced Approach","metadata":{}},{"cell_type":"markdown","source":"First of all we are going to do some feature engineering to show how it effects the performance of our final model, copared to previous simple models.","metadata":{}},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"all_data","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.733974Z","iopub.execute_input":"2022-06-15T06:45:45.734626Z","iopub.status.idle":"2022-06-15T06:45:45.832496Z","shell.execute_reply.started":"2022-06-15T06:45:45.734572Z","shell.execute_reply":"2022-06-15T06:45:45.831331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create some new features with combining other features to add some context to our dataset.","metadata":{}},{"cell_type":"code","source":"all_data = all_data.drop([ 'Utilities','Street', 'PoolQC',], axis=1)\n\n# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\nall_data['Total_Bathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) +\n                               all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))\n\nall_data['Total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] +\n                              all_data['EnclosedPorch'] + all_data['ScreenPorch'] +\n                              all_data['WoodDeckSF'])\n\nall_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.838424Z","iopub.execute_input":"2022-06-15T06:45:45.841582Z","iopub.status.idle":"2022-06-15T06:45:45.869628Z","shell.execute_reply.started":"2022-06-15T06:45:45.841516Z","shell.execute_reply":"2022-06-15T06:45:45.868971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.870953Z","iopub.execute_input":"2022-06-15T06:45:45.871366Z","iopub.status.idle":"2022-06-15T06:45:45.889209Z","shell.execute_reply.started":"2022-06-15T06:45:45.871322Z","shell.execute_reply":"2022-06-15T06:45:45.888325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will apply LabelEncoder to our categorical values.","metadata":{}},{"cell_type":"code","source":"cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC',  'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.891727Z","iopub.execute_input":"2022-06-15T06:45:45.892118Z","iopub.status.idle":"2022-06-15T06:45:45.977157Z","shell.execute_reply.started":"2022-06-15T06:45:45.892084Z","shell.execute_reply":"2022-06-15T06:45:45.976148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another improvement we can do is fixing the skewness of our numerical values.","metadata":{}},{"cell_type":"code","source":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:45.978311Z","iopub.execute_input":"2022-06-15T06:45:45.978638Z","iopub.status.idle":"2022-06-15T06:45:46.018114Z","shell.execute_reply.started":"2022-06-15T06:45:45.978609Z","shell.execute_reply":"2022-06-15T06:45:46.017159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:46.01946Z","iopub.execute_input":"2022-06-15T06:45:46.019817Z","iopub.status.idle":"2022-06-15T06:45:46.062221Z","shell.execute_reply.started":"2022-06-15T06:45:46.019786Z","shell.execute_reply":"2022-06-15T06:45:46.061232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creating our dummy variables just like before.","metadata":{}},{"cell_type":"code","source":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:46.063485Z","iopub.execute_input":"2022-06-15T06:45:46.064501Z","iopub.status.idle":"2022-06-15T06:45:46.097817Z","shell.execute_reply.started":"2022-06-15T06:45:46.064459Z","shell.execute_reply":"2022-06-15T06:45:46.096803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = all_data[:1456]\ntest = all_data[1456:]\ny_train = np.log1p(salesprice)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:46.099314Z","iopub.execute_input":"2022-06-15T06:45:46.099632Z","iopub.status.idle":"2022-06-15T06:45:46.105105Z","shell.execute_reply.started":"2022-06-15T06:45:46.099605Z","shell.execute_reply":"2022-06-15T06:45:46.10397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Constructing Our Model","metadata":{}},{"cell_type":"markdown","source":"Before we construct our advanced model let's define our validation fucntion first.","metadata":{}},{"cell_type":"code","source":"#Validation function\nn_folds = 10\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:46.106362Z","iopub.execute_input":"2022-06-15T06:45:46.106694Z","iopub.status.idle":"2022-06-15T06:45:46.116903Z","shell.execute_reply.started":"2022-06-15T06:45:46.106666Z","shell.execute_reply":"2022-06-15T06:45:46.115946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we are going to construct 6 different models which are LASSO Regression, Kernel Ridge Regression, Elastic Net Regression, Gradient Boosting Regression, XGBoost and LightGBM. After that we are going to check their rmse scores and use the one with the best score. We will use RobustScaler() in our models because this dataset may be sensitive to outliers.","metadata":{}},{"cell_type":"code","source":"#LASSO Regression\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\n#Kernel Ridge Regression\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n#Elastic Net Regression\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n\n#Gradient Boosting Regression\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\n#XGBoost\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\n#LightGBM\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:46.118088Z","iopub.execute_input":"2022-06-15T06:45:46.118445Z","iopub.status.idle":"2022-06-15T06:45:46.129348Z","shell.execute_reply.started":"2022-06-15T06:45:46.118414Z","shell.execute_reply":"2022-06-15T06:45:46.128522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the score of each model.","metadata":{}},{"cell_type":"code","source":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:45:46.13071Z","iopub.execute_input":"2022-06-15T06:45:46.131328Z","iopub.status.idle":"2022-06-15T06:50:46.71627Z","shell.execute_reply.started":"2022-06-15T06:45:46.131286Z","shell.execute_reply":"2022-06-15T06:50:46.715337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:50:46.717364Z","iopub.execute_input":"2022-06-15T06:50:46.718162Z","iopub.status.idle":"2022-06-15T06:50:46.72206Z","shell.execute_reply.started":"2022-06-15T06:50:46.718125Z","shell.execute_reply":"2022-06-15T06:50:46.721335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:50:46.72328Z","iopub.execute_input":"2022-06-15T06:50:46.723759Z","iopub.status.idle":"2022-06-15T06:50:58.771084Z","shell.execute_reply.started":"2022-06-15T06:50:46.723728Z","shell.execute_reply":"2022-06-15T06:50:58.77022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ENet.fit(train,y_train)\nENet_train_pred = ENet.predict(train)\nENet_pred = np.expm1(ENet.predict(test))\nprint(rmsle(y_train, ENet_train_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:50:58.772703Z","iopub.execute_input":"2022-06-15T06:50:58.773436Z","iopub.status.idle":"2022-06-15T06:50:59.049971Z","shell.execute_reply.started":"2022-06-15T06:50:58.773391Z","shell.execute_reply":"2022-06-15T06:50:59.048683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test))\nprint(rmsle(y_train, lgb_train_pred))","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:50:59.051822Z","iopub.execute_input":"2022-06-15T06:50:59.052537Z","iopub.status.idle":"2022-06-15T06:50:59.596322Z","shell.execute_reply.started":"2022-06-15T06:50:59.052483Z","shell.execute_reply":"2022-06-15T06:50:59.595502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After checking the scores we can see that Elastic Net Regression is the best scoring model therefore we are going to use it to predout our price values.","metadata":{}},{"cell_type":"code","source":"y_hat = lgb_pred","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:50:59.597861Z","iopub.execute_input":"2022-06-15T06:50:59.598523Z","iopub.status.idle":"2022-06-15T06:50:59.60307Z","shell.execute_reply.started":"2022-06-15T06:50:59.598485Z","shell.execute_reply":"2022-06-15T06:50:59.602165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Appending Predictions","metadata":{}},{"cell_type":"code","source":"sale_pred = []\nfor k in range(len(y_hat)):\n    sale_pred.append(y_hat[k])","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:50:59.604158Z","iopub.execute_input":"2022-06-15T06:50:59.604449Z","iopub.status.idle":"2022-06-15T06:50:59.614606Z","shell.execute_reply.started":"2022-06-15T06:50:59.604422Z","shell.execute_reply":"2022-06-15T06:50:59.613653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df = pd.DataFrame(\n    {'Id': old_test['Id'].values.tolist(),\n     'SalePrice': sale_pred\n    })","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:50:59.615535Z","iopub.execute_input":"2022-06-15T06:50:59.616122Z","iopub.status.idle":"2022-06-15T06:50:59.627057Z","shell.execute_reply.started":"2022-06-15T06:50:59.616088Z","shell.execute_reply":"2022-06-15T06:50:59.626378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:50:59.627922Z","iopub.execute_input":"2022-06-15T06:50:59.628543Z","iopub.status.idle":"2022-06-15T06:50:59.644029Z","shell.execute_reply.started":"2022-06-15T06:50:59.628513Z","shell.execute_reply":"2022-06-15T06:50:59.643293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_df.to_csv('predictions.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-15T06:50:59.645019Z","iopub.execute_input":"2022-06-15T06:50:59.645626Z","iopub.status.idle":"2022-06-15T06:50:59.655155Z","shell.execute_reply.started":"2022-06-15T06:50:59.645597Z","shell.execute_reply":"2022-06-15T06:50:59.653982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you have come this far, Congratulations!!\n\n### If this notebook helped you in any way or you liked it, please upvote and/or leave a comment!! :) ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}