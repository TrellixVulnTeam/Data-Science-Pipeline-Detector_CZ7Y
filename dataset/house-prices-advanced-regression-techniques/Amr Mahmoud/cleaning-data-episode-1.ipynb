{"cells":[{"metadata":{"_uuid":"9fb7fec08d046e64fcb1c44c7c7d32146f1b017f"},"cell_type":"markdown","source":"![Imgur](https://i.imgur.com/pXOeQKf.png)"},{"metadata":{"_uuid":"38838e63eb550a3bb889366bc08b7d0d8defdd78"},"cell_type":"markdown","source":"**Intro**\n\nDid you know that all data scientists spend 80% of there project time cleaning there data? "},{"metadata":{"_uuid":"32c40c6ecd2500bca0307ffeaf14b6fd4b981f1c"},"cell_type":"markdown","source":"![Imgur](https://i.imgur.com/QtHdbod.jpg)"},{"metadata":{"_uuid":"71637b1800b619026752cd5101732a8f02bd9dd1"},"cell_type":"markdown","source":"yes, not modeling or optimization , it is cleaning data. so you must be careful in cleaning your data because it is the building block of your model. in this tutorial I will discuss the first episode of cleaning data simply so as any one can understand. Let's go "},{"metadata":{"_uuid":"fb0c749b4124118fa2d8199184a21b2f761c8dbe"},"cell_type":"markdown","source":"**Topics covered in this tutorials**\n\n1. Measuring of Divergence\n2. Missing values\n3. Outliers Acquiescence"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#invite people for the Kaggle party\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fad727714bf28309a01ddb3bf636e70a16088b38"},"cell_type":"markdown","source":"**Importing our Data Files**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#read the file position\ntrain=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efca3342c2bb782a006b88b50e7f2a1d7d4c64ed"},"cell_type":"markdown","source":"**Discover you Data what is look like??**"},{"metadata":{"trusted":true,"_uuid":"5fed5712aeb7e176d32fd9e095f9d6a9c87c52e7"},"cell_type":"code","source":"train.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d72bbc22fbab31a1e2bd7ea6d1f8fe5348f7ecd"},"cell_type":"code","source":"test.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7eb092f4032310a00658cd9483a1d1a3497cb3aa"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac132ece7c166211c07bc4de3869fc4400de64f0"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"204bfc2403e5cb448152e796a2477b8d8d224d24"},"cell_type":"markdown","source":"there is one missed column which is the y we want to predict"},{"metadata":{"trusted":true,"_uuid":"651f933b6ee80fbe0b9194710e4b020655487bca"},"cell_type":"code","source":"len(train.columns)#get number of variables","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b82b09847cc86b6fb4800b5c473e7621f103f54"},"cell_type":"code","source":"train.info()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8817a6027672d7436d2585ef01676610249601b4"},"cell_type":"code","source":"#data types of columns\ntrain.get_dtype_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c082998cc0cdf4c17c173495c13526c2471122bb"},"cell_type":"code","source":"train.describe()\ntrain.describe(include=['object'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1849ebc6dcca5a22c63389ca47586532642180ea"},"cell_type":"markdown","source":"**Exploratory Data Analysis and Cleaning our Data**\n\ncleaning is very important since data without cleaning produce very inaccurate models and a lot of randomness in our work.\ncleaning data will include alot of steps. in this tutorial I will include some of them with a lot of examinations.Before any work of your data , it is important to clean it since any data which model don't like will produce an error.\n\n"},{"metadata":{"_uuid":"45fbdd5fbb962a40ff5b587859ab782dca56e1e7"},"cell_type":"markdown","source":"\n    "},{"metadata":{"_uuid":"eb2174009a7f7acd5274cc589f98894624fdc413"},"cell_type":"markdown","source":"** Measuring of Divergence**"},{"metadata":{"_uuid":"8d0f72733ac743a6b98240de12c5127794e2cda9"},"cell_type":"markdown","source":"![Imgur](https://i.imgur.com/syb5i0F.png)"},{"metadata":{"_uuid":"522bc984c17507d36994fb34a0d006bbcd08087f"},"cell_type":"markdown","source":"**Skewness**\n\nIt is the degree of distortion from the symmetrical bell curve or the normal distribution. It measures the lack of symmetry in data distribution. It differentiates extreme values in one versus the other tail. A symmetrical distribution will have a skewness of 0.\n\nSo, when is the skewness too much? The rule of thumb seems to be:\n\n* If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n\n* If the skewness is between -1 and – 0.5 or between 0.5 and 1, the data are moderately skewed.\n\n* If the skewness is less than -1 or greater than 1, the data are highly skewed."},{"metadata":{"trusted":true,"_uuid":"6934a07bfe8f82270d0997dfcc71abcecd5a7650"},"cell_type":"code","source":"sns.distplot(train['SalePrice'], color=\"y\", kde=True)\nplt.title(\"Distribution of Sale Price\")\nplt.ylabel(\"Number of Occurences\")\nplt.xlabel(\"Sale Price\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"75fdb7d2832be03fe8e51829c8627f96bbbcfa3d"},"cell_type":"code","source":"#measung of symmetry\ntrain['SalePrice'].skew()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0f7a8ccadeb9f5d7d0f33627906d33222564e9f"},"cell_type":"markdown","source":"Right skewed distribution\n\n"},{"metadata":{"_uuid":"4523d4f8f98b834a308179e00cda51759e99b464"},"cell_type":"markdown","source":"**Kurtosis**\nKurtosis is all about the tails of the distribution — not the peakedness or flatness. It is used to describe the extreme values in one versus the other tail. It is actually the measure of outliers present in the distribution.\n\n* High kurtosis\n\nin a data set is an indicator that data has heavy tails or outliers. If there is a high kurtosis, then, we need to investigate why do we have so many outliers. It indicates a lot of things, maybe wrong data entry or other things. Investigate!.\n\n* Low kurtosis\n\nin a data set is an indicator that data has light tails or lack of outliers. If we get low kurtosis(too good to be true), then also we need to investigate and trim the dataset of unwanted results.\n\n* Mesokurtic:\n\nThis distribution has kurtosis statistic similar to that of the normal distribution. It means that the extreme values of the distribution are similar to that of a normal distribution characteristic. This definition is used so that the standard normal distribution has a kurtosis of three.\n\n* Leptokurtic\n\n(Kurtosis > 3): Distribution is longer, tails are fatter. Peak is higher and sharper than Mesokurtic, which means that data are heavy-tailed or profusion of outliers. Outliers stretch the horizontal axis of the histogram graph, which makes the bulk of the data appear in a narrow (“skinny”) vertical range, thereby giving the “skinniness” of a leptokurtic distribution. \n\n* Platykurtic:\n\n(Kurtosis < 3): Distribution is shorter, tails are thinner than the normal distribution. The peak is lower and broader than Mesokurtic, which means that data are light-tailed or lack of outliers. The reason for this is because the extreme values are less than that of the normal distribution."},{"metadata":{"trusted":true,"_uuid":"7e06de68ab863438d278616716b000946b48e93d"},"cell_type":"code","source":"#measure of peakness\ntrain['SalePrice'].kurt()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d657221d77ed6fb86f244a847994322f6213f2f6"},"cell_type":"markdown","source":"**leptokurtic** kurtosis is the type of kurtosis\n\n"},{"metadata":{"_uuid":"c3748232a2fb08756ff43149b52735c6ca76cc39"},"cell_type":"markdown","source":"**Missing values**"},{"metadata":{"_uuid":"f410054ddc13bdc1416d480b69d288b06bfc5f3a"},"cell_type":"markdown","source":"**Data Preparation = Data Cleansing + Feature Engineering Missing data\n**\n\nIn statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data."},{"metadata":{"_uuid":"4f2f73efcc7e834c4f43954c33e7ad4ff28903d5"},"cell_type":"markdown","source":"![Imgur](https://i.imgur.com/JmVq0Av.jpg)"},{"metadata":{"_uuid":"838a200dcfce4ea4293810ed3fddd17213af85be"},"cell_type":"markdown","source":"**Types of missed data **\n\n**1- missing completely at random(MCAR):**\n\nMissing value (y) neither depends on x nor y when data is missed due to random. that mean when there is no obvious cause for missing of the data.missed for unknown cause. \n\n**2- missing at random(MAR):**\n\nMissing value (y) depends on x, but not y when data missed due to a variable present in the data . ex: data is missed due to sex.ex: if the man is business man so their is higher probability that he will not enter salary.\n\n**3-missing not at random(MNAR):**\n\nThe probability of a missing value depends on the variable that is missing probability of missing occurs dependant on another missing value. ex: when the business man does't enter his income , so he will not classify his icome rate(high, medium , low)\n\n\n"},{"metadata":{"_uuid":"b873709ebbc806e18f2c1f6a122096b65032eb3b"},"cell_type":"markdown","source":"**why missing data make problems?**\n\n1-standard analysis assume that their are no missing data.\n\n2- ignoring missing data may lead to loss of power of analysis, biased\n"},{"metadata":{"_uuid":"f99332d569226b9b16d42573ce4600cae5d0b473"},"cell_type":"markdown","source":"**How to deal with missing data?**\n"},{"metadata":{"trusted":true,"_uuid":"57b91d61138c3197e9f0f3f99b33136bcb48e76b"},"cell_type":"code","source":"total=train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()*100/train.isnull().count()).sort_values(ascending=False)\n#types=train.dtypes\nDataMissing=pd.concat([total,percent],axis=1,keys=['total','percentage'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67577f6f938e3376bd41e1a29ae2ac2c235035c2"},"cell_type":"code","source":"#show missing values\nDataMissing","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6bcb4b30a72738825675921b3916a95953e3972"},"cell_type":"markdown","source":"if you take a look at the discription of the variables you will notice that this is actually not missin data , it means no item is found , no pool is found for example and so on for other variables so we will replace these values by 'None'."},{"metadata":{"trusted":true,"_uuid":"914b383aecb556340ced1d0ee6e21c6fa3ed80a2"},"cell_type":"code","source":"total=train.isnull().sum().sort_values(ascending=False)\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()*100/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8438b98474e999df00037c52ca4f115e9dfa0289"},"cell_type":"markdown","source":"In my opinion, it is always better to keep data than to discard it. Sometimes you can drop variables if the data is missing for more than 60% observations but only if that variable is insignificant.Imputation is always a preferred choice over dropping variables\n\n"},{"metadata":{"_uuid":"2124aa780edc3ae3e8948b10fea7e9f4eefe6d56"},"cell_type":"markdown","source":"So lets determine why the data is missed? well lets make a rule : Don't trust an algorythm depends on feature that 80% of its observations are lost .here are PoolQC,MiscFeature,Alley,Fence these variables anu imputation in their observaton values mea that volatin in the reult, it is n real varrable well the most useful method to deal with is Feature deletion."},{"metadata":{"_uuid":"b3a2b7489d4b3e06a0948f50d644ea25b60f57db"},"cell_type":"markdown","source":"**Handling columns with missing data**\n"},{"metadata":{"trusted":true,"_uuid":"3fed94a334d00256996ab5fbe80e88287899dbe7"},"cell_type":"code","source":"for col in ('FireplaceQu',\n            'Fence',\n            'Alley',\n            'MiscFeature',\n            'PoolQC',\n            'GarageType', \n            'GarageFinish', \n            'GarageQual', \n            'GarageCond'):\n    train[col]=train[col].fillna('None')\n    test[col]=test[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"049aede11c92748450c04d9f77cbebcd57d12d4b"},"cell_type":"markdown","source":"LotFrontage : we can fill in missing values by the median LotFrontage of the neighborhood.\n\n"},{"metadata":{"trusted":true,"_uuid":"bc744edbd3c3e226a2f98b10442d99ba0d6cc39e"},"cell_type":"code","source":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ntrain[\"LotFrontage\"] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\ntest[\"LotFrontage\"] = test.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nfor col in ('GarageYrBlt', \n            'GarageArea', \n            'GarageCars',\n            'BsmtFinSF1',\n            'BsmtFinSF2',\n            'BsmtUnfSF',\n            'TotalBsmtSF', \n            'BsmtFullBath',\n            'BsmtHalfBath',\n            'BsmtQual', \n            'BsmtCond', \n            'BsmtExposure',\n            'BsmtFinType1',\n            'BsmtFinType2'):\n    train[col]=train[col].fillna(0)\n    test[col]=test[col].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8b3474a7444f1d153f7d2b9fbc455932b9738b6"},"cell_type":"markdown","source":"MasVnrArea and MasVnrType : NA= no masonry veneer for these houses. We can fill 0 for the area and None for the type."},{"metadata":{"trusted":true,"_uuid":"39e7e3646579b460fefb1f31ce66e65c0ca28937"},"cell_type":"code","source":"train[\"MasVnrType\"] = train[\"MasVnrType\"].fillna(\"None\")\ntrain[\"MasVnrArea\"] = train[\"MasVnrArea\"].fillna(0)\ntest[\"MasVnrType\"] = test[\"MasVnrType\"].fillna(\"None\")\ntest[\"MasVnrArea\"] = test[\"MasVnrArea\"].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"532d32e65e50eb6e2f4f6cf32487310e2416d697"},"cell_type":"markdown","source":"MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'"},{"metadata":{"trusted":true,"_uuid":"affaf560a932489346896cc37264786ce35c06d7"},"cell_type":"code","source":"train['MSZoning'] = train['MSZoning'].fillna(train['MSZoning'].mode()[0])\ntest['MSZoning'] = test['MSZoning'].fillna(test['MSZoning'].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02c573e409d2fa7c2f2720605b3b952ddd2149ea"},"cell_type":"markdown","source":"Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it."},{"metadata":{"trusted":true,"_uuid":"168eb54838b3c6b827260b29c623615f44f1ef0e"},"cell_type":"code","source":"train = train.drop(['Utilities'], axis=1)\ntest=test.drop(['Utilities'],axis=1)\n#Functional : data description says NA means typical\n\ntrain[\"Functional\"] = train[\"Functional\"].fillna(\"Typ\")\ntest[\"Functional\"] = test[\"Functional\"].fillna(\"Typ\")\n#Electrical :\n\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\ntest['Electrical'] = test['Electrical'].fillna(test['Electrical'].mode()[0])\n#KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\n\ntrain['KitchenQual'] = train['KitchenQual'].fillna(train['KitchenQual'].mode()[0])\ntest['KitchenQual'] = test['KitchenQual'].fillna(test['KitchenQual'].mode()[0])\n#Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n\ntrain['Exterior1st'] = train['Exterior1st'].fillna(train['Exterior1st'].mode()[0])\ntrain['Exterior2nd'] = train['Exterior2nd'].fillna(train['Exterior2nd'].mode()[0])\ntest['Exterior1st'] = test['Exterior1st'].fillna(test['Exterior1st'].mode()[0])\ntest['Exterior2nd'] = test['Exterior2nd'].fillna(test['Exterior2nd'].mode()[0])\n#SaleType : Fill in again with most frequent which is \"WD\"\n\ntrain['SaleType'] = train['SaleType'].fillna(train['SaleType'].mode()[0])\ntest['SaleType'] = test['SaleType'].fillna(test['SaleType'].mode()[0])\n#MSSubClass : Na most likely means No building class. We can replace missing values with None\n\ntrain['MSSubClass'] = train['MSSubClass'].fillna(\"None\")\ntest['MSSubClass'] = test['MSSubClass'].fillna(\"None\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7871fb8ab80a0b882391651c236bed64fb66b5a"},"cell_type":"markdown","source":"check for any ramaining messing values\n\n"},{"metadata":{"trusted":true,"_uuid":"70d586e1bd9501978351c69364da9b576b1ec80b"},"cell_type":"code","source":"train.isna().sum().sum()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f9d3ae7190ea5dc09ea18b610a1cbe50849bda3"},"cell_type":"code","source":"test.isna().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5cfc298ad4e31b286d005ce6008d0d1261c3549"},"cell_type":"markdown","source":"now we don't have any missing values\n"},{"metadata":{"_uuid":"7b3787f3b7a41b718dcbefab7456724efd9f814a"},"cell_type":"markdown","source":"**Transform Categorical to numerical\n**\n"},{"metadata":{"trusted":true,"_uuid":"b444f81ada0097a6db5bcdbc9a8ff5ab8a3a3d1a"},"cell_type":"code","source":"#MSSubClass=The building class\ntrain['MSSubClass'] = train['MSSubClass'].apply(str)\ntest['MSSubClass'] = test['MSSubClass'].apply(str)\n\n\n#OverallCond >categorical variable\ntrain['OverallCond'] = train['OverallCond'].astype(str)\ntest['OverallCond'] = test['OverallCond'].astype(str)\n\n#Year and month sold >>categorical features.\ntrain['YrSold'] = train['YrSold'].astype(str)\ntest['YrSold']=test['YrSold'].astype(str)\n\ntrain['MoSold'] = train['MoSold'].astype(str)\ntest['MoSold']=test['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"977870ca105d3310e402d78312b49866877b546e"},"cell_type":"markdown","source":"**Correlation**"},{"metadata":{"trusted":true,"_uuid":"9359616a41af017b811e005e35567780325534a9"},"cell_type":"code","source":"#saleprice correlation matrix\n\nk = 10#number of variables for heatmap\ncorrmat = train.corr()\n\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10d326eba1ebb580d7f42238e90e21d1653b3ee1"},"cell_type":"code","source":"#Correlation map \ncorrmat = train.corr()\nplt.subplots(figsize=(20,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a0e98df51986d213ad31ff118f68e2c02bb3067"},"cell_type":"markdown","source":"**Outliers Acquiescence **\n"},{"metadata":{"_uuid":"b5f3c1427b051c3d0a9a5d2665a39cf584d9b375"},"cell_type":"markdown","source":"An outlier is a data point in a data set that is distant from all other observations. A data point that lies outside the overall distribution of the dataset."},{"metadata":{"_uuid":"3a74119a25e161469296bce99d38eba12e8aacc6"},"cell_type":"markdown","source":"**What is the reason for an outlier to exists in a dataset?**\n\nAn outlier could exist in a dataset due to Variability in the data An experimental measurement error\n\n**Discover outliers with visualization tools **\n\nBox plot- Histogram Scatter plot-\n\n**Discover outliers with mathematical function **\n\nZ-Score-\n\nIQR score -\n\nIn regression problems outliers are very problematic unlike classification problems so lets take an overall look on our data"},{"metadata":{"_uuid":"4df59fea315e4031ed7b22661e1efc974f29ef5a"},"cell_type":"markdown","source":"![Imgur](https://i.imgur.com/Trf5gsm.jpg)"},{"metadata":{"_uuid":"7e500fbe28eadb7935b9c8b4ddbe57a610c184c9"},"cell_type":"markdown","source":"![Imgur](https://i.imgur.com/NmPoUH3.png)"},{"metadata":{"_uuid":"f423495add2de1502df6e884fa47299d69329da6"},"cell_type":"markdown","source":"lets show outliers by some graphs in some variables\n\n"},{"metadata":{"trusted":true,"_uuid":"0c36b3e3a0f1ea15763d46267e75569fbca7d7a0"},"cell_type":"code","source":"g = sns.jointplot(x = train['GrLivArea'], y = train['SalePrice'],kind=\"reg\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35eeafd7d26d6b3993535bc07c8e37d3e8338fad"},"cell_type":"code","source":"sns.boxplot(x=train['GrLivArea'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6ef62f392cbbe8418d689d8306f04d76c48cd52e"},"cell_type":"code","source":"sns.boxplot(x=train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"457cf8adb835566487ddb71ffe7494d1b93cda87"},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,8))\nax.scatter(train['GrLivArea'], train['SalePrice'])\nax.set_xlabel('x')\nax.set_ylabel('y')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"064e43fccbfff5dda95ba7179ac0930ca09c15b3"},"cell_type":"markdown","source":"\n"},{"metadata":{"_uuid":"97c1bbcbef5e378202f83b1012e8573b9990e8b2"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"06b05f461fa8aae5a9dce9ea8619546158a3d3b2"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"cf718b47d511c4489ebb73d632714eecd824472c"},"cell_type":"code","source":"from scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(train['SalePrice']))\nprint(z)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a37e929b3868388742686a1fdb3bbecec74efb9"},"cell_type":"code","source":"#identify a threshold for you outlier z-score to detct them\nthreshold = 3#or 2 any value-search for them-\nprint(np.where(z > 3))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df00b36495a99547445ad5a718ea9fe556e2bebe"},"cell_type":"markdown","source":"**IQR\n**\n"},{"metadata":{"trusted":true,"_uuid":"e6e0aca932a26f40ef0e6bd650c60e9a0808839e"},"cell_type":"code","source":"from scipy.stats import iqr\niqr(train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8e5c651574c3d0cd4aa2e354a656a15676fc251"},"cell_type":"markdown","source":"now lets use zscore for each value in our numeric columns\n\n"},{"metadata":{"_uuid":"0689897ddb959437de9620a4d688f668e388f6f2"},"cell_type":"markdown","source":"**z score**"},{"metadata":{"_uuid":"7e7598b350c2e66d93fb1d56c93ca4b71d970a47"},"cell_type":"markdown","source":"**Understanding z score**\n\nThe Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.\nThe intuition behind Z-score is to describe any data point by finding their relationship with the Standard Deviation and Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1 i.e. normal distribution.\nThis method assumes that the variable has a Gaussian distribution. It represents the number of standard deviations an observation is away from the mean:\n\n\n"},{"metadata":{"_uuid":"05159c29d969ec3f004fd8bc5b46ec241cf27806"},"cell_type":"markdown","source":"**How to define your threshold ? **\n\nYou must be wondering that, how does this help in identifying the outliers? Well, while calculating the Z-score we re-scale and center the data and look for data points which are too far from zero. These data points which are way too far from zero will be treated as the outliers. In most of the cases a threshold of 3 or -3 is used i.e if the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers.\n\nHere, we normally define outliers as points whose modulus of z-score is greater than a threshold value. This threshold value is usually greater than 2 (3 is a common value)."},{"metadata":{"_uuid":"75bc0799b84e47479c48379ef9a1178e4b8ed140"},"cell_type":"markdown","source":"![Imgur](https://i.imgur.com/FJyli3W.jpg)"},{"metadata":{"_uuid":"6548a16263e96c40784ee1314718d143ceb0066a"},"cell_type":"markdown","source":""},{"metadata":{"trusted":true,"_uuid":"dc98c09682e2da6b319efd9de6450d4f99863033"},"cell_type":"code","source":"calculating z-score of only numeric columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8f25a2a935a28a38c5cf46ef9f5cb0f940d5ce2"},"cell_type":"code","source":"stats.zscore(train.select_dtypes(include=np.number))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e960d32c93047172fd0cc588d88813bf64146613"},"cell_type":"markdown","source":"**removing  outliers of numeric columns by this brilliant function\n**"},{"metadata":{"_uuid":"6b2442272ed8c3d285d57617f1adea7970f6ac9a"},"cell_type":"markdown","source":"this is a tricky function to remove outliers , I have searched a inbetween various methods but this was the fantastic one"},{"metadata":{"trusted":true,"_uuid":"d4c17b57ba2c4e7c92e4153ee03ad3ff174bf2a4"},"cell_type":"code","source":"newtrain=train[(np.abs(stats.zscore(train.select_dtypes(include=np.number))) < 3).all(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae900d57ff4b631799cdcbadbaf5b6f23873ff11"},"cell_type":"markdown","source":"this is the last step  the most important by this method you can delete outliers, there are a lot of terms to deal with in the term of cleaning of your data .just use your intuition , plus mathimatical skills and you will solve it. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}