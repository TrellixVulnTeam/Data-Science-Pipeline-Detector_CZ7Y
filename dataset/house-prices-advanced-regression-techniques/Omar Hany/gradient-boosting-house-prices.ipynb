{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Import visualization libraries \nimport matplotlib.pyplot as plt # Matlab-style plotting\nimport seaborn as sns\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## First we read our data into a pandas dataframe "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the train and the test datasets \ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now let's start understanding our data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many rows and columns we have in each dataset \nprint(train.shape)\nprint('*' * 10)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's try to understand our data\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get some information about our data\n# As we can see we have null values which we need to deal with \nprint(train.info())\nprint('*' * 30)\nprint(test.info())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dealing with missing values"},{"metadata":{},"cell_type":"markdown","source":"### First Identify the missing data "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding null values \n# As our data is large so we better visualize them\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate percentage of our missing values\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nf, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\n# As our data is large it's better to visualize the missing values\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same for testing data \ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nf, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A common approach is that we drop all columns which their missing values exceeds 60%\ntrain.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1, inplace=True)\ntest.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's deal with the rest of the missing data\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to know the values we have to determine which approach to use when filling the data \ntrain.LotFrontage.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# My option would be to randomly fill these missing values with values close to the mean but within one standard deviation.\n# As our missing data is large so we don't want to fill the data with mean values so now change in the dist happen\nLotFrontage_avg = train['LotFrontage'].mean()\nLotFrontage_std = train['LotFrontage'].std()\nLotFrontage_null_count = train['LotFrontage'].isnull().sum()\nLotFrontage_null_random_list = np.random.randint(LotFrontage_avg - LotFrontage_std, LotFrontage_avg + LotFrontage_std, size=LotFrontage_null_count)\ntrain['LotFrontage'][np.isnan(train['LotFrontage'])] = LotFrontage_null_random_list\ntrain['LotFrontage'] = train['LotFrontage'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same for Test dataset\nLotFrontage_avg = test['LotFrontage'].mean()\nLotFrontage_std = test['LotFrontage'].std()\nLotFrontage_null_count = test['LotFrontage'].isnull().sum()\nLotFrontage_null_random_list = np.random.randint(LotFrontage_avg - LotFrontage_std, LotFrontage_avg + LotFrontage_std, size=LotFrontage_null_count)\ntest['LotFrontage'][np.isnan(test['LotFrontage'])] = LotFrontage_null_random_list\ntest['LotFrontage'] = test['LotFrontage'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating percentage of missing values \ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating percentage of missing values \ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before dealing with GarageFinish we need to remember it's data type \n# This will help us determine which way we fill the missing data\ntrain.GarageFinish.dtype","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the values of GarageFinish\ntrain.GarageFinish.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying to find if their is a relation between GarageCars & GarageFinish\n# We found that whenever a GarageFinish is null Garage cars == 0\ntrain['GarageCars'][train['GarageFinish'].isnull() == True].head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['GarageCars'][test['GarageFinish'].isnull() == True].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['GarageCars'].fillna(value=0, inplace=True)\ntest['GarageCars'].fillna(value=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So we found out that we can convert  the missing values into a category \n#### We 'll call it Nfn which stands for No Finish "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new category we will call it Nfn\ntrain['GarageFinish'].fillna(value='Nfn', inplace=True)\ntest['GarageFinish'].fillna(value='Nfn', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we find all values in GarageType\ntrain.GarageType.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.GarageType.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same as above we create a new category called Nogarage\ntrain['GarageType'].fillna(value='Nogarage', inplace=True)\ntest['GarageType'].fillna(value='Nogarage', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.GarageCond.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.GarageCond.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['GarageCond'].fillna(value='NG', inplace=True)\ntest['GarageCond'].fillna(value='NG', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now to see the remaining features and their missing value percentage \ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.GarageQual.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.GarageQual.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling missing data in both train and test with NG\ntrain['GarageQual'].fillna(value='NG', inplace=True)\ntest['GarageQual'].fillna(value='NG', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.GarageYrBlt.nunique(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we can see in the test GarageYrBlt there are outliers  \nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))\ntrain.GarageYrBlt.hist(ax=ax[0])\nax[0].set_title('Train GarageYrBlt', fontsize=15)\ntest.GarageYrBlt.hist(ax=ax[1])\nax[1].set_title('Test GarageYrBlt', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 10))\ntrain.YearBuilt.hist(ax=ax[0])\ntest.YearBuilt.hist(ax=ax[1])\n\nax[0].set_title('Train YearBuilt', fontsize=15)\nax[1].set_title('Test YearBuilt', fontsize=15)\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we can see that they are similar \ntrain[['GarageYrBlt', 'YearBuilt']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Also in the test dataset\ntest[['GarageYrBlt', 'YearBuilt']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So we can fill the missing values with it's corresponding YearBuilt data\ntrain['YearBuilt'][train.GarageYrBlt.isnull() == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling the GarageYrBlt missing values with it's corresponding YearBuilt values \ntrain.GarageYrBlt.fillna(value=train['YearBuilt'][train.GarageYrBlt.isnull() == True], inplace=True)\ntest.GarageYrBlt.fillna(value=test['YearBuilt'][test.GarageYrBlt.isnull() == True], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's see the remaining missing values in the train data \ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's see the remaining missing values in the test data \ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.BsmtFinType2.nunique(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.BsmtFinType2.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.BsmtFinType2.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['BsmtFinType2', 'BsmtFinSF2']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.BsmtFinType2.value_counts(dropna=False))\nprint('*' * 40)\nprint(test.BsmtFinType2.value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling the BsmrFinType2 with it's mode\ntrain.BsmtFinType2.fillna(train.BsmtFinType2.mode()[0], inplace=True)\ntest.BsmtFinType2.fillna(test.BsmtFinType2.mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['BsmtFinSF1'][train.BsmtFinType1.isnull() == True].head(8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.BsmtFinType1.value_counts(dropna=False))\nprint('*' * 40)\nprint(test.BsmtFinType1.value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.BsmtFinType1.mode()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same as before we will fill the missing data with the mode\ntrain.BsmtFinType1.fillna(train.BsmtFinType1.mode()[0], inplace=True)\ntest.BsmtFinType1.fillna(test.BsmtFinType1.mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We now understand that all basement features depend on each other \n# So fill the rest with their mode\ntrain.BsmtExposure.fillna(value=train.BsmtExposure.mode()[0], inplace=True) \ntrain.BsmtQual.fillna(value=train.BsmtQual.mode()[0], inplace=True) \ntrain.BsmtCond.fillna(value=train.BsmtCond.mode()[0], inplace=True)\n\n# Same as for testing data\ntest.BsmtExposure.fillna(value=test.BsmtExposure.mode()[0], inplace=True) \ntest.BsmtQual.fillna(value=test.BsmtQual.mode()[0], inplace=True) \ntest.BsmtCond.fillna(value=test.BsmtCond.mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's see what is still missing  \ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"total = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.FireplaceQu.nunique(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the values for FireplaceQu feature\nprint(train.FireplaceQu.value_counts(dropna=False))\nprint('*' * 40)\nprint(test.FireplaceQu.value_counts(dropna=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we can see here the FireplaceQu values are missing when Fireplaces value is equal to 0\ntrain[['Fireplaces', 'FireplaceQu']].head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling the missing data with NG which we created \ntrain['FireplaceQu'].fillna(value='NG', inplace=True)\ntest['FireplaceQu'].fillna(value='NG', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the values of MasVnrType\ntrain.MasVnrType.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# What are the values of MasVnrArea\ntrain.MasVnrArea.value_counts(dropna=False).head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to find the relation between MasVnrType & MasVnrArea\n# As there are null values in both of them we can't get any information like the others before \ntrain['MasVnrType'][train['MasVnrArea'].isnull() == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the remaining missing rows \ntrain.dropna(inplace=True)\ntest.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's see our data if we missed anything by accident   \ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()/train.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# As we can see we finished cleaning the training dataset but the testing still needs a little bit more cleaning\n# Now let's see our data if we missed anything by accident   \ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = ((test.isnull().sum()/test.isnull().count()) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(train.shape, test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Time for some EDA (Exploratory Data Analysis)"},{"metadata":{},"cell_type":"markdown","source":"Using pearson correlation heatmap won't be useful as the data is large"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using correlation heatmap\nplt.figure(figsize=(17, 10))\nsns.heatmap(train.corr(), annot=True, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()['SalePrice'].sort_values(ascending=False)[:11]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.OverallQual.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17, 10))\nsns.countplot(x='OverallQual', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(17, 10))\nsns.barplot(x='OverallQual', y='SalePrice', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.GrLivArea.nunique(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nplt.scatter(x=train.GrLivArea, y=train.SalePrice, edgecolors=\"black\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x='GrLivArea', y='SalePrice',data=train, size=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.GarageCars.nunique(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.GarageCars.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nsns.countplot(x='GarageCars', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nsns.boxplot(x='GarageCars', y='SalePrice', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nsns.barplot(x='GarageCars', y='SalePrice', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.GarageArea.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = ['MSZoning', 'Street',\n       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', \n       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n       'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n       'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = pd.concat([train, test], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def One_hot_encoding(columns):\n    df_final=final_df\n    i=0\n    for fields in columns:\n        df1=pd.get_dummies(final_df[fields],drop_first=True)\n        \n        final_df.drop([fields],axis=1,inplace=True)\n        if i==0:\n            df_final=df1.copy()\n        else:           \n            df_final=pd.concat([df_final,df1],axis=1)\n        i=i+1\n       \n        \n    df_final=pd.concat([final_df,df_final],axis=1)\n        \n    return df_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = One_hot_encoding(columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df =final_df.loc[:,~final_df.columns.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Train=final_df.iloc[:1422,:]\ndf_Test=final_df.iloc[1422:,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Test.drop(['SalePrice'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=df_Train.drop(['SalePrice'],axis=1)\ny_train=df_Train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using GradientBoosting with RandomizedSearch "},{"metadata":{"trusted":true},"cell_type":"code","source":"num_estimators = [500,1000]\nlearn_rates = [0.02, 0.05]\nmax_depths = [1, 2]\nmin_samples_leaf = [5,10]\nmin_samples_split = [5,10]\n\nparam_grid = {'n_estimators': num_estimators,\n              'learning_rate': learn_rates,\n              'max_depth': max_depths,\n              'min_samples_leaf': min_samples_leaf,\n              'min_samples_split': min_samples_split}\n\nrandom_search =RandomizedSearchCV(GradientBoostingRegressor(loss='huber'), param_grid, random_state=1, n_iter=100, cv=5, verbose=0, n_jobs=-1)\n\nrandom_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best params\nrandom_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train the model \nrandom_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy for training data\ngboost_score=random_search.score(X_train,y_train)\nprint(f'{round(gboost_score * 100, 2)}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predictions\npred = random_search.predict(df_Test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_df=pd.DataFrame(pred)\nsample = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\ndata= pd.concat([sample['Id'],pred_df], axis=1)\ndata.columns=['Id','SalePrice']\ndata.to_csv('sample_submission1.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}