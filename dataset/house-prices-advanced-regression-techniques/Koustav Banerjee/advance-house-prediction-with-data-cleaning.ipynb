{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nimport math\n\n\ntrain_path=\"../input/house-prices-advanced-regression-techniques/train.csv\"\ntest_path=\"../input/house-prices-advanced-regression-techniques/test.csv\"\n\ntrain=pd.read_csv(train_path)\ntest=pd.read_csv(test_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Copying Train And Test Datatset\nc_train=train.copy()\nc_test=test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":"train.head()"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To see the null values using a heatmap\n#plt.figure(figsize = (16,7))\n#sns.heatmap(train.isnull(),yticklabels=False,cbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Lets Concat Train And Test Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"c_train[\"train\"]=1\nc_test[\"train\"]=0\ndf=pd.concat([c_train,c_test],axis=0,sort=False)\n#Vertically Concat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"83 = train/test features+train id +test id+SalePrice"},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We Have Added Two New Features Train and Test To Show Which Values Are Present In Either Train Or Test"},{"metadata":{},"cell_type":"markdown","source":"* Calculating the percentage of missing values of each feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"NAN=[(k,df[k].isnull().mean()*100) for k in df]\nprint(NAN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Making a new dataframe\nNAN=pd.DataFrame(NAN,columns=[\"column_name\",\"percentage\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Displaying Features Which Has More Than 50% Null Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"NAN=NAN[NAN.percentage>50]\nNAN.sort_values(\"percentage\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping All The Above Features\ndf=df.drop([\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Now we will select numerical and categorical features "},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns = df.select_dtypes(include=['O'])\nnum_columns =df.select_dtypes(exclude=['O'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dealing With Categorical Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_cat_columns=cat_columns.isnull().sum()\nnull_cat_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will fill -- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageType, GarageFinish, GarageQual, FireplaceQu, GarageCond** -- with \"None\" (Check The Above Output,We Place The Node In Features Where There Are Many NULL Values).\n\nWe will fill the rest of features with th most frequent value (using its own most frequent value)."},{"metadata":{"trusted":true},"cell_type":"code","source":"columns_None = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n                'GarageType','GarageFinish','GarageQual','FireplaceQu','GarageCond']\ncat_columns[columns_None]= cat_columns[columns_None].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mode_cols = ['MSZoning','Utilities','Exterior1st','Exterior2nd',\n             'MasVnrType','Electrical','KitchenQual','Functional','SaleType']\n\n#fill missing values for each column (using its own most frequent value)\ncat_columns[mode_cols] = cat_columns[mode_cols].fillna(cat_columns.mode().iloc[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"null_cat_columns=cat_columns.isnull().sum()\nnull_cat_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dealing With Num Vals"},{"metadata":{"trusted":true},"cell_type":"code","source":"null_num_columns=num_columns.isnull().sum()\nnull_num_columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**By The Above Table We Can See That We Gotta Fill LotFrontage and GarageYrBlt**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print((num_columns[\"GarageYrBlt\"]).median())\nprint(num_columns[\"LotFrontage\"].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*We Can Use Either Mean Or Median Depending Upon Us*"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_columns['GarageYrBlt'] = num_columns['GarageYrBlt'].fillna(1979)\nnum_columns['LotFrontage'] = num_columns['LotFrontage'].fillna(68)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets Fill The Other Null Num Features With 0"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_columns=num_columns.fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_columns.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets Make Some New Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_columns[\"House_Age\"]= num_columns['YrSold']- num_columns['YearBuilt']\nnum_columns[\"House_Age\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Lets Find If There Is A Negative Value In House_Age"},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_house=num_columns[num_columns[\"House_Age\"]<0]\nneg_house","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"neg_house['YrSold']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Like we see here that the minimun is -1 ???\n* It is strange to find that the house was sold in 2007 and built in 2008.\n* So we decide to change the year of sold to 2009"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_columns.loc[num_columns[\"YrSold\"]<num_columns[\"YearRemodAdd\"],\"YrSold\"]=2009\n\nnum_columns[\"House_Age\"]= num_columns['YrSold']- num_columns['YearBuilt']\nnum_columns[\"House_Age\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**According To The Feature Description**\n\n* TotalBsmtBath : Sum of : 1/2 * BsmtFullBath + BsmtHalfBath\n\n* TotalBath : Sum of : 1/2 * FullBath and HalfBath\n\n* TotalSA : Sum of : 1stFlrSF and 2ndFlrSF and TotalBsmtSF"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_columns['TotalBsmtBath'] = num_columns['BsmtFullBath']*0.5 + num_columns['BsmtHalfBath']\nnum_columns['TotalBath'] = num_columns['FullBath']*0.5  + num_columns['HalfBath']\nnum_columns['TotalSA']=num_columns['TotalBsmtSF'] + num_columns['1stFlrSF'] + num_columns['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_columns.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets Encode Categorical Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using One Hot Encoder\n#cat_columns=pd.get_dummies(cat_columns,columns=cat_columns.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* It simply creates additional features based on the number of unique values in the categorical feature. Every unique value in the category will be added as a feature.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\nencoder=LabelEncoder()\n\nfor features in cat_columns.columns:\n    cat_columns[features]=encoder.fit_transform(cat_columns[features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Label Encoding is a popular encoding technique for handling categorical variables. In this technique, each label is assigned a unique integer based on alphabetical ordering.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_columns.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now Lets Concat The Num And Cat Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = pd.concat([cat_columns, num_columns], axis=1,sort=False)\ndf_final.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Initialising Train,Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final = df_final.drop(['Id'],axis=1)\n\n#We Had Initialised Train Data As 1 And Test Data As 0 In the Beggining\ndf_train = df_final[df_final['train'] == 1]\ndf_train = df_train.drop(['train'],axis=1)\n\n#Vertically Splitting The Train And Test\n\ndf_test = df_final[df_final['train'] == 0]\ndf_test = df_test.drop(['SalePrice'],axis=1)\ndf_test = df_test.drop(['train'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Now We Separate Train And Target Features*"},{"metadata":{"trusted":true},"cell_type":"code","source":"target= df_train['SalePrice']\ndf_train = df_train.drop(['SalePrice'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test=train_test_split(df_train,target,test_size=0.33,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XGB Documentation**: https://xgboost.readthedocs.io/en/latest/parameter.html\n\n**LGBM Documentation**: https://lightgbm.readthedocs.io/en/latest/Parameters.html#core-parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb =XGBRegressor( booster='gbtree', colsample_bylevel=1,\n                  colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n                  importance_type='gain', learning_rate=0.01, max_delta_step=0,\n                  max_depth=4, min_child_weight=1.5, n_estimators=2400,\n                  n_jobs=1, nthread=None, objective='reg:linear',\n                  reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n                  silent=None, subsample=0.8, verbosity=1)\n\n\nlgbm = LGBMRegressor(objective='regression', \n                     num_leaves=4,\n                     learning_rate=0.01, \n                     n_estimators=12000, \n                     max_bin=200, \n                     bagging_fraction=0.75,\n                     bagging_freq=5, \n                     bagging_seed=7,\n                     feature_fraction=0.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I Found This Article Very Helpful To Know About LGBM And XGBoost\nhttps://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/                                                "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Fitting\nxgb.fit(x_train, y_train)\nlgbm.fit(x_train, y_train,eval_metric='rmse')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict1 = xgb.predict(x_test)\npredict = lgbm.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Root Mean Square Error test(XGBOOST) = ' + str(math.sqrt(metrics.mean_squared_error(y_test, predict1))))\nprint('Root Mean Square Error test(LGBM) = ' + str(math.sqrt(metrics.mean_squared_error(y_test, predict))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Fitting With all the dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb.fit(df_train, target)\nlgbm.fit(df_train, target,eval_metric='rmse')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Root Mean Square Error (RMSE) is a standard way to measure the error of a model in predicting quantitative data."},{"metadata":{},"cell_type":"markdown","source":"Merging Both Predictions (45% of XGB and 55% of LGBM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict4 = lgbm.predict(df_test)\npredict3 = xgb.predict(df_test)\npredict_y = (predict3*0.45 + predict4*0.55)\nprint(predict_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": predict_y\n    })\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}