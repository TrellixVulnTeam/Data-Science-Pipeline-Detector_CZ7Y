{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Hello everyone, this is the first thing that I'm putting on the kaggle and from now I will continously updating it.<br>\nSo as a beginner it is difficult to get this done I have referred to others expert's kernel's which is [here](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) and [here](https://www.kaggle.com/masumrumi/a-detailed-regression-guide-with-house-pricing) and there are so many other great kernels to look into.<br>\nThe main aim behind this kernel to make understand basics to beginner and give little understanding about exploratory data analysis approach.<br>\n\n","metadata":{}},{"cell_type":"code","source":"#importing required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom scipy.stats import skew\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.35959Z","iopub.execute_input":"2021-08-24T18:36:49.360014Z","iopub.status.idle":"2021-08-24T18:36:49.36682Z","shell.execute_reply.started":"2021-08-24T18:36:49.359966Z","shell.execute_reply":"2021-08-24T18:36:49.365589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading data\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.368227Z","iopub.execute_input":"2021-08-24T18:36:49.368645Z","iopub.status.idle":"2021-08-24T18:36:49.445649Z","shell.execute_reply.started":"2021-08-24T18:36:49.368602Z","shell.execute_reply":"2021-08-24T18:36:49.444609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking the dimension of datasets\ntrain.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.447174Z","iopub.execute_input":"2021-08-24T18:36:49.447431Z","iopub.status.idle":"2021-08-24T18:36:49.453417Z","shell.execute_reply.started":"2021-08-24T18:36:49.447405Z","shell.execute_reply":"2021-08-24T18:36:49.452348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ckeck the columns\ntrain.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.45472Z","iopub.execute_input":"2021-08-24T18:36:49.45515Z","iopub.status.idle":"2021-08-24T18:36:49.467998Z","shell.execute_reply.started":"2021-08-24T18:36:49.455016Z","shell.execute_reply":"2021-08-24T18:36:49.467299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"linear regression model makes a good amount of **assumptions** for the data you provide\n\n1. Linearity \n2. No noise \n3. No collinearity\n4. Normal distribution\n5. Scale","metadata":{}},{"cell_type":"markdown","source":"## Missing data","metadata":{}},{"cell_type":"markdown","source":"### Handling missing data from both test and train datasets","metadata":{}},{"cell_type":"markdown","source":"_Handling missing values is an essential part of data cleaning and preparation process_.<br>\n***np.nan, None and NaT (for datetime64 types)*** _are standard missing value for Pandas_.<br>\n_Not all missing values come in nice and clean **np.nan or None format** instead there may be some characters like `??`or `--`,etc_.","metadata":{}},{"cell_type":"code","source":"# missing data from train dataset\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data.head(30)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.468874Z","iopub.execute_input":"2021-08-24T18:36:49.469307Z","iopub.status.idle":"2021-08-24T18:36:49.503226Z","shell.execute_reply.started":"2021-08-24T18:36:49.469278Z","shell.execute_reply":"2021-08-24T18:36:49.502469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will delete the variable when there is more then **15% missing data**, according to this 'PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage' will get deleted.\n\n'GarageType','GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond' have the same number of missing data so this missing data refers to the same set of observation so we delete this variables and we have considered 'GarageCars' which expressed the most of the information about the garage.\n\nSame logic goes for 'BsmtExposure', 'BsmtFinType1','BsmtQual','BsmtCond','BsmtFinType2'.\n'MasVnrArea' and 'MasVnrType' haave strong corelation with 'YearBuilt' and 'OverallQual' which we have already considered.\n\nFinally 'Electrical' have only one null value so we replace it with its mode.\n","metadata":{}},{"cell_type":"code","source":"# Dealing with missing data\ntrain = train.drop((missing_data[missing_data['Total']>1]).index,1)\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\ntrain.isnull().sum().max()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.504354Z","iopub.execute_input":"2021-08-24T18:36:49.504718Z","iopub.status.idle":"2021-08-24T18:36:49.517971Z","shell.execute_reply.started":"2021-08-24T18:36:49.504688Z","shell.execute_reply":"2021-08-24T18:36:49.516933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# After deleting the columns \ntrain.columns","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.519158Z","iopub.execute_input":"2021-08-24T18:36:49.519543Z","iopub.status.idle":"2021-08-24T18:36:49.534189Z","shell.execute_reply.started":"2021-08-24T18:36:49.519512Z","shell.execute_reply":"2021-08-24T18:36:49.533266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing data from test dataset\ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)\nmissing_data1 = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data1.head(40)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.536349Z","iopub.execute_input":"2021-08-24T18:36:49.537006Z","iopub.status.idle":"2021-08-24T18:36:49.572326Z","shell.execute_reply.started":"2021-08-24T18:36:49.536961Z","shell.execute_reply":"2021-08-24T18:36:49.57121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we did before for **train** data same logic applies here for **test** data.","metadata":{}},{"cell_type":"code","source":"test = test.drop((missing_data1[missing_data1['Total']>4]).index,1)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.573722Z","iopub.execute_input":"2021-08-24T18:36:49.574027Z","iopub.status.idle":"2021-08-24T18:36:49.580666Z","shell.execute_reply.started":"2021-08-24T18:36:49.573997Z","shell.execute_reply":"2021-08-24T18:36:49.579997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# missing data from test dataset\ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()/test.isnull().count()).sort_values(ascending=False)\nmissing_data1 = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data1.head(40)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.581758Z","iopub.execute_input":"2021-08-24T18:36:49.58213Z","iopub.status.idle":"2021-08-24T18:36:49.615806Z","shell.execute_reply.started":"2021-08-24T18:36:49.582089Z","shell.execute_reply":"2021-08-24T18:36:49.615123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For `categorical variables` and `numerical variable` we are just filling the **Null** values with most frequent(**mode**) from specified columns.","metadata":{}},{"cell_type":"code","source":"null_features = (missing_data1[missing_data1['Total']>0]).index\nnull_features","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.617095Z","iopub.execute_input":"2021-08-24T18:36:49.617514Z","iopub.status.idle":"2021-08-24T18:36:49.624934Z","shell.execute_reply.started":"2021-08-24T18:36:49.617472Z","shell.execute_reply":"2021-08-24T18:36:49.623773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for feature in null_features:\n    test[feature] = test[feature].fillna(test[feature].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.626433Z","iopub.execute_input":"2021-08-24T18:36:49.626695Z","iopub.status.idle":"2021-08-24T18:36:49.650431Z","shell.execute_reply.started":"2021-08-24T18:36:49.626668Z","shell.execute_reply":"2021-08-24T18:36:49.649394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check again if there is any null values\ntest.isnull().sum().max()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.652028Z","iopub.execute_input":"2021-08-24T18:36:49.652515Z","iopub.status.idle":"2021-08-24T18:36:49.661889Z","shell.execute_reply.started":"2021-08-24T18:36:49.652478Z","shell.execute_reply":"2021-08-24T18:36:49.661052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Analysing SalePrice ","metadata":{}},{"cell_type":"code","source":"#Descriptive statistics summary\ntrain['SalePrice'].describe()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.663738Z","iopub.execute_input":"2021-08-24T18:36:49.664209Z","iopub.status.idle":"2021-08-24T18:36:49.678379Z","shell.execute_reply.started":"2021-08-24T18:36:49.664168Z","shell.execute_reply":"2021-08-24T18:36:49.677268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#histogram\nsns.distplot(train['SalePrice']);","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:49.680204Z","iopub.execute_input":"2021-08-24T18:36:49.68052Z","iopub.status.idle":"2021-08-24T18:36:50.002592Z","shell.execute_reply.started":"2021-08-24T18:36:49.680493Z","shell.execute_reply":"2021-08-24T18:36:50.001564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Let's focus on 1st asumption","metadata":{}},{"cell_type":"code","source":"# correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corrmat,vmax=.8,square = True);","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:50.003608Z","iopub.execute_input":"2021-08-24T18:36:50.003875Z","iopub.status.idle":"2021-08-24T18:36:51.10672Z","shell.execute_reply.started":"2021-08-24T18:36:50.003846Z","shell.execute_reply":"2021-08-24T18:36:51.105702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We used **heatmap** here, so we can get the overview of all the features relationship ","metadata":{}},{"cell_type":"code","source":"# most correlated features \ncorrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat['SalePrice'])>0.5]\nplt.figure(figsize=(10,10))\nsns.heatmap(train[top_corr_features].corr(),annot = True);\ntop_corr_features","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:51.108553Z","iopub.execute_input":"2021-08-24T18:36:51.108841Z","iopub.status.idle":"2021-08-24T18:36:52.062619Z","shell.execute_reply.started":"2021-08-24T18:36:51.10881Z","shell.execute_reply":"2021-08-24T18:36:52.061525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This are the features most **correlated** with the 'SalePrice'.\n\n**OverallQual, GrLivArea, GarageCars and TotalBsmtSF** are strongly correlated with the 'SalePrice'.\n\n**GarageCars** and **GarageArea** are also some of the most strongly correlated variables.\n\nSame goes for **TotalBsmtSF** and **1stFloor**.","metadata":{}},{"cell_type":"markdown","source":"## Scatter plot between 'SalePrice' and its correlated Variables ","metadata":{}},{"cell_type":"code","source":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:36:52.064095Z","iopub.execute_input":"2021-08-24T18:36:52.064397Z","iopub.status.idle":"2021-08-24T18:37:00.567834Z","shell.execute_reply.started":"2021-08-24T18:36:52.064367Z","shell.execute_reply":"2021-08-24T18:37:00.566838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fixing Skewness","metadata":{}},{"cell_type":"markdown","source":"##### Lets focus on 4th assumption, and that is that predictors and target variable should follow a gaussian distribution. ","metadata":{}},{"cell_type":"markdown","source":"Here we are using **Log Transform**","metadata":{}},{"cell_type":"code","source":"# differentiate between numerical and categorical varibles\ncategorical_features = train.select_dtypes(include = [\"object\"]).columns\nnumerical_features = train.select_dtypes(exclude = [\"object\"]).columns","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.569176Z","iopub.execute_input":"2021-08-24T18:37:00.569459Z","iopub.status.idle":"2021-08-24T18:37:00.577247Z","shell.execute_reply.started":"2021-08-24T18:37:00.56943Z","shell.execute_reply":"2021-08-24T18:37:00.576506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taking numerical dataset and categorical datasets separately \ntrain_num = train[numerical_features]\ntrain_cat = train[categorical_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.578218Z","iopub.execute_input":"2021-08-24T18:37:00.578595Z","iopub.status.idle":"2021-08-24T18:37:00.589131Z","shell.execute_reply.started":"2021-08-24T18:37:00.578558Z","shell.execute_reply":"2021-08-24T18:37:00.588446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cat.shape,train_num.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.59005Z","iopub.execute_input":"2021-08-24T18:37:00.590403Z","iopub.status.idle":"2021-08-24T18:37:00.604606Z","shell.execute_reply.started":"2021-08-24T18:37:00.590377Z","shell.execute_reply":"2021-08-24T18:37:00.603572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checkin skewmess of all features\nskewness = train_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.605559Z","iopub.execute_input":"2021-08-24T18:37:00.605856Z","iopub.status.idle":"2021-08-24T18:37:00.637563Z","shell.execute_reply.started":"2021-08-24T18:37:00.605824Z","shell.execute_reply":"2021-08-24T18:37:00.63616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### we are selecting features where skewness is greater than 0.5 to fix their skewness","metadata":{}},{"cell_type":"code","source":"skewness = skewness[abs(skewness)>0.5]\nskewness.index","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.639546Z","iopub.execute_input":"2021-08-24T18:37:00.639802Z","iopub.status.idle":"2021-08-24T18:37:00.646725Z","shell.execute_reply.started":"2021-08-24T18:37:00.639775Z","shell.execute_reply":"2021-08-24T18:37:00.645674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# applying log tranform\n#train_num[skewness.index] = np.log1p(train_num[skewness.index])","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.649038Z","iopub.execute_input":"2021-08-24T18:37:00.649404Z","iopub.status.idle":"2021-08-24T18:37:00.657586Z","shell.execute_reply.started":"2021-08-24T18:37:00.649377Z","shell.execute_reply":"2021-08-24T18:37:00.656956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cat.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.658971Z","iopub.execute_input":"2021-08-24T18:37:00.65933Z","iopub.status.idle":"2021-08-24T18:37:00.689449Z","shell.execute_reply.started":"2021-08-24T18:37:00.659303Z","shell.execute_reply":"2021-08-24T18:37:00.688363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using get_dummies here it is used for data manipulation. It converts categorical data into dummy or indicator variables\ntrain_cat = pd.get_dummies(train_cat)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.69065Z","iopub.execute_input":"2021-08-24T18:37:00.691166Z","iopub.status.idle":"2021-08-24T18:37:00.730693Z","shell.execute_reply.started":"2021-08-24T18:37:00.691121Z","shell.execute_reply":"2021-08-24T18:37:00.72978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cat.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.732025Z","iopub.execute_input":"2021-08-24T18:37:00.732572Z","iopub.status.idle":"2021-08-24T18:37:00.738078Z","shell.execute_reply.started":"2021-08-24T18:37:00.732529Z","shell.execute_reply":"2021-08-24T18:37:00.737413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenating train_num (numerical variable) and train_cat (categorical variable)\ntrain1 = pd.concat([train_cat,train_num],axis=1)\ntrain1.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.739075Z","iopub.execute_input":"2021-08-24T18:37:00.739445Z","iopub.status.idle":"2021-08-24T18:37:00.753038Z","shell.execute_reply.started":"2021-08-24T18:37:00.739413Z","shell.execute_reply":"2021-08-24T18:37:00.75199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Skewness of test data","metadata":{}},{"cell_type":"markdown","source":"#### same applies here as we did for train data","metadata":{}},{"cell_type":"code","source":"# differentiate between numerical and categorical varibles\ncategorical_features = test.select_dtypes(include = [\"object\"]).columns\nnumerical_features = test.select_dtypes(exclude = [\"object\"]).columns\n\ntest_num = test[numerical_features]\ntest_cat = test[categorical_features]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.754352Z","iopub.execute_input":"2021-08-24T18:37:00.754723Z","iopub.status.idle":"2021-08-24T18:37:00.76787Z","shell.execute_reply.started":"2021-08-24T18:37:00.754684Z","shell.execute_reply":"2021-08-24T18:37:00.76705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_num.shape,test_cat.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.769192Z","iopub.execute_input":"2021-08-24T18:37:00.769567Z","iopub.status.idle":"2021-08-24T18:37:00.781306Z","shell.execute_reply.started":"2021-08-24T18:37:00.769528Z","shell.execute_reply":"2021-08-24T18:37:00.780292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finding skewness of all features\nskewness = test_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.782715Z","iopub.execute_input":"2021-08-24T18:37:00.783114Z","iopub.status.idle":"2021-08-24T18:37:00.812809Z","shell.execute_reply.started":"2021-08-24T18:37:00.783072Z","shell.execute_reply":"2021-08-24T18:37:00.811991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we are selecting features where skewness is greater than 0.5 to fix their skewness\n#skewness = skewness[abs(skewness)>0.5]\n#len(skewness.index)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.813925Z","iopub.execute_input":"2021-08-24T18:37:00.814171Z","iopub.status.idle":"2021-08-24T18:37:00.817701Z","shell.execute_reply.started":"2021-08-24T18:37:00.814146Z","shell.execute_reply":"2021-08-24T18:37:00.816732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# applying log tranform\n#test_num[skewness.index] = np.log1p(test_num[skewness.index])","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.818935Z","iopub.execute_input":"2021-08-24T18:37:00.819198Z","iopub.status.idle":"2021-08-24T18:37:00.828725Z","shell.execute_reply.started":"2021-08-24T18:37:00.819164Z","shell.execute_reply":"2021-08-24T18:37:00.827884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cat.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.829739Z","iopub.execute_input":"2021-08-24T18:37:00.830006Z","iopub.status.idle":"2021-08-24T18:37:00.857416Z","shell.execute_reply.started":"2021-08-24T18:37:00.829979Z","shell.execute_reply":"2021-08-24T18:37:00.856436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cat = pd.get_dummies(test_cat)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.858782Z","iopub.execute_input":"2021-08-24T18:37:00.859079Z","iopub.status.idle":"2021-08-24T18:37:00.8954Z","shell.execute_reply.started":"2021-08-24T18:37:00.859051Z","shell.execute_reply":"2021-08-24T18:37:00.894142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_cat.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.896718Z","iopub.execute_input":"2021-08-24T18:37:00.897019Z","iopub.status.idle":"2021-08-24T18:37:00.917421Z","shell.execute_reply.started":"2021-08-24T18:37:00.896989Z","shell.execute_reply":"2021-08-24T18:37:00.916545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1 = pd.concat([test_cat,test_num],axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.918591Z","iopub.execute_input":"2021-08-24T18:37:00.91885Z","iopub.status.idle":"2021-08-24T18:37:00.925753Z","shell.execute_reply.started":"2021-08-24T18:37:00.918822Z","shell.execute_reply":"2021-08-24T18:37:00.924936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test1.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.927176Z","iopub.execute_input":"2021-08-24T18:37:00.927525Z","iopub.status.idle":"2021-08-24T18:37:00.935213Z","shell.execute_reply.started":"2021-08-24T18:37:00.927498Z","shell.execute_reply":"2021-08-24T18:37:00.934354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outliers","metadata":{}},{"cell_type":"markdown","source":"### Using 3 standard deviation to remove outliers ","metadata":{}},{"cell_type":"markdown","source":"**`Standard Deviation `** means simply it shows you how far away data point from the mean.<br>\nIf our dataset is normally distributed then most of the data fall under `1 standard deviation nearly 68% of data`.<br>\nHere we will use **`3 standard deviation` to `remove outliers`**. \nNearly **`99.7%`** of values are within **`3 standard deviation`** of the mean and the data `outside the 3 standard deviation` we consider them as `outlier's`. ","metadata":{}},{"cell_type":"code","source":"# set minimum and maximum threshold values to detect ouliers using standard deviation\nmin_threshold = train1.SalePrice.mean() - 3*train1.SalePrice.std()\nmax_threshold = train1.SalePrice.mean() + 3*train1.SalePrice.std()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.936374Z","iopub.execute_input":"2021-08-24T18:37:00.936754Z","iopub.status.idle":"2021-08-24T18:37:00.945615Z","shell.execute_reply.started":"2021-08-24T18:37:00.936723Z","shell.execute_reply":"2021-08-24T18:37:00.94463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_threshold,max_threshold","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.946651Z","iopub.execute_input":"2021-08-24T18:37:00.947152Z","iopub.status.idle":"2021-08-24T18:37:00.957139Z","shell.execute_reply.started":"2021-08-24T18:37:00.947124Z","shell.execute_reply":"2021-08-24T18:37:00.956277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing the outlier's from dataset\ntrain1 = train1[(train1.SalePrice<max_threshold) & (train1.SalePrice)>min_threshold]","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.958406Z","iopub.execute_input":"2021-08-24T18:37:00.958687Z","iopub.status.idle":"2021-08-24T18:37:00.971375Z","shell.execute_reply.started":"2021-08-24T18:37:00.958656Z","shell.execute_reply":"2021-08-24T18:37:00.970427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.shape,test1.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.972835Z","iopub.execute_input":"2021-08-24T18:37:00.973116Z","iopub.status.idle":"2021-08-24T18:37:00.979833Z","shell.execute_reply.started":"2021-08-24T18:37:00.973089Z","shell.execute_reply":"2021-08-24T18:37:00.979253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling ","metadata":{}},{"cell_type":"code","source":"# importing all the required library for modeling here we are going to use statsmodels \nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.982428Z","iopub.execute_input":"2021-08-24T18:37:00.982684Z","iopub.status.idle":"2021-08-24T18:37:00.98751Z","shell.execute_reply.started":"2021-08-24T18:37:00.982657Z","shell.execute_reply":"2021-08-24T18:37:00.98695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [col for col in train1.columns if col not in test1.columns]\ncols.remove('SalePrice')\ntrain1 = train1.drop(cols,axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:00.988674Z","iopub.execute_input":"2021-08-24T18:37:00.989221Z","iopub.status.idle":"2021-08-24T18:37:01.003302Z","shell.execute_reply.started":"2021-08-24T18:37:00.989177Z","shell.execute_reply":"2021-08-24T18:37:01.002381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now our task will be training and testing the data, so we need to drop the `SalePrice` from the training dataset and will assign it to the `y`","metadata":{}},{"cell_type":"code","source":"# assining the required data to the respective variables  \nX = train1.drop(['SalePrice'],axis=1)\ny = train1['SalePrice']","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:01.007185Z","iopub.execute_input":"2021-08-24T18:37:01.00774Z","iopub.status.idle":"2021-08-24T18:37:01.013916Z","shell.execute_reply.started":"2021-08-24T18:37:01.007697Z","shell.execute_reply":"2021-08-24T18:37:01.013041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X1 = sm.add_constant(X)\n#test2 = sm.add_constant(test1)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:01.015527Z","iopub.execute_input":"2021-08-24T18:37:01.016105Z","iopub.status.idle":"2021-08-24T18:37:01.02434Z","shell.execute_reply.started":"2021-08-24T18:37:01.016072Z","shell.execute_reply":"2021-08-24T18:37:01.023605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking shapes\ntest1.shape,train1.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:01.02577Z","iopub.execute_input":"2021-08-24T18:37:01.026284Z","iopub.status.idle":"2021-08-24T18:37:01.036446Z","shell.execute_reply.started":"2021-08-24T18:37:01.026251Z","shell.execute_reply":"2021-08-24T18:37:01.035687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train1.shape","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:01.037554Z","iopub.execute_input":"2021-08-24T18:37:01.038085Z","iopub.status.idle":"2021-08-24T18:37:01.047613Z","shell.execute_reply.started":"2021-08-24T18:37:01.038055Z","shell.execute_reply":"2021-08-24T18:37:01.04673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = sm.OLS(y, X).fit()\n#predictions = model.predict(test2)\n#print(\"ROOT MEAN SQUARED ERROR : \",math.sqrt(sum((y-predictions)**2)/len(y)))\n","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:01.049Z","iopub.execute_input":"2021-08-24T18:37:01.050178Z","iopub.status.idle":"2021-08-24T18:37:01.05521Z","shell.execute_reply.started":"2021-08-24T18:37:01.050146Z","shell.execute_reply":"2021-08-24T18:37:01.054562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parity plot for statsmodels OLS\n''''plt.scatter(predictions,y,color='blue')\nplt.title('Linear Regression')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.plot([10.5,13.5],[10.5,13.5],c='red')\nplt.show()'''","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:37:01.056563Z","iopub.execute_input":"2021-08-24T18:37:01.057059Z","iopub.status.idle":"2021-08-24T18:37:01.067288Z","shell.execute_reply.started":"2021-08-24T18:37:01.057029Z","shell.execute_reply":"2021-08-24T18:37:01.06635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"trying to optimize model with **XGBoost** stands for **extreme gradient boosting**","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\nX_train,X_valid,y_train,y_valid = train_test_split(X,y,random_state=0)\nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmodel.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\npredictions1 = model.predict(X_valid)\nprint(\"ROOT MEAN SQUARED ERROR : \",math.sqrt(sum((y_valid-predictions1)**2)/len(y_valid)))","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:38:00.620666Z","iopub.execute_input":"2021-08-24T18:38:00.621511Z","iopub.status.idle":"2021-08-24T18:38:02.280838Z","shell.execute_reply.started":"2021-08-24T18:38:00.621459Z","shell.execute_reply":"2021-08-24T18:38:02.279948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parity plot \nplt.scatter(predictions1,y_valid,color='blue')\nplt.title('Linear Regression')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.plot([50000,700000],[50000,700000],c='red')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:39:02.619188Z","iopub.execute_input":"2021-08-24T18:39:02.619543Z","iopub.status.idle":"2021-08-24T18:39:02.814624Z","shell.execute_reply.started":"2021-08-24T18:39:02.619508Z","shell.execute_reply":"2021-08-24T18:39:02.813937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above scatterplot comparing the actual values against predicted values in an easy understandable way.","metadata":{}},{"cell_type":"code","source":"# lets now test for the test set\npredictions = model.predict(test1)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:40:09.475372Z","iopub.execute_input":"2021-08-24T18:40:09.475704Z","iopub.status.idle":"2021-08-24T18:40:09.493361Z","shell.execute_reply.started":"2021-08-24T18:40:09.475675Z","shell.execute_reply":"2021-08-24T18:40:09.492378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\nfinal_submission = pd.DataFrame({'Id':submission['Id'],'SalePrice':predictions})\nfinal_submission.to_csv('submission1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-24T18:40:10.941675Z","iopub.execute_input":"2021-08-24T18:40:10.942008Z","iopub.status.idle":"2021-08-24T18:40:10.957119Z","shell.execute_reply.started":"2021-08-24T18:40:10.941979Z","shell.execute_reply":"2021-08-24T18:40:10.956119Z"},"trusted":true},"execution_count":null,"outputs":[]}]}