{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ***PyCaret Extended Tutorial for Classification, Regression and Clustering***\n\n\n# *1. Evaluate 3 different types of modelling*\n* **Classification:** We will use Titanic challenge dataset\n* **Regression:** We will use House prices advanced regression challenge dataset\n* **Clustering:** We will use Titanic challenge dataset again\n\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# *2. Installing necessary libraries*","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install pycaret\n!pip install pandas_profiling","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *3. Importing neccessary libraries*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd \nimport pandas_profiling as pp","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *4. Read the data*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#read the data\ntrain_titanic=pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_titanic=pd.read_csv('/kaggle/input/titanic/test.csv')\ntrain_house=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest_house=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data dimensions\nprint('Titanic train:',train_titanic.shape)\nprint('Titanic test:',test_titanic.shape)\nprint('House price train:',train_house.shape)\nprint('House price test:',test_house.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *5. Classification*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#pandas profiling report for Titanic train data\npp.ProfileReport(train_titanic)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****Warning: PyCaret setup will work for last imported PyCaret library. \n# So, if you import clustering library, you will get an error for classification setup****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#import pycaret classification library\nfrom pycaret.classification import * #for classification\n#classification setup\nclassification_setup =setup(data = train_titanic, \n             target = 'Survived',\n             numeric_imputation = 'mean', #fill missing value with mean for numeric features\n             categorical_features = ['Sex','Embarked','Pclass','Ticket','Cabin'], #we know categorical features from pandas profiling report\n             ignore_features = ['Name','PassengerId'],\n             train_size=0.8, #0.7 as default\n             high_cardinality_features=['Cabin'],\n             normalize=True,\n             normalize_method='minmax',\n             handle_unknown_categorical=True,\n             unknown_categorical_method='most_frequent',  #fill missing value with most frequent value for categorical features\n             remove_outliers=True, #it automatically applies PCA for removing outliers,\n             outliers_threshold=0.05, #By default, 0.05 is used which means 0.025 of the values on each side of the distribution’s tail are dropped from training data.\n             silent=True,\n             profile=True #a data profile for Exploratory Data Analysis will be displayed in an interactive HTML report. It also generates pandas profiling report\n     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#comparing models\nblacklist_models = ['svm','rbfsvm','mlp']\n\ncompare_models(\n    blacklist=blacklist_models, #blacklisted models won't work.\n    fold = 5,\n    sort = 'Accuracy', ## competition metric\n    turbo = True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating model with selected estimator.\nxgb=create_model(estimator='xgboost',fold=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tune the model\ntuned_xgb = tune_model('xgboost')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensembling a trained xgboost model\nxgb_bagged = ensemble_model(xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(xgb, plot = 'boundary')# Decision Boundary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(xgb, plot = 'pr')# Precision Recall Curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(xgb, plot = 'vc')# Validation Curve","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_model(xgb, plot='confusion_matrix') # Confusion Matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluating model is a good option. Because you don't need to plot different plots seperately. It provides all of them in the same cell.\nevaluate_model(xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#As you remember, we split 80% of the data for training. The rest of the data can be used for holdout prediction.\nxgb_holdout_pred = predict_model(xgb)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Or you can use your test data for prediction.\ntitanic_prediction =  predict_model(xgb, data=test_titanic)\ntitanic_prediction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare the submission file\ntitanic_prediction['Survived'] = round(titanic_prediction['Score']).astype(int)\nsubmission=titanic_prediction[['PassengerId','Survived']]\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *6. Regression*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#pandas profiling report for House price train data\npp.ProfileReport(train_house)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pycaret.regression import * #for regression\n\n#regression setup\nregression_setup =setup(data = train_house, \n             target = 'SalePrice',\n             numeric_imputation = 'mean', #fill missing value with mean for numeric features\n             categorical_features = ['MSZoning','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType',\n                                     'Street','LotShape','LandContour','LotConfig','LandSlope','Neighborhood',   \n                                     'Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl',    \n                                     'MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond',   \n                                     'BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir',   \n                                     'Electrical','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive',\n                                     'SaleCondition'], #we know categorical features from pandas profiling report\n             ignore_features = ['Id'],\n             train_size=0.8, #0.7 as default\n             normalize=True,\n             normalize_method='minmax',\n             handle_unknown_categorical=True,\n             unknown_categorical_method='most_frequent',  #fill missing value with most frequent value for categorical features\n             remove_outliers=True, #it automatically applies PCA for removing outliers,\n             outliers_threshold=0.05, #By default, 0.05 is used which means 0.025 of the values on each side of the distribution’s tail are dropped from training data.\n             silent=True,\n             profile=True #a data profile for Exploratory Data Analysis will be displayed in an interactive HTML report. It also generates pandas profiling report\n     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bl_models = ['ransac', 'tr', 'rf', 'et', 'ada', 'gbr']\n\ncompare_models(\n    blacklist = bl_models,\n    fold = 5,\n    sort = 'MAE', ## competition metric\n    turbo = True\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#creating model.\nlgbm = create_model(\n    estimator='lightgbm',\n    fold=5\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluating model.\nevaluate_model(lgbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# use rest of the training data for holdout prediction.\nlgbm_holdout_pred = predict_model(lgbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prediction with test data.\nhouse_prediction =  predict_model(lgbm, data=test_house)\nhouse_prediction.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#prepare the submission file\nhouse_prediction.rename(columns={'Label':'SalePrice'}, inplace=True)\nhouse_prediction[['Id','SalePrice']].to_csv('submission_house.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *7. Clustering*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pycaret.clustering import * #for clustering\n#clustering setup doesn't support silent True options. So, you need to hit ENTER manually.\n#if silent is set True, it means that you approve data types which were inferred from PyCaret.\n\n#clustering setup\nclustering_setup =setup(data = train_titanic, \n             numeric_imputation = 'mean', #fill missing value with mean for numeric features\n             categorical_features = ['Sex','Embarked','Pclass','Ticket','Cabin'], #we know categorical features from pandas profiling report\n             ignore_features = ['Name','PassengerId'],\n             high_cardinality_features=['Cabin'],\n             normalize=True,\n             normalize_method='minmax',\n             handle_unknown_categorical=True,\n             unknown_categorical_method='most_frequent',  #fill missing value with most frequent value for categorical features\n             verbose=False        \n    \n     )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#use k-means for clustering. You can check other clustering algorithms from https://pycaret.org/clustering/\nkmeans = create_model('kmeans')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#assigning data to clusters.\nkmeans_df = assign_model(kmeans)\nkmeans_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PCA Plot\nplot_model(kmeans) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Also you can plot Silhouette\nplot_model(kmeans, plot='silhouette') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Or you can plot Elbow etc.\nplot_model(kmeans, plot='elbow') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tunes the num_clusters model parameter using a predefined grid with the objective of optimizing a supervised learning metric as defined in the optimize param. \ntuned_kmeans = tune_model(model = 'kmeans', supervised_target = 'Survived')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Also, you can specify estimator for tuning.\ntuned_kmeans = tune_model(model = 'kmeans', supervised_target = 'Survived', estimator='xgboost')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# *8. Some last words*\n\nIn one of my previous notebooks, I prepared a Simple Guide of PyCaret for how to use it for regression type of problems. After getting good feedbacks for PyCaret, I just want to prepare a new extended guide for PyCaret. My main objective was not improving the results. I just want to show as many as features and functions of PyCaret. You can find more detailed information from https://pycaret.org/\n\nIf you have any ideas to feedback please let me know in comments, and if you liked my work please don't forget to vote, thank you!\n\n![](https://pycaret.org/wp-content/uploads/2020/03/Divi93_43.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}