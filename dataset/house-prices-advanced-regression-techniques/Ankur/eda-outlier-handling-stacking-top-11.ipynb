{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Housing Prices Prediction - Stacking,Feature Engineering & Outlier Handing\n\nWe would follow the below major steps:-\n\n1. EDA\n\n2. Handling missing  values\n\n3. Feature Engineering\n\n4. Outlier handling\n\n5. Modelling\n\nThere are various steps other than the ones listed above which you would encounter in the notebook.\n\nPS - Would suggest everyone to please check the Outlier Handling section as I haven't seen many notebooks using this method. Most of the notebooks have used manual methods. Cheers !!\n\nAlso please leave your suggestions to improve this notebook in the comments. Would be of great help in learning.\n\n### Edit - For some interesting hyperparameter tuning techniques, check out the link below:-\n\nhttps://www.kaggle.com/ankur123xyz/advanced-hyperparameter-tuning-techniques\n\nLet us start of by importing all the necessary packages.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm,skew\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor , StackingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge,Lasso,LinearRegression\nfrom sklearn.model_selection import KFold,cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom xgboost import XGBRegressor\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We import both the train and test sets into different dataframes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should check the dimensions of these dataframes before proceeding.\nWe have 1460 and 1459 rows in the train and test set respectively. We have 80 features in both the train and test set. The extra column in the train set would be the prediction.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Dimensions of train set are:-\",train.shape[0],\",\",train.shape[1])\nprint(\"Dimensions of test set are:-\",test.shape[0],\",\",test.shape[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's expand our column view since we have 81 columns in the train set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option(\"display.max_columns\",81)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having a peek at the train set we can see all features along with the value we have to predict - SalePrice","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"link1\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n## 1. Exploratory Data Analysis\n\nTo get a sense of which features are important and from where do we start we can create a correlation matrix. By default it uses the pearson correlation coefficent.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = train.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having a look at the correlation of different numerical feature with SalePrice.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr[\"SalePrice\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can zoom in to the 10 most important variables according to the pearson correlation coefficient and check the matrix.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x=corr[\"SalePrice\"].sort_values(ascending=False)[:11].index\nmask = np.zeros_like(corr.loc[x,x])\nmask[np.triu_indices_from(mask)]=True\n\nsns.heatmap(corr.loc[x,x],mask=mask,annot=True,cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us pair the 5 most important variables according to our matrix with sale price.\\n\nWe can see some outliers, which we will take care of later.\n\nThe OverallQual and GarageCars plot with Sale Price will be better respresented with box plots, since they are ordinal features.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(train[x[0:6]],x_vars=list(x[1:6]),y_vars=[x[0]],diag_kind=\"kde\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OverallQual and GarageCards have a positive correlation with the Sale Price. Price for houses with 4 garages are lower which seems to be an anomaly or there might be some other feature which is impacting the . We have only 5 records with 4 Garage cars which can cause such cases.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(1,2,figsize=(15,8))\nsns.boxplot(\"OverallQual\",\"SalePrice\",data=train,ax=ax[0])\nsns.boxplot(\"GarageCars\",\"SalePrice\",data=train,ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now it is time to combine our train and test sets since we need to preprocess it the same way so that we can feed it later into our model.\n\nAlso we would drop the SalePrice column before merging and copy it to another series - y.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"index = len(train)\ny=train[\"SalePrice\"]\ntrain.drop(\"SalePrice\",axis=1,inplace=True)\ndataset = pd.concat([train,test]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"link2\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n## 2. Handling missing values\n\n\nChecking for null values in our new combined dataset.\n    \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.isnull().sum()[dataset.isnull().sum()>0].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We impute the missing values for zone using neighborhood as an indicator.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_mszon=dataset.groupby([\"Neighborhood\",\"MSZoning\"])[\"Id\"].count().reset_index().groupby([\"Neighborhood\"])\nmaximum_mszon = df_mszon.max()\nmaximum_mszon= maximum_mszon.drop(\"Id\",axis=1)\n\nmax_dict_mszon = maximum_mszon.to_dict()\ndef mapper_mszon(x):\n    for index,val in zip(max_dict_mszon,max_dict_mszon.values()):\n        for index1,val1 in val.items():\n            if(x==index1):\n                return val1\n\ndataset.loc[dataset[\"MSZoning\"].isnull(),\"MSZoning\"] = dataset.loc[dataset[\"MSZoning\"].isnull(),\"Neighborhood\"].apply(lambda x: mapper_mszon(x))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly we impute the values for LotFrontage using Neighborhood and LotConfig as indicators. We taken the median of similar Neighborhood and LotFrontage values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_lotf=dataset.groupby([\"Neighborhood\",\"LotConfig\"])[\"LotFrontage\"].mean()\n\nmax_dict_lotf = df_lotf.to_dict()\ndef mapper_lotf(x1,x2):\n    for index,val in zip(max_dict_lotf,max_dict_lotf.values()):\n        if((x1==index[0]) & (x2==index[1])):\n            return val\n\ndataset.loc[dataset[\"LotFrontage\"].isnull(),\"LotFrontage\"] = dataset.loc[dataset[\"LotFrontage\"].isnull(),[\"Neighborhood\",\"LotConfig\"]].apply(lambda x: mapper_lotf(x[0],x[1]),axis=1)\ndataset[\"LotFrontage\"] = dataset.groupby([\"Neighborhood\"])[\"LotFrontage\"].transform(lambda x: x.median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Base on the definition of these features give in the data description we impute the rest of the values to NA or 0 which signifies absence of that feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[\"Alley\"].fillna(\"NA\",inplace=True)\ndataset[\"Utilities\"].fillna(\"NA\",inplace=True)\ndataset[\"Exterior1st\"].fillna(\"NA\",inplace=True)\ndataset[\"Exterior2nd\"].fillna(\"NA\",inplace=True)\ndataset[\"MasVnrType\"].fillna(\"NA\",inplace=True)\ndataset[\"MasVnrArea\"].fillna(0,inplace=True)\ndataset[\"BsmtQual\"].fillna(\"NA\",inplace=True)\ndataset[\"BsmtCond\"].fillna(\"NA\",inplace=True)\ndataset[\"BsmtExposure\"].fillna(\"No\",inplace=True)\ndataset[\"BsmtFinType1\"].fillna(\"NA\",inplace=True)\ndataset[\"BsmtFinSF1\"].fillna(0,inplace=True)\ndataset[\"BsmtFinType2\"].fillna(\"NA\",inplace=True)\ndataset[\"BsmtFinSF2\"].fillna(0,inplace=True)\ndataset[\"BsmtUnfSF\"].fillna(0,inplace=True)\ndataset[\"TotalBsmtSF\"].fillna(0,inplace=True)\ndataset[\"BsmtFullBath\"].fillna(0,inplace=True)\ndataset[\"BsmtHalfBath\"].fillna(0,inplace=True)\ndataset[\"FireplaceQu\"].fillna(\"NA\",inplace=True)\ndataset[\"GarageType\"].fillna(\"NA\",inplace=True)\ndataset[\"GarageYrBlt\"].fillna(0,inplace=True)\ndataset[\"GarageFinish\"].fillna(\"NA\",inplace=True)\ndataset[\"GarageCars\"].fillna(0,inplace=True)\ndataset[\"GarageArea\"].fillna(0,inplace=True)\ndataset[\"GarageQual\"].fillna(\"NA\",inplace=True)\ndataset[\"GarageCond\"].fillna(\"NA\",inplace=True)\ndataset[\"PoolQC\"].fillna(\"NA\",inplace=True)\ndataset[\"Fence\"].fillna(\"NA\",inplace=True)\ndataset[\"MiscFeature\"].fillna(\"NA\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For features that can't be missing we have taken the mode and the default value as per data description.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[\"Functional\"].fillna(\"Typ\",inplace=True)\ndataset[\"Electrical\"].fillna(dataset[\"Electrical\"].mode()[0],inplace=True)\ndataset[\"KitchenQual\"].fillna(dataset[\"KitchenQual\"].mode()[0],inplace=True)\ndataset[\"SaleType\"].fillna(\"Oth\",inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the SubClass are categories and not of numeric data type we covert the feature to category type.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset[\"MSSubClass\"]=dataset[\"MSSubClass\"].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check the distribution of our dependent variable. It doesnt seem to be a normal distribution and leans towards a positive skewed distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x=sns.distplot(y)\nx.set_title(\"Distribution plot for Sale Price\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us log transform it and recheck. Post log transform it is close to a normal distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x=sns.distplot(np.log1p(y),fit=norm)\nx.set_title(\"Distribution plot for Sale Price\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We log transform SalePrice as we have seen earlier it helps in getting a normal distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y= np.log1p(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"link3\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n## 3. Feature Engineering\n\nWe have seen quite a few of the non numerical features have ordinal nature. We have transformed them below to numeric type to maintain the ordinal relation.\n    \n    \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = dataset.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2,\"NA\":0},\n                       \"BsmtCond\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtQual\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageFinish\" : {\"Fin\" : 3, \"RFn\" : 2, \"Unf\" : 1, \"NA\" : 0},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"NA\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2, \"NA\":0},\n                       \"Utilities\" : {\"ELO\" : 1, \"NASeWa\" : 2, \"NASewr\" : 3, \"AllPub\" : 4}}\n                     )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have created flags for different feature of the house to check if it is available or not.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cats = [\"basement_flag\",\"fire_flag\",\"wooddeck_flag\",\"garage_flag\",\"pool_flag\",\"fence_flag\"]\ndataset[\"basement_flag\"]=np.where(dataset[\"TotalBsmtSF\"]>0,1,0)\ndataset[\"fire_flag\"]=np.where(dataset[\"Fireplaces\"]>0,1,0)\ndataset[\"wooddeck_flag\"]=np.where(dataset[\"WoodDeckSF\"]>0,1,0)\ndataset[\"garage_flag\"]=np.where(dataset[\"GarageArea\"]>0,1,0)\ndataset[\"porch_flag\"]=np.where((dataset[\"OpenPorchSF\"]+dataset[\"EnclosedPorch\"]+dataset[\"3SsnPorch\"]\\\n                                +dataset[\"ScreenPorch\"])>0,1,0)\ndataset[\"pool_flag\"] = np.where(dataset[\"PoolArea\"]>0,1,0)\ndataset[\"fence_flag\"] = np.where(dataset[\"Fence\"]==\"NA\",1,0)\ndataset[cats]=dataset[cats].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We create a TotalSF feature which includes the surface area for basement, 1st floor and the 2nd floor.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset['TotalSF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"link4\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n\n## 4. Outlier Handling \nOutlier handling is done using the mean and the standard deviation. We have taken three features which have highest correlation to the sale price based on which outliers have been removed.\n    \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"outlier_features = [\"GrLivArea\",\"TotalSF\",\"GarageArea\"]\n\nmeans,sds = np.mean(dataset[outlier_features]),np.std(dataset[outlier_features])\nlower,upper = means - 2.5*sds , means + 2.5*sds\ndef compare(x):\n    count=0\n    for col in outlier_features:\n        if x[col]>lower[col] and x[col]<upper[col]:\n            count=count+1\n    if count==len(outlier_features):\n        return True\n    else: \n        return False\n\ny= y.array\nsub_data = dataset.loc[:index-1]  \ntest_1 = dataset.loc[index:,:]\n\ndata_train_new = sub_data.loc[sub_data.apply(lambda x: compare(x),axis=1)]\ny= y[sub_data.apply(lambda x: compare(x),axis=1)]\n\nindex = len(data_train_new)\ndataset = pd.concat([data_train_new,test_1]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For all the numeric features we have checked for skew and have log transformed those features which have high skew (greater than 0.5 in our case) to get a normal distribtuion.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_feats = dataset.dtypes[(dataset.dtypes != \"object\") & (dataset.dtypes != \"category\")].index\nskewed_feats = dataset[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness = skewness[abs(skewness) > 0.5]\n\nskewed_features = skewness.index\ndataset[skewed_features] = np.log1p(dataset[skewed_features])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are dropping the Id feature since it would not add any useful information to the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.drop(\"Id\",axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We one hot encode the dataset and then split it to the orginal train set (with outliers removed) and the test set(all records intact).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = pd.get_dummies(dataset, drop_first= True)\ntrain_1 = dataset.loc[:index-1,:]\ntest_1 = dataset.loc[index:,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us now split out train set further in to train and test sets for validation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(train_1,y,test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we have a high number of variables and is prone to have outliers as we worked only on a few features we should opt for Robust Scaler transformation to handle the outliers.\nBelow is a good read on different scaling methods:-\nhttps://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\nX_train= rs.fit_transform(X_train)\nX_test = rs.transform(X_test)\nX_submit = rs.transform(test_1.values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"link5\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n\n## 5. Modelling\n\nWe use 10 fold KFold for cross validation.\n\nWe have used the following models:-\n1. Random Forest\n2. XGBoost\n3. Gradient Boosting\n4. Support Vector\n5. Ridge (L2)\n6. Lasso (L1)\n\nWe have run these models individually using a grid search for the right set of parameters.\nThen we have used the best models in all of them and used them in our stacking model.\n    \n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=10,shuffle=True,random_state=42)\nrf = RandomForestRegressor(n_estimators=500)\nxgb = XGBRegressor()\ngb= GradientBoostingRegressor()\nsvr =SVR(C=70,epsilon=.115)\nridge = Ridge(alpha=1.75,solver =\"lsqr\")\nlasso = Lasso(alpha=.0009)\n\nclf=[\\\n     (\"XGBoost\",xgb,{\"n_estimators\":[200],\"max_depth\":[3],\"learning_rate\":[.1],\"subsample\":[1],\"colsample_bytree\":[1],\"gamma\":[0],\"lambda\":[.001,.01]})\\\n     ,(\"Gradient Boost\",gb,{})\\\n     ,(\"SVR\",svr,{\"C\":[60,65,70,75,80],\"epsilon\":[.11,.115,.12]})\\\n    ,(\"Ridge\",ridge,{\"alpha\":[1.4,1.5,1.6,1.75,1.8]})\\\n     ,(\"Lasso\",lasso,{\"alpha\":[.0005,.0006,.0007,.001,.01,.1]})]\n\nest=[]\nresults_data=pd.DataFrame(columns=[\"Name\",\"Train_Score\",\"Test_Score\"])\n\ni=0\nfor name,reg,param_grid in clf:\n    gs= GridSearchCV(reg,param_grid=param_grid,cv=kf,scoring=\"neg_mean_squared_error\")\n    gs.fit(X_train,y_train)\n    y_pred = gs.best_estimator_.predict(X_test)\n    test_score = np.sqrt(mean_squared_error(y_test, y_pred))\n    results_data.loc[i,]= [name,-gs.best_score_,test_score]\n    i=i+1\n    est.append([name,gs.best_estimator_])\n    \nsc = StackingRegressor(estimators=est[1:],final_estimator= ridge,cv=kf)\nsc.fit(X_train,y_train)\ny_trn= sc.predict(X_train)\ny_tst= sc.predict(X_test)    \ntrain_score = np.sqrt(mean_squared_error(y_train, y_trn))\ntest_score = np.sqrt(mean_squared_error(y_test, y_tst))\nresults_data.loc[i,]= [\"Stack\",train_score,test_score]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking the train and test results for all the models.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"results_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since stacking gave us the best scores we would used that to get the predictions to be used to submit our scores.\nWe would do an inverse log transform since we had log transformed the SalePrice earlier.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_submit= np.expm1(sc.predict(X_submit))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a dataset which would be submitted for evaluation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_submit= pd.DataFrame({\"Id\":test.Id,\"SalePrice\":y_submit})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exporting our submission dataset to a csv file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data_submit.to_csv(\"submit_housing.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}