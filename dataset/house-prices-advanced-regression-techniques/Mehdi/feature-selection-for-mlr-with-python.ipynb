{"nbformat":4,"cells":[{"metadata":{},"source":"## Feature selection for multiple linear regression using Scikit-learn\n\nI'll explore the following questions:\n\n* Which properties of a house most affect the final sale price?\n* How effectively can we predict the sale price from just its properties?\n\nThe code is in two seperate parts. In the first part, I have written three seperate functions as follows:\n\n* `transform_features()` : transforms features in the dataset to something meaningful for analysis\n* `select_features()` : feature selection happens here\n* `train_and_test()` : we will train and test our model and this function reports the final rmse of the model.\n\nSecond part summarizes how I came up with the way these functions are and have provided a backup calculation for the rational of these functions.","cell_type":"markdown"},{"metadata":{"_cell_guid":"961f0a58-8c16-4b0c-a931-4689aa31eb8f","_uuid":"74f9d95cc17555c19e84600b2303b882ced67812"},"source":"## Part 1: Creating functions\nThis part is written after analysing the dataset in the second part. By analysing dataset in the second part I got an idea of how the functions should look like and what should be flow of infromation.","cell_type":"markdown"},{"outputs":[],"metadata":{"collapsed":true,"_cell_guid":"ee1eb77c-36c2-432e-8722-771458745b29","_uuid":"77c5e91c94d9b6eb0368f16ba7b7ba01b2bd0e1b"},"source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import linear_model\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"473285a5-85a1-423e-8c69-662167605e38","_uuid":"ba4f5b693dc0e61beef6c1c58a7bc8ee3ac0a352"},"source":"#### 1.1 Feature Engineering","cell_type":"markdown"},{"outputs":[],"metadata":{"collapsed":true,"_cell_guid":"9d2aa9d5-aa2d-47b6-8d01-41cee6b0078d","_uuid":"c941633bb02dc8d616c124d4f6edbcbd6a5ef577"},"source":"def transform_features(df,null_cutoff):\n    #dropping columns with more then a missing values\n    null_values=df.isnull().sum()\n    dorp_missing_values=null_values[null_values>(null_cutoff*len(null_values))]\n    df=df.drop(dorp_missing_values.index, axis=1)\n    \n    # counting null values in text columns\n    text_cols_nullcount=df.select_dtypes(include=['object']).isnull().sum().sort_values(ascending=False)\n    text_cols_nullcols=text_cols_nullcount.index\n    for col in text_cols_nullcols:\n        mostcounts=df[col].value_counts().index.tolist()\n        df[col]=df[col].fillna(mostcounts[0]) #replacing the missing column in a text with the highest number of values\n    \n    #missing values in numerical columns \n    num_cols=df.select_dtypes(include=['integer','float']).columns #selecting numerical columns\n    num_null_counts=df[num_cols].isnull().sum().sort_values(ascending=False) #counting null values in columns\n    num_null_cols=num_null_counts[num_null_counts!=0].index #selecting the ones that have missing values\n    df=df.fillna(df[num_null_cols].mode().to_dict(orient='records')[0]) #replacing missing with mode\n    \n    #transfomring year sold and year built into a meaningful feature\n    years_sold = df['YrSold'] - df['YearBuilt']\n    years_since_remod = df['YrSold'] - df['YearRemodAdd']\n    df['Years Before Sale'] = years_sold\n    df['Years Since Remod'] = years_since_remod\n    #df = df.drop([1702, 2180, 2181], axis=0) #these rows caused negative values for both of these features\n\n    #drop columns that are not meaningful in ML, or they leak information in sale.\n    df = df.drop([\"Id\", \"MoSold\", \"SaleCondition\", \"SaleType\", \"YearBuilt\", \"YearRemodAdd\"], axis=1)\n    return df","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"77431931-5a4c-4bd6-8431-e42a1d0359d2","_uuid":"9ae2128b1a9f4e5412593475685096c0c4971e70"},"source":"#### 1.2 Feature selection","cell_type":"markdown"},{"outputs":[],"metadata":{"collapsed":true,"_cell_guid":"a5e78656-be1d-4410-b9f8-e75731f1cefc","_uuid":"f9caef78e3961b56f4bca69663d7467008527dbe"},"source":"def select_features(df, coeff_threshold=0.4, uniq_threshold=10):\n    num_df=df.select_dtypes(include=['integer','float'])\n    corrs=num_df.corr()[target].abs()\n    #keeping only columns that have correlation with target higher than threshold\n    df=df.drop(corrs[corrs<coeff_threshold].index, axis=1)\n    \n    nominal_features = [\"PID\", \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\", \n                        \"Lot Config\", \"Neighborhood\",\"Condition 1\", \"Condition 2\", \"Bldg Type\",\n                        \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\",\"Exterior 2nd\",\n                        \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\",\n                        \"Misc Feature\", \"Sale Type\", \"Sale Condition\"]\n    \n    #check to see if our current dataset still keeps the nominal features\n    transform_cat_cols=[]\n    for col in nominal_features:\n        if col in df.columns:\n            transform_cat_cols.append(col)\n    \n    #getting rid of nominal columns with too many unique values\n    for col in transform_cat_cols:\n        len(df[col].unique())>uniq_threshold\n        df=df.drop(col, axis=1)\n        \n    #convert text columns to dummy variables\n    text_cols=df.select_dtypes(include=['object'])\n    for col in text_cols:\n        df[col]=df[col].astype('category')\n    \n    df=pd.concat([df,pd.get_dummies(df.select_dtypes(include=['category']))],axis=1)\n    \n    return df","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"18771e95-96f0-47d5-87d8-94acc518fd82","_uuid":"9ac1687ada404d097818f76c5d1953167d9b9f45"},"source":"#### 1.3 Model fitting\n\nParameter k deteremines the cross validation type. K=0 holdout, k=1 simple cross validation, else: k fold cross validation","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"8cec0392-d627-415b-ab11-8c9efdb3aa47","_uuid":"d28101ac2629683b5a4bf384284560b30142011e"},"source":"def train_and_test(df,k=0):\n    num_df=df.select_dtypes(include=['integer','float'])\n    features=num_df.columns.drop(target)\n    model=linear_model.LinearRegression()\n    \n    if k==0:\n        cut=int(num_df.shape[0]/2)\n        train=num_df.iloc[:cut]\n        test=num_df.iloc[cut:]\n        model.fit(train[feature],train[target])\n        prediction=model.predict(test[target])\n        mse = mean_squared_error(test[target], predictions)\n        rmse = np.sqrt(mse)\n\n        return rmse\n    elif k==1:\n        # Randomize *all* rows (frac=1) from `df` and return\n        shuffled_df = df.sample(frac=1, )\n        train = df[:1460]\n        test = df[1460:]\n        \n        model.fit(train[features], train[target])\n        predictions_one = model.predict(test[features])        \n        \n        mse_one = mean_squared_error(test[target], predictions_one)\n        rmse_one = np.sqrt(mse_one)\n        \n        model.fit(test[features], test[target])\n        predictions_two = model.predict(train[features])        \n        mse_two = mean_squared_error(train[target], predictions_two)\n        rmse_two = np.sqrt(mse_two)\n        \n        avg_rmse = np.mean([rmse_one, rmse_two])\n        print(rmse_one)\n        print(rmse_two)\n        return avg_rmse\n    else:\n        kf = KFold(n_splits=k, shuffle=True)\n        rmse_values = []\n        for train_index, test_index, in kf.split(df):\n            train = df.iloc[train_index]\n            test = df.iloc[test_index]\n            model.fit(train[features], train[target])\n            predictions = model.predict(test[features])\n            mse = mean_squared_error(test[target], predictions)\n            rmse = np.sqrt(mse)\n            rmse_values.append(rmse)\n        print(rmse_values)\n        avg_rmse = np.mean(rmse_values)\n        return avg_rmse\n    \ndf = pd.read_csv('../input/train.csv')\ntarget='SalePrice'\ntransform_df = transform_features(df,null_cutoff=0.05)\nfiltered_df = select_features(transform_df)\nrmse = train_and_test(filtered_df, k=4)\n\nprint('RMSE is:', rmse)","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"5be67953-ec6a-4cc0-b83b-eafd393fa5c8","_uuid":"e7e5382798efe387ac3e8ac676f0a92eb7190489"},"source":"## Part 2. Preparation for creating functions","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"19f3a93d-eced-467f-98b0-b17233f84e58","_uuid":"e96a88b8de4edf69f948c1ec4265716b47c73f13"},"source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport io\nimport urllib.request\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndata=pd.read_csv('../input/train.csv')\ntrain=data.iloc[:1120]\ntest=data.iloc[1120:]\ntarget='SalePrice'\ntrain.shape","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"3ca2eb21-08ef-4030-a085-318f72de61a8","_uuid":"0b1604c7b70ae0641dd4b28ee4854d9193387d6d"},"source":"corr=train.corr()\nfig,ax=plt.subplots(figsize=(8,6))\n\nsns.heatmap(corr)","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"42428f87-559d-4346-ac1a-efbed88c0413","_uuid":"a5daa6719d0fd51ecfa11870c3ce1010ffa3aa65"},"source":"sns.lmplot(x='GrLivArea',y='SalePrice',data=train)","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"ac2edec2-cbb0-4f10-a74a-fa82f6901d4b","_uuid":"094c57c368cf02cdcbf6cf0986e8f4003561b61e"},"source":"#### Linear model fitting using Scikit-learn","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"f981a43f-b3cf-4b40-a725-a431c56fb75c","_uuid":"75bb47c0b64a6fb2a631212decf5061fc2731235"},"source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nlr=LinearRegression()\nlr.fit(train[['GrLivArea']],train['SalePrice'])\nprint('Coeff is:',lr.coef_)\nprint('Intercept is:',lr.intercept_)\n\nprediction_test=lr.predict(test[['GrLivArea']])\nprediction_train=lr.predict(train[['GrLivArea']])\n\nmse_test=mean_squared_error(test['SalePrice'],prediction_test)\nmse_train=mean_squared_error(train['SalePrice'],prediction_train)\n\nrmse_test=mse_test**(1/2)\nrmse_train=mse_train**(1/2)\nprint('rmse_test is:',rmse_test)\nprint('rmse_train is:',rmse_train)","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"d0ff774b-6f8e-411d-99bb-ec29046f6237","_uuid":"fdcfd96a667ab6e02ee3217c01f4e0ad508ea090"},"source":"#### Multiple Regression ","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"ab889428-4252-4d7f-a01c-bb1203ee882a","_uuid":"4173a6effea3615194e2f004ff6db8f16c2d2d6d"},"source":"#only selects the integer and float columns.\nnum_train=train.select_dtypes(include=['int','float'])\nnum_train.info()","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"collapsed":true,"_cell_guid":"d4aea3d9-282f-4d67-b7d9-4ffb472cb961","_uuid":"fa503fb8e8bc9b5acf806f469ee360a0f02900a3"},"source":"#dropping invalid columns in regression\nnum_train=num_train.drop(['Id','MoSold','YrSold'],axis=1)\n\n#displaying numerical columns with no missing values\nnull_series=num_train.isnull().sum()\nfull_cols_train=null_series[null_series==0]\n#full_cols_train","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"c5c6aabd-ab33-4030-9142-b18eff111102","_uuid":"6d07a0420582e35c299687e4efe7fe245d854b0e"},"source":"#selecting a subset of train dataset that only contains numerical values and do not have missing values.\ntrain_subset=train[full_cols_train.index]\n\n#find the correlation of these features with target variable.\ncorrmatrix=train_subset.corr()\nsorted_corrs=corrmatrix['SalePrice'].abs().sort_values(ascending=False)\nsorted_corrs","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"66082116-530e-4d19-a395-ebcc1ed0d4ba","_uuid":"6cb30b3321dfa89b911bdfe95ae27e6b5649dad4"},"source":"#### Collinearity\nNow, let's define a cutoff ration for strong correlations, and explore collinearity.\n\nAlso, we define a cutoff variable for low variance features (b)","cell_type":"markdown"},{"outputs":[],"metadata":{"_cell_guid":"5a1a40ef-1fa7-4482-bc7d-bb271409fa3e","_uuid":"255d110211fbd1a66cf4b8f81cf5afe9ea86375c"},"source":"#strong correlation cut-off\na=0.25\n\n#cutoff value for features variance(features with lower variance than this number will be dropped)\nb=0.015\n\nstrong_corrs=sorted_corrs[sorted_corrs>a]\ncorrmatrix=train_subset[strong_corrs.index].corr()\nfig,ax=plt.subplots(figsize=(8,6))\nsns.heatmap(corrmatrix,ax=ax)","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"384c6041-aa25-4f92-92de-4986d9424d6a","_uuid":"6fee6cd1080cf7d19346559a62c32e3432d4af58"},"source":"`Garage Area` and `Garage Cars`, as well as `TotRms AbvGrd` and `Gr Liv Area` are highly correlated. We will drop one of each. \nAlso, we will check test dataset to see if all data are available for our final selected features. ","cell_type":"markdown"},{"outputs":[],"metadata":{"collapsed":true,"_cell_guid":"8603c9fe-d059-452d-8664-b02e4e06cb2b","_uuid":"ad7a39b7c4a6cdfac806864cc651965a2243d338"},"source":"final_corr_cols=strong_corrs.drop(['GarageCars', 'TotRmsAbvGrd'])\n\nfeatures=final_corr_cols.drop(['SalePrice']).index\n\nclean_test=test[final_corr_cols.index].dropna()","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"916f6b3f-82c2-4f13-83bd-c82569a19be0","_uuid":"c3cea163dd48c98af9a970e96eb5a1290b05281c"},"source":"target='SalePrice'\nlr=LinearRegression()\nlr.fit(train[features],train[target])\n\ntrain_predictions=lr.predict(train[features])\ntest_predictions=lr.predict(clean_test[features])\n\ntrain_rmse=np.sqrt(mean_squared_error(train[target],train_predictions))\ntest_rmse=np.sqrt(mean_squared_error(clean_test[target],test_predictions))\n\nprint('train rmse:',train_rmse)\nprint('test rmse:',test_rmse)","execution_count":null,"cell_type":"code"},{"metadata":{"_cell_guid":"2d3e040c-dfaa-4054-9607-fe50d3cf39a5","_uuid":"553fff6e832fd7871ef7efc93f6ee5487da8875a"},"source":"Now we'll try to remove features with lowest variance. A very low variance indicates that variables does not change with respect to our target, and hence does not affect the target values. But we'd like to bring all variances to 0-1 range in order to be consistent among our features. This task is called re-scalling.","cell_type":"markdown"},{"outputs":[],"metadata":{"collapsed":true,"_cell_guid":"51b4ed43-fc14-4c97-b5d1-e9859a59605c","_uuid":"6dac931f7b2eb446c9f2a2863bc4e255958a20fc"},"source":"rescale_train=(train[features]-train[features].min())/(train[features].max()-train[features].min())","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"f77e87d4-521b-491f-bfc9-3b130de9846d","_uuid":"d87f8ae95c59bfe8914075701585d5698ba63a7d"},"source":"sorted_vars=rescale_train.var().sort_values()\nsorted_vars","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"42e74336-7dcb-490e-a882-13cb09dcc2ea","_uuid":"5f56ac8c75b8746ea74a61da79042f91ae377b4b"},"source":"features.drop(sorted_vars[sorted_vars<b].index)\n#fetures.drop(sorted_vars<b)\n#features.drop(['Lot Area','Open Porch SF'])","execution_count":null,"cell_type":"code"},{"outputs":[],"metadata":{"_cell_guid":"cc6e6bca-9f06-4066-a283-71a7af75df91","_uuid":"25cde9eee590b22e49ec9d56a4c0a362d2a7c1a7"},"source":"#re-fittin the model with the new features list\nlr=LinearRegression()\nlr.fit(train[features],train[target])\n\ntrain_predictions=lr.predict(train[features])\ntest_predictions=lr.predict(clean_test[features])\n\ntrain_rmse_2=np.sqrt(mean_squared_error(train[target],train_predictions))\ntest_rmse_2=np.sqrt(mean_squared_error(clean_test[target],test_predictions))\n\nprint('train rmse_2:',train_rmse_2)\nprint('test rmse_2:',test_rmse_2)","execution_count":null,"cell_type":"code"}],"nbformat_minor":1,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"pygments_lexer":"ipython3","version":"3.6.4","mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","file_extension":".py"}}}