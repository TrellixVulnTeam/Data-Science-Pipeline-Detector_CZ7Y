{"cells":[{"metadata":{},"cell_type":"markdown","source":"![](https://www.mipropertygroup.com.au/wp-content/uploads/2016/10/house-prices-double.jpeg)"},{"metadata":{"_uuid":"72980386cbe06e42a5bd7b0c3f77a97a0226de65"},"cell_type":"markdown","source":"# **House Prices: Plotly, Pipelines and Ensembles**\nThis is my second Kaggle kernel for the House Prices competition.  \nFor a more basic, beginner level study of this data set check [my first House Prices kernel.](https://www.kaggle.com/dejavu23/house-prices-eda-to-ml-beginner)  \nIn this kernel, I am going to explore the following a bit more advanced approaches and techniques:  \n* EDA with **Seaborn** and interactive charts with **Plotly**  \n* possible improvements by **Feature Engineering**  \n* Preprocessing using **sklearn Pipeline**    \n* use **GridSearchCV** with Pipelines     \n* apply linear models like **Ridge, Lasso, ElasticNet**   \n* and Ensemble models like **Boosting, Stacking, Voting**    \n* compare the performance of the Regression models for validation and test data"},{"metadata":{"_uuid":"672ab97783d9acc28ca9bddbdf6b8f38394370d7"},"cell_type":"markdown","source":"### **Outline of this kernel:**\n\n[**Part 0: Imports, functions and info on data**](#Part 0: Imports, useful functions)  \n**0.1 data fields**  \n**0.2 data_description.txt**  \n[**Part 1: Exploratory Data Analysis**](#PART-1:-Exploratory-Data-Analysis)  \n**1.1 First look with Pandas**  \nshape, info, head   \ndescribe for [numerical](#describe-for-numerical-features) and [categorical](#describe-for-categorical-features) columns  \n**1.2 Handling missing values**  \n[Barchart: NaN in test and train](#Barchart:-NaN-in-test-and-train)  \n[Drop columns with lots of missing data](#Drop-columns-with-lots-of-missing-data)   \n**1.3 Visualizations for numerical features**    \n1.3.0 Distribution of the target  \n[Distplot for SalePrice and SalePrice_Log](#Distplot-for-SalePrice-and-SalePrice_Log)  \n[Skewness and Kurtosis](#Skewness-and-Kurtosis)  \n1.3.1 Correlation of numerical features to SalePrice  \n[Barchart: Correlation to SalePrice](#Barchart:-Correlation-to-SalePrice)  \n1.3.2 Area features  \n[Scatterplot: SalePrice vs GrLivArea](#Scatterplot:-SalePrice-vs-GrLivArea)   \n[Scatterplots: SalePrice vs Area features](#Scatterplots:-SalePrice-vs-Area-features)  \n[New feature: all_SF = sum of many area features](#New-feature:-Sum-of-many-area-features)  \n[Scatterplot: SalePrice vs all_SF](#Scatterplot:-SalePrice-vs-all_SF)  \n[Boxplot: SalePrice vs. OverallQual](#Boxplot:-SalePrice-vs.-OverallQual)  \n[Scatterplot categorical colors: SalePrice vs. all_SF and OverallQual](http://)  \n**1.4 Visualizations for categorical features**    \n[Boxplot: SalePrice for Neighborhood](#Boxplot:-SalePrice-for-Neighborhood)    \n[Boxplot: SalePrice for MSZoning](#Boxplot:-SalePrice-for-MSZoning)  \n\n"},{"metadata":{},"cell_type":"markdown","source":"[**PART 2: Preprocessing and Pipelines**](#PART-2:-Preprocessing-and-Pipelines)  \n**2.0 Define data for regression models**  \n**2.1 Pipeline approach**  \n**2.2 Preproccessing Pipeline**  \nfor [numerical](#for-numerical-features) and [categorical](#for-categorical-features) features   \n[ColumnTransformer](#ColumnTransformer)  \n**2.3 Append regressors to pipeline**  \n[2.3.1 Linear Models](#2.3.1-Linear-Models)  \nLinearRegression +++ Ridge +++ Lasso +++ ElaNet  \n[2.3.2 Ensemble Models](#2.3.2-Ensemble-Models)  \nGradientBoostingregressor +++ XGB +++ LGBM +++ ADABoost  \n\n[**Part 3: Crossvalidation**](#Part-3:-Crossvalidation)  \n**3.1 Linear Models**  \n[Loop over Pipelines: Linear](#Loop-over-Pipelines:-Linear)  \n**3.2 Ensemble Models**  \n[Loop over Pipelines: Ensembles](#Loop-over-Pipelines:-Ensembles)\n\n[**Part 4: GridSearchCV**](#Part-4:-GridSearchCV)  \n**4.1 Linear Models**  \nLoop over GridSearchCV Pipelines: Linear  \n**4.2 Ensemble Models**  \nLoop over GridSearchCV Pipelines: Ensembles\n\n[**Part 5: Predictions for test data**](#Part-5:-Predictions-for-test-data)  \n\n[Stacking](#5.3-Stacking)"},{"metadata":{"_uuid":"7bf9ec2cdac9a6beaef69333ea4c753c28dd8fcb","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ToDo:\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a329e03edbb491beb28cca732eef3c94109b0c94"},"cell_type":"markdown","source":"# PART 0: Imports, info"},{"metadata":{"_uuid":"764c4c2a0d9c229a9797c992c24071e51823e7ba"},"cell_type":"markdown","source":"### Imports"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', 105)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom math import sqrt\n\n# plotly\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# sklearn\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge, HuberRegressor, Lasso, ElasticNet, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\nfrom mlxtend.regressor import StackingRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n#warnings.filterwarnings(\"ignore\")\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39d3c7f1b27923542e3f0ad2a6f4c903c93e16d6"},"cell_type":"markdown","source":"### some useful functions"},{"metadata":{"_uuid":"37794c613b5c2eed69e4d3541dd161ff8487a967","trusted":true},"cell_type":"code","source":"def get_best_score(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"545d6c35149e7a33be635b672d58eaf8e7558629","trusted":true},"cell_type":"code","source":"def plotly_scatter_x_y(df_plot, val_x, val_y):\n    \n    value_x = df_plot[val_x] \n    value_y = df_plot[val_y]\n    \n    trace_1 = go.Scatter( x = value_x, y = value_y, name = val_x, \n                         mode=\"markers\", opacity=0.8 )\n\n    data = [trace_1]\n    \n    plot_title = val_y + \" vs. \" + val_x\n    \n    layout = dict(title = plot_title, \n                  xaxis=dict(title = val_x, ticklen=5, zeroline= False),\n                  yaxis=dict(title = val_y, side='left'),                                  \n                  legend=dict(orientation=\"h\", x=0.4, y=1.0),\n                  autosize=False, width=750, height=500,\n                 )\n\n    fig = dict(data = data, layout = layout)\n    iplot(fig)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28b2a78719f68a23498bd0f4ba9e283b60b13f95","trusted":true},"cell_type":"code","source":"def plotly_scatter_x_y_color(df_plot, val_x, val_y, val_z):\n    \n    value_x = df_plot[val_x] \n    value_y = df_plot[val_y]\n    value_z = df_plot[val_z]\n    \n    trace_1 = go.Scatter( \n                         x = value_x, y = value_y, name = val_x, \n                         mode=\"markers\", opacity=0.8, text=value_z,\n                         marker=dict(size=6, color = value_z, \n                                     colorscale='Jet', showscale=True),                        \n                        )\n                            \n    data = [trace_1]\n    \n    plot_title = val_y + \" vs. \" + val_x\n    \n    layout = dict(title = plot_title, \n                  xaxis=dict(title = val_x, ticklen=5, zeroline= False),\n                  yaxis=dict(title = val_y, side='left'),                                  \n                  legend=dict(orientation=\"h\", x=0.4, y=1.0),\n                  autosize=False, width=750, height=500,\n                 )\n\n    fig = dict(data = data, layout = layout)\n    iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotly_scatter_x_y_catg_color(df, val_x, val_y, val_z):\n    \n    catg_for_colors = sorted(df[val_z].unique().tolist())\n\n    fig = { 'data': [{ 'x': df[df[val_z]==catg][val_x],\n                       'y': df[df[val_z]==catg][val_y],    \n                       'name': catg, \n                       'text': df[val_z][df[val_z]==catg], \n                       'mode': 'markers',\n                       'marker': {'size': 6},\n                      \n                     } for catg in catg_for_colors       ],\n                       \n            'layout': { 'xaxis': {'title': val_x},\n                        'yaxis': {'title': val_y},                    \n                        'colorway' : ['#a9a9a9', '#e6beff', '#911eb4', '#4363d8', '#42d4f4',\n                                      '#3cb44b', '#bfef45', '#ffe119', '#f58231', '#e6194B'],\n                        'autosize' : False, \n                        'width' : 750, \n                        'height' : 600,\n                      }\n           }\n  \n    iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotly_scatter3d(data, feat1, feat2, target) :\n\n    df = data\n    x = df[feat1]\n    y = df[feat2]\n    z = df[target]\n\n    trace1 = go.Scatter3d( x = x, y = y, z = z,\n                           mode='markers',\n                           marker=dict( size=5, color=y,               \n                                        colorscale='Viridis',  \n                                        opacity=0.8 )\n                          )\n    data = [trace1]\n    camera = dict( up=dict(x=0, y=0, z=1),\n                   center=dict(x=0, y=0, z=0.0),\n                   eye=dict(x=2.5, y=0.1, z=0.8) )\n\n    layout = go.Layout( title= target + \" as function of \" +  \n                               feat1 + \" and \" + feat2 ,\n                        autosize=False, width=700, height=600,               \n                        margin=dict( l=15, r=25, b=15, t=30 ) ,\n                        scene=dict(camera=camera,\n                                   xaxis = dict(title=feat1),\n                                   yaxis = dict(title=feat2),\n                                   zaxis = dict(title=target),                                   \n                                  ),\n                       )\n\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 0.1 data fields"},{"metadata":{},"cell_type":"markdown","source":"from the Kaggle data overview,  \ngrouped by House and Land features as explored in this kernel"},{"metadata":{"_kg_hide-input":false,"_uuid":"a0e958c21ac660913c9e20d2476113a0a12d408d"},"cell_type":"markdown","source":"**SalePrice** - the property's sale price in dollars.  \nThis is the target variable that you're trying to predict.  \n\n\n**Areas**  \n1stFlrSF: First Floor square feet  \n2ndFlrSF: Second floor square feet  \nGrLivArea: Above grade (ground) living area square feet  \nTotalBsmtSF: Total square feet of basement area  \nMasVnrArea: Masonry veneer area in square feet  \nGarageArea: Size of garage in square feet  \n\nLowQualFinSF: Low quality finished square feet (all floors)  \nBsmtFinSF1: Type 1 finished square feet  \nBsmtFinSF2: Type 2 finished square feet  \nBsmtUnfSF: Unfinished square feet of basement area  \n\nWoodDeckSF: Wood deck area in square feet  \nOpenPorchSF: Open porch area in square feet  \nEnclosedPorch: Enclosed porch area in square feet  \n3SsnPorch: Three season porch area in square feet  \nScreenPorch: Screen porch area in square feet  \nPoolArea: Pool area in square feet  \n  \n  \n**Class, Condition, Quality**  \nOverallQual: Overall material and finish quality  \nOverallCond: Overall condition rating  \nMSSubClass: The building class  \nMSZoning: The general zoning classification  \nNeighborhood: Physical locations within Ames city limits  \nBldgType: Type of dwelling  \nHouseStyle: Style of dwelling   \nFoundation: Type of foundation  \nFunctional: Home functionality rating  \n\nRoofStyle: Type of roof  \nRoofMatl: Roof material  \nExterior1st: Exterior covering on house  \nExterior2nd: Exterior covering on house (if more than one material)  \nMasVnrType: Masonry veneer type  \n  \nKitchenQual: Kitchen quality  \nExterQual: Exterior material quality  \nExterCond: Present condition of the material on the exterior  \nFireplaceQu: Fireplace quality  \n  \nPoolQC: Pool quality  \nFence: Fence quality \n\nUtilities: Type of utilities available  \nHeating: Type of heating  \nHeatingQC: Heating quality and condition  \nCentralAir: Central air conditioning  \nElectrical: Electrical system  \n\n\n**Rooms, numbers**  \nFullBath: Full bathrooms above grade  \nHalfBath: Half baths above grade  \nBedroom: Number of bedrooms above basement level  \nKitchen: Number of kitchens  \nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)  \n\nFireplaces: Number of fireplaces  \n\n\n**Lot, Street, Alley**  \nLotFrontage: Linear feet of street connected to property  \nLotArea: Lot size in square feet  \nStreet: Type of road access  \nAlley: Type of alley access  \nLotShape: General shape of property  \nLandContour: Flatness of the property  \nLotConfig: Lot configuration  \nLandSlope: Slope of property  \nCondition1: Proximity to main road or railroad  \nCondition2: Proximity to main road or railroad (if a second is present)  \nPavedDrive: Paved driveway \n\n\n**BASEMENT**  \nBsmtQual: Height of the basement  \nBsmtCond: General condition of the basement  \nBsmtExposure: Walkout or garden level basement walls  \nBsmtFinType1: Quality of basement finished area  \nBsmtFullBath: Basement full bathrooms  \nBsmtHalfBath: Basement half bathrooms  \n\n\n**Garage**  \nGarageType: Garage location  \nGarageYrBlt: Year garage was built  \nGarageFinish: Interior finish of the garage  \nGarageCars: Size of garage in car capacity    \nGarageQual: Garage quality  \nGarageCond: Garage condition  \n\n**Years**  \nYearBuilt: Original construction date  \nYearRemodAdd: Remodel date  \nMoSold: Month Sold  \nYrSold: Year Sold  \n\n \nMiscFeature: Miscellaneous feature not covered in other categories  \nMiscVal: $Value of miscellaneous feature  \n\nSaleType: Type of sale  \nSaleCondition: Condition of sale "},{"metadata":{},"cell_type":"markdown","source":"### 0.2 data_description.txt"},{"metadata":{},"cell_type":"markdown","source":"**For a detailed description of the 79 features**  \n**including a list of all categorical entries,** \n**see** [this file](https://www.kaggle.com/c/5407/download/data_description.txt)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38e7d233148d4aa9dcecb7f823ede0a164e0d74c"},"cell_type":"markdown","source":"# PART 1: Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"### 1.1 First look with Pandas"},{"metadata":{"_uuid":"acac0cc9e14183dc3caef535e741730787680d8d","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e90c3d7670eeaf08a68dca90e1fb3aa217f14f5","trusted":true},"cell_type":"code","source":"print(\"df_train.shape : \" , df_train.shape)\nprint(\"*\"*50)\nprint(\"df_test.shape  : \" , df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"d2f5b30aa976c09b6c76b2133cae41a674308a95","trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc2d353cf84825256faea8e3f086c356470f3ff9","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping the column \"Id\" since it is not useful for predicting SalePrice\ndf_train.drop('Id',axis=1,inplace=True )\nid_test = df_test['Id']                      # for submissions\ndf_test.drop('Id',axis=1,inplace=True )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43b371d283fbd4fedc29c07b0c4e2ef767e4ea59"},"cell_type":"markdown","source":"#### describe for numerical features"},{"metadata":{"_uuid":"6cddc70502759109e5b45509c03f862bf6704367","trusted":true},"cell_type":"code","source":"df_train.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18b7db4ef5cc3782cd1b10d125bd163e0f342a92"},"cell_type":"markdown","source":"For the numerical columns, only three have missing values: LotFrontage, MasVnrArea and GarageYrBlt."},{"metadata":{"_kg_hide-output":true,"_uuid":"2cce3f4b1dd284c33e7d7e06be449a15b8bca56e","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ace80fdd77a8557394c30831f3d4dbf499f1e75"},"cell_type":"markdown","source":"#### describe for categorical features"},{"metadata":{"_uuid":"f96843a4fa25e28a34e183525db75b779fa78dd1","trusted":true},"cell_type":"code","source":"df_train.describe(include = ['O']).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adeef3a55146f713df87518ac41c2d706ef5aec3"},"cell_type":"markdown","source":"## 1.2 Handling missing values  "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_null = pd.DataFrame()\ndf_train_null['missing'] = df_train.isnull().sum()[df_train.isnull().sum() > 0].sort_values(ascending=False)\n\ndf_test_null = pd.DataFrame(df_test.isnull().sum(), columns = ['missing'])\ndf_test_null = df_test_null.loc[df_test_null['missing'] > 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Barchart: NaN in test and train"},{"metadata":{"_uuid":"31cf6f093a17046656f15bc96ea78b24875a1711","trusted":true},"cell_type":"code","source":"trace1 = go.Bar(x = df_train_null.index, \n                y = df_train_null['missing'],\n                name=\"df_train\", \n                text = df_train_null.index)\n\ntrace2 = go.Bar(x = df_test_null.index, \n                y = df_test_null['missing'],\n                name=\"df_test\", \n                text = df_test_null.index)\n\ndata = [trace1, trace2]\n\nlayout = dict(title = \"NaN in test and train\", \n              xaxis=dict(ticklen=10, zeroline= False),\n              yaxis=dict(title = \"number of rows\", side='left', ticklen=10,),                                  \n              legend=dict(orientation=\"v\", x=1.05, y=1.0),\n              autosize=False, width=750, height=500,\n              barmode='stack'\n              )\n\nfig = dict(data = data, layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Drop columns with lots of missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['PoolQC', 'FireplaceQu', 'Fence', \n               'Alley', 'MiscFeature'], axis=1, inplace=True)\ndf_test.drop(['PoolQC', 'FireplaceQu', 'Fence',\n               'Alley', 'MiscFeature'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note:**  \n**Dropping these features improves the performance of the linear regressors**"},{"metadata":{},"cell_type":"markdown","source":"**For the remaining missing values we use a preprocessing Pipeline**  \n**with Imputer from sklearn (see: 2.2 Preprocessing pipeline)**"},{"metadata":{"_uuid":"62bb956ad5168512c8485ef7244ca566e73abc97","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08c9cf5b345888f1617242fb84f90b4fdb54d869"},"cell_type":"markdown","source":"## 1.3 Visualizations for numerical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"numerical_columns = df_train.select_dtypes(exclude=['object']).columns.tolist()\nprint(numerical_columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3.0 Distribution of the target"},{"metadata":{},"cell_type":"markdown","source":"#### Distplot for SalePrice and SalePrice_Log"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[\"SalePrice_Log\"] = np.log1p(df_train[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = tools.make_subplots(rows=1, cols=2, print_grid=False, \n                          subplot_titles=[\"SalePrice\", \"SalePriceLog\"])\n\n\ntrace_1 = go.Histogram(x=df_train[\"SalePrice\"], name=\"SalePrice\")\ntrace_2 = go.Histogram(x=df_train[\"SalePrice_Log\"], name=\"SalePriceLog\")\n\nfig.append_trace(trace_1, 1, 1)\nfig.append_trace(trace_2, 1, 2)\n\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Skewness and Kurtosis"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import skew, kurtosis\nprint(df_train[\"SalePrice\"].skew(),\"   \", df_train[\"SalePrice\"].kurtosis())\nprint(df_train[\"SalePrice_Log\"].skew(),\"  \", df_train[\"SalePrice_Log\"].kurtosis())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3.1 Correlation of numerical features to SalePrice"},{"metadata":{},"cell_type":"markdown","source":"#### Barchart: Correlation to SalePrice"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr = df_train.corrwith(df_train['SalePrice']).abs().sort_values(ascending=False)[2:]\n\ndata = go.Bar(x=df_corr.index, \n              y=df_corr.values )\n       \nlayout = go.Layout(title = 'Correlation to Sale Price', \n                   xaxis = dict(title = ''), \n                   yaxis = dict(title = 'correlation'),\n                   autosize=False, width=750, height=500,)\n\nfig = dict(data = [data], layout = layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note on correlation of numerical features to SalePrice**  \nKeeping other parameters constant, we expect the value of a House to increase with its size and area.    \nAlso for this dataset, large correlations to SalePrice are observed for many of the Area features:  \nGrLivArea, GarageArea, TotalBsmtSF, 1stFlrSF, etc.   \nLets explore these in more detail and see how the results can be used for  \noutlier detection and feature engineering"},{"metadata":{},"cell_type":"markdown","source":"### 1.3.2 Area features"},{"metadata":{"_uuid":"476914e72b44bb96e2d5d6aca5217109d0320ba4"},"cell_type":"markdown","source":"#### Scatterplot: SalePrice vs GrLivArea"},{"metadata":{},"cell_type":"markdown","source":"Of the Area features, 'GrLivArea' has the largest correlation to SalePrice."},{"metadata":{"_uuid":"f76958265351359f2d8bac6a8488ad67dfff4f5a","trusted":true},"cell_type":"code","source":"plotly_scatter_x_y(df_train, 'GrLivArea', 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note on Outlier Detection**  \nWe store the index of the two data points to the lower right,  \nwith SalePrice < 200 k and GrLivArea > 4000"},{"metadata":{"trusted":true},"cell_type":"code","source":"# outliers GrLivArea\noutliers_GrLivArea = df_train.loc[(df_train['GrLivArea']>4000.0) & (df_train['SalePrice']<300000.0)]\noutliers_GrLivArea[['GrLivArea' , 'SalePrice']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Note on Feature Engineering**  \nIn the data fields description it says that  \nGrLivArea: Above grade (ground) living area square feet  \nWe find that for all entries in train and test data,  \nGrLivArea is equal to the sum of the 1st and 2nd Floor square feet  \ntogether with the LowQualFinSF: "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['sum_1SF_2SF_LowQualSF'] =  df_train['1stFlrSF'] + df_train['2ndFlrSF'] + df_train['LowQualFinSF']  \ndf_test['sum_1SF_2SF_LowQualSF'] =  df_test['1stFlrSF'] + df_test['2ndFlrSF'] + df_test['LowQualFinSF'] \nprint(sum(df_train['sum_1SF_2SF_LowQualSF'] != df_train['GrLivArea']))\nprint(sum(df_test['sum_1SF_2SF_LowQualSF'] != df_test['GrLivArea']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'1stFlrSF' has a correlation to SalePrice of 0.605  \n'2ndFlrSF' has a correlation to SalePrice of 0.32  \n'LowQualFinSF' has a correlation to SalePrice of 0.02  \nBy adding these three areas we get a feature that has a correlation to target of 0.709  \nIn the following we check if we can derive further useful features by adding or  \nsubtracting some of the area features."},{"metadata":{},"cell_type":"markdown","source":"**Dropping the column \"sum_1SF_2SF_LowQualSF\" again since it already exists as GrLivArea**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop('sum_1SF_2SF_LowQualSF',axis=1,inplace=True )\ndf_test.drop('sum_1SF_2SF_LowQualSF',axis=1,inplace=True )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['GrLivArea'].corr(df_train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(df_train['GrLivArea']-df_train['LowQualFinSF']).corr(df_train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'GrLivArea' minus 'LowQualFinSF' which is equal to the sum of  \n'1stFlrSF' + '2ndFlrSF' has a larger correlation to SalePrice than 'GrLivArea'"},{"metadata":{},"cell_type":"markdown","source":"Lets look at the other Area features and see if we can derive a feature  \nthat has even larger correlation to SalePrice"},{"metadata":{},"cell_type":"markdown","source":"### Scatterplots: SalePrice vs Area features"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_col_vals = 'SalePrice'\narea_features = ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF',\n                 'MasVnrArea', 'GarageArea', 'LotArea',\n                 'WoodDeckSF', 'OpenPorchSF', 'BsmtFinSF1']\n                # 'ScreenPorch'\nx_col_vals = area_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nr_rows=3\nnr_cols=3\n\nfig = tools.make_subplots(rows=nr_rows, cols=nr_cols, print_grid=False,\n                          subplot_titles=area_features )\n                                                                \nfor row in range(1,nr_rows+1):\n    for col in range(1,nr_cols+1): \n        \n        i = (row-1) * nr_cols + col-1\n                   \n        trace = go.Scatter(x = df_train[x_col_vals[i]], \n                           y = df_train[y_col_vals], \n                           name=x_col_vals[i], \n                           mode=\"markers\", \n                           opacity=0.8)\n\n        fig.append_trace(trace, row, col,)\n \n                                                                                                  \nfig['layout'].update(height=700, width=900, showlegend=False,\n                     title='SalePrice' + ' vs. Area features')\niplot(fig)                                                ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05daa679f1ed84ab8690e69bbee7ee0fab451e29","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c0800346aa8fa933b9334a839eb82f9141de851"},"cell_type":"markdown","source":"**new feature : sum of all Living SF areas**  \n**all_Liv_SF = 'TotalBsmtSF' + '1stFlrSF' + '2ndFlrSF'**"},{"metadata":{"_uuid":"c33bea67f2c711bb28a3439258e545a4996d7cd3","trusted":true},"cell_type":"code","source":"df_train['all_Liv_SF'] = df_train['TotalBsmtSF'] + df_train['1stFlrSF'] + df_train['2ndFlrSF'] \ndf_test['all_Liv_SF'] = df_test['TotalBsmtSF'] + df_test['1stFlrSF'] + df_test['2ndFlrSF'] \n\nprint(df_train['all_Liv_SF'].corr(df_train['SalePrice']))\nprint(df_train['all_Liv_SF'].corr(df_train['SalePrice_Log']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By summming up square feet for Basement, 1st and 2nd floor. we derive  \na feature 'all_Liv_SF' that has a correlation to SalePrice of 0.78"},{"metadata":{},"cell_type":"markdown","source":"### New feature: Sum of many area features"},{"metadata":{},"cell_type":"markdown","source":"  \n**all_SF = 'all_Liv_SF' + 'GarageArea' + 'MasVnrArea' + 'WoodDeckSF' + 'OpenPorchSF' + 'ScreenPorch'**"},{"metadata":{},"cell_type":"markdown","source":"For 'all_SF' we further add some of the outside area values.    \nThis results in a correlation to SalePrice and also SalePriceLog of around 0.82"},{"metadata":{"_uuid":"7013a2cb4e4db408cc55e916f7f8d374ab4b3e42","trusted":true},"cell_type":"code","source":"df_train['all_SF'] = ( df_train['all_Liv_SF'] + df_train['GarageArea'] + df_train['MasVnrArea'] \n                       + df_train['WoodDeckSF'] + df_train['OpenPorchSF'] + df_train['ScreenPorch'] )\ndf_test['all_SF'] = ( df_test['all_Liv_SF'] + df_test['GarageArea'] + df_test['MasVnrArea']\n                      + df_test['WoodDeckSF'] + df_test['OpenPorchSF'] + df_train['ScreenPorch'] )\n\nprint(df_train['all_SF'].corr(df_train['SalePrice']))\nprint(df_train['all_SF'].corr(df_train['SalePrice_Log']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two new features are highly correlated.  \nThis multicorrelation may be a problem for some linear models.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['all_SF'].corr(df_train['all_Liv_SF'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65744973043d72fc97e2254448f44cd26e1f3621"},"cell_type":"markdown","source":"#### Scatterplot: SalePrice vs all_SF"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotly_scatter_x_y(df_train, 'all_SF', 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Like for GrlivArea, there are two outliers at the lower right also for all_SF**  \n**We are going to drop these now.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers_allSF = df_train.loc[(df_train['all_SF']>8000.0) & (df_train['SalePrice']<200000.0)]\noutliers_allSF[['all_SF' , 'SalePrice']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Indexes for the outliers are the same like for GrLivArea**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(outliers_allSF.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.corr().abs()[['SalePrice','SalePrice_Log']].sort_values(by='SalePrice', ascending=False)[2:16]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**After dropping these two outliers all_SF has a correlation**  \n**to SalePrice (and also SalePriceLog) of 0.86**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f70b86840955544eda4a55ac0b0f65fe477ae26c","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53ba0ff12f0dee3951163284554df4e0561fef91"},"cell_type":"markdown","source":"### Boxplot: SalePrice vs. OverallQual"},{"metadata":{"_uuid":"7d9d6e8824a0c2c63e2c5814956ec92e28df77a9","trusted":true},"cell_type":"code","source":"trace = []\nfor name, group in df_train[[\"SalePrice\", \"OverallQual\"]].groupby(\"OverallQual\"):\n    trace.append( go.Box( y=group[\"SalePrice\"].values, name=name ) )\n    \nlayout = go.Layout(title=\"OverallQual\", \n                   xaxis=dict(title='OverallQual',ticklen=5, zeroline= False),\n                   yaxis=dict(title='SalePrice', side='left'),\n                   autosize=False, width=750, height=500)\n\nfig = go.Figure(data=trace, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As can be expected from the large correlation coefficient of 0.796 ,  \nthere is an almost perfect linear increase of SalePrice with the OverallQual.  \nWe notice that this feature is in fact categorical (ordinal),  \nonly the discrete values 1,2..10 occur.  \nAlso there are a few outliers for some of the OverallQual values.  \nWe are dropping those that are very far from the upper fences:"},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers_OverallQual_4 = df_train.loc[(df_train['OverallQual']==4) & (df_train['SalePrice']>200000.0)]\noutliers_OverallQual_8 = df_train.loc[(df_train['OverallQual']==8) & (df_train['SalePrice']>500000.0)]\noutliers_OverallQual_9 = df_train.loc[(df_train['OverallQual']==9) & (df_train['SalePrice']>500000.0)]\noutliers_OverallQual_10 = df_train.loc[(df_train['OverallQual']==10) & (df_train['SalePrice']>700000.0)]\n\noutliers_OverallQual = pd.concat([outliers_OverallQual_4, outliers_OverallQual_8, \n                                  outliers_OverallQual_9, outliers_OverallQual_10])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers_OverallQual[['OverallQual' , 'SalePrice']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(outliers_OverallQual.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.corr().abs()[['SalePrice','SalePrice_Log']].sort_values(by='SalePrice', ascending=False)[2:16]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scatterplot colors: SalePrice vs. all_SF and OverallQual"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotly_scatter_x_y_catg_color(df_train, 'all_SF', 'SalePrice', 'OverallQual')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As seen before in the simple xatter plot, there is a strong tendency for \nincreasing SalePrice with a higher value for OverallQual.\nBut this color plot also shows a correlation of all_SF and OverallQual.  \nSo, the probability that a house has a large area increases with its Overall Quality.  \nAnd vice versa: Quality increases with House size.  \nThis corrrelation is not necessary, one would expect that there are also small houses with high  \nquality and big houses with low quality.  \nIt would be nice to know how the rating for OverallQual is calculated or estimated, \nbut that info is not included in the data description."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another option to highlight the correlation of SalePrice to all_SF and  \nOverallQual as well as the correlation between all_SF and OverallQual is  \na 3d scatter plot:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plotly_scatter3d(df_train, 'all_SF', 'OverallQual', 'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Rotating the 3d view reveals that:\n\n* SalePrice increases almost linearly with all_SF and OverallQual  \n* all_SF increases almost linearly with OverallQual and vice versa  \n\nIn fact, the bulk of the data follows the 45 degree line in 3 dim space.  \nThis also results in the high correlation coefficient for OverallQual and all_SF:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train['OverallQual'].corr(df_train['all_SF']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**other numerical features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train['OverallCond'].corr(df_train['SalePrice']))\nprint(df_train['OverallCond'].corr(df_train['SalePrice_Log']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train['MSSubClass'].corr(df_train['SalePrice']))\nprint(df_train['MSSubClass'].corr(df_train['SalePrice_Log']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.4 Visualizations for Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = df_train.select_dtypes(include=['object']).columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc3c7fd60c0acd75425a925e707665b4ce65eb0e"},"cell_type":"markdown","source":"### Boxplots for categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotly_boxplots_sorted_by_yvals(df, catg_feature, sort_by_target):\n    \n    df_by_catg   = df.groupby([catg_feature])\n    sortedlist_catg_str = df_by_catg[sort_by_target].median().sort_values().keys().tolist()\n    \n    \n    data = []\n    for i in sortedlist_catg_str :\n        data.append(go.Box(y = df[df[catg_feature]==i][sort_by_target], name = i))\n\n    layout = go.Layout(title = sort_by_target + \" vs \" + catg_feature, \n                       xaxis = dict(title = catg_feature), \n                       yaxis = dict(title = sort_by_target))\n\n    fig = dict(data = data, layout = layout)\n    return fig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boxplot: SalePrice for Neighborhood"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plotly_boxplots_sorted_by_yvals(df_train, 'Neighborhood', 'SalePrice')\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Boxplot: SalePrice for MSZoning"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plotly_boxplots_sorted_by_yvals(df_train, 'MSZoning', 'SalePrice')\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"325f2f96f0b0f34ddbd8d15a001a4be08daa339b"},"cell_type":"markdown","source":"# PART 2: Preprocessing and Pipelines"},{"metadata":{},"cell_type":"markdown","source":"## 2.0 define data for regression models"},{"metadata":{"trusted":true},"cell_type":"code","source":"outliers_all = []\ndf_train = df_train.drop(outliers_all)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e401a2c0a567df1cf18cc49663b229e6f225881c","trusted":true},"cell_type":"code","source":"# store target as y and y_log:\ny , y_log = df_train[\"SalePrice\"] , df_train[\"SalePrice_Log\"]\n# drop target from df_train:\ndf_train.drop([\"SalePrice\", \"SalePrice_Log\"] , axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**1: SalePriceLog as target**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_1 = df_train\ny_1 = y_log","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2: SalePrice as target**"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_2 = df_train\ny_2 = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd869c0d7b0487525d936a78d55e9f0240ca54de"},"cell_type":"markdown","source":"## 2.1 Pipeline approach"},{"metadata":{},"cell_type":"markdown","source":"**sklearn.pipeline**  \nThe sklearn.pipeline module implements utilities to build a composite estimator, as a chain of transforms and estimators."},{"metadata":{},"cell_type":"markdown","source":"https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline"},{"metadata":{},"cell_type":"markdown","source":"**sklearn.pipeline.Pipeline**  \nPipeline of transforms with a final estimator.  \nSequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.  \nThe purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a ‘__’, as in the example below. A step’s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting to None."},{"metadata":{"_uuid":"14918965246ba43a2484e2a171c8bc2194e9fb84","trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df9929876c17d4d2fe10a3a1fa37d928bd8a3275"},"cell_type":"markdown","source":"## 2.2 Preprocessing pipeline"},{"metadata":{"_uuid":"cbce0bf15b79185360f9d2e16c48f479bd35b0e1","trusted":true},"cell_type":"code","source":"numerical_features   = df_train.select_dtypes(exclude=['object']).columns.tolist()\ncategorical_features = df_train.select_dtypes(include=['object']).columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### for numerical features"},{"metadata":{"_uuid":"0027e1c088d52789b31bd21d21b0bf5d09b70074","trusted":true},"cell_type":"code","source":"numerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### for categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ColumnTransformer"},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer,   numerical_features),\n        ('cat', categorical_transformer, categorical_features)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08bd81345be465260598841f16be24f54f4e8515"},"cell_type":"markdown","source":"## 2.3 Append regressors to pipeline"},{"metadata":{},"cell_type":"markdown","source":"### 2.3.1 Linear Models"},{"metadata":{},"cell_type":"markdown","source":"Pipelines with default model parameters"},{"metadata":{},"cell_type":"markdown","source":"**LinearRegression** minimizes the residual sum of squares between the observed targets and the targets predicted by the linear approximation = Ordinary Least Squares fit  \n**Ridge** regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares.  \n**HuberRegressor** is different to Ridge because it applies a linear loss to samples that are classified as outliers.\n"},{"metadata":{"_uuid":"7f9ce258ff557f66bc23c6a994247215d0ab0a23","trusted":true},"cell_type":"code","source":"# LinearRegression\npipe_Linear = Pipeline(\n    steps   = [('preprocessor', preprocessor),\n               ('Linear', LinearRegression()) ])    \n# Ridge\npipe_Ridge = Pipeline(\n    steps  = [('preprocessor', preprocessor),\n              ('Ridge', Ridge(random_state=5)) ])  \n# Huber\npipe_Huber = Pipeline(\n    steps  = [('preprocessor', preprocessor),\n              ('Huber', HuberRegressor()) ])  \n# Lasso\npipe_Lasso = Pipeline(\n    steps  = [ ('preprocessor', preprocessor),\n               ('Lasso', Lasso(random_state=5)) ])\n# ElasticNet\npipe_ElaNet = Pipeline(\n    steps   = [ ('preprocessor', preprocessor),\n                ('ElaNet', ElasticNet(random_state=5)) ])\n\n# BayesianRidge\npipe_BayesRidge = Pipeline(\n    steps   = [ ('preprocessor', preprocessor),\n                ('BayesRidge', BayesianRidge(n_iter=500, compute_score=True)) ])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3.2 Ensemble Models"},{"metadata":{},"cell_type":"markdown","source":"Pipelines with default model parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# GradientBoostingRegressor\npipe_GBR  = Pipeline(\n    steps = [ ('preprocessor', preprocessor),\n              ('GBR', GradientBoostingRegressor(random_state=5 )) ])\n\n# XGBRegressor\npipe_XGB  = Pipeline(\n    steps = [ ('preprocessor', preprocessor),\n              ('XGB', XGBRegressor(objective='reg:squarederror', metric='rmse', \n                      random_state=5, nthread = -1)) ])\n# LGBM\npipe_LGBM = Pipeline(\n    steps= [('preprocessor', preprocessor),\n            ('LGBM', LGBMRegressor(objective='regression', metric='rmse',\n                                  random_state=5)) ])\n# AdaBoostRegressor\npipe_ADA = Pipeline(\n    steps= [('preprocessor', preprocessor),\n            ('ADA', AdaBoostRegressor(DecisionTreeRegressor(), \n                random_state=5, loss='exponential')) ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 3: Crossvalidation"},{"metadata":{},"cell_type":"markdown","source":"We now run a 5 fold cross validation for each pipeline/model:  \nLinear Models: Linear Regression, Ridge, Lasso, Elastic Net  \nEnsembles: GBR, XGB, LGBM, ADA  \nFor this we create loops over two list of pipelines (Linear models and Ensembles) and calculate  \nthe mean, std and min score (=error) for every model.  \nBy this we get a first estimate for the different regression pipelines (Linear models and Ensembles):   \nWe fit the the data (features X and target y) using the default model parameters."},{"metadata":{},"cell_type":"markdown","source":"## 3.1 Linear Models"},{"metadata":{},"cell_type":"markdown","source":"### Loop over Pipelines: Linear"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_pipelines = [pipe_Linear, pipe_Ridge, pipe_Huber, pipe_Lasso, pipe_ElaNet]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\nfor pipe in list_pipelines :\n    \n    scores = cross_val_score(pipe, X_1, y_1, scoring='neg_mean_squared_error', cv=5)\n    scores = np.sqrt(-scores)\n    print(pipe.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Linear Regression and especially Ridge model give quite good results already with default parameters.  \nFor Huber, Lasso and Elastic Net we need to tune hyperparameters (see Part 4: GridSearchCV)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f2f9fa1c8eed3454e0020ef26d4eaec54a6d55d"},"cell_type":"markdown","source":"## 3.2 Ensemble Models   "},{"metadata":{},"cell_type":"markdown","source":"### Loop over Pipelines: Ensembles"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_pipelines = [pipe_GBR, pipe_XGB, pipe_LGBM, pipe_ADA]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\n\nfor pipe in list_pipelines :\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\",category=FutureWarning)\n        scores = cross_val_score(pipe, X_1, y_1, scoring='neg_mean_squared_error', cv=5)\n        scores = np.sqrt(-scores)\n        print(pipe.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Except for ADA Boost, the ensemble models with default parameters give  \nalready good results for this task."},{"metadata":{},"cell_type":"markdown","source":"In the following we check if we can improve these scores when we  \noptimize the model hyperparameters with GridSearchCV."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 4: GridSearchCV"},{"metadata":{},"cell_type":"markdown","source":"Preprocessing: Scalers"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_scalers = [StandardScaler(), \n                RobustScaler(), \n                QuantileTransformer(output_distribution='normal')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For some linear models, QuantileTransformer gives best score for CV.  \nBut for test score, performance is best with StandardScaler for all models.  \nTherefore:"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_scalers = [StandardScaler()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.1 Linear Models"},{"metadata":{},"cell_type":"markdown","source":"#### Linear Regression"},{"metadata":{},"cell_type":"markdown","source":"**fit_intercept** : boolean, optional, default True  \n**normalize** : boolean, optional, default False  \n**copy_X** : boolean, optional, default True  \n**n_jobs** : int or None, optional (default=None)  \nThe number of jobs to use for the computation. This will only provide speedup for n_targets > 1 and sufficient large problems. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors."},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"parameters_Linear = { 'preprocessor__num__scaler': list_scalers,\n                     'Linear__fit_intercept':  [True,False],\n                     'Linear__normalize':  [True,False] }\n\ngscv_Linear = GridSearchCV(pipe_Linear, parameters_Linear, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=0, cv=5)\ngscv_Linear.fit(X_1, y_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.sqrt(-gscv_Linear.best_score_))  \ngscv_Linear.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Ridge"},{"metadata":{},"cell_type":"markdown","source":"**alpha** :\nRegularization strength, must be a positive float  \n**fit_intercept** : bool, default True  \n**normalize** : boolean, optional, default False  \n**copy_X** : boolean, optional, default True  \n**max_iter** : int  \n**tol** : float  \nPrecision of the solution  \n**solver** : {‘auto’, ‘svd’, ‘cholesky’, ‘lsqr’, ‘sparse_cg’, ‘sag’, ‘saga’}  \nSolver to use in the computational routines"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"parameters_Ridge = { 'preprocessor__num__scaler': list_scalers,\n                     'Ridge__alpha': [7,8,9],\n                     'Ridge__fit_intercept':  [True,False],\n                     'Ridge__normalize':  [True,False] }\n\ngscv_Ridge = GridSearchCV(pipe_Ridge, parameters_Ridge, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=0, cv=5)\ngscv_Ridge.fit(X_1, y_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.sqrt(-gscv_Ridge.best_score_))  \ngscv_Ridge.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Huber Regressor**"},{"metadata":{},"cell_type":"markdown","source":"**epsilon** : > 1.0, default 1.35  \ncontrols the number of samples that should be classified as outliers.  \nThe smaller the epsilon, the more robust it is to outliers.  \n**max_iter** : int, default 100  \nMaximum number of iterations that scipy.optimize.fmin_l_bfgs_b should run for.  \n**alpha** : float, default 0.0001  \nRegularization parameter.  \n**fit_intercept** : bool, default True"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"parameters_Huber = { 'preprocessor__num__scaler': list_scalers,                   \n                     'Huber__epsilon': [1.3, 1.35, 1.4],    \n                     'Huber__max_iter': [150, 200, 250],                    \n                     'Huber__alpha': [0.0005, 0.001, 0.002],\n                     'Huber__fit_intercept':  [True], }\n\ngscv_Huber = GridSearchCV(pipe_Huber, parameters_Huber, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_Huber.fit(X_1, y_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.sqrt(-gscv_Huber.best_score_))  \ngscv_Huber.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Lasso"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"parameters_Lasso = { 'preprocessor__num__scaler': list_scalers,\n                     'Lasso__alpha': [0.0005, 0.001],\n                     'Lasso__fit_intercept':  [True],\n                     'Lasso__normalize':  [True,False] }\n\ngscv_Lasso = GridSearchCV(pipe_Lasso, parameters_Lasso, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_Lasso.fit(X_1, y_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.sqrt(-gscv_Lasso.best_score_))  \ngscv_Lasso.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ElasticNet**"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"parameters_ElaNet = { 'ElaNet__alpha': [0.0005, 0.001],\n                      'ElaNet__l1_ratio':  [0.85, 0.9],\n                      'ElaNet__normalize':  [True,False] }\n\ngscv_ElaNet = GridSearchCV(pipe_ElaNet, parameters_ElaNet, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_ElaNet.fit(X_1, y_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.sqrt(-gscv_ElaNet.best_score_))  \ngscv_ElaNet.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loop over GridSearchCV Pipelines: Linear"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_pipelines_gscv = [gscv_Linear,gscv_Ridge,gscv_Huber,gscv_Lasso,gscv_ElaNet]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\nfor gscv in list_pipelines_gscv :\n    \n    scores = cross_val_score(gscv.best_estimator_, X_1, y_1, \n                             scoring='neg_mean_squared_error', cv=5)\n    scores = np.sqrt(-scores)\n    print(gscv.estimator.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After GridSearchCV, results for Lasso and Elastic Net are much better compared to using the default parameters.  \nThe Ridge model also improves a bit, score for Linear Regression is the same as with default parameters.  \nHuber regression is not better than Ordinary least Squares for this task."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.2 Ensemble Models"},{"metadata":{},"cell_type":"markdown","source":"**GradientBoostingRegressor**"},{"metadata":{},"cell_type":"markdown","source":"**loss** : {‘ls’, ‘lad’, ‘huber’, ‘quantile’}, optional (default=’ls’)  \n**learning_rate** : float, optional (default=0.1)  \n**n_estimators** : int (default=100)  \n**subsample** : float, optional (default=1.0)  \n**criterion** : string, optional (default=”friedman_mse”)  \n**min_samples_split** : int, float, optional (default=2)  \nIf int: minimum number. If float: fraction  \n**min_samples_leaf** : int, float, optional (default=1)   \nIf int minimum number. If float fraction   \n**min_weight_fraction_leaf** : float, optional (default=0.)  \n**max_depth** : integer, optional (default=3)\nmaximum depth of the individual regression estimators.  \n**min_impurity_decrease** : float, optional (default=0.)\n\n**max_features** : int, float, string or None, optional (default=None)  \nThe number of features to consider when looking for the best split:  \nIf float: fraction  \nIf “auto”, then max_features=n_features.  \nIf “sqrt”, then max_features=sqrt(n_features).  \nIf “log2”, then max_features=log2(n_features).  \nIf None, then max_features=n_features.  \nChoosing max_features < n_features leads to a reduction of variance and an increase in bias.  \n"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"parameters_GBR = { 'GBR__n_estimators':  [400], \n                   'GBR__max_depth':  [3,4],\n                   'GBR__min_samples_leaf':  [5,6],                 \n                   'GBR__max_features':  [\"auto\",0.5,0.7],                  \n                 }\n                   \ngscv_GBR = GridSearchCV(pipe_GBR, parameters_GBR, n_jobs=-1, \n                        scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_GBR.fit(X_1, y_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.sqrt(-gscv_GBR.best_score_))  \ngscv_GBR.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**XGB**"},{"metadata":{},"cell_type":"markdown","source":"https://xgboost.readthedocs.io/en/latest/parameter.html  \nhttps://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html  \nhttps://xgboost.readthedocs.io/en/latest/python/python_api.html"},{"metadata":{},"cell_type":"markdown","source":"General Parameters  \n**booster**: gbtree, gblinear or dart, default= gbtree   \n\nParameters for Tree Booster  \n**eta**, alias: learning_rate, 0<eta<1 , default=0.3  \n**gamma**, alias: min_split_loss,  default=0,  \nMinimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.  \n**max_depth**, default=6  \n**min_child_weight**, default=1\nThe larger min_child_weight is, the more conservative the algorithm will be.  \n**max_delta_step** [default=0]  \n**subsample** [default=1],  range: (0,1]  \nSetting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. Subsampling will occur once in every boosting iteration.  \n**colsample_bytree, colsample_bylevel, colsample_bynode** [default=1]  \nThis is a family of parameters for subsampling of columns.  \nAll colsample_by* parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.  \ncolsample_by* parameters work cumulatively. For instance, the combination {'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5} with 64 features will leave 8 features to choose from at each split.  \n**lambda** [default=1, alias: reg_lambda]  \nL2 regularization term on weights. Increasing this value will make model more conservative.  \n**alpha** [default=0, alias: reg_alpha]  \nL1 regularization term on weights. Increasing this value will make model more conservative.  \n**tree_method** string [default= auto]  \nChoices: auto, exact, approx, hist, gpu_hist  "},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"parameters_XGB = { 'XGB__learning_rate': [0.021,0.022],\n                   'XGB__max_depth':  [2,3],\n                   'XGB__n_estimators':  [2000], \n                   'XGB__reg_lambda':  [1.5, 1.6], \n                   'XGB__reg_alpha':  [1,1.5],                   \n# colsample_bytree , subsample               \n                  }\n                   \ngscv_XGB = GridSearchCV(pipe_XGB, parameters_XGB, n_jobs=-1, \n                        scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_XGB.fit(X_1, y_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.sqrt(-gscv_XGB.best_score_))  \ngscv_XGB.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### LGBM"},{"metadata":{},"cell_type":"markdown","source":"https://testlightgbm.readthedocs.io/en/latest/Parameters.html  \nhttps://testlightgbm.readthedocs.io/en/latest/Parameters-tuning.html  "},{"metadata":{},"cell_type":"markdown","source":"**num_iterations**, default=100, alias=num_iteration,num_tree,num_trees,num_round,num_rounds  \n**learning_rate**, default=0.1, alias=shrinkage_rate  \n**num_leaves**, default=31\n\n\n**max_depth**, default=-1, < 0 means no limit  \n**min_data_in_leaf**, default=20, type=int, alias=min_data_per_leaf , min_data  \n**min_sum_hessian_in_leaf**, default=1e-3, alias=min_sum_hessian_per_leaf, min_sum_hessian, min_hessian  \n**feature_fraction**, default=1.0, 0.0 < feature_fraction < 1.0, alias=sub_feature  \n**bagging_fraction**, default=1.0, 0.0 < bagging_fraction < 1.0, alias=sub_row  \n**bagging_freq**, default=0,   \nFrequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration   \n**early_stopping_round** , default=0, type=int, alias=early_stopping_rounds,early_stopping  \nWill stop training if one metric of one validation data doesn’t improve in last early_stopping_round rounds  \n**lambda_l1** , default=0  \n**lambda_l2** , default=0"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"parameters_LGBM = { 'LGBM__learning_rate': [0.01,0.02],\n                    'LGBM__n_estimators':  [1000], \n                    'LGBM__num_leaves':  [8,10],\n                    'LGBM__bagging_fraction':  [0.7,0.8],\n                    'LGBM__bagging_freq':  [1,2],                  \n                   }\n\ngscv_LGBM = GridSearchCV(pipe_LGBM, parameters_LGBM, n_jobs=-1, \n                       scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_LGBM.fit(X_1, y_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.sqrt(-gscv_LGBM.best_score_))  \ngscv_LGBM.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### AdaBoostRegressor"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"parameters_ADA = { 'ADA__learning_rate': [3.5],\n                   'ADA__n_estimators':  [500], \n                   'ADA__base_estimator__max_depth':  [8,9,10],                  \n                 }\n\npipe_ADA = Pipeline(\n    steps= [('preprocessor', preprocessor),\n            ('ADA', AdaBoostRegressor(\n                DecisionTreeRegressor(min_samples_leaf=5,\n                                      min_samples_split=5), \n                random_state=5,loss='exponential')) ])\n\ngscv_ADA = GridSearchCV(pipe_ADA, parameters_ADA, n_jobs=-1, \n                       scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_ADA.fit(X_1, y_1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(np.sqrt(-gscv_ADA.best_score_))  \ngscv_ADA.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loop over GridSearchCV Pipelines: Ensembles"},{"metadata":{"trusted":true},"cell_type":"code","source":"list_pipelines_gscv = [gscv_GBR, gscv_XGB,gscv_LGBM,gscv_ADA]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\nfor gscv in list_pipelines_gscv :\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\",category=FutureWarning)    \n        scores = cross_val_score(gscv.best_estimator_, X_1, y_1, \n                             scoring='neg_mean_squared_error', cv=5)\n        scores = np.sqrt(-scores)\n        print(gscv.estimator.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Part 5: Predictions for test data"},{"metadata":{},"cell_type":"markdown","source":"list of models"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_models = [gscv_Linear,gscv_Ridge,gscv_Huber,gscv_Lasso,gscv_ElaNet]\nboost_models  = [gscv_GBR, gscv_XGB,gscv_LGBM,gscv_ADA]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear Models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_Linear = gscv_Linear.predict(df_test)\npred_Ridge  = gscv_Ridge.predict(df_test)\npred_Huber  = gscv_Huber.predict(df_test)\npred_Lasso  = gscv_Lasso.predict(df_test)\npred_ElaNet = gscv_ElaNet.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_linear = {'Linear': pred_Linear, 'Ridge': pred_Ridge, 'Huber': pred_Huber,\n                      'Lasso':  pred_Lasso, 'ElaNet': pred_ElaNet }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model,values in predictions_linear.items():\n    str_filename = model + \".csv\"\n    print(\"witing submission to : \", str_filename)\n    subm = pd.DataFrame()\n    subm['Id'] = id_test\n    subm['SalePrice'] = np.expm1(values)\n    subm.to_csv(str_filename, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**blend_1: gscv_Ridge and gscv_Lasso**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_Blend_1 = (pred_Lasso + pred_Ridge) / 2\nsub_Blend_1 = pd.DataFrame()\nsub_Blend_1['Id'] = id_test\nsub_Blend_1['SalePrice'] = np.expm1(pred_Blend_1)\nsub_Blend_1.to_csv('Blend_Ridge_Lasso.csv',index=False)\nsub_Blend_1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**blend_2: gscv_Lasso and gscv_ElaNet**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_Blend_2 = (pred_Lasso + pred_ElaNet) / 2\nsub_Blend_2 = pd.DataFrame()\nsub_Blend_2['Id'] = id_test\nsub_Blend_2['SalePrice'] = np.expm1(pred_Blend_2)\nsub_Blend_2.to_csv('Blend_2.csv',index=False)\nsub_Blend_2.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**blend_3: gscv_Ridge, gscv_Lasso and gscv_ElaNet**"},{"metadata":{"_uuid":"105762beb291438bd0dd3796169aeb3c733070fc","trusted":true},"cell_type":"code","source":"pred_Blend_3 = (pred_Ridge + pred_Lasso + pred_ElaNet) / 3\nsub_Blend_3 = pd.DataFrame()\nsub_Blend_3['Id'] = id_test\nsub_Blend_3['SalePrice'] = np.expm1(pred_Blend_3)\nsub_Blend_3.to_csv('Blend_3.csv',index=False)\nsub_Blend_3.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Boost Models**"},{"metadata":{"trusted":true},"cell_type":"code","source":"boost_models  = [gscv_GBR, gscv_XGB,gscv_LGBM,gscv_ADA]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_GBR  = gscv_GBR.predict(df_test)\npred_XGB  = gscv_XGB.predict(df_test)\npred_LGBM = gscv_LGBM.predict(df_test)\npred_ADA  = gscv_ADA.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_boost = {'GBR': pred_GBR, 'XGB': pred_XGB, 'LGBM': pred_LGBM,\n                     'ADA': pred_ADA }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model,values in predictions_boost.items():\n    str_filename = model + \".csv\"\n    print(\"witing submission to : \", str_filename)\n    subm = pd.DataFrame()\n    subm['Id'] = id_test\n    subm['SalePrice'] = np.expm1(values)\n    subm.to_csv(str_filename, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.1 Correlation of predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = {'Ridge': pred_Ridge, 'Lasso': pred_Lasso, 'ElaNet': pred_ElaNet, \n               'GBR': pred_GBR, 'XGB': pred_XGB, 'LGBM': pred_LGBM, 'ADA': pred_ADA}\ndf_predictions = pd.DataFrame(data=predictions) \ndf_predictions.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**to be continued**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Blend: Ridge + XGB"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_Blend_10 = (pred_Ridge + pred_XGB) / 2\nsub_Blend_10 = pd.DataFrame()\nsub_Blend_10['Id'] = id_test\nsub_Blend_10['SalePrice'] = np.expm1(pred_Blend_10)\nsub_Blend_10.to_csv('Blend_Ridge_XGB.csv',index=False)\nsub_Blend_10.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5.3 Stacking"},{"metadata":{"_uuid":"fafb82bdd53c234f67c7c240decb5931b3700184","trusted":true},"cell_type":"code","source":"lnr = LinearRegression(n_jobs = -1)\n\nrdg = Ridge(alpha=3.0, copy_X=True, fit_intercept=True, random_state=1)\n\nrft = RandomForestRegressor(n_estimators = 12, max_depth = 3, n_jobs = -1, random_state=1)\n\ngbr = GradientBoostingRegressor(n_estimators = 40, max_depth = 2, random_state=1)\n\nmlp = MLPRegressor(hidden_layer_sizes = (90, 90), alpha = 2.75, random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"stack1"},{"metadata":{"trusted":true},"cell_type":"code","source":"stack1 = StackingRegressor(regressors = [rdg, rft, gbr], \n                           meta_regressor = lnr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_STACK_1 = Pipeline(steps=[ ('preprocessor', preprocessor),\n                                ('stack1', stack1) ])\n\npipe_STACK_1.fit(X_1, y_1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6803a56a136d30b64f876d73f51f1f3a1a6d8da1","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://stackoverflow.com/questions/50722270/convergence-warningstochastic-optimizer-maximum-iterations-200-reached-and-t?rq=1"},{"metadata":{"_uuid":"cb45c3f064c0b7e61f5bb287e6ff66afce69cda2","trusted":true},"cell_type":"code","source":"pred_stack1 = pipe_STACK_1.predict(df_test)\nsub_stack1 = pd.DataFrame()\nsub_stack1['Id'] = id_test\nsub_stack1['SalePrice'] = np.expm1(pred_stack1)\nsub_stack1.to_csv('pipe_stack1.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1c0396c5a019878098cd7df3cbeb59d7c3b56f6","trusted":true},"cell_type":"code","source":"sub_stack1.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}