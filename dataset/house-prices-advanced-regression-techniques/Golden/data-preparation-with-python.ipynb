{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Data Preparation with Python\nOn this notebook I intend to explore the topic data preparation with python."},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\nfrom scipy.stats import zscore\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom yellowbrick.features.pca import PCADecomposition\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression, RFE\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1- Knowing the dataset"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 - pandas_profiling\npandas_profiling produces reports from pandas dataframe providing a great exploratory analysis.  \nhttps://pandas-profiling.github.io/pandas-profiling/docs/"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile_report = ProfileReport(df, title='Profile Report', html={'style':{'full_width':True}})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"profile_report.to_notebook_iframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.2 - Knowing more about the dataset subsetting and groupping\n\nLet's know more about the basements. I'm still on Parasite movie mood ðŸ‘€  \n\nGoals:  \n* Check basement related features\n* Check bigger basements\n* Exclude the bigger basement and select some features\n* Check about Basement Condition"},{"metadata":{"trusted":true},"cell_type":"code","source":"# basement features\ndf.filter(regex='\\Bsmt').columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5 higher square feet\ndfbs=df.nlargest(5, 'TotalBsmtSF')\ndfbs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# excluding the bigger basement and selecting some features\ndfbs2=dfbs.loc[[332, 496, 523,440], [\"LotArea\", \"YearBuilt\", \"TotalBsmtSF\", \"BsmtCond\",\"SalePrice\" ]]\ndfbs2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# other options for selecting/slicing\nprint(dfbs2.equals(dfbs.loc[332:440, [\"LotArea\", \"YearBuilt\", \"TotalBsmtSF\", \"BsmtCond\",\"SalePrice\" ]]))\nprint(dfbs2.equals(dfbs.iloc[[1,2,3,4],[4, 19, 38, 31, 80]]))\nprint(dfbs2.equals(dfbs.iloc[1:5,[4, 19, 38, 31, 80]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking about Basement Condition\nprint(df[\"BsmtCond\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking about Basement Condition\ndf.groupby(by=\"BsmtCond\")['SalePrice'].mean().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2- Cleaning\nHere, I am going to build functions to check, more directly, some possible problems: duplicate rows, inconsistent variable types, missing values and outliers."},{"metadata":{},"cell_type":"markdown","source":"### 2.1 - Any duplicated rows?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The dataset has {} rows and {} columns. {} duplicated rows\".format(df.shape[0], df.shape[1],df.duplicated().sum()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2 - Inconsistent types?"},{"metadata":{"trusted":true},"cell_type":"code","source":"s_types= df.dtypes\ns_head= df.apply(lambda x: x[0:3].tolist())\n\nexplo1 = pd.DataFrame({'Types': s_types,\n                      'Head': s_head}).sort_values(by=['Types'],ascending=False)\nexplo1.transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While checking \"Types\" and \"Head\" I didn't find inconsistent types"},{"metadata":{},"cell_type":"markdown","source":"### 2.3 - Missing values?"},{"metadata":{"trusted":true},"cell_type":"code","source":"s_missing= df.isnull().sum()\ns_missingper= (df.isnull().sum()/df.shape[0])*100\n\nexplo2 = pd.DataFrame({'Types': s_types,\n                       'Missing': s_missing,\n                      'Missing%': s_missingper,}).sort_values(by=['Missing%','Types'],ascending=False)\nexplo2.transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Depending on the categorical variable, missing value can means \"None\" (which I will fill with \"None\") or \"Not Available\" (which I will fill with the mode).  \nDepending on the numeric variable, missing value can means 0 (which I will fill with 0) or \"Not Available\" (which I will fill with the mean)."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('Alley','Utilities','MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n            'BsmtFinType2','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond',\n           'PoolQC','Fence','MiscFeature'):\n    df[col]=df[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('Electrical','MSZoning','Exterior1st','Exterior2nd','KitchenQual','SaleType','Functional'):\n    df[col]=df[col].fillna(df[col].mode()[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\n            'GarageYrBlt','GarageCars','GarageArea'):\n    df[col]=df[col].fillna(0) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['LotFrontage']=df['LotFrontage'].fillna(df['LotFrontage'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.4 - Outliers?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#npo= number of possible outliers\nlist_of_numerics=df.select_dtypes(include=['float','int']).columns\ns_npo= df.apply(lambda x: sum(i>3 for i in np.abs(zscore(x)))if x.name in list_of_numerics else '') \ns_npo2= df.apply(lambda x: sum((x < (x.quantile(0.25) - 1.5 * (x.quantile(0.75)- x.quantile(0.25))))|\n                               (x > (x.quantile(0.75) + 1.5 * (x.quantile(0.75)- x.quantile(0.25)))))\n                 if x.name in list_of_numerics else '')\n\nexplo3 = pd.DataFrame({'NPO': s_npo,\n                       'NPO2': s_npo2}).sort_values(by=['NPO', 'NPO2'])\nexplo3.transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although Zscore and IQR methods suggest several outliers, for while I'm going to focus on outliers with remotion recommended by the dataset author."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,2, figsize=(12,5))\n\nax1= sns.scatterplot(x='GrLivArea', y='SalePrice', data= df,ax=axes[0])\nax2= sns.boxplot(x='GrLivArea', data= df,ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing outliers recomended by author\ndf= df[df['GrLivArea']<4000]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3- Transforming\nFeature scaling is an important phase in data preparation. Two common methods are normalization (in general about rescale to range 0-1) and standardization (in general about rescale to have mean 0 and standard deviation 1).\nThese transformations doesn't change data shape as the log transformation does. Examples below.\nOn this kernel, the transformations will be done according to the need."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('classic')\nfig, axes = plt.subplots(1,4, figsize=(22,5))\n\nax1= sns.distplot(df.LotFrontage, bins= 30, hist_kws={'edgecolor':'k'},ax=axes[0])\nax1.set_title('LotFrontage')\n\nax2= sns.distplot(MinMaxScaler().fit_transform(df[['LotFrontage']]), bins= 30, hist_kws={'edgecolor':'k'},ax=axes[1])\nax2.set_title('LotFrontage with normalization')\n\nax3= sns.distplot(StandardScaler().fit_transform(df[['LotFrontage']]), bins= 30, hist_kws={'edgecolor':'k'},ax=axes[2])\nax3.set_title('LotFrontage with standardization')\n\nax4= sns.distplot(np.log(df[['LotFrontage']]), bins= 30, hist_kws={'edgecolor':'k'},ax=axes[3])\nax4.set_title('LotFrontage with log transformation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4- Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### 4.1- Feature Extraction\nFeature extraction is about to get new features and lower dimensionality. Here, the idea is to transform the dataset.  Principal Component Analysis is one of the main technique.  \nI'm going to use Yellowbrick to visualize PCA Decomposition. All will be based on official documentation: http://www.scikit-yb.org/en/latest/api/index.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num=pd.get_dummies(df)\nx= df_num.drop(['SalePrice'], axis=1)\ny= df_num.SalePrice\n\nvisu= PCADecomposition(scale=True)\nvisu.fit_transform(x,y)\nvisu.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2- Feature Selection\nFeature Selection is about select the most important features. This selection can help improving model performance and results.  \nI'm going to try:\n* Univariate Selection\n* Recursive Feature Elimination\n* Selection by Feature Importance\n\n(Selecting only 5 features for better visualizations)"},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.1 Univariate Selection \nThe features will be selected based on univariate statistical test. What variables have stronger relationship with the response?"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1)\nuni=SelectKBest( mutual_info_regression, k=5).fit(x,y)\nprint(x.columns[uni.get_support(indices=True)].tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.2 Recursive Feature Elimination\nThis method will remove less important features while building different models and checking the accuracy. "},{"metadata":{"trusted":true},"cell_type":"code","source":"est= DecisionTreeRegressor(random_state=1)\neli=RFE(est, 5).fit(x,y)\nprint(x.columns[eli.support_].tolist())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.3 Feature Importance\nLet's use Random Forest algorithm to get the feature importances "},{"metadata":{"trusted":true},"cell_type":"code","source":"RF= RandomForestRegressor(random_state=1)\nRF.fit(x,y)\nimportances=RF.feature_importances_\ns_importances=pd.Series(importances, index=x.columns).sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('dark_background')\nfig, axes = plt.subplots(1,2, figsize=(13,5), sharey=True) \n\nax1= sns.barplot(x=s_importances.index[0:5], y= s_importances[0:5], ax=axes[0])\nax1.set_title('Feature Importance')\n\ntrees=pd.DataFrame(data=[tree.feature_importances_ for tree in RF], columns=x.columns)[s_importances[0:5].index.tolist()]\nax2= sns.boxplot(data=trees, ax=axes[1])\nax2.set_title('Feature Importance Distributions')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4.3- Feature Creation"},{"metadata":{},"cell_type":"markdown","source":"### 4.3.1 - Discretization\nDiscretization is about transforming numeric variables into intervals.  \nIt's useful when the exact value is not so important and we can simplify. This action can improve model performance.\nBinning is a kind of discretization which has mainly 2 types: equal interval and equal frequency."},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"LotFrontage_bin_interval\"]= pd.cut(df.LotFrontage, 3, labels=[\"small\",\"medium\",\"large\"])\n\ndf[\"LotFrontage_bin_frequency\"]= pd.qcut(df.LotFrontage, 3, labels=[\"small\",\"medium\",\"large\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('default')\nfig, axes= plt.subplots(1,3, figsize=(13,4),sharey=True)\n\nax1= sns.distplot(df.LotFrontage, bins=40, hist_kws={'edgecolor':'k'}, color='darkorchid',kde=False,ax=axes[0])\nax1.set_title('Histogram')\nax1= sns.despine()\n\nax2= sns.countplot(x='LotFrontage_bin_frequency', data=df, palette=\"Purples\", ax=axes[1])\nax2.set_title('Binning with equal frequency')\nax2= sns.despine()\n\nax3= sns.countplot(x='LotFrontage_bin_interval', data=df, palette=\"Purples\", ax=axes[2])\nax3.set_title('Binning with equal interval')\nax3= sns.despine();","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}