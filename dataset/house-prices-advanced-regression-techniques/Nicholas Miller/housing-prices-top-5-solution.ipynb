{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Housing Prices - My Submission\n\nI created this notebook to share my work with my friends over at #kaggle_competitions for the Machine Learning Foundations Scholarship with Microsoft Azure.\n\n### A brief disclamer before I get started\n\nThis is a copy of what I did in Jupyter notebooks on my local machine using Anaconda 4.8.2 (Python 3.7.6).  I know there are some differences between Kaggle's version of python libraries and my own so I don't know if all this will work verbatim in Kaggle and give the same results as I got but I did give it a run through and it seems to return similar figures.  You can find a copy of my Jupyter notebooks here: https://github.com/cassova/Kaggle-HousingPrices\n\n* [Data Cleanup](#section-data-cleanup)\n    - [MSZoning](#subsection-MSZoning)\n    - [MSSubClass](#subsection-MSSubClass)\n    - [LotFrontage](#subsection-LotFrontage)\n    - [LotArea](#subsection-LotArea)\n    - [Street and Alley](#subsection-StreetAlley)\n    - [Drop unused fields](#subsection-DropFields)\n    - [LotConfig](#subsection-LotConfig)\n    - [Neighborhood](#subsection-Neighborhood)\n    - [NeighborhoodLotArea](#subsection-NeighborhoodLotArea)\n    - [BldgType](#subsection-BldgType)\n    - [NeighborhoodBldgType](#subsection-NeighborhoodBldgType)\n    - [Total Square Feet](#subsection-TotalSquareFeet)\n    - [HouseStyle](#subsection-HouseStyle)\n    - [NeighborhoodHouseStyle](#subsection-NeighborhoodHouseStyle)\n    - [OverallQualNCond](#subsection-OverallQualNCond)\n    - [NeighborhoodOverallQualNCond](#subsection-NeighborhoodOverallQualNCond)\n    - [Age](#subsection-Age)\n    - [Average Sale Price](#subsection-AverageSalePrice)\n    - [Exterior](#subsection-Exterior)\n    - [Foundation and Basement](#subsection-FondBase)\n    - [Heating / Cooling](#subsection-HeatCool)\n    - [Floor Sizes and Room Conditions](#subsection-FloorRoom)\n    - [Garage](#subsection-Garage)\n    - [Other Features](#Other)\n    - [Sale Info](#subsection-SaleInfo)\n    - [Results](#subsection-Results)\n* [One-Hot Encoding](#section-onehot)\n* [Column Selection](#section-columnselect)\n* [Training](#section-training)\n    - [Linear Regression](#subsection-LinearReg)\n    - [Stochastic Gradient Descent (SGD)](#subsection-StoGradDes)\n    - [Random Forest Classifier](#subsection-RandFore)\n    - [Polynomial Regression](#subsection-PolyReg)\n    - [Logistic Regression](#subsection-LogReg)\n    - [Gaussian Naive Bayes](#subsection-GNB)\n    - [Perceptron](#subsection-Perceptron)\n    - [Linear Support Vector Machine](#subsection-SVM)\n    - [Decision Tree](#subsection-DesiTree)\n    - [Random Forest Regressor](#subsection-RanForReg)\n    - [Gradient Boost Regressor](#subsection-GradBooReg)\n    - [AdaBoost](#subsection-AdaBoost)\n    - [Extremely Randomized Trees](#subsection-ExtRanTree)\n    - [Ensemble: VotingRegessor](#subsection-EnsVotReg)\n    - [Ensemble: Stacked Generalization](#subsection-EnsStakGen)\n    - [XGBoost](#subsection-XGBoost)\n    - [Best](#subsection-Best)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-data-cleanup\"></a>\n# Data Cleanup\n\nHere I went through all the columns and either dropped them outright or cleaned them.  I cleaned categorical items by converting them to numbers sorting their means by sale price.  I normalized specifications like number of rooms, area, and others using mean normalization (z-score).  Others were normalized with min-max.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\ntrain_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\nX_train = train_df.drop(['SalePrice'], axis=1)\ny_train = train_df['SalePrice']\ntrain_df\n\n# We will use a \"combined\" set to look across both the train and test to ensure we normalize our values and make our checks across both data sets.\ncombined = pd.concat([train_df,test_df])\nall_data = [train_df,test_df]\n\ntotal = combined.isnull().sum().sort_values(ascending=False)\npercent_1 = combined.isnull().sum()/combined.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df['SalePrice'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-MSZoning\"></a>\n## MSZoning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['MSZoning'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.groupby('MSZoning')['Id'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.groupby('MSZoning')['SalePrice'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined[combined['MSZoning'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\nBased on all this, I think we can default the MSZoning null values (which only appear in the test data set) with the most frequent value 'RL'. We can also convert 'RL' to number.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"MSZoningMap = {'FV': 1, 'RL': 2, 'RH': 3, 'RM': 4, 'C (all)': 5}\n\nfor d in all_data:\n    d['MSZoning'] = d['MSZoning'].fillna('RL')\n    d['MSZoning'] = d['MSZoning'].map(MSZoningMap)\n\ncombined['MSZoning'] = combined['MSZoning'].fillna('RL')\ncombined['MSZoning'] = combined['MSZoning'].map(MSZoningMap)\n    \ncombined['MSZoning'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[0].groupby('MSZoning')['SalePrice'].describe().sort_values('mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-MSSubClass\"></a>\n## MSSubClass","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subclass_map = train_df.groupby('MSSubClass')['SalePrice'].mean().sort_values()\nfor d in all_data:\n    for idx, s in enumerate(subclass_map.index):\n        d.loc[d['MSSubClass'] == s,'MSSubClass'] = idx+1\n\n        \nfor idx, s in enumerate(subclass_map.index):\n    combined.loc[combined['MSSubClass'] == s,'MSSubClass'] = idx+1\n\ntrain_df.groupby('MSSubClass')['SalePrice'].describe().sort_values('mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['MSSubClass'].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\nI don't see much coorelation directly between this and sale price but not sure at this point.  For now, will check other features to see if this might be useful in some way or if it should be dropped.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-LotFrontage\"></a>\n## LotFrontage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nplt.scatter(y_train, X_train['LotFrontage'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use mean normalization\ncombined['LotFrontage'] = combined['LotFrontage'].fillna(0)\n\nfor d in all_data:\n    d['LotFrontage'] = d['LotFrontage'].fillna(0)\n    d['LotFrontage'] = (d['LotFrontage']-combined['LotFrontage'].mean())/combined['LotFrontage'].std()\n    \n    \ncombined['LotFrontage'] = (combined['LotFrontage']-combined['LotFrontage'].mean())/combined['LotFrontage'].std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary:\nLot frontage is loosey coorelated to sale price so we probably have to add some features to take advantage of this.  I've put these into categories for now.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-LotArea\"></a>\n## LotArea","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[0].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_train, X_train['LotArea'], 'o')\nm, b = np.polyfit(y_train, X_train['LotArea'], 1)\nplt.plot(y_train, m*y_train + b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use mean normalization\ncombined['LotArea'] = combined['LotArea'].fillna(0)\n\nfor d in all_data:\n    d['LotArea'] = d['LotArea'].fillna(0)\n    d['LotArea'] = (d['LotArea']-combined['LotArea'].mean())/combined['LotArea'].std()\n\ncombined['LotArea'] = (combined['LotArea']-combined['LotArea'].mean())/combined['LotArea'].std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\nThe lot area is correlated to the price so we'll normalize it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-StreetAlley\"></a>\n## Street and Alley","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Alley'] = combined['Alley'].fillna('None')\n\nfor d in all_data:\n    d['Alley'] = d['Alley'].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.groupby(['Street', 'Alley']).describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data[0].groupby('Street')['SalePrice'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\nI don't think either of these features really contribute at all so I'm going to drop them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop(['Street','Alley'], axis=1, inplace=True)\n\nfor d in all_data:\n    d.drop(['Street','Alley'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-DropFields\"></a>\n## Drop unused fields\nHere is where I decided which fields I wouldn't use at all.  I did some analysis on these elsewhere but I also trusted my gut on others.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop(['LotShape','LandContour', 'Utilities', 'LandSlope', \\\n               'Condition1', 'Condition2', 'YearBuilt', 'RoofStyle', \\\n               'RoofMatl', 'Heating', 'Electrical', 'Functional', \\\n               'GarageYrBlt', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \\\n               'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', \\\n               'SaleType'], axis=1, inplace=True)\n\nfor d in all_data:\n    d.drop(['LotShape','LandContour', 'Utilities', 'LandSlope', \\\n            'Condition1', 'Condition2', 'YearBuilt', 'RoofStyle', \\\n            'RoofMatl', 'Heating', 'Electrical', 'Functional', \\\n            'GarageYrBlt', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \\\n            'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MiscVal', \\\n            'SaleType'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-LotConfig\"></a>\n## LotConfig","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subclass_map = train_df.groupby('LotConfig')['SalePrice'].mean().sort_values()\nfor d in all_data:\n    for idx, s in enumerate(subclass_map.index):\n        d.loc[d['LotConfig'] == s,'LotConfig'] = idx+1\n\n        \nfor idx, s in enumerate(subclass_map.index):\n    combined.loc[combined['LotConfig'] == s,'LotConfig'] = idx+1\n\ntrain_df.groupby('LotConfig')['SalePrice'].describe().sort_values('mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-Neighborhood\"></a>\n## Neighborhood","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subclass_map = train_df.groupby('Neighborhood')['SalePrice'].mean().sort_values()\nfor d in all_data:\n    for idx, s in enumerate(subclass_map.index):\n        d.loc[d['Neighborhood'] == s,'Neighborhood'] = idx+1\n\n        \nfor idx, s in enumerate(subclass_map.index):\n    combined.loc[combined['Neighborhood'] == s,'Neighborhood'] = idx+1\n\ntrain_df.groupby('Neighborhood')['SalePrice'].describe().sort_values('mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-NeighborhoodLotArea\"></a>\n## NeighborhoodLotArea","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Added new column and used min/max normalization\n\ncombined['NeighborhoodLotArea'] = combined['Neighborhood'] * combined['LotArea']\n\nfor d in all_data:\n    d['NeighborhoodLotArea'] = d['Neighborhood'] * d['LotArea']\n    d['NeighborhoodLotArea'] = (d['NeighborhoodLotArea'] - combined['NeighborhoodLotArea'].min()) \\\n                                      / (combined['NeighborhoodLotArea'].max()-combined['NeighborhoodLotArea'].min())\n\ncombined['NeighborhoodLotArea'] = (combined['NeighborhoodLotArea'] - combined['NeighborhoodLotArea'].min()) \\\n                                  / (combined['NeighborhoodLotArea'].max()-combined['NeighborhoodLotArea'].min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-BldgType\"></a>\n## BldgType","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subclass_map = train_df.groupby('BldgType')['SalePrice'].mean().sort_values()\nfor d in all_data:\n    for idx, s in enumerate(subclass_map.index):\n        d.loc[d['BldgType'] == s,'BldgType'] = idx+1\n\n        \nfor idx, s in enumerate(subclass_map.index):\n    combined.loc[combined['BldgType'] == s,'BldgType'] = idx+1\n\ntrain_df.groupby('BldgType')['SalePrice'].describe().sort_values('mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-NeighborhoodBldgType\"></a>\n## NeighborhoodBldgType","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Added new column and used min/max normalization\n\ncombined['NeighborhoodBldgType'] = combined['Neighborhood'] * combined['BldgType']\n\nfor d in all_data:\n    d['NeighborhoodBldgType'] = d['Neighborhood'] * d['BldgType']\n    d['NeighborhoodBldgType'] = (d['NeighborhoodBldgType'] - combined['NeighborhoodBldgType'].min()) \\\n                                      / (combined['NeighborhoodBldgType'].max()-combined['NeighborhoodBldgType'].min())\n\ncombined['NeighborhoodBldgType'] = (combined['NeighborhoodBldgType'] - combined['NeighborhoodBldgType'].min()) \\\n                                  / (combined['NeighborhoodBldgType'].max()-combined['NeighborhoodBldgType'].min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-TotalSquareFeet\"></a>\n## Total Square Feet","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['TotalBsmtSF'] = combined['TotalBsmtSF'].fillna(0)\ncombined['GarageArea'] = combined['GarageArea'].fillna(0)\n\ncombined['TotalSquareFeet'] = combined['TotalBsmtSF'] + combined['1stFlrSF'] \\\n                            + combined['2ndFlrSF'] + combined['GarageArea'] \\\n                            + combined['WoodDeckSF']\n\nfor d in all_data:\n    d['TotalBsmtSF'] = d['TotalBsmtSF'].fillna(0)\n    d['GarageArea'] = d['GarageArea'].fillna(0)\n    d['TotalSquareFeet'] = d['TotalBsmtSF'] + d['1stFlrSF'] + d['2ndFlrSF'] + d['GarageArea'] + d['WoodDeckSF']\n    d['TotalSquareFeet'] = (d['TotalSquareFeet']-combined['TotalSquareFeet'].mean())/combined['TotalSquareFeet'].std()\n    \ncombined['TotalSquareFeet'] = (combined['TotalSquareFeet']-combined['TotalSquareFeet'].mean())/combined['TotalSquareFeet'].std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-HouseStyle\"></a>\n## HouseStyle","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subclass_map = train_df.groupby('HouseStyle')['SalePrice'].mean().sort_values()\nfor d in all_data:\n    for idx, s in enumerate(subclass_map.index):\n        d.loc[d['HouseStyle'] == s,'HouseStyle'] = idx+1\n\n        \nfor idx, s in enumerate(subclass_map.index):\n    combined.loc[combined['HouseStyle'] == s,'HouseStyle'] = idx+1\n\ntrain_df.groupby('HouseStyle')['SalePrice'].describe().sort_values('mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-NeighborhoodHouseStyle\"></a>\n## NeighborhoodHouseStyle","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Added new column and used min/max normalization\n\ncombined['NeighborhoodHouseStyle'] = combined['Neighborhood'] * combined['HouseStyle']\n\nfor d in all_data:\n    d['NeighborhoodHouseStyle'] = d['Neighborhood'] * d['HouseStyle']\n    d['NeighborhoodHouseStyle'] = (d['NeighborhoodHouseStyle'] - combined['NeighborhoodHouseStyle'].min()) \\\n                                      / (combined['NeighborhoodHouseStyle'].max()-combined['NeighborhoodHouseStyle'].min())\n\ncombined['NeighborhoodHouseStyle'] = (combined['NeighborhoodHouseStyle'] - combined['NeighborhoodHouseStyle'].min()) \\\n                                  / (combined['NeighborhoodHouseStyle'].max()-combined['NeighborhoodHouseStyle'].min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-OverallQualNCond\"></a>\n## OverallQualNCond","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Added new column and used min/max normalization\n\ncombined['OverallQualNCond'] = combined['OverallQual'] * combined['OverallCond']\n\nfor d in all_data:\n    d['OverallQualNCond'] = d['OverallQual'] * d['OverallCond']\n    d['OverallQualNCond'] = (d['OverallQualNCond'] - combined['OverallQualNCond'].min()) \\\n                                      / (combined['OverallQualNCond'].max()-combined['OverallQualNCond'].min())\n\ncombined['OverallQualNCond'] = (combined['OverallQualNCond'] - combined['OverallQualNCond'].min()) \\\n                                  / (combined['OverallQualNCond'].max()-combined['OverallQualNCond'].min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-NeighborhoodOverallQualNCond\"></a>\n## NeighborhoodOverallQualNCond","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Added new column and used min/max normalization\n\ncombined['NeighborhoodOverallQualNCond'] = combined['Neighborhood'] * combined['OverallQualNCond']\n\nfor d in all_data:\n    d['NeighborhoodOverallQualNCond'] = d['Neighborhood'] * d['OverallQualNCond']\n    d['NeighborhoodOverallQualNCond'] = (d['NeighborhoodOverallQualNCond'] - combined['NeighborhoodOverallQualNCond'].min()) \\\n                                      / (combined['NeighborhoodOverallQualNCond'].max()-combined['NeighborhoodOverallQualNCond'].min())\n\ncombined['NeighborhoodOverallQualNCond'] = (combined['NeighborhoodOverallQualNCond'] - combined['NeighborhoodOverallQualNCond'].min()) \\\n                                  / (combined['NeighborhoodOverallQualNCond'].max()-combined['NeighborhoodOverallQualNCond'].min())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-Age\"></a>\n## Age\nWe'll determine this by getting the max MoSold/YrSold and count the months back to last remodelled since we determined build date is less correlated than remodelled date (assuming built & remodelled on 1st month of the year)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_year = combined['YrSold'].max()\nmax_month = combined.loc[combined['YrSold'] == max_year, 'MoSold'].max()\n\nfor d in all_data:\n    months_ago = (max_year-d['YearRemodAdd'])*12 + max_month\n    d['AdjRemodAdd'] = (months_ago - months_ago.min()) / (months_ago.max()-months_ago.min()) \\\n                     * -1 + 1 # reverse the order\n    d.drop(['YearRemodAdd'], axis=1, inplace=True)\n\nmonths_ago = (max_year-combined['YearRemodAdd'])*12 + max_month\ncombined['AdjRemodAdd'] = (months_ago - months_ago.min()) / (months_ago.max()-months_ago.min()) \\\n                        * -1 + 1 # reverse the order\ncombined.drop(['YearRemodAdd'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nX_train = train_df.drop(['SalePrice'], axis=1)\nplt.plot(y_train, X_train['AdjRemodAdd'], 'o')\nm, b = np.polyfit(y_train, X_train['AdjRemodAdd'], 1)\nplt.plot(y_train, m*y_train + b)\nprint (mean_squared_error(y_train, m*y_train + b))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-AverageSalePrice\"></a>\n## Average Sale Price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get the average of each year/month\ndf = train_df.groupby(['YrSold','MoSold'])[['SalePrice']].mean()\ndf = df.rename({'SalePrice': 'MonthMean'}, axis=1)\n\n# get the rolling average spanning 3 months\nqt = (df[['MonthMean']] + df[['MonthMean']].shift(1) + df[['MonthMean']].shift(-1)) / 3\ndf['QuarterMean'] = qt['MonthMean']\ndf['QuarterMean'] = df['QuarterMean'].fillna(df['MonthMean'])\nprint(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge the averages onto our datasets\nfor idx, d in enumerate(all_data):\n    all_data[idx] = pd.merge(d, df, how='left', left_on=['YrSold','MoSold'], right_on=['YrSold','MoSold'])\n    all_data[idx].drop(['MoSold', 'YrSold'], axis=1, inplace=True)\n    \ncombined = pd.merge(combined, df, how='left', left_on=['YrSold','MoSold'], right_on=['YrSold','MoSold'])\ncombined.drop(['MoSold', 'YrSold'], axis=1, inplace=True)\n\n# Fix the reference since above we generated new references for all_data\ntrain_df = all_data[0]\ntest_df = all_data[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the values and drop the old ones\n\nfor d in all_data:\n    d['QuarterMeanNorm'] = (d['QuarterMean']-combined['QuarterMean'].mean())/combined['QuarterMean'].std()\n    d['MonthMeanNorm'] = (d['MonthMean']-combined['MonthMean'].mean())/combined['MonthMean'].std()\n    #d.drop(['QuarterMean','MonthMean'], axis=1, inplace=True)\n\ncombined['QuarterMeanNorm'] = (combined['QuarterMean']-combined['QuarterMean'].mean())/combined['QuarterMean'].std()\ncombined['MonthMeanNorm'] = (combined['MonthMean']-combined['MonthMean'].mean())/combined['MonthMean'].std()\n#combined.drop(['QuarterMean','MonthMean'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-Exterior\"></a>\n## Exterior","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ex1_map = {'Other': 0, 'BrkFace': 1, 'CemntBd': 2, 'Plywood': 3, 'Wd Sdng': 4, 'MetalSd': 5, 'HdBoard': 6, 'VinylSd': 7}\nex2_map = {'Other': 0, 'AsbShng': 1, 'BrkFace': 2, 'Stucco': 3, 'Wd Shng': 4, 'CmentBd': 5, 'Plywood': 7, 'Wd Sdng': 8, 'HdBoard': 9, 'MetalSd': 10, 'VinylSd': 11}\nmax_map = {'None': 0, 'BrkCmn': 1, 'Stone': 2, 'BrkFace': 3}\nexg_map = {'Po': 0.0, 'Fa': 0.25, 'TA': 0.5, 'Gd': 0.75, 'Ex': 1.0}\n\ncombined['MasVnrArea'] = combined['MasVnrArea'].fillna(0)\n\nfor d in all_data:\n    d['Exterior1st'] = d['Exterior1st'].fillna(combined['Exterior1st'].mode()[0])\n    d['Exterior1st'] = d['Exterior1st'].replace(['AsphShn','CBlock','ImStucc','BrkComm','Stone','AsbShng','Stucco','WdShing'], 'Other')\n    d['Exterior1st'] = d['Exterior1st'].map(ex1_map)\n    d['Exterior2nd'] = d['Exterior2nd'].fillna(combined['Exterior2nd'].mode()[0])\n    d['Exterior2nd'] = d['Exterior2nd'].replace(['CBlock','AsphShn','Stone','Brk Cmn','ImStucc'], 'Other')\n    d['Exterior2nd'] = d['Exterior2nd'].map(ex2_map)\n    d['MasVnrType'] = d['MasVnrType'].fillna('None')\n    d['MasVnrType'] = d['MasVnrType'].map(max_map)\n    d['MasVnrArea'] = d['MasVnrArea'].fillna(0)\n    d['MasVnrArea'] = (d['MasVnrArea'] - combined['MasVnrArea'].min()) \\\n                      / (combined['MasVnrArea'].max()-combined['MasVnrArea'].min())\n    d['ExterQual'] = d['ExterQual'].map(exg_map)\n    d['ExterCond'] = d['ExterCond'].map(exg_map)\n\ncombined['Exterior1st'] = combined['Exterior1st'].fillna(combined['Exterior1st'].mode()[0])\ncombined['Exterior1st'] = combined['Exterior1st'].replace(['AsphShn','CBlock','ImStucc','BrkComm','Stone','AsbShng','Stucco','WdShing'], 'Other')\ncombined['Exterior1st'] = combined['Exterior1st'].map(ex1_map)\ncombined['Exterior2nd'] = combined['Exterior2nd'].fillna(combined['Exterior2nd'].mode()[0])\ncombined['Exterior2nd'] = combined['Exterior2nd'].replace(['CBlock','AsphShn','Stone','Brk Cmn','ImStucc'], 'Other')\ncombined['Exterior2nd'] = combined['Exterior2nd'].map(ex2_map)\ncombined['MasVnrType'] = combined['MasVnrType'].fillna('None')\ncombined['MasVnrType'] = combined['MasVnrType'].map(max_map)\ncombined['MasVnrArea'] = (combined['MasVnrArea'] - combined['MasVnrArea'].min()) \\\n                         / (combined['MasVnrArea'].max()-combined['MasVnrArea'].min())\ncombined['ExterQual'] = combined['ExterQual'].map(exg_map)\ncombined['ExterCond'] = combined['ExterCond'].map(exg_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-FondBase\"></a>\n## Foundation and Basement","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fnd_map = {'Slab': 0, 'BrkTil': 1, 'CBlock': 2, 'Other': 3, 'PConc': 4}\nbsm_map = {'Po': 0.0, 'Fa': 0.25, 'TA': 0.5, 'Gd': 0.75, 'Ex': 1.0}\nbex_map = {'NA': 0.0, 'No': 0.25, 'Mn': 0.5, 'Av': 0.75, 'Gd': 1.0}\nbfi_map = {'NA': 0.0, 'Unf': 0.17, 'LwQ': 0.33, 'Rec': 0.5, 'BLQ': 0.67, 'ALQ': 0.83, 'GLQ': 1.0}\n\ncombined['BsmtFinSF1'] = combined['BsmtFinSF1'].fillna(0)\ncombined['BsmtFinSF2'] = combined['BsmtFinSF2'].fillna(0)\ncombined['BsmtUnfSF'] = combined['BsmtUnfSF'].fillna(0)\ncombined['TotalBsmtSF'] = combined['TotalBsmtSF'].fillna(0)\ncombined['BsmtFullBath'] = combined['BsmtFullBath'].fillna(0)\ncombined['BsmtHalfBath'] = combined['BsmtFullBath'].fillna(0)\n\nfor d in all_data:\n    d['Foundation'] = d['Foundation'].replace(['Stone', 'Wood'], 'Other')\n    d['Foundation'] = d['Foundation'].map(fnd_map)\n    d['BsmtQual'] = d['BsmtQual'].fillna('Po')\n    d['BsmtQual'] = d['BsmtQual'].map(bsm_map)\n    d['BsmtCond'] = d['BsmtCond'].fillna('Po')\n    d['BsmtCond'] = d['BsmtCond'].map(bsm_map)\n    d['BsmtExposure'] = d['BsmtExposure'].fillna('NA')\n    d['BsmtExposure'] = d['BsmtExposure'].map(bex_map)\n    d['BsmtFinType1'] = d['BsmtFinType1'].fillna('NA')\n    d['BsmtFinType1'] = d['BsmtFinType1'].map(bfi_map)\n    d['BsmtFinType2'] = d['BsmtFinType2'].fillna('NA')\n    d['BsmtFinType2'] = d['BsmtFinType2'].map(bfi_map)\n    d['BsmtFinSF1'] = d['BsmtFinSF1'].fillna(0)\n#     d['BsmtFinSF1'] = (d['BsmtFinSF1'] - combined['BsmtFinSF1'].min()) \\\n#                       / (combined['BsmtFinSF1'].max()-combined['BsmtFinSF1'].min())\n    d['BsmtFinSF1'] = (d['BsmtFinSF1']-combined['BsmtFinSF1'].mean())/combined['BsmtFinSF1'].std()\n    d['BsmtFinSF2'] = d['BsmtFinSF2'].fillna(0)\n#     d['BsmtFinSF2'] = (d['BsmtFinSF2'] - combined['BsmtFinSF2'].min()) \\\n#                       / (combined['BsmtFinSF2'].max()-combined['BsmtFinSF2'].min())\n    d['BsmtFinSF2'] = (d['BsmtFinSF2']-combined['BsmtFinSF2'].mean())/combined['BsmtFinSF2'].std()\n    d['BsmtUnfSF'] = d['BsmtUnfSF'].fillna(0)\n#     d['BsmtUnfSF'] = (d['BsmtUnfSF'] - combined['BsmtUnfSF'].min()) \\\n#                       / (combined['BsmtUnfSF'].max()-combined['BsmtUnfSF'].min())\n    d['BsmtUnfSF'] = (d['BsmtUnfSF']-combined['BsmtUnfSF'].mean())/combined['BsmtUnfSF'].std()\n    d['TotalBsmtSF'] = d['TotalBsmtSF'].fillna(0)\n#     d['TotalBsmtSF'] = (d['TotalBsmtSF'] - combined['TotalBsmtSF'].min()) \\\n#                       / (combined['TotalBsmtSF'].max()-combined['TotalBsmtSF'].min())\n    d['TotalBsmtSF'] = (d['TotalBsmtSF']-combined['TotalBsmtSF'].mean())/combined['TotalBsmtSF'].std()\n    d['BsmtFullBath'] = d['BsmtFullBath'].fillna(0)\n    d['BsmtHalfBath'] = d['BsmtFullBath'].fillna(0)\n    d['BsmtFullBath'] = (d['BsmtFullBath']-combined['BsmtFullBath'].mean())/combined['BsmtFullBath'].std()\n    d['BsmtHalfBath'] = (d['BsmtHalfBath']-combined['BsmtHalfBath'].mean())/combined['BsmtHalfBath'].std()\n\ncombined['Foundation'] = combined['Foundation'].replace(['Stone', 'Wood'], 'Other')\ncombined['Foundation'] = combined['Foundation'].map(fnd_map)\ncombined['BsmtQual'] = combined['BsmtQual'].fillna('Po')\ncombined['BsmtQual'] = combined['BsmtQual'].map(bsm_map)\ncombined['BsmtCond'] = combined['BsmtCond'].fillna('Po')\ncombined['BsmtCond'] = combined['BsmtCond'].map(bsm_map)\ncombined['BsmtExposure'] = combined['BsmtExposure'].fillna('NA')\ncombined['BsmtExposure'] = combined['BsmtExposure'].map(bex_map)\ncombined['BsmtFinType1'] = combined['BsmtFinType1'].fillna('NA')\ncombined['BsmtFinType1'] = combined['BsmtFinType1'].map(bfi_map)\ncombined['BsmtFinType2'] = combined['BsmtFinType2'].fillna('NA')\ncombined['BsmtFinType2'] = combined['BsmtFinType2'].map(bfi_map)\n# combined['BsmtFinSF1'] = (combined['BsmtFinSF1'] - combined['BsmtFinSF1'].min()) \\\n#                          / (combined['BsmtFinSF1'].max()-combined['BsmtFinSF1'].min())\ncombined['BsmtFinSF1'] = (combined['BsmtFinSF1']-combined['BsmtFinSF1'].mean())/combined['BsmtFinSF1'].std()\n# combined['BsmtFinSF2'] = (combined['BsmtFinSF2'] - combined['BsmtFinSF2'].min()) \\\n#                          / (combined['BsmtFinSF2'].max()-combined['BsmtFinSF2'].min())\ncombined['BsmtFinSF2'] = (combined['BsmtFinSF2']-combined['BsmtFinSF2'].mean())/combined['BsmtFinSF2'].std()\n# combined['BsmtUnfSF'] = (combined['BsmtUnfSF'] - combined['BsmtUnfSF'].min()) \\\n#                          / (combined['BsmtUnfSF'].max()-combined['BsmtUnfSF'].min())\ncombined['BsmtUnfSF'] = (combined['BsmtUnfSF']-combined['BsmtUnfSF'].mean())/combined['BsmtUnfSF'].std()\n# combined['TotalBsmtSF'] = (combined['TotalBsmtSF'] - combined['TotalBsmtSF'].min()) \\\n#                          / (combined['TotalBsmtSF'].max()-combined['TotalBsmtSF'].min())\ncombined['TotalBsmtSF'] = (combined['TotalBsmtSF']-combined['TotalBsmtSF'].mean())/combined['TotalBsmtSF'].std()\ncombined['BsmtFullBath'] = (combined['BsmtFullBath']-combined['BsmtFullBath'].mean())/combined['BsmtFullBath'].std()\ncombined['BsmtHalfBath'] = (combined['BsmtHalfBath']-combined['BsmtHalfBath'].mean())/combined['BsmtHalfBath'].std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-HeatCool\"></a>\n## Heating / Cooling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ht_map = {'Po': 0.0, 'Fa': 0.25, 'TA': 0.5, 'Gd': 0.75, 'Ex': 1.0}\nyn_map = {'N': 0, 'Y': 1}\n\nfor d in all_data:\n    d['HeatingQC'] = d['HeatingQC'].map(ht_map)\n    d['CentralAir'] = d['CentralAir'].map(yn_map)\n    \ncombined['HeatingQC'] = combined['HeatingQC'].map(ht_map)\ncombined['CentralAir'] = combined['CentralAir'].map(yn_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-FloorRoom\"></a>\n## Floor Sizes and Room Conditions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rqc_map = {'Po': 0.0, 'Fa': 0.25, 'TA': 0.5, 'Gd': 0.75, 'Ex': 1.0}\n\nfor d in all_data:\n    d['OverallQual'] = (d['OverallQual'] - combined['OverallQual'].min()) \\\n                       / (combined['OverallQual'].max()-combined['OverallQual'].min())\n    d['OverallCond'] = (d['OverallCond'] - combined['OverallCond'].min()) \\\n                       / (combined['OverallCond'].max()-combined['OverallCond'].min())\n#     d['1stFlrSF'] = (d['1stFlrSF'] - combined['1stFlrSF'].min()) \\\n#                     / (combined['1stFlrSF'].max()-combined['1stFlrSF'].min())\n    d['1stFlrSF'] = (d['1stFlrSF']-combined['1stFlrSF'].mean())/combined['1stFlrSF'].std()\n#     d['2ndFlrSF'] = (d['2ndFlrSF'] - combined['2ndFlrSF'].min()) \\\n#                     / (combined['2ndFlrSF'].max()-combined['2ndFlrSF'].min())\n    d['2ndFlrSF'] = (d['2ndFlrSF']-combined['2ndFlrSF'].mean())/combined['2ndFlrSF'].std()\n#     d['LowQualFinSF'] = (d['LowQualFinSF'] - combined['LowQualFinSF'].min()) \\\n#                     / (combined['LowQualFinSF'].max()-combined['LowQualFinSF'].min())\n    d['LowQualFinSF'] = (d['LowQualFinSF']-combined['LowQualFinSF'].mean())/combined['LowQualFinSF'].std()\n#     d['GrLivArea'] = (d['GrLivArea'] - combined['GrLivArea'].min()) \\\n#                     / (combined['GrLivArea'].max()-combined['GrLivArea'].min())\n    d['GrLivArea'] = (d['GrLivArea']-combined['GrLivArea'].mean())/combined['GrLivArea'].std()\n    d['KitchenQual'] = d['KitchenQual'].fillna(d['KitchenQual'].mode()[0])\n    d['KitchenQual'] = d['KitchenQual'].map(rqc_map)\n    d['FireplaceQu'] = d['FireplaceQu'].fillna('Po')\n    d['FireplaceQu'] = d['FireplaceQu'].map(rqc_map)\n    d['FullBath'] = (d['FullBath']-combined['FullBath'].mean())/combined['FullBath'].std()\n    d['HalfBath'] = (d['HalfBath']-combined['HalfBath'].mean())/combined['HalfBath'].std()\n    d['BedroomAbvGr'] = (d['BedroomAbvGr']-combined['BedroomAbvGr'].mean())/combined['BedroomAbvGr'].std()\n    d['KitchenAbvGr'] = (d['KitchenAbvGr']-combined['KitchenAbvGr'].mean())/combined['KitchenAbvGr'].std()\n    d['Fireplaces'] = (d['Fireplaces']-combined['Fireplaces'].mean())/combined['Fireplaces'].std()\n    d['TotRmsAbvGrd'] = (d['TotRmsAbvGrd']-combined['TotRmsAbvGrd'].mean())/combined['TotRmsAbvGrd'].std()\n\ncombined['OverallQual'] = (combined['OverallQual'] - combined['OverallQual'].min()) \\\n                         / (combined['OverallQual'].max()-combined['OverallQual'].min())\ncombined['OverallCond'] = (combined['OverallCond'] - combined['OverallCond'].min()) \\\n                         / (combined['OverallCond'].max()-combined['OverallCond'].min())\n# combined['1stFlrSF'] = (combined['1stFlrSF'] - combined['1stFlrSF'].min()) \\\n#                          / (combined['1stFlrSF'].max()-combined['1stFlrSF'].min())\ncombined['1stFlrSF'] = (combined['1stFlrSF']-combined['1stFlrSF'].mean())/combined['1stFlrSF'].std()\n# combined['2ndFlrSF'] = (combined['2ndFlrSF'] - combined['2ndFlrSF'].min()) \\\n#                          / (combined['2ndFlrSF'].max()-combined['2ndFlrSF'].min())\ncombined['2ndFlrSF'] = (combined['2ndFlrSF']-combined['2ndFlrSF'].mean())/combined['2ndFlrSF'].std()\n# combined['LowQualFinSF'] = (combined['LowQualFinSF'] - combined['LowQualFinSF'].min()) \\\n#                          / (combined['LowQualFinSF'].max()-combined['LowQualFinSF'].min())\ncombined['LowQualFinSF'] = (combined['LowQualFinSF']-combined['LowQualFinSF'].mean())/combined['LowQualFinSF'].std()\n# combined['GrLivArea'] = (combined['GrLivArea'] - combined['GrLivArea'].min()) \\\n#                          / (combined['GrLivArea'].max()-combined['GrLivArea'].min())\ncombined['GrLivArea'] = (combined['GrLivArea']-combined['GrLivArea'].mean())/combined['GrLivArea'].std()\ncombined['KitchenQual'] = combined['KitchenQual'].fillna(combined['KitchenQual'].mode()[0])\ncombined['KitchenQual'] = combined['KitchenQual'].map(rqc_map)\ncombined['FireplaceQu'] = combined['FireplaceQu'].fillna('Po')\ncombined['FireplaceQu'] = combined['FireplaceQu'].map(rqc_map)\ncombined['FullBath'] = (combined['FullBath']-combined['FullBath'].mean())/combined['FullBath'].std()\ncombined['HalfBath'] = (combined['HalfBath']-combined['HalfBath'].mean())/combined['HalfBath'].std()\ncombined['BedroomAbvGr'] = (combined['BedroomAbvGr']-combined['BedroomAbvGr'].mean())/combined['BedroomAbvGr'].std()\ncombined['KitchenAbvGr'] = (combined['KitchenAbvGr']-combined['KitchenAbvGr'].mean())/combined['KitchenAbvGr'].std()\ncombined['Fireplaces'] = (combined['Fireplaces']-combined['Fireplaces'].mean())/combined['Fireplaces'].std()\ncombined['TotRmsAbvGrd'] = (combined['TotRmsAbvGrd']-combined['TotRmsAbvGrd'].mean())/combined['TotRmsAbvGrd'].std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-Garage\"></a>\n## Garage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grt_map = {'None': 0, 'Detchd': 1, 'Other': 2, 'Attchd': 3, 'BuiltIn': 4}\ngrf_map = {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\ngqc_map = {'Po': 0.0, 'Fa': 0.25, 'TA': 0.5, 'Gd': 0.75, 'Ex': 1.0}\npdr_map = {'N': 0.0, 'P': 0.5, 'Y': 1.0}\n\ncombined['GarageCars'] = combined['GarageCars'].fillna(0)\ncombined['GarageArea'] = combined['GarageArea'].fillna(0)\n\nfor d in all_data:\n    d['GarageType'] = d['GarageType'].fillna('None')\n    d['GarageType'] = d['GarageType'].replace(['CarPort', '2Types', 'Basment'], 'Other')\n    d['GarageType'] = d['GarageType'].map(grt_map)\n    d['GarageFinish'] = d['GarageFinish'].fillna('None')\n    d['GarageFinish'] = d['GarageFinish'].map(grf_map)\n    d['GarageCars'] = d['GarageCars'].fillna(0)\n    d['GarageCars'] = (d['GarageCars']-combined['GarageCars'].mean())/combined['GarageCars'].std()\n    d['GarageArea'] = d['GarageArea'].fillna(0)\n    d['GarageArea'] = (d['GarageArea']-combined['GarageArea'].mean())/combined['GarageArea'].std()\n    d['GarageQual'] = d['GarageQual'].fillna('Po')\n    d['GarageQual'] = d['GarageQual'].map(gqc_map)\n    d['GarageCond'] = d['GarageCond'].fillna('Po')\n    d['GarageCond'] = d['GarageCond'].map(gqc_map)\n    d['PavedDrive'] = d['PavedDrive'].map(pdr_map)\n    \ncombined['GarageType'] = combined['GarageType'].fillna('None')\ncombined['GarageType'] = combined['GarageType'].replace(['CarPort', '2Types', 'Basment'], 'Other')\ncombined['GarageType'] = combined['GarageType'].map(grt_map)\ncombined['GarageFinish'] = combined['GarageFinish'].fillna('None')\ncombined['GarageFinish'] = combined['GarageFinish'].map(grf_map)\ncombined['GarageCars'] = (combined['GarageCars']-combined['GarageCars'].mean())/combined['GarageCars'].std()\ncombined['GarageArea'] = (combined['GarageArea']-combined['GarageArea'].mean())/combined['GarageArea'].std()\ncombined['GarageQual'] = combined['GarageQual'].fillna('Po')\ncombined['GarageQual'] = combined['GarageQual'].map(gqc_map)\ncombined['GarageCond'] = combined['GarageCond'].fillna('Po')\ncombined['GarageCond'] = combined['GarageCond'].map(gqc_map)\ncombined['PavedDrive'] = combined['PavedDrive'].map(pdr_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-Other\"></a>\n## Other Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(y_train, X_train['WoodDeckSF'], 'o')\nm, b = np.polyfit(y_train, X_train['WoodDeckSF'], 1)\nplt.plot(y_train, m*y_train + b)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for d in all_data:\n    d['WoodDeckSF'] = (d['WoodDeckSF']-combined['WoodDeckSF'].mean())/combined['WoodDeckSF'].std()\n    d['OpenPorchSF'] = (d['OpenPorchSF']-combined['OpenPorchSF'].mean())/combined['OpenPorchSF'].std()\n    \ncombined['WoodDeckSF'] = (combined['WoodDeckSF']-combined['WoodDeckSF'].mean())/combined['WoodDeckSF'].std()\ncombined['OpenPorchSF'] = (combined['OpenPorchSF']-combined['OpenPorchSF'].mean())/combined['OpenPorchSF'].std()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-SaleInfo\"></a>\n## Sale Info","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sac_map = {'Other': 0, 'Abnorml': 1, 'Normal': 2, 'Partial': 3}\n\nfor d in all_data:\n    d['SaleCondition'] = d['SaleCondition'].replace(['AdjLand', 'Family', 'Alloca'], 'Other')\n    d['SaleCondition'] = d['SaleCondition'].map(sac_map)\n    \ncombined['SaleCondition'] = combined['SaleCondition'].replace(['AdjLand', 'Family', 'Alloca'], 'Other')\ncombined['SaleCondition'] = combined['SaleCondition'].map(sac_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-Results\"></a>\n## Results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ntrain_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"field = 'SaleCondition'\nprint(f'{combined[field].isnull().sum()} or {combined[field].isnull().sum() / 2919. * 100}% empty')\ntrain_df.groupby(field)['SalePrice'].describe().sort_values('mean')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Output new files","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.to_csv('new_train.csv', index=False)\ntest_df.to_csv('new_test.csv', index=False)\nprint(\"New files have been created.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-onehot\"></a>\n# One-Hot Encoding\n\nSo I know some of you might be thinking why I have a whole different section for this.  Well, to be honest, one-hot encoding was an after thought for me.  After I did the above data cleanup, I tested it and got good results and wanted to see what it would look like if I did one-hot encoding and I got better results so I kept it in.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.metrics import mean_squared_log_error\n\nX = pd.read_csv(\"/kaggle/working/new_train.csv\")\ny = pd.read_csv(\"/kaggle/working/new_test.csv\")\n\nall_data = [X,y]\n\npd.set_option('display.max_columns', None)\nX.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"B = pd.concat([X,y])\n\nB = pd.concat([B, pd.get_dummies(B['Neighborhood'], prefix='Neighborhood', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['LotConfig'], prefix='LotConfig', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['BldgType'], prefix='BldgType', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['HouseStyle'], prefix='HouseStyle', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['SaleCondition'], prefix='SaleCondition', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['GarageFinish'], prefix='GarageFinish', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['GarageType'], prefix='GarageType', drop_first=True)], axis=1)\nB = pd.concat([B, pd.get_dummies(B['Foundation'], prefix='Foundation', drop_first=True)], axis=1)\n\n#B.drop(['Neighborhood'], axis=1, inplace=True)\nB.drop(['LotConfig', 'BldgType', 'HouseStyle', 'SaleCondition', 'GarageFinish', 'GarageType', 'Foundation'], axis=1, inplace=True)\nB.drop(['MasVnrType', 'Exterior2nd', 'Exterior1st', 'MSZoning', 'MSSubClass'], axis=1, inplace=True)\n\nB.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = B[:len(X)]\ny = B[len(X):]\ny = y.drop('SalePrice', axis=1)\n\nlen(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.to_csv('new_train_1h.csv', index=False)\ny.to_csv('new_test_1h.csv', index=False)\nprint(\"New files have been created.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-columnselect\"></a>\n# Column Selection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_log_error\n\n# X_train = pd.read_csv(\"new_train.csv\")\n# X_test = pd.read_csv(\"new_test.csv\")\nX = pd.read_csv(\"/kaggle/working/new_train_1h.csv\")\ntest_df = pd.read_csv(\"/kaggle/working/new_test_1h.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = X.sample(frac=0.6,random_state=200)\nevaluate_df = X.drop(train_df.index)\n\nX_train = train_df.drop(['Id','SalePrice'], axis=1)\nY_train = train_df[\"SalePrice\"]\nX_train_full = X.drop(['Id','SalePrice'], axis=1)\nY_train_full = X[\"SalePrice\"]\nX_eval  = evaluate_df.drop(['Id','SalePrice'], axis=1)\nY_eval  = evaluate_df[\"SalePrice\"]\nX_test = test_df.drop('Id', axis=1)\n\nlen(X_train_full.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\n\n# grad_boost = GradientBoostingRegressor(random_state=0, n_estimators=500, max_features= 0.3)\n# grad_boost_full = GradientBoostingRegressor(random_state=0, n_estimators=500, max_features= 0.3)\ngrad_boost = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=5, min_child_weight=1.5, n_estimators=1900, \n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\ngrad_boost_full = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=5, min_child_weight=1.5, n_estimators=1900, \n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\nnum_feat_to_drop = 1\ndropped_features = []\ni = 0\nbest_acc = 1\n\nwhile len(X_train.columns) > 5:\n    # Train our model with the set\n    grad_boost.fit(X_train, Y_train)\n    grad_boost_full.fit(X_train_full, Y_train_full)\n    \n    # Determine our accurancy with this model\n    acc_train = np.sqrt(mean_squared_log_error(Y_train, grad_boost.predict(X_train)))\n    acc_eval = np.sqrt(mean_squared_log_error(Y_eval, grad_boost.predict(X_eval)))\n    acc_full = np.sqrt(mean_squared_log_error(Y_train_full, grad_boost_full.predict(X_train_full)))\n    avg_acc = (acc_train + acc_eval + acc_full) / 3\n    if (avg_acc >= best_acc):\n        print (f'{i}: Train = {acc_train}  Evaluate = {acc_eval} Full = {acc_full} Average = {avg_acc}')\n    else:\n        best_acc = avg_acc\n        print (f'{i}: Train = {acc_train}  Evaluate = {acc_eval} Full = {acc_full} Average = {avg_acc} - NEW BEST!!!')\n        print (f'  Dropped: {dropped_features}')\n    \n    # Determine the least important features\n    importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(grad_boost.feature_importances_,3)})\n    importances = importances.sort_values('importance',ascending=True).set_index('feature')\n    \n    # Drop the X least important features\n    features_to_drop = importances[0:num_feat_to_drop].index.tolist()\n    if len(features_to_drop) <= 0:\n        break\n    X_train.drop(features_to_drop, axis=1, inplace=True)\n    X_eval.drop(features_to_drop, axis=1, inplace=True)\n    X_train_full.drop(features_to_drop, axis=1, inplace=True)\n    dropped_features.append(features_to_drop)\n    \n    i += num_feat_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this is the result of my run on my own machine so it will differ with Kaggle\nbest_dropped = [['LotConfig_4'], ['Foundation_3'], ['Neighborhood_3'], ['BldgType_2'], ['Neighborhood_21'], ['Neighborhood_12'], ['Neighborhood_8'], ['HouseStyle_8'], ['BldgType_3'], ['HouseStyle_2'], ['Neighborhood_10']]\nbest_dropped = [x for inner in best_dropped for x in inner]\nbest_dropped","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-training\"></a>\n# Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_log_error\n\nX = pd.read_csv(\"/kaggle/working/new_train_1h.csv\")\ntest_df = pd.read_csv(\"/kaggle/working/new_test_1h.csv\")\n\ntrain_df = X.sample(frac=0.6,random_state=200)\nevaluate_df = X.drop(train_df.index)\n\nX_train = train_df.drop(['Id','SalePrice'], axis=1)\nY_train = train_df[\"SalePrice\"]\nX_train_full = X.drop(['Id','SalePrice'], axis=1)\nY_train_full = X[\"SalePrice\"]\nX_eval  = evaluate_df.drop(['Id','SalePrice'], axis=1)\nY_eval  = evaluate_df[\"SalePrice\"]\nX_test = test_df.drop('Id', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check if any columns have null values\nfor (columnName, columnData) in X_test.iteritems():\n    print (f'{columnName}: {X_test[columnName].isna().sum()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', None)\nX_test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Final Column Drop**\n\nUse the drop list from above","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list = ['LotConfig_4', 'Foundation_3', 'Neighborhood_3', 'BldgType_2', 'Neighborhood_21', 'Neighborhood_12', 'Neighborhood_8', 'HouseStyle_8', 'BldgType_3', 'HouseStyle_2', 'Neighborhood_10']\n\nX_train.drop(drop_list, axis=1,inplace=True)\nX_eval.drop(drop_list, axis=1,inplace=True)\nX_train_full.drop(drop_list, axis=1,inplace=True)\nX_test.drop(drop_list, axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-LinearReg\"></a>\n## Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\n\nlr = linear_model.LinearRegression()\nlr.fit(X_train, Y_train)\n\nacc_train_lr = np.sqrt(mean_squared_log_error(Y_train, lr.predict(X_train)))\nacc_eval_lr = np.sqrt(mean_squared_log_error(Y_eval, lr.predict(X_eval)))\n\nprint (f'LR: Train = {acc_train_lr}  Evaluate = {acc_eval_lr}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-StoGradDes\"></a>\n## Stochastic Gradient Descent (SGD)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import linear_model\n\nsgd = linear_model.SGDClassifier(random_state=1, max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\n\nacc_train_sgd = np.sqrt(mean_squared_log_error(Y_train, sgd.predict(X_train)))\nacc_eval_sgd = np.sqrt(mean_squared_log_error(Y_eval, sgd.predict(X_eval)))\n\nprint (f'SGD: Train = {acc_train_sgd}  Evaluate = {acc_eval_sgd}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-RandFore\"></a>\n## Random Forest Classifier\nI don't know why I did this.  This isn't a classifier problem...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nacc_train_rf = np.sqrt(mean_squared_log_error(Y_train, random_forest.predict(X_train)))\nacc_eval_rf = np.sqrt(mean_squared_log_error(Y_eval, random_forest.predict(X_eval)))\n\nprint (f'RF: Train = {acc_train_rf}  Evaluate = {acc_eval_rf}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-PolyReg\"></a>\n## Polynomial Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_features = PolynomialFeatures(degree=1)\nX_train_poly = poly_features.fit_transform(X_train)\nX_eval_poly = poly_features.fit_transform(X_eval)\npoly_model = LinearRegression()\npoly_model.fit(X_train_poly, Y_train)\n\nacc_train_pr = np.sqrt(mean_squared_log_error(Y_train, poly_model.predict(X_train_poly)))\nacc_eval_pr = np.sqrt(mean_squared_log_error(Y_eval, abs(poly_model.predict(X_eval_poly))))\n\nprint (f'PR: Train = {acc_train_pr}  Evaluate = {acc_eval_pr}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-LogReg\"></a>\n## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n#logreg = LogisticRegression(random_state=1, max_iter=1000)\nlogreg = LogisticRegression(random_state=1)\nlogreg.fit(X_train, Y_train)\n\nacc_train_lr = np.sqrt(mean_squared_log_error(Y_train, logreg.predict(X_train)))\nacc_eval_lr = np.sqrt(mean_squared_log_error(Y_eval, logreg.predict(X_eval)))\n\nprint (f'LR: Train = {acc_train_lr}  Evaluate = {acc_eval_lr}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-GNB\"></a>\n## Gaussian Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\n\nacc_train_gnb = np.sqrt(mean_squared_log_error(Y_train, gaussian.predict(X_train)))\nacc_eval_gnb = np.sqrt(mean_squared_log_error(Y_eval, gaussian.predict(X_eval)))\n\nprint (f'GNB: Train = {acc_train_gnb}  Evaluate = {acc_eval_gnb}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-Perceptron\"></a>\n## Perceptron","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Perceptron\n\nperceptron = Perceptron(random_state=1, max_iter=50)\nperceptron.fit(X_train, Y_train)\n\nacc_train_prc = np.sqrt(mean_squared_log_error(Y_train, perceptron.predict(X_train)))\nacc_eval_prc = np.sqrt(mean_squared_log_error(Y_eval, perceptron.predict(X_eval)))\n\nprint (f'Perceptron: Train = {acc_train_prc}  Evaluate = {acc_eval_prc}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-SVM\"></a>\n## Linear Support Vector Machine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC, LinearSVC\n\n#linear_svc = LinearSVC(random_state=1, max_iter=100000)\nlinear_svc = LinearSVC(random_state=1)\nlinear_svc.fit(X_train, Y_train)\n\nacc_train_svm = np.sqrt(mean_squared_log_error(Y_train, linear_svc.predict(X_train)))\nacc_eval_svm = np.sqrt(mean_squared_log_error(Y_eval, linear_svc.predict(X_eval)))\n\nprint (f'SVM: Train = {acc_train_svm}  Evaluate = {acc_eval_svm}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-DesiTree\"></a>\n## Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(random_state=1)\ndecision_tree.fit(X_train, Y_train)\n\nacc_train_dt = np.sqrt(mean_squared_log_error(Y_train, decision_tree.predict(X_train)))\nacc_eval_dt = np.sqrt(mean_squared_log_error(Y_eval, decision_tree.predict(X_eval)))\n\nprint (f'DT: Train = {acc_train_dt}  Evaluate = {acc_eval_dt}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-RanForReg\"></a>\n## Random Forest Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrandom_forest_reg = RandomForestRegressor(random_state=1, n_estimators=100)\nrandom_forest_reg.fit(X_train, Y_train)\n\nacc_train_rfr = np.sqrt(mean_squared_log_error(Y_train, random_forest_reg.predict(X_train)))\nacc_eval_rfr = np.sqrt(mean_squared_log_error(Y_eval, random_forest_reg.predict(X_eval)))\n\nprint (f'RFR: Train = {acc_train_rfr}  Evaluate = {acc_eval_rfr}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-GradBooReg\"></a>\n## Gradient Boost Regressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Current Best\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngrad_boost = GradientBoostingRegressor(random_state=1, n_estimators=500, max_features= 0.3)\ngrad_boost.fit(X_train, Y_train)\n\nacc_train_gb = np.sqrt(mean_squared_log_error(Y_train, grad_boost.predict(X_train)))\nacc_eval_gb = np.sqrt(mean_squared_log_error(Y_eval, grad_boost.predict(X_eval)))\n\nprint (f'GB: Train = {acc_train_gb}  Evaluate = {acc_eval_gb}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-AdaBoost\"></a>\n## AdaBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostRegressor\n\nada_boost = AdaBoostRegressor(random_state=1, n_estimators=100)\nada_boost.fit(X_train, Y_train)\n\nacc_train_ab = np.sqrt(mean_squared_log_error(Y_train, ada_boost.predict(X_train)))\nacc_eval_ab = np.sqrt(mean_squared_log_error(Y_eval, ada_boost.predict(X_eval)))\n\nprint (f'AdaBoost: Train = {acc_train_ab}  Evaluate = {acc_eval_ab}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-ExtRanTree\"></a>\n## Extremely Randomized Trees","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesRegressor\n\nex_trees = ExtraTreesRegressor(random_state=1, n_estimators=100)\nex_trees.fit(X_train, Y_train)\n\nacc_train_et = np.sqrt(mean_squared_log_error(Y_train, ex_trees.predict(X_train)))\nacc_eval_et = np.sqrt(mean_squared_log_error(Y_eval, ex_trees.predict(X_eval)))\n\nprint (f'ET: Train = {acc_train_et}  Evaluate = {acc_eval_et}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-EnsVotReg\"></a>\n## Ensemble: VotingRegessor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import VotingRegressor\nimport xgboost as xgb\n\nreg1 = GradientBoostingRegressor(random_state=1, n_estimators=100)\nreg2 = RandomForestRegressor(random_state=1, n_estimators=100)\nreg3 = ExtraTreesRegressor(random_state=1, n_estimators=100)\nreg4 = AdaBoostRegressor(random_state=1, n_estimators=100)\nreg5 = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2400,\n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\nereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('et', reg3), ('ad', reg4), ('xg', reg5)])\nereg = ereg.fit(X_train, Y_train)\n\nacc_train_vr = np.sqrt(mean_squared_log_error(Y_train, ereg.predict(X_train)))\nacc_eval_vr = np.sqrt(mean_squared_log_error(Y_eval, ereg.predict(X_eval)))\n\nprint (f'VR: Train = {acc_train_vr}  Evaluate = {acc_eval_vr}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-EnsStakGen\"></a>\n## Ensemble: Stacked Generalization","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV, LassoCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import StackingRegressor\n\nest_cnt = 100\nseed = 42\n\nfinal_estimator = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2400,\n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\n\nfinal_layer = StackingRegressor(\n    estimators=[\n                ('abar', AdaBoostRegressor(random_state=seed, n_estimators=est_cnt))\n                #,('extr', ExtraTreesRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('adar', AdaBoostRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('gbrt', GradientBoostingRegressor(random_state=seed, n_estimators=est_cnt))\n                #,('rf', RandomForestRegressor(random_state=seed, n_estimators=est_cnt))\n               ],\n    final_estimator=final_estimator\n)\n\nsecond_layer = StackingRegressor(\n    estimators=[\n                ('abar', AdaBoostRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('extr', ExtraTreesRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('adar', AdaBoostRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('gbrt', GradientBoostingRegressor(random_state=seed, n_estimators=est_cnt))\n                ,('rf', RandomForestRegressor(random_state=seed, n_estimators=est_cnt))\n               ],\n    final_estimator=final_layer\n)\n\nmulti_layer_regressor = StackingRegressor(\n    estimators=[\n                #('ridge', RidgeCV())\n                ('lasso', LassoCV(random_state=seed))\n                ,('svr', SVR(C=1, gamma=1e-6, kernel='rbf'))\n                ,('etr', ExtraTreesRegressor(random_state=seed, n_estimators=est_cnt))\n               ],\n    final_estimator=second_layer\n)\n\nmulti_layer_regressor.fit(X_train, Y_train)\n\nacc_train_sg = np.sqrt(mean_squared_log_error(Y_train, multi_layer_regressor.predict(X_train)))\nacc_eval_sg = np.sqrt(mean_squared_log_error(Y_eval, multi_layer_regressor.predict(X_eval)))\n\nprint (f'SG: Train = {acc_train_sg}  Evaluate = {acc_eval_sg}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-XGBoost\"></a>\n## XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\n\ngbm = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.7, gamma=2,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=5, min_child_weight=0, n_estimators=1900, \n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.3, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1).fit(X_train, Y_train)\nacc_train_xg = np.sqrt(mean_squared_log_error(Y_train, gbm.predict(X_train)))\nacc_eval_xg = np.sqrt(mean_squared_log_error(Y_eval, gbm.predict(X_eval)))\n\nprint (f'XG: Train = {acc_train_xg}  Evaluate = {acc_eval_xg}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-Best\"></a>\n## Best","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nbest_reg = xgb.XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=5, min_child_weight=1.5, n_estimators=1900, \n             n_jobs=1, nthread=None, objective='reg:squarederror',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\nbest_reg.fit(X_train_full, Y_train_full)\n\nacc_train_full = np.sqrt(mean_squared_log_error(Y_train_full, best_reg.predict(X_train_full)))\n\nprint (f'Best (previous): Train = {acc_train_xg}  Evaluate = {acc_eval_xg}')\nprint (f'Best FULL Train = {acc_train_full}')\n\npredictions = best_reg.predict(X_test)\noutput = pd.DataFrame({'Id': test_df.Id, 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntest_predictions = best_reg.predict(X_train_full).flatten()\n\na = plt.axes(aspect='equal')\nplt.scatter(Y_train_full, test_predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predictions')\nlims = [0, 1000000]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}