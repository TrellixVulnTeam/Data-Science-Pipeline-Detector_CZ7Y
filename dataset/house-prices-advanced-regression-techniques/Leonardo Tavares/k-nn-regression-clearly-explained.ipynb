{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# k-NN regression clearly explained\n\n![](https://i.imgflip.com/6kxcbl.jpg)\n\n## Need statment:\n\n**Consider the following scenario:** A friend of yours would like to sell his house and invites you to help him assess what price his house should sell for. The problem here is that you don't know how to calculate that price (you've never bought or sold a house).** And then what do you do?**\n\n\n* a) You simply tell your friend that you cannot help him ðŸ˜•ðŸ™„ðŸ˜¢\n* b) You use your knowledge of mathematics, statistics and astrology to help him, thus recognizing **the value of sincere and true friendship**!!! ðŸ˜ŠðŸ˜ŠðŸ˜Š\n\nI knew you would choose option b)! That's exactly why we're here!!! \n\n\nNow comes the question: how can you help him? Your challenge now is to develop a technique that takes into account **all aspects of the house** so that the price is **as fair as possible**. To try to help you, I will introduce you to the k-nearest neighbors algorithm (**kNN** for short) algorithm, this way you can help your friend, okay? But first... **upvote my notebook, please!**\n\nPart of the work presented here is based on the following books:\n\n![](https://images-na.ssl-images-amazon.com/images/I/41RgG05lZaL._SY344_BO1,204,203,200_.jpg)\n[Link to amazon](https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1071614177/)\n\n![](https://images-na.ssl-images-amazon.com/images/I/41TmbdP0EZL._SY344_BO1,204,203,200_.jpg)\n[Link to amazon](https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/)\n\n![](https://thumbs.dreamstime.com/b/lets-go-handwritten-white-background-169989567.jpg)\n\n## x_trainx_trainPlease upvote me if you like, ok? (this is really really important to me)","metadata":{}},{"cell_type":"markdown","source":"# 1. What is the k-NN algorithm?\n\nIn a nutshell, the k-NN is *memory-based* algorithm and require no model to be fit. It can be used for both **classification** and **regression** problems (with small changes of course). At this point I will focus only on the regression problem. If you want I can make another notebook for classification problems, **just ask in the comments, ok?**\n\n\nDespite its simplicity, the k-NN algorithm is quite competitive. And what do we need to implement this method? Just three things: i) the **k** parameter, some **reference samples** and a **distance** measurement.\n\n\n## 1.1 k-NN regression algorithm\n\nThe algorithm consists of \"only\" 1 equation:\n\n<img src=\"https://latex.codecogs.com/svg.image?\\LARGE&space;\\hat{f}(x_0)&space;=\\frac{1}{k}&space;\\sum_{x_i&space;\\in&space;\\mathit{N}_0}y_i\" />\n[https://latex.codecogs.com/](https://latex.codecogs.com/)\n\nWhere:\n* $\\hat{f}()$ is the estimated function of the true (and unknown) function $f()$\n* $k$ is the the **k** parameter\n* $x_0$ is the query point\n* $\\mathit{N}_0$ are the points  nearest to $x_0$ (or the **reference samples**)\n* $y_i$ is the value of $f(x_i)$\n\nAnd now? What is missing? The **distance** measurement...\n\nAccording to wikipedia: \"*Distance is a numerical measurement of how far apart objects or points are.*\" And how can we calculate it? To simplify a bit I will use the **Euclidean distance**. Considering an $n$-dimensional space, the Euclidean distance is given by:\n\n<img src=\"https://latex.codecogs.com/svg.image?\\LARGE&space;d(p,&space;q)&space;=&space;\\sqrt{\\sum_{i=1}^{n}(p_i&space;-&space;q_i)^2}\" />\n\nWhere:\n* $n$ is the dimension of the function\n* $p$ and $q$ are points from which we want to know the distance between them\n\nOther distance measures can be used. You can learn more about it here: [4 Distance Measures for Machine Learning](https://machinelearningmastery.com/distance-measures-for-machine-learning/)\n\n\nOkay, now we have everything we need to implement our own k-NN algorithm. **Here we go**?","metadata":{}},{"cell_type":"code","source":"# general imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n# sklearn\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import BaseEstimator\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\n\n# yellow bricks\nfrom yellowbrick.regressor import residuals_plot\n\n# constant for reproducibility\nnp.random.seed = 42        ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-27T15:03:56.256867Z","iopub.execute_input":"2022-06-27T15:03:56.257278Z","iopub.status.idle":"2022-06-27T15:03:56.268014Z","shell.execute_reply.started":"2022-06-27T15:03:56.25725Z","shell.execute_reply":"2022-06-27T15:03:56.266844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 Implementing our own k-NN\n\nSuppose you want to regression a function:\n\n<img src=\"https://latex.codecogs.com/svg.image?\\LARGE&space;f(x)=x^3-2x^2-11x&plus;12\" />\n\n\nAs it is a 1-dimensional function it is possible to present it in a simple graph. For this we will generate **100** random points (uniformly distributed) in the interval [-5, 5]. Of these **100** points I will display only **80** at this time, the other **20** I will use to check the performance of the k-NN algorithm.","metadata":{}},{"cell_type":"code","source":"def f(x):\n    \"\"\"\n    Didatical function\n    \"\"\"\n    return x**3 - 2*(x**2) - 11*x + 12\n\n\n# Generation of random points \nlower_bound = -5\nupper_bound = 5\nn_points = 100\nx = np.random.uniform(lower_bound, upper_bound, n_points)\n\n#Points that will be displayed\nx_train = x[0:80]\ny_train = f(x_train)\n\n# Points that will be used to measure the performance of the algorithm\nx_test = x[80:100]\ny_test = f(x_test)\n\n# Function graph\nplt.figure(figsize=(8,8))\nsns.scatterplot(x=x_train, y=y_train)\nplt.legend(['Train points'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.29656Z","iopub.execute_input":"2022-06-27T15:03:56.296973Z","iopub.status.idle":"2022-06-27T15:03:56.521208Z","shell.execute_reply.started":"2022-06-27T15:03:56.296937Z","shell.execute_reply":"2022-06-27T15:03:56.520375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 How does knn work?\n\n\nSuppose I wanted to estimate the value of $f(-2)$ using the k-NN algorithm. The first step is to choose the parameter *k* (and trust me, this is not an easy task...). In our example we are going to experiment with k=2, 3, 5 and 7 (This means that we will use the average of the 2 nearest  points, the 3 nearest  points and so on...). First with k=2.\n\n\n","metadata":{}},{"cell_type":"code","source":"def knn(x_train, y_train, x_0, k=2):\n    \"\"\"\n    Simulate the k-NN algorithm \n    \n    :params:\n    x: x train samples\n    \n    y: y train samples\n    \n    x_0: query points\n    \n    k: Number of nearest  neighbors taken into account\n    \"\"\"\n    \n    # calculates the euclidian distance between the query point and the training set\n    distances = [np.linalg.norm(x - x_0) for x in x_train] \n    \n    # for each point in the set x_train, create a record containing the distance (to point x_0)\n    # and the corresponding value in y_train\n    result = []\n    for d, y in zip(distances, y_train):\n        result.append((d, y))\n    \n    # sort the list by distance\n    result.sort(key=lambda tup: tup[0]) \n    \n    # transform to a numpy object to facilitate operations\n    result = np.array(result)\n    \n    # get the first k results (only the column that contains \n    # the second column, which contains the values of y_train)\n    k_results = result[:k, 1]\n    \n    # calculate the mean of the k_results\n    return np.mean(k_results)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.523489Z","iopub.execute_input":"2022-06-27T15:03:56.524187Z","iopub.status.idle":"2022-06-27T15:03:56.533853Z","shell.execute_reply.started":"2022-06-27T15:03:56.524138Z","shell.execute_reply":"2022-06-27T15:03:56.532868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"$\\hat{f}(-2)$ for $k=2$","metadata":{}},{"cell_type":"code","source":"y_hat = knn(x_train, y_train, -2, k=2)\ny_hat","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.535496Z","iopub.execute_input":"2022-06-27T15:03:56.535975Z","iopub.status.idle":"2022-06-27T15:03:56.551466Z","shell.execute_reply.started":"2022-06-27T15:03:56.535928Z","shell.execute_reply":"2022-06-27T15:03:56.550578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Real value for $f(-2)$","metadata":{}},{"cell_type":"code","source":"y_real = f(-2)\ny_real","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.553732Z","iopub.execute_input":"2022-06-27T15:03:56.554306Z","iopub.status.idle":"2022-06-27T15:03:56.562911Z","shell.execute_reply.started":"2022-06-27T15:03:56.554269Z","shell.execute_reply":"2022-06-27T15:03:56.561933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the difference between them","metadata":{}},{"cell_type":"code","source":"y_real - y_hat","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.563856Z","iopub.execute_input":"2022-06-27T15:03:56.564685Z","iopub.status.idle":"2022-06-27T15:03:56.575488Z","shell.execute_reply.started":"2022-06-27T15:03:56.564645Z","shell.execute_reply":"2022-06-27T15:03:56.574656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the difference is very small between the estimated value and the actual value of the function. Now let's test with $k=3$","metadata":{}},{"cell_type":"code","source":"y_hat = knn(x_train, y_train, -2, k=3)\ny_hat","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.576645Z","iopub.execute_input":"2022-06-27T15:03:56.577545Z","iopub.status.idle":"2022-06-27T15:03:56.588646Z","shell.execute_reply.started":"2022-06-27T15:03:56.577514Z","shell.execute_reply":"2022-06-27T15:03:56.587637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_real - y_hat","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.590027Z","iopub.execute_input":"2022-06-27T15:03:56.591127Z","iopub.status.idle":"2022-06-27T15:03:56.600727Z","shell.execute_reply.started":"2022-06-27T15:03:56.591079Z","shell.execute_reply":"2022-06-27T15:03:56.599941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that the difference changes a little bit. Now we will be with $k=5$","metadata":{}},{"cell_type":"code","source":"y_hat = knn(x_train, y_train, -2, k=5)\ny_hat","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.603568Z","iopub.execute_input":"2022-06-27T15:03:56.603877Z","iopub.status.idle":"2022-06-27T15:03:56.61331Z","shell.execute_reply.started":"2022-06-27T15:03:56.60385Z","shell.execute_reply":"2022-06-27T15:03:56.612459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_real - y_hat","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.615763Z","iopub.execute_input":"2022-06-27T15:03:56.616494Z","iopub.status.idle":"2022-06-27T15:03:56.624044Z","shell.execute_reply.started":"2022-06-27T15:03:56.616459Z","shell.execute_reply":"2022-06-27T15:03:56.623268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that the algorithm** is very sensitive** to the parameter k. \n\nGenerally, low values of *k* makes the algorithm more flexible, but leave it with a greater variance, it means,**low bias but high variance**.\n\nAs *k* gorws, the algorithm becomes less flexible, it means, **high bias but low variance**.\n\nTo learn more about bias and variance, visit [Biasâ€“variance tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)\n\nTo demonstrate this statement, let's do the following experiment: let's use $k=40$ and calculate $\\hat{f}(-3)$, $\\hat{f}(0)$, $\\hat{f}(2)$","metadata":{}},{"cell_type":"code","source":"knn(x_train, y_train, -3, k=80)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.625078Z","iopub.execute_input":"2022-06-27T15:03:56.625957Z","iopub.status.idle":"2022-06-27T15:03:56.637305Z","shell.execute_reply.started":"2022-06-27T15:03:56.625926Z","shell.execute_reply":"2022-06-27T15:03:56.636257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn(x_train, y_train, 0, k=80)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.642155Z","iopub.execute_input":"2022-06-27T15:03:56.642673Z","iopub.status.idle":"2022-06-27T15:03:56.652116Z","shell.execute_reply.started":"2022-06-27T15:03:56.642638Z","shell.execute_reply":"2022-06-27T15:03:56.6511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn(x_train, y_train, 2, k=80)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.672204Z","iopub.execute_input":"2022-06-27T15:03:56.673076Z","iopub.status.idle":"2022-06-27T15:03:56.681099Z","shell.execute_reply.started":"2022-06-27T15:03:56.673043Z","shell.execute_reply":"2022-06-27T15:03:56.680137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, these values are too very close to the mean of *y_train*","metadata":{}},{"cell_type":"code","source":"np.mean(y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.704929Z","iopub.execute_input":"2022-06-27T15:03:56.705694Z","iopub.status.idle":"2022-06-27T15:03:56.712607Z","shell.execute_reply.started":"2022-06-27T15:03:56.705656Z","shell.execute_reply":"2022-06-27T15:03:56.71147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Applying the algorithm to all test set points\n\nNow let's see how the algorithm behaves for **all points in the test set**. After the calculation I will show the graph with the test data and estimated values. ","metadata":{}},{"cell_type":"code","source":"# Calculates the estimated value of y for all points in the test set\n# usa k=3\n\ny_hat = [knn(x_train, y_train, x, k=3) for x in x_test]\n\n# Function graph\nplt.figure(figsize=(8,8))\nsns.scatterplot(x=x_test,y=y_test)\nsns.scatterplot(x=x_test, y=y_hat)\nplt.legend(['Test points', 'Estimated point'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.736733Z","iopub.execute_input":"2022-06-27T15:03:56.737667Z","iopub.status.idle":"2022-06-27T15:03:56.983918Z","shell.execute_reply.started":"2022-06-27T15:03:56.737631Z","shell.execute_reply":"2022-06-27T15:03:56.982742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But is our estimator any good? let's use 2 metrics implemented in sklearn: Mean squared error regression loss and $R^2$ (coefficient of determination) regression score function.\n\n\n### RMSE\n\nTo learn more about the RMSE metric: [Mean squared error regression loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)","metadata":{}},{"cell_type":"code","source":"mean_squared_error(y_test, y_hat, squared=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.986575Z","iopub.execute_input":"2022-06-27T15:03:56.987048Z","iopub.status.idle":"2022-06-27T15:03:56.994589Z","shell.execute_reply.started":"2022-06-27T15:03:56.986993Z","shell.execute_reply":"2022-06-27T15:03:56.993437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### $R^2$\n\n\nTo learn more about the $R^2$ metric: [coefficient of determination](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)","metadata":{}},{"cell_type":"code","source":"r2_score(y_test, y_hat)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:56.995811Z","iopub.execute_input":"2022-06-27T15:03:56.996132Z","iopub.status.idle":"2022-06-27T15:03:57.006407Z","shell.execute_reply.started":"2022-06-27T15:03:56.996103Z","shell.execute_reply":"2022-06-27T15:03:57.005604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Oh, it looks really good!!!!\n\n\n### Now let's implement in sklearn style","metadata":{}},{"cell_type":"code","source":"class MyOwnKnnRegression(BaseEstimator):\n    # Class responsible for simulating a Knn Regression class.\n\n    def __init__(self, k_neighbors):\n        \"\"\"\n        Here we will define the pipeline for each tree.\n        \n        :params:\n        n_estimators: The number of nearest points\n        \"\"\"\n               \n        self.k = k_neighbors\n        self.X_train = None\n        self.y_train = None\n    \n    def fit(self, X, y):\n        # do nothing....\n        # Only stores training points\n        self.X_train = X\n        self.y_train = y\n\n    def predict(self, X):\n        # predicts all\n        return [self.knn(x) for x in X]\n    \n    \n    def knn(self, x_0):\n        \"\"\"\n        Simulate the k-NN algorithm \n\n        :params:\n        x: x train samples\n\n        y: y train samples\n\n        x_0: query points\n\n        k: Number of nearest  neighbors taken into account\n        \"\"\"\n\n        # calculates the euclidian distance between the query point and the training set\n        distances = [np.linalg.norm(x - x_0) for x in self.X_train] \n\n        # for each point in the set x_train, create a record containing the distance (to point x_0)\n        # and the corresponding value in y_train\n        result = []\n        for d, y in zip(distances, self.y_train):\n            result.append((d, y))\n        \n        # sort the list by distance\n        result.sort(key=lambda tup: tup[0]) \n\n        # transform to a numpy object to facilitate operations\n        result = np.array(result)\n\n        # get the first k results (only the column that contains \n        # the second column, which contains the values of y_train)\n        k_results = result[:self.k, 1]\n\n        # calculate the mean of the k_results\n        return np.mean(k_results)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:57.00862Z","iopub.execute_input":"2022-06-27T15:03:57.009557Z","iopub.status.idle":"2022-06-27T15:03:57.020995Z","shell.execute_reply.started":"2022-06-27T15:03:57.009521Z","shell.execute_reply":"2022-06-27T15:03:57.019993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Does it work? Let's check it out...","metadata":{}},{"cell_type":"code","source":"knn = MyOwnKnnRegression(k_neighbors=3)\nknn.fit(x_train, y_train)\ny_hat = knn.predict(x_test)\n\n# Function graph\nplt.figure(figsize=(8,8))\nsns.scatterplot(x=x_test,y=y_test)\nsns.scatterplot(x=x_test, y=y_hat)\nplt.legend(['Test points', 'Estimated point'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:57.021954Z","iopub.execute_input":"2022-06-27T15:03:57.022337Z","iopub.status.idle":"2022-06-27T15:03:57.278479Z","shell.execute_reply.started":"2022-06-27T15:03:57.022304Z","shell.execute_reply":"2022-06-27T15:03:57.277478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RMSE\n\nTo learn more about the RMSE metric: [Mean squared error regression loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)","metadata":{}},{"cell_type":"code","source":"mean_squared_error(y_test, y_hat)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:57.279774Z","iopub.execute_input":"2022-06-27T15:03:57.280193Z","iopub.status.idle":"2022-06-27T15:03:57.287679Z","shell.execute_reply.started":"2022-06-27T15:03:57.280161Z","shell.execute_reply":"2022-06-27T15:03:57.286596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### $R^2$\n\n\nTo learn more about the $R^2$ metric: [coefficient of determination](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)","metadata":{}},{"cell_type":"code","source":"r2_score(y_test, y_hat)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:57.290987Z","iopub.execute_input":"2022-06-27T15:03:57.291871Z","iopub.status.idle":"2022-06-27T15:03:57.303192Z","shell.execute_reply.started":"2022-06-27T15:03:57.29183Z","shell.execute_reply":"2022-06-27T15:03:57.302323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Looks like it worked... now let's apply it to a slightly more difficult problem... ","metadata":{}},{"cell_type":"markdown","source":"# 3. House Prices - Advanced Regression Techniques\n\n![](https://azbigmedia.com/wp-content/uploads/2020/08/selling-home.jpg)\n\n\n\nLet's go back to the original problem of helping your friend sell his house at a fair price. \n\nNow that you know the k-NN algorithm you can apply it to this problem!\n\nFirst, let's see how the data is...","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:57.304671Z","iopub.execute_input":"2022-06-27T15:03:57.305286Z","iopub.status.idle":"2022-06-27T15:03:57.355534Z","shell.execute_reply.started":"2022-06-27T15:03:57.305243Z","shell.execute_reply":"2022-06-27T15:03:57.354253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's make a little exploratory data analysis + data engineering","metadata":{}},{"cell_type":"code","source":"count = 1\nfor c in df.columns:\n    print(f'{count} - {c}')\n    print(f'- # of unique elements: {df[c].nunique()}')\n    print(f'- Sample: {df[c].unique()[0:20]}')\n    print(f'- Dtype: {df[c].dtype}')\n    print(f'- # of missing values: {df[c].isnull().sum()} of {df.shape[0]}')\n    print(f'- % of missing values: {np.round(df[c].isnull().sum() / df.shape[0], 3)}')\n    \n    \n    if df[c].dtype == int or df[c].dtype == float:\n        s = \"- Statistics:\\n\"\n\n        me = np.round(df[c].mean(), 2)\n        st = np.round(df[c].std(), 2)\n        s += f\"-- Mean (std): {me} ({st})\\n\"\n\n        q1 = np.round(df[c].quantile(0.25), 2)\n        q2 = np.round(df[c].quantile(0.5), 2)\n        q3 = np.round(df[c].quantile(0.75), 2)\n        s += f\"-- Quantiles: q1={q1}, q2={q2}, q3={q3}\\n\"\n        s += f\"-- Min {df[c].min()}\\n\"\n        s += f\"-- Max {df[c].max()}\"    \n        print(s)\n        \n    print('='*30)\n    count += 1","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:57.357421Z","iopub.execute_input":"2022-06-27T15:03:57.357936Z","iopub.status.idle":"2022-06-27T15:03:57.571351Z","shell.execute_reply.started":"2022-06-27T15:03:57.357873Z","shell.execute_reply":"2022-06-27T15:03:57.570544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What we have?\n\n* Numerical variables:\n    * LotFrontage\n    * LotArea\n    * OverallQual\n    * OverallCond\n    * YearBuilt\n    * YearRemodAdd\n    * MasVnrArea\n    * BsmtFinSF2\n    * BsmtUnfSF\n    * TotalBsmtSF\n    * 1stFlrSF\n    * 2ndFlrSF\n    * LowQualFinSF\n    * GrLivArea\n    * BsmtFullBath\n    * BsmtHalfBath\n    * FullBath\n    * HalfBath\n    * BedroomAbvGr\n    * KitchenAbvGr\n    * TotRmsAbvGrd\n    * Fireplaces\n    * GarageYrBlt\n    * GarageCars\n    * GarageArea\n    * WoodDeckSF\n    * OpenPorchSF\n    * EnclosedPorch\n    * 3SsnPorch\n    * ScreenPorch\n    * PoolArea\n    * MiscVal\n    * MoSold\n    * YrSold \n* Categorical variables:\n    * MSSubClass\n    * MSZoning\n    * LotShape\n    * Alley\n    * LandContour\n    * LotConfig\n    * LandSlope\n    * Neighborhood\n    * Condition1\n    * Condition2\n    * BldgType\n    * HouseStyle \n    * RoofStyle\n    * RoofMatl\n    * Exterior1st\n    * Exterior2nd\n    * MasVnrType\n    * ExterQual\n    * ExterCond\n    * Foundation\n    * BsmtQual\n    * BsmtCond\n    * BsmtExposure\n    * BsmtFinType1\n    * BsmtFinType2\n    * Heating\n    * HeatingQC\n    * Electrical\n    * KitchenQual\n    * Functional\n    * FireplaceQu\n    * GarageType\n    * GarageFinish\n    * GarageQual\n    * GarageCond\n    * PavedDrive\n    * PoolQC\n    * Fence\n    * MiscFeature\n    * SaleType\n    * SaleCondition\n* Binary variables:\n    * Street\n    * Utilities\n    * CentralAir\n* Columns to drop:\n    * Id (Not interesting for the model)\n* Outcome:\n    * SalePrice","metadata":{}},{"cell_type":"markdown","source":"Let's create a (very very very simple) pipeline for the predictor variables, which performs the following tasks:\n\n* Numeric features:\n    * Imputer: KNNImputer (k=5)\n    * Scaler: StandardScaler\n* Categorial features:\n    * Imputer: Most frequent\n    * Encoder: One Hot Encoder\n* Binary features:\n    * Imputer: Most frequent\n    * Encoder: Ordinal Encoder\nThis pipeline will be used later, at the time of the experiments, ok?","metadata":{}},{"cell_type":"code","source":"# numerical features\nnumeric_features = [\"LotFrontage\", \"LotArea\", \"OverallQual\", \"OverallCond\", \"YearBuilt\", \"YearRemodAdd\",\n                    \"MasVnrArea\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\",\n                    \"LowQualFinSF\", \"GrLivArea\", \"BsmtFullBath\", \"BsmtHalfBath\",\"FullBath\",\"HalfBath\",\"BedroomAbvGr\",\n                    \"KitchenAbvGr\", \"TotRmsAbvGrd\", \"Fireplaces\", \"GarageYrBlt\",\"GarageCars\", \"GarageArea\",\"WoodDeckSF\",\n                    \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\",\"PoolArea\",\"MiscVal\",\"MoSold\",\"YrSold\"]\nnumeric_transformer = Pipeline(\n    steps=[(\"imputer\", KNNImputer(n_neighbors=5)), \n           (\"scaler\", StandardScaler())]\n)\n\nPipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), \n           (\"scaler\", StandardScaler())])\n\n# categorial features\ncategorical_features = [\"MSSubClass\", \"MSZoning\", \"LotShape\", \"Alley\", \"LandContour\", \"LotConfig\", \"LandSlope\",\n                        \"Neighborhood\",\"Condition1\", \"Condition2\",\"BldgType\",\"HouseStyle\", \"RoofStyle\",\"RoofMatl\",\n                        \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"ExterQual\", \"ExterCond\",\"Foundation\",\n                        \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\",\"BsmtFinType2\",\"Heating\",\n                        \"HeatingQC\", \"Electrical\", \"KitchenQual\", \"Functional\", \"FireplaceQu\",\"GarageType\",\n                        \"GarageFinish\", \"GarageQual\", \"GarageCond\",\"PavedDrive\",\"PoolQC\",\"Fence\",\"MiscFeature\",\"SaleType\",\n                        \"SaleCondition\"\n]\ncategorical_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n           (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n    \n# binary features\nbinary_features = [\"Street\", \"Utilities\", \"CentralAir\"]\nbinary_transformer = Pipeline(\n    steps=[(\"imputer\", SimpleImputer(strategy=\"most_frequent\")), \n           (\"ohe\", OrdinalEncoder())])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_transformer, numeric_features),\n        (\"cat\", categorical_transformer, categorical_features),\n        (\"bin\", binary_transformer, binary_features),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:57.572637Z","iopub.execute_input":"2022-06-27T15:03:57.573166Z","iopub.status.idle":"2022-06-27T15:03:57.586574Z","shell.execute_reply.started":"2022-06-27T15:03:57.573131Z","shell.execute_reply":"2022-06-27T15:03:57.585754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's separate the predictor variables ( X ) and the outcome ( y )...","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns=['SalePrice'])\ny = df['SalePrice']\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:57.587855Z","iopub.execute_input":"2022-06-27T15:03:57.58839Z","iopub.status.idle":"2022-06-27T15:03:57.602752Z","shell.execute_reply.started":"2022-06-27T15:03:57.588342Z","shell.execute_reply":"2022-06-27T15:03:57.601844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's separate training and test sets, being 70% and 30% respectively. These sets will be used by the following experiments...","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(f'X_train shape {X_train.shape}')\nprint(f'y_train shape {y_train.shape}')\nprint('-'*20)\nprint(f'X_test shape {X_test.shape}')\nprint(f'y_test shape {y_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:57.604431Z","iopub.execute_input":"2022-06-27T15:03:57.60516Z","iopub.status.idle":"2022-06-27T15:03:57.617151Z","shell.execute_reply.started":"2022-06-27T15:03:57.605116Z","shell.execute_reply":"2022-06-27T15:03:57.615932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Now let's apply the method we created from knn and see the result...","metadata":{}},{"cell_type":"code","source":"X_train_transformed = preprocessor.fit_transform(X_train).toarray()\nX_test_transformed = preprocessor.transform(X_test).toarray()\n\n\nknn = MyOwnKnnRegression(k_neighbors=3)\nknn.fit(X_train_transformed, y_train)\ny_hat = knn.predict(X_test_transformed)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:03:57.618245Z","iopub.execute_input":"2022-06-27T15:03:57.618952Z","iopub.status.idle":"2022-06-27T15:04:03.253895Z","shell.execute_reply.started":"2022-06-27T15:03:57.618916Z","shell.execute_reply":"2022-06-27T15:04:03.252588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RMSE\n\nTo learn more about the RMSE metric: [Mean squared error regression loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)","metadata":{}},{"cell_type":"code","source":"mean_squared_error(y_test, y_hat, squared=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:04:03.257689Z","iopub.execute_input":"2022-06-27T15:04:03.258031Z","iopub.status.idle":"2022-06-27T15:04:03.265738Z","shell.execute_reply.started":"2022-06-27T15:04:03.257997Z","shell.execute_reply":"2022-06-27T15:04:03.264728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### $R^2$\n\n\nTo learn more about the $R^2$ metric: [coefficient of determination](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)","metadata":{}},{"cell_type":"code","source":"r2_score(y_test, y_hat)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:04:03.26707Z","iopub.execute_input":"2022-06-27T15:04:03.267387Z","iopub.status.idle":"2022-06-27T15:04:03.278608Z","shell.execute_reply.started":"2022-06-27T15:04:03.26735Z","shell.execute_reply":"2022-06-27T15:04:03.277308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Residuals plot\n\nTo learn more about the Residuals plot: [Residuals Plot](https://www.scikit-yb.org/en/latest/api/regressor/residuals.html)","metadata":{}},{"cell_type":"code","source":"# Plotting the residuals of y and pred_y\nsns.residplot(y_test,y_hat)\nplt.title('Model Residuals')\nplt.xlabel('Obsevation #')\nplt.ylabel('Error')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:04:03.280295Z","iopub.execute_input":"2022-06-27T15:04:03.280598Z","iopub.status.idle":"2022-06-27T15:04:03.489257Z","shell.execute_reply.started":"2022-06-27T15:04:03.280571Z","shell.execute_reply":"2022-06-27T15:04:03.488228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histogram of prediction errors\n","metadata":{}},{"cell_type":"code","source":"diff = y_test - y_hat\ndiff.hist(bins = 40)\nplt.title('Histogram of prediction errors')\nplt.xlabel('House price prediction error')\nplt.ylabel('Frequency')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:04:03.490994Z","iopub.execute_input":"2022-06-27T15:04:03.491725Z","iopub.status.idle":"2022-06-27T15:04:03.748137Z","shell.execute_reply.started":"2022-06-27T15:04:03.491681Z","shell.execute_reply":"2022-06-27T15:04:03.747087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction Error Plot\n\nTo learn more about the Prediction Error Plot metric: [Prediction Error Plot](https://www.scikit-yb.org/en/latest/api/regressor/peplot.html)\n","metadata":{}},{"cell_type":"code","source":"plt.scatter(y_test,y_hat)\nplt.xlabel('Actual values')\nplt.ylabel('Predicted values')\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_hat, 1))(np.unique(y_test)))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:04:03.749594Z","iopub.execute_input":"2022-06-27T15:04:03.749967Z","iopub.status.idle":"2022-06-27T15:04:03.94484Z","shell.execute_reply.started":"2022-06-27T15:04:03.749932Z","shell.execute_reply":"2022-06-27T15:04:03.943764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 Selecting the best parameter k\n\nNow let's choose, in a somewhat rudimentary way, the best parameter *k*. For this we will iterate on the parameters $k=3, 5, 7, ...97, 99$. The one with the lowest value for the **RMSE metric** (in the test set) will be chosen.\n\n\n*Later I will write something about gridsearch and cross-validation*","metadata":{}},{"cell_type":"code","source":"results = []\nresults_train = []\npossibles_k = np.arange(3,99, 2)\n\nfor k in tqdm(possibles_k):\n    knn = MyOwnKnnRegression(k_neighbors=k)\n    knn.fit(X_train_transformed, y_train)\n    y_hat = knn.predict(X_test_transformed)\n    \n    results.append(mean_squared_error(y_test, y_hat, squared=False))\n    \n    y_hat = knn.predict(X_train_transformed)\n    results_train.append(mean_squared_error(y_train, y_hat, squared=False))\n    \n    \nidx = np.argmin(results)   \nprint(f\"Best k: {possibles_k[idx]}\")\nprint(f\"Best RMSE: {results[idx]}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:04:03.946219Z","iopub.execute_input":"2022-06-27T15:04:03.947061Z","iopub.status.idle":"2022-06-27T15:18:28.242409Z","shell.execute_reply.started":"2022-06-27T15:04:03.947029Z","shell.execute_reply":"2022-06-27T15:18:28.241214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_k = possibles_k[idx]\n\nplt.figure(figsize=(8,8))\nsns.lineplot(x=possibles_k, y=results)\nsns.lineplot(x=possibles_k, y=results_train)\nplt.title('RMSE variation in relation to parameter k (lower is better)')\nplt.xlabel('K')\nplt.ylabel('RMSE')\nplt.axvline(x = best_k, color = 'black', label = 'axvline - full height')\nplt.legend(['Test points', 'Train point', 'best-k'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:18:28.243868Z","iopub.execute_input":"2022-06-27T15:18:28.244831Z","iopub.status.idle":"2022-06-27T15:18:28.495325Z","shell.execute_reply.started":"2022-06-27T15:18:28.244794Z","shell.execute_reply":"2022-06-27T15:18:28.494273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the result...","metadata":{}},{"cell_type":"code","source":"\n\nknn = MyOwnKnnRegression(k_neighbors=best_k)\nknn.fit(X_train_transformed, y_train)\ny_hat = knn.predict(X_test_transformed)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:18:28.496518Z","iopub.execute_input":"2022-06-27T15:18:28.496824Z","iopub.status.idle":"2022-06-27T15:18:33.999194Z","shell.execute_reply.started":"2022-06-27T15:18:28.496796Z","shell.execute_reply":"2022-06-27T15:18:33.997848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### RMSE\n\nTo learn more about the RMSE metric: [Mean squared error regression loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)","metadata":{}},{"cell_type":"code","source":"mean_squared_error(y_test, y_hat, squared=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:18:34.000935Z","iopub.execute_input":"2022-06-27T15:18:34.001571Z","iopub.status.idle":"2022-06-27T15:18:34.010711Z","shell.execute_reply.started":"2022-06-27T15:18:34.001524Z","shell.execute_reply":"2022-06-27T15:18:34.009445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### $R^2$\n\n\nTo learn more about the $R^2$ metric: [coefficient of determination](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)","metadata":{}},{"cell_type":"code","source":"r2_score(y_test, y_hat)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:18:34.012129Z","iopub.execute_input":"2022-06-27T15:18:34.012427Z","iopub.status.idle":"2022-06-27T15:18:34.027852Z","shell.execute_reply.started":"2022-06-27T15:18:34.0124Z","shell.execute_reply":"2022-06-27T15:18:34.026621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Residuals plot\n\nTo learn more about the Residuals plot: [Residuals Plot](https://www.scikit-yb.org/en/latest/api/regressor/residuals.html)","metadata":{}},{"cell_type":"code","source":"# Plotting the residuals of y and pred_y\nsns.residplot(y_test,y_hat)\nplt.title('Model Residuals')\nplt.xlabel('Obsevation #')\nplt.ylabel('Error')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:18:34.030489Z","iopub.execute_input":"2022-06-27T15:18:34.030827Z","iopub.status.idle":"2022-06-27T15:18:34.221433Z","shell.execute_reply.started":"2022-06-27T15:18:34.030801Z","shell.execute_reply":"2022-06-27T15:18:34.220216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Histogram of prediction errors","metadata":{}},{"cell_type":"code","source":"diff = y_test - y_hat\ndiff.hist(bins = 40)\nplt.title('Histogram of prediction errors')\nplt.xlabel('House price prediction error')\nplt.ylabel('Frequency')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:18:34.222697Z","iopub.execute_input":"2022-06-27T15:18:34.223249Z","iopub.status.idle":"2022-06-27T15:18:34.449821Z","shell.execute_reply.started":"2022-06-27T15:18:34.223213Z","shell.execute_reply":"2022-06-27T15:18:34.448971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction Error Plot\n\nTo learn more about the Prediction Error Plot metric: [Prediction Error Plot](https://www.scikit-yb.org/en/latest/api/regressor/peplot.html)\n","metadata":{}},{"cell_type":"code","source":"plt.scatter(y_test,y_hat)\nplt.xlabel('Actual values')\nplt.ylabel('Predicted values')\nplt.plot(np.unique(y_test), np.poly1d(np.polyfit(y_test, y_hat, 1))(np.unique(y_test)))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:18:34.450877Z","iopub.execute_input":"2022-06-27T15:18:34.451483Z","iopub.status.idle":"2022-06-27T15:18:34.641508Z","shell.execute_reply.started":"2022-06-27T15:18:34.451452Z","shell.execute_reply":"2022-06-27T15:18:34.64023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"# 4. (Some) Conclusions:\n\n1. I tried to show here the main concepts related to the k-NN algorithm\n1. Obviously the algorithm I implemented is a rudimentary version and needs optimization\n1. If you have any suggestions (or criticisms), leave them in the comments...\n1. We can see that the algorithm is very simple and, at the same time, very competitive!","metadata":{}},{"cell_type":"markdown","source":"# 5. Generating the prediction for the test data\n\n","metadata":{}},{"cell_type":"code","source":"# Load test data\ndf_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ndf_test","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:48:09.763134Z","iopub.execute_input":"2022-06-27T15:48:09.763779Z","iopub.status.idle":"2022-06-27T15:48:09.824928Z","shell.execute_reply.started":"2022-06-27T15:48:09.763744Z","shell.execute_reply":"2022-06-27T15:48:09.823912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ids = df_test['Id']\n\nX_test_transformed = preprocessor.transform(df_test).toarray()\ny_hat = knn.predict(X_test_transformed)\n\nresult = {\n    'Id':[],\n    'SalePrice':[]\n}\n\n\nfor _id, price in zip(_ids, y_hat):\n    result['Id'].append(_id)\n    result['SalePrice'].append(price)\n\nresult_df = pd.DataFrame(result)\nresult_df","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:51:31.746104Z","iopub.execute_input":"2022-06-27T15:51:31.746506Z","iopub.status.idle":"2022-06-27T15:51:50.097205Z","shell.execute_reply.started":"2022-06-27T15:51:31.746471Z","shell.execute_reply":"2022-06-27T15:51:50.095924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save submission\nresult_df.to_csv('submission.csv', index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T15:54:55.867606Z","iopub.execute_input":"2022-06-27T15:54:55.868002Z","iopub.status.idle":"2022-06-27T15:54:55.878731Z","shell.execute_reply.started":"2022-06-27T15:54:55.867972Z","shell.execute_reply":"2022-06-27T15:54:55.877574Z"},"trusted":true},"execution_count":null,"outputs":[]}]}