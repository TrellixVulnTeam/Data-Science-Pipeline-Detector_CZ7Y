{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Contents\n\n[01. Import Library](#01)<br>\n\n[02. Load Data](#02)<br>\n\n[03. Exploratory Data Analysis (EDA)](#03)<br>\n\n&nbsp;&nbsp;&nbsp;[3.1. Dependent Variable](#3.1)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1. Use matplotlib](#3.1.1)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.2. Use plotly_express](#3.1.2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.3. Use seaborn](#3.1.3)<br>\n\n&nbsp;&nbsp;&nbsp;[3.2. Independent Variables](#3.2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1. Outliers](#3.2.1)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.2. Quantitative - Qualitative Features](#3.2.2)<br>\n\n[04. Prediction](#04)<br>\n\n&nbsp;&nbsp;&nbsp;[4.1. Baseline Model](#4.1)<br>\n&nbsp;&nbsp;&nbsp;[4.2. Feature Engineering](#4.2)<br>\n&nbsp;&nbsp;&nbsp;[4.3. Feature Scaling](#4.3)<br>\n\n\n[05. Results](#05)<br>\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 01. Import Library<a id='01'></a>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nimport plotly_express as px\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n\nimport pandas_profiling\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n\nfrom lightgbm import LGBMRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 02. Load Data <a id='02'></a>","metadata":{}},{"cell_type":"code","source":"print(\"List of files:\", os.listdir('/kaggle/input/house-prices-advanced-regression-techniques'))\n\n# Train data\ndf_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\nprint(\"\\nTrain data length:\",df_train.shape)\nprint(\"\\nTrain data columns:\",df_train.columns)\nprint(\"\\nTrain data columns:\",df_train.info())\nprint(\"\\nTrain data:\\n\\n\",df_train.head())","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test data\ndf_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\nprint(\"\\nTest data length:\",df_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 03. Exploratory Data Analysis (EDA)<a id='03'></a>","metadata":{}},{"cell_type":"code","source":"# Correlation\ndf_train_corr = df_train.corr()\ndf_train_corr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train_corr.style.background_gradient(cmap='coolwarm', axis=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SalePrice has highest corr with OverallQual\ndf_train_corr[['SalePrice','OverallQual']].style.background_gradient(cmap='coolwarm', axis=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use panda profile report\n# df_train.profile_report()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1. Dependent Variable<a id='3.1'></a>","metadata":{}},{"cell_type":"code","source":"df_train['SalePrice'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax=df_train['SalePrice'].plot.hist(bins=100, alpha=0.6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.1. Use matplotlib<a id='3.1.1'></a>","metadata":{}},{"cell_type":"code","source":"# Use matplotlib\n\n# plt.style.use('ggplot')\nplt.hist(df_train['SalePrice'], bins = 100)\n\n# Add title and axis names\nplt.title('Sales Price')\nplt.xlabel('Price')\nplt.ylabel('Frequency') \n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter Plot\nfig, ax = plt.subplots()\nax.scatter(df_train['GrLivArea'], df_train['SalePrice'])\nplt.xlabel('GrLivArea', fontsize=12)\nplt.ylabel('SalePrice', fontsize=12)\nplt.title('Sale Price', fontsize=16)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# QQ-plot\nfig = plt.figure()\nax = fig.add_subplot()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.2. Use plotly_express<a id='3.1.2'></a>","metadata":{}},{"cell_type":"code","source":"# Scatter Plot with color from 2nd variable\npx.scatter(df_train, x='GrLivArea', y='SalePrice', color='OverallQual')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter Plot with color from 2nd variable\npx.scatter(df_train, x='TotalBsmtSF', y='SalePrice', color='OverallQual')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Box Plot\npx.box(df_train[['OverallQual', 'SalePrice']].sort_values(by='OverallQual')\n       , x='OverallQual'\n       , y='SalePrice'\n       , color='OverallQual')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Box Plot\npx.box(df_train[['SaleCondition', 'SalePrice']].sort_values(by='SaleCondition')\n       , x='SaleCondition'\n       , y='SalePrice'\n       , color='SaleCondition')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Box Plot\npx.box(df_train[['ExterQual', 'SalePrice']].sort_values(by='ExterQual')\n       , x='ExterQual'\n       , y='SalePrice'\n       , color='ExterQual')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.1.3. Use seaborn<a id='3.1.3'></a>","metadata":{}},{"cell_type":"code","source":"sns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# Plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='upper right')\n\nax = plt.axes()\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(df_train_corr, \n            xticklabels=df_train_corr.columns.values,\n            yticklabels=df_train_corr.columns.values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2. Independent Variables<a id='3.2'></a>","metadata":{"trusted":true}},{"cell_type":"code","source":"df_train.drop(['SalePrice'], axis = 1).describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.1. Outliers<a id='3.2.1'></a>","metadata":{}},{"cell_type":"code","source":"# Clean outliers\nprint(\"Length of data before dropping outliers:\", len(df_train))\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) \n                                & (df_train['SalePrice']<300000)].index)\nprint(\"Length of data after dropping outliers:\", len(df_train))\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>5000) \n                                | (df_train['SalePrice']>500000)].index)\nprint(\"Length of data after dropping outliers:\", len(df_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2. Quantitative - Qualitative Features<a id='3.2.2'></a>","metadata":{}},{"cell_type":"code","source":"# Quantitative Variables\nquan_var = [q for q in df_train.columns if df_train.dtypes[q] != 'object']\nquan_var.remove('SalePrice') \nquan_var.remove('Id')\nprint(\"Quantitative Variables:\\n\", quan_var)\n\n# Qualitative Variables\nqual_var = [q for q in df_train.columns if df_train.dtypes[q] == 'object']\nprint(\"\\nQualitative Variables:\\n\", qual_var)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine all data\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\ny_train = df_train.SalePrice.values\ndf_all_data = pd.concat((df_train, df_test)).reset_index(drop=True)\ndf_all_data.drop(['Id','SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(df_all_data.shape))\n\n# Calculate missing data ratio\ndf_all_data_na = (df_all_data.isnull().sum() / len(df_all_data)) * 100\ndf_all_data_na = df_all_data_na.drop(df_all_data_na[df_all_data_na == 0].index).sort_values(ascending=False)[:50]\nmissing_data = pd.DataFrame({'Missing Ratio' :df_all_data_na})\nprint('Missing data percentage:\\n',missing_data.head(50))\n\n# Plot\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nax.set_facecolor(\"white\")\nsns.barplot(x=df_all_data_na.index, y=df_all_data_na)\nsns.color_palette('pastel')\nplt.xlabel('Features', fontsize=12)\nplt.ylabel('Percent of missing values', fontsize=12)\nplt.title('Percent missing data by feature', fontsize=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 04. Prediction <a id='04'></a>","metadata":{"trusted":true}},{"cell_type":"code","source":"df_result = pd.DataFrame(columns=['Model','RMSE','MSE','Summary'])\nprint(df_result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.1. Baseline Model <a id='4.1'></a>","metadata":{}},{"cell_type":"markdown","source":"### Linear Regression","metadata":{}},{"cell_type":"code","source":"# Run Linear Regression on a single variable that has the highest corr with dependent variable\nX = df_train[['OverallQual']]\ny = df_train['SalePrice']\n\n# Train Test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Linear Regression Model\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\n# RMSE\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['Linear Regression'\n                                            , rmse\n                                            , mse\n                                            ,'Baseline model'                               \n                                           ]], columns=df_result.columns))\nprint(df_result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest","metadata":{}},{"cell_type":"code","source":"# RandomForestRegressor\nrf = RandomForestRegressor(random_state=10)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Baseline model'                               \n                                           ]], columns=df_result.columns))\nprint(df_result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features - Missing Ratio","metadata":{}},{"cell_type":"code","source":"# Get the list of variable based on missing data ratio\nfeatures_for_reg = missing_data[missing_data['Missing Ratio']<70].index.values.tolist()\n\n\n# Get Dummies\nX_all = pd.get_dummies(df_all_data[features_for_reg])\nX_all.fillna(0, inplace=True)\n\nX = X_all[0:len(df_train)]\ny = df_train['SalePrice']\n\n# Initiate train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf = RandomForestRegressor(random_state=3)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features with less than 70% missing data'                               \n                                           ]], columns=df_result.columns))\n\n\n\n# Calculate feature importances\nimportances = rf.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns[i] for i in indices]\n\nprint(\"Most important:\\n\", names[:10])\nprint(\"Least important:\\n\", names[(-10):])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features - Importance","metadata":{}},{"cell_type":"code","source":"# Get the list of variable based on rf feature importance\nn_features = 45\nfeatures_for_reg = names[:n_features]\n\n\n# Run Linear Regression\nX_all = X_all[features_for_reg]\nX_all.fillna(0, inplace=True)\n\nX = X_all[0:len(df_train)]\ny = df_train['SalePrice']\n\n# Initiate train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf = RandomForestRegressor(random_state=3)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Important features based on RF'                               \n                                           ]], columns=df_result.columns))\n\n\n\n# Calculate feature importances\nimportances = rf.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns[i] for i in indices]\n\nprint(\"Most important:\\n\", names[:10])\nprint(\"Least important:\\n\", names[(-10):])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2. Feature Engineering <a id='4.2'></a>","metadata":{}},{"cell_type":"code","source":"# New feature\ndf_all_data[\"OverallQual_Garage_GrLivArea\"] = df_all_data[\"OverallQual\"] * df_all_data[\"GarageArea\"] * df_all_data[\"GrLivArea\"]\n\n# Get Dummies\nX_all = pd.get_dummies(df_all_data)\nX_all.fillna(0, inplace=True)\n\nX = X_all[0:len(df_train)]\ny = df_train['SalePrice']\n\n# Initiate train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf = RandomForestRegressor(random_state=3)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features engineering'                               \n                                           ]], columns=df_result.columns))\n\n\n\n# Calculate feature importances\nimportances = rf.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns[i] for i in indices]\n\nprint(\"Most important:\\n\", names[:10])\nprint(\"Least important:\\n\", names[(-10):])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering and Missing Value","metadata":{}},{"cell_type":"code","source":"# Get the list of variable based on missing data ratio\nfeatures_to_drop = missing_data[missing_data['Missing Ratio']>=70].index.values.tolist()\n\n\n# Get Dummies\nX_all = pd.get_dummies(df_all_data[df_all_data.columns.difference(features_to_drop)])\nX_all.fillna(0, inplace=True)\n\nX = X_all[0:len(df_train)]\ny = df_train['SalePrice']\n\n# Initiate train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf = RandomForestRegressor(random_state=3)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features Engineering & < 70% missing data'                               \n                                           ]], columns=df_result.columns))\n\n\n\n# Calculate feature importances\nimportances = rf.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns[i] for i in indices]\n\nprint(\"Most important:\\n\", names[:10])\nprint(\"Least important:\\n\", names[(-10):])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Light GBM","metadata":{}},{"cell_type":"code","source":"lgb_model = LGBMRegressor().fit(X_train, y_train)\ny_pred_lgb = lgb_model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_lgb)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['LGBMRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features Engineering & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Light GBM best params","metadata":{}},{"cell_type":"code","source":"# Grid search (narrow down to this grid after several iterations)\nlgb_params = {\"learning_rate\": [0.005, 0.01],\n               \"n_estimators\": [5000],\n               \"max_depth\": [4, 5],\n               \"feature_fraction\": [0.1, 0.2, 0.3],\n               \"colsample_bytree\": [0.8],\n               'num_leaves': [4, 5]}\n                              \nlgb_cv_model = GridSearchCV(lgb_model,\n                             lgb_params,\n                             cv=10,\n                             n_jobs=-1,\n                             verbose=2).fit(X_train, y_train)\n\nprint(lgb_cv_model.best_params_)\n\n# use best params\nlgb_tuned = LGBMRegressor(**lgb_cv_model.best_params_).fit(X_train, y_train)\ny_pred_lgb = lgb_tuned.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_lgb)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\n\ndf_result = df_result.append(pd.DataFrame([['LGBMRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Tuned model with Features Engineering & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Boosting Regressor","metadata":{}},{"cell_type":"code","source":"gb_model = GradientBoostingRegressor()\ngb_model.fit(X_train, y_train)\ny_pred_gb = gb_model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_gb)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_gb))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\n\ndf_result = df_result.append(pd.DataFrame([['GradientBoostingRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features Engineering & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GB with Best Params","metadata":{}},{"cell_type":"code","source":"# Grid search (narrow down to this grid after several iterations)\ngb_params = {'n_estimators': [1000,5000],\n             'max_depth': [4,5],\n             'min_samples_split': [3,5],\n             'learning_rate': [0.005, 0.01],\n             'loss': ['ls']}\n                              \ngb_cv_model = GridSearchCV(gb_model,\n                           gb_params,\n                           cv=10,\n                           n_jobs=-1,\n                           verbose=2).fit(X_train, y_train)\n\nprint(gb_cv_model.best_params_)\n\n# use best params\ngb_tuned = GradientBoostingRegressor(**gb_cv_model.best_params_).fit(X_train, y_train)\ny_pred_gb = gb_tuned.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_gb)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_gb))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\n\ndf_result = df_result.append(pd.DataFrame([['GradientBoostingRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Tuned & Features Engineering & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.3. Feature Scaling <a id='4.3'></a>","metadata":{}},{"cell_type":"code","source":"df_all_data.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all_data_scaled =  pd.DataFrame()\n\nscaler = StandardScaler()\n\nfor col in quan_var:    \n    scaler.fit((np.array(df_all_data[col])).reshape(-1, 1))\n    scaled_list = scaler.transform((np.array(df_all_data[col])).reshape(-1, 1))\n    \n    # Convert list of list to flat list before putting back to the df\n    df_all_data_scaled[col] = [item for elem in scaled_list for item in elem]\n    \ndf_all_data_scaled.describe().T\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# New feature\ndf_all_data_scaled[\"OverallQual_Garage_GrLivArea\"] = df_all_data_scaled[\"OverallQual\"] * \\\n                                                     df_all_data_scaled[\"GarageArea\"] * \\\n                                                     df_all_data_scaled[\"GrLivArea\"]\n\n# Get Dummies\nX_all = pd.get_dummies(df_all_data_scaled)\nX_all.fillna(0, inplace=True)\n\nX = X_all[0:len(df_train)]\ny = df_train['SalePrice']\n\n# Initiate train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf_scaled = RandomForestRegressor(random_state=3)\nrf_scaled.fit(X_train,y_train)\ny_pred_rf = rf_scaled.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features engineering & Scaled'                               \n                                           ]], columns=df_result.columns))\n\n\n\n# Calculate feature importances\nimportances = rf_scaled.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns[i] for i in indices]\n\nprint(\"Most important:\\n\", names[:10])\nprint(\"Least important:\\n\", names[(-10):])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the list of variable based on missing data ratio\nfeatures_to_drop = missing_data[missing_data['Missing Ratio']>=70].index.values.tolist()\n\n\n# Get Dummies\nX_all = pd.get_dummies(df_all_data_scaled[df_all_data_scaled.columns.difference(features_to_drop)])\nX_all.fillna(0, inplace=True)\n\nlgb_scaled = LGBMRegressor().fit(X_train, y_train)\ny_pred_lgb = lgb_scaled.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_lgb)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['LGBMRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features Engineering & Scaled & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Grid search (narrow down to this grid after several iterations)\nlgb_params = {\"learning_rate\": [0.005, 0.01],\n               \"n_estimators\": [5000],\n               \"max_depth\": [4, 5],\n               \"feature_fraction\": [0.1, 0.2, 0.3],\n               \"colsample_bytree\": [0.8],\n               'num_leaves': [4, 5]}\n                              \nlgb_cv_model = GridSearchCV(lgb_scaled,\n                             lgb_params,\n                             cv=10,\n                             n_jobs=-1,\n                             verbose=2).fit(X_train, y_train)\n\nprint(lgb_cv_model.best_params_)\n\n# use best params\nlgb_scaled_tuned = LGBMRegressor(**lgb_cv_model.best_params_).fit(X_train, y_train)\ny_pred_lgb_scaled_tuned = lgb_scaled_tuned.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_lgb_scaled_tuned)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb_scaled_tuned))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\n\ndf_result = df_result.append(pd.DataFrame([['LGBMRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Tuned model with Features Engineering & Scaled & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 05. Results<a id='05'></a>","metadata":{}},{"cell_type":"code","source":"df_result['RMSE'] = df_result['RMSE'].astype(int)\ndf_result['MSE'] = df_result['MSE'].astype(int)\ndf_result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction Submission","metadata":{}},{"cell_type":"code","source":"# # Predict using rf\n# X_test = X_all.iloc[len(df_train):len(X_all)]\n# y_pred_rf = rf.predict(X_test)\n\n# # Predict using lgb\n# X_test = X_all.iloc[len(df_train):len(X_all)]\n# y_pred_lgb = lgb_model.predict(X_test)\n\n# # Predict using lgb_tuned\n# X_test = X_all.iloc[len(df_train):len(X_all)]\n# y_pred_lgb_tuned = lgb_tuned.predict(X_test)\n\n# Predict using rf_scaled\nX_test = X_all.iloc[len(df_train):len(X_all)]\ny_pred_rf_scaled = rf_scaled.predict(X_test)\n\n# Predict using lgb_scaled\nX_test = X_all.iloc[len(df_train):len(X_all)]\ny_pred_lgb_tuned = lgb_scaled_tuned.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission\nsub = pd.DataFrame()\nsub['Id'] = df_test['Id']\nsub['SalePrice'] = y_pred_lgb_tuned\nsub.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}