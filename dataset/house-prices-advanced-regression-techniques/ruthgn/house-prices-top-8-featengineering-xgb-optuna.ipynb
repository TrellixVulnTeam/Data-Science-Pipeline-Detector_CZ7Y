{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"This notebook outlines the step-by-step process of creating a house price prediction modelâ€”it includes data pre-processing, feature engineering, model training, hyperparameter tuning, and model explainability. The prediction model generated currently ranks in the top 8% of Kaggle's [House Price Prediction Competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) leaderboard and top 1% of [Housing Prices Competition for Kaggle Learn Users](https://www.kaggle.com/c/home-data-for-ml-course) leaderboard (as of 10/29/2021).\n\nIf you're interested in learning more about model deployment (as an interactive [web app](https://share.streamlit.io/ruthgn/ames-housing-price-prediction/main/ames-house-ml-app.py)), check out [this notebook](https://www.kaggle.com/ruthgn/top-1-model-interpretation-deployment).","metadata":{}},{"cell_type":"markdown","source":"# Part 1 - Preliminaries","metadata":{"id":"LdW7VRfuBpZ6"}},{"cell_type":"markdown","source":"## Imports and Configuration","metadata":{"id":"To9sYN-7ByMR"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas.api.types import CategoricalDtype\nimport optuna\nimport shap\nimport pickle\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\nfrom pathlib import Path","metadata":{"id":"oSkg9hc9BxnS","outputId":"6ac572db-1943-4583-f98e-e8ce85cbd5f5","execution":{"iopub.status.busy":"2021-10-29T23:13:33.006121Z","iopub.execute_input":"2021-10-29T23:13:33.006888Z","iopub.status.idle":"2021-10-29T23:13:33.012458Z","shell.execute_reply.started":"2021-10-29T23:13:33.006849Z","shell.execute_reply":"2021-10-29T23:13:33.01164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{"id":"LtNjB4ZwClik"}},{"cell_type":"code","source":"def load_data():\n    # Read data\n    data_dir = Path(\"../input/house-prices-advanced-regression-techniques/\")\n    df_train = pd.read_csv(data_dir / \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir / \"test.csv\", index_col=\"Id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing steps\n    df = clean(df)\n    df = encode(df)\n    df = impute_plus(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","metadata":{"id":"hE1DXBLf-yyJ","execution":{"iopub.status.busy":"2021-10-29T23:13:33.013992Z","iopub.execute_input":"2021-10-29T23:13:33.014456Z","iopub.status.idle":"2021-10-29T23:13:33.034084Z","shell.execute_reply.started":"2021-10-29T23:13:33.01442Z","shell.execute_reply":"2021-10-29T23:13:33.032991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clean Data","metadata":{"id":"gr67rwCoJpql"}},{"cell_type":"markdown","source":"A closer look at the dataset make it clear that there are categorical features with typos in the categories:","metadata":{"id":"5y3GXHxmKxm3"}},{"cell_type":"code","source":"data_dir = Path(\"../input/house-prices-advanced-regression-techniques/\")\ndf = pd.read_csv(data_dir / \"train.csv\", index_col=\"Id\")\n\ndf.Exterior2nd.unique()","metadata":{"id":"ojQ3SOABJjcd","execution":{"iopub.status.busy":"2021-10-29T23:13:33.036222Z","iopub.execute_input":"2021-10-29T23:13:33.036744Z","iopub.status.idle":"2021-10-29T23:13:33.073828Z","shell.execute_reply.started":"2021-10-29T23:13:33.036696Z","shell.execute_reply":"2021-10-29T23:13:33.07326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will create a function to make corrections on several detected issues within the dataset:","metadata":{"id":"9XKU4Et4NazS"}},{"cell_type":"code","source":"def clean(df):\n    # Correct typo on Exterior2nd\n    df['Exterior2nd'] = df['Exterior2nd'].replace({'Brk Cmn': 'BrkComm'})\n    # Some values of GarageYrBlt are corrupt, so we'll replace them with the year house was built\n    df['GarageYrBlt'] = df['GarageYrBlt'].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n    # Name beginning with numbers are awkward to work with\n    df.rename(columns={\n        '1stFlrSF': 'FirstFlrSF',\n        '2ndFlrSF': 'SecondFlrSF',\n        '3SsnPorch': 'Threeseasonporch'\n        }, inplace=True)\n    return df","metadata":{"id":"o9HR6SEKNZab","execution":{"iopub.status.busy":"2021-10-29T23:13:33.075118Z","iopub.execute_input":"2021-10-29T23:13:33.075519Z","iopub.status.idle":"2021-10-29T23:13:33.081052Z","shell.execute_reply.started":"2021-10-29T23:13:33.07549Z","shell.execute_reply":"2021-10-29T23:13:33.080361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encode the Statistical Data Type","metadata":{"id":"ZteGqGhJQy5h"}},{"cell_type":"markdown","source":"Next, we will encode each feature with its correct data type to ensure each feature is treated appropriately by whatever functions we use moving forward.\n\nThe numeric features in our particular dataset are already encoded correctly (`float` for continuous and `int` for discrete features). What we need to pay closer attention to is the categorical features. For instance, note in particular, that the 'MSSubClass' feature is read as an `int` type, but is actually a nominative categorical.","metadata":{"id":"UCETwsi_Wvv1"}},{"cell_type":"code","source":"# The nominative (unordered) categorical features\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \n                \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \n                \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \n                \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \n                \"SaleType\", \"SaleCondition\"]\n\n# The ordinal (ordered) categorical features \n\n# Pandas calls the categories \"levels\"\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\ndef encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df","metadata":{"id":"OONq0RYrQ0Ij","execution":{"iopub.status.busy":"2021-10-29T23:13:33.0826Z","iopub.execute_input":"2021-10-29T23:13:33.083097Z","iopub.status.idle":"2021-10-29T23:13:33.098971Z","shell.execute_reply.started":"2021-10-29T23:13:33.083065Z","shell.execute_reply":"2021-10-29T23:13:33.097997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handle Missing Values","metadata":{"id":"D71of21OZD5m"}},{"cell_type":"markdown","source":"We'll impute 0 for missing numeric values and \"None\" for missing categorical values. Additionally, we will create \"missing value\" indicator columns--these columns will contain boolean values indicating whether a particular feature value was imputed for a sample.","metadata":{"id":"a33x38_oal6X"}},{"cell_type":"code","source":"def impute_plus(df):\n    # Get names of columns with missing values\n    cols_with_missing = [col for col in df.columns if col != 'SalePrice' and df[col].isnull().any()]\n    # Make new columns indicating imputed features (`SalePrice` column exluded)\n    for col in cols_with_missing:\n        df[col + '_was_missing'] = df[col].isnull()\n        df[col + '_was_missing'] = (df[col + '_was_missing']) * 1\n    # Impute 0 for missing numeric values\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    # Impute \"None\" for missing categorical values\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    return df","metadata":{"id":"GymGblNQanYe","execution":{"iopub.status.busy":"2021-10-29T23:13:33.100682Z","iopub.execute_input":"2021-10-29T23:13:33.101457Z","iopub.status.idle":"2021-10-29T23:13:33.115206Z","shell.execute_reply.started":"2021-10-29T23:13:33.101385Z","shell.execute_reply":"2021-10-29T23:13:33.114299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{"id":"SBAnz5MA3JYL"}},{"cell_type":"markdown","source":"Now we can call the data loader and get the processed data splits--let's take a quick look:","metadata":{"id":"GtJU0niI5siw"}},{"cell_type":"code","source":"df_train, df_test = load_data()","metadata":{"id":"WrME6PyQ4IMA","execution":{"iopub.status.busy":"2021-10-29T23:13:33.116602Z","iopub.execute_input":"2021-10-29T23:13:33.117246Z","iopub.status.idle":"2021-10-29T23:13:33.371704Z","shell.execute_reply.started":"2021-10-29T23:13:33.117197Z","shell.execute_reply":"2021-10-29T23:13:33.370536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Peek at the values\ndisplay(df_train)\ndisplay(df_test)\n\n# # Display information about dtypes and missing values\n# display(df_train.info())\n# display(df_test.info())","metadata":{"id":"1pJHPBME4Wd6","execution":{"iopub.status.busy":"2021-10-29T23:13:33.373353Z","iopub.execute_input":"2021-10-29T23:13:33.373685Z","iopub.status.idle":"2021-10-29T23:13:33.377191Z","shell.execute_reply.started":"2021-10-29T23:13:33.373647Z","shell.execute_reply":"2021-10-29T23:13:33.376504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Establish Baseline","metadata":{"id":"Yv3OxvRx6sL_"}},{"cell_type":"markdown","source":"Before we delve into feature engineering, we're going to establish a baseline score to judge our upcoming feature sets against. We will make our predictions with an XGBoost model and create a function to compute the cross-validated *Root Mean Squared Error* (RMSE) score for each feature set our model trains on. \n\nXGBoost minimizes *Mean Squared Error* (MSE), but we want to minimize Root Mean Squared Error (RMSE) specifically, requiring us to \"reshape\" our target feature (`Sale Price`) using log transformation for training and later applying exponential transform to the predictions. Mathematically, this makes sense because we typically use the log scale for variables that change multiplicatively with other factors. How do we know when variables should be modeled as changing multiplicatively? \n- Every day language (surprise, surprise!). Examples include prices (\"foreclosed homes sell at a 20% to 30% discount\"), and sales (\"your yoy sales are up 20% accross models\").\n- More generally, variables that are strictly non-negative (e.g, volatility, counts of errors or events, rainfall) are often treated as changing linearly in a log scale.","metadata":{"id":"jAmXLZe87DLv"}},{"cell_type":"code","source":"# My default XGB parameters\n\nxgb_params = dict(\n    max_depth=3,                           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.1,                     # effect of each tree - try 0.0001 to 0.1\n    n_estimators=100,                      # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,                    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=1,                    # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=1,                           # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0,                           # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1,                          # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,                   # set > 1 for boosted random forests\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:13:33.379917Z","iopub.execute_input":"2021-10-29T23:13:33.380558Z","iopub.status.idle":"2021-10-29T23:13:33.388867Z","shell.execute_reply.started":"2021-10-29T23:13:33.380519Z","shell.execute_reply":"2021-10-29T23:13:33.388221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def score_dataset(X, y, model=XGBRegressor(**xgb_params)):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Use RMSLE (Root Mean Squared Log Error) instead of MSE (Mean Squared Error)as evaluation metric\n    # (So, we need to log-transform y to train and exp-transform the predictions)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring='neg_mean_squared_error'\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score","metadata":{"id":"A9mAb8X35aQP","execution":{"iopub.status.busy":"2021-10-29T23:13:33.389919Z","iopub.execute_input":"2021-10-29T23:13:33.390146Z","iopub.status.idle":"2021-10-29T23:13:33.400826Z","shell.execute_reply.started":"2021-10-29T23:13:33.390119Z","shell.execute_reply":"2021-10-29T23:13:33.400162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's run our new function on the processed data and get a baseline score:","metadata":{"id":"xWUf1Xo3GG52"}},{"cell_type":"code","source":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X, y)\nprint(f\"Baseline score: {baseline_score:.5f} RMSE\")","metadata":{"id":"UIMF2ZerGFGj","execution":{"iopub.status.busy":"2021-10-29T23:13:33.401938Z","iopub.execute_input":"2021-10-29T23:13:33.402252Z","iopub.status.idle":"2021-10-29T23:13:34.883207Z","shell.execute_reply.started":"2021-10-29T23:13:33.40222Z","shell.execute_reply":"2021-10-29T23:13:34.88247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This baseline score should help us in knowing whether some set of features we've assembled in our experimentation actually led to any improvement or not.","metadata":{"id":"3WtxoyWoHAGe"}},{"cell_type":"markdown","source":"# Part 2 - Feature Engineering","metadata":{"id":"zwY3dYLZHSzx"}},{"cell_type":"markdown","source":"## Feature Utility Scores","metadata":{"id":"1A39cZ5xHfo-"}},{"cell_type":"markdown","source":"It's time for us to take a closer look at the features we have in our dataset. We will analyze how much potential a feature has by computing its *utility score*.","metadata":{"id":"zjIk1PalH0ci"}},{"cell_type":"code","source":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name='MI Scores', index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores): \n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n","metadata":{"id":"3CnOYpL5HdHF","execution":{"iopub.status.busy":"2021-10-29T23:13:34.886729Z","iopub.execute_input":"2021-10-29T23:13:34.888969Z","iopub.status.idle":"2021-10-29T23:13:34.899655Z","shell.execute_reply.started":"2021-10-29T23:13:34.888921Z","shell.execute_reply":"2021-10-29T23:13:34.898631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our feature scores are listed below:","metadata":{"id":"TCCcjnFAjU1Q"}},{"cell_type":"code","source":"mi_scores = make_mi_scores(X, y)\n\n# Show Mutual Information (MI) score plot\nplt.figure(dpi=120, figsize=(8, 20))\nplot_mi_scores(mi_scores)\n","metadata":{"id":"OIYP7SvKjZC6","outputId":"435f625b-c148-4d67-e097-891fd171fb57","execution":{"iopub.status.busy":"2021-10-29T23:13:34.901024Z","iopub.execute_input":"2021-10-29T23:13:34.901366Z","iopub.status.idle":"2021-10-29T23:13:40.413525Z","shell.execute_reply.started":"2021-10-29T23:13:34.9013Z","shell.execute_reply":"2021-10-29T23:13:40.412503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have a number of features that are highly informative and several that don't seem to be informative at all (at least by themselves). Therefore, we will focus our efforts on the top scoring features. Training on uninformative features can lead to overfitting as well, so features with 0.0 MI scores are going to be dropped entirely.","metadata":{"id":"emC5s4-JtI86"}},{"cell_type":"code","source":"def drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores > 0.0]","metadata":{"id":"e_eO6HNzuVcx","execution":{"iopub.status.busy":"2021-10-29T23:13:40.41499Z","iopub.execute_input":"2021-10-29T23:13:40.415405Z","iopub.status.idle":"2021-10-29T23:13:40.420215Z","shell.execute_reply.started":"2021-10-29T23:13:40.415354Z","shell.execute_reply":"2021-10-29T23:13:40.419108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check if removing these features actually lead to a performance gain.","metadata":{"id":"Q9pKqb_6ukok"}},{"cell_type":"code","source":"drop_uninformative(X, mi_scores)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:13:40.421768Z","iopub.execute_input":"2021-10-29T23:13:40.422062Z","iopub.status.idle":"2021-10-29T23:13:40.458167Z","shell.execute_reply.started":"2021-10-29T23:13:40.421993Z","shell.execute_reply":"2021-10-29T23:13:40.457357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_uninformative(X, mi_scores).info()","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:13:40.459475Z","iopub.execute_input":"2021-10-29T23:13:40.459692Z","iopub.status.idle":"2021-10-29T23:13:40.476851Z","shell.execute_reply.started":"2021-10-29T23:13:40.459666Z","shell.execute_reply":"2021-10-29T23:13:40.47591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\nX = drop_uninformative(X, mi_scores)\n\n# Check out if this results in any improvement from the baseline score\nscore_dataset(X, y)","metadata":{"id":"p5cZMz6iuv1O","outputId":"c632440a-f0d4-493f-edcb-09790f5bfe7e","execution":{"iopub.status.busy":"2021-10-29T23:13:40.478562Z","iopub.execute_input":"2021-10-29T23:13:40.47911Z","iopub.status.idle":"2021-10-29T23:13:41.738166Z","shell.execute_reply.started":"2021-10-29T23:13:40.479064Z","shell.execute_reply":"2021-10-29T23:13:41.737118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nice! Removing our uninformative features does lead to a slight performance gain. We will add our new `drop_uninformative` function to our feature-creation pipeline.","metadata":{"id":"GIuf8tWSvEqL"}},{"cell_type":"markdown","source":"## Create Features","metadata":{"id":"E5pMT5qtHsQ9"}},{"cell_type":"markdown","source":"To make our feature engineering workflow more modular, we'll define a function that will take a prepared dataframe and pass it through a pipeline of transformations to get the final feature set.","metadata":{"id":"SGDBBWIKdnU6"}},{"cell_type":"markdown","source":"Let's go ahead and define one transformation now--a label encoding for the categorical features. \n\n*Note that we're specifically using label encoding for our unordered categories because we are using a tree-ensemble, XGBoost, particularly. If instead we decide to try using a linear regression model, we're going to have to use one-hot encoding for features with unordered categories.*","metadata":{"id":"BU7bIKWJeYKe"}},{"cell_type":"code","source":"def label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes(['category']):\n        X[colname] = X[colname].cat.codes\n    return X","metadata":{"id":"HLNuFqUJeXx_","execution":{"iopub.status.busy":"2021-10-29T23:13:41.73966Z","iopub.execute_input":"2021-10-29T23:13:41.740137Z","iopub.status.idle":"2021-10-29T23:13:41.745053Z","shell.execute_reply.started":"2021-10-29T23:13:41.740098Z","shell.execute_reply":"2021-10-29T23:13:41.744365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Features with Pandas (Data Wrangling)","metadata":{"id":"eJkbRlADgCxw"}},{"cell_type":"markdown","source":"#### Mathematical Transforms (Ratios)","metadata":{"id":"8YSHudx2mWh7"}},{"cell_type":"markdown","source":"Ratios seem to be difficult for most models to learn, so creating new features expressing ratio combinations can often lead to some easy performance gains. We're going to create two new features expressing important ratios using mathematical transformation:\n\n- `LivLotRatio`: the ratio of `GrLivArea` to `LotArea`\n- `Spaciousness`: the sum of `FirstFlrSF` and `SecondFlrSF` divided by `TotRmsAbvGrd`\n","metadata":{"id":"yVwTGdAUgL5R"}},{"cell_type":"code","source":"def mathematical_transforms(df):\n    X = pd.DataFrame() # Just a dataframe to hold new features\n    X['LivLotRatio'] = df.GrLivArea / df.LotArea\n    X['Spaciousness'] = (df.FirstFlrSF + df.SecondFlrSF) / df.TotRmsAbvGrd\n    return X","metadata":{"id":"mSvrtBoAkAdT","execution":{"iopub.status.busy":"2021-10-29T23:13:41.746191Z","iopub.execute_input":"2021-10-29T23:13:41.746715Z","iopub.status.idle":"2021-10-29T23:13:41.762801Z","shell.execute_reply.started":"2021-10-29T23:13:41.746681Z","shell.execute_reply":"2021-10-29T23:13:41.762061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Interactions","metadata":{"id":"TtIl1ZJumih6"}},{"cell_type":"markdown","source":"During an exploratory analysis of our data, something interesting came up:","metadata":{"id":"uR7GAMmtn4Co"}},{"cell_type":"code","source":"# Check out interaction between `BldgType` and `GrLivArea`\nfeature = \"GrLivArea\"\n\nsns.lmplot(\n    x=feature, y=\"SalePrice\", hue=\"BldgType\", col=\"BldgType\",\n    data=df_train, scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n);","metadata":{"id":"OhQa7fx4n27T","outputId":"28009d10-8d88-480d-d7d5-c51c5e06cae0","execution":{"iopub.status.busy":"2021-10-29T23:13:41.763977Z","iopub.execute_input":"2021-10-29T23:13:41.764681Z","iopub.status.idle":"2021-10-29T23:13:43.79401Z","shell.execute_reply.started":"2021-10-29T23:13:41.764596Z","shell.execute_reply":"2021-10-29T23:13:43.792974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The trend lines being significantly different from one category to the next indicates an interaction effect between `GrLivArea` and `BldgType` that relates to a home's `SalePrice`. \n\nBelow are several other detected interaction effects between categorical and numerical variables:","metadata":{"id":"yv2DEwe-rlRM"}},{"cell_type":"code","source":"# Check out interaction between `BsmtCond` and `TotalBsmtSF`\nfeature = \"TotalBsmtSF\"\n\nsns.lmplot(\n    x=feature, y=\"SalePrice\", hue=\"BsmtCond\", col=\"BsmtCond\",\n    data=df, scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n);","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:13:43.7955Z","iopub.execute_input":"2021-10-29T23:13:43.795816Z","iopub.status.idle":"2021-10-29T23:13:45.646494Z","shell.execute_reply.started":"2021-10-29T23:13:43.795773Z","shell.execute_reply":"2021-10-29T23:13:45.645481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check out interaction between `GarageQual` and `GarageArea`\nfeature = \"GarageArea\"\n\nsns.lmplot(\n    x=feature, y=\"SalePrice\", hue=\"GarageQual\", col=\"GarageQual\",\n    data=df, scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n);","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:13:45.648252Z","iopub.execute_input":"2021-10-29T23:13:45.648602Z","iopub.status.idle":"2021-10-29T23:13:47.55522Z","shell.execute_reply.started":"2021-10-29T23:13:45.648556Z","shell.execute_reply":"2021-10-29T23:13:47.554557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def interactions(df):\n    # BldgType interaction\n    X_inter_1 = pd.get_dummies(df.BldgType, prefix='Bldg')\n    X_inter_1 = X_inter_1.mul(df.GrLivArea, axis=0)\n    # Bsmt interaction\n    X_inter_2 = pd.get_dummies(df.BsmtCond, prefix='BsmtCond')\n    X_inter_2 = X_inter_2.mul(df.TotalBsmtSF, axis=0)\n    # Garage interaction\n    X_inter_3 = pd.get_dummies(df.GarageQual, prefix='GarageQual')\n    X_inter_3 = X_inter_3.mul(df.GarageArea, axis=0)\n    # Combine into one DataFrame\n    X = X_inter_1.join(X_inter_2)\n    #X = X.join(X_inter_3)\n    return X","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:13:47.556948Z","iopub.execute_input":"2021-10-29T23:13:47.55756Z","iopub.status.idle":"2021-10-29T23:13:47.565516Z","shell.execute_reply.started":"2021-10-29T23:13:47.557512Z","shell.execute_reply":"2021-10-29T23:13:47.564785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Counts","metadata":{"id":"ArDMbxuctquQ"}},{"cell_type":"markdown","source":"Let's create a new feature called `PorchTypes` that describes how many kinds of outdoor areas a dwelling has. We will count how many of the following are greater than 0.0:\n\n-`WoodDeckSF`\n\n-`OpenPorchSF`\n\n-`EnclosedPorch`\n\n-`Threeseasonporch`\n\n-`ScreenPorch`","metadata":{"id":"qBnrpglkt1l7"}},{"cell_type":"markdown","source":"And then we're going to create another new feature `TotalHalfBath` that contains the sum of half-bathrooms within the property.","metadata":{}},{"cell_type":"markdown","source":"Additionally, we will also sum up the total number of rooms (including full and half bathrooms) in each property and store them in a new feature called `TotalRoom`.","metadata":{}},{"cell_type":"code","source":"def counts(df):\n    X = pd.DataFrame()\n    X['PorchTypes'] = df[['WoodDeckSF',\n                        'OpenPorchSF',\n                        'EnclosedPorch',\n                        'Threeseasonporch',\n                        'ScreenPorch'\n                        ]].gt(0.0).sum(axis=1)\n    X['TotalHalfBath'] = df.BsmtFullBath + df.BsmtHalfBath\n    X['TotalRoom'] = df.TotRmsAbvGrd + df.FullBath + df.HalfBath\n    return X","metadata":{"id":"_JgcBG1ztqU3","execution":{"iopub.status.busy":"2021-10-29T23:13:47.567043Z","iopub.execute_input":"2021-10-29T23:13:47.568151Z","iopub.status.idle":"2021-10-29T23:13:47.580876Z","shell.execute_reply.started":"2021-10-29T23:13:47.5681Z","shell.execute_reply":"2021-10-29T23:13:47.580037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Grouped Transform","metadata":{"id":"TB_uDN-pzDK7"}},{"cell_type":"markdown","source":"The value of a home often depends on how it compares to typical homes in its neighborhood. Therefore, let's create a new feature called `MedNhbdArea` that describes the *median* of `GrLivArea` grouped on `Neighborhood`.","metadata":{"id":"aTj85upyzoLV"}},{"cell_type":"code","source":"def group_transforms(df):\n    X = pd.DataFrame()\n    X['MedNhbdArea'] = df.groupby('Neighborhood')['GrLivArea'].transform('median')\n    return X","metadata":{"id":"frv5B1ZIznhf","execution":{"iopub.status.busy":"2021-10-29T23:13:47.584829Z","iopub.execute_input":"2021-10-29T23:13:47.585475Z","iopub.status.idle":"2021-10-29T23:13:47.597679Z","shell.execute_reply.started":"2021-10-29T23:13:47.585428Z","shell.execute_reply":"2021-10-29T23:13:47.596888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### k-Means Clustering","metadata":{"id":"vmAydYXR0_Ou"}},{"cell_type":"markdown","source":"Let's use the help of an unsupervised algorithm (k-mean clustering) to create new features. We've selected the following features to determine what the clusters are based on.","metadata":{"id":"5HxF89O32TBd"}},{"cell_type":"code","source":"cluster_features = [\n    \"LotArea\",\n    \"TotalBsmtSF\",\n    \"FirstFlrSF\",\n    \"SecondFlrSF\",\n    \"GrLivArea\",\n]","metadata":{"id":"RtfRjP0F5bHV","execution":{"iopub.status.busy":"2021-10-29T23:13:47.598722Z","iopub.execute_input":"2021-10-29T23:13:47.599579Z","iopub.status.idle":"2021-10-29T23:13:47.610279Z","shell.execute_reply.started":"2021-10-29T23:13:47.599544Z","shell.execute_reply":"2021-10-29T23:13:47.609558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First, we will use the cluster labels generated by the algorithm as a new feature.","metadata":{"id":"QvLTZfJW5Ab1"}},{"cell_type":"code","source":"def cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new","metadata":{"id":"3iuITKlS5U0K","execution":{"iopub.status.busy":"2021-10-29T23:13:47.61485Z","iopub.execute_input":"2021-10-29T23:13:47.615402Z","iopub.status.idle":"2021-10-29T23:13:47.623501Z","shell.execute_reply.started":"2021-10-29T23:13:47.615349Z","shell.execute_reply":"2021-10-29T23:13:47.622684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Optimize number of clusters\n# # by comparing cross validation scores\n# # (Will take some time--like HOURS)\n\n# for n in list(range(1,21)):\n#     X_orig = df_train.copy().drop(\"SalePrice\", axis=1)\n#     X = cluster_labels(df_train, cluster_features, n_clusters=n)\n#     X = X_orig.join(X)\n#     score = score_dataset(X, y, xgb)\n#     print(\"Cross-validation score:\", score, \n#         \"\\n Value used for n_clusters (number of clusters) for labeling:n=\", n)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:13:47.624785Z","iopub.execute_input":"2021-10-29T23:13:47.625192Z","iopub.status.idle":"2021-10-29T23:13:47.636624Z","shell.execute_reply.started":"2021-10-29T23:13:47.625159Z","shell.execute_reply":"2021-10-29T23:13:47.635843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we will use the *distance* of the observations to each cluster as another new feature.","metadata":{"id":"eehQW7fV5GTl"}},{"cell_type":"code","source":"def cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd","metadata":{"id":"zImgTvoW5PYq","execution":{"iopub.status.busy":"2021-10-29T23:13:47.637786Z","iopub.execute_input":"2021-10-29T23:13:47.6382Z","iopub.status.idle":"2021-10-29T23:13:47.650809Z","shell.execute_reply.started":"2021-10-29T23:13:47.638153Z","shell.execute_reply":"2021-10-29T23:13:47.650158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Optimize number of clusters\n# # by comparing cross validation scores\n# # (Will take some time--like HOURS)\n\n# for n in list(range(1,21)):\n#     X_orig = df_train.copy().drop(\"SalePrice\", axis=1)\n#     X = cluster_distance(df_train, cluster_features, n_clusters=n)\n#     X = X_orig.join(X)\n#     score = score_dataset(X, y, xgb)\n#     print(\"Cross-validation score:\", score, \n#         \"\\n Value used for n_clusters (number of clusters) for labeling:n=\", n)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:13:47.651968Z","iopub.execute_input":"2021-10-29T23:13:47.652374Z","iopub.status.idle":"2021-10-29T23:13:47.663138Z","shell.execute_reply.started":"2021-10-29T23:13:47.652341Z","shell.execute_reply":"2021-10-29T23:13:47.662387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Principal Component Analysis","metadata":{"id":"airAceFT1ivS"}},{"cell_type":"markdown","source":"This time we'll use PCA, another unsupervised learning method, to create more new features.\n\n*Note: We are not including missing value indicator columns when assessing correlations because the combinations of rows having many 0s in these indicator columns will yield a NaN result when the given numerator and denominator are equal to 0 during the calculation.*","metadata":{"id":"ZDZE-LxC5xfg"}},{"cell_type":"code","source":"def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )\n\ncorrplot(df_train.iloc[:,:80], annot=None)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:13:47.664236Z","iopub.execute_input":"2021-10-29T23:13:47.664873Z","iopub.status.idle":"2021-10-29T23:13:49.061331Z","shell.execute_reply.started":"2021-10-29T23:13:47.664839Z","shell.execute_reply":"2021-10-29T23:13:49.060596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) / X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,        # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,          # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","metadata":{"id":"AeRUiBAD5wnq","execution":{"iopub.status.busy":"2021-10-29T23:13:49.062511Z","iopub.execute_input":"2021-10-29T23:13:49.062858Z","iopub.status.idle":"2021-10-29T23:13:49.074601Z","shell.execute_reply.started":"2021-10-29T23:13:49.062828Z","shell.execute_reply":"2021-10-29T23:13:49.073481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's pick a subset of features for PCA:","metadata":{"id":"zaZnYnTVAw2o"}},{"cell_type":"code","source":"pca_features = [\n    \"GarageArea\",\n    \"YearRemodAdd\",\n    \"TotalBsmtSF\",\n    \"GrLivArea\",\n]","metadata":{"id":"_NdGRrbgDjdX","execution":{"iopub.status.busy":"2021-10-29T23:13:49.076182Z","iopub.execute_input":"2021-10-29T23:13:49.077065Z","iopub.status.idle":"2021-10-29T23:13:49.087686Z","shell.execute_reply.started":"2021-10-29T23:13:49.077024Z","shell.execute_reply":"2021-10-29T23:13:49.086814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The PCA algorithm gives us *loadings* which describe each *component* of variation, and also the components which were the transformed datapoints. The loadings can suggest features to create. Additionally, we can use the components as features directly.","metadata":{"id":"ydygZ748MJ1U"}},{"cell_type":"code","source":"X_temp = X.loc[:, pca_features]\n\n# `apply_pca`, defined above\npca, X_pca, loadings = apply_pca(X_temp)\nprint(loadings)","metadata":{"id":"tZnHDmmlFaBe","outputId":"269dfb55-7730-482f-9887-0e8066d309eb","execution":{"iopub.status.busy":"2021-10-29T23:13:49.088937Z","iopub.execute_input":"2021-10-29T23:13:49.089746Z","iopub.status.idle":"2021-10-29T23:13:49.115185Z","shell.execute_reply.started":"2021-10-29T23:13:49.089697Z","shell.execute_reply":"2021-10-29T23:13:49.114338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot explained variance based on components from PCA\nplot_variance(pca)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:13:49.116771Z","iopub.execute_input":"2021-10-29T23:13:49.117217Z","iopub.status.idle":"2021-10-29T23:13:49.464367Z","shell.execute_reply.started":"2021-10-29T23:13:49.117183Z","shell.execute_reply":"2021-10-29T23:13:49.46375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since our goal right now is to discover as many useful features as possible, let's create features inspired by the PCA loadings while also using the exact components as a different set of features:","metadata":{"id":"WQbzE7t8Mywr"}},{"cell_type":"code","source":"def pca_inspired(df):\n    X = pd.DataFrame()\n    X[\"GrLivAreaPlusBsmtSF\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"RecentRemodLargeBsmt\"] = df.YearRemodAdd * df.TotalBsmtSF\n    return X\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca","metadata":{"id":"qVZOZZjUBkjo","execution":{"iopub.status.busy":"2021-10-29T23:13:49.465559Z","iopub.execute_input":"2021-10-29T23:13:49.465959Z","iopub.status.idle":"2021-10-29T23:13:49.471384Z","shell.execute_reply.started":"2021-10-29T23:13:49.465929Z","shell.execute_reply":"2021-10-29T23:13:49.470692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### PCA Application - Indicate Outliers","metadata":{"id":"QGSE0pGIRGGF"}},{"cell_type":"markdown","source":"Applying PCA can also help us determine houses that are outliers or houses with values that are not well represented by the rest of the data set.","metadata":{"id":"fIzudlUoRXNs"}},{"cell_type":"code","source":"sns.catplot(\n    y=\"value\",\n    col=\"variable\",\n    data=X_pca.melt(),\n    kind='boxen',\n    sharey=False,\n    col_wrap=2,\n);","metadata":{"id":"pbTgGVRISMyJ","outputId":"934f5c72-1e50-4068-ed74-c0d16534a984","execution":{"iopub.status.busy":"2021-10-29T23:13:49.4731Z","iopub.execute_input":"2021-10-29T23:13:49.473747Z","iopub.status.idle":"2021-10-29T23:13:50.300222Z","shell.execute_reply.started":"2021-10-29T23:13:49.473706Z","shell.execute_reply":"2021-10-29T23:13:50.29959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, for each of the components there are several points lying at the extreme ends of the distributions -- they are outliers. Let's see those houses that sit at the extremes of a component:","metadata":{"id":"ODHsLaPuSf35"}},{"cell_type":"code","source":"# Can change PC1 to PC2, PC3, or PC4\ncomponent = \"PC1\"\n\nidx = X_pca[component].sort_values(ascending=False).index\n\ndf_train[[\"SalePrice\", \"Neighborhood\", \"SaleCondition\"] + pca_features].iloc[idx]","metadata":{"id":"rYuI6Z4AShdS","outputId":"0828e0d9-3847-4b96-f63d-f61180ddb534","execution":{"iopub.status.busy":"2021-10-29T23:13:50.301125Z","iopub.execute_input":"2021-10-29T23:13:50.301642Z","iopub.status.idle":"2021-10-29T23:13:50.328969Z","shell.execute_reply.started":"2021-10-29T23:13:50.301607Z","shell.execute_reply":"2021-10-29T23:13:50.327979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that there are several dwellings listed as `Partial` sales in the `Edwards` neighborhood that stand out. A partial sale is what occurs when there are multiple owners of a property and one or more of them sell their \"partial\" ownership of the property. These kinds of sales often happen during the settlement of a family estate or the dissolution of a business and aren't advertised publicly, making these cases true outliers, especially within our supposed research context--houses on the open market.","metadata":{"id":"V3UnyndlX8KN"}},{"cell_type":"markdown","source":"Some models can benefit from having these outliers indicated, which is what this next transform will do.","metadata":{"id":"T-tJoRU0SMJA"}},{"cell_type":"code","source":"def indicate_outliers(df):\n    X_new = pd.DataFrame()\n    X_new[\"Outlier\"] = (df.Neighborhood == \"Edwards\") & (df.SaleCondition == \"Partial\")\n    return X_new","metadata":{"id":"2Lhj49gwRK7Z","execution":{"iopub.status.busy":"2021-10-29T23:13:50.33038Z","iopub.execute_input":"2021-10-29T23:13:50.331485Z","iopub.status.idle":"2021-10-29T23:13:50.337015Z","shell.execute_reply.started":"2021-10-29T23:13:50.331436Z","shell.execute_reply":"2021-10-29T23:13:50.336191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target Encoding","metadata":{"id":"ZWok7dimaAHt"}},{"cell_type":"markdown","source":"We're going to use target encoding using the following steps:\n1. Split the data into folds, each fold having two splits of the dataset.\n2. Train the encoder on one split but transform the values of the other.\n3. Repeat for all the splits.\n\nThe next cell contains a wrapper we can use with any target encoder:","metadata":{"id":"FZ796hJ8dc-7"}},{"cell_type":"code","source":"class CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"id":"RSSNP2zJc5q-","execution":{"iopub.status.busy":"2021-10-29T23:13:50.338776Z","iopub.execute_input":"2021-10-29T23:13:50.339339Z","iopub.status.idle":"2021-10-29T23:13:50.351968Z","shell.execute_reply.started":"2021-10-29T23:13:50.339279Z","shell.execute_reply":"2021-10-29T23:13:50.351266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note:\n\nTo use, follow code sample below:\n\n`encoder = CrossFoldEncoder(MEstimateEncoder, m=1)`\n\n`X_encoded = encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))`\n\n","metadata":{"id":"oo4RtHZtdHjm"}},{"cell_type":"markdown","source":"### Create Final Feature Set","metadata":{"id":"w35fFbiOegYV"}},{"cell_type":"markdown","source":"Time to combine everything together.","metadata":{"id":"bh9bYNkLeqP8"}},{"cell_type":"code","source":"def create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop('SalePrice')\n    mi_scores = make_mi_scores(X, y)\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Step 1: Drop features with low Mutual Information scores\n    X = drop_uninformative(X, mi_scores)\n\n    # Step 2: Add features from mathematical transforms \n    ######## (`LivLotRatio`, `Spaciousness`)\n    X = X.join(mathematical_transforms(X))\n\n    # Step 3: Add features from known interaction effects \n    ######## (categorical-`BldgType`and continuous-`GrLivArea`)\n    #X = X.join(interactions(X))\n\n    # Step 4: Add new feature from counts \n    ######## (`PorchTypes`, `TotalHalfBath`, `TotalRoom`)\n    X = X.join(counts(X))\n\n    # Step 5: Add new feature from group transform\n    ######## (median `GrLivArea` by `neighborhood`)\n    X = X.join(group_transforms(X))\n\n    # Step 6: Add features from k-means clustering \n    ######## (cluster labels, cluster distance)\n    #X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n    #X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n\n    # Step 7: Add features from PCA\n    ######## (loadings-inspired features , PCA components, & outlier indicators)\n    X = X.join(pca_inspired(X))\n    #X = X.join(pca_components(X, pca_features))\n    #X = X.join(indicate_outliers(X))\n  \n    # Label encoding for the categorical features\n    X = label_encode(X)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Step 8: Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X","metadata":{"id":"ZXMOHEXjfC06","execution":{"iopub.status.busy":"2021-10-29T23:13:50.353072Z","iopub.execute_input":"2021-10-29T23:13:50.353379Z","iopub.status.idle":"2021-10-29T23:13:50.369739Z","shell.execute_reply.started":"2021-10-29T23:13:50.353339Z","shell.execute_reply":"2021-10-29T23:13:50.368736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Putting the transformations into separate functions makes it easier to experiment with various combination.\n\nWe can modify any of these transformations or come up with some more ideas to add to the pipeline. At this stage, we have left the ones that gave the best results uncommented.","metadata":{"id":"bqz3VtNp24HH"}},{"cell_type":"code","source":"df_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, 'SalePrice']\n\nscore_dataset(X_train, y_train)","metadata":{"id":"2l2vp8mNoVim","outputId":"a1b29012-3167-45ab-eaed-13b8c81bac5c","execution":{"iopub.status.busy":"2021-10-29T23:13:50.370871Z","iopub.execute_input":"2021-10-29T23:13:50.371214Z","iopub.status.idle":"2021-10-29T23:13:54.953802Z","shell.execute_reply.started":"2021-10-29T23:13:50.371186Z","shell.execute_reply":"2021-10-29T23:13:54.953051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3 - Hyperparameter Tuning","metadata":{"id":"13fxNJQi3noK"}},{"cell_type":"markdown","source":"Now that we are done creating out final set of features, it's time to do some hyperparameter tuning with XGBoost to optimize our model performance further.","metadata":{"id":"ip3yv8yW47bz"}},{"cell_type":"code","source":"X_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb_params = dict(\n    max_depth=4,                           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.0058603076512435655,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=5045,                     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=2,                    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.22556099175248345,   # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.5632348136091383,          # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.09888625622197889,        # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=0.00890758697724437,         # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,                   # set > 1 for boosted random forests\n)\n\nxgb = XGBRegressor(**xgb_params)\nscore_dataset(X_train, y_train, xgb)","metadata":{"id":"ZizQXG9L5E5B","outputId":"bab545af-001b-4da4-d79c-48969dfb0e6b","execution":{"iopub.status.busy":"2021-10-29T23:13:54.955119Z","iopub.execute_input":"2021-10-29T23:13:54.956046Z","iopub.status.idle":"2021-10-29T23:14:39.352194Z","shell.execute_reply.started":"2021-10-29T23:13:54.956005Z","shell.execute_reply":"2021-10-29T23:14:39.351442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rather than just tuning them by hand, we're going to use a tuning library, Optuna, with XGBoost:","metadata":{"id":"b-MJqYBi_9BE"}},{"cell_type":"code","source":"# def objective(trial):\n#     xgb_params = dict(\n#         max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n#         learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n#         n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n#         min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n#         colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n#         subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n#         reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n#         reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n#     )\n#     xgb = XGBRegressor(**xgb_params)\n#     return score_dataset(X_train, y_train, xgb)\n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=40)\n# xgb_params = study.best_params","metadata":{"id":"SLPYeWDwAIu0","execution":{"iopub.status.busy":"2021-10-29T23:14:39.353491Z","iopub.execute_input":"2021-10-29T23:14:39.357511Z","iopub.status.idle":"2021-10-29T23:14:39.361933Z","shell.execute_reply.started":"2021-10-29T23:14:39.357457Z","shell.execute_reply":"2021-10-29T23:14:39.36073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optuna's recommended parameter values:\n\n\n```\nParameters: \n{'max_depth': 4, 'learning_rate': 0.0058603076512435655, 'n_estimators': 5045, 'min_child_weight': 2, 'colsample_bytree': 0.22556099175248345, 'subsample': 0.5632348136091383, 'reg_alpha': 0.09888625622197889, 'reg_lambda': 0.00890758697724437}. \nScore: 0.11442743288078303.\n```\n\n","metadata":{"id":"ujj0dmBiDwNJ"}},{"cell_type":"markdown","source":"# Part 4 - Train Model and Create Predictions","metadata":{"id":"K2mfpSRqA_fp"}},{"cell_type":"markdown","source":"To create our final predictions, we will take the following steps:\n* create your feature set from the original data\n* train XGBoost on the training data\n* use the trained model to make predictions from the test set\n* save the predictions to a CSV file\n\n","metadata":{"id":"-gyFjrw6BTRy"}},{"cell_type":"code","source":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb = XGBRegressor(**xgb_params)\n# XGB minimizes MSE, but we want to minimize RMSLE\n# So, we need to log-transform y to train and exp-transform the predictions\nxgb.fit(X_train, np.log(y))\npredictions = np.exp(xgb.predict(X_test))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})","metadata":{"id":"9TYEiKprBfeQ","outputId":"16118465-106a-4547-c0d9-570192f932c0","execution":{"iopub.status.busy":"2021-10-29T23:14:39.363181Z","iopub.execute_input":"2021-10-29T23:14:39.363442Z","iopub.status.idle":"2021-10-29T23:14:51.707877Z","shell.execute_reply.started":"2021-10-29T23:14:39.363412Z","shell.execute_reply":"2021-10-29T23:14:51.707161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save Submission\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your predictions are successfully saved!\")","metadata":{"id":"bjuSWh6-S5-y","outputId":"d98e18c6-dc54-4071-a81e-af4863a3fa97","execution":{"iopub.status.busy":"2021-10-29T23:14:51.709422Z","iopub.execute_input":"2021-10-29T23:14:51.709913Z","iopub.status.idle":"2021-10-29T23:14:51.723423Z","shell.execute_reply.started":"2021-10-29T23:14:51.709876Z","shell.execute_reply":"2021-10-29T23:14:51.722468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the XGB model\nfilename = 'ames_house_xgb_model.pkl'\npickle.dump(xgb, open(filename, 'wb'))\n\n# Save processed test data\nX_test.to_csv('df_test_processed.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:14:51.724627Z","iopub.execute_input":"2021-10-29T23:14:51.725246Z","iopub.status.idle":"2021-10-29T23:14:52.114927Z","shell.execute_reply.started":"2021-10-29T23:14:51.725212Z","shell.execute_reply":"2021-10-29T23:14:52.113861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 5 - Model Interpretation","metadata":{}},{"cell_type":"markdown","source":"Our final model landed in the top 8% of Kaggle House Prices Prediction leaderboard (as of 10/24/2021). Groovy! However, it's important for us to take it a step further.\n\n> Many people say machine learning models are \"black boxes\", in the sense that they can make good predictions but you can't understand the logic behind those predictions. This statement is true in the sense that most data scientists don't know how to extract insights from models yet.\n\nThere is an increasing need for data scientists who are able to extract insights from sophisticated machine learning models to help inform human decision-making. Some decisions are made automatically by models like the ones we have just built, but many important decisions are made by humans. For these decisions, insights can be more valuable than predictions. Beyond informing human decision-making, insights extracted from machine learning models have many other uses, including:\n- Debugging\n- Informing feature engineering\n- Directing future data collection\n- Building Trust\n\nRight now, we want to answer the following questions about our model:\n* What features in the data did the model think are most important?\n* For any single prediction from a model, how did each feature in the data affect that particular prediction?\n* How does each feature affect the model's predictions in a big-picture sense (what is its typical effect when considered over a large number of possible predictions)?","metadata":{}},{"cell_type":"markdown","source":"First, we'll use SHAP Values to explain individual predictions. Afterwards, we will look at model-level insights.","metadata":{}},{"cell_type":"code","source":"# Pick an arbitrary row (first row starts at 0)\nrow_to_show = 42\ndata_for_prediction = X_test.iloc[[row_to_show]]\n\n# Generate prediction\ny_sample = np.exp(xgb.predict(data_for_prediction))\n\n# Create object that can calculate Shap values\nexplainer = shap.TreeExplainer(xgb)\n\n# Calculate Shap values from prediction\nshap_values = explainer.shap_values(data_for_prediction)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:14:52.186699Z","iopub.execute_input":"2021-10-29T23:14:52.187078Z","iopub.status.idle":"2021-10-29T23:14:57.046912Z","shell.execute_reply.started":"2021-10-29T23:14:52.187048Z","shell.execute_reply":"2021-10-29T23:14:57.046022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For a single prediction, what features in the data did the model think are most important?**","metadata":{}},{"cell_type":"code","source":"plt.title('Feature importance based on SHAP values?')\nshap.summary_plot(shap_values, data_for_prediction, plot_type=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:14:57.050788Z","iopub.execute_input":"2021-10-29T23:14:57.051249Z","iopub.status.idle":"2021-10-29T23:14:57.459286Z","shell.execute_reply.started":"2021-10-29T23:14:57.051209Z","shell.execute_reply":"2021-10-29T23:14:57.458602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**How did each feature in the data affect that particular prediction?**","metadata":{}},{"cell_type":"code","source":"plt.title('Feature impact on model output (feature impact in details below)')\nshap.summary_plot(shap_values, data_for_prediction)\n\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:14:57.460453Z","iopub.execute_input":"2021-10-29T23:14:57.460877Z","iopub.status.idle":"2021-10-29T23:14:58.504982Z","shell.execute_reply.started":"2021-10-29T23:14:57.460845Z","shell.execute_reply":"2021-10-29T23:14:58.504247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we've seen the inner workings of our model in making an individual prediction, let's aggregate all the information into powerful model-level insights.","metadata":{}},{"cell_type":"code","source":"# Use test set to get predictions\ndata_for_prediction = X_test\n\n# Generate predictions\ny_sample = np.exp(xgb.predict(data_for_prediction))\n\n# Create object that can calculate Shap values\nexplainer = shap.TreeExplainer(xgb)\n\n# Calculate Shap values from predictions\nshap_values = explainer.shap_values(data_for_prediction)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:14:58.505986Z","iopub.execute_input":"2021-10-29T23:14:58.506192Z","iopub.status.idle":"2021-10-29T23:15:10.640635Z","shell.execute_reply.started":"2021-10-29T23:14:58.506167Z","shell.execute_reply":"2021-10-29T23:15:10.63991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**How does each feature affect the model's predictions in a big-picture sense? In other words, what is its typical effect when considered over a large number of possible predictions?**","metadata":{}},{"cell_type":"code","source":"plt.title('Feature impact on overall model output (feature impact in details below)')\nshap.summary_plot(shap_values, data_for_prediction)\n\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction)","metadata":{"execution":{"iopub.status.busy":"2021-10-29T23:15:10.642376Z","iopub.execute_input":"2021-10-29T23:15:10.642971Z","iopub.status.idle":"2021-10-29T23:15:24.564622Z","shell.execute_reply.started":"2021-10-29T23:15:10.642923Z","shell.execute_reply":"2021-10-29T23:15:24.562131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"_____\n\n## Acknowledgement\n\nSteps taken throughout the model-building process in this notebook are inspired by [this Kaggle notebook](https://www.kaggle.com/ryanholbrook/feature-engineering-for-house-prices) by Ryan Holbrook and Alexis Cook (modified for better performance). Check out their notebook for more ideas to improve the prediction model.\n\nSome text in the beginning of the Model Interpretation section is copied from Kaggle's fantastic [Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability) course.\n\nOther quoted sources include [Business Data Science](https://www.amazon.com/Business-Data-Science-Combining-Accelerate/dp/1260452778) by Matt Taddy.","metadata":{}}]}