{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Thanks for passing by this kernel , this is Marvin Garcia, Giving another approach to this Data set , It is a bit large becasue I went through all the variables*** \n1. You will find that I divided them into Numerical (integer, float) and Categorical (ordinal,Nominal)\n2. I worked train-test combined as data, and train set parallely, because I wanted to make sure about variables/SalePrice correlation\n3. I tried to fill the missing values as much as I can\n4. I only used Linear Regression and Xboost Because I wanted to prove that the data processing it is more important than the algorithm , then I used Xboost Randomsearch Cv for tunning for a better result \n5. I will keep working on this data set and try another algorithms... \n6. anyfeedback, comment it is more than welcome \n7. One more time thanks to all kaggle's member I have learned a lot by doing this \n8. take this notebook and if you want try to make some changes remove more variable (becaue there are more variable that can be removed and outlier)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Analyses \nimport pandas as pd \nimport numpy as np \nimport math, re\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n#Visualization \nimport seaborn as  sns\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom nltk.tokenize import sent_tokenize,word_tokenize\n%matplotlib inline \nsns.set_style('whitegrid')\n\n\n#\nfrom sklearn.preprocessing import LabelEncoder\npd.options.mode.chained_assignment = None\n\nfrom IPython.display import display\nfrom PIL import Image\npath=('../input/houses-2/houses.jfif')\ndisplay(Image.open(path))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Special Thanks to @Mehmet Sungur, the box plot, histogram were taken from his notebook .. Awesome..!!!"},{"metadata":{},"cell_type":"markdown","source":"# A. Let's Start With the Target Variable SalePrice"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#-->Importing Data\ntrain= pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest= pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\n\n#---> Concat iloc[:,:-1] is excluding 'SalePrice'\ndata  = pd.concat([train,test], axis = 0 , sort = False).reset_index(drop=True)\ndata.drop('SalePrice', axis =1 , inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# --- > General Fuctions\n\ndef multi_plotting (df, feature): \n\n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Histogram')\n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('QQ_plot')\n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ax3 = fig.add_subplot(grid[:, 2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n\n    print(\"Skewness: \"+ str(train['SalePrice'].skew().round(3))) \n    print(\"Kurtosis: \" + str(train['SalePrice'].kurt().round(3)))\n\n#---> Analyzing the Target (what I want to predict)   ** SalePrice ** \n    \nmulti_plotting (train,'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can noticed skewness (large tail - right - therefore we have to standardize)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming 'Sale Price' using log1p\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nmulti_plotting (train,'SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### B. Dropping Columns\n1. Filter columns , if they have more than 60 % Missing/Nan , They Will be Dropped \n2. I will work Parellaly Train and Data Set, I can work with Data but I have to define 'saleprice'.notnull\n3. The Data Description Mentioned all NAN values can be interprate as NONE / Dont have "},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# --> Columns missing Rate\n\ndef missing_rate(frame):\n    TL = frame.isna().sum()/len(frame)  # Total len\n    TCMR = TL[TL >0.6].index  #Total % Missing Rate in Columns\n    frame.drop(TCMR, axis= 1 , inplace = True)\n    print(f'Colums to be dropped:{TCMR}')\n    \n# --> Checking trainand Data \nmissing_rate(train)\nmissing_rate(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  C. Understanding Variables \n1. The key Issue Here is to Choose The Best Variables, however the \"0\" and Nan Values make it hard, therefore we must fill them and them drop no significant variables.\n2. Basically we have to distinguishe Between A nominal variable which has no intrinsic ordering to its categories. For example, gender is a categorical variable having two categories (male and female). An ordinal variable has a clear ordering.For example, temperature as a variable with three orderly categories (low, medium and high)\n3. Categorical variable must be transformed into numerical value 1,2,3,4 etc , but It has to make sense *** I.e \"PoolQC\" Ex=4, Ta=2 etc, because the model will understand that 4 is greater than 3 and will affect the final output but \"MiscFeature\" has tennis Court, Garage ,Elevator ,these tags are not related so The best approach will be OnehotEncoder ***\n4. Fill columns with Numerical Values"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# --> General Fuctions \n\ndef corr_heat (frame):   #<---Heat Map\n    correlation = frame.corr()\n    f,ax = plt.subplots(figsize=(30,20))\n    mask = np.triu(correlation)\n    sns.heatmap(correlation, annot=True, mask=mask,ax=ax,cmap='viridis')\n    bottom,top = ax.get_ylim()\n    ax.set_ylim(bottom+ 0.5, top - 0.5)\n    \ndef box_plot (column1,column2,column3,data):  #<-- Box_plot\n    f,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(15,5))\n    sns.boxplot(y='SalePrice', x=column1 , data=data,ax=ax1)\n    sns.boxplot(y='SalePrice', x=column2 , data=data,ax=ax2)\n    sns.boxplot(y='SalePrice', x=column3 , data=data,ax=ax3)\n    \ndef finding_zeros (frame): #<--- Finding 0 Values\n    Zeros = frame[frame==0].count().sort_values(ascending=False)\n    print(Zeros,\"/\" ,end ='')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Numeric & Float  *** ---> Into the Battle Field ***"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# -- > Extracting the Columns Name \nnumeric = data.dtypes[data.dtypes != 'object'].index\ntnumeric = train.dtypes[train.dtypes != 'object'].index\n\n# Creating sub_dataset  - all columsn typed integer / float\ninteger = data[numeric]\ntinteger = train[tnumeric]\n\n# Filling Missing Values \ninteger.fillna(0, inplace=True)\ntinteger.fillna(0 , inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_heat(tinteger)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***The heatmap shows many columns with strongl correlation, I.e Yearbuild/GarageBuild *** the target is to reduce the dimensionality (number of columns), let's Find the columns correlated and then sort by the Sale price correlation"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"FC = tinteger.corr() # Features Correlation\nTarget = tinteger.corr()['SalePrice'].to_frame().reset_index() #Feature Correlation related to SalePrice\nFR = FC.unstack().to_frame(name='Correlation') # Feature Relation\nFeature = FR[(FR['Correlation']>=0.8)&(FR['Correlation']<1)].sort_values(by='Correlation', ascending = False).reset_index()\nFeature.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"Final  = Feature.merge(Target,left_on='level_1', right_on='index')\nFinal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will Drop the following Columns because they are strongly Related , but btween them I will chose the one with stronger relation with Saleprice, (iD will be dropped as well)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tinteger.drop(['GarageArea','GarageYrBlt','TotRmsAbvGrd','Id'],axis=1, inplace=True)\ninteger.drop(['GarageArea','GarageYrBlt','TotRmsAbvGrd','Id'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.1. Feature Engineer --  Dealing with 0 values "},{"metadata":{"trusted":true},"cell_type":"code","source":"t = tinteger[tinteger==0].count().sort_values(ascending=False).head(25)\nd = integer[integer==0].count().sort_values(ascending=False).head(25)\nsource = {'Train':t, 'Data':d}\nZeros = pd.DataFrame.from_dict(source)\nZeros.sort_values(by=['Train','Data'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.2. Dealing with ***'3SsnPorch','ScreenPorch','EnclosedPorch','OpenPorchSF'*** which is the Area outside the House \n1. 24 with 3SsnPorch, 116 ScreenPorch , 804 OpenPorchSf, i will sum all of them and then create a columns  Have/No have (0/1)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#--> Train Dealing with Porch \n\ntinteger['Porch'] = tinteger['3SsnPorch']+tinteger['ScreenPorch']+tinteger['EnclosedPorch']+tinteger['OpenPorchSF']\ntinteger['YNPorch'] = tinteger['Porch'].apply(lambda x : 1 if x>0 else 0)\ntinteger.drop(['3SsnPorch','ScreenPorch','EnclosedPorch','OpenPorchSF','PoolArea'], axis= 1, inplace=True)\n\n# --> Data\n\ninteger['Porch'] = integer['3SsnPorch']+integer['ScreenPorch']+integer['EnclosedPorch']+integer['OpenPorchSF']\ninteger['YNPorch'] = integer['Porch'].apply(lambda x : 1 if x>0 else 0)\ninteger.drop(['3SsnPorch','ScreenPorch','EnclosedPorch','OpenPorchSF','PoolArea'], axis= 1, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tinteger","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.3. Dealing With **'BsmtFinSF1','BsmtFinSF2','BsmtUnfSF' = TotalBsmtSF *** Please notice the dependency \n1. BsmtFinSF1 +BsmtFinSF2+ BsmtUnfSF = 'TotalBsmtSF' , now I will Create a column \"Bsmntbuilt\" , \n"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tinteger[['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF']].tail(5)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def cat_basmentfin (df):\n\n    # Creating New Column\n    df['Bsmntbuilt'] = 0\n\n    # --> 2 =\"Built (type1/type2)\"  1 = \"no built but available\" and 0 means No Basement \n    A= df[(df['BsmtFinSF1']>0) | (df['BsmtFinSF2']>0) & (df['BsmtUnfSF'] >= 0)].index\n    B= df[(df['BsmtFinSF1']==0) & (df['BsmtFinSF2']==0) & (df['BsmtUnfSF'] > 0)].index\n    C= df[(df['BsmtFinSF1']==0) & (df['BsmtFinSF2']==0) & (df['BsmtUnfSF'] == 0)].index\n\n    # --> Replacing \n    df.loc[A, 'Bsmntbuilt'] = 2\n    df.loc[B, 'Bsmntbuilt'] = 1\n    df.loc[C, 'Bsmntbuilt'] = 0\n    \n    #Dropping\n    df.drop(['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF'],axis= 1 , inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tinteger.iloc[:,10:]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Categorizing Basement built\n\ncat_basmentfin(tinteger) #---> Train \ncat_basmentfin(integer) #---> data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.4.  Dealing with With FullBath/HalfBath\n1. FullBath.unique() = [0,1,2,3] and HalflBath.unique(0,1,2) , After iterating all the Combination this is the final Result"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def basmentbath (df):  \n    \n    No_BsmtFullBath=df[(df['BsmtFullBath']==0) & (df['BsmtHalfBath']==0)].index\n    Only_halfBath=df[(df['BsmtFullBath']==0) & (df['BsmtHalfBath']==1)].index\n    Two_halfBath =df[(df['BsmtFullBath']==0) & (df['BsmtHalfBath']==2)].index\n    Only_FullBath=df[(df['BsmtFullBath']==1) & (df['BsmtHalfBath']==0)].index\n    One_Full_Half = df[(df['BsmtFullBath']==1) & (df['BsmtHalfBath']==1)].index\n    Two_Full = df[(df['BsmtFullBath']==2) & (df['BsmtHalfBath']==0)].index\n    Three_Full = df[(df['BsmtFullBath']==3) & (df['BsmtHalfBath']==0)].index\n\n    # --> 9. Creating new Columns and Replacing\n    df['BsmtBathCat'] = 0\n\n    #--> Replacing 6=3 FullBath , 5=2 FullBath, 4=1Full/1Half, 3= 1FullBath, 5=2 HalfBath , 1=1 Half, 7=No Bath in Basement \n    df.loc[No_BsmtFullBath, 'BsmtBathCat'] = 0\n    df.loc[Only_halfBath, 'BsmtBathCat'] = 1\n    df.loc[Two_halfBath, 'BsmtBathCat'] = 2\n    df.loc[Only_FullBath, 'BsmtBathCat'] = 3\n    df.loc[One_Full_Half, 'BsmtBathCat'] = 4\n    df.loc[Two_Full, 'BsmtBathCat'] = 5\n    df.loc[Three_Full, 'BsmtBathCat'] = 6\n    \n    #Dropping\n    df.drop(['BsmtFullBath','BsmtHalfBath'],axis= 1 , inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorizing Basement Bath\n\nbasmentbath(tinteger) #---> Train \nbasmentbath(integer) #---> data","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tinteger.iloc[:,13:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.5.  Dealing with 2nd floor \n1. 1stFlrSf + 2ndFlrSf = 'GrLivArea' , then I will Create a Column \"1st_2nd_floor , where 0/1 = Have/no Have (2nd floor)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"integer['2ndFloor']=integer['2ndFlrSF'].apply(lambda x: 0 if x==0 else 1)\ntinteger['2ndFloor']=tinteger['2ndFlrSF'].apply(lambda x: 0 if x==0 else 1)\n\ninteger.drop(['1stFlrSF','2ndFlrSF'],axis =1, inplace = True)\ntinteger.drop(['1stFlrSF','2ndFlrSF'],axis =1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# ---> train \ntinteger= tinteger.join(train['Neighborhood'])\nlot_mean = tinteger.groupby(\"Neighborhood\")[\"LotFrontage\"].mean().round().to_dict()\ntinteger.loc[(tinteger['LotFrontage']==0) & (tinteger['Neighborhood'].isin(lot_mean.keys())), 'LotFrontage'] = tinteger['Neighborhood'].map(lot_mean)\ntinteger.drop('Neighborhood',axis=1 , inplace=True)\n\n# ---> test\ninteger= integer.join(data['Neighborhood'])\nlot_mean = integer.groupby(\"Neighborhood\")[\"LotFrontage\"].mean().round().to_dict()\ninteger.loc[(integer['LotFrontage']==0) & (integer['Neighborhood'].isin(lot_mean.keys())), 'LotFrontage'] = integer['Neighborhood'].map(lot_mean)\ninteger.drop('Neighborhood',axis=1 , inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"tinteger.iloc[:,13:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.7. Dealing with LowQualFinSF,  MiscVal\n1. LowQualFinSF: Low quality finished square feet (all floors) so if it is 0 is because has been built (so I iwll Drop it)\n2. MiscVal: $Value of miscellaneous feature, Well there is no way that we can know that (drop)"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"integer.drop(['LowQualFinSF','MiscVal'],axis =1, inplace = True)\ntinteger.drop(['LowQualFinSF','MiscVal'],axis =1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1.7. Final View"},{"metadata":{"trusted":true},"cell_type":"code","source":"t = tinteger[tinteger==0].count().sort_values(ascending=False).head(14)\nd = integer[integer==0].count().sort_values(ascending=False).head(14)\nsource = {'Train':t, 'Data':d}\nZeros = pd.DataFrame.from_dict(source)\nZeros.sort_values(by=['Train','Data'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_heat(tinteger)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('MoSold','YrSold','KitchenAbvGr',tinteger)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"box_plot('OverallCond','MSSubClass','KitchenAbvGr',tinteger)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can Notice that there is no relation among this variables , instead seems to behave as a constant look at the Yr Sold/MoSold there is no significant changes so i will Drop ***'MoSold','YrSold','KitchenAbvGr'***"},{"metadata":{"trusted":true},"cell_type":"code","source":"integer.drop(['MoSold','YrSold','KitchenAbvGr'],axis =1, inplace = True)\ntinteger.drop(['MoSold','YrSold','KitchenAbvGr'],axis =1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Categorical Ordinal \n1. I will evaluated from 0-5 / 0-6 in some cases   \n2. I created Dicctionaties to replace the values based on the Description\n3. OnelabelEncoder can do this but it will give them random weight and that would affect the final model"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"categorical = data[['KitchenQual','FireplaceQu','GarageQual','GarageFinish','GarageCond','BsmtCond','BsmtExposure','BsmtQual','BsmtFinType2',\n                    'BsmtFinType1','ExterCond','ExterQual','HeatingQC','LandSlope','LotShape','PavedDrive','Street',\n                    'BldgType','Functional','SaleType','GarageType','Electrical','Foundation','LandContour','RoofMatl','RoofStyle']]\n\n\ntcategorical = train[['SalePrice','KitchenQual','FireplaceQu','GarageQual','GarageFinish','GarageCond','BsmtCond','BsmtExposure','BsmtQual','BsmtFinType2',\n                     'BsmtFinType1','ExterCond','ExterQual','HeatingQC','LandSlope','LotShape','PavedDrive','Street',\n                     'BldgType','Functional','SaleType','GarageType','Electrical','Foundation','LandContour','RoofMatl','RoofStyle']]\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# ---->> Creating Dicctionaty (I created with nltk)\n\nkit ={\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1,\"None\":0}               #KitchenQual: Kitchen quality \n#alley = {\"Grvl\":1,\"Pave\":2,\"None\":0}                      #Alley: Type of alley access to property \nfire ={\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1,\"None\":0}       #FireplaceQu: Fireplace quality\ngaragequa ={\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1,\"None\":0}     #GarageQual: Garage quality \ngarage_finish = {\"Fin\":3,\"RFn\":2,\"Unf\":1,\"None\":0}        #GarageFinish: Interior finish of the garage\ngaragecond= {\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1,\"None\":0} #GarageCond: Garage condition \nbsmtcond= {\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1,\"None\":0}          #BsmtCond: Evaluates the general condition of the basement\nbsmtexposure = {\"Gd\":4,\"Av\":3,\"Mn\":2,\"No\":1,\"None\":0}     #BsmtExposure: Refers to walkout or garden level walls\nbsmtqual = {\"Ex\":4,\"Gd\":3,\"TA\":2,\"Fa\":1,\"None\":0}         #BsmtQual: Evaluates the height of the basement\nbsmtype2={\"GLQ\":6,\"ALQ\":5,\"BLQ\":4,\"Rec\":3,\"LwQ\":2,\"Unf\":1,\"None\":0}   #BsmtFinType2: Rating of basement finished area (if multiple types)\nbsmtype1 = {\"GLQ\":6,\"ALQ\":5,\"BLQ\":4,\"Rec\":3,\"LwQ\":2,\"Unf\":1,\"None\":0} #BsmtFinType1: Rating of basement finished area\n#poolqc={\"Ex\":3,\"Gd\":2,\"Fa\":1,\"None\":0}                    #PoolQC: Pool quality\nextercond = {\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1}         #ExterCond: Evaluates the present condition of the material on the exterior\nexterqual =  {\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1}        #ExterQual: Evaluates the quality of the material on the exterior\n#fence = {\"GdPrv\":4,\"MnPrv\":3,\"GdWo\":2,\"MnWw\":1, \"None\":0}   #Fence: Fence quality\nheatingqc = {\"Ex\":5,\"Gd\":4,\"TA\":3,\"Fa\":2,\"Po\":1}         #HeatingQC: Heating quality and condition\nlandslope={\"Gtl\":3,\"Mod\":2,\"Sev\":1}                        #LandSlope: Slope of property\nlotshape={\"Reg\":4,\"IR1\":3,\"IR2\":2,\"IR3\":1}                #LotShape: General shape of property\npavedrive= {\"Y\":3,\"P\":2,\"N\":1}                            #PavedDrive: Paved driveway\nstreet={\"Grvl\":2,\"Pave\":1}                                #Street: Type of road access to property\nbldgtype = {'1Fam': 1, '2fmCon': 2, 'Duplex': 3, 'TwnhsE': 4, 'Twnhs': 5}\nfunctional = {'Typ': 8,'Min1': 7,'Min2': 6,'Mod': 5,'Maj1': 4,'Maj2': 3,'Sev': 2,'Sal': 1,'None':1}\nsaletype ={'WD': 10,'CWD': 9,'VWD': 8,'New': 7,'COD': 6,'Con': 5,'ConLw': 4,'ConLI': 3,'ConLD': 2,'Oth': 1,'None':0}\ngarage={'2Types': 6, 'Attchd': 5, 'Basment': 4, 'BuiltIn': 3, 'CarPort': 2, 'Detchd': 1, 'None': 0}\nelectrical={'SBrkr': 5, 'FuseA': 4, 'FuseF': 3, 'FuseP': 2, 'Mix': 1,'None':0}\nfoundation={'BrkTil': 6, 'CBlock': 5, 'PConc': 4, 'Slab': 3, 'Stone': 2, 'Wood': 1}\nlandcontour={'Lvl': 4, 'Bnk': 3, 'HLS': 2, 'Low': 1}\nroofmatl={'ClyTile': 8, 'CompShg': 7, 'Membran': 6, 'Metal': 5, 'Roll': 4, 'Tar&Grv': 3, 'WdShake': 2, 'WdShngl': 1}\nroofstyle ={'Flat': 6, 'Gable': 5, 'Gambrel': 4, 'Hip': 3, 'Mansard': 2, 'Shed': 1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ---> Replacing Values General fuction\nfor i in tcategorical:  #-->Train\n    tcategorical[i].fillna('None',inplace=True)\n    \nfor i in categorical: #--> Test\n    categorical[i].fillna('None',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# --> a quick glance \n\nfor i in tcategorical.columns[1:].to_list():  #---> Train\n    v= tcategorical[i].unique()\n    print(i, v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -->  Joining Dict \n\nlist_dict =[kit,fire,garagequa,garage_finish,garagecond,bsmtcond,bsmtexposure,bsmtqual,bsmtype2,\n            bsmtype1,extercond,exterqual,heatingqc,landslope,lotshape,pavedrive,street,\n            bldgtype,functional,saletype,garage,electrical,foundation,landcontour,roofmatl,roofstyle]\n    \n#-->  Replacing using Loop + Fuction + Dicct  \n\ndef replace_t (i,diccionaty): #fuction for Train \n    tcategorical[i].replace(diccionaty, inplace=True) \n    \ndef replace (i,diccionaty): #fuction for data \n    categorical[i].replace(diccionaty, inplace=True) \n\n# --->  Replacing / Mapping values   \n    \nfor i,j in zip(tcategorical.columns[1:].to_list() ,list_dict):  #[1:] because Im not inclueding SalePrice\n    replace_t(i,j)\n    \nfor i,j in zip(categorical.columns.to_list() ,list_dict):  #[1:] because Im not inclueding SalePrice\n    replace(i,j)\n    \nfor i in categorical:\n    v= categorical[i].unique()\n    print(i, v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_heat(tcategorical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Before going foward I will  try to reduce the dimensinalty by dropping those columns that has many 0 / None"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tcategorical:\n    v = tcategorical[i].value_counts().to_frame().reset_index().iloc[0].to_list()\n    w= tcategorical[i].value_counts().reset_index().iloc[0][0]\n    if w == 0:\n        print(i,v)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tcategorical.drop('FireplaceQu',axis=1, inplace=True)\ncategorical.drop('FireplaceQu',axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Categorical Nominal \n1. From this point I will work with Nominal (data)"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"nominal =data[['Utilities','Exterior1st','Exterior2nd','MasVnrType',\n                     'MSZoning','CentralAir','Condition1','Condition2','Heating','HouseStyle','LotConfig',\n                     'Neighborhood','SaleCondition']]\n\n\n#--> Finding Null Values \nnominal.isnull().sum().head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# -- using Label Encoder \n\nfor i in nominal:\n    encoding = pd.get_dummies(nominal[i], prefix=i, drop_first= True)\n    nominal= pd.concat([nominal,encoding],axis=1)\n    nominal.drop(i,axis=1, inplace=True)\n\nnominal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#--> Removing Feature with low Variance\n\nall_features = nominal.keys()\n# Removing features.\nnominal = nominal.drop(nominal.loc[:,(nominal==0).sum()>=(nominal.shape[0]*0.9994)],axis=1)\nnominal = nominal.drop(nominal.loc[:,(nominal==1).sum()>=(nominal.shape[0]*0.9994)],axis=1) \n# Getting and printing the remaining features.\nremain_features = nominal.keys()\nremov_features = [st for st in all_features if st not in remain_features]\nprint(len(remov_features), 'features were removed:', remov_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Combining Data Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(integer.shape, categorical.shape, nominal.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_data = pd.concat([integer,categorical,nominal], axis = 1 , sort=False)\nnew_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data =new_data.iloc[:1460].reset_index(drop=True)\ntest_data = new_data.iloc[1460:].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Machine Learning Section\n1. It is important to use np.expm1 at the predicttion because of the transformation done in the very beggining "},{"metadata":{},"cell_type":"markdown","source":"## 5.1. Linear Regression "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nX = train_data\ny = train['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nLR = LinearRegression().fit(X_train,y_train)\ny_pred_LR = LR.predict(test_data)\n\n\ntest_Id=test['Id']\nmy_submission = pd.DataFrame({'Id': test_Id, 'SalePrice': np.expm1(y_pred_LR)})\nmy_submission.to_csv('KaggelsubmissionLR.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"with Linear Regression I got 0.13 error rate in Kaggel. "},{"metadata":{},"cell_type":"markdown","source":"## 5.2. XgBoost.Regressor  "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\ntrain_data = train_data.astype(int)\ntest_data = test_data.astype(int)\n\nX = train_data\ny = train['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost \nclassifier = xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0,\n             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n             max_depth=2, min_child_weight=4, missing=None, n_estimators=900,\n             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=0)\n\nclassifier = classifier.fit(X_train,y_train)\ny_pred_XG = classifier.predict(test_data)\nsub = pd.DataFrame()\nsub = pd.DataFrame()\nsub['Id'] = test['Id']\nsub['SalePrice'] = np.expm1(y_pred_XG)\nsub.to_csv('KaggelXBoostsubmission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"with Linear Regression I got 0.12 error rate in Kaggel. "},{"metadata":{},"cell_type":"markdown","source":"## 5.3. Other\n1. Im still working on that "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}