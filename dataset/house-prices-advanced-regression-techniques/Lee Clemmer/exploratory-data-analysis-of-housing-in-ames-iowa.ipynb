{"metadata":{"language_info":{"codemirror_mode":{"version":3,"name":"ipython"},"name":"python","file_extension":".py","nbconvert_exporter":"python","version":"3.6.1","mimetype":"text/x-python","pygments_lexer":"ipython3"},"hide_input":false,"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"hide_code_all_hidden":false},"nbformat":4,"nbformat_minor":1,"cells":[{"source":"Author: Lee Clemmer\n\n# Introduction\n\nIn this notebook we'll explore the Ames, Iowa Housing Dataset, prepared by Dean De Cock, and described in the paper *[Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf)*.\n\nWe start by conducting some exploratory data analysis followed by processing the data based on our findings. The goal is ultimately to accurately predict the sales price of houses in the test set given the data in the training set.\n\n# Acknowledgements\n\nThis notebook marks the end of my lurker status on Kaggle and my first foray into publishing kernels here. I'd like to call out several other notebooks that were tremendously valuable in helping me put this together, generate ideas, and learn a lot.\n\n* **[Comprehensive data exploration with Python](https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python)** by Pedro Marcelina (most entertaining notebook ever btw)\n* **[A study on Regression applied to the Ames dataset](https://www.kaggle.com/juliencs/a-study-on-regression-applied-to-the-ames-dataset)** by juliencs\n* **[House Prices EDA](https://www.kaggle.com/dgawlik/house-prices-eda)** by Dominik Gawlik\n* **[Stacked Regressions : Top 4% on LeaderBoard](https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard)** by Serigne\n* **[Python Machine Learning (Book)](https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka/dp/1783555130)** by Sebastiona Raschka\n\nOf course a big shout out to the Google and StackOverflow for promptly delivering answers to my many questions.\n\n# Table of Contents\n\n* Discussion and Background\n* Intuition\n* Univariate Analysis\n* Multivariate Analysis\n* Data Preprocessing\n* Modeling","metadata":{"_cell_guid":"4f2848a6-f17a-4bb5-b3a0-7c91bd0ca7d7","hidePrompt":false,"hideCode":false,"_uuid":"caf2302222ae7347af85de26f61cca11dd1ebbbe"},"cell_type":"markdown"},{"source":"# Discussion and Background\n\nWithout doing any analysis of the data itself we can learn a lot by simply reading the documentation:\n* [Original dataset](https://ww2.amstat.org/publications/jse/v19n3/decock/AmesHousing.txt)\n* [Dataset documentation](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt)\n\nFrom the documentation, we learn that the there are **2,930 observations** and **82 variables**, two of which (Order and Parcel ID) are not included in the dataset given in this competition. \n\n*Sidebar:* The Parcel ID actually allows us to look up individual parcels on the [Ames Assessor's Office's website](http://www.cityofames.org/government/departments-divisions-a-h/city-assessor) (click on \"Property Search\" and put in the PID on the search page). [Here](https://beacon.schneidercorp.com/Application.aspx?AppID=165&LayerID=2145&PageTypeID=4&PageID=1108&Q=1957777753&KeyValue=0526301100), for example, is the parcel of Obervation No. 1 in the original dataset. The Assessor's Office, incidentially, is also the source of the data itself. \n\nThe properties were sold in Ames, IA from 2006 to 2010. This raises some questions, such as \"do properties appear more than once if there were repeatedly sold in the time period?\" or \"does the data include time of sale?\" As I go through the data I like to keep a list of open questions that I can come back to later. \n\nBefore we dive into the variables, let's look at the *Special Notes* section of the documentation, which informs us: \n> There are 5 observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will indicate them quickly). Three of them are true outliers (Partial Sales that likely don’t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these 5 unusual observations) before assigning it to students.\n\nLet's be sure to check out these outliers and consider whether we want to in fact exclude any houses bigger than 4,000 sq ft.\n\nFinally, the documentation also references a couple of papers that may help us with our analysis:\n* [Kuiper , S. (2008), “Introduction to Multiple Regression: How Much Is Your Car Worth?”, Journal of Statistics Education Volume 16, Number 3 (2008).](http://ww2.amstat.org/publications/jse/v16n3/datasets.kuiper.html)\n* [Pardoe , I. (2008), “Modeling home prices using realtor data”, Journal of Statistics Education Volume 16, Number 2 (2008).](http://ww2.amstat.org/publications/jse/v16n2/datasets.pardoe.html)\n\nOk, let's dive into the data.\n","metadata":{"_cell_guid":"383063af-c1ee-42bc-9829-1ff6aa859a7d","hidePrompt":false,"hideCode":false,"_uuid":"20df3c3cb62473968221e68397bb0694218f2e27"},"cell_type":"markdown"},{"source":"# Intuition: getting to know the data\nBefore we start our more rigorous analysis, let's start by familiarizing ourselves with the data first. We don't need to look at every feature (yet), but getting a feel for some features that are immediately relevant to us may help us build some intuition and generate ideas or questions.","metadata":{"_cell_guid":"67c2fc87-5901-4baf-9177-cefafcd159e0","hidePrompt":false,"hideCode":false,"_uuid":"4b4212ece14f776e7cea745ad8408d56d0278202"},"cell_type":"markdown"},{"source":"## Imports and Data Load","metadata":{"_cell_guid":"cc32cd24-1cd4-49fb-88c6-c707cbca6904","hidePrompt":false,"hideCode":false,"_uuid":"ecabd98da4b29142a726cfac95b00d6737fdb767"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"ed6762bc-02a9-4369-9258-dfd661985333","hidePrompt":false,"collapsed":true,"hideCode":false,"_uuid":"2aaf87c81cdc340b7c144468d83ad2565cea87b2"},"outputs":[],"execution_count":null,"source":"# Imports\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport scipy.stats\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns","cell_type":"code"},{"metadata":{"_cell_guid":"7d26ddeb-82d1-4dcc-a9d2-3bbdc0fe78c5","hidePrompt":false,"collapsed":true,"hideCode":false,"_uuid":"af54c91ffc16721c9b171c017f2a750769e5599f"},"outputs":[],"execution_count":null,"source":"# Let's get some data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","cell_type":"code"},{"source":"## High Level Overview\nLet's begin by getting high-level look at our data.","metadata":{"_cell_guid":"00f3d49c-503a-4fc9-ac1a-d20db42f3ec3","hidePrompt":false,"hideCode":false,"_uuid":"a68f99787996c9652c654030790ddedfdaed1190"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"ab9eb4fa-5bc0-48ab-8351-495253816c5d","hidePrompt":false,"hideCode":false,"_uuid":"6a52321a16ff3e63a19f0aa6db06f48ac93b2ce6"},"outputs":[],"execution_count":null,"source":"# How is our data shaping up?\nprint('Our training dataset has {} rows and {} columns.'.format(train.shape[0], train.shape[1]))\nprint('Our test dataset has {} rows and {} columns.'.format(test.shape[0], test.shape[1]))","cell_type":"code"},{"metadata":{"_cell_guid":"23706f5d-5067-498b-9942-40dd13bf23db","hidePrompt":false,"scrolled":false,"hideCode":false,"_uuid":"990da11b099c0fef2cc2b9e984a54549b99b3c53"},"outputs":[],"execution_count":null,"source":"# Take a sneak peak at the data\ntrain.head()","cell_type":"code"},{"metadata":{"_cell_guid":"c8b0689b-5944-4832-bf61-939c3e847280","hidePrompt":false,"hideCode":false,"_uuid":"c7bb57fd1c3419d5574e25a721971b36cdc5d765"},"outputs":[],"execution_count":null,"source":"test.head() # obviously missing the SalePrice","cell_type":"code"},{"metadata":{"_cell_guid":"dedea9c6-e379-497a-980c-9357193c02e9","hidePrompt":false,"hideCode":false,"_uuid":"628e71bd2fac2e1ee9fb935d3016f3ca08b0095e"},"outputs":[],"execution_count":null,"source":"# Get info on our columns and data size\ntrain.info(memory_usage='deep')","cell_type":"code"},{"source":"For larger datasets it pays off to load categorical variables as such (instead of as dtype \"object\"), but with a memory footprint of 3.9 MB we're not exactly dealing with Big Data :) For a great article on this topic, check out [\"Using pandas with large data\"](https://www.dataquest.io/blog/pandas-big-data/).","metadata":{"_cell_guid":"418eab80-f3fb-4847-8b6d-cdf98d6bee25","hidePrompt":false,"hideCode":false,"_uuid":"2c58470b253b842daa5f83eac447873bfa729e19"},"cell_type":"markdown"},{"source":"## Poking around\nNow that we have a 50,000 ft view of the data, let's start poking around and ask some questions based on our existing knowledge of housing in general, without yet going into the weeds of every feature.","metadata":{"_cell_guid":"b6fc020e-f653-4ac1-9422-66d609179150","hidePrompt":false,"hideCode":false,"_uuid":"79f76b6a488d4a01ea473c8bc91ca31b2e61a989"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"91f8d325-98af-4449-9912-4a0c5ede9611","hidePrompt":false,"hideCode":false,"_uuid":"6c7bb359f868f7fc8eddba267c5654a42d9cca4d"},"outputs":[],"execution_count":null,"source":"# How expensive are houses?\nprint('The cheapest house sold for ${:,.0f} and the most expensive for ${:,.0f}'.format(\n    train.SalePrice.min(), train.SalePrice.max()))\nprint('The average sales price is ${:,.0f}, while median is ${:,.0f}'.format(\n    train.SalePrice.mean(), train.SalePrice.median()))\ntrain.SalePrice.hist(bins=75, rwidth=.8, figsize=(14,4))\nplt.title('How expensive are houses?')\nplt.show()","cell_type":"code"},{"source":"We note that the distribution is positively skewed to the right with a good number of outliers.","metadata":{"_cell_guid":"8679c67a-613a-4f9f-9f2c-4c3a8416a130","hidePrompt":false,"hideCode":false,"_uuid":"b742bd584f8ae842df56c3952ae5aec81f80c05d"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"c5267ce0-cb1d-4053-839e-15c9e46d681f","hidePrompt":false,"hideCode":false,"_uuid":"d3ed4dddc704a1b7e73e38933a09c6ae04eb5608"},"outputs":[],"execution_count":null,"source":"# When were the houses built?\nprint('Oldest house built in {}. Newest house built in {}.'.format(\n    train.YearBuilt.min(), train.YearBuilt.max()))\ntrain.YearBuilt.hist(bins=14, rwidth=.9, figsize=(12,4))\nplt.title('When were the houses built?')\nplt.show()","cell_type":"code"},{"source":"Not much action in the 80s apparently. Looks like majority of houses were built in the 50s and after, which good chunk of new houses built in the aughts.","metadata":{"_cell_guid":"0afc82a2-39bd-420f-b7ba-a1cee7458d2d","hidePrompt":false,"hideCode":false,"_uuid":"828d949cbec4a1a1df17bc1abb818cfc912226ed"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"27f5aed1-df51-4623-b3bc-15a8a54471a9","hidePrompt":false,"hideCode":false,"_uuid":"f58e2a9a4f340676912e9a2d9fc8a9db41e000ec"},"outputs":[],"execution_count":null,"source":"# When where houses sold?\ntrain.groupby(['YrSold','MoSold']).Id.count().plot(kind='bar', figsize=(14,4))\nplt.title('When where houses sold?')\nplt.show()","cell_type":"code"},{"source":"So this is interesting. We see a strong seasonal pattern in house sales, with peaks in June and July. We verify that the dataset spans 2006 to 2010, but note that data steps mid-year in July of 2010.\n\nAt this point I'm wondering if the time of year a house is sold has any effect on sales price. We'll address this question once we start our multivariate analysis later on.","metadata":{"_cell_guid":"5ba95c57-d8c2-4810-97b1-d25f4109c62f","hidePrompt":false,"hideCode":false,"_uuid":"4bd60da5df1896313cac5ad8f606ca8b23b826b8"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"99ad104e-481e-47fd-b6ef-03de91e39616","hidePrompt":false,"scrolled":false,"hideCode":false,"_uuid":"89f0631d514d82534256c446b69afafc37da1587"},"outputs":[],"execution_count":null,"source":"# Where are houses?\ntrain.groupby('Neighborhood').Id.count().\\\n    sort_values().\\\n    plot(kind='barh', figsize=(6,6))\nplt.title('What neighborhoods are houses in?')\nplt.show()","cell_type":"code"},{"source":"Looks like a good chunk of houses are in North Ames, Collect Creek, and Old Town, with few houses in Bluestem, Northpark Villa and Veenker.\n\nIf there's any truth to the real estate dictum \"location, location, location\" we should see this feature correlate with the Sales Price. It would also be interesting to augment the dataset with additional information as it relates to neighborhoods. For example, from my own time spent on Zillow, there seems to be an undeniable correlation between house prices and school quality, the latter of which is not capture in our dataset.","metadata":{"_cell_guid":"5828fad9-73ff-4c42-a0ce-89d2477ffb36","hidePrompt":false,"hideCode":false,"_uuid":"84c231d2eee8736f57803be34f678f6ed2501147"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"007d1480-5d77-4b59-bfbc-2ee3a1f2cf45","hidePrompt":false,"hideCode":false,"_uuid":"cea7859c9cfd2f6acb9bb3e632e24e94cc7925a6"},"outputs":[],"execution_count":null,"source":"# How big are houses\nprint('The average house has {:,.0f} sq ft of space, the median {:,.0f} sq ft'.format(\n    train.GrLivArea.mean(), train.GrLivArea.median()))\nprint('The biggest house has {:,.0f} sq ft of space, the smallest {:,.0f} sq ft'.format(\n    train.GrLivArea.max(), train.GrLivArea.min()))\ntrain.GrLivArea.hist(bins=21, rwidth=.8, figsize=(8,4))\nplt.title('How big are houses? (in sq feet)')\nplt.show()","cell_type":"code"},{"source":"Here's the smallest house (334 sq ft):\n<img src=\"https://beacon.schneidercorp.com/PhotoEngine/Photo/165/0534450090/0/1.jpg\" />\n\nAnd the biggest house (5,642 sq ft):\n<img src=\"https://beacon.schneidercorp.com/PhotoEngine/Photo/165/0908154235/1/1.jpg\" />","metadata":{"_cell_guid":"a810454c-41ce-4ec4-8f96-7a600aa8e5c2","hidePrompt":false,"hideCode":false,"_uuid":"8a3f7b7e405a152ffcb79c13d9435acd67e91491"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"046f6f81-6257-4aad-a244-145366400b9b","hidePrompt":false,"scrolled":true,"hideCode":false,"_uuid":"2f3ebc8e2d972d95d67e738521061c8617c4d2c0"},"outputs":[],"execution_count":null,"source":"# How big are lots\nsqft_to_acres = 43560.\nprint('The average lot is {:,.2f} acres, the median {:,.2f} acres'.format(\n    train.LotArea.mean()/sqft_to_acres, train.LotArea.median()/sqft_to_acres))\nprint('The biggest lot is {:,.2f} acres, the smallest {:,.2f} acres'.format(\n    train.LotArea.max()/sqft_to_acres, train.LotArea.min()/sqft_to_acres))\n(train.LotArea/sqft_to_acres).hist(bins=50, rwidth=.7, figsize=(8,4))\nplt.title('How big are lots? (in acres)')\nplt.show()","cell_type":"code"},{"source":"We can see that both the square footage of a house and lot size are positively skewed, but lot size much more so with plenty of outliers. I expect both of these to show significant correlation with sales price. ","metadata":{"_cell_guid":"cc698b0a-cf21-4d11-a81c-4cd91d620bf2","hidePrompt":false,"hideCode":false,"_uuid":"c8f1e3381606da1698f7362f00cff4f05df88917"},"cell_type":"markdown"},{"source":"Ok, that's good for now. We took a look at how expensive houses are, when they were built, where they are, and how big they and the lots they're on are. \n\nLet's now dive into the details more systematically.","metadata":{"_cell_guid":"edd7a9f4-8f13-427b-b235-e04d63c34448","hidePrompt":false,"hideCode":false,"_uuid":"b40d1c85c3920a15d1c5b0e45f881c41dbc5497f"},"cell_type":"markdown"},{"source":"# Univariate Analysis","metadata":{"_cell_guid":"5ab74a20-5a36-44ea-abdd-0e909d327e23","hidePrompt":false,"hideCode":false,"_uuid":"9d67867f195eaf21660a88cdd014f25edd8aeaba"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"af6b48aa-664e-4a1a-8a06-75901306227b","hidePrompt":false,"collapsed":true,"hideCode":false,"_uuid":"b53b1ac27f84566612acd11fe0f1d7948a99a8d8"},"outputs":[],"execution_count":null,"source":"def get_feature_groups():\n    \"\"\" Returns a list of numerical and categorical features,\n    excluding SalePrice and Id. \"\"\"\n    # Numerical Features\n    num_features = train.select_dtypes(include=['int64','float64']).columns\n    num_features = num_features.drop(['Id','SalePrice']) # drop ID and SalePrice\n\n    # Categorical Features\n    cat_features = train.select_dtypes(include=['object']).columns\n    return list(num_features), list(cat_features)\n\nnum_features, cat_features = get_feature_groups()","cell_type":"code"},{"source":"## Numerical Features","metadata":{"_cell_guid":"d16a02c4-9d95-4209-b7a7-eb7f0897e9bb","hidePrompt":false,"hideCode":false,"_uuid":"88ba1c22eddcc6f1347b21287b4563ee38894cbf"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"1c22509d-98ff-4a1e-967b-563f79beed53","hidePrompt":false,"hideCode":false,"_uuid":"c28ebaaaec570ab230b082123ce62b2d8bd5e738"},"outputs":[],"execution_count":null,"source":"# Let's start with our dependent variable, SalePrice\nplt.figure(figsize=(10,6))\nsns.distplot(train.SalePrice)\nplt.show()","cell_type":"code"},{"source":"We see that SalePrice is positively skewed. In fact, we get:","metadata":{"_cell_guid":"764dab4a-17d8-4614-8a59-bf13eb98e96b","hidePrompt":false,"hideCode":false,"_uuid":"3c8609d604d03f996ccfb68260b01c32599d622d"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"dfb85664-4ef8-48e3-883b-9d6e7b9696b5","hidePrompt":false,"hideCode":false,"_uuid":"51356ccbc3eb4a3190e3fc5a9409aeac7edf0d7d"},"outputs":[],"execution_count":null,"source":"print('Skew: {:.3f} | Kurtosis: {:.3f}'.format(\n    train.SalePrice.skew(), train.SalePrice.kurtosis()))","cell_type":"code"},{"source":"So it's positively skewed and \"peaky\" with fat tails, or outliers, namely to the right. We'll be transforming this feature later on. ","metadata":{"_cell_guid":"971aecf3-4724-478c-b71e-219f0ebbe13b","hidePrompt":false,"hideCode":false,"_uuid":"88ee301f4197e51bd482e1f70d0a61a06217194a"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"83cc92f2-575f-462c-b162-2d94332d49d6","hidePrompt":false,"scrolled":false,"hideCode":false,"_uuid":"20c88ec6931ff74da2f62d680f966ecb6e7b783c"},"outputs":[],"execution_count":null,"source":"# Grid of distribution plots of all numerical features\nf = pd.melt(train, value_vars=sorted(num_features))\ng = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\ng = g.map(sns.distplot, 'value')","cell_type":"code"},{"source":"We see a bunch of features that look positively skewed, similar to *SalePrice*. We'll want to log transform these, include: *LotFrontage, LotArea, BsmtUnfSF, TotalBsmtSF, 1stFlrSF, GrLivAre, GarageArea*\n\nSome features can be considered as count data, and maybe [we don't want to transform them?](http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2010.00021.x/abstract;jsessionid=9283C2745CFF8F62B3D9E48E9D463F30.f04t04) *(Note to self: look more deeply into this issue*): *BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, TotalRmsAbvGr, Fireplaces, GarageCars*\n\nWe see time related features: *YearBuilt, YearRemodAdd, GarageYrBlt, MoSold, YrSold*\n\nWe note that *MSSubclass* should really be categorical, and make a note to ourselves to take care of this when we process the data later on. For purposes of regression, we should also treat *MoSold* as categorical as the Euclidean distance between them doesn't make sense in this application. Same for *YrSold*. For YearBuilt, however, the distance is relevant as it implies age of the house.\n\nFinally, we have plenty of sparse features that have a large zero count, e.g. PoolArea, which is 0 for houses that have no pool. We'll have to think about how to handle these. \n\nLet's take a closer look.","metadata":{"_cell_guid":"fa85eafc-c3ee-42aa-baa2-fae5578369fa","hidePrompt":false,"hideCode":false,"_uuid":"90a938de77e2b0f66aebb4f3df47c60c30725085"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"0e465f4c-84fe-47a5-afac-1a3504331848","hidePrompt":false,"hideCode":false,"_uuid":"570557d183954c03808314e51862f306e75430f0"},"outputs":[],"execution_count":null,"source":"# Percentage of zero values\ncount_features = ['BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr',\n                  'KitchenAbvGr','TotalRmsAbvGr','Fireplaces','GarageCars']\nnon_count_features = [f for f in num_features if f not in count_features]\nsparse_features = (train[non_count_features] == 0).sum() / train.shape[0]\nsparse_features[sparse_features > 0].\\\n    sort_values(ascending=True).\\\n    plot(kind='barh', figsize=(10,6))\nplt.title('Level of Sparsity')\nplt.show()","cell_type":"code"},{"source":"## Categorical Features","metadata":{"_cell_guid":"f83fa22f-5f80-4977-b8c2-cb01245443f3","hidePrompt":false,"hideCode":false,"_uuid":"76ae9da7b94f8662cd3e81c7ddce904170f5fe6a"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"e42c25fc-3fab-49d6-9bc0-937afd58f90b","hidePrompt":false,"collapsed":true,"hideCode":false,"_uuid":"c382e3858167258b6ea7e0f2a86681460bfe97de"},"outputs":[],"execution_count":null,"source":"# First off, earlier we said we'll need to transform\n# a couple features to categorical. Since we're looking \n# at categorical data here, let's go ahead and do that now\n# so they are included in the analysis.\ntrain['MSSubClass'] = train.MSSubClass.apply(lambda x: str(x))\ntrain['MoSold'] = train.MoSold.apply(lambda x: str(x))\ntrain['YrSold'] = train.YrSold.apply(lambda x: str(x))\n\n# Update our list of numerical and categorical features\nnum_features, cat_features = get_feature_groups()","cell_type":"code"},{"metadata":{"_cell_guid":"b36036ca-ba26-48c5-bb2d-35d370e095a7","hidePrompt":false,"scrolled":false,"hideCode":false,"_uuid":"e6e11180788242c553be0c9c6a7af02d5c35a38e"},"outputs":[],"execution_count":null,"source":"# Count plots of categorical features\nf = pd.melt(train, value_vars=sorted(cat_features))\ng = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\nplt.xticks(rotation='vertical')\ng = g.map(sns.countplot, 'value')\n[plt.setp(ax.get_xticklabels(), rotation=60) for ax in g.axes.flat]\ng.fig.tight_layout()\nplt.show()","cell_type":"code"},{"source":"The categorical features will be much more interesting when compaired to our target feature SalePrice, but we can note a couple of things nevertheless. First, we note that there are plenty of feature were one value is heavily overrpresented, e.g. *Condition2* (Proximity to various conditions (if more than one is present)), where nearly 99% of houses are listed as \"Norm\". That's fine though, as those edge cases may help us predict outliers.\n\nThe second thing to realize is that a number of categorical features actually contain rank information in them and should thus be converted to discrete quantitative features similar to *OverallQual*. For example, *ExterQual* has the following values:\n* Ex - Excellent\n* Gd - Good\n* TA - Average/Typical\n* Fa - Fair\n* Po - Poor\n\nWe should change these to be 1 to 5. We'll do that for the following features: *Alley, LotShape, LandContour, Utilities, LandSlope, ExterQual, ExterCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, HeatingQC, KitchenQual, Functional, FireplaceQu, GarageFinish, GarageQuality, GarageCond, PavedDrive, PoolQC*\n\nLooks like we had a bunch of numerical features hiding out amongst the categorical fellas!","metadata":{"_cell_guid":"57023cc4-0ed8-40f9-a802-6d1efdc77d8f","hidePrompt":false,"hideCode":false,"_uuid":"e369a1f58796c7fba289d700e51d7acb2ca76019"},"cell_type":"markdown"},{"source":"# Bivariate Analysis","metadata":{"_cell_guid":"4ca6e443-32ed-476f-8b5f-049e006098a0","hidePrompt":false,"hideCode":false,"_uuid":"1a9a79ae2d42c8a1002bc94e03c64714dd075495"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"cdbf8786-90d3-48c4-9bc9-24473a53a5b4","hidePrompt":false,"collapsed":true,"hideCode":false,"_uuid":"9bdf86b8f069ea93254bd8418b2137fb4c5e0d45"},"outputs":[],"execution_count":null,"source":"# In the last section we identified a list of features\n# that we want to change from categorical to numerical\n# because the contain ranked information (e.g. quality ratings).\n# We'll make those transforms now already so that they are\n# properly included in the following section.\n# We're also going to replace missing values with 0 already.\n\n# Alley\ntrain.Alley.replace({'Grvl':1, 'Pave':2}, inplace=True)\n\n# Lot Shape\ntrain.LotShape.replace({'Reg':1, 'IR1':2, 'IR2':3, 'IR3':4}, inplace=True)\n\n# Land Contour\ntrain.LandContour.replace({'Low':1, 'HLS':2, 'Bnk':3, 'Lvl':4}, inplace=True)\n\n# Utilities\ntrain.Utilities.replace({'ELO':1, 'NoSeWa':2, 'NoSewr':3, 'AllPub':4}, inplace=True)\n\n# Land Slope\ntrain.LandSlope.replace({'Sev':1, 'Mod':2, 'Gtl':3}, inplace=True)\n\n# Exterior Quality\ntrain.ExterQual.replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n\n# Exterior Condition\ntrain.ExterCond.replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n\n# Basement Quality\ntrain.BsmtQual.replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n\n# Basement Condition\ntrain.BsmtCond.replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n\n# Basement Exposure\ntrain.BsmtExposure.replace({'No':1, 'Mn':2, 'Av':3, 'Gd':4}, inplace=True)\n\n# Finished Basement 1 Rating\ntrain.BsmtFinType1.replace({'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}, inplace=True)\n\n# Finished Basement 2 Rating\ntrain.BsmtFinType2.replace({'Unf':1, 'LwQ':2, 'Rec':3, 'BLQ':4, 'ALQ':5, 'GLQ':6}, inplace=True)\n\n# Heating Quality and Condition\ntrain.HeatingQC.replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n\n# Kitchen Quality\ntrain.KitchenQual.replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n\n# Home functionality\ntrain.Functional.replace({'Sal':1, 'Sev':2, 'Maj2':3, 'Maj1':4, 'Mod':5, 'Min2':6, 'Min1':7, 'Typ':8}, inplace=True)\n\n# Fireplace Quality\ntrain.FireplaceQu.replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n\n# Garage Finish\ntrain.GarageFinish.replace({'Unf':1, 'RFn':2, 'Fin':3}, inplace=True)\n\n# Garage Quality\ntrain.GarageQual.replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n\n# Garage Condition\ntrain.GarageCond.replace({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n\n# Paved Driveway\ntrain.PavedDrive.replace({'N':1, 'P':2, 'Y':3}, inplace=True)\n\n# Pool Quality\ntrain.PoolQC.replace({'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}, inplace=True)\n\n# We'll set all missing values in our newly converted features to 0\nconverted_features = ['Alley','LotShape','LandContour','Utilities','LandSlope','ExterQual','ExterCond',\n        'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC',\n        'KitchenQual','Functional','FireplaceQu','GarageFinish','GarageQual',\n        'GarageCond','PavedDrive','PoolQC']\ntrain[converted_features] = train[converted_features].fillna(0)\n\n# Update our list of numerical and categorical features\nnum_features, cat_features = get_feature_groups()","cell_type":"code"},{"source":"## Numerical Features","metadata":{"_cell_guid":"5093f049-dab5-4b9c-9f98-c0b73a26ee45","hidePrompt":false,"hideCode":false,"_uuid":"71ea3810c9cc830d19b602c254bc6c421657b13f"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"f4d1a4eb-586c-4c02-8ecd-f309065cecc6","hidePrompt":false,"scrolled":false,"hideCode":false,"_uuid":"44f8ace6b83c480c29af9c4ef4bf75beb83714d6"},"outputs":[],"execution_count":null,"source":"# Scatter plots of numerical features against SalePrice\nf = pd.melt(train, id_vars=['SalePrice'], value_vars=sorted(num_features))\ng = sns.FacetGrid(f, col='variable', col_wrap=4, sharex=False, sharey=False)\nplt.xticks(rotation='vertical')\ng = g.map(sns.regplot, 'value', 'SalePrice', scatter_kws={'alpha':0.3})\n[plt.setp(ax.get_xticklabels(), rotation=60) for ax in g.axes.flat]\ng.fig.tight_layout()\nplt.show()","cell_type":"code"},{"source":"We find that there are quite a few features that seem to show strong correlation to SalePrice, such as OverallQual, TotalBsmtSF, GrLivArea, and TotRmsAbvGrd. This confirms our natural intuition: we would expect that high quality, big house with big basements and lots of rooms to be more expensive.\n\nWe also note the occurrence of cone shapes, an artifact of our skewed distributions. Once we log transform some of these features, the relationship will be more linear.","metadata":{"_cell_guid":"b224e1b1-76bb-41cb-8d09-48e4c8c4ca98","hidePrompt":false,"hideCode":false,"_uuid":"912e361bc174395fddcb8d2e53a51d402f7b7f00"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"176e9ad8-019d-412e-8cf3-d25db9992b1c","hidePrompt":false,"hideCode":false,"_uuid":"c76583621ef7ff73f59894380b7c23a4d77cb7c6"},"outputs":[],"execution_count":null,"source":"plt.figure(figsize=(12,6))\nplt.subplot(121)\nsns.regplot(train.GrLivArea, train.SalePrice, scatter_kws={'alpha':0.3})\nplt.title('Cone shape visible before log transform')\n\nplt.subplot(122)\nsns.regplot(np.log1p(train.GrLivArea), np.log1p(train.SalePrice), scatter_kws={'alpha':0.3})\nplt.title('After log transform')\nplt.show()","cell_type":"code"},{"metadata":{"_cell_guid":"53d516c4-52ce-4483-8673-80d791316d3d","hidePrompt":false,"hideCode":false,"_uuid":"e0816193458a8ece47bd84a5d8e51392aa691a30"},"outputs":[],"execution_count":null,"source":"# Let's take a look at the correlation between numerical features\ncorr = train[['SalePrice'] + num_features].corr()\nfig = plt.figure(figsize=(16,15))\nax = fig.add_subplot(111)\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, \n           xticklabels=corr.columns.values,\n           yticklabels=corr.index.values,\n           cmap=cmap)\nax.xaxis.tick_top()\nplt.setp(ax.get_xticklabels(), rotation=90)\nplt.show()","cell_type":"code"},{"source":"We see that there is quite bit of correlation between features, many of which one might expect to find, and perhaps some surprising ones. For example, we find that *GarageYrBlt* is highly correlated with *YearBuilt*, which means that most garages were built along with the house. *BsmtQual* correlates with *OverallQual* and *TotalBsmtSF* correlates highly with *1stFlrSF*, which both make a lot of sense. We also see that *KitchenQual* correlates highly with *ExteriorQual*; kitchens and housing exteriors are of course two separate things, but may point to a lurking variable, such as \"new house\": a newer house is likely to have both a better exterior and better kitchen. \n\nLet's hone in on how features correlate with *SalePrice*.","metadata":{"_cell_guid":"a389a59f-7d6d-4c53-8882-8e632d2c367c","hidePrompt":false,"hideCode":false,"_uuid":"2b278d3f3e985b892deb7a71708136cead09b52d"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"6f3b9143-e6fd-4884-a655-5f9f8ed03e64","hidePrompt":false,"hideCode":false,"_uuid":"f7d117460ace1cd7dd78ae5cb9e2311f2dd3bae2"},"outputs":[],"execution_count":null,"source":"# Feature sorted by correlation to SalePrice, from positive to negative\ncorr = corr.sort_values('SalePrice', ascending=False)\nplt.figure(figsize=(8,10))\nsns.barplot( corr.SalePrice[1:], corr.index[1:], orient='h')\nplt.show()","cell_type":"code"},{"source":"The feature most correlated with *SalePrice* is *OverallQual*, which makes sense, followed by the size of the house (*GrLivArea*) and then three more quality related features: *ExterQual*, *KitchenQual*, and *BsmtQual*. We noted earlier that *ExterQual* and *KitchenQual* were highly correlated with one another, and now we find out that they are both also highly correlated with *SalePrice*. In regression we usually want to avoid including predictors highly correlated with one another to minimize **multicollinearity**. From [Wikipedia](https://en.wikipedia.org/wiki/Multicollinearity):\n> In statistics, **multicollinearity** (also **collinearity**) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multiple regression model with colinear predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others.\n\nIn other words, if we include, for example, both *ExterQual* and *KitchenQual* in our regression, we won't be able to say which of those features is \"more important\" than the other (i.e. we can't trust their coefficients). But the overall predictive power of the model is not affected.\n\nSo do we care? I'd say no, because we're not really interested in *explaining* what exactly impacts the sales price of a house; the ultimate goal is to simply predict accurately what the sales price may be. ","metadata":{"_cell_guid":"b14124a4-02dd-424d-978e-4edb5dfa865f","hidePrompt":false,"hideCode":false,"_uuid":"1e01afceeefcede8dc71a6c442fa87121e80dde7"},"cell_type":"markdown"},{"source":"## Categorical Features","metadata":{"_cell_guid":"feae3223-8f9f-443f-ac14-b7386774bcb7","hidePrompt":false,"hideCode":false,"_uuid":"1c0706efa9f5f8bcb9a62584ff58af973c315e4b"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"55a23b22-3b09-48f6-bc12-8f1fec9da9ac","hidePrompt":false,"scrolled":false,"hideCode":false,"_uuid":"25904415185136f47b7254877fd4ea606a60d39c"},"outputs":[],"execution_count":null,"source":"# Count plots of categorical features\nf = pd.melt(train, id_vars=['SalePrice'], value_vars=sorted(cat_features))\ng = sns.FacetGrid(f, col='variable', col_wrap=3, sharex=False, sharey=False, size=4)\ng = g.map(sns.boxplot, 'value', 'SalePrice')\n[plt.setp(ax.get_xticklabels(), rotation=90) for ax in g.axes.flat]\ng.fig.tight_layout()\nplt.show()","cell_type":"code"},{"source":"It looks like some features show significant variance in the mean of *SalePrice* between different groups, eg. *Neighborhood*, *SaleType* or *MSSubClass*.\n\nHowever, we’d like to have a better sense of which feature influences *SalePrice* more than others. What we’ll do is run one-way ANOVA tests for each categorical feature againt *SalePrice*. This will give us both the F statistic and p-values for each feature. The higher the F statistic, the higher the p-value (i.e. the more confident we can be in rejecting the null hypothesis), but since the p-value will take into consideration a given F distribution (based on number of groups and number of observations), we will ultimately sort the features by p-value (instead of F). What does the p-value tell us? Again it tells how confident we can be in rejecting the null hypothesis; put differently, it answers the question \"how likely was it to see data for each group if each group had in reality no effect on the dependent variable?\" The unlikelier it is, the greater the difference in groups and therefore the more significant of an influence the feature has on the dependent variable *SalePrice*.\n\nLet's see what that looks like.","metadata":{"_cell_guid":"0711f8c3-6ffb-44ea-94b3-7ef7473eadab","hidePrompt":false,"hideCode":false,"_uuid":"ab78e96d853fd46810ab4495dc058e0e1f01df4d"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"6907ca35-074a-4e64-be98-30a99344e707","hidePrompt":false,"hideCode":false,"_uuid":"1666113d2cc887345d5ee2b159ea915cb0ee9d66"},"outputs":[],"execution_count":null,"source":"# In order for ANOVA to work, we have to take care of missing values first\ntrain[cat_features] = train[cat_features].fillna('Missing')\n\n# Onward...\nanova = {'feature':[], 'f':[], 'p':[]}\nfor cat in cat_features:\n    group_prices = []\n    for group in train[cat].unique():\n        group_prices.append(train[train[cat] == group]['SalePrice'].values)\n    f, p = scipy.stats.f_oneway(*group_prices)\n    anova['feature'].append(cat)\n    anova['f'].append(f)\n    anova['p'].append(p)\nanova = pd.DataFrame(anova)\nanova = anova[['feature','f','p']]\nanova.sort_values('p', inplace=True)\n\n# Plot\nplt.figure(figsize=(14,6))\nsns.barplot(anova.feature, np.log(1./anova['p']))\nplt.xticks(rotation=90)\nplt.show()","cell_type":"code"},{"source":"We confirm the old adage \"location, location, location\"! Of all our categorical features, *Neighborhood* appears to have the greatest influence on *SalePrice*. \n\nIt's important to note here that the chart really undersells how much more influence *Neighborhood* has. We took the log of the inverse of the p-value (`np.log(1./anova['p']`): the inverse so that when we take the log we get positive numbers, and log so that we don't just see a single bar. In other words, the p-value is a magnitude of about 300 times smaller than the next feature!","metadata":{"_cell_guid":"31eab08e-4615-4ab7-9953-f83ace653d7c","hidePrompt":false,"hideCode":false,"_uuid":"d981c1882145789f08bf5ec3e2e122862ae4c015"},"cell_type":"markdown"},{"source":"### Bonus Plots\nAt this point we're about ready to formally process our data and get it ready for modelling, but before we do, let's look at a cool visualization that was new to me and does a great job showing the relationship between two categorical and one numerical variable.","metadata":{"_cell_guid":"f6458e67-fb47-4d72-b6e2-4370608f7118","hidePrompt":false,"hideCode":false,"_uuid":"a6a2fdaaec4667e1b051342c8302aa9de68cb479"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"6864b6a3-726d-43b2-b0da-36e1e9dfef0b","hidePrompt":false,"hideCode":false,"_uuid":"bafe1be057927384430accdfd3a12bb5cd2f75e2"},"outputs":[],"execution_count":null,"source":"# Swarming Factorplot FTW\nfig = sns.factorplot(x='Neighborhood', y='SalePrice', hue='MSZoning', data=train, kind='swarm', size=12, aspect=1.5)\nax = fig.axes[0][0]\nax.set_yscale('log')\nplt.show()","cell_type":"code"},{"source":"Here we're looking at sales price by neighborhood and color-coding by zoning classificaton of the sale. It looks like there is clearly some strong relationships between some neighborhoods and zoning classifications: Old Town, Brooksize, Iowa DOT and Rail Road, Meadow Village and Briardale are all predominantly in a \"Residental Medium Density\" zone, while Somerset is in a \"Floating Village Residential\" zone (I'm assuming this might be like a gated community?)","metadata":{"_cell_guid":"7f16f6bb-3a10-448c-8004-4bc5ff2a9447","_uuid":"fca2e0c6dd054a5e15751e0dc2dc5e1b9393815e"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"7667814c-0863-4a70-984d-0f3c581d8ee8","_uuid":"28803b8b24f549bb51bd129da7a257fcbb432a14"},"outputs":[],"execution_count":null,"source":"# Peering into the history of Ames, IA\nfig = sns.factorplot(x='Neighborhood', y='YearBuilt', hue='HouseStyle', data=train, kind='swarm', size=12, aspect=1.5)\nax = fig.axes[0][0]\n#ax.set_yscale('log')\n#plt.show()","cell_type":"code"},{"source":"I imagine this type of chart to be quite interesting for an historian. We're plotting houses by the year they were built, in which neighborhood, and what kind of house style was used (e.g. \"2.5Fin -> Two and one-half story: 2nd level finished\"). We can easily see why Old Town is called Old Town: that's where houses were being built before 1900. We see some neighborhoods gradually developing, like Crawford, while others were developed in a short amount of time, e.g. Northridge Heights or College Creek. It also looks like the housing style changed over time, and in some cases clusters in a neighborhood. For example, the 50s and 60s were a popular time for one story houses (I'm picturing post-WWII newly built suburbs), while two-story houses were more in vogue in the 90s. \"Split Foyer\" had their golden age in the 70s. As we noted earlier, there seems to have been a dearth in houses built in the 80s for some reason. \n\nOk, enough with this history lesson, let's move on. ","metadata":{"_cell_guid":"dccb205a-90be-4e2b-a034-0de1fed06a68","_uuid":"e1c0f2f9febb22777e057d5902615a089aea0ccf"},"cell_type":"markdown"},{"source":"# Data Preprocessing","metadata":{"_cell_guid":"6c5e3a04-d637-4438-ba32-21a7d4251b17","hidePrompt":false,"hideCode":false,"_uuid":"9dd5ffaf78945244ded35a72d246657bb30dbfa6"},"cell_type":"markdown"},{"source":"## De-duplication\nLet's double-check to to see that we don't have any dupes in our data.","metadata":{"_cell_guid":"f2b9d317-9ef0-41c4-b1bd-ec98fb535fa8","_uuid":"e7ccce55491848d9841d5784a7dd670704e8afc0"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"8d3960e6-5b8d-463e-a3bd-9c91d51d0eaf","collapsed":true,"_uuid":"110bef4f9ecd4b87893bb104688d6b3ac3566ce7"},"outputs":[],"execution_count":null,"source":"# To check for dupes, let's just get the original dataset\n# and make sure that no Parcel IDs appear twice, possibly \n# from resales\n\n# Kaggle's not a fan of the code below\n#original_dataset = 'https://ww2.amstat.org/publications/jse/v19n3/decock/AmesHousing.txt'\n#og = pd.read_table(original_dataset)\n#print('There are {} duplicate observations'.format(og.PID.count() - og.PID.nunique()))","cell_type":"code"},{"metadata":{"_cell_guid":"c65567c1-5e1b-4d3f-b4c5-ae7374cc2a9b","_uuid":"428ef849e5fdcf27b86e8e7a091eda721eb47ca9"},"outputs":[],"execution_count":null,"source":"# ... and let's just get peace of mind for our data\nprint('Train set duplicate IDs: {}'.format(train.duplicated('Id').sum()))\nprint('Test set duplicate IDs: {}'.format(test.duplicated('Id').sum()))","cell_type":"code"},{"source":"## Missing Data\nIn the course of our analysis, we've actually had to deal with missing values already. Let's document here what we've already done and see if there are any missing value left we have to deal with. \n\nEarlier when we converted several categorical features to numerical we also filled in any missing values with 0. This made sense as those categorical feature values had rank, so when we converted the values to 1, 2, 3, etc. it seems like it only makes sense to set missing values equal to 0.  We did this for the following features: *Alley, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, ExterCond, ExterQual, FireplaceQu, Functional, GarageCond, GarageFinish, GarageQual, HeatingQC, KitchenQual, LandContour, LandSlope, LotShape, PavedDrive, PoolQC, Utilities*.\n\nPrior to doing the ANOVA evaluation of categorial features, we filled the missing values with simply \"Missing\". Let's see what impact that had.","metadata":{"_cell_guid":"2dcb2d8a-3eff-4f1b-b253-ac8e72d34d79","hidePrompt":false,"hideCode":false,"_uuid":"e740745c1073c7c2a47fb62cc32e87a3cc6c5310"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"b2038f23-828d-4077-a1c1-316f6317019b","_uuid":"a76c99ad0b692bc1c6d9c0a3ae6c2e335d849e20"},"outputs":[],"execution_count":null,"source":"# Categorical features with \"Missing\" values\nmissing_values = (train[cat_features] == 'Missing').sum().sort_values(ascending=False)\nmissing_values[missing_values > 0]","cell_type":"code"},{"metadata":{"_cell_guid":"8c6da5e5-4f7f-4c9e-9703-095b9ee0158e","collapsed":true,"_uuid":"be5dbbc35b515423d2adfacacdf08799929b8d23"},"outputs":[],"execution_count":null,"source":"# Since there's only one missing Electrical, let's\n# set it equal to the most common type\ntrain.loc[train.Electrical == 'Missing', 'Electrical'] = train.Electrical.mode()[0]","cell_type":"code"},{"metadata":{"_cell_guid":"930403b9-e498-41d0-b13e-94f784507d25","_uuid":"c5186df6bc44cebde09f252215382412184f2581"},"outputs":[],"execution_count":null,"source":"# There's a tiny mistake that crept into our data.\n# Unlike some other features, for MasVnrType \"None\" \n# was actually called out as an explicit value. \n# Now we have \"None\" and \"Missing\"\ntrain.MasVnrType.value_counts()","cell_type":"code"},{"metadata":{"_cell_guid":"f5ff960c-f22f-4da3-93c3-f6766254ab52","collapsed":true,"_uuid":"d299e1ba147b2ef65a0a2d132abfade0548d5cfa"},"outputs":[],"execution_count":null,"source":"# Let's change \"Missing\" to \"None\" here.\ntrain.MasVnrType.replace({'Missing':'None'}, inplace=True)","cell_type":"code"},{"metadata":{"_cell_guid":"99ae5be0-9961-445a-92bf-a4e219b3d011","collapsed":true,"_uuid":"07195acefe61c43beff4101313a2af1d463a3ba1"},"outputs":[],"execution_count":null,"source":"# We also notice a data discrepancy, we have \n# houses with MasVnrType == None but MasVnrArea > 0 and\n# house with MasVnrArea == 0 but MasVnrType != None\n# Let's fix these with the assumption that there was a mason veneer\ntrain.loc[(train.MasVnrType == 'None') & (train.MasVnrArea > 1), 'MasVnrType'] = 'BrkFace' # most common \ntrain.loc[(train.MasVnrType == 'None') & (train.MasVnrArea == 1), 'MasVnrArea'] = 0 # 1 sq ft is basically 0\nfor vnr_type in train.MasVnrType.unique():\n    # so here we set the area equal to the mean of the given veneer type\n    train.loc[(train.MasVnrType == vnr_type) & (train.MasVnrArea == 0), 'MasVnrArea'] = \\\n        train[train.MasVnrType == vnr_type].MasVnrArea.mean() \n","cell_type":"code"},{"source":"Now let's see what's left over.","metadata":{"_cell_guid":"872114ca-1641-4b53-b49c-1060778097f6","_uuid":"13de5c6ff22d27642a799b6f46e78de285853fcb"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"7ef0dbfc-1a63-4108-9d1e-65141093de99","hidePrompt":false,"hideCode":false,"_uuid":"2bead79e559d857664f8349b6641d5203ed1f140"},"outputs":[],"execution_count":null,"source":"# Remaining missing values\nmissing_data = train.isnull().sum() / train.shape[0]\nmissing_data[missing_data > 0].\\\n    sort_values(ascending=True).\\\n    plot(kind='barh', figsize=(10,6))\nplt.title('Percentage of missing values')\nplt.show()","cell_type":"code"},{"metadata":{"_cell_guid":"84e0f399-6a65-4a16-abcb-fcb57b934bda","collapsed":true,"_uuid":"5341e1ea0c61ad5e5c469f6a28f1eefa126effe3"},"outputs":[],"execution_count":null,"source":"# LotFrontage is \"Linear feet of street connected to property\"\n# Since it seems unlikely that there's no street connected\n# to a lot, we'll set it equal to the median LotFrontage of that street.\ntrain.LotFrontage = train.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","cell_type":"code"},{"metadata":{"_cell_guid":"a3cfb92e-d5ef-4301-b32c-31a62dc7ddba","collapsed":true,"_uuid":"c672d4955e6452be1f1d6b6b79dd194873845d1c"},"outputs":[],"execution_count":null,"source":"# MasVnrArea is \"Masonry veneer area in square feet\"\n# This feature consists of a majority of 0 values, so \n# it's odd to find missing values here. Let's set to 0\ntrain.MasVnrArea.fillna(0, inplace=True)","cell_type":"code"},{"metadata":{"_cell_guid":"249c1532-4bff-4a49-9b89-924a68a377fc","collapsed":true,"_uuid":"7c2b9b32ba5ed1b62583269c5bb318d3297c10bc"},"outputs":[],"execution_count":null,"source":"# Since GarageYrBlt missing means there's no garage\n# we'll set it equal to 0\ntrain.GarageYrBlt.fillna(0, inplace=True)","cell_type":"code"},{"metadata":{"_cell_guid":"80583856-5c50-4655-a060-e08f7ae41699","scrolled":true,"_uuid":"7a6f46bbf68e6abe7a5997555e882844ac7a84bd"},"outputs":[],"execution_count":null,"source":"# Anything left?\ntrain.isnull().sum().sum()","cell_type":"code"},{"source":"## Outliers\nWe recall that the [dataset documentation](https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt) mentioned that there were outliers and that we should probably get rid of them:\n> There are 5 observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will indicate them quickly). Three of them are true outliers (Partial Sales that likely don’t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these 5 unusual observations) before assigning it to students.\n\nLet's check it out for ourselves.","metadata":{"_cell_guid":"a593b503-ae87-4f98-a2fa-ac03b096499d","_uuid":"5823cbd34a0e517909c2471610a160646248af7c"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"57f073fd-4af2-4dc6-a2d3-c2dafb3d2016","_uuid":"a0432b9fd5aa63089459bbaef98186c331808a9c"},"outputs":[],"execution_count":null,"source":"# First we visually inspect a scatter plot of GrLivArea vs. SalePrice\nplt.figure(figsize=(10,6))\nsns.regplot(train.GrLivArea, train.SalePrice, scatter_kws={'alpha':0.3})\nplt.show()","cell_type":"code"},{"metadata":{"_cell_guid":"ed0b8e73-80ed-42b3-a13e-40fd433fe093","_uuid":"7b34596486128432efdbfd99ebcfa9acbe5886bc"},"outputs":[],"execution_count":null,"source":"# We see the outliers but are also interested in those Partial Sale outliers\nplt.figure(figsize=(10,6))\nsns.regplot(train[train.SaleCondition == 'Partial'].GrLivArea,\n            train[train.SaleCondition == 'Partial'].SalePrice, scatter_kws={'alpha':0.3})\nplt.show()","cell_type":"code"},{"metadata":{"_cell_guid":"09b52895-38a4-4d7c-be47-6c77eb7047fe","collapsed":true,"_uuid":"a5720f58424864f974ab1d577d64abe43202bcda"},"outputs":[],"execution_count":null,"source":"# We heed the author's advice and cut out anything over 4,000 sq ft\ntrain.drop(train[train.GrLivArea >= 4000].index, inplace=True)","cell_type":"code"},{"source":"## More Feature Enginnering","metadata":{"_cell_guid":"1c1b321d-2c68-4096-af7b-fd52326eb489","_uuid":"381f8caa6a5bebeaf5dbf3509714b95d1a8dd608"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"09986592-66ca-4716-be5b-bc7f9e12e629","collapsed":true,"_uuid":"d05a83f87e29af62c8ecc8d3bd41b27d451d3fca"},"outputs":[],"execution_count":null,"source":"# Let's add some additional features\n\n# Total Square Footage\ntrain['TotalSF'] = train.TotalBsmtSF + train.GrLivArea\ntrain['TotalFloorSF'] = train['1stFlrSF'] + train['2ndFlrSF']\ntrain['TotalPorchSF'] = train.OpenPorchSF + train.EnclosedPorch + \\\n    train['3SsnPorch'] + train.ScreenPorch\n    \n# Total Bathrooms\ntrain['TotalBathrooms'] = train.FullBath + .5 * train.HalfBath + \\\n    train.BsmtFullBath + .5 * train.BsmtHalfBath\n\n# Booleans\ntrain['HasBasement'] = train.TotalBsmtSF.apply(lambda x: 1 if x > 0 else 0)\ntrain['HasGarage'] = train.GarageArea.apply(lambda x: 1 if x > 0 else 0)\ntrain['HasPorch'] = train.TotalPorchSF.apply(lambda x: 1 if x > 0 else 0)\ntrain['HasPool'] = train.PoolArea.apply(lambda x: 1 if x > 0 else 0)\ntrain['WasRemodeled'] = (train.YearRemodAdd != train.YearBuilt).astype(np.int64)\ntrain['IsNew'] = (train.YearBuilt > 2000).astype(np.int64)\ntrain['WasCompleted'] = (train.SaleCondition != 'Partial').astype(np.int64)\n\nboolean_features = ['HasBasement', 'HasGarage', 'HasPorch', 'HasPool', \n                    'WasRemodeled', 'IsNew', 'WasCompleted']","cell_type":"code"},{"metadata":{"_cell_guid":"8be5bca4-2825-4e45-86de-fa9398c6255b","collapsed":true,"_uuid":"b832406457303171f974c07ddcb4787474280858"},"outputs":[],"execution_count":null,"source":"num_features, cat_features = get_feature_groups()\nnum_features = [f for f in num_features if f not in boolean_features]","cell_type":"code"},{"source":"## Transforms\nEarlier we noted that some of the numerical features exhibit positive skew and could benefit from a log transform. Let's go ahead and do that now.","metadata":{"_cell_guid":"18eff607-ba46-466b-a283-271c2b8da82d","hidePrompt":false,"hideCode":false,"_uuid":"2ccb649008d5238cefbad954e505489fd86d4d14"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"90e0dcc7-e629-4a69-b1e6-eab722bbd7ad","collapsed":true,"_uuid":"5ddc7a7785b684942eb11e610b103d2af403f464"},"outputs":[],"execution_count":null,"source":"# Here we will be simplistic about it and simply\n# log transform any numerical feature with a \n# skew greater than 0.5\nfeatures = num_features + ['SalePrice']\nfor f in features:\n    train.loc[:,f] = np.log1p(train[f])","cell_type":"code"},{"source":"## Dummy Variables\nWe'll one-hot encode all of our categorical features now.","metadata":{"_cell_guid":"a6899153-4706-453b-a849-b897c110702b","hidePrompt":false,"collapsed":true,"hideCode":false,"_uuid":"03ceee5887f684b62ef07820c4aeedb2df2f8aef"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"14d4e397-42bc-4a35-8c21-8ff7c849e93d","collapsed":true,"_uuid":"9e5a5145b5695773b4f2754253fdc1b8729190c5"},"outputs":[],"execution_count":null,"source":"# before we continue, let's drop some cols\ny = train['SalePrice']\ntrain.drop('SalePrice', axis=1, inplace=True)\ntrain.drop('Id', axis=1, inplace=True)","cell_type":"code"},{"metadata":{"_cell_guid":"1ad5aaaa-c67a-4fff-b9ea-99209b45f54b","collapsed":true,"_uuid":"83efae5558750a00209749fe02a8214c403703ab"},"outputs":[],"execution_count":null,"source":"# ... and go\nmodel_data = pd.get_dummies(train).copy()","cell_type":"code"},{"source":"# Modelling","metadata":{"_cell_guid":"c56b1c0c-f31d-45ad-b8b5-45831aad6c82","_uuid":"0ff54b6e4b1a57df72a19ae9523125b1b2af7c0c"},"cell_type":"markdown"},{"source":"## Partition\nWe want to divide our data into three parts: a training, validation, and testing set. We use our validation set to optimize hyperparameters and ultimately get a score at the very end using our test set. However, in order to not throw out too much data for training, we will use a cross validation strategy whereby random groups of our training set will be held out for validation scoring and finally average","metadata":{"_cell_guid":"94d26f7c-b7ac-4e21-808f-d2798cb8844b","_uuid":"a3b57e7daf18eec39342a9778cf06e13b282bd04"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"81d19f9c-2b47-4276-ac79-db4227a1e7f6","collapsed":true,"_uuid":"719e65dbf84413fd51f6fa588be8c27d2a691f96"},"outputs":[],"execution_count":null,"source":"from sklearn.model_selection import train_test_split","cell_type":"code"},{"metadata":{"_cell_guid":"6b7e2527-3288-4f8c-82d5-efe948a39cc0","_uuid":"3bd001abbc4bb4662e5b41107a245703bbc8f97c"},"outputs":[],"execution_count":null,"source":"# Split data intro train and validation sets\nX_train, X_test, y_train, y_test = \\\n    train_test_split(model_data.copy(), y, test_size=0.3, random_state=42)\nprint('Shapes')\nprint('X_train:', X_train.shape)\nprint('X_val:', X_test.shape)\nprint('y_train:', y_train.shape)\nprint('y_val:', y_test.shape)","cell_type":"code"},{"source":"## Standardize\nNext we'll standardize our numerical data, meaning we center the feature means at 0 with standard deviation of 1. This makes it easier to learn weights. ","metadata":{"_cell_guid":"15693d4c-42fa-4fad-bbe1-4d7fb28d22bb","_uuid":"3e982600492af88389fddd7e86d2af0e49beda98"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"742193f9-bfcd-4068-bcfa-4e41b4a63c7c","collapsed":true,"_uuid":"a09e00d7efd6f3d25ea881da9b0ed5326c5696be"},"outputs":[],"execution_count":null,"source":"from sklearn.preprocessing import RobustScaler, StandardScaler","cell_type":"code"},{"metadata":{"_cell_guid":"fd8056fc-ab47-4209-8974-ac6221a77abe","collapsed":true,"_uuid":"3c946c12b20f7ad93140a9faf577d0cb69ec1e0d"},"outputs":[],"execution_count":null,"source":"# We'll use the convenient sklearn RobustScaler.\n# Note we're only standardizing numerical features, not\n# the dummy features. The RobustScaler helps us deal with outliers.\nstdsc = StandardScaler()\nX_train.loc[:,num_features] = stdsc.fit_transform(X_train[num_features])\nX_test.loc[:,num_features] = stdsc.transform(X_test[num_features])","cell_type":"code"},{"source":"## Errors\n\nFrom the competition evaluation page:\n> Submissions are evaluated on [Root-Mean-Squared-Error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n\nOn the Wikipedia page we find the definition of RMSE (aka RMSD for \"Deviation\"):\n<img src='https://wikimedia.org/api/rest_v1/media/math/render/svg/197385368628b8495a746f7bd490d3d1cc83e86c' />","metadata":{"_cell_guid":"927803f2-8d3f-46c6-bbf7-50e097015bb6","_uuid":"3a44797f021bebd12a21209016725477da12dedb"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"64da032a-cb67-468c-832a-d62697c61676","collapsed":true,"_uuid":"a6d47c1647a00c568af6a4805a3afc6f19104309"},"outputs":[],"execution_count":null,"source":"from sklearn.model_selection import cross_val_score","cell_type":"code"},{"metadata":{"_cell_guid":"83003766-6fa4-47b4-ad85-b57d1c2508b7","collapsed":true,"_uuid":"430417a32070e93f7b13fbc018076e1958f646e2"},"outputs":[],"execution_count":null,"source":"def rsme(model, X, y):\n    cv_scores = -cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=10)\n    return np.sqrt(cv_scores)","cell_type":"code"},{"source":"## Ordinary Least Squares (OLS) Linear Regression, No Regularization\nWe'll start by doing a simple OLS LR and see how we do.\n\nLinear regression models can be heavily impacted by outliers, and I in fact ran into this very issue when I started running models. I ended up discovering that keeping all of the dummy features was blowing up the coefficients, and thereby the predictions and RSME.","metadata":{"_cell_guid":"8a4bed66-51a6-44b0-b421-99b2fee575f0","_uuid":"e56eae99fa6674edb7a3f7d51d59a2ec2ef07787"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"1a3cb1df-40fd-4ed7-a9fa-1c4b1e277141","collapsed":true,"_uuid":"088cd4346da7b9722c1edba1f46b45aafa0cf605"},"outputs":[],"execution_count":null,"source":"from sklearn.linear_model import LinearRegression","cell_type":"code"},{"metadata":{"_cell_guid":"0d2136b7-dc70-4315-aa11-4ee70936657b","_uuid":"1d1d35f3b6f1d95f73244e865d7e87aa8489f654"},"outputs":[],"execution_count":null,"source":"# What we're doing here is adding the dummy features for \n# one categorical feature at a time and running the regression.\ndummy_cols = [col for col in model_data.columns if col not in num_features]\nfeatures_to_try = []\nfor cat in cat_features:\n    cat_dummies = [c for c in dummy_cols if c.startswith(cat)]\n    features_to_try += cat_dummies\n    X_train_subset = X_train[num_features + boolean_features + features_to_try]\n    X_test_subset = X_test[num_features + boolean_features + features_to_try]\n    \n    lr = LinearRegression()\n    lr.fit(X_train_subset, y_train)\n    \n    print('Dummy Features: {} | Train RSME: {:.3f} | Test RSME: {:.3f}'.format(\n        len(features_to_try), rsme(lr, X_train_subset, y_train).min(), rsme(lr, X_test_subset, y_test).min()))","cell_type":"code"},{"source":"We can see that both the Train as well as Test RSME score ends up exploding, so let's skip ahead to regressions using regularization, which keep just this type of issue from happening.","metadata":{"_cell_guid":"8980eb72-c623-4355-b77c-3f2569e4919a","_uuid":"caad77325493d5a3591e084143b603c85e4ba1a4"},"cell_type":"markdown"},{"source":"## Ridge Regression\nRidge Regression is an L2 penalized model where the squared sum of the weights are added to the OLS cost function.","metadata":{"_cell_guid":"41bb184d-8990-422f-a951-de7e7b8d5936","_uuid":"16f5fa72b9acb3be1887fe8eabee0576efb64ae1"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"057474a7-f362-4090-b122-5d9a82b8c03d","collapsed":true,"_uuid":"cc1fa3bbf99d8c54fc55e57ba06b44f80938ca6c"},"outputs":[],"execution_count":null,"source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge","cell_type":"code"},{"metadata":{"_cell_guid":"146afaf2-312e-4814-b2b1-3861d584a16b","_uuid":"4046b9cef9107ff7d5e034ec73f524154d534546"},"outputs":[],"execution_count":null,"source":"# We're using GridSearch here to find the optimal alpha value\n# Get the ballpark\nparam_grid = {'alpha': [0.01, 0.1, 1., 5., 10., 25., 50., 100.]}\nridge = GridSearchCV(Ridge(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error')\nridge.fit(X_train, y_train)\nalpha = ridge.best_params_['alpha']\n\n# Hone in\nparam_grid = {'alpha': [x/100. * alpha for x in range(50, 150, 5)]}\nridge = GridSearchCV(Ridge(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error')\nridge.fit(X_train, y_train)\nalpha = ridge.best_params_['alpha']\nridge = ridge.best_estimator_\n\nprint('Ridge -> Train RSME: {:.5f} | Test RSME: {:.5f} | alpha: {:.5f}'.format(\n    rsme(ridge, X_train, y_train).mean(), rsme(ridge, X_test, y_test).mean(), alpha))","cell_type":"code"},{"metadata":{"_cell_guid":"327beaa4-bf08-4feb-9abd-9854d1120892","collapsed":true,"_uuid":"cd5d1f9fd9903e598c5ea061607831a624aaa80c"},"outputs":[],"execution_count":null,"source":"def model_evaluation_plots(model, X_train, y_train, X_test, y_test):\n    y_train_preds = model.predict(X_train)\n    y_test_preds = model.predict(X_test)\n    \n    plt.figure(figsize=(12,6))\n    # Residuals\n    plt.subplot(121)\n    plt.scatter(y_train_preds, y_train_preds - y_train, c='blue', marker='o', label='Training data')\n    plt.scatter(y_test_preds, y_test_preds - y_test, c='orange', marker='s', label='Validation data')\n    plt.title('Residuals')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residuals')\n    plt.legend(loc='upper left')\n    plt.hlines(y=0, xmin=y_train.min(), xmax=y_train.max(), color='red')\n\n    # Predictions\n    plt.subplot(122)\n    plt.scatter(y_train_preds, y_train, c='blue', marker='o', label='Training data')\n    plt.scatter(y_test_preds, y_test, c='orange', marker='s', label='Validation data')\n    plt.title('Predictions')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Real values')\n    plt.legend(loc='upper left')\n    plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], c='red')\n    plt.tight_layout()\n    plt.show()","cell_type":"code"},{"metadata":{"_cell_guid":"970b3347-aff3-4a10-8531-5d8d63cfe754","_uuid":"6403cbc24091899ebeb9bef1d3f98ef2fe64dafe"},"outputs":[],"execution_count":null,"source":"model_evaluation_plots(ridge, X_train, y_train, X_test, y_test)","cell_type":"code"},{"source":"Looking at the residual plot, we see that the residuals are randomly distributed around the center line, which is good. If we saw patterns here it means that the model is unable to capture some explanatory information. We see that our predictions plotted against real values center around the line of perfect fit, again a good sign","metadata":{"_cell_guid":"6d9f3ae5-44e0-450d-940c-6394deb918c9","_uuid":"7c1cdeafc3320baee2954c84091aee56c4dfa41e"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"9a14b380-6c62-43e4-b632-1af6836c12e7","_uuid":"451aa9417220ce749c28a8abc19b4fefc97d108f"},"outputs":[],"execution_count":null,"source":"# Let's take a look at what the Ridge regression made of our features.\ncoefs = pd.DataFrame({'coefs':ridge.coef_,'Positive':ridge.coef_ > 0}, index=X_train.columns)\ncoefs['coefs_abs'] = np.abs(coefs.coefs)\nprint('Ridge dropped {} of {} features.'.format(\n    sum(coefs.coefs == 0), coefs.shape[0]))\n\ntop_coefs = coefs.sort_values('coefs_abs', ascending=False).head(20)\nplt.figure(figsize=(8,10))\nsns.barplot( top_coefs.coefs_abs, top_coefs.index, orient='h', hue=top_coefs.Positive)\nplt.title('Ridge Regression: Top Features')\nplt.xlabel('Absolute Coeficient')\nplt.show()","cell_type":"code"},{"source":"This seems to make a lot of sense. What drives house price? House size, quality, neighborhood, age...","metadata":{"_cell_guid":"1561f8bf-970d-429f-aa8e-3cb05e8457d4","_uuid":"92c9f78352e48f753a4495ad491f048befd00710"},"cell_type":"markdown"},{"source":"## LASSO Regression\nTHe LASSO (Least Absolute Shrinkage and Selection Operator) regression is an L1 regularizaton model in which the sum of weights is added to the cost function","metadata":{"_cell_guid":"b67e4245-80ab-441c-8f90-382e6ff71a84","_uuid":"d81599213bad5ad8f3c0309edcb38b2d4f820698"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"6d5972e2-9848-420d-a1b0-d79a741ce276","collapsed":true,"_uuid":"3e47243261f67a3731a9cfaf64479a05a113c444"},"outputs":[],"execution_count":null,"source":"from sklearn.linear_model import Lasso","cell_type":"code"},{"metadata":{"_cell_guid":"d02923b3-53cf-422b-81a0-e2480f9d5c99","_uuid":"bde2e5ef1422bf3bfc882ab800944ad90f5f5ca4"},"outputs":[],"execution_count":null,"source":"# We're using GridSearch here to find the optimal alpha value\n# Get the ballpark\nparam_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1., 5., 10., 25.], 'max_iter': [50000]}\nlasso = GridSearchCV(Lasso(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error')\nlasso.fit(X_train, y_train)\nalpha = lasso.best_params_['alpha']\n\n# Hone in\nparam_grid = {'alpha': [x/100. * alpha for x in range(50, 150, 5)], 'max_iter': [50000]}\nlasso = GridSearchCV(Lasso(), cv=5, param_grid=param_grid, scoring='neg_mean_squared_error')\nlasso.fit(X_train, y_train)\nalpha = lasso.best_params_['alpha']\nlasso = lasso.best_estimator_\n\nprint('Lasso -> Train RSME: {:.5f} | Test RSME: {:.5f} | alpha: {:.5f}'.format(\n    rsme(lasso, X_train, y_train).mean(), rsme(lasso, X_test, y_test).mean(), alpha))","cell_type":"code"},{"source":"Nice! We've improved our Train and Test score and reduced the difference between the two, suggesting we are dealing with overfitting.","metadata":{"_cell_guid":"9d36d245-828b-4df4-8817-89df308cfd12","_uuid":"1a24ebeb69b6204ca6b801bd3322ee03f27315a1"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"5c478cae-be79-4615-9bd9-abf90f3981c8","_uuid":"16fb32246318d76984076658e6721904ff969542"},"outputs":[],"execution_count":null,"source":"# Let's take a look at what the LASSO regression made of our features.\ncoefs = pd.DataFrame({'coefs':lasso.coef_,'Positive':lasso.coef_ > 0}, index=X_train.columns)\ncoefs['coefs_abs'] = np.abs(coefs.coefs)\nprint('LASSO dropped {} of {} features.'.format(\n    sum(coefs.coefs == 0), coefs.shape[0]))\n\ntop_coefs = coefs.sort_values('coefs_abs', ascending=False).head(20)\nplt.figure(figsize=(8,10))\nsns.barplot( top_coefs.coefs_abs, top_coefs.index, orient='h', hue=top_coefs.Positive)\nplt.title('LASSO Regression: Top Features')\nplt.xlabel('Absolute Coeficient')\nplt.show()","cell_type":"code"},{"source":"Interesting. LASSO dropped almost 2/3 of features. Some of the negative coefficients also surprise me, e.g. *HasPorch* and *TotalBsmtSF*.","metadata":{"_cell_guid":"9d687e72-f9db-4144-a83c-e4fcd4294447","_uuid":"f88c1ca73f780864c7fab8a7643a2f9b8be9225b"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"a7fc186c-7c7c-4227-b907-6977025cc866","_uuid":"d8d6d17f95255ac7b0572b9f2fadd4615ed3486c"},"outputs":[],"execution_count":null,"source":"model_evaluation_plots(lasso, X_train, y_train, X_test, y_test)","cell_type":"code"},{"source":"## Kernel Ridge Regression\n\nKernel Ridge Regression combines ridge regression with the kernel trick.","metadata":{"_cell_guid":"7707e184-41b9-40a1-a068-26ee24c42b37","_uuid":"adb0d4bedee4040c3fe96614c7677e2cd4a6cf5b"},"cell_type":"markdown"},{"metadata":{"_cell_guid":"d9e14787-b8e7-4dcd-a7df-de3823190f6f","collapsed":true,"_uuid":"d2b939f71f22494f3dec11bb899f6dce04106745"},"outputs":[],"execution_count":null,"source":"from sklearn.kernel_ridge import KernelRidge","cell_type":"code"},{"metadata":{"_cell_guid":"0f8191db-21d5-4e3f-b7ac-5247e8402665","_uuid":"4986e21237d736672ab656284b2797ceecc4c407"},"outputs":[],"execution_count":null,"source":"param_grid = {'alpha': [1e0, 1e-1, 1e-2],\n              'kernel': ['polynomial'],\n              'degree': [2,3,4],\n              'gamma': [1e-2, 1e-3, 1e-4]}\nkrr = GridSearchCV(KernelRidge(), cv=5, param_grid=param_grid)\nkrr.fit(X_train, y_train)\nalpha = krr.best_params_['alpha']\ndegree = krr.best_params_['degree']\ngamma = krr.best_params_['gamma']\n\n# Hone in\nparam_grid = {'alpha': [x/100. * alpha for x in range(50, 150, 25)],\n              'kernel': ['polynomial'],\n              'degree': [degree],\n              'gamma': [x/100. * gamma for x in range(50, 151, 50)]}\nkrr = GridSearchCV(KernelRidge(), cv=5, param_grid=param_grid)\nkrr.fit(X_train, y_train)\nalpha = krr.best_params_['alpha']\ndegree = krr.best_params_['degree']\ngamma = krr.best_params_['gamma']\nkrr = krr.best_estimator_\n\nprint('Ridge -> Train RSME: {:.5f} | Test RSME: {:.5f} | alpha: {:.5f} | degree: {} | gamma: {:.5f}'.format(\n    rsme(ridge, X_train, y_train).mean(), rsme(ridge, X_test, y_test).mean(), alpha, degree, gamma))","cell_type":"code"},{"source":"LASSO still seems to perform better.","metadata":{"_cell_guid":"7eb9e595-6622-44a4-827f-54d8ce86e51f","_uuid":"475212188039a6b21e5061dec26abcbc7be21f39"},"cell_type":"markdown"},{"source":"# Conclusion\n\nWe started off building an intuition about the data, dove into the details, made decisions about what to do with our features, and finally compared several different regression models. In the end we achieved the best results using the LASSO regression. \n\nNext steps would be to do additional feature engineering, further experiment with other models, and start combining them into ensembles to push the envelope. \n\nI hope this notebook was helpful, and if you see any mistakes, have any questions, or any other comments, please leave them below. \n\nThanks for reading!","metadata":{"_cell_guid":"274b9783-e36a-4800-ac15-53f63a0e96b4","_uuid":"29a32518933d08e39f894b68aa847b974c632a51"},"cell_type":"markdown"}]}