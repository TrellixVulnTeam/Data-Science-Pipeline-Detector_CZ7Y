{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import graphviz\nimport itertools\nimport matplotlib.pyplot as plt # graphing with insane defaults\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # graphing with sane defaults\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nfrom sklearn.base import BaseEstimator\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, KFold\nimport sklearn.preprocessing as preprocessing # Preprocess data (e.g. scale numerical data to 0-1\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom termcolor import colored, cprint\nimport typing # Apply common types to objects)\nfrom yellowbrick.features import PCA as yellowPCA\n\nfeature_engineering = typing.TypeVar('rabbitml.feature_engineering')\n\nclass rabbitml:\n    \"\"\"\n    An automl library designed for tabular data\n\n    @Taran Sean Marley \n    https://www.kaggle.com/taranmarley\n    \"\"\"\n    class feature_engineering:\n        \"\"\"\n        A class intended to move through and improve the features of a dataset.\n        \"\"\"\n        \n        def auto_casefold(self, df : pd.DataFrame) -> pd.DataFrame:\n            \"\"\"\n            Take a dataframe, find the string columns and convert them all to lower case through casefold\n\n            Parameters\n            ----------\n            dataframe : pd.DataFrame\n                The dataframe to casefold over to convert to lower case\n\n            Returns\n            -------\n            pd.DataFrame\n                The same dataframe given with the new lorrrwer case values if applied\n\n            \"\"\"\n            for col in df.columns:\n                if self.is_string_type(df[col]):\n                    df[col] = df[col].astype(str).str.casefold()\n            return df\n\n        def break_up_by_string(self, df_temp : pd.DataFrame, splitting_string : str) -> pd.DataFrame:\n            \"\"\"\n            Break up columns by string to create new columns from each split.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to start splitting up object columns\n            splitting_string : str\n                String to split up columns by\n\n\n            Returns\n            -------\n            pd.DataFrame\n                modified dataframe with extra columns containing split up values\n            \"\"\"\n            obj_cols = df_temp.select_dtypes(include=[object])\n            # count spaces\n            for col in obj_cols:\n                if df_temp[col].str.contains(splitting_string).sum() > 0:\n                    df2 = df_temp[col].str.split(splitting_string, expand=True)\n                    # Rename columns\n                    rename_dict = {}\n                    for rename_col in df2.columns:\n                        if (splitting_string != \" \"):\n                            rename_dict[rename_col] = col + splitting_string + str(rename_col)\n                        else:\n                            rename_dict[rename_col] = col + str(rename_col)\n                    df2 = df2.rename(columns=rename_dict)\n                    df2 = df2.fillna(0)\n                    df_temp = pd.concat([df_temp,df2], axis=1) \n            return df_temp\n        \n        def compare_object_columns(self, df_temp : pd.DataFrame, df_temp_2 : pd.DataFrame, silent = False, replace = False) -> None:\n            \"\"\"\n            Compare object columns and print out the if there is a difference between them. This helps determining the differences between a test dataframe and a training dataframe\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                First dataframe to compare columns with\n            df_temp_2 : pd.DataFrame\n                Second dataframe to compare columns with\n            silent : bool\n                Print the results or not\n            replace : bool\n                Replace bad values in df_temp with NaN values\n            \"\"\"\n            for col in df_temp.select_dtypes(include=\"object\").columns:\n                if col in df_temp_2.columns:\n                    unique_df_list = df_temp[col].unique().tolist()\n                    test_df_list = df_temp_2[col].unique().tolist()\n                    if set(unique_df_list) != set(test_df_list):\n                        unique_df_list = [\"nan\" if x is np.nan else x for x in unique_df_list]\n                        test_df_list = [\"nan\" if x is np.nan else x for x in test_df_list] \n                        unique_df_list.sort()\n                        test_df_list.sort()\n                        # Print lists if requested\n                        if not silent:\n                            print(\"***\",col)\n                            print(unique_df_list)\n                            print(test_df_list)\n                        # Replace with NaN if requested by parameter \n                        for x in unique_df_list:\n                            if x not in test_df_list:\n                                df_temp[col].replace({x:np.nan})\n\n        def create_interactions(self, df_temp : pd.DataFrame, column_list : typing.List) -> pd.DataFrame:\n            \"\"\"\n            Create interactions by totalling and multiplying columns within a dataframe\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to create interactions in\n            column_list : typing.List\n                List of columns to create interactions from\n\n            Returns\n            ----------\n            pd.DataFrame\n                Dataframe with interactions added\n            \"\"\"\n            # Cross wise multiplication interactions\n            for x in itertools.combinations(column_list, 2):\n                df_temp[x[0]+\"_X_\"+x[1]] = df_temp[x[0]] * df_temp[x[1]]\n                df_temp = df_temp.copy()\n            # Iterative Totals\n            iterative_total = 0\n            i = 0\n            for j in (column_list):\n                iterative_total = iterative_total + df_temp[j]\n                if i > 0:\n                    df_temp[\"A\" + str(i) + \"_iter_score\"] = iterative_total\n                    df_temp = df_temp.copy()\n                i = i + 1\n            return df_temp\n            \n        def detect_continous_columns(self, df_temp : pd.DataFrame, ratio : float = 0.05, continous_columns : typing.List = []) -> typing.List:\n            \"\"\"\n            Detect the continous columns in a dataframe. Columns that have more than the given ratio by total length of dataframe will be considered continous.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to detect continous columns in. This is assumed to already be encoded to a numerical format\n            ratio : float / int\n                Ratio of the total length of dataframe that will be used to cull continous from discrete data, if given as an int then this is consider to be a discrete number instead of a ratio\n            continous_columns : typing.List\n                Continous columns that can be given to the function without checking\n\n            Returns\n            ----------\n            typing.List\n                List of columns found\n            \"\"\"\n            continous_cutoff : int = round(ratio * len(df_temp))\n            if ratio > 1:\n                continous_cutoff = ratio\n            for col in df_temp.columns:\n                if not self.is_string_type(df_temp[col]):\n                    if col not in continous_columns:\n                        if df_temp[col].nunique() > continous_cutoff:\n                            continous_columns.append(col)\n            return continous_columns\n\n        def detect_duplicates(self, df_temp : pd.DataFrame, silent : bool = False, id_cols : typing.List = []) -> None: \n            \"\"\"\n            Detect duplicates in data and return the columns in which duplicates where detected.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to detect duplicates in\n            silent : bool\n                Whether to run print statements \n            id_cols : typing.List\n                Given id cols that aren't auto detected - Useful if there is an obvious ID column that also wants to be detected for duplication\n            \"\"\"\n            # Filter out identity columns\n            cols_to_use = []\n            for col in df_temp.columns:\n                if len(df_temp[col].unique()) != len(df_temp[col]):\n                    cols_to_use.append(col)\n                elif col not in id_cols:\n                    id_cols.append(col)\n            id_temp = df_temp.copy()[id_cols]\n            df_temp = df_temp.copy()[cols_to_use]    \n            count_dupes = df_temp.duplicated().sum()\n            count_dupes_in_ID = id_temp.duplicated().sum()\n            if not silent:\n                print('Duplicates in data: ', str(count_dupes))\n                print('Duplicates in id columns: ', str(count_dupes_in_ID))\n                print('When filtering out id columns: ', str(id_cols))\n\n        def detect_nans(self, df_temp : pd.DataFrame, name = '', silent : bool = False, plot : bool = True) -> typing.List:\n            \"\"\"\n            Detect NaNs in a provided dataframe and return the columns that NaNs were detected in     \n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to detect NaN values in\n            name : str\n                Name of the dataframe which helps give a more descriptive read out\n            silent : bool\n                Whether the print statements should fire\n            plot : bool\n                Whether to return a plot of the counts of NaNs in the data\n\n            Returns\n            -------\n            typing.List\n                List of columns in the provided dataframe that contain NaN values\n            \"\"\"\n            plt.rcParams[\"figure.figsize\"] = (9,9)\n            \n            count_nulls = df_temp.isnull().sum().sum()\n            columns_with_NaNs = []\n            # Count NaNs by column\n            if count_nulls > 0:\n                for col in df_temp.columns:\n                    if df_temp[col].isnull().sum().sum() > 0:\n                        columns_with_NaNs.append(col)\n            # Print out the NaN values\n            if not silent:            \n                if name != '': \n                    print('******')\n                    cprint('Detecting NaNs in ' + str(name), attrs=['bold'])\n                    print('******')\n                print('NaNs in data:', count_nulls)\n                if count_nulls > 0:\n                    print('******')\n                    for col in columns_with_NaNs:\n                        print('NaNs in', col + \": \", df_temp[col].isnull().sum().sum())\n                    print('******')\n            print('')\n            # Plot the NaN values in columns in bar plot\n            if plot and count_nulls > 0:\n                sns.barplot(y=df_temp[columns_with_NaNs].isnull().sum().index, x=df_temp[columns_with_NaNs].isnull().sum().values).set_title(str(name) + \" NaNs\")\n                plt.show()\n            return columns_with_NaNs\n        \n        def detect_id_columns(self, df_temp : pd.DataFrame) -> typing.List:\n            \"\"\"\n            Detect which columns are ID columns, those for which one unique value exists for each row.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to detect ID columns\n\n            Returns\n            -------\n            typing.List\n                List of Identity columns that were detected\n            \"\"\"\n            id_cols = []\n            for col in df_temp.columns:\n                if len(df_temp[col].unique()) == len(df_temp[col]):\n                    id_cols.append(col)\n            return id_cols\n\n        def drop_unshared_columns(self, df_temp : pd.DataFrame, df_temp_2 : pd.DataFrame, exclude_columns : typing.List) -> None:\n            \"\"\"\n            Detect which columns are not shared between the two dataframes excepting for a target_col if provided.\n            Delete in place.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to check for shared columns        \n            df_temp_2 : pd.DataFrame\n                Second dataframe to check for shared columns\n            exclude_columns : typing.List\n                Columns not to remove in this process\n            \"\"\"    \n            drop_cols : typing.List = []\n            for col in df_temp_2.columns:\n                if col not in df_temp.columns:\n                    if col not in exclude_columns:\n                        drop_cols.append(col)\n            df_temp_2.drop(columns=drop_cols, axis=1, inplace=True)\n            drop_cols : typing.List = []\n            for col in df_temp.columns:\n                if col not in df_temp_2.columns:\n                    if col not in exclude_columns:\n                        drop_cols.append(col)\n            df_temp.drop(columns=drop_cols, axis=1, inplace=True)\n                        \n        def encode_binary_object(self, series : pd.Series) -> pd.Series:\n            \"\"\"\n            Encode a binary object series\n\n            Parameters\n            ----------\n            series : pd.Series\n                The series to be encoded. \n\n            Returns\n            -------\n            pd.Series\n                The encoded series\n            \"\"\"\n            map_dict = {}\n            series_list = series.unique().tolist()\n            series_list.sort()\n            for i, x in enumerate(series_list):\n                map_dict[x] = i\n            series = series.map(map_dict)\n            return series\n        \n        def encode_columns(self, df : pd.DataFrame, columns : pd.Series, test_df : pd.DataFrame = None, cutoff : int = 12) -> typing.Tuple[pd.DataFrame, pd.DataFrame]:\n            \"\"\"\n            Encode columns based on the number of unique values in each column\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to encode columns in \n            columns : pd.Series\n                Columns to encode\n            test_df : pd.DataFrame\n                Test dataframe to encode based on classes in the Dataframe\n            cut_off : int\n                The cut off number of classes to choose between label encoding and get dummies. This keeps the dimensionality under control\n\n            Returns\n            -------\n            (pd.DataFrame, pd.DataFrame)\n                Original dataframe and the test dataframe\n            \"\"\"    \n            for col in columns:\n                le = preprocessing.LabelEncoder()\n                classes_to_encode = df[col].astype(str).unique().tolist()\n                classes_to_encode.sort()\n                classes_to_encode.append('None')\n                le.fit(classes_to_encode)\n                # Get dummies except for binary variables which are handled by len(le.classes) != 3\n                if len(le.classes_) < cutoff and len(le.classes_) != 3:\n                    df = pd.get_dummies(df, columns = [col])\n                    if test_df is not None:\n                        test_df = pd.get_dummies(test_df, columns = [col])\n                else:\n                    # First test for binary variables that should be encoded and change things if that is the case\n                    binary_detected = False\n                    if df[col].nunique() == 2 and df[col].isnull().sum().sum() == 0:\n                        # Detect test_df exists and is binary and the unqiue values of test compare the unique values in regular df\n                        if test_df is not None and test_df[col].nunique() == 2 and test_df[col].isnull().sum().sum() == 0 and set(test_df[col].unique()) == set(df[col].unique()): \n                            binary_detected = True\n                        elif test_df is None:\n                            binary_detected = True\n                    if binary_detected:\n                        classes_to_encode.remove('None')\n                        le.fit(classes_to_encode)\n                    # If no test dataframe encode as normal else we should clear out classes not found in test\n                    if test_df is None:\n                        df[col] = le.transform(df[col].astype(str))\n                    else:\n                        check_col = df.copy()[col]\n                        #Clean out labels in train that aren't in test\n                        input_dict = {}\n                        for unique in df[col].unique():\n                            if unique not in pd.unique(test_df[col]) and not binary_detected:\n                                input_dict[unique] = 'None'\n                        df[col] = df[col].replace(input_dict)                        \n                        df[col] = le.transform(df[col].astype(str))\n                        #Clean out unseen labels in test\n                        input_dict = {}\n                        for unique in test_df[col].unique():\n                            if unique not in pd.unique(check_col) and not binary_detected:\n                                input_dict[unique] = 'None'\n                        test_df[col] = test_df[col].replace(input_dict)\n                        test_df[col] = le.transform(test_df[col].astype(str))\n            return df, test_df\n\n        def fill_nans_create_columns(self, df_temp : pd.DataFrame, columns : typing.List, value : float = 0) -> pd.DataFrame:\n            \"\"\"\n            Fill NaN of provided columns and create columns to signify they weren't there.\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to modify\n            columns : typing.List\n                Columns of the provided dataframe to modify\n            value : float\n                Value to replace the NaN values with\n\n            Returns\n            -------\n            pd.DataFrame\n                Modified Dataframe with NaNs filled and new columns signifying the rows that contained NaNs\n            \"\"\"\n            for col in columns:\n                df_temp[col + \"_was_null\"] = df_temp[col].isnull().astype(int)\n                df_temp[col] = df_temp[col].fillna(value)\n            return(df_temp)\n        \n        def is_string_type(self, series: pd.Series) -> bool:\n            \"\"\"\n            Detect if a series contains is a string type \n\n            Parameters\n            ----------\n            series : pd.Series\n                The series to detect the presence of a string type\n\n            Returns\n            -------\n            bool\n                Whether a string type was detected or not\n\n            @Inspired by work by https://stackoverflow.com/users/3876599/yourstruly\n            \"\"\"\n            if pd.StringDtype.is_dtype(series.dtype):\n                # Is a string extension type\n                return True\n\n            if series.dtype != \"object\":\n                # No object column - definitely not a string\n                return False\n\n            try:\n                series.str\n            except AttributeError:\n                return False\n\n            return True        \n\n        def quantile_transform_column_wise(self, df_temp : pd.DataFrame, target_col : str = \"\") -> pd.DataFrame:\n            \"\"\"\n            Transform values in dataframe to quantile uniform distribution\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to quantile transform \n            target_col : str\n                This is the target col and is not transformed\n\n            Returns\n            -------\n            pd.DataFrame\n                Modified dataframe\n            \"\"\"    \n            df_temp = df_temp.copy()\n            # find n_samples\n            n_samples : int = 1000\n            if len(df_temp) < 1000:\n                n_samples = len(df_temp)\n            for col in df_temp.columns:\n                if col != target_col:\n                    transformed = preprocessing.QuantileTransformer(random_state=1, n_quantiles=n_samples).fit_transform(df_temp[col].values.reshape(-1, 1))\n                    df_temp[col] = pd.Series(transformed[:,0], index=df_temp[col].index, name=df_temp[col].name)\n            return df_temp\n        \n        def min_max_column_wise(self, df_temp : pd.DataFrame, target_col : str = \"\"):\n            df_temp = df_temp.copy()\n            for col in df_temp.columns:\n                if col != target_col:\n                    df_temp[col] = preprocessing.MinMaxScaler().fit_transform(df_temp[col].values.reshape(-1, 1))\n            return df_temp\n        \n        def pipeline(self, df_temp : pd.DataFrame, test_df_temp : pd.DataFrame = None, target_col : str = None) -> typing.Tuple[pd.DataFrame, pd.DataFrame]:\n            \"\"\"\n            A pipeline through which the data is processed and feature engineered\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to process features of \n            test_df_temp : pd.DataFrame\n                The test dataframe to process features of \n            target_col : str\n                The optional target column that won't be processed due to this being problematic for the end result\n                \n            Returns \n            -------\n            (pd.DataFrame, pd.DataFrame)\n                \n            \"\"\"\n            df_temp = df_temp.copy()\n            target = None\n            if target_col != \"\":\n                target = df_temp[target_col]\n                df_temp = df_temp.drop(columns=target_col) \n            self.detect_nans(df_temp, \"Training Data\")\n            self.fill_nans_create_columns(df_temp, df_temp.columns)\n            self.detect_duplicates(df_temp)\n            continous_columns = self.detect_continous_columns(df_temp, 20, continous_columns=[])\n            self.create_interactions(df_temp, continous_columns).copy()\n            # Process test dataframe if it exists\n            if test_df_temp is not None:\n                self.detect_nans(test_df_temp, \"Testing Data\" )\n                self.fill_nans_create_columns(test_df_temp, test_df_temp.columns)\n                self.detect_duplicates(test_df_temp)\n                continous_columns = self.detect_continous_columns(test_df_temp,20, continous_columns=[])\n                self.create_interactions(test_df_temp, continous_columns).copy()\n            df_temp, test_df_temp = self.encode_columns(df_temp, df_temp.select_dtypes(include=\"object\").columns, test_df_temp)\n            # Detect Ids and remove them\n            id_cols = self.detect_id_columns(df_temp)\n            if id_cols is not None and len(id_cols) > 0:\n                df_temp = df_temp.drop(columns=id_cols)\n            if test_df_temp is not None:\n                self.drop_unshared_columns(df_temp, test_df_temp, target_col)\n            if target_col != \"\":\n                df_temp[target_col] = target\n            return df_temp, test_df_temp\n        \n    class eda:\n        \"\"\"\n        A set of tools for Exploratory Data Analysis\n        \"\"\"\n\n        def class_balance(self, df_temp : pd.DataFrame, target_col : str) -> None:\n            \"\"\"\n            Display and show a plot of the target categorical value\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to find class balance in \n            target_col : str\n                Name of column with which to find the target categorical value\n            \"\"\"\n            sns.countplot(x=df_temp[target_col])\n            plt.show()\n            column_values = df_temp[target_col].values.ravel()\n            unique_values = pd.unique(column_values)\n            unique_values = np.sort(unique_values)\n            for value in unique_values:\n                print(value,\":\",(len(df_temp.loc[df_temp[target_col] == value]) / len(df_temp)) * 100, \"%\")\n\n        def pca_dimension_reduction_info(self, df_temp : pd.DataFrame, target_col : str) -> None:\n            \"\"\"\n            Examine the results of dimensionality reduction on the dataset\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                DataFrame to conduct PCA on\n            target_col : str\n                target column to remove before conducting PCA         \n            \"\"\"\n            df_temp = df_temp.copy()\n            y = df_temp[target_col]\n            X = df_temp.drop(columns=target_col, axis=1)\n            X_scaled = preprocessing.MinMaxScaler().fit_transform(X)\n            print(str(len(X_scaled[0])) + \" initial feature components\")\n            pca = PCA(n_components=0.95)\n            X_p = pca.fit(X_scaled).transform(X_scaled)\n            print(\"95% variance explained by \" + str(len(X_p[0])) + \" components by principle component analysis\")\n            pca = PCA(n_components=3)\n            X_p = pca.fit(X_scaled).transform(X_scaled)\n            print(str(round(pca.explained_variance_ratio_.sum() * 100)) + \"% variance explained by 3 components by principle component analysis\")\n            pca = PCA(n_components=2)\n            X_p = pca.fit(X_scaled).transform(X_scaled)\n            print(str(round(pca.explained_variance_ratio_.sum() * 100)) + \"% variance explained by 2 components by principle component analysis\")\n        \n        def pca_visualisation_2d(self, df_temp : pd.DataFrame, target_col : str) -> None:\n            \"\"\"\n            Visualize 2d\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                The dataframe to use features from for embedding\n            target_col : str\n                The target variable to be dropped from dataframe\n            \"\"\"\n            if len(df_temp) > 3000:\n                df_temp = df_temp.copy().sample(n=2999, random_state=1)\n            y = df_temp[target_col]\n            X = df_temp.drop(columns=[target_col])\n            visualizer = yellowPCA(scale=True, projection=2, alpha=0.4)\n            visualizer.fit_transform(X, y)\n            visualizer.show()\n            plt.show()\n            \n        def pca_visualisation_3d(self, df_temp : pd.DataFrame, target_col : str) -> None:\n            \"\"\"\n            Visualize 3d PCA embedding\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                The dataframe to use features from for embedding\n            target_col : str\n                The target variable to be dropped from dataframe\n            \"\"\"\n            if len(df_temp) > 3000:\n                df_temp = df_temp.copy().sample(n=2999, random_state=1)\n            y = df_temp[target_col]\n            X = df_temp.drop(columns=[target_col])\n            visualizer = yellowPCA(scale=True, projection=3, alpha=0.4, size=(700,700))\n            visualizer.fit_transform(X, y)\n            visualizer.show()\n            plt.show()\n        \n        def box_plots(self, df_temp : pd.DataFrame, columns : typing.List) -> None:\n            \"\"\"\n            Make box plots of different continous columns\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to make box plots of\n            columns : typing.List\n                A list of continous columns to use\n            \"\"\"\n            if len(columns) > 25:\n                columns = columns[:25]\n            fig = plt.figure(figsize = (15, 9))\n            for index,col in enumerate(df_temp[columns]):\n                plt.subplot(5, 5, index + 1)\n                sns.boxplot(y = col, data = df_temp[columns])\n                plt.tight_layout()\n            plt.show()\n        \n        def line_plots(self, df_temp : pd.DataFrame, columns : typing.List) -> None:\n            \"\"\"\n            Make line plots of different continous columns\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Dataframe to make line plots of\n            columns : typing.List\n                A list of continous columns to use\n            \"\"\"    \n            pltdf = df_temp.copy()\n            pltdf = pltdf[columns]\n            pltdf = pltdf.sample(frac=1, random_state=42).reset_index(drop=True)\n            pltdf.iloc[:50, :25].plot(subplots=True, layout=(5,5), figsize=(15,10))\n            plt.show()\n        \n        def calculate_correlations(self, df_temp : pd.DataFrame, target_col : str, n_cols : int = 10, silent : bool = False, visualise : bool = False) -> typing.List:\n            \"\"\"\n            Calculate the pearson correlations between the target variable and the dataframe and returns columns that are beyond a certain ratio correlation \n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                dataframe to examine\n            target_col : str\n                the target column to measure correlation against\n            n_cols : int\n                number of columns to return, this amount of columns with the highest correlation\n            silent : bool\n                whether to print to console\n            visualise : bool\n                whether to display a heatmap of correlations\n            \"\"\"\n            df_temp = df_temp.copy()\n            if not silent:\n                print(\"Correlations with\",target_col + \":\")\n            # Generate correlation list\n            correlations_list = []\n            for col_one in df_temp.iloc[:,:].columns:\n                correlation_value =  abs(df_temp[col_one].corr(df_temp[target_col]))\n                # Check for NaN\n                if (correlation_value == correlation_value):\n                    correlations_list.append((correlation_value,col_one))\n            # Sort List\n            correlations_list = sorted(correlations_list, key=lambda tup: tup[0], reverse=True)\n            # Go through list to find columns to return\n            cols = []\n            for i, row in enumerate(correlations_list):\n                correlation = row[0]\n                col = row[1]\n                if i < n_cols:\n                    cols.append(col)\n                    # print the correlation\n                    if not silent:\n                        print(col, \":\", correlation)            \n            corrdf = df_temp.copy()\n            corrdf = corrdf[cols].corr()\n            sns.heatmap(abs(corrdf), cmap=\"Blues\")\n            return cols\n        \n        def pair_grid_plot(self, df_temp : pd.DataFrame, cols : typing.List) -> None:\n            \"\"\"\n            Pair grid plots of given columns\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                Data to plot from\n            cols : typing.List\n                Columns to make a pairgrid from\n            \"\"\"\n            g = sns.PairGrid(df_temp[cols].iloc[:500,:], diag_sharey=False)\n            g.map_upper(sns.histplot, multiple=\"stack\")\n            g.map_lower(sns.kdeplot)\n            g.map_diag(sns.kdeplot, lw=2)\n\n        def decision_tree(self, df_temp : pd.DataFrame, depth : int, target_col : str, kind : str) -> None:\n            \"\"\"\n            Draw a decision_tree from the given dataframe to the given depth and display it\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                The dataframe to create a decision tree from\n            depth : int\n                The depth of the decision tree to create\n            target_col : str\n                The target column to make a decision tree towards\n            \"\"\"\n            tree_set = df_temp.copy()\n            target = tree_set[target_col]\n            tree_set.drop([target_col], axis=1, inplace=True)\n            tree_clf = DecisionTreeClassifier(max_depth=depth, random_state=1)\n            tree_clf.fit(tree_set, target)\n            text_representation = tree.export_text(tree_clf, feature_names=tree_set.columns.tolist())\n            print(text_representation)\n            print(\"accuracy: \" + str(tree_clf.score(tree_set, target)))    \n            plt.rcParams[\"figure.figsize\"] = (18,18)\n            # tree.plot_tree(tree_clf, feature_names=tree_set.columns, filled=True)\n            if kind == \"categorical\":\n                class_column_values = df_temp[target_col].values.ravel()\n                class_unique_values = pd.unique(class_column_values)\n                class_unique_values = np.sort(class_unique_values)\n                class_unique_values = class_unique_values.astype('str')\n                le = preprocessing.LabelEncoder()\n                target = le.fit_transform(target)\n                dot_data = tree.export_graphviz(tree_clf, out_file=None, \n                                                feature_names=tree_set.columns,  \n                                                class_names=class_unique_values,\n                                                filled=True)\n                return graphviz.Source(dot_data, format=\"png\")\n            return None\n        \n        def pipeline(self, df_temp, target_col : str, fe : feature_engineering, kind=\"categorical\"):\n            \"\"\"\n            A pipeline to apply a bunch of different EDA procedures against the \n            \"\"\"\n            if kind == \"categorical\":\n                self.class_balance(df_temp, target_col)\n                plt.show()\n            else: \n                sns.displot(x=df_temp[target_col])\n                plt.title(\"Class Balance\")\n                plt.show()\n            print(\"***\")\n            cprint(\"Dimensional Reduction\", attrs=['bold'])\n            print(\"***\")\n            self.pca_dimension_reduction_info(df_temp, target_col)\n            plt.show()\n            self.pca_visualisation_2d(df_temp, target_col)\n            plt.show()\n            self.pca_visualisation_3d(df_temp, target_col)\n            plt.show()\n            print(\"***\")\n            cprint(\"Box Plots\", attrs=['bold'])\n            print(\"***\")\n            continous_columns = fe.detect_continous_columns(df_temp, 20, continous_columns=[])\n            self.box_plots(df_temp, continous_columns)\n            plt.show()\n            print(\"***\")\n            cprint(\"Line Plots\", attrs=['bold'])\n            print(\"***\")\n            continous_columns = fe.detect_continous_columns(df_temp, 20, continous_columns=[])\n            self.line_plots(df_temp, continous_columns)\n            plt.show()\n            print(\"***\")\n            cprint(\"Correlations\", attrs=['bold'])\n            print(\"***\")\n            scaled_df = fe.quantile_transform_column_wise(df_temp, target_col)\n            correlations = self.calculate_correlations(df_temp, target_col, 10)\n            plt.show()\n            print(\"***\")\n            cprint(\"Pair Grid Plot\", attrs=['bold'])\n            print(\"***\")\n            self.pair_grid_plot(df_temp, self.calculate_correlations(df_temp, target_col, 5))\n            plt.show()\n            print(\"***\")\n            cprint(\"Decision Tree\", attrs=['bold'])\n            print(\"***\")            \n            tree_graph = self.decision_tree(df_temp, 2, target_col, kind)\n            if tree_graph is not None:\n                display(tree_graph)\n                plt.show()\n    \n    class feature_selection:\n        \"\"\"\n        Determine which features to use for training. \n        \"\"\"\n        def kendall_tau_feature_elimination(df_temp : pd.DataFrame, columns : typing.List, target_col : str, test_p_value : float = 0.001) -> typing.List:\n            \"\"\"\n            Eliminate features that don't pass a Kendall Tau test in regards to the target variable. \n            This would tend to eliminate useless or unhelpful features from the dataset while retaining appropriate ones.\n            A list of features the do pass the test is created and returned. \n\n            Should be used with continous columns only.\n            \n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                DataFrame to conduct Kendall Tau on features \n            columns : typing.List\n                A list of continous columns\n            target_col : str\n                Target column to conduct Kendall Tau towards\n            test_p_value : float\n                P value to test against. The sensitivity for \n                \n            Returns\n            -------\n            typing.List\n                features that passed the Kendall Tau features\n            \"\"\"\n            new_features = []\n            for feature in columns:\n                tau, p_value = stats.kendalltau(df_temp[target_col], df_temp[feature])\n                if p_value <= test_p_value:\n                    new_features.append(feature)\n            return new_features    \n\n        def select_from_model_features(df_temp: pd.DataFrame, target_col : str, estimator : BaseEstimator, threshold : float = None) -> typing.Tuple[typing.List, np.ndarray]:\n            \"\"\"\n            Use a compatible sklearn estimator for determining a list of features that are important to the dataset\n\n            Parameters\n            ----------\n            df_temp : pd.DataFrame\n                dataframe to find the features in\n            target_col : str\n                the name of the target column so it can be selected\n            estimator : BaseEstimator\n                The sklearn estimator to fit on. Must have coef_\n            threshold : float\n                Threshold to decide which features to keep or not\n\n            Returns\n            -------\n            typing.Tuple[typing.List, np.ndarray]\n                List of features selected, array of the coefficients that have been selected\n            \"\"\"\n            X = df_temp.drop(columns=target_col)\n            y = df_temp[target_col]\n            estimator.fit(X, y)\n            model = SelectFromModel(estimator, threshold = threshold, prefit=True) \n            feature_names = np.array(df_temp.drop(columns=target_col).columns)\n            return feature_names[model.get_support()].tolist(), model.estimator.coef_\n    \n    class prediction:\n        \"\"\"\n        Create and compare predictors than analyse them\n        \"\"\"\n        def confusion_matrix_display(clf : BaseEstimator, X : np.ndarray, y : np.ndarray) -> None:\n            \"\"\"\n            Shown the confusion matrix for a given sklearn estimator\n            \n            Parameters\n            ----------\n            clf : BaseEstimator\n                The classifier to use for the confusion matrix\n            X : np.ndarray\n                The numpy array of features\n            y : np.ndarray\n                The numpy array of the target variable\n            \"\"\"\n            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n            predictions = clf.predict(X_test)\n            cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clf.classes_)\n            disp.plot()\n            plt.grid(False)\n            plt.show()\n\n        def linear_classification_compare(X : np.ndarray, y : np.ndarray) -> typing.List:\n            \"\"\"\n            Classify the target col using a number of linear classifiers\n            \n            Parameters\n            ----------\n            X : np.ndarray\n                The features to use for classification\n            y : np.ndarray\n                The target to use the classifiers on\n                \n            Returns\n            -------\n            typing.List\n                A list of classifiers that have been fitted on the data provided\n            \"\"\"\n            classifiers = [(\"RidgeClassifier\", linear_model.RidgeClassifier()), (\"SGDClassifier\", linear_model.SGDClassifier()), (\"LogisticRegression\", linear_model.LogisticRegression(solver=\"liblinear\"))]\n            classification_results = []\n            kfold = KFold(n_splits=5)\n            for entry in classifiers:\n                name : str = entry[0]\n                classifier = entry[1]\n                scores = []\n                for train_index, test_index in kfold.split(X, y):\n                    classifier.fit(X[train_index], y[train_index])\n                    scores.append(classifier.score(X[test_index], y[test_index]))\n                classification_results.append((classifier, (sum(scores) / len(scores))))\n                print(name, \"Score:\", sum(scores) / len(scores))        \n            return classification_results","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-25T01:25:45.705253Z","iopub.execute_input":"2022-03-25T01:25:45.70555Z","iopub.status.idle":"2022-03-25T01:25:45.856112Z","shell.execute_reply.started":"2022-03-25T01:25:45.705521Z","shell.execute_reply":"2022-03-25T01:25:45.855158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nThis is part of a continuing series I have been working on to develop an AutoML library from base components. This has been mainly a learning experience for me but I do intend to eventually establish a github page for it and release it a little more formally.\n\nSo far the results have been promising. The main effort is to produce an AutoML library that is interpretable and uses extremely limited user input to create a baseline ML model to compare against that is free of human bias. \n\nEarlier notebooks can be found at:\n\n[AutoML from Scratch #1](https://www.kaggle.com/code/taranmarley/automl-from-scratch-1/notebook)\n\n[AutoML from Scratch #2](https://www.kaggle.com/code/taranmarley/automl-from-scratch-2/notebook)\n\n[AutoML from Scratch #3](https://www.kaggle.com/code/taranmarley/automl-from-scratch-3/notebook)\n\n[AutoML from Scratch #4](https://www.kaggle.com/code/taranmarley/automl-from-scratch-4/notebook)","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-25T01:25:45.998338Z","iopub.execute_input":"2022-03-25T01:25:45.998649Z","iopub.status.idle":"2022-03-25T01:25:46.045457Z","shell.execute_reply.started":"2022-03-25T01:25:45.998617Z","shell.execute_reply":"2022-03-25T01:25:46.044581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Process Data\n\nWe will make heavy use of our feature engineering pipeline here. The dataframe will be made to be numeric and all object columns will be encoded.","metadata":{}},{"cell_type":"code","source":"fe = rabbitml.feature_engineering()\ndf, test_df = fe.pipeline(df, test_df, target_col=\"SalePrice\")\ndf.head(4)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T01:25:46.397642Z","iopub.execute_input":"2022-03-25T01:25:46.397918Z","iopub.status.idle":"2022-03-25T01:25:49.254223Z","shell.execute_reply.started":"2022-03-25T01:25:46.397889Z","shell.execute_reply":"2022-03-25T01:25:49.253367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"eda = rabbitml.eda()\neda.pipeline(df, \"SalePrice\", fe, \"regression\")\ndisplay()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T01:25:58.787543Z","iopub.execute_input":"2022-03-25T01:25:58.788066Z","iopub.status.idle":"2022-03-25T01:26:19.16028Z","shell.execute_reply.started":"2022-03-25T01:25:58.788025Z","shell.execute_reply":"2022-03-25T01:26:19.159421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-25T01:26:19.161962Z","iopub.execute_input":"2022-03-25T01:26:19.162194Z","iopub.status.idle":"2022-03-25T01:26:19.182699Z","shell.execute_reply.started":"2022-03-25T01:26:19.162166Z","shell.execute_reply":"2022-03-25T01:26:19.181808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_df = fe.quantile_transform_column_wise(df, \"SalePrice\")\nscaled_test_df = fe.quantile_transform_column_wise(test_df, \"SalePrice\")\nnew_features = rabbitml.feature_selection.kendall_tau_feature_elimination(scaled_df, scaled_df.columns, \"SalePrice\", 0.001)\neliminated_df = scaled_df.copy()[new_features]\nnew_features.remove(\"SalePrice\")\neliminated_test_df = scaled_test_df.copy()[new_features]\neliminated_df.head(1)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T02:02:07.964338Z","iopub.execute_input":"2022-03-25T02:02:07.964628Z","iopub.status.idle":"2022-03-25T02:02:10.538706Z","shell.execute_reply.started":"2022-03-25T02:02:07.9646Z","shell.execute_reply":"2022-03-25T02:02:10.537861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction\n\nCurrently there is nothing in my autoML library to tackle regression problems. I aim to attempt to solve that now.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.utils._testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n@ignore_warnings(category=ConvergenceWarning)\ndef linear_regression_compare(X : np.ndarray, y : np.ndarray) -> typing.List:\n    \"\"\"\n    Regress on the target col using a number of linear regressors\n\n    Parameters\n    ----------\n    X : np.ndarray\n        The features to use for regression\n    y : np.ndarray\n        The target to use the regression on\n\n    Returns\n    -------\n    typing.List\n        A list of regressor that have been fitted on the data provided\n    \"\"\"\n    regressors = [(\"LinearRegression\", linear_model.LinearRegression()), (\"RidgeRegression\", linear_model.Ridge()), (\"Lasso\", linear_model.Lasso(max_iter=4000)), (\"ElasticNet\", linear_model.ElasticNet()),(\"LassoLARS\", linear_model.LassoLars(normalize=True)),(\"BayesianRidge\", linear_model.BayesianRidge()), (\"ARDRegression\", linear_model.ARDRegression()), (\"Stochastic Gradient Descent\", linear_model.SGDRegressor())]\n\n    regressor_results = []\n    kfold = KFold(n_splits=5)\n    for entry in regressors:\n        name : str = entry[0]\n        regressor = entry[1]\n        scores = []\n        mae_scores = []\n        for train_index, test_index in kfold.split(X, y):\n            regressor.fit(X[train_index], y[train_index])\n            scores.append(regressor.score(X[test_index], y[test_index]))\n            mae_scores.append(mean_absolute_error(y[test_index], regressor.predict(X[test_index])))\n        regressor_results.append((regressor, (sum(scores) / len(scores))))\n        print(name, \"Score:\", sum(scores) / len(scores))        \n        print(name, \"MAE Accuracy:\", sum(mae_scores) / len(mae_scores))        \n        print(\"***\")        \n    return regressor_results\nregressors = linear_regression_compare(eliminated_df.drop(columns=\"SalePrice\").values, eliminated_df[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-25T01:32:56.613277Z","iopub.execute_input":"2022-03-25T01:32:56.613899Z","iopub.status.idle":"2022-03-25T01:33:34.782733Z","shell.execute_reply.started":"2022-03-25T01:32:56.61386Z","shell.execute_reply":"2022-03-25T01:33:34.781833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sort List\nregressors = sorted(regressors, key=lambda tup: tup[1], reverse=True)\nregressor = regressors[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-25T01:34:26.329443Z","iopub.execute_input":"2022-03-25T01:34:26.32975Z","iopub.status.idle":"2022-03-25T01:34:26.334315Z","shell.execute_reply.started":"2022-03-25T01:34:26.329708Z","shell.execute_reply":"2022-03-25T01:34:26.333689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(eliminated_df.drop(columns=\"SalePrice\").values)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T01:38:50.702934Z","iopub.execute_input":"2022-03-25T01:38:50.703234Z","iopub.status.idle":"2022-03-25T01:38:50.711651Z","shell.execute_reply.started":"2022-03-25T01:38:50.703198Z","shell.execute_reply":"2022-03-25T01:38:50.710838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from yellowbrick.regressor import ResidualsPlot\n\ndef residuals_plot(model : BaseEstimator, X_train : np.ndarray, y_train : np.ndarray, X_test : np.ndarray, y_test : np.ndarray) -> None:\n    \"\"\"\n    Create a residuals plot for a given model\n    \n    Parameters\n    ----------\n    model : BaseEstimator\n        The trained regression model that will be used to make a regression model from\n    X_train : np.ndarray\n        The features to train against\n    y_train : np.ndarray\n        The target to train against\n    X_test : np.ndarray\n        The features to score against\n    y_test : np.ndarray\n        The target to score against\n    \"\"\"\n    plt.rcParams[\"figure.figsize\"] = (9,9)\n    visualizer = ResidualsPlot(model)\n    visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n    visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n    visualizer.show()                 # Finalize and render the figure\n\n# Create the train and test data\nX_train, X_test, y_train, y_test = train_test_split(eliminated_df.drop(columns=\"SalePrice\").values, eliminated_df[\"SalePrice\"], test_size=0.2, random_state=42)\nresiduals_plot(regressor[0], X_train, y_train, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T01:51:02.045062Z","iopub.execute_input":"2022-03-25T01:51:02.045933Z","iopub.status.idle":"2022-03-25T01:51:02.732232Z","shell.execute_reply.started":"2022-03-25T01:51:02.045882Z","shell.execute_reply":"2022-03-25T01:51:02.731408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission","metadata":{}},{"cell_type":"markdown","source":"Train on all the data for submitting","metadata":{}},{"cell_type":"code","source":"regressor[0].fit(eliminated_df.drop(columns=\"SalePrice\").values, eliminated_df[\"SalePrice\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-25T01:53:30.3863Z","iopub.execute_input":"2022-03-25T01:53:30.386594Z","iopub.status.idle":"2022-03-25T01:53:30.432105Z","shell.execute_reply.started":"2022-03-25T01:53:30.386564Z","shell.execute_reply":"2022-03-25T01:53:30.43126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-25T01:56:58.730297Z","iopub.execute_input":"2022-03-25T01:56:58.730596Z","iopub.status.idle":"2022-03-25T01:56:58.738043Z","shell.execute_reply.started":"2022-03-25T01:56:58.730567Z","shell.execute_reply":"2022-03-25T01:56:58.737432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df[\"SalePrice\"] = regressor[0].predict(eliminated_test_df.values)\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-25T01:58:00.639841Z","iopub.execute_input":"2022-03-25T01:58:00.640141Z","iopub.status.idle":"2022-03-25T01:58:00.658453Z","shell.execute_reply.started":"2022-03-25T01:58:00.640107Z","shell.execute_reply":"2022-03-25T01:58:00.657427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nThis has been productive. Previously I had only created an end to end solution in the library for binary classification whereas now I have the rudimentary work done for end to end tackling of regression tasks. Between notebooks there has been development on the library to create the pipelines and the next notebook should have pretty much a full pipeline for regression. I may then work on model interpretability and obviously look at testing more models. I will also need to work on efficiency and ironing out some conventions in my code that I believe could be improved. ","metadata":{}}]}