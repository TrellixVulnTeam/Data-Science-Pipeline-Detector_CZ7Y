{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA and Price Prediction\n### Competition Description\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n### Acknowledgments\n\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. \n<font color = \"green\" >\n\n#### Content:   \n    \n1. [Loading and Checking Data Set](#1)\n1. [Variable Analysis and Visualization](#2) \n    * [Visualization of Categorical Variables](#3)\n    * [Visualization of Numerical Variables](#4)\n1. [Data Analysis](#5)\n    * [Handling Outliers](#7)\n    * [Handling Missing Values and Feature Engineering](#6)\n    * [Relationship Between Some Variables](#8)\n1. [Modelling](#19)\n    * [Hyperparameter Tuning - Grid Search - Cross Validation](#20)","metadata":{"execution":{"iopub.status.busy":"2021-07-26T18:55:27.588113Z","iopub.execute_input":"2021-07-26T18:55:27.588541Z","iopub.status.idle":"2021-07-26T18:55:27.601824Z","shell.execute_reply.started":"2021-07-26T18:55:27.588449Z","shell.execute_reply":"2021-07-26T18:55:27.600272Z"}}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-04T12:09:08.905537Z","iopub.execute_input":"2021-08-04T12:09:08.905911Z","iopub.status.idle":"2021-08-04T12:09:08.911352Z","shell.execute_reply.started":"2021-08-04T12:09:08.905878Z","shell.execute_reply":"2021-08-04T12:09:08.910118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\nd_test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:08.928514Z","iopub.execute_input":"2021-08-04T12:09:08.92888Z","iopub.status.idle":"2021-08-04T12:09:09.005009Z","shell.execute_reply.started":"2021-08-04T12:09:08.928849Z","shell.execute_reply":"2021-08-04T12:09:09.00389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:09.007073Z","iopub.execute_input":"2021-08-04T12:09:09.007399Z","iopub.status.idle":"2021-08-04T12:09:09.03862Z","shell.execute_reply.started":"2021-08-04T12:09:09.007365Z","shell.execute_reply":"2021-08-04T12:09:09.037383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We have 1460 total entries and 80 different column.\n* We have null values in some columns.","metadata":{}},{"cell_type":"code","source":"print(\"First 5 raws of data:\")\nd_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:09.33676Z","iopub.execute_input":"2021-08-04T12:09:09.337271Z","iopub.status.idle":"2021-08-04T12:09:09.403598Z","shell.execute_reply.started":"2021-08-04T12:09:09.337239Z","shell.execute_reply":"2021-08-04T12:09:09.402469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Last 5 raws of data:\")\nd_train.tail()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:09.704518Z","iopub.execute_input":"2021-08-04T12:09:09.704897Z","iopub.status.idle":"2021-08-04T12:09:09.769216Z","shell.execute_reply.started":"2021-08-04T12:09:09.704862Z","shell.execute_reply":"2021-08-04T12:09:09.768269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_train.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:10.052483Z","iopub.execute_input":"2021-08-04T12:09:10.052871Z","iopub.status.idle":"2021-08-04T12:09:10.170716Z","shell.execute_reply.started":"2021-08-04T12:09:10.052839Z","shell.execute_reply":"2021-08-04T12:09:10.169528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"2\"></a>\n## Variable Analysis\n\n#### Here's a brief version of what you'll find in the data description file.\n\n1. SalePrice : the property's sale price in dollars. This is the target variable that you're trying to predict.\n2. MSSubClass: The building class\n3. MSZoning: The general zoning classification\n4. LotFrontage: Linear feet of street connected to property\n5. LotArea: Lot size in square feet\n6. Street: Type of road access\n7. Alley: Type of alley access\n8. LotShape: General shape of property\n9. LandContour: Flatness of the property\n10. Utilities: Type of utilities available\n11. LotConfig: Lot configuration\n12. LandSlope: Slope of property\n13. Neighborhood: Physical locations within Ames city limits\n14. Condition1: Proximity to main road or railroad\n15. Condition2: Proximity to main road or railroad (if a second is present)\n16. BldgType: Type of dwelling\n17. HouseStyle: Style of dwelling\n18. OverallQual: Overall material and finish quality\n19. OverallCond: Overall condition rating\n20. YearBuilt: Original construction date\n21. YearRemodAdd: Remodel date\n22. RoofStyle: Type of roof\n23. RoofMatl: Roof material\n24. Exterior1st: Exterior covering on house\n25. Exterior2nd: Exterior covering on house (if more than one material)\n26. MasVnrType: Masonry veneer type\n27. MasVnrArea: Masonry veneer area in square feet\n28. ExterQual: Exterior material quality\n29. ExterCond: Present condition of the material on the exterior\n30. Foundation: Type of foundation\n31. BsmtQual: Height of the basement\n32. BsmtCond: General condition of the basement\n33. BsmtExposure: Walkout or garden level basement walls\n34. BsmtFinType1: Quality of basement finished area\n35. BsmtFinSF1: Type 1 finished square feet\n36. BsmtFinType2: Quality of second finished area (if present)\n37. BsmtFinSF2: Type 2 finished square feet\n38. BsmtUnfSF: Unfinished square feet of basement area\n38. TotalBsmtSF: Total square feet of basement area\n39. Heating: Type of heating\n40. HeatingQC: Heating quality and condition\n41. CentralAir: Central air conditioning\n42. Electrical: Electrical system\n43. 1stFlrSF: First Floor square feet\n44. 2ndFlrSF: Second floor square feet\n45. LowQualFinSF: Low quality finished square feet (all floors)\n46. GrLivArea: Above grade (ground) living area square feet\n47. BsmtFullBath: Basement full bathrooms\n48. BsmtHalfBath: Basement half bathrooms\n49. FullBath: Full bathrooms above grade\n50. HalfBath: Half baths above grade\n51. Bedroom: Number of bedrooms above basement level\n52. Kitchen: Number of kitchens\n53. KitchenQual: Kitchen quality\n54. TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n55. Functional: Home functionality rating\n56. Fireplaces: Number of fireplaces\n57. FireplaceQu: Fireplace quality\n58. GarageType: Garage location\n59. GarageYrBlt: Year garage was built\n60. GarageFinish: Interior finish of the garage\n61. GarageCars: Size of garage in car capacity\n62. GarageArea: Size of garage in square feet\n63. GarageQual: Garage quality\n64. GarageCond: Garage condition\n65. PavedDrive: Paved driveway\n66. WoodDeckSF: Wood deck area in square feet\n67. OpenPorchSF: Open porch area in square feet\n68. EnclosedPorch: Enclosed porch area in square feet\n69. 3SsnPorch: Three season porch area in square feet\n70. ScreenPorch: Screen porch area in square feet\n71. PoolArea: Pool area in square feet\n72. PoolQC: Pool quality\n73. Fence: Fence quality\n74. MiscFeature: Miscellaneous feature not covered in other categories\n75. MiscVal: \"$\" Value of miscellaneous feature\n76. MoSold: Month Sold\n77. YrSold: Year Sold\n78. SaleType: Type of sale\n79. SaleCondition: Condition of sale\n\n\n<font color = \"green\" >\ndtypes:\n    \n<font color = \"black\" >\n    \n* object : 43\n* int64 : 35\n*float64 3\n\n\n\n<font color = \"green\" >\n\n\nWe have 57 categorical, 24 numerical variable.\n\n<font color = \"black\" >\n\n* **Categorical variables:** 'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'OverallQual', 'OverallCond', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolArea', 'PoolQC', 'Fence', 'MiscFeature', 'MoSold', 'YrSold', 'SaleType', 'SaleCondition'\n\n* **Numerical variables:** 'Id', 'LotFrontage', 'LotArea', 'Neighborhood', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageYrBlt', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'MiscVal', 'SalePrice'","metadata":{}},{"cell_type":"markdown","source":"We have 80 columns in our train data. So it will take time to manually find the types of variables. In order to determine whether the variable is categorical or numerical I will code few lines.\n\nI determine the treshold as 20 for categorical variables.","metadata":{}},{"cell_type":"code","source":"categorical_features = []\nthreshold = 20\nfor each in d_train.columns:\n    if d_train[each].nunique() < threshold:\n        categorical_features.append(each)\n    \nnumerical_features = []\nfor each in d_train.columns:\n    if each not in categorical_features:\n        numerical_features.append(each)\n        \nprint(\"Categorical Variables:\\n\\n\",categorical_features,\"\\n\\n\")        \nprint(\"Numerical Variables:\\n\\n\",numerical_features )","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:10.172185Z","iopub.execute_input":"2021-08-04T12:09:10.172516Z","iopub.status.idle":"2021-08-04T12:09:10.211755Z","shell.execute_reply.started":"2021-08-04T12:09:10.172487Z","shell.execute_reply":"2021-08-04T12:09:10.210913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_features","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:10.213186Z","iopub.execute_input":"2021-08-04T12:09:10.213497Z","iopub.status.idle":"2021-08-04T12:09:10.219912Z","shell.execute_reply.started":"2021-08-04T12:09:10.213468Z","shell.execute_reply":"2021-08-04T12:09:10.218877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"3\"></a>\n### Visualization of Categorical Variables","metadata":{}},{"cell_type":"markdown","source":"Now, I will visualize categorical variables as bar plot. Thus, the densities of categorical variables will be more understandable.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(30,70))\n\nfor i,var in enumerate(categorical_features):\n    \n    plt.subplot(15,4,i+1)\n    sns.countplot(data = d_train, x = var, alpha = 0.3, color=\"red\")\n    sns.countplot(data = d_test, x = var, alpha = 0.5, color = \"green\")","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:10.644716Z","iopub.execute_input":"2021-08-04T12:09:10.645116Z","iopub.status.idle":"2021-08-04T12:09:21.544728Z","shell.execute_reply.started":"2021-08-04T12:09:10.645082Z","shell.execute_reply":"2021-08-04T12:09:21.543552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"4\"></a>\n### Visualization of Numerical Variables","metadata":{}},{"cell_type":"markdown","source":"Now, we will visualize numerical variables as histogram plot. Thus, the densities of numerical variables will be more understandable.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n    \nplt.figure(figsize=(30,40))\n\nfor i,var in enumerate(numerical_features[1:]):\n    if var == \"SalePrice\":\n        break    \n    else:\n        \n        plt.subplot(6,4,i+1)\n    \n        plt.hist(d_train[var], bins=50, color = \"red\", alpha = 0.5, label= \"Train Data\")\n        plt.xlabel(var)\n        plt.ylabel(\"Count\")\n        plt.legend()\n    \n\n\n        plt.hist(d_test[var], bins=50, color = \"green\", alpha = 0.5, label= \"Test Data\")\n        plt.legend()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:21.546327Z","iopub.execute_input":"2021-08-04T12:09:21.546631Z","iopub.status.idle":"2021-08-04T12:09:32.728791Z","shell.execute_reply.started":"2021-08-04T12:09:21.546603Z","shell.execute_reply":"2021-08-04T12:09:32.727628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"5\"></a>\n## Data Analysis","metadata":{}},{"cell_type":"markdown","source":"<a id = \"7\"></a>\n\n\n### Handling Outliers","metadata":{}},{"cell_type":"markdown","source":"Wikipedia definition: In statistics, an **outlier** is an observation point that is distant from other observations.\n\nOutliers may be caused by many different reasons. Such as a mistake during data collection or it can be just an sign of variance in your data. \n\n    1st quartile (Q1): %25\n    2nd quartile (Q2): Median value\n    3rd quartile (Q3): %75\n\n    IQR = Q3 - Q1\n\n    Lower Outlier Limit = Q1 - (1.5 * IQR)\n    Higher Outlier Limit = Q3 + (1.5 * IQR)\n    \n    Values that lower than lower outlier limit and higher than higher outlier limit are our outliers.\n\n\n\nBox plots are one of the good ways to see outliers.\n\n\n","metadata":{}},{"cell_type":"code","source":"outlier_indexes = []\ndef outlier_plotting(feature):\n    outlier = [] \n    # Plotting section\n    plt.figure(figsize=(6,3))\n    sns.boxplot(x=d_train[feature], palette=\"Set3\")\n    plt.title(\"{}'s Outlier Box Plot\".format(feature), weight = \"bold\")\n    plt.xlabel(feature, weight = \"bold\")\n    plt.show()\n    \n    # Outlier computing\n    Q1 = d_train[feature].quantile(0.25)\n    Q3 = d_train[feature].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_outlier_limit = Q1 - (1.5 * IQR)\n    higher_outlier_limit = Q3 + (1.5 * IQR)\n    \n    print(\"Values lower than {} and higher than {} are outliers for {}.\\n\".format(lower_outlier_limit,higher_outlier_limit,feature))\n\n    # There are different ways to detect and show outlier values, I will use Z-Score method instead of writing conditional function.\n    \n    # Outlier detecting\n    threshold = 3\n\n    for i in d_train[feature]:\n        z = (i-d_train[feature].mean())/d_train[feature].std()\n        if z > threshold: \n            outlier.append(i)\n            index = d_train[d_train[feature] == i].index[0]\n            outlier_indexes.append(index)      \n    if outlier == []:\n        print(\"No any outliers for {}.\".format(feature))\n    else:\n        print(\"There are {} outliers for {}:\".format(len(outlier),feature), outlier)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:32.731374Z","iopub.execute_input":"2021-08-04T12:09:32.731833Z","iopub.status.idle":"2021-08-04T12:09:32.74438Z","shell.execute_reply.started":"2021-08-04T12:09:32.731785Z","shell.execute_reply":"2021-08-04T12:09:32.743284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We need to eliminate Object data type columns. We need to numeric data for outlier analysis \nfor i in [col for col in d_train.columns if d_train[col].dtype != 'O']:\n    if i != \"Id\":\n        outlier_plotting(i)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:32.74688Z","iopub.execute_input":"2021-08-04T12:09:32.747409Z","iopub.status.idle":"2021-08-04T12:09:42.453795Z","shell.execute_reply.started":"2021-08-04T12:09:32.747347Z","shell.execute_reply":"2021-08-04T12:09:42.452691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d_train.loc[outlier_indexes]","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:42.455219Z","iopub.execute_input":"2021-08-04T12:09:42.455618Z","iopub.status.idle":"2021-08-04T12:09:42.559775Z","shell.execute_reply.started":"2021-08-04T12:09:42.455574Z","shell.execute_reply":"2021-08-04T12:09:42.558804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can see the entire values of the index which has outliers. \n\nReady to drop:","metadata":{}},{"cell_type":"code","source":"d_train = d_train.drop(outlier_indexes,axis = 0).reset_index(drop = True)\n# There are no outliers in data anymore.\nd_train.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:42.561023Z","iopub.execute_input":"2021-08-04T12:09:42.561303Z","iopub.status.idle":"2021-08-04T12:09:42.590274Z","shell.execute_reply.started":"2021-08-04T12:09:42.561276Z","shell.execute_reply":"2021-08-04T12:09:42.589522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"6\"></a>\n\n\n# Handling Missing Values and Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"##### We must combine train and test datasets. Because his process are must be carried out together.","metadata":{}},{"cell_type":"code","source":"alldata = pd.concat([d_train,d_test],axis=0,sort=False)\nalldata[\"SalePrice\"].head()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:42.591334Z","iopub.execute_input":"2021-08-04T12:09:42.591757Z","iopub.status.idle":"2021-08-04T12:09:42.615263Z","shell.execute_reply.started":"2021-08-04T12:09:42.591715Z","shell.execute_reply":"2021-08-04T12:09:42.614519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alldata[\"SalePrice\"].tail()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:42.616326Z","iopub.execute_input":"2021-08-04T12:09:42.616755Z","iopub.status.idle":"2021-08-04T12:09:42.623208Z","shell.execute_reply.started":"2021-08-04T12:09:42.616714Z","shell.execute_reply":"2021-08-04T12:09:42.622346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 100)\ninfo_count = pd.DataFrame(alldata.isnull().sum(),columns=['Count of NaN'])\ndtype = pd.DataFrame(alldata.dtypes,columns=['DataTypes'])\ninfo = pd.concat([info_count,dtype],axis=1)\ninfo\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:42.62598Z","iopub.execute_input":"2021-08-04T12:09:42.626264Z","iopub.status.idle":"2021-08-04T12:09:42.672376Z","shell.execute_reply.started":"2021-08-04T12:09:42.626235Z","shell.execute_reply":"2021-08-04T12:09:42.671429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now we can see how many NaN values are in which column. \n\n* I will fill those containing a reasonable number of NaN values with most common values.\n\n* I will fill those containing many NaN values with Sklearn Label Encoder. In this way, some of the object type data will also be transformed into numeric data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Filling 433 LotFrontage values. I will use linear interpolation to fill these NaN values.\nalldata['LotFrontage'].interpolate(method='linear',inplace=True)\n\n# Filling other NaNs\nfor i in info.T:\n    if i == \"Id\" or i == \"SalePrice\" or i == \"LotFrontage\":\n        continue\n    else:\n        if (info.T[i][0] == 0):\n            continue\n        elif (info.T[i][0] < 400):\n            alldata[i].fillna(alldata[i].value_counts().index[0], inplace = True)\n        else:\n            lbl_enc = LabelEncoder() \n            lbl_enc.fit(list(alldata[i].values)) \n            alldata[i] = lbl_enc.transform(list(alldata[i].values))\n            ","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:42.674247Z","iopub.execute_input":"2021-08-04T12:09:42.674562Z","iopub.status.idle":"2021-08-04T12:09:43.282979Z","shell.execute_reply.started":"2021-08-04T12:09:42.674532Z","shell.execute_reply":"2021-08-04T12:09:43.282027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alldata.isna().any().value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:43.284405Z","iopub.execute_input":"2021-08-04T12:09:43.285021Z","iopub.status.idle":"2021-08-04T12:09:43.309867Z","shell.execute_reply.started":"2021-08-04T12:09:43.284973Z","shell.execute_reply":"2021-08-04T12:09:43.308718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* As you can see there are NaN values in only one column. This column is SalePrice which come from test dataset. So everything looks fine.","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_columns', 81)\nalldata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:43.311364Z","iopub.execute_input":"2021-08-04T12:09:43.311763Z","iopub.status.idle":"2021-08-04T12:09:43.381813Z","shell.execute_reply.started":"2021-08-04T12:09:43.311719Z","shell.execute_reply":"2021-08-04T12:09:43.380571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We have still columns in object datatype. We need to handle this columns. Because we will use machine learning algorithms.","metadata":{}},{"cell_type":"code","source":"list_ = [\"MSZoning\", \"Street\", \"LotShape\", \"LandContour\", \"Utilities\", \"LotConfig\",\n        \"LandSlope\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\",\n        \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\",\n        \"MasVnrType\", \"ExterQual\", \"ExterCond\", \"Foundation\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \n        \"BsmtFinType1\", \"BsmtFinType2\", \"Heating\", \"HeatingQC\", \"CentralAir\", \"Electrical\", \"KitchenQual\",\n        \"Functional\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PavedDrive\", \"SaleType\",\n        \"SaleCondition\"]\n\nfor feature in list_:\n    alldata[feature]= alldata[feature].astype(\"category\")\n    alldata = pd.get_dummies(alldata, columns=[feature])","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:43.383387Z","iopub.execute_input":"2021-08-04T12:09:43.383742Z","iopub.status.idle":"2021-08-04T12:09:43.827725Z","shell.execute_reply.started":"2021-08-04T12:09:43.383712Z","shell.execute_reply":"2021-08-04T12:09:43.826752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', 500)\nalldata.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:43.829058Z","iopub.execute_input":"2021-08-04T12:09:43.829352Z","iopub.status.idle":"2021-08-04T12:09:43.96688Z","shell.execute_reply.started":"2021-08-04T12:09:43.829324Z","shell.execute_reply":"2021-08-04T12:09:43.966082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', 500)\nalldata.tail()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:43.968036Z","iopub.execute_input":"2021-08-04T12:09:43.968582Z","iopub.status.idle":"2021-08-04T12:09:44.104108Z","shell.execute_reply.started":"2021-08-04T12:09:43.968536Z","shell.execute_reply":"2021-08-04T12:09:44.103237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Now we can seperate train and test data.","metadata":{}},{"cell_type":"code","source":"train = alldata[0:1195]\ntest = alldata[1195:2919]","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:44.10526Z","iopub.execute_input":"2021-08-04T12:09:44.1057Z","iopub.status.idle":"2021-08-04T12:09:44.110128Z","shell.execute_reply.started":"2021-08-04T12:09:44.105653Z","shell.execute_reply":"2021-08-04T12:09:44.109129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Let's take a look at our train data. Correlation and relationship between some variables.\n* Corelation defines as a mutual relationship or connection between two or more things. A negative, or inverse correlation, between two variables, indicates that one variable increases while the other decreases. A positive correlation is a relationship between two variables in which both variables move in the same direction.\n\nLet's look at our data.","metadata":{}},{"cell_type":"markdown","source":"<a id = \"8\"></a>\n\n\n# Relationship Between Some Variables","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\ncorr_new_train=train.corr()\n\nplt.figure(figsize=(3,15))\nsns.heatmap(corr_new_train[['SalePrice']].sort_values(by=['SalePrice'],ascending=False).head(30),\n            annot_kws={\"size\": 16, \"color\": \"black\"},vmin=-1, cmap='PiYG', annot=True)\nplt.title(\"Positive Corelation Sorting\",fontweight=\"bold\", fontsize = 20)\nplt.show()\n\nplt.figure(figsize=(3,15))\nsns.heatmap(corr_new_train[['SalePrice']].sort_values(by=['SalePrice'],ascending=False).tail(30),\n            annot_kws={\"size\": 16, \"color\": \"black\"},vmin=-1, cmap='PiYG', annot=True)\nplt.title(\"Negative Corelation Sorting\",fontweight=\"bold\", fontsize = 20)\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:44.111476Z","iopub.execute_input":"2021-08-04T12:09:44.111802Z","iopub.status.idle":"2021-08-04T12:09:45.647974Z","shell.execute_reply.started":"2021-08-04T12:09:44.111726Z","shell.execute_reply":"2021-08-04T12:09:45.646894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We can easily understand that OverallQual and GrLiveArea affect the sale price more than other features, and this effect is positive.\n\n* We can easily understand that ExterQual_TA and GarageFinish_Unf affect the sale price more than other features, and this effect is negative.\n\n\nNow let's look at the relationship between some features and sale price.","metadata":{}},{"cell_type":"code","source":"list_ = [\"MSZoning\", \"Street\", \"LotShape\", \"LandContour\", \"Utilities\", \"LotConfig\",\n        \"LandSlope\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\",\n        \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\",\n        \"MasVnrType\", \"ExterQual\", \"ExterCond\", \"Foundation\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \n        \"BsmtFinType1\", \"BsmtFinType2\", \"Heating\", \"HeatingQC\", \"CentralAir\", \"Electrical\", \"KitchenQual\",\n        \"Functional\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PavedDrive\", \"SaleType\",\n        \"SaleCondition\"]\n\nnumerical_feature = []\nfor i in train.columns:\n    if i not in list_:\n        if i == \"Id\":\n            continue\n        else:\n            numerical_feature.append(i)\n\n        \nplt.style.use(\"seaborn-white\")\nfig, axes = plt.subplots(18, 2,figsize=(20,80))\nfig.subplots_adjust(hspace=0.6)\ncolors=[plt.cm.prism_r(each) for each in np.linspace(0, 1, len(numerical_feature))]\n\nfor i,ax,color in zip(numerical_feature,axes.flatten(),colors):\n    \n    sns.regplot(x=train[i], y=train[\"SalePrice\"], fit_reg=True,marker='o',scatter_kws={'s':50,'alpha':0.8},color=color,ax=ax)\n    plt.xlabel(i,fontsize=12)\n    plt.ylabel('SalePrice',fontsize=12)\n    ax.set_yticks(np.arange(0,900001,100000))\n    ax.set_title('SalePrice'+' - '+str(i),color=color,fontweight='bold',size=20)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:45.649623Z","iopub.execute_input":"2021-08-04T12:09:45.650217Z","iopub.status.idle":"2021-08-04T12:09:59.276469Z","shell.execute_reply.started":"2021-08-04T12:09:45.65017Z","shell.execute_reply":"2021-08-04T12:09:59.275487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"19\"></a>\n# Modeling\n\nFirst of all, we need to prepare our test and train data.\n\nOur test data contain SalePrice column and has NaN values on this column. We have to drop this column.\nTest data also contain Id column which have to be drop.","metadata":{}},{"cell_type":"code","source":"test = test.drop(\"SalePrice\", axis=1)\ntest = test.drop(\"Id\", axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:59.27792Z","iopub.execute_input":"2021-08-04T12:09:59.278467Z","iopub.status.idle":"2021-08-04T12:09:59.288633Z","shell.execute_reply.started":"2021-08-04T12:09:59.278422Z","shell.execute_reply":"2021-08-04T12:09:59.287566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use train data while modelling and this train data must not contain the Sale Price and insignificant Id column. After trained model, we will test it with our absolute SalePrice values. So I separate the SalePrice data from the train yield and transfer it into a dataframe called \"y\". I do this by scaling my data.","metadata":{}},{"cell_type":"code","source":"y = np.log1p(train['SalePrice'])\nx = train.drop([\"Id\", \"SalePrice\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:59.290214Z","iopub.execute_input":"2021-08-04T12:09:59.290539Z","iopub.status.idle":"2021-08-04T12:09:59.306778Z","shell.execute_reply.started":"2021-08-04T12:09:59.290506Z","shell.execute_reply":"2021-08-04T12:09:59.305581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I will create the some regression models with default parameter values and calculate RMSE for each.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:59.308243Z","iopub.execute_input":"2021-08-04T12:09:59.308642Z","iopub.status.idle":"2021-08-04T12:09:59.324315Z","shell.execute_reply.started":"2021-08-04T12:09:59.308608Z","shell.execute_reply":"2021-08-04T12:09:59.323005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import GridSearchCV, cross_val_score,StratifiedKFold\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn import metrics\nimport random as rd\nmodels = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))]","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:59.326436Z","iopub.execute_input":"2021-08-04T12:09:59.326914Z","iopub.status.idle":"2021-08-04T12:09:59.339243Z","shell.execute_reply.started":"2021-08-04T12:09:59.326864Z","shell.execute_reply":"2021-08-04T12:09:59.338327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name, regressor in models:\n    model = regressor\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    MSE = metrics.mean_squared_error(y_test,y_pred)\n    RMSE = np.sqrt(MSE)\n    print(f\"RMSE: {round(RMSE, 4)} ({name})\")","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:09:59.340697Z","iopub.execute_input":"2021-08-04T12:09:59.341261Z","iopub.status.idle":"2021-08-04T12:10:07.616597Z","shell.execute_reply.started":"2021-08-04T12:09:59.341221Z","shell.execute_reply":"2021-08-04T12:10:07.6155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"20\"></a>\n## Hyperparameter Tuning - Grid Search - Cross Validation\n\nNow, time to hyperparameter tuning with the ones with the lowest mean square error from the above machine learning algorithms.","metadata":{}},{"cell_type":"code","source":"random_state = 42\nclassifier = [Ridge(random_state = random_state),\n              DecisionTreeRegressor(random_state = random_state),\n             LGBMRegressor(random_state = random_state),\n              GradientBoostingRegressor(random_state = random_state),\n             CatBoostRegressor(verbose=False, random_state = random_state)]\n\nridge_param_grid = {\"solver\" : [\"auto\", \"svd\", \"cholesky\", \"lsqr\", \"sparse_cg\", \"sag\", \"saga\"],\n                   \"normalize\" : [True, False]}\n\ndtr_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2),\n                 \"splitter\": [\"best\", \"random\"]}\n\nlgbmr_param_grid = {\"learning_rate\": [0.001, 0.01, 0.05],\n               \"n_estimators\": [200, 500, 750],\n               \"max_depth\": [-1, 2, 5],\n               \"colsample_bytree\": [1, 0.50, 0.75]}\n\ngbr_param_grid = {\"loss\": [\"ls\", \"huber\", \"quantile\"],\n                 \"n_estimators\":[100,300],\n                 \"min_samples_split\" : range(10,400,50)}\n\ncatboost_param_grid = {\"learning_rate\": np.linspace(0,0.2,5),\n                 \"n_estimators\":[100, 200, 300]}\n\n\nclassifier_param = [ridge_param_grid,\n                   dtr_param_grid,\n                   lgbmr_param_grid,\n                   gbr_param_grid,\n                   catboost_param_grid]\n\nerror = []\nestimator = []\nfor i in range(len(classifier)):\n    \n    model = GridSearchCV(classifier[i],\n                            classifier_param[i],\n                            cv=10,\n                            n_jobs=-1, \n                            scoring = \"neg_mean_squared_error\",\n                            verbose=True).fit(X_train, y_train)\n    rmse = np.mean(np.sqrt(-cross_val_score(model, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\")))\n    error.append(rmse)\n    estimator.append(str(classifier[i]))","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:53:55.022882Z","iopub.execute_input":"2021-08-04T12:53:55.023242Z","iopub.status.idle":"2021-08-04T13:46:28.147107Z","shell.execute_reply.started":"2021-08-04T12:53:55.023211Z","shell.execute_reply":"2021-08-04T13:46:28.145891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_results = pd.DataFrame({\"Cross Validation Errors\":error, \"ML Models\":[\"Ridge\", \"DecisionTreeRegressor\",\n                                                                         \"LGBMRegressor\",\"GradientBoostingRegressor\",\n                                                                        \"CatBoostRegressor\"]})\n\ng = sns.barplot(\"Cross Validation Errors\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"neg_mean_squared_error\")\ng.set_title(\"Cross Validation Scores\")","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:54:59.934674Z","iopub.execute_input":"2021-08-04T13:54:59.935176Z","iopub.status.idle":"2021-08-04T13:55:00.135642Z","shell.execute_reply.started":"2021-08-04T13:54:59.935145Z","shell.execute_reply":"2021-08-04T13:55:00.134954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"error_results = pd.DataFrame({\"ML Models\":[\"Ridge\", \"DecisionTreeRegressor\",\n                                            \"LGBMRegressor\",\"GradientBoostingRegressor\",\n                                          \"CatBoostRegressor\"], \n                                              'Mean Squared Error':error})\nerror_results","metadata":{"execution":{"iopub.status.busy":"2021-08-04T13:55:19.415068Z","iopub.execute_input":"2021-08-04T13:55:19.41558Z","iopub.status.idle":"2021-08-04T13:55:19.427085Z","shell.execute_reply.started":"2021-08-04T13:55:19.415548Z","shell.execute_reply":"2021-08-04T13:55:19.426234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see CatBoostRegressor has the best accuracy. Let's look at the which parameters are the best for this algorithm.","metadata":{}},{"cell_type":"code","source":"catboost_param_grid = {\"learning_rate\": np.linspace(0,0.2,5),\n                 \"n_estimators\":[100, 200, 300],\n                      \"max_depth\": [3,4,5],\n                      \"silent\": [True]}\n\nCatBoostRegressor_model = GridSearchCV(CatBoostRegressor(random_state = random_state),\n                            catboost_param_grid,\n                            cv=10,\n                            n_jobs=-1,\n                            verbose=True).fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T14:55:27.19726Z","iopub.execute_input":"2021-08-04T14:55:27.197828Z","iopub.status.idle":"2021-08-04T14:55:50.698149Z","shell.execute_reply.started":"2021-08-04T14:55:27.197793Z","shell.execute_reply":"2021-08-04T14:55:50.695136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CatBoostRegressor_model.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-08-04T14:00:55.752958Z","iopub.execute_input":"2021-08-04T14:00:55.753376Z","iopub.status.idle":"2021-08-04T14:00:55.75976Z","shell.execute_reply.started":"2021-08-04T14:00:55.753332Z","shell.execute_reply":"2021-08-04T14:00:55.758963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can create final model with GradientBoostingRegressor and its best parameters.","metadata":{}},{"cell_type":"code","source":"params = {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300, \"silent\" : True}\n\nfinal_model = CatBoostRegressor(**params)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T14:03:43.348882Z","iopub.execute_input":"2021-08-04T14:03:43.349302Z","iopub.status.idle":"2021-08-04T14:03:43.354035Z","shell.execute_reply.started":"2021-08-04T14:03:43.349268Z","shell.execute_reply":"2021-08-04T14:03:43.353222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T14:03:44.918464Z","iopub.execute_input":"2021-08-04T14:03:44.918831Z","iopub.status.idle":"2021-08-04T14:03:45.773075Z","shell.execute_reply.started":"2021-08-04T14:03:44.9188Z","shell.execute_reply":"2021-08-04T14:03:45.771944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse = np.mean(np.sqrt(-cross_val_score(final_model, X_train, y_train, cv=20, scoring=\"neg_mean_squared_error\")))\nrmse","metadata":{"execution":{"iopub.status.busy":"2021-08-04T14:03:49.174928Z","iopub.execute_input":"2021-08-04T14:03:49.175289Z","iopub.status.idle":"2021-08-04T14:04:06.641608Z","shell.execute_reply.started":"2021-08-04T14:03:49.17526Z","shell.execute_reply":"2021-08-04T14:04:06.64045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally we can predict SalePrice of test data.","metadata":{}},{"cell_type":"code","source":"submission_absolute_prices = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\ny_pred = final_model.predict(test)\ny_pred = np.expm1(y_pred)\ndf = pd.DataFrame({'Actual':submission_absolute_prices[\"SalePrice\"], 'Predicted':y_pred})","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:50:12.221936Z","iopub.status.idle":"2021-08-04T12:50:12.222571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame()\nsubmission_df[\"Id\"] = d_test[\"Id\"] \nsubmission_df['SalePrice'] = df[\"Predicted\"]\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:50:12.223725Z","iopub.status.idle":"2021-08-04T12:50:12.22437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-04T12:50:12.225603Z","iopub.status.idle":"2021-08-04T12:50:12.22612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Please upvote and make comment if you like my notebook.\n# Thanks!","metadata":{}}]}