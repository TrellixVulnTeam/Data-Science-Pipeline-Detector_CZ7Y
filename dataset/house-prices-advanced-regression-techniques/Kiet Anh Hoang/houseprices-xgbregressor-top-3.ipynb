{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Consider attributes that are highly correlated with `SalePrice`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"abs(df_train.corr()['SalePrice']).nlargest(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use the scatter chart to show the linear relationship between `GrLivArea` and `SalePrice`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df_train['GrLivArea'], df_train['SalePrice'])\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the chart we can see that these two attributes are linearly related and have two outliers with a value of GrLivArea > 4500.We need to remove these two points::","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train[df_train[\"GrLivArea\"] < 4500]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Consider the distribution of `SalePrice`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df_train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The chart above shows that the distribution of `SalePrice` is disproportionate and is` Positive Skew`, so we need to handle it to make a more symmetric distribution. Here we handle by taking logarithm. *(In addition, the metric used in this problem is to get the logarithm of SalePrice of the two predicted values and actually values then calculate the **rmse** between these two values. Therefore, it is quite reasonable to take SalePrice's logarithm to get the father more symmetrically for better results)*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['SalePrice'] = np.log1p(df_train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Distribution of `SalePrice` after using logarithm:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import norm\nsns.distplot(df_train['SalePrice'], fit=norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the distribution of `SalePrice` is more proportionate (close to the standard distribution), not deviating as before processing.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Consider attributes with large missing data:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n = df_train.shape[0]\nfor col in df_train.columns:\n    missing_pct = sum(df_train[col].isnull())*100.0/n\n    if missing_pct > 95.0:\n        print('{}: {:0.2f}%'.format(col, missing_pct))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that `PoolQC` and` MiscFeature` have a large missing data rate, so we will drop these two attributes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['PoolQC', 'MiscFeature'], axis=1, inplace=True)\ndf_test.drop(['PoolQC', 'MiscFeature'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The `Electrical` attribute has only one missing value:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['Electrical'].isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there is only one missing value, we will remove this data point instead of trying to fill the missing value.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train[~df_train['Electrical'].isna()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we need to return the appropriate value type for the attributes `MSSubClass`,` YrSold` and `MoSold` instead of the current numeric type (int64). The reason is because `MSSubClass` is categorical; `YrSold` and` MoSold` are numerical attributes, but we should consider them as categorical attributes which are more appropriate *(for example, with the `MoSold` attribute value 2 (corresponding to February) does not make sense is greater than value 1 (corresponding to January))*. Therefore, we return them to type str (instead of int64).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = [\"MSSubClass\", \"YrSold\", 'MoSold']\ndf_train[cols] = df_train[cols].astype(str)\ndf_test[cols] = df_test[cols].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we need to remove highly correlated attributes. As the `GarageArea` property is highly correlated with the` GarageCars` property, we will remove the `GarageArea` attribute and retain the` GarageCars` property since `GarageCars` has a higher correlation with` SalePrice`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.corr()['GarageArea']['GarageCars']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.corr()['GarageArea']['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.corr()['GarageCars']['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similarly, we find `1stFlrSF` highly correlated with` TotalBsmtSF`; `2ndFlrSF` and` TotRmsAbvGrd` are highly correlated with `GrLivArea`; `GarageYrBlt` is highly correlated with` YearBuilt`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cname = 'GarageYrBlt'\ndf_train.corr()[cname][abs(df_train.corr()[cname]) > 0.65]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cname = 'GrLivArea'\ndf_train.corr()[cname][abs(df_train.corr()[cname]) > 0.65]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cname = 'TotalBsmtSF'\ndf_train.corr()[cname][abs(df_train.corr()[cname]) > 0.65]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Therefore we will drop the attributes `GarageArea`,` 1stFlrSF`, `2ndFlrSF`,` TotRmsAbvGrd` and `GarageYrBlt`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"col = ['GarageArea','1stFlrSF', '2ndFlrSF','TotRmsAbvGrd', 'GarageYrBlt']\ndf_train.drop(col, axis=1, inplace=True)\ndf_test.drop(col, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will declare the function `fill_missing_data ()`, with the categorical attribute we will fill in the value 'None', with the numeric attribute we will fill in the value 0.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def fill_missing_data(df):\n    df_data = df.copy()\n    categoricals = []\n    for cname,dtype in df_data.dtypes.items():\n        if dtype == 'object':\n            categoricals.append(cname)\n    # Fill 'None' for the Categorical attribute\n    df_data[categoricals] = df_data[categoricals].fillna('None')\n    \n    for cname in df_data.columns:\n        if cname not in categoricals:\n            df_data[cname] = df_data[cname].fillna(0) #Fill 0 for the Numeric attribute\n    return df_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = fill_missing_data(df_train)\ndf_test = fill_missing_data(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Summary of attributes related to Porch:\n* OpenPorchSF\n* EnclosedPorch\n* 3SsnPorch\n* ScreenPorch","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['TotalPorchSF'] = df_train['OpenPorchSF'] + df_train['EnclosedPorch'] + df_train['3SsnPorch'] + df_train['ScreenPorch']\ndf_test['TotalPorchSF'] = df_test['OpenPorchSF'] + df_test['EnclosedPorch'] + df_test['3SsnPorch'] + df_test['ScreenPorch']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Total number of Bathrooms. There are 4 attributes pertaining to the bathroom:\n* BsmtFullBath\n* BsmtHalfBath\n* FullBath\n* HalfBath","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['TotalBaths'] = df_train['BsmtFullBath'] + df_train['FullBath'] + 0.5*(df_train['BsmtHalfBath'] + df_train['HalfBath'])\ndf_test['TotalBaths'] = df_test['BsmtFullBath'] + df_test['FullBath'] + 0.5*(df_test['BsmtHalfBath'] + df_test['HalfBath'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Total area:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['TotalAreaSF'] = df_train['TotalBsmtSF'] + df_train['GrLivArea']\ndf_test['TotalAreaSF'] = df_test['TotalBsmtSF'] + df_test['GrLivArea']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Age of house from construction to sold:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Age'] = df_train['YrSold'].astype('int64') - df_train['YearBuilt']\ndf_test['Age'] = df_test['YrSold'].astype('int64') - df_test['YearBuilt']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we will declare the function `feature_engineering()` to convert categorical properties into one-hot vector, binary attributes into 0/1 form and ordinal attributes into ordered numbers (large values carry meaning better than small values): *(Instead of using the get_dummies function)*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(df):\n    df_data = df.copy()\n    \n    feature = {\n        'categorical':{\n            'MSSubClass': ['20', '30', '40', '45', '50', '60', '70', '75', '80', '85', '90', '120', '150', '160', '180', '190'],\n            'MSZoning': ['A', 'C', 'FV', 'I', 'RH', 'RL', 'RP', 'RM'],\n            'Alley': ['Grvl', 'Pave', 'None'],\n            'LandContour': ['Lvl', 'Bnk', 'HLS', 'Low'],\n            'LotConfig': ['Inside', 'Corner', 'CulDSac', 'FR2', 'FR3'],\n            'Neighborhood': ['Blmngtn', 'Blueste', 'BrDale', 'BrkSide', 'ClearCr', 'CollgCr', 'Crawfor', 'Edwards', 'Gilbert', 'IDOTRR', 'MeadowV', 'Mitchel',\n                            'Names', 'NoRidge', 'NPkVill', 'NridgHt', 'NWAmes', 'OldTown', 'SWISU', 'Sawyer', 'SawyerW', 'Somerst', 'StoneBr', 'Timber', 'Veenker'],\n            'Condition1': ['Artery', 'Feedr', 'Norm', 'RRNn', 'RRAn', 'PosN', 'PosA', 'RRNe', 'RRAe'],\n            'Condition2': ['Artery', 'Feedr', 'Norm', 'RRNn', 'RRAn', 'PosN', 'PosA', 'RRNe', 'RRAe'],\n            'BldgType': ['1Fam', '2FmCon', 'Duplx', 'TwnhsE', 'TwnhsI'],\n            'HouseStyle': ['1Story', '1.5Fin', '1.5Unf', '2Story', '2.5Fin', '2.5Unf', 'SFoyer', 'SLvl'],\n            'RoofStyle': ['Flat', 'Gable', 'Gambrel', 'Hip', 'Mansard', 'Shed'],\n            'RoofMatl': ['ClyTile', 'CompShg', 'Membran', 'Metal', 'Roll', 'Tar&Grv', 'WdShake', 'WdShngl'],\n            'Exterior1st': ['AsbShng', 'AsphShn', 'BrkComm', 'BrkFace', 'CBlock', 'CemntBd', 'HdBoard', 'ImStucc', 'MetalSd', 'Other', 'Plywood', 'PreCast', 'Stone', 'Stucco',\n                           'VinylSd', 'Wd Sdng', 'WdShing'],\n            'Exterior2nd': ['AsbShng', 'AsphShn', 'BrkComm', 'BrkFace', 'CBlock', 'CemntBd', 'HdBoard', 'ImStucc', 'MetalSd', 'Other', 'Plywood', 'PreCast', 'Stone', 'Stucco',\n                           'VinylSd', 'Wd Sdng', 'WdShing'],\n            'MasVnrType': ['BrkCmn', 'BrkFace', 'CBlock', 'None', 'Stone'],\n            'Foundation': ['BrkTil', 'CBlock', 'PConc', 'Slab', 'Stone', 'Wood'],\n            'Heating': ['Floor', 'GasA', 'GasW', 'Grav', 'OthW', 'Wall'],\n            'Electrical': ['SBrkr', 'FuseA', 'FuseF', 'FuseP', 'Mix'],\n            'Functional': ['Typ', 'Min1', 'Min2', 'Mod', 'Maj1', 'Maj2', 'Sev', 'Sal'],\n            'GarageType': ['2Types', 'Attchd', 'Basment', 'BuiltIn', 'CarPort', 'Detchd', 'None'],\n            'GarageFinish': ['Fin', 'RFn', 'Unf', 'None'],\n            'PavedDrive': ['Y', 'P', 'N'],\n            'MiscFeature': ['Elev', 'Gar2', 'Othr', 'Shed', 'TenC', 'None'],\n            'MoSold': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'],\n            'YrSold': ['2006', '2007', '2008', '2009', '2010'],\n            'SaleType': ['WD', 'CWD', 'VWD', 'New', 'COD', 'Con', 'ConLw', 'ConLI', 'ConLD', 'Oth'],\n            'SaleCondition': ['Normal', 'Abnorml', 'AdjLand', 'Alloca', 'Family', 'Partial']\n        },\n        'binary': {\n            'Street': ['Pave', 'Grvl'],\n            'CentralAir': ['Y', 'N']          \n        },\n        'ordinal': {\n            'LotShape': ['None', 'IR3', 'IR2', 'IR1', 'Reg'],\n            'Utilities': ['None', 'NoSeWa', 'NoSewr', 'AllPub'],\n            'LandSlope': ['None', 'Sev', 'Mod', 'Gtl'],\n            'ExterQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n            'ExterCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n            'BsmtQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n            'BsmtCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n            'BsmtExposure': ['None', 'No', 'Mn', 'Av', 'Gd'],\n            'BsmtFinType1': ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n            'BsmtFinType2': ['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n            'HeatingQC': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n            'KitchenQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n            'FireplaceQu': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n            'GarageQual': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n            'GarageCond': ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n            'Fence': ['None', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'],\n            'PoolQC': ['None', 'Fa', 'Ta', 'Gd', 'Ex']\n        },\n    }\n    \n    selected = []\n    for cname in df_data.columns:\n        if cname in feature['binary']: # Convert the binary attributes to 0/1\n            default_value = feature['binary'][cname][0]\n            feature_name = cname + \"_is_\" + default_value\n            selected.append(feature_name)\n            df_data[feature_name] = df_data[cname].apply(lambda x: int(x == default_value))\n        elif cname in feature['categorical']: # Convert Categorical attributes into One-hot vector\n            values = feature['categorical'][cname]\n            for val in values:\n                try:\n                    new_name = \"{}_{}\".format(cname, val)\n\n                    selected.append(new_name)\n                    df_data[new_name] = df_data[cname].apply(lambda x: int(x == val))\n                except Exception as err:\n                    print(\"One-hot encoding for {}_{}. Error: {}\".format(cname, val, err))\n        elif cname in feature['ordinal']: # Convert the Ordinal attributes to a number\n            new_name = cname + \"_ordinal\"\n            selected.append(new_name)\n            df_data[new_name] = df_data[cname].apply(lambda x: int(feature['ordinal'][cname].index(x)))\n        else: # The remaining attributes are numeric so they remain the same\n#             print(cname)\n            selected.append(cname)\n            \n    return df_data[selected]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply for `df_train` and `df_test`:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = feature_engineering(df_train)\ndf_test = feature_engineering(df_test)\ndf_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we need to drop the columns that only contain the value 0 (these columns will not make much sense):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df_train.columns:\n    if any(df_train[col]) == False:\n        df_train.drop([col], axis=1, inplace=True)\n        df_test.drop([col], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, drop the `Id` and` SalePrice` columns before going to the following sections, which need to save the `Id` of the df_test and` SalePrice` of df_train:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = df_test['Id']\ny = df_train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.drop(['Id', 'SalePrice'], axis=1, inplace=True)\ndf_test.drop(['Id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaler:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df_train)\ntrain = scaler.transform(df_train)\ntest = scaler.transform(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split Train/Test/Validation","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Split df_train into 3 parts: train (75%), test (12.5%), validation (12.5%)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.25, random_state=1)\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=123)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this section we will run some regression model. Then choose the best model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## XGBRegressor","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"XGBRegressor initialization parameters (hyperparameters have been selected to achieve good results):","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"param_init = {\n    \"max_depth\": 5, # default: 3 only for depthwise\n    \"n_estimators\": 3000, # default: 500\n    \"learning_rate\": 0.01, # default: 0.05\n    \"subsample\": 0.5,\n    \"colsample_bytree\": 0.7,  # default:  1.0\n    \"min_child_weight\": 1.5,\n    \"reg_alpha\": 0.75,\n    \"reg_lambda\": 0.4,\n    \"seed\": 42,\n#     \"eval_metric\": \"rmse\"\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nxgb_model = xgboost.XGBRegressor(**param_init)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_fit = {\n    \"eval_metric\": \"rmse\",\n    \"early_stopping_rounds\": 500, # default: 100\n    \"verbose\": 200,\n    \"eval_set\": [(X_val, y_val)]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = xgb_model.fit(X_train, y_train, **param_fit)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_xgb = xgb_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_squared_error(y_test, y_pred_xgb, squared=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see with `XBGRegressor` the error of test set is` 0.11370`.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## DecisionTreeRegressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor  \n\nregressorTree = DecisionTreeRegressor(random_state = 0, min_samples_split=2, max_depth=6)  \nregressorTree.fit(X_train, y_train) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_Tree = regressorTree.predict(X_test)\nmean_squared_error(y_test, y_pred_Tree, squared=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## GradientBoostingRegressor","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\nregressorGB = GradientBoostingRegressor(\n    max_depth=5,\n    n_estimators=10000,\n    learning_rate=0.25\n)\nregressorGB.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_GB = regressorGB.predict(X_test)\nmean_squared_error(y_test, y_pred_GB, squared=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lasso","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nregressorLasso = Lasso(alpha=0.0007)\nregressorLasso.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_Lasso = regressorLasso.predict(X_test)\nmean_squared_error(y_test, y_pred_Lasso, squared=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above results show that `XGBRegressor` gives the best results. Therefore use `XGBRegressor` to predict.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Use `XGBRegressor` to predict:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SalePrice_pred = xgb_model.predict(test)\n# Because it takes log () to train, it is necessary to take exp () the predicted result\nSalePrice_pred = np.exp(SalePrice_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = {'Id': ids, 'SalePrice': SalePrice_pred}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission = pd.DataFrame(submission)\ndf_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}