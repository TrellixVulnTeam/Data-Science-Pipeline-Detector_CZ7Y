{"cells":[{"metadata":{},"cell_type":"markdown","source":"# I hope everyone is safe\n\n\n# A FRIENDLY MESSAGE\n\n\nHi Below is my first submission to the ever famous house price regression competition.The dataset for beginners into kaggle, I realized a lot of kernels to be full of long codes that intimidated me as a beginner. I landed up at top 22% in my 17th submission. Learned a lot in the process and looking forward to keep going on with other submissions.\n\nI know it is a very common problem set for anyone to submit and upload a kernel but I thought of sharing it anyway to share my approach hoping to get feedbacks from the community on how I can improve.\n\nHere I've tried to optimize my code and still providing the best results which I believe will not be intimidating for beginners and learners who are trying to get their first submission.\n\nI have taken a very basic approach and I hope you find it useful. If you do, please upvote, it'll just motivate me more to keep trying more and more problems.\n\nHOPING TO HEAR FROM YOU ALL. THANK YOU IN ADVANCE :)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"My main objectives on this project are:\n\n* Applying exploratory data analysis and trying to get some insights about our dataset\n* Getting data in better shape by transforming and feature engineering to help us in building better models\n* Building and tuning couple models to get some stable results on predicting housing prices","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Meeting the data\nWe're going to start by loading the data and taking first look on it as usual. For the column names we have great dictionary file in our dataset location so we can get familiar with them in no time. I highly recommend looking at that before you start working on the dataset.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\ndf_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we can see that in train there are 1460 rows with 81 columns and in test dataset 1459 rows with 80 columns. our dependent varibale is **'SalePrice'**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns , df_test.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 1460 observations of 80 variables in the training dataframe. The variables are described below:\n\nSalePrice - This is the target variable/dependent variable that you're trying to predict.\n\n* MSSubClass: The building class\n* MSZoning: The general zoning classification\n* LotFrontage: Linear feet of street connected to property\n* LotArea: Lot size in square feet\n* Street: Type of road access\n* Alley: Type of alley access\n* LotShape: General shape of property\n* LandContour: Flatness of the property\n* Utilities: Type of utilities available\n* LotConfig: Lot configuration\n* LandSlope: Slope of property\n* Neighborhood: Physical locations within Ames city limits\n* Condition1: Proximity to main road or railroad\n* Condition2: Proximity to main road or railroad (if a second is present)\n* BldgType: Type of dwelling\n* HouseStyle: Style of dwelling\n* OverallQual: Overall material and finish quality\n* OverallCond: Overall condition rating\n* YearBuilt: Original construction date\n* YearRemodAdd: Remodel date\n* RoofStyle: Type of roof\n* RoofMatl: Roof material\n* Exterior1st: Exterior covering on house\n* Exterior2nd: Exterior covering on house (if more than one material)\n* MasVnrType: Masonry veneer type\n* MasVnrArea: Masonry veneer area in square feet\n* ExterQual: Exterior material quality\n* ExterCond: Present condition of the material on the exterior\n* Foundation: Type of foundation\n* BsmtQual: Height of the basement\n* BsmtCond: General condition of the basement\n* BsmtExposure: Walkout or garden level basement walls\n* BsmtFinType1: Quality of basement finished area\n* BsmtFinSF1: Type 1 finished square feet\n* BsmtFinType2: Quality of second finished area (if present)\n* BsmtFinSF2: Type 2 finished square feet\n* BsmtUnfSF: Unfinished square feet of basement area\n* TotalBsmtSF: Total square feet of basement area\n* Heating: Type of heating\n* HeatingQC: Heating quality and condition\n* CentralAir: Central air conditioning\n* Electrical: Electrical system\n* 1stFlrSF: First Floor square feet\n* 2ndFlrSF: Second floor square feet\n* LowQualFinSF: Low quality finished square feet (all floors)\n* GrLivArea: Above grade (ground) living area square feet\n* BsmtFullBath: Basement full bathrooms\n* BsmtHalfBath: Basement half bathrooms\n* FullBath: Full bathrooms above grade\n* HalfBath: Half baths above grade\n* Bedroom: Number of bedrooms above basement level\n* Kitchen: Number of kitchens\n* KitchenQual: Kitchen quality\n* TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n* Functional: Home functionality rating Fireplaces: Number of fireplaces\n* FireplaceQu: Fireplace quality\n* GarageType: Garage location\n* GarageYrBlt: Year garage was built\n* GarageFinish: Interior finish of the garage\n* GarageCars: Size of garage in car capacity\n* GarageArea: Size of garage in square feet\n* GarageQual: Garage quality\n* GarageCond: Garage condition\n* PavedDrive: Paved driveway\n* WoodDeckSF: Wood deck area in square feet\n* OpenPorchSF: Open porch area in square feet\n* EnclosedPorch: Enclosed porch area in square feet\n* 3SsnPorch: Three season porch area in square feet\n* ScreenPorch: Screen porch area in square feet\n* PoolArea: Pool area in square feet\n* PoolQC: Pool quality\n* Fence: Fence quality\n* MiscFeature: Miscellaneous feature not covered in other categories\n* MiscVal: $Value of miscellaneous feature\n* MoSold: Month Sold\n* YrSold: Year Sold\n* SaleType: Type of sale\n* SaleCondition: Condition of sale","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(15, 12))\nsns.heatmap(corrmat, vmax=.8, square=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nplt.figure(figsize=(10,10))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**so what is log transformation:-log transformation is used to transform skewed data to approximately conform to normality.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#before log transformation\nsns.distplot(df_train['SalePrice']);\nfig_saleprice = plt.figure(figsize=(12,5))\nresult1 = stats.probplot(df_train['SalePrice'],plot = plt)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#applying log transformation\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#after log transformation\nsns.distplot(df_train['SalePrice']);\nfig_saleprice2 = plt.figure(figsize=(12,5))\nresult3 = stats.probplot(df_train['SalePrice'],plot = plt)'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"below code is used to see top 10 highly correlated columns with saleprice in which OverallQual,GrLiveArea,Garagecars,GarageArea,TotalBsmtSF and 1stFlrSF are highly correlated      ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#below code is used to see which column is more correlated to dependent varibale so first ten columns are more correlated compare to other columns\ncorr = df_train.corr()[\"SalePrice\"]\ncorr[np.argsort(corr, axis=0)[::-1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **OUTLIERS**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"we are going to plot first 10 highly correlated columns to see how many outliers we have in our dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.subplots()\nplt.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1= plt.subplots()\nplt.scatter(x = df_train['OverallQual'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig2= plt.subplots()\nplt.scatter(x = df_train['GarageCars'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageCars', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig3= plt.subplots()\nplt.scatter(x = df_train['GarageArea'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageArea', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig4= plt.subplots()\nplt.scatter(x = df_train['TotalBsmtSF'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig5= plt.subplots()\nplt.scatter(x = df_train['1stFlrSF'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('1stFlrSF', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig6= plt.subplots()\nplt.scatter(x = df_train['FullBath'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('FullBath', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig7= plt.subplots()\nplt.scatter(x = df_train['TotRmsAbvGrd'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotRmsAbvGrd', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig8= plt.subplots()\nplt.scatter(x = df_train['YearBuilt'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('YearBuilt', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"i have seen lots of notebooks in which they delete outliers but i am not going to do it because if i do my accuracy will be decrease. i don't know why if you know then just tell me in comment box.it will be appreciated.\n\nbut below i am adding that code incase someone want it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#deleting outliers\ndf = df.drop(df[(df['GrLivArea']>4000) & (df['SalePrice']<300000)].index)\ndf = df.drop(df[(df['GarageArea']>1200) & (df['SalePrice']<500000)].index)\ndf = df.drop(df[(df['TotalBsmtSF']>3000) & (df['SalePrice']<700000)].index)\ndf = df.drop(df[(df['1stFlrSF']>2700) & (df['1stFlrSF']<700000)].index)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatterplot\nsns.set()\ncolumns = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', '1stFlrSF']\nsns.pairplot(df_train[columns], size = 3)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# some feature engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"here i have merged some columns to just reduce complexity i have tried with all the columns but i didn't get this much accuracy which i am getting right now","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature engineering\ndf_train['TotalSF'] = df_train['TotalBsmtSF']+df_train['1stFlrSF']+df_train['2ndFlrSF']\ndf_train=df_train.drop(columns={'1stFlrSF', '2ndFlrSF','TotalBsmtSF'})\ndf_train['wholeExterior'] = df_train['Exterior1st']+df_train['Exterior2nd']\ndf_train=df_train.drop(columns={'Exterior1st','Exterior2nd'})\ndf_train['Bsmt'] = df_train['BsmtFinSF1']+ df_train['BsmtFinSF2']\ndf_train = df_train.drop(columns={'BsmtFinSF1','BsmtFinSF2'})\ndf_train['TotalBathroom'] = df_train['FullBath'] + df_train['HalfBath']\ndf_train = df_train.drop(columns={'FullBath','HalfBath'})\n\n\ndf_test['TotalSF'] = df_test['TotalBsmtSF']+df_test['1stFlrSF']+df_test['2ndFlrSF']\ndf_test=df_test.drop(columns={'1stFlrSF', '2ndFlrSF','TotalBsmtSF'})\ndf_test['wholeExterior'] = df_test['Exterior1st']+df_test['Exterior2nd']\ndf_test=df_test.drop(columns={'Exterior1st','Exterior2nd'})\ndf_test['Bsmt'] = df_test['BsmtFinSF1']+ df_test['BsmtFinSF2']\ndf_test = df_test.drop(columns={'BsmtFinSF1','BsmtFinSF2'})\ndf_test['TotalBathroom'] = df_test['FullBath'] + df_test['HalfBath']\ndf_test = df_test.drop(columns={'FullBath','HalfBath'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We're going to merge the datasets here before we start editing it so we don't have to do these operations twice. Let's call it features since it has features only. So our data has 2919 observations and 79 features to begin with...**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"frames = [df_train,df_test]\ndf = pd.concat(frames,keys=['train','test'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are 2919 observations with 76 columns. including the target variable SalePrice and Id.The train set has 1460 observations while the test set has 1459 observations, the target variable SalePrice is absent in test. The aim of this study is to train a model on the train set and use it to predict the target SalePrice of the test set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_missing=df.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_missing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now we are saperating categorical columns and numerical columns for filling missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col = df.select_dtypes(include=['object'])\ncat_col.isnull().sum()\ncat_col.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_col = df.select_dtypes(include=['int64', 'float64'])\nnum_col.isnull().sum()\nnum_col.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In below cell you have your numerical columns so i just replace NaN by 0. I have also tried mode,median and mean but i got best result in 0.if you want to do it then just fork my notebook and apply that functions.if you want that other function's code then just comment below i will give you the code in comment section.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Handling Missing Data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# numerical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# handling missing values of numerical columns\ndf['LotFrontage'] = df['LotFrontage'].fillna(value=0)\ndf['GarageYrBlt'] = df['GarageYrBlt'].fillna(value=0)\ndf['MasVnrArea'] = df['MasVnrArea'].fillna(value=0)\ndf['BsmtFullBath'] = df['BsmtFullBath'].fillna(value=0)\ndf['BsmtHalfBath'] = df['BsmtHalfBath'].fillna(value=0)\ndf['GarageArea'] = df['GarageArea'].fillna(value=0)\ndf['GarageCars'] = df['GarageCars'].fillna(value=0)\ndf['BsmtUnfSF'] = df['BsmtUnfSF'].fillna(value=0)\ndf['Bsmt'] = df['Bsmt'].fillna(value=0)\ndf['TotalSF'] = df['TotalSF'].fillna(value=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"i have applied same technique as i applied in numerical columns where i put 0 and here i have replaced all the NaN values with None.That means if the original dataset have nan values , it means that the particular house is doesn't have that thing.for example, if id no = 220 do not have garage then why we put values that id no = 220 has a garage.\n\nso i replaced them with None.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# categorical columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# handling missing values of categorical columns\ndf['MSZoning'] = df['MSZoning'].fillna(value='None')\ndf['GarageQual'] = df['GarageQual'].fillna(value='None')\ndf['GarageCond'] = df['GarageCond'].fillna(value='None')\ndf['GarageFinish'] = df['GarageFinish'].fillna(value='None')\ndf['GarageType'] = df['GarageType'].fillna(value='None')\ndf['BsmtExposure'] = df['BsmtExposure'].fillna(value='None')\ndf['BsmtCond'] = df['BsmtCond'].fillna(value='None')\ndf['BsmtQual'] = df['BsmtQual'].fillna(value='None')\ndf['BsmtFinType2'] = df['BsmtFinType2'].fillna(value='None')\ndf['BsmtFinType1'] = df['BsmtFinType1'].fillna(value='None')\ndf['MasVnrType'] = df['MasVnrType'].fillna(value='None')\ndf['Utilities'] = df['Utilities'].fillna(value='None')\ndf['Functional'] = df['Functional'].fillna(value='None')\ndf['Electrical'] = df['Electrical'].fillna(value='None')\ndf['KitchenQual'] = df['KitchenQual'].fillna(value='None')\ndf['SaleType'] = df['SaleType'].fillna(value='None')\ndf['wholeExterior'] = df['wholeExterior'].fillna(value='None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**most missing values columns**\n\n\n**PoolQC**           2909\n\n\n**MiscFeature**      2814\n\n\n**Alley**            2721\n\n\n**Fence**            2348\n\n\n**FireplaceQu**      1420  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"as you can see above this five columns has more missing values arounf 90% + so i decided to drop them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(columns={'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main = pd.get_dummies(df)\ndf_main","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_main.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"i am going to plot a heatmap for just fun so if you don't understand it go ahead even i don't understand this below heatmap.LOL","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncorrmat = df_main.corr()\nf, ax = plt.subplots(figsize=(15, 12))\nsns.heatmap(corrmat, vmax=.8, square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"top 40 correlated columns after data preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#saleprice correlation matrix\nk = 40 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_main[cols].values.T)\nsns.set(font_scale=1.25)\nplt.figure(figsize=(10,10))\nhm = sns.heatmap(cm, cbar=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EID = df_main.loc['test']\ndf_test = df_main.loc['test']\ndf_train = df_main.loc['train']\nEID = EID.Id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.drop(['SalePrice','Id'], axis =1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train.drop(['SalePrice','Id'], axis = 1)\ny_train = df_train['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost\nxgboost = xgboost.XGBRegressor(learning_rate=0.05,\n                      colsample_bytree = 0.5,\n                      subsample = 0.8,\n                      n_estimators=1000,\n                      max_depth=5,\n                      gamma=5)\n\nxgboost.fit(X_train, y_train)\ny_pred = xgboost.predict(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making main csv file\nmain_submission = pd.DataFrame({'Id': EID, 'SalePrice': y_pred})\n\nmain_submission.to_csv(\"submission.csv\", index=False)\nmain_submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# please do *upvote* if you like","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}