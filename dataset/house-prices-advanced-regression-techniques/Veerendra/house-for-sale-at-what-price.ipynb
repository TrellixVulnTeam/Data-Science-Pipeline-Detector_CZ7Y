{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing other libraries that are required for our study\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nplt.style.use('bmh') #Setting matplot style option to 'Bayesian Methods for Hackers style'\n\n#setting max number of columns to display == 100 in pandas options.\npd.options.display.max_columns = 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Importing data:","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#train set\ntrain_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n\n#test set\ntest_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df\nprint(train_df.shape)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 1460 records in test set and 81 feature, target variable is **'SalePrice'**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_df\nprint(test_df.shape)\n\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 1459 records in test set and 80 feature (target feature 'SalePrice' needs to predicted.) ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now lets clean the data from both train and test dataset, \n> For that we will concatenate the both train_df and test_df to maintain homogeneity in this process for boththe datasets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_test = pd.concat([train_df, test_df], axis=0, join='outer', ignore_index=False, keys=None,\n          levels=None, names=None, verify_integrity=False, copy=True) \nprint(df_train_test.shape)\n\ndf_train_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Observations:**\n\n**From the above we can see that, there are *2919 rows and 81 columns* in combined dataset(both train and test together).**\n\nAnd we have some columns with very less no. of. non-NaN values in them.\n\nLike: \n* >Alley with 198 no. of. non-NaN values. \n* >FireplaceQu with 1499 no. of. non-NaN values.\n* >PoolQu -10, Fence - 571, MiscFeature -105 etc.\n* >SalePrice also has shown on 1460 non-NaN values, this is because this our target feature to be predictedfor test dataset, our test set dont have SalePrice column(Observed above.). So we wont be dealing with these missing values.\n\nThis means we have to clean our data as our data contains missing values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Lets identify and drop the columns of such (The onces with  no. of. NaN values more than 30%) kind. Because these might effect the aggregations and analysis in our study with these NaN values.\n\nAnd the remaing columns with less than that can be imputed with median of that respective column.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#****Checking for null values % in all feature of this df********","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"(df_train_test.isnull().sum()/len(df_train_test) * 100).round(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#No of columns with atleast 1 NaN Values in both train and test sets together\nprint('No of columns with atleast 1 NaN Values:',\n      (df_train_test.isnull().sum()/len(df_train_test) * 100).round(2)[(df_train_test.isnull().sum()/len(df_train_test) * 100).round(2) > 0.00].count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The Features with 30% and more NaN values \n(df_train_test.isnull().sum()/len(df_train_test) * 100).round(2)[(df_train_test.isnull().sum()/len(df_train_test) * 100).round(2) >= 30.00]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list of Features that can be droped for now\ncols_with_30pct_n_more = (df_train_test.isnull().sum()/len(df_train_test) * 100).round(2)[(df_train_test.isnull().sum()/len(df_train_test) * 100).round(2) >= 30.00].index.to_list()\n\ncols_with_30pct_n_more.remove('SalePrice') #As we are not going to deal with missing values in this\n\ncols_with_30pct_n_more","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Lets also drop Id column alogng with the above\ncols_to_drop = ['Id']\n\n#adding the columns to be dropped due to high NaN values\ncols_to_drop.extend(cols_with_30pct_n_more)\n\ncols_to_drop","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping\n\ndf_train_test = df_train_test.drop(cols_to_drop, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking the shape of df after dropping \n\ndf_train_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After dropping 'Id' and other columns with High no. of. NaN values, we are left out with 75 columns.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#No of columns with atleast 1 NaN Values in both train and test sets together \n#After dropping top cols of them\nprint('No of columns with atleast 1 NaN Values:',\n      (df_train_test.isnull().sum()/len(df_train_test) * 100).round(2)[(df_train_test.isnull().sum()/len(df_train_test) * 100).round(2) > 0.00].count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Columns with atleast 1 NaN Values:',\n      (df_train_test.isnull().sum()/len(df_train_test) * 100).round(2)[(df_train_test.isnull().sum()/len(df_train_test) * 100).round(2) > 0.00].sort_values(ascending = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we can deal with these missing values in 2 step, identifing categorical, and numerical feature separately.\n\nAnd imputing their NaN values with appropriate methods like median for numericals features, and mode for categorical features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"nan_cols = (df_train_test.isnull().sum()/len(df_train_test) * 100).round(2)[(df_train_test.isnull().sum()/len(df_train_test) * 100).round(2) > 0.00].sort_values(ascending = False).index.to_list()\n\nnan_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#their data types\n\nt_f_obj_nan_cols = (df_train_test[nan_cols].dtypes == object)\nt_f_obj_nan_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we have 18 object type feature, 12 numeric.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Type casting bool to str\nt_f_obj_nan_cols = t_f_obj_nan_cols.astype('str')\n\nt_f_obj_nan_cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#list object type features\nobj_nan_cols = t_f_obj_nan_cols[t_f_obj_nan_cols == 'True'].index.to_list()\n\n#list of numeric type features\nnum_nan_cols = t_f_obj_nan_cols[t_f_obj_nan_cols == 'False'].index.to_list()\n\nprint(\"object type features:\")\nprint(obj_nan_cols)\nprint('\\n')\nprint(\"numeric type features:\")\nprint(num_nan_cols)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have two lists, one with object type feature names and the other with numerical type feature names.\n\nnow lets impute meadian under numerical features and mode under object type features.\n\nNote: We have to exclude the 'SalePrice' as discussed before.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***Firstly, lets convert string type values of object features to categorical values***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nfor i in range(df_train_test.shape[1]):\n    if df_train_test.iloc[:,i].dtypes == object:\n        lbl = LabelEncoder()\n        lbl.fit(list(df_train_test.iloc[:,i].values))\n        df_train_test.iloc[:,i] = lbl.transform(list(df_train_test.iloc[:,i].values))\n\nprint(df_train_test['SaleCondition'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing SalePrice from num_nan_cols\nnum_nan_cols.remove('SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#imputing NaNs\n\ndf_train_test[num_nan_cols] = df_train_test[num_nan_cols].fillna(df_train_test[num_nan_cols].median())\n\n#df_train_test[obj_nan_cols] = df_train_test[obj_nan_cols].fillna(df_train_test[obj_nan_cols].mode())\nfor column in df_train_test[obj_nan_cols]:\n    mode = df_train_test[column].mode()\n    df_train_test[column] = df_train_test[column].fillna(mode)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train_test.isnull().sum()[df_train_test.isnull().sum()>0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now,as we can see we successfully dealth with NaN values in all features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering\n\nAdding new feature TotalSFA which is total Surface area of the house for sale. Which is generally thought of immediately when you come to know that a house is for sale.\n\nThis is nothing but 'TotalBsmtSF'+ '1stFlrSF' + '2ndFlrSF'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_test['TotalSFA'] = df_train_test['TotalBsmtSF'] + df_train_test['1stFlrSF'] + df_train_test['2ndFlrSF']\n\ndf_train_test[['TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'TotalSFA']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Spliting data back into train_df and test_df**\n\n> As we know, we can do that in different ways, lets use those NaN values under SalePrice to this.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_test[df_train_test.SalePrice.isnull() == True].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see the test set after dealing with NaNs. Lets name it back as **df_test**.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = df_train_test[df_train_test.SalePrice.isnull() == True]\n\ndf_test = df_test.drop('SalePrice', axis = 1)\n\ndf_test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train_test.dropna(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we go, now we have our train set as well.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*From here lets call our train set as **'df'***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----------------------\n# **Lets now understand how the 'Housing Prices -> SalePrice' is distributed **","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Target variable -> SalePrice\n\ndf.SalePrice.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the top 5 recorded values under SalePrice column, and it is * **int** * type variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df.SalePrice.describe().round(2))\nplt.figure(figsize=(9, 8))\nsns.distplot(df.SalePrice, color='orange', bins=100, hist_kws={'alpha': 0.4}); #; will avoid the matplotlib verbose informations","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n\nFrom describe()\n---\n* No NaN(Null) Values. Counting to **1460**.\n* **Average SalePrice** of a house from the data availabe is **180921.2**\n* **Middle value of ordered SalePrice = 163000.0** ('Median' or 'Q3')\n* **Minimum SalePrice** = **34900.0** and **Maximum SalePrice** = **755000.0**\n\nFrom distplot()\n----\n* The prices in **SalePrice** are observed to be **Right-Skewed**.\n* There are outliers above ~500,000 \n\nEventually to attain the normal distribution to data of SalePrice, we will treat these outliers with appropriate approach.\n----\n\n--------------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We use log-transform to make them normally distributed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# log-transform the target variable for normality\ndf['SalePrice'] = np.log(df['SalePrice'])\n\nplt.figure(figsize=(9, 8))\nsns.distplot(df.SalePrice, color='orange', bins=100, hist_kws={'alpha': 0.4});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Yes, now our target variable is approximately following normal distribution.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Checking Feature data distribution**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"---------------\n***Lets look at the distribution of all of the features by ploting them***","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.hist(figsize = (30, 35), bins = 50, xlabelsize = 8, ylabelsize = 8, color='orange');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"-------------------\n# What to analyse from the above histogram graphs?\nThe first thing that cae into my mind by seeing, observing these graphs is 'Okay! I understand few features are distributed approximately normal and some are skewed. But, what about other shapes, other feature distribution what do they say, what to analyse, what to understand from those others graphs?', then in search of answers to these question striked in my mind, I came accross these points.\n\n1. What is Histogram?\n> **Histogram:** A graphic summary of variation in a set of data. The pictorial nature of a histogram lets people see patterns that are difficult to detect in a simple table of      numbers.\n> A histogram is the most commonly used graph to show frequency distributions. It looks very much like a bar chart, but there are important differences between them. \n\n\n>> Answers to my questions: How to analyze the meaning of your histogram's shape. [Typical histogram shapes and what they mean](http://asq.org/quality-resources/histogram#Shapes).\n     \n   ------------------------------ ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Observations from the above Histograms:**\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"* 1stFlrSF, GrLivArea, LotFrontage, TotalBsmtSF and 'TotalSFA' are all almost similarlly distributed as the way previously our SalePrice, Right-Skewed.There are outliers in these Features.\n\n* Some values such as GarageCars -> SalePrice or Fireplaces -> SalePrice shows a particular pattern with verticals lines roughly meaning that they are discrete variables with a short range.\n\n* In most of the other features the mode of datapoints lie at 0, this might because those features may not be available for majority of the records(houses).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Correlation between features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Pandas **dataframe.corr()** is used to find the *pairwise correlation* of all columns in the dataframe. Any **na values are automatically excluded.** For any *non-numeric data type columns in the dataframe it is ignored*.\n\n> **Syntax: DataFrame.corr(self, method=’pearson’, min_periods=1)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_corr = df.corr()\n\n#Only the reltion coefficients between all other features to SalePrice.\ndf_corr = df_corr.SalePrice \n\ndf_corr = df_corr.drop('SalePrice')# Because we dont need the correlation SalePrice - SalePrice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can find the **features** which are *strongly correlated with SalePrice* from the above. And now we will store those Feature names in a list called, ***strong_corr_features***.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#strong correlation\n#sorted in descending order of correlation\nstrong_corr_features = df_corr[abs(df_corr) > 0.6].sort_values(ascending = False) #abs() to avoid the effect of sign\n\nprint('There are {} strongly correlated features with SalePrice:\\n{}'.format(len(strong_corr_features), strong_corr_features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at correlation between features we discovered 7 features which have a strong relationship to a house price. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Lets Check\n# Feature to Feture correlation\n\nThis will help us in feture reduction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df.drop('SalePrice', axis=1).corr() # We already examined SalePrice correlations\nplt.figure(figsize=(25, 25))\n\nsns.heatmap(corr[(corr >= 0.8) | (corr <= -0.8)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Obersvations:\n\n* A lot of features seems to be correlated between each other.\n* Some of them such as YearBuild -> GarageYrBlt may just indicate a price inflation over the years.\n* As for 1stFlrSF -> TotalBsmtSF, it is normal that the more the 1st floor is large (considering many houses have only 1 floor), the more the total basement will be large.\n* Very high correlation is found between [TotalSFA] -> [TotalBsmtSF, 1stFlrSF, GrLiveArea], where the relation between first two features is obvious but the relation between TotalSFA -> GrLivArea is interesting. \n\nLike wise there are many interesting things found from above.\n\n-------------","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now lets move ahead and check for  \n# Feature Importance","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Which of the features are more influencing the target variable?\nBy understanding this insted of using all 76 feature, we can use the top most influencing features to train the model.\n\n*We will use a random forest regressor to do that.*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Spliting data to X_train, y_train and X_test\ny_train = df['SalePrice']\n\nX_train = df.drop('SalePrice', axis = 1)\n\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature importance using random forest\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=80, max_features='auto')\nrf.fit(X_train, y_train)\nprint('Training done using Random Forest')\n\nranking = np.argsort(-rf.feature_importances_)\nf, ax = plt.subplots(figsize=(18, 12))\nsns.barplot(x=rf.feature_importances_[ranking], y=X_train.columns.values[ranking], orient='h')\nax.set_xlabel(\"Feature Importance\")\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Surprisingly, only 2 features are dominant: **'OverallQual'** and **'TotalSF'**. So instead of using all the 77 features, maybe just using the top 30 features is good enough (dimensionality reduction, in a way).\n\nHere, we make a new feature called 'Interaction': simply the multiplication between the top 2 features. Also, we normalize the data via z-scoring.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# use the top 30 features only\nX_train = X_train.iloc[:,ranking[:30]]\nX_test = X_test.iloc[:,ranking[:30]]\n\n# interaction between the top 2\nX_train[\"Interaction\"] = X_train[\"TotalSFA\"]*X_train[\"OverallQual\"]\nX_test[\"Interaction\"] = X_test[\"TotalSFA\"]*X_test[\"OverallQual\"]\n\n# zscoring\nX_train = (X_train - X_train.mean())/X_train.std()\nX_test = (X_test - X_test.mean())/X_test.std()\n    \n# heatmap\nf, ax = plt.subplots(figsize=(11, 5))\ncmap = sns.cubehelix_palette(light=1, as_cmap=True)\nsns.heatmap(X_train, cmap=cmap)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are ready with our most influencing features, lets check again how the are related to 'SalePrice' visually.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# relation to the target\nfig = plt.figure(figsize=(12,7))\nfor i in np.arange(30):\n    ax = fig.add_subplot(5,6,i+1)\n    sns.regplot(x=X_train.iloc[:,i], y=y_train)\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,7))\nsns.regplot(x=X_train.iloc[:,3], y=y_train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,7))\nsns.regplot(x=X_train.iloc[:,7], y=y_train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Observations:\n* TotalSFA show good linear relation toward SalePrice, we can observe few outlier in the TotalSFA panel, we can remove them for better results.\n* Its Samecase with GrLivArea, this shows linear relation and also have few outliers.\n* Under 'GarageArea' \nLets treat these outliers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_temp = X_train\nX_temp['SalePrice'] = y_train\nX_temp = X_temp.drop(X_temp[(X_temp['TotalSFA']>5) & (X_temp['SalePrice']<12.5)].index)\nX_temp = X_temp.drop(X_temp[(X_temp['GrLivArea']>5) & (X_temp['SalePrice']<13)].index)\nX_temp = X_temp.drop(X_temp[(X_temp['GarageArea']>3) & (X_temp['SalePrice']<12.5)].index)\nX_temp = X_temp.drop(X_temp[(X_temp['BsmtFinSF1']>2) & (X_temp['SalePrice']>13.25)].index)\nX_temp = X_temp.drop(X_temp[(X_temp['BsmtFinSF1']>-1) & (X_temp['SalePrice']<11.2)].index)\n# recover\ny_train = X_temp['SalePrice']\nX_train = X_temp.drop(['SalePrice'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, its time to use ensambling ML model: \n# XGBOOST","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGBoost\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\nprint(\"Parameter optimization\")\nxgb_model = xgb.XGBRegressor()\nreg_xgb = GridSearchCV(xgb_model,\n                   {'max_depth': [2,4,6],\n                    'n_estimators': [50,100,200]}, verbose=1)\nreg_xgb.fit(X_train, y_train)\nprint(reg_xgb.best_score_)\nprint(reg_xgb.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\ndef create_model(optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    return model\n\nmodel = KerasRegressor(build_fn=create_model, verbose=0)\n# define the grid search parameters\noptimizer = ['SGD','Adam']\nbatch_size = [10, 30, 50]\nepochs = [10, 50, 100]\nparam_grid = dict(optimizer=optimizer, batch_size=batch_size, epochs=epochs)\nreg_dl = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\nreg_dl.fit(X_train, y_train)\n\nprint(reg_dl.best_score_)\nprint(reg_dl.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVR\nfrom sklearn.svm import SVR\n\nreg_svr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n                               \"gamma\": np.logspace(-2, 2, 5)})\nreg_svr.fit(X_train, y_train)\n\nprint(reg_svr.best_score_)\nprint(reg_svr.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# second feature matrix\nX_train2 = pd.DataFrame( {'XGB': reg_xgb.predict(X_train),\n     'NN': reg_dl.predict(X_train).ravel(),\n     'SVR': reg_svr.predict(X_train),\n    })\nX_train2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# second-feature modeling using linear regression\nfrom sklearn import linear_model\n\nreg = linear_model.LinearRegression()\nreg.fit(X_train2, y_train)\n\n# prediction using the test set\nX_test2 = pd.DataFrame( {'XGB': reg_xgb.predict(X_test),\n     'DL': reg_dl.predict(X_test).ravel(),\n     'SVR': reg_svr.predict(X_test),\n    })\n\n# Don't forget to convert the prediction back to non-log scale\ny_pred = np.exp(reg.predict(X_test2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_pred = np.exp(reg_xgb.predict(X_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_Id = test_df['Id']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({ \n    \"Id\": test_Id, \n    \"SalePrice\": y_pred }) \n\nsubmission.to_csv('houseprice_111.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}