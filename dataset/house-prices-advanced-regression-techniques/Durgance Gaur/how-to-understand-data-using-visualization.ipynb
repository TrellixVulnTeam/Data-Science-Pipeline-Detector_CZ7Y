{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hi \n\n<img src=\"https://media.giphy.com/media/Vbtc9VG51NtzT1Qnv1/giphy.gif\">\n\nThis is a guide for understanding visualization and how to understand them for beginners.\nplease do upvote, and comment your insites.\n## Lets Get Started\n","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\npd.set_option('display.max_columns', None)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn import preprocessing, impute\n\nplt.style.use(\"ggplot\")\nrandom_state=42\nrng = np.random.default_rng(random_state)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-16T15:14:59.056628Z","iopub.execute_input":"2022-02-16T15:14:59.057041Z","iopub.status.idle":"2022-02-16T15:15:00.836873Z","shell.execute_reply.started":"2022-02-16T15:14:59.057012Z","shell.execute_reply":"2022-02-16T15:15:00.836405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/szeged-weather/weatherHistory.csv\")\ndf[\"Formatted Date\"]= pd.to_datetime(df[\"Formatted Date\"], errors='coerce', utc=True)\ndf.drop(\"Loud Cover\",inplace =True,axis =1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:15:00.837908Z","iopub.execute_input":"2022-02-16T15:15:00.838165Z","iopub.status.idle":"2022-02-16T15:15:01.667582Z","shell.execute_reply.started":"2022-02-16T15:15:00.838143Z","shell.execute_reply":"2022-02-16T15:15:01.66707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://media.giphy.com/media/waBshc4T2RcQGVd9MR/giphy.gif\">","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:15:01.668399Z","iopub.execute_input":"2022-02-16T15:15:01.668563Z","iopub.status.idle":"2022-02-16T15:15:01.694357Z","shell.execute_reply.started":"2022-02-16T15:15:01.66854Z","shell.execute_reply":"2022-02-16T15:15:01.693683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_col = [\"Summary\",\"Precip Type\",\"Daily Summary\"]\nnum_col = [\"Temperature (C)\",\"Apparent Temperature (C)\",\"Humidity\",\"Wind Speed (km/h)\",\"Wind Bearing (degrees)\",\"Visibility (km)\",\"Pressure (millibars)\"]\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:15:01.69581Z","iopub.execute_input":"2022-02-16T15:15:01.696005Z","iopub.status.idle":"2022-02-16T15:15:01.713852Z","shell.execute_reply.started":"2022-02-16T15:15:01.695977Z","shell.execute_reply":"2022-02-16T15:15:01.713305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n<img src=\"https://media.giphy.com/media/3og0IExSrnfW2kUaaI/giphy.gif\">\n\n## First Question should be why do we need this ??\n\nOut Come of this phase is as given below : \n\n- Understanding the given dataset and helps clean up the given dataset.\n- It gives you a clear picture of the features and the relationships between them.\n- Providing guidelines for essential variables and leaving behind/removing non-essential variables.\n- Handling Missing values or human error.\n- Identifying outliers.\n- EDA process would be maximizing insights of a dataset.\n- This process is time-consuming but very effective,","metadata":{}},{"cell_type":"markdown","source":"# First we start with checking for missing values :\n\nMissing values are not always due to sore problem, they can have conceptual meaning to a particular feature , no meaninig for some features, \n```python\n%timeit df.isnull().any().any()\n46.2 ms ¬± 899 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n%timeit df.isnull().values.any()\n44.6 ms ¬± 731 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n%timeit df.isna().values.any()\n41.8 ms ¬± 229 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n%timeit np.isnan(df.values).any()\n41.3 ms ¬± 368 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n```\n\nWe will come back to this in some time","metadata":{}},{"cell_type":"code","source":"import missingno as msno\nmsno.matrix(df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:15:01.714799Z","iopub.execute_input":"2022-02-16T15:15:01.714974Z","iopub.status.idle":"2022-02-16T15:15:02.271622Z","shell.execute_reply.started":"2022-02-16T15:15:01.71494Z","shell.execute_reply":"2022-02-16T15:15:02.271173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:15:02.272392Z","iopub.execute_input":"2022-02-16T15:15:02.272904Z","iopub.status.idle":"2022-02-16T15:15:03.003032Z","shell.execute_reply.started":"2022-02-16T15:15:02.272878Z","shell.execute_reply":"2022-02-16T15:15:03.002162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## HeatMap : \n\n<img src=\"https://media.giphy.com/media/uk6hDJFlvFNGxjssqT/giphy.gif\">\n\nA heatmap is a graphical representation where individual values of a matrix are represented as colors. A heatmap is very useful in visualizing the concentration of values between two dimensions of a matrix. This helps in finding patterns and gives a perspective of depth.","metadata":{}},{"cell_type":"code","source":"## Feature Interactions:\nplt.figure(figsize = (14,10))\nmask = np.triu(np.ones_like(df.corr()))\nsns.heatmap(df.corr(),cmap=\"RdYlGn\",mask = mask,annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:15:03.003908Z","iopub.execute_input":"2022-02-16T15:15:03.004287Z","iopub.status.idle":"2022-02-16T15:15:03.31461Z","shell.execute_reply.started":"2022-02-16T15:15:03.004264Z","shell.execute_reply":"2022-02-16T15:15:03.313989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.apply(lambda x: x.nunique()))\ndf.describe().T.style.background_gradient(\n    vmin=-1, vmax=1, cmap=sns.color_palette(\"vlag\", as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:15:03.315624Z","iopub.execute_input":"2022-02-16T15:15:03.315793Z","iopub.status.idle":"2022-02-16T15:15:03.456754Z","shell.execute_reply.started":"2022-02-16T15:15:03.315769Z","shell.execute_reply":"2022-02-16T15:15:03.456188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting out tabular data Always looks better to the eyes, to the heart and to our logical thinking as well. üòè","metadata":{}},{"cell_type":"code","source":"def annotate_plot(plots):\n    for bar in plots.patches:\n        plots.annotate(format(bar.get_height(), '.2f'),\n                       (bar.get_x() + bar.get_width() / 2,\n                        bar.get_height()), ha='center', va='center',\n                       size=15, xytext=(0, 8),\n                       textcoords='offset points')","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:15:03.457933Z","iopub.execute_input":"2022-02-16T15:15:03.458341Z","iopub.status.idle":"2022-02-16T15:15:03.464172Z","shell.execute_reply.started":"2022-02-16T15:15:03.45831Z","shell.execute_reply":"2022-02-16T15:15:03.46284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nunique_df = pd.DataFrame(df.apply(lambda x: x.nunique()))\nunique_df.drop(\"Formatted Date\",inplace =True)\nunique_df = unique_df.reset_index()\nunique_df.columns = [\"col\",\"values\"]\ng = sns.barplot(x=\"col\",data = unique_df, y= \"values\")\nannotate_plot(g)\nplt.xticks(rotation=90)\nplt.title(\"Unique Value Count\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:15:03.467157Z","iopub.execute_input":"2022-02-16T15:15:03.467414Z","iopub.status.idle":"2022-02-16T15:15:03.819069Z","shell.execute_reply.started":"2022-02-16T15:15:03.467377Z","shell.execute_reply":"2022-02-16T15:15:03.818507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(30, len(df.columns)*4))\nfor i,col in enumerate(num_col,1):\n    ax = plt.subplot(8,1,i)\n    sns.lineplot(data = df , x= \"Formatted Date\", y =col, hue = cat_col[1] )\n    \nplt.suptitle(f\"Distribution of Data With Respect to {cat_col[1]}\", fontsize = 20)\n\nplt.tight_layout(pad = 5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:35:32.06861Z","iopub.execute_input":"2022-02-16T15:35:32.068849Z","iopub.status.idle":"2022-02-16T15:36:04.605137Z","shell.execute_reply.started":"2022-02-16T15:35:32.068824Z","shell.execute_reply":"2022-02-16T15:36:04.604109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_date = df.set_index(\"Formatted Date\")","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:22:50.987269Z","iopub.execute_input":"2022-02-16T15:22:50.987548Z","iopub.status.idle":"2022-02-16T15:22:50.994628Z","shell.execute_reply.started":"2022-02-16T15:22:50.987515Z","shell.execute_reply":"2022-02-16T15:22:50.993657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Auto Correlation Plot for Time Series Plot\n\nWe can quantify the strength and type of relationship between observations and their lags.\n\nIn statistics, this is called correlation, and when calculated against lag values in time series, it is called autocorrelation (self-correlation).\n\nA correlation value calculated between two groups of numbers, such as observations and their lag1 values, results in a number between -1 and 1. The sign of this number indicates a negative or positive correlation respectively. A value close to zero suggests a weak correlation, whereas a value closer to -1 or 1 indicates a strong correlation.\n\nCorrelation values, called correlation coefficients, can be calculated for each observation and different lag values. Once calculated, a plot can be created to help better understand how this relationship changes over the lag.\n\nThis type of plot is called an autocorrelation plot and Pandas provides this capability built in, called the autocorrelation_plot() function.\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(30, len(df.columns)*4))\nfor i,col in enumerate(num_col,1):\n    ax = plt.subplot(8,1,i)\n    pd.plotting.autocorrelation_plot(df[col])\n    plt.xlabel(\"X-label\")\nplt.suptitle(f\"Distribution of Data With Respect to {cat_col[1]}\", fontsize = 20)\n\nplt.tight_layout(pad = 5)\nplt.show()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:36:51.179496Z","iopub.execute_input":"2022-02-16T15:36:51.179724Z","iopub.status.idle":"2022-02-16T15:37:39.014485Z","shell.execute_reply.started":"2022-02-16T15:36:51.179699Z","shell.execute_reply":"2022-02-16T15:37:39.013653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Understanding Data Using Some Statistics: \n\n<img src= \"https://media.giphy.com/media/9ADoZQgs0tyww/giphy.gif\">\n\nOkay so dont worry we are not going to take a deep dive in statistics here, So In Statistics, QQ(Quantile Quantile) Plot plays a very vital role to graphically analyse and compare two probability distributions by plotting there quantiles agains each other. If the two distributions whic we have compaired are exactly equal then the points on the QQ plot will form a perfect straight line y=x.\n\nIt is very important for us to know weather our data is normal or not so as to apply various statistical measures on the data and interpret it in much more human understandable visualization and there Q-Q Plot comes into the picture.The most fundamental question answered by QQ-q plot out data normally ditributed\n\n\n<img src = \"https://i.stack.imgur.com/NpI0O.png\">\n\nThis is how a Normally Distributed Q-Q Plot looks like much like a straight line Y=X.\n\n## Now must be asking why the heck do we need our data to be Normally Distributed?\n\nQQ plot are used to find the type of distribution for a random variable weather it be a Gaussian Disribution, Uniform Distribution , Exponential Distribution . We can tell the type of distribution just by looking at the QQ plot. In general we are talking about Normal Distribution as it works the best while fitting data to our ML model. And when we have a normal distribution of data we have some properties of our data such as we know the first standard deviation, second std. deviation of our data.\n\n\n**Q-Q plots** take your sample data, sort it in ascending order, and then plot them versus quantiles calculated from a theoretical distribution. The number of quantiles is selected to match the size of your sample data. While Normal Q-Q Plots are the ones most often used in practice due to so many statistical methods assuming normality, Q-Q Plots can actually be created for any distribution\n\n\n## Now comes the part when we get some understanding from our QQ plots.\n\nNow we will focus on the ends of the straight line. If the points at the ends of the curve formed from the points are not falling on a straight line but indeed are scattered significantly from the positions then we cannot conclude a relationship between the x and y axes which clearly signifies that our ordered values which we wanted to calculate are not Normally distributed.\nJust like out plots of ***Humidity , Wind , Wind Bending , Visibility*** which are not a complete straight line.\n\nThese Plots which are called **Skewed Q-Q Plots :**\n\nWhen we plot theoretical quantiles on the x-axis and the sample quantiles whose distribution we want to know on the y-axis then we see a very peculiar shape of a Normally distributed Q-Q plot for skewness. If the bottom end of the Q-Q plot deviates from the straight line but the upper end is not, then we can clearly say that the distribution has a longer tail to its left or simply it is left-skewed (or negatively skewed) but when we see the upper end of the Q-Q plot to deviate from the straight line and the lower and follows a straight line then the curve has a longer till to its right and it is right-skewed (or positively skewed).\n\nWe Have some more type of QQplot which is **Tailed QQ Plots :**\n\nlike : ***Temperature , Apparent Temperature***\n\n\n\nSo we did all this observation to get an understanding of which type of preprcessing technique to appply on our data to make our data normal, which in turn helps out machine Learning model to give better prediction\n","metadata":{}},{"cell_type":"code","source":"fig,axes = plt.subplots(len(num_col),2,figsize=(12, len(df.columns)*4))\nfor i, col in enumerate(num_col):\n    sns.histplot(df[col], ax = axes[i,0],kde =True)\n    sm.qqplot(df[col].dropna(),line = \"s\",fmt = \"b\",ax = axes[i,1])\n    axes[i, 1].set_title(col)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:04:46.767083Z","iopub.execute_input":"2022-02-16T14:04:46.767593Z","iopub.status.idle":"2022-02-16T14:05:00.781152Z","shell.execute_reply.started":"2022-02-16T14:04:46.767558Z","shell.execute_reply":"2022-02-16T14:05:00.780389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So now, we have seen the QQ plot some of the plots seems to a lot deviating of the desired for of distribution, we can start with applying log transformation on the features which are skewed. As a start and try to look for various distribution which can be helpfull for overcomming the skewness and Tialed characteristcs of the distribution,","metadata":{}},{"cell_type":"code","source":"# Applying varoius Trasfomation on data.\n_ = sm.qqplot(\n    df['Pressure (millibars)'].dropna().apply(\n        lambda x: np.log1p(-x)),\n    line=\"s\", fmt='b'\n)\n\n_ = sm.qqplot(\n    df['Visibility (km)'].dropna().apply(\n        lambda x: np.sqrt(x)),\n    line=\"s\", fmt='b'\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:05:00.791178Z","iopub.execute_input":"2022-02-16T14:05:00.792136Z","iopub.status.idle":"2022-02-16T14:05:02.035243Z","shell.execute_reply.started":"2022-02-16T14:05:00.792092Z","shell.execute_reply":"2022-02-16T14:05:02.034114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we are checking for differnt transformation and tying to find the best transformation which can convert our distribution to a normal Distribution","metadata":{}},{"cell_type":"code","source":"def scale_data(X):\n    inv_sigmoid = lambda x: np.log(x / (1-x))\n    \n    X = X.copy()\n    for col in num_col:\n        X[col] = preprocessing.minmax_scale(X[col], feature_range=(0+1e-6, 1-1e-6))\n        X[col] = X[col].apply(inv_sigmoid)\n    X[num_col] = preprocessing.power_transform(X[num_col])\n    #X['loudness'] = X['loudness'].apply(lambda x: np.log1p(-x))\n    return X\ndf_scaled = scale_data(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:05:02.036898Z","iopub.execute_input":"2022-02-16T14:05:02.037664Z","iopub.status.idle":"2022-02-16T14:05:04.046246Z","shell.execute_reply.started":"2022-02-16T14:05:02.037608Z","shell.execute_reply":"2022-02-16T14:05:04.045316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig,axes = plt.subplots(len(num_col),2,figsize=(12, len(df.columns)*4))\nfor i, col in enumerate(num_col):\n    sns.histplot(df_scaled[col], ax = axes[i,0],kde =True)\n    sm.qqplot(df_scaled[col].dropna(),line = \"s\",fmt = \"b\",ax = axes[i,1])\n    axes[i, 1].set_title(col)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:05:04.047632Z","iopub.execute_input":"2022-02-16T14:05:04.047979Z","iopub.status.idle":"2022-02-16T14:05:26.785622Z","shell.execute_reply.started":"2022-02-16T14:05:04.047935Z","shell.execute_reply":"2022-02-16T14:05:26.784632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Now New Question What to do with the data \n\n### What is Normalization?\n\nNormalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. It is also known as Min-Max scaling.\n\nHere's the fromula for normalization : \n\n<img src=\"https://i.stack.imgur.com/EuitP.png\" width=40%>\n\nHere, Xmax and Xmin are the maximum and the minimum values of the feature respectively.\n\nWhen the value of X is the minimum value in the column, the numerator will be 0, and hence X‚Äô is 0\nOn the other hand, when the value of X is the maximum value in the column, the numerator is equal to the denominator and thus the value of X‚Äô is 1\nIf the value of X is between the minimum and the maximum value, then the value of X‚Äô is between 0 and 1\n\n## What is Standardization?\n\nStandardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation\n\nHere's the formula for Standarization:\n\n<img src=\"https://clavelresearch.files.wordpress.com/2019/03/z-score-population.png\" width=30%>\n\n## The Big Question ‚Äì Normalize or Standardize?\n\nNormalization vs. standardization is an eternal question among machine learning newcomers. Let me elaborate on the answer in this section.\n\n- Normalization is good to use when you know that the distribution of your data does not follow a Gaussian distribution. This can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors and Neural Networks.\n\n- Standardization, on the other hand, can be helpful in cases where the data follows a Gaussian distribution. However, this does not have to be necessarily true. Also, unlike normalization, standardization does not have a bounding range. So, even if you have outliers in your data, they will not be affected by standardization.\n\nHowever, at the end of the day, the choice of using normalization or standardization will depend on your problem and the machine learning algorithm you are using. There is no hard and fast rule to tell you when to normalize or standardize your data. \n\n### Robust Scaler\nWhen working with outliers we can use Robust Scaling for scakling our data,\nIt scales features using statistics that are robust to outliers. This method removes the median and scales the data in the range between 1st quartile and 3rd quartile. i.e., in between 25th quantile and 75th quantile range. This range is also called an Interquartile range. \nThe median and the interquartile range are then stored so that it could be used upon future data using the transform method. If outliers are present in the dataset, then the median and the interquartile range provide better results and outperform the sample mean and variance. \nRobustScaler uses the interquartile range so that it is robust to outliers","metadata":{}},{"cell_type":"code","source":"# data\nx = pd.DataFrame({\n    # Distribution with lower outliers\n    'x1': np.concatenate([np.random.normal(20, 2, 1000), np.random.normal(1, 2, 25)]),\n    # Distribution with higher outliers\n    'x2': np.concatenate([np.random.normal(30, 2, 1000), np.random.normal(50, 2, 25)]),\n})\nnp.random.normal\n \nscaler = preprocessing.RobustScaler()\nrobust_df = scaler.fit_transform(x)\nrobust_df = pd.DataFrame(robust_df, columns =['x1', 'x2'])\n \nscaler = preprocessing.StandardScaler()\nstandard_df = scaler.fit_transform(x)\nstandard_df = pd.DataFrame(standard_df, columns =['x1', 'x2'])\n \nscaler = preprocessing.MinMaxScaler()\nminmax_df = scaler.fit_transform(x)\nminmax_df = pd.DataFrame(minmax_df, columns =['x1', 'x2'])\n \nfig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols = 4, figsize =(20, 5))\nax1.set_title('Before Scaling')\n \nsns.kdeplot(x['x1'], ax = ax1, color ='r')\nsns.kdeplot(x['x2'], ax = ax1, color ='b')\nax2.set_title('After Robust Scaling')\n \nsns.kdeplot(robust_df['x1'], ax = ax2, color ='red')\nsns.kdeplot(robust_df['x2'], ax = ax2, color ='blue')\nax3.set_title('After Standard Scaling')\n \nsns.kdeplot(standard_df['x1'], ax = ax3, color ='black')\nsns.kdeplot(standard_df['x2'], ax = ax3, color ='g')\nax4.set_title('After Min-Max Scaling')\n \nsns.kdeplot(minmax_df['x1'], ax = ax4, color ='black')\nsns.kdeplot(minmax_df['x2'], ax = ax4, color ='g')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see now our distribution if mostly Normalised and is ready to be used for processing and out ML model\n\n# Cluster Map\n\n## Your Question here should be What is Clustering?\n\nClustering is basically grouping data based on relationships among the variables in the data.  Clustering algorithms help in getting structured data in unsupervised learning. The most common types of clustering are shown below.\n\n<img src= \"https://media.geeksforgeeks.org/wp-content/uploads/20201123104401/Untitleddesign1.png\">\n\nHere we are going to see hierarchical clustering especially Agglomerative(bottom-up) hierarchical clustering. In Agglomerative clustering, we start with considering each data point as a cluster and then repeatedly combine two nearest clusters into larger clusters until we are left with a single cluster. The graph we plot after performing agglomerative clustering on data is called Dendrogram\n\n## Plotting Hierarchically clustered Heatmaps\n\nComing to the heat map, it is a graphical representation of data where values are represented using colors. Variation in the intensity of color depicts how data is clustered or varies over space.\n\nThe clustermap() function of seaborn plots a hierarchically-clustered heat map of the given matrix dataset. It returns a clustered grid index. \n\nIf we take an example of the [Flights](https://github.com/mwaskom/seaborn-data/blob/master/flights.csv) Dataset : \n\n\n```python\n# Importing the library\nimport seaborn as sns\nfrom sunbird.categorical_encoding import frequency_encoding\n  \n# Load dataset\ndata = sns.load_dataset('flights')\n  \n# Categorical encoding\nfrequency_encoding(data, 'month')\n  \n# Clustering data row-wise and\n# changing color of the map.\nsns.clustermap(data, figsize=(7, 7))\n```\n\n**Output :**\n\n<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20201126143308/e1.png\">\n\nThe legend to the left of the cluster map indicates information about the cluster map e.g bright color indicates more passengers and dark color indicates fewer passengers.\n\n```python\n# Importing the library\nimport seaborn as sns\nfrom sunbird.categorical_encoding import frequency_encoding\n  \n# Load dataset\ndata = sns.load_dataset('flights')\n  \n# Categorical encoding\nfrequency_encoding(data, 'month')\n  \n# Clustering data row-wise and\n# changing color of the map.\nsns.clustermap(data, cmap='coolwarm', figsize=(7, 7))\n```\n\n**Output :**\n\n<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20201126143656/e2.png\">\n","metadata":{}},{"cell_type":"code","source":"sns.clustermap(df.corr(), center=0, cmap=\"vlag\")","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:05:26.786875Z","iopub.execute_input":"2022-02-16T14:05:26.787134Z","iopub.status.idle":"2022-02-16T14:05:27.249996Z","shell.execute_reply.started":"2022-02-16T14:05:26.787104Z","shell.execute_reply":"2022-02-16T14:05:27.249072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA (Principle Component Analysis) : \n\nMany Machine Learning problems involve thousands or even millions of features for each training instance. Not only does this make training extremely slow, it can also make it much harder to find a good solution, as we will see. This problem is often referred to as *the curse of dimensionality*. Fortunately, in real-world problems, it is often possible to reduce the number of features considerably, turning an intractable problem into a tractable one. For example, consider the MNIST images the pixels on the image borders are almost always white, so you could completely drop these pixels from the training set without losing much information. Moreover, two neighboring pixels are often highly correlated, if you merge them into a single pixel (e.g., by taking the mean of the two pixel intensities), you will not lose much information\n\n**THE CURSE OF DIMENTIONALITY**\n\n<img src=\"https://media.giphy.com/media/xT9DPPHKfbz58057OM/giphy.gif\">\n\n>***Reducing dimensionality does lose some information (just like compressing an image to JPEG can degrade its quality), so even though it will speed up training, it may also make your system perform slightly worse. It also makes your pipelines a bit more complex and thus harder to maintain. So you should first try to train your system with the original data before considering using dimensionality reduction if training is too slow. In some cases, however, reducing the dimensionality of the training data may filter out some noise and unnecessary details and thus result in higher performance (but in general it won‚Äôt; it will just speed up training)***\n\n# So Why to do PCA in for DataViz must be you Question :\n\nApart from speeding up training, dimensionality reduction is also extremely useful for data visualization (or DataViz). Reducing the number of dimensions down to two (or three) makes it possible to plot a condensed view of a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such as clusters. Moreover, DataViz is essential to communicate your conclusions to people who are not data scientists, in particular decision makers who will use your results\n\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(random_state=random_state).fit(df_scaled[num_col])\n\nX_pca = pd.DataFrame(pca.transform(df[num_col]), index=df_scaled.index)\n\npca_comp = pd.DataFrame(pca.components_, index=num_col)\npca_comp.style.background_gradient(\n    vmin=-1, vmax=1, cmap=sns.color_palette(\"vlag\", as_cmap=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:05:27.251533Z","iopub.execute_input":"2022-02-16T14:05:27.251865Z","iopub.status.idle":"2022-02-16T14:05:27.379149Z","shell.execute_reply.started":"2022-02-16T14:05:27.251809Z","shell.execute_reply":"2022-02-16T14:05:27.378204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from umap import UMAP\nimport umap.plot\nfrom sklearn.manifold import TSNE\nplt.figure(figsize=(12,10))\nproj = UMAP().fit_transform(df_scaled[num_col])\nsns.scatterplot(x=proj[:, 0], y=proj[:, 1], hue=df['Precip Type'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T17:29:52.101291Z","iopub.execute_input":"2022-02-16T17:29:52.104577Z","iopub.status.idle":"2022-02-16T17:30:15.839622Z","shell.execute_reply.started":"2022-02-16T17:29:52.104067Z","shell.execute_reply":"2022-02-16T17:30:15.838063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we can see the distribution of Precip Type in our complete data, it seems fairly easy to look and tell using these techniques which part of our data point will will have rain or snow.","metadata":{}},{"cell_type":"markdown","source":"# One of the Most Usefull Plot: \nYes it is what you already knew PairPlot\n\nPair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df ,hue = \"Precip Type\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:07:31.409138Z","iopub.execute_input":"2022-02-16T14:07:31.40949Z","iopub.status.idle":"2022-02-16T14:11:39.446192Z","shell.execute_reply.started":"2022-02-16T14:07:31.409455Z","shell.execute_reply":"2022-02-16T14:11:39.442898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_distplot(data_,color_,col_name):\n    from scipy import stats\n    sns.set(font_scale=1.5)\n    fig = plt.figure(figsize=(12,10))\n    sns.distplot(data_ , color = color_, fit=stats.norm)\n    plt.title(f'Distribution of {col_name}')\n\n    # Get the fitted parameters used by sns\n    (mu, sigma) = stats.norm.fit(data_.dropna())\n    print(\"mu={0}, sigma={1}\".format(mu, sigma))\n\n    # Legend and labels \n    plt.legend([\"normal dist. fit ($\\mu=${0:.2g}, $\\sigma=${1:.2f})\".format(mu, sigma)])\n    plt.ylabel('Frequency')\n\n    # Cross-check this is indeed the case - should be overlaid over black curve\n    x_dummy = np.linspace(stats.norm.ppf(0.01), stats.norm.ppf(0.99), 100)\n    plt.plot(x_dummy, stats.norm.pdf(x_dummy, mu, sigma))\n    plt.legend([\"normal dist. fit ($\\mu=${0:.2g}, $\\sigma=${1:.2f})\".format(mu, sigma),\n            \"cross-check\"])\n    plt.box(False)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:11:39.448357Z","iopub.execute_input":"2022-02-16T14:11:39.448675Z","iopub.status.idle":"2022-02-16T14:11:39.458539Z","shell.execute_reply.started":"2022-02-16T14:11:39.44864Z","shell.execute_reply":"2022-02-16T14:11:39.457518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_t = num_col[1]\ncreate_distplot(df[plot_t],\"blue\",plot_t)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:11:39.45997Z","iopub.execute_input":"2022-02-16T14:11:39.460207Z","iopub.status.idle":"2022-02-16T14:11:40.583248Z","shell.execute_reply.started":"2022-02-16T14:11:39.46018Z","shell.execute_reply":"2022-02-16T14:11:40.582196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,10))\nplt.subplot(2,1,1)\nsns.regplot(data=df , x=\"Temperature (C)\",y=\"Visibility (km)\" ,scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"} )\nsns.kdeplot(data =df , x=\"Temperature (C)\",y=\"Visibility (km)\")\n\nplt.subplot(2,1,2)\nsns.regplot(data=df , x=\"Temperature (C)\",y=\"Humidity\" ,scatter_kws={\"color\": \"black\"}, line_kws={\"color\": \"red\"} )\nsns.kdeplot(data =df , x=\"Temperature (C)\",y=\"Humidity\")\n\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here the plot which we have made is the Scatter Plot of the data with Regreesion line which has the contour of the Disribution Plotted over it.\n\n# And I know here comes the Why, do i need to this ...!\n\nafter everything i have dont to get a better understanding of the data.\n\nLets see, \n\n- First thing we have here is a scatter plot.\n\n    The regression plots in seaborn are primarily intended to add a visual guide that helps to emphasize patterns in a dataset during exploratory data analyses \nfor Example : \n\n```python\nsns.lmplot(x ='total_bill', y ='tip', data = dataset, \n           col ='sex', row ='time', hue ='smoker')\n```\n<img src=\"https://seaborn.pydata.org/_images/regression_42_0.png\">\n\nIn the above code, we draw multiple plots by specifying a separation with the help of the rows and columns. Each row contains the plots of tips vs the total bill for the different times specified in the dataset. Each column contains the plots of tips vs the total bill for the different genders. A further separation is done by specifying the hue parameter on the basis of whether the person smokes.\n\n- Contour Plot : \n    A contour plot is a graph that you can use to explore the potential relationship between three variables.\n\n    Contour plots display the 3-dimensional relationship in two dimensions, with x- and y-factors (predictors) plotted on the x- and y-scales and response values represented by contours.\n\n    A contour plot is like a topographical map in which x-, y-, and z-values are plotted instead of longitude, latitude, and elevation.\n\n```r\nlibrary(plotly)\nplot_ly(z = volcano, type = \"contour\")\n```\n\n\n<img src=\"https://qphs.fs.quoracdn.net/main-qimg-38871bb72d6021bcadd74d934e1ae9f6-pjlq\">","metadata":{}},{"cell_type":"markdown","source":"# Handling Categorical Variables\n\nCategorical variables/features are any feature type can be classified into two major types:\n- Nominal\n- Ordinal\n\nNominal variables are variables that have two or more categories which do not have any kind of order associated with them. For example, if gender is classified into two groups, i.e. male and female, it can be considered as a nominal variable.Ordinal variables, on the other hand, have ‚Äúlevels‚Äù or categories with a particular order associated with them. For example, an ordinal categorical variable can be a feature with three different levels: low, medium and high. Order is important.\n\nIt is a binary classification problem:\nthe target here is **not skewed** but we use the best metric for this binary classification problem which would be Area Under the ROC Curve (AUC). We can use precision and recall too, but AUC combines these two metrics. Thus, we will be using AUC to evaluate the model that we build on this dataset.\n\nWe have to know that computers do not understand text data and thus, we need to convert these categories to numbers. A simple way of doing that can be to use :\n- Label Encoding\n```python\nfrom sklearn.preprocessing import LabelEncoder\n```\n- One Hot Encoding\n```python\npd.get_dummies()\n```\n\nbut we need to understand where to use which type of label encoding:\n\n**For not Tree based Machine Learning Algorithms the best way to go will be to use One-Hot Encoding**\n- One-Hot-Encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. \n- The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality. In these cases, I typically employ one-hot-encoding followed by PCA for dimensionality reduction. I find that the judicious combination of one-hot plus PCA can seldom be beat by other encoding schemes. PCA finds the linear overlap, so will naturally tend to group similar features into the same feature\n\n**For Tree based Machine Learning Algorithms the best way to go is with Label Encoding**\n\n- LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat. Still there are algorithms like decision trees and random forests that can work with categorical variables just fine and LabelEncoder can be used to store values using less disk space.","metadata":{}},{"cell_type":"markdown","source":"# Looking at Other Non-Numeric Variables in our Data\n\n<img src=\"https://media.giphy.com/media/3oKIPEqDGUULpEU0aQ/giphy.gif\">","metadata":{"execution":{"iopub.status.busy":"2022-02-15T19:13:00.394036Z","iopub.execute_input":"2022-02-15T19:13:00.394974Z","iopub.status.idle":"2022-02-15T19:13:00.39822Z","shell.execute_reply.started":"2022-02-15T19:13:00.394936Z","shell.execute_reply":"2022-02-15T19:13:00.397682Z"}}},{"cell_type":"markdown","source":"# What are Word Clouds?\n\nWord clouds (also known as text clouds or tag clouds) work in a simple way: the more a specific word appears in a source of textual data (such as a speech, blog post, or database), the bigger and bolder it appears in the word cloud.\n\nA word cloud is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it‚Äôs mentioned within a given text and the more important it is.\n\nAlso known as tag clouds or text clouds, these are ideal ways to pull out the most pertinent parts of textual data, from blog posts to databases. They can also help business users compare and contrast two different pieces of text to find the wording similarities between the two.\n\nPerhaps you‚Äôre already leveraging advanced data visualization techniques to turn your important analytics into charts, graphs, and infographics. This is an excellent first step, as our brains prefer visual information over any other format.\n\nYet, what do you do if your raw data is text-based in nature?\n\nMuch of the research your organization conducts will include at least some form of an open-ended inquiry that prompts respondents to give a textual answer.\n\nFor instance, you might ask current customers what they like or don‚Äôt like about your new product line. Or, you could ask them to give suggestions on how your organization could improve. They could also have the chance to elaborate on any pain points they‚Äôre experiencing.\n\nThere are industry tools that allow you to code such open-ended data so users can understand it quantitatively. Yet, these don‚Äôt come cheap. Word clouds offer a cost-effective, yet powerful, alternative.\n\nWith these, you can still quantify your text-based insights into measurable analytics. The only difference? You won‚Äôt create a chart or graph as you would with a set of numbers.\n\nInstead, you‚Äôll create a word cloud generator to transform the most critical information into a word cloud.","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\ncomment_words = ''\nstopwords = set(STOPWORDS)\n \n# iterate through the csv file\nfor val in df[\"Daily Summary\"]:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n    pass\n \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\nplt.gca().set_title('Total Daily Description of Weather')\nplt.show()   ","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:14:24.468673Z","iopub.execute_input":"2022-02-16T14:14:24.46893Z","iopub.status.idle":"2022-02-16T14:14:27.484977Z","shell.execute_reply.started":"2022-02-16T14:14:24.468897Z","shell.execute_reply":"2022-02-16T14:14:27.484284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating plots which have diifferent meaning for Raning and Snowing. To get the correct Type of Daily Summary which was used on which kind of weather","metadata":{}},{"cell_type":"code","source":"# iterate through the csv file\nfor val in df[df[\"Precip Type\"] == \"rain\"][\"Daily Summary\"]:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n    pass\n \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n \n# plot the WordCloud image   \n\nplt.figure(figsize = (20, 18), facecolor = None)\nax = plt.subplot(1,2,1)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.gca().set_title('Rain')\n\nfor val in df[df[\"Precip Type\"] == \"snow\"][\"Daily Summary\"]:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n    pass\n \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\nplt.subplot(1,2,2)\nplt.imshow(wordcloud)\nplt.gca().set_title('Snow')\nplt.axis(\"off\")\nplt.show()   ","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:14:27.48636Z","iopub.execute_input":"2022-02-16T14:14:27.487053Z","iopub.status.idle":"2022-02-16T14:14:35.717617Z","shell.execute_reply.started":"2022-02-16T14:14:27.487006Z","shell.execute_reply":"2022-02-16T14:14:35.716848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,16))\ng = sns.countplot(df.Summary)\nannotate_plot(g)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T14:14:35.719318Z","iopub.execute_input":"2022-02-16T14:14:35.719939Z","iopub.status.idle":"2022-02-16T14:14:36.503509Z","shell.execute_reply.started":"2022-02-16T14:14:35.719879Z","shell.execute_reply":"2022-02-16T14:14:36.502533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Man This was a long but i hope you guys enjoyed it.\n\nThankyou for reading this far.\n\n**If You liked this Kernel Please do upvote it üëçüëç**\n\n<img src=\"https://media.giphy.com/media/NKkZbYYsZmRzi/giphy.gif\">\n\n# Further Readings if interested!!!!\n\nI have created these notebooks as well for anyone who is intereseted in learning new things in the domain of machine learning application. Please refer to them if interested.üòÅ\n- Notebook : [A Guide to Any Classification Problem](https://www.kaggle.com/durgancegaur/a-guide-to-any-classification-problem) \n- Notebook : Automating the Machine Learning workflow using [AutoXGB : XGBoost + Optuna](https://www.kaggle.com/durgancegaur/autoxgb-xgboost-optuna-score-0-95437-dec) \n- Notebook : [Working with CatBoost](https://www.kaggle.com/durgancegaur/tps-dec-hyperp-tuning-catboost-score-0-95451)\n- Notebook : Working with data imbalance and [upsampling of data SMOTE](https://www.kaggle.com/durgancegaur/working-with-data-imbalance-eda-99-auc)\n- Notebook : Working wtih [H2OAuoML](https://www.kaggle.com/durgancegaur/eda-and-working-with-h2oautoml)\n\n\n<img src=\"https://media.giphy.com/media/wsWaiO3gBYXyZeq9v2/giphy.gif\">","metadata":{}}]}