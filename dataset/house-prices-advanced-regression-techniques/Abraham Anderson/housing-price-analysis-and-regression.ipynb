{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. We need to import libraries and data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cufflinks as cf\ncf.go_offline()\n%matplotlib inline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn import metrics \nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()\n#Now we get an overall statistical picture of our data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()\n#There are 1460 rows and 81 columns, 43 of which are objects, 35 of it are int64 and 3 of them are float64","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Visualization of the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"SalePrice\"].iplot(kind=\"hist\")\n#The target variable seems right-skewed distributed  mostly between 100 000 dolars and 200 000 dolars\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"SalePrice\"].mean(),train[\"SalePrice\"].median(),train[\"SalePrice\"].mode()\n#Right skewed distribution means that the mean(180921) and median(163000) will be greater than the mode(140000) similar to this dataset. \n#Which means more houses were sold by less than the average price(180921).","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"SalePrice\"].iplot(kind=\"box\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(15,10))\ntrain[\"SalePrice\"].plot.box()\n#The target variable has many outliers as seen both of box plots","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The next step is to find out the correlation between target variable and other variables\n#But this step only include numerical variables and non-numerical variables are excluded \n \ntrain.corr()[\"SalePrice\"].sort_values(ascending = False)\n#OverallQual column has the highest correlation point","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Skewness: {train['SalePrice'].skew()}\")\nprint(f\"Kurtosis: {train['SalePrice'].kurt()}\")\nfigure = plt.figure(figsize=(15,10))\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'] , fit=norm);\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.ylabel('Frequency')\nplt.title('The Distribution of sale Prices of Houses')\n\nplt.subplot(1,2,2)\nstats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Sale Prices of Houses is right skewed as it is seen above which means that  the mean greater than mode and median and median is greater than mode.We need to fix this skewness because many of the algorithms assume that the data science is normal and calculate various stats assuming this.We can fix this by using numpy.log1p() method normalise skweness by adding one(one added so that the zeros are being transformed to one as log of 0 is not defined) and taking natural log.  So the more the data is close to normal the more it fits the assumption.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[[\"OverallQual\",\"SalePrice\"]].scatter_matrix()\n#The figures below there is a string correlation between the two ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,15))\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(train.corr(),annot=True,mask = mask,cmap=\"coolwarm\")\n#Here we visualize the correlation between all variables in the data set\nplt.title(\"Heatmap of all the Features\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"SalePrice\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we save target value for later and drop it from the train data set\ny = train[\"SalePrice\"]\ntrain.drop('SalePrice', axis=1, inplace=True)\ny","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Dealing with the Missing Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now it is time to deal with the missing values in the columns of the data\ntrain.isnull().sum().sort_values(ascending = False).head(35)\n#There 35 columns which have missing values as seen below:","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum().sort_values(ascending = False).head(35)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30,15))\nsns.heatmap(train.isnull(),cmap=\"coolwarm\")\n# Now we visualize the missing values of the columns in our train data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing=train.isnull().sum().sort_values(ascending=False)\ntotal=train.isnull().count().sort_values(ascending=False)\npercentage=(train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\npd.concat([missing,total,percentage],axis=1,keys=[\"Missing\",\"Total\",\"Percentage\"]).head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing=test.isnull().sum().sort_values(ascending=False)\ntotal=test.isnull().count().sort_values(ascending=False)\npercentage=(test.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\npd.concat([missing,total,percentage],axis=1,keys=[\"Missing\",\"Total\",\"Percentage\"]).head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Some of the columns have many missing values, so it is better just drop them\n#Because more than 80% of the column are missing in both train dataset and test dataset\ntrain.drop([\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\"],axis=1,inplace=True)\ntest.drop([\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\"],axis=1,inplace=True)\nplt.figure(figsize=(30,15))\nsns.heatmap(train.isnull(),cmap=\"coolwarm\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(\"FireplaceQu\",axis=1,inplace=True) #This column also have many missing values. we will just get rid of them\ntest.drop(\"FireplaceQu\",axis=1,inplace=True)\nplt.figure(figsize=(30,15))\nsns.heatmap(train.isnull(),cmap=\"coolwarm\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"Now the data is ready to ampute the missing values for the rest of columns\n   We will segregate numeric and categoric columns in order to ampute them differently\"\"\"\nnumeric_cols = train.select_dtypes(['float64','int64']).columns\ncategoric_cols = train.select_dtypes('object').columns\n\nnumeric = train[numeric_cols]\ncategoric = train[categoric_cols]\n\ndef mean_imputation(numeric):\n    \"\"\"\n    Filling the missing values with the mean of the categorical columns\n    \"\"\"\n    for col in numeric.columns:\n        mean = numeric[col].mean()\n        numeric[col] = numeric[col].fillna(mean)\n    return numeric\n\nmean_imp = mean_imputation(numeric)\n\ndef mode_imputation(categoric):\n    \"\"\"\n    Filling the missing values with the mode of the categorical columns\n    \"\"\"\n    for col in categoric.columns:\n        mode = categoric[col].mode().iloc[0]\n        categoric[col] = categoric[col].fillna(mode)\n    return categoric\n\nmode_imp = mode_imputation(categoric)\n'''We will concatenate the mean and mode imputed columns together'''\ntrain = pd.concat([mean_imp, mode_imp], axis = 1)\ntrain\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_cols = test.select_dtypes(['float64','int64']).columns\ncategoric_cols = test.select_dtypes('object').columns\n\nnumeric = test[numeric_cols]\ncategoric = test[categoric_cols]\n\ndef mean_imputation(numeric):\n    \"\"\"\n    Filling the missing values with the mean of the categorical columns\n    \"\"\"\n    for col in numeric.columns:\n        mean = numeric[col].mean()\n        numeric[col] = numeric[col].fillna(mean)\n    return numeric\n\nmean_imp = mean_imputation(numeric)\n\ndef mode_imputation(categoric):\n    \"\"\"\n    Filling the missing values with the mode of the categorical columns\n    \"\"\"\n    for col in categoric.columns:\n        mode = categoric[col].mode().iloc[0]\n        categoric[col] = categoric[col].fillna(mode)\n    return categoric\n\nmode_imp = mode_imputation(categoric)\n'''We will concatenate the mean and mode imputed columns together'''\ntest = pd.concat([mean_imp, mode_imp], axis = 1)\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum().sort_values(ascending = False).head()\n#Now there is not any column with missing data in our train dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum().sort_values(ascending = False).head()\n#Now there is not any column with missing data in our test dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()\n#Now we have 75 columns and 1460 rows without any missing valued column in the train  data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()\n#Now we have 74 columns and 1459 rows without any missing valued column\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"4.Future Engineering:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#In order to be more precise, we need to convert floats into integers,because all floats have not any data after \".\"\nfor col in train.select_dtypes(include=\"float64\").columns:\n    train[col]=train[col].astype(np.int64)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will do the same for the train data set as follows:\nfor col in test.select_dtypes(include=\"float64\").columns:\n    test[col]=train[col].astype(np.int64)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.select_dtypes(include=\"object\").shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.select_dtypes(include=\"object\").shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Now both train and test data set have been syncronized in terms of types of data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Machine learning algorithms cannot run with categorical columns so we need to make them numerical:\nWe will use Ordinal Encoder for this purpose:\nOrdinalEncoder(*, categories='auto', dtype=<class 'numpy.float64'>) |\n| Encode categorical features as an integer array. |\n| The input to this transformer should be an array-like of integers or | strings, denoting the values taken on by categorical (discrete) features. | The features are converted to ordinal integers. This results in | a single column of integers (0 to n_categories - 1) per feature.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"categoric_cols = train.select_dtypes('object').columns\nnumeric_cols=train.select_dtypes('int64').columns\ncategoric = train[categoric_cols]\nnumeric=train[numeric_cols]\nfrom sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nfor col in categoric.columns:\n    categoric[col] = pd.DataFrame(ordinal_encoder.fit_transform(categoric)) #This returns a numpy array\ncategoric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will do the same for our test data set:\ntest_categoric_cols = test.select_dtypes('object').columns\ntest_numeric_cols=test.select_dtypes('int64').columns\ntest_categoric = test[test_categoric_cols]\ntest_numeric=test[test_numeric_cols]\nfrom sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nfor col in test_categoric.columns:\n    test_categoric[col] = pd.DataFrame(ordinal_encoder.fit_transform(test_categoric)) #This returns a numpy array\ntest_categoric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we will concetanate the previous numerical variables with the converted categorical variables\ntrain_ready = pd.concat([numeric, categoric], axis = 1)\ntest_ready = pd.concat([test_numeric, test_categoric], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_ready.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ready.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5.Splitting the Data and Training the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X=train_ready # here we assing train data set as our X value for the algorithm\nX.shape,y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.4,random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlm=LinearRegression()\nlm.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#In order to evaluate the performance we need to predict the test data and compare the predictions with the actual test data values\npredictions=lm.predict(X_test)\n#here we predict the y_test values from X_test data according to the our trained Linear Regression data\npredictions\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# here I will visualize the real test values(y_test) versus the predicted values.\nsns.scatterplot(y_test,predictions)\n#It seems that our linear regression model predict ver well","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We will evaluate our model performance by calculating the residual sum of squares and the explained variance score\nfrom sklearn import metrics\nprint(\"MAE:\",metrics.mean_absolute_error(y_test,predictions))\nprint (\"MSE:\",metrics.mean_squared_error(y_test,predictions))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test,predictions)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Evaluation of  the explained variance score (R^2)\nmetrics.explained_variance_score(y_test,predictions) #This shows our model predict %79.9 of the target correctly","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now we will visualize the differences between our predictions and actual y test data\nsns.distplot(y_test-predictions,bins=50) #this figure also proves that our model fits very good\n#There is no huge differences between our predictions and actual y data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cdf=pd.DataFrame(lm.coef_,X.columns,columns=[\"Coefficients\"])\ncdf[\"Coefficients\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we se that our model predict good, we will predict the test data for this time\npredictions=lm.predict(test_ready)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=pd.DataFrame(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit=pd.concat([test['Id'],predictions],axis=1)\nsubmit.columns=['Id','SalePrice']\nlen(submit.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv(\"submission.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}