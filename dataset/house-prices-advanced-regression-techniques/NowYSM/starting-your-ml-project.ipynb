{"cells":[{"metadata":{"_uuid":"8c8591f89df1d36843546a8241150b966682d4c8","_cell_guid":"5f1611b2-6aae-4694-9516-d7651880b314"},"cell_type":"markdown","source":"*This tutorial is part of the [Learn Machine Learning](https://www.kaggle.com/learn/machine-learning) educational track.*\n\n# Starting Your Project\n\nYou are about to build a simple model and then continually improve it. It is easiest to keep one browser tab (or window) for the tutorials you are reading, and a separate browser window with the code you are writing. You will continue writing code in the same place even as you progress through the sequence of tutorials.\n\n** The starting point for your project is at [THIS LINK](https://www.kaggle.com/dansbecker/my-model/).  Open that link in a new tab. Then hit the \"Fork Notebook\" button towards the top of the screen.**\n\n![Imgur](https://i.imgur.com/GRtMTWw.png)\n\n**You will see examples predicting home prices using data from Melbourne, Australia. You will then write code to build a model predicting prices in the US state of Iowa. The Iowa data is pre-loaded in your coding notebook.**\n\n### Working in Kaggle Notebooks\nYou will be coding in a \"notebook\" environment. These allow you to easily see your code and its output in one place.  A couple tips on the Kaggle notebook environment:\n\n1) It is composed of \"cells.\"  You will write code in the cells. Add a new cell by clicking on a cell, and then using the buttons in that look like this. ![Imgur](https://i.imgur.com/Lscji3d.png) The arrows indicate whether the new cell goes above or below your current location. <br><br>\n2) Execute the code in the current cell with the keyboard shortcut Control-Enter.\n\n\n---\n# Using Pandas to Get Familiar With Your Data\n\nThe first thing you'll want to do is familiarize yourself with the data.  You'll use the Pandas library for this.  Pandas is the primary tool that modern data scientists use for exploring and manipulating data.  Most people abbreviate pandas in their code as `pd`.  We do this with the command"},{"metadata":{"_uuid":"0d9fdf505bc0a0c98d4f3d8bb5ef387f05cda426","collapsed":true,"_cell_guid":"1dde37af-64d3-45fc-b1b9-c300a1d49fc2","trusted":true},"cell_type":"code","source":"import pandas as pd","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"390958537e6e6f663708b2d2cd4bd42b3a3d4cfd","_cell_guid":"b62d4ca0-38c8-4cbf-91f8-d5b95eb293fc"},"cell_type":"markdown","source":"The most important part of the Pandas library is the DataFrame.  A DataFrame holds the type of data you might think of as a table. This is similar to a sheet in Excel, or a table in a SQL database. The Pandas DataFrame has powerful methods for most things you'll want to do with this type of data.  Let's start by looking at a basic data overview with our example data from Melbourne and the data you'll be working with from Iowa.\n\nThe example will use data at the file path **`../input/melbourne-housing-snapshot/melb_data.csv`**.  Your data will be available in your notebook at `../input/train.csv` (which is already typed into the sample code for you).\n\nWe load and explore the data with the following:"},{"metadata":{"_uuid":"25c8f2fc298bb94a2c40075dff37f3fe3d132f4e","_cell_guid":"051872bc-8ad1-4e9e-b9e2-af7ec905f9c5","trusted":true},"cell_type":"code","source":"# save filepath to variable for easier access\nmelbourne_file_path = '../input/melbourne-housing-snapshot/melb_data.csv'\n# read the data and store data in DataFrame titled melbourne_data\nmelbourne_data = pd.read_csv(melbourne_file_path) \n# print a summary of the data in Melbourne data\nmelbourne_data.describe()","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"365b5901e2d7f932c7dd91a237c6341cc932a8b7","_cell_guid":"007667cb-5c8d-4fcc-b727-2432d2ebb037","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntrain.describe()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"503ac4c159eb745678c4756aa8d1d2b81697be2a","_cell_guid":"5538ff34-3427-4cb5-a11d-30a6378a631e"},"cell_type":"markdown","source":"# Interpreting Data Description\nThe results show 8 numbers for each column in your original dataset. The first number, the **count**,  shows how many rows have non-missing values.  \n\nMissing values arise for many reasons. For example, the size of the 2nd bedroom wouldn't be collected when surveying a 1 bedroom house. We'll come back to the topic of missing data.\n\nThe second value is the **mean**, which is the average.  Under that, **std** is the standard deviation, which measures how numerically spread out the values are.\n\nTo interpret the **min**, **25%**, **50%**, **75%** and **max** values, imagine sorting each column from lowest to highest value.  The first (smallest) value is the min.  If you go a quarter way through the list, you'll find a number that is bigger than 25% of the values and smaller than 75% of the values.  That is the **25%** value (pronounced \"25th percentile\").  The 50th and 75th percentiles are defined analgously, and the **max** is the largest number.\n\n--- \n# Your Turn\n**Remember, the notebook you want to \"fork\" is [here](https://www.kaggle.com/dansbecker/my-model/).**\n\nRun the equivalent commands (to read the data and print the summary) in the code cell below.  The file path for your data is already shown in your coding notebook. Look at the mean, minimum and maximum values for the first few fields. Are any of the values so crazy that it makes you think you've misinterpreted the data?\n\nThere are a lot of fields in this data.  You don't need to look at it all quite yet.\n\nWhen your code is correct, you'll see the size, in square feet, of the smallest lot in your dataset.  This is from the **min** value of **LotArea**, and you can see the **max** size too.  You should notice that it's a big range of lot sizes! \n\nYou'll also see some columns filled with `...`.  That indicates that we had too many columns of data to print, so the middle ones were omitted from printing.\n\nWe'll take care of both issues in the next step.\n\n# Continue\nMove on to the next [page](https://www.kaggle.com/dansbecker/Selecting-And-Filtering-In-Pandas/) where you will focus in on the most relevant columns."},{"metadata":{"_uuid":"c08e86b4dc17d973551f3ec3d27869d1947bc553","scrolled":true,"_cell_guid":"5c91f7c7-80e5-478d-aba1-8d3559c61295","trusted":true},"cell_type":"code","source":"print(melbourne_data.columns)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"00c246f8374037953ec2b84e850e139cd7c0c487","_cell_guid":"e2d43993-a5dc-4006-b107-03341b657a3a"},"cell_type":"markdown","source":"# Selecting and Filtering Data"},{"metadata":{"_uuid":"7d103be705d7fccce1c7da3d67e012459430fcf5","_cell_guid":"ba76177c-b6ce-4e29-97e0-3af5a279cfbf"},"cell_type":"markdown","source":"## Selecting a Single Column\nYou can pull out any variable (or column) with **dot-notation**. This single column is stored in a **Series**, which is broadly like a DataFrame with only a single column of data. Here's an example:"},{"metadata":{"_uuid":"db8ee4bdcb76b493a2e6abbda8672cdd01d5bc96","_cell_guid":"4b4e32ee-fada-4733-a1bd-200c257dc485","trusted":true},"cell_type":"code","source":"melbourne_price_data = melbourne_data.Price\nprint(melbourne_price_data.head())","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"fe81b3a698a40b14bd2309162acb812b3f8a62cd","_cell_guid":"e2e264a2-f2ee-4ea1-8ace-8bef5c5b9a29"},"cell_type":"markdown","source":"## Selecting Multiple Columns\nYou can select multiple columns from a DataFrame by providing a list of column names inside brackets. Remember, each item in that list should be a string (with quotes)."},{"metadata":{"_uuid":"a475eadfa6458a8fbeb3bb42c78c8c8239881914","_cell_guid":"d6448e8e-a328-4bdf-a24f-ebf9c7806fde","trusted":true},"cell_type":"code","source":"column_of_interest = ['Landsize','BuildingArea']\ntwo_column_of_data = melbourne_data[column_of_interest]\ntwo_column_of_data.describe()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"dad0c926f55e81a89f6ef0855516721e7cc00680","_cell_guid":"e06b76b7-d146-4638-9c0b-6961ff0575c1","trusted":true},"cell_type":"code","source":"# Print the train dataset Colummns\ntrain.columns","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"27ebe550f58b0719236072334723a5bffe37d43a","_cell_guid":"78e2dc1c-7cb7-44be-81bf-f49e03e644a1","trusted":true},"cell_type":"code","source":"house_sales_price = train.SalePrice\nhouse_sales_price.head()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"b3a9b2374ef0afd96da2710a15277991f72f965a","_cell_guid":"025dfce3-9082-40b7-b0b9-343ebf9074b4","trusted":true},"cell_type":"code","source":"column_of_interest1 = ['SaleCondition','SaleType']\ntwo_columns_of_data_train = train[column_of_interest1]\ntwo_columns_of_data_train.describe()","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"a2c08229cb64d869916afc5c4793dc4bf861d4c5","_cell_guid":"b0e224d5-d218-454b-a56b-b4f804ae0841","trusted":true},"cell_type":"code","source":"y = train.SalePrice\nmelbourne_predictors = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', \n                        'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nx = train[melbourne_predictors]\nx.head()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"5b97443b85be39f468490df87e6bb7c59367dc3b","_cell_guid":"c55f1e67-3c9d-4f8f-bff6-0f4e42b07031"},"cell_type":"markdown","source":"# Building Your Model\nYou will use the **scikit-learn** library to create your models. When coding, this library is written as sklearn, as you will see in the sample code. Scikit-learn is easily the most popular library for modeling the types of data typically stored in DataFrames.\n\nThe steps to building and using a model are:\n\n**Define:** What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.  \n**Fit: **Capture patterns from provided data. This is the heart of modeling.\nPredict: Just what it sounds like  \n**Evaluate:** Determine how accurate the model's predictions are.  \nHere is the example for defining and fitting the model.  "},{"metadata":{"_uuid":"c2861d8a8a72242aaadf45118660cf5bb0ab78f1","_kg_hide-output":false,"_cell_guid":"daf02334-d12f-44bf-ada0-75a00608b2a7","trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\n# Define model\nmy_model  = DecisionTreeRegressor()\n\n# Fit model\nmy_model.fit(x, y)","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"00fc9845118840f97ef371918b1b4dc552d325cb","_cell_guid":"a4d2daea-78b9-43d5-9bc3-7d71ee1e0e0f","trusted":true},"cell_type":"code","source":"print(\"Making Prediction for following 5 houses:\")\nprint(x.head())\nprint(\"The Predictions are\")\nprint(my_model.predict(x.head()))","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"d3def317f9557b29d90b731ea3374fb56b252e07","_cell_guid":"ee30fdf5-6975-4fc3-9472-853a0d1be57e","trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n\npredicated_home_sale_prices = my_model.predict(x)\nmean_absolute_error(y,predicated_home_sale_prices)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"61924354458816103a6a255909ac9c1a237decdb","_cell_guid":"a58c84a9-3c26-44dc-9ff7-0bf35ee8205c","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_X,val_X, train_y,val_y =  train_test_split(x, y, random_state = 0)\nmy_model1 = DecisionTreeRegressor()\n\nmy_model1.fit(train_X, train_y)\nval_predictions = my_model1.predict(val_X)\nprint(mean_absolute_error(val_y,val_predictions))","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"1e13e605cd1539663ebd9c69014098677f3b32a1","_cell_guid":"d63b5018-d462-45a9-a40f-d587de55f2a7"},"cell_type":"markdown","source":"# Checking Underfitting and Overfitting"},{"metadata":{"_uuid":"f9a5118e771d074356f7d71c3462e821e4ee948f","collapsed":true,"_cell_guid":"5bfbbf92-0ead-4139-9a71-e258d21da388","trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef get_mae(max_leaf_nodes, predictors_train, predictors_val, targ_train, targ_val):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(predictors_train, targ_train)\n    preds_val = model.predict(predictors_val)\n    mae = mean_absolute_error(targ_val, preds_val)\n    return(mae)","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"a93de57e6b57937b0d5f3df99081aef9e014db61","_cell_guid":"d3042ed9-85d5-460a-adbe-0679c28d7cd8","trusted":true},"cell_type":"code","source":"# compare MAE with differing values of max_leaf_nodes\nfor max_leaf_nodes in [5, 50, 500, 5000]:\n    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n    print(\"Max leaf nodes: %d \\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"ae81f402651c143d270ac34769b593ea78c00e7e","_cell_guid":"8d795741-612b-4d59-bc74-de3664f2755f"},"cell_type":"markdown","source":"# Of the options listed, 50 is the optimal number of leaves. Apply the function to your Iowa data to find the best decision tree."},{"metadata":{"_uuid":"f11ce761770983a9df95322f9ff822c3954ee89f","_cell_guid":"0823a6e5-f466-42a5-8e40-e637ce5f8e18"},"cell_type":"markdown","source":"> # **Model Train With Random Forest**"},{"metadata":{"_uuid":"06a047297b0a4dfcd6cc89aad09bbfb35f153f15","_cell_guid":"ba022c16-935d-46ae-84e5-0ede1915368d","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestRegressor(n_estimators=1000)\nforest_model.fit(train_X, train_y)\nmelb_preds = forest_model.predict(val_X)\nprint(\"Mean Square Error is:\",mean_absolute_error(val_y, melb_preds))","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"f1d5027c1b6cf641c406381ee3ffe6624b0c85f7","_cell_guid":"0b989c6a-c823-43e1-a5ac-b8d987e5ee34"},"cell_type":"markdown","source":"> # Reduce the mean square error Model by using Imputation"},{"metadata":{"_uuid":"b88a50f97252914f9fb91cc4aa1a88cd8edab635","_cell_guid":"ed5065e6-a77d-4162-b760-de0c15bc3afd","trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# Load data\nmelb_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\ny_target = melb_data.SalePrice\ny_predictors = melb_data.drop(['SalePrice'], axis=1)\n\n# For the sake of keeping the example simple, we'll use only numeric predictors. \ny_numeric_predictors = y_predictors.select_dtypes(exclude=['object'])","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"4b93675e277bf998227f702528cfe5c8800d5a56","_cell_guid":"2f97b58f-d42c-4468-9543-2c6e54d455c6","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(y_numeric_predictors, \n                                                    y_target,\n                                                    train_size=0.7, \n                                                    test_size=0.3, \n                                                    random_state=0)\n\ndef score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"47cb7772c851a27b4bc1511a8070146ab16af953","_cell_guid":"828105c8-2edd-4216-8ceb-1f7fa87a03d9","trusted":true},"cell_type":"code","source":"cols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_test  = X_test.drop(cols_with_missing, axis=1)\nprint(\"Mean Absolute Error from dropping columns with Missing Values:\")\nprint(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"fcf8e9d4c89bc36166a001fe0e960aed83a0995b","_cell_guid":"2d947c6c-6ac9-4ae8-95e9-ded228cae499","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n\nmy_imputer = Imputer()\nimputed_X_train_plus = X_train.copy()\nimputed_X_test_plus = X_test.copy()\n\ncols_with_missing = (col for col in X_train.columns \n                                 if X_train[col].isnull().any())\nfor col in cols_with_missing:\n    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n\n# Imputation\nmy_imputer = Imputer()\nimputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\nimputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n\nprint(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"c527f1f240fbe77fa84053a0b52e03276306ac8e","_cell_guid":"e134223e-7c39-411b-8588-4bee9e575e34"},"cell_type":"markdown","source":"# Using Categorical Data with One Hot Encoding\n"},{"metadata":{"_uuid":"249977364596558054ecf3b68cddbfa3c5e2f0cd","_cell_guid":"78cecf91-cc29-45f0-82b5-95f0b65affbc","trusted":true},"cell_type":"code","source":"# Reading the dataset\nimport pandas as pd\ntrain_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n\n\n#Drop houses where the target is missing\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n\ntarget = train_data.SalePrice\n\n# Since missing values isn't the focus of this tutorial, we use the simplest\n# possible approach, which drops these columns. \n# For more detail (and a better approach) to missing values, see\n# https://www.kaggle.com/dansbecker/handling-missing-values\ncols_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]                                  \nprint(cols_with_missing)\ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\ncandidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n\n# \"cardinality\" means the number of unique values in a column.\n# We use it as our only way to select categorical columns here. This is convenient, though\n# a little arbitrary.\nlow_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].nunique() < 10 and\n                                candidate_train_predictors[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\nmy_cols = low_cardinality_cols + numeric_cols\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_test_predictors[my_cols]","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"5da8457e9a5cf0de478279dc91548be2d79826ed","_cell_guid":"f3c1745f-510e-4a46-ac9a-65da9ea07160","trusted":true},"cell_type":"code","source":"train_predictors.dtypes.sample(10)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"0ef67d7f555187a289d84490fc15efe584625faa","_cell_guid":"7a06d391-ad39-4ce0-98c4-5922dc61b483","collapsed":true,"trusted":true},"cell_type":"code","source":"one_hot_encoded_training_predictors =  pd.get_dummies(train_predictors)","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"02f5edbaa42bca5f20e23f52be0e4fe5f656351e","_cell_guid":"6664232e-9d3c-481c-ad06-f76310f1f071","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef get_mae(X, y):\n    # multiple by -1 to make positive MAE score instead of neg value returned as sklearn convention\n    return -1 * cross_val_score(RandomForestRegressor(50), \n                                X, y, \n                                scoring = 'neg_mean_absolute_error').mean()\n\npredictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n\nmae_without_categoricals = get_mae(predictors_without_categoricals, target)\n\nmae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n\nprint('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\nprint('Mean Absolute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"d5d6230e61b92f7e0652c4572aa9cde0436c3815","_cell_guid":"f9b0fd72-fe6b-4243-ae42-fd9387026cdc","collapsed":true,"trusted":true},"cell_type":"code","source":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"4ddfb966ed2cfb6c78f5d778b361f08fc8e78042","_cell_guid":"df05238a-7e53-4284-8821-0f294065831b","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}