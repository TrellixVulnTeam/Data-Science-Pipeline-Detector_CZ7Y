{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Feature selection using the Boruta-SHAP package\n\nFeature selection (taken from [Wikipedia](https://en.wikipedia.org/wiki/Feature_selection)):\n\n> *In machine learning and statistics feature selection is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:*\n> *  *simplification of models to make them easier to interpret*\n> *  *shorter training times,*\n> *  *to avoid the curse of dimensionality,*\n> *  *enhanced generalization by reducing overfitting (reduction of variance)* \n\n        \nThe BorutaShap package, as the name suggests, combines the [Boruta feature selection algorithm](https://www.jstatsoft.org/article/view/v036i11) with the [SHAP (SHapley Additive exPlanations) technique](https://christophm.github.io/interpretable-ml-book/shap.html). The Boruta algorithm (named after a god of the forest in Slavic mythology) is tasked with [finding a minimal optimal feature set](https://dl.acm.org/doi/10.5555/1314498.1314519) rather than finding all the features relevant to the target variable. This leads to an unbiased and stable selection of important and non-important attributes.\nThe BorutaShap package was written by Eoghan Keany, who has also written an introductory article [\"Is this the Best Feature Selection Algorithm 'BorutaShap'?\"](https://medium.com/analytics-vidhya/is-this-the-best-feature-selection-algorithm-borutashap-8bc238aa1677) providing an overview of how BorutaShap works.\n\nThis is a simple example script to perform Boruta-SHAP on the kaggle [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) competition data. \n\nFirst we shall install [BorutaShap](https://github.com/Ekeany/Boruta-Shap):"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install BorutaShap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now load in the kaggle House Prices data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas  as pd\n\n#===========================================================================\n# read in the House Prices data\n#===========================================================================\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\n#===========================================================================\n# select some features (These are all 'integer' fields for today).\n#===========================================================================\nfeatures = ['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', \n            'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', \n            '1stFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', \n            'BsmtHalfBath', 'HalfBath', 'BedroomAbvGr',  'Fireplaces', \n            'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n            'EnclosedPorch',  'PoolArea', 'YrSold']\n\n#===========================================================================\n#===========================================================================\nX_train       = train_data[features]\ny_train       = train_data[\"SalePrice\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now for the `BorutaShap`, here using 50 trials. Other options are:\n* `importance_measure`: can be `shap`, `gain` or `permutation`.\n\nThe default `model` is the Random Forest. However other  tree based models can be used instead, such as  `DecisionTreeClassifier`, `RandomForestClassifier`, `XGBClassifier` and `CatBoostClassifier` "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from BorutaShap import BorutaShap\n\n# If no model is selected default is the Random Forest\n# If classification is True it is a classification problem\nFeature_Selector = BorutaShap(importance_measure='shap', classification=False)\n\nFeature_Selector.fit(X=X_train, y=y_train, n_trials=50, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now return a [box-plot](https://en.wikipedia.org/wiki/Box_plot) of the features. The `which_features` parameter can be: `all`, `accepted`, `rejected` and `tentative`. "},{"metadata":{"trusted":true},"cell_type":"code","source":"Feature_Selector.plot(which_features='all', figsize=(16,12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Return a subset of the original data with the selected features\nFeature_Selector.Subset()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These results compare well with those obtained both via [recursive feature elimination (RFE)](https://www.kaggle.com/carlmcbrideellis/recursive-feature-elimination-rfe-example) and via [permutation importance](https://www.kaggle.com/carlmcbrideellis/house-prices-permutation-importance-example), both applied to the very same dataset.\n### Links:\n* [Boruta-Shap](https://github.com/Ekeany/Boruta-Shap) (GitHub)\n* [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap) by Scott Lundberg (GitHub)\n* [Christoph Molnar \"SHAP (SHapley Additive exPlanations)\" in \"Interpretable Machine Learning: A Guide for Making Black Box Models Explainable\"](https://christophm.github.io/interpretable-ml-book/shap.html)\n* Miron B. Kursa, Witold R. Rudnicki \"Feature Selection with the Boruta Package\", Journal of Statistical Software Volume 36, Issue 11 (2010) [doi: 10.18637/jss.v036.i11](https://www.jstatsoft.org/article/view/v036i11) ([pdf](https://www.jstatsoft.org/article/view/v036i11/v36i11.pdf))\n\n**Related notebooks:**\n\n* [Automated feature selection with boruta](https://www.kaggle.com/residentmario/automated-feature-selection-with-boruta) by [Aleksey Bilogur](https://www.kaggle.com/residentmario)\n* [Boruta Beats 'em all-New look at Feature Selection](https://www.kaggle.com/ajaysamp/boruta-beats-em-all-new-look-at-feature-selection) by [Ajay Sampath](https://www.kaggle.com/ajaysamp)\n* [SHAP Values](https://www.kaggle.com/dansbecker/shap-values) by [DanB](https://www.kaggle.com/dansbecker)\n* [Advanced Uses of SHAP values](https://www.kaggle.com/dansbecker/advanced-uses-of-shap-values) by [DanB](https://www.kaggle.com/dansbecker)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}