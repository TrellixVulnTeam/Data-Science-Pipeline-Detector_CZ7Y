{"cells":[{"metadata":{},"cell_type":"markdown","source":"# TabNet: A very simple regression example using the House Prices data\n[**TabNet**](https://arxiv.org/pdf/1908.07442.pdf) brings deep learning to tabular data. TabNet has been developed by researchers at Google Cloud AI and achieves SOTA performance on a number of test cases.\nThis notebook is a simple example of performing a regression using the [pyTorch implementation](https://pypi.org/project/pytorch-tabnet/). \n\n`TabNetRegressor()` has a number of options for the `device_name`: `cpu`, `cuda`, `mkldnn`, `opengl`, `opencl`, `ideep`, `hip`, `msnpu`, and `xla`.\nThe `fit()` has a variety of `eval_metric`: `auc`, `accuracy`, `balanced_accuracy`, `logloss`, `mae`, `mse`, and `rmse`. TabNet can also perform classification using `TabNetClassifier()` as well as perform [multi-task learning](https://en.wikipedia.org/wiki/Multi-task_learning).\n\nWe shall use the [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques) data for this demonstration. In this example I undertake no feature engineering, nor data cleaning, such as the removal of outliers *etc*., and perform  only the most basic imputation simply to account for any missing values.\n\n#### Install TabNet:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"!pip install pytorch-tabnet\nimport pandas as pd\nimport numpy  as np\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom sklearn.model_selection import KFold","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data  = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\nsample     = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\nsolution   = pd.read_csv('../input/house-prices-advanced-regression-solution-file/solution.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#===========================================================================\n# select some features\n#===========================================================================\nfeatures = ['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', \n            'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', \n            '1stFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', \n            'BsmtHalfBath', 'HalfBath', 'BedroomAbvGr',  'Fireplaces', \n            'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n            'EnclosedPorch',  'PoolArea', 'YrSold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X      = train_data[features]\ny      = np.log1p(train_data[\"SalePrice\"])\nX_test = test_data[features]\ny_true = solution[\"SalePrice\"]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We shall impute any missing data with a simple mean value. As to the relative merits of doing this *before* using cross-validation see [Byron C. Jaeger, Nicholas J. Tierney, and Noah R. Simon \"*When to Impute? Imputation before and during cross-validation*\" arXiv:2010.00718](https://arxiv.org/pdf/2010.00718.pdf).\nFor a much better imputation method take a look at the notebook [\"MissForest - The best imputation algorithm\"](https://www.kaggle.com/lmorgan95/missforest-the-best-imputation-algorithm) by [Liam Morgan](https://www.kaggle.com/lmorgan95). It deals with the R implementation, and MissForest can also be used in python via the [missingpy](https://github.com/epsilon-machine/missingpy) package."},{"metadata":{"trusted":true},"cell_type":"code","source":"X      =      X.apply(lambda x: x.fillna(x.mean()),axis=0)\nX_test = X_test.apply(lambda x: x.fillna(x.mean()),axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert the data to [numpy.array](https://numpy.org/doc/stable/reference/generated/numpy.array.html)"},{"metadata":{"trusted":true},"cell_type":"code","source":"X      = X.to_numpy()\ny      = y.to_numpy().reshape(-1, 1)\nX_test = X_test.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"run the TabNet deep neural network, averaging over 5 folds:"},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"kf = KFold(n_splits=5, random_state=42, shuffle=True)\npredictions_array =[]\nCV_score_array    =[]\nfor train_index, test_index in kf.split(X):\n    X_train, X_valid = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    regressor = TabNetRegressor(verbose=0,seed=42)\n    regressor.fit(X_train=X_train, y_train=y_train,\n              eval_set=[(X_valid, y_valid)],\n              patience=300, max_epochs=2000,\n              eval_metric=['rmse'])\n    CV_score_array.append(regressor.best_cost)\n    predictions_array.append(np.expm1(regressor.predict(X_test)))\n\npredictions = np.mean(predictions_array,axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"calculate our average CV score"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The CV score is %.5f\" % np.mean(CV_score_array,axis=0) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now calculate our leaderboard score (See: [\"House Prices: How to work offline\"](https://www.kaggle.com/carlmcbrideellis/house-prices-how-to-work-offline))."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_log_error\nRMSLE = np.sqrt( mean_squared_log_error(y_true, predictions) )\nprint(\"The LB score is %.5f\" % RMSLE )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that our CV score corresponds nicely with our leaderboard score, so we do not seem to be [overfitting or underfitting](https://www.kaggle.com/carlmcbrideellis/overfitting-and-underfitting-the-titanic) by too much.\n\nFinally write out a `submission.csv` file:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample.iloc[:,1:] = predictions\nsample.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Related reading\n* [Sercan O. Arik and Tomas Pfister \"TabNet: Attentive Interpretable Tabular Learning\", arXiv:1908.07442 (2019)](https://arxiv.org/pdf/1908.07442.pdf)\n* [pytorch-tabnet](https://github.com/dreamquark-ai/tabnet) (GitHub)\n* [\"TabNet on AI Platform: High-performance, Explainable Tabular Learning\"](https://cloud.google.com/blog/products/ai-machine-learning/ml-model-tabnet-is-easy-to-use-on-cloud-ai-platform) (Google Cloud)\n* Notebook: [TabNet: A simple binary classification example](https://www.kaggle.com/carlmcbrideellis/tabnet-simple-binary-classification-example) (using the Santander Customer Satisfaction data on kaggle)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}