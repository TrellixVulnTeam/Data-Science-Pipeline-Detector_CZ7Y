{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.ensemble import VotingClassifier\nimport torch\nfrom torch import nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport math\nfrom torch.utils.data import TensorDataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport catboost\nfrom sklearn.metrics import mean_squared_error\nimport scipy\nfrom scipy import stats\nfrom scipy.stats import norm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\ntrain = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', sep=',')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"first of all we should drop the most useless feature. It is a feature \"*Id*\""},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ID = test['Id']\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysis of target"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = 'SalePrice'\nprint(train.loc[:, target].isnull().any()) # all target values is filled\n(mu, sigma) = norm.fit(train['SalePrice']) # get the fitted parameters used by the function\nprint(f'mu = {mu:.2f} and sigma = {sigma:.2f}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train[target]\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(21, 4))\nfig.suptitle(f'Original $\\mu=$ {mu:.2f}, $\\sigma=$ {sigma:.2f}', fontsize=16)\nsns.distplot(y, fit=norm, ax=ax[0], label = 'asdasdasdasd')\nax[0].set_title('SalePrice distribution')\nax[0].set_ylabel('Frequency')\nax[0].legend(labels=['Normal dist', 'Our dist.'])\n\nres = stats.probplot(y, plot=ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Target have massive right tail. It's not seems like normal destribution. We need to change it!"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train[target]\nlog_y = np.log1p(y)\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(21, 4))\nfig.suptitle(f'Log-transformation $\\mu=$ {mu:.2f}, $\\sigma=$ {sigma:.2f}', fontsize=16)\nsns.distplot(log_y, fit=norm, ax=ax[0])\nax[0].set_title('SalePrice distribution')\nax[0].set_ylabel('Frequency')\nax[0].legend(labels=['Normal dist.', 'Our log transformed dist.'])\n\nres = stats.probplot(log_y, plot=ax[1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.corr(np.expm1(log_y))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That is better! :)"},{"metadata":{},"cell_type":"markdown","source":"# Analysis of featutes"},{"metadata":{},"cell_type":"markdown","source":"First of all in this section we should fill empty values in features by data description"},{"metadata":{},"cell_type":"markdown","source":"### Filling empties from data description"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Alley'] = train['Alley'].fillna('No alley access')\ntrain['BsmtQual'] = train['BsmtQual'].fillna('No Basement')\ntrain['BsmtCond'] = train['BsmtCond'].fillna('No Basement')\ntrain['BsmtExposure'] = train['BsmtExposure'].fillna('No Basement')\ntrain['BsmtFinType1'] = train['BsmtFinType1'].fillna('No Basement')\ntrain['BsmtFinType2'] = train['BsmtFinType2'].fillna('No Basement')\ntrain['FireplaceQu'] = train['FireplaceQu'].fillna('No Fireplace')\ntrain['GarageType'] = train['GarageType'].fillna('No Garage')\ntrain['GarageFinish'] = train['GarageFinish'].fillna('No Garage')\ntrain['GarageQual'] = train['GarageQual'].fillna('No Garage')\ntrain['GarageCond'] = train['GarageCond'].fillna('No Garage')\ntrain['PoolQC'] = train['PoolQC'].fillna('No Pool')\ntrain['Fence'] = train['Fence'].fillna('No Fence')\ntrain['MiscFeature'] = train['MiscFeature'].fillna('None')\n\ntest['Alley'] = test['Alley'].fillna('No alley access')\ntest['BsmtQual'] = test['BsmtQual'].fillna('No Basement')\ntest['BsmtCond'] = test['BsmtCond'].fillna('No Basement')\ntest['BsmtExposure'] = test['BsmtExposure'].fillna('No Basement')\ntest['BsmtFinType1'] = test['BsmtFinType1'].fillna('No Basement')\ntest['BsmtFinType2'] = test['BsmtFinType2'].fillna('No Basement')\ntest['FireplaceQu'] = test['FireplaceQu'].fillna('No Fireplace')\ntest['GarageType'] = test['GarageType'].fillna('No Garage')\ntest['GarageFinish'] = test['GarageFinish'].fillna('No Garage')\ntest['GarageQual'] = test['GarageQual'].fillna('No Garage')\ntest['GarageCond'] = test['GarageCond'].fillna('No Garage')\ntest['PoolQC'] = test['PoolQC'].fillna('No Pool')\ntest['Fence'] = test['Fence'].fillna('No Fence')\ntest['MiscFeature'] = test['MiscFeature'].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Little feature engeneering"},{"metadata":{},"cell_type":"markdown","source":"So \"CentralAir\" and \"Street\" is a binary features, so we can encoding ones with 0 and 1. Also we can create a new feature that mean pool presence"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Pool_presence'] = [0 if val=='No Pool' else 1 for val in train['PoolQC']]\ntest['Pool_presence'] = [0 if val=='No Pool' else 1 for val in test['PoolQC']]\ntrain['CentralAir'] = [1 if x == train['CentralAir'].unique()[0] else 0 for x in train.loc[:,'CentralAir'].values]\ntest['CentralAir'] = [1 if x == test['CentralAir'].unique()[0] else 0 for x in test.loc[:,'CentralAir'].values]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save changed train dataset"},{"metadata":{},"cell_type":"markdown","source":"### Feature meaning evaluation by Catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost.eval.catboost_evaluation import *\nfrom catboost.utils import create_cd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_features = list(train.select_dtypes(include='object').columns)\ntrain[cat_features].describe().T.sort_values('unique', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = list(train.columns)\na, b = cols.index('SalePrice'), cols.index('Pool_presence')\ncols[b], cols[a] = cols[a], cols[b]\ntrain = train[cols]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.to_csv('train2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = dict(list(enumerate(train.keys())))\ncat_dict  = {i:label for i,label in enumerate(train.columns) if label in cat_features+['Fence', 'MiscFeature', 'Alley', 'PoolQC']}\ndel feature_names[80]\n# train[cat_dict.values()].dtypes, \\\n# cat_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_cd(\n    label=80,\n    cat_features=cat_dict.keys(),\n    feature_names=feature_names,\n    output_path='train.cd'\n)\n!cat ./train.cd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fold_size = int(round(len(train)/2, -2))\nfolds_count=10\ndescription_file='./train.cd'\ntrain2_file='./train2.csv'\n\n# We can chose best params with grid_search function\nlearn_params={'iterations':505,\n              'task_type' : 'GPU',\n              'random_seed': 2, \n              'learning_rate' : 0.1, \n              'max_depth': 10, \n              'l2_leaf_reg': 9.8, \n              'loss_function': 'RMSE', \n              'max_ctr_complexity' : 2, \n              'logging_level': 'Silent', \n              'boosting_type': 'Plain',}\n\nevaluator = CatboostEvaluation(train2_file,\n                               fold_size,\n                               folds_count,\n                               delimiter=',',\n                               column_description=description_file,\n                               partition_random_seed=2,\n                               has_header=True,\n)\n\n# print(evaluator.get_working_dir())\n\nresult = evaluator.eval_features(learn_config=learn_params,\n                                 eval_metrics=[\"RMSE\"],\n                                 features_to_eval=range(1,80),\n                                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logloss_result = result.get_metric_results(\"RMSE\")\nlogloss_result.get_baseline_comparison()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_features = logloss_result.get_baseline_comparison()[logloss_result.get_baseline_comparison()['Decision']=='GOOD'].index\ngood_features = [pd.read_csv(train2_file).columns[int(str(feature)[-2:])] for i,feature in enumerate(good_features)]\nlen(good_features), good_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We will use only 'good' features"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[good_features+[target]]\ntest = test[good_features]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all in this section we should fill empty values in features by data description"},{"metadata":{},"cell_type":"markdown","source":"### Almost empty features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def empty_features(test, train, threshhold=0, verbose=False):\n    if verbose:\n        print(' column \\t test \\t\\t train\\n', '*'*40)\n    i=1\n    empty_list=[]\n    for col in test.columns:\n        percentage_train = (train[col].isnull().sum()/len(train))*100\n        percentage_test = (test[col].isnull().sum()/len(test))*100\n        if percentage_train and percentage_test:\n            if ((percentage_test>=threshhold)|(percentage_train>=threshhold)):\n                empty_list.append(col)\n                if verbose:\n                    print(i,'{}{}% \\t{}%'.format(col.ljust(15,' '), round(percentage_test, 3), round(percentage_train,3 )))\n                    i+=1\n    return empty_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"empty_features_list=empty_features(test=test, train=train, verbose=True, threshhold=80) # features with more then 80% (threshold) empty values\nempty_features_list","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should drop this features"},{"metadata":{},"cell_type":"markdown","source":"### Fetures with only 1 value"},{"metadata":{"trusted":true},"cell_type":"code","source":"only_1_value = [feature for feature in train.nunique().index if train.nunique()[feature]==1]\nonly_1_value","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation-Matrix with Heatmap"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = train.corr()\n\nsns.set(rc={'figure.figsize':(20,15)})\nax = sns.heatmap(corr_matrix,\n                 annot = True, \n                 annot_kws = {'size': 8}, \n                 fmt = '.1f', \n                 cmap = 'PiYG', \n                 linewidths = 1, \n                )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" # Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.85\ndrop_by_corr = [column for column in upper.columns if any(upper[column] > 0.875)]\nprint(drop_by_corr)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We also should drop this useless feature"},{"metadata":{},"cell_type":"markdown","source":"### Almost constant features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def quasi_constant(df, threshold=0, verbose=True):\n    features_list = []\n    for feature in df:\n        table = df[feature].value_counts() / np.float(len(df))\n        first=1\n        for name in table.index:\n            if table[name]>=threshold:\n                if first:\n                    if verbose:\n                        print(f'for feature \"{feature}\":', 'value\\t\\tscore', sep='\\n')\n                    first=0\n                if verbose:\n                    print(name, table[name], sep='\\t', end='\\n'+'*'*50+'\\n')\n        if not first:\n            features_list.append(feature)\n    return features_list","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"quasi_constant_features = quasi_constant(train, 0.95)\nquasi_constant_features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This features are almost constant. We should drop it too"},{"metadata":{"trusted":true},"cell_type":"code","source":"bad_features = drop_by_corr+only_1_value+quasi_constant_features+empty_features_list\nlen(bad_features), bad_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train.columns), train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_features = [feature for feature in good_features if feature not in bad_features]\nlen(good_features), good_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train[good_features+[target]]\ntest = test[good_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train.columns), len(pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', sep=',').columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We droped almost 40 useless features"},{"metadata":{},"cell_type":"markdown","source":"---\n# Pipeline function making with embedding categorical features, fillna and normalization"},{"metadata":{},"cell_type":"markdown","source":"Trying to use embeddings for encoding categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pipelining_preprocessor(df1, dropcolls=None, target=None, filler=None, type='cat'): # return processed  dataFrame\n    def fill_empty_by(df, filler):\n        df_nums = df.select_dtypes(exclude='object')\n        df_cats = df.select_dtypes(include='object')\n        if filler == 'pop':\n            obj = df_cats.describe().loc['top',:]\n            digits = df_nums.median()\n        elif filler == 'zero':\n            obj = 'No info'\n            digits = 0\n        elif filler == 'out_of_range':\n            obj = 'No info'\n            digits = -9999\n        elif filler == None:\n            return df\n        else:\n            raise ValueError('filler vallues is not allowed [\"zero\", \"pop\", \"out_of_range\", None]')\n        return df_cats.fillna(obj).join(df_nums.fillna(digits))[df.columns]\n    \n    df = df1.copy()\n    if target:\n        y = df[target]\n        df.drop(target, axis=1, inplace=True)\n    if dropcolls:\n        df.drop(dropcolls, axis=1, inplace=True)\n    if type=='cat':\n        output = fill_empty_by(df, filler=filler)\n    elif type=='ohe':\n        output = pd.get_dummies(fill_empty_by(df, filler=filler), drop_first=True, prefix_sep=': ',)\n    else:\n        raise ValueError('type vallues is not allowed [\"cat\", \"ohe\"]')\n    if target:\n        return output,y\n    else:\n        return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{},"cell_type":"markdown","source":"## Gradient boosting with CatBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y = pipelining_preprocessor(train, filler = 'out_of_range', target='SalePrice', type='cat')\ny = np.log1p(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, shuffle=True, random_state=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cats = list(train.select_dtypes(include='object').columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"full_pool = catboost.Pool(X, label = y, cat_features = cats)\ntrain_pool = catboost.Pool(X_train, label = y_train, cat_features = cats)\nval_pool = catboost.Pool(X_val, label = y_val, cat_features = cats)\ntest_pool = catboost.Pool(pipelining_preprocessor(test[X.columns], filler = 'out_of_range', type='cat'), cat_features = cats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_params = {\n#     'l2_leaf_reg' : [7, 8, 9],\n#     'random_strength' : [1,2,3,4],\n#     'learning_rate' : [0.01, 0.05, 0.005],\n#     'max_ctr_complexity': [1, 2, 3],\n#     'grow_policy': ['SymmetricTree', 'Depthwise', 'Lossguide']\n}\n\ncat_model = catboost.CatBoostRegressor(loss_function='RMSE', \n                                       random_seed=2, \n                                       max_depth = 10,\n                                       learning_rate = 0.05,\n                                       random_strength=1,\n                                       max_ctr_complexity=1,\n                                       l2_leaf_reg=8,\n                                       grow_policy = 'Lossguide',\n                                       task_type='GPU',\n                                      )\ngrid_search_results = cat_model.grid_search(cat_params, full_pool, \n                                            partition_random_seed=2, cv = skf, \n                                            search_by_train_test_split=True, \n                                            plot=True)\n\ngrid_search_results['params']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_model = catboost.CatBoostRegressor(loss_function='RMSE', \n                                       random_seed=2, \n                                       max_depth = 10,\n                                       learning_rate = 0.05,\n                                       random_strength=1,\n                                       max_ctr_complexity=1,\n                                       l2_leaf_reg=8,\n                                       grow_policy = 'Lossguide',\n                                       task_type='GPU',\n                                      )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_model.fit(train_pool, eval_set=val_pool)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SHAP"},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shap_values = cat_model.get_feature_importance(train_pool, type='ShapValues')\nshap.summary_plot(shap_values[:,:-1], X_train, plot_type='bar', max_display=30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vals= np.abs(shap_values).mean(0)\nfeature_importance_shap = pd.DataFrame(list(zip(X_train.columns, vals)), columns=['feature','feature_importance_shap'])\nfeature_importance_shap.sort_values(by=['feature_importance_shap'], ascending=False, inplace=True)\nfeature_importance_shap.reset_index(inplace=True, drop=True)\nfeature_importance_shap","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final step"},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_pred = list(map(int, map(np.expm1, cat_model.predict(test_pool))))\ngb_output = pd.DataFrame({'Id': pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')['Id'], \n                          'SalePrice': gb_pred})\n\ngb_output.to_csv('submission_catboost.csv', index=False)\ngb_output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}