{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hello and welcome to my feature importance tutorial!\n\n\n**In this tutorial i will focus on feature importance and feature selection, i will simply show and explain some common feature selection methods and techniques that reveal information about the importance and other properties of features. I will not focus on performance of models or optimizing parameters.**    \n\n**This tutorial will have a lot of comments on why certain things are done in this way and why a different approach could be problematic or not, I definitely recommend doing all the\nkaggle learn courses before reading through this tutorial.**\n\n\n**This notebook is oriented towards this tutorial:**  https://www.kaggle.com/apapiu/regularized-linear-models/notebook\n\n\n**Hopefully it will be helpful for some of you.**\n\n\n**Feel free to comment and ask questions :)**","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Overview\n \n## '1'. Data preparation\n \n1. load data\n1. concat train and test data to one dataframe\n1. log1p transform target 'SalePrice'\n1. determine numeric features \n1. calculate skewness and pick skewed features, then log1p transform them\n1. dummy encode categorical variables of entire dataframe\n1. fill all NaNs with mean of column\n1. split up into X_train, X_test and target y\n\n## '2'. Correlations of features\n\n1. Analyze cross-correlation matrix    \n    \n    \n## '3'. Use coefficients of linear models\n\n1. Ridge model with L2 regularization\n2. Lasso model with L1 regularization\n3. RandomForestRegressor\n    \n    \n## '4'. General feature selection methods\n    \n1. SelectKBest\n2. Permutation importance\n3. Shap values\n4. Recursive feature elimination","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\n\ntrain = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\nprint(train.head(), \"\\n\")\nprint(train.info())","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data preparation\n\nThis tutorial for now  focuses on feature selection and feature importance,  hence we will only do some rudimentary data processing,\nsuch that we can fit our models. Some methods, like the dummy encoding of our target 'SalePrice' and replacing missing values are necessary \nsuch that the code works at all, some methods are used for getting more accurate results. \n\n\n1. Transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal \n2. Dummy encode all the categorical features\n3. Replace the numeric missing values (NaN's) with the mean of their respective columns","metadata":{}},{"cell_type":"code","source":"# we dont want to include our target SalePrice in our data, \n# hence we pick all columns except SalePrice\n\nall_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))   \n\n\n# now we plot the original and the log1p-transformed target SalePrice to see how it looks \n\nprices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(1+price)\":np.log1p(train[\"SalePrice\"])})  \nprices.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see, the log1p transformed SalePrice looks much closer to a normal distribution than before.**","metadata":{}},{"cell_type":"code","source":"#log1p transform the target:\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n# exclude all columns that have 'object' as dtype to get numeric columns \nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n\n# compute skewness of numeric features, but\n# drop NaNs since they cause errors in the skewness computation\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) \n\n\n# Now we want to plot the skewness of all numeric features\n# you can also use sns.barplot(), but i find this version prettier than the sns.barplot\n\nskewed_feats.head(30).plot(kind='barh', figsize=(10,6))    \n\n\n# As you maybe already know, positive skewness can be transformed nicely with a log plot\n# to make the feature 'more' normal distributed.\n# Here we choose a threshold of 0.75 for the skewness, threshold must be exceeded,\n# such that this skewed feature gets log1p transformed.\n\n\nskewed_feats = skewed_feats[skewed_feats > 0.75]\n\nall_data[skewed_feats.index] = np.log1p(all_data[skewed_feats.index])  \n\n#print(all_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  dummy encoding of our entire dataframe, it's necessary for the fitting of models.\nall_data = pd.get_dummies(all_data)\n\n\n#  filling NA's with the mean of the column, removing missing values/NaNs is always recommended:\nall_data = all_data.fillna(all_data.mean())\n\nprint(\"all_data.shape: \", all_data.shape, \"\\n\")\nprint(\"successfully dummy encoded!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  Now we split up our entire dataset into train data, test data and our target y\n\nX_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train.SalePrice\n\ndel all_data\n\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Correlations of features\n\n**As you maybe already know, highly correlated features in your data can cause problems.**\n\n**To analyze the correlations between the features we will create a cross-correlation matrix.**\n\n**But first we need to create a dataframe containing only numeric features.**","metadata":{}},{"cell_type":"code","source":"print(numeric_feats, \"\\n\")\nprint(\"number of numeric features: \", len(numeric_feats), \"\\n\")\n\nnumeric_data = X_train[numeric_feats]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Now that we have all the numeric features in one dataframe we can create the cross-correlation matrix.**","metadata":{}},{"cell_type":"code","source":"corrmat = numeric_data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=1.0, square=True);\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanation of the plot:**\n\n**We can see a square matrix of size 36x36 and each pixel shows the cross-correlation value of feature i and feature j.**\n\n**Hence the main diagonal is always 1, since a feature cross-correlated with itself is always 1. The brighter a pixel is, the more correlated these two features are.**\n\n**What you do now is to look for bright pixels (except for the main diagonal of course) indicating highly correlated features.**\n\n**I can find the following bright pixels:**\n\n* **GarageYrBlt and YearBuilt, this makes sense because when you build the house you also build the garage at the same time**\n* **GarageCars and GarageArea, this makes sense because when you have a bigger GarageArea, you can fit more cars in it**\n* **TotRmsAbvGrd (Total rooms above grade) and GrLivArea, this makes sense, because the more GrLivArea a house has, the more rooms there are**\n\n**I will not remove any of these features, since there are not many highly correlated features.**\n\n**For example if you find huge bright areas in your cross-correlation matrix, you should remove some of these highly correlated features.**\n\n**Or if you find one bright line of pixels, this would indicate that on feature is highly correlated with many other features, and you can then remove this feature.**","metadata":{}},{"cell_type":"markdown","source":"# 3. Use coefficients of linear models\n\n\n**In the following section i will train two different linear models and analyze the coefficients of each model to get the most important features.**\n\n**These two linear models, Ridge and Lasso, are simply fit to the train data, but Ridge uses the L2 regularization while Lasso uses the L1 regularization.**\n\n**After the training process we can simply analyze the 'coefs_' attribute of the trained model, which resembles the coefficients of all features.**\n\n1. Ridge model with L2 regularization\n2. LassoCV model with L1 regularization","metadata":{}},{"cell_type":"markdown","source":"## 3.1 Ridge model with L2 regularization","metadata":{}},{"cell_type":"code","source":"#####################################################################################\n#     3.1.)  Ridge model with L2 regularization\n#####################################################################################\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import Ridge\n\n\nselector = SelectFromModel(estimator = Ridge()).fit(X_train, y)\n\ncoefs = selector.estimator_.coef_\n\ncoefs = pd.Series(coefs, index = X_train.columns)\n\nimportant_coefs = pd.concat([coefs.sort_values().head(15),\n                     coefs.sort_values().tail(15)])\n\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n\n\nimportant_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see we get a nice overview over the features with their corresponding coefficient, the higher the coefficient, the more important this feature is for the used model. The coefficients are distributed in a pleasant way and the values ranging from -0.6 to 0.4 look good as well.**\n\n**We did not specify or optimize any parameters, we simply used the Ridge() model with all default parameters, now let's do the same for the Lasso model.**","metadata":{}},{"cell_type":"code","source":"# the top 10 features of the Ridge model are:\n\n# GrLivArea\n# RoofMatl_WdShngl\n# Neighborhood_StoneBr\n# GarageQual_Ex\n# PoolQC_Ex\n# Condition2_PosA\n# Functional_Typ\n# Neighborhood_Crawfor\n# MSZoning_FV\n# Condition2_Feedr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Lasso model with L1 regularization","metadata":{}},{"cell_type":"markdown","source":"**Now we try the Lasso model:**","metadata":{}},{"cell_type":"code","source":"#####################################################################################\n#     3.2.)  Lasso model with L1 regularization\n#####################################################################################\n\n\nfrom sklearn.linear_model import Lasso\n\nselector = SelectFromModel(estimator = Lasso()).fit(X_train, y)\n\ncoefs = selector.estimator_.coef_\n\ncoefs = pd.Series(coefs, index = X_train.columns)\n\nimportant_coefs = pd.concat([coefs.sort_values().head(15),\n                     coefs.sort_values().tail(15)])\n\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n\n\nimportant_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see only 3 features have a coefficient that is not 0, this is a very unsatisfying result.**\n\n**They main difference between the Ridge and the Lasso model is the different regularization term.** \n\n**I dont wanna go into deep mathematical detail, but simply said the regularization term looks as follows:**\n\n**Lasso L1 regu:             alpha * |weights|**\n\n**Ridge L2 regu:             alpha * |weights|^2**\n\n**So the only difference lies in the squaring of the weights, the parameter alpha (called regularization strength) is the same and the default value is alpha = 1.0 for both models.**\n\n**Increasing the parameter alpha results in smaller coefficients, hence the default value of alpha = 1.0 seems too high for the Lasso model.**\n\n**We will lower alpha to alpha = 0.001 in the Lasso model and let's see if this helped:**","metadata":{}},{"cell_type":"code","source":"selector = SelectFromModel(estimator = Lasso(alpha = 0.001)).fit(X_train, y)\n\ncoefs = selector.estimator_.coef_\n\ncoefs = pd.Series(coefs, index = X_train.columns)\n\nimportant_coefs = pd.concat([coefs.sort_values().head(15),\n                     coefs.sort_values().tail(15)])\n\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n\n\nimportant_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Yes, lowering the parameter alpha was definitely correct, now all the coefficients are distributed in a good way and have reasonable values from -0.2 to 0.35.**\n\n**The problematic thing is that the parameter alpha must be optimized and tweaked in order for the model to predict accurately, but this directly influences the coefficients at the same time. And sadly the order of features changes drastically. If you compare alpha = 0.1, alpha = 0.01, and alpha = 0.001 (not shown in this notebook, but I have checked it) you get entirely different features and coefficients.** \n\n\n**Basically the 'coefs_' attribute gives us the opportunity to look into our fitted model and analyze which features the model \"thinks\" are important, but these coefficients are heavily influenced by parameters like the regularization strength alpha.**","metadata":{}},{"cell_type":"markdown","source":"# 4. General feature selection methods\n\n**So far we have simply used the coefficients of two different linear models to find the most important features, lets try some different methods and compare the results:**\n\n1. **Feature_selection.SelectKBest**\n2. **Permutation importance**\n3. **SHAP values**\n4. **Recursive feature elimination (RFE)**\n\n## 4.1 Feature_selection.SelectKBest","metadata":{}},{"cell_type":"markdown","source":"**In this section we are going to use the Feature_selection.SelectKBest  function of sklearn.**\n\n**SelectKBest takes a certain score_func (score function) and runs univariate statistical tests depending on which score_func we use.**\n\n**We do not need to train any model for this feature selection method, since only statistical calculations are carried out on the data.** \n\n**The common score functions used are:**\n\n* **f_classif : used for classification tasks, uses the ANOVA F value**\n* **f_regression: used for regression tasks, uses the F value**\n* **chi2: used for classification tasks, uses the chi squared test**","metadata":{}},{"cell_type":"code","source":"################################################################################################\n#     4.1.)  feature_selection.SelectKBest    with f_regression\n################################################################################################\n\n#  SelectKBest works with univariate statistical tests, \n#  that calculate how the features relate with the target\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression      \n\nfeature_cols = X_train.columns\n\n\n#  f_regression stands for the ANOVA F-value test and is used for regression tasks\nselector = SelectKBest(score_func = f_regression, k=10)    \nX_new = selector.fit_transform(X_train[feature_cols], y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new),\n                                 index = X_train.index,\n                                 columns=feature_cols)\n\nselected_columns = selected_features.columns[selected_features.var() != 0]  \nprint(\"selected_columns: \", selected_columns, \"\\n\")\n\n\nscores = pd.Series(selector.scores_)\n\nsorted_scores = sorted(scores)[::-1][0:10]\n\nprint(\"sorted_scores: \", sorted_scores, \"\\n\")\n\n#print(selected_columns)\nsns.barplot(sorted_scores, selected_columns, orient = \"h\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is our second top 10 list of important features.**","metadata":{}},{"cell_type":"code","source":"# top10 of SelectKBest, f_regression:          \n\n#'OverallQual', \n#'YearBuilt', \n#'YearRemodAdd', \n#'1stFlrSF', \n#'GrLivArea',\n#'FullBath', \n#'GarageCars', \n#'GarageArea', \n#'ExterQual_TA',\n#'KitchenQual_TA'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is the result for score_func = f_regression,  now we will try  score_func = mutual_info_regression**","metadata":{}},{"cell_type":"code","source":"################################################################################################\n#     4.1.)  feature_selection.SelectKBest  with mutual_info_regression\n################################################################################################\n\n\n#  mutual_info_regression \nselector = SelectKBest(score_func = mutual_info_regression, k=10)    \n\nprint(\"started fitting.....\\n\")\nX_new = selector.fit_transform(X_train[feature_cols], y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new),\n                                 index = X_train.index,\n                                 columns=feature_cols)\n\nselected_columns = selected_features.columns[selected_features.var() != 0]  \nprint(\"selected_columns: \", selected_columns, \"\\n\")\n\n\nscores = pd.Series(selector.scores_)\n\nsorted_scores = sorted(scores)[::-1][0:10]\n\nprint(\"sorted_scores: \", sorted_scores, \"\\n\")\n\n#print(selected_columns)\nsns.barplot(sorted_scores, selected_columns, orient = \"h\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# top10 of SelectKBest, mutual_info_regression:     # top10 of f_regression:      \n\n#'MSSubClass'                                        #'OverallQual', \n#'OverallQual',                                      #'YearBuilt', \n#'YearBuilt',                                        #'YearRemodAdd', \n#'TotalBsmtSF'                                       #'1stFlrSF', \n#'1stFlrSF',                                         #'GrLivArea',\n#'GrLivArea',                                        #'FullBath', \n#'GarageYrBlt',                                      #'GarageCars', \n#'GarageCars',                                       #'GarageArea', \n#'GarageArea',                                       #'ExterQual_TA',\n#'ExterQual_TA',                                     #'KitchenQual_TA'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**When we compare the top10 lists of both score functions f_regression and mutual_info_regression we can see, that they have 7 features in common and in roughly the same rank and order.** \n\n**Since we cannot analyze any performance for these univariate statistical feature selection methods, we have to try them out one by one and compare the results afterwards.**\n\n**For example if 4 methods yield the same result and the 5th method yields completely different results, you can assume that the first 4 methods are better in finding the important features than the 5th method.  But the final and most important test will always be the performance of the model.**","metadata":{}},{"cell_type":"markdown","source":"## 4.2 Permutation importance\n\n**In this section we will use Permutation Importance to find the most important features.**\n\n**Permutation importance is calculated after a model has been fitted by randomly shuffling the values of one single feature and then analyzing how the prediction of the model is affected by this now shuffled data.** \n\n**Simply said: If a feature is unimportant anyway, the prediction of a trained model does not change much when you randomly shuffle the values of an unimportant feature.**\n\n**For more information I can recommend this kaggle course:**  https://www.kaggle.com/learn/machine-learning-explainability","metadata":{}},{"cell_type":"code","source":"################################################################################################\n#   4.2.)  Permutation importance\n################################################################################################\n\n\nfrom IPython.display import display\nimport eli5\nfrom eli5.sklearn import PermutationImportance","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**For Permutation Importance we have to choose a list of features to calculate the permutation importance, calculating it for all 288 features would take too long. Hence we must choose about 15 features.(just for this kaggle notebook here, 15 is not a general number)** \n\n**Let's use the insights we already gained to choose adequate features to analyze their permutation importance.**","metadata":{}},{"cell_type":"code","source":"# important features so far:\n\n# top 10 of Lasso model:         top 10 of selectKBest f_regression:  \n\n# GrLivArea                               #'OverallQual', \n# Neighborhood_StoneBr                    #'YearBuilt', \n# Neighborhood_Crawfor                    #'YearRemodAdd', \n# Neighborhood_NoRidge                    #'1stFlrSF', \n# Functional_Typ                          #'GrLivArea',\n# LotArea                                 #'FullBath',  \n# Neighborhood_NridgHt                    #'GarageCars',   \n# Exterior1st_BrkFace                     #'GarageArea', \n# KitchenQual_Ex                          #'ExterQual_TA',\n# OverallQual                             #'KitchenQual_TA'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Right now i will just pick the top 10 of selectKBest with f_regression and add 5 handpicked features of the Lasso model,\nbeing  'Neighborhood_StoneBr', 'Functional_Typ', 'LotArea', 'Exterior1st_BrkFace', 'KitchenQual_Ex'.**\n\n**I only wanted to choose one of the Neighborhood features, i picked the seemingly most important one,  and then only the other 4 remained.**\n\n**Now we will compute the permutation importance of these features for the Ridge model, then afterwards for the Lasso model.** ","metadata":{}},{"cell_type":"code","source":"################################################################################################\n#   4.2.1)  Permutation importance with Ridge\n################################################################################################\n\n#  which alpha to choose\n\n\nX_train_short = X_train.loc[:, ['OverallQual', 'YearBuilt', 'YearRemodAdd', '1stFlrSF', 'GrLivArea',\n                                'FullBath', 'GarageCars', 'GarageArea', 'ExterQual_TA', 'KitchenQual_TA', \n                                'Neighborhood_StoneBr', 'Functional_Typ', 'LotArea', 'Exterior1st_BrkFace', \n                                'KitchenQual_Ex']]     \n\nprint(\"fitting model...\")\nmy_model = Ridge().fit(X_train_short, y)\n\n#  for performance reasons i will only use n_estimators = 100 in this tutorial,\n#  i have tested it with 500 as well, the results didnt really change, \n#  but everything took much much longer to compute.\n\n\nprint(\"calculating permutation importance...(takes about 1-2 minutes)\")\nperm = PermutationImportance(my_model, random_state=1).fit(X_train_short, y)\n\ndisplay(eli5.show_weights(perm, feature_names = X_train_short.columns.tolist()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ok lets compare the top10 list of permutation importance with the top10 of the coefficients:**","metadata":{}},{"cell_type":"code","source":"# top10 of Ridge coefficients:          top10 of permutation importance:\n\n# GrLivArea                                 # GrLivArea \n# RoofMatl_WdShngl                          # OverallQual\n# Neighborhood_StoneBr                      # YearBuilt\n# GarageQual_Ex                             # LotArea\n# PoolQC_Ex                                 # YearRemodAdd\n# Condition2_PosA                           # 1stFlrSF\n# Functional_Typ                            # GarageCars\n# Neighborhood_Crawfor                      # Functional_Typ\n# MSZoning_FV                               # KitchenQual_Ex\n# Condition2_Feedr                          # Exterior1st_BrkFace","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**As we can see only 3 features appear in both top10 lists:  GrLivArea, Neighborhood_StoneBr and Functional_Typ.**\n\n**Since we cannot know which of these 2 methods yields the \"better\" top10 list of features, or maybe both methods are wrong, we cannot know at this point, we have to gather more information.**\n\n**For example if you have 5 methods for finding important features and 4 methods yield the same features, only the other method yields different results, you can safely assume that the features deemed important by these 4 methods are actually important.**\n\n**The final test will always be the performance of the model though.**\n\n**Let's try the Lasso model next:**","metadata":{}},{"cell_type":"code","source":"################################################################################################\n#   4.2.2)  Permutation importance with Lasso \n################################################################################################\n\n\nX_train_short = X_train.loc[:, ['OverallQual', 'YearBuilt', 'YearRemodAdd', '1stFlrSF', 'GrLivArea',\n                                'FullBath', 'GarageCars', 'GarageArea', 'ExterQual_TA', 'KitchenQual_TA', \n                                'Neighborhood_StoneBr', 'Functional_Typ', 'LotArea', 'Exterior1st_BrkFace', \n                                'KitchenQual_Ex']]     \n\nprint(\"fitting model...\")\nmy_model = Lasso().fit(X_train_short, y)\n\n#  for performance reasons i will only use n_estimators = 100 in this tutorial,\n#  i have tested it with 500 as well, the results didnt really change, \n#  but everything took much much longer to compute.\n\n\nprint(\"calculating permutation importance...(takes about 1-2 minutes)\")\nperm = PermutationImportance(my_model, random_state=1).fit(X_train_short, y)\n\ndisplay(eli5.show_weights(perm, feature_names = X_train_short.columns.tolist()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is the exact same problem we had in Section 2, where the default value of alpha = 1.0 as too high, such that all coefficients were pushed down to 0, except for 3 features: GarageArea, YearBuilt, YearRemodAdd.**\n\n**And exactly these 3 features can be found in this list right here. We will try again with a value of alpha = 0.001, let's see if we get the same features as through the coefficients:**","metadata":{}},{"cell_type":"code","source":"print(\"fitting model...\")\nmy_model = Lasso(alpha = 0.001).fit(X_train_short, y)\n\n#  for performance reasons i will only use n_estimators = 100 in this tutorial,\n#  i have tested it with 500 as well, the results didnt really change, \n#  but everything took much much longer to compute.\n\n\nprint(\"calculating permutation importance...(takes about 1-2 minutes)\")\nperm = PermutationImportance(my_model, random_state=1).fit(X_train_short, y)\n\ndisplay(eli5.show_weights(perm, feature_names = X_train_short.columns.tolist()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Ah well, in this list we can still find some features like GrLivArea, OverallQual and LotArea, but generally this list looks very different from the list of coefficients of the Lasso model with alpha = 0.001.**\n\n**It seems like that in the Lasso model alpha has the same heavy impact on the results for the permutation importance as for the coefficients.**\n\n**Let's try a model we have not looked at before:  The RandomForestRegressor.**","metadata":{}},{"cell_type":"code","source":"################################################################################################\n#   4.2.3)  Permutation importance with RandomForestRegressor\n################################################################################################\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nX_train_short = X_train.loc[:, ['OverallQual', 'YearBuilt', 'YearRemodAdd', '1stFlrSF', 'GrLivArea',\n                                'FullBath', 'GarageCars', 'GarageArea', 'ExterQual_TA', 'KitchenQual_TA', \n                                'Neighborhood_StoneBr', 'Functional_Typ', 'LotArea', 'Exterior1st_BrkFace', \n                                'KitchenQual_Ex']]     \n\nprint(\"fitting model...\")\nmy_model = RandomForestRegressor(n_estimators=100, random_state=1).fit(X_train_short, y)\n\n#  for performance reasons i will only use n_estimators = 100 in this tutorial,\n#  i have tested it with 500 as well, the results didnt really change, \n#  but everything took much much longer to compute.\n\n\nprint(\"calculating permutation importance...(takes about 1-2 minutes)\")\nperm = PermutationImportance(my_model, random_state=1).fit(X_train_short, y)\n\ndisplay(eli5.show_weights(perm, feature_names = X_train_short.columns.tolist()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**That looks good, when we compare this list of the RandomForestRegressor with the list of the Ridge model, we notice that they contain the same features, only the ranks and the order is slightly different.** \n\n**Generally one can say that the permutation importance of both the RandomForestRegressor and the Ridge model yield the same important features.**\n\n**The problem with the permutation importance is that the results are based on a trained model, and if that model was a bad choice or is not well trained, the results of the permutation importance will not reveal any useful or correct information.**","metadata":{}},{"cell_type":"markdown","source":"## 4.3 Shap values","metadata":{}},{"cell_type":"markdown","source":"**I will not explain the Shap values in great detail,  if you are interested in that, i can recommend this link:**\n\nhttps://www.kaggle.com/prashant111/explain-your-model-predictions-with-shapley-values\n\n**Simply said SHAP values measure the impact of having a certain value for a feature in comparison to the prediction we would get if that feature took some baseline value.**\n\n**By doing this we can detect important features.**","metadata":{}},{"cell_type":"code","source":"################################################################################################\n#  4.3.)  SHAP Values\n################################################################################################\n\n\nimport shap # package used to calculate Shap values\n#from sklearn                        import metrics, svm\nfrom sklearn import preprocessing\nfrom sklearn import utils\n\n#  This cell takes about 2 minutes to run\nprint(\"This cell takes about 2 minutes to run. \\n\")\n\n\n#  fit model\nprint(\"fitting model...\")\nmy_model = RandomForestRegressor(n_estimators = 100, random_state=1).fit(X_train, y) \n\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(my_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(X_train)\n\nshap.initjs()\n\ndisplay(shap.force_plot(explainer.expected_value, shap_values[0, :], X_train.iloc[0,:]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**This is the typical shap value plot, i will not go into deep detail here, but we can see the most important features and how they impact the model output.**\n\n**In these plots it's hard to compare the importance of red and blue features, but there is an awesome solution for this problem, such that we can see the most important features more clearly. Behold!**","metadata":{}},{"cell_type":"code","source":"display(shap.summary_plot(shap_values, X_train, plot_type=\"bar\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Wow, isnt that beautiful? Look at this beautiful plot, the color blue, the font of the labels, the ......sorry,  i got carried away a little bit.**\n\n**It takes about 2 minutes computation time, but we could put our entire X_train data into it and\nit gives us the most important features in a very clear and structured plot.**\n\n**Before we compare the results of this method with the results of the other methods, I want to show you one last awesome shap plot:**","metadata":{}},{"cell_type":"code","source":"display(shap.summary_plot(shap_values, X_train))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In this plot you can see all the features, how important they are, and how they impact the prediction.**\n\n**When we compare the important features of the Shap values of the RandomForestRegressor with the other methods, we can again find many features, only the rank and order is changed a little bit.**","metadata":{}},{"cell_type":"markdown","source":"## 4.4 Recursive feature elimination","metadata":{}},{"cell_type":"markdown","source":"**Recursive feature elimination (RFE) works by analyzing the weights of a certain model.**\n\n**By recursively taking smaller and smaller sets of features depending on the weights of the features, it will return the most important N features of that model.**\n\n\n**RFE takes relatively long to compute compared to all other methods we used before.**\n\n**I tried it on my PC which has a better CPU than the kaggle notebooks use,  \nthere it took about 10 minutes for n_features_to_select = 1, \nwhen i fed the entire X_train dataframe into the model.**\n\n\n**I will show you the top 15 features that RFE  estimates important for the RandomForestRegressor, the code I used is below,\nyou can try letting it run here on kaggle, but i cannot tell you how long it will take.**","metadata":{}},{"cell_type":"code","source":"#################################################################\n#  4.4  Recursive  Feature Elimination   (RFE)\n#################################################################\n\n'''\nfrom sklearn.feature_selection import RFE\n\nprint(\"doing  recursive feature elimination...\")\nrfe = RFE(my_model, n_features_to_select=1)\n\n\nprint(\"fitting...\")\nrfe.fit(X_train, y)\n\nfrom operator import itemgetter\n\n# you have to pass the list 'features' with all the features which you trained the model with\nfor x, y in (sorted(zip(rfe.ranking_ , features), key=itemgetter(0))):   \n    print(x, y)     # this line will print the rank x and the name of the feature y\n'''\n\n###########################################################################################################################\n\n#  top 15  of the one run on my PC\n\n#1 OverallQual\n#2 GrLivArea\n#3 TotalBsmtSF\n#4 YearBuilt\n#5 GarageCars\n#6 1stFlrSF\n#7 BsmtFinSF1\n#8 GarageArea\n#9 LotArea\n#10 OverallCond\n#11 YearRemodAdd\n#12 CentralAir_Y\n#13 LotFrontage\n#14 BsmtUnfSF\n#15 2ndFlrSF","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**When we compare this list of RFE with the other methods we have used so far,  we can find many features, some even at the same rank and position.**","metadata":{}},{"cell_type":"markdown","source":"# Thank you for reading my feature selection tutorial!\n\n# Feel free to comment or ask questions :)","metadata":{}}]}