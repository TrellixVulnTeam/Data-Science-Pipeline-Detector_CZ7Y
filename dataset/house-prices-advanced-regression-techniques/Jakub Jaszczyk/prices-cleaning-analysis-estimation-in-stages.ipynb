{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Prices Analysis"},{"metadata":{},"cell_type":"markdown","source":"Thanks for coming to my notebook. I will be very interested in your comments or corrections.\n\nThe notebook is divided into following parts:\n   1. Libraries and data\n   \n      Definition of basic libraries used for this notebook and data downloading. For the data cleaning train and test sets are combined keeping auxiliary variable to differentiate them later.\n      \n   2. Data cleaning\n   \n      The longest stage of the investigation focused on complete cleaning and understanding the data peculiarities. We define numeric and categorical variables, differentiating nominal and ordinal features as subgroups of categorical ones. We add one interaction and correct empty entries. Last we transform nominal variables into dummy ones.\n      \n   3. Data analysis\n   \n      We scale our numeric variables. Then divide the data set into train and test for the next stages. Further, we analyse the target variable moments and resulting distribution. Next, correlation analysis is done and two features' selection methods are used: F statistic and LASSO. At this stage we assumed linearity.\n   \n   4. Estimation\n   \n      Some basic methods are explored: Generalised Linear Model, Random Forest and Extreme Boosting. We figure out which method offers the best prediction at this stage and fine-tune it."},{"metadata":{},"cell_type":"markdown","source":"# **1. Libraries and data**"},{"metadata":{},"cell_type":"markdown","source":"First, we will define basic libraries and data source. For this we will use popular data set which contains house prices data. The set is small (<1Mb), but the quality of data seems to be good so it sounds like attractive toy."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel\nfrom sklearn.linear_model import LogisticRegression,TweedieRegressor\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom scipy.stats import variation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mlxtend.preprocessing import minmax_scaling\nimport math\nfrom xgboost import XGBRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nRandState = 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define train and test data sets. For this we import data set, which is already divided into two parts. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv', index_col='Id')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv', index_col='Id')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we want to apply the same data cleaning and preprocessing, then we will temporarily connect them"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['IsTrain']  = 1\ntest['IsTrain']  = 0\n\nDataRaw = pd.concat([train, test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DataRaw.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 80 columns for test and 81 of them for training. First column is just ID, let's keep this one but of course not use for estimation. The difference in columns' numbers is variable \"SalePrice\" so our target one."},{"metadata":{},"cell_type":"markdown","source":"First, we look at some global measures of our data set. Among them:\n* count - number of non-empty entries\n* mean - average value\n* std - standard deviation\n* min, max - lowest and highest value ifor the variable\n* 25%,50%,75% - next percentiles"},{"metadata":{"trusted":true},"cell_type":"code","source":"DataRaw.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of features not counting the target:\" + str(len(DataRaw.columns) - 1 - 1 )) #First \"-1\" is our target; second \"-1\" is \"IsTrain\" binary factor","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **2. Data cleaning**"},{"metadata":{},"cell_type":"markdown","source":"Define variables' categories. First we will focus on numeric variables, second on categorical ones. The scope of this stage is checking the quality of data, namely what values are missing, but also what values do not make sense (for example: years of construction > 2020)."},{"metadata":{"trusted":true},"cell_type":"code","source":"C = (DataRaw.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\n\nprint(CategoricalVariables)\nprint(\"\")\nprint(\"The number of categorical variables:\" + str(len(CategoricalVariables)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Integer = (DataRaw.dtypes == 'int64') \nFloat   = (DataRaw.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\nprint(NumericVariables)\nprint(\"\")\nprint(\"The number of numeric variables:\" + str(len(NumericVariables)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, we see that we have 43 categorica variables and 36 numeric ones. It makes sense since we checked before that not counting ID, we should find 79 of them."},{"metadata":{},"cell_type":"markdown","source":"Now, we will investigate the quality of the given data. First, let's check how many entries are missing"},{"metadata":{"trusted":true},"cell_type":"code","source":"Missing_Percentage = (DataRaw.isnull().sum()).sum()/np.product(DataRaw.shape)*100\n\nprint(\"The number of missing entries: \" + str(round(Missing_Percentage,2)) + \" %\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"About 6% is quite a good score. Definitely we have couple of useful variables then"},{"metadata":{},"cell_type":"markdown","source":"Let's look at missing values per variable, starting from numeric features as they usually play decisive role in modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"Numeric_NaN = DataRaw[NumericVariables].isnull().sum()\nRowsCount = len(DataRaw.index)\n\nprint(\"The percentage number of missing entries per variable: \", format(round(Numeric_NaN/RowsCount * 100)) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So for all of them the number of missing entries is reasonable. The biggest value we can observe for LotFrontage (17%), what is still acceptable. Anyway, we don't want to leave with these empty entries. For that, we will just apply imputation. It is simple proxy method which automatically fills empty spaces with 'imputed' values, the default value is 'median'. For variable \"GarageYrBlt\" some records have values around 2200, all these futuristic garages which came from future will be placed as median"},{"metadata":{"trusted":true},"cell_type":"code","source":"CleanedNumeric = DataRaw[NumericVariables]\n\nCleanedNumeric['GarageYrBlt']=CleanedNumeric['GarageYrBlt'].fillna(CleanedNumeric['GarageYrBlt'].median())\nCleanedNumeric.GarageYrBlt[CleanedNumeric.GarageYrBlt > 2020]=CleanedNumeric['GarageYrBlt'].median()\nCleanedNumeric['LotFrontage']=CleanedNumeric['LotFrontage'].fillna(CleanedNumeric['LotFrontage'].median())\nCleanedNumeric=CleanedNumeric.fillna(0)\n\nCleanedNumeric.head()\nCleanedNumeric.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will investigate which variables can be useful. Namely, we will investigate their variance, the variables which have whole exposure in just one level can't be very supportive for this analysis. Why? Let's assume that we have a variable \"Apartment Type\" having the data of Tokio center. Almost all the records will go to level 'one-storey flat' and remaining categories like 'detached house', 'mansion' etc. will be empty. "},{"metadata":{},"cell_type":"markdown","source":"In the previous part, we used \"DataRaw.describe()\" to figure out the standard deviation of our variables. It is definitely useful information, but standard deviation's disadvantage is that we need to know the proportion between it and the mean. Namely, it is very hard to say whether for example 'std = 1000' is of big size or not not analysing the particular variable. For this we will use another simple measure - coefficient of variation which takes into account also the volume of variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"CoefVar = pd.DataFrame(variation(CleanedNumeric),index=NumericVariables,columns=['CoefVar']).sort_values(by=['CoefVar'])\n\nCoefVar","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, we look at variables with the lowest coefficient of variation, then starting with: \"YrSold\""},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(a=CleanedNumeric['YrSold'], kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, obviously all dispersion measures are low, cause the difference of 4-6 years in comparison to ~2000 is small. However, of course the variable is useful as we are not interested in last 2000 years, but just 5. We repeat this practive for the next variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"def PlotDist(NameOfVar):\n    sns.distplot(a=CleanedNumeric[NameOfVar], kde=False)   \n    \nsns.distplot(a=CleanedNumeric['PoolArea'], kde=False)\nsns.distplot(a=CleanedNumeric['MiscVal'], kde=False)\nsns.distplot(a=CleanedNumeric['LowQualFinSF'], kde=False)\nsns.distplot(a=CleanedNumeric['3SsnPorch'], kde=False)\nsns.distplot(a=CleanedNumeric['BsmtHalfBath'], kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above five variables were listed starting from \"PoolArea\". These ones have huge coefficient of variance, as we can see on the above graph almost all values remain at 0, then the mean is around 0 what leads to tiny huge value for measure of dispersion. These features with coefficient of variation higher than let's say 2 are highly neglectable. It is important information for us for future choices, for this analysis we will keep them in scope."},{"metadata":{},"cell_type":"markdown","source":"Let's say that these all works should resolve numeric problems. \n****Now, we will look at categorical variables.**** For this the idea will be very similar, so starting from NaN's"},{"metadata":{"trusted":true},"cell_type":"code","source":"Categorical_NaN = DataRaw[CategoricalVariables].isnull().sum()\nRowsCount = len(DataRaw.index)\n\nprint(\"The percentage number of missing entries per variable: \", format(round(Categorical_NaN/RowsCount * 100)) )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The situation is interesting. For some variables multiple entries are lacking. Investigating it a bit, we observe that variables with high number of missing entries correspond only to very luxurious houses. For example: \"Alley\" informs us what type of lane we have in our garden, FireplaceQu determines the fireplace material, MiscFeature informs about another features, according to description these features can be tennis court, elevator (!) etc. All in all, as these variables have significant number of empty entries, we will drop them but use them to create ordinal variable as interaction between them."},{"metadata":{"trusted":true},"cell_type":"code","source":"LuxuriousCategoricalVariables = ['Alley','FireplaceQu','PoolQC','Fence','MiscFeature']\n\nCategoricalVariables = [x for x in CategoricalVariables if x not in LuxuriousCategoricalVariables]\n\nprint(CategoricalVariables)\nprint(LuxuriousCategoricalVariables)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For all categorical variables which don't belong to class 'luxurious', we will apply NA correction by imputing level \"Unknown\""},{"metadata":{"trusted":true},"cell_type":"code","source":"CleanedCategorical= DataRaw[CategoricalVariables].fillna('Unknown')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's define our 'luxurious' interaction. The idea behind is simple, the more features you have from the list of 5 fancy thingies, the higher you are. Namely, something like: you have only fireplace but nothing else, then you receive 1, you have elevator, fireplace and alley in garden, then you receive 3 etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"LuxuriousCategorical = DataRaw[LuxuriousCategoricalVariables]\n\nLuxuriousCategorical = pd.concat([LuxuriousCategorical, pd.DataFrame(DataRaw[LuxuriousCategoricalVariables].isnull().sum(axis = 1),\n                                                                     columns=['Luxurious_Features'])], axis=1,sort=False)\n\n#The function was calculating the number of NaN, hence we inverted it to make more intuitive\nLuxuriousCategorical['Luxurious_Features']=-LuxuriousCategorical['Luxurious_Features']+6 \n\nLuxuriousCategorical.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's now add the new interaction to our main categorical data set. In that way, we create ordinal variable which is simply an interaction"},{"metadata":{"trusted":true},"cell_type":"code","source":"CleanedCategorical = pd.merge(CleanedCategorical,\n                 LuxuriousCategorical['Luxurious_Features'],\n                 on='Id')\n\nCleanedCategorical.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, now this set of categorical variables looks a bit cleaner. Let's do three things now:\n    1. Investigate which variables can be transformed into ordinal ones\n    2. Analyse cardinality of our remaining categorical variables: check simply how many different levels they have. Categorical variables which can't be represented in ordered list will be referred as \"nominal variables\"\n    3. Apply encoding to transform these categorical variables"},{"metadata":{},"cell_type":"markdown","source":"It's good idea to investigate first variables which have \"Qual\" in names cause this shortcut refers to \"Quality\", in other words we expect that some levels will indicate lower quality, while other ones higher one, what can enable us to order them. For this we print unique levels as follows"},{"metadata":{"trusted":true},"cell_type":"code","source":"CleanedCategorical['ExterQual'].unique()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indeed, our guide informs us that:\n1. Ex\tExcellent\n2. Gd\tGood\n3. TA\tAverage/Typical\n4. Fa\tFair\n5. Po\tPoor\n\nAlright, on the basis of this we will make mappings. Honestly, these levels were not very intuitive, at the moment it should be simpler. For me the higher numbers is, the better is, so till the end of this notebook all positive levels will receive high numbers while bad ones low numbers (as below)."},{"metadata":{"trusted":true},"cell_type":"code","source":"Quality_map  = {'NaN':1, 'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5}\n\nCleanedCategorical['ExterQual'] = CleanedCategorical['ExterQual'].map(Quality_map)\nCleanedCategorical['ExterCond'] = CleanedCategorical['ExterCond'].map(Quality_map)\nCleanedCategorical['HeatingQC'] = CleanedCategorical['HeatingQC'].map(Quality_map)\nCleanedCategorical['KitchenQual'] = CleanedCategorical['KitchenQual'].map(Quality_map)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These were quite simple, let's look further. We notice the variable: \"BsmtQual\" what corresponds to basement quality. And here arises the question: is it better to not have basement or have poor basement. For me it's still better to have any basement, even if it's poor. In other words, in further part of this mapping I will make some arbitrary decisions (btw I see that this basement topic is really a thing in this data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"Quality2_map  = {'NaN':1,  'NA':1,'Po':2,'Fa':3,'TA':4,'Gd':5,'Ex':6}\n\nCleanedCategorical['BsmtQual'] = CleanedCategorical['BsmtQual'].map(Quality2_map)\nCleanedCategorical['BsmtCond'] = CleanedCategorical['BsmtCond'].map(Quality2_map)\nCleanedCategorical['GarageQual'] = CleanedCategorical['GarageQual'].map(Quality2_map)\nCleanedCategorical['GarageCond'] = CleanedCategorical['GarageCond'].map(Quality2_map)\n\nQuality3_map  = {'NaN':1, 'NA':1,'No':2,'Mn':3,'Av':4,'Gd':5}\n\nCleanedCategorical['BsmtExposure'] = CleanedCategorical['BsmtExposure'].map(Quality3_map)\n\nQuality4_map  = {'NaN':1, 'NA':1,'Unf':2,'LwQ':3,'Rec':4,'BLQ':5,'ALQ':7,'GLQ':7}\n\nCleanedCategorical['BsmtFinType1'] = CleanedCategorical['BsmtFinType1'].map(Quality4_map)\nCleanedCategorical['BsmtFinType2'] = CleanedCategorical['BsmtFinType2'].map(Quality4_map)\n\nQuality5_map  = {'NaN':1, 'Sal':1,'Sev':2,'Maj2':3,'Maj1':3,'Mod':4,'Min1':5,'Min2':5,'Typ':6}\n\nCleanedCategorical['Functional'] = CleanedCategorical['Functional'].map(Quality5_map)\n\nQuality6_map  = {'NaN':1, 'NA':1,'Unf':2,'RFn':3,'Fin':4}\n\nCleanedCategorical['GarageFinish'] = CleanedCategorical['GarageFinish'].map(Quality6_map)\n\nOrdinalVariables = ['ExterQual','ExterCond','HeatingQC','KitchenQual','BsmtQual','BsmtCond',\n                    'GarageQual','GarageCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n                    'Functional','GarageFinish']\n\nCleanedOrdinal = CleanedCategorical[OrdinalVariables]\n\n#It's also the proper place where we should add our ordered interaction - luxurious interaction\nCleanedOrdinal = pd.merge(CleanedOrdinal,\n                 LuxuriousCategorical['Luxurious_Features'],\n                 on='Id')\nOrdinalVariables = ['ExterQual','ExterCond','HeatingQC','KitchenQual','BsmtQual','BsmtCond',\n                    'GarageQual','GarageCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n                    'Functional','GarageFinish','Luxurious_Features']\n\nCleanedOrdinal= CleanedOrdinal[OrdinalVariables].fillna(1)\n\nCleanedOrdinal.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(CleanedOrdinal['BsmtQual'].loc[[18]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ordering is finished.\n\nNow we will check cardinality."},{"metadata":{"trusted":true},"cell_type":"code","source":"NominalVariables = [x for x in CategoricalVariables if x not in OrdinalVariables]\n\nAllLevelsPerVar = CleanedCategorical[NominalVariables].nunique()\nAllLevels = CleanedCategorical[NominalVariables].nunique().sum()\n\nprint(AllLevelsPerVar)\nprint(\"Number of all levels coming from nominal variables: \" + str(AllLevels))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We want to apply 'hot encoding' so we needed to check the number of levels in our data set. Method 'hot encoding' is used to receive so-called dummies, variables which will correspond to just one level of particular variable. For example having variable \"Apartment Type\", our dummy feature will be equal to 1 only if particular record has certain class. This method has a lot of advantages, for instance you do not loose information. However, on other side it may lead to massive increase of your data set.\n\nIn the case of this data set it is not real danger, cause we have very little number of records and no variables which would have really a lot of levels."},{"metadata":{"trusted":true},"cell_type":"code","source":"CleanedCategoricalDummy = pd.get_dummies(CleanedCategorical[NominalVariables], columns=NominalVariables)\n\nCleanedCategoricalDummy.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Correct, 180 variables present in the data set. Precisely, 180 dummy variables, in other words 180 binary features."},{"metadata":{},"cell_type":"markdown","source":"Let's make big cross-over and combine:\n* Numeric variables (38 variables like at the beginning of the project)\n* Ordinal variables (14 variables: 13 as transformation of 13 categorical ones + 1 interaction as a result of 5 features)\n* Nominal variables (180 variables as dummy transfomration of 25 nominal variables)"},{"metadata":{"trusted":true},"cell_type":"code","source":"CleanedTotal = pd.merge(CleanedNumeric,\n                 CleanedOrdinal,\n                 on='Id')\n\nCleanedTotal = pd.merge(CleanedTotal,\n                 CleanedCategoricalDummy,\n                 on='Id')\n\nCleanedTotal.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, we cleaned the data. There are no missing values, ordinal changes were applied and dummy transformation was done for nominal variables. This is the end of the cleaning stage."},{"metadata":{},"cell_type":"markdown","source":"# 3. Data analysis"},{"metadata":{},"cell_type":"markdown","source":"In this part, we focus on the data analysis. In other words, we will check the dependencies between features, their one- and multi-dimensional behaviour. Furthermore, we will check which variables may be most useful and which ones are the least interesting."},{"metadata":{},"cell_type":"markdown","source":"The important thing working with any data is scaling topic. This modificacation simply transforms numeric values to have 'scaled' values, in other words, it puts them in frame from 0 to 1. If we plotted it, we could see that shape of distribution is not changed, but only numbers on x axis. "},{"metadata":{},"cell_type":"markdown","source":"What variables should be scaled? Only numeric ones, cause nomial features are already binary (in their dummy form), and ordinal features have its own scale which is acceptable."},{"metadata":{"trusted":true},"cell_type":"code","source":"Target = ['IsTrain','SalePrice']\nAllVariables = list(CleanedTotal.columns) \nNumericVariablesNoTarget = [x for x in NumericVariables if x not in Target]\nAllVariablesNoTarget = [x for x in AllVariables if x not in Target]\n\nScaledCleanedTotal = CleanedTotal\nScaledCleanedTotal[NumericVariablesNoTarget] = minmax_scaling(CleanedTotal, columns=NumericVariablesNoTarget)\n\nScaledCleanedTotal.head()\n#print(len(AllVariablesNoTarget)) = 230 = 232 - 2","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks pretty."},{"metadata":{},"cell_type":"markdown","source":"Before we start antyhing, we have to split our data into two parts:\n* training set\n* test set\n\nFor this we will use the same structure as it was defined in basic data."},{"metadata":{"trusted":true},"cell_type":"code","source":"DataTrain=CleanedTotal[ScaledCleanedTotal.IsTrain==1]\nDataTest=CleanedTotal[ScaledCleanedTotal.IsTrain==0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the data analysis will be done only with use of train data. First, let's look at sale price distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(DataTrain['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, let's look at its moments. They will give some information about our target. First moment, mean is raw one and gives information about central tendency so just expected value (EV). Second moment, variance is central one and remains key dispersion measure (how much values differ from EV). Third moment, skewness *informs to what side distribution is skewed* . Fourth one, kurtosis will fullfil skewness bringing some information about tails."},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainTargetMean = DataTrain['SalePrice'].mean()\nTrainTargetVar = DataTrain['SalePrice'].var()\nTrainTargetSkew = DataTrain['SalePrice'].skew()\nTrainTargetKurt = DataTrain['SalePrice'].kurt()\n\nprint(\"Mean: \" + str(round(TrainTargetMean)) + \" with std: \" + str(round(TrainTargetVar**(1/2))) + \", skewness: \"\n      + str(round(TrainTargetSkew,1))+ \", and kurtosis: \"+ str(round(TrainTargetKurt,1)) +\".\"  )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Having some information about the target, we choose 3 interesting numeric variables to analyse whether they are correlated in any way with our target."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 3))\n\nplt.subplot(131)\nplt.scatter('OverallQual', 'SalePrice',  data=DataTrain)\nplt.subplot(132)\nplt.scatter('GrLivArea', 'SalePrice',  data=DataTrain)\nplt.subplot(133)\nplt.scatter('YrSold', 'SalePrice',  data=DataTrain)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We observe that our sale price is linearly correalted with overal quality variable, what makes definitely sense. Similarly regarding the liveable area. What's interesting but not shocking, sale price is not really correlated with sold year. Looking in all this, we are sure that we need more global method to assess the correlation between target and data."},{"metadata":{},"cell_type":"markdown","source":"In this case, let's use most common method, correlation amtrix which presents the linear behaviour strength between factors."},{"metadata":{"trusted":true},"cell_type":"code","source":"CorrelationMatrix = DataTrain.corr()\nfig, axe = plt.subplots(figsize=(15, 10))\nsns.heatmap(CorrelationMatrix, vmax=.9, square=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Even though the graph is huge, this is not really informative. All in all we have 232 variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"VarNo = 15\nTopCorrelatedColumns = CorrelationMatrix.nlargest(VarNo, 'SalePrice')['SalePrice'].index\nReduced = np.corrcoef(DataTrain[TopCorrelatedColumns].values.T)\nfig, axe = plt.subplots(figsize=(15, 10))\n\nsns.heatmap(Reduced, vmax=.9, square=True,yticklabels=TopCorrelatedColumns.values, xticklabels=TopCorrelatedColumns.values, annot=True, annot_kws={'size': 10});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Top 15 strongest linear correlation between our target and features is positive. In other words, all these features growth lead to growth of our target. Does it make sense looking into variables' names? For variables like: 'overall quality', 'garage cars' or 'living area' definitely. Some of them like 'Year Built\" are a bit surprising cause we didn't really see it in our graph. Above matrix will be relevant input for our further analysis."},{"metadata":{},"cell_type":"markdown","source":"We have cleaned and scaled data with defined linear correlation. This is good time for features selection. Variable: \"VarNo\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of features is coming from previous block (correlation matrix)\nselector_F = SelectKBest(f_classif, k=VarNo)\n\n# We do it on train data\nSelected_F = selector_F.fit_transform(DataTrain[AllVariablesNoTarget], DataTrain['SalePrice'])\n\nSelectedOrdered_F = pd.DataFrame(selector_F.inverse_transform(Selected_F), index=DataTrain.index, columns=AllVariablesNoTarget)\n\nSelectedOrdered_F.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that all dropped variables have entries set to 0. Let's focus only on non-zero ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"SelectedVariables_F = list(SelectedOrdered_F.columns[SelectedOrdered_F.var() > 0])\n\n# Get the valid dataset with the selected features.\nDataTrain[SelectedVariables_F].head()\n#print(DataTrain[SelectedVariables].shape) # 15 variables, 1460 records, alright","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so using F statistic we receive these 15 variables as listed above. Reminder: this statistic assumes linearity so the score might underestimate the relation between a feature and the target if the relationship is nonlinear."},{"metadata":{},"cell_type":"markdown","source":"What is the big disadvantage of the above method (besides linearity assumption)? F statistic takes into account only one feature at the moment. Definitely, it doesn't find then the globally best features set. For this we have another methods, traditionally called norms or regulraizations. The great example of very useful application of this mathematical concept is LASSO (L1 regularization) which allows for finding 'optimal' (linear) solution for the set of features. Let's investigate it."},{"metadata":{"trusted":true},"cell_type":"code","source":"L1_par = 0.22 # This parameter is size of penalty (paradoxically, the lower the bigger penalty)\n\n#Define parameters of LASSO\nLogisReg = LogisticRegression(C=L1_par, penalty=\"l1\", solver='liblinear', random_state=RandState).fit(DataTrain[AllVariablesNoTarget], DataTrain['SalePrice'])\n\n#Fir model\nLASSO = SelectFromModel(LogisReg, prefit=True)\n\n#Apply model to the data\nLASSO_transform = LASSO.transform(DataTrain[AllVariablesNoTarget])\n\n#Restrcuture the data\nSelectedOrdered_LASSO = pd.DataFrame(LASSO.inverse_transform(LASSO_transform), index=DataTrain[AllVariablesNoTarget].index,columns=DataTrain[AllVariablesNoTarget].columns)\n\n#Choose relevant columns\nSelectedVariables_LASSO = list(SelectedOrdered_LASSO.columns[SelectedOrdered_LASSO.var() > 0])\n\n#Get the valid dataset with the selected features.\nDataTrain[SelectedVariables_LASSO].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We had some fun with LASSO penalty parameter adjustment. Basically, its level decides how strict the algorithm is regarding features' importance. We decided to put it very low to limit the number of variables. Putting it at level 0.22, we receive only 15 variables (we should mention that the default level is 1, so 0.22 is quite low).\n\nAre LASSO variables the same as for F statistic? Let's see"},{"metadata":{"trusted":true},"cell_type":"code","source":"SelectedVariables = pd.DataFrame(SelectedVariables_F,columns=['F variables']).sort_values(by=['F variables'])\nSelectedVariables['LASSO variables'] = SelectedVariables_LASSO\n\nSelectedVariables","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's really interesting cause only \"ExterQual\" appears in both variables' sets. That's true, LASSO variables were very limited by big penalty, but still the algorithm produced completely different results than F statistic. LASSO is really powerful and we believe that it is superior to univariate method like F statistic. This list has to be more reliable (even assuming linearity). \n\nLook: LASSO thinks that our luxurious interaction is useful, nice."},{"metadata":{},"cell_type":"markdown","source":"# 4. Estimation"},{"metadata":{},"cell_type":"markdown","source":"**Important remark:** This operation will be a bit surprising: we have to split our train data set to differentiate train and test for modeling. The so-called test set from data doesn't contain target so we would be not able to make evaluation on the basis of this. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"Target= DataTrain['SalePrice']\nDataTrainFinal = DataTrain.drop(['SalePrice','IsTrain'],axis=1)\nDataTestFinal = DataTest.drop(['SalePrice','IsTrain'],axis=1)\n\nx_train,x_test,y_train,y_test = train_test_split(DataTrainFinal,Target,test_size=0.2,random_state=0)\n\nprint(\"Train set contains: \" + str(x_train.shape[1]) + \" variables in \" + str(x_train.shape[0]) + \" rows.\")\nprint(\"Test set contains: \" + str(x_test.shape[1]) + \" variables in \" + str(x_test.shape[0]) + \" rows.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First, let's prepare the data set containing predictions of our future models. The average will be the first one as the simplest possible 'model'. We will treat it as a type of benchmark for our prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"ModelAverage = y_train.mean()\nprint(str(round(ModelAverage)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Predictions = pd.DataFrame(y_test,columns=['SalePrice'])\nPredictions['ModelAverage'] = ModelAverage\n\nScoreAverage = math.sqrt(metrics.mean_squared_error(y_test, Predictions['ModelAverage']))\n\nprint('Average: RMSE = ' + str(ScoreAverage))\nPredictions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How we can see: average is very authentic and completely doesn't care, it looks always the same regardless the circumstances."},{"metadata":{},"cell_type":"markdown","source":"Alright, let's fit GLM (Generalised Linear Model). This is really useful model from linear family, which introduces link function to facilitate normality requirement. However, one of disadvantages is that we should suppose what distribution should be used. For this three classic distributions are proposed - first is just Gaussian one."},{"metadata":{"trusted":true},"cell_type":"code","source":"NormalReg = TweedieRegressor(power=0, alpha=0, link='identity')\nPoissonReg = TweedieRegressor(power=1, alpha=0, link='log')\nGammaReg = TweedieRegressor(power=2, alpha=0, link='log')\n\nNormalReg.fit(x_train[SelectedVariables_LASSO],y_train)\nPoissonReg.fit(x_train[SelectedVariables_LASSO],y_train)\nGammaReg.fit(x_train[SelectedVariables_LASSO],y_train)\n\nPredictNormalReg = NormalReg.predict(x_test[SelectedVariables_LASSO])\nPredictPoissonReg = PoissonReg.predict(x_test[SelectedVariables_LASSO])\nPredictGammaReg = GammaReg.predict(x_test[SelectedVariables_LASSO])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Normal Dist: RMSE = ' + str(math.sqrt(metrics.mean_squared_error(y_test, PredictNormalReg))))\nprint('Poisson Dist: RMSE = ' + str(math.sqrt(metrics.mean_squared_error(y_test, PredictPoissonReg))))\nprint('Gamma Dist: RMSE = ' + str(math.sqrt(metrics.mean_squared_error(y_test, PredictGammaReg))))\nprint('Poisson wins')\n\nScoreGLM = math.sqrt(metrics.mean_squared_error(y_test, PredictPoissonReg))\n\nPredictions['GLM Poisson'] = PredictGammaReg\nPredictions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check what random forest can do in this case."},{"metadata":{"trusted":true},"cell_type":"code","source":"RandomForest = RandomForestRegressor(random_state=RandState)\nRandomForest.fit(x_train, y_train)\nPredictRandomForest = RandomForest.predict(x_test)\n\nScoreRandomForest = math.sqrt(metrics.mean_squared_error(y_test, PredictRandomForest))\n\nprint('Random Forest: RMSE = ' + str(ScoreRandomForest))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Predictions['Random Forest'] = PredictRandomForest\nPredictions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, not bad, already great improvement. But we will go further, this can be still boosted. Literally, we can apply extreme boosting for this."},{"metadata":{"trusted":true},"cell_type":"code","source":"XBoost_1 =XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n             importance_type='gain', learning_rate=0.008, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=4000, objective='reg:linear',\n             reg_alpha=0.5, reg_lambda=0.5, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\n\nXBoost_1.fit(x_train, y_train)\n\nPredictXBoost = XBoost_1.predict(x_test)\n\nprint('Extreme boosting for first try: RMSE = ' + str(math.sqrt(metrics.mean_squared_error(y_test, PredictXBoost))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With these parameters, we achieve the RMSE = 26,949. This is not bad, but we can still improve it."},{"metadata":{"trusted":true},"cell_type":"code","source":"XBoost_final =XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n             importance_type='gain', learning_rate=0.0081, max_delta_step=0,\n             max_depth=4, min_child_weight=1.8, n_estimators=4200, objective='reg:linear',\n             reg_alpha=0.6, reg_lambda=0.51, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)\n\nXBoost_final.fit(x_train, y_train)\n\nPredictXBoost_final = XBoost_final.predict(x_test)\n\nScoreXBoost = math.sqrt(metrics.mean_squared_error(y_test, PredictXBoost_final))\n\nprint('Final extreme boosting: RMSE = ' + str(ScoreXBoost))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We improved it just a bit. Let's continue with these results. It is not very surprising that extreme boosting performed better than earlier methods, it is well-known of its great predictive power."},{"metadata":{"trusted":true},"cell_type":"code","source":"Predictions['Extreme boosting'] = PredictXBoost_final\nPredictions.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Comparing used methods**"},{"metadata":{"trusted":true},"cell_type":"code","source":"FinalRMSE = pd.DataFrame([[ScoreAverage],[ScoreGLM],[ScoreRandomForest],[ScoreXBoost]],columns=[\"RMSE\"],index=['Expected value','GLM Poisson','Random Forest','Extreme boosting'])\nFinalRMSE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we will fit the model on all training data we have. At the end, we will make prediction on the whole test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"XBoost_final.fit(DataTrainFinal, Target)\n\nFinalPrediction = XBoost_final.predict(DataTestFinal)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The results have to be prepared with matching order variables (\"Id\")."},{"metadata":{"trusted":true},"cell_type":"code","source":"Submission = pd.DataFrame({'Id': test.index, 'SalePrice': FinalPrediction})\n\nSubmission.to_csv('Submission.csv', index=False)\nSubmission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}