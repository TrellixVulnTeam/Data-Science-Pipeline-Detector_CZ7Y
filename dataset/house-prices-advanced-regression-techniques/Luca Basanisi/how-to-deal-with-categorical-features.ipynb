{"cells":[{"metadata":{"_uuid":"9a986ac8fd00506bb7d6fc5140eef73b84e8ffb5"},"cell_type":"markdown","source":"This kernel is not going to be a high scoring one but, hopefully, will clarify some aspects of categorical variables that can be confusing when approached for the first time.\n\nWe will make experiments together and observe the results. We are going to use only a few categorical features to keep it simple, thus compromising the score since we are not going to use the most relevant features. \n\nTo do so, we are going to use various approaches and see how 3 different models (ordinary least squares, lasso regression, and decision tree) behave.\n\nThe main topics we are going to explore are:\n\n* Categorical vs ordinal features\n* Dummies and Dummy mismatch\n* Multicollinearity\n* Creating categories out of continuous variables\n* Dealing with numerous categories and frequency encoders\n\n***Note***: As everything on the internet, you should not believe to everything you read but, in the spirit of this notebook, it is an excellent idea to run more experiments to prove me wrong. (And please let the feedback coming)\n\n# Data preparation\n\nFollowing [this other kernel of mine](https://www.kaggle.com/lucabasa/an-agile-approach-get-incrementally-better), we can quickly prepare the data by handling missing values and selecting the features we want to focus on. As the linked kernel shows, this is the step that gives the biggest improvement in score (both in cross-validation and on the public LB). However, we won't focus on this step too much since it is not the topic of this kernel."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import KFold, cross_validate, GridSearchCV, train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nimport statsmodels.formula.api as sm\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nimport warnings\n\n# seaborn and scipy versions are not aligned in the docker image\nwarnings.filterwarnings(\"ignore\", message=\"Using a non-tuple sequence for multidimensional indexing is deprecated\")","execution_count":1,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1c602c58d52033d862627cd498c45bd5818fa67","_kg_hide-input":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\n\ndf_train = df_train[df_train.GrLivArea < 4500].copy()  # the documentation says they are outliers\n\ncombine = [df_train, df_test]\ndf_train.name = 'Train'\ndf_test.name = 'Test'\n\nfor df in combine:\n    # LotFrontage\n    df.loc[df.LotFrontage.isnull(), 'LotFrontage'] = 0\n    # Alley\n    df.loc[df.Alley.isnull(), 'Alley'] = \"NoAlley\"\n    # MSSubClass\n    df['MSSubClass'] = df['MSSubClass'].astype(str)\n    # MissingBasement\n    fil = ((df.BsmtQual.isnull()) & (df.BsmtCond.isnull()) & (df.BsmtExposure.isnull()) &\n          (df.BsmtFinType1.isnull()) & (df.BsmtFinType2.isnull()))\n    fil1 = ((df.BsmtQual.notnull()) | (df.BsmtCond.notnull()) | (df.BsmtExposure.notnull()) |\n          (df.BsmtFinType1.notnull()) | (df.BsmtFinType2.notnull()))\n    df.loc[fil1, 'MisBsm'] = 0\n    df.loc[fil, 'MisBsm'] = 1\n    # BsmtQual\n    df.loc[fil, 'BsmtQual'] = \"NoBsmt\" # missing basement\n    # BsmtCond\n    df.loc[fil, 'BsmtCond'] = \"NoBsmt\" # missing basement\n    # BsmtExposure\n    df.loc[fil, 'BsmtExposure'] = \"NoBsmt\" # missing basement\n    # BsmtFinType1\n    df.loc[fil, 'BsmtFinType1'] = \"NoBsmt\" # missing basement\n    # BsmtFinType2\n    df.loc[fil, 'BsmtFinType2'] = \"NoBsmt\" # missing basement\n    # FireplaceQu\n    df.loc[(df.Fireplaces == 0) & (df.FireplaceQu.isnull()), 'FireplaceQu'] = \"NoFire\" # missing\n    # MisGarage\n    fil = ((df.GarageYrBlt.isnull()) & (df.GarageType.isnull()) & (df.GarageFinish.isnull()) &\n          (df.GarageQual.isnull()) & (df.GarageCond.isnull()))\n    fil1 = ((df.GarageYrBlt.notnull()) | (df.GarageType.notnull()) | (df.GarageFinish.notnull()) |\n          (df.GarageQual.notnull()) | (df.GarageCond.notnull()))\n    df.loc[fil1, 'MisGarage'] = 0\n    df.loc[fil, 'MisGarage'] = 1\n    # GarageYrBlt\n    df.loc[df.GarageYrBlt > 2200, 'GarageYrBlt'] = 2007  # correct mistake\n    df.loc[fil, 'GarageYrBlt'] = 0\n    # GarageType\n    df.loc[fil, 'GarageType'] = \"NoGrg\" # missing garage\n    # GarageFinish\n    df.loc[fil, 'GarageFinish'] = \"NoGrg\" # missing\n    # GarageQual\n    df.loc[fil, 'GarageQual'] = \"NoGrg\" # missing\n    # GarageCond\n    df.loc[fil, 'GarageCond'] = \"NoGrg\" # missing\n    # Fence\n    df.loc[df.Fence.isnull(), 'Fence'] = \"NoFence\" # missing fence\n    # Dropping stuff\n    del df['PoolQC']\n    del df['MiscFeature']\n\n# Fixing some entries in test\ndf_test[['BsmtUnfSF', \n         'TotalBsmtSF', \n         'BsmtFinSF1', \n         'BsmtFinSF2']] = df_test[['BsmtUnfSF', \n                                   'TotalBsmtSF', \n                                   'BsmtFinSF1', \n                                   'BsmtFinSF2']].fillna(0) # checked\n\n# eliminating entries in train with missing values\nfor f in df_train.columns:\n    df_train = df_train[pd.notnull(df_train[f])]\n    \n    \ndf_train['target'] = np.log1p(df_train.SalePrice)","execution_count":2,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d12e755b74c3a37762693f5dff730d002aaad65a"},"cell_type":"code","source":"#To find the segment of the missing values, can be useful to impute the missing values\ndef find_segment(df, feat): \n    mis = df[feat].isnull().sum()\n    cols = df.columns\n    seg = []\n    for col in cols:\n        vc = df[df[feat].isnull()][col].value_counts(dropna=False).iloc[0]\n        if (vc == mis): #returns the columns for which the missing entries have only 1 possible value\n            seg.append(col)\n    return seg\n\n# to find the mode of the missing feature, by choosing the right segment to compare (uses find_segment)\ndef find_mode(df, feat): #returns the mode to fill in the missing feat\n    md = df[df[feat].isnull()][find_segment(df, feat)].dropna(axis=1).mode()\n    md = pd.merge(df, md, how='inner')[feat].mode().iloc[0]\n    return md\n\n# identical to the previous one, but with the median\ndef find_median(df, feat): #returns the median to fill in the missing feat\n    md = df[df[feat].isnull()][find_segment(df, feat)].dropna(axis=1).mode()\n    md = pd.merge(df, md, how='inner')[feat].median()\n    return md\n\n# find the mode in a segment defined by the user\ndef similar_mode(df, col, feats): #returns the mode in a segment made by similarity in feats\n    sm = df[df[col].isnull()][feats]\n    md = pd.merge(df, sm, how='inner')[col].mode().iloc[0]\n    return md\n\n# Find the median in a segment defined by the user\ndef similar_median(df, col, feats): #returns the median in a segment made by similarity in feats\n    sm = df[df[col].isnull()][feats]\n    md = pd.merge(df, sm, how='inner')[col].median()\n    return md","execution_count":3,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"0a17c34e9146ef80fcd53d9ee8d1485c1f3268d9"},"cell_type":"code","source":"# Cleaning of test \n\n# MSZoning\nmd = find_mode(df_test, 'MSZoning')\nprint(\"MSZoning {}\".format(md))\ndf_test[['MSZoning']] = df_test[['MSZoning']].fillna(md)\n# Utilities\nmd = 'AllPub'\ndf_test[['Utilities']] = df_test[['Utilities']].fillna(md)\n# MasVnrType\nmd = find_mode(df_test, 'MasVnrType')\nprint(\"MasVnrType {}\".format(md))\ndf_test[['MasVnrType']] = df_test[['MasVnrType']].fillna(md)\n# MasVnrArea\nmd = find_mode(df_test, 'MasVnrArea')\nprint(\"MasVnrArea {}\".format(md))\ndf_test[['MasVnrArea']] = df_test[['MasVnrArea']].fillna(md)\n# BsmtQual\nsimi = ['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nmd = similar_mode(df_test, 'BsmtQual', simi)\nprint(\"BsmtQual {}\".format(md))\ndf_test[['BsmtQual']] = df_test[['BsmtQual']].fillna(md)\n# BsmtCond\nsimi = ['BsmtQual', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nmd = similar_mode(df_test, 'BsmtCond', simi)\nprint(\"BsmtCond {}\".format(md))\ndf_test[['BsmtCond']] = df_test[['BsmtCond']].fillna(md)\n# BsmtCond\nsimi = ['BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2']\nmd = similar_mode(df_test, 'BsmtExposure', simi)\nprint(\"BsmtExposure {}\".format(md))\ndf_test[['BsmtExposure']] = df_test[['BsmtExposure']].fillna(md)\n# BsmtFullBath\nsimi = ['BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2']\nmd = similar_median(df_test, 'BsmtFullBath', simi)\nprint(\"BsmtFullBath {}\".format(md))\ndf_test[['BsmtFullBath']] = df_test[['BsmtFullBath']].fillna(md)\n# BsmtHalfBath\nsimi = ['BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2']\nmd = similar_median(df_test, 'BsmtHalfBath', simi)\nprint(\"BsmtHalfBath {}\".format(md))\ndf_test[['BsmtHalfBath']] = df_test[['BsmtHalfBath']].fillna(md)\n# KitchenQual\nmd = df_test.KitchenQual.mode().iloc[0]\nprint(\"KitchenQual {}\".format(md))\ndf_test[['KitchenQual']] = df_test[['KitchenQual']].fillna(md)\n# Functional\nmd = 'Typ'\ndf_test[['Functional']] = df_test[['Functional']].fillna(md)\n# GarageYrBlt\nsimi = ['GarageType', 'MisGarage']\nmd = similar_median(df_test, 'GarageYrBlt', simi)\nprint(\"GarageYrBlt {}\".format(md))\ndf_test[['GarageYrBlt']] = df_test[['GarageYrBlt']].fillna(md)\n# GarageFinish\nmd = 'Unf'\nprint(\"GarageFinish {}\".format(md))\ndf_test[['GarageFinish']] = df_test[['GarageFinish']].fillna(md)\n# GarageArea\nsimi = ['GarageType', 'MisGarage']\nmd = similar_median(df_test, 'GarageArea', simi)\nprint(\"GarageArea {}\".format(md))\ndf_test[['GarageArea']] = df_test[['GarageArea']].fillna(md)\n# GarageQual\nsimi = ['GarageType', 'MisGarage', 'GarageFinish']\nmd = similar_mode(df_test, 'GarageQual', simi)\nprint(\"GarageQual {}\".format(md))\ndf_test[['GarageQual']] = df_test[['GarageQual']].fillna(md)\n# GarageCond\nsimi = ['GarageType', 'MisGarage', 'GarageFinish']\nmd = similar_mode(df_test, 'GarageCond', simi)\nprint(\"GarageCond {}\".format(md))\ndf_test[['GarageCond']] = df_test[['GarageCond']].fillna(md)\n# GarageCars\nsimi = ['GarageType', 'MisGarage']\nmd = similar_median(df_test, 'GarageCars', simi)\nprint(\"GarageCars {}\".format(md))\ndf_test[['GarageCars']] = df_test[['GarageCars']].fillna(md)\n\ncols = df_test.columns\nmis_test = []\nprint(\"Start printing the missing values...\")\nfor col in cols:\n    mis = df_test[col].isnull().sum()\n    if mis > 0:\n        print(\"{}: {} missing, {}%\".format(col, mis, round(mis/df_test.shape[0] * 100, 3)))\n        mis_test.append(col)\nprint(\"...done printing the missing values\")","execution_count":4,"outputs":[{"output_type":"stream","text":"MSZoning RM\nMasVnrType None\nMasVnrArea 0.0\nBsmtQual TA\nBsmtCond TA\nBsmtExposure No\nBsmtFullBath 0.0\nBsmtHalfBath 0.0\nKitchenQual TA\nGarageYrBlt 1959.0\nGarageFinish Unf\nGarageArea 384.0\nGarageQual TA\nGarageCond TA\nGarageCars 1.0\nStart printing the missing values...\nExterior1st: 1 missing, 0.069%\nExterior2nd: 1 missing, 0.069%\nSaleType: 1 missing, 0.069%\n...done printing the missing values\n","name":"stdout"}]},{"metadata":{"_uuid":"8ae0b73d5cad2c61511d120a7fbe04fd45049625"},"cell_type":"markdown","source":"The data are now reasonably clean and we can start working on our experiments. Despite the fact that, as everyone knows already, the powerful predictors in this problem are not the following features, we will focus on a model that takes into consideration only a few categorical (and ordinal) features. Namely\n\n* GarageQual (ordinal/categorical feature)\n* MSZoning (categorical feature)\n* Alley (categorical feature)\n* LotShape (categorical feature)\n* Foundation (categorical feature)\n* MasVnrType (categorical feature)\n* Heating (categorical feature)\n* KitchenQual (categorical/ordinal feature)\n* ExterQual (categorical/ordinal feature)\n\nThis choice is not mandatory, but these features will provide good examples for our experiments."},{"metadata":{"trusted":true,"_uuid":"41206249bd46574b16a55be5eb58a3f84ef018d1","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"sel_cols = ['MSZoning', 'Alley', 'LotShape', 'Foundation', \n             'Heating', 'GarageQual', 'MasVnrType', 'ExterQual']\n\nfor col in sel_cols:\n    print(col)\n    print(df_train[col].value_counts())\n    print('_'*20)\n    print(df_test[col].value_counts())\n    print('_'*40)\n    print('\\n')","execution_count":5,"outputs":[{"output_type":"stream","text":"MSZoning\nRL         1141\nRM          218\nFV           62\nRH           16\nC (all)      10\nName: MSZoning, dtype: int64\n____________________\nRL         1114\nRM          246\nFV           74\nC (all)      15\nRH           10\nName: MSZoning, dtype: int64\n________________________________________\n\n\nAlley\nNoAlley    1357\nGrvl         50\nPave         40\nName: Alley, dtype: int64\n____________________\nNoAlley    1352\nGrvl         70\nPave         37\nName: Alley, dtype: int64\n________________________________________\n\n\nLotShape\nReg    918\nIR1    479\nIR2     41\nIR3      9\nName: LotShape, dtype: int64\n____________________\nReg    934\nIR1    484\nIR2     35\nIR3      6\nName: LotShape, dtype: int64\n________________________________________\n\n\nFoundation\nPConc     634\nCBlock    634\nBrkTil    146\nSlab       24\nStone       6\nWood        3\nName: Foundation, dtype: int64\n____________________\nPConc     661\nCBlock    601\nBrkTil    165\nSlab       25\nStone       5\nWood        2\nName: Foundation, dtype: int64\n________________________________________\n\n\nHeating\nGasA     1415\nGasW       18\nGrav        7\nWall        4\nOthW        2\nFloor       1\nName: Heating, dtype: int64\n____________________\nGasA    1446\nGasW       9\nWall       2\nGrav       2\nName: Heating, dtype: int64\n________________________________________\n\n\nGarageQual\nTA       1298\nNoGrg      81\nFa         48\nGd         14\nEx          3\nPo          3\nName: GarageQual, dtype: int64\n____________________\nTA       1295\nFa         76\nNoGrg      76\nGd         10\nPo          2\nName: GarageQual, dtype: int64\n________________________________________\n\n\nMasVnrType\nNone       863\nBrkFace    443\nStone      126\nBrkCmn      15\nName: MasVnrType, dtype: int64\n____________________\nNone       894\nBrkFace    434\nStone      121\nBrkCmn      10\nName: MasVnrType, dtype: int64\n________________________________________\n\n\nExterQual\nTA    905\nGd    479\nEx     49\nFa     14\nName: ExterQual, dtype: int64\n____________________\nTA    892\nGd    491\nEx     55\nFa     21\nName: ExterQual, dtype: int64\n________________________________________\n\n\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"032369715549982a7272c8eabae9a76635862791"},"cell_type":"markdown","source":"# Introducing the models and making them work\n\nWe are going to use 3 approaches to fit the data and get a prediction.\n\n* Ordinary least squared regression, very simple linear model, very interpretable.\n* Lasso regression, another linear model but with regularization\n* A decision tree, the simplest tree-based model, easy to explain\n\nNone of them can work with entries like `RL` or `NoAlley` and we thus need to do some processing to help them. \n\nLet's first set up the experiment by creating train and validation sets. Moreover, I'd like to have some function to run the experiments quickly without repeating the same code over and over."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f29ccd27db6e4b81742f766b0c0ecbe6476e8148"},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df_train, df_train.target, test_size=0.20, random_state=42)\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=14)\n\nprint(f\"Original train set shape: \\t {df_train.shape}\")\nprint(f\"New train set shape: \\t {X_train.shape}\")\nprint(f\"Validation set shape: \\t {X_test.shape}\")","execution_count":6,"outputs":[{"output_type":"stream","text":"Original train set shape: \t (1447, 82)\nNew train set shape: \t (1157, 82)\nValidation set shape: \t (290, 82)\n","name":"stdout"}]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"db436e2383e1dfc8dbd90b363a94d8bcb34da2d0"},"cell_type":"code","source":"def OLS_experiment(train, target, validate, val_target):\n    train = train.copy()\n    validate = validate.copy()\n    train['intercept'] = 1 \n    validate['intercept'] = 1\n    \n    regressor_OLS = sm.OLS(endog = target, exog = train).fit()\n    print(regressor_OLS.summary())\n    \n    in_pred = regressor_OLS.predict(train)\n    score = mean_squared_error(y_pred=in_pred, y_true=target)\n    print('\\n')\n    print(f'Score in-sample: \\t {score}')\n    \n    pred = regressor_OLS.predict(validate)\n    score = mean_squared_error(y_pred=pred, y_true=val_target)\n    \n    print('\\n')\n    print(f'Score out of sample: \\t {score}')\n    \n    return pred, in_pred\n\n\ndef get_coef(clsf, ftrs):\n    imp = clsf.coef_.tolist() \n    feats = ftrs\n    result = pd.DataFrame({'feat':feats,'score':imp})\n    result = result.sort_values(by=['score'],ascending=False)\n    return result\n\n\ndef lasso_experiment(train, target, validate, val_target, folds):\n    param_grid = [{'alpha' : [0.0001, 0.0005, 0.00075,\n                                 0.001, 0.005, 0.0075, \n                                 0.01, 0.05, 0.075,\n                                 0.1, 0.5, 0.75, \n                                 1, 5, 7.5]}]\n\n    grid = GridSearchCV(Lasso(), param_grid=param_grid,\n                        cv=folds, scoring='neg_mean_squared_error', \n                        return_train_score=True, n_jobs=-1)\n    grid.fit(train, target)\n    \n    best_lasso = grid.best_estimator_\n    print(best_lasso)\n    print(\"_\"*40)\n    #with its score\n    print(np.sqrt(-grid.best_score_))\n    print(\"_\"*40)\n    \n    print(get_coef(best_lasso, train.columns))\n    \n    pred = best_lasso.predict(validate)\n    score = mean_squared_error(y_pred=pred, y_true=val_target)\n    \n    print(f'Validation score: \\t {score}')\n    \n    return pred\n\n\ndef get_feature_importance(clsf, ftrs):\n    imp = clsf.feature_importances_.tolist() \n    feats = ftrs\n    result = pd.DataFrame({'feat':feats,'score':imp})\n    result = result.sort_values(by=['score'],ascending=False)\n    return result\n\n\ndef tree_experiment(train, target, validate, val_target, folds):\n    param_grid = [{'max_depth': [2, 3, 5, 8, 10, 20], \n                   'max_leaf_nodes': [None, 5, 10, 20]}]\n    \n    grid = GridSearchCV(DecisionTreeRegressor(), param_grid=param_grid,\n                        cv=folds, scoring='neg_mean_squared_error', \n                        return_train_score=True, n_jobs=-1)\n    grid.fit(train, target)\n    \n    best_tree = grid.best_estimator_\n    print(best_tree)\n    print(\"_\"*40)\n    #with its score\n    print(np.sqrt(-grid.best_score_))\n    print(\"_\"*40)\n    \n    print(get_feature_importance(best_tree, train.columns))\n    \n    pred = best_tree.predict(validate)\n    score = mean_squared_error(y_pred=pred, y_true=val_target)\n    \n    print(f'Validation score: \\t {score}')\n    \n    return pred\n\n\ndef plot_predictions(val_target, ols, lasso, tree):\n    line = pd.DataFrame({'x': np.arange(10.5,13.5,0.01), # small hack for a diagonal line\n                         'y': np.arange(10.5,13.5,0.01)})\n    plt.figure(figsize=(10,6))\n    plt.scatter(val_target, ols, label='OLS')\n    plt.scatter(val_target, lasso, label='Lasso')\n    plt.scatter(val_target, tree, label='Tree')\n    plt.plot(line.x, line.y, color='black')\n    plt.xlabel('True value', fontsize=12)\n    plt.ylabel('Prediction', fontsize=12)\n    plt.legend()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2985b585e6e71fd779d44645bb21bff5d41bfe54","_kg_hide-input":true},"cell_type":"code","source":"exp_train = X_train[['OverallQual', 'OverallCond', 'GrLivArea', 'GarageArea']].copy()\nexp_test = X_test[['OverallQual', 'OverallCond', 'GrLivArea', 'GarageArea']].copy()\n\nprint('OLS experiment')\nols_pred, ols_i_pred = OLS_experiment(exp_train, y_train, exp_test, y_test)\nprint('\\n')\nprint('_'*40)\nprint('Lasso experiment')\nlasso_pred = lasso_experiment(exp_train, y_train, exp_test, y_test, kfolds)\nprint('\\n')\nprint('_'*40)\nprint('Tree experiment')\ntree_pred = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)","execution_count":8,"outputs":[{"output_type":"stream","text":"OLS experiment\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.806\nModel:                            OLS   Adj. R-squared:                  0.805\nMethod:                 Least Squares   F-statistic:                     1195.\nDate:                Sun, 09 Jun 2019   Prob (F-statistic):               0.00\nTime:                        22:57:13   Log-Likelihood:                 390.54\nNo. Observations:                1157   AIC:                            -771.1\nDf Residuals:                    1152   BIC:                            -745.8\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nOverallQual     0.1476      0.005     29.732      0.000       0.138       0.157\nOverallCond     0.0248      0.005      5.282      0.000       0.016       0.034\nGrLivArea       0.0003   1.29e-05     19.410      0.000       0.000       0.000\nGarageArea      0.0004   2.94e-05     15.186      0.000       0.000       0.001\nintercept      10.3973      0.037    282.909      0.000      10.325      10.469\n==============================================================================\nOmnibus:                      102.521   Durbin-Watson:                   1.886\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              187.584\nSkew:                          -0.593   Prob(JB):                     1.85e-41\nKurtosis:                       4.577   Cond. No.                     1.22e+04\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.22e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\nScore in-sample: \t 0.029808468829643237\n\n\nScore out of sample: \t 0.03884464559176631\n\n\n________________________________________\nLasso experiment\nLasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)\n________________________________________\n0.17347401296507595\n________________________________________\n          feat     score\n0  OverallQual  0.147489\n1  OverallCond  0.024757\n3   GarageArea  0.000447\n2    GrLivArea  0.000250\nValidation score: \t 0.03884358007606977\n\n\n________________________________________\nTree experiment\nDecisionTreeRegressor(criterion='mse', max_depth=20, max_features=None,\n           max_leaf_nodes=20, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best')\n________________________________________\n0.19806730298046898\n________________________________________\n          feat     score\n0  OverallQual  0.789000\n2    GrLivArea  0.152775\n3   GarageArea  0.051559\n1  OverallCond  0.006667\nValidation score: \t 0.04465101911159043\n","name":"stdout"}]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"93065675b6c7c461e2fc3297cb893d4e6e2f633c"},"cell_type":"code","source":"plot_predictions(y_test, ols_pred, lasso_pred, tree_pred)","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"_uuid":"c9e4498e0f4ca555ee9e4f624d5310eeec4d7a4b"},"cell_type":"markdown","source":"This example is just to show you the expected output of the 3 experiments: scores on training and validation sets, coefficients or features importance. Do not focus too much on the coefficients of OLS and Lasso since we should first normalize the variables to get some insights out of it (`OverallQual` is on a 1-10 scale, `GrLivArea` is on a much bigger scale).\n\nAll that being said, we will now focus on categorical variables and ignore very powerful predictors such as `GrLivArea`.\n\nBefore moving on, notice how the predictions of the DecisionTree are all distributed in *discrete levels*, I would recommend to think about the reason for that and, then, what would happen if we use a RandomForest instead.\n\n# Dummies and encoders\n\nThe standard approach to deal with categorical variables is to create **dummy variables**. To better understand what a dummy variable is, let's take the following data frame."},{"metadata":{"trusted":true,"_uuid":"8d38234ea4dbbe3d6fb53b1032f5a66b9b6c29dc","_kg_hide-input":true},"cell_type":"code","source":"dum_test = df_train[['MasVnrType']].copy()\ndum_test.head(10)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"  MasVnrType\n0    BrkFace\n1       None\n2    BrkFace\n3       None\n4    BrkFace\n5       None\n6      Stone\n7      Stone\n8       None\n9       None","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MasVnrType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BrkFace</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BrkFace</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BrkFace</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Stone</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Stone</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>None</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"f3ac0d432f4cbff3013c66a5355dc735a88c0d82"},"cell_type":"markdown","source":"Here we display only the first 10 lines but it is not hard to see what this dataframe contains"},{"metadata":{"trusted":true,"_uuid":"a7072b2ede02a382a1599959d927aba9eae4091f","_kg_hide-input":true},"cell_type":"code","source":"dum_test.MasVnrType.value_counts(dropna=False)","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"None       863\nBrkFace    443\nStone      126\nBrkCmn      15\nName: MasVnrType, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"bd2a5d59b87df95c61a933caa70f38b8d6c69fcc"},"cell_type":"markdown","source":"We can use a pandas function called `get_dummies()` to, as the function says, get our dummies"},{"metadata":{"trusted":true,"_uuid":"31784c5326800e89c69d234d6ee347141f3bd5a9"},"cell_type":"code","source":"dum_transf = pd.get_dummies(dum_test)\ndum_transf.head()","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"   MasVnrType_BrkCmn        ...         MasVnrType_Stone\n0                  0        ...                        0\n1                  0        ...                        0\n2                  0        ...                        0\n3                  0        ...                        0\n4                  0        ...                        0\n\n[5 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MasVnrType_BrkCmn</th>\n      <th>MasVnrType_BrkFace</th>\n      <th>MasVnrType_None</th>\n      <th>MasVnrType_Stone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"2e7e63410690fa464be38b4ab5206c1fbfd9346a","_kg_hide-input":true},"cell_type":"code","source":"for col in dum_transf.columns:\n    print(col)\n    print(dum_transf[col].value_counts())\n    print('\\n')","execution_count":13,"outputs":[{"output_type":"stream","text":"MasVnrType_BrkCmn\n0    1432\n1      15\nName: MasVnrType_BrkCmn, dtype: int64\n\n\nMasVnrType_BrkFace\n0    1004\n1     443\nName: MasVnrType_BrkFace, dtype: int64\n\n\nMasVnrType_None\n1    863\n0    584\nName: MasVnrType_None, dtype: int64\n\n\nMasVnrType_Stone\n0    1321\n1     126\nName: MasVnrType_Stone, dtype: int64\n\n\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"725b2cfe9d6cfdb68ea78c45796b5bb3b67ec894"},"cell_type":"markdown","source":"In other words, we take all the unique values of the column, we create a new column for each one of them and set the entry to 1 or 0 depending on the values of each observation. This is confirmed by the count of `1`'s in each column that corresponds to the original one.\n\nAnother way of obtaining the same result is to use the sklearn method `OneHotEncoder`"},{"metadata":{"trusted":true,"_uuid":"52e9d35eae5cf17226d17a62e76f488f42bec678"},"cell_type":"code","source":"encoder = OneHotEncoder()\ndum_transf = encoder.fit_transform(dum_test)\npd.DataFrame(dum_transf.todense(), columns=encoder.get_feature_names()).head(10)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"   x0_BrkCmn  x0_BrkFace  x0_None  x0_Stone\n0        0.0         1.0      0.0       0.0\n1        0.0         0.0      1.0       0.0\n2        0.0         1.0      0.0       0.0\n3        0.0         0.0      1.0       0.0\n4        0.0         1.0      0.0       0.0\n5        0.0         0.0      1.0       0.0\n6        0.0         0.0      0.0       1.0\n7        0.0         0.0      0.0       1.0\n8        0.0         0.0      1.0       0.0\n9        0.0         0.0      1.0       0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x0_BrkCmn</th>\n      <th>x0_BrkFace</th>\n      <th>x0_None</th>\n      <th>x0_Stone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"012817c03be63999aa5bf35ba0e2c4c48317b650"},"cell_type":"markdown","source":"With 2 more lines of code (we have to fit the encoder, use it transform the data, convert the sparse matrix it returns into a matrix, making a data frame out of it) we get to the same result. I personally prefer the first approach, but OneHotEncoder can be used inside a Pipeline very easily and it can be very handy in some situations. \n\nFor example, your model has to first impute the missing values, then create the dummies, then fit and predict. You are not even sure what kind of dummies are going to be there before the imputation. OneHotEncoder solves the problem and saves you a headache.\n\n## OLS experiment, multicollinearity\n\nLet's assume we want to predict the Sale Price only with from `MasVnrType`, `Alley`, and `LotShape`. Then we have to prepare the data as"},{"metadata":{"trusted":true,"_uuid":"cea3be35d3b7183675a62d30aaa597a0da4406dc","_kg_hide-input":true},"cell_type":"code","source":"exp_train = X_train[['MasVnrType', 'Alley', 'LotShape']].copy()\nexp_test = X_test[['MasVnrType', 'Alley', 'LotShape']].copy()\n\nexp_train = pd.get_dummies(exp_train)\nexp_test = pd.get_dummies(exp_test)\n\nexp_train.head()","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"      MasVnrType_BrkCmn      ...       LotShape_Reg\n480                   0      ...                  0\n1445                  0      ...                  1\n382                   0      ...                  0\n1100                  0      ...                  1\n1136                  0      ...                  1\n\n[5 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MasVnrType_BrkCmn</th>\n      <th>MasVnrType_BrkFace</th>\n      <th>MasVnrType_None</th>\n      <th>MasVnrType_Stone</th>\n      <th>Alley_Grvl</th>\n      <th>Alley_NoAlley</th>\n      <th>Alley_Pave</th>\n      <th>LotShape_IR1</th>\n      <th>LotShape_IR2</th>\n      <th>LotShape_IR3</th>\n      <th>LotShape_Reg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>480</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1445</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1100</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1136</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"6ca7df6bf531a396bc4769b77db597bc0082f48c","_kg_hide-input":true},"cell_type":"code","source":"res_ols, res_ols_i = OLS_experiment(exp_train, y_train, exp_test, y_test)","execution_count":16,"outputs":[{"output_type":"stream","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.241\nModel:                            OLS   Adj. R-squared:                  0.235\nMethod:                 Least Squares   F-statistic:                     45.47\nDate:                Sun, 09 Jun 2019   Prob (F-statistic):           1.09e-63\nTime:                        22:57:18   Log-Likelihood:                -398.34\nNo. Observations:                1157   AIC:                             814.7\nDf Residuals:                    1148   BIC:                             860.2\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nMasVnrType_BrkCmn   7.028e+11   1.01e+12      0.694      0.488   -1.28e+12    2.69e+12\nMasVnrType_BrkFace  7.028e+11   1.01e+12      0.694      0.488   -1.28e+12    2.69e+12\nMasVnrType_None     7.028e+11   1.01e+12      0.694      0.488   -1.28e+12    2.69e+12\nMasVnrType_Stone    7.028e+11   1.01e+12      0.694      0.488   -1.28e+12    2.69e+12\nAlley_Grvl          9.641e+11   1.39e+12      0.694      0.488   -1.76e+12    3.69e+12\nAlley_NoAlley       9.641e+11   1.39e+12      0.694      0.488   -1.76e+12    3.69e+12\nAlley_Pave          9.641e+11   1.39e+12      0.694      0.488   -1.76e+12    3.69e+12\nLotShape_IR1         8.56e+11   1.23e+12      0.694      0.488   -1.56e+12    3.28e+12\nLotShape_IR2         8.56e+11   1.23e+12      0.694      0.488   -1.56e+12    3.28e+12\nLotShape_IR3         8.56e+11   1.23e+12      0.694      0.488   -1.56e+12    3.28e+12\nLotShape_Reg         8.56e+11   1.23e+12      0.694      0.488   -1.56e+12    3.28e+12\nintercept          -2.523e+12   3.64e+12     -0.694      0.488   -9.66e+12    4.61e+12\n==============================================================================\nOmnibus:                       15.685   Durbin-Watson:                   1.847\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               26.368\nSkew:                           0.012   Prob(JB):                     1.88e-06\nKurtosis:                       3.739   Cond. No.                     3.85e+15\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 2.24e-28. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\nScore in-sample: \t 0.11656485911279985\n\n\nScore out of sample: \t 0.13922363557693382\n","name":"stdout"}]},{"metadata":{"_uuid":"eb830eca0dbc0ba53f9922657c6e20a46ad7ebdb"},"cell_type":"markdown","source":"We can see that the validation result is not that bad but the model is clearly doing something weird: all the dummies are getting the same coefficient. Moreover, their standard deviation is way too high to trust that coefficient. This means that **a small change in the data could lead to a big change of the coefficients**.\n\nHow can we interpret that?\n\nThe model is not able to distinguish the effect of the individual dummies, an effect of what is called [**multicollinearity**](https://en.wikipedia.org/wiki/Multicollinearity). This happens when two or more variables are linearly related and, in this case, it is happening among every group of dummies.\n\n### Avoiding multicollinearity\n\nTo avoid that, we can simply drop one of the dummies. The same effect can be obtained by not fitting the intercept as well. However, this would mean changing the function for the experiment (and we have no time for that) and also not seeing that pandas provide a very easy way to drop one dummy."},{"metadata":{"trusted":true,"_uuid":"c645a7bda7e8943779ca2bb59bada8c4b94483c8"},"cell_type":"code","source":"exp_train = X_train[['MasVnrType', 'Alley', 'LotShape']].copy()\nexp_test = X_test[['MasVnrType', 'Alley', 'LotShape']].copy()\n\nexp_train = pd.get_dummies(exp_train, drop_first=True)\nexp_test = pd.get_dummies(exp_test, drop_first=True)\n\nexp_train.head()","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"      MasVnrType_BrkFace      ...       LotShape_Reg\n480                    1      ...                  0\n1445                   0      ...                  1\n382                    0      ...                  0\n1100                   0      ...                  1\n1136                   0      ...                  1\n\n[5 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MasVnrType_BrkFace</th>\n      <th>MasVnrType_None</th>\n      <th>MasVnrType_Stone</th>\n      <th>Alley_NoAlley</th>\n      <th>Alley_Pave</th>\n      <th>LotShape_IR2</th>\n      <th>LotShape_IR3</th>\n      <th>LotShape_Reg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>480</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1445</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1100</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1136</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"d5c05deb1518339f2745bd26cdad510827a489d3","_kg_hide-input":true},"cell_type":"code","source":"res_ols_o, res_ols_i = OLS_experiment(exp_train, y_train, exp_test, y_test)","execution_count":18,"outputs":[{"output_type":"stream","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.283\nModel:                            OLS   Adj. R-squared:                  0.278\nMethod:                 Least Squares   F-statistic:                     56.67\nDate:                Sun, 09 Jun 2019   Prob (F-statistic):           7.90e-78\nTime:                        22:57:18   Log-Likelihood:                -365.04\nNo. Observations:                1157   AIC:                             748.1\nDf Residuals:                    1148   BIC:                             793.6\nDf Model:                           8                                         \nCovariance Type:            nonrobust                                         \n======================================================================================\n                         coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------------------\nMasVnrType_BrkFace     0.3829      0.102      3.753      0.000       0.183       0.583\nMasVnrType_None        0.1366      0.101      1.347      0.178      -0.062       0.336\nMasVnrType_Stone       0.6310      0.106      5.981      0.000       0.424       0.838\nAlley_NoAlley          0.2261      0.054      4.198      0.000       0.120       0.332\nAlley_Pave             0.3043      0.079      3.861      0.000       0.150       0.459\nLotShape_IR2           0.2255      0.061      3.721      0.000       0.107       0.344\nLotShape_IR3           0.2980      0.138      2.167      0.030       0.028       0.568\nLotShape_Reg          -0.1743      0.021     -8.239      0.000      -0.216      -0.133\nintercept             11.6573      0.115    101.364      0.000      11.432      11.883\n==============================================================================\nOmnibus:                       15.725   Durbin-Watson:                   1.956\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               26.416\nSkew:                           0.017   Prob(JB):                     1.84e-06\nKurtosis:                       3.739   Cond. No.                         34.8\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nScore in-sample: \t 0.11004455398344794\n\n\nScore out of sample: \t 0.13895710433363168\n","name":"stdout"}]},{"metadata":{"_uuid":"844ab28d892de0f11b5066193718d3f31f8172a8"},"cell_type":"markdown","source":"Now we are able to see the effects of each variable and any evaluation of the model improved (R-squared, AIC, BIC, etc). The scores, both in sample and out of sample, did not improve that much but we are talking about very simple models, we can't expect too much out of them."},{"metadata":{"_uuid":"4ed8d4d9dcdb7aa4c50a68b87e39f71951edb8c9"},"cell_type":"markdown","source":"## Lasso experiment, regularization\n\nLasso is a method that uses variable selection and regularization. This makes the previous issue with multicollinearity more under control as we can see from the next 2 experiments."},{"metadata":{"trusted":true,"_uuid":"d5e5f24040de9fa4b0ea1db367570996a2d05d11","_kg_hide-input":true},"cell_type":"code","source":"exp_train = X_train[['MasVnrType', 'Alley', 'LotShape']].copy()\nexp_test = X_test[['MasVnrType', 'Alley', 'LotShape']].copy()\n\nexp_train = pd.get_dummies(exp_train)\nexp_test = pd.get_dummies(exp_test)\n\nres_lasso = lasso_experiment(exp_train, y_train, exp_test, y_test, kfolds)\n\nprint('\\n')\nprint('_'*40)\nprint('Now, let\\'s drop one dummy')\nprint('\\n')\n\nexp_train = X_train[['MasVnrType', 'Alley', 'LotShape']].copy()\nexp_test = X_test[['MasVnrType', 'Alley', 'LotShape']].copy()\n\nexp_train = pd.get_dummies(exp_train, drop_first=True)\nexp_test = pd.get_dummies(exp_test, drop_first=True)\n\nres_lasso = lasso_experiment(exp_train, y_train, exp_test, y_test, kfolds)","execution_count":19,"outputs":[{"output_type":"stream","text":"Lasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)\n________________________________________\n0.33400733010139644\n________________________________________\n                  feat     score\n3     MasVnrType_Stone  0.339704\n9         LotShape_IR3  0.277655\n8         LotShape_IR2  0.221499\n1   MasVnrType_BrkFace  0.092237\n6           Alley_Pave  0.074727\n5        Alley_NoAlley -0.000000\n7         LotShape_IR1 -0.000000\n2      MasVnrType_None -0.153377\n10        LotShape_Reg -0.174480\n4           Alley_Grvl -0.223098\n0    MasVnrType_BrkCmn -0.279572\nValidation score: \t 0.13879188101720352\n\n\n________________________________________\nNow, let's drop one dummy\n\n\nLasso(alpha=0.0001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)\n________________________________________\n0.33402560079025945\n________________________________________\n                 feat     score\n2    MasVnrType_Stone  0.598003\n0  MasVnrType_BrkFace  0.350511\n4          Alley_Pave  0.294930\n6        LotShape_IR3  0.277654\n5        LotShape_IR2  0.221224\n3       Alley_NoAlley  0.219778\n1     MasVnrType_None  0.104388\n7        LotShape_Reg -0.174455\nValidation score: \t 0.13867654673932073\n","name":"stdout"}]},{"metadata":{"_uuid":"a27c899bb8a1a4d60b210cf59383b2cecb84ba15"},"cell_type":"markdown","source":"We see that dropping one dummy changes the coefficients and, marginally, the performance, but the effect is not as drastic as before. The feature selection and the regularization were already taking care of part of the problem (notice how `LotShape_IR1` had already a very small coefficient).\n\n## Tree experiment\n\nAboud DecisionTrees and more complicated models (RandomForest, XGBoost) I wrote more extensively in this other kernel: [Fantastic Trees and How to tune them](https://www.kaggle.com/lucabasa/fantastic-trees-and-how-to-tune-them). In this kernel, we are definitely not going for performance and, as you can see from the functions above, we don't try too hard to make our tree perform well."},{"metadata":{"trusted":true,"_uuid":"20204118449be66c7d811abb5b76c6845f671108","_kg_hide-input":true},"cell_type":"code","source":"exp_train = X_train[['MasVnrType', 'Alley', 'LotShape']].copy()\nexp_test = X_test[['MasVnrType', 'Alley', 'LotShape']].copy()\n\nexp_train = pd.get_dummies(exp_train)\nexp_test = pd.get_dummies(exp_test)\n\nres_tree = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)\n\nprint('\\n')\nprint('_'*40)\nprint('Now, let\\'s drop one dummy')\nprint('\\n')\n\nexp_train = X_train[['MasVnrType', 'Alley', 'LotShape']].copy()\nexp_test = X_test[['MasVnrType', 'Alley', 'LotShape']].copy()\n\nexp_train = pd.get_dummies(exp_train, drop_first=True)\nexp_test = pd.get_dummies(exp_test, drop_first=True)\n\nres_tree = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)","execution_count":20,"outputs":[{"output_type":"stream","text":"DecisionTreeRegressor(criterion='mse', max_depth=5, max_features=None,\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best')\n________________________________________\n0.3351024277337352\n________________________________________\n                  feat     score\n2      MasVnrType_None  0.516690\n10        LotShape_Reg  0.210111\n3     MasVnrType_Stone  0.121652\n4           Alley_Grvl  0.046161\n1   MasVnrType_BrkFace  0.032684\n8         LotShape_IR2  0.025686\n7         LotShape_IR1  0.024243\n6           Alley_Pave  0.012690\n5        Alley_NoAlley  0.007727\n9         LotShape_IR3  0.002354\n0    MasVnrType_BrkCmn  0.000000\nValidation score: \t 0.13972665599342327\n\n\n________________________________________\nNow, let's drop one dummy\n\n\nDecisionTreeRegressor(criterion='mse', max_depth=8, max_features=None,\n           max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best')\n________________________________________\n0.335173055860714\n________________________________________\n                 feat     score\n1     MasVnrType_None  0.516064\n7        LotShape_Reg  0.209857\n2    MasVnrType_Stone  0.121505\n3       Alley_NoAlley  0.064157\n0  MasVnrType_BrkFace  0.032645\n5        LotShape_IR2  0.032017\n4          Alley_Pave  0.016982\n6        LotShape_IR3  0.006772\nValidation score: \t 0.13976924694516724\n","name":"stdout"}]},{"metadata":{"_uuid":"063ae5a55f8d243be2f56ba55d3d4178e35d4b33"},"cell_type":"markdown","source":"As in the previous case, our tree is not showing improvements if we drop a dummy.\n\nWe can finally see how the predictions of the 3 experiments on the validation set are."},{"metadata":{"trusted":true,"_uuid":"f709c805d019539e527e96610604866a8f6ec3d3","_kg_hide-input":true},"cell_type":"code","source":"plot_predictions(y_test, res_ols_o, res_lasso, res_tree)","execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"18db9bf1861d678f2dac4e91eef3361134cd3ccc"},"cell_type":"markdown","source":"As we see, the 3 models are very much in agreement on getting the predictions wrong and, since we are using only discrete values while before there were continuous features, the predictions tend to be on more definite *levels*.\n\n## The wrong way of handling dummies\n\nSometimes it is tempting to simply encode the unique values into numerical one. One way to do so is to use sklearn's method `LabelEncoder` and we see how it works in the following cell (I keep the original column for clarity)"},{"metadata":{"trusted":true,"_uuid":"7967a15d8e79e3bee939c53a310119f15a74fa6f"},"cell_type":"code","source":"le = LabelEncoder()\ndum_test['encoded'] = le.fit_transform(dum_test.MasVnrType)\ndum_test.head(10)","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"  MasVnrType  encoded\n0    BrkFace        1\n1       None        2\n2    BrkFace        1\n3       None        2\n4    BrkFace        1\n5       None        2\n6      Stone        3\n7      Stone        3\n8       None        2\n9       None        2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MasVnrType</th>\n      <th>encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BrkFace</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>None</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BrkFace</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>None</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BrkFace</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>None</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Stone</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Stone</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>None</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>None</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"db8ba4f091c1dc048c25b49bd30de2dd8e85cf12"},"cell_type":"markdown","source":"The error here is that our encoding **forced an order among the categories**. It is saying to our models that `BrkFace < None < Stone` and this can be very harmful to both the performance and the explainability of the model because that ordering does not make any sense. "},{"metadata":{"trusted":true,"_uuid":"b8e356d9970499ff3d8c5a175fd0b6c2a9fbae54"},"cell_type":"code","source":"exp_train = X_train[['MasVnrType', 'Alley', 'LotShape']].copy()\nexp_test = X_test[['MasVnrType', 'Alley', 'LotShape']].copy()\n\nexp_train = exp_train.apply(le.fit_transform)\nexp_test = exp_test.apply(le.fit_transform)\n\nexp_train.head()","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"      MasVnrType  Alley  LotShape\n480            1      1         0\n1445           2      1         3\n382            2      1         0\n1100           2      1         3\n1136           2      1         3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MasVnrType</th>\n      <th>Alley</th>\n      <th>LotShape</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>480</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1445</th>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1100</th>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1136</th>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"5bc40a844f785eff1b48c482a69d8f90ce915289","_kg_hide-input":true},"cell_type":"code","source":"tmp,tmp_1 = OLS_experiment(exp_train, y_train, exp_test, y_test)\n\nprint('\\n')\n\ntmp = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)","execution_count":24,"outputs":[{"output_type":"stream","text":"                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                 target   R-squared:                       0.093\nModel:                            OLS   Adj. R-squared:                  0.091\nMethod:                 Least Squares   F-statistic:                     39.38\nDate:                Sun, 09 Jun 2019   Prob (F-statistic):           3.15e-24\nTime:                        22:57:20   Log-Likelihood:                -501.14\nNo. Observations:                1157   AIC:                             1010.\nDf Residuals:                    1153   BIC:                             1031.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nMasVnrType     0.0016      0.018      0.091      0.928      -0.033       0.036\nAlley          0.2027      0.044      4.628      0.000       0.117       0.289\nLotShape      -0.0749      0.008     -9.650      0.000      -0.090      -0.060\nintercept     11.9753      0.057    209.984      0.000      11.863      12.087\n==============================================================================\nOmnibus:                       25.073   Durbin-Watson:                   1.919\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               41.657\nSkew:                           0.166   Prob(JB):                     9.00e-10\nKurtosis:                       3.868   Cond. No.                         19.5\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nScore in-sample: \t 0.13923477337899384\n\n\nScore out of sample: \t 0.1718941345664725\n\n\nDecisionTreeRegressor(criterion='mse', max_depth=5, max_features=None,\n           max_leaf_nodes=20, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           presort=False, random_state=None, splitter='best')\n________________________________________\n0.33514291280626013\n________________________________________\n         feat     score\n0  MasVnrType  0.670688\n2    LotShape  0.262294\n1       Alley  0.067019\nValidation score: \t 0.13973800228017397\n","name":"stdout"}]},{"metadata":{"_uuid":"39e6b93a9e7b9d5178c436e778118b352a0cdd7f"},"cell_type":"markdown","source":"Which is significantly worse for OLS (and Lasso, not displayed) and puts us in trouble if we have to explain the model to someone else. \n\nThe result for the DecisionTree is not worse. To understand why, let's visualize the decisions of our tree if only had one variable, like the following"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0a093320c2d85b5b6286c592c7d1b6a5da9fe4ed"},"cell_type":"code","source":"exp_train = X_train[['MasVnrType']].copy()\nexp_train = pd.get_dummies(exp_train)\nexp_train.head()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"      MasVnrType_BrkCmn        ...         MasVnrType_Stone\n480                   0        ...                        0\n1445                  0        ...                        0\n382                   0        ...                        0\n1100                  0        ...                        0\n1136                  0        ...                        0\n\n[5 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MasVnrType_BrkCmn</th>\n      <th>MasVnrType_BrkFace</th>\n      <th>MasVnrType_None</th>\n      <th>MasVnrType_Stone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>480</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1445</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1100</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1136</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d1759624804b4698420a0a567de1ebc2e218abbf"},"cell_type":"code","source":"from sklearn import tree\nimport graphviz\n\ndt = DecisionTreeRegressor().fit(exp_train, y_train)\n\ndot_data = tree.export_graphviz(dt, out_file=None, filled=True)  \ngraph = graphviz.Source(dot_data)  \ngraph","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"<graphviz.files.Source at 0x7f9d2cc7ac50>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"340pt\" height=\"373pt\"\n viewBox=\"0.00 0.00 340.00 373.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 369)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-369 336,-369 336,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\"><title>0</title>\n<polygon fill=\"#e58139\" fill-opacity=\"0.349020\" stroke=\"black\" points=\"275.5,-365 171.5,-365 171.5,-297 275.5,-297 275.5,-365\"/>\n<text text-anchor=\"middle\" x=\"223.5\" y=\"-349.8\" font-family=\"Times,serif\" font-size=\"14.00\">X[2] &lt;= 0.5</text>\n<text text-anchor=\"middle\" x=\"223.5\" y=\"-334.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.154</text>\n<text text-anchor=\"middle\" x=\"223.5\" y=\"-319.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 1157</text>\n<text text-anchor=\"middle\" x=\"223.5\" y=\"-304.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 12.036</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\"><title>1</title>\n<polygon fill=\"#e58139\" fill-opacity=\"0.647059\" stroke=\"black\" points=\"215,-261 116,-261 116,-193 215,-193 215,-261\"/>\n<text text-anchor=\"middle\" x=\"165.5\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\">X[3] &lt;= 0.5</text>\n<text text-anchor=\"middle\" x=\"165.5\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.136</text>\n<text text-anchor=\"middle\" x=\"165.5\" y=\"-215.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 485</text>\n<text text-anchor=\"middle\" x=\"165.5\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 12.217</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M204.669,-296.884C199.807,-288.332 194.508,-279.013 189.423,-270.072\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"192.421,-268.262 184.435,-261.299 186.335,-271.722 192.421,-268.262\"/>\n<text text-anchor=\"middle\" x=\"177.806\" y=\"-281.704\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\"><title>6</title>\n<polygon fill=\"#e58139\" fill-opacity=\"0.133333\" stroke=\"black\" points=\"332,-253.5 233,-253.5 233,-200.5 332,-200.5 332,-253.5\"/>\n<text text-anchor=\"middle\" x=\"282.5\" y=\"-238.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.125</text>\n<text text-anchor=\"middle\" x=\"282.5\" y=\"-223.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 672</text>\n<text text-anchor=\"middle\" x=\"282.5\" y=\"-208.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 11.905</text>\n</g>\n<!-- 0&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\"><title>0&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M242.656,-296.884C249.081,-285.776 256.255,-273.372 262.727,-262.184\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"265.761,-263.929 267.738,-253.52 259.702,-260.424 265.761,-263.929\"/>\n<text text-anchor=\"middle\" x=\"274.189\" y=\"-273.973\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\"><title>2</title>\n<polygon fill=\"#e58139\" fill-opacity=\"0.545098\" stroke=\"black\" points=\"157,-157 58,-157 58,-89 157,-89 157,-157\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\">X[1] &lt;= 0.5</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.124</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 377</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 12.155</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M146.669,-192.884C141.807,-184.332 136.508,-175.013 131.423,-166.072\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"134.421,-164.262 126.435,-157.299 128.335,-167.722 134.421,-164.262\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\"><title>5</title>\n<polygon fill=\"#e58139\" stroke=\"black\" points=\"274,-149.5 175,-149.5 175,-96.5 274,-96.5 274,-149.5\"/>\n<text text-anchor=\"middle\" x=\"224.5\" y=\"-134.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.12</text>\n<text text-anchor=\"middle\" x=\"224.5\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 108</text>\n<text text-anchor=\"middle\" x=\"224.5\" y=\"-104.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 12.432</text>\n</g>\n<!-- 1&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\"><title>1&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M184.656,-192.884C191.081,-181.776 198.255,-169.372 204.727,-158.184\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"207.761,-159.929 209.738,-149.52 201.702,-156.424 207.761,-159.929\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\"><title>3</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"99,-53 0,-53 0,-0 99,-0 99,-53\"/>\n<text text-anchor=\"middle\" x=\"49.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.086</text>\n<text text-anchor=\"middle\" x=\"49.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 11</text>\n<text text-anchor=\"middle\" x=\"49.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 11.825</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M87.2144,-88.9485C81.8244,-80.1664 75.9915,-70.6629 70.5611,-61.815\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"73.51,-59.9287 65.2961,-53.2367 67.5441,-63.5903 73.51,-59.9287\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\"><title>4</title>\n<polygon fill=\"#e58139\" fill-opacity=\"0.560784\" stroke=\"black\" points=\"216,-53 117,-53 117,-0 216,-0 216,-53\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.121</text>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 366</text>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 12.165</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M128.135,-88.9485C133.618,-80.1664 139.552,-70.6629 145.076,-61.815\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"148.104,-63.5728 150.432,-53.2367 142.167,-59.8656 148.104,-63.5728\"/>\n</g>\n</g>\n</svg>\n"},"metadata":{}}]},{"metadata":{"_uuid":"aba47bd80da375703d36047fb8fde18b084b66da"},"cell_type":"markdown","source":"As we see, the tree does a first split on `MasVnrType_None`, then on `MasVnrType_Stone`, and finally on `MasVnrType_BrkFace`.\n\nNow, if we use the label encoder instead, we get"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"6d9bc03cf1354406e370c0cb6e3530d4b6677d06"},"cell_type":"code","source":"exp_train = X_train[['MasVnrType']].copy()\nexp_train['MasVnrType'] = le.fit_transform(exp_train.MasVnrType)\nexp_train.head()","execution_count":27,"outputs":[{"output_type":"execute_result","execution_count":27,"data":{"text/plain":"      MasVnrType\n480            1\n1445           2\n382            2\n1100           2\n1136           2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MasVnrType</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>480</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1445</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1100</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1136</th>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"be32b2ceeb2f82f20a60ceafc90caf75adbe28e1"},"cell_type":"code","source":"dt = DecisionTreeRegressor().fit(exp_train, y_train)\n\ndot_data = tree.export_graphviz(dt, out_file=None, filled=True)  \ngraph = graphviz.Source(dot_data)  \ngraph","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"<graphviz.files.Source at 0x7f9d2cc7acc0>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"343pt\" height=\"373pt\"\n viewBox=\"0.00 0.00 343.00 373.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 369)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-369 339,-369 339,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\"><title>0</title>\n<polygon fill=\"#e58139\" fill-opacity=\"0.349020\" stroke=\"black\" points=\"277.5,-365 173.5,-365 173.5,-297 277.5,-297 277.5,-365\"/>\n<text text-anchor=\"middle\" x=\"225.5\" y=\"-349.8\" font-family=\"Times,serif\" font-size=\"14.00\">X[0] &lt;= 2.5</text>\n<text text-anchor=\"middle\" x=\"225.5\" y=\"-334.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.154</text>\n<text text-anchor=\"middle\" x=\"225.5\" y=\"-319.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 1157</text>\n<text text-anchor=\"middle\" x=\"225.5\" y=\"-304.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 12.036</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\"><title>1</title>\n<polygon fill=\"#e58139\" fill-opacity=\"0.282353\" stroke=\"black\" points=\"217.5,-261 113.5,-261 113.5,-193 217.5,-193 217.5,-261\"/>\n<text text-anchor=\"middle\" x=\"165.5\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\">X[0] &lt;= 1.5</text>\n<text text-anchor=\"middle\" x=\"165.5\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.139</text>\n<text text-anchor=\"middle\" x=\"165.5\" y=\"-215.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 1049</text>\n<text text-anchor=\"middle\" x=\"165.5\" y=\"-200.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 11.995</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M206.02,-296.884C200.99,-288.332 195.508,-279.013 190.248,-270.072\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"193.175,-268.144 185.088,-261.299 187.141,-271.693 193.175,-268.144\"/>\n<text text-anchor=\"middle\" x=\"178.814\" y=\"-281.799\" font-family=\"Times,serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\"><title>6</title>\n<polygon fill=\"#e58139\" stroke=\"black\" points=\"335,-253.5 236,-253.5 236,-200.5 335,-200.5 335,-253.5\"/>\n<text text-anchor=\"middle\" x=\"285.5\" y=\"-238.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.12</text>\n<text text-anchor=\"middle\" x=\"285.5\" y=\"-223.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 108</text>\n<text text-anchor=\"middle\" x=\"285.5\" y=\"-208.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 12.432</text>\n</g>\n<!-- 0&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\"><title>0&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M244.98,-296.884C251.514,-285.776 258.811,-273.372 265.392,-262.184\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"268.435,-263.914 270.488,-253.52 262.401,-260.365 268.435,-263.914\"/>\n<text text-anchor=\"middle\" x=\"276.762\" y=\"-274.02\" font-family=\"Times,serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\"><title>2</title>\n<polygon fill=\"#e58139\" fill-opacity=\"0.545098\" stroke=\"black\" points=\"157,-157 58,-157 58,-89 157,-89 157,-157\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\">X[0] &lt;= 0.5</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.124</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 377</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 12.155</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\"><title>1&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M146.669,-192.884C141.807,-184.332 136.508,-175.013 131.423,-166.072\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"134.421,-164.262 126.435,-157.299 128.335,-167.722 134.421,-164.262\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\"><title>5</title>\n<polygon fill=\"#e58139\" fill-opacity=\"0.133333\" stroke=\"black\" points=\"274,-149.5 175,-149.5 175,-96.5 274,-96.5 274,-149.5\"/>\n<text text-anchor=\"middle\" x=\"224.5\" y=\"-134.3\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.125</text>\n<text text-anchor=\"middle\" x=\"224.5\" y=\"-119.3\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 672</text>\n<text text-anchor=\"middle\" x=\"224.5\" y=\"-104.3\" font-family=\"Times,serif\" font-size=\"14.00\">value = 11.905</text>\n</g>\n<!-- 1&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\"><title>1&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M184.656,-192.884C191.081,-181.776 198.255,-169.372 204.727,-158.184\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"207.761,-159.929 209.738,-149.52 201.702,-156.424 207.761,-159.929\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\"><title>3</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"99,-53 0,-53 0,-0 99,-0 99,-53\"/>\n<text text-anchor=\"middle\" x=\"49.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.086</text>\n<text text-anchor=\"middle\" x=\"49.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 11</text>\n<text text-anchor=\"middle\" x=\"49.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 11.825</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M87.2144,-88.9485C81.8244,-80.1664 75.9915,-70.6629 70.5611,-61.815\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"73.51,-59.9287 65.2961,-53.2367 67.5441,-63.5903 73.51,-59.9287\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\"><title>4</title>\n<polygon fill=\"#e58139\" fill-opacity=\"0.560784\" stroke=\"black\" points=\"216,-53 117,-53 117,-0 216,-0 216,-53\"/>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\">mse = 0.121</text>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\">samples = 366</text>\n<text text-anchor=\"middle\" x=\"166.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\">value = 12.165</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\"><title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M128.135,-88.9485C133.618,-80.1664 139.552,-70.6629 145.076,-61.815\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"148.104,-63.5728 150.432,-53.2367 142.167,-59.8656 148.104,-63.5728\"/>\n</g>\n</g>\n</svg>\n"},"metadata":{}}]},{"metadata":{"_uuid":"e2e2fbec6142c65e5069956caeda02220cb4b268"},"cell_type":"markdown","source":"Now there is only one variable and it is splitting the same amount of times but, if we check what are the values it is splitting on, we notice it is doing the same splits and thus getting the same results.\n\nAll that being said, introducing an ordering when it was not there, at the very least, makes your model less explainable and this is mostly an issue when you have to start correcting something about it (or you have to explain it to a colleague, or a client).\n\n# Ordinal variables and measurement errors\n\nThere are situations where the categories suggest an order. For example, let's have a look at `ExterQual`"},{"metadata":{"trusted":true,"_uuid":"d78896fc4f083d1ce30e0350fe9262a8702bd101","_kg_hide-input":true},"cell_type":"code","source":"df_train.ExterQual.value_counts()","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"TA    905\nGd    479\nEx     49\nFa     14\nName: ExterQual, dtype: int64"},"metadata":{}}]},{"metadata":{"_uuid":"b3ebf2931fe095d6c21797b4fa89ece482f0a6a5"},"cell_type":"markdown","source":"Here, it is easy to argue that `Ex > Gd > TA > Fa` because *Excellent* is better than *Good* and so on. Now using the LabelEncoder would give us"},{"metadata":{"trusted":true,"_uuid":"585e06fdb87c8a8fd093b64c39c59b83a8724f20","_kg_hide-input":true},"cell_type":"code","source":"ord_test = df_train[['ExterQual']].copy()\nord_test['encoded'] = le.fit_transform(ord_test.ExterQual)\nord_test.head()","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"  ExterQual  encoded\n0        Gd        2\n1        TA        3\n2        Gd        2\n3        TA        3\n4        Gd        2","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ExterQual</th>\n      <th>encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Gd</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TA</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Gd</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TA</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Gd</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"bf3eaa156f2b189233a5a50e81e4494b962dd93a"},"cell_type":"code","source":"pd.crosstab(ord_test.encoded, ord_test.ExterQual)","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"ExterQual  Ex  Fa   Gd   TA\nencoded                    \n0          49   0    0    0\n1           0  14    0    0\n2           0   0  479    0\n3           0   0    0  905","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>ExterQual</th>\n      <th>Ex</th>\n      <th>Fa</th>\n      <th>Gd</th>\n      <th>TA</th>\n    </tr>\n    <tr>\n      <th>encoded</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>49</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>14</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>479</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>905</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"_uuid":"983f4a449545d7e5fd2a041ccf5051cd1a5ed911"},"cell_type":"markdown","source":"Again, not quite what we want. There is an order, but not a reasonable one. The encoder is assigning an alphabetical order. \n\nLet's see a couple more approaches to get what we want. First, a quick summary of what are we dealing with."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a9ec8a425ebf62d17839fd4465257285a6b44b31"},"cell_type":"code","source":"ord_test = df_train[['ExterQual', 'target']].copy()\nord_test.groupby('ExterQual').agg(['mean', 'median', 'max', 'min', 'count'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dbe7fe54fe78138faa7291b4335fff152866be6"},"cell_type":"markdown","source":"To give a simple ordering to these categories, we can do the following"},{"metadata":{"trusted":true,"_uuid":"4fd2850fa14458679498c2cc1202432ba1256af8","_kg_hide-input":true},"cell_type":"code","source":"ord_test.loc[ord_test.ExterQual == 'Fa', 'ExtQuGroup'] = 0\nord_test.loc[ord_test.ExterQual == 'TA', 'ExtQuGroup'] = 1\nord_test.loc[ord_test.ExterQual == 'Gd', 'ExtQuGroup'] = 2\nord_test.loc[ord_test.ExterQual == 'Ex', 'ExtQuGroup'] = 3\n\nord_test['LabelEncoded'] = le.fit_transform(ord_test.ExterQual)\n\nx1 = ord_test['ExtQuGroup']\nx2 = ord_test['LabelEncoded']\ny = ord_test['target']\n\nprint('Correlations with target')\nprint(ord_test[['ExtQuGroup','LabelEncoded' ,'target' ]].corr())\n\nfig, ax= plt.subplots(1,2, figsize=(15, 6))\n\nsns.regplot(x = x1, y = y, x_estimator = np.mean, ax=ax[0])\nsns.regplot(x = x2, y = y, x_estimator = np.mean, ax=ax[1])\n\nax[0].set_title('Manual encoding', fontsize=16)\nax[1].set_title('Automatic encoding', fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84b0ec34bc037d59dabb43181018a44dd4fc739f"},"cell_type":"markdown","source":"We see that the automatic encoding makes much less sense (the order would not be a problem, of course). All that being said, the weird encoding is only for the `FA` category, which is present only 14 times.\n\nAs a further example, let's take "},{"metadata":{"trusted":true,"_uuid":"e98e5f1f567ee08458c44d572261749da1b6a0c7","_kg_hide-input":true},"cell_type":"code","source":"ord_test = df_train[['HeatingQC', 'target']].copy()\n\nord_test.loc[ord_test.HeatingQC == 'Po', 'HeatQGroup'] = 1\nord_test.loc[ord_test.HeatingQC == 'Fa', 'HeatQGroup'] = 2\nord_test.loc[ord_test.HeatingQC == 'TA', 'HeatQGroup'] = 3\nord_test.loc[ord_test.HeatingQC == 'Gd', 'HeatQGroup'] = 4\nord_test.loc[ord_test.HeatingQC == 'Ex', 'HeatQGroup'] = 5\n\nord_test['LabelEncoded'] = le.fit_transform(ord_test.HeatingQC)\n\nx1 = ord_test['HeatQGroup']\nx2 = ord_test['LabelEncoded']\ny = ord_test['target']\n\nprint('Correlations with target')\nprint(ord_test[['HeatQGroup','LabelEncoded' ,'target' ]].corr())\n\nfig, ax= plt.subplots(1,2, figsize=(15, 6))\n\nsns.regplot(x = x1, y = y, x_estimator = np.mean, ax=ax[0])\nsns.regplot(x = x2, y = y, x_estimator = np.mean, ax=ax[1])\n\nax[0].set_title('Manual encoding', fontsize=16)\nax[1].set_title('Automatic encoding', fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e2490a5a987c944f1f6a65a17ad928e4a43b57c"},"cell_type":"markdown","source":"Not only here we see a significant difference between the manual and the automatic encoding (again strongly in favor of the manual one), but we can also notice that it is possible that our encoding is not the optimal one. Although it is often a bad idea to tweak too much the processing of the data and enforce patterns that we believe are important, one may argue that **there is no objective scale that says that Good is 1 point better than Typical and 1 point worse than Excellent**, we are enforcing that pattern as well.\n\nIn this particular case, we don't even know if the data were gathered by the same people, that have the same parameters to judge the quality of the heating. In fact, we can easily find a better numerical encoding."},{"metadata":{"trusted":true,"_uuid":"092e03096b0ced1d10a106f554cb124698679fb7","_kg_hide-input":true},"cell_type":"code","source":"ord_test = df_train[['HeatingQC', 'target']].copy()\n\nord_test.loc[ord_test.HeatingQC == 'Po', 'HeatQGroup'] = 1\nord_test.loc[ord_test.HeatingQC == 'Fa', 'HeatQGroup'] = 2\nord_test.loc[ord_test.HeatingQC == 'TA', 'HeatQGroup'] = 3\nord_test.loc[ord_test.HeatingQC == 'Gd', 'HeatQGroup'] = 4\nord_test.loc[ord_test.HeatingQC == 'Ex', 'HeatQGroup'] = 5\n\nord_test.loc[ord_test.HeatingQC == 'Po', 'HeatQGroup_2'] = 1\nord_test.loc[ord_test.HeatingQC == 'Fa', 'HeatQGroup_2'] = 1\nord_test.loc[ord_test.HeatingQC == 'TA', 'HeatQGroup_2'] = 3\nord_test.loc[ord_test.HeatingQC == 'Gd', 'HeatQGroup_2'] = 4\nord_test.loc[ord_test.HeatingQC == 'Ex', 'HeatQGroup_2'] = 7\n\nx1 = ord_test['HeatQGroup']\nx2 = ord_test['HeatQGroup_2']\ny = ord_test['target']\n\nprint('Correlations with target')\nprint(ord_test[['HeatQGroup','HeatQGroup_2' ,'target' ]].corr())\n\nfig, ax= plt.subplots(1,2, figsize=(15, 6))\n\nsns.regplot(x = x1, y = y, x_estimator = np.mean, ax=ax[0])\nsns.regplot(x = x2, y = y, x_estimator = np.mean, ax=ax[1])\n\nax[0].set_title('Old encoding', fontsize=16)\nax[1].set_title('New encoding', fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41a8d558aa1d25446741eade69e0b3e5647bf9c2"},"cell_type":"markdown","source":"Now, which one is better? \n\nIn principle, both and neither. Since there is no real rule, we are allowed to make assumptions and go along with them. Indeed, one may even argue that, since the ordering is clear but the magnitude is not, we can again use dummies and let the model decide what is the relation with the target.\n\nThis paves the way to the next set of experiments. This time we will use only a few ordinal variables and see if the models *learn* differently if we use a different encoding."},{"metadata":{"trusted":true,"_uuid":"3fdced38e6867e38dc102e629337b02bd89f139f","_kg_hide-input":true},"cell_type":"code","source":"print('With a normal encoding')\nprint('\\n')\n\nexp_train = X_train[['HeatingQC']].copy() # , 'ExterQual', 'KitchenQual'\nexp_test = X_test[['HeatingQC']].copy()\n\nnorm_encode = {'Po': 1, 'Fa': 2, 'TA':3, 'Gd':4, 'Ex': 5}\n\nexp_train['HeatingQC'] = exp_train.HeatingQC.map(norm_encode).astype(int)\nexp_test['HeatingQC'] = exp_test.HeatingQC.map(norm_encode).astype(int)\n#exp_train['ExterQual'] = exp_train.ExterQual.map(norm_encode).astype(int)\n#exp_test['ExterQual'] = exp_test.ExterQual.map(norm_encode).astype(int)\n#exp_train['KitchenQual'] = exp_train.KitchenQual.map(norm_encode).astype(int)\n#exp_test['KitchenQual'] = exp_test.KitchenQual.map(norm_encode).astype(int)\n\nlasso_pred_norm = lasso_experiment(exp_train, y_train, exp_test, y_test, kfolds)\n\nprint('_'*40)\n\nprint('\\n Now by encoding HeatingQC differently')\nprint('\\n')\n\nexp_train = X_train[['HeatingQC']].copy()\nexp_test = X_test[['HeatingQC']].copy()\n\nnorm_encode = {'Po': 1, 'Fa': 2, 'TA':3, 'Gd':4, 'Ex': 5}\nnew_encode = {'Po': 1, 'Fa': 1, 'TA':3, 'Gd':4, 'Ex': 7}\n\nexp_train['HeatingQC'] = exp_train.HeatingQC.map(new_encode).astype(int)\nexp_test['HeatingQC'] = exp_test.HeatingQC.map(new_encode).astype(int)\n#exp_train['ExterQual'] = exp_train.ExterQual.map(norm_encode).astype(int)\n#exp_test['ExterQual'] = exp_test.ExterQual.map(norm_encode).astype(int)\n#exp_train['KitchenQual'] = exp_train.KitchenQual.map(norm_encode).astype(int)\n#exp_test['KitchenQual'] = exp_test.KitchenQual.map(norm_encode).astype(int)\n\nlasso_pred_new = lasso_experiment(exp_train, y_train, exp_test, y_test, kfolds)\n\nprint('_'*40)\n\nprint('\\n Now by getting dummies')\nprint('\\n')\n\nexp_train = X_train[['HeatingQC']].copy()\nexp_test = X_test[['HeatingQC']].copy()\n\n# dirty fix for dummies mismatch\nexp_test.loc[exp_test.HeatingQC == 'Po', 'HeatingQC'] = 'Fa' \n\nexp_train = pd.get_dummies(exp_train, drop_first=True)\nexp_test = pd.get_dummies(exp_test, drop_first=True)\n\nlasso_pred_dum = lasso_experiment(exp_train, y_train, exp_test, y_test, kfolds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9b071f39006cec6fe52b763f8c1486eb3988ce1"},"cell_type":"markdown","source":"We see that using dummies is better than giving a *standard* ordering but they are both worse than our *enhanced* one.\n\nIn summary, the lesson here is that we can't give an ordering to something that is not supposed to have one (as in the previous section) but, when we do, we should keep in mind that **we are making an assumption** when we put numerical values in our data. As such, we should keep track of our choices and how the model is reacting to that.\n\nAll that being said, trees are indeed an amazing thing and, for reasons that should be clear by now given our previous experiment, they really don't care much about what number we put in as far the order stays the same (and yes, they perform poorly all the times but, hey, consistency is a virtue). "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"fe583ecff1ed632d675cc53ae96cf2959cfb7c43"},"cell_type":"code","source":"print('With a normal encoding')\nprint('\\n')\n\nexp_train = X_train[['HeatingQC']].copy() # , 'ExterQual', 'KitchenQual'\nexp_test = X_test[['HeatingQC']].copy()\n\nnorm_encode = {'Po': 1, 'Fa': 2, 'TA':3, 'Gd':4, 'Ex': 5}\n\nexp_train['HeatingQC'] = exp_train.HeatingQC.map(norm_encode).astype(int)\nexp_test['HeatingQC'] = exp_test.HeatingQC.map(norm_encode).astype(int)\n#exp_train['ExterQual'] = exp_train.ExterQual.map(norm_encode).astype(int)\n#exp_test['ExterQual'] = exp_test.ExterQual.map(norm_encode).astype(int)\n#exp_train['KitchenQual'] = exp_train.KitchenQual.map(norm_encode).astype(int)\n#exp_test['KitchenQual'] = exp_test.KitchenQual.map(norm_encode).astype(int)\n\ntree_pred_norm = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)\n\nprint('_'*40)\n\nprint('\\n Now by encoding HeatingQC differently')\nprint('\\n')\n\nexp_train = X_train[['HeatingQC']].copy()\nexp_test = X_test[['HeatingQC']].copy()\n\nnorm_encode = {'Po': 1, 'Fa': 2, 'TA':3, 'Gd':4, 'Ex': 5}\nnew_encode = {'Po': 1, 'Fa': 1, 'TA':3, 'Gd':4, 'Ex': 7}\n\nexp_train['HeatingQC'] = exp_train.HeatingQC.map(new_encode).astype(int)\nexp_test['HeatingQC'] = exp_test.HeatingQC.map(new_encode).astype(int)\n#exp_train['ExterQual'] = exp_train.ExterQual.map(norm_encode).astype(int)\n#exp_test['ExterQual'] = exp_test.ExterQual.map(norm_encode).astype(int)\n#exp_train['KitchenQual'] = exp_train.KitchenQual.map(norm_encode).astype(int)\n#exp_test['KitchenQual'] = exp_test.KitchenQual.map(norm_encode).astype(int)\n\ntree_pred_new = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)\n\nprint('_'*40)\n\nprint('\\n Now by getting dummies')\nprint('\\n')\n\nexp_train = X_train[['HeatingQC']].copy()\nexp_test = X_test[['HeatingQC']].copy()\n\n# dirty fix for dummies mismatch\nexp_test.loc[exp_test.HeatingQC == 'Po', 'HeatingQC'] = 'Fa' \n\nexp_train = pd.get_dummies(exp_train, drop_first=True)\nexp_test = pd.get_dummies(exp_test, drop_first=True)\n\ntree_pred_dum = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0816d74aa604acbc956d5ab364cd676630c693fb"},"cell_type":"markdown","source":"# Not all the dummies show up every time\n\nAs the most curious of you noticed from my hidden code, I made a small hack in the previous cells when I made dummies out of my variable. Indeed, we can see that there is a **mismatch between training and validation set**."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"452cb5f6431c17004bda083d4e9ece98ecd60efe"},"cell_type":"code","source":"print('Training set')\nexp_train = X_train[['HeatingQC']].copy()\nexp_train = pd.get_dummies(exp_train, drop_first=True)\nexp_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b694a46ddbcad3338f62d89be7055fa458b062b9","_kg_hide-input":true},"cell_type":"code","source":"print('Validation set')\nexp_test= X_test[['HeatingQC']].copy()\nexp_test = pd.get_dummies(exp_test, drop_first=True)\nexp_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d34f3fc55b67bf35248ab108c8ecde509f35187"},"cell_type":"markdown","source":"We see that the value `Po` is missing in the training set (`Ex` has been dropped in both sets by get_dummies). If we try to run a model now, we will get an error because the columns are different. \n\nThe dirty fix I adopted before is to simply **recode `Po` to `Fa`** before creating the dummies so that the mismatch is not there. This can be justifiable if we look at the counts of each category."},{"metadata":{"trusted":true,"_uuid":"9735066c94dc938e5e6ee512d37865bd9fdc494c","_kg_hide-input":true},"cell_type":"code","source":"df_train.HeatingQC.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d42f69d70db37032dba252d560cbc4a112f26c5"},"cell_type":"markdown","source":"It is indeed an annoying little observation and changing it should not compromise our model (to be fair, we should ask ourselves serious questions about the model if it does). \n\nAnother approach can be to **first create the dummies and then split into training and validation set**. In this case, we get"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5a550db52ab156d244b1d18b24d32fd781fe2ea6"},"cell_type":"code","source":"mis_dum = df_train[['HeatingQC']].copy()\nmis_dum = pd.get_dummies(mis_dum)\n\nmis_d_train, mis_d_test, mis_d_y_train,  mis_d_y_test = train_test_split(mis_dum, df_train.target, test_size=0.20, random_state=42)\n\nprint(mis_d_train.columns)\nprint(f'Number of `Po`: \\t {mis_d_train.HeatingQC_Po.sum()}')\nprint('\\n')\nprint(mis_d_test.columns)\nprint(f'Number of `Po`: \\t {mis_d_test.HeatingQC_Po.sum()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97f55f4562e55868c9d3931f3e6cac856d7f79aa"},"cell_type":"markdown","source":"The result is simply an extra column of 0's into the training set.\n\nAnother option would be to only keep the common columns and train with them. This can be annoying if the number of mismatched dummies grows, but we can write a couple of lines of code for that"},{"metadata":{"trusted":true,"_uuid":"406cba987ed290f119536b03ee63ceb456029d6e","_kg_hide-input":true},"cell_type":"code","source":"dum_col = ['HeatingQC', 'GarageQual', 'Condition2', 'Utilities', 'Heating']\n\nexp_train = X_train[dum_col].copy()\nexp_test = X_test[dum_col].copy()\nexp_train = pd.get_dummies(exp_train, drop_first=True)\nexp_test = pd.get_dummies(exp_test, drop_first=True)\n\nprint('Mismatched dummies:')\nprint(list(set(exp_train.columns) - set(exp_test.columns)))\n\nexp_train = exp_train[[col for col in exp_test.columns if col in exp_train.columns]]\nexp_test = exp_test[[col for col in exp_train.columns]]  # yes, it has to be done twice\n\nprint('\\nCommon columns: ')\nprint(list(exp_train.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"976fd185e3b783957cec8e8eadeeecc677ac7f6c"},"cell_type":"markdown","source":"And we thus are ready to train and validate our model. \n\nNaturally, there is always the *butchery* approach which is **ignore the observations with specific values** (can be tricky with many features to dummify) or **do not train the model with the features that give you this problem** (which would reduce the complexity of the model, not necessarily a bad thing).\n\nAs usual, understanding why the mismatch is happening is crucial to the decision to take about it. My default approach is to not use features that I don't understand but this can sometimes mean accept a drop in accuracy of the model at the advantage of its explainability.\n\nIn the previous examples, the reason for the mismatch was very clear and helps us introduce the next topic\n\n## Features with very rare categories\n\nIn this section, we will focus on those features that have very rare categories and test how our models struggle if we ask them to learn on those categories. Furthermore, we will see if the same behavior can be observed for both categorical and ordinal features.\n\nTo do so, we will focus on the following features"},{"metadata":{"trusted":true,"_uuid":"72a2e8f706427906bd4d36c1b9cfce1f679e626b","_kg_hide-input":true},"cell_type":"code","source":"rare_cat = ['MSZoning', 'LotShape', 'Foundation', 'MasVnrType']\nrare_ord = ['GarageQual', 'ExterQual']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab8cc20971d11f4a04f01f722d9a97261a8313a7"},"cell_type":"markdown","source":"For example, thisi is how `Foundation` looks like"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"640a4e9523bbda4769f1fb34d947ba11bd655593"},"cell_type":"code","source":"df_train.Foundation.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31f8fe02dea103a0b027b2364c234157f82ce813"},"cell_type":"markdown","source":"As we see, `Slab`, `Stone`, and `Wood` are very unfrequent. Let's see if we spot a difference in performance or not. We already have a problem because `Wood` appears only in the train set, but we just drop the column for now."},{"metadata":{"trusted":true,"_uuid":"22220366de4468927ebfd6905ee83bfee3707ebd","_kg_hide-input":true},"cell_type":"code","source":"exp_train = X_train[rare_cat].copy()\nexp_test = X_test[rare_cat].copy()\n\nexp_train = pd.get_dummies(exp_train, drop_first=True)\nexp_test = pd.get_dummies(exp_test, drop_first=True)\n\ndel exp_train['Foundation_Wood']\n\nprint('Lasso experiment')\nlasso_pred = lasso_experiment(exp_train, y_train, exp_test, y_test, kfolds)\nprint('\\n')\nprint('Tree experiment')\ntree_pred = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f311d36c2eee385acd8a2c65387acff72336aeca"},"cell_type":"markdown","source":"## Put the rare categories together by using the documentation\n\nThis approach aims to create less unfrequent categories by putting together 2 or more categories with the suggestion of the documentation. For example, in `LotShape` we could group all the `IR#` categories into a generic `irregular`one."},{"metadata":{"trusted":true,"_uuid":"df8e6a8fce7abf9559e0faba53d9ae90f83e1dae","_kg_hide-input":true},"cell_type":"code","source":"exp_train = X_train[rare_cat].copy()\nexp_test = X_test[rare_cat].copy()\n\n# floating villages can be low density residential area\nexp_train.loc[exp_train.MSZoning == 'FV', 'MSZoning'] = 'RL'  \nexp_test.loc[exp_test.MSZoning == 'FV', 'MSZoning'] = 'RL'\n# medium and high density residential areas can be considered, with commercial area, a non low density residential area\nexp_train.loc[exp_train.MSZoning != 'RL', 'MSZoning'] = 'notRL'  \nexp_test.loc[exp_test.MSZoning != 'RL', 'MSZoning'] = 'notRL'\n\n# LotShape is either regular or irregular\nexp_train.loc[exp_train.LotShape != 'Reg', 'LotShape'] = 'IRR'\nexp_test.loc[exp_test.LotShape != 'Reg', 'LotShape'] = 'IRR'\n\n# making foundations simpler\nfond = ['PConc', 'CBlock']\nexp_train.loc[~exp_train.Foundation.isin(fond), 'Foundation'] = 'Other'\nexp_test.loc[~exp_test.Foundation.isin(fond), 'Foundation'] = 'Other'\n\n# MasVnrType doesn't need 2 types of bricks\nexp_train.loc[exp_train.MasVnrType == 'BrkCmn', 'MasVnrType'] = 'BrkFace'\nexp_test.loc[exp_test.MasVnrType == 'BrkCmn', 'MasVnrType'] = 'BrkFace'\n\nexp_train = pd.get_dummies(exp_train, drop_first=True)\nexp_test = pd.get_dummies(exp_test, drop_first=True)\n\ntree_pred = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c584774a6f4ceba72b374d612f4ca7aa94fc6b9"},"cell_type":"markdown","source":"Since we train on fewer features, we observe a better fit (only for the tree) and a lower score on the validation set and one may argue that this experiment didn't go so well. Again, this gives us the opportunity to think about how the tree algorithm is acting. Having a smaller set of features to choose from, it can finish faster and learn the data better. As an exercise, one could try a different kind of grouping of the above features to see how this changes the result.\n\n## Let a model help you\n\nLooking at the first experiment of this section, we notice that, for example, `MSZoning_FV` and `MSZoning_RL` are getting similar coefficients. Since they also represent similar categories, we can use this information and group them up to reduce the sparsity of our categories. The other values do not look playing a similar role, so we let them separated. We expect to see the Lasso experiment producing slightly better results. \n\nI would not get too greedy with this strategy because it is important to keep in mind the meaning of each feature (thus do I would not group categories only because they have a similar coefficient). To test why it can get too greedy, try to uncomment the lines in the following code and see how the result is changing"},{"metadata":{"trusted":true,"_uuid":"788d56ce60d720a7df38922679a36e6a93b73420","_kg_hide-input":true},"cell_type":"code","source":"exp_train = X_train[rare_cat].copy()\nexp_test = X_test[rare_cat].copy()\n\n# floating villages can be low density residential area\nexp_train.loc[exp_train.MSZoning == 'FV', 'MSZoning'] = 'RL'  \nexp_test.loc[exp_test.MSZoning == 'FV', 'MSZoning'] = 'RL'\n\n# Smoothing out the levels or irregularity\nexp_train.loc[exp_train.LotShape == 'IR2', 'LotShape'] = 'IR3'\nexp_test.loc[exp_test.LotShape == 'IR2', 'LotShape'] = 'IR3'\n\n# making foundations simpler\n#exp_train.loc[exp_train.Foundation == 'Stone', 'Foundation'] = 'PConc' # Similar coefficients, but not similar categories\n#exp_test.loc[exp_test.Foundation == 'Stone', 'Foundation'] = 'PConc'\n#exp_train.loc[exp_train.Foundation == 'Wood', 'Foundation'] = 'PConc'\n\n\nexp_train = pd.get_dummies(exp_train, drop_first=True)\nexp_test = pd.get_dummies(exp_test, drop_first=True)\ndel exp_train['Foundation_Wood']\n\nprint('Lasso experiment')\nlasso_pred = lasso_experiment(exp_train, y_train, exp_test, y_test, kfolds)\nprint('\\n')\nprint('Tree experiment')\ntree_pred = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83bbd7ca212a7e22c5b93034940a0f63217307fc"},"cell_type":"markdown","source":"## Rare ordinal features\n\nThis is way much less of an issue if you have already converted it to numeric values (with any strategy) because this will not throw you an error. For example, `GarageQual` has very rarely the value `Ex` or `Po` but, as we see in the next two cells, correcting for this sparsity has little to no effect."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"5e7612eaede2da7916a3b7b20400933d169ae045"},"cell_type":"code","source":"exp_train = X_train[['GarageQual']].copy()\nexp_test = X_test[['GarageQual']].copy()\n\nto_num = {'NoGrg': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n\nexp_train['GarageQual'] = exp_train['GarageQual'].map(to_num).astype(int)\nexp_test['GarageQual'] = exp_test['GarageQual'].map(to_num).astype(int)\n\nprint('Tree experiment')\ntree_pred = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While, if we correct for the mentioned sparsity, we get"},{"metadata":{"trusted":true,"_uuid":"04a985edcded4fe49698a481419c78bf9840c888","_kg_hide-input":true},"cell_type":"code","source":"exp_train = X_train[['GarageQual']].copy()\nexp_test = X_test[['GarageQual']].copy()\n\nto_num = {'NoGrg': 0, 'Po': 2, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 4}\n\nexp_train['GarageQual'] = exp_train['GarageQual'].map(to_num).astype(int)\nexp_test['GarageQual'] = exp_test['GarageQual'].map(to_num).astype(int)\n\nprint('Tree experiment')\ntree_pred = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80557f61d44ef7a25e7d8ee258034f3278011c60"},"cell_type":"markdown","source":"# From continuous to categorical\n\nThere are situations where it is tempting to take a continuous variable and make bins out it (thus making it a categorical one). While in general this can lead to a model that fits the data less than before (due to the fact that you are being somewhat less accurate with your predictors), there are situations where you can have some success with it. A non-comprehensive list of these situations is\n\n* you don't trust entirely the accuracy of the measurements that led to those data. For example, you have a column containing many distances from a reference point and you are not sure if those distances were measured accurately, you might just group all the distances below 1 Km to a category, then another category with 1-2 Km, etc. In this way, **you are assuming there is no difference inside of the same bin** and, if the measure is really inaccurate as you think, you are removing some noise to make better predictions.\n* you don't think that a variable is influencing in a continuous way your response, then you can force your model to see the data in *steps* \n* A variable is continuous but it takes almost exclusively one specific value. The classic example is a feature that is 0 almost always and then it takes very rare positive values (even really high one). In this case, you can get a clearer signal by simply binarizing this variable (thus transforming every positive value into a 1).\n\nAs before, only knowledge, context, and performance will tell you what is a good idea and what isn't. Let's make some experiment. \n\nA good example is provided by the features `GarageArea` and `GarageCars`. If we plot them both against our target, we see how `GarageCars` could be interpreted as the categorical version of `GarageArea` (which makes a lot of sense, since bigger garages can contain more cars)."},{"metadata":{"trusted":true,"_uuid":"76df262838cbfafb793b4480a84c09431de127f9","_kg_hide-input":true},"cell_type":"code","source":"g = sns.FacetGrid(df_train, hue=\"GarageCars\", height=8)\ng.map(plt.scatter, \"GarageArea\", \"target\", edgecolor=\"w\")\ng.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we use `GarageArea` (thus a continuous variable), we get"},{"metadata":{"trusted":true,"_uuid":"116ca72e6ff43595f84cae976ec852d87fce0683","_kg_hide-input":true},"cell_type":"code","source":"exp_train = X_train[['GarageArea']].copy()\nexp_test = X_test[['GarageArea']].copy()\n\nprint('OLS experiment')  # lasso would give the same\nols_pred, ols_i_pred = OLS_experiment(exp_train, y_train, exp_test, y_test)\nprint('\\n')\nprint('Tree experiment')\ntree_pred = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)\n\nplot_predictions(y_test, ols_pred, ols_pred, tree_pred)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"While, by using the categorical (orginal) variable `GarageCars`"},{"metadata":{"trusted":true,"_uuid":"a364c7b9c3c0aa20cb623349c6eed8ffb262acc1","_kg_hide-input":true},"cell_type":"code","source":"exp_train = X_train[['GarageCars']].copy()\nexp_test = X_test[['GarageCars']].copy()\n\nprint('OLS experiment')\nols_pred, ols_i_pred = OLS_experiment(exp_train, y_train, exp_test, y_test)\nprint('\\n')\nprint('Tree experiment')\ntree_pred = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)\n\nplot_predictions(y_test, ols_pred, ols_pred, tree_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this case, we see that the simple models are learning the signal a bit better when we use `GarageCars`. One could argue that, despite the better performance according to our metric, a model that predicts only 4 values is better than the previous one but we are not here to win this competition. This would spark a conversation about evaluating your model with an appropriate metric (not necessarily the one Kaggle suggests), but this is not a kernel for high scoring models.\n\nThere is some difference in the predictions and this is most likely due to some non-linear effect that is difficult for the OLS and Lasso regressors. \n\nJust out of curiosity this is what happens if we make dummies out of `GarageCars`. This means removing the ordering between the values but helping the models to learn potentially non-linear relationships."},{"metadata":{"trusted":true,"_uuid":"fdbf4d8e517fe45c181218965c0d783dfa8b2901","_kg_hide-input":true},"cell_type":"code","source":"exp_train = X_train[['GarageCars']].copy()\nexp_test = X_test[['GarageCars']].copy()\n\nexp_train['GarageCars'] = exp_train['GarageCars'].astype(str)\nexp_test['GarageCars'] = exp_test['GarageCars'].astype(str)\n\nexp_train = pd.get_dummies(exp_train)\nexp_test = pd.get_dummies(exp_test)\n\ndel exp_train['GarageCars_4']\n\nprint('OLS experiment')\nols_pred, ols_i_pred = OLS_experiment(exp_train, y_train, exp_test, y_test)\nprint('\\n')\nprint('Tree experiment')\ntree_pred = tree_experiment(exp_train, y_train, exp_test, y_test, kfolds)\n\nplot_predictions(y_test, ols_pred, ols_pred, tree_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And we finally have all models learning the same relation and producing the same poor prediction. From the OLS experiment we see also that, even though we removed the ordering between the various values, the model still thinks that with a bigger garage the house costs more.\n\n# Features with a large number of categories\n\nSometimes it can happen that a categorical feature presents way too many individual values to be made a one-hot-encoded (no matter the technique). For example, let's have a look at the feature `Neighborhood`."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df_train.Neighborhood.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we make dummies out of this feature we end up with 25 columns and this can make harder for any model to learn the data. The fancy name of this is **curse of dimensionality** and, from its name, we don't need much to know that we don't want it.\n\nOne approach would be to group some neighborhoods in order to reduce the number of dummies. However, this would require both knowledge of the city in order to group them appropriately (which we don't have) and quite some time to do so. If the first problem is solvable, the second can become worse and worse as the number of categories grows.\n\n## Frequency encoding and hashing\n\nThe idea behind frequency encoding is to label each category by using its frequency of appearance in the dataset. This is useful for two reasons: \n\n* we convert the feature to a numerical type and this ensures that the models can run without errors\n* if there is a relationship between the target and how common (or uncommon) a category is, we can learn that relationship.\n\nAs for everything else, a good understanding of the problem will drive the decision of using this particular encoding or not. In this case, for example, it is not the most useful thing in terms of scoring, but since none of us is here for the scoring it is worth spending a minute to show how to implement it.\n\nA simple way to implement it is the following"},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_train = X_train[['Neighborhood']].copy()\nexp_test = X_test[['Neighborhood']].copy()\n# getting the frequency in the training set only, you can also use the full set if it is available\nfreq_train = exp_train.groupby('Neighborhood').size() / len(exp_train) \n\nexp_train['Enc_Neigh'] = exp_train['Neighborhood'].map(freq_train)\nexp_test['Enc_Neigh'] = exp_test['Neighborhood'].map(freq_train)\n\nexp_train.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another approach to obtain a similar result is to use `rankdata` from the library `scipy.stats`, necessary in case of ties. [Here a link to the documentation](https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rankdata.html).\n\nAt last, one could try the **hashing trick**.\n\nA hash function maps an integer to a finite integer range \\[1,m\\]. The input can take values larger than *m*, thus allowing us to reduce the number of categories. This also means that multiple numbers may get mapped to the same output (something that is called *collision*). Another advantage is that feature hashing compresses the original feature into a vector of dimension m and, since it can be applied even to strings, this lead to a significant saving on the memory side. \n\nA simple implementation to get a taste of how it works is the following."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction import FeatureHasher\n\nm = int(len(set(df_train.Neighborhood)))  # number of unique neighboorhoods\n\nh = FeatureHasher(n_features=m, input_type='string')\n\nf = h.transform(df_train['Neighborhood'])\n\nf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We now have a sparse matrix of the dimensions of number of rows times the number of unique Neighboorhoods, which is very close to what we obtain by using `get_dummies` and we have a loss in interpretability, given the fact that the matrix looks like this"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f.toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But we can decide, for example, to half the number of bins to map our feature to, obtaining"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"m = int(len(set(df_train.Neighborhood))/2)\nh = FeatureHasher(n_features=m, input_type='string')\nf = h.transform(df_train['Neighborhood'])\nf","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"f.toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final tips\n\nUnderstanding categorical features can give help you building a solid model, a model that you can explain to a colleague or to a client. I hope the techniques here displayed can inspire you for the next time you will explore further applications on your own.\n\nAmong the things we didn't explore here, there is how to use use a categorical feature together with another one (what we could call an interaction). For example, one can use the average size of the houses in a Neighborhood as a feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"exp_train = X_train[['GrLivArea', 'Neighborhood']].copy()\nexp_test = X_test[['GrLivArea', 'Neighborhood']].copy()\n\navg_area = exp_train.groupby('Neighborhood', as_index=False).mean()\n\nconversion = dict(zip(avg_area.Neighborhood, avg_area.GrLivArea))\n\nexp_train['Neigh_avg_area'] = exp_train['Neighborhood'].map(conversion)\nexp_test['Neigh_avg_area'] = exp_test['Neighborhood'].map(conversion)\n\nexp_test.sample(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Which is also a way of encoding Neighborhood to a numerical value (and this is why we used the same technique as in the previous section). Or you can use counts, simple multiplications, anything that your creativity and your understanding of the problem will suggest you.\n\nA word of caution about using several observations to create a feature like in the previous case that we use all the observations from a neighborhood at the same time. If your validation strategy is not solid, this can and will lead to some information leak in your model, meaning that your training data have somehow already seen the test data. This can lead to inflated cross-validation scores and poorer generalization power for your model. \n\nThe last point is especially dangerous if you start using the target variable in your feature engineering (sometimes called target encoding). For example, you can use the mean price of the house in a neighborhood as a feature. Such a feature makes a lot of sense because the average price of a house in an area can be well known a priori, but expose you to the situation where the model *sees* the target more than once. One way is to properly separate training, validation, and test set (as we did here) but it turns out that using a statistics that does not change its distribution with or without any data point can be considered approximatively leakage-proof. This can be achieved, for example, by adding some random noise."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2759685e52c47210224891bf2828d0fc90d41d20"},"cell_type":"markdown","source":"## ***Please comment if something is not clear or completely wrong. Or upvote if you found this kernel useful. Or both. Or neither, really, it is great that you read this far already.***"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}