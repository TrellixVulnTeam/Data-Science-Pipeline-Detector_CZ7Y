{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nhp_train=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\nhp_test=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\nsample_submission=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsaleprice=hp_train[\"SalePrice\"]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-04T05:34:33.612019Z","iopub.execute_input":"2022-03-04T05:34:33.612305Z","iopub.status.idle":"2022-03-04T05:34:33.704892Z","shell.execute_reply.started":"2022-03-04T05:34:33.612227Z","shell.execute_reply":"2022-03-04T05:34:33.704121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1. Clean Data**","metadata":{}},{"cell_type":"code","source":"hp=pd.concat([hp_train.drop([\"SalePrice\"],axis=1),hp_test],axis=0,ignore_index=True,sort=False)\nhp[\"MSSubClass\"]=hp[\"MSSubClass\"].astype(\"str\") \n##because each number represents different types of dwelling, thus it should be categorial rather than numeric.\n\n#Separate categorial and numeric variables\nnumeric_column=[]\ncategory_column=[]\nfor i in hp.columns:\n    if np.issubdtype(hp[i],np.number):\n        numeric_column.append(i)\n    else:\n        category_column.append(i)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-04T05:45:39.247156Z","iopub.execute_input":"2022-03-04T05:45:39.247406Z","iopub.status.idle":"2022-03-04T05:45:39.276128Z","shell.execute_reply.started":"2022-03-04T05:45:39.247376Z","shell.execute_reply":"2022-03-04T05:45:39.275492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tell how many NaN values in our data\ndummy_catagory=pd.DataFrame(columns=[\"col\",\"NaValue\"])\nfor i in category_column:\n    if hp[i].isna().sum()>0:\n        val=hp[i].isna().sum()\n        dummy_catagory=dummy_catagory.append({\"col\":i,\"NaValue\":val},ignore_index=True)\ndummy_catagory=dummy_catagory.assign(rate=lambda x: x.NaValue/len(hp)*100)\n\ndummy_numeric=pd.DataFrame(columns=[\"col\",\"NaValue\"])\nfor i in numeric_column:\n    if hp[i].isna().sum()>0:\n        val=hp[i].isna().sum()\n        dummy_numeric=dummy_numeric.append({\"col\":i,\"NaValue\":val},ignore_index=True)\ndummy_numeric=dummy_numeric.assign(rate=lambda x: x.NaValue/len(hp)*100)\n\nprint(dummy_catagory)\nprint(\"\")\nprint(dummy_numeric)\n\"\"\"\nAs the following results, both two types of varialbes contains NaN. \nHowever, in categorial ones, it's relatively easy by transforming into dummy variable \nso we just treate the NaN value as a feature.\nFor numeric varialbes, we feel that patching them by KMeans is a comfortable way.\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-04T05:45:40.752626Z","iopub.execute_input":"2022-03-04T05:45:40.753431Z","iopub.status.idle":"2022-03-04T05:45:40.88048Z","shell.execute_reply.started":"2022-03-04T05:45:40.753394Z","shell.execute_reply":"2022-03-04T05:45:40.879845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Transform categorial variables into dummies\nhp_dummy=pd.DataFrame()\nfor i in category_column:\n    if hp[i].isna().sum()>0:\n        j=pd.get_dummies((i+\"_\"+hp[i]).fillna(i+\"_NoValue\"),drop_first=True)\n    else:\n        j=pd.get_dummies((i+\"_\"+hp[i]),drop_first=True)\n    hp_dummy=pd.concat([hp_dummy,j],axis=1)\nhp_dummy=pd.concat([hp[\"Id\"],hp_dummy],axis=1)\nhp_dummy","metadata":{"execution":{"iopub.status.busy":"2022-03-04T05:45:43.273059Z","iopub.execute_input":"2022-03-04T05:45:43.273535Z","iopub.status.idle":"2022-03-04T05:45:43.410199Z","shell.execute_reply.started":"2022-03-04T05:45:43.273501Z","shell.execute_reply":"2022-03-04T05:45:43.40952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker as mtick\nimport seaborn as sns\nimport math\nfrom sklearn.cluster import KMeans\n\n#Concatenate dummies and numeric variables\nhp_clean=pd.concat([hp_dummy,hp[numeric_column].drop(\"Id\",axis=1)],axis=1)\ndf=hp_clean.drop(dummy_numeric[\"col\"],axis=1)\n\n#Determine the optimal number of clusters\nsse=[]\nfor i in range(1,16):\n    kmeans_cluster=KMeans(init=\"k-means++\",n_clusters=i,max_iter=500,n_init=50).fit(df)\n    sse.append(kmeans_cluster.inertia_)\nplt.plot(range(1,16),sse,marker=\"o\") \n##by Elbow Method, the optimal is 4 to 5 and in this case, we choose 5.","metadata":{"execution":{"iopub.status.busy":"2022-03-04T05:45:59.360053Z","iopub.execute_input":"2022-03-04T05:45:59.360299Z","iopub.status.idle":"2022-03-04T05:47:19.908717Z","shell.execute_reply.started":"2022-03-04T05:45:59.360271Z","shell.execute_reply":"2022-03-04T05:47:19.908199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Patch NaN by KMeans\nkmeans_cluster=KMeans(init=\"k-means++\",n_clusters=5,max_iter=500,n_init=50).fit(df)\nhp_clean.insert(1,\"labels\",kmeans_cluster.labels_)\n\nfor i in dummy_numeric[\"col\"]:\n    row=np.where(hp_clean[i].isna())[0]\n    x=hp_clean[[\"labels\",i]].groupby(\"labels\").mean()\n    for j in row:\n        hp_clean.loc[j,i]=x.iloc[hp_clean.loc[j,\"labels\"],0]","metadata":{"execution":{"iopub.status.busy":"2022-03-04T05:47:40.682417Z","iopub.execute_input":"2022-03-04T05:47:40.682688Z","iopub.status.idle":"2022-03-04T05:47:46.82758Z","shell.execute_reply.started":"2022-03-04T05:47:40.682638Z","shell.execute_reply":"2022-03-04T05:47:46.826999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Spot highly linear dependent variable(because these features can only provide little information)\nhp_corr=hp_dummy.drop(\"Id\",axis=1).corr(method ='pearson')\nfor i in np.arange(hp_corr.shape[0]):\n    j=i\n    while j<hp_corr.shape[0]:\n        hp_corr.values[i][j]=0\n        j=j+1\n\nx=np.absolute(hp_corr)\nlinear_dependence={}\nfor i in x.columns:\n    if x[i].max()>0.9:\n        linear_dependence[i]=list(x[i][x[i]>0.9].index)\nlinear_dependence \n##we define high linear dependency for the correlation of any two variables which is greater than 0.9 or less than -0.9.","metadata":{"execution":{"iopub.status.busy":"2022-03-04T05:47:48.971629Z","iopub.execute_input":"2022-03-04T05:47:48.972021Z","iopub.status.idle":"2022-03-04T05:47:49.640237Z","shell.execute_reply.started":"2022-03-04T05:47:48.971988Z","shell.execute_reply":"2022-03-04T05:47:49.639514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Truncate those variable and finish the data clean process.\nx=list(linear_dependence.values())\nx=set([i for j in x for i in j])\nhp_clean=hp_clean.drop(x,axis=1)\nhp_clean=(hp_clean-hp_clean.min())/(hp_clean.max()-hp_clean.min()) ##standardization","metadata":{"execution":{"iopub.status.busy":"2022-03-04T05:47:52.236857Z","iopub.execute_input":"2022-03-04T05:47:52.23766Z","iopub.status.idle":"2022-03-04T05:47:52.260537Z","shell.execute_reply.started":"2022-03-04T05:47:52.237623Z","shell.execute_reply":"2022-03-04T05:47:52.259891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2. Reduce Features**","metadata":{}},{"cell_type":"code","source":"\n#We now have 267 varialbes, but not all variables can provide useful information.\n#PCA (Principal Component Analysis) is a perfect tool to reduce and extract crucial features.\nfrom sklearn.decomposition import PCA\npca_decomp=PCA(n_components=265).fit(hp_clean.drop([\"Id\",\"labels\"],axis=1))\npd.DataFrame({\n    \"pca\":[\"{:.4f}\".format(x*100) for x in pca_decomp.explained_variance_ratio_],\n    \"cumulated_rate\":[\"{:.4f}\".format(x*100) for x in np.cumsum(pca_decomp.explained_variance_ratio_)]\n}).head(166) ##our data can be explained over 99% by 166 components","metadata":{"execution":{"iopub.status.busy":"2022-03-04T06:01:57.436706Z","iopub.execute_input":"2022-03-04T06:01:57.437233Z","iopub.status.idle":"2022-03-04T06:01:57.58832Z","shell.execute_reply.started":"2022-03-04T06:01:57.437196Z","shell.execute_reply":"2022-03-04T06:01:57.586622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Finish the feature reduce process.\nx=pd.DataFrame(pca_decomp.fit_transform(hp_clean.drop([\"Id\",\"labels\"],axis=1)))\nhp_pca19=x.iloc[:,0:166]\ny=[]\nfor i in range(166):\n    y.append(\"pca\"+str(i+1))\n\nhp_pca19.columns=y\nhp_pca19","metadata":{"execution":{"iopub.status.busy":"2022-03-04T06:09:54.064113Z","iopub.execute_input":"2022-03-04T06:09:54.06437Z","iopub.status.idle":"2022-03-04T06:09:54.218689Z","shell.execute_reply.started":"2022-03-04T06:09:54.064341Z","shell.execute_reply":"2022-03-04T06:09:54.217915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3. Regression Anlysis and Prediction**","metadata":{}},{"cell_type":"markdown","source":"# 3-1. Simple Linear Regression","metadata":{}},{"cell_type":"code","source":"hp_pca19_train=hp_pca19.iloc[range(len(saleprice))]\nhp_pca19_test=hp_pca19.iloc[range(len(saleprice),len(hp_pca19))]\n\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n#OLS Regression fitting\nX=hp_pca19_train\nX=sm.add_constant(X)\nlm=sm.OLS(np.log(saleprice),X).fit() \n##it's a common way to use log price rather than price because it can make sure our model always produce positive number (exp(y)>0)\n##and log can also depict the percentage change of house price.\n\n#Breusch-Pagen Test for heteroscedasticity\nfrom statsmodels.stats.diagnostic import het_breuschpagan\nbp_test = het_breuschpagan((lm.resid),lm.model.exog)\n\nprint(lm.summary())\nprint(\"\")\nprint(\n    pd.DataFrame({\n    \"stats\":[\"Lagrange\",\"Lagrange-score\",\"f-test\",\"f-score\"],\n    \"score\":[\"{:.6f}\".format(i) for i in bp_test]})\n) ##we reject null hypothesis, the regression is heteroscedastic\n\n##as the following results, the overall f-test of OLS regression and almost all coefficients are significant.\n##R-square tells us that this model can explain over 90% of variance of the house price.\n##However, Breusch-Pagen Test is failed so this model has the heteroscedastic problem,\n##so even though our coefficients are still unbiased, they are on longer BLUE (best linear unbiased estimator) \n##which means our coefficients are inefficient and inconsistent.","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:59:02.245351Z","iopub.execute_input":"2022-03-04T09:59:02.24579Z","iopub.status.idle":"2022-03-04T09:59:02.464337Z","shell.execute_reply.started":"2022-03-04T09:59:02.24575Z","shell.execute_reply":"2022-03-04T09:59:02.462557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot residuals\nx=pd.concat([np.log(saleprice),(lm.resid)],axis=1)\nx.rename(columns={0:\"lm_resid\"},inplace=True)\nfig, axe=plt.subplots(figsize=(10,6))\naxe.scatter(x.iloc[:,0],x.iloc[:,1],s=3)\naxe.plot([10.5,13.5],[0,0],color=\"black\",alpha=0.5,linestyle=\"--\")\nplt.show()\n##as the following plot, we can observe residuals of the OLS has some positive relationship with log price.\n##therefore, the variance of residuals is not a constant and violates Gauss-Markov Theorem.","metadata":{"execution":{"iopub.status.busy":"2022-03-04T06:22:14.133558Z","iopub.execute_input":"2022-03-04T06:22:14.134097Z","iopub.status.idle":"2022-03-04T06:22:14.331164Z","shell.execute_reply.started":"2022-03-04T06:22:14.134058Z","shell.execute_reply":"2022-03-04T06:22:14.330513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#GLS Regression fitting\nresid=sm.OLS(np.log(list(lm.resid**2)),X).fit()\nsigma=np.zeros((X.shape[0],X.shape[0]))\n##we estimate the sigma matrix to weight our regression model (Generalized least squares).\n##this process is a feasible way to alleviate the heteroscedastic problem.\nnp.fill_diagonal(sigma,np.exp(resid.predict(X)))\nX2=pd.DataFrame.dot(pd.DataFrame(sigma),X)\nglm=sm.GLS(np.log(saleprice),X,sigma=sigma).fit()\n\nbp_test2 = het_breuschpagan(np.dot(np.linalg.inv(sigma**0.5),glm.resid),glm.model.exog)\n\nprint(glm.summary())\nprint(\"\")\nprint(\n    pd.DataFrame({\n    \"stats\":[\"Lagrange\",\"Lagrange-score\",\"f-test\",\"f-score\"],\n    \"score\":[\"{:.6f}\".format(i) for i in bp_test2]\n})\n) #not rejecting null hypothesis\n","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:59:07.46651Z","iopub.execute_input":"2022-03-04T09:59:07.46717Z","iopub.status.idle":"2022-03-04T09:59:08.440267Z","shell.execute_reply.started":"2022-03-04T09:59:07.467134Z","shell.execute_reply":"2022-03-04T09:59:08.439501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compare OSL and GSL coefficients\nx=[]\nfor i in lm.pvalues:\n    if i<0.01: x.append(\"***\")\n    else: \n        if i<0.05: x.append(\"**\")\n        else: \n            if i <0.1: x.append(\"*\")\n            else: x.append(\"insignificant\")\ny=[]\nfor i in glm.pvalues:\n    if i<0.01: y.append(\"***\")\n    else: \n        if i<0.05: y.append(\"**\")\n        else: \n            if i <0.1: y.append(\"*\")\n            else: y.append(\"insignificant\")\n            \npd.DataFrame({\"ols\":list(lm.params),\"gls\":list(glm.params),\"ols_pvalue\":x,\"gls_pvalue\":y},index=lm.params.index)\n##GLS will tend to suppress insignificant coefficients(make it closer to 0) and elevate significant ones.","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:59:48.869071Z","iopub.execute_input":"2022-03-04T09:59:48.869649Z","iopub.status.idle":"2022-03-04T09:59:48.890589Z","shell.execute_reply.started":"2022-03-04T09:59:48.869612Z","shell.execute_reply":"2022-03-04T09:59:48.889788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3-1. Spline Regression","metadata":{}},{"cell_type":"code","source":"#Linear regression assumes the relationship detween explanatory variables and the outcome is linear and continuous.\n#However, this assumption is too strong to be true in almost every cases.\n#Because the slope of relationship usually fluctuates, we need to capture the chage.\n#Here, we apply the B-Spline function to describe the potential change over log(saleprice)\n\nimport patsy\nknot=[]\nfor i in range(10):\n    knot.append(np.quantile(X.iloc[:,1],q=i/10))\n\nsimpleOLS=sm.OLS(np.log(saleprice),X.iloc[:,[0,1]]).fit().predict(X.iloc[:,[0,1]])\n#line=np.linspace(X[\"pca1\"].min(),X[\"pca1\"].max(),len(X[\"pca1\"]))\nb=pd.DataFrame(patsy.dmatrix(\"bs(x,knots=knot,degree=1)\",{\"x\":X.iloc[:,1]}))\nspline1=sm.OLS(np.log(saleprice),b).fit().predict(b)\nb=pd.DataFrame(patsy.dmatrix(\"bs(x,knots=knot,degree=2)\",{\"x\":X.iloc[:,1]}))\nspline2=sm.OLS(np.log(saleprice),b).fit().predict(b)\nb=pd.DataFrame(patsy.dmatrix(\"bs(x,knots=knot,degree=3)\",{\"x\":X.iloc[:,1]}))\nspline3=sm.OLS(np.log(saleprice),b).fit().predict(b)\n\n\nfig, axe=plt.subplots(figsize=(14,8))\naxe.scatter(X.iloc[:,1],np.log(saleprice),color=\"#696969\",s=2)\naxe.scatter(X.iloc[:,1], simpleOLS, s=3,color='#B22222')\naxe.scatter(X.iloc[:,1], spline1,s=3, color='#008000')\naxe.scatter(X.iloc[:,1], spline2, s=3,color='#FF8C00',alpha=0.8)\naxe.scatter(X.iloc[:,1], spline3, s=3,color='#5686BF',alpha=0.8)\naxe.legend([\"Data Points\",\"Simple OLS\",\"Spline Regression (order 1)\",\"Spline Regression (order 2)\",\"Spline Regression (order 3)\"])\n\n#as the following graph, spline regressions can describe the change of the relationship among lower and higher prices.\n#the higher the order is, the more it can fit; but overfitting problem may also rise","metadata":{"execution":{"iopub.status.busy":"2022-03-04T06:30:45.333072Z","iopub.execute_input":"2022-03-04T06:30:45.33332Z","iopub.status.idle":"2022-03-04T06:30:46.020686Z","shell.execute_reply.started":"2022-03-04T06:30:45.333293Z","shell.execute_reply":"2022-03-04T06:30:46.019989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We don't have a module for spline regression (python 3.7 or higher version didn't support sklearn_contrib_py_earth)\n#But we can write one to achieve our goal.\ndef spline_parm(X=X,y=saleprice,q=40,degree=1): #q for maxium knots\n\n    spline=sm.OLS(np.log(y),X).fit()\n    AIC0=spline.aic #AIC for picking the optional parameter\n    b=X\n\n    parm=[]\n    for k in range(len(X.columns)-1):\n        b0=b.drop(\"pca\"+str(k+1),axis=1)\n        for i in range(1,q):\n            knot=[]\n            for j in range(1,i):\n                knot.append(np.quantile(X[\"pca\"+str(k+1)],q=j/(i+1)))\n            b=pd.DataFrame(patsy.dmatrix(\"bs(x,knots=knot,degree=degree)\",{\"x\":X[\"pca\"+str(k+1)]})).drop(0,axis=1)\n            column_name=[]\n            for j in range(len(b.columns)):\n                column_name.append(\"pca\"+str(k+1)+\"_\"+str(j+1))\n            b.columns=column_name\n            b=pd.concat([b0,b],axis=1)\n            spline=sm.OLS(np.log(y),b).fit()\n            if spline.aic>AIC0:break #coordinate descent and greedy algorithm \n            else:AIC0=spline.aic\n        parm.append(i-1)\n    return parm, spline # parm is to get the optional parameters in each b-spline degree\n\nparm1, spline1=spline_parm(degree=1)\nparm2, spline2=spline_parm(degree=2)\nparm3, spline3=spline_parm(degree=3)\n\npd.DataFrame({\n    \"r2_order1\":[spline1.rsquared],\n    \"r2_order2\":[spline2.rsquared],\n    \"r2_order3\":[spline3.rsquared]\n},) \n#higher order (like order2 and order3) gets better r2-scores; \n#however, it may also causes overfitting problem.","metadata":{"execution":{"iopub.status.busy":"2022-03-04T08:04:59.863146Z","iopub.execute_input":"2022-03-04T08:04:59.863409Z","iopub.status.idle":"2022-03-04T08:05:57.243587Z","shell.execute_reply.started":"2022-03-04T08:04:59.863378Z","shell.execute_reply":"2022-03-04T08:05:57.242895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4. Random Forest**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrfr=RandomForestRegressor(n_estimators=1000).fit(X.drop(\"const\",axis=1),np.log(saleprice))\n##It's very easy to run since we have a powerful tool \"sklearn\" in Python.\n##we use 1000 trees to determine the house prices.\n","metadata":{"execution":{"iopub.status.busy":"2022-03-04T06:31:26.374614Z","iopub.execute_input":"2022-03-04T06:31:26.375169Z","iopub.status.idle":"2022-03-04T06:33:24.114841Z","shell.execute_reply.started":"2022-03-04T06:31:26.375121Z","shell.execute_reply":"2022-03-04T06:33:24.114112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **5. Compare Results**","metadata":{}},{"cell_type":"markdown","source":"# 5-1 Cross Validation","metadata":{}},{"cell_type":"code","source":"#Cross Validation\n##we can compare the expected performance of each model(OLS, GLS RFR) by CV.\n##However, it seems there is no such an off-the-shelf CV function for GLS so we need to write one by our own.\n\nX=hp_pca19_train\nX=sm.add_constant(X)\nimport random\n\ndef cv_data(cv=20,x=X,y=saleprice): #generating CV data\n##to ramdomly split the dataset\n    k0=int(np.floor(len(x)/cv))\n    k1=random.sample(range(len(x)),len(x))\n    cv_index=[];k=k1\n    for j in range(cv-1):\n        cv_index.append([k1.index(a) for a in range(k0*j,k0*(j+1))])\n        k=[a for a in k if a not in cv_index[j]]\n    cv_index.append(k)\n\n    x_train,y_train,x_val,y_val=[],[],[],[]\n    for i in range(cv):\n        x_val.append(x.iloc[cv_index[i],:])\n        x_train.append(x.drop(cv_index[i],axis=0))\n        y_train.append(np.log(y.drop(cv_index[i],axis=0)))\n        y_val.append(np.log(y[cv_index[i]]))\n    return x_train,y_train,x_val,y_val\n\nx_train,y_train,x_val,y_val=cv_data(x=X,y=saleprice)\n\ndef OLS(xtrain=x_train,ytrain=y_train,xval=x_val,yval=y_val):\n    r2=[]\n    for i in range(len(xtrain)):\n        yperid=sm.OLS(ytrain[i],xtrain[i]).fit().predict(xval[i])\n        sse=((yperid-yval[i])**2).sum()\n        sst=((yval[i].mean()-yval[i])**2).sum()\n        r2.append(1-(sse/sst))\n    return(r2)\n\ndef GLS(xtrain=x_train,ytrain=y_train,xval=x_val,yval=y_val):\n    r2=[]\n    for i in range(len(xtrain)):\n\n        yperid0=sm.OLS(ytrain[i],xtrain[i]).fit().predict(xtrain[i])\n        resid=((yperid0-ytrain[i]))\n        sigma=np.exp(sm.OLS(np.log(resid**2),xtrain[i]).fit().predict(xtrain[i]))\n\n        yperid=sm.GLS(ytrain[i],xtrain[i],sigma=sigma).fit().predict(xval[i])\n        sse=((yperid-yval[i])**2).sum()\n        sst=((yval[i].mean()-yval[i])**2).sum()\n        r2.append(1-(sse/sst))\n    return(r2)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-04T08:59:47.705801Z","iopub.execute_input":"2022-03-04T08:59:47.706051Z","iopub.status.idle":"2022-03-04T08:59:47.881603Z","shell.execute_reply.started":"2022-03-04T08:59:47.706023Z","shell.execute_reply":"2022-03-04T08:59:47.880911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CV for spline regression\ndef spline_regression_cv(parm,degree=2,x_train=x_train,y_train=y_train,x_val=x_val,y_val=y_val):\n\n    r2=[]\n    for k in range(len(x_train)):\n\n        x_train[k]=x_train[k].reset_index(drop=True)\n        y_train[k]=y_train[k].reset_index(drop=True)\n        x_val[k]=x_val[k].reset_index(drop=True)\n        y_val[k]=y_val[k].reset_index(drop=True)\n\n        b1=x_train[k][\"const\"]\n        b1_val=x_val[k][\"const\"]\n\n        min=pd.DataFrame(x_train[k].min(),columns=[\"min\"]).T\n        max=pd.DataFrame(x_train[k].max(),columns=[\"max\"]).T\n        tmp=pd.concat([x_val[k],min,max])\n\n\n        for i in range(len(parm)):\n            knot=[]\n            if parm[i]>0:\n                for j in range(1,parm[i]+1):\n                    a=x_train[k][\"pca\"+str(i+1)]\n                    knot.append(np.quantile(a,q=j/(parm[i]+1)))\n            b0=pd.DataFrame(patsy.dmatrix(\"bs(x,knots=knot,degree=degree)\",{\"x\":x_train[k][\"pca\"+str(i+1)]})).drop(0,axis=1)\n            b0_val=pd.DataFrame(patsy.dmatrix(\"bs(x,knots=knot,degree=degree)\",{\"x\":tmp[\"pca\"+str(i+1)]})).drop(0,axis=1)\n            column_name=[]\n            for j in range(len(b0.columns)):\n                column_name.append(\"pca\"+str(i+1)+\"_\"+str(j+1))\n            b0.columns=column_name\n            b0_val.columns=column_name\n            b1=pd.concat([b1,b0],axis=1)\n            b1_val=pd.concat([b1_val,b0_val],axis=1)\n        b1_val=b1_val.drop(len(tmp)-1).drop(len(tmp)-2)\n\n        yv=sm.OLS(y_train[k],b1).fit().predict(b1_val)\n\n        sse=((yv-y_val[k])**2).sum()\n        sst=((y_val[k].mean()-y_val[k])**2).sum()\n        r2.append(1-sse/sst)\n    return(r2)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T08:59:49.511091Z","iopub.execute_input":"2022-03-04T08:59:49.512845Z","iopub.status.idle":"2022-03-04T08:59:49.52884Z","shell.execute_reply.started":"2022-03-04T08:59:49.512793Z","shell.execute_reply":"2022-03-04T08:59:49.528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compare the performance of predictions of OLS, GLS and Random Forest\nfrom sklearn.model_selection import cross_val_score\ncvOLS=OLS()\ncvGLS=GLS()\ncvSpline1=spline_regression_cv(parm=parm1,degree=1)\ncvSpline2=spline_regression_cv(parm=parm2,degree=2)\ncvSpline3=spline_regression_cv(parm=parm3,degree=3)\ncvRFR=cross_val_score(rfr,X,np.log(saleprice),cv=20,scoring=\"r2\")","metadata":{"execution":{"iopub.status.busy":"2022-03-04T08:59:52.716575Z","iopub.execute_input":"2022-03-04T08:59:52.717228Z","iopub.status.idle":"2022-03-04T09:38:43.662595Z","shell.execute_reply.started":"2022-03-04T08:59:52.717194Z","shell.execute_reply":"2022-03-04T09:38:43.661865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame({\n    \"OLS_cv\":cvOLS,\n    \"GLS_cv\":cvGLS,\n    \"Spline1_cv\":cvSpline1,\n    \"Spline2_cv\":cvSpline2,\n    \"Spline3_cv\":cvSpline3,\n    \"RFR_cv\":cvRFR\n}).describe()\n\n##we use R2 score to gauge the performance\n\n##Spline1(b-spline regression order1) gets the highest score.\n##even thought Spline2 and Spline3 has better r2-scores on the previous occasion, \n##the overfitting problem reduce its performance of perdiction.\n\n#there is no significant difference between OLS, GLS.\n\n##also, RFR's performance is relatively stable in compare with others.(the smallest std)\n##it's actually straightforward because RFR is a kind of bagging algorithm to reduce variance and stablize the fluctuation.","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:38:58.801403Z","iopub.execute_input":"2022-03-04T09:38:58.801677Z","iopub.status.idle":"2022-03-04T09:38:58.833022Z","shell.execute_reply.started":"2022-03-04T09:38:58.801637Z","shell.execute_reply":"2022-03-04T09:38:58.832319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5-2 Prediction","metadata":{}},{"cell_type":"code","source":"hp_pca19_test_lm=sm.add_constant(hp_pca19_test)\n\nmin=pd.DataFrame(hp_pca19_test.min(),columns=[\"min\"]).T\nmax=pd.DataFrame(hp_pca19_test.max(),columns=[\"max\"]).T\ntmp=sm.add_constant((pd.concat([hp_pca19_test,min,max])).reset_index(drop=True))\nb_test=tmp[\"const\"]\n\nparm,spline=spline_parm(X=X,y=saleprice,q=40,degree=1)\nfor i in range(len(parm)):\n    knot=[]\n    if parm[i]>0:\n        for j in range(1,parm[i]+1):\n            a=tmp[\"pca\"+str(i+1)].reset_index(drop=True)\n            knot.append(np.quantile(a,q=j/(parm[i]+1)))\n\n    b=pd.DataFrame(patsy.dmatrix(\"bs(x,knots=knot,degree=1)\",{\"x\":tmp[\"pca\"+str(i+1)].reset_index(drop=True)})).drop(0,axis=1)\n    column_name=[]\n    for j in range(len(b.columns)):\n        column_name.append(\"pca\"+str(i+1)+\"_\"+str(j+1))\n    b.columns=column_name\n    b_test=pd.concat([b_test,b],axis=1)\n\nb_test=b_test.drop(len(tmp)-1).drop(len(tmp)-2)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:39:21.895152Z","iopub.execute_input":"2022-03-04T09:39:21.895414Z","iopub.status.idle":"2022-03-04T09:39:30.504578Z","shell.execute_reply.started":"2022-03-04T09:39:21.895385Z","shell.execute_reply":"2022-03-04T09:39:30.503862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lm_test=np.exp(lm.predict(hp_pca19_test_lm))\nglm_test=np.exp(glm.predict(hp_pca19_test_lm))\nrfr_test=np.exp(rfr.predict(hp_pca19_test))\nspline1_test=np.exp(spline.predict(b_test))\n\nregress_test=pd.DataFrame({\n    \"guideline\":sample_submission[\"SalePrice\"],\n    \"OLS\":lm_test.reset_index(drop=True),\n    \"GLS\":glm_test.reset_index(drop=True),\n    \"Spline1\":spline1_test,\n    \"RFR\":rfr_test  \n})\nregress_test","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:50:04.562722Z","iopub.execute_input":"2022-03-04T09:50:04.562986Z","iopub.status.idle":"2022-03-04T09:50:04.958266Z","shell.execute_reply.started":"2022-03-04T09:50:04.562957Z","shell.execute_reply":"2022-03-04T09:50:04.957445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axe=plt.subplots(figsize=(15,6))\nfor i in range(1,10):\n    q_up=np.quantile(regress_test[\"guideline\"],i/10)\n    q_low=np.quantile(regress_test[\"guideline\"],(i-1)/10)\n    y=regress_test[(\n        regress_test[\"guideline\"]<q_up) & (regress_test[\"guideline\"]>=q_low)].median()\n    axe.bar(i-0.3,y.values[0],width=0.15,color=\"#A9A9A9\",edgecolor=\"black\",align=\"center\")\n    axe.bar(i-0.15,y.values[1],width=0.15,color=\"#6495ED\",edgecolor=\"black\",align=\"center\")\n    axe.bar(i,y.values[2],width=0.15,color=\"#FF4D40\",edgecolor=\"black\",align=\"center\")\n    axe.bar(i+0.15,y.values[3],width=0.15,color=\"#008080\",edgecolor=\"black\",align=\"center\")\n    axe.bar(i+0.3,y.values[4],width=0.15,color=\"#9ACD32\",edgecolor=\"black\",align=\"center\")\nfrom matplotlib import ticker as mtick\naxe.xaxis.set_major_locator(mtick.FixedLocator([i for i in range(1,10)]))\naxe.set_xticklabels([\"q\"+str(i*10) for i in range(1,10)],fontsize=11)\nval=[int(i) for i in axe.get_yticks()]\naxe.yaxis.set_major_locator(mtick.FixedLocator([i for i in val]))\naxe.set_yticklabels([\"{:,}\".format(i) for i in val],fontsize=11)\naxe.set_xlabel(xlabel=\"Quantiles for each house price\",fontsize=16);axe.set_ylabel(ylabel=\"\")\naxe.legend(y.index,loc=4)","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:50:09.933493Z","iopub.execute_input":"2022-03-04T09:50:09.933752Z","iopub.status.idle":"2022-03-04T09:50:10.271137Z","shell.execute_reply.started":"2022-03-04T09:50:09.933722Z","shell.execute_reply":"2022-03-04T09:50:10.270482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission=pd.DataFrame({\n     \"id\":[i for i in range(1461,2920)],\n      \"SalePrice\":spline1_test\n})\nsubmission.to_csv('submission.csv',index=False)\nsubmission[\"SalePrice\"].describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T09:50:16.504129Z","iopub.execute_input":"2022-03-04T09:50:16.504382Z","iopub.status.idle":"2022-03-04T09:50:16.522235Z","shell.execute_reply.started":"2022-03-04T09:50:16.504353Z","shell.execute_reply":"2022-03-04T09:50:16.521527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}