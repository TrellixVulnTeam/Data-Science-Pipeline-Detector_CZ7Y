{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House Price Prediction"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://ritu-19.github.io/images/housing_0_2.jpg\" width=\"800\" height=\"400\">"},{"metadata":{},"cell_type":"markdown","source":"In this Kernel we will try to predict the house **Saleprice** using different linear regression techniques. We will see how we can modify the features and fit into a linear model and get good results. Before going further we must understand the assumptions of linear regression."},{"metadata":{},"cell_type":"markdown","source":"\n**Assumptions**\n\n1. **Linear relationship: There should be a linear relationship between input and target variables**\n\n2. **Multivariate Normality: Assumes that the resiudals after prediction are normally distributed.**\n\n3. **No Multicollinearity: Assumes that input variables are not highly coorelated with each other**\n\n4. **Homoscedacity: Assumes variance of error terms are similar across input variables.**\n\nYou can read about valdating assumptions from my kernel\nhttps://www.kaggle.com/arunmohan003/linear-regression-analysis-validating-assumptions on another house prediction dataset.\n\nYou can read more from [here](https://www.statisticssolutions.com/assumptions-of-linear-regression/)\n\nWe must keep these points in mind before proceeding further "},{"metadata":{},"cell_type":"markdown","source":"### Importing libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nimport scipy\nimport matplotlib.gridspec as gridspec\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score,cross_validate\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,ElasticNet\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nimport matplotlib.style as style\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Loading data\ntrain_csv = '/kaggle/input/house-prices-advanced-regression-techniques/train.csv'\ntest_csv = '/kaggle/input/house-prices-advanced-regression-techniques/test.csv'\n\ndf_train = pd.read_csv(train_csv)\ndf_test = pd.read_csv(test_csv)\nprint(f'The shape of train data is {df_train.shape}')\nprint(f'The shape of test data is {df_test.shape}')\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis\n\n## EDA on SalePrice\n\nLet's create a histogram to see if the target variable is Normally distributed. If we want to create a linear model, it is better that the features follow a normal distribution.\n\n* We will plot a histplot(Also we will check skweness) and Q-Q plot to check this.\n* Also we will plot a boxplot to check outliers\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#function for ploting Histogram,Q-Q plot and \n# Box plot of target and also print skewness\ndef target_analysis(target):\n    fig = plt.figure(constrained_layout=True, figsize=(14,10))\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    ax1 = fig.add_subplot(grid[0, :2])\n    ax1.set_title('Histogram')\n    sns.distplot(target,norm_hist=True,ax=ax1)\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('Q-Q Plot')\n    stats.probplot(target,plot=ax2)\n    ax3 = fig.add_subplot(grid[:,2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(target,orient='v',ax=ax3)\n    print(f'skweness is { target.skew()}')\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_analysis(df_train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations: \n\n* We can see that SalePrice are not normaly distributed.\n\n* It is right skwed by 1.8828757597682129(Positive skweness). Skweness means is the degree of distortion from the symmetrical bell curve or the normal curve. Exact normal distribution will have skweness of 0.\n\n* Here Positive skweness means more than half of the houses will be sold at less than average price.\n\n* When performing regression, sometimes it makes sense to log-transform the target variable when it is skewed. One reason for this is to improve the linearity of the data.\n\n* We can see some outliers(above 700000) using box plot[outliers]. For now we will keep it.\n\n* To make distribution symmeric we can apply log to it (np.log1()).We will handle that later.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_analysis(np.log1p(df_train['SalePrice']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: \n\n* We can see that now our target variable is more skwed towards zero and symmetric(follows a approx normal distribution). "},{"metadata":{},"cell_type":"markdown","source":"## EDA of continous variables\n\n* Continous variables plays an important role in prediction of linear regression models. Let us explore more on these"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nnum_cols_tr = df_train.select_dtypes('number').columns.tolist()\nnum_cols_te = df_test.select_dtypes('number').columns.tolist()\n\nprint(f'There are {len(num_cols_tr)} numeric columns in train data')\ndf_num = df_train[num_cols_tr]\ndf_test_num = df_test[num_cols_te]\n\nprint('Train/test numeric shapes')\nprint(df_num.shape)\nprint(df_test_num.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# droping column 'Id' as it is not needed\ndf_num = df_num.drop(columns=['Id'])\ndf_test_num = df_test_num.drop(columns=['Id'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Coorelation matrix and removing multicollinearity"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr = df_num.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, cmap=sns.diverging_palette(20, 220, n=200))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Heatmap is the better way to understand coorelation independent variables as well as with target variables.\n\n* We know as per assumption of linear regression multicollinearity should not exist.That means two or more independent features cannot be strongly coorelated.Coorelation values exist between [-1,1]. \n\n* Here we have no  strong negative coorelation,but there are some strong positive coorelations \n\n* From the first sight itself we see deep blue at intersection of features '1stFlrSF'(First Floor square feet) and 'TotalBSMTSF'(Total square feet of basement area) .Another blue spot we see is between 'Garage area' and 'Garage Cars'. That means that they are highly coorelated.We can say that they almost give the same information, so multicollinearity really occurs. \n\n* So a better approach is to keep one of them.For eg, that we will either keep Garage area or Garage Cars, as they both give same information. We will not be abe to seperate those. So we can keep one of them \n\n* Similarly features like 'YearBuilt' and 'GarageYrBlt' have strong coorelation.Much more features behave like this.\n\n* We will analyse strong multicollinear pairs.They are ('GarageArea','GarageCars'),('GarageYrBlt','YearBuilt')('TotRmsAbvGrd','GrLivArea'),('1stFlrSF','TotalBsmtSF')\n\n* The above pairs are multicollinear. We will kee only one in each pair.We do this on an assumption that we are using Linear regression.For ridge they will automatically take are of multicollinearity.\n\n\n**How to decide out of two which feature we should remove?**\n\nFor that we will check coorelation with SalePrice and their linearity which is an important assumption for linear regression\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"multicoll_pairs = ['GarageArea','GarageCars',\n        'GarageYrBlt','YearBuilt','TotRmsAbvGrd','GrLivArea',\n                   '1stFlrSF','TotalBsmtSF']\n\nfig,axes = plt.subplots(4,2,figsize=(15,20))\n\ndef plot_two(feat,i,j):\n    sns.regplot(x=df_num[feat], y=df_num['SalePrice'], ax=axes[i,j])\n    sns.scatterplot(y=df_num['SalePrice'],x=df_num[feat],color=('orange'),ax=axes[i,j])   \n    fig.tight_layout(pad=5.0)\n    \n\nfor i,feat in enumerate(multicoll_pairs):\n    j = i%2 #0 or 1\n    plot_two(feat,i//2,j)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# multicoll_pairs.append('SalePrice')\ndf_num.corr()['SalePrice'].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So based on this we will drop 'GarageArea','TotRmsAbvGrd' and 'GarageYrBlt','1stFlrSF'.\n \n* Here we can't see any negative strong coorelation\n\n*  We can see that 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'."},{"metadata":{"trusted":true},"cell_type":"code","source":"# remaning numeric columns(input variables)\ndf_num = df_num.drop(columns = ['1stFlrSF','GarageArea','TotRmsAbvGrd','GarageYrBlt'])\ndf_test_num = df_test_num.drop(columns = ['1stFlrSF','GarageArea','TotRmsAbvGrd','GarageYrBlt'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for Linearity\n\nLinearity of independent variables with target variable is another imporatnt assumption of linear regression. Let us plot features with SalePrice and check\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(16,2,figsize=(15,60))\n\nlinear_num_cols = df_num.select_dtypes(include='number').columns.tolist() \nlinear_num_cols.remove('SalePrice')\n\ndef plot_two(feat,i,j):\n    sns.regplot(x=df_num[feat], y=df_num['SalePrice'], ax=axes[i,j])\n    sns.scatterplot(y=df_num['SalePrice'],x=df_num[feat],color=('orange'),ax=axes[i,j])   \n    fig.tight_layout(pad=5.0)\n    \n\nfor i,feat in enumerate(linear_num_cols):\n    j = i%2 #0 or 1\n    plot_two(feat,i//2,j)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are many non linear features.Some behave like categorical features.\n\nNote: If there are some features which exhibit little bit of non linearity, but having strong coorelation with target, we keep it\n\nNon linear features that can be converted to categorical: 'YrSold','MSSold','PoolArea','BsmtFullBath','BsmtHalfBath','Halfbath','BedroomAbvGvr','Fireplaces',\n\nNon-linear features that we will drop 'OverallCond','LowQualFinSF', 'MiscVal',\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num = df_num.drop(columns=['OverallCond','LowQualFinSF', 'MiscVal'])\ndf_test_num = df_test_num.drop(columns=['OverallCond','LowQualFinSF', 'MiscVal'])\nprint('Train/test numeric shapes')\nprint(df_num.shape)\nprint(df_test_num.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing outliers\n# removing outliers\n# removing some outliers\ndf_num = df_num[df_num['LotFrontage'] < 300]\ndf_num = df_num[df_num['BsmtFinSF1'] < 5000]\ndf_num = df_num[df_num['TotalBsmtSF'] < 6000]\ndf_num = df_num[df_num['GrLivArea'] < 4600]\ndf_num = df_num[df_num['SalePrice'] < 700000]\n\nprint(df_num.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Converting some numeric variables to categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"non_linear_cat_cols = ['YrSold','MoSold','PoolArea','BsmtFullBath',\n            'BsmtHalfBath','HalfBath','BedroomAbvGr','Fireplaces']\n\ndf_num = df_num.drop(columns = non_linear_cat_cols)\ndf_test_num = df_test_num.drop(columns = non_linear_cat_cols)\n\nfor col in non_linear_cat_cols:\n    df_train[col] = df_train[col].astype(object)\n    df_test[col] = df_train[col].astype(object)\n    \nprint(df_num.shape)\nprint(df_test_num.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_num.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing values in numeric features\ndef missing_cols(df):\n    cols = df.columns[df.isna().any()].tolist()\n    print(f'Columns | Percentage missing')\n    for column in cols:\n        percent = round((sum(df[column].isnull())/df.shape[0])*100,2)\n        print(f'{column} : {percent}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check missing values in numeric columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_cols(df_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_cols(df_test_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets convert the target variable and keep\ntarget = np.log1p(df_num['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA on Categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# helper functions\ndef categories_plot(df,col,xlabel='Values',size=(8,4)):\n    y_train = df[col].value_counts().values\n    x_train = df[col].value_counts().index.tolist()\n    plt.figure(figsize=size)\n    plt.title(col)\n    sns.barplot(x_train,y_train)\n    plt.xlabel(xlabel)\n    plt.xticks(rotation=90, ha='right')\n    plt.ylabel('count')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_idx = df_num.index.to_list()\ncat_cols = df_train.select_dtypes(exclude=[np.number]).columns.tolist()\n\ndf_cat = df_train.loc[num_idx][cat_cols]\ndf_cat_test = df_test[cat_cols]\nprint('Train/test categoric shapes')\nprint(df_cat.shape)\nprint(df_cat_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us check for the missing values first"},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_cols(df_cat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_cols(df_cat_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observations:\n\n* We have missing values in both train and test data\n\n* In categorical columns we can clearly see that Alley,MiscFeature,PoolQC,Fence has more than 80% missing values and hence we drop them.\n\n* For rest of the categorical fetures we will do imputation of max repeating value on later section.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dropping some features\ndf_cat = df_cat.drop(columns=['Alley','MiscFeature','PoolQC','Fence'])\ndf_cat_test = df_cat_test.drop(columns=['Alley','MiscFeature','PoolQC','Fence'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us go through different categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#MSZoning Identifies the general zoning classification of the sale. \ncategories_plot(df_cat,'MSZoning')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After much exploration I found that In some columns one category is highly dominating.We will see those features and make some modifications"},{"metadata":{"trusted":true},"cell_type":"code","source":"modified_cols = ['PoolArea','Street','MasVnrType','RoofMatl','Utilities']\n\nfor col in modified_cols:\n    categories_plot(df_cat,col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some feature engineering on cat features\n\nmodified_cols = ['PoolArea','Street','MasVnrType','RoofMatl']\n\n\ndf_cat = df_cat.drop(columns = ['Utilities'])\ndf_cat_test = df_cat_test.drop(columns = ['Utilities'])\n\ndf_cat['PoolArea'] = df_cat['PoolArea'].apply(lambda x: 'Y' if x>1 else 'N') \ndf_cat_test['PoolArea'] = df_cat_test['PoolArea'].apply(lambda x: 'Y' if x>1 else 'N')\n\ndf_cat['Street'] = df_cat['Street'].apply(lambda x: 'Pave' if x == 'Pave' else 'No Pave')\ndf_cat_test['Street'] = df_cat_test['Street'].apply(lambda x: 1 if x == 'Pave' else 0)\n\ndf_cat['MasVnrType'] = df_cat['MasVnrType'].apply(lambda x: 'N' if x == 'None' else 'Y')\ndf_cat_test['MasVnrType'] = df_cat_test['MasVnrType'].apply(lambda x: 'N' if x == 'None' else 'Y')\n\ndf_cat['RoofMatl'] = df_cat['RoofMatl'].apply(lambda x: 'CompShg' if x == 'CompShg' else 'Other')\ndf_cat_test['RoofMatl'] = df_cat_test['RoofMatl'].apply(lambda x: 'CompShg' if x == 'CompShg' else 'Other')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Concating numeric and categorical features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving coumn names\ncat_cols = df_cat.columns.to_list()\nnum_cols = df_num.columns.to_list()\n\ndf_test_num = df_test_num.reset_index(drop=True)\ndf_num = df_num.reset_index(drop=True)\ndf_cat = df_cat.reset_index(drop=True)\ndf_test_cat = df_cat_test.reset_index(drop=True)\n\nfinal_train = pd.concat([df_num,df_cat],axis=1)\nfinal_test = pd.concat([df_test_num,df_test_cat],axis=1)\n\nprint('Final shapes:')\nprint(final_train.shape)\nprint(final_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# apply box cox transform to features having skweness > 0.5\ndef sqrt_skew(df):\n    \n    sk_feats = df.apply(lambda x: stats.skew(x)).sort_values(ascending=False)\n    high_skew = sk_feats[abs(sk_feats) > 0.5].index\n    for feat in high_skew:\n#         df[feat] = boxcox1p(df[feat], boxcox_normmax(df[feat] + 1))\n          df[feat] = np.sqrt(df[feat])\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ncols = num_cols[:-1]\n# final_train[ncols] = sqrt_skew(final_train[ncols])\n# final_test[ncols] = sqrt_skew(final_test[ncols])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"Y = target.values\nX = final_train\nx_test = final_test.copy()\nx_train,x_cv,y_train,y_cv = train_test_split(X,Y,train_size=0.7,random_state=100)\nprint('Train cv data shape')\nprint(x_train.shape)\nprint(x_cv.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing values in numeric features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputing numeric values\n# Featurization of numeric data\n# df_num = df_num.drop(columns=['SalePrice'])\n\nnum_cols_in = df_num.columns.to_list()\nnum_cols_in.remove('SalePrice')\n\nimputer = SimpleImputer(strategy='median')\nx_train_num = imputer.fit_transform(x_train[num_cols_in])\nx_cv_num = imputer.transform(x_cv[num_cols_in])\nx_test_num = imputer.transform(x_test[num_cols_in])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalization\n\nEventhough there is no assumption that in linear regression input data is to be normalized, it is a good option to normalize beacuse,\n\n* MSE is prone to outliers\n* Normalization helps for faster convergence\n* Helps to remove skweness\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Normalizing\nscaler = RobustScaler()\nx_train_num = scaler.fit_transform(x_train_num)\nx_cv_num = scaler.transform(x_cv_num)\nx_test_num = scaler.transform(x_test_num)\n\ndf_num = pd.DataFrame(x_train_num, columns=num_cols_in)\ndf_cv_num = pd.DataFrame(x_cv_num, columns=num_cols_in)\ndf_test_num = pd.DataFrame(x_test_num, columns=num_cols_in)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing values in categoric features"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# cat_cols = df_cat.columns.to_list()\n\n# # missing values in df\n# imputer = SimpleImputer(strategy='constant', fill_value='MISSING')\n# df_cat = imputer.fit_transform(df_cat[cat_cols])\n# df_cat_test = imputer.transform(df_cat_test[cat_cols])\n\n# df_cat = pd.DataFrame(df_cat, columns=cat_cols)\n# df_cat_test = pd.DataFrame(df_cat_test, columns=cat_cols)\n\nx_train_cat = x_train[cat_cols]\nx_cv_cat = x_cv[cat_cols]\nx_test_cat = x_test[cat_cols]\n\nfor col in cat_cols:\n    val = x_train_cat[col].mode()[0]\n    x_train_cat[col] = x_train_cat[col].fillna(val)\n    x_cv_cat[col] = x_cv_cat[col].fillna(val)\n    x_test_cat[col] = x_test_cat[col].fillna(val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Onehot encoding categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_cat_dummy = pd.get_dummies(x_train_cat, columns=cat_cols,drop_first=True)\ndf_cv_cat_dummy = pd.get_dummies(x_cv_cat, columns=cat_cols,drop_first=True)\ndf_test_cat_dummy = pd.get_dummies(x_test_cat, columns=cat_cols,drop_first=True)\nprint(df_cat_dummy.shape)\nprint(df_cv_cat_dummy.shape)\nprint(df_test_cat_dummy.shape)\n\ndf_cat, df_cat_cv = df_cat_dummy.align(df_cv_cat_dummy, join='left', axis=1) \ndf_cat, df_cat_test = df_cat_dummy.align(df_test_cat_dummy, join='left', axis=1) \ndf_cat_test = df_cat_test.fillna(0)\ndf_cat_cv = df_cat_cv.fillna(0)\n\nprint('dummy categorical data shapes after aligning with train data')\nprint(df_cat.shape)\nprint(df_cat_cv.shape)\nprint(df_cat_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reseting index\ndf_cat_dummy = df_cat.reset_index(drop=True)\ndf_cv_cat_dummy = df_cat_cv.reset_index(drop=True)\ndf_test_cat_dummy = df_cat_test.reset_index(drop=True)\ndf_num = df_num.reset_index(drop=True)\ndf_cv_num = df_cv_num.reset_index(drop=True)\ndf_test_num = df_test_num.reset_index(drop=True)\n\nfinal_train = pd.concat([df_num,df_cat_dummy],axis=1)\nfinal_cv = pd.concat([df_cv_num,df_cv_cat_dummy],axis=1)\nfinal_test = pd.concat([df_test_num,df_test_cat_dummy],axis=1)\n\nprint('Final shapes:')\nprint(final_train.shape)\nprint(final_cv.shape)\nprint(final_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">*Note: It is notable that, In most of the kernels,they have used fitting and transforming whole data first and then splitting to train and cross validation. I think that this can cause data leakage issue as we have to handle cross validation and test data as totally unseen.That means we have to first split data to train, cv and test.Then fit on train and transform on cv and test.\nFor example in one hot encoding, train,cv and test should only contain categories from train.\n*"},{"metadata":{},"cell_type":"markdown","source":"# ML Models"},{"metadata":{},"cell_type":"markdown","source":"## Linear regression\n\nYou can read more about linear regression and metrics [here](http://https://www.kaggle.com/masumrumi/a-detailed-regression-guide-with-house-pricing)"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = final_train.copy()\nx_cv = final_cv.copy()\nx_test = final_test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linear = LinearRegression()\nlinear.fit(x_train,y_train)\n\ny_pred_train = linear.predict(x_train)\ny_pred_cv = linear.predict(x_cv)\n\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(y_cv, y_pred_cv)))) \nprint('R2 score = ' + str(r2_score(y_cv,y_pred_cv)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nWith only numerical columns \nRoot Mean Square Error train = 0.13270971547191085\nRoot Mean Square Error test = 0.14181733405952673\n\n# with categorical cols\nRoot Mean Square Error train = 0.0864434894176389\nRoot Mean Square Error test = 0.137230822787648\nR2 score = 0.888797531197542\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ridge Regression\nIf you wish to read more on regularization [here](http://https://medium.com/@arunm8489/an-overview-on-regularization-f2a878507eae)"},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge()\nridge.fit(x_train,y_train)\n\ny_pred_train = ridge.predict(x_train)\ny_pred_cv = ridge.predict(x_cv)\n\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(y_cv, y_pred_cv)))) \nprint('R2 score = ' + str(r2_score(y_cv,y_pred_cv)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nRoot Mean Square Error train = 0.08799576547555985\nRoot Mean Square Error test = 0.12886916737699786\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lasso"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso()\nlasso.fit(x_train,y_train)\n\ny_pred_train = lasso.predict(x_train)\ny_pred_cv = lasso.predict(x_cv)\n\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(y_cv, y_pred_cv)))) \nprint('R2 score = ' + str(r2_score(y_cv,y_pred_cv)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature selection using Lasso(This is done just for demonstaration purpose)\n* Let us check how we can do feature selection using Lasso\n* Default alpha value is 1, using it will select minimum number of features(I end up with 1 feature).So I choose a lower value for alpha, so that it does not penalizes weights much."},{"metadata":{"trusted":true},"cell_type":"code","source":"\nlasso = Lasso(alpha=0.05)\nlasso.fit(x_train,y_train)\n\ny_pred_train = lasso.predict(x_train)\ny_pred_cv = lasso.predict(x_cv)\n\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(y_cv, y_pred_cv)))) \nprint('R2 score = ' + str(r2_score(y_cv,y_pred_cv)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coef = pd.Series(lasso.coef_, index = x_train.columns).sort_values()\nimp_coef = pd.concat([coef.head(10), coef.tail(10)])\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: We got 5 important features. 'OverallQual','GrLivArea','TotalBsmtSF','GarageCars' and 'LotArea'."},{"metadata":{},"cell_type":"markdown","source":"## Elastic Net"},{"metadata":{"trusted":true},"cell_type":"code","source":"el = ElasticNet()\nel.fit(x_train,y_train)\n\ny_pred_train = el.predict(x_train)\ny_pred_cv = el.predict(x_cv)\n\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(y_cv, y_pred_cv)))) \nprint('R2 score = ' + str(r2_score(y_cv,y_pred_cv)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross validation on Ridge regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range (-2, 3):\n    alpha = 10**i\n    rm = Ridge(alpha=alpha)\n    ridge_model = rm.fit(x_train, y_train)\n    preds_ridge = ridge_model.predict(x_cv)\n\n    plt.scatter(preds_ridge, y_cv, alpha=.75, color='b')\n    plt.xlabel('Predicted Price')\n    plt.ylabel('Actual Price')\n    plt.title('Ridge Regularization with alpha = {}'.format(alpha))\n    overlay = 'R^2 is: {}\\nRMSE is: {}'.format(\n                    ridge_model.score(x_cv, y_cv),\n                    np.sqrt(mean_squared_error(y_cv, preds_ridge)))\n    plt.annotate(s=overlay,xy=(12.1,10.6),size='x-large')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = Ridge(alpha=10)\nridge.fit(x_train,y_train)\n\ny_pred_train = ridge.predict(x_train)\ny_pred_cv = ridge.predict(x_cv)\n\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(y_cv, y_pred_cv)))) \nprint('R2 score = ' + str(r2_score(y_cv,y_pred_cv)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will try to interpret the coefficients"},{"metadata":{"trusted":true},"cell_type":"code","source":"coef = pd.Series(ridge.coef_, index = x_train.columns).sort_values()\nimp_coef = pd.concat([coef.head(10), coef.tail(10)])\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Model\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How to interpret these coefficients?\n\nIts rather simple.However I will explain with an example.Take case of GrLivArea. Its coefficient is 0.150052. It means that for 1 unit increase in **GrLivArea**, SalePrice increase by  0.150052 units."},{"metadata":{},"cell_type":"markdown","source":"Now we will check homoscedacity and uniform distribution of residuals with fitted value(Assumptions)."},{"metadata":{"trusted":true},"cell_type":"code","source":"x_plot = plt.scatter(y_pred_cv, (y_pred_cv - y_cv), c='b')\nplt.title('Residual plot')\nplt.show()\nsns.distplot((y_pred_cv - y_cv))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Future Scope\n\n* Our model is slightly overfitting.We can overcome this if we use more advanced models like randomforest,xgboost etc. Soon I will include those in my kernel.\n\n* There is lot of scope for feature engineering which can improve our result.\n\n* Feel free to correct me if i had done any mistakes."},{"metadata":{},"cell_type":"markdown","source":"### If you find my kernel useful do not forget to Upvote.\n\nSoon I will incoperate more details in this kernel. Until then **Happy Machine Learning!**"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}