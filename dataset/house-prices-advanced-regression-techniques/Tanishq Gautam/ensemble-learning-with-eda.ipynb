{"cells":[{"metadata":{},"cell_type":"markdown","source":"# What is EDA?\nExploratory Data Analysis (EDA) is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to :                                                                                                  \n* Maximize insight into a data set;\n* Uncover underlying structure;\n* Extract important variables;\n* Detect outliers and anomalies;\n* Test underlying assumptions;\n* Develop parsimonious models; and\n* Determine optimal factor settings.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# What is Ensemble Learning?\n> An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. The trained ensemble, therefore, represents a single hypothesis. This hypothesis, however, is not necessarily contained within the hypothesis space of the models from which it is built. Thus, ensembles can be shown to have more flexibility in the functions they can represent. This flexibility can, in theory, enable them to over-fit the training data more than a single model would, but in practice, some ensemble techniques (especially bagging) tend to reduce problems related to over-fitting of the training data.\n\n> Empirically, ensembles tend to yield better results when there is a significant diversity among the models.Many ensemble methods, therefore, seek to promote diversity among the models they combine.Although perhaps non-intuitive, more random algorithms (like random decision trees) can be used to produce a stronger ensemble than very deliberate algorithms (like entropy-reducing decision trees).Using a variety of strong learning algorithms, however, has been shown to be more effective than using techniques that attempt to dumb-down the models in order to promote diversity.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# *Please upvote the kernel if you find it insightful!*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries","execution_count":null},{"metadata":{"_cell_guid":"65a44fc9-763e-4344-80b6-dacc0f37856f","_uuid":"0435c5b157c6686500b877eba8026674f9359c0f","trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport numpy as np \nimport pandas as pd \nimport random as rnd\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\npd.options.display.max_columns = 100\npd.options.display.max_rows = 100\nmatplotlib.style.use('ggplot')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load train and test data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eaaa473b-b5fd-4f63-9c74-7740fdd7a779","_uuid":"6f29fb5777df0e86b77d23544207e363a6a66b22","trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"da6378da-7e3b-49e7-9020-5f2d8e2c7b78","_uuid":"89dfb4494d3d3c133ca504d71ad74d2b13832c2f"},"cell_type":"markdown","source":"**Getting Correlation between variables**","execution_count":null},{"metadata":{"_cell_guid":"c4ac9738-f616-4a19-b33b-fe8192e30965","_uuid":"55e2387dfcf9277d5a3b3bb10823396846ef3b15","trusted":true},"cell_type":"code","source":"corr = train.select_dtypes(include = ['float64', 'int64']).iloc[:, 1:].corr()\nplt.figure(figsize=(12, 12))\nsns.heatmap(corr, vmax=1, square=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d7fb7932-5832-4851-873f-5b8cc7f74356","_uuid":"ac91f15b16d3011f94a83559d504389fba23cd2c"},"cell_type":"markdown","source":"**Top 20 variables correlated with SalePrice with score**","execution_count":null},{"metadata":{"_cell_guid":"722ce684-113f-4039-a4b3-f472b7310895","_uuid":"25e2f1dcd353174360309fe441e0ffd52bfde578","trusted":true},"cell_type":"code","source":"k = 20 #number of variables for heatmap\ncols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nplt.figure(figsize=(12, 12))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9e4a6068-a852-4bac-8246-72cee8ce511c","_uuid":"d366b71771079dada912432092f10891b2965c47"},"cell_type":"markdown","source":"**Finding outliers in GrLivArea**","execution_count":null},{"metadata":{"_cell_guid":"d00c188f-4c37-4f2e-a68e-e43f876cee7c","_uuid":"0e341d0f370f9c26caec79fe7726ec7261f07acb","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5a850361-66ec-49e8-ae5b-a2f96e1df753","_uuid":"9d099e7b0e35f15abd7eb1a15c82fbceac1d5423"},"cell_type":"markdown","source":"**Deleting the 2 outliers in bottom right**","execution_count":null},{"metadata":{"_cell_guid":"2bf071d5-1f66-4d30-841d-8129c465907a","_uuid":"d3835ff6936a77f84fed17140c95d29e1afe70a5","trusted":true},"cell_type":"code","source":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ffa374a2-0504-4352-9389-392638239afc","_uuid":"8d27f0c06e0ef99329e731201e4d48aab1a6072a"},"cell_type":"markdown","source":"**Finding Skewness in SalePrice**","execution_count":null},{"metadata":{"_cell_guid":"e762370d-f1d1-4420-961d-5f5607617c0c","_uuid":"565d880bb96e6e0325439ef1da323bc39f0efb7a","trusted":true},"cell_type":"code","source":"from scipy.stats import norm, skew\nfrom scipy import stats\nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0ee891de-b7c5-4a97-ba4f-bbc2c4dd4e0b","_uuid":"6469d68517339faeaf916d34ffafe3689c1090f1"},"cell_type":"markdown","source":"**So SalePrice is skewed and it needs to be normally distributed.**","execution_count":null},{"metadata":{"_cell_guid":"438a2db9-0a80-4d8d-8625-7711cfcc6adc","_uuid":"4b92ed827b572f9e88b46e2d966bd612975d5743","trusted":true},"cell_type":"code","source":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e06b9233-7304-440f-99b7-dff04340168e","_uuid":"980833ccc4d8cfcd424870b1914187fcb83ba1ec","trusted":true},"cell_type":"code","source":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0ef23d44-d48f-4e0d-aa86-c8778a121fd8","_uuid":"3f1064e2f2252548ce49e976bbf59ef982686dc0"},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{"_cell_guid":"7156ba0b-4efe-4b9c-b6b8-30dc4d3e6a87","_uuid":"7f33e657a04a8a11565b2e842091a0beb95f324d","trusted":true},"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\n\n# get the targets\ny_train_sale = train.SalePrice.values\n\n# combine train and test\ncombined = pd.concat((train, test)).reset_index(drop=True)\ncombined.drop(['SalePrice'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c850b167-968d-49ba-b500-2ffa99d35b7a","_uuid":"bc244c62c95e547bf775b6397a1b25d8caf39c11"},"cell_type":"markdown","source":"**Finding features with NA values**","execution_count":null},{"metadata":{"_cell_guid":"d8aa9ebc-8508-4192-8472-ce63044050c9","_uuid":"c84f3be23558ca57747e677aade6c07f429c2921","trusted":true},"cell_type":"code","source":"all_data_na = (combined.isnull().sum() / len(combined)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"de8b1747-eecd-4449-a739-0b5bacecc7de","_uuid":"4b327602a9f3764b6773e2a937ddafbd4d3c5f5f","trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"63c467c4-cf72-46dc-b8ae-a5d682265c4c","_uuid":"9b4b8509c13590a14cebaf9d327129a111dfa8cd"},"cell_type":"markdown","source":"**Filling NAs**                                                                                                          \nUsing the variable description file provided, the features with missing values can be filled as below.","execution_count":null},{"metadata":{"_cell_guid":"16068016-939e-47bd-8de4-ffb7bb489b31","_uuid":"e7fe980adde87810eceb7dc4b75de5f0e2b55881","trusted":true},"cell_type":"code","source":"combined['MasVnrArea'] = combined['MasVnrArea'].fillna(0.0)\ncombined[\"MasVnrType\"] = combined[\"MasVnrType\"].fillna(\"None\")\ncombined['LotFrontage'] = combined['LotFrontage'].fillna(combined['LotFrontage'].median())\ncombined['BsmtFinSF1'] = combined['BsmtFinSF1'].fillna(0.0)\ncombined['BsmtFinSF2'] = combined['BsmtFinSF2'].fillna(0.0)\ncombined['BsmtUnfSF'] = combined['BsmtUnfSF'].fillna(0.0)\ncombined['TotalBsmtSF'] = combined['TotalBsmtSF'].fillna(0.0)\ncombined['BsmtFullBath'] = combined['BsmtFullBath'].fillna(0)\ncombined['BsmtHalfBath'] = combined['BsmtHalfBath'].fillna(0)\ncombined['GarageYrBlt'] = combined['GarageYrBlt'].fillna(0)\ncombined['GarageCars'] = combined['GarageCars'].fillna(0)\ncombined['GarageArea'] = combined['GarageArea'].fillna(0)\ncombined['GarageFinish'] = combined['GarageFinish'].fillna('None')\n\n# using the most frequent zone\ncombined['MSZoning'] = combined['MSZoning'].fillna(combined['MSZoning'].mode()[0])\n\ncombined = combined.drop(['Utilities'], axis=1)\n\n# most common functionality\ncombined[\"Functional\"] = combined[\"Functional\"].fillna(\"Typ\")\n\ncombined['Electrical'] = combined['Electrical'].fillna(combined['Electrical'].mode()[0])\ncombined['KitchenQual'] = combined['KitchenQual'].fillna(combined['KitchenQual'].mode()[0])\ncombined['Exterior1st'] = combined['Exterior1st'].fillna(combined['Exterior1st'].mode()[0])\ncombined['Exterior2nd'] = combined['Exterior2nd'].fillna(combined['Exterior2nd'].mode()[0])\ncombined['SaleType'] = combined['SaleType'].fillna(combined['SaleType'].mode()[0])\ncombined['MSSubClass'] = combined['MSSubClass'].fillna(\"None\")\ncombined['PoolQC'] = combined['PoolQC'].fillna('None')\ncombined['MiscFeature'] = combined['MiscFeature'].fillna('None')\ncombined['Alley'] = combined['Alley'].fillna('None')\ncombined['Fence'] = combined['Fence'].fillna('None')\ncombined['FireplaceQu'] = combined['FireplaceQu'].fillna('None')\ncombined[\"LotFrontage\"] = combined.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    combined[col] = combined[col].fillna('None')\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    combined[col] = combined[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5f5ecab1-5366-4196-8729-bf3f05cd3026","_uuid":"458eadc81e7cf912f35a5f07e942cc375f5dbf3e"},"cell_type":"markdown","source":"**Label Encoding of some categorical features**","execution_count":null},{"metadata":{"_cell_guid":"da336f0a-78ee-4026-84bf-a5e501e79dbd","_uuid":"2577ca6ce1ff5a375c6f1e4b0817404d2c0ff2d5","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(combined[c].values)) \n    combined[c] = lbl.transform(list(combined[c].values))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ca7b436e-4e47-448f-b044-e8214d66023a","_uuid":"7546a9df716c627a140c462fcb03617d4fbb81f6"},"cell_type":"markdown","source":"**Combining all area features to a single feature**","execution_count":null},{"metadata":{"_cell_guid":"901acac7-2c25-465e-bfc9-8a1d3287a932","_uuid":"736577173c5ac87ea3d8e79d9e6a00480f5a27b1","trusted":true},"cell_type":"code","source":"combined['TotalSF'] = combined['TotalBsmtSF'] + combined['1stFlrSF'] + combined['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77ae6115-9b90-4057-8084-7baf6b0d32ed","_uuid":"f99608c6efa04806e47c281084de5924c879d8fe"},"cell_type":"markdown","source":"**Generating Dummies**","execution_count":null},{"metadata":{"_cell_guid":"33ec1a80-3130-4b45-9c7d-cb532319b076","_uuid":"4b1d8b410bc8bc2c17a3cfa6b6e27152495198ef","trusted":true},"cell_type":"code","source":"combined = pd.get_dummies(combined)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e253100-c913-411e-aaee-936474221057","_uuid":"53796711b586d40756773922e66a21dd3b58e75c","trusted":true},"cell_type":"code","source":"train = combined[:ntrain]\ntest = combined[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split the data into Train and Test","execution_count":null},{"metadata":{"_cell_guid":"ef3b8631-4358-4399-9036-15a2f6847be9","_uuid":"2b3972dbf1bfb1ec329cdec45e450f1a548794b6","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split, cross_val_score\nx_train, x_test, y_train, y_test = train_test_split(train, y_train_sale, test_size=0.1, random_state=200)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import libraries for Ensemble modeling","execution_count":null},{"metadata":{"_cell_guid":"908a4b88-d6a6-4e45-9134-d7822409649f","_uuid":"0b8659d9e445e6c9c7198c213d55aa935a6ea43a","trusted":true},"cell_type":"code","source":"from sklearn import ensemble, tree, linear_model\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e72db85c-31ea-4ea0-a4df-738d0669eb76","_uuid":"dc7048a3af38b6c0241d533bc4c20136290c5d08"},"cell_type":"markdown","source":"# Function for Scoring, Training and Testing","execution_count":null},{"metadata":{"_cell_guid":"e347babe-fabf-45fc-ba47-8c82f5d081a2","_uuid":"516e30ede6bfa9f2e8bd675e050e2c1f6d509757","trusted":true},"cell_type":"code","source":"def get_score(prediction, lables):    \n    print('R2: {}'.format(r2_score(prediction, lables)))\n    print('RMSE: {}'.format(np.sqrt(mean_squared_error(prediction, lables))))\n\ndef train_test(estimator, x_trn, x_tst, y_trn, y_tst):\n    prediction_train = estimator.predict(x_trn)\n    \n    get_score(prediction_train, y_trn)\n    prediction_test = estimator.predict(x_tst)\n    \n    get_score(prediction_test, y_tst)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8e09fb9a-089e-42b1-a2b0-b0806bcdc8e3","_uuid":"e0ce7fa8778c1230ba6ea023c9e0166c727771eb"},"cell_type":"markdown","source":"# Ensembling","execution_count":null},{"metadata":{"_cell_guid":"360e2a1e-6b23-4bf8-944f-c6bff4a7f075","_uuid":"f6ae3af84110a9ec020626562cae1c0c5a8d1141"},"cell_type":"markdown","source":"**Gradient Boosting Regressor**\n> Gradient boosting is a machine learning technique for regression problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n\n> GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.","execution_count":null},{"metadata":{"_cell_guid":"1bdfbc9d-587b-4f0b-bd8a-99894940d68c","_uuid":"18dc0f2e24c63834eddae200e890010549507634","trusted":true},"cell_type":"code","source":"GBR = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=3, max_features='sqrt',\n                                               min_samples_leaf=15, min_samples_split=10, loss='huber').fit(x_train, y_train)\ntrain_test(GBR, x_train, x_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c9259d9-9653-4d36-9946-0fea7784979e","_uuid":"d0a1f688bc77460bc6701a86a0c24f06b082c322"},"cell_type":"markdown","source":"**Lasso**\n> Lasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.","execution_count":null},{"metadata":{"_cell_guid":"d48db64b-9fdf-4dd1-af9a-e48d35f37bac","_uuid":"1927628cd5ff0b59f21801d25e23760a11d25878","trusted":true},"cell_type":"code","source":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0007000000000000001, random_state=1)).fit(x_train, y_train)\ntrain_test(lasso, x_train, x_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ba5141e8-b6e5-4744-9965-883265b3f85c","_uuid":"ff2de9a9644b6de4d77a859231b4d29c593033d4"},"cell_type":"markdown","source":"**Light GBM**\n> Light GBM is a gradient boosting framework that uses tree based learning algorithm.Light GBM is prefixed as ‘Light’ because of its high speed. It can handle the large size of data and takes lower memory to run.\n\n> It grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.","execution_count":null},{"metadata":{"_cell_guid":"d55b5df1-1b6f-43cd-8cd0-ff5440f7b333","_uuid":"c304d34d8ca1781c16e21e002ba230860b238b6c","trusted":true},"cell_type":"code","source":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11).fit(x_train, y_train)\ntrain_test(model_lgb, x_train, x_test, y_train, y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling ","execution_count":null},{"metadata":{"_cell_guid":"6ed381c9-008d-43ea-af01-2abbfa0d3e98","_uuid":"7677638fba8f7beb7aceb787ea874c1941c590c9","trusted":true},"cell_type":"code","source":"GB_model = GBR.fit(train, y_train_sale)\ngbr_labels = np.expm1(GB_model.predict(test))\n\nlasso_model = lasso.fit(train, y_train_sale)\nlasso_labels = np.expm1(lasso_model.predict(test))\n\nlgb_model = model_lgb.fit(train, y_train_sale)\nlgb_labels = np.expm1(lgb_model.predict(test))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b974f6cd-5123-4d21-ba2b-556ef28b1664","_uuid":"c221e71856ad84d191ecadd92e518281c0c5fd22","trusted":true},"cell_type":"code","source":"# scores decided on testing for a few values\noutput = lgb_labels*0.50 + lasso_labels*0.25 + gbr_labels*0.25","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d7ed7365-b42e-4f3d-8b66-41b0c488b9d9","_uuid":"e5b2446ef3a7cd4cb7057e0e645929169f46d3ff","trusted":true},"cell_type":"code","source":"pd.DataFrame({'Id': test.Id, 'SalePrice': output}).to_csv('submission.csv', index =False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}