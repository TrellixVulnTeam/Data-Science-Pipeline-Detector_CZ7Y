{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport pandas as pd,numpy as np\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch import optim\nfrom torch import nn\nimport os\nimport matplotlib.pyplot as plt\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.backends.cudnn.benchmarks =True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading the data houseSalesPrediction\npath = r'../input/house-prices-advanced-regression-techniques'\ndata = pd.read_csv(os.path.join(path,'train.csv'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lets understand the data","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Train Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding out the columns\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the NA columns\ndata = data.replace(to_replace='NA',value=np.NaN)\ndata.dropna(inplace=True,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets take columns with unique values less than 90% or less than total values ( This will contribute to some learning)\n\ndataImpColumns = {columns:data[columns].count() for columns in list(data.columns) if len(data[columns].value_counts())<=0.9*data.shape[0]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Take columns which are only important\ndata = data[list(dataImpColumns.keys())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dict(zip(data['MSZoning'].unique(),range(len(data['MSZoning'].unique()))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#These columns needs to be lable encoded and will be required for Testing Purpose also\nlableEncodedColumnsDict = {column:dict(zip(data[column].unique(),range(len(data[column].unique())))) for column in data.columns if data[column].dtype=='O'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lableEncodedColumnsDict.keys()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets Label Encode the columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.replace(lableEncodedColumnsDict, inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the individual column\nnormalizeDict = {}\ndef normalize(series):\n    global normalizeDict\n    columnName = series.name\n    mean,std = series.mean(),series.std()\n    normalizeDict[columnName] = (mean,std)\n    return (series-mean)/std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Taking Features\ndatax = data[[column for column in data.columns if column!='SalePrice']].apply(normalize,axis=0)\n\n# The Predictor\ndatay = data[['SalePrice']].apply(normalize,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The Scaled Data\ndatascaled = (torch.tensor(datax.values,requires_grad=True).type(torch.float32),torch.tensor(datay.values,requires_grad=True).type(torch.float32))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Will be used to store the scaling factors\nnormalizeDict.keys()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model and Dataset Building","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class datasetclass(Dataset):\n    \n    def __init__(self,):\n        super(datasetclass,self).__init__()\n        self.trainX = torch.Tensor(datascaled[0]).to(device)\n        self.trainY = torch.Tensor(datascaled[1]).to(device)\n\n\n    def __getitem__(self,index):\n        return self.trainX[index],self.trainY[index]\n    \n    def __len__(self,):\n        return len(self.trainX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class modelClass(nn.Module):\n    \n    def __init__(self,inputDim,outputDim):\n        super(modelClass,self).__init__()\n        self.LinearLayer1 = nn.Linear(inputDim,inputDim)\n        self.LinearLayer2 = nn.Linear(inputDim,inputDim)\n        self.LinearLayer3 = nn.Linear(inputDim,inputDim)\n        self.LinearLayer4 = nn.Linear(inputDim,inputDim)\n        self.LinearLayer5 = nn.Linear(inputDim,outputDim)\n        self.ReLULayer1 =  nn.ReLU(inputDim)\n        self.ReLULayer2 = nn.ReLU(inputDim)\n        self.ReLULayer3 = nn.ReLU(inputDim)\n        self.ReLULayer4 = nn.ReLU(inputDim)\n        self.LReLULayer1 = nn.LeakyReLU(inputDim)\n        self.BatchNorm1 = nn.BatchNorm1d(inputDim)\n        self.BatchNorm2 = nn.BatchNorm1d(inputDim)\n        self.BatchNorm3 = nn.BatchNorm1d(inputDim)\n        self.optimizer = torch.optim.Adam(self.parameters(),lr=0.0001)\n        self.lossMSE = nn.modules.MSELoss()\n        self.output = torch.tensor([0.1])\n        \n    def forward(self,x):\n        x = self.LinearLayer1(x)\n        x = self.BatchNorm1(x)\n        x = self.ReLULayer1(x)\n        x = self.LinearLayer2(x)\n        x = self.BatchNorm2(x)\n        x = self.ReLULayer2(x)\n        x = self.LinearLayer3(x)\n        x = self.BatchNorm3(x)\n        x = self.ReLULayer3(x)\n        x = self.LinearLayer4(x)\n        x = self.ReLULayer4(x)\n        x = self.LinearLayer5(x)\n        return x\n    \n    def lossFunc(self,y,yhat):\n        return self.lossMSE(yhat,y)\n    \n    def backward(self,x,y):\n        yhat = self.forward(x)\n        self.output = self.lossFunc(y,yhat)\n        self.output.backward()\n        with torch.no_grad():\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The batch size is 512\nbs = 512\nepochs = 500\ninputDim = len(datascaled[0][0])\noutputDim = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasetObj = datasetclass()\ndata_loader = DataLoader(datasetObj, batch_size=bs, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8 feature is taken into consideration and a single output is compared to the actual to the loss.\nmodelObj = modelClass(inputDim,outputDim)\nmodelObj.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loader = DataLoader(datasetObj, batch_size=bs, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 500 epochs are taken and trained and loss is calculated.\nloss_values = []\nfor epoch in range(epochs):\n    for x,y in data_loader:\n        modelObj.train()\n        modelObj.backward(x,y)\n        running_loss = modelObj.output\n    loss_values.append(running_loss)\n    plt.plot(loss_values)\nprint('epoch is ', epoch , 'Final loss is ',modelObj.output)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Denormalized And Testing Module","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Columns\ndataImpColumnsWithoutSalePrice = list(set(list(dataImpColumns.keys()))-{'SalePrice'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submission is read for comparing the actual\ndataValid = pd.read_csv(os.path.join(path,'test.csv'))\nsampleSubmission = pd.read_csv(os.path.join(path,'sample_submission.csv'))[['SalePrice']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Filling NaN values with 0 , so the input to the model is not NaN\ndataValid = dataValid[dataImpColumnsWithoutSalePrice].fillna(0)\nsampleSubmission = sampleSubmission.fillna(0)\nprint(dataValid.shape,sampleSubmission.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Label Encode for the test sample\ndataValid.replace(lableEncodedColumnsDict, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the columns\ndef normalizeValid(series):\n    columnName = series.name\n    mean,std = normalizeDict[columnName]\n    return (series-mean)/std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the columns\ndatascaledTestX = dataValid.apply(normalizeValid,axis=0)\ndatascaledTestY = sampleSubmission.apply(normalizeValid,axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize the columns\ndatascaledTestX = torch.tensor(datascaledTestX.values).type(torch.float32)\ndatascaledTestY = torch.tensor(datascaledTestY.values).type(torch.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Batch Input the columns\nclass datasetclassValid(Dataset):\n    \n    def __init__(self,):\n        super(datasetclassValid,self).__init__()\n        self.testX = datascaledTestX\n        self.testY = datascaledTestY\n        \n    def __getitem__(self,index):\n        return self.testX[index],self.testY[index]\n    \n    def __len__(self,):\n        return self.testX.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"datasetclassValidObj = datasetclassValid()\ndata_loader_valid = DataLoader(datasetclassValidObj, batch_size=bs, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean,std = normalizeDict['SalePrice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Submission Frame\nsubmit = pd.DataFrame(columns=['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = []\nfor x,y in data_loader_valid:\n    modelObj.eval()\n    prediction = modelObj(x.to(device))*std+mean\n    submit = submit.append(pd.DataFrame(prediction.to(torch.device('cpu')).detach().numpy(),columns=['SalePrice']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('submisson.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Next Steps\n1. Feature importance is required for training the model and increase the accuracy\n2. EDA needs to be done.","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}