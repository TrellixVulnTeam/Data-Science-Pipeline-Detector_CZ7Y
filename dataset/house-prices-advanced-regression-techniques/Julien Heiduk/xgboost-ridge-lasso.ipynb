{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"668bf1c1-1623-dd32-9d87-88f1bae59b2a"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b4b10fd4-7bf5-a005-da4e-8880abb2eaa4"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom IPython.display import display\nfrom scipy.stats import skew\n\n# Load data\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n# First look at the data\nprint(train.shape)\ndisplay(train.head())\n\n#NaN\ntrain.fillna(value=-999.0,inplace=True)\ntest.fillna(value=-999.0,inplace=True)\n\nprint('Type numeric:')\nlist_feature_Nan = []\nfor i in train.select_dtypes(exclude=['object']).columns:\n    if (train[i] == -999.0).astype(int).sum() > 0:\n        print(\"Feature: \", i, \"has\", round(((train[i] == -999.0).astype(int).sum()/1460)*100), \"% of NaN\")\n        list_feature_Nan.append(i)\n\nprint('Type object:') \n\nfor i in train.select_dtypes(include=['object']).columns:\n    if (train[i] == -999.0).astype(int).sum() > 0:\n        print(\"Feature: \", i, \"has\", round(((train[i] == -999.0).astype(int).sum()/1460)*100), \"% of NaN\") \n   "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f0748a80-adcc-6614-25f6-91b032d56b51"},"outputs":[],"source":"#Replace numeric feature by mean\n#train_replace_mean = train\n#test_replace_mean = test\n\nfor i in list_feature_Nan:\n    train[i].replace(-999.0,train[i].mean(),inplace=True)\n    test[i].replace(-999.0,train[i].mean(),inplace=True)\n\n#train = train[np.log(train['SalePrice'])<13.5]    \n#train = train[np.log(train['SalePrice'])>10.60]  \n#train.reset_index(inplace=True,drop=True)\n\ny = np.array(pd.DataFrame(np.log(train.SalePrice)))\ny2 = np.array(pd.DataFrame(np.log(train.SalePrice)))\n\nmean_saleprice = pd.DataFrame(train.groupby(['GrLivArea'])['LotArea'].mean())\nmean_saleprice.columns = ['LotArea_bis']\n\ntrain.drop('SalePrice',axis=1,inplace=True)    \n\n#log transform skewed numeric features:\nall_data = all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],test.loc[:,'MSSubClass':'SaleCondition']))\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna()))\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nprint(skewed_feats)\n\ntrain[skewed_feats] = np.log1p(train[skewed_feats])\ntest[skewed_feats] = np.log1p(test[skewed_feats])\n\ntrain[skewed_feats] = train[skewed_feats].fillna(all_data[skewed_feats].mean())\ntest[skewed_feats] = test[skewed_feats].fillna(all_data[skewed_feats].mean())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"077f00b3-0dbc-f207-4f90-ac55034e3ec4"},"outputs":[],"source":"#Label Encoder\nle = preprocessing.LabelEncoder()\ntrain_str = train.select_dtypes(include=['object'])\ntest_str = test.select_dtypes(include=['object']) \ndisplay(train_str.head())\n\nprint(train_str.columns.values)\n\ntrain.drop(train_str.columns.values,axis=1,inplace=True)\ntest.drop(train_str.columns.values,axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2908a1fe-e11c-b3ed-aed4-cc0c804b0109"},"outputs":[],"source":"train_str_dum = pd.get_dummies(train_str)\ntest_str_dum = pd.get_dummies(test_str)\n\ncolumns_dum = list(set(train_str_dum) & set(test_str_dum))\n\ntrain_str_dum = train_str_dum[columns_dum]\ntest_str_dum = test_str_dum[columns_dum]\n\n#New train and New test\ntrain_flo = train.select_dtypes(exclude=['object'])\ntest_flo = test.select_dtypes(exclude=['object']) \n\nnew_train = pd.merge(train_flo,train_str_dum,left_index=True,right_index=True)\nnew_test = pd.merge(test_flo,test_str_dum,left_index=True,right_index=True)\n\ndisplay(new_train.head())\nprint(new_train.columns.values)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31666b8c-f4ca-249c-98d0-613222bc5aa6"},"outputs":[],"source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.cross_validation import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA, KernelPCA, TruncatedSVD, SparsePCA\n\n#train_clf = new_train.drop('SalePrice',axis=1)\ntrain_clf = new_train.copy()\ntrain_clf.drop('Id',axis=1,inplace=True)\n\nindex = pd.DataFrame(test.Id,columns = ['Id'])\ntest_clf = new_test.drop('Id',axis=1)\n\ntrain_clf2 = train_clf.drop(['LotFrontage','MasVnrArea','GarageYrBlt'],axis=1)\ntrain_clf2 = pd.merge(train_clf2,train[['LotFrontage','MasVnrArea','GarageYrBlt']],left_index=True,right_index=True)\ntrain_clf2['tot_sf'] = train_clf2['TotalBsmtSF'] + train_clf2['GrLivArea']\ntrain_clf2['ratio_fl'] = train_clf2['2ndFlrSF'] / train_clf2['1stFlrSF'] \ntrain_clf2['garage_ex'] = (train_clf2['GarageQual_Gd'] + train_clf2['GarageQual_TA'] + train_clf2['GarageQual_Fa'] + train_clf2['GarageQual_Po']) * (train_clf2['GarageCond_Ex'])\n\nclus = KernelPCA(n_components = 25)\ntrain_clf2_pca = clus.fit_transform(train_clf2)\ntrain_clf3 = pd.merge(train_clf2,pd.DataFrame(train_clf2_pca),left_index=True,right_index=True)\n\ntest_clf2 = test_clf.drop(['LotFrontage','MasVnrArea','GarageYrBlt'],axis=1)\ntest_clf2 = pd.merge(test_clf2,test[['LotFrontage','MasVnrArea','GarageYrBlt']],left_index=True,right_index=True)\ntest_clf2['tot_sf'] = test_clf2['TotalBsmtSF'] + test_clf2['GrLivArea']\ntest_clf2['ratio_fl'] = test_clf2['2ndFlrSF']  / test_clf2['1stFlrSF']\ntest_clf2['garage_ex'] = (test_clf2['GarageQual_Gd'] + test_clf2['GarageQual_TA'] + test_clf2['GarageQual_Fa'] + test_clf2['GarageQual_Po']) * (test_clf2['GarageCond_Ex'])\n\ntest_clf3 = pd.merge(test_clf2,pd.DataFrame(clus.transform(test_clf2)),left_index=True,right_index=True)\n\nindex = pd.DataFrame(test.Id,columns = ['Id'])\n\n\ndef rmse(predictions, targets):\n    return np.sqrt(((predictions - targets) ** 2).mean())\n\ndef score(clf, train_np, random_state, folds, target, length):\n    kf = KFold(n = length , n_folds=folds, shuffle = True, random_state = random_state)\n    for itrain, itest in kf:\n        Xtr, Xte = train_np[itrain], train_np[itest]\n        ytr, yte = target[itrain], target[itest]\n        clf.fit(Xtr, ytr.ravel())\n        pred = pd.DataFrame(clf.predict(Xte))\n        return rmse(yte, pred)\n    return rmse(y, pred)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da8c3734-c95f-c904-d620-76a1372af9ee"},"outputs":[],"source":"from scipy import stats\n\n#Delete z_score > 4 feature = GrLivArea\n#z_score = pd.DataFrame(stats.zscore(pd.DataFrame(y), axis=1))\n#z_score.columns = train_clf3.columns\n#print(z_score[z_score['GrLivArea']>4].index)\n\n#train_clf4 = train_clf3.drop(train_clf3.index[z_score[z_score['GrLivArea']>4].index])\n#y_bis =  pd.DataFrame(y).drop(pd.DataFrame(y).index[z_score[z_score['GrLivArea']>4].index])\n#y_bis_array = np.array(y_bis)\ntrain_clf4 = train_clf3.copy()\n#train_clf4['LotArea'] = np.sqrt(train_clf3['LotArea'])\n\n#train_clf5 = train_clf3.copy()\n#train_clf5['LotArea'] = np.log1p(train_clf3['LotArea'])\n\ntest_clf4 = test_clf3.copy()\n#test_clf4['LotArea'] = np.sqrt(test_clf3['LotArea'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ed9fae0f-7af4-3609-9f93-028e76946216"},"outputs":[],"source":"for i in train_clf4.select_dtypes(include=['float64']).columns:\n    if train_clf4[i].min() > test_clf4[i].min():\n        train_clf4[i][train_clf4[i]==train_clf4[i].min()] = test_clf4[i].min()\n\nfor i in train_clf4.select_dtypes(include=['float64']).columns:\n    if train_clf4[i].min() < test_clf4[i].min():\n        test_clf4[i][test_clf4[i]==test_clf4[i].min()] = train_clf4[i].min()     "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eb7d8937-a54b-18c1-e896-b20d20a8e3ee"},"outputs":[],"source":"for i in train_clf4.select_dtypes(include=['float64']).columns:\n    if train_clf4[i].max() > test_clf4[i].max():\n        test_clf4[i][test_clf4[i]==test_clf4[i].max()] = train_clf4[i].max()\n\nfor i in train_clf4.select_dtypes(include=['float64']).columns:\n    if train_clf4[i].max() < test_clf4[i].max():\n        train_clf4[i][train_clf4[i]==train_clf4[i].max()] = test_clf4[i].max()  "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f32a5774-a24e-b626-95cb-7b724ee1235b"},"outputs":[],"source":"def put_corr_feat(data,test,target):\n    data_new = data.copy()\n    test_new = test.copy()\n    for i in data.columns:\n        corr1=stats.pearsonr(np.array(pd.DataFrame(data[i])),target)[0]\n        corr2=stats.pearsonr(np.array(pd.DataFrame(data[i]**3)),target)[0]\n        if abs(corr2)>abs(corr1)*1.2:\n            print(i)\n            #print(data[i])\n            #print(abs(corr2),'VS',abs(corr1))\n            data_new[i]=np.array(data_new[i])**3\n            #print(data_new[i])\n            test_new[i]=np.array(test_new[i])**3\n    return data_new, test_new\n    \ndata1, test1 = put_corr_feat(train_clf3,test_clf3,y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5bbc93c8-4c9c-f982-28ee-8b39b2189dc2"},"outputs":[],"source":"import xgboost as xgb\nfrom sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import f_regression, SelectKBest\nfrom sklearn.linear_model import Ridge, LassoCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import LarsCV\n\ndef frange(start, stop, step):\n    i = start\n    a = []\n    while i < stop:\n        yield i\n        a.append(i)\n        i += step\n    return a\n\n#Cs1 = list(range(2,10,1))\n#Cs1 = [15,16,17,18,19,20,21]\nCs1 = [5000,6000,7000,8000,9000,10000]\n#Cs1 = [0.6,0.7,0.75,0.8,0.85,0.9]\nres = []\nres1 = []\nres2 = []\nres3 = []\nres4 = []\nCs3=[]\n\ntrain_np = np.array(train_clf3)\n#train_np2 = np.array(data1)\nrobust_scaler = RobustScaler()\n\n#for C in Cs1:\n#    res1.append(score(xgb.XGBRegressor(n_estimators = C, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 ),train_np,random_state = 0, folds = 7, target = y , length = 1450))\n#    res2.append(score(xgb.XGBRegressor(n_estimators = C, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 ),train_np,random_state = 0, folds = 7, target = y , length = 1450))\n    #res3.append(score(LassoCV(alphas = [C, 1, 0.1, 0.01, 0.001, 0.0005]), train_np, random_state = 0, folds = 7, target = y , length = 1450))\n    #res4.append(score(LassoCV(alphas = [C, 1, 0.1, 0.01, 0.001, 0.0005]), train_np2, random_state = 0, folds = 7, target = y , length = 1450))\n    #res1.append(score(xgb.XGBRegressor(n_estimators = 100, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 ),SelectKBest(f_regression, k = 268).fit_transform(train_np,y),random_state = 0, folds = 7, target = y , length = 1450))\n    #res2.append(score(GradientBoostingRegressor(n_estimators = 100,learning_rate=0.005, max_depth = 3, min_samples_split=800,min_samples_leaf = 40,max_features=230,subsample = 0.85,random_state = 0),train_np,random_state = 0, folds = 7, target = y , length = 1450))\n    #res3.append(score(xgb.XGBRegressor(n_estimators = C, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 ),train_np,random_state = 0, folds = 7, target = y , length = 1450))\n    #res3.append(score(MLPRegressor(hidden_layer_sizes=(800, ), activation = 'logistic', random_state = 0),train_np,random_state = 0, folds = C, target = y , length = 1450))\n    #res4.append(score(ExtraTreesRegressor(n_estimators = 700,  max_depth = 12, min_samples_leaf = 5, random_state = 0),train_np,random_state = 0, folds = 7, target = y , length = 1450))\n    #res2.append(score(xgb.XGBRegressor(n_estimators = C, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 ),SelectKBest(f_regression, k = 268).fit_transform(train_np,y),random_state = 0, folds = 7))\n#     res.append(score(LassoCV(alphas = [ 1, 0.1, 0.01, 0.001, 0.0005]),SelectKBest(f_regression, k = C).fit_transform(train_clf3,y),random_state = 0, folds = 7))\n#     res2.append(score(LassoCV(alphas = [ 1, 0.1, 0.01, 0.001, 0.0005]),train_clf3,random_state = 0, folds = 7))\n     #res2.append(score(LassoCV(alphas = [ 1, 0.1, 0.001, 0.0005]),SelectKBest(f_regression, k = 268).fit_transform(train_clf3,y),random_state = 0, folds = 7))\n#for C in Cs2:    \n#    res2.append(score(AdaBoostRegressor(n_estimators = C, random_state = 42, learning_rate = 0.01, base_estimator = xgb.XGBRegressor(max_depth = 8, seed = 0)),train_np,random_state = 0, folds = 10))\n\n#p1, = plt.plot(Cs1, res1,'r-o',label=\"V1\")\n#p2, = plt.plot(Cs1, res2,'b-o',label=\"V2\")\n#p3, = plt.plot(Cs1, res3,'g-o',label=\"V3\")\n#p4, = plt.plot(Cs1, res4,'y-o',label=\"V4\")\n#plt.legend([p1,p2])\n#plt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b59d40b1-3d27-0207-d099-783490d2bbe2"},"outputs":[],"source":"print(res)\nprint(res2)\nprint(res3)\nprint(res4)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"984a0060-f59d-91d4-7fe3-4e00bc802e35"},"outputs":[],"source":"plt.hist(y)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5efe18c7-dfc7-bc12-8b95-c0b1d023dc7a"},"outputs":[],"source":"plt.boxplot(y)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"914ade60-fba3-06af-02e4-1cff44caea70"},"outputs":[],"source":"index = pd.DataFrame(test.Id,columns = ['Id'])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0b1a4c4e-c20d-079a-414a-8c212d50cca6"},"outputs":[],"source":"## import libraries\nimport numpy as np\nnp.random.seed(123)\n\nimport pandas as pd\nimport subprocess\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers.advanced_activations import PReLU\n\nntrain = train_clf4.shape[0]\nsparse_data = []\ntr_te = pd.concat((train_clf4, test_clf4), axis = 0)\n\ntmp = csr_matrix(tr_te)\nsparse_data.append(tmp)\n\nxtr_te = hstack(sparse_data, format = 'csr')\nxtrain = xtr_te[:ntrain, :]\nxtest = xtr_te[ntrain:, :]\n\ndef batch_generator(X, y, batch_size, shuffle):\n    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n    number_of_batches = np.ceil(X.shape[0]/batch_size)\n    counter = 0\n    sample_index = np.arange(X.shape[0])\n    if shuffle:\n        np.random.shuffle(sample_index)\n    while True:\n        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n        X_batch = X[batch_index,:].toarray()\n        y_batch = y[batch_index]\n        counter += 1\n        yield X_batch, y_batch\n        if (counter == number_of_batches):\n            if shuffle:\n                np.random.shuffle(sample_index)\n            counter = 0\n\ndef batch_generatorp(X, batch_size, shuffle):\n    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n    counter = 0\n    sample_index = np.arange(X.shape[0])\n    while True:\n        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n        X_batch = X[batch_index, :].toarray()\n        counter += 1\n        yield X_batch\n        if (counter == number_of_batches):\n            counter = 0\ndef nn_model():\n    model = Sequential()\n    model.add(Dense(10, input_dim = xtrain.shape[1], init = 'he_normal'))\n    model.add(PReLU())\n    model.add(Dropout(0.4))\n    model.add(Dense(5, init = 'he_normal'))\n    model.add(PReLU())\n    model.add(Dropout(0.2))\n    model.add(Dense(1, init = 'he_normal'))\n    model.compile(loss = 'mse', optimizer = 'adadelta')\n    return(model)\n\n## cv-folds\nnfolds = 2\nfolds = KFold(len(y), n_folds = nfolds, shuffle = True, random_state = 111)\n\n## train models\ni = 0\nnbags = 2\nnepochs = 1\npred_oob = np.zeros(xtrain.shape[0])\npred_test = np.zeros(xtest.shape[0])\ndef model():\n    for (inTr, inTe) in folds:\n        xtr = xtrain[inTr]\n        ytr = y[inTr]\n        xte = xtrain[inTe]\n        yte = y[inTe]\n        pred = np.zeros(xte.shape[0])\n        for j in range(nbags):\n            model = nn_model()\n            fit = model.fit_generator(generator = batch_generator(xtr, ytr, 64, True),\n                                      nb_epoch = nepochs,\n                                      samples_per_epoch = xtr.shape[0],\n                                      verbose = 0)\n            pred += model.predict_generator(generator = batch_generatorp(xte, 50, False), val_samples = xte.shape[0])[:,0]\n            score = rmse(np.exp(yte), np.exp(pred))\n            print(score)\n            pred_test += model.predict_generator(generator = batch_generatorp(xtest, 25, False), val_samples = xtest.shape[0])[:,0]\n        pred /= nbags\n        pred_oob[inTe] = pred\n        #score = rmse(yte, pred)\n        i += 1\n        #print('Fold ', i, '- rmse:', score)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b60eb19f-7987-7187-bd14-a4488e718bbf"},"outputs":[],"source":"from sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\n\n#clf = LassoCV(alphas = [ 1, 0.1, 0.001, 0.0005], max_iter = 1000)\nclf = xgb.XGBRegressor(n_estimators = 6000, seed = 0, learning_rate = 0.01, max_depth = 3, subsample = 0.8, colsample_bytree = 0.8, colsample_bylevel = 0.8 )\nclf2 = Ridge(alpha=21)\nclf3 = LassoCV(alphas = [1, 0.1, 0.001, 0.0005])\n#clf4 = GradientBoostingRegressor(n_estimators = 6000,learning_rate=0.005, max_depth = 3, min_samples_split=800,min_samples_leaf = 40,max_features=230,subsample = 0.85,random_state = 0)\n#clf5 = RandomForestRegressor(n_estimators = 750, max_depth = 8, min_samples_leaf = 2, random_state = 0)\nSelect = SelectKBest(f_regression, k = 268)\n\ntrain_fin = np.array(Select.fit_transform(train_clf4,y))\ntest_fin = np.array(Select.transform(test_clf4))\n\ntrain_fin_ridge = np.array(robust_scaler.fit_transform(train_clf3))\ntest_fin_ridge = np.array(robust_scaler.transform(test_clf3))\n\nkf1 = KFold(n = 1450 , n_folds=7, random_state=0, shuffle = True)\n#kf2 = KFold(n = 1450 , n_folds=39, random_state=0, shuffle = True)\n\ni = 0\nres1 = []\nres4 = []\n\nfor itrain, itest in kf1:\n    i = i + 1\n    Xtr, Xte = train_fin[itrain], train_fin[itest]\n    ytr, yte = y[itrain], y[itest]\n    clf.fit(Xtr, ytr.ravel())\n    if i == 1:\n        pred1 = pd.DataFrame(clf.predict(test_fin))\n        print(\"Fold 1 :\", rmse(yte, pd.DataFrame(clf.predict(Xte))))\n        res1.append(rmse(yte, pd.DataFrame(clf.predict(Xte))))\n    if i > 1 :\n        pred1 = pred1 + pd.DataFrame(clf.predict(test_fin))\n        print(\"Fold \",i, \" :\", rmse(yte, pd.DataFrame(clf.predict(Xte))))\n        res1.append(rmse(yte, pd.DataFrame(clf.predict(Xte))))\n\n        \n#clf.fit(train_fin, y)\n#pred_1 = pd.DataFrame(clf.predict(test_fin))\n        \nclf2.fit(train_fin_ridge, y)\npred2 = pd.DataFrame(clf2.predict(test_fin_ridge))\n        \nclf3.fit(train_fin_ridge, y)\npred3 = pd.DataFrame(clf3.predict(test_fin_ridge))\n        \npred_1 = pred1/7 \n\n\nfor i in pred2[np.exp(pred2).values<1].index.values:\n    pred2[pred2.index == i] = pred_1[pred_1.index == i]\n\n\nfor i in pred2[pred2.values>12].index.values:\n    pred2[pred2.index == i] = pred_1[pred_1.index == i]\n    \nprint(np.mean(res1))\n\npred_1 = (pred_1+2*pred2+pred3)/4\npred_2 = (pred_1+pred3)/2\npred_xg = pred_1\n\npred_1.columns = ['SalePrice']\npred_2.columns = ['SalePrice']\npred_xg.columns = ['SalePrice']\npred3.columns = ['SalePrice']\n\npred_final_1 = pd.DataFrame(np.exp(pred_1), index = new_test.index, columns = ['SalePrice'])\npred_final_2 = pd.DataFrame(np.exp(pred_2), index = new_test.index, columns = ['SalePrice'])\npred_Lasso = pd.DataFrame(np.exp(pred3), index = new_test.index, columns = ['SalePrice'])\npred_xg = pd.DataFrame(np.exp(pred_xg), index = new_test.index, columns = ['SalePrice'])\n\n#replace by mean\nfor i in pred_final_1[pred_final_1.values<100].index.values:\n    pred_final_1[pred_final_1.index == i] = 180921.1959\n    \npred_final = pred_final_1\n\npred_submit = pd.merge(index,pred_final,left_index=True,right_index=True)\npred_Lasso_submit = pd.merge(index,pred_Lasso,left_index=True,right_index=True)\npred_xg_submit = pd.merge(index,pred_xg,left_index=True,right_index=True)\npred_xg_lasso_submit = pd.merge(index,pred_final_2,left_index=True,right_index=True)\n\npred_submit.to_csv('XG_Ridge_Lasso.csv',index=False)\npred_Lasso_submit.to_csv('Lasso.csv',index=False)\npred_xg_submit.to_csv('XGB.csv',index=False)\npred_xg_lasso_submit.to_csv('XGB_Lasso.csv',index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}