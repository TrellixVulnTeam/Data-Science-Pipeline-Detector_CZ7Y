{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A FRIENDLY MESSAGE","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"This is my first ever kernel in Kaggle. As nervous as I was, I landed up at top 18% in my first submission. Learned a lot in the process and looking forward to keep going on with other submissions.\n\nI know it is a very common problem set for anyone to submit and upload a kernel but I thought of sharing it anyway to share my approach hoping to get feedbacks from the community on how I can improve. \n\nI have shared my mistakes (commented out) along with my code to showcase my different approaches and the results I got from them. If you are a beginner it makes you see the thought process of other beginners while approaching a problem and become more comfortable with your doubts.\n\nI have taken a very basic approach and I hope you find it useful. If you do, please upvote, it'll just motivate me more to keep trying more and more problems. \n\nHOPING TO HEAR FROM YOU ALL. THANK YOU IN ADVANCE :)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# IMPORTS & GETTING READY","execution_count":null},{"metadata":{"_uuid":"4e9512e5-a416-4d5c-ab08-752ce527eded","_cell_guid":"58a6ba35-a45c-44d6-8126-13f9d75f63dc","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns #data visualization\nfrom scipy import stats\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing as prep\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\nprint('train data shape: ', train.shape, '\\ntest data shape: ', test.shape)\nprint('Reading done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('Id', axis = 1, inplace = True)\ntest.drop('Id', axis = 1, inplace = True)\nprint(\"Dropped redundant column 'Id' from both train and test data\")\n\ntarget = 'SalePrice'\nprint('Target variable saved in a variable for further use!')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c225f4d2-aca0-4fe2-82c6-21541d38a5c7","_cell_guid":"2c81d5cb-3a6e-4a03-b386-e79932cbc545","trusted":true},"cell_type":"code","source":"def getnumcatfeat(df):\n    \n    \"\"\"Returns two lists of numeric and categorical features\"\"\"\n    \n    numfeat, catfeat = list(df.select_dtypes(include=np.number)), list(df.select_dtypes(exclude=np.number))\n    return numfeat, catfeat\n\nnumfeat, catfeat = getnumcatfeat(train)\nnumfeat.remove(target)\n\nprint('Categorical & Numeric features seperated in two lists!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Completing seperation of features wrt their type, the first thing I want to see is how my data looks.\n### Starting off with the target variable.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, a = plt.subplots(nrows=1, ncols=2, figsize = (20,7))\n\n\nsns.distplot(train[target], fit = norm, ax=a[0])\n(mu, sig) = norm.fit(train[target]) \na[0].legend(['$\\mu=$ {:.2f}, $\\sigma=$ {:.2f}, Skew = {:.4f}, Kurtosis = {:.4f}'.format(mu,sig,train[target].skew(),train[target].kurt())])\na[0].set_ylabel('Frequency')\na[0].axvline(train[target].mean(), color = 'Red') \na[0].axvline(train[target].median(), color = 'Green') \na[0].set_title(target + ' distribution')\n\ntemp=np.log1p(train[target])\n\nsns.distplot(temp, fit = norm, ax=a[1])\n(mu, sig) = norm.fit(temp) \na[1].legend(['$\\mu=$ {:.2f}, $\\sigma=$ {:.2f}, Skew = {:.4f}, Kurtosis = {:.4f}'.format(mu,sig,temp.skew(),temp.kurt())])\na[1].set_ylabel('Frequency')\na[1].axvline(temp.mean(), color = 'Red')\na[1].axvline(temp.median(), color = 'Green') \na[1].set_title('Transformed '+ target + ' distribution')\n\ntrain[target] = np.log1p(train[target])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Log transform gives the best result as data has almost exponential tendency.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The target variable is very close to a Normal Distribution now. \n### Moving ahead to another variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = 'OverallQual'\nf,a = plt.subplots(figsize=(8,6))\nsns.boxplot(x= temp, y = target, data = train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data has linear form with a few outliers. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(train[((train[temp]==3) | (train[temp]==4)) & (train[target]<10.75)].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3 outliers removed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = 'GrLivArea'\nf,a = plt.subplots(figsize=(8,6))\nsns.scatterplot(x=temp, y=target, data=train)\ncorr, _ = stats.pearsonr(train[temp], train[target])\nplt.title('Pearsons correlation: %.3f' % corr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks linear but two outliers very evident.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(train[(train[temp]>4000) & (train[target]<12.5)].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Removed 2 outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = 'GarageCars'\nf,a = plt.subplots(figsize=(8,6))\nsns.boxplot(x=temp,y=target,data=train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I feel nothing requires removal for garagecars","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = 'TotalBsmtSF'\nf,a = plt.subplots(figsize=(8,6))\nsns.scatterplot(x=temp,y=target,data=train)\ncorr, _ = stats.pearsonr(train[temp], train[target])\nplt.title('Pearsons correlation: %.3f' % corr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Shows a very good correlation shown. So nothing requires removal.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"I feel like dropping a few columns which show multicolinearity.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"cor = train.corr()\nf,a = plt.subplots(figsize=(15,10))\nsns.heatmap(cor)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Numerical data will help better decide which rows to drop.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"topn = 20\nprint('Top ', topn, ' correlated features to target features')\ncor[target].sort_values(ascending=False)[1:(topn+1)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s = cor.unstack().sort_values(ascending = False)[len(cor):]\n\ntopn = 20\nprint('Top', int(topn/2), 'correlated features\\n')\ns[:topn:2]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train[target].reset_index(drop=True)\ntrain_features = train.drop(target, axis=1)\ntest_features = test\n\ndf = pd.concat([train_features, test_features]).reset_index(drop=True)\n\n## dropping the columns with multicollinearity\ndf.drop(['GarageArea', 'GarageYrBlt', 'TotRmsAbvGrd', '1stFlrSF'], axis=1, inplace=True)\n\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I tried to remove a few least correlated features from the training set. The model turned out to give bad results so I stuck with not dropping them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# topn = 20\n# # print(cor[target].sort_values()[:topn])\n# temp = list(cor[target].sort_values()[:topn].index)\n# df.drop(temp, axis=1, inplace=True)\n\n# print('Removed least ', topn, ' correlated features')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numfeat, catfeat = getnumcatfeat(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### To check if any feature type is misclassified","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert(temp<len(catfeat))\nprint('Feature name: ', catfeat[temp], '\\nUnique values: ', df[catfeat[temp]].unique(), \n      '\\nData type: ', df[catfeat[temp]].dtype, '\\nValue ', temp, ' out of ', len(catfeat))\ntemp +=1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of the categorical features seem to be misclassified. Checking for numeric features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"assert(temp<len(numfeat))\nprint('Feature name: ', numfeat[temp], '\\nUnique values: ', df[numfeat[temp]].unique(), \n      '\\nData type: ', df[numfeat[temp]].dtype, '\\nIndex ', temp, ' out of ', len(numfeat)-1)\ntemp +=1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['YearBuilt'] = df['YearBuilt'].astype('category')\ndf['YearRemodAdd'] = df['YearRemodAdd'].astype('category')\ndf['MoSold'] = df['MoSold'].astype('category')\ndf['YrSold'] = df['YrSold'].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"numfeat, catfeat = getnumcatfeat(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Changed variable type of category as misclassified in data. Chose category instead of object/string as models work faster on them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Feeling good about the data. Moving towards the combined data preprocessing.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# DATA PREPROCESSING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Data Cleaning","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Dealing with missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df.isnull().sum().sort_values(ascending=False)/df.shape[0]*100\ntemp = temp[temp>0]\ntemp = pd.DataFrame(temp, columns = ['MissPercent'])\n\nf,a = plt.subplots(figsize=(12,10))\n\nsub = sns.barplot(x='MissPercent', y = temp.index, data=temp, orient='h')\nplt.title('Percent of missing values, size = '+ str(temp.shape[0]))\n## Annotating the bar chart\nfor p,t in zip(sub.patches, temp['MissPercent']):\n    plt.text(2.3+p.get_width(), p.get_y()+p.get_height()/2, '{:.2f}'.format(t), ha='center', va = 'center')\n\nsns.despine(top=True, right=True)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def findit(df, strin):\n    \"\"\"\n    CONVENIENCE FUNCTION FOR ANOTHER FUNCTION\n    \"\"\"\n\n    temp = []\n    for col in df.columns:\n        if col[:len(strin)]==strin:\n            temp.append(col)\n    if len(temp)==0:\n        return 0\n    return temp\n\ndef fillit(df, strin):\n    \"\"\"\n    \n    CONVENIENCE FUNCTION\n    \n    Finds features beginning with 'strin' in its beginning.\n    Then fills null values of categorical and numeric features\n    with str('None') and int(0) values respectively.\n    \n    \"\"\"\n    temp = findit(df,strin)\n    for col in temp:\n        if df[col].dtype == object:\n            df[col].fillna('None', inplace=True)\n        else:\n            df[col].fillna(0, inplace=True)\n    return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['PoolQC'].fillna('None', inplace=True)\ndf['MiscFeature'].fillna('None', inplace=True)\ndf['Alley'].fillna(df['Alley'].mode()[0], inplace=True)\ndf['Fence'].fillna('None', inplace=True)\ndf['FireplaceQu'].fillna(\"None\", inplace=True)\ndf['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nfillit(df,'Garage')\nfillit(df,'Bsmt')\nfillit(df,'Mas')\ndf['MSZoning'] = df.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n# df['MSZoning'].fillna(df['MSZoning'].mode()[0], inplace=True)\ndf['Functional'].fillna('Typ', inplace=True)\n# Replace the missing values in each of the columns below with their mode\ndf['Electrical'].fillna(df['Electrical'].mode()[0],inplace=True)\ndf['KitchenQual'].fillna(df['KitchenQual'].mode()[0],inplace=True)\ndf['Exterior1st'].fillna(df['Exterior1st'].mode()[0],inplace=True)\ndf['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0],inplace=True)\ndf['SaleType'].fillna(df['SaleType'].mode()[0],inplace=True)\ndf['Utilities'].fillna(df['Utilities'].mode()[0], inplace=True)\ndf['TotalBsmtSF'].fillna(df['TotalBsmtSF'].mode()[0],inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = list(df[df['PoolQC']=='None']['PoolArea'].unique())\ndf['PoolArea'] = df['PoolArea'].replace(temp,0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A few wrong values exist for pool area where PoolQC is None. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"No more null values. Looking forward to outlier removal and data transformation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def minfun(lamb):\n    return round(pd.Series(stats.boxcox(1+df[temp],lmbda=lamb)).skew(), 2)\n\ndef retlamb(df, numfeat, tol, n_iter=100):\n    \"\"\"\n    \n    CONVENIENCE FUNCTION\n    \n    Returns optimized values of lambda to be used in\n    boxcox transformation for each feature so that \n    skewness is minimized\n    \n    \"\"\"\n    valLambda = {}\n    lim1, lim2 = 0, 4\n    idx=0\n    for temp in numfeat:\n        lim1, lim2 = 0, 2\n        for i in range(n_iter):\n            lamb1=0.5*(lim1+lim2)-tol\n            cal1 = round(pd.Series(stats.boxcox(1+df[temp],lmbda=lamb1)).skew(), 4)\n            lamb2=0.5*(lim1+lim2)+tol\n            cal2 = round(pd.Series(stats.boxcox(1+df[temp],lmbda=lamb2)).skew(), 4)\n            if abs(cal1)<abs(cal2):\n                lim2=lamb2\n            elif abs(cal1)>=abs(cal2) :\n                lim1=lamb1\n        valLambda[idx] = 0.5*(lim1+lim2)\n        idx+=1\n    return valLambda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valLambda = retlamb(df,numfeat, tol=0.0001, n_iter=1000)\nvalLambda","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = 2\nlamb = valLambda[temp]\n# temp, lamb = 0, 1\nf,a = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n\nsns.distplot(df[numfeat[temp]], ax=a[0], kde=False)\na[0].legend(['$\\mu=$ {:.2f}, $\\sigma=$ {:.2f}, Skew = {:.4f}, Kurtosis = {:.4f}'.format(mu,sig,df[numfeat[temp]].skew(),df[numfeat[temp]].kurt())])\na[0].set_title('Distribution of '+ numfeat[temp])\n\ntempdf = pd.Series(stats.boxcox(1+df[numfeat[temp]],lmbda=lamb))\n\nsns.distplot(tempdf, ax=a[1], kde=False)\na[1].legend(['$\\mu=$ {:.2f}, $\\sigma=$ {:.2f}, Skew = {:.4f}, Kurtosis = {:.4f}'.format(mu,sig,tempdf.skew(),tempdf.kurt())])\na[1].set_title('Transformed distribution of '+ numfeat[temp])\n\nvalLambda[temp] = lamb\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for temp, lamb in valLambda.items():\n    df[numfeat[temp]] = stats.boxcox(1+df[numfeat[temp]],lmbda=lamb)\n    \n# ## SIMPLE SIMPLE FIX FOR ALL DRAMA DONE IS ALL THE ABOVE 3 CELLS but my approach gave better results so I happily stuck with it\n# for temp in range(len(numfeat)):\n#     df[numfeat[temp]] = stats.boxcox(1+df[numfeat[temp]], stats.boxcox_normmax(df[numfeat[temp]] + 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[numfeat].skew()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.get_dummies(df).reset_index(drop=True)\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"minmaxscalar = prep.MinMaxScaler()\ndf1 = pd.DataFrame(minmaxscalar.fit_transform(df), columns = df.columns)\ndf1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df1.iloc[:len(train_labels),:]\nX_test = df1.iloc[len(train_labels):, :]\ny = train_labels\n\nX.shape, X_test.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODEL FITTING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV \nfrom sklearn.linear_model import Lasso\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\nkf = KFold(n_splits = 7, random_state=0, shuffle=True)\n\nscores = {}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Light GB model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.009, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgb'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XG Boost model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Showing one scenario of how I tuned the parameters of my model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 50)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 300, 100)]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [int(x) for x in np.linspace(2,500,100)]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [int(x) for x in np.linspace(2,500,100)]\n\n# Create the random grid\nrforestgrid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rforest = RandomForestRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clf = RandomizedSearchCV(rforest, rforestgrid, cv=5, n_iter=50, n_jobs=1)\n# # search = clf.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rforest_tuned = RandomForestRegressor(n_estimators = 1000,\n                                       min_samples_split = 2,\n                                       min_samples_leaf = 1,\n                                       max_features = 'auto',\n                                       max_depth = 100\n                                      )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(rforest_tuned)\nprint(\"rforest: {:4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rforest'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Lasso Regr Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso(alpha=0.000328)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a parameter I tuned myself and it gave pretty good results. :P","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"score = cv_rmse(lasso)\nprint(\"lasso: {:4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lasso'] = (score.mean(), score.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lasso.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.predict(X_test).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.iloc[:,1] = np.floor(np.expm1(model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission_try4.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Downloading the submission file\n\nfrom IPython.display import FileLink\nFileLink('submission_try4.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}