{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"7a9c45fc-a4d5-7d81-e665-52e0b570447d"},"source":"# Import Libraries #"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f310b2e5-aad0-deca-d2b2-9017534e17e2"},"outputs":[],"source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.preprocessing import Imputer\n\nfrom scipy.stats import skew\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.style.use('ggplot')"},{"cell_type":"markdown","metadata":{"_cell_guid":"a7852b95-f9d3-524c-7380-227f577d65ec"},"source":"#Import Data#"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0644a46d-7f03-5b79-5133-2f1b08dcb50d"},"outputs":[],"source":"train = '../input/train.csv'\ntest = '../input/test.csv'\n\ndf_train = pd.read_csv(train)\ndf_test = pd.read_csv(test)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"dd676753-7afa-7425-c334-efa01f9c0d9f"},"source":"#Define Median Absolute Deviation Function#\n\nFunction found in this link: http://stackoverflow.com/a/22357811/5082694"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36b14aa2-cb6b-ee22-822e-b25987a92622"},"outputs":[],"source":"def is_outlier(points, thresh = 3.5):\n    if len(points.shape) == 1:\n        points = points[:,None]\n    median = np.median(points, axis=0)\n    diff = np.sum((points - median)**2, axis=-1)\n    diff = np.sqrt(diff)\n    med_abs_deviation = np.median(diff)\n\n    modified_z_score = 0.6745 * diff / med_abs_deviation\n\n    return modified_z_score > thresh"},{"cell_type":"markdown","metadata":{"_cell_guid":"93d67941-46ed-4c88-d962-5104c00424f0"},"source":"# Remove Skew from SalesPrice data#"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"fe73d25a-8c44-f7c1-3690-335f9360dd2b"},"outputs":[],"source":"target = df_train[df_train.columns.values[-1]]\ntarget_log = np.log(target)\n\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nsns.distplot(target, bins=50)\nplt.title('Original Data')\nplt.xlabel('Sale Price')\n\nplt.subplot(1,2,2)\nsns.distplot(target_log, bins=50)\nplt.title('Natural Log of Data')\nplt.xlabel('Natural Log of Sale Price')\nplt.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"611075d7-c509-bd29-5d69-d0ddc08eebf1"},"source":"# Merge Train and Test to evaluate ranges and missing values #\nThis was done primarily to ensure that Categorical data in the training and testing data sets were consistent."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5356a37-704d-c07b-fc15-cd30b029fc2d"},"outputs":[],"source":"df_train = df_train[df_train.columns.values[:-1]]\ndf = df_train.append(df_test, ignore_index = True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"51369f66-1ba0-79de-169b-fdf1dfff957a"},"source":"#Find all categorical data#"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"db834360-7793-f4cd-4da7-89d2ea29f510"},"outputs":[],"source":"cats = []\nfor col in df.columns.values:\n    if df[col].dtype == 'object':\n        cats.append(col)"},{"cell_type":"markdown","metadata":{"_cell_guid":"d257aa8f-66db-821e-c86e-0b43d097e28a"},"source":"# Create separte datasets for Continuous vs Categorical #\nCreating two data sets allowed me to handle the data in more appropriate ways."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"94bb44ab-c0be-76e2-8da1-16ddd171b29b"},"outputs":[],"source":"df_cont = df.drop(cats, axis=1)\ndf_cat = df[cats]"},{"cell_type":"markdown","metadata":{"_cell_guid":"8e1ec7df-d89d-4d81-6a6a-e8a79231d2b4"},"source":"# Handle Missing Data for continuous data #\n\n - If any column contains more than 50 entries of missing data, drop the column\n - If any column contains fewer that 50 entries of missing data, replace those missing values with the median for that column\n - Remove outliers using Median Absolute Deviation\n - Calculate skewness for each variable and if greater than 0.75 transform it\n - Apply the sklearn.Normalizer to each column"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e15b692-770c-8bb1-47ec-159de3fdfb56"},"outputs":[],"source":"for col in df_cont.columns.values:\n    if np.sum(df_cont[col].isnull()) > 50:\n        df_cont = df_cont.drop(col, axis = 1)\n    elif np.sum(df_cont[col].isnull()) > 0:\n        median = df_cont[col].median()\n        idx = np.where(df_cont[col].isnull())[0]\n        df_cont[col].iloc[idx] = median\n\n        outliers = np.where(is_outlier(df_cont[col]))\n        df_cont[col].iloc[outliers] = median\n        \n        if skew(df_cont[col]) > 0.75:\n            df_cont[col] = np.log(df_cont[col])\n            df_cont[col] = df_cont[col].apply(lambda x: 0 if x == -np.inf else x)\n        \n        df_cont[col] = Normalizer().fit_transform(df_cont[col].reshape(1,-1))[0]"},{"cell_type":"markdown","metadata":{"_cell_guid":"9a0edf82-078d-f72f-1809-ffe74fc0758c"},"source":"# Handle Missing Data for Categorical Data #\n\n - If any column contains more than 50 entries of missing data, drop the column\n - If any column contains fewer that 50 entries of missing data, replace those values with the 'MIA'\n - Apply the sklearn.LabelEncoder\n - For each categorical variable determine the number of unique values and for each, create a new column that is binary"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"11ea26ef-680a-8e58-6c3e-f2af9e7889a3"},"outputs":[],"source":"for col in df_cat.columns.values:\n    if np.sum(df_cat[col].isnull()) > 50:\n        df_cat = df_cat.drop(col, axis = 1)\n        continue\n    elif np.sum(df_cat[col].isnull()) > 0:\n        df_cat[col] = df_cat[col].fillna('MIA')\n        \n    df_cat[col] = LabelEncoder().fit_transform(df_cat[col])\n    \n    num_cols = df_cat[col].max()\n    for i in range(num_cols):\n        col_name = col + '_' + str(i)\n        df_cat[col_name] = df_cat[col].apply(lambda x: 1 if x == i else 0)\n        \n    df_cat = df_cat.drop(col, axis = 1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"927398e4-5f02-e0d8-7cc2-c2b10eb9c878"},"source":"# Merge Numeric and Categorical Datasets and Create Training and Testing Data #"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"36b6edfe-8a54-9ab8-2820-e34e455ffddf"},"outputs":[],"source":"df_new = df_cont.join(df_cat)\n\ndf_train = df_new.iloc[:len(df_train) - 1]\ndf_train = df_train.join(target_log)\n\ndf_test = df_new.iloc[len(df_train) + 1:]\n\nX_train = df_train[df_train.columns.values[1:-1]]\ny_train = df_train[df_train.columns.values[-1]]\n\nX_test = df_test[df_test.columns.values[1:]]"},{"cell_type":"markdown","metadata":{"_cell_guid":"d81a1436-1857-dae4-fd54-ce3b3b2801d6"},"source":"# Create Estimator and Apply Cross Validation #\n\nWe can gauge the accuracy of our model by implementing an multi-fold cross validation and outputting the score.  In this case I chose to run 15 iterations and output the score as Root Mean Squared Error.\n\nThe results range from ~0.11-0.17 with a mean of ~0.14."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"96aebe1a-1ace-a7ad-b3f7-3d0871613da0"},"outputs":[],"source":"from sklearn.metrics import make_scorer, mean_squared_error\nscorer = make_scorer(mean_squared_error, False)\n\nclf = RandomForestRegressor(n_estimators=500, n_jobs=-1)\ncv_score = np.sqrt(-cross_val_score(estimator=clf, X=X_train, y=y_train, cv=15, scoring = scorer))\n\nplt.figure(figsize=(10,5))\nplt.bar(range(len(cv_score)), cv_score)\nplt.title('Cross Validation Score')\nplt.ylabel('RMSE')\nplt.xlabel('Iteration')\n\nplt.plot(range(len(cv_score) + 1), [cv_score.mean()] * (len(cv_score) + 1))\nplt.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"b0296b6f-0e95-abc6-1084-e7039d186a46"},"source":"# Evaluate Feature Significance #\n\nInvestigating feature importance is a relatively straight forward process:\n\n 1. Out feature importance coefficients\n 2. Map coefficients to their feature name\n 3. Sort features in descending order\n\nGiven our choice of model and methods for preprocessing data the most significant features are:\n\n 1. OverallQual\n 2. GrLivArea\n 3. TotalBsmtSF\n 4. GarageArea\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"87300d58-d4b0-0446-4c3c-29e8e4ff8d4e"},"outputs":[],"source":"# Fit model with training data\nclf.fit(X_train, y_train)\n\n# Output feature importance coefficients, map them to their feature name, and sort values\ncoef = pd.Series(clf.feature_importances_, index = X_train.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(10, 5))\ncoef.head(25).plot(kind='bar')\nplt.title('Feature Significance')\nplt.tight_layout()"},{"cell_type":"markdown","metadata":{"_cell_guid":"0c83c64c-95ed-9ca2-a795-c7748d653fb7"},"source":"# Visualize Predicted vs. Actual Sales Price #\n\nIn order to visualize our predicted values vs our actual values we need to split our data into training and testing data sets.  This can easily be accomplished using sklearn's **train_test_split** module.\n\nWe will train the model using a random sampling of our data set and then compare visually against the actual values."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"989d36e8-cb4b-d5b6-6011-a06a93049151"},"outputs":[],"source":"from sklearn.cross_validation import train_test_split\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train)\nclf = RandomForestRegressor(n_estimators=500, n_jobs=-1)\n\nclf.fit(X_train1, y_train1)\ny_pred = clf.predict(X_test1)\n\nplt.figure(figsize=(10, 5))\nplt.scatter(y_test1, y_pred, s=20)\nplt.title('Predicted vs. Actual')\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\n\nplt.plot([min(y_test1), max(y_test1)], [min(y_test1), max(y_test1)])\nplt.tight_layout()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9b0d67ae-9cc3-6b8f-9745-367700b8ef84"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}