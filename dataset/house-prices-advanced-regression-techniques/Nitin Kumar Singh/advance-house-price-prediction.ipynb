{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Kaggle Dataset: Advance Regression Techniques"},{"metadata":{},"cell_type":"markdown","source":"## 1. Understanding Data\n\n#### Quite Large Description of the Dataset. Please seek into \"description.txt\"\n\n#### Brief of Columns:\n- SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n- MSSubClass: The building class\n- MSZoning: The general zoning classification\n- LotFrontage: Linear feet of street connected to property\n- LotArea: Lot size in square feet\n- Street: Type of road access\n- Alley: Type of alley access\n- LotShape: General shape of property\n- LandContour: Flatness of the property\n- Utilities: Type of utilities available\n- LotConfig: Lot configuration\n- LandSlope: Slope of property\n- Neighborhood: Physical locations within Ames city limits\n- Condition1: Proximity to main road or railroad\n- Condition2: Proximity to main road or railroad (if a second is present)\n- BldgType: Type of dwelling\n- HouseStyle: Style of dwelling\n- OverallQual: Overall material and finish quality\n- OverallCond: Overall condition rating\n- YearBuilt: Original construction date\n- YearRemodAdd: Remodel date\n- RoofStyle: Type of roof\n- RoofMatl: Roof material\n- Exterior1st: Exterior covering on house\n- Exterior2nd: Exterior covering on house (if more than one material)\n- MasVnrType: Masonry veneer type\n- MasVnrArea: Masonry veneer area in square feet\n- ExterQual: Exterior material quality\n- ExterCond: Present condition of the material on the exterior\n- Foundation: Type of foundation\n- BsmtQual: Height of the basement\n- BsmtCond: General condition of the basement\n- BsmtExposure: Walkout or garden level basement walls\n- BsmtFinType1: Quality of basement finished area\n- BsmtFinSF1: Type 1 finished square feet\n- BsmtFinType2: Quality of second finished area (if present)\n- BsmtFinSF2: Type 2 finished square feet\n- BsmtUnfSF: Unfinished square feet of basement area\n- TotalBsmtSF: Total square feet of basement area\n- Heating: Type of heating\n- HeatingQC: Heating quality and condition\n- CentralAir: Central air conditioning\n- Electrical: Electrical system\n- 1stFlrSF: First Floor square feet\n- 2ndFlrSF: Second floor square feet\n- LowQualFinSF: Low quality finished square feet (all floors)\n- GrLivArea: Above grade (ground) living area square feet\n- BsmtFullBath: Basement full bathrooms\n- BsmtHalfBath: Basement half bathrooms\n- FullBath: Full bathrooms above grade\n- HalfBath: Half baths above grade\n- Bedroom: Number of bedrooms above basement level\n- Kitchen: Number of kitchens\n- KitchenQual: Kitchen quality\n- TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n- Functional: Home functionality rating\n- Fireplaces: Number of fireplaces\n- FireplaceQu: Fireplace quality\n- GarageType: Garage location\n- GarageYrBlt: Year garage was built\n- GarageFinish: Interior finish of the garage\n- GarageCars: Size of garage in car capacity\n- GarageArea: Size of garage in square feet\n- GarageQual: Garage quality\n- GarageCond: Garage condition\n- PavedDrive: Paved driveway\n- WoodDeckSF: Wood deck area in square feet\n- OpenPorchSF: Open porch area in square feet\n- EnclosedPorch: Enclosed porch area in square feet\n- 3SsnPorch: Three season porch area in square feet\n- ScreenPorch: Screen porch area in square feet\n- PoolArea: Pool area in square feet\n- PoolQC: Pool quality\n- Fence: Fence quality\n- MiscFeature: Miscellaneous feature not covered in other categories\n- MiscVal: $Value of miscellaneous feature\n- MoSold: Month Sold\n- YrSold: Year Sold\n- SaleType: Type of sale\n- SaleCondition: Condition of sale"},{"metadata":{},"cell_type":"markdown","source":"## 2. Understanding Task: \nTo predict SalePrice(target variable) on the basis of other columns(independent variable)."},{"metadata":{},"cell_type":"markdown","source":"## 3. Data Loading"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline \n#This is known as magic inline function.\n#When using the 'inline' backend, our matplotlib graphs will be included in our notebook, next to the code.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading dataset\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#copying dataset\ntrain_df = train_data.copy()\ntest_df = test_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#in large dataset we use set_option to display maximum rows and columns.\npd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here the one missing column is test_df must be our dependent variable (or target variable). "},{"metadata":{},"cell_type":"markdown","source":"## 4. Exploratory Data Analysis (EDA)"},{"metadata":{},"cell_type":"markdown","source":"### 4.1. Understanding Variables and DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, our train & test dataframe have datatypes like int, float and object. Also, our datasets have missing values."},{"metadata":{},"cell_type":"markdown","source":"### 4.2. Data Pre-Processing or Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.1. Missing Values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#count of missing values in columns having any missing values\ntrain_df[train_df.columns[train_df.isnull().any()]].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#percentage of missing values in columns having any missing values\n((train_df[train_df.columns[train_df.isnull().any()]].isnull().sum()* 100)/(len(train_df)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, few of the columns have a lot of missing values, which can be treated. Ideally we can treat ~15-20% of the data, because more we make chnages in our dataset more we'll get deviated from the accuracy. So we'll remove such columns in further steps. But first let's check for our test_df."},{"metadata":{"trusted":true},"cell_type":"code","source":"#count of missing values in columns having any missing values\ntest_df[test_df.columns[test_df.isnull().any()]].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#percentage of missing values in columns having any missing values\n((test_df[test_df.columns[test_df.isnull().any()]].isnull().sum()* 100)/(len(test_df)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can also observe our train_df has less missing values than test_df. So firstly get rid of columns having missing values more than 20%."},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping columns which have more than 20% missing values.\ntrain_df.drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping columns which have more than 20% missing values.\ntest_df.drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing missing values with median or mode according to their datatypes.\n\ntrain_missing = train_df.columns[train_df.isnull().any()]\nmissing_obj = []\nmissing_not_obj = []\n\nfor i in train_missing:\n    if train_df[i].dtypes == object:\n        missing_obj.append(i)\n    else:\n        missing_not_obj.append(i)\n        \nfor i in missing_obj:\n    train_df[i] = train_df[i].fillna(train_df[i].mode()[0])\n\nfor i in missing_not_obj:\n    train_df[i] = train_df[i].fillna(train_df[i].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replacing missing values with median or mode according to their datatypes.\n\ntest_missing = test_df.columns[test_df.isnull().any()]\nmissing_obj = []\nmissing_not_obj = []\n\nfor i in test_missing:\n    if test_df[i].dtypes == object:\n        missing_obj.append(i)\n    else:\n        missing_not_obj.append(i)\n        \nfor i in missing_obj:\n    test_df[i] = test_df[i].fillna(test_df[i].mode()[0])\n\nfor i in missing_not_obj:\n    test_df[i] = test_df[i].fillna(test_df[i].median())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{},"cell_type":"markdown","source":"Now, as we have 2 datasets, one is for train and other is for test. Afterwards I'll only perform changes in train dataset and will also perform similar changes in test data without checking into it."},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.2. Dividing columns on the basis of datatypes."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Id is nothing but a serial number which will never affect our target variable.\n\ntrain_df.drop(['Id'], axis=1, inplace=True)\ntest_df.drop(['Id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"year_cols = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_num = [] #numerical columns\ncols_obj = [] #object columns\n\nfor i in train_df.columns:\n    if i in year_cols:\n        pass\n    elif train_df[i].dtypes == object:\n        cols_obj.append(i)\n    else:\n        cols_num.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_num_dis = [] # discrete numerical values\ncols_num_con = [] # continuous numerical values\n\nfor i in cols_num:\n    if train_df[i].nunique()>12:\n        cols_num_con.append(i)\n    else:\n        cols_num_dis.append(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.2. (a) Handling Continuous Numerical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[cols_num_con].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Skewness, Kurtosis and Outliers"},{"metadata":{},"cell_type":"markdown","source":"**Skewness**: Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.\n- If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n- If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n- If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n\n**Kurtosis**: Kurtosis is a statistical measure that defines how heavily the tails of a distribution differ from the tails of a normal distribution. In other words, kurtosis identifies whether the tails of a given distribution contain extreme values.\n- A normal distribution has kurtosis exactly 3 (excess kurtosis exactly 0). Any distribution with kurtosis ≈3 (excess ≈0) is called mesokurtic.\n- A distribution with kurtosis <3 (excess kurtosis <0) is called platykurtic. Compared to a normal distribution, its tails are shorter and thinner, and often its central peak is lower and broader.\n- A distribution with kurtosis >3 (excess kurtosis >0) is called leptokurtic. Compared to a normal distribution, its tails are longer and fatter, and often its central peak is higher and sharper.\n\n**Outliers**: They are data records that differ dramatically from all others, they distinguish themselves in one or more characteristics. In other words, an outlier is a value that escapes normality and can (and probably will) cause anomalies in the results obtained through algorithms and analytical systems."},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for skewness and kurtosis values in dataset.\nfor i in cols_num_con:\n    print(f'For {i} Skewness is {round(train_df[i].skew(),2)} and Kurtosis is {round(train_df[i].kurtosis(),2)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting histplot for dataset to check skewness.\nfor i in cols_num_con:\n    sns.histplot(train_df[i], kde=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting scatter-plot for a dataset to check outliers.\nfor i in cols_num_con:\n    sns.scatterplot(data=train_df, x=train_df[i].index, y=i)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can also observe almost all the columns have outliers and therefore skewed. One of the solution is Log Transformation.\n\n**Log transformation**: Log transformation is a data transformation method in which it replaces each variable x with a log(x). Benefits of log transformation is, we can deal with outliers and skewness at the same time bacause as you know skewness happens because of outlier values present in our data.\n\nBut here as I noticed many of the columns have 0 values so, what I'm going to do is apply log(x+1) instead of log(x) because log 0 is undefined. It's not a real number, because you can never get zero by raising anything to the power of anything else. \n\nSo, what I'm going to do is log transform only those values which do not have any 0s in it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking our columns after outliers removal.\nfor i in cols_num_con:\n    train_df[i] = np.log(train_df[i]+1)\n    sns.histplot(train_df[i], kde=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[cols_num_con].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_num_com = ['MSSubClass','LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF', '2ndFlrSF',\n 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch','MiscVal']\nfor i in cols_num_com:\n    test_df[i] = np.log(test_df[i]+1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done with all continuous numerical columns."},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.2. (b) Handling Discrete Numerical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[cols_num_dis].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cols_num_dis:\n    sns.catplot(x = i, y = 'SalePrice', data = train_df)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Qual = train_df.groupby(['OverallQual']).SalePrice.agg([len, min, max])\nQual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['OverallQual'] = np.where((train_df.OverallQual>8 ), 8, train_df.OverallQual)\ntrain_df['OverallQual'] = np.where((train_df.OverallQual<4 ), 4, train_df.OverallQual)\n\ntest_df['OverallQual'] = np.where((test_df.OverallQual>8 ), 8, test_df.OverallQual)\ntest_df['OverallQual'] = np.where((test_df.OverallQual<4 ), 4, test_df.OverallQual)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Cond = train_df.groupby(['OverallCond']).SalePrice.agg([len, min, max])\nCond","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['OverallCond'] = np.where((train_df.OverallCond<3), 3, train_df.OverallCond)\n\ntest_df['OverallCond'] = np.where((test_df.OverallCond<3), 3, test_df.OverallCond)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BsmtFullBath = train_df.groupby(['BsmtFullBath']).SalePrice.agg([len, min, max])\nBsmtFullBath","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['BsmtFullBath'] = np.where((train_df.BsmtFullBath!=0), 1, train_df.BsmtFullBath)\n\ntest_df['BsmtFullBath'] = np.where((test_df.BsmtFullBath!=0), 1, test_df.BsmtFullBath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BsmtHalfBath = train_df.groupby(['BsmtHalfBath']).SalePrice.agg([len, min, max])\nBsmtHalfBath","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['BsmtHalfBath'] = np.where((train_df.BsmtHalfBath!=0), 1, train_df.BsmtHalfBath)\n\ntest_df['BsmtHalfBath'] = np.where((test_df.BsmtHalfBath!=0), 1, test_df.BsmtHalfBath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FullBath = train_df.groupby(['FullBath']).SalePrice.agg([len, min, max])\nFullBath","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['FullBath'] = np.where((train_df.FullBath!=0 ), 1, train_df.FullBath)\n\ntest_df['FullBath'] = np.where((test_df.FullBath!=0 ), 1, test_df.FullBath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"HalfBath = train_df.groupby(['HalfBath']).SalePrice.agg([len, min, max])\nHalfBath","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['HalfBath'] = np.where((train_df.HalfBath!=0 ), 1, train_df.HalfBath)\n\ntest_df['HalfBath'] = np.where((test_df.HalfBath!=0 ), 1, test_df.HalfBath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BedroomAbvGr = train_df.groupby(['BedroomAbvGr']).SalePrice.agg([len, min, max])\nBedroomAbvGr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['BedroomAbvGr'] = np.where((train_df.BedroomAbvGr<2 ), 2, train_df.BedroomAbvGr)\ntrain_df['BedroomAbvGr'] = np.where((train_df.BedroomAbvGr>4 ), 4, train_df.BedroomAbvGr)\n\ntest_df['BedroomAbvGr'] = np.where((test_df.BedroomAbvGr<2 ), 2, test_df.BedroomAbvGr)\ntest_df['BedroomAbvGr'] = np.where((test_df.BedroomAbvGr>4 ), 4, test_df.BedroomAbvGr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"KitchenAbvGr = train_df.groupby(['KitchenAbvGr']).SalePrice.agg([len, min, max])\nKitchenAbvGr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['KitchenAbvGr'] = np.where((train_df.KitchenAbvGr<1 ), 1, train_df.KitchenAbvGr)\ntrain_df['KitchenAbvGr'] = np.where((train_df.KitchenAbvGr>2 ), 2, train_df.KitchenAbvGr)\n\ntest_df['KitchenAbvGr'] = np.where((test_df.KitchenAbvGr<1 ), 1, test_df.KitchenAbvGr)\ntest_df['KitchenAbvGr'] = np.where((test_df.KitchenAbvGr>2 ), 2, test_df.KitchenAbvGr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TotRmsAbvGrd = train_df.groupby(['TotRmsAbvGrd']).SalePrice.agg([len, min, max])\nTotRmsAbvGrd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['TotRmsAbvGrd'] = np.where((train_df.TotRmsAbvGrd>10 ), 10, train_df.TotRmsAbvGrd)\ntrain_df['TotRmsAbvGrd'] = np.where((train_df.TotRmsAbvGrd<3 ), 3, train_df.TotRmsAbvGrd)\n\ntest_df['TotRmsAbvGrd'] = np.where((test_df.TotRmsAbvGrd>10 ), 10, test_df.TotRmsAbvGrd)\ntest_df['TotRmsAbvGrd'] = np.where((test_df.TotRmsAbvGrd<3 ), 3, test_df.TotRmsAbvGrd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Fireplaces = train_df.groupby(['Fireplaces']).SalePrice.agg([len, min, max])\nFireplaces","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Fireplaces'] = np.where((train_df.Fireplaces!=0 ), 1, train_df.Fireplaces)\n\ntest_df['Fireplaces'] = np.where((test_df.Fireplaces!=0 ), 1, test_df.Fireplaces)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GarageCars = train_df.groupby(['GarageCars']).SalePrice.agg([len, min, max])\nGarageCars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['GarageCars'] = np.where((train_df.GarageCars==4 ), 3, train_df.GarageCars)\n\ntest_df['GarageCars'] = np.where((test_df.GarageCars==4 ), 3, test_df.GarageCars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MoSold = train_df.groupby(['MoSold']).SalePrice.agg([len, min, max])\nMoSold","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done with Discrete Continuous Variables."},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.2. (c) Handling Year Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[year_cols].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in year_cols:\n    sns.catplot(x = i, y = 'SalePrice', data = train_df)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Quite satisfactory data distribution.\n- Newer houses have more SalePrice than older ones.\n- Something similar to Remodeled houses have higher SalePrice.\n- Similar to GarageBuild"},{"metadata":{},"cell_type":"markdown","source":"#### 4.2.2. (d) Handling Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[cols_obj].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cols_obj:\n    sns.catplot(x = i, y = 'SalePrice', data = train_df)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in cols_obj:\n    print(train_df[i].value_counts(normalize=True)*100)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can observe that occurance of some of the enteries of few columns have more than 90% of the values. Such columns needs to be dropped.\n\nAlso few columns columns have 1 entry with moderately highl value than rest of the columns, shoulld be transformed.\n\nAnd, rest should be encoded."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(['Street','Utilities','LandSlope','Condition2','RoofMatl','BsmtCond','Heating','CentralAir','Electrical','Functional','GarageQual','GarageCond','PavedDrive'],axis=1, inplace=True)\ntest_df.drop(['Street','Utilities','LandSlope','Condition2','RoofMatl','BsmtCond','Heating','CentralAir','Electrical','Functional','GarageQual','GarageCond','PavedDrive'],axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MSZoning = train_df.groupby(['MSZoning']).SalePrice.agg([len, min, max])\nMSZoning","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['MSZoning'] = np.where((train_df.MSZoning=='RL' ), 1, 0)\n\ntest_df['MSZoning'] = np.where((test_df.MSZoning=='RL' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LotShape = train_df.groupby(['LotShape']).SalePrice.agg([len, min, max])\nLotShape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['LotShape'] = np.where((train_df.LotShape=='Reg' ), 1, 0)\n\ntest_df['LotShape'] = np.where((test_df.LotShape=='Reg' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LandContour = train_df.groupby(['LandContour']).SalePrice.agg([len, min, max])\nLandContour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['LandContour'] = np.where((train_df.LandContour=='Lvl' ), 1, 0)\n\ntest_df['LandContour'] = np.where((test_df.LandContour=='Lvl' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LotConfig = train_df.groupby(['LotConfig']).SalePrice.agg([len, min, max])\nLotConfig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['LotConfig'] = np.where((train_df.LotConfig=='Inside' ), 1, 0)\n\ntest_df['LotConfig'] = np.where((test_df.LotConfig=='Inside' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Condition1 = train_df.groupby(['Condition1']).SalePrice.agg([len, min, max])\nCondition1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Condition1'] = np.where((train_df.Condition1=='Norm' ), 1, 0)\n\ntest_df['Condition1'] = np.where((test_df.Condition1=='Norm' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BldgType = train_df.groupby(['BldgType']).SalePrice.agg([len, min, max])\nBldgType","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['BldgType'] = np.where((train_df.BldgType=='1Fam' ), 1, 0)\n\ntest_df['BldgType'] = np.where((test_df.BldgType=='1Fam' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RoofStyle = train_df.groupby(['RoofStyle']).SalePrice.agg([len, min, max])\nRoofStyle","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['RoofStyle'] = np.where((train_df.RoofStyle=='Gable' ), 1, 0)\n\ntest_df['RoofStyle'] = np.where((test_df.RoofStyle=='Gable' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MasVnrType = train_df.groupby(['MasVnrType']).SalePrice.agg([len, min, max])\nMasVnrType","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['MasVnrType'] = np.where((train_df.MasVnrType=='None' ), 0, 1)\n\ntest_df['MasVnrType'] = np.where((test_df.MasVnrType=='None' ), 0, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ExterQual = train_df.groupby(['ExterQual']).SalePrice.agg([len, min, max])\nExterQual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ExterQual'] = np.where((train_df.ExterQual=='Ex' ), 'Gd', train_df.ExterQual)\ntrain_df['ExterQual'] = np.where((train_df.ExterQual=='Fa' ), 'TA', train_df.ExterQual)\ntrain_df['ExterQual'] = np.where((train_df.ExterQual=='TA' ), 1, 0)\n\ntest_df['ExterQual'] = np.where((test_df.ExterQual=='Ex' ), 'Gd', test_df.ExterQual)\ntest_df['ExterQual'] = np.where((test_df.ExterQual=='Fa' ), 'TA', test_df.ExterQual)\ntest_df['ExterQual'] = np.where((test_df.ExterQual=='TA' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ExterCond = train_df.groupby(['ExterCond']).SalePrice.agg([len, min, max])\nExterCond","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['ExterCond'] = np.where((train_df.ExterCond=='TA' ), 1, 0)\n\ntest_df['ExterCond'] = np.where((test_df.ExterCond=='TA' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Foundation = train_df.groupby(['Foundation']).SalePrice.agg([len, min, max])\nFoundation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Foundation'] = np.where((train_df.Foundation=='PConc' ), 1, 0)\n\ntest_df['Foundation'] = np.where((test_df.Foundation=='PConc' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BsmtQual = train_df.groupby(['BsmtQual']).SalePrice.agg([len, min, max])\nBsmtQual","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['BsmtQual'] = np.where((train_df.BsmtQual=='Ex' ), 'Gd', train_df.BsmtQual)\ntrain_df['BsmtQual'] = np.where((train_df.BsmtQual=='Fa' ), 'TA', train_df.BsmtQual)\ntrain_df['BsmtQual'] = np.where((train_df.BsmtQual=='TA' ), 1, 0)\n\ntest_df['BsmtQual'] = np.where((test_df.BsmtQual=='Ex' ), 'Gd', test_df.BsmtQual)\ntest_df['BsmtQual'] = np.where((test_df.BsmtQual=='Fa' ), 'TA', test_df.BsmtQual)\ntest_df['BsmtQual'] = np.where((test_df.BsmtQual=='TA' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BsmtExposure = train_df.groupby(['BsmtQual']).SalePrice.agg([len, min, max])\nBsmtExposure","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['BsmtExposure'] = np.where((train_df.BsmtExposure=='Mn' ), 'No', train_df.BsmtExposure)\ntrain_df['BsmtExposure'] = np.where((train_df.BsmtExposure=='Gd' ), 'Av', train_df.BsmtExposure)\ntrain_df['BsmtExposure'] = np.where((train_df.BsmtExposure=='No' ), 1, 0)\n\ntest_df['BsmtExposure'] = np.where((test_df.BsmtExposure=='Mn' ), 'No', test_df.BsmtExposure)\ntest_df['BsmtExposure'] = np.where((test_df.BsmtExposure=='Gd' ), 'Av', test_df.BsmtExposure)\ntest_df['BsmtExposure'] = np.where((test_df.BsmtExposure=='No' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"GarageType = train_df.groupby(['GarageType']).SalePrice.agg([len, min, max])\nGarageType","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['GarageType'] = np.where((train_df.GarageType=='Attchd' ), 1, 0)\n\ntest_df['GarageType'] = np.where((test_df.GarageType=='Attchd' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SaleType = train_df.groupby(['SaleType']).SalePrice.agg([len, min, max])\nSaleType","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['SaleType'] = np.where((train_df.SaleType=='WD' ), 1, 0)\n\ntest_df['SaleType'] = np.where((test_df.SaleType=='WD' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SaleCondition = train_df.groupby(['SaleCondition']).SalePrice.agg([len, min, max])\nSaleCondition","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['SaleCondition'] = np.where((train_df.SaleCondition=='Normal' ), 1, 0)\n\ntest_df['SaleCondition'] = np.where((test_df.SaleCondition=='Normal' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#GarageFinish, KitchenQual, HeatingQC, BsmtFinType2, BsmtFinType1, Exterior2nd, Exterior1st, HouseStyle, Neighborhood","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.catplot(x = 'Neighborhood', y = 'SalePrice', data = train_df,height=5, aspect=2)\nax.set_xticklabels(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#By the Visualisation from the graph, I decided to convert entries from Neighbourhood column into three groups.\n#[CollgCr, Veenker, Crawfor, Mitchel,  NWAmes, NAmes, SawyerW, Edwards, NPkVill] = 0\n#[NoRidge, Somerst,NridgHt, Timber, Gilbert, StoneBr, ClearCr, Blmngth] = 1\n#[OldTown, BrkSide, Sawyer, IDOTRR, MeadowV, BrDale, SWISU, Blueste] = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp1 = ['CollgCr', 'Veenker', 'Crawfor', 'Mitchel',  'NWAmes', 'NAmes', 'SawyerW', 'Edwards', 'NPkVill']\ntemp2 = ['NoRidge', 'Somerst' ,'NridgHt', 'Timber', 'Gilbert', 'StoneBr', 'ClearCr', 'Blmngtn']\ntemp3 = ['OldTown', 'BrkSide', 'Sawyer', 'IDOTRR', 'MeadowV', 'BrDale', 'SWISU', 'Blueste']\n\nfor i in temp1:\n    train_df['Neighborhood'] = np.where((train_df.Neighborhood==i), 0, train_df.Neighborhood)\n    \nfor j in temp2:\n    train_df['Neighborhood'] = np.where((train_df.Neighborhood==j), 1, train_df.Neighborhood)\n    \nfor k in temp3:\n    train_df['Neighborhood'] = np.where((train_df.Neighborhood==k), 2, train_df.Neighborhood)\n    \n\nfor i in temp1:\n    test_df['Neighborhood'] = np.where((test_df.Neighborhood==i), 0, test_df.Neighborhood)\n    \nfor j in temp2:\n    test_df['Neighborhood'] = np.where((test_df.Neighborhood==j), 1, test_df.Neighborhood)\n    \nfor k in temp3:\n    test_df['Neighborhood'] = np.where((test_df.Neighborhood==k), 2, test_df.Neighborhood)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.catplot(x = 'GarageFinish', y = 'SalePrice', data = train_df,height=5, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['GarageFinish'] = np.where((train_df.GarageFinish=='Unf' ), 1, 0)\n\ntest_df['GarageFinish'] = np.where((test_df.GarageFinish=='Unf' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.catplot(x = 'KitchenQual', y = 'SalePrice', data = train_df,height=5, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['KitchenQual'] = np.where((train_df.KitchenQual=='Ex' ), 'Gd', train_df.KitchenQual)\ntrain_df['KitchenQual'] = np.where((train_df.KitchenQual=='Fa' ), 'TA', train_df.KitchenQual)\ntrain_df['KitchenQual'] = np.where((train_df.KitchenQual=='TA' ), 1, 0)\n\ntest_df['KitchenQual'] = np.where((test_df.KitchenQual=='Ex' ), 'Gd', test_df.KitchenQual)\ntest_df['KitchenQual'] = np.where((test_df.KitchenQual=='Fa' ), 'TA', test_df.KitchenQual)\ntest_df['KitchenQual'] = np.where((test_df.KitchenQual=='TA' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.catplot(x = 'HeatingQC', y = 'SalePrice', data = train_df,height=5, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['HeatingQC'] = np.where((train_df.HeatingQC=='Ex' ), 1, 0)\n\ntest_df['HeatingQC'] = np.where((test_df.HeatingQC=='Ex' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.catplot(x = 'BsmtFinType1', y = 'SalePrice', data = train_df,height=5, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['BsmtFinType1'] = np.where((train_df.BsmtFinType1=='Rec'), 'Unf', train_df.BsmtFinType1)\ntrain_df['BsmtFinType1'] = np.where((train_df.BsmtFinType1=='BLQ'), 'Unf', train_df.BsmtFinType1)\ntrain_df['BsmtFinType1'] = np.where((train_df.BsmtFinType1=='ALQ'), 'GLQ', train_df.BsmtFinType1)\ntrain_df['BsmtFinType1'] = np.where((train_df.BsmtFinType1=='LwQ'), 'GLQ', train_df.BsmtFinType1)\ntrain_df['BsmtFinType1'] = np.where((train_df.BsmtFinType1=='Unf' ), 1, 0)\n\ntest_df['BsmtFinType1'] = np.where((test_df.BsmtFinType1=='Rec'), 'Unf', test_df.BsmtFinType1)\ntest_df['BsmtFinType1'] = np.where((test_df.BsmtFinType1=='BLQ'), 'Unf', test_df.BsmtFinType1)\ntest_df['BsmtFinType1'] = np.where((test_df.BsmtFinType1=='ALQ'), 'GLQ', test_df.BsmtFinType1)\ntest_df['BsmtFinType1'] = np.where((test_df.BsmtFinType1=='LwQ'), 'GLQ', test_df.BsmtFinType1)\ntest_df['BsmtFinType1'] = np.where((test_df.BsmtFinType1=='Unf' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.catplot(x = 'BsmtFinType2', y = 'SalePrice', data = train_df,height=5, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['BsmtFinType2'] = np.where((train_df.BsmtFinType2=='Unf' ), 1, 0)\n\ntest_df['BsmtFinType2'] = np.where((test_df.BsmtFinType2=='Unf' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.catplot(x = 'Exterior1st', y = 'SalePrice', data = train_df,height=5, aspect=2)\nax.set_xticklabels(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Exterior1st'] = np.where((train_df.Exterior1st=='AsbShng'), 'VinylSd', train_df.Exterior1st)\ntrain_df['Exterior1st'] = np.where((train_df.Exterior1st=='BrkFace'), 'VinylSd', train_df.Exterior1st)\ntrain_df['Exterior1st'] = np.where((train_df.Exterior1st=='Wd Sdng'), 'VinylSd', train_df.Exterior1st)\ntrain_df['Exterior1st'] = np.where((train_df.Exterior1st=='VinylSd' ), 1, 0)\n\ntest_df['Exterior1st'] = np.where((test_df.Exterior1st=='AsbShng'), 'VinylSd', test_df.Exterior1st)\ntest_df['Exterior1st'] = np.where((test_df.Exterior1st=='BrkFace'), 'VinylSd', test_df.Exterior1st)\ntest_df['Exterior1st'] = np.where((test_df.Exterior1st=='Wd Sdng'), 'VinylSd', test_df.Exterior1st)\ntest_df['Exterior1st'] = np.where((test_df.Exterior1st=='VinylSd' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax = sns.catplot(x = 'Exterior2nd', y = 'SalePrice', data = train_df,height=5, aspect=2)\nax.set_xticklabels(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Exterior2nd'] = np.where((train_df.Exterior2nd=='AsbShng'), 'VinylSd', train_df.Exterior2nd)\ntrain_df['Exterior2nd'] = np.where((train_df.Exterior2nd=='BrkFace'), 'VinylSd', train_df.Exterior2nd)\ntrain_df['Exterior2nd'] = np.where((train_df.Exterior2nd=='Wd Sdng'), 'VinylSd', train_df.Exterior2nd)\ntrain_df['Exterior2nd'] = np.where((train_df.Exterior2nd=='VinylSd' ), 1, 0)\n\ntest_df['Exterior2nd'] = np.where((test_df.Exterior2nd=='AsbShng'), 'VinylSd', test_df.Exterior2nd)\ntest_df['Exterior2nd'] = np.where((test_df.Exterior2nd=='BrkFace'), 'VinylSd', test_df.Exterior2nd)\ntest_df['Exterior2nd'] = np.where((test_df.Exterior2nd=='Wd Sdng'), 'VinylSd', test_df.Exterior2nd)\ntest_df['Exterior2nd'] = np.where((test_df.Exterior2nd=='VinylSd' ), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"ax = sns.catplot(x = 'HouseStyle', y = 'SalePrice', data = train_df,height=5, aspect=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['HouseStyle'] = np.where((train_df.HouseStyle=='1.5Fin'), 'Other', train_df.HouseStyle)\ntrain_df['HouseStyle'] = np.where((train_df.HouseStyle=='1.5Unf'), 'Other', train_df.HouseStyle)\ntrain_df['HouseStyle'] = np.where((train_df.HouseStyle=='SFoyer'), 'Other', train_df.HouseStyle)\ntrain_df['HouseStyle'] = np.where((train_df.HouseStyle=='SLvl'), 'Other', train_df.HouseStyle)\ntrain_df['HouseStyle'] = np.where((train_df.HouseStyle=='2.5Unf'), 'Other', train_df.HouseStyle)\ntrain_df['HouseStyle'] = np.where((train_df.HouseStyle=='2.5Fin'), 'Other', train_df.HouseStyle)\ntrain_df['HouseStyle'] = np.where((train_df.HouseStyle=='Other' ), 0, train_df.HouseStyle)\ntrain_df['HouseStyle'] = np.where((train_df.HouseStyle=='1Story' ), 1, train_df.HouseStyle)\ntrain_df['HouseStyle'] = np.where((train_df.HouseStyle=='2Story' ), 2, train_df.HouseStyle)\n\ntest_df['HouseStyle'] = np.where((test_df.HouseStyle=='1.5Fin'), 'Other', test_df.HouseStyle)\ntest_df['HouseStyle'] = np.where((test_df.HouseStyle=='1.5Unf'), 'Other', test_df.HouseStyle)\ntest_df['HouseStyle'] = np.where((test_df.HouseStyle=='SFoyer'), 'Other', test_df.HouseStyle)\ntest_df['HouseStyle'] = np.where((test_df.HouseStyle=='SLvl'), 'Other', test_df.HouseStyle)\ntest_df['HouseStyle'] = np.where((test_df.HouseStyle=='2.5Unf'), 'Other', test_df.HouseStyle)\ntest_df['HouseStyle'] = np.where((test_df.HouseStyle=='2.5Fin'), 'Other', test_df.HouseStyle)\ntest_df['HouseStyle'] = np.where((test_df.HouseStyle=='Other' ), 0, test_df.HouseStyle)\ntest_df['HouseStyle'] = np.where((test_df.HouseStyle=='1Story' ), 1, test_df.HouseStyle)\ntest_df['HouseStyle'] = np.where((test_df.HouseStyle=='2Story' ), 2, test_df.HouseStyle)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's it for Categorical Varibles."},{"metadata":{},"cell_type":"markdown","source":"### 4.3. Duplicate Columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for duplicate rows\ntrain_df[train_df.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[test_df.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No duplicate rows in both of the dataframe."},{"metadata":{},"cell_type":"markdown","source":"So, we left with just two categorical columns i.e. Neighborhood and HouseStyle. Will perform One Hot Encoding for these two columns."},{"metadata":{},"cell_type":"markdown","source":"### 4.4. One-Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['HouseStyle'] = pd.get_dummies(train_df['HouseStyle'])\ntrain_df['Neighborhood'] = pd.get_dummies(train_df['Neighborhood'])\n\ntest_df['HouseStyle'] = pd.get_dummies(test_df['HouseStyle'])\ntest_df['Neighborhood'] = pd.get_dummies(test_df['Neighborhood'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.5. Feature Scaling\n**Feature Scaling or Standardization**: It is a step of Data Pre-Processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm.\n\nStandardisation replaces the values by their Z scores."},{"metadata":{},"cell_type":"raw","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nscaler.fit(train_df)\nscaler.fit(test_df)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.6. Mulitcolinearity\n\n**Multicollinearity**: Multicollinearity occurs when two or more independent variables are highly correlated with one another in a regression model.\n\n**Why not Multicollinearity?**: Multicollinearity can be a problem in a regression model because we would not be able to distinguish between the individual effects of the independent variables on the dependent variable.\n\n**Detection of Multicollinearity**: Multicollinearity can be detected via various methods. One of the popular method is using VIF.\n\n**VIF**: VIF stands for Variable Inflation Factors. VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = train_df.drop(['SalePrice'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nX_vif = add_constant(X1)\n\npd.Series([variance_inflation_factor(X_vif.values, i) \n               for i in range(X_vif.shape[1])], \n              index=X_vif.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_high = ['GrLivArea', '2ndFlrSF', '1stFlrSF', 'BsmtFinSF2', 'BsmtFinType2', 'MasVnrArea', 'MasVnrType'] #these columns have high multicolinearity.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, VIF of few columns are very high. That means we have to drop some of the columns because it's not at all good for our model. \n\nBut Wait! How will we decide which of the columns should be dropped?\n\nHere comes the role of Significancy.\n\n**4.6.1. Significancy**: In statistics, statistical significance means that the result that was produced has a reason behind it, it was not produced randomly, or by chance.\n\n(a) **Correlation**: Correlation is a statistic that measures the degree to which two variables move in relation to each other. We use this technique to find correlation between two continuous columns. The correlation coefficient has values between -1 to 1\n- A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n- A value closer to 1 implies stronger positive correlation\n- A value closer to -1 implies stronger negative correlation\n\n(b) **ANOVA**: ANOVA stands for Analysis of Variance. It is performed to figure out the relation between the different group of categorical data. Under ANOVA we have two measures as result:\n- F-testscore : which shows the variaton of groups mean over variation\n- p-value: it shows the importance of the result\n- We use this technique to find relation between continuous and categorical columns.\n- As a conclusion, we can say that there is a strong correlation between other variables and a categorical variable if the ANOVA test gives us a large F-test value and a small p-value."},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.stats import pearsonr\n\nfor i in temp_high:\n    for j in temp_high:\n        if i in cols_num_con:\n            corr, _ = pearsonr(train_df[i], train_df[j])\n            print(i,'&',j ,'correlation: %.3f' % corr)\n        else:\n            print(i,'&',j ,\"ANOVA: \",stats.f_oneway(train_df[i],train_df[j]))\n    \n    \n    if i in cols_num_con:\n        corr, _ = pearsonr(train_df[i], train_df['SalePrice'])\n        print(i, '& SalePrice', 'correlation: %.3f' % corr)\n        print()\n    else:\n        print(i,\" & SalePrice ANOVA: \",stats.f_oneway(train_df[i],train_df['SalePrice']))\n        print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So according to my observations:\n- BsmtFinType2 is high multicolinearity with BsmtFinSF2 but less related with SalePrice.\n- MasVnrType is high multicolinearity with MasVnrArea but less related with SalePrice.\n- 2ndFloorSF is GrLivArea are having high multicolinearity but less with SalePrice."},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"#according to our results for significance, I'm again checking muliticolinearity after\n#dropping few columns\nX_vif = X_vif.drop(['BsmtFinSF2','2ndFlrSF','MasVnrType'],axis = 1)\npd.Series([variance_inflation_factor(X_vif.values, i) \n               for i in range(X_vif.shape[1])], \n              index=X_vif.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Done!!..  Now let's drop these columns from train_df and test_df."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(['BsmtFinSF2','2ndFlrSF','MasVnrType'],axis = 1, inplace=True)\ntest_df.drop(['BsmtFinSF2','2ndFlrSF','MasVnrType'],axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Train-Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = train_df.drop(['SalePrice'], axis=1)\ny = train_df['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n\nprint('Train', X_train.shape, y_train.shape)\nprint('Test', X_test.shape, y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.1, random_state=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"### 6.1. Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nLR = LinearRegression()\nLR.fit(X_train,y_train)\ny_predicted = LR.predict(X_test)\n\nprint(round(LR.score(X_train, y_train)*100,2))\nprint(round(LR.score(X_test, y_test)*100,2))\nmean_squared_error(y_test, y_predicted, squared=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.2. Regularization:\n\n#### 6.2. (a) Lasso Regression:\nLasso regression stands for Least Absolute Shrinkage and Selection Operator. It adds penalty term to the cost function. This term is the absolute sum of the coefficients. As the value of coefficients increases from 0 this term penalizes, cause model, to decrease the value of coefficients in order to reduce loss. The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero.\n\n**Limitation**: \n- Lasso sometimes struggles with some types of data. If the number of predictors (p) is greater than the number of observations (n), Lasso will pick at most n predictors as non-zero, even if all predictors are relevant (or may be used in the test set).\n- If there are two or more highly collinear variables then LASSO regression select one of them randomly which is not good for the interpretation of data\n\n#### 6.2. (b) Ridge Regression:\nIn Ridge regression, we add a penalty term which is equal to the square of the coefficient. The L2 term is equal to the square of the magnitude of the coefficients. We also add a coefficient lambda to control that penalty term. In this case if lambda  is zero then the equation is the basic OLS else if `lambda > 0` then it will add a constraint to the coefficient. As we increase the value of lambda this constraint causes the value of the coefficient to tend towards zero. This leads to both low variance (as some coefficient leads to negligible effect on prediction) and low bias (minimization of coefficient reduce the dependency of prediction on a particular variable).\n\n**Limitation**:\nRidge regression decreases the complexity of a model but does not reduce the number of variables since it never leads to a coefficient been zero rather only minimizes it. Hence, this model is not good for feature reduction."},{"metadata":{},"cell_type":"markdown","source":"**Lasso Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'alpha':[0,0.1,0.5,1,5,10],\n              'normalize': [True,False]}\n\nLassoReg = Lasso()\n\nLasso_reg= GridSearchCV(LassoReg, parameters, scoring='neg_mean_squared_error',cv=20)\nLasso_reg.fit(X_train,y_train)\n\n# best estimator\nprint(Lasso_reg.best_estimator_)\n\n# best model\nbest_model = Lasso_reg.best_estimator_\nbest_model.fit(X_train,y_train)\ny_predicted = best_model.predict(X_test)\nprint(best_model.score(X_train,y_train)*100)\nprint(mean_squared_error(y_test, y_predicted, squared=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ridge**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'alpha':[0.001,0.01,0.1,0.2,0.4, 0.5,0.7,0.9,1,5,10],\n              'normalize': [True,False]}\n\nRidgeReg = Ridge()\n\nRidge_reg= GridSearchCV(RidgeReg, parameters, scoring='neg_mean_squared_error',cv=20)\nRidge_reg.fit(X_train,y_train)\n\n# best estimator\nprint(Ridge_reg.best_estimator_)\n\n# best model\nbest_model = Ridge_reg.best_estimator_\nbest_model.fit(X_train,y_train)\ny_predicted = best_model.predict(X_test)\nprint(best_model.score(X_train,y_train)*100)\nprint(mean_squared_error(y_test, y_predicted, squared=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.3. SVR"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVR\n\nsvr = make_pipeline(RobustScaler(), SVR(kernel ='rbf' ,C= 20))\nsvr.fit(X_train,y_train)\ny_predicted = svr.predict(X_test)\nprint(svr.score(X_train,y_train)*100)\nprint(mean_squared_error(y_test, y_predicted, squared=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.4. Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\nDt = DecisionTreeRegressor(criterion='mse',max_depth=15, min_samples_split=5, min_samples_leaf=5,\n                           max_features=None, random_state=42)\nDt.fit(X_train,y_train)\ny_predicted = Dt.predict(X_test)\nprint(Dt.score(X_train,y_train)*100)\nprint(mean_squared_error(y_test, y_predicted, squared=False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can observe least MSE is with SVR model. So I'm going to use this model to predict my test_df."},{"metadata":{},"cell_type":"markdown","source":"## 7. Submission CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['SalePrice'] = np.exp(svr.predict(test_df))\ntest_df['Id'] = test_data['Id']\nPredicted_outcome=  test_df[['Id','SalePrice']]\nPredicted_outcome.to_csv(\"Predicted_outcome.csv\", index=False)\nPredicted_outcome.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}