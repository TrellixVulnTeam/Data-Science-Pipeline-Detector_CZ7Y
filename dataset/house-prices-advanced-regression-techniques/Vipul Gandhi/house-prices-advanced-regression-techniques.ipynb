{"cells":[{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#House-Prices:-Advanced-Regression-Techniques\" data-toc-modified-id=\"House-Prices:-Advanced-Regression-Techniques-1\">House Prices: Advanced Regression Techniques</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-1.1\">Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1.1.1\">Import Libraries</a></span></li><li><span><a href=\"#Extract-data-and-get-info-about-the-dataframe\" data-toc-modified-id=\"Extract-data-and-get-info-about-the-dataframe-1.1.2\">Extract data and get info about the dataframe</a></span></li><li><span><a href=\"#Check-for-duplicates\" data-toc-modified-id=\"Check-for-duplicates-1.1.3\">Check for duplicates</a></span></li><li><span><a href=\"#Visualising-missing-values\" data-toc-modified-id=\"Visualising-missing-values-1.1.4\">Visualising missing values</a></span></li><li><span><a href=\"#Estimate-Skewness-and-Kurtosis\" data-toc-modified-id=\"Estimate-Skewness-and-Kurtosis-1.1.5\">Estimate Skewness and Kurtosis</a></span></li><li><span><a href=\"#Analysing-'SalePrice'\" data-toc-modified-id=\"Analysing-'SalePrice'-1.1.6\">Analysing 'SalePrice'</a></span></li><li><span><a href=\"#Multicollinearity-Check\" data-toc-modified-id=\"Multicollinearity-Check-1.1.7\">Multicollinearity Check</a></span></li><li><span><a href=\"#Correlation-between--'SalePrice'-and-numeric-features\" data-toc-modified-id=\"Correlation-between--'SalePrice'-and-numeric-features-1.1.8\">Correlation between  'SalePrice' and numeric features</a></span></li><li><span><a href=\"#Correlation-between-'SalePrice'-and-'OverallQual'\" data-toc-modified-id=\"Correlation-between-'SalePrice'-and-'OverallQual'-1.1.9\">Correlation between 'SalePrice' and 'OverallQual'</a></span></li><li><span><a href=\"#Correlation-between-'SalePrice'-and-categorical-features\" data-toc-modified-id=\"Correlation-between-'SalePrice'-and-categorical-features-1.1.10\">Correlation between 'SalePrice' and categorical features</a></span></li><li><span><a href=\"#Normalizing-independent-variables\" data-toc-modified-id=\"Normalizing-independent-variables-1.1.11\">Normalizing independent variables</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-1.2\">Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Upfront-remove-unnecessary-columns(id's-etc.)\" data-toc-modified-id=\"Upfront-remove-unnecessary-columns(id's-etc.)-1.2.1\">Upfront remove unnecessary columns(id's etc.)</a></span></li><li><span><a href=\"#Remove-outliers\" data-toc-modified-id=\"Remove-outliers-1.2.2\">Remove outliers</a></span></li><li><span><a href=\"#Combine-train,-test-dataset\" data-toc-modified-id=\"Combine-train,-test-dataset-1.2.3\">Combine train, test dataset</a></span></li><li><span><a href=\"#Fill-missing-values\" data-toc-modified-id=\"Fill-missing-values-1.2.4\">Fill missing values</a></span><ul class=\"toc-item\"><li><span><a href=\"#Missing-numeric-values\" data-toc-modified-id=\"Missing-numeric-values-1.2.4.1\">Missing numeric values</a></span></li><li><span><a href=\"#Missing-object-values\" data-toc-modified-id=\"Missing-object-values-1.2.4.2\">Missing object values</a></span></li></ul></li><li><span><a href=\"#Create-interesting-features:\" data-toc-modified-id=\"Create-interesting-features:-1.2.5\">Create interesting features:</a></span></li><li><span><a href=\"#Feature-transformation---Categorical-to-ordinal\" data-toc-modified-id=\"Feature-transformation---Categorical-to-ordinal-1.2.6\">Feature transformation - Categorical to ordinal</a></span></li><li><span><a href=\"#Fix-skewed-features\" data-toc-modified-id=\"Fix-skewed-features-1.2.7\">Fix skewed features</a></span></li><li><span><a href=\"#Feature-transformation---Numeric-to-categorical\" data-toc-modified-id=\"Feature-transformation---Numeric-to-categorical-1.2.8\">Feature transformation - Numeric to categorical</a></span></li><li><span><a href=\"#Normalize-'SalePrice'\" data-toc-modified-id=\"Normalize-'SalePrice'-1.2.9\">Normalize 'SalePrice'</a></span></li><li><span><a href=\"#Getting-dummy-categorical-features\" data-toc-modified-id=\"Getting-dummy-categorical-features-1.2.10\">Getting dummy categorical features</a></span></li><li><span><a href=\"#Train-test-split\" data-toc-modified-id=\"Train-test-split-1.2.11\">Train test split</a></span></li></ul></li><li><span><a href=\"#Modelling\" data-toc-modified-id=\"Modelling-1.3\">Modelling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-1.3.1\">Linear Regression</a></span></li></ul></li></ul></li></ul></div>"},{"metadata":{},"cell_type":"markdown","source":"# House Prices: Advanced Regression Techniques\n<b> Kaggle : https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview </b>\n\n<b> Metric : Root-Mean-Squared-Error (RMSE) </b> between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)"},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\n\nhttps://scikit-learn.org/stable/modules/preprocessing.html\n\n - Univariate visualization, Bivariate visualization, Multivariate visualization\n \n   Identify: \n   \n - Trends\n - Distribution\n - Mean\n - Median\n - Outlier\n - Spread measurement (SD)\n - Correlations\n - Hypothesis testing\n - Visual Exploration"},{"metadata":{},"cell_type":"markdown","source":"### Import Libraries"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:57:45.539141Z","start_time":"2019-07-28T08:57:38.236323Z"},"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport scipy.stats as st\nfrom scipy.special import boxcox1p\n\nimport missingno as msno\nimport seaborn as sns\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport warnings\n\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-17T11:10:22.078975Z","start_time":"2019-07-17T11:10:22.075327Z"}},"cell_type":"markdown","source":"### Extract data and get info about the dataframe"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:57:47.986914Z","start_time":"2019-07-28T08:57:47.85032Z"},"trusted":true},"cell_type":"code","source":"df_raw_train = pd.read_csv('../input/train.csv')\ndf_raw_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:57:49.138917Z","start_time":"2019-07-28T08:57:49.131122Z"},"trusted":true},"cell_type":"code","source":"df_raw_train.shape, df_raw_test.shape","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:57:49.811994Z","start_time":"2019-07-28T08:57:49.784545Z"},"trusted":true},"cell_type":"code","source":"df_raw_train.info()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:57:50.423969Z","start_time":"2019-07-28T08:57:50.41658Z"},"trusted":true},"cell_type":"code","source":"df_raw_train.get_dtype_counts()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:57:51.225632Z","start_time":"2019-07-28T08:57:51.18481Z"},"trusted":true},"cell_type":"code","source":"# segregate numeric and categotical features\nnumeric_features = df_raw_train.select_dtypes(include=[np.number])\ncategorical_features = df_raw_train.select_dtypes(include=[np.object])","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:57:51.89067Z","start_time":"2019-07-28T08:57:51.808763Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', len(df_raw_train.columns))\ndisplay(df_raw_train.head())","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:57:52.576664Z","start_time":"2019-07-28T08:57:52.399736Z"},"trusted":true},"cell_type":"code","source":"df_raw_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check for duplicates"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:57:54.143051Z","start_time":"2019-07-28T08:57:54.136908Z"},"trusted":true},"cell_type":"code","source":"idsUnique = len(set(df_raw_train.Id))\nidsTotal = df_raw_train.shape[0]\nidsDupli = idsTotal - idsUnique\nprint(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualising missing values"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:57:56.827402Z","start_time":"2019-07-28T08:57:55.96043Z"},"trusted":true},"cell_type":"code","source":"# Visualizing the patterns of missing value occurrence in training set\n\nsns.heatmap(df_raw_train.isnull(), cbar=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\"BsmtX\", \"GarageX\" have missing values in same rows. We may predict that the house doesn't have these features."},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:02.312251Z","start_time":"2019-07-28T08:58:02.282056Z"},"trusted":true},"cell_type":"code","source":"missing_df = df_raw_train.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df['missing_ratio'] = missing_df['missing_count'] / df_raw_train.shape[0]\nmissing_df = missing_df.sort_values('missing_count', ascending = False)\nmissing_df.loc[missing_df['missing_count'] > 0]\n#missing_df","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:05.307458Z","start_time":"2019-07-28T08:58:05.225383Z"},"trusted":true},"cell_type":"code","source":"# selecting rows whose column value is null / None / nan\ndf_raw_train[df_raw_train['MasVnrType'].isna()]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:06.962823Z","start_time":"2019-07-28T08:58:05.96789Z"},"trusted":true},"cell_type":"code","source":"# missingno correlation heatmap measures nullity correlation: how strongly the presence/absence of one variable affects another variable\nmsno.heatmap(df_raw_train)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:08.766587Z","start_time":"2019-07-28T08:58:07.014031Z"},"trusted":true},"cell_type":"code","source":"# Dendrogram\n\nmsno.dendrogram(df_raw_train)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-16T16:57:08.973294Z","start_time":"2019-07-16T16:57:08.966779Z"}},"cell_type":"markdown","source":"### Estimate Skewness and Kurtosis\n\nhttps://towardsdatascience.com/intro-to-descriptive-statistics-252e9c464ac9"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:11.040388Z","start_time":"2019-07-28T08:58:10.720672Z"},"trusted":true},"cell_type":"code","source":"sns.distplot(df_raw_train.skew(),color='blue',axlabel ='Skewness')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In a distplot, y-axis is probability density and not probability, the y-axis can take values greater than one. The only requirement of the density plot is that the total area under the curve integrates to one."},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:12.927686Z","start_time":"2019-07-28T08:58:12.881143Z"},"trusted":true},"cell_type":"code","source":"skewness = df_raw_train.skew().sort_values(ascending = False)\nskewness","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:13.570337Z","start_time":"2019-07-28T08:58:13.560237Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"skew_features = df_raw_train[skewness[abs(skewness)>0.5].index]\nskew_features.columns","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:14.39544Z","start_time":"2019-07-28T08:58:14.392082Z"},"trusted":true},"cell_type":"code","source":"# We can treat skewness of a feature with the help of log transformation\n\n# skew_features = np.log1p(skew_features)\n\n# OR\n\n# We can use the scipy function boxcox1p which computes the Box-Cox transformation.\n# The goal is to find a simple transformation that lets us normalize data.\n\n# for i in skew_features:\n#    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1)) # all_features = train + test","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:15.202814Z","start_time":"2019-07-28T08:58:15.15259Z"},"trusted":true},"cell_type":"code","source":"kurtosis = df_raw_train.kurt().sort_values(ascending = False)\nkurtosis","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:16.118784Z","start_time":"2019-07-28T08:58:15.727453Z"},"scrolled":true,"trusted":true},"cell_type":"code","source":"sns.distplot(df_raw_train.kurt(),color='blue',axlabel ='Kurtosis')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-17T11:37:25.638852Z","start_time":"2019-07-17T11:37:25.635278Z"}},"cell_type":"markdown","source":"### Analysing 'SalePrice'"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:18.211936Z","start_time":"2019-07-28T08:58:18.199713Z"},"trusted":true},"cell_type":"code","source":"df_raw_train['SalePrice'].describe()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:20.006668Z","start_time":"2019-07-28T08:58:19.160489Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"# Histogram -  To get an idea of the distribution.\nplt.figure(figsize=(14,10))\nplt.subplot(2,2,1)\nplt.hist(df_raw_train['SalePrice'])\n\nplt.subplot(2,2,2)\nsns.distplot(df_raw_train['SalePrice'], color=\"r\", kde=True)\nplt.title(\"Distribution of Sale Price\")\nplt.ylabel(\"Number of Occurences\")\nplt.xlabel(\"Sale Price\")\n\nplt.subplot(2,2,3)\nplt.scatter(range(df_raw_train.shape[0]), df_raw_train[\"SalePrice\"].values,color='orange')\nplt.title(\"Distribution of Sale Price\")\nplt.xlabel(\"Number of Occurences\")\nplt.ylabel(\"Sale Price\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some outliers after Sale Price > 6000000"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:20.900542Z","start_time":"2019-07-28T08:58:20.895715Z"},"trusted":true},"cell_type":"code","source":"# Removing Outliers\n# upperlimit = np.percentile(df_raw_train.SalePrice.values, 99.5)\n# df_raw_train['SalePrice'].ix[houses['SalePrice']>upperlimit] = upperlimit","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:21.790725Z","start_time":"2019-07-28T08:58:21.783821Z"},"trusted":true},"cell_type":"code","source":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_raw_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_raw_train['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:22.433401Z","start_time":"2019-07-28T08:58:22.427053Z"},"trusted":true},"cell_type":"code","source":"(mu, sigma) = st.norm.fit(df_raw_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:23.665012Z","start_time":"2019-07-28T08:58:23.143621Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\n\nplt.subplot(2,2,1)\nplt.title('Normal')\nsns.distplot(df_raw_train['SalePrice'], kde=False, fit=st.norm)\n\nplt.subplot(2,2,2)\nst.probplot(df_raw_train['SalePrice'], plot=plt)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:24.726904Z","start_time":"2019-07-28T08:58:24.073686Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,8))\n\nplt.subplot(2,2,1)\nplt.title('Normal')\nsns.distplot(np.log1p(df_raw_train['SalePrice']), kde=False, fit=st.lognorm)\n\nplt.subplot(2,2,2)\nst.probplot(np.log1p(df_raw_train['SalePrice']), plot=plt)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is apparent that SalePrice doesn't follow normal distribution, so before performing regression it has to be transformed to log."},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:26.319926Z","start_time":"2019-07-28T08:58:26.315765Z"},"trusted":true},"cell_type":"code","source":"# df_raw_train.SalePrice = np.log1p(df_raw_train.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Multicollinearity Check\n\nMulticollinearity refers to features that are correlated with other features. Multicollinearity occurs when your model includes multiple factors that are correlated not just to your target variable, but also to each other.\n\nProblem:\n- Multicollinearity increases the standard errors of the coefficients. That means, multicollinearity makes some variables statistically insignificant when they should be significant.\n\nTo avoid this we can do 3 things:\n- Completely remove those variables\n- Make new feature by adding them or by some other operation.\n- Use PCA, which will reduce feature set to small number of non-collinear features."},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:28.212836Z","start_time":"2019-07-28T08:58:28.135472Z"},"trusted":true},"cell_type":"code","source":"correlations = df_raw_train.corr()\nattrs = correlations.iloc[:-1, :-1]  # all except target\n\nthreshold = 0.5\nimportant_corrs = (attrs[abs(attrs) > threshold][attrs != 1.0]) \\\n    .unstack().dropna().to_dict()\n\nunique_important_corrs = pd.DataFrame(\n    list(set([(tuple(sorted(key)), important_corrs[key])\n              for key in important_corrs])),\n    columns=['Attribute Pair', 'Correlation'])\n\n# sorted by absolute value\nunique_important_corrs = unique_important_corrs.ix[\n    abs(unique_important_corrs['Correlation']).argsort()[::-1]]\n\nunique_important_corrs","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation between  'SalePrice' and numeric features\n - Correlation Heat Map\n - Zoomed Heat Map\n - Pair Plot\n - Scatter Plot"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:30.389374Z","start_time":"2019-07-28T08:58:30.375008Z"},"trusted":true},"cell_type":"code","source":"correlation = numeric_features.corr()\nprint(correlation['SalePrice'].sort_values(ascending = False),'\\n')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b> Correlation Heat Map </b>"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:33.42865Z","start_time":"2019-07-28T08:58:32.31065Z"},"trusted":true},"cell_type":"code","source":"f , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of Numeric Features with Sale Price',y=1,size=16)\nsns.heatmap(correlation,square = True,  vmax=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At initial glance it is observed that there are two light-red colored squares.\n\n - The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables.\n - Second one refers to the 'GarageX' variables.We can conclude that they give almost the same information.(Multicollinearity)"},{"metadata":{},"cell_type":"markdown","source":"<b> Zoomed Heat Map </b>"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:36.299363Z","start_time":"2019-07-28T08:58:35.287694Z"},"trusted":true},"cell_type":"code","source":"k= 11\ntop_corr_features = correlation.nlargest(k,'SalePrice')['SalePrice'].index\nprint(top_corr_features)\ncm = np.corrcoef(df_raw_train[top_corr_features].values.T)\nf , ax = plt.subplots(figsize = (14,12))\nsns.heatmap(cm, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            linecolor=\"white\",xticklabels = top_corr_features.values ,annot_kws = {'size':10},yticklabels = top_corr_features.values)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:37.602546Z","start_time":"2019-07-28T08:58:36.302698Z"},"trusted":true},"cell_type":"code","source":"corrMatrix=df_raw_train[['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea',\n       'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt',\n       'YearRemodAdd', 'GarageYrBlt', 'MasVnrArea', 'Fireplaces']].corr()\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(14, 12))\n\nsns.heatmap(corrMatrix, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='viridis',linecolor=\"white\")\nplt.title('Correlation between features');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- GarageCars & GarageArea are closely correlated.\n- TotalBsmtSF and 1stFlrSF are also closely correlated."},{"metadata":{},"cell_type":"markdown","source":"<b> Pair Plot </b>\n - Check Outliers\n - Check relations b/w variables"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:58:55.845724Z","start_time":"2019-07-28T08:58:38.734673Z"},"trusted":true},"cell_type":"code","source":"sns.set()\ndf_raw_train_copy = df_raw_train.copy()\ndf_raw_train_copy['SalePrice'] = np.log1p(df_raw_train_copy['SalePrice'])\nsns.pairplot(df_raw_train[top_corr_features],size = 2 ,kind ='scatter',diag_kind='kde')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- One interesting observation is between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area.\n\n- One more interesting observation is between 'SalePrice' and 'YearBuilt'. In the bottom of the 'dots cloud', we see what almost appears to be a exponential function.We can also see this same tendency in the upper limit of the 'dots cloud'\n\n-  Last observation is that prices are increasing faster now with respect to previous years.\n\n-  Visible outliers in  'TotalBsmtSF', 'GrLivArea'"},{"metadata":{},"cell_type":"markdown","source":"<b> Scatter Plot </b>"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:04.900353Z","start_time":"2019-07-28T08:58:59.277764Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"columns = top_corr_features.drop('SalePrice')\nfor c in columns:\n    df = pd.concat([df_raw_train_copy['SalePrice'], df_raw_train_copy[c]], axis=1)\n    sns.lmplot(x=c, y=\"SalePrice\", data=df)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Quick Tip:</b>\n\ndf [['a', 'b']] vs df ['a']\n\ndf [['a', 'b']]:\n - dtype = dataframe\n - can select multiple columns from a dataframe\n \ndf ['a']:\n - dtype = series \n - can select single column from a dataframe"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-17T12:24:41.926065Z","start_time":"2019-07-17T12:24:41.914531Z"}},"cell_type":"markdown","source":"### Correlation between 'SalePrice' and 'OverallQual'"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:06.828798Z","start_time":"2019-07-28T08:59:06.810428Z"},"trusted":true},"cell_type":"code","source":"df_raw_train[['OverallQual', 'SalePrice']].groupby(['OverallQual'],\n                                                   as_index=False).mean().sort_values(by='OverallQual', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:08.436135Z","start_time":"2019-07-28T08:59:07.802539Z"},"trusted":true},"cell_type":"code","source":"sns.barplot(df_raw_train.OverallQual, df_raw_train.SalePrice)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:09.111534Z","start_time":"2019-07-28T08:59:08.66254Z"},"trusted":true},"cell_type":"code","source":"# boxplot is a method for graphically depicting groups of numerical data through their quartiles.\n\nvar = 'OverallQual'\ndata = pd.concat([df_raw_train['SalePrice'], df_raw_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(12, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:09.773319Z","start_time":"2019-07-28T08:59:09.533236Z"},"trusted":true},"cell_type":"code","source":"df_raw_train['OverallQual'].value_counts().plot(kind=\"bar\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Correlation between 'SalePrice' and categorical features"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:26.744574Z","start_time":"2019-07-28T08:59:10.826575Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"# Filling 'NaN' with string 'MISSING' in all categrical variables\n# Creating a copy, will impute relevant missing values to the main dataframe in the later section\n\n'''\nCopying advice\n\n# IS STILL POINTING TO ORIGINAL, EDITS TO NEW WILL ALSO BE APPLIED TO ORIGINAL\nnew_df = master_df \n\nnew_df[ 1,2 ] = b # will also update `master_df` as well. Careful!\n\n# makes a new copy to avoid this issue\nnew_df = df.copy()\n'''\n\ndf_raw_train_copy = df_raw_train.copy()\nfor c in categorical_features:\n    df_raw_train_copy[c] = df_raw_train_copy[c].astype('category')\n    if df_raw_train_copy[c].isnull().any():\n        df_raw_train_copy[c] = df_raw_train_copy[c].cat.add_categories(['MISSING'])\n        df_raw_train_copy[c] = df_raw_train_copy[c].fillna('MISSING')\n\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(df_raw_train_copy, id_vars=['SalePrice'], value_vars=categorical_features)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:27.64808Z","start_time":"2019-07-28T08:59:26.747678Z"},"trusted":true},"cell_type":"code","source":"var = 'Neighborhood'\ndata = pd.concat([df_raw_train['SalePrice'], df_raw_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 10))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nxt = plt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:28.047179Z","start_time":"2019-07-28T08:59:27.651788Z"},"scrolled":false,"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12, 6))\nsns.countplot(x = 'Neighborhood', data = data)\nxt = plt.xticks(rotation=45)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:31.878024Z","start_time":"2019-07-28T08:59:28.050435Z"},"trusted":true},"cell_type":"code","source":"data = pd.concat([df_raw_train['SalePrice'], df_raw_train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=df_raw_train['YearBuilt'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:32.162663Z","start_time":"2019-07-28T08:59:31.880725Z"},"trusted":true},"cell_type":"code","source":"sns.distplot(df_raw_train[\"YearBuilt\"], kde=False);","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:32.437276Z","start_time":"2019-07-28T08:59:32.16568Z"},"trusted":true},"cell_type":"code","source":"ConstructionAge =  df_raw_train['YrSold'] - df_raw_train['YearBuilt']\nplt.scatter(ConstructionAge, df_raw_train['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel(\"Construction Age of house\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Price of house goes down with its age."},{"metadata":{},"cell_type":"markdown","source":"### Normalizing independent variables\n\n- <b> Normality </b> - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n\n- <b> Homoscedasticity </b> - Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n- <b> Linearity </b> - The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n\n- <b> Absence of correlated errors </b> - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.\n\n"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-17T14:53:22.437809Z","start_time":"2019-07-17T14:53:22.434168Z"}},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Upfront remove unnecessary columns(id's etc.)"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:32.454979Z","start_time":"2019-07-28T08:59:32.440649Z"},"trusted":true},"cell_type":"code","source":"# Remove the Ids from train and test, as they are unique for each row and hence not useful for the model\ntrain_ID = df_raw_train['Id']\ntest_ID = df_raw_test['Id']\ndf_raw_train.drop(['Id'], axis=1, inplace=True)\ndf_raw_test.drop(['Id'], axis=1, inplace=True)\ndf_raw_train.shape, df_raw_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove outliers\n\n<b> look into 'TotalBsmtSF', 'GrLivArea' </b>\n\n - Outliers removal is note always safe. We decided to delete these two as they are very huge and really bad ( extremely large areas for very low prices).\n\n - There are probably others outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. That's why , instead of removing them all, we will just manage to make some of our models robust on them. You can refer to the modelling part of this notebook for that."},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:33.052464Z","start_time":"2019-07-28T08:59:32.459791Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\n\nplt.subplot(1,3,1)\nplt.title('GrLivArea')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.GrLivArea)\n\nplt.subplot(1,3,2)\nplt.title('TotalBsmtSF')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.TotalBsmtSF)\n\nplt.subplot(1,3,3)\nplt.title('OverallQual')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.OverallQual)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:33.072325Z","start_time":"2019-07-28T08:59:33.057102Z"},"trusted":true},"cell_type":"code","source":"df_raw_train.drop(df_raw_train[(df_raw_train['GrLivArea']>4000) & (df_raw_train['SalePrice']<300000)].index, inplace=True)\ndf_raw_train.drop(df_raw_train[(df_raw_train['OverallQual']<5) & (df_raw_train['SalePrice']>200000)].index, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:33.817604Z","start_time":"2019-07-28T08:59:33.074811Z"},"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,4))\n\nplt.subplot(1,3,1)\nplt.title('GrLivArea')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.GrLivArea)\n\nplt.subplot(1,3,2)\nplt.title('TotalBsmtSF')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.TotalBsmtSF)\n\nplt.subplot(1,3,3)\nplt.title('OverallQual')\nplt.scatter(y =df_raw_train.SalePrice,x = df_raw_train.OverallQual)\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:33.825608Z","start_time":"2019-07-28T08:59:33.820638Z"},"trusted":true},"cell_type":"code","source":"df_raw_train.reset_index(drop=True, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Combine train, test dataset\nCombine train and test features in order to apply the feature transformation pipeline to the entire dataset"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:33.838775Z","start_time":"2019-07-28T08:59:33.828971Z"},"trusted":true},"cell_type":"code","source":"y_train = df_raw_train.SalePrice\ndf_raw_train.drop('SalePrice', axis=1, inplace=True)\ndf_raw_train.shape, df_raw_test.shape","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:33.895762Z","start_time":"2019-07-28T08:59:33.841853Z"},"trusted":true},"cell_type":"code","source":"all_features = pd.concat([df_raw_train, df_raw_test]).reset_index(drop=True)\nall_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Quick Tip:</b>\nFeature Scaling:\n - StandardScaler - subtract the mean and divide by std\n - MaxAbsScaler - transform down to [-1, 1] bounds\n - QuantileTransformer - transform down to [0 1] bounds\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Fill missing values\n\n<b>Note the difference between NaN, '', None.</b>\n\n- NaN = not a number, still a float type, so think of it as an empty space that can still be passed through numerical operations\n\n- '' = is a empty string type\n\n- None = is also a empty space, but in DataFrames it is considered an object which cannot be processed through optimized numerical operations"},{"metadata":{},"cell_type":"markdown","source":"#### Missing numeric values\n<b>Try different strategies of filling in missing values (modes/means/medians/etc.)</b>"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:33.945952Z","start_time":"2019-07-28T08:59:33.898701Z"},"trusted":true},"cell_type":"code","source":"numeric_df =  all_features.select_dtypes(include=[np.number])\n\nmissing_numeric_df = numeric_df.isnull().sum(axis=0).reset_index()\nmissing_numeric_df.columns = ['column_name', 'missing_count']\nmissing_numeric_df['missing_ratio'] = missing_numeric_df['missing_count'] / numeric_df.shape[0]\nmissing_numeric_df = missing_numeric_df.sort_values('missing_count', ascending = False)\nmissing_numeric_df.loc[missing_numeric_df['missing_count'] > 0]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:33.98725Z","start_time":"2019-07-28T08:59:33.949052Z"},"trusted":true},"cell_type":"code","source":"# Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\nall_features['LotFrontage'] = all_features.groupby(\n    'Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# Replacing the missing values with 0, since no garage = no cars in garage\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        all_features[col] = all_features[col].fillna(0)\n        \n# Replacing the missing values with 0, since no basement = no bathrooms, no surface area\nfor col in ('BsmtHalfBath', 'BsmtFullBath', 'TotalBsmtSF', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF'):\n        all_features[col] = all_features[col].fillna(0)\n        \nall_features['MasVnrArea'] = all_features['MasVnrArea'].fillna(0)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Missing object values"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:34.050952Z","start_time":"2019-07-28T08:59:33.989545Z"},"trusted":true},"cell_type":"code","source":"object_df =  all_features.select_dtypes(include='object')\n\nmissing_object_df = object_df.isnull().sum(axis=0).reset_index()\nmissing_object_df.columns = ['column_name', 'missing_count']\nmissing_object_df['missing_ratio'] = missing_object_df['missing_count'] / object_df.shape[0]\nmissing_object_df = missing_object_df.sort_values('missing_count', ascending = False)\nmissing_object_df.loc[missing_object_df['missing_count'] > 0]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:34.126718Z","start_time":"2019-07-28T08:59:34.054146Z"},"trusted":true},"cell_type":"code","source":"# For a few columns there is lots of NaN entries.\n# However, reading the data description we find this is not missing data:\n# For PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.\n\ncols_fillna = ['PoolQC', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond', 'GarageQual', 'GarageFinish',\n               'GarageType', 'BsmtExposure', 'BsmtCond', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1', 'MasVnrType']\n\n# replace 'NaN' with 'None' in these columns\nfor col in cols_fillna:\n    all_features[col].fillna('None', inplace=True)\n    all_features[col].fillna('None', inplace=True)\nall_features['MSZoning'] = all_features.groupby(\n    'MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nall_features['Exterior1st'] = all_features['Exterior1st'].fillna(\n    all_features['Exterior1st'].mode()[0])\nall_features['Exterior2nd'] = all_features['Exterior2nd'].fillna(\n    all_features['Exterior2nd'].mode()[0])\nall_features['SaleType'] = all_features['SaleType'].fillna(\n    all_features['SaleType'].mode()[0])\n\n# the data description states that NA refers to typical ('Typ') values\nall_features['Functional'] = all_features['Functional'].fillna('Typ')\n\nall_features['KitchenQual'] = all_features['KitchenQual'].fillna(\"TA\")\n# Replacing missing values with most frequent ones.\nall_features['Electrical'] = all_features['Electrical'].fillna(\"SBrkr\")\n\nall_features[\"Utilities\"] = all_features[\"Utilities\"].fillna(\"None\")\nall_features['MiscFeature'] = all_features['MiscFeature'].fillna(\"None\")","execution_count":null,"outputs":[]},{"metadata":{"hide_input":true},"cell_type":"markdown","source":"### Create interesting features:\n\n- Simplifications of existing features\n- Combinations of existing features\n- Polynomials on the top 10 existing features"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:34.203841Z","start_time":"2019-07-28T08:59:34.13144Z"},"trusted":true},"cell_type":"code","source":"all_features['HasWoodDeck'] = all_features['WoodDeckSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['HasOpenPorch'] = all_features['OpenPorchSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['Has3SsnPorch'] = all_features['3SsnPorch'].apply(lambda x: 1 if x > 0 else 0)\nall_features['HasScreenPorch'] = all_features['ScreenPorch'].apply(lambda x: 1 if x > 0 else 0)\nall_features['haspool'] = all_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['has2ndfloor'] = all_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasgarage'] = all_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasbsmt'] = all_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_features['hasfireplace'] = all_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\n\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\n\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\n\nall_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\n\n#all_features['ConstructionAge'] = all_features['YrSold'] - all_features['YearBuilt']","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:34.632816Z","start_time":"2019-07-28T08:59:34.628945Z"},"trusted":true},"cell_type":"code","source":"#test = all_features.ix[all_features['ConstructionAge'] < 0]\n#test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Adding squares, sqrt is motivated by non-linearities in scatterplots \"predictor vs. log(SalePrice)/SalePrice\""},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:36.735056Z","start_time":"2019-07-28T08:59:36.240541Z"},"trusted":true},"cell_type":"code","source":"def addSquared(dataframe, column_list):\n    m = dataframe.shape[1]\n    for col in column_list:\n        dataframe = dataframe.assign(newcol=pd.Series(dataframe[col]*dataframe[col]).values)   \n        dataframe.columns.values[m] = col + '_sq'\n        m += 1\n    return dataframe \n\ndef addCubed(dataframe, column_list):\n    m = dataframe.shape[1]\n    for col in column_list:\n        dataframe = dataframe.assign(newcol=pd.Series(dataframe[col]*dataframe[col]*dataframe[col]).values)   \n        dataframe.columns.values[m] = col + '_cube'\n        m += 1\n    return dataframe \n\ndef addsqrt(dataframe, column_list):\n    m = dataframe.shape[1]\n    for col in column_list:\n        dataframe = dataframe.assign(newcol=pd.Series(np.sqrt(dataframe[col])).values)   \n        dataframe.columns.values[m] = col + '_sqrt'\n        m += 1\n    return dataframe \n\ncolumns = top_corr_features.drop('SalePrice')\naddSquared(all_features, columns)\naddCubed(all_features, columns)\naddsqrt(all_features, columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature transformation - Categorical to ordinal\n<b>Label Encoding some categorical variables that may contain information in their ordering set</b>"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:38.340358Z","start_time":"2019-07-28T08:59:38.17199Z"},"trusted":true},"cell_type":"code","source":"all_features['ExterQual'] = pd.Categorical(all_features['ExterQual'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po'], ordered=True).codes\nall_features['ExterCond'] = pd.Categorical(all_features['ExterCond'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po'], ordered=True).codes\nall_features['KitchenQual'] = pd.Categorical(all_features['KitchenQual'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po'], ordered=True).codes\nall_features['HeatingQC'] = pd.Categorical(all_features['HeatingQC'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po'], ordered=True).codes\nall_features['BsmtQual'] = pd.Categorical(all_features['BsmtQual'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], ordered=True).codes\nall_features['BsmtCond'] = pd.Categorical(all_features['BsmtCond'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], ordered=True).codes\nall_features['BsmtFinType1'] = pd.Categorical(all_features['BsmtFinType1'], categories=[\n    'GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None'], ordered=True).codes\nall_features['BsmtFinType2'] = pd.Categorical(all_features['BsmtFinType2'], categories=[\n    'GLQ', 'ALQ', 'BLQ', 'Rec', 'LwQ', 'Unf', 'None'], ordered=True).codes\nall_features['FireplaceQu'] = pd.Categorical(all_features['FireplaceQu'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], ordered=True).codes\nall_features['GarageQual'] = pd.Categorical(all_features['GarageQual'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], ordered=True).codes\nall_features['GarageCond'] = pd.Categorical(all_features['GarageCond'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], ordered=True).codes\nall_features['PoolQC'] = pd.Categorical(all_features['PoolQC'], categories=[\n    'Ex', 'Gd', 'TA', 'Fa', 'None'], ordered=True).codes\nall_features['LandSlope'] = pd.Categorical(all_features['LandSlope'], categories=[\n    'Gtl', 'Mod', 'Sev'], ordered=True).codes\nall_features['PavedDrive'] = pd.Categorical(all_features['PavedDrive'], categories=[\n    'Y', 'P', 'N'], ordered=True).codes\nall_features['GarageFinish'] = pd.Categorical(all_features['GarageFinish'], categories=[\n    'Fin', 'RFn', 'Unf', 'None'], ordered=True).codes\nall_features['BsmtExposure'] = pd.Categorical(all_features['BsmtExposure'], categories=[\n    'Gd', 'Av', 'Mn', 'No', 'None'], ordered=True).codes\nall_features['Functional'] = pd.Categorical(all_features['Functional'], categories=[\n    'Typ', 'Min1', 'Min2', 'Mod', 'Maj1', 'Maj2', 'Sev', 'Sal', 'None'], ordered=True).codes\nall_features['Fence'] = pd.Categorical(all_features['Fence'], categories=[\n    'GdPrv', 'MnPrv', 'GdWo', 'MnWw', 'None'], ordered=True).codes\nall_features['LotShape'] = pd.Categorical(all_features['LotShape'], categories=[\n    'Reg', 'IR1', 'IR2', 'IR3'], ordered=True).codes\n\n\nall_features['CentralAir'] = all_features['CentralAir'].apply(\n    lambda x: 0 if x == 'N' else 1)\nall_features['Street'] = all_features['Street'].apply(\n    lambda x: 0 if x == 'Pave' else 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Or, Let sklearn select the best ordering for them."},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:40.061268Z","start_time":"2019-07-28T08:59:40.05786Z"},"trusted":true},"cell_type":"code","source":"#from sklearn.preprocessing import LabelEncoder\n#cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',\n#        'ExterQual', 'ExterCond', 'HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1',\n#        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n#        'LotShape', 'PavedDrive', 'CentralAir')\n# process columns, apply LabelEncoder to categorical features\n#for c in cols:\n#    lbl = LabelEncoder()\n#    lbl.fit(list(all_features[c].values))\n#    all_features[c] = lbl.transform(list(all_features[c].values))\n\n# shape\n#print('Shape all_data: {}'.format(all_features.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<b>Quick Tip </b>\nfit vs. fit_transform vs. transform\n - fit_transform\n   - Xtrain−norm=Xtrain−μtrainσtrain\n - transform - note that we divide by the previously fit values\n   - Xtest−norm=Xtest−μtrainσtrain\n - fit\n   - when you fit a scaler to dataset A, it calculates mean of A, and the standard deviation of A \n - transform\n   - this will actually look at ANY dataset and subtract previously fitted (calculated) variables mean A and divide by standard deviation of A fit_transform does both of these things in two steps.\n   \n   \nFor consistency purposes, it is best to fit_transform on your training dataset, but only transform your validation set. This ensures your validation and training set has been consistently transformed"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-18T16:22:29.280708Z","start_time":"2019-07-18T16:22:29.276556Z"}},"cell_type":"markdown","source":"### Fix skewed features\nFixing skewness is motivated by the fact that:\n- linear methods might fit such predictors with very small weights and most of the information contained in the values might be lost\n- predictions when such predictors take very high values might be also very high or misleading.\n\n<b>http://onlinestatbook.com/2/transformations/box-cox.html</b>\n\n<b>https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.special.boxcox1p.html</b>"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:44.292251Z","start_time":"2019-07-28T08:59:43.617267Z"},"trusted":true},"cell_type":"code","source":"skewness = all_features.skew().sort_values(ascending = False)\nskew_features = all_features[skewness[abs(skewness)>0.5].index]\nfor i in skew_features:\n    all_features[i] = boxcox1p(all_features[i], st.boxcox_normmax(all_features[i] + 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Or, we may let original skewed columns in the dataframe and add additional log/boxcox(column) as new columns."},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:45.234788Z","start_time":"2019-07-28T08:59:45.230864Z"},"trusted":true},"cell_type":"code","source":"#def add_log_columns(dataframe, column_list):\n#    m = dataframe.shape[1]\n#    for column_name in column_list:\n#        dataframe = dataframe.assign(newcol=pd.Series(np.log1p(dataframe[column_name])).values)\n#        dataframe.columns.values[m] = column_name + '_log'\n#        m += 1\n#    return dataframe\n\n#all_features = add_log_columns(all_features, skew_features)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:45.908023Z","start_time":"2019-07-28T08:59:45.904546Z"},"trusted":true},"cell_type":"code","source":"#def add_boxcox_columns(dataframe, column_list):\n#    m = dataframe.shape[1]\n#    for column_name in column_list:\n#        dataframe = dataframe.assign(newcol=pd.Series(boxcox1p(\n#       all_features[i], st.boxcox_normmax(all_features[i] + 1))).values)\n#        dataframe.columns.values[m] = column_name + '_log'\n#        m += 1\n#    return dataframe\n\n#all_features = add_boxcox_columns(all_features, skew_features)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature transformation - Numeric to categorical\n\n<b>Transforming some numerical variables that are really categorical</b>"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:47.743343Z","start_time":"2019-07-28T08:59:47.721207Z"},"trusted":true},"cell_type":"code","source":"# MSSubClass = building class\nall_features['MSSubClass'] = all_features['MSSubClass'].astype(str)\n\n\n#Changing OverallCond into a categorical variable\nall_features['OverallCond'] = all_features['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normalize 'SalePrice'\n- The SalePrice is skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data. We can apply a log(1+x) tranform to fix the skew.\n- Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally."},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:50.078698Z","start_time":"2019-07-28T08:59:50.072122Z"},"trusted":true},"cell_type":"code","source":"y_train = np.log1p(y_train)\n(mu, sigma) = st.norm.fit(y_train)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:50.867906Z","start_time":"2019-07-28T08:59:50.848098Z"},"trusted":true},"cell_type":"code","source":"# Remove any duplicated column names\nall_features = all_features.loc[:,~all_features.columns.duplicated()]\nall_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Getting dummy categorical features"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:52.355314Z","start_time":"2019-07-28T08:59:52.234838Z"},"trusted":true},"cell_type":"code","source":"all_features = pd.get_dummies(all_features).reset_index(drop=True)\nall_features.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train test split"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:53.897185Z","start_time":"2019-07-28T08:59:53.888818Z"},"trusted":true},"cell_type":"code","source":"X_train = all_features[:df_raw_train.shape[0]]\nX_test = all_features[df_raw_train.shape[0]:]\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T08:59:55.980732Z","start_time":"2019-07-28T08:59:55.968862Z"},"trusted":true},"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.2)\nprint(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-07-27T13:48:57.05396Z","start_time":"2019-07-27T13:48:57.050495Z"}},"cell_type":"markdown","source":"### Linear Regression"},{"metadata":{"ExecuteTime":{"end_time":"2019-07-28T09:04:34.026556Z","start_time":"2019-07-28T09:04:33.973457Z"},"trusted":true},"cell_type":"code","source":"lm = LinearRegression()\nlm.fit(X_train,y_train)\ny_pred = lm.predict(X_valid)\nrmse = np.sqrt(metrics.mean_squared_error(y_pred, y_valid))\n\nlm.score(X_train,y_train), lm.score(X_valid,y_valid), rmse","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ridge Regression\n\nhttps://www.quora.com/What-is-Ridge-Regression-in-laymans-terms\n    \nhttps://www.youtube.com/watch?v=Q81RR3yKn30"},{"metadata":{},"cell_type":"markdown","source":"Lasso\n\nhttps://chrisalbon.com/machine_learning/linear_regression/effect_of_alpha_on_lasso_regression/"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}