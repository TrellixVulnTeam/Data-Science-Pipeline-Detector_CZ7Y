{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nThe following kernel shows how to run a regression using [LightGBM](https://lightgbm.readthedocs.io/en/latest/). It also runs some feature engineering to improve the score.","metadata":{"_uuid":"9799d5a6b1b727e72d9c1b107890cab57ab8ecc1"}},{"cell_type":"code","source":"import math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import norm\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, Imputer\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nimport os\nprint(os.listdir(\"../input\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-23T16:31:19.51949Z","iopub.execute_input":"2021-08-23T16:31:19.520073Z","iopub.status.idle":"2021-08-23T16:31:20.671217Z","shell.execute_reply.started":"2021-08-23T16:31:19.520022Z","shell.execute_reply":"2021-08-23T16:31:20.670089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration\nThe following is a brief exploratory data analysis (EDA) of the data. There are other kernels on the competition page which do this more justice. Here I just focus on a summary of the data, the missing values and looking at the target. Normally we would also look at correlations, this has been done outside of this notebook and is used later in aspects of the feature engineering.","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train.csv\")\ndf_test = pd.read_csv(\"../input/test.csv\")","metadata":{"_uuid":"3ac39eaf527dfd510911b350575dcf07d1d4dd93","execution":{"iopub.status.busy":"2021-08-23T16:31:20.67306Z","iopub.execute_input":"2021-08-23T16:31:20.673735Z","iopub.status.idle":"2021-08-23T16:31:20.756282Z","shell.execute_reply.started":"2021-08-23T16:31:20.673669Z","shell.execute_reply":"2021-08-23T16:31:20.755367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.sample(3)","metadata":{"_uuid":"245a227c96a3e647907618fab5b60ad6481a637b","execution":{"iopub.status.busy":"2021-08-23T16:31:20.757715Z","iopub.execute_input":"2021-08-23T16:31:20.758072Z","iopub.status.idle":"2021-08-23T16:31:20.906517Z","shell.execute_reply.started":"2021-08-23T16:31:20.758007Z","shell.execute_reply":"2021-08-23T16:31:20.905662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.sample(3)","metadata":{"_uuid":"e6de15085d126b863f1b7a917da6e5d70c486aa3","execution":{"iopub.status.busy":"2021-08-23T16:31:20.908638Z","iopub.execute_input":"2021-08-23T16:31:20.908993Z","iopub.status.idle":"2021-08-23T16:31:21.040328Z","shell.execute_reply.started":"2021-08-23T16:31:20.90892Z","shell.execute_reply":"2021-08-23T16:31:21.039402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape train: %s, test: %s\" % (df_train.shape, df_test.shape))","metadata":{"_uuid":"fe6bbb7c5e42c2c227b7b39d05c87c7e7765af3e","execution":{"iopub.status.busy":"2021-08-23T16:31:21.043583Z","iopub.execute_input":"2021-08-23T16:31:21.043905Z","iopub.status.idle":"2021-08-23T16:31:21.050233Z","shell.execute_reply.started":"2021-08-23T16:31:21.043851Z","shell.execute_reply":"2021-08-23T16:31:21.049175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we have ~1500 records with 80 features. Lets look at some basic statistics of the current numerical features.","metadata":{"_uuid":"c29625ef13fdc3c293809f2ff4c55abe3be475d7"}},{"cell_type":"code","source":"pd.options.display.max_columns = None # Show all cols\ndf_train.describe()","metadata":{"_uuid":"c43ddd7a9ac480e813ef2a8bad67691525a5a987","execution":{"iopub.status.busy":"2021-08-23T16:31:21.051989Z","iopub.execute_input":"2021-08-23T16:31:21.052617Z","iopub.status.idle":"2021-08-23T16:31:21.18754Z","shell.execute_reply.started":"2021-08-23T16:31:21.052535Z","shell.execute_reply":"2021-08-23T16:31:21.186624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Data\nFrom the above we can see any missing values (in the numeric fields), that will later need imputing and any significant outliers will also need to be dealt with. For example \"LotFrontage\" is missing on 259 rows. The max sales price is over 7 standard deviations from the mean which would suggest outliers. Lets look at the top N features with missing values.","metadata":{"_uuid":"8ae460e074a2aed61de23957c8d29b2cb4560a4e"}},{"cell_type":"code","source":"df_na = (df_train.isnull().sum() / len(df_train)) * 100\ndf_na = df_na.drop(df_na[df_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :df_na})\nmissing_data.head(10)","metadata":{"_uuid":"fe7beb3725eedf30fab83781f1d77ffb84a4eab0","execution":{"iopub.status.busy":"2021-08-23T16:31:21.188849Z","iopub.execute_input":"2021-08-23T16:31:21.189349Z","iopub.status.idle":"2021-08-23T16:31:21.310294Z","shell.execute_reply.started":"2021-08-23T16:31:21.189291Z","shell.execute_reply":"2021-08-23T16:31:21.309365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Target analysis\nTarget distribution is skewed and can be seen in the plots below - this is generally not a good thing. So we will need to adjust it so its normally distributed. This can be achieved with a log transform or something more powerful like box cox. On the diagram below, the left pane shows the original (skewed) data. The right pane show the transformed data.","metadata":{"_uuid":"cf6ecb4e07a18ab38af31a06b144f4e9d935fb74"}},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2)\nwidth, height = fig.get_size_inches()\nfig.set_size_inches(width*2, height)\nsns.distplot(df_train['SalePrice'], ax=ax[0], fit=norm)\nsns.distplot(np.log(df_train[('SalePrice')]+1), ax=ax[1], fit= norm)","metadata":{"_uuid":"a3640466fc8cfa71750c7294afdd52d36038ae0e","execution":{"iopub.status.busy":"2021-08-23T16:31:21.31149Z","iopub.execute_input":"2021-08-23T16:31:21.312011Z","iopub.status.idle":"2021-08-23T16:31:21.991259Z","shell.execute_reply.started":"2021-08-23T16:31:21.311952Z","shell.execute_reply":"2021-08-23T16:31:21.990364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Engineering\nWe have to do some work to get the data into a format that will work with LightGBM. This covers:\n* Handling categoricals\n* Handling numericals\n* Feature engineering - To generate new features\n\nThis would normally be packaged into some form of utility library as a separate step in the ML pipeline. In production setups this would typically be either Python or perhaps Spark for larger data sets.\n## Basic data engineering\nFirst lets define some useful functions. Again this *should* be encapsulated in an external function library. For simplicity these are defined here.","metadata":{"_uuid":"154383db7019ff11a2fc996326710ffbf337acf8"}},{"cell_type":"code","source":"def fill_missing(df, cols, val):\n    \"\"\" Fill with the supplied val \"\"\"\n    for col in cols:\n        df[col] = df[col].fillna(val)\n\ndef fill_missing_with_mode(df, cols):\n    \"\"\" Fill with the mode \"\"\"\n    for col in cols:\n        df[col] = df[col].fillna(df[col].mode()[0])\n        \ndef addlogs(res, cols):\n    \"\"\" Log transform feature list\"\"\"\n    m = res.shape[1]\n    for c in cols:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[c])).values)   \n        res.columns.values[m] = c + '_log'\n        m += 1\n    return res","metadata":{"_uuid":"0b3e9595cc62c1e73b227225b3c299c07f5bb6e4","execution":{"iopub.status.busy":"2021-08-23T16:31:21.992652Z","iopub.execute_input":"2021-08-23T16:31:21.993234Z","iopub.status.idle":"2021-08-23T16:31:22.001475Z","shell.execute_reply.started":"2021-08-23T16:31:21.993172Z","shell.execute_reply":"2021-08-23T16:31:22.000083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some basic calculated cols","metadata":{"_uuid":"e283168527f05658d79cabe8ff666045e59a2f6a"}},{"cell_type":"code","source":"df_train['TotalSF'] = df_train['TotalBsmtSF'] + df_train['1stFlrSF'] + df_train['2ndFlrSF']","metadata":{"_uuid":"4793a8dad38ce3bfcd027bec79ea68ab38704915","execution":{"iopub.status.busy":"2021-08-23T16:31:22.003268Z","iopub.execute_input":"2021-08-23T16:31:22.004069Z","iopub.status.idle":"2021-08-23T16:31:22.016062Z","shell.execute_reply.started":"2021-08-23T16:31:22.004001Z","shell.execute_reply":"2021-08-23T16:31:22.015219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add Log transform columns for simple integer features.","metadata":{"_uuid":"5ff86f111d652a5513bcbb2fc76d8695f3f3d0a0"}},{"cell_type":"code","source":"loglist = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n\ndf_train = addlogs(df_train, loglist)","metadata":{"_uuid":"e6a0d222b61367b96e638807342162ae7775bc94","execution":{"iopub.status.busy":"2021-08-23T16:31:22.017638Z","iopub.execute_input":"2021-08-23T16:31:22.018338Z","iopub.status.idle":"2021-08-23T16:31:22.074132Z","shell.execute_reply.started":"2021-08-23T16:31:22.01827Z","shell.execute_reply":"2021-08-23T16:31:22.073379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For sale price we have effectively a real valued number, so we need to use [log1p](https://docs.scipy.org/doc/numpy/reference/generated/numpy.log1p.html) to ensure the log transform is accurate. This is particularly important when the numbers are small but is just good practice for real numbers.","metadata":{"_uuid":"78df3f81bfaabc0bd0d2b6bdc8b481ec113412c6"}},{"cell_type":"code","source":"df_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])","metadata":{"_uuid":"1b10058ee0f116f7d52c7f0f5d7cd966f0c1aed9","execution":{"iopub.status.busy":"2021-08-23T16:31:22.07541Z","iopub.execute_input":"2021-08-23T16:31:22.076016Z","iopub.status.idle":"2021-08-23T16:31:22.081501Z","shell.execute_reply.started":"2021-08-23T16:31:22.075954Z","shell.execute_reply":"2021-08-23T16:31:22.080372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now impute the missing values with something sensible","metadata":{"_uuid":"3500ea021145fff76e3d1ae00691a507797b29b4"}},{"cell_type":"code","source":"fill_missing(df_train, [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \n                        \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n                       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n                       \"MasVnrType\", \"MSSubClass\"], \"None\")\nfill_missing(df_train, [\"GarageYrBlt\", \"GarageArea\", \"GarageCars\",\n                       'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n                       \"MasVnrArea\"], 0)\nfill_missing_with_mode(df_train, [\"MSZoning\", \"KitchenQual\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\"])\nfill_missing(df_train, [\"Functional\"],\"Typ\")\n# Utils is pointless as there is only one row with a value\ndf_train.drop(['Utilities'], axis=1, inplace=True)\n# For lot frontage we take the median of the neighbourhood. In general this would be a good approximation as most \n# house co located are similar in size \ndf_train[\"LotFrontage\"] = df_train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","metadata":{"_uuid":"11d99afee296f3fc2422d92f027e5ccc8d2eb920","execution":{"iopub.status.busy":"2021-08-23T16:31:22.082717Z","iopub.execute_input":"2021-08-23T16:31:22.083053Z","iopub.status.idle":"2021-08-23T16:31:22.125777Z","shell.execute_reply.started":"2021-08-23T16:31:22.083003Z","shell.execute_reply":"2021-08-23T16:31:22.124909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Remove outliers. These can seriously mess up a model so its best to either cap them, or drop them. Here we drop them.","metadata":{"_uuid":"ab7145643ad97f154a142bac5ea47fcc5f4018cb"}},{"cell_type":"code","source":"df_train.drop(df_train[(df_train['OverallQual']<5) & (df_train['SalePrice']>200000)].index, inplace=True)\ndf_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index, inplace=True)\ndf_train.reset_index(drop=True, inplace=True)","metadata":{"_uuid":"ba294311620c3ab84587e7bceeddaab41a478c31","execution":{"iopub.status.busy":"2021-08-23T16:31:22.129068Z","iopub.execute_input":"2021-08-23T16:31:22.129309Z","iopub.status.idle":"2021-08-23T16:31:22.1397Z","shell.execute_reply.started":"2021-08-23T16:31:22.129265Z","shell.execute_reply":"2021-08-23T16:31:22.138708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And finally there are some fields that are categorical and we should not treat them as numbers. So we have to convert non-numeric -> string where approriate ","metadata":{"_uuid":"2f1e1186487845144de55c420e8d77c3c13b80db"}},{"cell_type":"code","source":"df_train['MSSubClass'] = df_train['MSSubClass'].apply(str)\ndf_train['YrSold'] = df_train['YrSold'].astype(str)\ndf_train['MoSold'] = df_train['MoSold'].astype(str)","metadata":{"_uuid":"343872792f61aa1bc457e8217f6f449ef0896e12","execution":{"iopub.status.busy":"2021-08-23T16:31:22.141246Z","iopub.execute_input":"2021-08-23T16:31:22.141564Z","iopub.status.idle":"2021-08-23T16:31:22.154179Z","shell.execute_reply.started":"2021-08-23T16:31:22.141519Z","shell.execute_reply":"2021-08-23T16:31:22.153026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Handle categoricals\nFirst some util functions to dummy encode the categoricals. LightGBM can handle these natively but for now we do it manually as this could then easily be applied a pre process step for other algorithms.","metadata":{"_uuid":"e8c053e13278f1e58d1fee3505b1322ed565fd78"}},{"cell_type":"code","source":"def fix_missing_cols(in_train, in_test):\n    missing_cols = set(in_train.columns) - set(in_test.columns)\n    # Add a missing column in test set with default value equal to 0\n    for c in missing_cols:\n        in_test[c] = 0\n    # Ensure the order of column in the test set is in the same order than in train set\n    in_test = in_test[in_train.columns]\n    return in_test\n\ndef dummy_encode(in_df_train, in_df_test):\n    df_train = in_df_train\n    df_test = in_df_test\n    categorical_feats = [\n        f for f in df_train.columns if df_train[f].dtype == 'object'\n    ]\n    print(categorical_feats)\n    for f_ in categorical_feats:\n        prefix = f_\n        df_train = pd.concat([df_train, pd.get_dummies(df_train[f_], prefix=prefix)], axis=1).drop(f_, axis=1)\n        df_test = pd.concat([df_test, pd.get_dummies(df_test[f_], prefix=prefix)], axis=1).drop(f_, axis=1)\n        df_test = fix_missing_cols(df_train, df_test)\n    return df_train, df_test\n","metadata":{"_uuid":"ee001ffa0d532c3b9535ca3a9eeb761c83973077","execution":{"iopub.status.busy":"2021-08-23T16:31:22.155921Z","iopub.execute_input":"2021-08-23T16:31:22.156294Z","iopub.status.idle":"2021-08-23T16:31:22.170541Z","shell.execute_reply.started":"2021-08-23T16:31:22.156194Z","shell.execute_reply":"2021-08-23T16:31:22.169604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train, df_test = dummy_encode(df_train, df_test)\nprint(\"Shape train: %s, test: %s\" % (df_train.shape, df_test.shape))","metadata":{"_uuid":"ebcb5a3ed099bdb989ced308d5bfcd8b4939dcdf","execution":{"iopub.status.busy":"2021-08-23T16:31:22.172189Z","iopub.execute_input":"2021-08-23T16:31:22.172679Z","iopub.status.idle":"2021-08-23T16:31:23.140202Z","shell.execute_reply.started":"2021-08-23T16:31:22.172562Z","shell.execute_reply":"2021-08-23T16:31:23.139522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Additional Feature Engineering\nAdditional daa engineering often involves some complex computations. For small data sets like this its not a problem. For larger data sets you need to verify the performance vs the change in performance / accuracy after adding the features. This should be in terms of both accuracy and time to train. If there is a real benefit, then these can be applied but could be perhaps be done as a pre procesing step (eg via a Spark Job). This depends on the feature / data but the point is to ensure you test with and without the new feature.\n\n### Interaction Terms\nFirst generate some interaction terms based on the highest correlated features (these were pre-computed). \n\nSee https://en.wikipedia.org/wiki/Interaction_(statistics)\n\nAnalysis of the features selected here showed they were more correlated (either +ve or -ve) with the sales price. This can be seen by looking using the [dataframe.corr](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html) method, then selecting the top N features. All the ones here had a positive correlation over 0.6.","metadata":{"_uuid":"bafeb75f45394d0a2aec6716a65c17447fb5dabf"}},{"cell_type":"code","source":"def load_poly_features(df_train, df_test, cols):\n    \"\"\"\n    USeful function to generate poly terms\n    :param df_train: The training data frame\n    :param df_test: The test data frame\n    :return: df_poly_features, df_poly_features_test - The training polynomial features + the test\n    \"\"\"\n    print('Loading polynomial features..')\n    # Make a new dataframe for polynomial features\n    poly_features = df_train[cols]\n    poly_features_test = df_test[cols]\n\n    # imputer for handling missing values\n    imputer = Imputer(strategy='median')\n\n    # Need to impute missing values\n    poly_features = imputer.fit_transform(poly_features)\n    poly_features_test = imputer.transform(poly_features_test)\n\n    # Create the polynomial object with specified degree\n    poly_transformer = PolynomialFeatures(degree=3)\n    # Train the polynomial features\n    poly_transformer.fit(poly_features)\n\n    # Transform the features\n    poly_features = poly_transformer.transform(poly_features)\n    poly_features_test = poly_transformer.transform(poly_features_test)\n    print('Polynomial Features shape: %s' % str(poly_features.shape))\n\n    df_poly_features = pd.DataFrame(poly_features,\n                                    columns=poly_transformer.get_feature_names(cols))\n    df_poly_features_test = pd.DataFrame(poly_features_test,\n                                         columns=poly_transformer.get_feature_names(cols))\n    df_poly_features['Id'] = df_train['Id']\n    df_poly_features_test['Id'] = df_test['Id']\n    print('Loaded polynomial features')\n    return df_poly_features, df_poly_features_test","metadata":{"_uuid":"4a2b8c9fd9a05e32cd9b873aa2bfe921fad99c64","execution":{"iopub.status.busy":"2021-08-23T16:31:23.141105Z","iopub.execute_input":"2021-08-23T16:31:23.141459Z","iopub.status.idle":"2021-08-23T16:31:23.147666Z","shell.execute_reply.started":"2021-08-23T16:31:23.141419Z","shell.execute_reply":"2021-08-23T16:31:23.146963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlated_cols = ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF']\ndf_train_poly, df_test_poly =  load_poly_features(df_train, df_test, cols=correlated_cols)\nprint(\"Shape train: %s, test: %s\" % (df_train_poly.shape, df_test_poly.shape))","metadata":{"_uuid":"94f1f932e6a7a05ea358eb279b1e1781227a92c6","execution":{"iopub.status.busy":"2021-08-23T16:31:23.148524Z","iopub.execute_input":"2021-08-23T16:31:23.148893Z","iopub.status.idle":"2021-08-23T16:31:23.187637Z","shell.execute_reply.started":"2021-08-23T16:31:23.148848Z","shell.execute_reply":"2021-08-23T16:31:23.186769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = df_train.merge(right=df_train_poly.reset_index(), how='left', on='Id')\ndf_test = df_test.merge(right=df_test_poly.reset_index(), how='left', on='Id')","metadata":{"_uuid":"44428aeaa983f04e1ca423fe07210cfea93e9d3c","execution":{"iopub.status.busy":"2021-08-23T16:31:23.188897Z","iopub.execute_input":"2021-08-23T16:31:23.189384Z","iopub.status.idle":"2021-08-23T16:31:23.249062Z","shell.execute_reply.started":"2021-08-23T16:31:23.189327Z","shell.execute_reply":"2021-08-23T16:31:23.248098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape train: %s, test: %s\" % (df_train.shape, df_test.shape))","metadata":{"_uuid":"1a5cfdf81883861e128272db5a96682247ee32e3","execution":{"iopub.status.busy":"2021-08-23T16:31:23.250423Z","iopub.execute_input":"2021-08-23T16:31:23.250779Z","iopub.status.idle":"2021-08-23T16:31:23.255063Z","shell.execute_reply.started":"2021-08-23T16:31:23.250712Z","shell.execute_reply":"2021-08-23T16:31:23.254267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So in the end our features have gone from the original 80 to 446. The number of records has slightly reduced as we dropped the outliers.[](http://)","metadata":{"_uuid":"6e41a567f2a186ad7a23bdbe54a9388c53ec0776"}},{"cell_type":"markdown","source":"# Light GBM\nNow lets run our regression!\n\nFirst lets split up the data into our training data (X_train), our testing data frame (X_test) and our target variables that we want to predict for both training and testing (y_train and y_test respectively).","metadata":{"_uuid":"c9c880e34f1acf1df28072f78e9c84627304c43e"}},{"cell_type":"code","source":"y = df_train[\"SalePrice\"]\ny.sample(3)","metadata":{"_uuid":"1e5aa0db98379c50d9f67bf6c8938cc2a126ab84","execution":{"iopub.status.busy":"2021-08-23T16:31:23.256906Z","iopub.execute_input":"2021-08-23T16:31:23.257154Z","iopub.status.idle":"2021-08-23T16:31:23.27504Z","shell.execute_reply.started":"2021-08-23T16:31:23.257106Z","shell.execute_reply":"2021-08-23T16:31:23.274284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.drop([\"SalePrice\"], axis=1, inplace=True)\n# The fix missing cols above will have added the target column to the test data frame, so this is a workaround to remove it\ndf_test.drop([\"SalePrice\"], axis=1, inplace=True) ","metadata":{"_uuid":"0c5ee51293c79900615eb68d55059b1654c68981","execution":{"iopub.status.busy":"2021-08-23T16:31:23.276093Z","iopub.execute_input":"2021-08-23T16:31:23.276498Z","iopub.status.idle":"2021-08-23T16:31:23.293744Z","shell.execute_reply.started":"2021-08-23T16:31:23.276439Z","shell.execute_reply":"2021-08-23T16:31:23.292582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape train: %s, test: %s\" % (df_train.shape, df_test.shape))","metadata":{"_uuid":"8d6ab8b4c5a36b902b04b49ccc4b0d7e1ea24b53","execution":{"iopub.status.busy":"2021-08-23T16:31:23.295317Z","iopub.execute_input":"2021-08-23T16:31:23.295663Z","iopub.status.idle":"2021-08-23T16:31:23.300262Z","shell.execute_reply.started":"2021-08-23T16:31:23.29557Z","shell.execute_reply":"2021-08-23T16:31:23.299551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split the data set into training and testing data with a fixed random value.","metadata":{"_uuid":"2b4988d425d5b0ebf2e7ade349b16f24b05914b2"}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split( df_train, y, test_size=0.2, random_state=42)","metadata":{"_uuid":"3e7cb1bb609f4270769076c22ec631a167ac66f0","execution":{"iopub.status.busy":"2021-08-23T16:31:23.301724Z","iopub.execute_input":"2021-08-23T16:31:23.301994Z","iopub.status.idle":"2021-08-23T16:31:23.316538Z","shell.execute_reply.started":"2021-08-23T16:31:23.301943Z","shell.execute_reply":"2021-08-23T16:31:23.315719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The hyper parameter settings are below. The settings below are best on a few iterations of training with some guided attempts driven by the documentation on the LightGBM website. They are far from the optimum..\n\nIn a real application we would adjust these to see the impact on loss - over numerous iterations. This would either be manual or more likely use a tool like [Optunity](https://optunity.readthedocs.io/en/latest/) or Hyperopt, to run automated hyper parameter tuning.","metadata":{"_uuid":"3c10c801e2efdf78eb3bcbe28eeabd838ceb7095"}},{"cell_type":"code","source":"hyper_params = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': ['l1','l2'],\n    'learning_rate': 0.005,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 10,\n    'verbose': 0,\n    \"max_depth\": 8,\n    \"num_leaves\": 128,  \n    \"max_bin\": 512,\n    \"num_iterations\": 100000\n}","metadata":{"_uuid":"4e61dd9377fcbea03fcb78f86a208d51d576c554","execution":{"iopub.status.busy":"2021-08-23T16:31:23.317975Z","iopub.execute_input":"2021-08-23T16:31:23.318258Z","iopub.status.idle":"2021-08-23T16:31:23.324891Z","shell.execute_reply.started":"2021-08-23T16:31:23.318206Z","shell.execute_reply":"2021-08-23T16:31:23.324212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbm = lgb.LGBMRegressor(**hyper_params)","metadata":{"_uuid":"147172da1ddde9ab949bbbafd3f6e56ff75dfeeb","execution":{"iopub.status.busy":"2021-08-23T16:31:23.326048Z","iopub.execute_input":"2021-08-23T16:31:23.326339Z","iopub.status.idle":"2021-08-23T16:31:23.337516Z","shell.execute_reply.started":"2021-08-23T16:31:23.326282Z","shell.execute_reply":"2021-08-23T16:31:23.33687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gbm.fit(X_train, y_train,\n        eval_set=[(X_test, y_test)],\n        eval_metric='l1',\n        early_stopping_rounds=1000)","metadata":{"_uuid":"0a8a4cde01d0d1b7674d5fe2a36166a7cf685ff8","execution":{"iopub.status.busy":"2021-08-23T16:31:23.338395Z","iopub.execute_input":"2021-08-23T16:31:23.338757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = gbm.predict(X_train, num_iteration=gbm.best_iteration_)","metadata":{"_uuid":"def64a5887d5c3dd090e72a8675dcd0134983251","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Basic RMSE\nprint('The rmse of prediction is:', round(mean_squared_log_error(y_pred, y_train) ** 0.5, 5))","metadata":{"_uuid":"c0cd1cf92c94391be8718b68195d53dc7e44d6f7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results\nFirst lets create the prediction CSV required for model submission. This could be submitted via the Kaggle command line but for simplicity, was uploaded via the web UI.","metadata":{"_uuid":"3b096fa5995a76aafb4efedad6c6c9a3142ba7fc"}},{"cell_type":"code","source":"test_pred = np.expm1(gbm.predict(df_test, num_iteration=gbm.best_iteration_))","metadata":{"_uuid":"5e820a69537df7d5def7f05a1f1db0aeb98fec27","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test[\"SalePrice\"] = test_pred\ndf_test.to_csv(\"results.csv\", columns=[\"Id\", \"SalePrice\"], index=False)","metadata":{"_uuid":"64990688312e6142c1d3a585957d993e7b279197","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model developed above is a first draft to highlight the code required to implement LightGBM on a regression problem. Its current performance can be seen on the [leaderboard](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/leaderboard). As of writing this kernel the score was 0.13302, which gets to around the top 40% of the leaderboard (position 1917).\n\n\n## Conclusion\nLightGBM provides a robust implementation of gradient boosting for decision trees. The training times are comparably short and out of the box and with minimal tuning you can achieve excellent model accuracy. There is an good write up about LightGBM on [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/).\n\nTo take this model to the next level of performance, other areas to try would be:\n* Different transformations to normalise the skewed data (eg Box Cox transform)\n* Stacking or ensembling of multiple models together. Other possible models could be the [SKlearn XGBOOST](https://github.com/dmlc/xgboost) or something completely different like an ANN.\n* Hyperparameter tuning should be applied. Currently only a few parameters have been tried. The parameter space is large for LightGBM with numerous possibilities. Something like Optunity would be able to automate finding a much better set, albeit taking some time to run.\n\nWith a few tweaks and some additional time on feature enrichment, then significant advances in accuracy could be achieved.","metadata":{"_uuid":"307111af91c6a3e79ee17206cf1763c0387037c2"}}]}