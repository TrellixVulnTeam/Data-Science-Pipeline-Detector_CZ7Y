{"cells":[{"cell_type":"markdown","metadata":{},"source":"<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/3810/media/treebank.png\"/>"},{"cell_type":"markdown","metadata":{},"source":"# About Competition\nThe Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity make this task very challenging.Competition file is available [here](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)."},{"cell_type":"markdown","metadata":{},"source":"# Load Competition Dataset"},{"cell_type":"markdown","metadata":{},"source":"Competition dataset located in \"/kaggle/input\"; This path defined by Kaggle to access the competition file. We will list two files from this path as input files."},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        path=os.path.join(dirname, filename)\n        if 'train' in path:\n            __training_path=path\n        elif 'test' in path:\n            __test_path=path"},{"cell_type":"markdown","metadata":{},"source":"## Input Dataset"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"#loaded files\nprint(f'Training path:{__training_path}\\nTest path:{__test_path}')"},{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-output":"True"},"outputs":[],"source":"# Kaggle Environment Prepration\n#update kaggle env\nimport sys\n#you may update the environment that allow you to run the whole code\n!{sys.executable} -m pip install --upgrade scikit-learn==\"0.24.2\""},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"#record this information if you need to run the Kernel internally\nimport sklearn; sklearn.show_versions() "},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{},"source":"# Exploratory Data Analysis (EDA)\n## General Structure\nMercedes-Benz Greener Manufacturing includes <b>4</b> columns and <b>156060</b> rows.\nThere are <b>2</b> different data types as follows: *int64, object*."},{"cell_type":"markdown","metadata":{},"source":"# Finding Intresting Datapoints\nLet's process each field by their histogram frequency and check if there is any intresting data point.\n\nThere is 1 number of intresting value in the following column.\nThe below table shows each <b>Value</b> of each <b>Field</b>(column) with their total frequencies, <b>Lower</b> shows the lower frequency of normal distribution, <b>Upper</b> shows the upper bound frequency of normal distribution, and <b>Criteria</b> shows if the frequnecy passed <b>Upper bound</b> or <b>Lower bound</b>.\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Field</th>\n      <th>Value</th>\n      <th>Frequency</th>\n      <th>Lower</th>\n      <th>Upper</th>\n      <th>Criteria</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Sentiment</td>\n      <td>2</td>\n      <td>79582</td>\n      <td>7072.8536</td>\n      <td>79563.338</td>\n      <td>Upper</td>\n    </tr>\n  </tbody>\n</table>\n\n\nFor example, in the <b>Sentiment</b> column the value of <b>2</b> has <b>79582</b> repeatation but this number is not between Lower bound(7072.8536) and Upper bound(79563.338).\n\n\nLet     $C_0=2$   and   $Freq(C_0)=79582$     ,   $Upper(C_0)=79563.338$     ,   $Lower(C_0)=7072.8536$\n\n$Freq(C_0) > Upper(C_0)$."},{"cell_type":"markdown","metadata":{},"source":"# Input Dataset"},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"source":"def __load__data(__training_path, __test_path, concat=False):\n\t\"\"\"load data as input dataset\n\tparams: __training_path: the training path of input dataset\n\tparams: __test_path: the path of test dataset\n\tparams: if it is True, then it will concatinate the training and test dataset as output\n\treturns: generate final loaded dataset as dataset, input and test\n\t\"\"\"\n\t# LOAD DATA\n\timport pandas as pd\n\t__train_dataset = pd.read_csv(__training_path, delimiter=',' if __training_path.endswith('csv') else '\\t')\n\t__test_dataset = pd.read_csv(__test_path, delimiter=',' if __training_path.endswith('csv') else '\\t')\n\treturn __train_dataset, __test_dataset\n__train_dataset, __test_dataset = __load__data(__training_path, __test_path, concat=True)\n__train_dataset.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"source":"# STORE SUBMISSION RELEVANT COLUMNS\n__test_dataset_submission_columns = __test_dataset['PhraseId']"},{"cell_type":"markdown","metadata":{},"source":"### Discard Irrelevant Columns\nIn the given input dataset there are <b>2</b> columns that can be removed as follows:* PhraseId,SentenceId *."},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"source":"# DISCARD IRRELEVANT COLUMNS\n__train_dataset.drop(['PhraseId', 'SentenceId'], axis=1, inplace=True)\n__test_dataset.drop(['PhraseId', 'SentenceId'], axis=1, inplace=True)"},{"cell_type":"markdown","metadata":{},"source":"# Text Processing\nThe dataset has <b>1</b> text value as follows: <b>Phrase</b>.\nNow, let's covert the text as follows.\n\n- First, convert text to lowercase;\n\n- Second, strip all punctuations;\n\n- Finally, convert all numbers in text to 'num'; therefore, in the next step our model will use a single token instead of valriety of tokens of numbers."},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"source":"# PREPROCESSING-1\nimport nltk\nimport re\nimport string\n_TEXT_COLUMNS = ['Phrase']\ndef process_text(__dataset):\n    for _col in _TEXT_COLUMNS:\n        process_text = [t.lower() for t in __dataset[_col]]\n        # strip all punctuation\n        table = str.maketrans('', '', string.punctuation)\n        process_text = [t.translate(table) for t in process_text]\n        # convert all numbers in text to 'num'\n        process_text = [re.sub(r'\\d+', 'num', t) for t in process_text]\n        __dataset[_col] = process_text\n    return __dataset\n__train_dataset = process_text(__train_dataset)\n__test_dataset = process_text(__test_dataset)"},{"cell_type":"markdown","metadata":{},"source":"### Target Column\nThe target column is the value which we need to predict.\nTherefore, we need to detach the target columns in prediction.\nNote that if we don't drop this fields, it will generate a model with high accuracy on training and worst accuracy on test (because the value in test dataset is Null).\nHere is the list of *target column*: <b>Sentiment</b>"},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"source":"# DETACH TARGET\n__feature_train = __train_dataset.drop(['Sentiment'], axis=1)\n__target_train =__train_dataset['Sentiment']\n__feature_test = __test_dataset"},{"cell_type":"markdown","metadata":{},"source":"# Text Vectorizer\nIn the next step, we will transfer pre-processed text columns to a vector representation. The vector representations allows us to train a model based on numerical representations.\nWe will use TfidfVectorizer and more detail can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"source":"# PREPROCESSING-2\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport scipy.sparse as sparse\nfrom scipy.sparse import hstack, csr_matrix\n_TEXT_COLUMNS = ['Phrase']\n__temp_train_data = __feature_train[_TEXT_COLUMNS]\n__feature_train.drop(_TEXT_COLUMNS, axis=1, inplace=True)\n__feature_train_object_array = []\n__temp_test_data = __feature_test[_TEXT_COLUMNS]\n__feature_test.drop(_TEXT_COLUMNS, axis=1, inplace=True)\n__feature_test_object_array = []\nfor _col in _TEXT_COLUMNS:\n    __tfidfvectorizer = TfidfVectorizer(max_features=3000)\n    vector_train = __tfidfvectorizer.fit_transform(__temp_train_data[_col])\n    __feature_train_object_array.append(vector_train)\n    vector_test = __tfidfvectorizer.transform(__temp_test_data[_col])\n    __feature_test_object_array.append(vector_test)\n__feature_train = sparse.hstack([__feature_train] + __feature_train_object_array).tocsr()\n__feature_test = sparse.hstack([__feature_test] + __feature_test_object_array).tocsr()"},{"cell_type":"markdown","metadata":{},"source":"# Training Model and Prediction\nFirst, we will train a model based on preprocessed values of training data set.\nSecond, let's predict test values based on the trained model."},{"cell_type":"markdown","metadata":{},"source":"## Random Forest Classifier\nWe will use *RandomForestClassifier* which is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\nMore detail about *RandomForestClassifier* can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)."},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"source":"# MODEL\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n__model = RandomForestClassifier()\n__model.fit(__feature_train, __target_train)\n__y_pred = __model.predict(__feature_test)"},{"cell_type":"markdown","metadata":{},"source":"# Submission File\nWe have to maintain the target columns in \"submission.csv\" which will be submitted as our prediction results."},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"source":"# SUBMISSION\nsubmission = pd.DataFrame(columns=['PhraseId'], data=__test_dataset_submission_columns)\nsubmission['Sentiment'] = __y_pred\nsubmission.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false},"outputs":[],"source":"# save submission file\nsubmission.to_csv(\"kaggle_submission.csv\", index=False)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":4}