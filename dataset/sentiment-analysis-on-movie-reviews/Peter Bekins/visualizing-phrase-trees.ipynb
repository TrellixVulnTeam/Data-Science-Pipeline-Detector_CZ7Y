{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Visualizing Phrase Trees\n## 1. Overview\n\nThe Rotten Tomatoes movie review dataset includes a random set of individual sentences. These sentences have been scored for sentiment on a 5-part scale (0-4). Grammatical analysis has also been applied to each root sentence, with each constituent phrase at each level of the resulting phrase tree structure also scored for sentiment and added to the dataset under the root sentence. The hypothesis is that the inclusion of these constituents in the model can help improve weaknesses in the traditional \"bag-of-words\" approach related to the loss of grammatical information (e.g., scope of negation, reverse polarity). \n\nIn this notebook I will provide a method to reconstruct and visualize the phrase tree structure using the NLTK package. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 2. Data Exploration\n\nThe data has already been split into separate train and test datasets, which are loaded into pandas below.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd \n\ntrain_file = '../input/sentiment-analysis-on-movie-reviews/train.tsv.zip'\ntest_file = '../input/sentiment-analysis-on-movie-reviews/test.tsv.zip'\n\ntrain = pd.read_csv(train_file, delimiter = '\\t', compression = 'zip')\ntest = pd.read_csv(test_file, delimiter = '\\t', compression = 'zip')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I am also going to add a new column to the dataframe that counts the number of words (N) within the phrase. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk import RegexpTokenizer\ndef n_grams(phrase):\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    words = tokenizer.tokenize(phrase)\n    return len(words)\n\ntrain['N'] = train['Phrase'].apply(n_grams)\ntest['N'] = test['Phrase'].apply(n_grams)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can examine the first rows of the dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following histogram shows the distribution of phrase lengths in the dataset. The maximum phrase length in the train data is 48. The distribution is obviously weighted towards the smaller lengths due to the recursive nature of the data (each smaller unit is part of a larger unit).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['N'].hist(bins = 20)\ntrain['N'].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The maximum phrase length in the test data is 52. Note that the distribution seems to be weighted more towards short phrases than in the train set.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test['N'].hist(bins = 20)\ntest['N'].max()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Phrases are identified by both a **SentenceId** and a **PhraseId**. Since the root sentence is the first phrase in each set, we can isolate the root sentences using **group_by** on the SentenceId and then taking the first row using **first()**. The distribution of sentence lengths is closer to a normal distribution for the train and test sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sentences = train.groupby(['SentenceId']).first().reset_index()\ntrain_sentences['N'].hist(bins = 20)\ntrain_sentences.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_sentences = test.groupby(['SentenceId']).first().reset_index()\ntest_sentences['N'].hist(bins = 20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Visualizing Trees\n\nA phrase tree structure is a method of representing the hierarchical grammatical relationships between the constituents of a sentence. The **NLTK.Tree** package includes methods to both construct and visualize trees. For instance, the following code builds a tree to represent the simple sentence *Poor John ran away* with the nodes labelled with PoS tags. (Note that NLTK incudes a **draw** method, but the resulting trees cannot be viewed inline within the notebook). ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk import Tree\nsent =  \"(S (NP (A Poor ) (N John)) (VP (V ran ) (Adv away)))\"\ntree = Tree.fromstring(sent)\ntree.pretty_print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data includes a list of every constituent phrase related to a root sentence, but the hierarchical structure has not been preserved. To reconstruct the tree, I will simply index the location of each sub phrase within the root sentence and add parentheses at the beginning and end of the constituent to define the node. Since we do not have node labels, I will use the sentiment score of the phrase as the label.  ","execution_count":null},{"metadata":{},"cell_type":"raw","source":"For instance, the following rows belong to sentence 2:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train['SentenceId'] == 2)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following code builds separate lists for each constituent phrase and its sentiment and then adds parentheses and a label to the root sentence to mark the nodes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"phrases = train.loc[(train['SentenceId'] == 2)]['Phrase'].to_list()\nsentiments = train.loc[(train['SentenceId'] == 2)]['Sentiment'].to_list()\nroot = phrases[0]\nfor p, s in zip(phrases,sentiments):\n    start = root.index(p)\n    end = start + len(p) + len(str(s)) + 2\n    root = root[:start] + '(' + str(s) + ' ' + root[start:]\n    root = root[:end] + ')' + root[end:]\n    print(root)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, the tree structure can be visualized using th epretty_print method.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = Tree.fromstring(root)\ntree.pretty_print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Examining Tree Structures in the Full Datasets\n\nUnfortunately, when we try to reconstruct the tree structure for the full train set, it is obvious that the data needs a significant mount of cleaning. The following code groups the data by sentence id and applys the mthod demonstrated above to add parentheses and node labels around the constituents. If a sub phrase is not found in the root sentence, it returns the string \"error\" rather than the tree structure.  ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def phrase_tree(phrase_group):\n    phrases = phrase_group['Phrase'].to_list()\n    sentiments = phrase_group['Sentiment'].to_list()\n    root = phrases[0]\n    for p, s in zip(phrases,sentiments):\n        try:\n            start = root.index(p)\n        except:\n            root = 'error'\n        else:\n            end = start + len(p) + len(str(s)) + 2\n            root = root[:start] + '(' + str(s) + ' ' + root[start:]\n            root = root[:end] + ')' + root[end:]\n    return root\n\ntrain_trees = []\ntrain_groups = train.groupby(['SentenceId'])\nfor key, group in train_groups:\n    root = phrase_tree(group)\n    train_trees.append((key,root))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When we list the sentence ids that caused an error, we find that 71 sentences out of the 8529 total are missing the root sentence (and possibly other subphrases), which is almost 1%. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"errors = 0\nfor tree in train_trees:\n    if tree[1] == 'error':\n        errors += 1\nprint(errors)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The following example shows one of the sentences that threw an error. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[(train['SentenceId'] == 8382)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Conclusions\n\nBoth the train and test datasets have already been parsed, so it is not necessary for you to implement a grammatical analysis. Nevertheless, understanding the tree structure has a few implications for the sentiment analysis problem. First, if you are going to seperate a validation set from the train data, you should keep subphrases together with the root sentence or you will introduce a leakage problem. Second, be aware that the automated parsing seems to have left the data a bit messy, so be sure to clean it up. ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}