{"cells":[{"metadata":{},"cell_type":"markdown","source":"This notebook aims to practice a typical deep learning approach to sentiment analysis. EDA is omitted and only the modelling part is presented.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom bs4 import BeautifulSoup #to extract words from HTML documents\n\nimport string\nfrom keras.utils import to_categorical\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Dense,Dropout,Embedding,LSTM,GlobalMaxPooling1D\nfrom keras.callbacks import EarlyStopping\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\n\n#set random seed for the session and also for tensorflow that runs in background for keras\ntf.random.set_seed(514)\n\n# load data\ntrain = pd.read_csv(\"/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip\", sep=\"\\t\")\ntest = pd.read_csv(\"/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip\", sep=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Have a first look into both the training and testing data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# train data\nprint(f\"The shape of training data is {train.shape}.\")\nprint(train.head())\n# test data\nprint(f\"The shape of testing data is {test.shape}.\")\nprint(test.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First prepare lists of words that will be to our model. Here we will extract lemmatized words in lower case without punctuation.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_seq = train['Phrase'].apply(str.lower).apply(lemmatizer.lemmatize) \\\n    .apply(lambda s: s.translate(str.maketrans('', '', string.punctuation))) \\\n    .apply(str.split).tolist()\ntest_seq = test['Phrase'].apply(str.lower).apply(lemmatizer.lemmatize) \\\n    .apply(lambda s: s.translate(str.maketrans('', '', string.punctuation))) \\\n    .apply(str.split).tolist()\n\nprint(f\"Length of sequences - training data: {len(train_seq)}, testing data: {len(test_seq)}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply one-hot encoding to target, and perform train/test split with proportion = 20%.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_target = to_categorical(train.Sentiment.values)\nnum_classes = y_target.shape[1]\n\nX_train,X_val,y_train,y_val = train_test_split(train_seq, y_target, test_size=0.2, stratify=y_target)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prepare a set containing words in the training data. If the set is considerably small (e.g. < 20000) we will use all the words to create a Tokenizer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# record the maximum word length of sequences in the training data\nunique_words = set()\nlen_max = 0\nfor sent in X_train:    \n    unique_words.update(sent)\n    if(len(sent) > len_max):\n        len_max = len(sent)\n\nprint(f\"Number of unique words in training set = {len(unique_words)}.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As discussed, we will use all the words in the training data to set up a word tokenizer by Keras. Fit the sequences to such tokenizer. Finally pad (or occasionally truncate since the maximum length here is obtained from the training data only) the sequences. We use pre-padding here since the typical LSTM unit (which is not bi-directional) will be used.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create tokenizer\ntokenizer = Tokenizer(num_words=len(unique_words))\ntokenizer.fit_on_texts(X_train)\n\n# transform the word sequences to numerical vectors\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(test_seq)\n\n# padding\nX_train = pad_sequences(X_train, maxlen=len_max, padding=\"pre\", truncating=\"pre\")\nX_val = pad_sequences(X_val, maxlen=len_max, padding=\"pre\", truncating=\"pre\")\nX_test = pad_sequences(X_test, maxlen=len_max, padding=\"pre\", truncating=\"pre\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally we build our network for sentiment analysis. We will add 2 layers of LSTM which are able to capture long-term dependencies. The maximum value of the return sequences is captured using a GlobalMaxPooling later. We pass the output to 2 layers of Dense and finally gives the probabilities of all the target classes.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"D = 50 #embedding dimensionality\ndropout_rate = 0.5\n\n# Define early stopping that will be used as callback\nearly_stopping = EarlyStopping(min_delta=0.01, mode='max', monitor='val_accuracy', patience=3)\ncallback = [early_stopping]\n\n# add layers to model\nmodel = Sequential()\nmodel.add(Embedding(len(tokenizer.word_index), D, input_length=len_max))\nmodel.add(LSTM(128, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\nmodel.add(LSTM(64, dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(dropout_rate))\nmodel.add(Dense(num_classes,activation='softmax'))\n\n# use a relatively low learning rate\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n\n# let the model fit with the data until early stopping criteria is met\nr = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=256, callbacks=callback)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create visualization of losses against number of epoches. Finally perform prediction on testing data and prepare the corresponding submission file.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"counts = range(1, len(r.history['loss'])+1)\nplt.plot(counts, r.history['loss'], 'r-')\nplt.plot(counts, r.history['val_loss'], 'b-')\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()\n\nsub_file = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv',sep=',')\nsub_file.Sentiment = model.predict_classes(X_test)\nsub_file.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}