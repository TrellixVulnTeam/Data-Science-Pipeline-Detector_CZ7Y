{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# General Stuff:\nimport numpy as np\nimport pandas as pd\nimport os\nimport sys\nfrom datetime import datetime\nimport gc\nfrom collections import defaultdict, Counter\n\n\n# Transformers:\nif 'transformers' not in sys.modules:\n  !pip install transformers\nimport transformers\nfrom transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\nfrom transformers import BertForSequenceClassification, BertTokenizer, BertConfig, BertModel\nfrom transformers import DistilBertTokenizer, DistilBertModel\n\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\nfrom transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig, DistilBertModel\n\n# Dimensionality Reduction:\nfrom sklearn.random_projection import johnson_lindenstrauss_min_dim\nfrom sklearn import random_projection\n\n# Neural Networks Stuff\nimport torch\nfrom torch import nn, optim\nfrom torch.utils import data\n\n# Statistics Stuff\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# Visualization Stuff\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_colwidth', 240)\n    \nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice\nif torch.cuda.is_available():\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Faster Realtime Sentiment Analysis with Random Projections"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"toc\"></a>\n## Table of Content\n1. [Introduction](#introduction)\n1. [Data Preparation](#preparation)\n1. [Embeddings](#embeddings)\n1. [Classification](#classification)\n1. [Affective Space](#affective)\n1. [Dimensionality Reduction](#reduction)\n1. [Conclusions](#conclusions)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"introduction\"></a>\n\n## Introduction\n\nIn this notebook I will try to improve the time and size complexity of sentiment analysis, in order to make it more suitable for online and real-time usage.\n\nMy approach is to reduce the dimensions of the word embedding using random progections, and choose such reduction that will maintain the \"affective distances\".\n\nBy \"affective distances\" I mean my heuristic for concluding if some reduced embedding maintain the following property: following the Plutchick emotion circumplex, the distances between all the words in the circumplex should maintain relatively the same.\n\nThe notebook is avaible on [Kaggle](https://www.kaggle.com/odedgolden/sentiment-analysis-with-dimensionality-reduction/)\n\n[Table of content](#toc)\n\n"},{"metadata":{},"cell_type":"markdown","source":"<table><td><img src='http://www.feelguide.com/wp-content/uploads/2011/06/Plutchik.jpg', width=\"600\"></td></table>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"preparation\"></a>\n## Data Preparation\n\n[Table of content](#toc)\n\n\n> “Creative minds are uneven, and the best of fabrics have their dull spots.” - H.P. Lovecraft"},{"metadata":{"trusted":true},"cell_type":"code","source":"root = \"/kaggle/input/sentiment-analysis-on-movie-reviews/\"\ntrain = pd.read_csv(root + 'train.tsv.zip', sep=\"\\t\")\ntest = pd.read_csv(root + 'test.tsv.zip', sep=\"\\t\")\nprint(train.shape,test.shape)\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Initiate BERT Model\n\nThe BERT model was proposed in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It’s a bidirectional transformer pre-trained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n\nIt will be very helpfull to use the pretrained BERT model, in order to get the tokenizer and the word embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"PRE_TRAINED_MODEL_NAME = 'distilbert-base-cased'\ntokenizer = DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\nbert_model = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n\nbert_embeddings = bert_model.get_input_embeddings()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_sentence = \"Creative minds are uneven, and the best of fabrics have their dull spots.\"\ntokens = tokenizer.tokenize(sample_sentence)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\nencoded = tokenizer.encode(sample_sentence)\nprint(tokens)\nprint(token_ids)\nprint(encoded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"embeddings\"></a>\n\n## Embeddings\n\n[Table of content](#toc)\n\n\n> \"Words have no power to impress the mind without the exquisite horror of their reality.\" - Edgar Allan Poe.\n\nWe will use the BERT pre-trained embeddings which map each word id to its (1,768) vector."},{"metadata":{"trusted":true},"cell_type":"code","source":"def word_to_index(word):\n    token_id = tokenizer.convert_tokens_to_ids(word)\n    return token_id\n\nprint(word_to_index('Like'))\n\ndef index_to_vec(word_id):\n    embeded_token = bert_embeddings(torch.Tensor([word_id]).to(torch.long))\n    return embeded_token.detach().numpy()\nvec = index_to_vec(word_to_index('Like'))\nprint(vec.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"classification\"></a>\n\n## Classification\n\nI will test three classification models.\n\n1. BERT assisted model.\n2. Simple linear model using only BERT embeddings.\n3. Simple linear model using BERT embeddings and dimensionality redection.\n\n[Table of content](#toc)\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Torch Classes"},{"metadata":{},"cell_type":"markdown","source":"First let's define the Datasets that will hold the train samples and test samples.\n\nThey will take the raw reviews, tokenize them and return the encoded vector. (not yet the embedding vectors)"},{"metadata":{"trusted":true},"cell_type":"code","source":"class TrainDataSet(data.Dataset):\n    def __init__(self, reviews, labels, tokenizer, max_len):\n        self.reviews = reviews\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.reviews)\n    \n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        \n        encoding  = self.tokenizer.encode_plus(\n            review,\n            max_length = self.max_len,\n            truncation=True,\n            add_special_tokens=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        \n        return {\n            'review_text': review,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'targets': torch.tensor(self.labels[item], dtype=torch.long)\n        }","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Note that for the test dataset we would like to have no labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"class TestDataSet(data.Dataset):\n    def __init__(self, ids, reviews, tokenizer, max_len):\n        self.ids = ids\n        self.reviews = reviews\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.reviews)\n    \n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        reviewexcerpt_id = str(self.ids[item])\n        \n        encoding  = self.tokenizer.encode_plus(\n            review,\n            max_length = self.max_len,\n            truncation=True,\n            add_special_tokens=True,\n            padding='max_length',\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        \n        return {\n            'review_id': review_id,\n            'review_text': review,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_train_data_loader(df, tokenizer, max_len, batch_size):\n    reviews = df['Phrase'].to_numpy(),\n    print(f'Reviews size: {len(reviews)}')\n    labels = df['Sentiment'].to_numpy(),\n    dataset = TrainDataSet(reviews=reviews[0], labels=labels[0], tokenizer=tokenizer, max_len=max_len)\n    print(f'Dataset size: {len(dataset)}')\n    return data.DataLoader(dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n\ndef create_test_data_loader(df, tokenizer, max_len, batch_size):\n    reviews = df['Phrase'].to_numpy(),\n    ids = df['id'].to_numpy(),\n    print(f'Reviews size: {len(reviews)}')\n    dataset = TestDataSet(ids= ids[0], reviews=reviews[0], tokenizer=tokenizer, max_len=max_len)\n    print(f'Dataset size: {len(dataset)}')\n    return data.DataLoader(dataset, batch_size=batch_size, num_workers=4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's split the train DataFrame into train set and validation set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, val_set = train_test_split(train, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 4\nMAX_LEN = 160\n\ntrain_data_loader = create_train_data_loader(train_set, tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE)\nval_data_loader = create_train_data_loader(val_set, tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = next(iter(train_data_loader))\nprint(sample['input_ids'].shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models:\n\n<a id=\"models\"></a>\n1. [BertReviewClassifier](#BertReviewClassifier)\n1. [LinearReviewClassifier](#LinearReviewClassifier)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"BertReviewClassifier\"></a>\n#### Bert Review Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertReviewClassifier(nn.Module):\n    def __init__(self):\n        super(BertReviewClassifier, self).__init__()\n        self.num_labels = 5\n\n        self.softmax = nn.Softmax(dim=1)\n        self.bert = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n#         print(self.bert.config)\n        self.classifier = nn.Linear(self.bert.config.dim, 5)\n        self.dropout = nn.Dropout(0.3)\n\n        nn.init.xavier_normal_(self.classifier.weight)\n\n    def forward(self, input_ids, attention_mask):\n        bert_output = self.bert(input_ids=input_ids,\n                                            attention_mask=attention_mask)\n        hidden_state = bert_output[0]\n        # print(f'hidden_state shape: {hidden_state.shape}')                \n        # print(f'hidden_state shape[2]: {hidden_state.shape[2]}')                \n        pooled_output = hidden_state[:, 0, :]                   \n        # pooled_output = self.pre_classifier(pooled_output)   \n        # pooled_output = nn.ReLU()(pooled_output)             \n        pooled_output = self.dropout(pooled_output)        \n        logits = self.classifier(pooled_output)\n        # logits = self.softmax(logits)\n        return logits","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"LinearReviewClassifier\"></a>\n#### Linear Review Classifier\n\nThe linear model will get an optional parameter - the reduction matrix, that will reduce the embedding vectors from their origin size (768 in our case) to a smaller size (32 in our case)."},{"metadata":{"trusted":true},"cell_type":"code","source":"class LinearReviewClassifier(nn.Module):\n    def __init__(self, reduction_transformer=None):\n        super(LinearReviewClassifier, self).__init__()\n        self.num_labels = 5\n\n        self.softmax = nn.Softmax(dim=1)\n        self.bert_embeddings = DistilBertModel.from_pretrained(PRE_TRAINED_MODEL_NAME).get_input_embeddings()\n        self.classifier = nn.Linear(768, 5) if reduction_transformer is None else nn.Linear(reduction_transformer.n_components, 5)\n        self.dropout = nn.Dropout(0.3)\n        self.reduction_transformer = reduction_transformer\n        self.reduction_matrix = torch.from_numpy(np.transpose(reduction_transformer.components_)).float().to(device) if reduction_transformer else None\n        \n        \n        nn.init.xavier_normal_(self.classifier.weight)\n\n    def forward(self, input_ids, attention_mask):\n        bert_embeddings_output = self.bert_embeddings(input_ids)\n#         print(f'bert_embeddings_output shape: {bert_embeddings_output.shape}')                \n        if self.reduction_transformer:\n#             print(f'bert_embeddings_output shape: {bert_embeddings_output.shape}')                \n#             print(f'self.reduction_matrix shape: {self.reduction_matrix.shape}')\n#             print(f'self.reduction_matrix type: {self.reduction_matrix.dtype}')\n#             print(f'self.bert_embeddings_output type: {bert_embeddings_output.dtype}')\n            bert_embeddings_output = torch.matmul(bert_embeddings_output, self.reduction_matrix)\n            \n        hidden_state = bert_embeddings_output[:, 0, :]\n#         print(f'hidden_state shape: {hidden_state.shape}')                \n        pooled_output = self.dropout(hidden_state)\n        output = self.classifier(pooled_output)\n        return output","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sanity test:"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = torch.LongTensor(1).random_(0, 10)\na = a.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LinearReviewClassifier().to(device)\ninput_ids = sample['input_ids'].to(device)\nattention_mask = sample['attention_mask'].to(device)\n\nprint(input_ids.shape)\nprint(attention_mask.shape)\nprob, pred = torch.max(model(input_ids=input_ids, attention_mask=attention_mask),dim=1)\nprint(prob)\nprint(pred)\nprint(sample['targets'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_epoch(model, data_loader, loss_fn, optimizer, scheduler, n_examples):\n    model = model.train()\n    \n    losses = []\n    correct_predictions = 0\n    \n    for d in data_loader:\n        input_ids = d['input_ids'].to(device)\n        attention_mask = d['attention_mask'].to(device)\n        targets = d['targets'].to(device)\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        # print(targets.shape)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        \n        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        del outputs\n        gc.collect()\n    return correct_predictions.double() / n_examples, np.mean(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_model(model, data_loader, loss_fn, n_examples):\n    model = model.eval()\n    losses = []\n    \n    correct_predictions = 0\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n            targets = d['targets'].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n    return correct_predictions.double() / n_examples, np.mean(losses)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_sentiment(model, data_loader, submission_df):\n    model = model.eval()\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].to(device)\n            attention_mask = d['attention_mask'].to(device)\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            print(outputs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we will train the linear model without the reduction:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(model, epoches, output_file):\n    optimizer = transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n    total_steps = len(train_data_loader) * epoches\n\n    scheduler = transformers.get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=total_steps\n    )\n\n    loss_fn = nn.CrossEntropyLoss().to(device)\n    model = model.to(device)\n\n    history = defaultdict(list)\n    best_accuracy = 0\n    start_time = datetime.now()\n\n    for epoch in range(epoches):\n        print(f'Epoch {epoch + 1}/{epoches} started at {datetime.now()}')\n        print('-'*10)\n\n        train_acc, train_loss = train_epoch(\n            model,\n            train_data_loader,\n            loss_fn,\n            optimizer,\n            scheduler,\n            len(train_set)   \n        )\n        print(f'Train loss: {train_loss}, accuracy: {train_acc}')\n\n        val_acc, val_loss = eval_model(\n            model,\n            val_data_loader,\n            loss_fn,\n            len(val_set)   \n        )\n        print(f'Validation loss: {val_loss}, accuracy: {val_acc}')\n        history['train_acc'].append(train_loss)\n        history['val_acc'].append(val_acc.cpu().detach().numpy())\n    \n    running_time = datetime.now() - start_time\n    torch.save(model.state_dict(), output_file)\n    return history, best_accuracy, running_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = '/kaggle/working/linear_model.pt'\nEPOCHES = 5\nhistory, best_accuracy, running_time = run_training(model, EPOCHES, filename)\nprint(f'Best accuracy: {best_accuracy}, took: {running_time}')\nprint(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# a = np.array(history['val_acc']).reshape((len(history['val_acc']),1))\n# print(a.shape)\n# b = np.array(history['train_acc']).reshape((len(history['train_acc']),1))\n# print(b.shape)\n# c = np.array([i in range(EPOCHES)]).reshape(EPOCHES,1))\n# print(c.shape)\n\n# data = pd.DataFrame(np.concatenate((a,b, c),axis=1), columns = [\"Validation Loss\",\"Train Loss\",\"Epochs\"])\n# # data.head()\n# ax = sns.lineplot(x=\"Epochs\", y=\"Loss\", data=data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\n\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"affective\"></a>\n## Affective Space\n1. [Table of content](#toc)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plutchick_words = [[\"ecstasy\", \"joy\", \"serenity\"],[\"admiration\", \"trust\", \"acceptance\"],\n                   [\"terror\", \"fear\", \"apprehension\"],[\"amazement\", \"surprise\", \"distraction\"],\n                   [\"grief\", \"sadness\", \"pensiveness\"],[\"loathing\", \"disgust\", \"boredom\"],\n                   [\"rage\", \"anger\", \"annoyance\"], [\"vigilance\", \"anticipation\", \"interest\"]]\n\ncircumplex_emotions = [\"ecstasy\", \"joy\", \"serenity\",\"admiration\", \"trust\", \"acceptance\",\n                   \"terror\", \"fear\", \"apprehension\",\"amazement\", \"surprise\", \"distraction\",\n                   \"grief\", \"sadness\", \"pensiveness\",\"loathing\", \"disgust\", \"boredom\",\n                   \"rage\", \"anger\", \"annoyance\", \"vigilance\", \"anticipation\", \"interest\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_distances_df(circumplex_emotions, index_to_vec):\n    l_emotions = len(circumplex_emotions)\n    distance_matrix = np.zeros((l_emotions,l_emotions))\n    for i in range(l_emotions):\n        for j in range(l_emotions):\n            word1 = index_to_vec(word_to_index((circumplex_emotions[i])))\n            word2 = index_to_vec(word_to_index((circumplex_emotions[j])))\n            distance_matrix[i][j] = np.linalg.norm(word1 - word2)\n\n    emotions_df = pd.DataFrame(distance_matrix, columns=circumplex_emotions)\n    emotions_df.index = circumplex_emotions\n    return emotions_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_df = get_distances_df(circumplex_emotions, index_to_vec)\n\n\nfig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(ax=ax, data=baseline_df, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"reduction\"></a>\n\n## Dimensionality Reduction\n\n[Table of content](#toc)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"emotions_vectors = np.array([index_to_vec(word_to_index((word))) for word in circumplex_emotions]).squeeze()\nemotions_vectors.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer = random_projection.GaussianRandomProjection(n_components=32)\ntransformer.fit(emotions_vectors)\nemotions_vectors_reduced = transformer.transform(emotions_vectors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transformer.components_.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def index_to_vec_transform(word_id):\n    full_vec = index_to_vec(word_id)\n#     print(full_vec.shape)\n    return transformer.transform(full_vec)\nindex_to_vec_transform(word_to_index(\"Like\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduced_df = get_distances_df(circumplex_emotions, index_to_vec_transform)\n\n\nfig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(ax=ax, data=reduced_df, annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that we have a fitted transformer, we can train a new linear model, this time with reduction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# del model\n# del bert_model\n# del bert_embeddings\n\ngc.collect()\n\nmodel = LinearReviewClassifier(reduction_transformer=transformer).to(device)\nfilename = '/kaggle/working/linear_reduced_model.pt'\nEPOCHES = 5\n\nhistory, best_accuracy, running_time = run_training(model, EPOCHES, filename)\nprint(f'Best accuracy: {best_accuracy}, took: {running_time}')\nprint(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\n\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"conclusions\"></a>\n\n## Conclusions\n\n[Table of content](#toc)"},{"metadata":{},"cell_type":"markdown","source":"Unfortunately, I didn't succeed in showing any improvement in accuracy or speed with this fairly simple setup.\nHowever, here are some thoughts and ideas for possible improvements:\n1. While implementing, one should be very careful when trying to write custom layers, and make sure to utilize the deep learning library (torch in this case) and do not use numpy functions for training.\n2. A major disadvantage of my method is the lack of fine tuning of the reduction matrix while training the complete network. This may be addressed by allowing some extent of training for the matrix as well.\n3. While I didn't really optimized the reduction matrix for the different emotions, but merely optimized for the actual words (some of them even missing from the corpus), one may want to use more elaborated method to optimized for the conceptual emotions. For example, one may use entire paragraphs that were validated in psychological experiments, and prooved to portray the intended emotions.\n4. One should certainly try to use a larger, perhaps more complex network, train it longer, and on more diverse and rich data.\n5. While this implementation is very naive and simple, the same idea can be used by a pure deep artificial neural network, where instead of fitting a reduction matrix and use it as is, one may add an affective target function that will represent the affective distances between the emotions.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}