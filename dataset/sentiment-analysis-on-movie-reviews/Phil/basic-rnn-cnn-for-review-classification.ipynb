{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\nCreated on 2020/6/12 15:25\n@author: phil\n\"\"\"\n\nimport pandas as pd\nimport os\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\n\n\ndef prepare_data(dataset_path, sent_col_name, label_col_name):\n    \"\"\"\n        读出tsv中的句子和标签\n    \"\"\"\n    file_path = os.path.join(dataset_path, \"train.tsv.zip\")\n    data = pd.read_csv(file_path, sep=\"\\t\")\n    X = data[sent_col_name].values\n    y = data[label_col_name].values\n    return X, y\n\n\nclass Language:\n    \"\"\" 根据句子列表建立词典并将单词列表转换为数值型表示 \"\"\"\n    def __init__(self):\n        self.word2id = {}\n        self.id2word = {}\n\n    def fit(self, sent_list):\n        vocab = set()\n        for sent in sent_list:\n            vocab.update(sent.split(\" \"))\n        word_list = [\"<pad>\", \"<unk>\"] + list(vocab)\n        self.word2id = {word: i for i, word in enumerate(word_list)}\n        self.id2word = {i: word for i, word in enumerate(word_list)}\n\n    def transform(self, sent_list, reverse=False):\n        sent_list_id = []\n        word_mapper = self.word2id if not reverse else self.id2word\n        unk = self.word2id[\"<unk>\"] if not reverse else None\n        for sent in sent_list:\n            sent_id = list(map(lambda x: word_mapper.get(x, unk), sent.split(\" \") if not reverse else sent))\n            sent_list_id.append(sent_id)\n        return sent_list_id\n\n\nclass ClsDataset(Dataset):\n    \"\"\" 文本分类数据集 \"\"\"\n    def __init__(self, sents, labels):\n        self.sents = sents\n        self.labels = labels\n\n    def __getitem__(self, item):\n        return self.sents[item], self.labels[item]\n\n    def __len__(self):\n        return len(self.sents)\n\n\ndef collate_fn(batch_data):\n    \"\"\" 自定义一个batch里面的数据的组织方式 \"\"\"\n    batch_data.sort(key=lambda data_pair: len(data_pair[0]), reverse=True)\n\n    sents, labels = zip(*batch_data)\n    sents_len = [len(sent) for sent in sents]\n    sents = [torch.LongTensor(sent) for sent in sents]\n    padded_sents = pad_sequence(sents, batch_first=True, padding_value=0)\n\n    return torch.LongTensor(padded_sents), torch.LongTensor(labels),  torch.FloatTensor(sents_len)\n\n\ndef get_wordvec(word2id, vec_file_path, vec_dim=50):\n    \"\"\" 读出txt文件的预训练词向量 \"\"\"\n    print(\"开始加载词向量\")\n    word_vectors = torch.nn.init.xavier_uniform_(torch.empty(len(word2id), vec_dim))\n    word_vectors[0, :] = 0  # <pad>\n    found = 0\n    with open(vec_file_path, \"r\", encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        for line in lines:\n            splited = line.split(\" \")\n            if splited[0] in word2id:\n                found += 1\n                word_vectors[word2id[splited[0]]] = torch.tensor(list(map(lambda x: float(x), splited[1:])))\n            if found == len(word2id) - 1:  # 允许<unk>找不到\n                break\n    print(\"总共 %d个词，其中%d个找到了对应的词向量\" % (len(word2id), found))\n    return word_vectors.float()\n\n\ndef make_dataloader(dataset_path=\"/kaggle/input/sentiment-analysis-on-movie-reviews\", sent_col_name=\"Phrase\", label_col_name=\"Sentiment\", batch_size=32, vec_file_path=\"/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\", debug=False):\n    # X, y = prepare_datapairs(dataset_path=\"../dataset/imdb\", sent_col_name=\"review\", label_col_name=\"sentiment\")\n    X, y = prepare_data(dataset_path=dataset_path, sent_col_name=sent_col_name, label_col_name=label_col_name)\n\n    if debug:\n        X, y = X[:100], y[:100]\n\n    X_language = Language()\n    X_language.fit(X)\n    X = X_language.transform(X)\n\n    word_vectors = get_wordvec(X_language.word2id, vec_file_path=vec_file_path, vec_dim=50)\n    # 总共 18229个词，其中12769个找到了对应的词向量 word_vectors = get_wordvec(X_language.word2id,\n    # vec_file_path=r\"F:\\NLP-pretrained-model\\glove.twitter.27B\\glove.twitter.27B.50d.txt\", vec_dim=50)\n\n    # 测试\n    # print(X[:2])\n    # X_id = X_language.transform(X[:2])\n    # print(X_language.transform(X_id, reverse=True))\n\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    cls_train_dataset, cls_val_dataset = ClsDataset(X_train, y_train), ClsDataset(X_val, y_val)\n    cls_train_dataloader = DataLoader(cls_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n    cls_val_dataloader = DataLoader(cls_val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n\n    return cls_train_dataloader, cls_val_dataloader, word_vectors, X_language\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\nCreated on 2020/5/15 22:23\n@author: phil\n\"\"\"\nimport torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\n\nclass TextRNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_of_class, weights=None, rnn_type=\"RNN\", device=\"cpu\"):\n        super(TextRNN, self).__init__()\n\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_of_class = num_of_class\n        self.embedding_dim = embedding_dim\n        self.rnn_type = rnn_type\n\n        if weights is not None:\n            self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, _weight=weights).to(device)\n        else:\n            self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim).to(device)\n\n        if rnn_type == \"RNN\":\n            self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True).to(device)\n            self.hidden2label = nn.Linear(hidden_size, num_of_class).to(device)\n        elif rnn_type == \"LSTM\":\n            self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True, bidirectional=True).to(device)\n            self.hidden2label = nn.Linear(hidden_size*2, num_of_class).to(device)\n\n    def forward(self, input_sents):\n        # input_sents (batch_size, seq_len)\n        batch_size, seq_len = input_sents.shape\n        # (batch_size, seq_len, embedding_dim)\n        embed_out = self.embed(input_sents)\n\n        if self.rnn_type == \"RNN\":\n            h0 = torch.randn(1, batch_size, self.hidden_size).to(device)\n            _, hn = self.rnn(embed_out, h0)\n        elif self.rnn_type == \"LSTM\":\n            h0, c0 = torch.randn(2, batch_size, self.hidden_size).to(device), torch.randn(2, batch_size, self.hidden_size).to(device)\n            output, (hn, _) = self.lstm(embed_out, (h0, c0))\n            hn = hn.reshape(batch_size, -1)\n\n        logits = self.hidden2label(hn).squeeze(0)\n\n        return logits\n\n\nclass TextCNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, num_of_class, embedding_vectors=None, kernel_num=100, kerner_size=[3, 4, 5], dropout=0.5, device=\"cpu\"):\n        super(TextCNN, self).__init__()\n        if embedding_vectors is None:\n            self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim).to(device)\n        else:\n            self.embed = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, _weight=embedding_vectors).to(device)\n        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_num, (K, embedding_dim)).to(device) for K in kerner_size])\n        self.dropout = nn.Dropout(dropout).to(device)\n        self.feature2label = nn.Linear(3*kernel_num, num_of_class).to(device)\n\n    def forward(self, x):\n        # x shape (batch_size, seq_len)\n        embed_out = self.embed(x).unsqueeze(1)\n        conv_out = [F.relu(conv(embed_out)).squeeze(3) for conv in self.convs]\n\n        pool_out = [F.max_pool1d(block, block.size(2)).squeeze(2) for block in conv_out]\n\n        pool_out = torch.cat(pool_out, 1)\n\n        logits = self.feature2label(pool_out)\n\n        return logits","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\"\"\"\nCreated on 2020/4/30 8:33\n@author: phil\n\"\"\"\nfrom torch import optim\nimport torch\n#from models import TextRNN, TextCNN\n# from dataloader_bytorchtext import dataset2dataloader\n#from dataloader_byhand import make_dataloader\nimport torch.nn.functional as F\nimport numpy as np\n\nif __name__ == \"__main__\":\n    model_names = [\"RNN\", \"LSTM\", \"CNN\"]\n    train_iter, val_iter, word_vectors, X_lang = make_dataloader(batch_size=256)\n    # train_iter, val_iter, word_vectors = dataset2dataloader(batch_size=128)\n\n    learning_rate = 0.001\n    epoch_num = 50\n    num_of_class = 5\n    MAX_LENGTH = 40\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n    \n    for model_name in model_names:\n        if model_name == \"RNN\":\n            model = TextRNN(vocab_size=len(word_vectors), embedding_dim=50, hidden_size=128, num_of_class=num_of_class, weights=word_vectors, device=device)\n        elif model_name == \"CNN\":\n            model = TextCNN(vocab_size=len(word_vectors), embedding_dim=50, num_of_class=num_of_class, embedding_vectors=word_vectors, device=device)\n        elif model_name == \"LSTM\":\n            model = TextRNN(vocab_size=len(word_vectors), embedding_dim=50, hidden_size=128, num_of_class=num_of_class, weights=word_vectors, rnn_type=\"LSTM\", device=device)\n        model.to(device)\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n        loss_fun = torch.nn.CrossEntropyLoss()\n\n        for epoch in range(epoch_num):\n            model.train()\n            for i, batch in enumerate(train_iter):\n                # torchtext\n                # x, y = batch.review.t()[:, :MAX_LENGTH], batch.sentiment\n                x, y, lens = batch\n                x = x.to(device)\n                y = y.to(device)\n                logits = model(x)\n                optimizer.zero_grad()\n                loss = loss_fun(logits, y)\n                loss.backward()\n                optimizer.step()\n            \n            model.eval()\n            train_accs = []\n            for i, batch in enumerate(train_iter):\n                # x, y = batch.review.t()[:, :MAX_LENGTH], batch.sentiment\n                x, y, lens = batch\n                x = x.to(device)\n                y = y.to(device)\n                logits = model(x)\n                _, y_pre = torch.max(logits.cpu(), -1)\n                acc = torch.mean((torch.tensor(y_pre.cpu() == y.cpu(), dtype=torch.float)))\n                train_accs.append(acc.cpu())\n            train_acc = np.array(train_accs).mean()\n\n            val_accs = []\n            for i, batch in enumerate(val_iter):\n                # x, y = batch.review.t()[:, :MAX_LENGTH], batch.sentiment\n                x, y, lens = batch\n                x = x.to(device)\n                y = y.to(device)\n                logits = model(x)\n                _, y_pre = torch.max(logits, -1)\n                acc = torch.mean((torch.tensor(y_pre.cpu() == y.cpu(), dtype=torch.float)))\n                val_accs.append(acc.cpu())\n            val_acc = np.array(val_accs).mean()\n            print(\"epoch %d train acc:%.2f, val acc:%.2f\" % (epoch, train_acc, val_acc))\n\"\"\"\nRNN \nepoch 49 train acc:0.51, val acc:0.50\n    \nLSTM\nepoch 49 train acc:0.52, val acc:0.51\n\nCNN\nepoch 8 train acc:0.83, val acc:0.67\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}