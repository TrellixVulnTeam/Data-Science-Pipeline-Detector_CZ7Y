{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-19T03:16:19.180268Z","iopub.execute_input":"2021-07-19T03:16:19.180706Z","iopub.status.idle":"2021-07-19T03:16:19.198226Z","shell.execute_reply.started":"2021-07-19T03:16:19.180617Z","shell.execute_reply":"2021-07-19T03:16:19.197249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nkeras = tf.keras\n\ntf.__version__","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:19.199745Z","iopub.execute_input":"2021-07-19T03:16:19.200076Z","iopub.status.idle":"2021-07-19T03:16:23.592939Z","shell.execute_reply.started":"2021-07-19T03:16:19.200042Z","shell.execute_reply":"2021-07-19T03:16:23.592148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/train.tsv.zip', sep = '\\t')\nprint(train_data.Phrase[0])\nprint(train_data.Phrase[1])\nprint(train_data.Phrase[2])\ntrain_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:23.594792Z","iopub.execute_input":"2021-07-19T03:16:23.59512Z","iopub.status.idle":"2021-07-19T03:16:23.822262Z","shell.execute_reply.started":"2021-07-19T03:16:23.595072Z","shell.execute_reply":"2021-07-19T03:16:23.821291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.Phrase[63])\ntrain_data[62:64+5]","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:23.824141Z","iopub.execute_input":"2021-07-19T03:16:23.824502Z","iopub.status.idle":"2021-07-19T03:16:23.835739Z","shell.execute_reply.started":"2021-07-19T03:16:23.824464Z","shell.execute_reply":"2021-07-19T03:16:23.834818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/test.tsv.zip', sep = '\\t')\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:23.8371Z","iopub.execute_input":"2021-07-19T03:16:23.837465Z","iopub.status.idle":"2021-07-19T03:16:23.93656Z","shell.execute_reply.started":"2021-07-19T03:16:23.837429Z","shell.execute_reply":"2021-07-19T03:16:23.935566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission_data = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv')\nsample_submission_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:23.938009Z","iopub.execute_input":"2021-07-19T03:16:23.938361Z","iopub.status.idle":"2021-07-19T03:16:23.979963Z","shell.execute_reply.started":"2021-07-19T03:16:23.938324Z","shell.execute_reply":"2021-07-19T03:16:23.97898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_size = int(len(train_data) * 0.8)\n\ntotal_phrases = train_data.Phrase.to_numpy()\ntrain_phrases = total_phrases[:split_size]\nvalid_phrases = total_phrases[split_size:]\n\ntotal_sentiments = train_data.Sentiment.to_numpy()\ntrain_sentiments = total_sentiments[:split_size]\nvalid_sentiments = total_sentiments[split_size:]","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:23.981302Z","iopub.execute_input":"2021-07-19T03:16:23.981676Z","iopub.status.idle":"2021-07-19T03:16:23.987944Z","shell.execute_reply.started":"2021-07-19T03:16:23.981638Z","shell.execute_reply":"2021-07-19T03:16:23.986812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((train_phrases, train_sentiments))\nvalid_ds = tf.data.Dataset.from_tensor_slices((valid_phrases, valid_sentiments))\ntotal_ds = tf.data.Dataset.from_tensor_slices((total_phrases, total_sentiments))\n\nfor phrase, sentiment in train_ds.take(4):\n    print(phrase, sentiment)\nprint()    \nfor phrase, sentiment in valid_ds.take(1):\n    print(phrase, sentiment)    ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:23.991261Z","iopub.execute_input":"2021-07-19T03:16:23.991618Z","iopub.status.idle":"2021-07-19T03:16:25.632443Z","shell.execute_reply.started":"2021-07-19T03:16:23.991532Z","shell.execute_reply":"2021-07-19T03:16:25.630976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_set = set()\nlines = []\nfor line, _ in train_ds:\n    line = line.numpy().decode('utf-8')\n    lines.append(line)\n    for w in line.split(' '):\n        word_set.add(w)\n\nprint(len(word_set))\nfor index, w in enumerate(word_set):\n    if index >= 10:\n        break\n    print(f'{index:3}: {w}')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:25.634437Z","iopub.execute_input":"2021-07-19T03:16:25.634839Z","iopub.status.idle":"2021-07-19T03:16:36.473029Z","shell.execute_reply.started":"2021-07-19T03:16:25.634799Z","shell.execute_reply":"2021-07-19T03:16:36.472108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\n# vocab_size = 1000\nvocab_size = 5000\n# vocab_size = 10000\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = '<OOV>')\ntokenizer.fit_on_texts(lines)\nword_index = tokenizer.word_index\nfor index, (a, b) in enumerate(word_index.items()):\n    if index >= 5:\n        break\n    print(a, b)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:36.474282Z","iopub.execute_input":"2021-07-19T03:16:36.474772Z","iopub.status.idle":"2021-07-19T03:16:37.942155Z","shell.execute_reply.started":"2021-07-19T03:16:36.474733Z","shell.execute_reply":"2021-07-19T03:16:37.941297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:37.943427Z","iopub.execute_input":"2021-07-19T03:16:37.943793Z","iopub.status.idle":"2021-07-19T03:16:37.953411Z","shell.execute_reply.started":"2021-07-19T03:16:37.943754Z","shell.execute_reply":"2021-07-19T03:16:37.95251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for line in lines:\n    print(line)\n    sequences = tokenizer.texts_to_sequences([line])[0]\n    print(sequences)\n    print([f'{id}: {reverse_word_index[id]}' for id in sequences])\n    break","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:37.954739Z","iopub.execute_input":"2021-07-19T03:16:37.955381Z","iopub.status.idle":"2021-07-19T03:16:37.963364Z","shell.execute_reply.started":"2021-07-19T03:16:37.955337Z","shell.execute_reply":"2021-07-19T03:16:37.962213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nsequences = tokenizer.texts_to_sequences(lines[:30])\nnp.max(list(map(len, sequences)))\n\nmaxlen = 50","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:37.96479Z","iopub.execute_input":"2021-07-19T03:16:37.965179Z","iopub.status.idle":"2021-07-19T03:16:37.971239Z","shell.execute_reply.started":"2021-07-19T03:16:37.965145Z","shell.execute_reply":"2021-07-19T03:16:37.970308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndef pad_f(sequences):\n    return pad_sequences(sequences, maxlen = maxlen, padding = 'post', truncating = 'post')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:37.972421Z","iopub.execute_input":"2021-07-19T03:16:37.972696Z","iopub.status.idle":"2021-07-19T03:16:37.979716Z","shell.execute_reply.started":"2021-07-19T03:16:37.972663Z","shell.execute_reply":"2021-07-19T03:16:37.978924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\ndef tokenize_and_pad_sequence(text_batch):\n    texts = map(lambda t: t.numpy().decode('utf-8'), text_batch)\n    sequences = tokenizer.texts_to_sequences(texts)\n    return pad_f(sequences)\n    \n# https://www.tensorflow.org/api_docs/python/tf/function\n@tf.function\ndef encode_text_batch(text_batch):\n    return tf.py_function(\n        func = tokenize_and_pad_sequence,\n        inp = [text_batch],\n        Tout = tf.int32,\n    )\n\ndef create_batch_ds_inner(ds):\n    ds = ds.batch(batch_size)\n    ds = ds.map(lambda text_batch, label_batch: (encode_text_batch(text_batch), label_batch))\n    return ds.cache()\n\ndef create_batch_ds(ds, do_shuffle = True):\n    ds = create_batch_ds_inner(ds)\n    if do_shuffle:\n        ds = ds.shuffle(100)\n    return ds.prefetch(tf.data.AUTOTUNE)\n\nfor text, _ in train_ds:\n    print(text)\n    break\n    \ntrain_batch_ds = create_batch_ds(train_ds)\nvalid_batch_ds = create_batch_ds(valid_ds, do_shuffle = False)\ntotal_batch_ds = create_batch_ds(total_ds, do_shuffle = False)\n\nfor text_batch, label_batch in create_batch_ds(train_ds, do_shuffle = False).take(1):\n    print(label_batch.shape)\n    print(text_batch.shape)\n    print(text_batch[0])\n    for index in text_batch[0]:\n        index = index.numpy()\n        if index > 0:\n            print(f'{index}: {reverse_word_index[index]}')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:37.981227Z","iopub.execute_input":"2021-07-19T03:16:37.981683Z","iopub.status.idle":"2021-07-19T03:16:38.444883Z","shell.execute_reply.started":"2021-07-19T03:16:37.981554Z","shell.execute_reply":"2021-07-19T03:16:38.444088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build and Train Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_item(history_df, colname = 'loss', f = np.min, ax = None):\n    val_colname = f'val_{colname}'\n    print(f'{colname}: {f(history_df[colname]):.4f} - {val_colname}: {f(history_df[val_colname]):.4f}')\n    history_df.loc[:, [colname, val_colname]].plot(title = colname.capitalize() , ax = ax)\n\ndef show_history(history):\n    history_df = pd.DataFrame(history.history)\n    \n    fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (15, 5))\n    plot_item(history_df, 'loss', ax = axes[0])\n    plot_item(history_df, 'accuracy', ax = axes[1], f = np.max)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:38.446285Z","iopub.execute_input":"2021-07-19T03:16:38.446632Z","iopub.status.idle":"2021-07-19T03:16:38.4532Z","shell.execute_reply.started":"2021-07-19T03:16:38.446595Z","shell.execute_reply":"2021-07-19T03:16:38.452338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_model(model, train_batch_ds = train_batch_ds, epochs = 500, patience = 2):\n    model.compile(\n        optimizer = 'adam',\n        loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n        metrics = ['accuracy'],\n    )\n\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience = patience,\n        restore_best_weights = True,\n    )\n\n    history = model.fit(\n        train_batch_ds, \n        validation_data = valid_batch_ds,\n        epochs = epochs,\n        callbacks = [early_stopping],\n    )\n    return history ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:38.454506Z","iopub.execute_input":"2021-07-19T03:16:38.454864Z","iopub.status.idle":"2021-07-19T03:16:38.464276Z","shell.execute_reply.started":"2021-07-19T03:16:38.454829Z","shell.execute_reply":"2021-07-19T03:16:38.463314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DNN","metadata":{}},{"cell_type":"code","source":"# embedding_dim = 16\n\n# word size: 1000\n#   6: val_accuracy: 0.5685\n#   16: val_accuracy: 0.5709\n#   32: val_accuracy: 0.5696\n# word size: 5000\n#   16: val_accuracy: 0.5944\n\nembedding_dim = 20\n\n# word size: 5000\n#   16: val_accuracy: 0.5955\n\nmodel_dnn = keras.Sequential([\n    keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n    keras.layers.GlobalAveragePooling1D(),\n    keras.layers.Dense(16, activation = 'relu'),\n    keras.layers.Dense(5),\n])\nmodel_dnn.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:38.465635Z","iopub.execute_input":"2021-07-19T03:16:38.466032Z","iopub.status.idle":"2021-07-19T03:16:38.535134Z","shell.execute_reply.started":"2021-07-19T03:16:38.465997Z","shell.execute_reply":"2021-07-19T03:16:38.534386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_dnn = fit_model(model_dnn, patience = 5)  \nshow_history(history_dnn)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:16:38.537817Z","iopub.execute_input":"2021-07-19T03:16:38.538055Z","iopub.status.idle":"2021-07-19T03:20:07.952247Z","shell.execute_reply.started":"2021-07-19T03:16:38.538031Z","shell.execute_reply":"2021-07-19T03:20:07.951478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"# embedding_dim = 16\n#   word size: 5000\n#     val_accuracy: 0.6009\n\n# embedding_dim = 20\n#   word size: 5000\n#     val_accuracy: 0.6065\n\nmodel_lstm_bi = keras.Sequential([\n    keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n    keras.layers.Bidirectional(keras.layers.LSTM(embedding_dim)),\n    keras.layers.Dense(16, activation = 'relu'),\n    keras.layers.Dense(5),\n])\n\nmodel_lstm_bi.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:20:07.953519Z","iopub.execute_input":"2021-07-19T03:20:07.953884Z","iopub.status.idle":"2021-07-19T03:20:08.38053Z","shell.execute_reply.started":"2021-07-19T03:20:07.953846Z","shell.execute_reply":"2021-07-19T03:20:08.37974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_lstm_bi = fit_model(model_lstm_bi, patience = 5)  \nshow_history(history_lstm_bi)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:20:08.382422Z","iopub.execute_input":"2021-07-19T03:20:08.383058Z","iopub.status.idle":"2021-07-19T03:25:00.717388Z","shell.execute_reply.started":"2021-07-19T03:20:08.383019Z","shell.execute_reply":"2021-07-19T03:25:00.71661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# val_accuracy: 0.5996\nmodel_lstm_multiple_bi = keras.Sequential([\n    keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n    keras.layers.Bidirectional(keras.layers.LSTM(embedding_dim, return_sequences = True)),\n    keras.layers.Bidirectional(keras.layers.LSTM(embedding_dim)),\n    keras.layers.Dense(16, activation = 'relu'),\n    keras.layers.Dense(5),\n])\n\nmodel_lstm_multiple_bi.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:25:00.719658Z","iopub.execute_input":"2021-07-19T03:25:00.720022Z","iopub.status.idle":"2021-07-19T03:25:01.523226Z","shell.execute_reply.started":"2021-07-19T03:25:00.719985Z","shell.execute_reply":"2021-07-19T03:25:01.522456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_lstm_multiple_bi = fit_model(model_lstm_multiple_bi, patience = 5)  \nshow_history(history_lstm_multiple_bi)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:25:01.527362Z","iopub.execute_input":"2021-07-19T03:25:01.527624Z","iopub.status.idle":"2021-07-19T03:33:40.020466Z","shell.execute_reply.started":"2021-07-19T03:25:01.527598Z","shell.execute_reply":"2021-07-19T03:33:40.019683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict on Test Data","metadata":{}},{"cell_type":"code","source":"test_ds = tf.data.Dataset.from_tensor_slices(test_data.Phrase.to_numpy())\ntest_ds = test_ds.batch(batch_size)\ntest_ds = test_ds.map(lambda text_batch: encode_text_batch(text_batch))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:33:40.021928Z","iopub.execute_input":"2021-07-19T03:33:40.02227Z","iopub.status.idle":"2021-07-19T03:33:40.050695Z","shell.execute_reply.started":"2021-07-19T03:33:40.022234Z","shell.execute_reply":"2021-07-19T03:33:40.049964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_and_write_csv(model, csv_name):\n    predicted = model.predict(test_ds)\n    labels = list(map(tf.argmax, predicted))\n    labels = list(map(lambda x: x.numpy(), labels))\n    result_df = pd.DataFrame({'PhraseId': test_data.PhraseId, 'Sentiment': labels})\n    result_df.to_csv(csv_name, index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:33:40.051857Z","iopub.execute_input":"2021-07-19T03:33:40.052204Z","iopub.status.idle":"2021-07-19T03:33:40.05798Z","shell.execute_reply.started":"2021-07-19T03:33:40.052168Z","shell.execute_reply":"2021-07-19T03:33:40.056649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_final_dnn = keras.Sequential([\n    keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n    keras.layers.GlobalAveragePooling1D(),\n    keras.layers.Dense(16, activation = 'relu'),\n    keras.layers.Dense(5),\n])\nfit_model(model_final_dnn, patience = 10)  \npredict_and_write_csv(model_final_dnn, 'final_dnn_submission.csv')  ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:33:40.059574Z","iopub.execute_input":"2021-07-19T03:33:40.059955Z","iopub.status.idle":"2021-07-19T03:38:26.8679Z","shell.execute_reply.started":"2021-07-19T03:33:40.059907Z","shell.execute_reply":"2021-07-19T03:38:26.867043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_final_lstm_bi = keras.Sequential([\n    keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n    keras.layers.Bidirectional(keras.layers.LSTM(embedding_dim)),\n    keras.layers.Dense(16, activation = 'relu'),\n    keras.layers.Dense(5),\n])\nfit_model(model_final_lstm_bi, patience = 10)  \npredict_and_write_csv(model_final_dnn, 'final_lstm_submission.csv')  ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:38:26.869298Z","iopub.execute_input":"2021-07-19T03:38:26.869647Z","iopub.status.idle":"2021-07-19T03:46:31.634667Z","shell.execute_reply.started":"2021-07-19T03:38:26.869604Z","shell.execute_reply":"2021-07-19T03:46:31.633822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model = keras.Sequential([\n    keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n    keras.layers.Bidirectional(keras.layers.LSTM(embedding_dim)),\n    keras.layers.Dense(16, activation = 'relu'),\n    keras.layers.Dense(5),\n])\nfinal_model.compile(\n    optimizer = 'adam',\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n    metrics = ['accuracy'],\n)\n\nfinal_model.fit(\n    total_batch_ds, \n    epochs = 6,\n)\npredict_and_write_csv(final_model, 'final_submission.csv')  ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T03:48:34.847537Z","iopub.execute_input":"2021-07-19T03:48:34.847891Z","iopub.status.idle":"2021-07-19T03:53:19.201615Z","shell.execute_reply.started":"2021-07-19T03:48:34.847861Z","shell.execute_reply":"2021-07-19T03:53:19.200776Z"},"trusted":true},"execution_count":null,"outputs":[]}]}