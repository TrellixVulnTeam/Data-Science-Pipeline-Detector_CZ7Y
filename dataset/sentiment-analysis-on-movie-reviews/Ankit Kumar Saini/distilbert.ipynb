{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Import and installation section"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\nimport os\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\n\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# install transformers\n!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the model and tokenizer\nfrom transformers import (DistilBertTokenizerFast, \n                         TFDistilBertForSequenceClassification)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Detect and initialze tpu"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \nexcept:\n    strategy = tf.distribute.get_strategy()\n    \nprint('Number of replicas in sync: ', strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataframe display settings\npd.set_option('display.max_colwidth', None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read train and test data into pandas dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip', sep = '\\t')\ntest_data = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip', sep = '\\t')\n\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the shape of the train data\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display the head of test data\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the shape of the test data\ntest_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the number of examples in each class\ntrain_data.Sentiment.value_counts(normalize = True).plot(kind = 'bar', figsize = (10, 6), xlabel = 'Sentiments');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Dataset is highly imbalanced"},{"metadata":{},"cell_type":"markdown","source":"## Cleaning of text"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set of stop words in english\nstop_words = set(stopwords.words('english'))\n\nneg = [\"aren't\", \"didn't\", \"doesn't\", \"hadn't\",  \"haven't\", \"isn't\", 'no', 'not', \"shouldn't\", \"wasn't\", \"weren't\", \"wouldn't\"]\nstop_words.difference_update(neg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function will clean the text\ndef text_cleaning(text):\n    if text:\n        text = ' '.join(text.split('.'))\n        text = re.sub('\\/', ' ', text)\n        text = re.sub(r'\\\\', ' ', text)\n        text = re.sub(r'((http)\\S+)', '', text)\n        text = re.sub(r'\\s+', ' ', re.sub('[^A-Za-z]', ' ', text.strip().lower())).strip()\n        text = re.sub(r'\\W+', ' ', text.strip().lower()).strip()\n        text = [word for word in text.split() if word not in stop_words]\n        return text\n    return []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean train and test dataframes\ntrain_data['Phrase'] = train_data['Phrase'].apply(lambda x: ' '.join(text_cleaning(x)))\ntest_data['Phrase'] = test_data['Phrase'].apply(lambda x: ' '.join(text_cleaning(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop duplicates from train data\ntrain_data.drop_duplicates(subset = ['Phrase'], inplace = True)\ntrain_data.head(8)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Calculate length of the phrase"},{"metadata":{"trusted":true},"cell_type":"code","source":"# add length column to train data\ntrain_data['length'] = train_data['Phrase'].apply(lambda x: len(x.split()))\n\n# add length column to test data\ntest_data['length'] = test_data['Phrase'].apply(lambda x: len(x.split()))\n\n# filter the phrases from the test data with zero length\nlen_zero_data = test_data[test_data['length'] == 0]\nlen_zero_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train and validation split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# select phrases with length > 1 and split data into train and validation set\nx_train, x_val, y_train, y_val = train_test_split(train_data[train_data['length'] > 1]['Phrase'], \n                                                  train_data[train_data['length'] > 1]['Sentiment'], \n                                                  test_size = 0.2, \n                                                  stratify = train_data[train_data['length'] > 1]['Sentiment'],\n                                                  random_state = 42)\n\nprint(f'Shape of x_train: {x_train.shape}\\nShape of y_train: {y_train.shape}')\nprint(f'Shape of x_val: {x_val.shape}\\nShape of y_val: {y_val.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# encode the training and validation data\ntrain_encodings = tokenizer(x_train.tolist(), truncation = True, padding = True)\nval_encodings = tokenizer(x_val.tolist(), truncation = True, padding = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create training and validation datasets for training"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings), y_train.values)).shuffle(10000).batch(32).repeat()\n\nval_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(val_encodings), y_val.values)).shuffle(10000).batch(32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create model"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = TFDistilBertForSequenceClassification.from_pretrained(\n      'distilbert-base-uncased', num_labels = 5)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5)\n    model.compile(optimizer = optimizer, loss = model.compute_loss, metrics = \n                ['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fit the model\nmodel.fit(train_dataset, epochs = 3, batch_size = 32, steps_per_epoch = len(x_train) // 32,\n          validation_data = val_dataset, validation_steps = len(x_val) // 32)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}