{"cells":[{"metadata":{"_uuid":"649aae11383b3f8c9273ae4410e6f5a35d2f70ed"},"cell_type":"markdown","source":"# Preface\n\nThis notebook was created as a home-work exercise for a job interview. The problem is a classic multi-class classification (instead of positive / negative predictions there is a gradual 0 to 4 sentiment scale). Most straight-forward way to solve this would be to use some classification algorythm, like K-Means, however I had recently done a MOOC on deep learning, and sentiment analysis was one of the topics (Udacity Deep Learning Nanodegree scholarship challenge, lesson on sentiment analysis with RNN), so I decided to try and apply that approach here. Besides, it has been found that recurring neural networks (RNN's) achieve higher accuracy at such tasks. \n\nI will be using the Pytorch library for model building and NLTK for text pre-processing if needed. I will also not try to re-invent the wheel, and will be re-using code examples from online sources, as well as Udacity course material, in an effort to save time.\n\nJudging from the leaderboard, anything above 0.65  score could be considered a good result for this competition, so I will use that as benchmark to judge my model performance. "},{"metadata":{"_uuid":"986ce81647d2c1a554e91d05692d30b593a144a4"},"cell_type":"markdown","source":"## Importing dependencies"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport unicodedata, re, string\nimport nltk\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nimport seaborn as sns\nsns.set(color_codes=True)\n\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ea4268a41e63dbf967bdaf1d8a66112e0e295de"},"cell_type":"markdown","source":"## Exploring The  Data"},{"metadata":{"trusted":true,"_uuid":"81953f0026a9fd8fc234e76b9dc225335efbc354"},"cell_type":"code","source":"df_train = pd.read_csv(\"../input/train.tsv\", sep=\"\\t\")\ndf_test = pd.read_csv(\"../input/test.tsv\", sep=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f69cc2ba0f2cb2ab79807a16b4d252b7a4339d6e"},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"109a0161dc81056c529f6f10cb558fd4e06a8206"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30d1ae62f1dc4f3decfd1fea4eca717c2c1ff288"},"cell_type":"code","source":"df_train['Phrase'][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bad32939bedaef058d0ecb0a0fae2ec5adee250e"},"cell_type":"code","source":"df_train.loc[df_train['SentenceId'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6b71ead238ca90002183a854334da49dd319360"},"cell_type":"markdown","source":"As was mentioned in the original competition description, there are no individual movie reviews but rather phrases taken out of context and split into smaller parts, each with an assigned sentiment category. The competition is evaluated based on scoring results of each test phrase, so the context of the whole review does not matter here. The data is also fairly clean, so there will not be need for much pre-processing.\nBefore proceeding it is also a good idea to look at distribution of data, to see if the classes in training set are evenly distributed. For that I borrowed code from another Kaggle kernel:"},{"metadata":{"trusted":true,"_uuid":"14a584929ac8ef23a3bdf184d8fb1f65556fac38"},"cell_type":"code","source":"dist = df_train.groupby([\"Sentiment\"]).size()\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.barplot(dist.keys(), dist.values);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3485562c7cd2ac11917b5e9953b761ad94110a2e"},"cell_type":"markdown","source":"Classes seem to follow a normal distribution, with most frequently distributed class being \"2\".  This could lead to model not having sufficient data to learn the less-represented classes. This is something to be aware of when evaluating the model."},{"metadata":{"_uuid":"5ff180b541a1e811cec7086397097281bca17b10"},"cell_type":"markdown","source":"## Pre-Processing\n\nWords need to be tokenized into numeric format to be passed to RNN. Before that, however, I will also filter out spaces and punctuation, and use lemmatization to further reduce dimensionality. At this moment I do not want to filter out \"stop-words\", as RNN's are good at learning context from previously encountered information. In case of movie reviews, phrase \"this movie is shit\" has opposite meaning of \"this movie is the shit\", so I want that information to be available to the model.\n\nBelow are some helper functions I borrowed online to help prepare the data:"},{"metadata":{"trusted":true,"_uuid":"271abb29213dabf44a4ac92038346dd00f2b83ec"},"cell_type":"code","source":"def remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_numbers(words):\n    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(\"\\d+\", \"\", word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    words = remove_numbers(words)\n#    words = remove_stopwords(words)\n    return words","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c62b3a073970c68147125e5dc4761fab0b111413"},"cell_type":"markdown","source":"Time to get the hands dirty. First I will go through the dataframe and tokenize each word using NLTK. Then I will pass each token through the prepping functions I created earlier, with the end result being a reduced list of lemmatized word tokens:"},{"metadata":{"trusted":true,"_uuid":"0c7e2fa70145639438d6a17c0af8e8c61517768f"},"cell_type":"code","source":"# First step - tokenizing phrases\ndf_train['Words'] = df_train['Phrase'].apply(nltk.word_tokenize)\n\n# Second step - passing through prep functions\ndf_train['Words'] = df_train['Words'].apply(normalize) \ndf_train['Words'].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"157930b980a9f30179276767d9b44bdef8c76af8"},"cell_type":"markdown","source":"Looks ok. Now the next prep step - converting words to number representations, as the embedding lookup requires that integers are passed to the network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Using this vocab then words in each phrase can be converted to integers:"},{"metadata":{"trusted":true,"_uuid":"d6ed075962d2b8cabb2193803d7cd5e8ea7b8ca4"},"cell_type":"code","source":"# Third step - creating a list of unique words to be used as dictionary for encoding\nword_set = set()\nfor l in df_train['Words']:\n    for e in l:\n        word_set.add(e)\n        \nword_to_int = {word: ii for ii, word in enumerate(word_set, 1)}\n\n# Check if they are still the same lenght\nprint(len(word_set))\nprint(len(word_to_int))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a41485c79f55a5981b07b77abc67cf2e2046ee7"},"cell_type":"code","source":"# Now the dict to tokenize each phrase\ndf_train['Tokens'] = df_train['Words'].apply(lambda l: [word_to_int[word] for word in l])\ndf_train['Tokens'].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40ce0f1b2671f9b542f55b634a80996d7d80ebed"},"cell_type":"markdown","source":"So far so good. But for the input to the network the length of each phrase sequence has to be equal, so the shorter phrases will need to be \"padded\" - zeros added so that their token numbers are the same length."},{"metadata":{"trusted":true,"_uuid":"bd4c04ded25ff052cff54f3a3695d7e880229dca"},"cell_type":"code","source":"# Step four - get the len of longest phrase\nmax_len = df_train['Tokens'].str.len().max()\nprint(max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"380adf993084f5aec052f0816be116680f44e403","scrolled":true},"cell_type":"code","source":"# Pad each phrase representation with zeroes, starting from the beginning of sequence\n# Will use a combined list of phrases as np array for further work. This is expected format for the Pytorch utils to be used later\n\nall_tokens = np.array([t for t in df_train['Tokens']])\nencoded_labels = np.array([l for l in df_train['Sentiment']])\n\n# Create blank rows\nfeatures = np.zeros((len(all_tokens), max_len), dtype=int)\n# for each phrase, add zeros at the end \nfor i, row in enumerate(all_tokens):\n    features[i, :len(row)] = row\n\n#print first 3 values of the feature matrix \nprint(features[:3])\n\n ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"513a47fbb21155a850e0845839aec12962afe5f8"},"cell_type":"markdown","source":"## Splitting the Data for Training, Validation, Test\n\nTime to split the data into training, validation, and test sets. For this purpose I will reserve 80% of training data for training, and remaining 20% will be split equally for validation and testing purposes.\n"},{"metadata":{"trusted":true,"_uuid":"9879d59cdaba3f64bafe7e1724251ee80dc3a16a"},"cell_type":"code","source":"split_frac = 0.8\n\n## split data into training, validation, and test data (features and labels, x and y)\n\nsplit_idx = int(len(features)*0.8)\ntrain_x, remaining_x = features[:split_idx], features[split_idx:]\ntrain_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n\ntest_idx = int(len(remaining_x)*0.5)\nval_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\nval_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n\n## print out the shapes of  resultant feature data\nprint(\"\\t\\t\\tFeature Shapes:\")\nprint(\"Train set: \\t\\t{}\".format(train_x.shape), \n      \"\\nValidation set: \\t{}\".format(val_x.shape),\n      \"\\nTest set: \\t\\t{}\".format(test_x.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77cd3cba1b1a3546fe3f6903399f399ef217884a"},"cell_type":"markdown","source":"## DataLoaders and Batching\nAfter creating training, test, and validation data, time top create DataLoaders. They are the expected way to pass data into the model for training / testing. Loaders are created by following two steps:\n\n1) Create a known format for accessing data, using TensorDataset which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n\n2) Create DataLoaders and batch our training, validation, and test Tensor datasets."},{"metadata":{"trusted":true,"_uuid":"1f71a409ddc465da2108550c6260120b11db5a7f"},"cell_type":"code","source":"# create Tensor datasets\ntrain_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\ntest_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n\n# dataloaders\nbatch_size = 54\n\n# make sure the SHUFFLE your training data\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nvalid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\n# Check the size of the loaders (how many batches inside)\nprint(len(train_loader))\nprint(len(valid_loader))\nprint(len(test_loader))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3621e913b85c6015d00b12b11adf5e0f68933a1"},"cell_type":"markdown","source":"## Creating a Deep Network\n\nThe following text is borrowed from another excersize file, but it describes the approach of model building very well:\n\n1) First, we'll pass in words to an embedding layer. We need an embedding layer because we have thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. In this case, the embedding layer is for dimensionality reduction, rather than for learning semantic representations.\n\n2) After input words are passed to an embedding layer, the new embeddings will be passed to LSTM cells. The LSTM cells will add recurrent connections to the network and give us the ability to include information about the sequence of words in the movie review data.\nLSTM takes in an input_size, a hidden_dim, a number of layers, a dropout probability (for dropout between multiple layers), and a batch_first parameter.\n\n3) Finally, the LSTM outputs will go to a linear layer for final classification, which outputs in turn will be passed to cross-entropy loss function to obtain probabilities for each predicted class.\n\nThe layers are as follows:\n* An embedding layer that converts word tokens (integers) into embeddings of a specific size.\n* An LSTM layer defined by a hidden_state size and number of layers\n* A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n* A softmax will be applyed later by Crossentropy loss function, which turns all outputs into a probability\n\nMost of the time, the network will have better performance with more layers; between 2-3. Adding more layers allows the network to learn really complex relationships.\n"},{"metadata":{"trusted":true,"_uuid":"4395f5c0690c68ea73bb0f0d8f78b453c95c49cd"},"cell_type":"code","source":"# First checking if GPU is available\ntrain_on_gpu=torch.cuda.is_available()\n\nif(train_on_gpu):\n    print('Training on GPU.')\nelse:\n    print('No GPU available, training on CPU.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"083cc4a1a4ea66e1c0188338b53a3f1ddecd1e24"},"cell_type":"code","source":"class SentimentRNN(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        \"\"\"\n        Initialize the model by setting up the layers.\n        \"\"\"\n        super(SentimentRNN, self).__init__()\n\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        # embedding and LSTM layers\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        # dropout layer\n        self.dropout = nn.Dropout(0.3)\n        \n        # linear\n        self.fc = nn.Linear(hidden_dim, output_size)\n        \n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size = x.size(0)\n\n        # embeddings and lstm_out\n        embeds = self.embedding(x)\n\n        lstm_out, hidden = self.lstm(embeds, hidden)\n\n        # transform lstm output to input size of linear layers\n        lstm_out = lstm_out.transpose(0,1)\n        lstm_out = lstm_out[-1]\n\n        out = self.dropout(lstm_out)\n        out = self.fc(out)        \n\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b3833c651a26de8c38d005a3566340e43d2a47b"},"cell_type":"markdown","source":"## Instantiate the network\nHere, we'll instantiate the network. First up, defining the hyperparameters.\n* vocab_size: Size of our vocabulary or the range of values for our input, word tokens.\n* output_size: Size of our desired output; the number of class scores we want to output (0..4).\n* embedding_dim: Number of columns in the embedding lookup table; size of our embeddings.\n* hidden_dim: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n* n_layers: Number of LSTM layers in the network. Typically between 1-3"},{"metadata":{"trusted":true,"_uuid":"3e1ebcf1dcdf3ae074696b41538b4f593bfa525d"},"cell_type":"code","source":"# Instantiate the model w/ hyperparams\nvocab_size = len(word_to_int)+1 # +1 for the 0 padding\noutput_size = 5\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2\n\nnet = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n\nprint(net)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"024b493c908d9323330a1824369b2f9339372c33"},"cell_type":"markdown","source":"## Training Routine\nBelow is the typical training code. \nCrossentropy loss will be used, since this is multi-class classification problem.\nWe also have some data and training hyparameters:\n* lr: Learning rate for our optimizer.\n* epochs: Number of times to iterate through the training dataset.\n* clip: The maximum gradient value to clip at (to prevent exploding gradients)."},{"metadata":{"trusted":true,"_uuid":"19f913418591679cf69d24df7b7e06b86b56a4be"},"cell_type":"code","source":"# loss and optimization functions\nlr=0.003\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97778b9a79c598347e9f73712b50323d03405fef"},"cell_type":"code","source":"# training params\nepochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n\n        if(train_on_gpu):\n            inputs, labels = inputs.cuda(), labels.cuda()\n\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n        # calculate the loss and perform backprop\n        loss = criterion(output, labels)\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n\n                if(train_on_gpu):\n                    inputs, labels = inputs.cuda(), labels.cuda()\n\n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output, labels)\n\n                val_losses.append(val_loss.item())\n\n            net.train()\n            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43750e21ea76ee1b90c0d3cffed952f68495e510"},"cell_type":"markdown","source":"## Testing\n\nThere are a few ways to test your network.\n* Test data performance: First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data.\n* Inference on user-generated data: Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts. Looking at new, user input data like this, and predicting an output label, is called inference.\n\nFor the practical purposes of this example, though, second option is not applicable, as the task is to classify syntetic colection of provided phrases."},{"metadata":{"trusted":true,"_uuid":"73615beffd56464165901f7589bacc2dafa0d920"},"cell_type":"code","source":"# Get test data loss and accuracy\n\ntest_losses = [] # track loss\nnum_correct = 0\n\n# init hidden state\nh = net.init_hidden(batch_size)\n\nnet.eval()\n# iterate over test data\nfor inputs, labels in test_loader:\n\n    # Creating new variables for the hidden state, otherwise\n    # we'd backprop through the entire training history\n    h = tuple([each.data for each in h])\n\n    if(train_on_gpu):\n        inputs, labels = inputs.cuda(), labels.cuda()\n    \n    # get predicted outputs\n    output, h = net(inputs, h)\n    \n    # calculate loss\n    test_loss = criterion(output, labels)\n    test_losses.append(test_loss.item())\n    \n    # convert output probabilities to predicted class\n    _, pred = torch.max(output,1)\n    \n    # compare predictions to true label\n    correct_tensor = pred.eq(labels.view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\n# -- stats! -- ##\n# avg test loss\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n\n# accuracy over all test data\ntest_acc = num_correct/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}\".format(test_acc))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f7a5a9451966ce30d7ed18c44e4db5569a62efb"},"cell_type":"markdown","source":"## Conclusions\n\nThe best accuracy score that I managed to achieve has been around 0.57\nThis is rather far from the best submissions at 0.65, but for a quickly strung-up model perhaps not so bad. That said, I have some doubts that I managed to correctly do all the matrix reshaping to correctly pass data from lstm to linear. The validation loss seems to not be reducing too much, but this may be also caused by poor choice in initial LR. One could try the approach of gradual LR decay to see if that improves accuracy. \n\nThis notebook can be used to further fine-tune the model to increase the accuracy, all the basics are in here.\nOverall, this was an interesting assignment, and this is what I managed to put together in a limited time provided. The most difficult part was making the model architecture to work, particularly trying to correctly reshape LSTM output to be further passed to linear layers. Even though I used code snippets provided online, it took more time and effort then I initially expected to make it all work together. Perhaps doing a more simple \"bag-of-words\" model would be a better idea. Something to try in the future. \n"},{"metadata":{"trusted":true,"_uuid":"d61cc2aad4ddc8dafe532c04903c1c6931eb3564"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}