{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!unzip /kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip\n!unzip /kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('train.tsv',sep = '\\t')\ndata.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in the train data, phrase is the sentence and Sentiment is the label","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data['Sentiment'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_d, test_d, train_y, test_y = train_test_split(\n    data['Phrase'], data['Sentiment'], test_size=0.25, random_state=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_dummies(labels, size = 5):\n    res = []\n    for i in labels:\n        temp = [0] * size\n        temp[i] = 1\n        res.append(temp)\n    return res\n\ntrain_labels, test_labels = get_dummies(train_y), get_dummies(test_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pytorch_transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_transformers import BertTokenizer\nmodel_name = 'bert-base-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\n\nsample_sentence = train_d[0]\n\nprint(sample_sentence)\n\nprint(tokenizer.tokenize('[CLS]' + sample_sentence + '[SEP]'))\n\nprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize('[CLS]' + sample_sentence + '[SEP]')))\n\n\n# print(tokenizer.encode_plus(\n#                         sample_sentence,    # Sentence to encode.\n#                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n#                         max_length = 100,           # Pad & truncate all sentences.\n#                         pad_to_max_length = True,\n#                         return_attention_mask = True,   # Construct attn. masks.\n# #                         return_tensors = 'pt',     # Return pytorch tensors.\n#                    ))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"BerT part","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_transformers import BertTokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\ntokenized_text = [tokenizer.tokenize('[CLS]' + i + '[SEP]') for i in train_d]\ninput_ids = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(input_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for j in range(len(input_ids)):\n    i = input_ids[j]\n    if len(i) != 128:\n        input_ids[j].extend([0] * (128 - len(i))) #extend sentence to 512, but we dont need dat much","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nimport torch\ntrain_set = TensorDataset(torch.LongTensor(input_ids),\n                         torch.FloatTensor(train_labels))\ntrain_loader = DataLoader(dataset = train_set, batch_size = 32, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_text = [tokenizer.tokenize('[CLS]' + i + '[SEP]') for i in test_d]\ninput_ids = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]\n\nfor j in range(len(input_ids)):\n    i = input_ids[j]\n    if len(i) != 128:\n        input_ids[j].extend([0] * (128 - len(i))) #extend sentence to 512, but we dont need dat much\n\ntest_set = TensorDataset(torch.LongTensor(input_ids),\n                         torch.FloatTensor(test_labels))\ntest_loader = DataLoader(dataset = test_set, batch_size = 32, shuffle = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"build our classifer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom pytorch_transformers import BertModel\n\nclass fn_cls(nn.Module):\n    def __init__(self):\n        super(fn_cls, self).__init__()\n        self.model = BertModel.from_pretrained(model_name)\n        self.model.to(device)\n        self.dropout = nn.Dropout(0.1)\n        self.l1 = nn.Linear(768, 5) #768 is the bert-base hidden size\n    def forward(self, x, attention_mask = None):\n        outputs = self.model(x, attention_mask = attention_mask)\n        x = outputs[1]\n        x = x.view(-1, 768)\n        x = self.dropout(x)\n        x = self.l1(x)\n        return x        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_transformers import BertModel\nmodel = BertModel.from_pretrained(model_name)\n# Get all of the model's parameters as a list of tuples.\nparams = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import optim\n\ncls = fn_cls()\ncls.to(device)\nlossF = nn.BCELoss()\nsigmoid = nn.Sigmoid()\noptimizer = optim.Adam(cls.parameters(), lr = 1e-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(logits):\n    res = torch.argmax(logits, 1)\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_model(data, target):\n    correct = 0\n    cls.train()\n    data = data.to(device)\n    target = target.to(device)\n    mask = []\n    for sample in data:\n        mask.append([1 if i != 0 else 0 for i in sample])\n    mask = torch.Tensor(mask).to(device)\n    \n    output = cls(data, attention_mask = mask)\n    loss = lossF(sigmoid(output).view(-1, 5), target)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    pred = predict(output)\n    correct += (pred == predict(target)).sum().item()\n    \n    return correct,loss\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def eval_model():\n    cls.eval()\n    correct = 0\n    total = 0\n    for batch_idx, (data, target) in enumerate(test_loader):\n        data = data.to(device)\n        target = target.to(device)\n        mask = []\n        for sample in data:\n            mask.append([1 if i != 0 else 0 for i in sample])\n        mask = torch.Tensor(mask).to(device)\n\n        output = cls(data, attention_mask=mask)\n        pred = predict(output)\n\n        correct += (pred == predict(target)).sum().item()\n        total += len(data)\n    return correct/total","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.autograd import Variable\nimport time\n\npre = time.time()\nepoch = 3\n\nfor i in range(epoch):\n    train_corrects = []\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data).to(device), Variable(target).view(-1, 5).to(device)\n        train_correct,loss = train_model(data, target)\n        train_corrects.append(train_correct)\n        if batch_idx % 1000 == 0:\n            test_correct = eval_model()\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss:{:.4f} \\tTraining_acc: {:.4f} \\tTesting_acc: {:.4f}'.format(\n                i+1, batch_idx, len(train_loader), 100. *\n                batch_idx/len(train_loader), loss.item(),\n                sum(train_corrects)/ ((batch_idx + 1) * 32),\n                test_correct))\n\n                   \nprint('time comsumed: ', time.time() - pre)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('test.tsv', sep = '\\t')\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_text = [tokenizer.tokenize('[CLS]' + i + '[SEP]') for i in test_df['Phrase']]\ninput_ids = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]\n\nfor j in range(len(input_ids)):\n    i = input_ids[j]\n    if len(i) != 128:\n        input_ids[j].extend([0] * (128 - len(i))) #extend sentence to 512, but we dont need dat much\n\noutput_data = TensorDataset(torch.LongTensor(input_ids))\noutput_loader = DataLoader(dataset = output_data, batch_size = 1, shuffle = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm\ncls.eval()\n\nres_pred = []\nfor batch_idx, (data) in enumerate(tqdm(output_loader)):\n    data = data[0].to(device)\n    \n    mask = []\n    for sample in data:\n        mask.append([1 if i != 0 else 0 for i in sample])\n    mask = torch.Tensor(mask).to(device)\n    \n    output = cls(data, attention_mask=mask)\n    pred = predict(output)\n    res_pred.append(pred)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_final = [result.item() for result in res_pred]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Sentiment'] = res_final\nresult_df = test_df[['PhraseId','Sentiment']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ids[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_df.to_csv('Submission.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}