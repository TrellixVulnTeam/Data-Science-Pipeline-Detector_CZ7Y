{"cells":[{"metadata":{},"cell_type":"markdown","source":"# BERT for Sentiment Analysis\nIn this notebook, I will use the [BERT](https://arxiv.org/abs/1810.04805) model to accomplish the classification task on movie reviews. The idea is pretty simple, to regard BERT as an embedding layer and then pass through the result to a 3-layer bidirectional GRU further processed by a linear layer and softmax function.\n\nBERT model are considerably large, so it is impossible to train a BERT from the very begining in this notebook. Fortunately, [transformers library](https://github.com/huggingface/transformers) offers a pre-trained BERT model with well-maintained documentation. We can take advantage of it.\n\nThe pipeline of building a BERT model in this notebook mainly follows a [tutorial](github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb) on pytorch and sentiment analysis. You can find more detials on how to build simple model based on BERT by clicking on the link above.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1 Prepare Data","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\n\ndef seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\n\nseed_all(2020)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize_and_cut(sentence):\n    tokens = tokenizer.tokenize(sentence)\n    max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n    tokens = tokens[:max_input_length-2]\n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext import data\n\nTEXT = data.Field(batch_first = True,\n                  use_vocab = False,\n                  tokenize = tokenize_and_cut,\n                  preprocessing = tokenizer.convert_tokens_to_ids,\n                  init_token = tokenizer.cls_token_id,\n                  eos_token = tokenizer.sep_token_id,\n                  pad_token = tokenizer.pad_token_id,\n                  unk_token = tokenizer.unk_token_id)\n\nLABEL = data.Field(sequential=False, use_vocab=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!mkdir ./data\n!unzip -d ./data ../input/sentiment-analysis-on-movie-reviews/train.tsv.zip\n!unzip -d ./data ../input/sentiment-analysis-on-movie-reviews/test.tsv.zip","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To simplify the case, I drop attribute `PhraseId` and `SentenceId` in the dataset, which might contain useful information.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext import datasets\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ntrain = pd.read_csv('./data/train.tsv', sep='\\t')\ntest = pd.read_csv('./data/test.tsv', sep='\\t')\ntrain, valid = train_test_split(train, test_size=0.2)\ntrain.to_csv('./data/train.csv', index=False)\nvalid.to_csv('./data/validation.csv', index=False)\n\ntrain, valid = data.TabularDataset.splits(\n    path='./data', train='train.csv', validation='validation.csv', format='csv', skip_header=True,\n    fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT), ('Sentiment', LABEL)])\ntest = data.TabularDataset('./data/test.tsv', format='tsv', skip_header=True,\n                           fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f\"Number of training examples: {len(train)}\")\nprint(f\"Number of validation examples: {len(valid)}\")\nprint(f\"Number of testing examples: {len(test)}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A single input and target pair is stored as a `torchtext.data.Example` object in the structured dataset. Check an example to ensure things go right.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(vars(train[6]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tokenizer.convert_ids_to_tokens(vars(train[6])['Phrase']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 256\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iter = data.BucketIterator(train, batch_size=BATCH_SIZE, shuffle=True, device=DEVICE)\nvalid_iter = data.BucketIterator(valid, batch_size=BATCH_SIZE, shuffle=True, device=DEVICE)\ntest_iter = data.Iterator(test, batch_size=BATCH_SIZE, train=False, sort=False, device=DEVICE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Have a look at a mini-batch.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch = next(iter(train_iter))\nphrase = batch.Phrase\nsent = batch.Sentiment\nprint(phrase.shape)\nprint(phrase)\nprint(sent.shape)\nprint(sent)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2 Build the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertModel\n\nbert = BertModel.from_pretrained('bert-base-uncased')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"None of Pytorch RNN modules support different hidden size across different layers, so I need to implement it by hand.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import nn\n\nclass BERTGRUSentiment(nn.Module):\n    def __init__(self, bert, output_dim):\n        super().__init__()\n        self.bert = bert\n        self.embedding_dim = bert.config.to_dict()['hidden_size']\n        self.gru11 = nn.GRU(self.embedding_dim, 512, num_layers=1, batch_first=True)\n        self.gru12 = nn.GRU(512, 256, num_layers=1, batch_first=True)\n        self.gru13 = nn.GRU(256, 128, num_layers=1, batch_first=True)\n        self.gru21 = nn.GRU(self.embedding_dim, 512, num_layers=1, batch_first=True)\n        self.gru22 = nn.GRU(512, 256, num_layers=1, batch_first=True)\n        self.gru23 = nn.GRU(256, 128, num_layers=1, batch_first=True)\n\n        self.fc = nn.Linear(256, output_dim)\n        \n        self.dropout1 = nn.Dropout(0.5)\n        self.dropout2 = nn.Dropout(0.3)\n\n        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    def forward(self, text):\n        # text = [batch size, sent len]\n\n        with torch.no_grad():\n            embedding = self.bert(text)[0]  # embedding = [batch size, sent len, emb dim]\n\n        output1, _ = self.gru11(embedding)\n        output1 = self.dropout1(output1)  # output1 = [batch size, sent len, 512]\n        \n        output1, _ = self.gru12(output1)\n        output1 = self.dropout2(output1)  # output1 = [batch size, sent len, 256]\n        \n        _, hidden1 = self.gru13(output1)  # hidden1 = [1, batch size, 128]\n\n        reversed_embedding = torch.from_numpy(embedding.detach().cpu().numpy()[:, ::-1, :].copy()).to(self.DEVICE)\n        \n        output2, _ = self.gru21(reversed_embedding)\n        output2 = self.dropout1(output2)  # output2 = [batch size, sent len, 512]\n        \n        output2, _ = self.gru22(output2)\n        output2 = self.dropout2(output2)  # output1 = [batch size, sent len, 256]\n        \n        _, hidden2 = self.gru23(output2)  # hidden2 = [1, batch size, 128]\n        \n        hidden = self.dropout2(torch.cat((hidden1[-1, :, :], hidden2[-1, :, :]), dim=1))  # hidden = [batch size, 256]\n\n        output = self.fc(hidden)  # output = [batch size, out dim]\n\n        return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OUTPUT_DIM = 5\n\nmodel = BERTGRUSentiment(bert, OUTPUT_DIM)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import torch.nn as nn\n\n# class BERTGRUSentiment(nn.Module):\n#     def __init__(self, bert, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n#         super().__init__()\n#         self.bert = bert\n#         embedding_dim = bert.config.to_dict()['hidden_size']\n#         self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional,\n#                           batch_first = True, dropout = 0 if n_layers < 2 else dropout)\n#         self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n#         self.dropout = nn.Dropout(dropout)\n        \n#     def forward(self, text):\n#         # text = [batch size, sent len]\n        \n#         with torch.no_grad():\n#             embedding = self.bert(text)[0]  # embedding = [batch size, sent len, emb dim]\n\n#         _, hidden = self.gru(embedding)  # hidden = [n layers * n directions, batch size, hid dim]\n        \n#         if self.gru.bidirectional:\n#             hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n#         else:\n#             hidden = self.dropout(hidden[-1,:,:])\n                \n#         # hidden = [batch size, hid dim]\n        \n#         output = self.fc(hidden)\n        \n#         # output = [batch size, out dim]\n        \n#         return output\n\n# HIDDEN_DIM = 256\n# OUTPUT_DIM = 5\n# N_LAYERS = 2\n# BIDIRECTIONAL = True\n# DROPOUT = 0.3\n\n# model = BERTGRUSentiment(bert, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set `requires_grad = False` for all parameters in `bert` model, so no fine-tuning on `bert`.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, param in model.named_parameters():                \n    if name.startswith('bert'):\n        param.requires_grad = False\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for name, param in model.named_parameters():                \n    if param.requires_grad:\n        print(name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3 Train the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\noptimizer = optim.Adam(model.parameters())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model.to(DEVICE)\ncriterion = criterion.to(DEVICE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\ndef accuracy(prediction, label):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"\n    prediction = torch.argmax(nn.functional.softmax(prediction, dim=1), dim=1)\n    acc = torch.sum(prediction == label).float() / len(prediction == label)\n    return acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        optimizer.zero_grad()\n        \n        data = batch.Phrase\n        label = batch.Sentiment\n        \n        prediction = model(data)\n        \n        loss = criterion(prediction, label)\n        \n        acc = accuracy(prediction, label)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n            \n            data = batch.Phrase\n            label = batch.Sentiment\n            \n            prediction = model(data)\n            \n            loss = criterion(prediction, label)\n            \n            acc = accuracy(prediction, label)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_EPOCHS = 10\n\nbest_epoch = 0\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n        \n    end_time = time.time()\n        \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n        \n    if valid_loss < best_valid_loss:\n        best_epoch = epoch + 1\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train the model on entire training set again before submission. Since the training set is larger, one more epoch is assigned.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchtext import data\ntrain_full = data.TabularDataset('./data/train.tsv', format='tsv', skip_header=True,\n                                 fields=[('PhraseId', None), ('SentenceId', None),\n                                         ('Phrase', TEXT), ('Sentiment', LABEL)])\n\ntrain_full_iter = data.BucketIterator(train_full, batch_size=BATCH_SIZE, shuffle=True, device=DEVICE)\n\nmodel = BERTGRUSentiment(bert, OUTPUT_DIM)\n\noptimizer = optim.Adam(model.parameters())\n\ncriterion = nn.CrossEntropyLoss()\n\nmodel = model.to(DEVICE)\ncriterion = criterion.to(DEVICE)\n\nfor epoch in range(best_epoch + 1):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_full_iter, optimizer, criterion)\n        \n    end_time = time.time()\n        \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n        \n    if epoch == best_epoch:\n        torch.save(model.state_dict(), 'model_full.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(model, iterator):\n    \n    model.eval()\n    \n    predictions = []\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n            \n            data = batch.Phrase\n            \n            prediction = model(data)\n            \n            prediction = torch.argmax(nn.functional.softmax(prediction, dim=1), dim=1)\n            \n            predictions.extend(prediction.tolist())\n        \n    return predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = predict(model, test_iter)\n\nsubmission = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv')\nsubmission['Sentiment'] = predictions\nsubmission.to_csv('submissionBERTGRU.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}