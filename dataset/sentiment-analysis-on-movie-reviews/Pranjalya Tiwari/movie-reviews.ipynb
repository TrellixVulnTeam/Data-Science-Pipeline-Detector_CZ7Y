{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Task\n\n**To use a model to guess the sentiment of the movie review, under 5 classes -** \n1. Highly Positive\n2. Somewhat Positive\n3. Neutral\n4. Somewhat Negative\n5. Highly Negative"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nos.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Insights and Distribution"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.tsv', delimiter='\\t', encoding='utf-8')\ntest = pd.read_csv('../input/test.tsv', delimiter='\\t', encoding='utf-8')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualisation\n\nLet's check upon how data is distributed."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plot.figure(figsize=(15, 5))\nsns.countplot(data=train, x='Sentiment')\nplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can clearly see, most of the sentiment is 'Neutral', which may cause problem, as it'd not distribute the training of the machine equally.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_count():\n    s0 = train[train.Sentiment == 0].Sentiment.count()\n    s1 = train[train.Sentiment == 1].Sentiment.count()\n    s2 = train[train.Sentiment == 2].Sentiment.count()\n    s3 = train[train.Sentiment == 3].Sentiment.count()\n    s4 = train[train.Sentiment == 4].Sentiment.count()\n    return s0, s1, s2, s3, s4\n\ns0, s1, s2, s3, s4 = get_count()\nprint(s0, s1, s2, s3, s4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df0 = s2 // s0 - 1\ndf1 = s2 // s1 - 1\ndf3 = s2 // s3 - 1\ndf4 = s2 // s4 - 1\n \ntrain = train.append([train[train.Sentiment == 0]] * df0, ignore_index=True)\ntrain = train.append([train[train.Sentiment == 1]] * df1, ignore_index=True)\ntrain = train.append([train[train.Sentiment == 3]] * df3, ignore_index=True)\ntrain = train.append([train[train.Sentiment == 4]] * df4, ignore_index=True)\ntrain = train.append([train[train.Sentiment == 0][0 : s2 % s0]], ignore_index=True)\ntrain = train.append([train[train.Sentiment == 1][0 : s2 % s1]], ignore_index=True)\ntrain = train.append([train[train.Sentiment == 3][0 : s2 % s3]], ignore_index=True)\ntrain = train.append([train[train.Sentiment == 4][0 : s2 % s4]], ignore_index=True)\n\ns0, s1, s2, s3, s4 = get_count()\nprint(s0, s1, s2, s3, s4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hence, what we've achieved is that now we have made a corpus of very equal distributions. This will help us to perform impartial training. We can visualise it here.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plot.figure(figsize=(15, 5))\nsns.countplot(data = train, x = 'Sentiment')\nplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing and Cleaning the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom joblib import Parallel, delayed\nimport string \nimport time \n\nlemma = WordNetLemmatizer() \nstopwords  = stopwords.words('english')\nstopwords.extend(['cinema', 'film', 'series', 'movie', 'one', 'like', 'story', 'plot'])\n\ndef clean_review(review):\n    tokens = review.lower().split()\n    filtered_tokens = [lemma.lemmatize(w) for w in tokens if w not in stopwords]\n    return \" \".join(filtered_tokens)\n\nstart_time = time.time()\nclean_train_data = train.copy()\nclean_train_data['Phrase'] = Parallel(n_jobs=4)(delayed(clean_review)(review) for review in train['Phrase'])\nend_time = time.time()\nprint(\"Cleaning Training Data Time - Processing Time = \", end_time - start_time)\n\n# remove missing values\nprint(\"Cleaned entries: \", clean_train_data.shape[0], \" out of \", train.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Forming Training and Cross-Validation Set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntarget = clean_train_data.Sentiment\ntrain_X_, validation_X_, train_y, validation_y = train_test_split(clean_train_data['Phrase'], target, test_size=0.2, random_state=22)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hence, we succesfully split our data into training and validation sets. Now, we convert the data into integers using TFIDF Vectorizer."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n\ntfidf_vec = tfidf(min_df=3,  max_features=None, ngram_range=(1, 2), use_idf=1)\ntrain_X = tfidf_vec.fit_transform(train_X_)\n\nprint(\"Succesfully vectorized the data.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating and applying various models"},{"metadata":{},"cell_type":"markdown","source":"**1. Using Bag-of-Words + Multinomial Naive Bayes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nprint(\"Using Multinomial Naive Bayes : \\n\")\nmodel = MultinomialNB()\nmodel.fit(train_X, train_y)\nvalidation_X = tfidf_vec.transform(validation_X_)\npredicted = model.predict(validation_X)\nexpected = validation_y\nprint(metrics.classification_report(expected, predicted))\nprint(\"Accuracy Score in Cross-Validation Set : \", metrics.accuracy_score(expected, predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**2. Using Linear SVM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC\nfrom sklearn import metrics\n\nprint(\"Using Linear SVM : \\n\")\nmodel = LinearSVC()\nmodel.fit(train_X, train_y)\nvalidation_X = tfidf_vec.transform(validation_X_)\npredicted = model.predict(validation_X)\nexpected = validation_y\nprint(metrics.classification_report(expected, predicted))\nprint(\"Accuracy Score in Cross-Validation Set : \", metrics.accuracy_score(expected, predicted))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I tried using Random Search Classifier too, but it took too long a time to execute, which is not desirable. And, it is known that Linear model of Support Vector Machines tend to do well with the text-based models, so we will now move forward and try tuning the hyperparamerters for this model.**"},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfinal_model = LinearSVC()\nparameters = [{'C': [1, 10, 100, 1000]}]\nfinal_tuned_model = GridSearchCV(final_model, parameters, cv = 2, n_jobs = 5, verbose=True)\n\nfinal_tuned_model.fit(train_X, train_y)\nvalidation_X = tfidf_vec.transform(validation_X_)\npredicted = final_tuned_model.predict(validation_X)\nexpected = validation_y\nprint(metrics.classification_report(expected, predicted))\nprint(\"Accuracy Score in Cross-Validation Set : \", metrics.accuracy_score(expected, predicted))\n\nprint(final_tuned_model.best_score_)\nprint(final_tuned_model.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Model on Test Set and Submission\n\n**Now, since we obtained the best tuned model with LinearSVC, now we apply it on test set, and make a submission.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_test_data = test.copy()\nstart_time = time.time()\nclean_test_data['Phrase'] = Parallel(n_jobs=4)(delayed(clean_review)(review) for review in test['Phrase'])\nend_time = time.time()\nprint(\"Cleaning Testing Data - Processing time = \", end_time - start_time)\n\n# Removing missing values\nprint(\"Clean entries: \", clean_test_data.shape[0], \" out of \", test.shape[0])\ntest_X = tfidf_vec.transform(clean_test_data['Phrase'])\n\ntest_pred = final_tuned_model.predict(test_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_file = pd.read_csv('../input/sampleSubmission.csv',sep=',')\nsub_file.Sentiment=test_pred\nsub_file.to_csv('final_submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We reached an accuracy score of 82.51% over 79.99% after tuning the hyperparameters, and we finally submitted our results on reviews provided in test set.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}