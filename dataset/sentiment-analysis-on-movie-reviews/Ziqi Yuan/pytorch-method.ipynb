{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Movie Review Sentiment Analysis with pytorch\n\nIn this notebook, we are going to use both traditional RNN using LSTM, a Faster RNN method and TextCNN as well."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport random\nimport time\nimport string\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport torch\nimport torch.nn as nn\nfrom torchtext import data\nimport torch.optim as optim\nfrom torchtext.vocab import Vectors\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def load_data(file_path, device):\n    tokenizer = lambda x: str(x).translate(str.maketrans('', '', string.punctuation)).strip().split()\n    TEXT = data.Field(sequential=True, lower=True, include_lengths=True, tokenize=tokenizer)\n    LABEL = data.Field(sequential=False, use_vocab=False)\n    \n    trn_dataField = [('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT), ('Sentiment', LABEL)]\n    tst_dataField = [('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT)]\n    \n    train = data.TabularDataset(path=os.path.join(file_path, 'train.tsv'), format='tsv', skip_header=True, fields=trn_dataField)\n    test = data.TabularDataset(path=os.path.join(file_path, 'test.tsv'), format='tsv', skip_header=True, fields=tst_dataField)\n    \n    train, valid = train.split(random_state=random.seed(1234))\n    cache = ('/kaggle/working/.vector_cache')\n    if not os.path.exists(cache):\n        os.mkdir(cache)\n    # using the pretrained word embedding.\n    vector = Vectors(name='/kaggle/input/glove6b100dtxt/glove.6B.100d.txt', cache=cache)\n    TEXT.build_vocab(train, vectors=vector, unk_init=torch.Tensor.normal_)\n    \n    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train, valid, test), device=device, batch_size=64, \n                                                       sort_key=lambda x:len(x.Phrase), sort_within_batch=True)\n    return TEXT, LABEL, train_iter, valid_iter, test_iter\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nTEXT, LABEL, train_iter, valid_iter, test_iter = load_data('/kaggle/input/sentiment-analysis-on-movie-reviews', device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Our Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SentimentModel(nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=pad_idx)\n        \n        self.rnn = nn.LSTM(embedding_size, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n        \n        if bidirectional:\n            self.fc = nn.Linear(2 * hidden_dim, output_dim)\n        else:\n            self.fc = nn.Linear(hidden_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, text, lengths):\n        \n        embedded = self.embedding(text)   #embedded : [sen_len, batch_size, emb_dim]\n        \n        packed_embedded = pack_padded_sequence(embedded, lengths)\n        \n        # packed_output : [num_word, emb_dim]     hidden : [num_layers * num_direction, batch_size, hid_dim]    \n        # cell : [num_layers * num_direction, batch_size, hid_dim]\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        \n        output, output_length = pad_packed_sequence(packed_output)\n        \n        hidden = self.dropout(torch.cat([hidden[-2,:,:], hidden[-1,:,:]], dim=1)).squeeze()\n         # hidden : [batch_size, hid_dim * num_dir]\n        return self.fc(hidden)\n    \n    \nINPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = 5\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.5\nPAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n\nmodel = SentimentModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_embeddings = TEXT.vocab.vectors\n\nprint(pretrained_embeddings.shape)\n\nmodel.embedding.weight.data.copy_(pretrained_embeddings)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Model\n\ndefinitions of optimizer and loss function. "},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer = optim.Adam(model.parameters())\n\ncriterion = nn.CrossEntropyLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = torch.Tensor([[1,1,2,1], [1,2,3,4]])\nprint(a.shape)\nb = torch.softmax(a, dim=1)\nprint(torch.argmax(b, dim=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(preds, y):\n    '''\n    Return accuracy per batch ..\n    '''\n    preds = torch.argmax(torch.softmax(preds, dim=1), dim=1)\n    correct = (preds == y).float()\n    acc = correct.sum() / len(correct)\n    return acc\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time  / 60)\n    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n    return  elapsed_mins, elapsed_secs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, iterator, optimizer, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for i, batch in enumerate(iterator):\n        \n        text, text_lengths = batch.Phrase\n        \n        if(torch.min(text_lengths) <= 0): \n            continue\n\n        predictions = model(text, text_lengths)\n        \n        loss = criterion(predictions, batch.Sentiment)\n        \n        acc = accuracy(predictions, batch.Sentiment)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n        if i % 100 == 99:\n            print(f\"[{i}/{len(iterator)}] : epoch_acc: {epoch_acc / len(iterator):.2f}\")\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            \n            text, text_lengths = batch.Phrase\n            \n            if(torch.min(text_lengths) <= 0): \n#                 continue\n                predictions = torch.Tensor([[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],\n                                           [0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0],[0,0,1,0,0]]).to(device)\n            else:\n                predictions = model(text, text_lengths)\n            \n            loss = criterion(predictions, batch.Sentiment)\n        \n            acc = accuracy(predictions, batch.Sentiment)\n            \n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n            \n    return epoch_loss / len(iterator),  epoch_acc / len(iterator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"N_epoches = 10\n\ntrainLossRecords = []\nvalidLossRecords = []\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_epoches):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n    # get the loss records to visualize.\n    trainLossRecords.append(train_loss)\n    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n    validLossRecords.append(valid_loss)\n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if train_loss < best_valid_loss:\n        best_valid_loss = train_loss\n        torch.save(model.state_dict(), 'Sentiment-model.pt')\n        \n    print(f'Epoch:  {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain  Loss: {train_loss: .3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\tValid  Loss: {valid_loss: .3f} | Valid Acc: {valid_acc*100:.2f}%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%pylab inline\nepoches = np.arange(1, N_epoches + 1, 1)\nplt.figure(figsize=(10, 10))\nplt.title('Train & Valid Loss')\nplt.xlabel(r'Epoch')\nplt.ylabel(r'Loss')\nplt.plot(epoches, trainLossRecords, 'r.', label='Train Loss')\nplt.plot(epoches, validLossRecords, 'b.', label='Valid loss')\nplt.grid()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Submission():\n    prediction = torch.Tensor([]).to(device)\n    \n    # load our best model parameters.\n    best_model = SentimentModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n    best_model.load_state_dict(torch.load('Sentiment-model.pt'))\n    best_model.eval()\n    best_model.to(device)\n    \n    # get the prediction \n    for i, batch in enumerate(test_iter):\n        text, text_lengths = batch.Phrase\n        if(torch.min(text_lengths) <= 0): \n            batch_predict = torch.Tensor([2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,\n                                         2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,\n                                         2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,\n                                         2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,]).to(device)\n        else:\n            batch_predict = best_model(text, text_lengths)\n            batch_predict = torch.argmax(torch.softmax(batch_predict, dim=1), dim=1)\n        prediction = torch.cat([prediction, batch_predict.float()], dim=0)\n    print(prediction[10000:10100])\n    # submission our results.\n    sub_file = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv',sep=',')\n    sub_file.Sentiment=prediction.cpu().numpy().astype(int).tolist()\n\n    sub_file.to_csv('Submission.csv', index=False)\n\nSubmission()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}