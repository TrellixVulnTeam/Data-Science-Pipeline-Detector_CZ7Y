{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n## Sentiment analysis with Sentence Transformers\n\nSentence Transformers: Sentence Embeddings using BERT / RoBERTa / DistilBERT / ALBERT / XLNet with PyTorch\nhttps://github.com/UKPLab/sentence-transformers\n\n\nIdea from this notebook \nhttps://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta\n\nMaximilien Roberti trains BERT-like transformers and get **0.7** score in Sentiment analisys competition.\n\nHere I use pre-trained models to get sentence embeddings and feed them into simple classifier.\nMy result is **0.67177** - not so bad and much faster.\n\nUpdate:\nPlus two features - num words in sentence and num chars - **0.67569**. It would be 10th result 6 years ago\n\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Libraries and external models.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -U sentence-transformers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\n#there are about 10 pretrained models\n#roberta-large-nli-stsb-mean-tokens - returns 1024 dimentional vector\n#distilbert-base-nli-stsb-mean-tokens - returns 768 dimentional vector\n\nPRETRAINED_MODEL='roberta-large-nli-stsb-mean-tokens'    # 'distilbert-base-nli-stsb-mean-tokens'        \nmodel = SentenceTransformer(PRETRAINED_MODEL)  \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path \nfrom sklearn import preprocessing\nimport os\nfrom timeit import default_timer\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport regex as re\nimport lightgbm as lgbm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sentiment Analysis on Movie Reviews\nhttps://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview\n\nThe sentiment labels are:\n\n0 - negative\n1 - somewhat negative\n2 - neutral\n3 - somewhat positive\n4 - positive\n\n\nRecords contains parts of reviews, some of them just one letter 'A'.\n**We will ignore fields PhraseId, SentenceId so result will be far from 100%**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"DATA_ROOT = Path(\"..\") / \"/kaggle/input/sentiment-analysis-on-movie-reviews\"\ntrain = pd.read_csv(DATA_ROOT / 'train.tsv.zip', sep=\"\\t\")\ntest = pd.read_csv(DATA_ROOT / 'test.tsv.zip', sep=\"\\t\")\nprint(train.shape,test.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#add two simple features - number of chars and words\ndef add_features (df):\n    df['nwords'] = df.Phrase.apply(lambda text: len(re.findall(r'\\w+', text)))\n    df['nchars'] = df.Phrase.apply(lambda text: len(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"add_features (test)\nadd_features (train)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Model was loaded. No need to format or tokenize text. All is done inside.\n\n**Should be done on GPU**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Do it with GPU !!!!!\n#on CPU 80 times slower\n\nTRANSFORMER_BATCH=128\n\ndef count_embedd (df):\n    idx_chunk=list(df.columns).index('Phrase')\n    embedd_lst = []\n    for index in range (0, df.shape[0], TRANSFORMER_BATCH):\n        embedds = model.encode(df.iloc[index:index+TRANSFORMER_BATCH, idx_chunk].values, show_progress_bar=False)\n        embedd_lst.append(embedds)\n    return np.concatenate(embedd_lst)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sentence embeddings for TRAIN dataset, 1024 dimentions each\nstart_time = default_timer()\ntrain_embedd = count_embedd(train)\nprint(\"Train embeddings: {}: in: {:5.2f}s\".format(train_embedd.shape, default_timer() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sentence embeddings for TEST dataset, 1024 dimentions each\nstart_time = default_timer()\ntest_embedd = count_embedd(test)\nprint(\"Test embeddings: {}: in: {:5.2f}s\".format(test_embedd.shape, default_timer() - start_time))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = np.array(train_embedd)\nX_test = np.array(test_embedd)\nX_train = np.concatenate((X_train, train[['nchars','nwords']].values), axis=1)\nX_test = np.concatenate((X_test, test[['nchars','nwords']].values), axis=1)\n\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert labels into 5-dimentional vector\n\nenc = preprocessing.OneHotEncoder()\nlabel = train['Sentiment'].values.reshape ((-1,1))\nenc.fit(label)\ny_train = enc.transform(label).toarray()\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train\n\nWe can use any kind of classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nKERAS_VALIDATION_SPLIT=0.05\nKERAS_EPOCHS=10\nKERAS_BATCH_SIZE=128\n\n# Create and train Keras model\nn_features=X_train.shape[1]\nn_labels = y_train.shape[1]\n\nstart_time=default_timer()\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(2048, input_dim=n_features, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(n_labels, activation='softmax')\n])\n\nLR=0.0001\nadam = keras.optimizers.Adam(learning_rate=LR, beta_1=0.9, beta_2=0.999, amsgrad=False)\n\nmodel.compile(optimizer=adam, \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, epochs=KERAS_EPOCHS, batch_size=KERAS_BATCH_SIZE, validation_split=KERAS_VALIDATION_SPLIT)\n\nprint(\"Training. Dataset size: {} {:5.2f}s\".format(X_train.shape, default_timer() - start_time))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_preds = model.predict(X_test)\ny_preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv(DATA_ROOT / 'sampleSubmission.csv')\nsample_submission['Sentiment'] = np.argmax(y_preds,axis=1)\nsample_submission.to_csv(\"predictions.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LightGBM\n\nonly **0.64979** scores","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n    'objective': 'multiclass',\n    'num_class':y_train.shape[1]\n    #'metric': 'multi_logloss'\n}\nlgbm_model = lgbm.LGBMClassifier(objective='multiclass')\nlgbm_model.fit(X_train, label)\n\ny_preds = lgbm_model.predict_proba(X_test)\n\nsample_submission = pd.read_csv(DATA_ROOT / 'sampleSubmission.csv')\nsample_submission['Sentiment'] = np.argmax(y_preds,axis=1)\nsample_submission.to_csv(\"predictions_lgbm.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compare records in sampleSubmission.csv and results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import HTML\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}</a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='predictions_lgbm.csv')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}