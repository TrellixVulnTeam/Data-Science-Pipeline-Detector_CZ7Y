{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Text classification using Deep Learning - Tensorflow v2 & Keras - Bi-directional GRU "},{"metadata":{},"cell_type":"markdown","source":"## Intro\nI've been playing with text classification problem of reviews dataset. First I've tried Bag of words methods: nltk, tokenize, lemmatize, remove stopwords, TF-IDF and then run it using Random Forest Classifier getting to **56%** accuracy on validation data. Not as good as I would expect so wanted to try deep nets what we can get out of here. My best result was **68%** accuracy on validation data and 65% accuracy on test data (submission)."},{"metadata":{},"cell_type":"markdown","source":"What I really loved here is that there was no need to preprocess and clean data (remove stop words, lemmatize, stem, ...) as using BOW method, however it still require to transform strings into integers/numbers with uniform dimension.**"},{"metadata":{},"cell_type":"markdown","source":"## Load libraries and data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Dense, GRU, Dropout, Bidirectional, SpatialDropout1D\nfrom tensorflow.keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip', sep='\\t', usecols=['Phrase', 'Sentiment'])\ndf_submission = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip', sep='\\t', usecols=['Phrase'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split data into training and testing set\nIt's good to mention we does not need to do this step, as we can ask tensorflow to put partition of data outside and use it as validation set"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df_train['Phrase'].values, df_train['Sentiment'].values, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data preparation/preprocessing\nSeveral simple steps has been performed in data preparation:\n* initialize tokenizer, calculate size of dataset\n* fit tokenizer to create mapping vocabulary (string/words -> integer)\n* calculate vocabulary size\n* convert words to integers (texts to sequences), as each sentence has different length, we must uniform sizes of arrays using padding with zeros\n* pad sequences to same length"},{"metadata":{"trusted":false},"cell_type":"code","source":"# initialize Tokenizer to encode strings into integers\ntokenizer = Tokenizer()\n\n# calculate number of rows in our dataset\nnum_rows = df_train.shape[0]\n\n# create vocabulary from all words in our dataset for encoding\ntokenizer.fit_on_texts(df_train['Phrase'].values)\n\n# max length of 1 row (number of words)\nrow_max_length = max([len(x.split()) for x in df_train['Phrase'].values])\n\n# count number of unique words\nvocabulary_size = len(tokenizer.word_index) + 1\n\n# convert words into integers\nX_train_tokens = tokenizer.texts_to_sequences(X_train)\nX_test_tokens = tokenizer.texts_to_sequences(X_test)\nX_sub_tokens = tokenizer.texts_to_sequences(df_submission['Phrase'].values)\n\n# ensure every row has same size - pad missing with zeros\nX_train_pad = pad_sequences(X_train_tokens, maxlen=row_max_length, padding='post')\nX_test_pad = pad_sequences(X_test_tokens, maxlen=row_max_length, padding='post')\nX_sub_pad = pad_sequences(X_sub_tokens, maxlen=row_max_length, padding='post')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Labels preprocessing\nIn tensorflow, if we deal with multinomial target, we must convert vector to matrix having as many columns as much targets we have. I.e. having target values 0-4, vector must be converted to matrix of 5 cols. It's actually same as one hot encoding."},{"metadata":{"trusted":false},"cell_type":"code","source":"y_train_cat = to_categorical(y_train)\ny_test_cat = to_categorical(y_test)\n\ntarget_length = y_train_cat.shape[1]\nprint('Original vector size: {}'.format(y_train.shape))\nprint('Converted vector size: {}'.format(y_train_cat.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling - train deep nets\n**Embedding** - same as dimensionality reduction. Can be 100,500 or even 1000. Basically if EMBEDDING_DIM == vocabulary_size, then it's identical to bag of words, but you cannot handle it if you have 300 000 of words, would be sparse matrix with 99.99% of values to be zero.\nInstead, embedding layer will be dense and have much smaller dimension.\n"},{"metadata":{},"cell_type":"markdown","source":"For modelling, we will use sequential model using spatial dropout, bidirectional GRU with 128 units and 2 dense layers. This combination gave me best results (Tested single-multi LSTM, GRU, Convolutional nets, ...)"},{"metadata":{"trusted":false},"cell_type":"code","source":"EMBEDDING_DIM = 256\n\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size, EMBEDDING_DIM, input_length=row_max_length))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Bidirectional(GRU(128)))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(target_length, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\nhistory = model.fit(X_train_pad, y_train_cat, epochs=5, validation_data=(X_test_pad, y_test_cat), batch_size=128, callbacks=[callback])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predict test data\nModel is completed and trained, it's time to predict our test data for submission and save it to CSV."},{"metadata":{"trusted":false},"cell_type":"code","source":"# predict test data\ny_sub_hat_ = model.predict(X_sub_pad)\ny_sub_hat = [np.argmax(x) for x in y_sub_hat_]\n\n# save to csv\ndf_save = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv')\ndf_save['Sentiment'] = y_sub_hat\ndf_save.to_csv('Submission.csv', index = False)\nprint('Submission saved!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conlusion\nMy solution use very simple model, however using more complex (using convolutional nets, double/triple gru, combination of bidirectional, more dense layers, different dropout, using LSTM, increasing units & embedding layer size had actually minimum effect on final results."},{"metadata":{},"cell_type":"markdown","source":"If you got idea how to improve score (using different model, different way of preprocessing), feel free to **leave comment**!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}