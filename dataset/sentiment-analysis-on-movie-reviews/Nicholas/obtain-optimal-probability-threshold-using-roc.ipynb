{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Obtain Optimal Probability Threshold Using ROC Curve\n\nThe aim of this notebook is to demonstrate how to use the ROC curve to obtain optimal probability threshold to improve the predictive capability of a machine learning model.\n\nAlso, **thresholder**, a Python package that I have created, which uses the ROC Curve to obtain optimal probability threshold to improve the predictive capability of a machine learning model, can obtain these thresholds and subsequent predictions easily.\n\nThe package can be found at the following Github Repo:\n\nhttps://github.com/nicholaslaw/roc-optimal-cutoff"},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\n1. [Import Packages](#1)\n2. [Import Data](#2)\n3. [Extracting X, Y](#3)\n4. [Preprocess Texts](#4)\n5. [Train Test Split](#5)\n6. [Feature Extraction and Train Model](#6)\n7. [Evaluate Model (Before Thresholding)](#7)\n8. [Confusion Matrix of Model (Before Thresholding)](#8)\n9. [ROC Curve](#9)\n10. [Obtain Optimal Probability Thresholds with ROC Curve](#10)\n11. [Evaluate Model (After Thresholding)](#11)\n12. [Confusion Matrix of Model (After Thresholding)](#12)\n13. [Conclusion](#13)\n14. [References](#14)"},{"metadata":{},"cell_type":"markdown","source":"## Import Packages <a class=\"anchor\" id=\"1\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\nimport nltk\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Import Data <a class=\"anchor\" id=\"2\"></a>\n\nWith respect to the dataset, we will only focus on the columns named \"Phrase\" and \"Sentiment\". The unique values in \"Sentiment\" are 0, 1, 2, 3, 4, where increasing values would represent more positive sentiment. Hence, this would make this a binary classification problem.\n\n**For this notebook, we shall just focus on the training set as the main objective is to showcase the retrieval of optimal probability thresholds using the ROC Curve.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip\", sep=\"\\t\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique Values of Sentiment are: {}\".format(\", \".join(list(map(str,df[\"Sentiment\"].unique())))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting X, Y <a class=\"anchor\" id=\"3\"></a>\n\nFor this dataset, we shall let sentiment values above 2 represent positive ones. As a result, positive movie reviews make up less than 50% of the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[\"Phrase\"].tolist()\nY = df[\"Sentiment\"].apply(lambda i: 0 if i <= 2 else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess Texts <a class=\"anchor\" id=\"4\"></a>\n\nSome typical text preprocessing steps will be performed:\n\n1. Removal of markup, html\n2. Obtain only words in lower case\n3. Lemmatization\n4. Removal of stop words"},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ndef proc_text(messy): #input is a single string\n    first = BeautifulSoup(messy, \"lxml\").get_text() #gets text without tags or markup, remove html\n    second = re.sub(\"[^a-zA-Z]\",\" \",first) #obtain only letters\n    third = second.lower().split() #obtains a list of words in lower case\n    fourth = set([lemmatizer.lemmatize(str(x)) for x in third]) #lemmatizing\n    stops = set(stopwords.words(\"english\")) #faster to search through a set than a list\n    almost = [w for w in fourth if not w in stops] #remove stop words\n    final = \" \".join(almost)\n    return final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = [proc_text(i) for i in X]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split <a class=\"anchor\" id=\"5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=100, test_size=0.2, stratify=Y)\nprint(\"Training Set has {} Positive Labels and {} Negative Labels\".format(sum(y_train), len(y_train) - sum(y_train)))\nprint(\"Test Set has {} Positive Labels and {} Negative Labels\".format(sum(y_test), len(y_test) - sum(y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction and Train Model <a class=\"anchor\" id=\"6\"></a>\n\nFeatures will be built using tfidf.\n\nModel selected here is the RandomForestClassifier, larger weight is given to the positive class since the number of samples with positive labels are significantly smaller. The weights would be calculated as \n\n$$ W_p = \\frac{N_n}{N_p}, $$\n\nwhere $ W_p $ is a float indicating the weight for positive class, $ N_n $ is the number of negative samples and $ N_p $ is the number of positive samples. The output of this computation will be included in the *class_weight* parameter of DecisionTreeClassifier.\n\nThese steps will be collated by using sklearn's Pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_weights = (len(y_train) - sum(y_train)) / (sum(y_train)) \npipeline_tf = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('classifier', DecisionTreeClassifier(random_state=100, class_weight={0: 1, 1: pos_weights}))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline_tf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate Model (Before Thresholding) <a class=\"anchor\" id=\"7\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pipeline_tf.predict(X_test)\npredicted_proba = pipeline_tf.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy Score Before Thresholding: {}\".format(accuracy_score(y_test, predictions)))\nprint(\"Precision Score Before Thresholding: {}\".format(precision_score(y_test, predictions)))\nprint(\"Recall Score Before Thresholding: {}\".format(recall_score(y_test, predictions)))\nprint(\"F1 Score Before Thresholding: {}\".format(f1_score(y_test, predictions)))\nprint(\"ROC AUC Score: {}\".format(roc_auc_score(y_test, predicted_proba[:, -1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix of Model (Before Thresholding) <a class=\"anchor\" id=\"8\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_actual = pd.Series(y_test, name='Actual')\ny_predict_tf = pd.Series(predictions, name='Predicted')\ndf_confusion = pd.crosstab(y_actual, y_predict_tf, rownames=['Actual'], colnames=['Predicted'], margins=True)\nprint(df_confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ROC Curve <a class=\"anchor\" id=\"9\"></a>\n\nThe curve is plots values of true positive rates (y-axis) against those of false positive rates (x-axis) and these values are plotted at various probability thresholds.\n\nThere can be two ways of obtaining a more optimal probability threshold for the positive class:\n\n1. Youden's J Statistic\n    - Its computed as $$ J = True Positive Rate + True Negative Rate - 1 = True Positive Rate - False Positive Rate $$\n    - Find the maximum difference between true positive rate and false positive rate and the probability threshold tied tagged to this largest difference would be the selected one\n2. Euclidean Distance\n    - The most optimal ROC curve would be one that leans towards the top left of the plot, i.e. true positive rate of 1 and false positive rate of 0.\n    - Select the probability threshold as the most optimal one if its true positive rate and false positive rate are closest to the ones mentioned in the previous point in terms of Euclidean distance, i.e. $$ d(tpr, fpr) = \\sqrt{({tpr_1 - tpr_2})^{2} + {fpr_1 - fpr_2})^{2}}. $$"},{"metadata":{"trusted":true},"cell_type":"code","source":"false_pos_rate, true_pos_rate, proba = roc_curve(y_test, predicted_proba[:, -1])\nplt.figure()\nplt.plot([0,1], [0,1], linestyle=\"--\") # plot random curve\nplt.plot(false_pos_rate, true_pos_rate, marker=\".\", label=f\"AUC = {roc_auc_score(y_test, predicted_proba[:, -1])}\")\nplt.title(\"ROC Curve\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.legend(loc=\"lower right\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Obtain Optimal Probability Thresholds with ROC Curve <a class=\"anchor\" id=\"10\"></a>\n\nIn this notebook, we will be using the Youden's J statistic to obtain the optimal probability threshold and this method gives equal weights to both false positives and false negatives."},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_proba_cutoff = sorted(list(zip(np.abs(true_pos_rate - false_pos_rate), proba)), key=lambda i: i[0], reverse=True)[0][1]\nroc_predictions = [1 if i >= optimal_proba_cutoff else 0 for i in predicted_proba[:, -1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate Model (After Thresholding) <a class=\"anchor\" id=\"11\"></a>\n\nWe can see that the optimal probability threshold managed to suppress the number of false positives"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy Score Before and After Thresholding: {}, {}\".format(accuracy_score(y_test, predictions), accuracy_score(y_test, roc_predictions)))\nprint(\"Precision Score Before and After Thresholding: {}, {}\".format(precision_score(y_test, predictions), precision_score(y_test, roc_predictions)))\nprint(\"Recall Score Before and After Thresholding: {}, {}\".format(recall_score(y_test, predictions), recall_score(y_test, roc_predictions)))\nprint(\"F1 Score Before and After Thresholding: {}, {}\".format(f1_score(y_test, predictions), f1_score(y_test, roc_predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix of Model (After Thresholding) <a class=\"anchor\" id=\"12\"></a>\n\nWe can see that the new predictions have fewer false positives but more false negatives in the process. F1 score and accuracy have improved."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_actual = pd.Series(y_test, name='Actual')\ny_predict_tf = pd.Series(roc_predictions, name='Predicted')\ndf_confusion = pd.crosstab(y_actual, y_predict_tf, rownames=['Actual'], colnames=['Predicted'], margins=True)\nprint (df_confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion <a class=\"anchor\" id=\"13\"></a>\n\nObtaining optimal probability thresholds using the ROC curves is one way of maximizing the predictive capability of your machine learning model. There are a few ways of obtaining these thresholds and they have different assumptions, so please choose wisely. \n\nAgain, please take a look at the Python package, *thresholder*, I have created which could make the generation of predictions based on the optimal thresholds obtained by the ROC curve convenient and easy. "},{"metadata":{},"cell_type":"markdown","source":"## References <a class=\"anchor\" id=\"14\"></a>\n\n- https://github.com/nicholaslaw/roc-optimal-cutoff\n- https://en.wikipedia.org/wiki/Youden%27s_J_statistic"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}