{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Obtain Optimal Probability Threshold Using PR Curve\n\nThe aim of this notebook is to demonstrate how to use the Precision Recall (PR) curve to obtain optimal probability threshold to improve the predictive capability of a machine learning model.\n\nThis notebook is similar to https://www.kaggle.com/nicholasgah/obtain-optimal-probability-threshold-using-roc, just that now, the curve used is that of precision-recall (PR)."},{"metadata":{},"cell_type":"markdown","source":"## Table of Contents\n\n1. [Import Packages](#1)\n2. [Import Data](#2)\n3. [Extracting X, Y](#3)\n4. [Preprocess Texts](#4)\n5. [Train Test Split](#5)\n6. [Feature Extraction and Train Model](#6)\n7. [Evaluate Model (Before Thresholding)](#7)\n8. [Confusion Matrix of Model (Before Thresholding)](#8)\n9. [PR Curve](#9)\n10. [Obtain Optimal Probability Thresholds with PR Curve](#10)\n11. [Evaluate Model (After Thresholding)](#11)\n12. [Confusion Matrix of Model (After Thresholding)](#12)\n13. [Conclusion](#13)\n14. [References](#14)"},{"metadata":{},"cell_type":"markdown","source":"## Import Packages <a class=\"anchor\" id=\"1\"></a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np, pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, plot_precision_recall_curve\nimport nltk\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import Data <a class=\"anchor\" id=\"2\"></a>\n\nWith respect to the dataset, we will only focus on the columns named \"Phrase\" and \"Sentiment\". The unique values in \"Sentiment\" are 0, 1, 2, 3, 4, where increasing values would represent more positive sentiment. Hence, this would make this a binary classification problem.\n\n**For this notebook, we shall just focus on the training set as the main objective is to showcase the retrieval of optimal probability thresholds using the PR Curve.**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip\", sep=\"\\t\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Unique Values of Sentiment are: {}\".format(\", \".join(list(map(str,df[\"Sentiment\"].unique())))))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extracting X, Y <a class=\"anchor\" id=\"3\"></a>\n\nFor this dataset, we shall let sentiment values above 2 represent positive ones. As a result, positive movie reviews make up less than 50% of the dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df[\"Phrase\"].tolist()\nY = df[\"Sentiment\"].apply(lambda i: 0 if i <= 2 else 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocess Texts <a class=\"anchor\" id=\"4\"></a>\n\nSome typical text preprocessing steps will be performed:\n\n1. Removal of markup, html\n2. Obtain only words in lower case\n3. Lemmatization\n4. Removal of stop words"},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ndef proc_text(messy): #input is a single string\n    first = BeautifulSoup(messy, \"lxml\").get_text() #gets text without tags or markup, remove html\n    second = re.sub(\"[^a-zA-Z]\",\" \",first) #obtain only letters\n    third = second.lower().split() #obtains a list of words in lower case\n    fourth = set([lemmatizer.lemmatize(str(x)) for x in third]) #lemmatizing\n    stops = set(stopwords.words(\"english\")) #faster to search through a set than a list\n    almost = [w for w in fourth if not w in stops] #remove stop words\n    final = \" \".join(almost)\n    return final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = [proc_text(i) for i in X]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train Test Split <a class=\"anchor\" id=\"5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=100, test_size=0.2, stratify=Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training Set has {} Positive Labels and {} Negative Labels\".format(sum(y_train), len(y_train) - sum(y_train)))\nprint(\"Test Set has {} Positive Labels and {} Negative Labels\".format(sum(y_test), len(y_test) - sum(y_test)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Extraction and Train Model <a class=\"anchor\" id=\"6\"></a>\n\nFeatures will be built using tfidf.\n\nModel selected here is the RandomForestClassifier, larger weight is given to the positive class since the number of samples with positive labels are significantly smaller. The weights would be calculated as \n\n$$ W_p = \\frac{N_n}{N_p}, $$\n\nwhere $ W_p $ is a float indicating the weight for positive class, $ N_n $ is the number of negative samples and $ N_p $ is the number of positive samples. The output of this computation will be included in the *class_weight* parameter of RandomForestClassifier.\n\nThese steps will be collated by using sklearn's Pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"pos_weights = (len(y_train) - sum(y_train)) / (sum(y_train)) \npipeline_tf = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('classifier', DecisionTreeClassifier(random_state=100, class_weight={0: 1, 1: pos_weights}))\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipeline_tf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate Model (Before Thresholding) <a class=\"anchor\" id=\"7\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pipeline_tf.predict(X_test)\npredicted_proba = pipeline_tf.predict_proba(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy Score Before Thresholding: {}\".format(accuracy_score(y_test, predictions)))\nprint(\"Precision Score Before Thresholding: {}\".format(precision_score(y_test, predictions)))\nprint(\"Recall Score Before Thresholding: {}\".format(recall_score(y_test, predictions)))\nprint(\"F1 Score Before Thresholding: {}\".format(f1_score(y_test, predictions)))\nprint(\"ROC AUC Score: {}\".format(roc_auc_score(y_test, predicted_proba[:, -1])))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix of Model (Before Thresholding) <a class=\"anchor\" id=\"8\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_actual = pd.Series(y_test, name='Actual')\ny_predict_tf = pd.Series(predictions, name='Predicted')\ndf_confusion = pd.crosstab(y_actual, y_predict_tf, rownames=['Actual'], colnames=['Predicted'], margins=True)\nprint(df_confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## PR Curve <a class=\"anchor\" id=\"9\"></a>\n\nThe curve is plots values of precision scores (y-axis) against those of recall scores (x-axis) and these values are plotted at various probability thresholds.\n\nThere can be two ways of obtaining a more optimal probability threshold for the positive class:\n\n1. Minimize the difference between precision and recall scores\n    - Select the probability threshold of which precision and recall scores are closest to each other\n   \n2. Euclidean Distance\n    - The most optimal point on the PR curve should be (1,1), i.e. precision and recall scores of 1.\n    - Select the probability threshold as the most optimal one if precision and recall scores are closest fo the ones mentioned in the previous point in terms of Euclidean distance, i.e. $$ d(recall, precision) = \\sqrt{({recall_1 - recall_2})^{2} + {precision_1 - precision_2})^{2}}. $$"},{"metadata":{"trusted":true},"cell_type":"code","source":"precision_, recall_, proba = precision_recall_curve(y_test, predicted_proba[:, -1])\n\ndisp = plot_precision_recall_curve(pipeline_tf, X_test, y_test)\ndisp.ax_.set_title('Precision-Recall curve')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Obtain Optimal Probability Thresholds with PR Curve <a class=\"anchor\" id=\"10\"></a>\n\nIn this notebook, we will be obtaining the optimal probability threshold based on minimizing the distance between precision and recall scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"optimal_proba_cutoff = sorted(list(zip(np.abs(precision_ - recall_), proba)), key=lambda i: i[0], reverse=False)[0][1]\nroc_predictions = [1 if i >= optimal_proba_cutoff else 0 for i in predicted_proba[:, -1]]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluate Model (After Thresholding) <a class=\"anchor\" id=\"11\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Accuracy Score Before and After Thresholding: {}, {}\".format(accuracy_score(y_test, predictions), accuracy_score(y_test, roc_predictions)))\nprint(\"Precision Score Before and After Thresholding: {}, {}\".format(precision_score(y_test, predictions), precision_score(y_test, roc_predictions)))\nprint(\"Recall Score Before and After Thresholding: {}, {}\".format(recall_score(y_test, predictions), recall_score(y_test, roc_predictions)))\nprint(\"F1 Score Before and After Thresholding: {}, {}\".format(f1_score(y_test, predictions), f1_score(y_test, roc_predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion Matrix of Model (After Thresholding) <a class=\"anchor\" id=\"12\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_actual = pd.Series(y_test, name='Actual')\ny_predict_tf = pd.Series(roc_predictions, name='Predicted')\ndf_confusion = pd.crosstab(y_actual, y_predict_tf, rownames=['Actual'], colnames=['Predicted'], margins=True)\nprint (df_confusion)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion <a class=\"anchor\" id=\"13\"></a>\n\nObtaining optimal probability thresholds using the PR curves is one way of maximizing the predictive capability of your machine learning model. There are a few ways of obtaining these thresholds and they do not necessarily have the same effects on performance. Of course, you can pick probability thresholds manually with the aim of maximizing either precision or recall, which depends on the problem you are trying to solve.\n\nExample problems which exemplify the need to maximize precision or recall are as follows:\n\n- **Minimize number of false positives, i.e. maximize precision**: You have a model which identifies spam and non-spam emails. This model should focus on reducing the number of falsely identified spam emails as this would increase the possibility of users missing out on important emails.\n\n- **Minimize number of false negatives, i.e. maximize recall**: You have a model which identifies cancer and non-cancer cases. This model should focus on reducing the number of false identified non-cancer cases since this would prevent concerned parties from seeking early treatment for cancer.\n\nAnother important point to note is that when obtaining optimal probability thresholds, precision-recall (PR) curves are normally preferred as compared to receiver operating characteristic (ROC) curves when dealing with datasets with **severe class imbalance**. PR curves focus more on the minority class whereas ROC curves attempts to place equal emphasis on both classes."},{"metadata":{},"cell_type":"markdown","source":"## References <a class=\"anchor\" id=\"14\"></a>\n\n- https://github.com/nicholaslaw/roc-optimal-cutoff\n- https://www.kaggle.com/nicholasgah/obtain-optimal-probability-threshold-using-roc\n- https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/\n- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html#sphx-glr-auto-examples-model-selection-plot-precision-recall-py"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}