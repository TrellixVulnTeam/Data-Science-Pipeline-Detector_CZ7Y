{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1>Movie Reviews Analysis</h1>\n\n<h3> Important Q&A</h3>\n\n<h4>1. What is the business question?</h4>\n<h5>What is the movie review phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive?</h5>\n\n<h4>2. What does each row represent?</h4>\n<h5>Each row repersents a phrase</h5>\n\n<h4>3. What is the evaluation method?</h4>\n<h5>What is the movie review phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive?</h5>\n","metadata":{}},{"cell_type":"markdown","source":"<h2>1. Imports</h2>","metadata":{}},{"cell_type":"code","source":"# load data libraries\nimport numpy as np # linear algebra library\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport zipfile # to read zip files\nfrom sklearn.model_selection import train_test_split\n\n\n# data understanding libraries\nimport matplotlib.pyplot as plt # ploting library\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom collections import Counter\nimport seaborn as sns\n\n\n\n# data preparation\nimport re\nfrom nltk.stem import PorterStemmer\n\n\n# ADS Creation\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import StandardScaler\n# Modeling\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Evaluation and Model Selection\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import GridSearchCV","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:12.029651Z","iopub.execute_input":"2021-05-20T23:38:12.030044Z","iopub.status.idle":"2021-05-20T23:38:12.046229Z","shell.execute_reply.started":"2021-05-20T23:38:12.03001Z","shell.execute_reply":"2021-05-20T23:38:12.045267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', 10000)\npd.set_option('display.max_columns', 500)\npd.set_option('display.precision',150)\npd.options.display.float_format = '{:,.3f}'.format","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:12.047565Z","iopub.execute_input":"2021-05-20T23:38:12.048003Z","iopub.status.idle":"2021-05-20T23:38:12.067958Z","shell.execute_reply.started":"2021-05-20T23:38:12.047971Z","shell.execute_reply":"2021-05-20T23:38:12.066908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:12.069799Z","iopub.execute_input":"2021-05-20T23:38:12.070241Z","iopub.status.idle":"2021-05-20T23:38:12.084358Z","shell.execute_reply.started":"2021-05-20T23:38:12.070193Z","shell.execute_reply":"2021-05-20T23:38:12.083532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>2. Load Data </h2>","metadata":{}},{"cell_type":"code","source":"#unzip the files\narchive_train = zipfile.ZipFile('../input/sentiment-analysis-on-movie-reviews/train.tsv.zip')\n\n#read training tsv file \ntrain = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/train.tsv.zip', delimiter='\\t')\ntrain.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:12.085826Z","iopub.execute_input":"2021-05-20T23:38:12.086299Z","iopub.status.idle":"2021-05-20T23:38:12.340442Z","shell.execute_reply.started":"2021-05-20T23:38:12.086259Z","shell.execute_reply":"2021-05-20T23:38:12.339672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>There are a total of 4 columns</h4>\n\n<h5>&emsp;PhraseId</h5>\n<h5>&emsp;SentenceId</h5>\n<h5>&emsp;Phrase</h5>\n\n<h5>&emsp;Sentiment:</h5> \n<h6>&emsp;&emsp;&emsp;&emsp;0 - negative<br>\n&emsp;&emsp;&emsp;&emsp;1 - somewhat negative<br>\n&emsp;&emsp;&emsp;&emsp;2 - neutral<br>\n&emsp;&emsp;&emsp;&emsp;3 - somewhat positive<br>\n&emsp;&emsp;&emsp;&emsp;4 - positive</h6>","metadata":{}},{"cell_type":"code","source":"#split data into train and test data\n#then split the test data into test and validation \ntrain_data, test_data = train_test_split(train, test_size=0.4, random_state=1)\nval_data, test_data = train_test_split(test_data, test_size=0.5, random_state=1)\n\n#resets index after splitting data\ntrain_data = train_data.reset_index(drop=True)\nval_data = val_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)\n\nprint(\"Train set size is \",len(train_data))\nprint(\"Val set size is \",len(val_data))\nprint(\"Test set size is \",len(test_data))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:12.341705Z","iopub.execute_input":"2021-05-20T23:38:12.342141Z","iopub.status.idle":"2021-05-20T23:38:12.408813Z","shell.execute_reply.started":"2021-05-20T23:38:12.342102Z","shell.execute_reply":"2021-05-20T23:38:12.408005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>2. Data Preparation</h3>","metadata":{}},{"cell_type":"code","source":"stopwords = set(['Laura', 'Pauline', 'BMW', 'Carol', 'Kane', 'Evelyn', 'Cahill',\n 'Adam',\n 'Garcia',\n 'X-Men',\n 'Glover',\n 'Disney',\n 'Antwone',\n 'Fisher',\n 'Gayton',\n 'Lyne',\n 'African',\n 'American',\n 'Godard',\n 'Dickens',\n 'Woody',\n 'Martin',\n 'Scorsese',\n 'New',\n 'York',\n 'Decasia',\n 'Limit',\n 'Hollywood',\n 'Michael',\n 'Moore',\n 'Robin',\n 'Williams',\n 'Rorschach',\n 'Watts',\n 'Broomfield',\n 'Washington',\n 'Argentine',\n 'Fabian',\n 'Bielinsky',\n 'Tunis',\n 'Bebe',\n 'Neuwirth',\n 'John',\n 'C.',\n 'Walsh',\n 'Ms.',\n 'Fontaine',\n 'Ritchie',\n 'Copmovieland',\n 'Egoyan',\n 'France',\n 'Carlen',\n 'Campanella',\n 'Norma',\n 'Rae',\n 'Bartlett',\n 'Chicago',\n 'Gere',\n 'Peter',\n 'Mattei',\n 'Famuyiwa',\n 'Tara',\n 'Reid',\n 'Hawaiian',\n 'Sendak',\n 'David',\n 'Cronenberg',\n 'Anne',\n 'Busby',\n 'Malkovich',\n 'Statham',\n 'Freudian',\n 'Cattaneo',\n 'Tim',\n 'Apted',\n 'Nicholas',\n 'Kazan',\n 'Kilmer',\n 'One',\n 'Jaglom',\n 'Twohy',\n 'Chinese',\n 'Jerry',\n 'Bruckheimer',\n 'Joel',\n 'Schumacher',\n 'Chris',\n 'Smith',\n 'Hawk',\n 'Katherine',\n 'Blake',\n 'Wilde',\n 'Clint',\n 'Eastwood',\n 'Miramax',\n 'Reno',\n 'Louiso',\n 'Ivory',\n 'Vietnam',\n 'Vietnamese',\n 'Shakespeare',\n 'America',\n 'Del',\n 'Toro',\n 'Hip-Hop',\n 'Scooby-Doo',\n 'Aan',\n 'Steve',\n 'Irwin',\n 'Taylor',\n 'Le',\n 'Jean-Luc',\n 'Pamela',\n 'Taymor',\n 'George',\n 'W.',\n 'Bush',\n 'Henry',\n 'Kissinger',\n 'Larry',\n 'King',\n 'Holly',\n 'Marina',\n 'Stanley',\n 'Kwan',\n 'Joseph',\n 'Heller',\n 'Kurt', \n 'Vonnegut',\n 'Titus',\n 'Minnie',\n 'Allen',\n 'Britney',\n 'Spears',\n 'Farrelly',\n 'Tsai',\n 'Rob',\n 'Schneider',\n 'Greenfingers',\n 'Harmon',\n 'Zhuangzhuang',\n 'Eric',\n 'Rohmer',\n 'Blanchett',\n 'Ribisi',\n 'Jacquot',\n 'Zaidan',\n 'Wiseman',\n 'Ian',\n 'Holm',\n 'Vincent',\n 'R.',\n 'Nebrida',\n 'Laurice',\n 'Guillen',\n 'De',\n 'Niro',\n 'Martha',\n 'Margaret',\n 'Thatcher',\n 'Assayas',\n 'Sara',\n 'Abdul',\n 'Amy',\n 'Neil',\n 'Dana',\n 'Sinise',\n 'Hartley',\n 'Bruce',\n 'Willis',\n 'Dean',\n 'Zelda',\n 'Tony',\n 'Jackal',\n 'Nelson',\n 'Harlem',\n 'Mick',\n 'Graham',\n 'Myer',\n 'Koepp',\n 'Wimmer',\n 'Robert',\n 'Travis',\n 'Bickle',\n 'Cockettes',\n 'Jez',\n 'Steven',\n 'Shainberg',\n 'Mary',\n 'Gaitskill',\n 'Reyes',\n 'Roberto',\n 'Benigni',\n 'Kaufman',\n 'Jonze',\n 'Ralph',\n 'Waltz',\n 'Roger',\n 'Kumble',\n 'Damon\\\\/Bourne',\n 'Charlie',\n 'Donald',\n 'Madonna'])\nporter = PorterStemmer()\n\ndef ret_words(Phrase):\n    Phrase = Phrase.lower()\n    Phrase = Phrase.replace('-', '')\n    Phrase = Phrase.replace(',', ' ')\n    Phrase = Phrase.replace(';', ' ')\n    Phrase = Phrase.replace('\\'', '')\n    Phrase = Phrase.replace('\\\\', '')\n    Phrase = Phrase.replace('/', '')\n    Phrase = Phrase.replace('.', '')\n    Phrase = Phrase.replace('...', '')\n    words = []\n    for word in Phrase.split():\n        if re.findall('[0-9]', word): continue\n        if word in stopwords: continue\n        if re.findall('[^a-zA-Z]',re.sub(r'[^\\w\\s]','',word)): continue\n        if len(word) > 0: words.append(porter.stem(re.sub(r'[^\\w\\s]','',word)))\n    return ' '.join(words)\n\ndef preprocess(df,flag):\n    # add column\n    df[\"words_num\"]=df['Phrase'].str.split().str.len()\n     # Remove recipes with only one Ingredient\n    if flag == 0 :\n        df = df.drop(df[df[\"words_num\"]<=1].index)\n    \n    # Convert list of ingredients to string\n    df['Phrase'] = df[\"Phrase\"].apply(ret_words)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:58.319561Z","iopub.execute_input":"2021-05-20T23:38:58.319964Z","iopub.status.idle":"2021-05-20T23:38:58.344487Z","shell.execute_reply.started":"2021-05-20T23:38:58.319929Z","shell.execute_reply":"2021-05-20T23:38:58.343097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_preprocessed = preprocess(train_data,0)\ntest_preprocessed = preprocess(test_data,1)\nval_preprocessed = preprocess(val_data,1)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:38:58.346132Z","iopub.execute_input":"2021-05-20T23:38:58.346618Z","iopub.status.idle":"2021-05-20T23:39:29.347942Z","shell.execute_reply.started":"2021-05-20T23:38:58.34657Z","shell.execute_reply":"2021-05-20T23:39:29.345511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_preprocessed.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.348971Z","iopub.status.idle":"2021-05-20T23:39:29.349623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>3. Data Modeling</h3>","metadata":{}},{"cell_type":"code","source":"id_train, X_train, y_train = train_preprocessed['PhraseId'], train_preprocessed['Phrase'], train_preprocessed['Sentiment']\nid_test, X_test, y_test = test_preprocessed['PhraseId'], test_preprocessed['Phrase'], test_preprocessed['Sentiment']\nid_val, X_val, y_val = val_preprocessed['PhraseId'], val_preprocessed['Phrase'], val_preprocessed['Sentiment']","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.350628Z","iopub.status.idle":"2021-05-20T23:39:29.351249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>3.1 BOW</h4>","metadata":{}},{"cell_type":"code","source":"LR_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', LogisticRegression(random_state=0, max_iter=2000))\n])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, LR_cnt_pred_tr))\nprint(precision_score(y_train, LR_cnt_pred_tr, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.352213Z","iopub.status.idle":"2021-05-20T23:39:29.352851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.353854Z","iopub.status.idle":"2021-05-20T23:39:29.354536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', LinearSVC(max_iter=3000))\n])\nSVM_clf_counts.fit(X_train, y_train)\nSVM_cnt_pred_tr = SVM_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, SVM_cnt_pred_tr))\nprint(precision_score(y_train, SVM_cnt_pred_tr, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.35558Z","iopub.status.idle":"2021-05-20T23:39:29.356254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(SVM_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.35725Z","iopub.status.idle":"2021-05-20T23:39:29.357882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', MultinomialNB())\n])\nNB_clf_counts.fit(X_train, y_train)\nNB_cnt_pred_tr = NB_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, NB_cnt_pred_tr))\nprint(precision_score(y_train, NB_cnt_pred_tr, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.358862Z","iopub.status.idle":"2021-05-20T23:39:29.359521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(NB_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.360541Z","iopub.status.idle":"2021-05-20T23:39:29.361152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>3.2 BOW</h4>","metadata":{}},{"cell_type":"code","source":"LR_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1',ngram_range=(1, 2), stop_words='english')),\n    ('clf', LogisticRegression(random_state=0, max_iter=2000))\n])\nLR_clf_tfidf.fit(X_train, y_train)\nLR_tfidf_pred_tr = LR_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, LR_tfidf_pred_tr))\nprint(precision_score(y_train, LR_tfidf_pred_tr, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.362126Z","iopub.status.idle":"2021-05-20T23:39:29.36275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.363768Z","iopub.status.idle":"2021-05-20T23:39:29.364423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SVM_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')),\n    ('clf', LinearSVC( max_iter=2000))\n])\nSVM_clf_tfidf.fit(X_train, y_train)\nSVM_tfidf_pred_tr = SVM_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, SVM_tfidf_pred_tr))\nprint(precision_score(y_train, SVM_tfidf_pred_tr, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.365379Z","iopub.status.idle":"2021-05-20T23:39:29.365988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(SVM_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.366961Z","iopub.status.idle":"2021-05-20T23:39:29.367538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NB_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')),\n    ('clf', MultinomialNB())\n])\nNB_clf_tfidf.fit(X_train, y_train)\nNB_tfidf_pred_tr = NB_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, NB_tfidf_pred_tr))\nprint(precision_score(y_train, NB_tfidf_pred_tr, average='weighted'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.368398Z","iopub.status.idle":"2021-05-20T23:39:29.368962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(NB_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.369815Z","iopub.status.idle":"2021-05-20T23:39:29.37039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Hyperparameter tuning</h4>","metadata":{}},{"cell_type":"code","source":"vect=  CountVectorizer()\nX_train_cnt = vect.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.371218Z","iopub.status.idle":"2021-05-20T23:39:29.371786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>Logistic Regression</h4>","metadata":{}},{"cell_type":"code","source":"def LR_param_selection(X, y, nfolds):\n    Cs = [0.01, 0.1, 1, 10]\n    param_grid = {'C': Cs}\n    grid_search = GridSearchCV(LogisticRegression(random_state=0,max_iter=2000), param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.37263Z","iopub.status.idle":"2021-05-20T23:39:29.373193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_param_selection( X_train_cnt,y_train,2)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.374025Z","iopub.status.idle":"2021-05-20T23:39:29.374592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_clf_counts = Pipeline([('vect', CountVectorizer()),\n                   ('clf', LogisticRegression(C=1,random_state=0, max_iter=2000)),\n                  ])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\nLR_cnt_pred_val = LR_clf_counts.predict(X_val)\n\nprint(\"accuracy on training: \",accuracy_score(y_train, LR_cnt_pred_tr))\nprint(\"precision on training: \",precision_score(y_train, LR_cnt_pred_tr, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, LR_cnt_pred_val, average='micro'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.37542Z","iopub.status.idle":"2021-05-20T23:39:29.375999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Final Model</h3>","metadata":{}},{"cell_type":"code","source":"LR_clf_counts = Pipeline([('vect', CountVectorizer()),\n                   ('clf', LogisticRegression(C=1,random_state=0, max_iter=2000)),\n                  ])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\nLR_cnt_pred_val = LR_clf_counts.predict(X_val)\nLR_cnt_pred_tst = LR_clf_counts.predict(X_test)\n\n\nprint(\"precision on training: \",precision_score(y_train, LR_cnt_pred_tr, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, LR_cnt_pred_val, average='micro'))\nprint(\"precision on testing: \",precision_score(y_test, LR_cnt_pred_tst, average='micro'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.376856Z","iopub.status.idle":"2021-05-20T23:39:29.377424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.378261Z","iopub.status.idle":"2021-05-20T23:39:29.378821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Logistic Regression</h3>","metadata":{}},{"cell_type":"code","source":"labels = train['Sentiment'].unique()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.37965Z","iopub.status.idle":"2021-05-20T23:39:29.380211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conf_mat = pd.DataFrame(confusion_matrix(y_val, LR_cnt_pred_val, labels=labels))\nconf_mat_n = conf_mat.divide(conf_mat.sum(axis=1), axis=0)\nfig, ax = plt.subplots(figsize=(20,12))\nsns.heatmap(conf_mat_n, annot=True,xticklabels=labels,yticklabels=labels)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.381055Z","iopub.status.idle":"2021-05-20T23:39:29.381648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Predicted_vals = pd.DataFrame({'id' : id_val, 'Sentiment': X_val, 'Actual': y_val, 'Preds': LR_cnt_pred_val})","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.382495Z","iopub.status.idle":"2021-05-20T23:39:29.383063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>Model Run:</h2>","metadata":{}},{"cell_type":"code","source":"#read\narchive_train = zipfile.ZipFile('../input/sentiment-analysis-on-movie-reviews/train.tsv.zip')\narchive_test = zipfile.ZipFile('../input/sentiment-analysis-on-movie-reviews/test.tsv.zip')\n\n\n#read training tsv file \nfinal_train = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/train.tsv.zip', delimiter='\\t')\nfinal_test = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/test.tsv.zip', delimiter='\\t')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.385355Z","iopub.status.idle":"2021-05-20T23:39:29.38592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepare\nftrain_preprocessed = preprocess(final_train,0)\nftest_preprocessed = preprocess(final_test,1)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.386747Z","iopub.status.idle":"2021-05-20T23:39:29.38731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_train, X_train, y_train = ftrain_preprocessed['PhraseId'], ftrain_preprocessed['Phrase'], ftrain_preprocessed['Sentiment']\nid_test, X_test, = ftest_preprocessed['PhraseId'], ftest_preprocessed['Phrase']","metadata":{"execution":{"iopub.status.busy":"2021-05-20T23:39:29.388152Z","iopub.status.idle":"2021-05-20T23:39:29.388724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#LR\nLR_clf = Pipeline([('vect', CountVectorizer()),\n                   ('clf', LogisticRegression(C=1,random_state=0, max_iter=2000)),\n                  ])\nLR_clf.fit(X_train, y_train)\npred_test = LR_clf_counts.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output=pd.DataFrame({'PhraseId':id_test, 'Sentiment':pred_test})\noutput.to_csv('Sentiment_preds_LR.csv',index= False)","metadata":{},"execution_count":null,"outputs":[]}]}