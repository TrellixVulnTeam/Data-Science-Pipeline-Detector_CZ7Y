{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-07-19T11:14:46.099487Z","iopub.execute_input":"2021-07-19T11:14:46.099948Z","iopub.status.idle":"2021-07-19T11:14:46.1142Z","shell.execute_reply.started":"2021-07-19T11:14:46.099902Z","shell.execute_reply":"2021-07-19T11:14:46.113217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nkeras = tf.keras\n\ntf.__version__","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:01.912087Z","iopub.execute_input":"2021-07-19T11:00:01.912479Z","iopub.status.idle":"2021-07-19T11:00:01.92393Z","shell.execute_reply.started":"2021-07-19T11:00:01.91244Z","shell.execute_reply":"2021-07-19T11:00:01.92297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/train.tsv.zip', sep = '\\t')\nprint(train_data.Phrase[0])\nprint(train_data.Phrase[1])\nprint(train_data.Phrase[2])\ntrain_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:01.926309Z","iopub.execute_input":"2021-07-19T11:00:01.926833Z","iopub.status.idle":"2021-07-19T11:00:02.118357Z","shell.execute_reply.started":"2021-07-19T11:00:01.926767Z","shell.execute_reply":"2021-07-19T11:00:02.117375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.Phrase[63])\ntrain_data[62:64+5]","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:02.120138Z","iopub.execute_input":"2021-07-19T11:00:02.120494Z","iopub.status.idle":"2021-07-19T11:00:02.133734Z","shell.execute_reply.started":"2021-07-19T11:00:02.120455Z","shell.execute_reply":"2021-07-19T11:00:02.132891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/test.tsv.zip', sep = '\\t')\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:02.135117Z","iopub.execute_input":"2021-07-19T11:00:02.13548Z","iopub.status.idle":"2021-07-19T11:00:02.218417Z","shell.execute_reply.started":"2021-07-19T11:00:02.135446Z","shell.execute_reply":"2021-07-19T11:00:02.217313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission_data = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv')\nsample_submission_data.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:02.2199Z","iopub.execute_input":"2021-07-19T11:00:02.220245Z","iopub.status.idle":"2021-07-19T11:00:02.241281Z","shell.execute_reply.started":"2021-07-19T11:00:02.22021Z","shell.execute_reply":"2021-07-19T11:00:02.240371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_size = int(len(train_data) * 0.8)\n\ntotal_phrases = train_data.Phrase.to_numpy()\ntrain_phrases = total_phrases[:split_size]\nvalid_phrases = total_phrases[split_size:]\n\ntotal_sentiments = train_data.Sentiment.to_numpy()\ntrain_sentiments = total_sentiments[:split_size]\nvalid_sentiments = total_sentiments[split_size:]","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:02.242557Z","iopub.execute_input":"2021-07-19T11:00:02.242925Z","iopub.status.idle":"2021-07-19T11:00:02.24817Z","shell.execute_reply.started":"2021-07-19T11:00:02.242881Z","shell.execute_reply":"2021-07-19T11:00:02.247005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((train_phrases, train_sentiments))\nvalid_ds = tf.data.Dataset.from_tensor_slices((valid_phrases, valid_sentiments))\ntotal_ds = tf.data.Dataset.from_tensor_slices((total_phrases, total_sentiments))\n\nfor phrase, sentiment in train_ds.take(4):\n    print(phrase, sentiment)\nprint()    \nfor phrase, sentiment in valid_ds.take(1):\n    print(phrase, sentiment)    ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:02.249601Z","iopub.execute_input":"2021-07-19T11:00:02.250151Z","iopub.status.idle":"2021-07-19T11:00:02.297496Z","shell.execute_reply.started":"2021-07-19T11:00:02.250113Z","shell.execute_reply":"2021-07-19T11:00:02.29667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_set = set()\nlines = []\nfor line, _ in train_ds:\n    line = line.numpy().decode('utf-8')\n    lines.append(line)\n    for w in line.split(' '):\n        word_set.add(w)\n\nprint(len(word_set))\nfor index, w in enumerate(word_set):\n    if index >= 10:\n        break\n    print(f'{index:3}: {w}')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:02.30044Z","iopub.execute_input":"2021-07-19T11:00:02.300724Z","iopub.status.idle":"2021-07-19T11:00:13.402443Z","shell.execute_reply.started":"2021-07-19T11:00:02.300694Z","shell.execute_reply":"2021-07-19T11:00:13.400655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nvocab_size = 5000\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = '<OOV>')\ntokenizer.fit_on_texts(lines)\nword_index = tokenizer.word_index\nfor index, (a, b) in enumerate(word_index.items()):\n    if index >= 5:\n        break\n    print(a, b)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:13.404496Z","iopub.execute_input":"2021-07-19T11:00:13.404892Z","iopub.status.idle":"2021-07-19T11:00:14.967586Z","shell.execute_reply.started":"2021-07-19T11:00:13.404829Z","shell.execute_reply":"2021-07-19T11:00:14.966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:14.969116Z","iopub.execute_input":"2021-07-19T11:00:14.96949Z","iopub.status.idle":"2021-07-19T11:00:14.979008Z","shell.execute_reply.started":"2021-07-19T11:00:14.969448Z","shell.execute_reply":"2021-07-19T11:00:14.977848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for line in lines:\n    print(line)\n    sequences = tokenizer.texts_to_sequences([line])[0]\n    print(sequences)\n    print([f'{id}: {reverse_word_index[id]}' for id in sequences])\n    break","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:14.980759Z","iopub.execute_input":"2021-07-19T11:00:14.981172Z","iopub.status.idle":"2021-07-19T11:00:14.99051Z","shell.execute_reply.started":"2021-07-19T11:00:14.981129Z","shell.execute_reply":"2021-07-19T11:00:14.989394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nsequences = tokenizer.texts_to_sequences(lines[:30])\nnp.max(list(map(len, sequences)))\n\nmaxlen = 50","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:14.992113Z","iopub.execute_input":"2021-07-19T11:00:14.992723Z","iopub.status.idle":"2021-07-19T11:00:15.00023Z","shell.execute_reply.started":"2021-07-19T11:00:14.992673Z","shell.execute_reply":"2021-07-19T11:00:14.999204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndef pad_f(sequences):\n    return pad_sequences(sequences, maxlen = maxlen, padding = 'post', truncating = 'post')","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:15.00206Z","iopub.execute_input":"2021-07-19T11:00:15.00241Z","iopub.status.idle":"2021-07-19T11:00:15.009508Z","shell.execute_reply.started":"2021-07-19T11:00:15.002368Z","shell.execute_reply":"2021-07-19T11:00:15.008209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\ndef tokenize_and_pad_sequence(text_batch):\n    texts = map(lambda t: t.numpy().decode('utf-8'), text_batch)\n    sequences = tokenizer.texts_to_sequences(texts)\n    return pad_f(sequences)\n@tf.function\ndef encode_text_batch(text_batch):\n    return tf.py_function(\n        func = tokenize_and_pad_sequence,\n        inp = [text_batch],\n        Tout = tf.int32,\n    )\n\ndef create_batch_ds_inner(ds):\n    ds = ds.batch(batch_size)\n    ds = ds.map(lambda text_batch, label_batch: (encode_text_batch(text_batch), label_batch))\n    return ds.cache()\n\ndef create_batch_ds(ds, do_shuffle = True):\n    ds = create_batch_ds_inner(ds)\n    if do_shuffle:\n        ds = ds.shuffle(100)\n    return ds.prefetch(tf.data.AUTOTUNE)\n\nfor text, _ in train_ds:\n    print(text)\n    break\n    \ntrain_batch_ds = create_batch_ds(train_ds)\nvalid_batch_ds = create_batch_ds(valid_ds, do_shuffle = False)\ntotal_batch_ds = create_batch_ds(total_ds, do_shuffle = False)\n\nfor text_batch, label_batch in create_batch_ds(train_ds, do_shuffle = False).take(1):\n    print(label_batch.shape)\n    print(text_batch.shape)\n    print(text_batch[0])\n    for index in text_batch[0]:\n        index = index.numpy()\n        if index > 0:\n            print(f'{index}: {reverse_word_index[index]}')\n    ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:15.011041Z","iopub.execute_input":"2021-07-19T11:00:15.011581Z","iopub.status.idle":"2021-07-19T11:00:15.144682Z","shell.execute_reply.started":"2021-07-19T11:00:15.01154Z","shell.execute_reply":"2021-07-19T11:00:15.143723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build and Train Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_item(history_df, colname = 'loss', f = np.min, ax = None):\n    val_colname = f'val_{colname}'\n    print(f'{colname}: {f(history_df[colname]):.4f} - {val_colname}: {f(history_df[val_colname]):.4f}')\n    history_df.loc[:, [colname, val_colname]].plot(title = colname.capitalize() , ax = ax)\n\ndef show_history(history):\n    history_df = pd.DataFrame(history.history)\n    \n    fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (15, 5))\n    plot_item(history_df, 'loss', ax = axes[0])\n    plot_item(history_df, 'accuracy', ax = axes[1], f = np.max)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:15.147755Z","iopub.execute_input":"2021-07-19T11:00:15.148083Z","iopub.status.idle":"2021-07-19T11:00:15.155885Z","shell.execute_reply.started":"2021-07-19T11:00:15.148038Z","shell.execute_reply":"2021-07-19T11:00:15.154957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fit_model(model, train_batch_ds = train_batch_ds, epochs = 500, patience = 2):\n    model.compile(\n        optimizer = 'adam',\n        loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n        metrics = ['accuracy'],\n    )\n\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience = patience,\n        restore_best_weights = True,\n    )\n\n    history = model.fit(\n        train_batch_ds, \n        validation_data = valid_batch_ds,\n        epochs = epochs,\n        callbacks = [early_stopping],\n    )\n    return history ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:15.157783Z","iopub.execute_input":"2021-07-19T11:00:15.158148Z","iopub.status.idle":"2021-07-19T11:00:15.165273Z","shell.execute_reply.started":"2021-07-19T11:00:15.158111Z","shell.execute_reply":"2021-07-19T11:00:15.164478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"model_lstm_bi = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n    tf.keras.layers.Dense(16, activation = 'relu'),\n    tf.keras.layers.Dense(5),\n])\n\nmodel_lstm_bi.summary()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:15.166437Z","iopub.execute_input":"2021-07-19T11:00:15.166904Z","iopub.status.idle":"2021-07-19T11:00:15.611581Z","shell.execute_reply.started":"2021-07-19T11:00:15.166849Z","shell.execute_reply":"2021-07-19T11:00:15.610756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.test.is_gpu_available()","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:15.614644Z","iopub.execute_input":"2021-07-19T11:00:15.614913Z","iopub.status.idle":"2021-07-19T11:00:15.627686Z","shell.execute_reply.started":"2021-07-19T11:00:15.614886Z","shell.execute_reply":"2021-07-19T11:00:15.626384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_lstm_bi = fit_model(model_lstm_bi, patience = 5,epochs=5)  \nshow_history(history_lstm_bi)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:00:15.629182Z","iopub.execute_input":"2021-07-19T11:00:15.629565Z","iopub.status.idle":"2021-07-19T11:03:33.249246Z","shell.execute_reply.started":"2021-07-19T11:00:15.629533Z","shell.execute_reply":"2021-07-19T11:03:33.248438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = tf.data.Dataset.from_tensor_slices(test_data.Phrase.to_numpy())\ntest_ds = test_ds.batch(batch_size)\ntest_ds = test_ds.map(lambda text_batch: encode_text_batch(text_batch))","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:03:33.2505Z","iopub.execute_input":"2021-07-19T11:03:33.250827Z","iopub.status.idle":"2021-07-19T11:03:33.279163Z","shell.execute_reply.started":"2021-07-19T11:03:33.25079Z","shell.execute_reply":"2021-07-19T11:03:33.278407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_and_write_csv(model, csv_name):\n    predicted = model.predict(test_ds)\n    labels = list(map(tf.argmax, predicted))\n    labels = list(map(lambda x: x.numpy(), labels))\n    result_df = pd.DataFrame({'PhraseId': test_data.PhraseId, 'Sentiment': labels})\n    result_df.to_csv(csv_name, index = False)","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:03:33.280329Z","iopub.execute_input":"2021-07-19T11:03:33.280851Z","iopub.status.idle":"2021-07-19T11:03:33.286705Z","shell.execute_reply.started":"2021-07-19T11:03:33.28081Z","shell.execute_reply":"2021-07-19T11:03:33.285649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model = keras.Sequential([\n    keras.layers.Embedding(vocab_size, embedding_dim, input_length = maxlen),\n    keras.layers.Bidirectional(keras.layers.LSTM(embedding_dim)),\n    keras.layers.Dense(16, activation = 'relu'),\n    keras.layers.Dense(5),\n])\nfinal_model.compile(\n    optimizer = 'adam',\n    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n    metrics = ['accuracy'],\n)\n\nfinal_model.fit(\n    total_batch_ds, \n    epochs = 10,\n)\npredict_and_write_csv(final_model, 'final_submission.csv')  ","metadata":{"execution":{"iopub.status.busy":"2021-07-19T11:15:27.272718Z","iopub.execute_input":"2021-07-19T11:15:27.273087Z"},"trusted":true},"execution_count":null,"outputs":[]}]}