{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sentiment Analysis on Movie Reviews","metadata":{"id":"y_5OGh9I-5JU"}},{"cell_type":"markdown","source":"## Final solution\n\nThe experiment phase highlighted how the Transformer model achieves the best result in this particular task with our dataset.\nSo, as final solution, we use the Transformer model.\nAlso, validating the model showed that the best results are obtained fine-tuning the pre-trained model *distilbert-base-uncased*.\nThe model seems to overfit pretty soon, possibly because it is very complex and we have little training data.\nSo, for our final version, we train it for only 4 epochs with all of our training data, increasing the dropout from 0.1 (default) to 0.2.\nWe then use this model to produce the final output.\n\n<br/>\n\nThis solution scored 0.69352 (accuracy) on the official Kaggle competition, which would have ranked 5th according to the official Leaderboard, among more than 850 submissions. \n\n\n\nThis section is designed to be independent from the rest of the notebook, so it can be run by itself.\n\n","metadata":{"id":"vmaKNjRXurvw"}},{"cell_type":"code","source":"! pip install datasets transformers","metadata":{"id":"26EFvbPAt2ev","outputId":"fd11804e-874c-4406-e744-6ffccf165403","execution":{"iopub.status.busy":"2022-06-12T21:04:31.252618Z","iopub.execute_input":"2022-06-12T21:04:31.253042Z","iopub.status.idle":"2022-06-12T21:04:43.173551Z","shell.execute_reply.started":"2022-06-12T21:04:31.252964Z","shell.execute_reply":"2022-06-12T21:04:43.17255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nimport torch\nimport pandas as pd\nfrom huggingface_hub import notebook_login\n\nmodel_checkpoint_final = \"distilbert-base-uncased\"\n\nclass MovieReviewDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.labels)\n      \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n\n# Login huggingface\n# Copy and paste the following in the form that pops up: \n# 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\n# The token enables only read operations\nnotebook_login()  ","metadata":{"id":"znje54V6umv2","outputId":"480c75f9-6173-4a9d-b939-f3a3ed85168f","execution":{"iopub.status.busy":"2022-06-12T21:04:54.705881Z","iopub.execute_input":"2022-06-12T21:04:54.706297Z","iopub.status.idle":"2022-06-12T21:05:01.457847Z","shell.execute_reply.started":"2022-06-12T21:04:54.706255Z","shell.execute_reply":"2022-06-12T21:05:01.457094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cell was used to train our final model.\nBecause it has been already done, this cell has been commented out. You can skip directly to the next where we download the model from the HugginFace hub and validate it.","metadata":{"id":"8_xjVWel-Yza"}},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split \n# from transformers import TrainingArguments, Trainer, AutoTokenizer, AutoModelForSequenceClassification\n# import pandas as pd\n# import numpy as np\n\n# # Load datatest \n# train_url = \"train.tsv\"\n# train_data = pd.read_csv(train_url, sep = '\\t')\n\n# X_train_final = train_data['Phrase'].tolist()           # training features\n# y_train_final = np.array(train_data['Sentiment'])         # training labels\n\n# # Model config label\n# id2label = {\n#   0: 0, \n#   1: 1,\n#   2: 2,\n#   3: 3,\n#   4: 4\n# }\n\n# # Model creation\n# model_final = AutoModelForSequenceClassification.from_pretrained(model_checkpoint_final, num_labels=5, dropout=0.2, id2label=id2label)\n# tokenizer_final = AutoTokenizer.from_pretrained(model_checkpoint_final, use_fast=True)\n\n# # Create a list of words ids from distilbert vocabulary\n# X_train_tokenized_final = tokenizer_final(X_train_final, truncation=True)\n\n# # Network Arguments\n# train_dataset_final = MovieReviewDataset(X_train_tokenized_final, y_train_final)\n\n# batch_size_final = 16\n# epochs_final = 4      # 15\n# learning_rate = 2e-5\n# weight_decay = 0.02\n# gradient_accumulation_steps = 1\n\n# args_final = TrainingArguments(\n#     f\"{model_checkpoint_final}-finetuned-final-nlp-1-epoch-default-dropout\",\n#     save_strategy = \"epoch\",\n#     learning_rate = learning_rate,\n#     per_device_train_batch_size = batch_size_final,\n#     per_device_eval_batch_size = batch_size_final * 4,\n#     gradient_accumulation_steps = gradient_accumulation_steps,\n#     num_train_epochs = epochs_final,\n#     weight_decay = weight_decay,\n#     do_eval = False,\n#     push_to_hub = True\n# )\n\n# trainer_final = Trainer(\n#     model_final,\n#     args_final,\n#     train_dataset=train_dataset_final,\n#     tokenizer=tokenizer_final\n# )\n\n# trainer_final.train()","metadata":{"id":"F2kmkAESurOO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we download the model that we have fine-tuned from the HuggingFace hub and produce our final output.\nWe also ensure that both the model and the data are stored on the same device, otherwise the pipeline would raise an error.\n\n<br/>\nBefore running the predictions, we need to set the model in eval mode to disable the dropout layers.\n\n<br/>\n\nThe predictions on the whole test set take about 7 minutes when using GPU and much more when using CPU.\n","metadata":{"id":"cP1xlxwi-wMW"}},{"cell_type":"code","source":"import csv\nfrom transformers import TextClassificationPipeline, AutoTokenizer, AutoModelForSequenceClassification\n\ndevice_model = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndevice_pipe = 0 if torch.cuda.is_available() else -1\n\n# Load dataset\ntest_url = \"../input/dataset/test.tsv\"\n\ntest_data = pd.read_csv(test_url, sep = '\\t')\nX_test_phrases = test_data['Phrase'].tolist()\nX_test_phrase_ids = test_data['PhraseId'].tolist()\n\ntokenizer = AutoTokenizer.from_pretrained(f\"gianfrancodemarco/{model_checkpoint_final}-finetuned-final-nlp\")\nmodel = AutoModelForSequenceClassification.from_pretrained(f\"gianfrancodemarco/{model_checkpoint_final}-finetuned-final-nlp\")\n\n# We need to set the model in eval mode in order to disable dropout layers. Otherwise, predictions would have a random component.\nmodel.eval()\n\npipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, device=device_pipe)\n\n# About 7 mins (on GPU)\npredictions = pipe(X_test_phrases)\n\noutput = [{'PhraseId': phrase_id, 'Sentiment': prediction['label']} for (phrase_id, prediction) in zip(X_test_phrase_ids, predictions)]\n\n\nwith open('./output.csv', 'w', newline='') as output_file:\n    dict_writer = csv.DictWriter(output_file, ['PhraseId', 'Sentiment'])\n    dict_writer.writeheader()\n    dict_writer.writerows(output)","metadata":{"id":"rmCGupK1Wh4U","outputId":"0602edf6-44da-42b3-81e6-ac3e0794ab94","execution":{"iopub.status.busy":"2022-06-12T21:05:23.437315Z","iopub.execute_input":"2022-06-12T21:05:23.438418Z","iopub.status.idle":"2022-06-12T21:10:42.159651Z","shell.execute_reply.started":"2022-06-12T21:05:23.438358Z","shell.execute_reply":"2022-06-12T21:10:42.158789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n\nAuthor : Gianfranco Demarco\n\nAuthor : Francesco Ranieri\n","metadata":{"id":"tttiafmeGnty"}}]}