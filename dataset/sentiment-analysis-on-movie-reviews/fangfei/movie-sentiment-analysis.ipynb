{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Movie sentiment analysis\n\n**Data:**\n\nThe data source is Movie Sentiment project from Kaggle, including train and test datasets; In trainning data, there is Phrase column and Sentiment column as the result score from 0 to 4 (negative to positve); In test data, there is Phrase column for us to analyze the sentiment of each phrase.\n\n**Goal:**\n\n\nPerform NLP analysis on trainning data and use different Machine learning models to compare model performances, and predict Phrase sentiment on test data. \n\n**Highlight:** \n\n- **Show NLP process step by step**\n- **Stremline NLP process with ML Pipeline**\n- **Using Object_oriented programming build class to perfrom mutiple ML models efficiently**\n\n**NLP process steps:**\n\n- Remove punctuation\n- Tokenize sentence\n- Remove stopwords\n- Stem or lemmatize words:\n  - Both methods aim to change the words to original form (if using both: better lemmartize first and then stem)\n     - Stemming change words based on rules on string: e.g.: delted 's' at the end of noun. While it has serious limitations on change the actual meaning of words. Since the algorithm is change based on rules for strings, it runs faster and it's a good choice if time is a concern in NLP process\n          - There are three stemmer: porter, snowball(porter2), lancaster; porter is the orginal and most gental one, while it's the most computationally intensive. snowball is a litter intensive than porter and it improves from porter (common option); lancaster is the most aggresive one, the faster one while the final words might obscure\n     - Lemmatization change words based on the dictionary from different algorithms, such as \"went\" to \"go\". Based on the differnt type of the word (verb, noun), it can change to differnt meaning of word which solve the disambiguation problem. While it demands more computaional power. (It can be used if you want to build a dictionary world: NLP system)\n- Calculate TFIDF \n- Train ML models\n- Compare models results and test model\n\n\n\n**Reference:**\n\n\nNLP process: \nhttps://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1\n\nTFIDF:\nhttps://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n\nHyperparameter on machine learning models:  https://github.com/davidsbatista/machine-learning-notebooks/blob/master/hyperparameter-across-models.ipynb\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Build ML models step by step\n## Step1:  Exploring Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Import basic packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Read data\ntrain = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip',sep=\"\\t\") \ntest = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip',sep=\"\\t\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Show the number of class distributed\nplt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(x=train.Sentiment,data=train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step2: NLP process (step by step)\n### Remove punctuation and lowercase"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Phrase[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\nstring.punctuation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Phrase=train.Phrase.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)).lower())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Phrase[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenize sentence"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Phrase=train.Phrase.str.split(' ')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Phrase[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove stopwords"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nstopwords_e=stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stopwords_e=stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Phrase=[w for w in train.Phrase if w not in stopwords_e]\ntrain.Phrase.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatize words"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\n##nltk.download()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemmar=WordNetLemmatizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Phrase=train.Phrase.apply(lambda x: [lemmar.lemmatize(w) for w in x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stemming words"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Method1:\nfrom nltk.stem import PorterStemmer\nporter=PorterStemmer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Phrase=train.Phrase.apply(lambda x: [porter.stem(w) for w in x])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Method2:\nfrom nltk.stem import SnowballStemmer\nsnow=SnowballStemmer('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Phrase=train.Phrase.apply(lambda x: [snow.stem(w) for w in x])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### TFIDF vectorize\n\nTFIDF: Term frequency inverse document frequency\n\n\n**formula:** \nTFIDF=Term frequency* Inverse Document frequency\n\n\n\n- Term frequency: count of same word w in a documents/ the total number of words in documents\n\n- Document frequency: number of documents have the word/the total number of documents\n\n- To avoid the number of documents too big, we take log of the IDF: if word not shows up, log(IDF)=0, and 0 cannot be divide, we add 1, so formula becomes: TF*log(N/DF+1) [More info in references]\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\nvector=TfidfVectorizer(stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Phrase=train.Phrase.apply(lambda x: ' '.join(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vector1=vector.fit(train.Phrase)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature=vector1.transform(train.Phrase)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feature.toarray()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Step3: Build ML models on train dataset"},{"metadata":{},"cell_type":"markdown","source":"### Multi_class logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nlr=LogisticRegression(multi_class='ovr')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"lr=lr.fit(train_feature,train.Sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Coefficient\nlr.coef_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Get the model performance on train dataset since we don't have test response data\ntrain_predict=lr.predict(train_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## the number of data in each class\ntrain.Sentiment.value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## number of data in predict result\nnp.unique(train_predict,return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Plot predict result\nplt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(train_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(train_predict, train.Sentiment))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Muti-class SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm1=svm.SVC(decision_function_shape='ovo')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm1.fit(train_feature, train.Sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svm_train_pred=svm1.predict(train_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Number of predict class\nnp.unique(svm_train_pred,return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(svm_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(svm_train_pred, train.Sentiment))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision tree model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds=DecisionTreeClassifier()\nds.fit(train_feature, train.Sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(ds.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds_train_pred=ds.predict(train_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Sentiment.value_counts().sort_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Number of predict class\nnp.unique(ds_train_pred,return_counts=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(ds_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(ds_train_pred, train.Sentiment))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random forest model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf=RandomForestClassifier()\nrf.fit(train_feature, train.Sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rf.feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_train_pred=rf.predict(train_feature)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(rf_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(rf_train_pred, train.Sentiment))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Stremline the process \n\n## Method1: Pipeline \nonly use LR model as an example"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_preprocess(text):\n    text_nonpunc=[w.lower() for w in text if w not in string.punctuation]\n    text_nonpunc=''.join(text_nonpunc)\n    text_rmstop=[x for x in text_nonpunc.split(' ') if x not in stopwords_e]\n    text_stem=[snow.stem(w) for w in text_rmstop]\n    text1=' '.join(text_stem)\n    return (text1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Can't use TfidVecterizer() because line: \n# https://stackoverflow.com/questions/50192763/python-sklearn-pipiline-fit-attributeerror-lower-not-found\n# TfidTransformer should combine with countVectorizer()\nlrpipeline=Pipeline([('preprocess',CountVectorizer(analyzer=data_preprocess)),\n                  ('Tfidf',TfidfTransformer()),\n                  ('classify',LogisticRegression())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lrpipeline.fit(train.Phrase,train.Sentiment)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## have to saved the vocabulary\nresult=lrpipeline.predict(test['Phrase'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(result)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method 2: OOP to built class perform all models\n\nPerform the TOP 3 models (based on accuracy on train data) in functions. "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Import every packages\nfrom scipy import stats\nimport string\nfrom nltk.corpus import stopwords\nstopwords_e=stopwords.words('english')\nfrom nltk.stem import SnowballStemmer\nsnow=SnowballStemmer('english')\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import  RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\nvector=TfidfVectorizer(stop_words='english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Preprocess function\ndef data_preprocess(text):\n    text_nonpunc=[w.lower() for w in text if w not in string.punctuation]\n    text_nonpunc=''.join(text_nonpunc)\n    text_rmstop=[x for x in text_nonpunc.split(' ') if x not in stopwords_e]\n    text_stem=[snow.stem(w) for w in text_rmstop]\n    text1=' '.join(text_stem)\n    return (text1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## OOP Class \n## Notice: Class name and the first def should have a blank line\nclass EstimatorSelection:\n    \n    def __init__(self, models):\n        self.models=models\n        self.keys=models.keys()\n        self.results={}\n        self.modelfit={}\n        self.modelpredict={}\n    def fit(self, x, y):\n        x1=x.apply(lambda i: data_preprocess(i))\n        x_feature1=vector.fit_transform(x1)\n        for key in self.keys:\n            model=self.models[key]\n            self.modelfit[key]=model.fit(x_feature1,y)\n            y_pred=model.predict(x_feature1)\n            self.results[key]=classification_report(y, y_pred,output_dict=True)\n    def predict(self,test_x):\n        test_x1=test_x.apply(lambda i: data_preprocess(i))\n        test_feature1=vector.transform(test_x1)\n        test_frames=[]\n        for key in self.keys:\n            modelfit=self.modelfit[key]\n            test_y=modelfit.predict(test_feature1)\n            test_frame=pd.DataFrame(test_y,columns=[key])\n            test_frames.append(test_frame)\n        predict_frame=pd.concat(test_frames,axis=1)            \n        return(predict_frame)     \n    def summary(self):\n        Frames=[]\n        for key in self.keys:\n            result=self.results[key]\n            Frame=pd.DataFrame(result['macro avg'], index=[key])\n            Frames.append(Frame)\n        result_sum=pd.concat(Frames)\n        return result_sum.iloc[:,:3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Models want to predict on test data\nmodels = { \n    'LogisticClassifier': LogisticRegression(multi_class='ovr'),\n    'RandomforestClassifier':RandomForestClassifier(),\n    'DecisionTreeClassifier':DecisionTreeClassifier()\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_compare=EstimatorSelection(models)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_compare.fit(train.Phrase, train.Sentiment)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Compare model performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"summary=model_compare.summary()\nsummary","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_result=model_compare.predict(test.Phrase)\npredict_result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Reshape result dataframe to plot\nMethod: Melt() and Pivottable()"},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_result1=predict_result.reset_index().rename(columns={'index':'case'})\npredict_result2=pd.melt(predict_result1,id_vars='case', value_vars=['LogisticClassifier', 'RandomforestClassifier', 'DecisionTreeClassifier'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_result2=pd.melt(predict_result1,id_vars='case', value_vars=['LogisticClassifier', 'RandomforestClassifier', 'DecisionTreeClassifier'])\npredict_result2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_result3=predict_result2.groupby(['variable','value']).size().reset_index().rename(columns={0:'count'})\npredict_result3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Compare ML models predict results"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of class for each methods')\nsns.barplot(x='value', y='count', hue='variable', data=predict_result3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get the Final result from the mode of three classification results"},{"metadata":{"trusted":true},"cell_type":"code","source":"Final_results=[]\nfor i in range(predict_result1.shape[0]):\n    Final_result=stats.mode(predict_result1.iloc[i,]).mode.item()\n    Final_results.append(Final_result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict_result1['Final_result']=Final_results\npredict_result1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['Sentiment']=Final_results\ntest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Submission"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"#make the predictions with trained model and submit the predictions.\nsub_file = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/sampleSubmission.csv',sep=',')\nsub_file.Sentiment=Final_results\nsub_file.to_csv('Submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":4}