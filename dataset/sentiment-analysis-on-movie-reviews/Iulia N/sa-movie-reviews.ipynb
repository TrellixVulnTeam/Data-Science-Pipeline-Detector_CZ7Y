{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport string\nimport matplotlib.pyplot as plot\nimport seaborn as seaborn\nfrom sklearn.model_selection import train_test_split\n\ntrain_df = pd.read_csv('../input/train.tsv', sep='\\t')\n\n# a glimpse at the training data\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# An insight into the data\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"figure = plot.figure(figsize=(10, 5))\nseaborn.countplot(data=train_df, x='Sentiment')\nplot.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Resample"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_count():\n    s0 = train_df[train_df.Sentiment == 0].Sentiment.count()\n    s1 = train_df[train_df.Sentiment == 1].Sentiment.count()\n    s2 = train_df[train_df.Sentiment == 2].Sentiment.count()\n    s3 = train_df[train_df.Sentiment == 3].Sentiment.count()\n    s4 = train_df[train_df.Sentiment == 4].Sentiment.count()\n    return s0, s1, s2, s3, s4\n\ns0, s1, s2, s3, s4 = get_count()\nprint(s0, s1, s2, s3, s4)\n\ndf0 = s2 // s0 - 1\ndf1 = s2 // s1 - 1\ndf3 = s2 // s3 - 1\ndf4 = s2 // s4 - 1\n \ntrain_df = train_df.append([train_df[train_df.Sentiment == 0]] * df0, ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 1]] * df1, ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 3]] * df3, ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 4]] * df4, ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 0][0 : s2 % s0]], ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 1][0 : s2 % s1]], ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 3][0 : s2 % s3]], ignore_index=True)\ntrain_df = train_df.append([train_df[train_df.Sentiment == 4][0 : s2 % s4]], ignore_index=True)\n\ns0, s1, s2, s3, s4 = get_count()\nprint(s0, s1, s2, s3, s4)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Several word clouds, to emphasize the mose frequent words per category:"},{"metadata":{"trusted":true},"cell_type":"code","source":"figure = plot.figure(figsize=(5, 2.5))\nseaborn.countplot(data=train_df, x='Sentiment')\nplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nsentiments = [0, 1, 2, 3, 4]\ncloud = WordCloud(background_color=\"white\", max_words=20, stopwords=stopwords.words('english'))\n\ndef draw_word_clouds(dataframe):\n    for i in sentiments: \n        category = cloud.generate(dataframe.loc[dataframe['Sentiment'] == i, 'Phrase'].str.cat(sep='\\n'))\n        plot.figure(figsize=(5, 2.5))\n        plot.imshow(category)\n        plot.axis(\"off\")\n        plot.title(i)\n        plot.show()\n\ndraw_word_clouds(train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clean the data\n"},{"metadata":{"trusted":true,"_kg_hide-output":false},"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom joblib import Parallel, delayed\nimport string \nimport time \n\nlemmatizer = WordNetLemmatizer() \nstop_words  = stopwords.words('english')\nstop_words.extend(['movie', 'film', 'series', 'story', 'one', 'like'])\n\ndef clean_review(review):\n#     review = re.sub(\"[^a-zA-Z]\", \" \", review)\n    tokens = review.lower().split()\n    filtered_tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n    return \" \".join(filtered_tokens)\n\nstart = time.time()\nclean_train_data = train_df.copy()\nclean_train_data['Phrase'] = Parallel(n_jobs=4)(delayed(clean_review)(review) for review in train_df['Phrase'])\nend = time.time()\nprint(\"Cleaning Training Data - Processing time = \", end - start)\n\n# remove missing values\nprint(\"Clean entries: \", clean_train_data.shape[0], \" out of \", train_df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Split: training & validation data"},{"metadata":{"trusted":true},"cell_type":"code","source":"target = clean_train_data.Sentiment\ntrain_X_, validation_X_, train_y, validation_y = train_test_split(clean_train_data['Phrase'], target, test_size=0.25, random_state=21)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TFIDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer as tfidf\n\ntfidf_vec = tfidf(min_df=3,  max_features=None, \n        ngram_range=(1, 2), use_idf=1)\nstart = time.time()\ntrain_X = tfidf_vec.fit_transform(train_X_)\nend = time.time()\nprint(\"TFIDF finished in: \", end - start)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\nmodel = MultinomialNB()\nmodel.fit(train_X, train_y)\nvalidation_X = tfidf_vec.transform(validation_X_)\npredicted = model.predict(validation_X)\nexpected = validation_y\nprint(metrics.classification_report(expected, predicted))\nprint(metrics.accuracy_score(expected, predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/test.tsv', sep='\\t')\n\nclean_test_data = test_df.copy()\nclean_test_data['Phrase'] = Parallel(n_jobs=4)(delayed(clean_review)(review) for review in test_df['Phrase'])\nend = time.time()\nprint(\"Cleaning Testing Data - Processing time = \", end - start)\n\n# remove missing values\nprint(\"Clean entries: \", clean_test_data.shape[0], \" out of \", test_df.shape[0])\ntest_X = tfidf_vec.transform(clean_test_data['Phrase'])\n\ntest_predictions = model.predict(test_X)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({\n    'PhraseId': test_df['PhraseId'],\n    'Sentiment': test_predictions\n})\n\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}