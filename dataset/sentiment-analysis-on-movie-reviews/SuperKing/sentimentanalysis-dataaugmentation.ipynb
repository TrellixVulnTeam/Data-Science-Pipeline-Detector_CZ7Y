{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!python3 -m spacy download en_core_web_md","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os, sys, csv, re\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom tqdm.auto import tqdm\n\nimport en_core_web_md\nnlp = en_core_web_md.load()\n\ndf = pd.read_csv(\"../input/sentiment-analysis-on-movie-reviews/train.tsv.zip\", sep=\"\\t\")\ndf_test = pd.read_csv('../input/sentiment-analysis-on-movie-reviews/test.tsv.zip', delimiter = '\\t')\n\nif True:\n    df = df.rename(columns = {'Phrase' : 'Text', 'Sentiment' : 'Score'})\n    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n    for i, (train_indices, test_indices) in enumerate(sss.split(df, df.Score)):\n        df_test = df.iloc[test_indices].reset_index(drop = True)\n        df = df.iloc[train_indices].reset_index(drop = True)\n\n    #df_test = df_test.rename(columns = {'Phrase' : 'Text', 'Sentiment' : 'Score'})\nelse:\n    df = df.rename(columns = {'Phrase' : 'Text'})\n    df['Score'] = 0\n    df.loc[df.Sentiment >= 4, 'Score'] = 1\n    df.loc[df.Sentiment <= 0, 'Score'] = -1\n    \n    #df_test = df_test.rename(columns = {'Phrase' : 'Text'})\n    #df_test['Score'] = 0\n    #df_test.loc[df_test.Sentiment >= 4, 'Score'] = 1\n    #df_test.loc[df_test.Sentiment <= 0, 'Score'] = -1\n    for i, (train_indices, test_indices) in enumerate(sss.split(df, df.Score)):\n        df_test = df.iloc[test_indices].reset_index(drop = True)\n        df = df.iloc[train_indices].reset_index(drop = True)\n\nprint(len(df), len(df_test))\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import sentiwordnet as swn\n\ntokenizer = RegexpTokenizer(r\"['\\-\\w\\.\\?!,]+\")\nlemmatizer = WordNetLemmatizer()\nstops = stopwords.words('english')\n\ndef tkn(text):\n    if type(text) == list:\n        text = ' '.join(text)\n    text = re.sub(r\" (n't|'[a-z]{1,3})\", r'\\1', text)\n    text = re.sub(r'[^a-z0-9\\s\\'\\-\\.\\?!,\\\"]', '', text.lower())\n    text = ' '.join([lemmatizer.lemmatize(w) for w in tokenizer.tokenize(text)])\n    return text\n\ntqdm.pandas()\nprint('train')\ndf['Tokens'] = df.Text.apply(tkn)\ndf['nb_words'] = df.Tokens.str.count(' ') + 1\ndf['rel_nb_words'] = df.nb_words / df.nb_words.max()\ndf['nb_chars'] = df.Tokens.str.len()\ndf['rel_nb_chars'] = df.nb_chars / df.nb_chars.max()\n\n\nprint('test')\ndf_test['Tokens'] = df_test.Text.apply(tkn)\ndf_test['nb_words'] = df_test.Tokens.str.count(' ') + 1\ndf_test['rel_nb_words'] = df_test.nb_words / df_test.nb_words.max()\ndf_test['nb_chars'] = df_test.Tokens.str.len()\ndf_test['rel_nb_chars'] = df_test.nb_chars / df_test.nb_chars.max()\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\noneword_sentiment = {}\ndf_len1 = df[df.nb_words == 1]\nfor label in df_len1.Score.value_counts().to_dict().keys():\n    for word in df_len1.Tokens[df_len1.Score == label]:\n        oneword_sentiment[word] = label\n\n\ndef vader_sentinet(word):\n    vader_scores = sid.polarity_scores(word)\n    v_neg = vader_scores['neg']\n    v_neu = vader_scores['neu']\n    v_pos = vader_scores['pos']\n    \n    senti_net = list(swn.senti_synsets(word))\n    if len(senti_net) > 0:\n        s_neg = senti_net[0].neg_score()\n        s_pos = senti_net[0].pos_score()\n        s_obj = senti_net[0].obj_score()\n    else:\n        s_neg, s_pos, s_obj = 0, 0, 0\n    return v_neg, v_neu, v_pos, s_neg, s_pos, s_obj\n\ndef mean_senti_vader_score(text):\n    if type(text) == str:\n        text = text.split()\n    score_lists = [[] for _ in range(6)]\n    for w in text:\n        scores = vader_sentinet(w)\n        for i, s in enumerate(scores):\n            score_lists[i].append(s)\n    mean_scores = [sum(l) / max(1, len(l)) for l in score_lists]\n    return mean_scores\n        \ndef nb_senti_words(text):\n    if type(text) == str:\n        text = text.split()\n    d = {-1 : 0, 0 : 0, 1 : 0, 2 : 0, 3 : 0, 4 : 0}\n    for w in text:\n        if w in oneword_sentiment:\n            d[oneword_sentiment[w]] += 1\n    for k in d:\n        d[k] /= max(1, len(text))\n        \n    return d[0], d[1], d[2], d[3], d[4], d[-1]\n\nprint('train')\ndf = df.merge(df.Tokens.apply(lambda t: pd.Series(nb_senti_words(t))), \n                left_index=True, right_index=True)\ndf = df.rename(columns={\n                        0 : \"nb_senti_0\", \n                        1 : \"nb_senti_1\", \n                        2 : \"nb_senti_2\", \n                        3 : \"nb_senti_3\",\n                        4 : \"nb_senti_4\",\n                        5 : \"nb_senti_-1\"\n                       })\n\nprint('\\tvader/sentinet')\ndf = df.merge(df.Tokens.apply(lambda t: pd.Series(mean_senti_vader_score(t))), \n                left_index=True, right_index=True)\ndf = df.rename(columns={\n                        0 : \"v_neg\", \n                        1 : \"v_neu\", \n                        2 : \"v_pos\", \n                        3 : \"s_neg\",\n                        4 : \"s_pos\",\n                        5 : \"s_obj\"\n                       })\n\nprint('test')\ndf_test = df_test.merge(df_test.Tokens.apply(lambda t: pd.Series(nb_senti_words(t))), \n                left_index=True, right_index=True)\n\ndf_test = df_test.rename(columns={\n                        0 : \"nb_senti_0\", \n                        1 : \"nb_senti_1\", \n                        2 : \"nb_senti_2\", \n                        3 : \"nb_senti_3\",\n                        4 : \"nb_senti_4\",\n                        5 : \"nb_senti_-1\",\n                       })\ndf_test = df_test.merge(df_test.Tokens.apply(lambda t: pd.Series(mean_senti_vader_score(t))), \n                left_index=True, right_index=True)\n\nprint('\\tvader/sentinet')\ndf_test = df_test.rename(columns={\n                        0 : \"v_neg\", \n                        1 : \"v_neu\", \n                        2 : \"v_pos\", \n                        3 : \"s_neg\",\n                        4 : \"s_pos\",\n                        5 : \"s_obj\"\n                       })\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import subjectivity, movie_reviews\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.svm import SVC\n\nprint('subjectivity')\nsubj_docs = []\nfor l in tqdm(['subj', 'obj']):\n    for s in subjectivity.sents(categories=l):\n        subj_docs.append((l, tkn(s)))\n\nprint('polarity')\nreview_docs = []\nfor l in tqdm(['pos', 'neg']):\n    for s in movie_reviews.sents(categories=l):\n        review_docs.append((l, tkn(s)))\n\nsubj_df = pd.DataFrame(subj_docs, columns = ['label', 'text'])\nreview_df = pd.DataFrame(review_docs, columns = ['label', 'text'])\n\nsubj_cv = CountVectorizer(binary = True, min_df = 5, max_df = .5, dtype = np.int8).fit(subj_df.text)\nreview_cv = CountVectorizer(binary = True, min_df = 5, max_df = .5, dtype = np.int8).fit(review_df.text)\n\nX_subj = subj_cv.transform(subj_df.text)\nX_review = review_cv.transform(review_df.text)\n\nprint('subj_logit')\nsubj_logit = LogisticRegressionCV(n_jobs = 3, max_iter = 400, \n                                  cv=6, random_state=0, verbose = True).fit(X_subj, subj_df.label)\nprint('review_logit')\nreview_logit = LogisticRegressionCV(n_jobs = 3, max_iter = 400, \n                                  cv=6, random_state=0, verbose = True).fit(X_review, review_df.label)\n\nprint('subj', subj_logit.score(X_subj, subj_df.label))\nprint('polarity', review_logit.score(X_review, review_df.label))\n\ndf['subj_0'], df['subj_1'] = zip(*subj_logit.predict_proba(subj_cv.transform(df.Tokens)))\ndf['review_0'], df['review_1'] = zip(*review_logit.predict_proba(review_cv.transform(df.Tokens)))\n\ndf_test['subj_0'], df_test['subj_1'] = zip(*subj_logit.predict_proba(subj_cv.transform(df_test.Tokens)))\ndf_test['review_0'], df_test['review_1'] = zip(*review_logit.predict_proba(review_cv.transform(df_test.Tokens)))\n\ndel subj_df, subj_logit, subj_cv\ndel review_df, review_logit, review_cv\n\ndf.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def vader_score(text):\n    scores = sid.polarity_scores(text)\n    return scores['neg'], scores['neu'], scores['pos'], scores['compound']\n\nprint('train')\ndf = df.merge(df.Tokens.apply(lambda t: pd.Series(vader_score(t))), \n         left_index=True, right_index=True)\ndf = df.rename(columns={0 : \"vader_neg\", 1 : \"vader_neu\", 2 : \"vader_pos\", 3 : \"vader_compound\"})\n\nprint('test')\ndf_test = df_test.merge(df_test.Tokens.apply(lambda t: pd.Series(vader_score(t))), \n         left_index=True, right_index=True)\ndf_test = df_test.rename(columns={0 : \"vader_neg\", 1 : \"vader_neu\", 2 : \"vader_pos\", 3 : \"vader_compound\"})\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.models.ldamodel import LdaModel\n\nfrom scipy.sparse import hstack, csr_matrix\n\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\ntexts = []\nfor i, t in tqdm(enumerate(df.Tokens)):\n    texts.append(tkn(t).split())\n    \ndictionary = Dictionary(texts)\ndictionary.filter_extremes(no_below=5, no_above=0.5)\nstop_ids = [i for i, w in dictionary.items() if w in stops]\ndictionary.filter_tokens(bad_ids = stop_ids)\n\nnb_lda_features = 64\ntext_corpus = [dictionary.doc2bow(t) for t in texts]\n\nprint('train lda')\nlda = LdaModel(text_corpus, id2word = dictionary,  \n               num_topics = nb_lda_features, \n               alpha = 'auto', eta = 'auto',\n               iterations = 400, eval_every = None)\n              \n\nX_lda = np.zeros((len(df), nb_lda_features), dtype = np.float32)\nfor i, t in tqdm(enumerate(text_corpus)):\n    for j, topic_score in lda[t]:\n        X_lda[i,j] = topic_score\n\nprint('test')\ntext_corpus_test = []\nfor i, t in tqdm(enumerate(df_test.Tokens)):\n    text_corpus_test.append(dictionary.doc2bow(tkn(t).split()))\n\nX_lda_test = np.zeros((len(df_test), nb_lda_features), dtype = np.float32)\nfor i, t in tqdm(enumerate(text_corpus_test)):\n    for j, topic_score in lda[t]:\n        X_lda_test[i,j] = topic_score\n\ndel texts\ndel text_corpus\ndel text_corpus_test\ndel lda\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import vstack, hstack, csr_matrix\nfrom scipy.sparse.linalg import svds\n\nprint('data')\ncv = CountVectorizer(binary = True, max_df = .5, min_df = 5, \n                        dtype = np.int8).fit(df.Tokens)\n#cv = TfidfVectorizer(max_df = .5, min_df = 5, dtype = np.float32).fit(df.Tokens)\n\nX_text = cv.transform(df.Tokens) #.toarray()\nX_text_final = cv.transform(df_test.Tokens)\nprint('starting shape:', X_text.shape)\n#X_text_svd, _, _ = svds(X_text, k=1000, return_singular_vectors = 'u')\n#print('SVD shape:', X_text_svd.shape)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import OneHotEncoder\n\n\ntknzr = Tokenizer()\ntknzr.fit_on_texts(df.Tokens)\n\n\nX_seq_list = tknzr.texts_to_sequences(df.Tokens)\n\nX_seq_list_final = tknzr.texts_to_sequences(df_test.Tokens)\n\nseq_len = max(max([len(s) for s in X_seq_list]),\n             max([len(s) for s in X_seq_list_final]))\n\nX_seq = pad_sequences(X_seq_list, maxlen = seq_len)\nX_seq_final = pad_sequences(X_seq_list_final, maxlen = seq_len)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX_other = np.array(df[['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound', \n                       'v_neg', 'v_neu', 'v_pos', 's_neg', 's_pos', 's_obj',\n                       'nb_words', 'nb_chars', 'rel_nb_words', 'rel_nb_chars',\n                       'nb_senti_-1', 'nb_senti_0', 'nb_senti_1', 'nb_senti_2', 'nb_senti_3', 'nb_senti_4', \n                       'subj_0', 'subj_1', 'review_0']])\nX_other_final = np.array(df_test[['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound', \n                       'v_neg', 'v_neu', 'v_pos', 's_neg', 's_pos', 's_obj',\n                       'nb_words', 'nb_chars', 'rel_nb_words', 'rel_nb_chars',\n                       'nb_senti_-1', 'nb_senti_0', 'nb_senti_1', 'nb_senti_2', 'nb_senti_3', 'nb_senti_4', \n                       'subj_0', 'subj_1', 'review_0']])\n\n#X = hstack((X_text, csr_matrix(X_lda), csr_matrix(X_other))).tocsr()\n#X_final = hstack((X_text_test, csr_matrix(X_lda_test), csr_matrix(X_other_test))).tocsr()\n\nX = hstack((X_text, csr_matrix(X_other))).tocsr()\nX_final = hstack((X_text_final, csr_matrix(X_other_final))).tocsr()\n\noh = OneHotEncoder().fit(df.Score.to_numpy().reshape(-1, 1))\ny = oh.transform(df.Score.to_numpy().reshape(-1,1)).toarray()\n\n\nprint(X.shape, X_seq.shape)\nprint(y.shape)\ndel X_seq_list\ndel X_seq_list_final","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ndef undersample_Xy(X, y):\n    y_argmax = y.argmax(axis = 1)\n    y_counts = Counter(y_argmax)\n    nb_to_sample = sorted(y_counts.values())[1]\n    \n    def undr(label):\n        X_by_label = X[y_argmax == label]\n        y_by_label = y[y_argmax == label]\n        count_for_label = X_by_label.shape[0]\n        sampled_indices = np.random.choice(np.arange(count_for_label), \n                                   size = min(nb_to_sample, count_for_label), replace = False)\n        undersampled_X = X_by_label[sampled_indices]\n        undersampled_y = y_by_label[sampled_indices]\n        return undersampled_X, undersampled_y\n    \n    Xs, ys = zip(*[undr(label) for label in y_counts.keys()])\n    return vstack(Xs), vstack(ys)\n\ndef rnn_model():\n    rnn_input_1 = Input(shape = (X_seq.shape[1],))\n    embed_1 = Embedding(output_dim = wvec.shape[1],\n                        weights = [wvec],\n                        mask_zero = True,\n                        input_dim = len(tkn.word_index) + 1, \n                        input_length = X_seq.shape[1])(rnn_input_1)\n    lstm_1 = LSTM(128, dropout = .1, recurrent_dropout = .1)(embed_1)\n    \n    output_1 = Dense(y.shape[1], activation = 'softmax', )(lstm_1) #bias_regularizer = 'l2'\n    \n    model = Model(rnn_input_1, output_1)\n    model.compile(optimizer = Adam(learning_rate=lr), \n                  metrics = ['accuracy'], loss = 'categorical_crossentropy')\n    return model\nlogit = None\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM \nfrom keras.layers import BatchNormalization, Dropout, Concatenate\n\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom keras.optimizers import Adagrad, Adam\n        \nwordvec_dim = 311\n\nwvec = np.zeros((len(tknzr.word_index) + 1, wordvec_dim), dtype = np.float32)\nfor w, i in tqdm(tknzr.word_index.items()):\n    if w in nlp.vocab:\n        wvec[i,:300] = nlp.vocab[w].vector   \n    else:\n        wvec[i,:300] = (2 * np.random.rand(300)) - 1\n     \n    wvec[i,300:306] = vader_sentinet(w)\n    if w in oneword_sentiment:\n        wvec[i, 306+oneword_sentiment[w]] = 1\n        \nlr = .00005\n\ndense_1_size = 250\ndense_2_size = 80\ncnn_1_size = 120\n\n\n\nopm = Adam(learning_rate=lr)\ndef logit_model():\n    input_1 = Input(shape = (X.shape[1],))\n    output_1 = Dense(y.shape[1], activation = 'softmax', )(input_1)\n    model = Model(input_1, output_1)\n    model.compile(optimizer = opm, \n                  metrics = ['accuracy'], loss = 'categorical_crossentropy')\n    return model\n\ndef nn_model():\n    input_1 = Input(shape = (X.shape[1],))\n    dense_1 = Dense(dense_1_size, activation = 'relu',)(input_1)\n    dense_1 = Dropout(.2)(dense_1)\n    #dropped_1 = Dropout(.1)(dense_1)\n    #normed_1 = BatchNormalization()(dense_1)\n    #dense_2 = Dense(32, activation = 'relu')(dropped_1)\n    #dropped_2 = Dropout(.1)(dense_2)\n    output_1 = Dense(y.shape[1], activation = 'softmax', \n                     kernel_regularizer = None, bias_regularizer = None)(dense_1) #\n    \n    model = Model(input_1, output_1)\n    model.compile(optimizer = opm, \n                  metrics = ['accuracy'], loss = 'categorical_crossentropy')\n    return model\n\n\ndef cnn_model():\n    cnn_input_1 = Input(shape = (X_seq.shape[1],))\n    embed_1 = Embedding(output_dim = wvec.shape[1],\n                        weights = [wvec],\n                        #mask_zero = True,\n                        input_dim = len(tknzr.word_index) + 1, \n                        input_length = X_seq.shape[1])(cnn_input_1)\n    cnn_1 = Conv1D(cnn_1_size, 2, activation = 'relu', \n                   kernel_regularizer='l2')(embed_1)\n    #pool_1 = MaxPooling1D(pool_size = 2)(cnn_1)\n    #cnn_2 = Conv1D(512, 2, activation = 'relu', strides = 1)(pool_1)\n    pool_2 = GlobalMaxPooling1D()(cnn_1)\n    \n    dense_1 = Dense(dense_2_size, activation = 'relu')(pool_2)\n    dense_1 = Dropout(.2)(dense_1)\n    output_1 = Dense(y.shape[1], activation = 'softmax', \n                     kernel_regularizer = 'l2', bias_regularizer = None)(dense_1)\n    \n    model = Model(cnn_input_1, output_1)\n\n    model.compile(optimizer = Adam(learning_rate=lr), \n                  metrics = ['accuracy'], loss = 'categorical_crossentropy')\n    return model\n\ndef cnn_dual_model():\n    input_1 = Input(shape = (X.shape[1],))\n    dense_1 = Dense(dense_1_size, activation = 'relu')(input_1)\n    dense_1 = Dropout(.25)(dense_1)\n    #normed_1 = BatchNormalization()(dense_1)\n    \n    cnn_input_1 = Input(shape = (X_seq.shape[1],))\n    embed_1 = Embedding(output_dim = wvec.shape[1],\n                        weights = [wvec],\n                        #mask_zero = True,\n                        input_dim = len(tknzr.word_index) + 1, \n                        input_length = X_seq.shape[1])(cnn_input_1)\n    cnn_1 = Conv1D(cnn_1_size, 3, activation = 'relu', \n                   kernel_regularizer='l2')(embed_1)\n    pool_1 = GlobalMaxPooling1D()(cnn_1)\n    \n    concat_1 = Concatenate()([dense_1, pool_1])\n    dense_2 = Dense(dense_2_size, activation = 'relu')(concat_1)\n    dense_2 = Dropout(.25)(dense_2)\n    output_1 = Dense(y.shape[1], activation = 'softmax', \n                     kernel_regularizer = 'l2', bias_regularizer = 'l2')(dense_2) \n    \n    model = Model([input_1, cnn_input_1], output_1)\n\n    model.compile(optimizer = opm, \n                  metrics = ['accuracy'], loss = 'categorical_crossentropy')\n    return model\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score, classification_report, roc_curve\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import ClusterCentroids\n\n\ndo_logit = False\ndo_svm = False\n\n#sm = SMOTE(k_neighbors = 5)\n\nmax_epochs = 200\nbatch_size = 512\npatience = 5\nclass_weight = {0 : 1, 1 : 1, \n                       2 : 1, \n                4 : 1, 3 : 1,}\n\nes = EarlyStopping(monitor = 'val_accuracy', patience = patience, min_delta = -.001,\n                   restore_best_weights = True)\nprint('fit')\n\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\ncc = ClusterCentroids(random_state=0)\nfor i, (train_indices, test_indices) in enumerate(sss.split(X, y)):\n    print(i)\n    print()\n    \n    X_train = X[train_indices] #.to_numpy().reshape(-1, 1)\n    X_other_train = X_other[train_indices]\n    X_seq_train = X_seq[train_indices]\n    y_train = y[train_indices]\n    \n    X_test = X[test_indices] #.to_numpy().reshape(-1, 1)\n    X_other_test = X_other[test_indices]\n    X_seq_test = X_seq[test_indices]\n    y_test = y[test_indices]\n    \n    \"\"\"\n    print(X_train.shape, y_train.shape)\n    X_train, y_train = undersample_Xy(X_train, y_train)\n    print(X_train.shape, y_train.shape)\n    \"\"\"\n    \"\"\"\n    if do_logit:\n        print()\n        print('\\tlogit')\n        \n        model = LogisticRegression(n_jobs = 3, verbose = True, max_iter = 400).fit(X_train, y_train.argmax(axis = 1))\n        y_hat, y_preds = model.predict(X_test), model.predict_proba(X_test)\n    \n        print('\\t\\t', f1_score(y_test.argmax(axis = 1), y_hat, average = 'weighted'), \n                    accuracy_score(y_test.argmax(axis = 1), y_hat))\n        print(classification_report(y_test.argmax(axis = 1), y_hat))\n\n        \n    if do_svm:\n        print()\n        print('\\tsvm')\n        model = SVC(verbose = True).fit(X_train, y_train.argmax(axis = 1))\n        y_hat = model.predict(X_test)\n        print('\\t\\t', f1_score(y_test.argmax(axis = 1), y_hat, average = 'weighted'), \n              accuracy_score(y_test.argmax(axis = 1), y_hat))\n    \"\"\"\n    if do_logit or logit is None:\n        print()\n        print()\n        print('\\tlogit')\n        logit = logit_model()\n        logit.fit(X_train, y_train, epochs = max_epochs, \n              batch_size = batch_size, \n              class_weight = class_weight,\n              validation_split=0.1, callbacks = [es], \n              verbose = 1)\n\n    \"\"\"\n    print()\n    print()\n    print('\\tNN')\n    n_model = nn_model()\n    n_model.fit(X_train, y_train, epochs = max_epochs, \n              batch_size = batch_size, \n              class_weight = class_weight,\n              validation_split=0.1, callbacks = [es], \n              verbose = 1)    \n    \n    print('\\n\\n')\n    print('\\tCNN')\n    c_model = cnn_model()\n    c_model.fit(X_seq_train, y_train, epochs = max_epochs, \n              batch_size = batch_size, \n              class_weight = class_weight,\n              validation_split=0.1, callbacks = [es], \n              verbose = 1)    \n    \"\"\"\n    print('\\n\\n')\n    print('\\tdual NN/CNN')\n    d_model = cnn_dual_model()\n    d_model.fit([X_train, X_seq_train], y_train, epochs = max_epochs, \n              batch_size = batch_size, \n              class_weight = class_weight,\n              validation_split=0.1, callbacks = [es], \n              verbose = 1)\n    \n    continue\n    print('\\n\\n')\n    print('\\tRNN') \n    r_model = rnn_model()\n    r_model.fit(X_seq_train, y_train, epochs = max_epochs, \n              batch_size = batch_size, \n              class_weight = class_weight,\n              validation_split=0.1, callbacks = [es], \n              verbose = 1)\n    y_hat = model.predict(X_seq_test)\n    \n    print('\\t\\t', f1_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1), average = 'weighted'),\n                  accuracy_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\n    print(classification_report(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\n\n    \nprint()\nprint()\n\nprint('logit')\ny_hat = logit.predict(X_test)\nprint('\\t\\t', f1_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1), average = 'weighted'), \n                  accuracy_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\n\nprint('nn')\ny_hat = n_model.predict(X_test)\nprint('\\t\\t', f1_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1), average = 'weighted'), \n                  accuracy_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\n\nprint('cnn')\ny_hat = c_model.predict(X_seq_test)\nprint('\\t\\t', f1_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1), average = 'weighted'),\n                  accuracy_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\n\nprint('dual')\ny_hat = d_model.predict([X_test, X_seq_test])\nprint('\\t\\t', f1_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1), average = 'weighted'),\n                  accuracy_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('logit')\ny_hat = logit.predict(X_test)\nprint('\\t\\t', f1_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1), average = 'weighted'), \n                  accuracy_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\nprint(classification_report(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\n    \n\nprint('nn')\ny_hat = n_model.predict(X_test)\nprint('\\t\\t', f1_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1), average = 'weighted'), \n                  accuracy_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\nprint(classification_report(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\n\n\nprint('cnn')\ny_hat = c_model.predict(X_seq_test)\nprint('\\t\\t', f1_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1), average = 'weighted'),\n                  accuracy_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\nprint(classification_report(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\n\nprint('dual')\ny_hat = d_model.predict([X_test, X_seq_test])\nprint('\\t\\t', f1_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1), average = 'weighted'),\n                  accuracy_score(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))\nprint(classification_report(y_test.argmax(axis = 1), y_hat.argmax(axis = 1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['Sentiment'] = d_model.predict([X_final, X_seq_final]).argmax(axis = 1)\ndf_final = df_test[['PhraseId', 'Sentiment']]\ndf_test.head()\nprint(classification_report(df_test.Score, df_test.Sentiment))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_final.to_csv('submit.csv', index = False)\ndf_final.head()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}