{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#the basics\nimport os, re, math, string, pandas as pd, numpy as np, seaborn as sns\n\n#graphing\nimport matplotlib.pyplot as plt\n\n#deep learning\nimport tensorflow as tf\n\n#nlp\nfrom wordcloud import STOPWORDS\n\n#scaling\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#choose batch size\nBATCH_SIZE = 16\n\n#how many epochs?\nEPOCHS = 2\n\n#clean Tweets?\nCLEAN_TWEETS = False\n\n#use meta data?\nUSE_META = False\n\n#add dense layer?\nADD_DENSE = True\nDENSE_DIM = 64\n\n#add dropout?\nADD_DROPOUT = True\nDROPOUT = .2\n\n#train BERT base model? \nTRAIN_BASE = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get data\ntrain = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip',sep='\\t')\ntest = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip',sep='\\t')\n\n#peek at train\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save ID\ntest_id = pd.DataFrame()\ntest_id = test['PhraseId']\ntest_id = test['SentenceId']\n\n#drop from train and test\ncolumns = {'PhraseId', 'SentenceId'}\ntrain = train.drop(columns = columns)\ntest = test.drop(columns = columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BERT\nfrom transformers import BertTokenizer\nTOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nenc = TOKENIZER.encode(\"Encode me!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFBertModel, BertModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bert_encode(data,maximum_len) :\n    input_ids = []\n    attention_masks = []\n  \n\n    for i in range(len(data.Phrase)):\n        encoded = TOKENIZER.encode_plus(data.Phrase[i],\n                                        add_special_tokens=True,\n                                        max_length=maximum_len,\n                                        pad_to_max_length=True,\n                                        return_attention_mask=True)\n      \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n        \n    return np.array(input_ids),np.array(attention_masks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(model_layer, learning_rate, use_meta = USE_META, add_dense = ADD_DENSE,\n               dense_dim = DENSE_DIM, add_dropout = ADD_DROPOUT, dropout = DROPOUT):\n    \n    #define inputs\n    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n    #meta_input = tf.keras.Input(shape = (meta_train.shape[1], ))\n    \n    #insert BERT layer\n    transformer_layer = model_layer([input_ids,attention_masks])\n    \n    #choose only last hidden-state\n    output = transformer_layer[1]\n    \n    #add meta data\n    if use_meta:\n        output = tf.keras.layers.Concatenate()([output, meta_input])\n    \n    #add dense relu layer\n    if add_dense:\n        print(\"Training with additional dense layer...\")\n        output = tf.keras.layers.Dense(dense_dim,activation='relu')(output)\n    \n    #add dropout\n    if add_dropout:\n        print(\"Training with dropout...\")\n        output = tf.keras.layers.Dropout(dropout)(output)\n    \n    #add final node for binary classification\n    output = tf.keras.layers.Dense(5,activation='softmax')(output)\n    \n    #assemble and compile\n    if use_meta:\n        print(\"Training with meta-data...\")\n        model = tf.keras.models.Model(inputs = [input_ids,attention_masks, meta_input],outputs = output)\n    \n    else:\n        print(\"Training without meta-data...\")\n        model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n\n    model.compile(tf.keras.optimizers.Adam(lr=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n#define conveient training function to visualize learning curves\ndef plot_learning_curves(history): \n    fig, ax = plt.subplots(1, 2, figsize = (20, 10))\n\n    ax[0].plot(history.history['accuracy'], color='#171820')\n    ax[0].plot(history.history['val_accuracy'], '#fdc029')\n\n    ax[1].plot(history.history['loss'], color='#171820')\n    ax[1].plot(history.history['val_loss'], '#fdc029')\n\n    ax[0].legend(['train', 'validation'], loc = 'upper left')\n    ax[1].legend(['train', 'validation'], loc = 'upper left')\n\n    fig.suptitle(\"Model Learning Curves\", fontsize=14)\n\n    ax[0].set_ylabel('Accuracy')\n    ax[0].set_xlabel('Epoch')\n    ax[1].set_ylabel('Loss')\n    ax[1].set_xlabel('Epoch')\n\n    return plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get BERT layer\nbert_large = TFBertModel.from_pretrained('bert-large-uncased')\n#bert_base = BertModel.from_pretrained('bert-large-uncased')          #to use with PyTorch\n\n#select BERT tokenizer\nTOKENIZER = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n\n#get our inputs\ntrain_input_ids,train_attention_masks = bert_encode(train,60)\ntest_input_ids,test_attention_masks = bert_encode(test,60)\n\n#debugging step\nprint('Train length:', len(train_input_ids))\nprint('Test length:', len(test_input_ids))\n\n#and build and view parameters\nBERT_large = build_model(bert_large, learning_rate = 1e-5)\nBERT_large.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint('large_model.h5', monitor='val_loss', save_best_only = True, save_weights_only = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train BERT\nif USE_META:\n    history = BERT_large.fit([train_input_ids,train_attention_masks, meta_train], train.Sentiment, validation_split = .2, epochs = EPOCHS, callbacks = [checkpoint], batch_size = BATCH_SIZE)\n    \nelse:\n    history = BERT_large.fit([train_input_ids,train_attention_masks], train.Sentiment, validation_split = .2, epochs = EPOCHS, callbacks = [checkpoint], batch_size = BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BERT_large.load_weights('large_model.h5') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predict with BERT\nif USE_META:\n    preds_large = BERT_large.predict([test_input_ids,test_attention_masks,meta_test])\n\nelse:\n    preds_large = BERT_large.predict([test_input_ids,test_attention_masks]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls=[]\nfor i in preds_large:\n    ls.append(np.argmax(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#save as dataframe\ntest = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip',sep='\\t')\nsubmission_large = test.copy()\nsubmission_large['Sentiment'] = ls\n\nsubmission_large.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_large = submission_large[['PhraseId', 'Sentiment']]\n\n#save to disk\nsubmission_large.to_csv('submission_bert_large.csv', index = False)\nprint('Submission saved')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}