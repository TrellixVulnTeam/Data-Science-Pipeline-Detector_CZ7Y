{"cells":[{"metadata":{"_uuid":"fbb57dbeb261bc13bc2ef14d3ac680698d1fdf06"},"cell_type":"markdown","source":"**Brief about the problem: **\n> We have been given a dataset which contains audio embeddings(sound of a video in form of numbers). It also contains youtube video id and start and end time seconds for that particular audio embedding i.e. the audio that is played between that start and end time is represeted in audio embedding. Now only Audio embdding feature is of use to solve probelm in hand and target feature 'isTurkey'. Now the audio embedding in each observation may or may not be of turkey. So we have to train our model on each audio embedding to classify which audio embedding is of turkey."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Let's load in some basics and make sure our files are all here\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport math\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nprint(os.listdir(\"../input\"))\n\nfrom keras import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import *\nfrom keras.models import Sequential,Model\nfrom keras.layers import *\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_json('../input/train.json')\ntest = pd.read_json('../input/test.json')\nsample_submission = pd.read_csv('../input/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c0ff8f5a2e049f5761d47468d65fd2b6b456ebb"},"cell_type":"markdown","source":"Our files are here and load without error. Let's check to make sure the columns match and see the submission format."},{"metadata":{"trusted":true,"_uuid":"1af04552eed6d0505deb35d1bdf10a72b4d092f2"},"cell_type":"code","source":"print(test.columns)\nprint(train.columns)\nprint(sample_submission.head(4))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13b80d7e02b0e26ced2aa27bedde15b466654957"},"cell_type":"markdown","source":"How much data is there?"},{"metadata":{"trusted":true,"_uuid":"c2a1c076d7f3174afadb3d1de3c3d10c2ae8e590"},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"803583ac2c768df3afa08ae0fd6087d05b61ee91"},"cell_type":"markdown","source":"Let's find a row that is labeled with \"is_turkey\" and play the clip it comes from."},{"metadata":{"trusted":true,"_uuid":"a75d481473aa99a159e26e5db6b1a78a0480620c"},"cell_type":"code","source":"print(train[train['is_turkey']==1].index)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e76eb7b63a972cdc3cd5d8508ab2ee746fcf66d"},"cell_type":"markdown","source":"Yep, that sounds like a turkey!\n\nWe also have some VGGish audio embeddings to take a look at. Let's see what the shape of those are."},{"metadata":{"trusted":true,"_uuid":"4acf2c1fdb041b24307abe418a50c47a3ce5b619","scrolled":true},"cell_type":"code","source":"print(train['audio_embedding'].head())\n\n#see the possible list lengths of the first dimension\nprint(\"train's audio_embedding have this many frames(seconds): \"+ str(train['audio_embedding'].apply(lambda x: len(x)).unique())) \nprint(\"test's audio_embedding have this many frames(seconds): \"+ str(test['audio_embedding'].apply(lambda x: len(x)).unique())) \n\n#see the possible list lengths of the first element\nprint(\"each frame has this many features: \"+str(train['audio_embedding'].apply(lambda x: len(x[0])).unique()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f549d90ea0c5eca5268a65c5697cc0d90c84539b"},"cell_type":"markdown","source":"Note that the difference between start and end time seconds youtube clip is same as length of audio embedding for each observation."},{"metadata":{"_uuid":"500a2e40c736949a12131fe996d520dc86041f9e"},"cell_type":"markdown","source":"For more information on what these features are and where they come from, see [this page.](https://github.com/tensorflow/models/tree/master/research/audioset#input-audio-features)\n\nNow that we've seen what the given data looks like, let's make a prediction using a basic LSTM model. "},{"metadata":{"_uuid":"b0790ca9da358c2b0870f85793473843188b0371"},"cell_type":"markdown","source":"Defining Attention layer:"},{"metadata":{"trusted":true,"_uuid":"6e72efecc78ff438de496c40f43d11bb8e68f36b"},"cell_type":"code","source":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1817e73d80ad4e28c293089c02d55b349b24197"},"cell_type":"code","source":"def get_model1():\n    model = Sequential()\n    model.add(BatchNormalization(input_shape=(10, 128)))\n    model.add(Bidirectional(GRU(128, dropout=0.4, recurrent_dropout=0.4, activation='relu', return_sequences=True)))\n    model.add(Attention(10))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4305009cf21c05954a86dacaf34de7e62f388c7"},"cell_type":"code","source":"xtrain = [k for k in train['audio_embedding']]\ntest_data = test['audio_embedding'].tolist()\nytrain = train['is_turkey'].values\n# Pad the audio features so that all are \"10 seconds\" long\nx_train = pad_sequences(xtrain,maxlen=10)\nx_test = pad_sequences(test_data,maxlen=10)\n\nkf = KFold(n_splits=10, shuffle=True, random_state=42069)\npreds = []\nfold = 0\naucs = 0\nfor train_idx, val_idx in kf.split(x_train):\n    x_train_f = x_train[train_idx]\n    y_train_f = ytrain[train_idx]\n    x_val_f = x_train[val_idx]\n    y_val_f = ytrain[val_idx]\n    model = get_model1()\n    model.fit(x_train_f, y_train_f,\n              batch_size=50,\n              epochs=16,\n              verbose = 0,\n              validation_data=(x_val_f, y_val_f))\n    # Get accuracy of model on validation data. It's not AUC but it's something at least!\n    preds_val = model.predict([x_val_f], batch_size=512)\n    preds.append(model.predict(x_test))\n    fold+=1\n    fpr, tpr, thresholds = roc_curve(y_val_f, preds_val, pos_label=1)\n    aucs += auc(fpr,tpr)\n    print('Fold {}, AUC = {}'.format(fold,auc(fpr, tpr)))\nprint(\"Cross Validation AUC = {}\".format(aucs/10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aac018a6eddfddcf04434c49d43bca759cd141a5"},"cell_type":"markdown","source":"Finally, let's get a prediction to submit. "},{"metadata":{"trusted":true,"_uuid":"d49025f0a75fc36da1f858237233280803e6056a"},"cell_type":"code","source":"test_data = [k for k in test['audio_embedding']]\nsubmission = model.predict_classes(pad_sequences(test_data))\nsubmission = pd.DataFrame({'vid_id':test['vid_id'].values,'is_turkey':[x for y in submission for x in y]})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"374b17df3212371cd7d43d5b0d130e2506da1173"},"cell_type":"code","source":"print(submission.head()) #check to see that it looks like the sample submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4f1308516d4107903af92b3c6d2dea7a027e7ad"},"cell_type":"code","source":"submission.to_csv('lstm_starter.csv', index=False) #drop the index so it matches the submission format.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}