{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMC-solution","metadata":{}},{"cell_type":"markdown","source":"## Dependencies and imports","metadata":{}},{"cell_type":"code","source":"!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n\n!mkdir -p pretrained/checkpoints\n!cp /kaggle/input/imc2022-dependencies/pretrained/dkm.pth pretrained/checkpoints/dkm_base_v11.pth\n\n!pip install -f /kaggle/input/imc2022-dependencies/wheels --no-index einops\n!cp -r /kaggle/input/imc2022-dependencies/DKM/ /kaggle/working/DKM/\n!cd /kaggle/working/DKM/; pip install -f /kaggle/input/imc2022-dependencies/wheels -e . \n\n!cp -a ../input/super-glue-pretrained-network ./\n!mv ./super-glue-pretrained-network/models ./super-glue-pretrained-network/models_superglue","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:21:25.304107Z","iopub.execute_input":"2022-06-02T16:21:25.304375Z","iopub.status.idle":"2022-06-02T16:22:26.548378Z","shell.execute_reply.started":"2022-06-02T16:21:25.304308Z","shell.execute_reply":"2022-06-02T16:22:26.547064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/einops')\nsys.path.append('/kaggle/input/imc2022-dependencies/DKM/')\nsys.path.append('../input/aslfeat')\nsys.path.append(\"./super-glue-pretrained-network\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport csv\nimport time\nimport math\nimport random\n\nfrom glob import glob\nfrom tqdm import tqdm\nfrom collections import namedtuple\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport yaml\n\nimport matplotlib\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\nimport kornia as K\nimport kornia.feature as KF\nfrom kornia_moons.feature import *\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchvision.models import ResNet\nfrom torchvision.models import resnet as tv_resnet\nfrom torchvision.transforms.functional import InterpolationMode\n\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\n\nfrom einops import rearrange\nfrom models import get_model\nfrom models_superglue.matching import Matching\nfrom models_superglue.utils import (compute_pose_error, compute_epipolar_error,\n                          estimate_pose, make_matching_plot,\n                          error_colormap, AverageTimer, pose_auc, read_image,\n                          rotate_intrinsics, rotate_pose_inplane,\n                          scale_intrinsics)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:23:18.930657Z","iopub.execute_input":"2022-06-02T16:23:18.931004Z","iopub.status.idle":"2022-06-02T16:23:18.959844Z","shell.execute_reply.started":"2022-06-02T16:23:18.930936Z","shell.execute_reply":"2022-06-02T16:23:18.958968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DKM code\n\nThe reason of placing it here is that I forced to rewrite it in some places in order to fix image resizing in the model itself.","metadata":{}},{"cell_type":"code","source":"def get_tuple_transform_ops(resize=None, normalize=True, unscale=False):\n    ops = []\n    if resize:\n        ops.append(TupleResize(resize))\n    if normalize:\n        ops.append(TupleToTensorScaled())\n        ops.append(\n            TupleNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        )  # Imagenet mean/std\n    else:\n        if unscale:\n            ops.append(TupleToTensorUnscaled())\n        else:\n            ops.append(TupleToTensorScaled())\n    return TupleCompose(ops)\n\n\nclass ToTensorScaled(object):\n    \"\"\"Convert a RGB PIL Image to a CHW ordered Tensor, scale the range to [0, 1]\"\"\"\n\n    def __call__(self, im):\n        if not isinstance(im, torch.Tensor):\n            im = np.array(im, dtype=np.float32).transpose((2, 0, 1))\n            im /= 255.0\n            return torch.from_numpy(im)\n        else:\n            return im\n\n    def __repr__(self):\n        return \"ToTensorScaled(./255)\"\n\n\nclass TupleToTensorScaled(object):\n    def __init__(self):\n        self.to_tensor = ToTensorScaled()\n\n    def __call__(self, im_tuple):\n        return [self.to_tensor(im) for im in im_tuple]\n\n    def __repr__(self):\n        return \"TupleToTensorScaled(./255)\"\n\n\nclass ToTensorUnscaled(object):\n    \"\"\"Convert a RGB PIL Image to a CHW ordered Tensor\"\"\"\n\n    def __call__(self, im):\n        return torch.from_numpy(np.array(im, dtype=np.float32).transpose((2, 0, 1)))\n\n    def __repr__(self):\n        return \"ToTensorUnscaled()\"\n\n\nclass TupleToTensorUnscaled(object):\n    \"\"\"Convert a RGB PIL Image to a CHW ordered Tensor\"\"\"\n\n    def __init__(self):\n        self.to_tensor = ToTensorUnscaled()\n\n    def __call__(self, im_tuple):\n        return [self.to_tensor(im) for im in im_tuple]\n\n    def __repr__(self):\n        return \"TupleToTensorUnscaled()\"\n\n\nclass TupleResize(object):\n    def __init__(self, size, mode=InterpolationMode.BICUBIC):\n        self.size = size\n        self.resize = transforms.Resize(size, mode)\n\n    def __call__(self, im_tuple):\n        return [self.resize(im) for im in im_tuple]\n\n    def __repr__(self):\n        return \"TupleResize(size={})\".format(self.size)\n\n\nclass TupleNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n        self.normalize = transforms.Normalize(mean=mean, std=std)\n\n    def __call__(self, im_tuple):\n        return [self.normalize(im) for im in im_tuple]\n\n    def __repr__(self):\n        return \"TupleNormalize(mean={}, std={})\".format(self.mean, self.std)\n\n\nclass TupleCompose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, im_tuple):\n        for t in self.transforms:\n            im_tuple = t(im_tuple)\n        return im_tuple\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \"(\"\n        for t in self.transforms:\n            format_string += \"\\n\"\n            format_string += \"    {0}\".format(t)\n        format_string += \"\\n)\"\n        return format_string\n\n\nclass ConvRefiner(nn.Module):\n    def __init__(\n        self,\n        in_dim=6,\n        hidden_dim=16,\n        out_dim=2,\n        bicubic_gradients=False,\n        dw=False,\n        residual=False,\n        groups=1,\n        kernel_size=5,\n        hidden_blocks=3,\n        weight_norm=False,\n    ):\n        super().__init__()\n        self.block1 = self.create_block(\n            in_dim, hidden_dim, dw=dw, groups=groups, kernel_size=kernel_size\n        )\n        self.hidden_blocks = nn.Sequential(\n            *[\n                self.create_block(\n                    hidden_dim,\n                    hidden_dim,\n                    dw=dw,\n                    groups=groups,\n                    kernel_size=kernel_size,\n                    weight_norm=weight_norm,\n                )\n                for hb in range(hidden_blocks)\n            ]\n        )\n        self.out_conv = nn.Conv2d(hidden_dim, out_dim, 1, 1, 0)\n        self.bicubic_gradients = bicubic_gradients\n        self.residual = residual\n\n    def create_block(\n        self,\n        in_dim,\n        out_dim,\n        dw=False,\n        kernel_size=5,\n        groups=1,\n        residual=False,\n        weight_norm=False,\n    ):\n        num_groups = 1 if not dw else in_dim\n        if dw:\n            assert (\n                out_dim % in_dim == 0\n            ), \"outdim must be divisible by indim for depthwise\"\n        conv1 = nn.Conv2d(\n            in_dim,\n            out_dim,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=kernel_size // 2,\n            groups=num_groups,\n        )\n        norm = nn.BatchNorm2d(out_dim)\n        relu = nn.ReLU(inplace=True)\n        conv2 = nn.Conv2d(out_dim, out_dim, 1, 1, 0)\n        return nn.Sequential(conv1, norm, relu, conv2)\n\n    def forward(self, x, y, flow):\n        \"\"\"Computes the relative refining displacement in pixels for a given image x,y and a coarse flow-field between them\n        Args:\n            x ([type]): [description]\n            y ([type]): [description]\n            flow ([type]): [description]\n        Returns:\n            [type]: [description]\n        \"\"\"\n        if self.bicubic_gradients:\n            x_hat = F.grid_sample(y, flow.permute(0, 2, 3, 1), align_corners=False)\n        else:\n            with torch.no_grad():\n                x_hat = F.grid_sample(\n                    y, flow.permute(0, 2, 3, 1), align_corners=False\n                )  # Propagating gradients through grid_sample seems to be unstable :/\n        d = torch.cat((x, x_hat), dim=1)\n        d = self.block1(d)\n        d = self.hidden_blocks(d)\n        d = self.out_conv(d)\n        certainty, displacement = d[:, :-2], d[:, -2:]\n        return certainty, displacement\n\n\nclass CosKernel(nn.Module):  # similar to softmax kernel\n    def __init__(self, T, learn_temperature=False):\n        super().__init__()\n        self.learn_temperature = learn_temperature\n        if self.learn_temperature:\n            self.T = nn.Parameter(torch.tensor(T))\n        else:\n            self.T = T\n\n    def __call__(self, x, y, eps=1e-6):\n        c = torch.einsum(\"bnd,bmd->bnm\", x, y) / (\n            x.norm(dim=-1)[..., None] * y.norm(dim=-1)[:, None] + eps\n        )\n        if self.learn_temperature:\n            T = self.T.abs() + 0.01\n        else:\n            T = torch.tensor(self.T, device=c.device)\n        K = ((c - 1.0) / T).exp()\n        return K\n\n\nclass CAB(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(CAB, self).__init__()\n        self.global_pooling = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = nn.Conv2d(\n            in_channels, out_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.relu = nn.ReLU()\n        self.conv2 = nn.Conv2d(\n            out_channels, out_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.sigmod = nn.Sigmoid()\n\n    def forward(self, x):\n        x1, x2 = x  # high, low (old, new)\n        x = torch.cat([x1, x2], dim=1)\n        x = self.global_pooling(x)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.sigmod(x)\n        x2 = x * x2\n        res = x2 + x1\n        return res\n\n\nclass RRB(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3):\n        super(RRB, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_channels, out_channels, kernel_size=1, stride=1, padding=0\n        )\n        self.conv2 = nn.Conv2d(\n            out_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=kernel_size // 2,\n        )\n        self.relu = nn.ReLU()\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.conv3 = nn.Conv2d(\n            out_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=kernel_size // 2,\n        )\n\n    def forward(self, x):\n        x = self.conv1(x)\n        res = self.conv2(x)\n        res = self.bn(res)\n        res = self.relu(res)\n        res = self.conv3(res)\n        return self.relu(x + res)\n\n\nclass DFN(nn.Module):\n    def __init__(\n        self,\n        internal_dim,\n        feat_input_modules,\n        pred_input_modules,\n        rrb_d_dict,\n        cab_dict,\n        rrb_u_dict,\n        use_global_context=False,\n        global_dim=None,\n        terminal_module=None,\n        upsample_mode=\"bilinear\",\n        align_corners=False,\n    ):\n        super().__init__()\n        if use_global_context:\n            assert (\n                global_dim is not None\n            ), \"Global dim must be provided when using global context\"\n        self.align_corners = align_corners\n        self.internal_dim = internal_dim\n        self.feat_input_modules = feat_input_modules\n        self.pred_input_modules = pred_input_modules\n        self.rrb_d = rrb_d_dict\n        self.cab = cab_dict\n        self.rrb_u = rrb_u_dict\n        self.use_global_context = use_global_context\n        if use_global_context:\n            self.global_to_internal = nn.Conv2d(global_dim, self.internal_dim, 1, 1, 0)\n            self.global_pooling = nn.AdaptiveAvgPool2d(1)\n        self.terminal_module = (\n            terminal_module if terminal_module is not None else nn.Identity()\n        )\n        self.upsample_mode = upsample_mode\n        self._scales = [int(key) for key in self.terminal_module.keys()]\n\n    def scales(self):\n        return self._scales.copy()\n\n    def forward(self, new_stuff, feats, old_stuff, key):\n        feats = self.feat_input_modules[str(key)](feats)\n        new_stuff = torch.cat([feats, new_stuff], dim=1)\n        new_stuff = self.rrb_d[str(key)](new_stuff)\n        new_old_stuff = self.cab[str(key)](\n            [old_stuff, new_stuff]\n        )  # TODO: maybe use cab output upwards in network instead of rrb_u?\n        new_old_stuff = self.rrb_u[str(key)](new_old_stuff)\n        preds = self.terminal_module[str(key)](new_old_stuff)\n        pred_coord = preds[:, -2:]\n        pred_certainty = preds[:, :-2]\n        return pred_coord, pred_certainty, new_old_stuff\n\n\nclass GP(nn.Module):\n    def __init__(\n        self,\n        kernel,\n        T=1,\n        learn_temperature=False,\n        only_attention=False,\n        gp_dim=64,\n        basis=\"fourier\",\n        covar_size=5,\n        only_nearest_neighbour=False,\n        sigma_noise=0.1,\n        no_cov=False,\n    ):\n        super().__init__()\n        self.K = kernel(T=T, learn_temperature=learn_temperature)\n        self.sigma_noise = sigma_noise\n        self.covar_size = covar_size\n        self.pos_conv = torch.nn.Conv2d(2, gp_dim, 1, 1)\n        self.only_attention = only_attention\n        self.only_nearest_neighbour = only_nearest_neighbour\n        self.basis = basis\n        self.no_cov = no_cov\n\n    def get_local_cov(self, cov):\n        K = self.covar_size\n        b, h, w, h, w = cov.shape\n        hw = h * w\n        cov = F.pad(cov, 4 * (K // 2,))  # pad v_q\n        delta = torch.stack(\n            torch.meshgrid(\n                torch.arange(-(K // 2), K // 2 + 1), torch.arange(-(K // 2), K // 2 + 1)\n            ),\n            dim=-1,\n        )\n        positions = torch.stack(\n            torch.meshgrid(\n                torch.arange(K // 2, h + K // 2), torch.arange(K // 2, w + K // 2)\n            ),\n            dim=-1,\n        )\n        neighbours = positions[:, :, None, None, :] + delta[None, :, :]\n        points = torch.arange(hw)[:, None].expand(hw, K**2)\n        local_cov = cov.reshape(b, hw, h + K - 1, w + K - 1)[\n            :,\n            points.flatten(),\n            neighbours[..., 0].flatten(),\n            neighbours[..., 1].flatten(),\n        ].reshape(b, h, w, K**2)\n        return local_cov\n\n    def reshape(self, x):\n        return rearrange(x, \"b d h w -> b (h w) d\")\n\n    def project_to_basis(self, x):\n        if self.basis == \"fourier\":\n            return torch.cos(8 * math.pi * self.pos_conv(x))\n        else:\n            raise ValueError(\n                \"No other bases other than fourier currently supported in public release\"\n            )\n\n    def get_pos_enc(self, y):\n        b, c, h, w = y.shape\n        coarse_coords = torch.meshgrid(\n            (\n                torch.linspace(-1 + 1 / h, 1 - 1 / h, h, device=y.device),\n                torch.linspace(-1 + 1 / w, 1 - 1 / w, w, device=y.device),\n            )\n        )\n\n        coarse_coords = torch.stack((coarse_coords[1], coarse_coords[0]), dim=-1)[\n            None\n        ].expand(b, h, w, 2)\n        coarse_coords = rearrange(coarse_coords, \"b h w d -> b d h w\")\n        coarse_embedded_coords = self.project_to_basis(coarse_coords)\n        return coarse_embedded_coords\n\n    def forward(self, x, y, **kwargs):\n        b, c, h, w = y.shape\n        f = self.get_pos_enc(y)\n        b, d, h, w = f.shape\n        assert x.shape == y.shape\n        x, y, f = self.reshape(x), self.reshape(y), self.reshape(f)\n        K_xx = self.K(x, x)\n        K_yy = self.K(y, y)\n        K_xy = self.K(x, y)\n        K_yx = K_xy.permute(0, 2, 1)\n        sigma_noise = self.sigma_noise * torch.eye(h * w, device=x.device)[None, :, :]\n        K_yy_inv = torch.linalg.inv(K_yy + sigma_noise)\n\n        mu_x = K_xy.matmul(K_yy_inv.matmul(f))\n        mu_x = rearrange(mu_x, \"b (h w) d -> b d h w\", h=h, w=w)\n        if not self.no_cov:\n            cov_x = K_xx - K_xy.matmul(K_yy_inv.matmul(K_yx))\n            cov_x = rearrange(cov_x, \"b (h w) (r c) -> b h w r c\", h=h, w=w, r=h, c=w)\n            local_cov_x = self.get_local_cov(cov_x)\n            local_cov_x = rearrange(local_cov_x, \"b h w K -> b K h w\")\n            gp_feats = torch.cat((mu_x, local_cov_x), dim=1)\n        else:\n            gp_feats = mu_x\n        return gp_feats\n\n\nclass Encoder(nn.Module):\n    def __init__(self, resnet):\n        super().__init__()\n        self.resnet = resnet\n\n    def forward(self, x):\n        x0 = x\n\n        x = self.resnet.conv1(x)\n        x = self.resnet.bn1(x)\n        x1 = self.resnet.relu(x)\n\n        x = self.resnet.maxpool(x1)\n        x2 = self.resnet.layer1(x)\n\n        x3 = self.resnet.layer2(x2)\n\n        x4 = self.resnet.layer3(x3)\n\n        x5 = self.resnet.layer4(x4)\n\n        return {32: x5, 16: x4, 8: x3, 4: x2, 2: x1, 1: x0}\n\n\nclass Decoder(nn.Module):\n    def __init__(self, embedding_decoder, gps, proj, conv_refiner, detach=False, scales=\"all\"):\n        super().__init__()\n        self.embedding_decoder = embedding_decoder\n        self.gps = gps\n        self.proj = proj\n        self.conv_refiner = conv_refiner\n        self.detach = detach\n        if scales == \"all\":\n            self.scales = [\"32\", \"16\", \"8\", \"4\", \"2\", \"1\"]\n        else:\n            self.scales = scales\n\n    def upsample_preds(self, flow, certainty, query, support):\n        b, hs, ws, d = flow.shape\n        b, c, h, w = query.shape\n\n        flow = flow.permute(0, 3, 1, 2)\n        certainty = F.interpolate(\n            certainty, size=query.shape[-2:], align_corners=False, mode=\"bilinear\"\n        )\n        flow = F.interpolate(\n            flow, size=query.shape[-2:], align_corners=False, mode=\"bilinear\"\n        )\n        for k in range(3):\n            delta_certainty, delta_flow = self.conv_refiner[\"1\"](query, support, flow)\n            boi = delta_certainty.sigmoid()\n            delta_flow = boi * delta_flow\n            flow = torch.stack(\n                (\n                    flow[:, 0] + delta_flow[:, 0] / (4 * w),\n                    flow[:, 1] + delta_flow[:, 1] / (4 * h),\n                ),\n                dim=1,\n            )\n            certainty = (\n                certainty + delta_certainty\n            )  # predict both certainty and displacement\n        flow = flow.permute(0, 2, 3, 1)\n        return flow, certainty\n\n    def get_placeholder_flow(self, b, h, w, device):\n        coarse_coords = torch.meshgrid(\n            (\n                torch.linspace(-1 + 1 / h, 1 - 1 / h, h, device=device),\n                torch.linspace(-1 + 1 / w, 1 - 1 / w, w, device=device),\n            )\n        )\n        coarse_coords = torch.stack((coarse_coords[1], coarse_coords[0]), dim=-1)[\n            None\n        ].expand(b, h, w, 2)\n        coarse_coords = rearrange(coarse_coords, \"b h w d -> b d h w\")\n        return coarse_coords\n\n    def forward(self, f1, f2):\n        \"\"\"[summary]\n        Args:\n            f1:\n            f2:\n        Returns:\n            dict:\n                scale (s32,s16,s8,s4,s2,s1):\n                    dense_flow: (b,n,h_scale,w_scale)\n                    dense_certainty: (b,h_scale,w_scale,2)\n        \"\"\"\n        coarse_scales = self.embedding_decoder.scales()\n        all_scales = self.scales\n        sizes = {scale: f1[scale].shape[-2:] for scale in f1}\n        h, w = sizes[1]\n        b = f1[32].shape[0]\n        device = f1[32].device\n        old_stuff = torch.zeros(\n            b, self.embedding_decoder.internal_dim, *sizes[32], device=f1[32].device\n        )\n        dense_corresps = {}\n        dense_flow = self.get_placeholder_flow(b, *sizes[32], device)\n        dense_certainty = 0.0\n\n        for new_scale in all_scales:\n            ins = int(new_scale)\n            f1_s, f2_s = f1[ins], f2[ins]\n\n            if new_scale in self.proj:\n                f1_s, f2_s = self.proj[new_scale](f1_s), self.proj[new_scale](f2_s)\n\n            if ins in coarse_scales:\n                old_stuff = F.interpolate(\n                    old_stuff, size=sizes[ins], mode=\"bilinear\", align_corners=False\n                )\n                new_stuff = self.gps[new_scale](f1_s, f2_s, dense_flow=dense_flow)\n                dense_flow, dense_certainty, old_stuff = self.embedding_decoder(\n                    new_stuff, f1_s, old_stuff, new_scale\n                )\n\n            if new_scale in self.conv_refiner:\n                delta_certainty, displacement = self.conv_refiner[new_scale](\n                    f1_s, f2_s, dense_flow\n                )  # TODO: concat dense certainty?\n                dense_flow = torch.stack(\n                    (\n                        dense_flow[:, 0] + ins * displacement[:, 0] / (4 * w),\n                        dense_flow[:, 1] + ins * displacement[:, 1] / (4 * h),\n                    ),\n                    dim=1,\n                )  # multiply with scale\n                dense_certainty = (\n                    dense_certainty + delta_certainty\n                )  # predict both certainty and displacement\n\n            dense_corresps[ins] = {\n                \"dense_flow\": dense_flow,\n                \"dense_certainty\": dense_certainty,\n            }\n\n            if new_scale != \"1\":\n                dense_flow = F.interpolate(\n                    dense_flow,\n                    size=sizes[ins // 2],\n                    align_corners=False,\n                    mode=\"bilinear\",\n                )\n\n                dense_certainty = F.interpolate(\n                    dense_certainty,\n                    size=sizes[ins // 2],\n                    align_corners=False,\n                    mode=\"bilinear\",\n                )\n                if self.detach:\n                    dense_flow = dense_flow.detach()\n                    dense_certainty = dense_certainty.detach()\n        return dense_corresps\n\n\nclass RegressionMatcher(nn.Module):\n    def __init__(\n        self,\n        encoder,\n        decoder,\n        h=384,\n        w=512,\n    ):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.w_resized = w\n        self.h_resized = h\n        self.og_transforms = get_tuple_transform_ops(resize=None, normalize=True)\n\n    def extract_backbone_features(self, batch):\n        x_q = batch[\"query\"]\n        x_s = batch[\"support\"]\n        X = torch.cat((x_q, x_s))\n        feature_pyramid = self.encoder(X)\n        return feature_pyramid\n    \n    def sample(self, dense_matches, dense_certainty, num = 200, relative_confidence_threshold = 0.0):\n        matches, certainty = (\n            dense_matches.reshape(-1, 4).cpu().numpy(),\n            dense_certainty.reshape(-1).cpu().numpy(),\n        )\n        relative_confidence = certainty/certainty.max()\n        matches, certainty = (\n            matches[relative_confidence > relative_confidence_threshold],\n            certainty[relative_confidence > relative_confidence_threshold],\n        )\n        good_samples = np.random.choice(\n            np.arange(len(matches)),\n            size=min(num, len(certainty)),\n            replace=False,\n            p=certainty/np.sum(certainty),\n        )\n        return matches[good_samples], certainty[good_samples]\n\n    def forward(self, batch):\n        feature_pyramid = self.extract_backbone_features(batch)\n        f_q_pyramid = {\n            scale: f_scale.chunk(2)[0] for scale, f_scale in feature_pyramid.items()\n        }\n        f_s_pyramid = {\n            scale: f_scale.chunk(2)[1] for scale, f_scale in feature_pyramid.items()\n        }\n        dense_corresps = self.decoder(f_q_pyramid, f_s_pyramid)\n        return dense_corresps\n\n    def forward_symmetric(self, batch):\n        feature_pyramid = self.extract_backbone_features(batch)\n        f_q_pyramid = feature_pyramid\n        f_s_pyramid = {\n            scale: torch.cat((f_scale.chunk(2)[1], f_scale.chunk(2)[0]))\n            for scale, f_scale in feature_pyramid.items()\n        }\n        dense_corresps = self.decoder(f_q_pyramid, f_s_pyramid)\n        return dense_corresps\n\n    def cycle_constraint(self, query_coords, query_to_support, support_to_query):\n        dist = query_coords - F.grid_sample(\n            support_to_query, query_to_support, mode=\"bilinear\"\n        )\n        return dist\n\n    def stable_neighbours(self, query_coords, query_to_support, support_to_query):\n        qts = query_to_support\n        for t in range(4):\n            _qts = qts\n            q = F.grid_sample(support_to_query, qts, mode=\"bilinear\")\n            qts = F.grid_sample(\n                query_to_support.permute(0, 3, 1, 2),\n                q.permute(0, 2, 3, 1),\n                mode=\"bilinear\",\n            ).permute(0, 2, 3, 1)\n        d = (qts - _qts).norm(dim=-1)\n        qd = (q - query_coords).norm(dim=1)\n        stabneigh = torch.logical_and(d < 1e-3, qd < 5e-3)\n        return q, qts, stabneigh\n\n    def match(\n        self,\n        im1,\n        im2,\n        w_resize,\n        h_resize,\n        batched=False,\n        check_cycle_consistency=False,\n        do_pred_in_og_res=False,\n    ):\n        self.w_resized = w_resize\n        self.h_resized = h_resize\n        self.train(False)\n        with torch.no_grad():\n            if not batched:\n                b = 1\n                w, h = im1.size\n                w2, h2 = im2.size\n                # Get images in good format\n                ws = self.w_resized\n                hs = self.h_resized\n                test_transform = get_tuple_transform_ops(\n                    resize=(hs, ws), normalize=True\n                )\n                query, support = test_transform((im1, im2))\n                batch = {\"query\": query[None].cuda(), \"support\": support[None].cuda()}\n            else:\n                b, c, h, w = im1.shape\n                b, c, h2, w2 = im2.shape\n                assert w == w2 and h == h2, \"wat\"\n                batch = {\"query\": im1.cuda(), \"support\": im2.cuda()}\n                hs, ws = self.h_resized, self.w_resized\n            finest_scale = 1  # i will assume that we go to the finest scale (otherwise min(list(dense_corresps.keys())) also works)\n            # Run matcher\n            if check_cycle_consistency:\n                dense_corresps = self.forward_symmetric(batch)\n                query_to_support, support_to_query = dense_corresps[finest_scale][\n                    \"dense_flow\"\n                ].chunk(2)\n                query_to_support = query_to_support.permute(0, 2, 3, 1)\n                dense_certainty, dc_s = dense_corresps[finest_scale][\n                    \"dense_certainty\"\n                ].chunk(\n                    2\n                )  # TODO: Here we could also use the reverse certainty\n            else:\n                dense_corresps = self.forward(batch)\n                query_to_support = dense_corresps[finest_scale][\"dense_flow\"].permute(\n                    0, 2, 3, 1\n                )\n                # Get certainty interpolation\n                dense_certainty = dense_corresps[finest_scale][\"dense_certainty\"]\n\n            if do_pred_in_og_res:  # Will assume that there is no batching going on.\n                og_query, og_support = self.og_transforms((im1, im2))\n                query_to_support, dense_certainty = self.decoder.upsample_preds(\n                    query_to_support,\n                    dense_certainty,\n                    og_query.cuda()[None],\n                    og_support.cuda()[None],\n                )\n                hs, ws = h, w\n            # Create im1 meshgrid\n            query_coords = torch.meshgrid(\n                (\n                    torch.linspace(-1 + 1 / hs, 1 - 1 / hs, hs, device=\"cuda:0\"),\n                    torch.linspace(-1 + 1 / ws, 1 - 1 / ws, ws, device=\"cuda:0\"),\n                )\n            )\n            query_coords = torch.stack((query_coords[1], query_coords[0]))\n            query_coords = query_coords[None].expand(b, 2, hs, ws)\n            dense_certainty = dense_certainty.sigmoid()  # logits -> probs\n            if check_cycle_consistency:\n                query_coords, query_to_support, stabneigh = self.stable_neighbours(\n                    query_coords, query_to_support, support_to_query\n                )\n                dense_certainty *= stabneigh.float() + 1e-3\n            # Return only matches better than threshold\n            query_coords = query_coords.permute(0, 2, 3, 1)\n\n            query_to_support = torch.clamp(query_to_support, -1, 1)\n            if batched:\n                return torch.cat((query_coords, query_to_support), dim=-1), dense_certainty[:, 0]\n            else:\n                return torch.cat((query_coords, query_to_support), dim=-1)[0], dense_certainty[0, 0]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:23:29.501433Z","iopub.execute_input":"2022-06-02T16:23:29.501783Z","iopub.status.idle":"2022-06-02T16:23:30.007272Z","shell.execute_reply.started":"2022-06-02T16:23:29.501736Z","shell.execute_reply":"2022-06-02T16:23:30.006281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dkm_pretrained_urls = {\"base\": {\"v11\":\"https://github.com/Parskatt/storage/releases/download/dkm/dkm_base_v11.pth\"}, \"v1\": \"\"}\n\n\ndef dkm_base(pretrained=True, version=\"v11\"):\n    gp_dim = 256\n    dfn_dim = 384\n    feat_dim = 256\n    coordinate_decoder = DFN(\n        internal_dim=dfn_dim,\n        feat_input_modules=nn.ModuleDict(\n            {\n                \"32\": nn.Conv2d(512, feat_dim, 1, 1),\n                \"16\": nn.Conv2d(512, feat_dim, 1, 1),\n            }\n        ),\n        pred_input_modules=nn.ModuleDict(\n            {\n                \"32\": nn.Identity(),\n                \"16\": nn.Identity(),\n            }\n        ),\n        rrb_d_dict=nn.ModuleDict(\n            {\n                \"32\": RRB(gp_dim + feat_dim, dfn_dim),\n                \"16\": RRB(gp_dim + feat_dim, dfn_dim),\n            }\n        ),\n        cab_dict=nn.ModuleDict(\n            {\n                \"32\": CAB(2 * dfn_dim, dfn_dim),\n                \"16\": CAB(2 * dfn_dim, dfn_dim),\n            }\n        ),\n        rrb_u_dict=nn.ModuleDict(\n            {\n                \"32\": RRB(dfn_dim, dfn_dim),\n                \"16\": RRB(dfn_dim, dfn_dim),\n            }\n        ),\n        terminal_module=nn.ModuleDict(\n            {\n                \"32\": nn.Conv2d(dfn_dim, 3, 1, 1, 0),\n                \"16\": nn.Conv2d(dfn_dim, 3, 1, 1, 0),\n            }\n        ),\n    )\n    dw = True\n    hidden_blocks = 8\n    kernel_size = 5\n    conv_refiner = nn.ModuleDict(\n        {\n            \"16\": ConvRefiner(\n                2 * 512,\n                1024,\n                3,\n                kernel_size=kernel_size,\n                dw=dw,\n                hidden_blocks=hidden_blocks,\n            ),\n            \"8\": ConvRefiner(\n                2 * 512,\n                1024,\n                3,\n                kernel_size=kernel_size,\n                dw=dw,\n                hidden_blocks=hidden_blocks,\n            ),\n            \"4\": ConvRefiner(\n                2 * 256,\n                512,\n                3,\n                kernel_size=kernel_size,\n                dw=dw,\n                hidden_blocks=hidden_blocks,\n            ),\n            \"2\": ConvRefiner(\n                2 * 64,\n                128,\n                3,\n                kernel_size=kernel_size,\n                dw=dw,\n                hidden_blocks=hidden_blocks,\n            ),\n            \"1\": ConvRefiner(\n                2 * 3,\n                24,\n                3,\n                kernel_size=kernel_size,\n                dw=dw,\n                hidden_blocks=hidden_blocks,\n            ),\n        }\n    )\n    kernel_temperature = 0.2\n    learn_temperature = False\n    no_cov = True\n    kernel = CosKernel\n    only_attention = False\n    basis = \"fourier\"\n    gp32 = GP(\n        kernel,\n        T=kernel_temperature,\n        learn_temperature=learn_temperature,\n        only_attention=only_attention,\n        gp_dim=gp_dim,\n        basis=basis,\n        no_cov=no_cov,\n    )\n    gp16 = GP(\n        kernel,\n        T=kernel_temperature,\n        learn_temperature=learn_temperature,\n        only_attention=only_attention,\n        gp_dim=gp_dim,\n        basis=basis,\n        no_cov=no_cov,\n    )\n    gps = nn.ModuleDict({\"32\": gp32, \"16\": gp16})\n    proj = nn.ModuleDict(\n        {\"16\": nn.Conv2d(1024, 512, 1, 1), \"32\": nn.Conv2d(2048, 512, 1, 1)}\n    )\n    decoder = Decoder(coordinate_decoder, gps, proj, conv_refiner, detach=True)\n    h, w = 384, 512\n    encoder = Encoder(\n        tv_resnet.resnet50(pretrained=not pretrained)\n    )  # only load pretrained weights if not loading a pretrained matcher ;)\n    matcher = RegressionMatcher(encoder, decoder, h=h, w=w).cuda()\n    if pretrained:\n        weights = torch.hub.load_state_dict_from_url(dkm_pretrained_urls[\"base\"][version])\n        matcher.load_state_dict(weights)\n    return matcher","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:23:30.011386Z","iopub.execute_input":"2022-06-02T16:23:30.011862Z","iopub.status.idle":"2022-06-02T16:23:30.035896Z","shell.execute_reply.started":"2022-06-02T16:23:30.011752Z","shell.execute_reply":"2022-06-02T16:23:30.034818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ASLFeat code","metadata":{}},{"cell_type":"code","source":"# ASLFeat Functions    \ndef load_imgs(img_paths, h, w):\n    rgb_list = []\n    gray_list = []\n    \n    for img_path in img_paths:\n        img = cv2.imread(img_path)\n        img = cv2.resize(img, (w, h))\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)[..., np.newaxis]\n        img = img[..., ::-1]\n        rgb_list.append(img)\n        gray_list.append(gray)\n        \n    return rgb_list, gray_list\n\ndef extract_local_features(gray_list):    \n    descs = []\n    kpts = []\n    \n    for gray_img in gray_list:\n        desc, kpt = [], []\n        desc, kpt, _ = asl.run_test_data(gray_img)\n        descs.append(desc)\n        kpts.append(kpt)\n        \n    return descs, kpts\n\nclass MatcherWrapper(object):\n    \"\"\"OpenCV matcher wrapper.\"\"\"\n\n    def __init__(self):\n        # Swapped BFMatcher to FlannBasedMatcher\n        # FLANN parameters\n        FLANN_INDEX_KDTREE = 0\n        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 6)\n        search_params = dict(checks = 125)   # or pass empty dictionary\n        self.matcher = cv2.FlannBasedMatcher(index_params, search_params)\n    \n    def get_matches(self, feat1, feat2, cv_kpts1, cv_kpts2, ratio = 0.8, cross_check = True, err_thld = 0.5):\n        \"\"\"Compute putative and inlier matches.\n        Args:\n            feat: (n_kpts, 128) Local features.\n            cv_kpts: A list of keypoints represented as cv2.KeyPoint.\n            ratio: The threshold to apply ratio test.\n            cross_check: (True by default) Whether to apply cross check.\n            err_thld: Epipolar error threshold.\n        Returns:\n            good_matches: Putative matches.\n            mask: The mask to distinguish inliers/outliers on putative matches.\n        \"\"\"\n        \n        init_matches1 = self.matcher.knnMatch(feat1, feat2, k = 2)\n        init_matches2 = self.matcher.knnMatch(feat2, feat1, k = 2)\n\n        good_matches = []\n\n        for i in range(len(init_matches1)):\n            cond = True\n            if cross_check:\n                cond1 = cross_check and init_matches2[init_matches1[i][0].trainIdx][0].trainIdx == i\n                cond *= cond1\n            if ratio is not None and ratio < 1:\n                cond2 = init_matches1[i][0].distance <= ratio * init_matches1[i][1].distance\n                cond *= cond2\n            if cond:\n                good_matches.append(init_matches1[i][0])\n\n        if type(cv_kpts1) is list and type(cv_kpts2) is list:\n            good_kpts1 = np.array([cv_kpts1[m.queryIdx].pt for m in good_matches])\n            good_kpts2 = np.array([cv_kpts2[m.trainIdx].pt for m in good_matches])\n        elif type(cv_kpts1) is np.ndarray and type(cv_kpts2) is np.ndarray:\n            good_kpts1 = np.array([cv_kpts1[m.queryIdx] for m in good_matches])\n            good_kpts2 = np.array([cv_kpts2[m.trainIdx] for m in good_matches])\n        else:\n            good_kpts1 = np.empty(0)\n            good_kpts2 = np.empty(0)\n            \n        return good_kpts1, good_kpts2","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:23:31.612847Z","iopub.execute_input":"2022-06-02T16:23:31.613344Z","iopub.status.idle":"2022-06-02T16:23:31.637885Z","shell.execute_reply.started":"2022-06-02T16:23:31.613298Z","shell.execute_reply":"2022-06-02T16:23:31.636822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\ndef load_torch_image(fname, device):\n    img = cv2.imread(fname)\n    img = cv2.resize(img, (img.shape[1]//8*8, img.shape[0]//8*8))\n    h, w = img.shape[0], img.shape[1]\n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img.to(device), h, w\n\n\ndef draw_matches(img1, cv_kpts1, img2, cv_kpts2, good_matches, mask, match_color = (0, 255, 0), pt_color = (0, 0, 255)):\n    \"\"\"Draw matches.\"\"\"\n    if type(cv_kpts1) is np.ndarray and type(cv_kpts2) is np.ndarray:\n        cv_kpts1 = [cv2.KeyPoint(cv_kpts1[i][0], cv_kpts1[i][1], 1) for i in range(cv_kpts1.shape[0])]\n        cv_kpts2 = [cv2.KeyPoint(cv_kpts2[i][0], cv_kpts2[i][1], 1) for i in range(cv_kpts2.shape[0])]\n\n    display = cv2.drawMatches(img1, cv_kpts1, img2, cv_kpts2, good_matches,\n                              None,\n                              matchColor = match_color,\n                              singlePointColor = pt_color,\n                              matchesMask = mask.ravel().tolist(), flags=4)\n    return display","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:23:31.596933Z","iopub.execute_input":"2022-06-02T16:23:31.597259Z","iopub.status.idle":"2022-06-02T16:23:31.611071Z","shell.execute_reply.started":"2022-06-02T16:23:31.597218Z","shell.execute_reply":"2022-06-02T16:23:31.610006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models initializing","metadata":{}},{"cell_type":"markdown","source":"### LoFTR","metadata":{}},{"cell_type":"code","source":"cfg = {'backbone_type': 'ResNetFPN',\n               'resolution': (8, 2),\n               'fine_window_size': 5,\n               'fine_concat_coarse_feat': True,\n               'resnetfpn': {'initial_dim': 128, 'block_dims': [128, 196, 256]},\n               'coarse': {'d_model': 256,\n                          'd_ffn': 256,\n                          'nhead': 8,\n                          'layer_names': ['self',\n                                          'cross',\n                                          'self',\n                                          'cross',\n                                          'self',\n                                          'cross',\n                                          'self',\n                                          'cross'],\n                          'attention': 'linear',\n                          'temp_bug_fix': False},\n               'match_coarse': {'thr': 0.2,\n                                'border_rm': 1,\n                                'match_type': 'dual_softmax',\n                                'dsmax_temperature': 0.11,\n                                'skh_iters': 3,\n                                'skh_init_bin_score': 1.0,\n                                'skh_prefilter': True,\n                                'train_coarse_percent': 0.4,\n                                'train_pad_num_gt_min': 200},\n               'fine': {'d_model': 128,\n                        'd_ffn': 128,\n                        'nhead': 8,\n                        'layer_names': ['self', 'cross'],\n                        'attention': 'linear'}}\n\ndevice = torch.device('cuda')\nmatcher = KF.LoFTR(pretrained=None, config=cfg)\nmatcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\nmatcher = matcher.to(device).eval()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SuperGlue","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nresize = [-1, ]\nresize_float = True\n\nconfig = {\n    \"superpoint\": {\n        \"nms_radius\": 4,\n        \"keypoint_threshold\": 0.005,\n        \"max_keypoints\": 2048,\n    },\n    \"superglue\": {\n        \"weights\": \"outdoor\",\n        \"sinkhorn_iterations\": 20,\n        \"match_threshold\": 0.2,\n    }\n}\nmatching_sg = Matching(config).eval().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:23:29.249858Z","iopub.execute_input":"2022-06-02T16:23:29.250239Z","iopub.status.idle":"2022-06-02T16:23:29.498903Z","shell.execute_reply.started":"2022-06-02T16:23:29.250196Z","shell.execute_reply":"2022-06-02T16:23:29.497897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DKM","metadata":{}},{"cell_type":"code","source":"torch.hub.set_dir('/kaggle/working/pretrained/')\nmodel = dkm_base(pretrained=True, version=\"v11\").to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:23:30.037915Z","iopub.execute_input":"2022-06-02T16:23:30.038648Z","iopub.status.idle":"2022-06-02T16:23:31.594854Z","shell.execute_reply.started":"2022-06-02T16:23:30.038603Z","shell.execute_reply":"2022-06-02T16:23:31.593901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ASLFeat","metadata":{}},{"cell_type":"code","source":"# Import Config\nwith open('../input/aslfeat/configs/matching_eval2.yaml', 'r') as f:\n        config = yaml.load(f, Loader=yaml.FullLoader)\n\n# There are 2 checkpoints in the pretrained folder. This one should be the best...\naslfeat_model_path = '../input/aslfeat/pretrained/aslfeatv2/model.ckpt-60000' \nconfig['model_path'] = aslfeat_model_path\nconfig['net']['config']['kpt_n'] = 2048 # Sames as original config ... just for convenience ;-)\n\n# Summary config\nprint(config)\n\n# Create Model\nasl = get_model('feat_model')(aslfeat_model_path, **config['net'])\nasl_matcher = MatcherWrapper()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:23:31.641659Z","iopub.execute_input":"2022-06-02T16:23:31.642499Z","iopub.status.idle":"2022-06-02T16:23:44.107778Z","shell.execute_reply.started":"2022-06-02T16:23:31.64242Z","shell.execute_reply":"2022-06-02T16:23:44.106711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"%matplotlib inline \n\nF_dict = {}\n\nfor i, row in enumerate(test_samples):\n    \n    # Get items\n    sample_id, batch_id, image_1_id, image_2_id = row\n    st = time.time()\n    \n    # LoFTR\n    \n    image_1, h1, w1 = load_torch_image(f'{src}/test_images/{batch_id}/{image_1_id}.png', device)\n    image_2, h2, w2 = load_torch_image(f'{src}/test_images/{batch_id}/{image_2_id}.png', device)\n\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1), \n              \"image1\": K.color.rgb_to_grayscale(image_2)}\n\n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n        \n    mkpts0_lf = correspondences['keypoints0'].cpu().numpy()\n    mkpts1_lf = correspondences['keypoints1'].cpu().numpy()\n    \n    # SuperGlue\n    \n    image_fpath_1 = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n    image_fpath_2 = f'{src}/test_images/{batch_id}/{image_2_id}.png'\n    \n    image_1_sg, inp_1, scales_1 = read_image(image_fpath_1, device, [w1, h1], 0, resize_float)\n    image_2_sg, inp_2, scales_2 = read_image(image_fpath_2, device, [w2, h2], 0, resize_float)\n    \n    pred = matching_sg({\"image0\": inp_1, \"image1\": inp_2})\n    pred = {k: v[0].detach().cpu().numpy() for k, v in pred.items()}\n    kpts0, kpts1 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n    matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n\n    valid = matches > -1\n    mkpts0_sg = kpts0[valid]\n    mkpts1_sg = kpts1[matches[valid]]\n    \n    # DKM\n    \n    img1 = cv2.imread(f'{src}/test_images/{batch_id}/{image_1_id}.png') \n    img2 = cv2.imread(f'{src}/test_images/{batch_id}/{image_2_id}.png')\n    img1 = cv2.resize(img1, (w1, h1))\n    img2 = cv2.resize(img2, (w2, h2))\n\n    img1PIL = Image.fromarray(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n    img2PIL = Image.fromarray(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n    \n    dense_matches, dense_certainty = model.match(img1PIL, img2PIL, w1, h1)\n    dense_certainty = dense_certainty.sqrt()\n    sparse_matches, sparse_certainty = model.sample(dense_matches, dense_certainty, 300)\n    \n    mkps1_dkm = sparse_matches[:, :2]\n    mkps2_dkm = sparse_matches[:, 2:]\n    \n    h, w, c = img1.shape\n    mkps1_dkm[:, 0] = ((mkps1_dkm[:, 0] + 1)/2) * w\n    mkps1_dkm[:, 1] = ((mkps1_dkm[:, 1] + 1)/2) * h\n\n    h, w, c = img2.shape\n    mkps2_dkm[:, 0] = ((mkps2_dkm[:, 0] + 1)/2) * w\n    mkps2_dkm[:, 1] = ((mkps2_dkm[:, 1] + 1)/2) * h\n    \n    # ASLFeat\n    \n    rgb_list, gray_list = load_imgs([image_fpath_1, image_fpath_2], h1, w1)    \n\n    descs, kpts = extract_local_features(gray_list)\n    \n    asl_points1, asl_points2 = asl_matcher.get_matches(descs[0], descs[1], kpts[0], kpts[1],\n                                                          ratio = None, cross_check = True, err_thld = 0.20)\n    \n    # Keypoint concatenation\n    \n    mkpts0 = np.concatenate((mkpts0_lf, mkpts0_sg, mkps1_dkm, asl_points1))\n    mkpts1 = np.concatenate((mkpts1_lf, mkpts1_sg, mkps2_dkm, asl_points2))\n    \n    # Compute fundamental matrix and get inliers\n    \n    if len(mkpts0) > 7:\n        FM, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.2, 0.9999, 100000)\n        inliers = inliers > 0\n        \n        # Handling broken matrixes\n        \n        if FM is None:\n            F_dict[sample_id] = np.zeros((3, 3))\n            continue\n        elif FM.shape != (3, 3):\n            F_dict[sample_id] = np.zeros((3, 3))\n            continue\n            \n        F_dict[sample_id] = FM\n    else:\n        F_dict[sample_id] = np.zeros((3, 3))\n        continue\n        \n    gc.collect()\n    nd = time.time()   \n    \n    # Debug info and plots\n    \n    if i < 3:\n        print(\"Running time: \", nd - st, \" s\")\n        print(mkpts0_lf.shape, mkpts0_sg.shape, mkps1_dkm.shape, asl_points1.shape)\n        draw_LAF_matches(\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n        torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n        K.tensor_to_image(image_1),\n        K.tensor_to_image(image_2),\n        inliers,\n        draw_dict={'inlier_color': (0.2, 1, 0.2),\n                   'tentative_color': None, \n                   'feature_color': (0.2, 0.5, 1), 'vertical': False})\n\n# Writing submission\n\nwith open('submission.csv', 'w') as f:\n    f.write('sample_id,fundamental_matrix\\n')\n    for sample_id, FM in F_dict.items():\n        f.write(f'{sample_id},{FlattenMatrix(FM)}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:23:44.12215Z","iopub.execute_input":"2022-06-02T16:23:44.122878Z","iopub.status.idle":"2022-06-02T16:24:22.310915Z","shell.execute_reply.started":"2022-06-02T16:23:44.122828Z","shell.execute_reply":"2022-06-02T16:24:22.310054Z"},"trusted":true},"execution_count":null,"outputs":[]}]}