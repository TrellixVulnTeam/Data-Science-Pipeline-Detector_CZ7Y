{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<center>\n    <h2 style=\"color: #022047\"> An image to get in the right mood... ðŸ˜‰  </h2>\n</center>\n\n![](https://storage.googleapis.com/kaggle-media/competitions/google-image-matching/trevi-canvas-licensed-nonoderivs.jpg)","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport torch\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport torch\nif not torch.cuda.is_available():\n    print('You may want to enable the GPU switch?')\n\nINSTALLED_LOG = {}\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:53:04.326401Z","iopub.execute_input":"2022-05-31T22:53:04.326898Z","iopub.status.idle":"2022-05-31T22:53:06.137489Z","shell.execute_reply.started":"2022-05-31T22:53:04.326799Z","shell.execute_reply":"2022-05-31T22:53:06.136745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OpenGlue","metadata":{}},{"cell_type":"code","source":"%%writefile /tmp/openglue_comment_out_cell.txt\n\n# Install OmegaConf\n!cp ../input/detectron2-v06/antlr4-python3-runtime-4.8.targz_compressed /tmp/antlr4-python3-runtime-4.8.tar.gz\n!pip install /tmp/antlr4-python3-runtime-4.8.tar.gz\n!pip install ../input/detectron2-v06/omegaconf-2.1.2-py3-none-any.whl\n\n# Make include dir for OpenGlue\n!mkdir /tmp/openglue/\n!cp -r ../input/openglue/OpenGlue-main /tmp/openglue/openglue\n!touch /tmp/openglue/openglue/__init__.py\n\n# Dump pretrained checkpoints\n!mkdir -p pretrained/checkpoints\n!cp ../input/openglue-models/* pretrained/checkpoints/\n!ls pretrained/checkpoints/\n\n\nimport sys\nsys.path.append('/tmp/openglue/openglue')\nimport cv2\nimport os\nfrom omegaconf import OmegaConf\nfrom typing import Dict, Optional\nimport argparse\nimport matplotlib.pyplot as plt\n\nimport kornia as K\nimport kornia.feature as KF\nimport kornia_moons.feature as KMF\n\nimport torch\nimport torch.nn as nn\n\nimport openglue\nfrom openglue.inference import initialize_models, OpenGlueMatcher\nfrom openglue.models.features import get_feature_extractor\nfrom openglue.models.superglue.superglue import SuperGlue as OpenGlueSuperGlue\n\n\nclass OpenGlueMatcherWrapper:\n    def __init__(self, device=None):\n        torch.hub.set_dir('/kaggle/working/pretrained/')\n        local_feature_extractor = get_feature_extractor('OPENCVDoGAffNetHardNet')(max_keypoints=2048, nms_diameter=9)\n        local_feature_extractor.to(device)\n        self.device = device\n        \n        yaml_cfg_string_new = \"\"\"\n                              inference:\n                                match_threshold: 0.2\n\n                              superglue:\n                                descriptor_dim: &DESCRIPTOR_DIM 128\n                                laf_to_sideinfo_method: affine\n                                positional_encoding:\n                                  hidden_layers_sizes: [ 32, 64, 128 ]\n                                  side_info_size: 6\n                                  output_size: *DESCRIPTOR_DIM\n                                attention_gnn:\n                                  num_stages: 9\n                                  num_heads: 4\n                                  embed_dim: *DESCRIPTOR_DIM\n                                  attention: 'softmax'\n                                  use_offset: False\n                                dustbin_score_init: 1.0\n                                otp:\n                                  num_iters: 20\n                                  reg: 1.0\n                                residual: True\n                              \"\"\"\n        self.config = OmegaConf.create(yaml_cfg_string_new)\n        \n        state_dict = torch.load('../input/openglue-models/openglue_SIFT-Affnet-Hardnet.ckpt', map_location='cpu')['state_dict']\n        for key in list(state_dict.keys()):\n            state_dict[key.replace('superglue.', '')] = state_dict.pop(key)\n        superglue_matcher = OpenGlueSuperGlue(self.config['superglue'])\n        message = superglue_matcher.load_state_dict(state_dict)\n        print(message)\n        superglue_matcher.to(device)\n        \n        self._openglue_matcher = OpenGlueMatcher(local_feature_extractor, superglue_matcher, self.config)\n        \n    def load_torch_image(self, image, resize_to=None):\n        timg = K.color.bgr_to_grayscale(K.image_to_tensor(image, False) / 255.).to(self.device)\n        if resize_to is not None:\n            new_w, new_h = resize_to\n            timg = K.geometry.resize(timg, (new_h, new_w))\n        return timg\n\n    def __call__(self, img_np1, img_np2):\n        with torch.no_grad():\n            img_ts1 = self.load_torch_image(img_np1)\n            img_ts2 = self.load_torch_image(img_np2)\n            out = self._openglue_matcher({\"image0\": img_ts1, \"image1\": img_ts2})\n            mkpts0 = out['keypoints0'].cpu().numpy()\n            mkpts1 = out['keypoints1'].cpu().numpy()\n            return mkpts0, mkpts1\n        \n        \nopenglue_matcher = OpenGlueMatcherWrapper(device=device)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:53:06.140969Z","iopub.execute_input":"2022-05-31T22:53:06.141174Z","iopub.status.idle":"2022-05-31T22:53:06.152364Z","shell.execute_reply.started":"2022-05-31T22:53:06.141144Z","shell.execute_reply":"2022-05-31T22:53:06.151531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Kornia LoFTR","metadata":{}},{"cell_type":"code","source":"# Install Kornia\nforce_kornialoftr_reinstall = False\n\nif 'KorniaLoFTR' not in INSTALLED_LOG or force_kornialoftr_reinstall:\n    dry_run = False\n    !pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n    !pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n    INSTALLED_LOG['KorniaLoFTR'] = True\nelse:\n    print('Already installed KorniaLoFTR. Set \"force_kornialoftr_reinstall=True\" to override this behavior.')\n    \n\n# Import and use Kornia\nimport kornia\nimport kornia as K\nimport kornia.feature as KF\nimport kornia_moons.feature as KMF\n\n\nclass LoFTRMatcher:\n    def __init__(self, device=None, input_longside=1200, conf_th=None):\n        self._loftr_matcher = KF.LoFTR(pretrained=None)\n        self._loftr_matcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\n        self._loftr_matcher = self._loftr_matcher.to(device).eval()\n        self.device = device\n        self.input_longside = input_longside\n        self.conf_thresh = conf_th\n        \n    def prep_img(self, img, long_side=1200):\n        if long_side is not None:\n            scale = long_side / max(img.shape[0], img.shape[1]) \n            w = int(img.shape[1] * scale)\n            h = int(img.shape[0] * scale)\n            img = cv2.resize(img, (w, h))\n        else:\n            scale = 1.0\n\n        img_ts = K.image_to_tensor(img, False).float() / 255.\n        img_ts = K.color.bgr_to_rgb(img_ts)\n        img_ts = K.color.rgb_to_grayscale(img_ts)\n        return img, img_ts.to(self.device), scale\n    \n    def tta_rotation_preprocess(self, img_np, angle):\n        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n        rot_img = cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0]))\n\n        rot_img_ts = K.image_to_tensor(rot_img, False).float() / 255.\n        rot_img_ts = K.color.bgr_to_rgb(rot_img_ts)\n        rot_img_ts = K.color.rgb_to_grayscale(rot_img_ts)\n        return rot_M, rot_img_ts.to(self.device), rot_M_inv\n\n    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n        hom = np.concatenate([kpts, ones], 1)\n        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n        return rot_kpts, mask\n\n    def __call__(self, img_np1, img_np2, tta=['orig']):\n        with torch.no_grad():\n            img_np1, img_ts0, scale0 = self.prep_img(img_np1, long_side=self.input_longside)\n            img_np2, img_ts1, scale1 = self.prep_img(img_np2, long_side=self.input_longside)\n            images0, images1 = [], []\n\n            # TTA\n            for tta_elem in tta:\n                if tta_elem == 'orig':\n                    img_ts0_aug, img_ts1_aug = img_ts0, img_ts1\n                elif tta_elem == 'flip_lr':\n                    img_ts0_aug = torch.flip(img_ts0, [3, ])\n                    img_ts1_aug = torch.flip(img_ts1, [3, ])\n                elif tta_elem == 'flip_ud':\n                    img_ts0_aug = torch.flip(img_ts0, [2, ])\n                    img_ts1_aug = torch.flip(img_ts1, [2, ])\n                elif tta_elem == 'rot_r10':\n                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np1, 10)\n                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np2, 10)\n                elif tta_elem == 'rot_l10':\n                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np1, -10)\n                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np2, -10)\n                else:\n                    raise ValueError('Unknown TTA method.')\n                images0.append(img_ts0_aug)\n                images1.append(img_ts1_aug)\n\n            # Inference\n            input_dict = {\"image0\": torch.cat(images0), \"image1\": torch.cat(images1)}\n            correspondences = self._loftr_matcher(input_dict)\n            mkpts0 = correspondences['keypoints0'].cpu().numpy()\n            mkpts1 = correspondences['keypoints1'].cpu().numpy()\n            batch_id = correspondences['batch_indexes'].cpu().numpy()\n            confidence = correspondences['confidence'].cpu().numpy()\n\n            # Reverse TTA\n            for idx, tta_elem in enumerate(tta):\n                batch_mask = batch_id == idx\n\n                if tta_elem == 'orig':\n                    pass\n                elif tta_elem == 'flip_lr':\n                    mkpts0[batch_mask, 0] = img_np1.shape[1] - mkpts0[batch_mask, 0]\n                    mkpts1[batch_mask, 0] = img_np2.shape[1] - mkpts1[batch_mask, 0]\n                elif tta_elem == 'flip_ud':\n                    mkpts0[batch_mask, 1] = img_np1.shape[0] - mkpts0[batch_mask, 1]\n                    mkpts1[batch_mask, 1] = img_np2.shape[0] - mkpts1[batch_mask, 1]\n                elif tta_elem == 'rot_r10':\n                    mkpts0[batch_mask], mask0 = self.tta_rotation_postprocess(mkpts0[batch_mask], img_np1, rot_r10_M0_inv)\n                    mkpts1[batch_mask], mask1 = self.tta_rotation_postprocess(mkpts1[batch_mask], img_np2, rot_r10_M1_inv)\n                    confidence[batch_mask] += (~(mask0 & mask1)).astype(np.float32) * -10.\n                elif tta_elem == 'rot_l10':\n                    mkpts0[batch_mask], mask0 = self.tta_rotation_postprocess(mkpts0[batch_mask], img_np1, rot_l10_M0_inv)\n                    mkpts1[batch_mask], mask1 = self.tta_rotation_postprocess(mkpts1[batch_mask], img_np2, rot_l10_M1_inv)\n                    confidence[batch_mask] += (~(mask0 & mask1)).astype(np.float32) * -10.\n                else:\n                    raise ValueError('Unknown TTA method.')\n                    \n            if self.conf_thresh is not None:\n                th_mask = confidence >= self.conf_thresh\n            else:\n                th_mask = confidence >= 0.\n            mkpts0, mkpts1 = mkpts0[th_mask, :], mkpts1[th_mask, :]\n\n            # Matching points\n            return mkpts0 / scale0, mkpts1 / scale1\n        \n\n# 1200 is the validation size, according to the paper\nloftr_matcher = LoFTRMatcher(device=device, input_longside=1200, conf_th=0.3)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:53:06.155383Z","iopub.execute_input":"2022-05-31T22:53:06.155618Z","iopub.status.idle":"2022-05-31T22:54:07.227485Z","shell.execute_reply.started":"2022-05-31T22:53:06.155581Z","shell.execute_reply":"2022-05-31T22:54:07.226559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SuperGlue","metadata":{}},{"cell_type":"code","source":"# Install superglue\nforce_superglue_reinstall = False\n\nif 'superglue' not in INSTALLED_LOG or force_superglue_reinstall:\n    !mkdir /tmp/superpoint\n    !cp -r ../input/super-glue-pretrained-network/models /tmp/superpoint/superpoint\n    !ls /tmp/superpoint/superpoint\n    !touch /tmp/superpoint/superpoint/__init__.py\n    INSTALLED_LOG['superglue'] = True\nelse:\n    print('Already installed SuperGlue. Set \"force_superglue_reinstall=True\" to override this behavior.')\n\n# Import superglue\nimport sys\nsys.path.append(\"/tmp/superpoint\")\nfrom superpoint.superpoint import SuperPoint\nfrom superpoint.superglue import SuperGlue\n\n\nclass SuperGlueCustomMatchingV2(torch.nn.Module):\n    \"\"\" Image Matching Frontend (SuperPoint + SuperGlue) \"\"\"\n    def __init__(self, config={}, device=None):\n        super().__init__()\n        self.superpoint = SuperPoint(config.get('superpoint', {}))\n        self.superglue = SuperGlue(config.get('superglue', {}))\n\n        self.tta_map = {\n            'orig': self.untta_none,\n            'eqhist': self.untta_none,\n            'clahe': self.untta_none,\n            'flip_lr': self.untta_fliplr,\n            'flip_ud': self.untta_flipud,\n            'rot_r10': self.untta_rotr10,\n            'rot_l10': self.untta_rotl10,\n            'fliplr_rotr10': self.untta_fliplr_rotr10,\n            'fliplr_rotl10': self.untta_fliplr_rotl10\n        }\n        self.device = device\n\n    def forward_flat(self, data, ttas=['orig', ], tta_groups=[['orig']]):\n        \"\"\" Run SuperPoint (optionally) and SuperGlue\n        SuperPoint is skipped if ['keypoints0', 'keypoints1'] exist in input\n        Args:\n          data: dictionary with minimal keys: ['image0', 'image1']\n        \"\"\"\n        pred = {}\n\n        # Extract SuperPoint (keypoints, scores, descriptors) if not provided\n        # sp_st = time.time()\n        if 'keypoints0' not in data:\n            pred0 = self.superpoint({'image': data['image0']})\n            pred = {**pred, **{k+'0': v for k, v in pred0.items()}}\n        if 'keypoints1' not in data:\n            pred1 = self.superpoint({'image': data['image1']})\n            pred = {**pred, **{k+'1': v for k, v in pred1.items()}}\n        # sp_nd = time.time()\n        # print('SP:', sp_nd - sp_st, 's')\n\n        # Reverse-tta before inference\n        pred['scores0'] = list(pred['scores0'])\n        pred['scores1'] = list(pred['scores1'])\n        for i in range(len(pred['keypoints0'])):\n            pred['keypoints0'][i], pred['descriptors0'][i], pred['scores0'][i] = self.tta_map[ttas[i]](\n                pred['keypoints0'][i], pred['descriptors0'][i], pred['scores0'][i],\n                w=data['image0'].shape[3], h=data['image0'].shape[2], inplace=True, mask_illegal=True)\n\n            pred['keypoints1'][i], pred['descriptors1'][i], pred['scores1'][i] = self.tta_map[ttas[i]](\n                pred['keypoints1'][i], pred['descriptors1'][i], pred['scores1'][i],\n                w=data['image1'].shape[3], h=data['image1'].shape[2], inplace=True, mask_illegal=True)\n\n        # Batch all features\n        # We should either have i) one image per batch, or\n        # ii) the same number of local features for all images in the batch.\n        data = {**data, **pred}\n\n        group_preds = []\n        for tta_group in tta_groups:\n            group_mask = torch.from_numpy(np.array([x in tta_group for x in ttas], dtype=np.bool))\n            group_data = {\n                **{f'keypoints{k}': [data[f'keypoints{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n                **{f'descriptors{k}': [data[f'descriptors{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n                **{f'scores{k}': [data[f'scores{k}'][i] for i in range(len(ttas)) if ttas[i] in tta_group] for k in [0, 1]},\n                **{f'image{k}': data[f'image{k}'][group_mask, ...] for k in [0, 1]},\n            }\n            for k, v in group_data.items():\n                if isinstance(group_data[k], (list, tuple)):\n                    if k.startswith('descriptor'):\n                        group_data[k] = torch.cat(group_data[k], 1)[None, ...]\n                    else:\n                        group_data[k] = torch.cat(group_data[k])[None, ...]\n                else:\n                    group_data[k] = torch.flatten(group_data[k], 0, 1)[None, ...]\n            # sg_st = time.time()\n            group_pred = {\n                # **{k: group_data[k] for k in group_data},\n                **group_data,\n                **self.superglue(group_data)\n            }\n            # sg_nd = time.time()\n            # print('SG:', sg_nd - sg_st, 's')\n            group_preds.append(group_pred)\n        return group_preds\n\n    def forward_cross(self, data, ttas=['orig', ], tta_groups=[('orig', 'orig')]):\n        pred = {}\n\n        # Extract SuperPoint (keypoints, scores, descriptors) if not provided\n        sp_st = time.time()\n        if 'keypoints0' not in data:\n            pred0 = self.superpoint({'image': data['image0']})\n            pred = {**pred, **{k+'0': v for k, v in pred0.items()}}\n        if 'keypoints1' not in data:\n            pred1 = self.superpoint({'image': data['image1']})\n            pred = {**pred, **{k+'1': v for k, v in pred1.items()}}\n        sp_nd = time.time()\n\n        # Batch all features\n        # We should either have i) one image per batch, or\n        # ii) the same number of local features for all images in the batch.\n        data = {**data, **pred}\n\n        # Group predictions (list, with elements with matches{0,1}, matching_scores{0,1} keys)\n        group_pred_list = []\n        tta2id = {k: i for i, k in enumerate(ttas)}\n        for tta_group in tta_groups:\n            group_idx = tta2id[tta_group[0]], tta2id[tta_group[1]]\n            group_data = {\n                **{f'image{i}': data[f'image{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n                **{f'keypoints{i}': data[f'keypoints{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n                **{f'descriptors{i}': data[f'descriptors{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n                **{f'scores{i}': data[f'scores{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n            }\n\n            for k in group_data:\n                if isinstance(group_data[k], (list, tuple)):\n                    group_data[k] = torch.stack(group_data[k])\n\n            group_sg_pred = self.superglue(group_data)\n            group_pred_list.append(group_sg_pred)\n\n        # UnTTA\n        data['scores0'] = list(data['scores0'])\n        data['scores1'] = list(data['scores1'])\n        for i in range(len(data['keypoints0'])):\n            data['keypoints0'][i], data['descriptors0'][i], data['scores0'][i] = self.tta_map[ttas[i]](\n                data['keypoints0'][i], data['descriptors0'][i], data['scores0'][i],\n                w=data['image0'].shape[3], h=data['image0'].shape[2], inplace=True, mask_illegal=False)\n\n            data['keypoints1'][i], data['descriptors1'][i], data['scores1'][i] = self.tta_map[ttas[i]](\n                data['keypoints1'][i], data['descriptors1'][i], data['scores1'][i],\n                w=data['image1'].shape[3], h=data['image1'].shape[2], inplace=True, mask_illegal=False)\n\n        # Sooo... groups?\n        for group_pred, tta_group in zip(group_pred_list, tta_groups):\n            group_idx = tta2id[tta_group[0]], tta2id[tta_group[1]]\n            group_pred.update({\n                **{f'keypoints{i}': data[f'keypoints{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n                **{f'scores{i}': data[f'scores{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n            })\n        return group_pred_list\n\n\n    def untta_none(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        if not inplace:\n            keypoints = keypoints.clone()\n        return keypoints, descriptors, scores\n    \n    def untta_fliplr(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        if not inplace:\n            keypoints = keypoints.clone()\n        keypoints[:, 0] = w - keypoints[:, 0] - 1.\n        return keypoints, descriptors, scores\n\n    def untta_flipud(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        if not inplace:\n            keypoints = keypoints.clone()\n        keypoints[:, 1] = h - keypoints[:, 1] - 1.\n        return keypoints, descriptors, scores\n\n    def untta_rotr10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        # rotr10 is +10, inverse is -10\n        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), -15, 1)).to(torch.float32).to(self.device)\n        ones = torch.ones_like(keypoints[:, 0])\n        hom = torch.cat([keypoints, ones[:, None]], 1)\n        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n        if mask_illegal:\n            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n        else:\n            return rot_kpts, descriptors, scores\n\n    def untta_rotl10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        # rotr10 is -10, inverse is +10\n        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), 15, 1)).to(torch.float32).to(self.device)\n        ones = torch.ones_like(keypoints[:, 0])\n        hom = torch.cat([keypoints, ones[:, None]], 1)\n        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n        if mask_illegal:\n            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n        else:\n            return rot_kpts, descriptors, scores\n        \n    def untta_fliplr_rotr10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        # rotr10 is +10, inverse is -10\n        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), -15, 1)).to(torch.float32).to(self.device)\n        ones = torch.ones_like(keypoints[:, 0])\n        hom = torch.cat([keypoints, ones[:, None]], 1)\n        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n        rot_kpts[:, 0] = w - rot_kpts[:, 0] - 1.\n        if mask_illegal:\n            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n        else:\n            return rot_kpts, descriptors, scores\n\n    def untta_fliplr_rotl10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        # rotr10 is -10, inverse is +10\n        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), 15, 1)).to(torch.float32).to(self.device)\n        ones = torch.ones_like(keypoints[:, 0])\n        hom = torch.cat([keypoints, ones[:, None]], 1)\n        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n        rot_kpts[:, 0] = w - rot_kpts[:, 0] - 1.\n        if mask_illegal:\n            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n        else:\n            return rot_kpts, descriptors, scores\n\n\nclass SuperGlueMatcherV2:\n    def __init__(self, device=None, conf_th=None):\n        config = {\n            \"superpoint\": {\n                \"nms_radius\": 3,\n                \"keypoint_threshold\": 0.005,\n                \"max_keypoints\": 2048,\n            },\n            \"superglue\": {\n                \"weights\": \"outdoor\",\n                \"sinkhorn_iterations\": 100,\n                \"match_threshold\": 0.2,\n            }\n        }\n        self.device = device\n        self._superglue_matcher = SuperGlueCustomMatchingV2(\n            config=config, device=self.device,\n            ).eval().to(device)\n\n        self.conf_thresh = conf_th\n    \n    def prep_np_img(self, img, long_side=None):\n        if long_side is not None:\n            scale = long_side / max(img.shape[0], img.shape[1])\n            w = int(img.shape[1] * scale)\n            h = int(img.shape[0] * scale)\n            img = cv2.resize(img, (w, h))\n        else:\n            scale = 1.0\n        return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), scale\n    \n    def frame2tensor(self, frame):\n        return (torch.from_numpy(frame).float()/255.)[None, None].to(self.device)\n            \n    def tta_rotation_preprocess(self, img_np, angle):\n        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n        rot_img = self.frame2tensor(cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0])))\n        return rot_M, rot_img, rot_M_inv\n\n    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n        hom = np.concatenate([kpts, ones], 1)\n        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n        return rot_kpts, mask\n\n    def __call__(self, img_np0, img_np1, tta_groups=[['orig']], forward_type='cross', input_longside=None):\n        with torch.no_grad():\n            img_np0, scale0 = self.prep_np_img(img_np0, input_longside)\n            img_np1, scale1 = self.prep_np_img(img_np1, input_longside)\n\n            img_ts0 = self.frame2tensor(img_np0)\n            img_ts1 = self.frame2tensor(img_np1)\n            images0, images1 = [], []\n\n            tta = []\n            for tta_g in tta_groups:\n                tta += tta_g\n            tta = list(set(tta))\n\n            # TTA\n            for tta_elem in tta:\n                if tta_elem == 'orig':\n                    img_ts0_aug, img_ts1_aug = img_ts0, img_ts1\n                elif tta_elem == 'flip_lr':\n                    img_ts0_aug = torch.flip(img_ts0, [3, ])\n                    img_ts1_aug = torch.flip(img_ts1, [3, ])\n                elif tta_elem == 'flip_ud':\n                    img_ts0_aug = torch.flip(img_ts0, [2, ])\n                    img_ts1_aug = torch.flip(img_ts1, [2, ])\n                elif tta_elem == 'rot_r10':\n                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np0, 15)\n                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np1, 15)\n                elif tta_elem == 'rot_l10':\n                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np0, -15)\n                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np1, -15)\n                elif tta_elem == 'fliplr_rotr10':\n                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np0[:, ::-1], 15)\n                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np1[:, ::-1], 15)\n                elif tta_elem == 'fliplr_rotl10':\n                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np0[:, ::-1], -15)\n                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np1[:, ::-1], -15)\n                elif tta_elem == 'eqhist':\n                    img_ts0_aug = self.frame2tensor(cv2.equalizeHist(img_np0))\n                    img_ts1_aug = self.frame2tensor(cv2.equalizeHist(img_np1))\n                elif tta_elem == 'clahe':\n                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n                    img_ts0_aug = self.frame2tensor(clahe.apply(img_np0))\n                    img_ts1_aug = self.frame2tensor(clahe.apply(img_np1))\n                else:\n                    raise ValueError('Unknown TTA method.')\n\n                images0.append(img_ts0_aug)\n                images1.append(img_ts1_aug)\n\n            # Inference\n            if forward_type == 'cross':\n                pred = self._superglue_matcher.forward_cross(\n                    data={\n                        \"image0\": torch.cat(images0),\n                        \"image1\": torch.cat(images1)\n                    },\n                    ttas=tta, tta_groups=tta_groups)\n            elif forward_type == 'flat':\n                pred = self._superglue_matcher.forward_flat(\n                data={\n                    \"image0\": torch.cat(images0),\n                    \"image1\": torch.cat(images1)\n                },\n                ttas=tta, tta_groups=tta_groups)\n            else:\n                raise RuntimeError(f'Unknown forward_type {forward_type}')\n\n            mkpts0, mkpts1, mconf = [], [], []\n            for group_pred in pred:\n                pred_aug = {k: v[0].detach().cpu().numpy().squeeze() for k, v in group_pred.items()}\n                kpts0, kpts1 = pred_aug[\"keypoints0\"], pred_aug[\"keypoints1\"]\n                matches, conf = pred_aug[\"matches0\"], pred_aug[\"matching_scores0\"]\n\n                if self.conf_thresh is None:\n                    valid = matches > -1\n                else:\n                    valid = (matches > -1) & (conf >= self.conf_thresh)\n                mkpts0.append(kpts0[valid])\n                mkpts1.append(kpts1[matches[valid]])\n                mconf.append(conf[valid])\n\n            cat_mkpts0 = np.concatenate(mkpts0)\n            cat_mkpts1 = np.concatenate(mkpts1)\n            mask0 = (cat_mkpts0[:, 0] >= 0) & (cat_mkpts0[:, 0] < img_np0.shape[1]) & (cat_mkpts0[:, 1] >= 0) & (cat_mkpts0[:, 1] < img_np0.shape[0])\n            mask1 = (cat_mkpts1[:, 0] >= 0) & (cat_mkpts1[:, 0] < img_np1.shape[1]) & (cat_mkpts1[:, 1] >= 0) & (cat_mkpts1[:, 1] < img_np1.shape[0])\n            return cat_mkpts0[mask0 & mask1] / scale0, cat_mkpts1[mask0 & mask1] / scale1\n\n\n# 1600 is the validation size in the paper\nsuperglue_matcher = SuperGlueMatcherV2(device=device, conf_th=0.4)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-31T23:48:12.349527Z","iopub.execute_input":"2022-05-31T23:48:12.349787Z","iopub.status.idle":"2022-05-31T23:48:12.838111Z","shell.execute_reply.started":"2022-05-31T23:48:12.349757Z","shell.execute_reply":"2022-05-31T23:48:12.837336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DKM","metadata":{}},{"cell_type":"code","source":"# Install DKM and move checkpoints to the local checkpoints dir\nforce_dkm_reinstall = False\n\nif 'DKM' not in INSTALLED_LOG or force_dkm_reinstall:\n    !mkdir -p pretrained/checkpoints\n    !cp /kaggle/input/imc2022-dependencies/pretrained/dkm.pth pretrained/checkpoints/dkm_base_v11.pth\n\n    !pip install -f /kaggle/input/imc2022-dependencies/wheels --no-index einops\n    !cp -r /kaggle/input/imc2022-dependencies/DKM/ /kaggle/working/DKM/\n    !cd /kaggle/working/DKM/; pip install -f /kaggle/input/imc2022-dependencies/wheels -e .\n    INSTALLED_LOG['DKM'] = True\nelse:\n    print('Already installed DKM. Set \"force_dkm_reinstall=True\" to override this behavior.')\n\n# imports for DKM\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sys, os, csv\nfrom PIL import Image\nimport cv2, gc\nimport matplotlib.pyplot as plt\nimport torch\nimport types\nimport torch.nn.functional as TorchFunc\nsys.path.append('/kaggle/input/imc2022-dependencies/DKM/')\n\ndry_run = False\n\n\nfrom dkm import dkm_base\nfrom torchvision import transforms\n\n\ndef custom_stable_neighbours(self, query_coords, query_to_support, support_to_query):\n    qts = query_to_support\n    for t in range(4):\n        _qts = qts\n        q = TorchFunc.grid_sample(support_to_query, qts, mode=\"bilinear\")\n        qts = TorchFunc.grid_sample(\n            query_to_support.permute(0, 3, 1, 2),\n            q.permute(0, 2, 3, 1),\n            mode=\"bilinear\",\n        ).permute(0, 2, 3, 1)\n    d = (qts - _qts).norm(dim=-1)\n    qd = (q - query_coords).norm(dim=1)\n    stabneigh = torch.logical_and(d < 1e-3, qd < 5e-3)\n    return q, qts, stabneigh\n\n\ndef custom_match(\n        self,\n        im1,\n        im2,\n        batched=False,\n        check_cycle_consistency=False,\n        do_pred_in_og_res=False,\n    ):\n    self.train(False)\n    with torch.no_grad():\n        if not batched:\n            b = 1\n            w, h = im1.size\n            w2, h2 = im2.size\n            # Get images in good format\n            ws = self.w_resized\n            hs = self.h_resized\n            test_transform = get_tuple_transform_ops(\n                resize=(hs, ws), normalize=True\n            )\n            query, support = test_transform((im1, im2))\n            batch = {\"query\": query[None].cuda(), \"support\": support[None].cuda()}\n        else:\n            b, c, h, w = im1.shape\n            b, c, h2, w2 = im2.shape\n            assert w == w2 and h == h2, \"wat\"\n            batch = {\"query\": im1.cuda(), \"support\": im2.cuda()}\n            hs, ws = self.h_resized, self.w_resized\n        finest_scale = 1  # i will assume that we go to the finest scale (otherwise min(list(dense_corresps.keys())) also works)\n        # Run matcher\n        if check_cycle_consistency:\n            dense_corresps = self.forward_symmetric(batch)\n            query_to_support, support_to_query = dense_corresps[finest_scale][\n                \"dense_flow\"\n            ].chunk(2)\n            query_to_support = query_to_support.permute(0, 2, 3, 1)\n            dense_certainty, dc_s = dense_corresps[finest_scale][\n                \"dense_certainty\"\n            ].chunk(\n                2\n            )  # TODO: Here we could also use the reverse certainty\n        else:\n            dense_corresps = self.forward(batch)\n            query_to_support = dense_corresps[finest_scale][\"dense_flow\"].permute(\n                0, 2, 3, 1\n            )\n            # Get certainty interpolation\n            dense_certainty = dense_corresps[finest_scale][\"dense_certainty\"]\n\n        if do_pred_in_og_res:  # Will assume that there is no batching going on.\n            og_query, og_support = self.og_transforms((im1, im2))\n            query_to_support, dense_certainty = self.decoder.upsample_preds(\n                query_to_support,\n                dense_certainty,\n                og_query.cuda()[None],\n                og_support.cuda()[None],\n            )\n            hs, ws = h, w\n        # Create im1 meshgrid\n        query_coords = torch.meshgrid(\n            (\n                torch.linspace(-1 + 1 / hs, 1 - 1 / hs, hs, device=\"cuda:0\"),\n                torch.linspace(-1 + 1 / ws, 1 - 1 / ws, ws, device=\"cuda:0\"),\n            )\n        )\n        query_coords = torch.stack((query_coords[1], query_coords[0]))\n        query_coords = query_coords[None].expand(b, 2, hs, ws)\n        dense_certainty = dense_certainty.sigmoid()  # logits -> probs\n        if check_cycle_consistency:\n            query_coords, query_to_support, stabneigh = self.custom_stable_neighbours(\n                query_coords, query_to_support, support_to_query\n            )\n            dense_certainty *= stabneigh[:, None, :, :].float() + 1e-3\n        # Return only matches better than threshold\n        query_coords = query_coords.permute(0, 2, 3, 1)\n\n        query_to_support = torch.clamp(query_to_support, -1, 1)\n        if batched:\n            return torch.cat((query_coords, query_to_support), dim=-1), dense_certainty[:, 0]\n        else:\n            return torch.cat((query_coords, query_to_support), dim=-1)[0], dense_certainty[0, 0]\n\n\nclass DKMMatcher:\n    _DEFAULT_CONFIG = {\n        'w': 512, 'h': 384,\n        'thresh': {\n            'method': 'abs_rnd',\n            'th': 0.9,\n            'take': 100,\n        }\n    }\n    def __init__(self, device=None, config=_DEFAULT_CONFIG):\n        torch.hub.set_dir('/kaggle/working/pretrained/')\n        self._dkm_matcher = dkm_base(pretrained=True, version=\"v11\").to(device).eval()\n        self.config = config\n        self._dkm_matcher.w_resized = config['w']\n        self._dkm_matcher.h_resized = config['h']\n        self._dkm_matcher.custom_stable_neighbours = types.MethodType(custom_stable_neighbours, self._dkm_matcher)\n        self._dkm_matcher.custom_match = types.MethodType(custom_match, self._dkm_matcher)\n        self.device=device\n\n        mean=[0.485, 0.456, 0.406]\n        std=[0.229, 0.224, 0.225]\n        self.normalize = transforms.Normalize(mean=mean, std=std)\n\n    def prepare_torch_image(self, img):\n        \"\"\"img - BGR image array\"\"\"\n        img_ts = K.image_to_tensor(img, False).float() / 255.\n        img_ts = K.color.bgr_to_rgb(img_ts)\n        return self.normalize(img_ts)\n\n    def results_thresholding(self, dense_matches, dense_certainty):\n        if self.config['thresh']['method'] == 'abs_rnd':\n            n_take = (dense_certainty >= self.config['thresh']['th']).count_nonzero().cpu().numpy()\n            n_take = min(max(n_take, 0), self.config['thresh']['take'])\n            sparse_matches, sparse_certainty = self._dkm_matcher.sample(dense_matches, dense_certainty, num=n_take)\n        elif self.config['thresh']['method'] == 'rel':\n            sparse_matches, sparse_certainty = self._dkm_matcher.sample(\n                dense_matches, dense_certainty,\n                num=self.config['thresh']['take'], relative_confidence_threshold=self.config['thresh']['th'])\n        elif self.config['thresh']['method'] == 'abs':\n            matches, certainty = (\n                dense_matches.reshape(-1, 4),\n                dense_certainty.reshape(-1),\n            )\n            th_matches, th_certainty = (\n                matches[certainty > self.config['thresh']['th']].cpu().numpy(),\n                certainty[certainty > self.config['thresh']['th']].cpu().numpy(),\n            )\n            if len(th_matches) > 0:\n                good_samples = np.random.choice(\n                    np.arange(len(th_matches)),\n                    size=min(self.config['thresh']['take'], len(th_certainty)),\n                    replace=False,\n                    p=th_certainty / (np.sum(th_certainty) + 1e-6),\n                )\n                sparse_matches = th_matches[good_samples]\n            else:\n                sparse_matches = th_matches\n        else:\n            raise ValueError('Unknown thresholding method ' + str(self.config['thresh']['method']))\n        return sparse_matches\n\n    def tta_rotation_preprocess(self, img_np, angle):\n        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n        rot_img = cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0]))\n        return rot_M, rot_img, rot_M_inv\n\n    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n        hom = np.concatenate([kpts, ones], 1)\n        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n        return rot_kpts, mask\n\n\n    def __call__(self, img_bgr_np1, img_bgr_np2, tta=['orig'], verbose=0):\n        with torch.no_grad():            \n            # TTA preparation. Affine first, and rotation only after\n            ttaprep_st = time.time()  \n            images0, images1 = [], []\n            for tta_elem in tta:\n                if tta_elem == 'orig':\n                    img_np0_aug, img_np1_aug = img_bgr_np1, img_bgr_np2\n                elif tta_elem == 'flip_lr':\n                    img_np0_aug = np.flip(img_bgr_np1, [1, ]).copy()\n                    img_np1_aug = np.flip(img_bgr_np2, [1, ]).copy()\n                elif tta_elem == 'flip_ud':\n                    img_np0_aug = np.flip(img_bgr_np1, [0, ]).copy()\n                    img_np1_aug = np.flip(img_bgr_np2, [0, ]).copy()\n                elif tta_elem == 'rot_r10':\n                    rot_r10_M0, img_np0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_bgr_np1, 10)\n                    rot_r10_M1, img_np1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_bgr_np2, 10)\n                elif tta_elem == 'rot_l10':\n                    rot_l10_M0, img_np0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_bgr_np1, -10)\n                    rot_l10_M1, img_np1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_bgr_np2, -10)\n                else:\n                    raise ValueError('Unknown TTA method.')\n\n                # Rotation is after for 2 reasons:\n                #   - I think rotation on smaller scale might lose some information\n                #   - Avoid any weird stuff in combining affine transformations\n                img_np0_aug = cv2.resize(img_np0_aug, (self._dkm_matcher.w_resized, self._dkm_matcher.h_resized))\n                img_np1_aug = cv2.resize(img_np1_aug, (self._dkm_matcher.w_resized, self._dkm_matcher.h_resized))\n                images0.append(self.prepare_torch_image(img_np0_aug))\n                images1.append(self.prepare_torch_image(img_np1_aug))\n            ttaprep_nd = time.time()\n\n            # Batched inference\n            batchinf_st = time.time()  # -- start\n            img0_batch_ts, img1_batch_ts = torch.cat(images0), torch.cat(images1)\n            dense_matches, dense_certainty = self._dkm_matcher.custom_match(\n                img0_batch_ts, img1_batch_ts, batched=True,\n                check_cycle_consistency=self.config['check_cycle_consistency'])\n            batchinf_nd = time.time()  # -- end\n\n            # SQRT of dense certainty\n            dense_certainty = dense_certainty.sqrt()\n\n            # Get sparse matching keypoints\n            getkpts_st = time.time()\n            mkps1, mkps2 = [], []\n            for idx, tta_elem in enumerate(tta):\n                ith_sparse_matches = self.results_thresholding(dense_matches[idx], dense_certainty[idx])\n\n                aug_mkps1 = ith_sparse_matches[:, :2]\n                aug_mkps2 = ith_sparse_matches[:, 2:]\n\n                h, w, c = img_bgr_np1.shape\n                aug_mkps1[:, 0] = ((aug_mkps1[:, 0] + 1.)/2.) * w\n                aug_mkps1[:, 1] = ((aug_mkps1[:, 1] + 1.)/2.) * h\n\n                h, w, c = img_bgr_np2.shape\n                aug_mkps2[:, 0] = ((aug_mkps2[:, 0] + 1.)/2.) * w\n                aug_mkps2[:, 1] = ((aug_mkps2[:, 1] + 1.)/2.) * h\n                \n                mkps1.append(aug_mkps1)\n                mkps2.append(aug_mkps2)\n            getkpts_nd = time.time()\n\n            # Reverse TTA\n            revtta_st = time.time()\n            for idx, tta_elem in enumerate(tta):\n                if tta_elem == 'orig':\n                    pass\n                elif tta_elem == 'flip_lr':\n                    mkps1[idx][:, 0] = img_bgr_np1.shape[1] - mkps1[idx][:, 0]\n                    mkps2[idx][:, 0] = img_bgr_np2.shape[1] - mkps2[idx][:, 0]\n                elif tta_elem == 'flip_ud':\n                    mkps1[idx][:, 1] = img_bgr_np1.shape[0] - mkps1[idx][:, 1]\n                    mkps2[idx][:, 1] = img_bgr_np2.shape[0] - mkps2[idx][:, 1]\n                elif tta_elem == 'rot_r10':\n                    mkps1[idx], mask0 = self.tta_rotation_postprocess(mkps1[idx], img_bgr_np1, rot_r10_M0_inv)\n                    mkps2[idx], mask1 = self.tta_rotation_postprocess(mkps2[idx], img_bgr_np2, rot_r10_M1_inv)\n                    mkps1[idx], mkps2[idx] = mkps1[idx][mask0 & mask1], mkps2[idx][mask0 & mask1]\n                elif tta_elem == 'rot_l10':\n                    mkps1[idx], mask0 = self.tta_rotation_postprocess(mkps1[idx], img_bgr_np1, rot_l10_M0_inv)\n                    mkps2[idx], mask1 = self.tta_rotation_postprocess(mkps2[idx], img_bgr_np2, rot_l10_M1_inv)\n                    mkps1[idx], mkps2[idx] = mkps1[idx][mask0 & mask1], mkps2[idx][mask0 & mask1]\n                else:\n                    raise ValueError('Unknown TTA method.')\n            revtta_nd = time.time()\n\n            if verbose >= 1:\n                print('    - DKM inner:')\n                print('      - ttaprep:', ttaprep_nd - ttaprep_st)\n                print('      - batchinf:', batchinf_nd - batchinf_st)\n                print('      - getkpts:', getkpts_nd - getkpts_st)\n                print('      - revtta:', revtta_nd - revtta_st)\n            return np.concatenate(mkps1), np.concatenate(mkps2)\n\n\n# abs_rnd_config = {\n#     'w': 512, 'h': 384,\n#     'thresh': {'method': 'abs', 'th': 0.9, 'take': 100}\n# }\nabs_config = {\n    'w': 800, 'h': 600, 'check_cycle_consistency': False,\n    'thresh': {'method': 'abs', 'th': 0.8, 'take': 200}\n} \n# rel_config = {\n#     'w': 512, 'h': 384,\n#     'thresh': {'method': 'rel', 'th': 0.95, 'take': 69}\n# }\n# abs_sqrt_config = {  # Abs sqrt is actualy abs_pow(1/4) because the dense_confidence score is already sqrt-processed, so double-sqrt\n#     'w': 800, 'h': 600,\n#     'thresh': {'method': 'abs_sqrt', 'th': 0.8, 'take': 250}\n# }\n\ndkm_matcher = DKMMatcher(device=device, config=abs_config)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:55:24.671287Z","iopub.execute_input":"2022-05-31T22:55:24.671569Z","iopub.status.idle":"2022-05-31T22:56:09.99274Z","shell.execute_reply.started":"2022-05-31T22:55:24.67154Z","shell.execute_reply":"2022-05-31T22:56:09.991845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DISK","metadata":{}},{"cell_type":"code","source":"%%writefile /tmp/disk_install.txt\n\n# Install DISK\nforce_disk_reinstall = False\n\nif not INSTALLED_LOG.get('disk_installed', False) or force_disk_reinstall:\n    # !pip install ../input/disk-deps/disk_repo/submodules/torch-localize --no-index --find-links ../input/disk-deps/disk_repo/wheel_files\n    # !pip install ../input/disk-deps/disk_repo/submodules/torch-dimcheck --no-index --find-links ../input/disk-deps/disk_repo/wheel_files\n    # !pip install ../input/disk-deps/disk_repo/submodules/unets --no-index --find-links ../input/disk-deps/disk_repo/wheel_files\n    !pip install -f /kaggle/input/k/eduardtrulls/imc2022-dependencies/wheels --no-index torch_dimcheck\n    !pip install -f /kaggle/input/k/eduardtrulls/imc2022-dependencies/wheels --no-index torch_localize\n    !pip install -f /kaggle/input/k/eduardtrulls/imc2022-dependencies/wheels --no-index unets\n    !pip install -f /kaggle/input/k/eduardtrulls/imc2022-dependencies/wheels --no-index disk\nelse:\n    print('Already installed DISK. Set \"force_disk_reinstall=True\" to override this behavior.')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:56:09.994877Z","iopub.execute_input":"2022-05-31T22:56:09.995137Z","iopub.status.idle":"2022-05-31T22:56:10.003735Z","shell.execute_reply.started":"2022-05-31T22:56:09.995106Z","shell.execute_reply":"2022-05-31T22:56:10.002817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /tmp/disk_usage.txt\n\nimport disk\nfrom disk import DISK, Features\n\n\nfrom torch_dimcheck import dimchecked\nfrom functools import partial\nfrom disk.geom import distance_matrix\nimport math\n\n\nDISK_REPO_PATH = '../input/disk-deps/disk_repo/'\n\n\nclass DiskImage:\n    def __init__(self, bitmap: ['C', 'H', 'W'], fname: str, orig_shape=None):\n        self.bitmap     = bitmap\n        self.fname      = fname\n        if orig_shape is None:\n            self.orig_shape = self.bitmap.shape[1:]\n        else:\n            self.orig_shape = orig_shape\n\n    def resize_to(self, shape):\n        return DiskImage(\n            self._pad(self._interpolate(self.bitmap, shape), shape),\n            self.fname,\n            orig_shape=self.bitmap.shape[1:],\n        )\n\n    @dimchecked\n    def to_image_coord(self, xys: [2, 'N']) -> ([2, 'N'], ['N']):\n        f, _size = self._compute_interpolation_size(self.bitmap.shape[1:])\n        scaled = xys / f\n\n        h, w = self.orig_shape\n        x, y = scaled\n\n        mask = (0 <= x) & (x < w) & (0 <= y) & (y < h)\n\n        return scaled, mask\n\n    def _compute_interpolation_size(self, shape):\n        x_factor = self.orig_shape[0] / shape[0]\n        y_factor = self.orig_shape[1] / shape[1]\n\n        f = 1 / max(x_factor, y_factor)\n\n        if x_factor > y_factor:\n            new_size = (shape[0], int(f * self.orig_shape[1]))\n        else:\n            new_size = (int(f * self.orig_shape[0]), shape[1])\n\n        return f, new_size\n\n    @dimchecked\n    def _interpolate(self, image: ['C', 'H', 'W'], shape) -> ['C', 'h', 'w']:\n        _f, size = self._compute_interpolation_size(shape)\n        return F.interpolate(\n            image.unsqueeze(0),\n            size=size,\n            mode='bilinear',\n            align_corners=False,\n        ).squeeze(0)\n    \n    @dimchecked\n    def _pad(self, image: ['C', 'H', 'W'], shape) -> ['C', 'h', 'w']:\n        x_pad = shape[0] - image.shape[1]\n        y_pad = shape[1] - image.shape[2]\n\n        if x_pad < 0 or y_pad < 0:\n            raise ValueError(\"Attempting to pad by negative value\")\n\n        return F.pad(image, (0, y_pad, 0, x_pad))\n\n\nclass DiskMatcher:\n\n    MAX_FULL_MATRIX = 10000**2\n    _help=('this is the biggest match matrix that will attempt to be '\n           'computed allocated in memory. Matrices bigger than that will '\n           'be split into chunks of at most this size. Reduce if your '\n           'script runs out of memory.')\n\n    @staticmethod\n    @dimchecked\n    def _binary_to_index(binary_mask: ['N'], ix2: ['M']) -> [2, 'M']:\n        return torch.stack([\n            torch.nonzero(binary_mask, as_tuple=False)[:, 0],\n            ix2\n        ], dim=0)\n\n    @staticmethod\n    @dimchecked\n    def _ratio_one_way(dist_m: ['N', 'M'], rt) -> [2, 'K']:\n        val, ix = torch.topk(dist_m, k=2, dim=1, largest=False)\n        ratio = val[:, 0] / val[:, 1]\n        passed_test = ratio < rt\n        ix2 = ix[passed_test, 0]\n\n        return DiskMatcher._binary_to_index(passed_test, ix2)\n\n    @staticmethod\n    @dimchecked\n    def _match_chunkwise(ds1: ['N', 'F'], ds2: ['M', 'F'], rt) -> [2, 'K']:\n        chunk_size = DiskMatcher.MAX_FULL_MATRIX // ds1.shape[0]\n        matches = []\n        start = 0\n\n        while start < ds2.shape[0]:\n            ds2_chunk = ds2[start:start+chunk_size]\n            dist_m = distance_matrix(ds1, ds2_chunk)\n            one_way = DiskMatcher._ratio_one_way(dist_m, rt)\n            one_way[1] += start\n            matches.append(one_way)\n            start += chunk_size\n\n        return torch.cat(matches, dim=1)\n\n    @staticmethod\n    @dimchecked\n    def _match(ds1: ['N', 'F'], ds2: ['M', 'F'], rt) -> [2, 'K']:\n        size = ds1.shape[0] * ds2.shape[0]\n\n        fwd = DiskMatcher._match_chunkwise(ds1, ds2, rt)\n        bck = DiskMatcher._match_chunkwise(ds2, ds1, rt)\n        bck = torch.flip(bck, (0, ))\n\n        merged = torch.cat([fwd, bck], dim=1)\n        unique, counts = torch.unique(merged, dim=1, return_counts=True)\n\n        return unique[:, counts == 2]\n\n    @staticmethod\n    def match(desc_1, desc_2, rt=1., u16=False):\n        matched_pairs = DiskMatcher._match(desc_1, desc_2, rt)\n        matches = matched_pairs.cpu().numpy()\n\n        if u16:\n            matches = matches.astype(np.uint16)\n\n        return matches\n\n\nclass DiskWrapper:\n    def __init__(self, n=2048, matcher='cv2_bf', long_side=1024, device=device):\n        state_dict = torch.load(os.path.join(DISK_REPO_PATH, 'depth-save.pth'), map_location='cpu')\n\n        # compatibility with older model saves which used the 'extractor' name\n        if 'extractor' in state_dict:\n            weights = state_dict['extractor']\n        elif 'disk' in state_dict:\n            weights = state_dict['disk']\n        else:\n            raise KeyError('Incompatible weight file!')\n\n        self.model = DISK(window=8, desc_dim=128)\n        self.model.load_state_dict(weights)\n        self.model = self.model.to(device)\n\n        self._extract = partial(\n            self.model.features,\n            kind='nms',\n            window_size=3,  # NMS window size\n            cutoff=0.,\n            n=n,  # None means unlimited keypoints\n        )\n\n        self.matcher = matcher\n        if self.matcher == 'cv2_bf':\n            # Brute-force matcher with bi-directionaly check.\n            self._cv2_match_fn = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n        elif self.matcher == 'disk':\n            # Match using the same algo as in DISK repo\n            self._disk_match_fn = partial(DiskMatcher.match, rt=1., u16=False)\n        else:\n            raise RuntimeError(f'Unknown matcher {self.matcher}')\n\n        self.long_side = long_side\n        self.device = device\n\n    def prep_img(self, img, long_side=1024):\n        # Resize so that the longest side is \"long_side\"\n        if long_side is not None:\n            scale = long_side / max(img.shape[0], img.shape[1]) \n            w = int(img.shape[1] * scale)\n            h = int(img.shape[0] * scale)\n        else:\n            w, h = img.shape[1], img.shape[0]\n\n        # round sides up to multiples of 16\n        if not w % 16 == 0:\n            w = int(math.ceil(w / 16.) * 16)\n        if not h % 16 == 0:\n            h = int(math.ceil(h / 16.) * 16)\n        \n        # To tensor\n        scalew, scaleh = float(w) / img.shape[1], float(h) / img.shape[0]\n        img = cv2.resize(img, (w, h))\n        img_ts = K.image_to_tensor(img, False).float() / 255.\n        img_ts = K.color.bgr_to_rgb(img_ts)\n        return img_ts.to(self.device), scalew, scaleh\n\n    def extract_features(self, bitmap_batch):\n        images = []\n        for i, bitmap in enumerate(bitmap_batch):\n            images.append(DiskImage(bitmap, f'img{i}'))\n\n        with torch.no_grad():\n            batched_features = self._extract(bitmap_batch)\n\n            ret_keypoints, ret_descriptors, ret_scores = [], [], []\n            for features, image in zip(batched_features.flat, images):\n                # features = features.to('cpu')\n\n                kps_crop_space = features.kp.T\n                kps_img_space, mask = image.to_image_coord(kps_crop_space)\n\n                keypoints   = kps_img_space.T[mask]\n                descriptors = features.desc[mask]\n                scores      = features.kp_logp[mask]\n\n                order = torch.flip(torch.argsort(scores), [0, ])\n\n                keypoints   = keypoints[order]\n                descriptors = descriptors[order]\n                scores      = scores[order]\n\n                ret_keypoints.append(keypoints)\n                ret_descriptors.append(descriptors)\n                ret_scores.append(scores)\n\n        return ret_keypoints, ret_descriptors, ret_scores\n\n    def __call__(self, img_bgr_np0, img_bgr_np1, tta=['orig', ]):\n        with torch.no_grad():\n            img_rgb_ts0, scale0_w, scale0_h = self.prep_img(img_bgr_np0)\n            img_rgb_ts1, scale1_w, scale1_h = self.prep_img(img_bgr_np1)\n\n            # TTA\n            images0, images1 = [], []\n            for tta_elem in tta:\n                if tta_elem == 'orig':\n                    img_ts0_aug, img_ts1_aug = img_rgb_ts0, img_rgb_ts1\n                elif tta_elem == 'flip_lr':\n                    img_ts0_aug = torch.flip(img_rgb_ts0, [3, ])\n                    img_ts1_aug = torch.flip(img_rgb_ts1, [3, ])\n                elif tta_elem == 'flip_ud':\n                    img_ts0_aug = torch.flip(img_rgb_ts0, [2, ])\n                    img_ts1_aug = torch.flip(img_rgb_ts1, [2, ])\n                else:\n                    raise ValueError('Unknown TTA method.')\n                images0.append(img_ts0_aug)\n                images1.append(img_ts1_aug)\n\n            batch_images0 = torch.cat(images0)\n            batch_images1 = torch.cat(images1)\n\n            # Batched inference\n            keypoints0, descriptors0, scores0 = self.extract_features(batch_images0)\n            keypoints1, descriptors1, scores1 = self.extract_features(batch_images1)\n\n            # Match and post-process\n            mkpts0, mkpts1 = [], []\n            for idx, tta_elem in enumerate(tta):\n                kpts0, kpts1 = keypoints0[idx], keypoints1[idx]\n                desc0, desc1 = descriptors0[idx], descriptors1[idx]\n                scor0, scor1 = scores0[idx], scores1[idx]\n\n                # Re-scale keypoints to the size of original image\n                kpts0[:, 0], kpts0[:, 1] = kpts0[:, 0] / scale0_w, kpts0[:, 1] / scale0_h\n                kpts1[:, 0], kpts1[:, 1] = kpts1[:, 0] / scale1_w, kpts1[:, 1] / scale1_h\n\n                # Reverse TTA on keypoints\n                if tta_elem == 'orig':\n                    pass\n                elif tta_elem == 'flip_lr':\n                    kpts0[:, 0] = img_bgr_np0.shape[1] - kpts0[:, 0]\n                    kpts1[:, 0] = img_bgr_np1.shape[1] - kpts1[:, 0]\n                elif tta_elem == 'flip_ud':\n                    kpts0[:, 1] = img_bgr_np0.shape[0] - kpts0[:, 1]\n                    kpts1[:, 1] = img_bgr_np1.shape[0] - kpts1[:, 1]\n                else:\n                    raise ValueError('Unknown TTA method.')\n\n                # Match keypoints\n                if self.matcher == 'cv2_bf':\n                    cv_matches = self._cv2_match_fn.match(\n                        desc0.cpu().numpy(),\n                        desc1.cpu().numpy())\n                    matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n                    mkpts0.append(kpts0.cpu().numpy()[matches[:, 0]])\n                    mkpts1.append(kpts1.cpu().numpy()[matches[:, 1]])\n                elif self.matcher == 'disk':\n                    disk_matches = self._disk_match_fn(desc0, desc1)\n                    mkpts0.append(kpts0[disk_matches[0, :]].cpu().numpy())\n                    mkpts1.append(kpts1[disk_matches[1, :]].cpu().numpy())\n                else:\n                    raise RuntimeError(f'Unknown matcher {self.matcher}')\n\n            # Return keypoints\n            return np.concatenate(mkpts0), np.concatenate(mkpts1)\n\ndisk_matcher = DiskWrapper(matcher='disk', n=2048)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:56:10.005628Z","iopub.execute_input":"2022-05-31T22:56:10.00628Z","iopub.status.idle":"2022-05-31T22:56:10.027753Z","shell.execute_reply.started":"2022-05-31T22:56:10.006201Z","shell.execute_reply":"2022-05-31T22:56:10.026945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])","metadata":{"execution":{"iopub.status.busy":"2022-05-31T22:56:10.029857Z","iopub.execute_input":"2022-05-31T22:56:10.030177Z","iopub.status.idle":"2022-05-31T22:56:10.041741Z","shell.execute_reply.started":"2022-05-31T22:56:10.030137Z","shell.execute_reply":"2022-05-31T22:56:10.040985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"from functools import partial\n\n\ndef match_images(sample_id, batch_id, image_1_id, image_2_id, verbose=0, ret_ims=False):\n    img_np1 = cv2.imread(f'{src}/test_images/{batch_id}/{image_1_id}.png')\n    img_np2 = cv2.imread(f'{src}/test_images/{batch_id}/{image_2_id}.png')\n    st = time.time()\n\n    matchers_cfg = [\n        {\n            'name': 'loftr',\n            'fn': partial(loftr_matcher, tta=[\n                'orig', 'flip_lr',\n            ]),\n        },\n        {\n            'name': 'superglue',\n            'fn': partial(superglue_matcher, tta_groups=[\n                ('orig', 'orig'),\n                ('orig', 'rot_r10'),\n                ('rot_r10', 'orig'),\n                ('flip_lr', 'flip_lr'),\n            ], forward_type='cross', input_longside=1600)\n        },\n        {\n            'name': 'dkm',\n            'fn': partial(dkm_matcher, tta=[\n                'orig', 'flip_lr', 'rot_r10'\n            ]),\n        },\n        # {'name': 'disk', 'fn': disk_matcher, 'tta': ['orig', ]},\n    ]\n    max_name_len = 0\n    mkpts0, mkpts1, runtime_str, kp_count_str = [], [], [], []\n    for m_cfg in matchers_cfg:\n        max_name_len = max(len(m_cfg['name']), max_name_len)\n        m_st = time.time()\n        m_mkpts0, m_mkpts1 = m_cfg['fn'](img_np1, img_np2)\n        m_nd = time.time()\n\n        mkpts0.append(m_mkpts0)\n        mkpts1.append(m_mkpts1)\n        runtime_str.append(f'{m_cfg[\"name\"].ljust(max_name_len)}: {m_nd - m_st:06f}s')\n        kp_count_str.append(f'{m_cfg[\"name\"]}={len(m_mkpts0)}')\n\n    mkpts0 = np.concatenate(mkpts0)\n    mkpts1 = np.concatenate(mkpts1)\n  \n    if verbose >= 1:\n        print(\"  - Matching:\")\n        for s in runtime_str:\n            print(\"    -\", s)\n        print(\"  - Keypoints:\", len(mkpts0), '|', ', '.join(kp_count_str))\n    if ret_ims:\n        return mkpts0, mkpts1, img_np1, img_np2\n    else:\n        return mkpts0, mkpts1\n\n\nclass SolutionHolder:\n    def __init__(self):\n        self.F_dict = dict()\n    \n    @staticmethod\n    def solve_keypoints(mkpts0, mkpts1, verbose=0, ret_inliers=False):\n        findmat_st = time.time()\n        if len(mkpts0) > 7:\n            #F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.25, 0.9999, 100000)\n            F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.2, 0.9999, 250000)  # EDITED, was 220000\n            #F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.1845, 0.999999, 150000)\n            inliers = inliers > 0\n            assert F.shape == (3, 3), 'Malformed F?'\n        else:\n            F = np.zeros((3, 3))\n        findmat_end = time.time()\n        if verbose >= 1:\n            print('  - Ransac time:', findmat_end - findmat_st, \"s\")\n        if ret_inliers:\n            return F, inliers\n        else:\n            return F\n\n    def add_solution(self, sample_id, mkpts0, mkpts1, verbose=0):\n        self.F_dict[sample_id] = SolutionHolder.solve_keypoints(mkpts0, mkpts1, verbose)\n  \n    def dump(self, output_file):\n        with open(output_file, 'w') as f:\n            f.write('sample_id,fundamental_matrix\\n')\n            for sample_id, F in self.F_dict.items():\n                f.write(f'{sample_id},{FlattenMatrix(F)}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-31T23:52:34.66742Z","iopub.execute_input":"2022-05-31T23:52:34.667695Z","iopub.status.idle":"2022-05-31T23:52:34.687367Z","shell.execute_reply.started":"2022-05-31T23:52:34.667663Z","shell.execute_reply":"2022-05-31T23:52:34.686582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"execution":{"iopub.status.busy":"2022-05-31T23:52:38.841801Z","iopub.execute_input":"2022-05-31T23:52:38.842382Z","iopub.status.idle":"2022-05-31T23:52:38.847345Z","shell.execute_reply.started":"2022-05-31T23:52:38.842346Z","shell.execute_reply":"2022-05-31T23:52:38.84633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VISUALIZE = True\nimport time\n\n\nif VISUALIZE and len(test_samples) == 3:\n    for i, row in enumerate(test_samples[:3]):\n        sample_id, batch_id, image_1_id, image_2_id = row\n        st = time.time()\n\n        mkpts0, mkpts1, img_np1, img_np2 = match_images(sample_id, batch_id, image_1_id, image_2_id, verbose=1, ret_ims=True)\n        F, inliers = SolutionHolder.solve_keypoints(mkpts0, mkpts1, verbose=1, ret_inliers=True)\n\n        gc_st = time.time()\n        gc.collect()\n        nd = time.time()    \n        if (i < 3):\n            print(\"  - gc:\", nd - gc_st, \"s\")\n            print(\"Running time: \", nd - st, \" s\")\n            KMF.draw_LAF_matches(\n                KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                             torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                             torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n                KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                             torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                             torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n                torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n                cv2.cvtColor(img_np1, cv2.COLOR_BGR2RGB),\n                cv2.cvtColor(img_np2, cv2.COLOR_BGR2RGB),\n                inliers,\n                draw_dict={'inlier_color': (0.2, 1, 0.2),\n                           'tentative_color': None, \n                           'feature_color': (0.2, 0.5, 1), 'vertical': False},\n            )\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-31T23:52:39.723207Z","iopub.execute_input":"2022-05-31T23:52:39.723741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport threading\n\n\ndef main_solution(test_samples, output_file):\n    # Prepare variables\n    solution_holder = SolutionHolder()\n    mkpts0, mkpts1 = None, None\n    mat_calc_thread = None\n  \n    for i, row in enumerate(test_samples):\n        # Print stats only for the first 3 samples\n        verbose = 1 if i < 3 else 0\n    \n        # Parse row\n        sample_id, batch_id, image_1_id, image_2_id = row\n        cyc_st = time.time()\n    \n        # Delete previous sample's results\n        del mkpts0\n        del mkpts1\n  \n        # Calculate matching pairs\n        mkpts0, mkpts1 = match_images(sample_id, batch_id, image_1_id, image_2_id, verbose=verbose)\n    \n        # If the RANSAC thread is not finished (it should though), wait...\n        if mat_calc_thread is not None:\n            mat_calc_thread.join()\n    \n        # Execute a RANSAC thread\n        mat_calc_thread = threading.Thread(\n            target=solution_holder.add_solution,\n            args=(sample_id, mkpts0, mkpts1, verbose))\n        mat_calc_thread.start()\n    \n        # Collect garbage and print logs if required\n        cyc_end = time.time()\n        gc_st = time.time()\n        gc.collect()\n        gc_end = time.time()\n        if verbose > 0:\n            print(f'Iter total: {gc_end - cyc_st:.06f}s  (runtime: {cyc_end - cyc_st:.06f}s, gc: {gc_end - gc_st:.06f}s)')\n  \n    # Finish and write solution\n    fin_st = time.time()\n    if mat_calc_thread is not None:\n        mat_calc_thread.join()\n    fin_end = time.time()\n  \n    if verbose > 0:\n        print('Final calc:', fin_end - fin_st, 's')\n    solution_holder.dump(output_file)\n\n\nmain_solution(test_samples, 'submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}