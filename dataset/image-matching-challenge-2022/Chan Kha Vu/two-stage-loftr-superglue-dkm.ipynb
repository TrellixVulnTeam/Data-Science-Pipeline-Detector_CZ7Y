{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<center>\n    <h2 style=\"color: #022047\"> An image to get in the right mood... ðŸ˜‰  </h2>\n</center>\n\n![](https://storage.googleapis.com/kaggle-media/competitions/google-image-matching/trevi-canvas-licensed-nonoderivs.jpg)","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport torch\nimport matplotlib.pyplot as plt\nimport gc\n\n\nimport torch\nif not torch.cuda.is_available():\n    print('You may want to enable the GPU switch?')\n\nINSTALLED_LOG = {}\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T16:43:58.160863Z","iopub.execute_input":"2022-06-01T16:43:58.1614Z","iopub.status.idle":"2022-06-01T16:44:00.091842Z","shell.execute_reply.started":"2022-06-01T16:43:58.161292Z","shell.execute_reply":"2022-06-01T16:44:00.091087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Kornia LoFTR","metadata":{}},{"cell_type":"code","source":"# Install Kornia\nforce_kornialoftr_reinstall = False\n\nif 'KorniaLoFTR' not in INSTALLED_LOG or force_kornialoftr_reinstall:\n    dry_run = False\n    !pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n    !pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n    INSTALLED_LOG['KorniaLoFTR'] = True\nelse:\n    print('Already installed KorniaLoFTR. Set \"force_kornialoftr_reinstall=True\" to override this behavior.')\n    \n\n# Import and use Kornia\nimport kornia\nimport kornia as K\nimport kornia.feature as KF\nimport kornia_moons.feature as KMF\n\n\nclass LoFTRMatcher:\n    def __init__(self, device=None):\n        self._loftr_matcher = KF.LoFTR(pretrained=None)\n        self._loftr_matcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\n        self._loftr_matcher = self._loftr_matcher.to(device).eval()\n        self.device = device\n        \n    def prep_img(self, img):\n        img_ts = K.image_to_tensor(img, False).float() / 255.\n        img_ts = K.color.bgr_to_rgb(img_ts)\n        img_ts = K.color.rgb_to_grayscale(img_ts)\n        return img_ts.to(self.device)\n    \n    def tta_rotation_preprocess(self, img_np, angle):\n        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n        rot_img = cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0]))\n\n        rot_img_ts = K.image_to_tensor(rot_img, False).float() / 255.\n        rot_img_ts = K.color.bgr_to_rgb(rot_img_ts)\n        rot_img_ts = K.color.rgb_to_grayscale(rot_img_ts)\n        return rot_M, rot_img_ts.to(self.device), rot_M_inv\n\n    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n        hom = np.concatenate([kpts, ones], 1)\n        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n        return rot_kpts, mask\n\n    def __call__(self, img_np1, img_np2, tta=['orig'], conf_th=None):\n        with torch.no_grad():\n            img_ts0 = self.prep_img(img_np1)\n            img_ts1 = self.prep_img(img_np2)\n            images0, images1 = [], []\n\n            # TTA\n            for tta_elem in tta:\n                if tta_elem == 'orig':\n                    img_ts0_aug, img_ts1_aug = img_ts0, img_ts1\n                elif tta_elem == 'flip_lr':\n                    img_ts0_aug = torch.flip(img_ts0, [3, ])\n                    img_ts1_aug = torch.flip(img_ts1, [3, ])\n                elif tta_elem == 'flip_ud':\n                    img_ts0_aug = torch.flip(img_ts0, [2, ])\n                    img_ts1_aug = torch.flip(img_ts1, [2, ])\n                elif tta_elem == 'rot_r10':\n                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np1, 10)\n                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np2, 10)\n                elif tta_elem == 'rot_l10':\n                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np1, -10)\n                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np2, -10)\n                else:\n                    raise ValueError('Unknown TTA method.')\n                images0.append(img_ts0_aug)\n                images1.append(img_ts1_aug)\n\n            # Inference\n            input_dict = {\"image0\": torch.cat(images0), \"image1\": torch.cat(images1)}\n            correspondences = self._loftr_matcher(input_dict)\n            mkpts0 = correspondences['keypoints0'].cpu().numpy()\n            mkpts1 = correspondences['keypoints1'].cpu().numpy()\n            batch_id = correspondences['batch_indexes'].cpu().numpy()\n            confidence = correspondences['confidence'].cpu().numpy()\n\n            # Reverse TTA\n            for idx, tta_elem in enumerate(tta):\n                batch_mask = batch_id == idx\n\n                if tta_elem == 'orig':\n                    pass\n                elif tta_elem == 'flip_lr':\n                    mkpts0[batch_mask, 0] = img_np1.shape[1] - mkpts0[batch_mask, 0]\n                    mkpts1[batch_mask, 0] = img_np2.shape[1] - mkpts1[batch_mask, 0]\n                elif tta_elem == 'flip_ud':\n                    mkpts0[batch_mask, 1] = img_np1.shape[0] - mkpts0[batch_mask, 1]\n                    mkpts1[batch_mask, 1] = img_np2.shape[0] - mkpts1[batch_mask, 1]\n                elif tta_elem == 'rot_r10':\n                    mkpts0[batch_mask], mask0 = self.tta_rotation_postprocess(mkpts0[batch_mask], img_np1, rot_r10_M0_inv)\n                    mkpts1[batch_mask], mask1 = self.tta_rotation_postprocess(mkpts1[batch_mask], img_np2, rot_r10_M1_inv)\n                    confidence[batch_mask] += (~(mask0 & mask1)).astype(np.float32) * -10.\n                elif tta_elem == 'rot_l10':\n                    mkpts0[batch_mask], mask0 = self.tta_rotation_postprocess(mkpts0[batch_mask], img_np1, rot_l10_M0_inv)\n                    mkpts1[batch_mask], mask1 = self.tta_rotation_postprocess(mkpts1[batch_mask], img_np2, rot_l10_M1_inv)\n                    confidence[batch_mask] += (~(mask0 & mask1)).astype(np.float32) * -10.\n                else:\n                    raise ValueError('Unknown TTA method.')\n                    \n            if conf_th is not None:\n                th_mask = confidence >= conf_th\n            else:\n                th_mask = confidence >= 0.\n            mkpts0, mkpts1 = mkpts0[th_mask, :], mkpts1[th_mask, :]\n\n            # Matching points\n            return mkpts0, mkpts1","metadata":{"execution":{"iopub.status.busy":"2022-06-01T16:44:00.093753Z","iopub.execute_input":"2022-06-01T16:44:00.093997Z","iopub.status.idle":"2022-06-01T16:44:58.577069Z","shell.execute_reply.started":"2022-06-01T16:44:00.093965Z","shell.execute_reply":"2022-06-01T16:44:58.576276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SuperGlue","metadata":{}},{"cell_type":"code","source":"# Install superglue\nforce_superglue_reinstall = False\n\nif 'superglue' not in INSTALLED_LOG or force_superglue_reinstall:\n    !mkdir /tmp/superpoint\n    !cp -r ../input/super-glue-pretrained-network/models /tmp/superpoint/superpoint\n    !ls /tmp/superpoint/superpoint\n    !touch /tmp/superpoint/superpoint/__init__.py\n    INSTALLED_LOG['superglue'] = True\nelse:\n    print('Already installed SuperGlue. Set \"force_superglue_reinstall=True\" to override this behavior.')\n\n# Import superglue\nimport sys\nsys.path.append(\"/tmp/superpoint\")\nfrom superpoint.superpoint import SuperPoint\nfrom superpoint.superglue import SuperGlue\n\n\nclass SuperGlueCustomMatching(torch.nn.Module):\n    \"\"\" Image Matching Frontend (SuperPoint + SuperGlue) \"\"\"\n    def __init__(self, config={}):\n        super().__init__()\n        self.superpoint = SuperPoint(config.get('superpoint', {}))\n        self.superglue = SuperGlue(config.get('superglue', {}))\n\n    def forward(self, data):\n        \"\"\" Run SuperPoint (optionally) and SuperGlue\n        SuperPoint is skipped if ['keypoints0', 'keypoints1'] exist in input\n        Args:\n          data: dictionary with minimal keys: ['image0', 'image1']\n        \"\"\"\n        pred = {}\n\n        # Extract SuperPoint (keypoints, scores, descriptors) if not provided\n        if 'keypoints0' not in data:\n            pred0 = self.superpoint({'image': data['image0']})\n            pred = {**pred, **{k+'0': v for k, v in pred0.items()}}\n        if 'keypoints1' not in data:\n            pred1 = self.superpoint({'image': data['image1']})\n            pred = {**pred, **{k+'1': v for k, v in pred1.items()}}\n\n        # Batch all features\n        # We should either have i) one image per batch, or\n        # ii) the same number of local features for all images in the batch.\n        data = {**data, **pred}\n\n        pred_list = []\n        for i in range(data['image0'].shape[0]):\n            ith_data = {k: v[i:i+1] for k, v in data.items()}\n            for k in ith_data:\n                if isinstance(ith_data[k], (list, tuple)):\n                    ith_data[k] = torch.stack(ith_data[k])\n            ith_pred = {\n                **{k: v[i] for k, v in pred.items()},\n                **self.superglue(ith_data)\n            }\n            pred_list.append(ith_pred)\n        return {k: [p[k] for p in pred_list] for k in ith_pred}\n\n\nclass SuperGlueMatcher:\n    def __init__(self, device=None):\n        config = {\n            \"superpoint\": {\n                \"nms_radius\": 4,\n                \"keypoint_threshold\": 0.005,\n                \"max_keypoints\": 1024\n            },\n            \"superglue\": {\n                \"weights\": \"outdoor\",\n                \"sinkhorn_iterations\": 100,\n                \"match_threshold\": 0.2,\n            }\n        }\n        self.device = device\n        self._superglue_matcher = SuperGlueCustomMatching(config).eval().to(device)\n        \n        # utils\n        self._clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n    \n    def prep_np_img(self, img):\n        return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    def frame2tensor(self, frame):\n        return (torch.from_numpy(frame).float()/255.)[None, None].to(self.device)\n            \n    def tta_rotation_preprocess(self, img_np, angle):\n        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n        rot_img = self.frame2tensor(cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0])))\n        return rot_M, rot_img, rot_M_inv\n\n    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n        hom = np.concatenate([kpts, ones], 1)\n        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n        return rot_kpts, mask\n\n    def __call__(self, img_np0, img_np1, tta=['orig'], conf_th=None):\n        with torch.no_grad():\n            img_np0 = self.prep_np_img(img_np0)\n            img_np1 = self.prep_np_img(img_np1)\n\n            img_ts0 = self.frame2tensor(img_np0)\n            img_ts1 = self.frame2tensor(img_np1)\n            images0, images1 = [], []\n\n            # TTA\n            for tta_elem in tta:\n                if tta_elem == 'orig':\n                    img_ts0_aug, img_ts1_aug = img_ts0, img_ts1\n                elif tta_elem == 'flip_lr':\n                    img_ts0_aug = torch.flip(img_ts0, [3, ])\n                    img_ts1_aug = torch.flip(img_ts1, [3, ])\n                elif tta_elem == 'flip_ud':\n                    img_ts0_aug = torch.flip(img_ts0, [2, ])\n                    img_ts1_aug = torch.flip(img_ts1, [2, ])\n                elif tta_elem == 'rot_r10':\n                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np0, 10)\n                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np1, 10)\n                elif tta_elem == 'rot_l10':\n                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np0, -10)\n                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np1, -10)\n                elif tta_elem == 'eqhist':\n                    img_ts0_aug = self.frame2tensor(cv2.equalizeHist(img_np0))\n                    img_ts1_aug = self.frame2tensor(cv2.equalizeHist(img_np1))\n                elif tta_elem == 'clahe':\n                    img_ts0_aug = self.frame2tensor(self._clahe.apply(img_np0))\n                    img_ts1_aug = self.frame2tensor(self._clahe.apply(img_np1))\n                else:\n                    raise ValueError('Unknown TTA method.')\n\n                images0.append(img_ts0_aug)\n                images1.append(img_ts1_aug)\n\n            # Inference\n            pred = self._superglue_matcher({\n                \"image0\": torch.cat(images0),\n                \"image1\": torch.cat(images1)})\n\n            # Reverse TTA\n            mkpts0, mkpts1, mconf = [], [], []\n            for idx, tta_elem in enumerate(tta):\n                pred_aug = {k: v[idx].detach().cpu().numpy().squeeze() for k, v in pred.items()}\n                kpts0, kpts1 = pred_aug[\"keypoints0\"], pred_aug[\"keypoints1\"]\n                matches, conf = pred_aug[\"matches0\"], pred_aug[\"matching_scores0\"]\n\n                if conf_th is None:\n                    valid = matches > -1\n                else:\n                    valid = (matches > -1) & (conf >= conf_th)\n                aug_mkpts0 = kpts0[valid]\n                aug_mkpts1 = kpts1[matches[valid]]\n                mconf.append(conf[valid])\n                \n                if tta_elem in {'orig', 'eqhist', 'clahe'}:\n                    pass\n                elif tta_elem == 'flip_lr':\n                    aug_mkpts0[:, 0] = img_np0.shape[1] - aug_mkpts0[:, 0]\n                    aug_mkpts1[:, 0] = img_np1.shape[1] - aug_mkpts1[:, 0]\n                elif tta_elem == 'flip_ud':\n                    aug_mkpts0[:, 1] = img_np0.shape[0] - aug_mkpts0[:, 1]\n                    aug_mkpts1[:, 1] = img_np1.shape[0] - aug_mkpts1[:, 1]\n                elif tta_elem == 'rot_r10':\n                    aug_mkpts0, mask0 = self.tta_rotation_postprocess(aug_mkpts0, img_np0, rot_r10_M0_inv)\n                    aug_mkpts1, mask1 = self.tta_rotation_postprocess(aug_mkpts1, img_np1, rot_r10_M1_inv)\n                    aug_mkpts0, aug_mkpts1 = aug_mkpts0[mask0 & mask1], aug_mkpts1[mask0 & mask1]\n                elif tta_elem == 'rot_l10':\n                    aug_mkpts0, mask0 = self.tta_rotation_postprocess(aug_mkpts0, img_np0, rot_l10_M0_inv)\n                    aug_mkpts1, mask1 = self.tta_rotation_postprocess(aug_mkpts1, img_np1, rot_l10_M1_inv)\n                    aug_mkpts0, aug_mkpts1 = aug_mkpts0[mask0 & mask1], aug_mkpts1[mask0 & mask1]\n                else:\n                    raise ValueError('Unknown TTA method.')\n                \n                mkpts0.append(aug_mkpts0)\n                mkpts1.append(aug_mkpts1)\n\n            # Matching points\n            return np.concatenate(mkpts0), np.concatenate(mkpts1)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-01T16:44:58.578683Z","iopub.execute_input":"2022-06-01T16:44:58.578955Z","iopub.status.idle":"2022-06-01T16:45:03.738517Z","shell.execute_reply.started":"2022-06-01T16:44:58.578917Z","shell.execute_reply":"2022-06-01T16:45:03.737599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SuperGlueCustomMatchingV2(torch.nn.Module):\n    \"\"\" Image Matching Frontend (SuperPoint + SuperGlue) \"\"\"\n    def __init__(self, config={}, device=None):\n        super().__init__()\n        self.superpoint = SuperPoint(config.get('superpoint', {}))\n        self.superglue = SuperGlue(config.get('superglue', {}))\n\n        self.tta_map = {\n            'orig': self.untta_none,\n            'eqhist': self.untta_none,\n            'clahe': self.untta_none,\n            'flip_lr': self.untta_fliplr,\n            'flip_ud': self.untta_flipud,\n            'rot_r10': self.untta_rotr10,\n            'rot_l10': self.untta_rotl10\n        }\n        self.device = device\n\n    def forward_cross(self, data, ttas=['orig', ], tta_groups=[('orig', 'orig')]):\n        pred = {}\n\n        # Extract SuperPoint (keypoints, scores, descriptors) if not provided\n        sp_st = time.time()\n        if 'keypoints0' not in data:\n            pred0 = self.superpoint({'image': data['image0']})\n            pred = {**pred, **{k+'0': v for k, v in pred0.items()}}\n        if 'keypoints1' not in data:\n            pred1 = self.superpoint({'image': data['image1']})\n            pred = {**pred, **{k+'1': v for k, v in pred1.items()}}\n        sp_nd = time.time()\n\n        # Batch all features\n        # We should either have i) one image per batch, or\n        # ii) the same number of local features for all images in the batch.\n        data = {**data, **pred}\n\n        # Group predictions (list, with elements with matches{0,1}, matching_scores{0,1} keys)\n        group_pred_list = []\n        tta2id = {k: i for i, k in enumerate(ttas)}\n        for tta_group in tta_groups:\n            group_idx = tta2id[tta_group[0]], tta2id[tta_group[1]]\n            group_data = {\n                **{f'image{i}': data[f'image{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n                **{f'keypoints{i}': data[f'keypoints{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n                **{f'descriptors{i}': data[f'descriptors{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n                **{f'scores{i}': data[f'scores{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n            }\n\n            for k in group_data:\n                if isinstance(group_data[k], (list, tuple)):\n                    group_data[k] = torch.stack(group_data[k])\n\n            group_sg_pred = self.superglue(group_data)\n            group_pred_list.append(group_sg_pred)\n\n        # UnTTA\n        data['scores0'] = list(data['scores0'])\n        data['scores1'] = list(data['scores1'])\n        for i in range(len(data['keypoints0'])):\n            data['keypoints0'][i], data['descriptors0'][i], data['scores0'][i] = self.tta_map[ttas[i]](\n                data['keypoints0'][i], data['descriptors0'][i], data['scores0'][i],\n                w=data['image0'].shape[3], h=data['image0'].shape[2], inplace=True, mask_illegal=False)\n\n            data['keypoints1'][i], data['descriptors1'][i], data['scores1'][i] = self.tta_map[ttas[i]](\n                data['keypoints1'][i], data['descriptors1'][i], data['scores1'][i],\n                w=data['image1'].shape[3], h=data['image1'].shape[2], inplace=True, mask_illegal=False)\n\n        # Sooo... groups?\n        for group_pred, tta_group in zip(group_pred_list, tta_groups):\n            group_idx = tta2id[tta_group[0]], tta2id[tta_group[1]]\n            group_pred.update({\n                **{f'keypoints{i}': data[f'keypoints{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n                **{f'scores{i}': data[f'scores{i}'][group_idx[i]:group_idx[i]+1] for i in [0, 1]},\n            })\n        return group_pred_list\n\n\n    def untta_none(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        if not inplace:\n            keypoints = keypoints.clone()\n        return keypoints, descriptors, scores\n    \n    def untta_fliplr(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        if not inplace:\n            keypoints = keypoints.clone()\n        keypoints[:, 0] = w - keypoints[:, 0] - 1.\n        return keypoints, descriptors, scores\n\n    def untta_flipud(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        if not inplace:\n            keypoints = keypoints.clone()\n        keypoints[:, 1] = h - keypoints[:, 1] - 1.\n        return keypoints, descriptors, scores\n\n    def untta_rotr10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        # rotr10 is +10, inverse is -10\n        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), -15, 1)).to(torch.float32).to(self.device)\n        ones = torch.ones_like(keypoints[:, 0])\n        hom = torch.cat([keypoints, ones[:, None]], 1)\n        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n        if mask_illegal:\n            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n        else:\n            return rot_kpts, descriptors, scores\n\n    def untta_rotl10(self, keypoints, descriptors, scores, w, h, inplace=True, mask_illegal=True):\n        # rotr10 is -10, inverse is +10\n        rot_M_inv = torch.from_numpy(cv2.getRotationMatrix2D((w / 2, h / 2), 15, 1)).to(torch.float32).to(self.device)\n        ones = torch.ones_like(keypoints[:, 0])\n        hom = torch.cat([keypoints, ones[:, None]], 1)\n        rot_kpts = torch.matmul(rot_M_inv, hom.T).T[:, :2]\n        if mask_illegal:\n            mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < w) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < h)\n            return rot_kpts[mask], descriptors[:, mask], scores[mask]\n        else:\n            return rot_kpts, descriptors, scores\n\n\nclass SuperGlueMatcherV2:\n    def __init__(self, device=None):\n        config = {\n            \"superpoint\": {\n                \"nms_radius\": 4,\n                \"keypoint_threshold\": 0.005,\n                \"max_keypoints\": 1024,\n            },\n            \"superglue\": {\n                \"weights\": \"outdoor\",\n                \"sinkhorn_iterations\": 100,\n                \"match_threshold\": 0.2,\n            }\n        }\n        self.device = device\n        self._superglue_matcher = SuperGlueCustomMatchingV2(\n            config=config, device=self.device,\n            ).eval().to(device)\n    \n    def prep_np_img(self, img, long_side=None):\n        return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    def frame2tensor(self, frame):\n        return (torch.from_numpy(frame).float()/255.)[None, None].to(self.device)\n            \n    def tta_rotation_preprocess(self, img_np, angle):\n        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n        rot_img = self.frame2tensor(cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0])))\n        return rot_M, rot_img, rot_M_inv\n\n    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n        hom = np.concatenate([kpts, ones], 1)\n        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n        return rot_kpts, mask\n\n    def __call__(self, img_np0, img_np1, tta_groups=[['orig']], forward_type='cross', conf_th=None):\n        with torch.no_grad():\n            img_np0 = self.prep_np_img(img_np0)\n            img_np1 = self.prep_np_img(img_np1)\n\n            img_ts0 = self.frame2tensor(img_np0)\n            img_ts1 = self.frame2tensor(img_np1)\n            images0, images1 = [], []\n\n            tta = []\n            for tta_g in tta_groups:\n                tta += tta_g\n            tta = list(set(tta))\n\n            # TTA\n            for tta_elem in tta:\n                if tta_elem == 'orig':\n                    img_ts0_aug, img_ts1_aug = img_ts0, img_ts1\n                elif tta_elem == 'flip_lr':\n                    img_ts0_aug = torch.flip(img_ts0, [3, ])\n                    img_ts1_aug = torch.flip(img_ts1, [3, ])\n                elif tta_elem == 'flip_ud':\n                    img_ts0_aug = torch.flip(img_ts0, [2, ])\n                    img_ts1_aug = torch.flip(img_ts1, [2, ])\n                elif tta_elem == 'rot_r10':\n                    rot_r10_M0, img_ts0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_np0, 15)\n                    rot_r10_M1, img_ts1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_np1, 15)\n                elif tta_elem == 'rot_l10':\n                    rot_l10_M0, img_ts0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_np0, -15)\n                    rot_l10_M1, img_ts1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_np1, -15)\n                elif tta_elem == 'eqhist':\n                    img_ts0_aug = self.frame2tensor(cv2.equalizeHist(img_np0))\n                    img_ts1_aug = self.frame2tensor(cv2.equalizeHist(img_np1))\n                elif tta_elem == 'clahe':\n                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n                    img_ts0_aug = self.frame2tensor(clahe.apply(img_np0))\n                    img_ts1_aug = self.frame2tensor(clahe.apply(img_np1))\n                else:\n                    raise ValueError('Unknown TTA method.')\n\n                images0.append(img_ts0_aug)\n                images1.append(img_ts1_aug)\n\n            # Inference\n            if forward_type == 'cross':\n                pred = self._superglue_matcher.forward_cross(\n                    data={\n                        \"image0\": torch.cat(images0),\n                        \"image1\": torch.cat(images1)\n                    },\n                    ttas=tta, tta_groups=tta_groups)\n            elif forward_type == 'flat':\n                pred = self._superglue_matcher.forward_flat(\n                data={\n                    \"image0\": torch.cat(images0),\n                    \"image1\": torch.cat(images1)\n                },\n                ttas=tta, tta_groups=tta_groups)\n            else:\n                raise RuntimeError(f'Unknown forward_type {forward_type}')\n\n            mkpts0, mkpts1, mconf = [], [], []\n            for group_pred in pred:\n                pred_aug = {k: v[0].detach().cpu().numpy().squeeze() for k, v in group_pred.items()}\n                kpts0, kpts1 = pred_aug[\"keypoints0\"], pred_aug[\"keypoints1\"]\n                matches, conf = pred_aug[\"matches0\"], pred_aug[\"matching_scores0\"]\n\n                if conf_th is None:\n                    valid = matches > -1\n                else:\n                    valid = (matches > -1) & (conf >= conf_th)\n                mkpts0.append(kpts0[valid])\n                mkpts1.append(kpts1[matches[valid]])\n                mconf.append(conf[valid])\n\n            cat_mkpts0 = np.concatenate(mkpts0)\n            cat_mkpts1 = np.concatenate(mkpts1)\n            mask0 = (cat_mkpts0[:, 0] >= 0) & (cat_mkpts0[:, 0] < img_np0.shape[1]) & (cat_mkpts0[:, 1] >= 0) & (cat_mkpts0[:, 1] < img_np0.shape[0])\n            mask1 = (cat_mkpts1[:, 0] >= 0) & (cat_mkpts1[:, 0] < img_np1.shape[1]) & (cat_mkpts1[:, 1] >= 0) & (cat_mkpts1[:, 1] < img_np1.shape[0])\n            return cat_mkpts0[mask0 & mask1], cat_mkpts1[mask0 & mask1]","metadata":{"execution":{"iopub.status.busy":"2022-06-01T16:55:33.432016Z","iopub.execute_input":"2022-06-01T16:55:33.432319Z","iopub.status.idle":"2022-06-01T16:55:33.49598Z","shell.execute_reply.started":"2022-06-01T16:55:33.432271Z","shell.execute_reply":"2022-06-01T16:55:33.49532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DKM","metadata":{}},{"cell_type":"code","source":"# Install DKM and move checkpoints to the local checkpoints dir\nforce_dkm_reinstall = False\n\nif 'DKM' not in INSTALLED_LOG or force_dkm_reinstall:\n    !mkdir -p pretrained/checkpoints\n    !cp /kaggle/input/imc2022-dependencies/pretrained/dkm.pth pretrained/checkpoints/dkm_base_v11.pth\n\n    !pip install -f /kaggle/input/imc2022-dependencies/wheels --no-index einops\n    !cp -r /kaggle/input/imc2022-dependencies/DKM/ /kaggle/working/DKM/\n    !cd /kaggle/working/DKM/; pip install -f /kaggle/input/imc2022-dependencies/wheels -e .\n    INSTALLED_LOG['DKM'] = True\nelse:\n    print('Already installed DKM. Set \"force_dkm_reinstall=True\" to override this behavior.')\n\n# imports for DKM\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sys, os, csv\nfrom PIL import Image\nimport cv2, gc\nimport matplotlib.pyplot as plt\nimport torch\nimport types\nimport torch.nn.functional as TorchFunc\nsys.path.append('/kaggle/input/imc2022-dependencies/DKM/')\n\ndry_run = False\n\n\nfrom dkm import dkm_base\nfrom torchvision import transforms\n\n\ndef custom_stable_neighbours(self, query_coords, query_to_support, support_to_query):\n    qts = query_to_support\n    for t in range(4):\n        _qts = qts\n        q = TorchFunc.grid_sample(support_to_query, qts, mode=\"bilinear\")\n        qts = TorchFunc.grid_sample(\n            query_to_support.permute(0, 3, 1, 2),\n            q.permute(0, 2, 3, 1),\n            mode=\"bilinear\",\n        ).permute(0, 2, 3, 1)\n    d = (qts - _qts).norm(dim=-1)\n    qd = (q - query_coords).norm(dim=1)\n    stabneigh = torch.logical_and(d < 1e-3, qd < 5e-3)\n    return q, qts, stabneigh\n\n\ndef custom_match(\n        self,\n        im1,\n        im2,\n        batched=False,\n        check_cycle_consistency=False,\n        do_pred_in_og_res=False,\n    ):\n    self.train(False)\n    with torch.no_grad():\n        if not batched:\n            b = 1\n            w, h = im1.size\n            w2, h2 = im2.size\n            # Get images in good format\n            ws = self.w_resized\n            hs = self.h_resized\n            test_transform = get_tuple_transform_ops(\n                resize=(hs, ws), normalize=True\n            )\n            query, support = test_transform((im1, im2))\n            batch = {\"query\": query[None].cuda(), \"support\": support[None].cuda()}\n        else:\n            b, c, h, w = im1.shape\n            b, c, h2, w2 = im2.shape\n            assert w == w2 and h == h2, \"wat\"\n            batch = {\"query\": im1.cuda(), \"support\": im2.cuda()}\n            hs, ws = self.h_resized, self.w_resized\n        finest_scale = 1  # i will assume that we go to the finest scale (otherwise min(list(dense_corresps.keys())) also works)\n        # Run matcher\n        if check_cycle_consistency:\n            dense_corresps = self.forward_symmetric(batch)\n            query_to_support, support_to_query = dense_corresps[finest_scale][\n                \"dense_flow\"\n            ].chunk(2)\n            query_to_support = query_to_support.permute(0, 2, 3, 1)\n            dense_certainty, dc_s = dense_corresps[finest_scale][\n                \"dense_certainty\"\n            ].chunk(\n                2\n            )  # TODO: Here we could also use the reverse certainty\n        else:\n            dense_corresps = self.forward(batch)\n            query_to_support = dense_corresps[finest_scale][\"dense_flow\"].permute(\n                0, 2, 3, 1\n            )\n            # Get certainty interpolation\n            dense_certainty = dense_corresps[finest_scale][\"dense_certainty\"]\n\n        if do_pred_in_og_res:  # Will assume that there is no batching going on.\n            og_query, og_support = self.og_transforms((im1, im2))\n            query_to_support, dense_certainty = self.decoder.upsample_preds(\n                query_to_support,\n                dense_certainty,\n                og_query.cuda()[None],\n                og_support.cuda()[None],\n            )\n            hs, ws = h, w\n        # Create im1 meshgrid\n        query_coords = torch.meshgrid(\n            (\n                torch.linspace(-1 + 1 / hs, 1 - 1 / hs, hs, device=\"cuda:0\"),\n                torch.linspace(-1 + 1 / ws, 1 - 1 / ws, ws, device=\"cuda:0\"),\n            )\n        )\n        query_coords = torch.stack((query_coords[1], query_coords[0]))\n        query_coords = query_coords[None].expand(b, 2, hs, ws)\n        dense_certainty = dense_certainty.sigmoid()  # logits -> probs\n        if check_cycle_consistency:\n            query_coords, query_to_support, stabneigh = self.custom_stable_neighbours(\n                query_coords, query_to_support, support_to_query\n            )\n            dense_certainty *= stabneigh[:, None, :, :].float() + 1e-3\n        # Return only matches better than threshold\n        query_coords = query_coords.permute(0, 2, 3, 1)\n\n        query_to_support = torch.clamp(query_to_support, -1, 1)\n        if batched:\n            return torch.cat((query_coords, query_to_support), dim=-1), dense_certainty[:, 0]\n        else:\n            return torch.cat((query_coords, query_to_support), dim=-1)[0], dense_certainty[0, 0]\n\n\nclass DKMMatcher:\n    _DEFAULT_CONFIG = {\n        'w': 512, 'h': 384,\n        'thresh': {\n            'method': 'abs_rnd',\n            'th': 0.9,\n            'take': 100,\n        }\n    }\n    def __init__(self, device=None, config=_DEFAULT_CONFIG):\n        torch.hub.set_dir('/kaggle/working/pretrained/')\n        self._dkm_matcher = dkm_base(pretrained=True, version=\"v11\").to(device).eval()\n        self.config = config\n        self._dkm_matcher.w_resized = config['w']\n        self._dkm_matcher.h_resized = config['h']\n        self._dkm_matcher.custom_stable_neighbours = types.MethodType(custom_stable_neighbours, self._dkm_matcher)\n        self._dkm_matcher.custom_match = types.MethodType(custom_match, self._dkm_matcher)\n        self.device=device\n\n        mean=[0.485, 0.456, 0.406]\n        std=[0.229, 0.224, 0.225]\n        self.normalize = transforms.Normalize(mean=mean, std=std)\n\n    def prepare_torch_image(self, img):\n        \"\"\"img - BGR image array\"\"\"\n        img_ts = K.image_to_tensor(img, False).float() / 255.\n        img_ts = K.color.bgr_to_rgb(img_ts)\n        return self.normalize(img_ts)\n\n    def results_thresholding(self, dense_matches, dense_certainty):\n        if self.config['thresh']['method'] == 'abs_rnd':\n            n_take = (dense_certainty >= self.config['thresh']['th']).count_nonzero().cpu().numpy()\n            n_take = min(max(n_take, 0), self.config['thresh']['take'])\n            sparse_matches, sparse_certainty = self._dkm_matcher.sample(dense_matches, dense_certainty, num=n_take)\n        elif self.config['thresh']['method'] == 'rel':\n            sparse_matches, sparse_certainty = self._dkm_matcher.sample(\n                dense_matches, dense_certainty,\n                num=self.config['thresh']['take'], relative_confidence_threshold=self.config['thresh']['th'])\n        elif self.config['thresh']['method'] == 'abs':\n            matches, certainty = (\n                dense_matches.reshape(-1, 4),\n                dense_certainty.reshape(-1),\n            )\n            th_matches, th_certainty = (\n                matches[certainty > self.config['thresh']['th']].cpu().numpy(),\n                certainty[certainty > self.config['thresh']['th']].cpu().numpy(),\n            )\n            if len(th_matches) > 0:\n                good_samples = np.random.choice(\n                    np.arange(len(th_matches)),\n                    size=min(self.config['thresh']['take'], len(th_certainty)),\n                    replace=False,\n                    p=th_certainty / (np.sum(th_certainty) + 1e-6),\n                )\n                sparse_matches = th_matches[good_samples]\n            else:\n                sparse_matches = th_matches\n        else:\n            raise ValueError('Unknown thresholding method ' + str(self.config['thresh']['method']))\n        return sparse_matches\n\n    def tta_rotation_preprocess(self, img_np, angle):\n        rot_M = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), angle, 1)\n        rot_M_inv = cv2.getRotationMatrix2D((img_np.shape[1] / 2, img_np.shape[0] / 2), -angle, 1)\n        rot_img = cv2.warpAffine(img_np, rot_M, (img_np.shape[1], img_np.shape[0]))\n        return rot_M, rot_img, rot_M_inv\n\n    def tta_rotation_postprocess(self, kpts, img_np, rot_M_inv):\n        ones = np.ones(shape=(kpts.shape[0], ), dtype=np.float32)[:, None]\n        hom = np.concatenate([kpts, ones], 1)\n        rot_kpts = rot_M_inv.dot(hom.T).T[:, :2]\n        mask = (rot_kpts[:, 0] >= 0) & (rot_kpts[:, 0] < img_np.shape[1]) & (rot_kpts[:, 1] >= 0) & (rot_kpts[:, 1] < img_np.shape[0])\n        return rot_kpts, mask\n\n\n    def __call__(self, img_bgr_np1, img_bgr_np2, tta=['orig'], verbose=0):\n        with torch.no_grad():            \n            # TTA preparation. Affine first, and rotation only after\n            ttaprep_st = time.time()  \n            images0, images1 = [], []\n            for tta_elem in tta:\n                if tta_elem == 'orig':\n                    img_np0_aug, img_np1_aug = img_bgr_np1, img_bgr_np2\n                elif tta_elem == 'flip_lr':\n                    img_np0_aug = np.flip(img_bgr_np1, [1, ]).copy()\n                    img_np1_aug = np.flip(img_bgr_np2, [1, ]).copy()\n                elif tta_elem == 'flip_ud':\n                    img_np0_aug = np.flip(img_bgr_np1, [0, ]).copy()\n                    img_np1_aug = np.flip(img_bgr_np2, [0, ]).copy()\n                elif tta_elem == 'rot_r10':\n                    rot_r10_M0, img_np0_aug, rot_r10_M0_inv = self.tta_rotation_preprocess(img_bgr_np1, 10)\n                    rot_r10_M1, img_np1_aug, rot_r10_M1_inv = self.tta_rotation_preprocess(img_bgr_np2, 10)\n                elif tta_elem == 'rot_l10':\n                    rot_l10_M0, img_np0_aug, rot_l10_M0_inv = self.tta_rotation_preprocess(img_bgr_np1, -10)\n                    rot_l10_M1, img_np1_aug, rot_l10_M1_inv = self.tta_rotation_preprocess(img_bgr_np2, -10)\n                else:\n                    raise ValueError('Unknown TTA method.')\n\n                # Rotation is after for 2 reasons:\n                #   - I think rotation on smaller scale might lose some information\n                #   - Avoid any weird stuff in combining affine transformations\n                img_np0_aug = cv2.resize(img_np0_aug, (self._dkm_matcher.w_resized, self._dkm_matcher.h_resized))\n                img_np1_aug = cv2.resize(img_np1_aug, (self._dkm_matcher.w_resized, self._dkm_matcher.h_resized))\n                images0.append(self.prepare_torch_image(img_np0_aug))\n                images1.append(self.prepare_torch_image(img_np1_aug))\n            ttaprep_nd = time.time()\n\n            # Batched inference\n            batchinf_st = time.time()  # -- start\n            img0_batch_ts, img1_batch_ts = torch.cat(images0), torch.cat(images1)\n            dense_matches, dense_certainty = self._dkm_matcher.custom_match(\n                img0_batch_ts, img1_batch_ts, batched=True,\n                check_cycle_consistency=self.config['check_cycle_consistency'])\n            batchinf_nd = time.time()  # -- end\n\n            # SQRT of dense certainty\n            dense_certainty = dense_certainty.sqrt()\n\n            # Get sparse matching keypoints\n            getkpts_st = time.time()\n            mkps1, mkps2 = [], []\n            for idx, tta_elem in enumerate(tta):\n                ith_sparse_matches = self.results_thresholding(dense_matches[idx], dense_certainty[idx])\n\n                aug_mkps1 = ith_sparse_matches[:, :2]\n                aug_mkps2 = ith_sparse_matches[:, 2:]\n\n                h, w, c = img_bgr_np1.shape\n                aug_mkps1[:, 0] = ((aug_mkps1[:, 0] + 1.)/2.) * w\n                aug_mkps1[:, 1] = ((aug_mkps1[:, 1] + 1.)/2.) * h\n\n                h, w, c = img_bgr_np2.shape\n                aug_mkps2[:, 0] = ((aug_mkps2[:, 0] + 1.)/2.) * w\n                aug_mkps2[:, 1] = ((aug_mkps2[:, 1] + 1.)/2.) * h\n                \n                mkps1.append(aug_mkps1)\n                mkps2.append(aug_mkps2)\n            getkpts_nd = time.time()\n\n            # Reverse TTA\n            revtta_st = time.time()\n            for idx, tta_elem in enumerate(tta):\n                if tta_elem == 'orig':\n                    pass\n                elif tta_elem == 'flip_lr':\n                    mkps1[idx][:, 0] = img_bgr_np1.shape[1] - mkps1[idx][:, 0]\n                    mkps2[idx][:, 0] = img_bgr_np2.shape[1] - mkps2[idx][:, 0]\n                elif tta_elem == 'flip_ud':\n                    mkps1[idx][:, 1] = img_bgr_np1.shape[0] - mkps1[idx][:, 1]\n                    mkps2[idx][:, 1] = img_bgr_np2.shape[0] - mkps2[idx][:, 1] \n                elif tta_elem == 'rot_r10':\n                    mkps1[idx], mask0 = self.tta_rotation_postprocess(mkps1[idx], img_bgr_np1, rot_r10_M0_inv)\n                    mkps2[idx], mask1 = self.tta_rotation_postprocess(mkps2[idx], img_bgr_np2, rot_r10_M1_inv)\n                    mkps1[idx], mkps2[idx] = mkps1[idx][mask0 & mask1], mkps2[idx][mask0 & mask1]\n                elif tta_elem == 'rot_l10':\n                    mkps1[idx], mask0 = self.tta_rotation_postprocess(mkps1[idx], img_bgr_np1, rot_l10_M0_inv)\n                    mkps2[idx], mask1 = self.tta_rotation_postprocess(mkps2[idx], img_bgr_np2, rot_l10_M1_inv)\n                    mkps1[idx], mkps2[idx] = mkps1[idx][mask0 & mask1], mkps2[idx][mask0 & mask1]\n                else:\n                    raise ValueError('Unknown TTA method.')\n            revtta_nd = time.time()\n\n            if verbose >= 1:\n                print('    - DKM inner:')\n                print('      - ttaprep:', ttaprep_nd - ttaprep_st)\n                print('      - batchinf:', batchinf_nd - batchinf_st)\n                print('      - getkpts:', getkpts_nd - getkpts_st)\n                print('      - revtta:', revtta_nd - revtta_st)\n            return np.concatenate(mkps1), np.concatenate(mkps2)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-01T16:45:03.80597Z","iopub.execute_input":"2022-06-01T16:45:03.806283Z","iopub.status.idle":"2022-06-01T16:45:48.049905Z","shell.execute_reply.started":"2022-06-01T16:45:03.806246Z","shell.execute_reply":"2022-06-01T16:45:48.049058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'\n\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])","metadata":{"execution":{"iopub.status.busy":"2022-06-01T16:45:48.051322Z","iopub.execute_input":"2022-06-01T16:45:48.051563Z","iopub.status.idle":"2022-06-01T16:45:48.057658Z","shell.execute_reply.started":"2022-06-01T16:45:48.05153Z","shell.execute_reply":"2022-06-01T16:45:48.05699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import cv2\n\n\nclass IMCDataset(torch.utils.data.Dataset):\n    \"\"\"\n    \"\"\"\n    def __init__(self, test_csv, img_dir, ext='png', sizes={'orig': {'type': 'orig'}}):\n        \"\"\"\n        Stores the dataset metadata in following variables:\n          - self.test_samples: a list of rows, each row is represented as tuple:\n                               (sample_id, batch_id, image_1_id, image_2_id)\n          - self.img_dir: top-level directory of images. Each image can be\n                          found at path \"<img_dir>/<batch_id>/<img_id>.{png,jpg}\n          - sizes: image scales to prepare. Example:\n                   {name1: {type: longside, longside: 1200},\n                    name2: {type: orig},\n                    name3: {type: targetwh, w: 800, h: 600}}\n        \"\"\"\n        self.test_samples = []\n        with open(test_csv, 'r') as f:\n            reader = csv.reader(f, delimiter=',')\n            for i, row in enumerate(reader):\n                # Skip header.\n                if i == 0:\n                    continue\n                self.test_samples += [row]\n        self.img_dir = img_dir\n        self.ext = ext\n        self.sizes = sizes\n\n    def __len__(self):\n        return len(self.test_samples)\n\n    def __getitem__(self, idx):\n        \"\"\"Returns in format:\n        {\n            sample_id: sample_id\n            img0_orig_size: [w0, h0]\n            img1_orig_size: [w1, h1]\n            scales:\n                orig: [img0_orig_scale, img1_orig_scale]\n                ls1200: [img0_with_longside_1200, img1_with_longside_1200]\n        }\n        \"\"\"\n        row = self.test_samples[idx]\n        sample_id, batch_id, image_1_id, image_2_id = row\n\n        img_np0 = cv2.imread(f'{self.img_dir}/{batch_id}/{image_1_id}.{self.ext}')\n        img_np1 = cv2.imread(f'{self.img_dir}/{batch_id}/{image_2_id}.{self.ext}')\n        assert img_np0 is not None and img_np1 is not None\n\n        ret_dict = {\n            'sample_id': sample_id,\n            'img0_orig_size': [img_np0.shape[1], img_np0.shape[0]],\n            'img1_orig_size': [img_np1.shape[1], img_np1.shape[0]],\n            'scales': {}\n        }\n        for size_name, size_cfg in self.sizes.items():\n            ret_dict['scales'][size_name] = [\n                IMCDataset.process_image(img_np0, size_cfg),\n                IMCDataset.process_image(img_np1, size_cfg),\n            ]\n        return ret_dict\n\n    @staticmethod\n    def process_image(img, size_cfg):\n        if size_cfg['type'] == 'orig':\n            return img\n        elif size_cfg['type'] == 'targetwh':\n            return cv2.resize(\n                img, (size_cfg['w'], size_cfg['h']),\n                interpolation=cv2.INTER_CUBIC)\n        elif size_cfg['type'] == 'longside':\n            scale = float(size_cfg['longside']) / max(img.shape[0], img.shape[1])\n            w = int(img.shape[1] * scale)\n            h = int(img.shape[0] * scale)\n            return cv2.resize(img, (w, h), interpolation=cv2.INTER_CUBIC)\n        elif size_cfg['type'] == 'scalelim':\n            max_scale = float(size_cfg['max_longside']) / max(img.shape[0], img.shape[1])\n            eff_scale = min(float(size_cfg['scale']), max_scale)\n            w = int(img.shape[1] * (size_cfg['scale']))\n            h = int(img.shape[0] * (size_cfg['scale']))\n            return cv2.resize(img, (w, h), interpolation=cv2.INTER_CUBIC)\n\ndef get_imc_dataloader(test_csv, img_dir, ext='png', sizes={'orig': {'type': 'orig'}}):\n    imc_dataset = IMCDataset(test_csv, img_dir, ext, sizes)\n    imc_dataloader = torch.utils.data.DataLoader(\n        imc_dataset, batch_size=None, sampler=None, batch_sampler=None,\n        num_workers=1, collate_fn=None, prefetch_factor=4)\n    return imc_dataloader","metadata":{"execution":{"iopub.status.busy":"2022-06-01T16:55:16.719674Z","iopub.execute_input":"2022-06-01T16:55:16.720113Z","iopub.status.idle":"2022-06-01T16:55:16.737395Z","shell.execute_reply.started":"2022-06-01T16:55:16.720078Z","shell.execute_reply":"2022-06-01T16:55:16.736697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\n\n\nclass MultiStageMatcher:\n    def __init__(self,\n                 stage1_matchers_cfg,\n                 stage2_matchers_cfg,\n                 crop_roi_method='max_box',\n                 crop_roi_min_side=200,\n                 crop_roi_margin=50,\n                 crop_roi_inliers_only=True,\n                 ):\n        self.stage1_matchers_cfg = stage1_matchers_cfg\n        self.stage2_matchers_cfg = stage2_matchers_cfg\n\n        # Crop Roi config\n        self.crop_roi_method = crop_roi_method\n        self.crop_roi_min_side = crop_roi_min_side\n        self.crop_roi_margin = crop_roi_margin\n        self.crop_roi_inliers_only = crop_roi_inliers_only\n        assert crop_roi_method in ['max_box', 'prob_3sigma'], \"Unknown crop_roi_method\"\n\n    def calc_roi_coords(self, w, h, mkpts, margin=40):\n        \"\"\"\n        mkpts is a list [(x1, y1), (x2, y2), ...],\n        where x is w axis and y is h axis.\n        \"\"\"\n        def wiggle(a, b, minl, bound):\n            \"\"\"evenly pad a and b so that [a, b) have length minl\"\"\"\n            if b < a:\n                a, b = b, a\n            if minl >= bound:\n                return 0, bound\n            if (b - a) >= minl:\n                return a, b\n            d = (minl - (b - a))\n            pad_l, pad_r = d // 2, d - d // 2\n            na, nb = a - pad_l, b + pad_r\n            if a < 0:\n                a, b = 0, minl\n            if b >= bound:\n                a, b = bound - minl, bound\n            return a, b\n\n        if self.crop_roi_method == 'max_box':\n            left, right = int(np.floor(mkpts[:, 0].min())), int(np.ceil(mkpts[:, 0].max()))\n            top, bottom = int(np.floor(mkpts[:, 1].min())), int(np.ceil(mkpts[:, 1].max()))\n            left, right = wiggle(left, right, self.crop_roi_min_side, w)\n            top, bottom = wiggle(top, bottom, self.crop_roi_min_side, h)\n            left, right = max(0, left - margin), min(right + margin, w)\n            top, bottom = max(0, top - margin), min(bottom + margin, h)\n        return left, right, top, bottom\n\n    def __call__(self, row_dict, verbose=0, ret_ims=False, ext='png'):\n        st = time.time()\n        orig_w0, orig_h0 = row_dict['img0_orig_size']\n        orig_w1, orig_h1 = row_dict['img1_orig_size']\n\n        if len(self.stage2_matchers_cfg) > 0 or ret_ims:\n            img_orig0 = row_dict['scales']['original'][0].numpy()\n            img_orig1 = row_dict['scales']['original'][1].numpy()\n\n        # ---------------------------\n        # STAGE 1\n        # ---------------------------\n        max_name_len = 0\n        stage1_incl_final_mkpts0, stage1_incl_final_mkpts1 = [], []\n        stage1_mkpts0, stage1_mkpts1, runtime_str, kp_count_str = [], [], [], []\n        for m_cfg in self.stage1_matchers_cfg:\n            max_name_len = max(len(m_cfg['name']), max_name_len)\n\n            # Inference\n            m_st = time.time()\n            img0, img1 = row_dict['scales'][m_cfg['input_key']]\n\n            # HACK: CONVERT BACK TO NUMPY ARRAY FOR FURTHER AUGS!!!!\n            if torch.is_tensor(img0):\n                img0 = img0.numpy()\n            if torch.is_tensor(img1):\n                img1 = img1.numpy()\n            row_dict['scales'][m_cfg['input_key']] = [img0, img1]\n\n            m_mkpts0, m_mkpts1 = m_cfg['fn'](img0, img1)\n\n            # Scale back\n            scale0_w, scale0_h = float(img0.shape[1] / orig_w0), float(img0.shape[0] / orig_h0)\n            scale1_w, scale1_h = float(img1.shape[1] / orig_w1), float(img1.shape[0] / orig_h1)\n            m_mkpts0[:, 0] /= scale0_w\n            m_mkpts0[:, 1] /= scale0_h\n            m_mkpts1[:, 0] /= scale1_w\n            m_mkpts1[:, 1] /= scale1_h\n            m_nd = time.time()\n\n            if m_cfg['use_for_roi']:\n                stage1_mkpts0.append(m_mkpts0)\n                stage1_mkpts1.append(m_mkpts1)\n            if m_cfg['use_for_final']:\n                stage1_incl_final_mkpts0.append(m_mkpts0)\n                stage1_incl_final_mkpts1.append(m_mkpts1)\n            runtime_str.append(f'(stage 1) {m_cfg[\"name\"].ljust(max_name_len)}: {m_nd - m_st:06f}s')\n            kp_count_str.append(f'(stage_1) {m_cfg[\"name\"]}={len(m_mkpts0)}')\n\n        stage1_mkpts0 = np.concatenate(stage1_mkpts0)\n        stage1_mkpts1 = np.concatenate(stage1_mkpts1)\n\n        # ---------------------------\n        # CROP ROI AND PREPARE STAGE 2\n        # ---------------------------\n\n        # quick inlier find\n        roi_st = time.time()\n        if len(self.stage2_matchers_cfg) > 0:\n            if self.crop_roi_inliers_only:\n                stage1_F, stage1_inliers = cv2.findFundamentalMat(\n                    stage1_mkpts0, stage1_mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 1000)\n                stage1_inlier_mask = np.array(\n                    [i[0] == 1 for i in stage1_inliers], dtype=np.bool)\n                stage1_mkpts0_inliers = stage1_mkpts0[stage1_inlier_mask]\n                stage1_mkpts1_inliers = stage1_mkpts1[stage1_inlier_mask]\n\n            roi_left0, roi_right0, roi_top0, roi_bottom0 = self.calc_roi_coords(\n                orig_w0, orig_h0, stage1_mkpts0_inliers, margin=self.crop_roi_margin)\n            roi_left1, roi_right1, roi_top1, roi_bottom1 = self.calc_roi_coords(\n                orig_w1, orig_h1, stage1_mkpts1_inliers, margin=self.crop_roi_margin)\n            stage2_img0 = img_orig0[roi_top0:roi_bottom0, roi_left0:roi_right0].copy()\n            stage2_img1 = img_orig1[roi_top1:roi_bottom1, roi_left1:roi_right1].copy()\n        roi_nd = time.time()\n\n        # ---------------------------\n        # STAGE 2\n        # ---------------------------\n        stage2_mkpts0, stage2_mkpts1 = [], []\n        for m_cfg in self.stage2_matchers_cfg:\n            # Inference\n            m_st = time.time()\n            img0 = IMCDataset.process_image(stage2_img0, m_cfg['resize'])\n            img1 = IMCDataset.process_image(stage2_img1, m_cfg['resize'])\n            m_mkpts0, m_mkpts1 = m_cfg['fn'](img0, img1)\n\n            # Scale back\n            scale0_w = float(img0.shape[1] / (roi_right0 - roi_left0))\n            scale0_h = float(img0.shape[0] / (roi_bottom0 - roi_top0))\n            scale1_w = float(img1.shape[1] / (roi_right1 - roi_left1))\n            scale1_h = float(img1.shape[0] / (roi_bottom1 - roi_top1))\n            m_mkpts0[:, 0] = float(roi_left0) + m_mkpts0[:, 0] / scale0_w\n            m_mkpts0[:, 1] = float(roi_top0) + m_mkpts0[:, 1] / scale0_h\n            m_mkpts1[:, 0] = float(roi_left1) + m_mkpts1[:, 0] / scale1_w\n            m_mkpts1[:, 1] = float(roi_top1) + m_mkpts1[:, 1] / scale1_h\n            m_nd = time.time()\n\n            stage2_mkpts0.append(m_mkpts0)\n            stage2_mkpts1.append(m_mkpts1)\n            runtime_str.append(f'(stage 2) {m_cfg[\"name\"].ljust(max_name_len)}: {m_nd - m_st:06f}s')\n            kp_count_str.append(f'(stage 2) {m_cfg[\"name\"]}={len(m_mkpts0)}')\n\n        mkpts0 = np.concatenate(stage1_incl_final_mkpts0 + stage2_mkpts0)\n        mkpts1 = np.concatenate(stage1_incl_final_mkpts1 + stage2_mkpts1)\n\n        if verbose >= 1:\n            print(\"  - Matching:\")\n            for s in runtime_str:\n                print(\"    -\", s)\n            print(f\"  - Stage 2 prep: {roi_nd - roi_st:06f}s\")\n            print(\"  - Keypoints:\", len(mkpts0), '|', ', '.join(kp_count_str))\n        if ret_ims:\n            if len(self.stage2_matchers_cfg) > 0:\n                cv2.rectangle(img_orig0, (roi_left0, roi_top0), (roi_right0, roi_bottom0), (0,0,255), 3)\n                cv2.rectangle(img_orig1, (roi_left1, roi_top1), (roi_right1, roi_bottom1), (0,0,255), 3)\n            return mkpts0, mkpts1, img_orig0, img_orig1\n        else:\n            return mkpts0, mkpts1\n\n\nclass SolutionHolder:\n    def __init__(self):\n        self.F_dict = dict()\n    \n    @staticmethod\n    def solve_keypoints(mkpts0, mkpts1, verbose=0, ret_inliers=False):\n        findmat_st = time.time()\n        if len(mkpts0) > 7:\n            #F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.25, 0.9999, 100000)\n            F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.2, 0.9999, 150000)  # EDITED, was 220000\n            inliers = inliers > 0\n            assert F.shape == (3, 3), 'Malformed F?'\n        else:\n            F = np.zeros((3, 3))\n        findmat_end = time.time()\n        if verbose >= 1:\n            print('  - Ransac time:', findmat_end - findmat_st, \"s\")\n        if ret_inliers:\n            return F, inliers\n        else:\n            return F\n\n    @staticmethod\n    def flatten_matrix(M, num_digits=8):\n        '''Convenience function to write CSV files.'''\n        return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n    def add_solution(self, sample_id, mkpts0, mkpts1, verbose=0):\n        self.F_dict[sample_id] = SolutionHolder.solve_keypoints(mkpts0, mkpts1, verbose)\n  \n    def dump(self, output_file):\n        with open(output_file, 'w') as f:\n            f.write('sample_id,fundamental_matrix\\n')\n            for sample_id, F in self.F_dict.items():\n                f.write(f'{sample_id},{SolutionHolder.flatten_matrix(F)}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T16:55:17.28315Z","iopub.execute_input":"2022-06-01T16:55:17.283704Z","iopub.status.idle":"2022-06-01T16:55:17.326548Z","shell.execute_reply.started":"2022-06-01T16:55:17.283668Z","shell.execute_reply":"2022-06-01T16:55:17.325754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialize Solution","metadata":{}},{"cell_type":"code","source":"# -------------------------------------------------\n#  Base Matchers\n# -------------------------------------------------\nsuperglue_matcher = SuperGlueMatcherV2(device=device)\nloftr_matcher = LoFTRMatcher(device=device)\n\nabs_config = {\n    'w': 640, 'h': 640, 'check_cycle_consistency': False,\n    'thresh': {'method': 'abs', 'th': 0.8, 'take': 120}\n} \ndkm_matcher = DKMMatcher(device=device, config=abs_config)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T16:55:43.920193Z","iopub.execute_input":"2022-06-01T16:55:43.920843Z","iopub.status.idle":"2022-06-01T16:55:45.483483Z","shell.execute_reply.started":"2022-06-01T16:55:43.920802Z","shell.execute_reply":"2022-06-01T16:55:45.482727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from functools import partial\n\n# -------------------------------------------------\n# DATALOADER\n#\n# the \"sizes\" should include keys as required by\n# uber_matcher below\n# -------------------------------------------------\ntest_src = '/kaggle/input/image-matching-challenge-2022/'\nimc_dataloader = get_imc_dataloader(\n    test_csv=f'{test_src}/test.csv',\n    img_dir=f'{test_src}/test_images',\n    sizes={\n        'original': {'type': 'orig'},\n        'ls840': {'type': 'longside', 'longside': 840},\n        'ls1200': {'type': 'longside', 'longside': 1200},\n        'ls1600': {'type': 'longside', 'longside': 1600},\n        # '800x600': {'type': 'targetwh', 'w': 800, 'h': 600},\n    })\n\n# -------------------------------------------------\n#  Uber Matcher\n# -------------------------------------------------\n\nuber_matcher = MultiStageMatcher(\n    stage1_matchers_cfg=[\n        {\n            'name': 'loftr',\n            'input_key': 'ls1200',\n            'fn': partial(loftr_matcher, conf_th=0.3, tta=['orig', ]),\n            'use_for_roi': True,\n            'use_for_final': True,\n        },\n        {\n            'name': 'superglue',\n            'input_key': 'ls1600',\n            'fn': partial(\n                superglue_matcher,\n                conf_th=0.4,\n                tta_groups=[\n                    ('orig', 'orig'),\n                    ('orig', 'rot_r10'),\n                    ('rot_r10', 'orig'),\n                    ('orig', 'rot_l10'),\n                    ('rot_l10', 'orig'),\n                    ('rot_r10', 'rot_l10'),\n                    ('flip_lr', 'flip_lr'),\n                ],\n                forward_type='cross'),\n            'use_for_roi': True,\n            'use_for_final': True,\n        },\n        # {\n        #     'name': 'dkm',\n        #     'fn': dkm_matcher,\n        #     'input_key': '800x600',\n        #     'tta': ['orig', 'flip_lr', 'rot_r10'],\n        #     'use_for_roi': False,\n        #     'use_for_final': True,\n        # },\n    ],\n    stage2_matchers_cfg=[\n        {\n            'name': 'loftr',\n            'fn': partial(loftr_matcher, conf_th=0.3),\n            'resize': {'type': 'scalelim', 'max_longside': 1200, 'scale': 2.0},\n            'tta': ['orig', ],\n        },\n        # {\n        #     'name': 'superglue',\n        #     'fn': partial(superglue_matcher, conf_th=0.4),\n        #     'resize': {'type': 'scalelim', 'max_longside': 1400, 'scale': 2.0},\n        #     'tta': ['orig', 'flip_lr', ]\n        # },\n        # {\n        #     'name': 'superglue',\n        #     'fn': superglue_matcher,\n        #     'resize': {'type': 'longside', 'longside': 1200},\n        #     'tta': ['orig', 'flip_lr', ]\n        # },\n        {\n            'name': 'dkm',\n            'fn': partial(dkm_matcher, tta=['orig', 'flip_lr', ]),\n            'resize': {'type': 'targetwh', 'w': 640, 'h': 640},\n        },\n    ],\n    crop_roi_min_side=200,\n    crop_roi_margin=20,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T17:00:19.22982Z","iopub.execute_input":"2022-06-01T17:00:19.230095Z","iopub.status.idle":"2022-06-01T17:00:19.244186Z","shell.execute_reply.started":"2022-06-01T17:00:19.230065Z","shell.execute_reply":"2022-06-01T17:00:19.243365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T17:00:20.113594Z","iopub.execute_input":"2022-06-01T17:00:20.114239Z","iopub.status.idle":"2022-06-01T17:00:20.119257Z","shell.execute_reply.started":"2022-06-01T17:00:20.114186Z","shell.execute_reply":"2022-06-01T17:00:20.118311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VISUALIZE = True\nimport time\n\n\n\nif VISUALIZE and len(imc_dataloader) == 3:\n    for i, row_dict in enumerate(imc_dataloader):\n        st = time.time()\n\n        mkpts0, mkpts1, img_np1, img_np2 = uber_matcher(row_dict, verbose=1, ret_ims=True)\n        F, inliers = SolutionHolder.solve_keypoints(mkpts0, mkpts1, verbose=1, ret_inliers=True)\n\n        gc_st = time.time()\n        gc.collect()\n        nd = time.time()    \n        if (i < 3):\n            print(\"  - gc:\", nd - gc_st, \"s\")\n            print(\"Running time: \", nd - st, \" s\")\n            print(\"Num inliers:\", sum(p[0] for p in inliers))\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n            KMF.draw_LAF_matches(\n                KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                             torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                             torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n                KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                             torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                             torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n                torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n                cv2.cvtColor(img_np1, cv2.COLOR_BGR2RGB),\n                cv2.cvtColor(img_np2, cv2.COLOR_BGR2RGB),\n                inliers,\n                draw_dict={'inlier_color': (0.2, 1, 0.2),\n                           'tentative_color': None, \n                           'feature_color': (0.2, 0.5, 1), 'vertical': False},\n            )\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T17:00:20.303174Z","iopub.execute_input":"2022-06-01T17:00:20.304465Z","iopub.status.idle":"2022-06-01T17:01:04.235787Z","shell.execute_reply.started":"2022-06-01T17:00:20.304417Z","shell.execute_reply":"2022-06-01T17:01:04.233388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nimport threading\n\n\ndef main_solution(output_file):\n\n    # Prepare variables\n    solution_holder = SolutionHolder()\n    mkpts0, mkpts1 = None, None\n    mat_calc_thread = None\n  \n    for i, row_dict in enumerate(imc_dataloader):\n        sample_st = time.time()\n\n        # Print stats only for the first 3 samples\n        verbose = 1 if i < 3 else 0\n    \n        # Parse row\n        sample_id = row_dict['sample_id']\n        cyc_st = time.time()\n    \n        # Delete previous sample's results\n        del mkpts0\n        del mkpts1\n  \n        # Calculate matching pairs\n        mkpts0, mkpts1 = uber_matcher(row_dict, verbose=1)\n    \n        # If the RANSAC thread is not finished (it should though), wait...\n        if mat_calc_thread is not None:\n            mat_calc_thread.join()\n    \n        # Execute a RANSAC thread\n        mat_calc_thread = threading.Thread(\n            target=solution_holder.add_solution,\n            args=(sample_id, mkpts0, mkpts1, verbose))\n        mat_calc_thread.start()\n    \n        # Collect garbage and print logs if required\n        cyc_end = time.time()\n        gc_st = time.time()\n        gc.collect()\n        gc_end = time.time()\n        if verbose > 0:\n            print(f'({i+1}/{len(imc_dataloader)})  Iter total: {gc_end - cyc_st:.06f}s  (runtime: {cyc_end - cyc_st:.06f}s, gc: {gc_end - gc_st:.06f}s)')\n        else:\n            print(f'({i+1}/{len(imc_dataloader)})  {time.time() - sample_st}s')\n  \n    # Finish and write solution\n    fin_st = time.time()\n    if mat_calc_thread is not None:\n        mat_calc_thread.join()\n    fin_end = time.time()\n  \n    if verbose > 0:\n        print('Final calc:', fin_end - fin_st, 's')\n    solution_holder.dump(output_file)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T17:01:04.237516Z","iopub.execute_input":"2022-06-01T17:01:04.238122Z","iopub.status.idle":"2022-06-01T17:01:04.252005Z","shell.execute_reply.started":"2022-06-01T17:01:04.238074Z","shell.execute_reply":"2022-06-01T17:01:04.251096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run pipeline on sample test data\nmain_solution(output_file='submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-01T17:01:04.253367Z","iopub.execute_input":"2022-06-01T17:01:04.253677Z","iopub.status.idle":"2022-06-01T17:01:14.859776Z","shell.execute_reply.started":"2022-06-01T17:01:04.253644Z","shell.execute_reply":"2022-06-01T17:01:14.858961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}