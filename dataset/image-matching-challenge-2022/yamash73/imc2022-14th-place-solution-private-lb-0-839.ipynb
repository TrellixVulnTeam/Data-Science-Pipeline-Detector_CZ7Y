{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is our final submission for Image Matching Challenge 2022\n\nWe referred following notebooks. Thank you for awesome works! \\\nLoFTR: https://www.kaggle.com/code/cbeaud/imc-2022-kornia-score-0-725 \\\nSuperGlue: https://www.kaggle.com/code/losveria/superglue-baseline \\\nQuadTreeAttention: https://www.kaggle.com/code/dschettler8845/quadtree-image-matching-challenge-2022 \\\nValidation: https://www.kaggle.com/code/namgalielei/loftr-validation-score","metadata":{}},{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport csv\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport gc\nfrom collections import namedtuple\nimport random\nimport time\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nimport torch\nfrom torchvision import io\nfrom torchvision import transforms as T\nfrom PIL import Image\n\nsys.path.append(\"../input/imc-utils\")\nfrom imc_metric import EvaluateSubmission, ReadCovisibilityData, FlattenMatrix\n\nimport warnings\nwarnings.simplefilter('ignore')\n\ndevice = torch.device('cuda')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-06T08:24:11.410192Z","iopub.execute_input":"2022-06-06T08:24:11.411052Z","iopub.status.idle":"2022-06-06T08:24:12.152707Z","shell.execute_reply.started":"2022-06-06T08:24:11.410953Z","shell.execute_reply":"2022-06-06T08:24:12.151914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration","metadata":{}},{"cell_type":"code","source":"class CFG:\n    mode = \"test\" #\"val\"\n    seed = 2022\n\n    # Image sizes for each model\n    longest_imgsize_mf = 840\n    longest_imgsize_loftr = 840\n    longest_imgsize_qta = 1024\n    longest_imgsize_sg = [2000, 1600, 1200]  # We used 2 or 3 image scales for superpoint+superglue\n    longest_imgsize_p2p = 2048\n    \n    # Number of pairs used for F matrix estimation\n    max_num_pairs_mf = 700\n    max_num_pairs_loftr = 500\n    max_num_pairs_qta = 700\n    max_num_pairs_sg = 250\n    max_num_pairs_p2p = 400\n    \n    # MAGSAC params\n    magsac_thresh = 0.2\n    magsac_conf = 0.99999\n    magsac_maxiter = 8000\n       \n    # In the final submission, we use MatchFormer, SuperGlue and QuadTreeAttention\n    use_loftr = False\n    use_matchformer = True\n    use_superglue = True\n    use_quadtreeattention = True\n    use_segmentation = False\n    use_patch2pix = False\n    \n    validate_scene_id = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n    validate_num_pairs = 10","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:24:12.154475Z","iopub.execute_input":"2022-06-06T08:24:12.154879Z","iopub.status.idle":"2022-06-06T08:24:12.162498Z","shell.execute_reply.started":"2022-06-06T08:24:12.154835Z","shell.execute_reply":"2022-06-06T08:24:12.161357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG.seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Patch2Pix","metadata":{}},{"cell_type":"code","source":"if CFG.use_patch2pix:\n    sys.path.append(\"../input/transforms3d/transforms3d-0.3.1/\")\n    sys.path.append(\"../input/immatch/image-matching-toolbox/\")\n    sys.path.append('../input/immatch/image-matching-toolbox/third_party/patch2pix/')\n    \n    from immatch.modules.patch2pix import Patch2Pix, Patch2PixRefined\n    \n    #method = \"patch2pix\"\n    method = \"patch2pix_superglue\"\n    \n    backbone_dict = {\n        'resnet34': '/kaggle/input/patch2pix-weights/resnet34-333f7ec4.pth'\n    }\n\n    if method == \"patch2pix\":\n        cfg = {\n            \"ckpt\": '../input/patch2pix-weights/patch2pix_pretrained.pth',\n            \"ksize\": 2,\n            \"imsize\": 2048,\n            \"match_threshold\": 0.15, #0.25\n            \"backbone_dict\": backbone_dict,\n        }\n        model_p2p = Patch2Pix(cfg)\n\n    elif method == \"patch2pix_superglue\":\n        cfg = {\n            \"ckpt\": \"../input/patch2pix-weights/patch2pix_pretrained.pth\",\n            \"imsize\": 2048,\n            \"match_threshold\": 0.1,\n            \"backbone_dict\": backbone_dict,\n            \"coarse\": {\n                \"name\": 'SuperGlue',\n                \"imsize\": 2048,              # 1024\n                \"weights\": 'outdoor',        # superglue \n                \"sinkhorn_iterations\": 20,   # superglue : 100 -> 20\n                \"match_threshold\": 0.2,      # superglue : 0.2\n                \"max_keypoints\": 2048,       # superpoint: 2048\n                \"nms_radius\": 3,             # superpoint: 4 -> 3\n                \"keypoint_threshold\": 0.005  # superpoint: 0.005\n            }\n        }\n        model_p2p = Patch2PixRefined(cfg)  \n    \n    # for test\n    if False:\n        im1 = \"../input/image-matching-challenge-2022/test_images/1cf87530/0143f47ee9e54243a1b8454f3e91621a.png\"\n        im2 = \"../input/image-matching-challenge-2022/test_images/1cf87530/a5a9975574c94ff9a285f58c39b53d2c.png\"\n        \n        matches, kpts1, kpts2, scores = model_p2p.match_pairs(im1, im2)   \n        drawMatches(cv2.imread(im1), matches[:, :2], cv2.imread(im2), matches[:, 2:], np.ones(len(matches)))\n        drawMatches(cv2.imread(im1), kpts1, cv2.imread(im2), kpts2, np.ones(len(kpts1)))\nelse:\n    model_p2p = None","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:24:12.164053Z","iopub.execute_input":"2022-06-06T08:24:12.164432Z","iopub.status.idle":"2022-06-06T08:24:12.176757Z","shell.execute_reply.started":"2022-06-06T08:24:12.164396Z","shell.execute_reply":"2022-06-06T08:24:12.176071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## kornia installation & LoFTR","metadata":{}},{"cell_type":"code","source":"!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n\nimport kornia\nfrom kornia_moons.feature import *\nimport kornia as K\nimport kornia.feature as KF\n\n# loftr\nif CFG.use_loftr:\n    matcher_loftr = KF.LoFTR(pretrained=None)\n    matcher_loftr.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\n    matcher_loftr = matcher_loftr.to(device).eval()\nelse:\n    matcher_loftr = None","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:24:12.179639Z","iopub.execute_input":"2022-06-06T08:24:12.1799Z","iopub.status.idle":"2022-06-06T08:25:12.214112Z","shell.execute_reply.started":"2022-06-06T08:24:12.179865Z","shell.execute_reply":"2022-06-06T08:25:12.213364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## MatchFormer","metadata":{}},{"cell_type":"code","source":"sys.path.append('../input/einops/einops-master')\n\nif CFG.use_matchformer:    \n    sys.path.append('../input/pytorchimagemodels/pytorch-image-models-master')\n    sys.path.append('../input/matchformer/MatchFormer-main')\n\n    from yacs.config import CfgNode as CN\n    from model.matchformer import Matchformer\n    from config import defaultmf\n\n    cfg = defaultmf.get_cfg_defaults()\n    cfg.MATCHFORMER.BACKBONE_TYPE = 'largela'\n    cfg.MATCHFORMER.SCENS = 'outdoor'\n    cfg.MATCHFORMER.RESOLUTION = (8,2)\n    cfg.MATCHFORMER.MATCH_COARSE.THR = 0.15 #0.2\n\n    def lower_config(yacs_cfg):\n        if not isinstance(yacs_cfg, CN):\n            return yacs_cfg\n        return {k.lower(): lower_config(v) for k, v in yacs_cfg.items()}\n\n    _cfg = lower_config(cfg)\n\n    matcher_mf = Matchformer(_cfg['matchformer'])\n\n    pretrained_ckpt = '../input/matchformer/outdoor-large-LA.ckpt'\n    matcher_mf.load_state_dict({k.replace('matcher.',''):v  for k,v in torch.load(pretrained_ckpt, map_location='cpu').items()})\n    matcher_mf = matcher_mf.to(device).eval()\nelse:\n    matcher_mf = None","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:25:12.215422Z","iopub.execute_input":"2022-06-06T08:25:12.215692Z","iopub.status.idle":"2022-06-06T08:25:24.858591Z","shell.execute_reply.started":"2022-06-06T08:25:12.215653Z","shell.execute_reply":"2022-06-06T08:25:24.857682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Super Glue","metadata":{}},{"cell_type":"code","source":"if CFG.use_superglue:\n    sys.path.append(\"../input/super-glue-pretrained-network\")\n    from models.matching import Matching\n    from models.utils import (compute_pose_error, compute_epipolar_error,\n                              estimate_pose, make_matching_plot,\n                              error_colormap, AverageTimer, pose_auc, read_image,\n                              rotate_intrinsics, rotate_pose_inplane,\n                              scale_intrinsics)\n\n    config = {\n        \"superpoint\": {\n            \"nms_radius\": 3, #4,\n            \"keypoint_threshold\": 0.005,\n            \"max_keypoints\": 2048,\n        },\n        \"superglue\": {\n            \"weights\": \"outdoor\",\n            \"sinkhorn_iterations\": 20,\n            \"match_threshold\": 0.2,\n        }\n    }\n    matcher_sg = Matching(config).eval().to(device)\nelse:\n    matcher_sg = None","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:25:24.86008Z","iopub.execute_input":"2022-06-06T08:25:24.860335Z","iopub.status.idle":"2022-06-06T08:25:25.996751Z","shell.execute_reply.started":"2022-06-06T08:25:24.860299Z","shell.execute_reply":"2022-06-06T08:25:25.99513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quad Tree Attention","metadata":{}},{"cell_type":"code","source":"if CFG.use_quadtreeattention:\n    !cp -r ../input/quadtreeattention/QuadTreeAttention-master/* .\n    %cd ./QuadTreeAttention\n    !pip install -e . --no-index --no-deps\n    %cd /kaggle/working\n    sys.path.insert(0, \"/kaggle/working/FeatureMatching\")\n    sys.path.insert(0, \"/kaggle/working/QuadTreeAttention\")\n    \n    from src.loftr import LoFTR\n    from src.config.default import get_cfg_defaults\n    from src.loftr.utils.cvpr_ds_config import default_cfg\n    from yacs.config import CfgNode as CN\n    from configs.loftr.outdoor.loftr_ds_quadtree import cfg\n    \n    cfg.LOFTR.MATCH_COARSE.THR = 0.15\n\n    def lower_config(yacs_cfg):\n        if not isinstance(yacs_cfg, CN):\n            return yacs_cfg\n        return {k.lower(): lower_config(v) for k, v in yacs_cfg.items()}\n\n    cfg = lower_config(cfg)[\"loftr\"]\n\n    torch.set_grad_enabled(False)\n\n    # Initialize LoFTR\n    matcher_qta = LoFTR(config=cfg)\n    matcher_qta.load_state_dict(torch.load(\"../input/quadtreeattention/outdoor.ckpt\")['state_dict'])\n    matcher_qta = matcher_qta.eval().to(device=device)\nelse:\n    matcher_qta = None","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:25:25.99799Z","iopub.execute_input":"2022-06-06T08:25:25.998453Z","iopub.status.idle":"2022-06-06T08:27:45.336873Z","shell.execute_reply.started":"2022-06-06T08:25:25.998405Z","shell.execute_reply":"2022-06-06T08:27:45.336046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Segmentation (not improving CV and LB score)","metadata":{}},{"cell_type":"code","source":"if CFG.use_segmentation:\n    sys.path.append('../input/semantic-segmentation/semantic-segmentation-main')\n    from semseg import show_models\n    from semseg.models import *\n    show_models()\n\n    segmodel = eval('SegFormer')(backbone='MiT-B1', num_classes=150)\n    segmodel.load_state_dict(torch.load('../input/semantic-segmentation/segformer.b1.ade.pth'))\n\n    #segmodel = eval('SegFormer')(backbone='MiT-B3', num_classes=150)\n    #segmodel.load_state_dict(torch.load('../input/semantic-segmentation/segformer.b3.ade.pth'))\n\n    segmodel = segmodel.to(device).eval()\nelse:\n    segmodel = None\n\n\ndef erode(img, target_label):\n    assert img.ndim == 2\n    img_bin = np.zeros(img.shape).astype(np.uint8)\n    img_bin[img == target_label] = 1\n    kernel = np.ones((5,5), np.uint8)\n    erosion = cv2.erode(img_bin, kernel, iterations=1)\n    img[(img_bin == 1) & (erosion != 1)] = 1\n    return img\n\n\ndef segmentation(img, segmodel, size=(512,512), device='cuda'):\n    image = cv2.resize(img, size)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = image.transpose((2,0,1))\n    image = torch.from_numpy(image)\n    image = image.float() / 255\n    image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image)\n    image = image.unsqueeze(0).to(device)\n\n    with torch.inference_mode():\n        seg_org = segmodel(image)\n\n    segmap = seg_org.softmax(1).argmax(1).to(int)\n    segmap = segmap.to('cpu').numpy().squeeze(0)    \n    segmap = cv2.resize(segmap, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n    \n    return segmap\n\n\ndef masking(img, segmodel, size=(512, 512), device='cuda'):\n    if segmodel is not None:\n        image = cv2.resize(img, size)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = image.transpose((2,0,1))\n        image = torch.from_numpy(image)\n        image = image.float() / 255\n        image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(image)\n        image = image.unsqueeze(0).to(device)\n        \n        with torch.inference_mode():\n            seg_org = segmodel(image)\n        \n        segmap = seg_org.softmax(1).argmax(1).to(int)\n        segmap = segmap.to('cpu').numpy().squeeze(0)\n        \n        remove_label = [2, 12, 20, 116, 127]\n        \n        for r in remove_label:\n            segmap = erode(segmap, r)\n        segmap = cv2.resize(segmap, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n        for r in remove_label:\n            img[segmap==r, :] = np.array([0,0,0])\n            \n    return img","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:27:45.338207Z","iopub.execute_input":"2022-06-06T08:27:45.338459Z","iopub.status.idle":"2022-06-06T08:27:45.356133Z","shell.execute_reply.started":"2022-06-06T08:27:45.338424Z","shell.execute_reply":"2022-06-06T08:27:45.355448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Keypoint Matching, F Matrix Estimation, Utilities","metadata":{}},{"cell_type":"code","source":"def FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\ndef get_F_matrix(mkpts0, mkpts1):\n    inliers = None\n    if len(mkpts0) > 8:\n        F, mask = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, CFG.magsac_thresh, CFG.magsac_conf, CFG.magsac_maxiter)\n        inliers = mask.flatten().astype(np.uint8)\n\n        assert F.shape == (3, 3), 'Malformed F?'\n    else:\n        F = np.zeros((3, 3))\n        inliers = np.zeros(mkpts0.shape[0]).astype(np.uint8)\n\n    return F, inliers\n\n\n%matplotlib inline\ndef drawMatches(img0, mkpts0, img1, mkpts1, inliers):\n    w0 = img0.shape[1]\n    w1 = img1.shape[1]\n    h0 = img0.shape[0]\n    h1 = img1.shape[0]\n    W = w0 + w1\n    H = max(h0, h1)\n    dst = np.zeros((H, W, 3)).astype(np.uint8)\n    dst[:h0, :w0, :] = img0\n    dst[:h1, w0:, :] = img1\n    \n    for idx, (p0, p1, inlier) in enumerate(zip(mkpts0, mkpts1, inliers)):\n        if inlier > 0:\n            cv2.line(dst, (int(p0[0]), int(p0[1])), (int(w0 + p1[0]), int(p1[1])), (0,255,0), 1)\n            \n    for idx, (p0, p1, inlier) in enumerate(zip(mkpts0, mkpts1, inliers)):\n        if inlier > 0:\n            color = (random.randint(50, 255), random.randint(50, 255), random.randint(50, 255))\n            cv2.circle(dst, (int(p0[0]), int(p0[1])), 5, color, 2)\n            cv2.circle(dst, (int(w0 + p1[0]), int(p1[1])), 5, color, 2)\n\n    dst = dst[:,:,::-1]\n\n    plt.figure(figsize=(15,10))\n    plt.imshow(dst)\n    plt.show()\n\n\ndef load_torch_image(fname, device, longest_imgsize, segmodel, padding=False):\n    img = cv2.imread(fname)\n    org_w = img.shape[1]\n    org_h = img.shape[0]\n    scale = longest_imgsize / max(img.shape[0], img.shape[1]) \n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    img = cv2.resize(img, (w, h))\n    if padding:\n        org_w = max(org_w, org_h)\n        org_h = max(org_w, org_h)\n        base = np.zeros((longest_imgsize, longest_imgsize, 3)).astype(np.uint8)\n        base[:img.shape[0], :img.shape[1], :] = img\n        img = base\n    if segmodel is not None:\n        img = masking(img, segmodel, size=(512,512), device=device)\n    img = K.image_to_tensor(img, False).float() / 255.\n    img = K.color.bgr_to_rgb(img)\n    return img.to(device), org_w, org_h\n\n\ndef match_mf(img_path0, img_path1, matcher, device, segmodel):\n    img0, org_w0, org_h0 = load_torch_image(img_path0, device=device, longest_imgsize=CFG.longest_imgsize_mf, segmodel=None, padding=True)\n    img1, org_w1, org_h1 = load_torch_image(img_path1, device=device, longest_imgsize=CFG.longest_imgsize_mf, segmodel=None, padding=True)\n\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0), \n                  \"image1\": K.color.rgb_to_grayscale(img1)}\n\n    with torch.inference_mode():\n        matcher(input_dict)\n    \n    conf = input_dict['mconf'].to('cpu').numpy()\n    mkpts0 = input_dict['mkpts0_f'].to('cpu').numpy()\n    mkpts1 = input_dict['mkpts1_f'].to('cpu').numpy()\n    \n    sorted_idx = np.argsort(-conf)\n    if len(conf) > CFG.max_num_pairs_mf:\n        mkpts0 = mkpts0[sorted_idx[:CFG.max_num_pairs_mf], :]\n        mkpts1 = mkpts1[sorted_idx[:CFG.max_num_pairs_mf], :]\n    \n    mkpts0[:,0] = mkpts0[:,0] * org_w0 / img0.shape[3]\n    mkpts0[:,1] = mkpts0[:,1] * org_h0 / img0.shape[2]\n    \n    mkpts1[:,0] = mkpts1[:,0] * org_w1 / img1.shape[3]\n    mkpts1[:,1] = mkpts1[:,1] * org_h1 / img1.shape[2]\n\n    return mkpts0, mkpts1\n\n\ndef match_loftr(img_path0, img_path1, matcher, device, segmodel):\n\n    img0, org_w0, org_h0 = load_torch_image(img_path0, device=device, longest_imgsize=CFG.longest_imgsize_loftr, segmodel=None, padding=True)\n    img1, org_w1, org_h1 = load_torch_image(img_path1, device=device, longest_imgsize=CFG.longest_imgsize_loftr, segmodel=None, padding=True)\n\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0), \n                  \"image1\": K.color.rgb_to_grayscale(img1)}\n   \n    with torch.inference_mode():\n        correspondences = matcher(input_dict)\n\n    conf = correspondences['confidence'].cpu().numpy()\n    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n    \n    sorted_idx = np.argsort(-conf)\n    if len(conf) > CFG.max_num_pairs_loftr:\n        mkpts0 = mkpts0[sorted_idx[:CFG.max_num_pairs_loftr], :]\n        mkpts1 = mkpts1[sorted_idx[:CFG.max_num_pairs_loftr], :]\n    \n    mkpts0[:,0] = mkpts0[:,0] * org_w0 / img0.shape[3]\n    mkpts0[:,1] = mkpts0[:,1] * org_h0 / img0.shape[2]\n    \n    mkpts1[:,0] = mkpts1[:,0] * org_w1 / img1.shape[3]\n    mkpts1[:,1] = mkpts1[:,1] * org_h1 / img1.shape[2]\n\n    return mkpts0, mkpts1\n\n\ndef match_qta(img_path0, img_path1, matcher, device, segmodel):\n\n    img0, org_w0, org_h0 = load_torch_image(img_path0, device=device, longest_imgsize=CFG.longest_imgsize_qta, segmodel=None, padding=True)\n    img1, org_w1, org_h1 = load_torch_image(img_path1, device=device, longest_imgsize=CFG.longest_imgsize_qta, segmodel=None, padding=True)\n\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0), \n                  \"image1\": K.color.rgb_to_grayscale(img1)}\n   \n    with torch.inference_mode():\n        matcher(input_dict)\n        \n    mkpts0 = input_dict['mkpts0_f'].cpu().numpy()\n    mkpts1 = input_dict['mkpts1_f'].cpu().numpy()\n    conf = input_dict['mconf'].cpu().numpy()\n\n    sorted_idx = np.argsort(-conf)\n    if len(conf) > CFG.max_num_pairs_loftr:\n        mkpts0 = mkpts0[sorted_idx[:CFG.max_num_pairs_qta], :]\n        mkpts1 = mkpts1[sorted_idx[:CFG.max_num_pairs_qta], :]\n    \n    mkpts0[:,0] = mkpts0[:,0] * org_w0 / img0.shape[3]\n    mkpts0[:,1] = mkpts0[:,1] * org_h0 / img0.shape[2]\n    \n    mkpts1[:,0] = mkpts1[:,0] * org_w1 / img1.shape[3]\n    mkpts1[:,1] = mkpts1[:,1] * org_h1 / img1.shape[2]\n\n    return mkpts0, mkpts1\n\n\ndef match_sg(img_path0, img_path1, matcher, device, segmodel):\n   \n    inp_1, org_w0, org_h0 = load_torch_image(img_path0, device=device, longest_imgsize=CFG.longest_imgsize_sg[0], segmodel=segmodel, padding=False)\n    inp_2, org_w1, org_h1 = load_torch_image(img_path1, device=device, longest_imgsize=CFG.longest_imgsize_sg[0], segmodel=segmodel, padding=False)\n\n    with torch.inference_mode():\n        pred = matcher_sg({\"image0\": K.color.rgb_to_grayscale(inp_1), \n                           \"image1\": K.color.rgb_to_grayscale(inp_2)})\n    pred = {k: v[0].detach().cpu().numpy() for k, v in pred.items()}\n    kpts1, kpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n    matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n\n    valid = matches > -1\n    mkpts0 = kpts1[valid]\n    mkpts1 = kpts2[matches[valid]]\n    mconf = conf[valid]\n    \n    sorted_idx = np.argsort(-mconf)\n    if len(mconf) > CFG.max_num_pairs_sg:\n        mkpts0 = mkpts0[sorted_idx[:CFG.max_num_pairs_sg], :]\n        mkpts1 = mkpts1[sorted_idx[:CFG.max_num_pairs_sg], :]\n       \n    mkpts0[:,0] = mkpts0[:,0] * org_w0 / inp_1.shape[3]\n    mkpts0[:,1] = mkpts0[:,1] * org_h0 / inp_1.shape[2]\n    \n    mkpts1[:,0] = mkpts1[:,0] * org_w1 / inp_2.shape[3]\n    mkpts1[:,1] = mkpts1[:,1] * org_h1 / inp_2.shape[2]\n\n    for lsize in CFG.longest_imgsize_sg[1:]:\n        inp_1, org_w0, org_h0 = load_torch_image(img_path0, device=device, longest_imgsize=lsize, segmodel=segmodel, padding=False)\n        inp_2, org_w1, org_h1 = load_torch_image(img_path1, device=device, longest_imgsize=lsize, segmodel=segmodel, padding=False)\n\n        with torch.inference_mode():\n            pred = matcher_sg({\"image0\": K.color.rgb_to_grayscale(inp_1), \n                               \"image1\": K.color.rgb_to_grayscale(inp_2)})\n        pred = {k: v[0].detach().cpu().numpy() for k, v in pred.items()}\n        kpts1, kpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n        matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n\n        valid = matches > -1\n        mkpts0_small = kpts1[valid]\n        mkpts1_small = kpts2[matches[valid]]\n        mconf_small = conf[valid]\n        \n        sorted_idx = np.argsort(-mconf_small)\n        if len(mconf_small) > CFG.max_num_pairs_sg:\n            mkpts0_small = mkpts0_small[sorted_idx[:CFG.max_num_pairs_sg], :]\n            mkpts1_small = mkpts1_small[sorted_idx[:CFG.max_num_pairs_sg], :]\n        \n        mconf = np.concatenate([mconf, mconf_small])\n\n        mkpts0_small[:,0] = mkpts0_small[:,0] * org_w0 / inp_1.shape[3]\n        mkpts0_small[:,1] = mkpts0_small[:,1] * org_h0 / inp_1.shape[2]\n\n        mkpts1_small[:,0] = mkpts1_small[:,0] * org_w1 / inp_2.shape[3]\n        mkpts1_small[:,1] = mkpts1_small[:,1] * org_h1 / inp_2.shape[2]\n\n        mkpts0 = np.vstack([mkpts0, mkpts0_small])\n        mkpts1 = np.vstack([mkpts1, mkpts1_small])\n    \n    return mkpts0, mkpts1\n\n\ndef match_p2p(img1_path, img2_path, model_p2p, segmodel):\n    with torch.inference_mode():\n        matches, kpts1, kpts2, scores = model_p2p.match_pairs(img1_path, img2_path)\n    mkpts0 = matches[:, :2]\n    mkpts1 = matches[:, 2:]\n    \n    sorted_idx = np.argsort(-scores)\n    if len(matches) > CFG.max_num_pairs_p2p:\n        mkpts0 = mkpts0[sorted_idx[:CFG.max_num_pairs_p2p], :]\n        mkpts1 = mkpts1[sorted_idx[:CFG.max_num_pairs_p2p], :]\n        \n    return mkpts0, mkpts1","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:27:45.357407Z","iopub.execute_input":"2022-06-06T08:27:45.358223Z","iopub.status.idle":"2022-06-06T08:27:45.45242Z","shell.execute_reply.started":"2022-06-06T08:27:45.358183Z","shell.execute_reply":"2022-06-06T08:27:45.451724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def inference_Fstr(img1_path, img2_path, matcher_mf, matcher_loftr, matcher_sg, model_p2p, matcher_qta, segmodel, device):\n    if matcher_mf is not None:\n        mkpts0_1, mkpts1_1 = match_mf(img1_path, img2_path, matcher_mf, device=device, segmodel=segmodel) \n    else:\n        mkpts0_1 = np.zeros((0,2))\n        mkpts1_1 = np.zeros((0,2))\n        \n    if matcher_loftr is not None:\n        mkpts0_2, mkpts1_2 = match_loftr(img1_path, img2_path, matcher_loftr, device=device, segmodel=segmodel)\n    else:\n        mkpts0_2 = np.zeros((0,2))\n        mkpts1_2 = np.zeros((0,2))\n        \n    if matcher_sg is not None:\n        mkpts0_3, mkpts1_3 = match_sg(img1_path, img2_path, matcher_sg, device=device, segmodel=segmodel)\n    else:\n        mkpts0_3 = np.zeros((0,2))\n        mkpts1_3 = np.zeros((0,2))\n        \n    if model_p2p is not None:\n        mkpts0_4, mkpts1_4 = match_p2p(img1_path, img2_path, model_p2p, segmodel=segmodel)\n    else:\n        mkpts0_4 = np.zeros((0,2))\n        mkpts1_4 = np.zeros((0,2))\n        \n    if matcher_qta is not None:\n        mkpts0_5, mkpts1_5 = match_qta(img1_path, img2_path, matcher_qta, device=device, segmodel=segmodel)\n    else:\n        mkpts0_5 = np.zeros((0,2))\n        mkpts1_5 = np.zeros((0,2))        \n    \n    mkpts0 = np.vstack([mkpts0_1, mkpts0_2, mkpts0_3, mkpts0_4, mkpts0_5])\n    mkpts1 = np.vstack([mkpts1_1, mkpts1_2, mkpts1_3, mkpts1_4, mkpts1_5])\n\n    F, inliers = get_F_matrix(mkpts0, mkpts1)\n    \n    F_str = FlattenMatrix(F)\n    return F_str, mkpts0, mkpts1, inliers\n\n\ndef inference(matcher_mf, matcher_loftr, matcher_sg, model_p2p, matcher_qta, mode, device, segmodel):\n    sub = pd.DataFrame(columns = ['sample_id', 'fundamental_matrix'])\n    \n    if CFG.mode == \"test\":\n        src = '../input/image-matching-challenge-2022/'\n        test_samples = []\n        with open(f'{src}/test.csv') as f:\n            reader = csv.reader(f, delimiter=',')\n            for i, row in enumerate(reader):\n                if i == 0:\n                    continue\n                row[2] = f'{src}/test_images/{row[1]}/{row[2]}.png'\n                row[3] = f'{src}/test_images/{row[1]}/{row[3]}.png'\n                test_samples += [row]\n                \n        pairs = test_samples\n\n    elif CFG.mode == \"val\":\n        src = '../input/image-matching-challenge-2022/train'\n        scaling_dict = {}\n        with open(f'{src}/scaling_factors.csv') as f:\n            reader = csv.reader(f, delimiter=',')\n            for i, row in enumerate(reader):\n                if i == 0:\n                    continue\n                scaling_dict[row[0]] = float(row[1])\n                \n        val_samples = []\n        for scene_id, scene in enumerate(scaling_dict.keys()):\n            if len(CFG.validate_scene_id) > 0 and scene_id not in CFG.validate_scene_id:\n                continue\n            covisibility_dict, F_gt_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')\n            pairs = list([key for key, covis in covisibility_dict.items() if covis >= 0.1])\n            random.shuffle(pairs)\n            n = len(pairs)\n            pairs = pairs[:CFG.validate_num_pairs]\n            print(f'Loading covisibility data for \"{scene}\"... kept {len(pairs)} out of {n} covisible pairs')\n\n            for pair in pairs:\n                image_1_id, image_2_id = pair.split('-')\n                image_1 = f'{src}/{scene}/images/{image_1_id}.jpg'\n                image_2 = f'{src}/{scene}/images/{image_2_id}.jpg'\n                val_samples.append([f'phototourism;{scene};{pair}', 0, image_1, image_2])\n                \n        pairs = val_samples\n\n    \n    for i, row in enumerate(tqdm(pairs)):\n        sample_id, batch_id, image_1, image_2 = row\n        F_str, mkpts0, mkpts1, inliers = inference_Fstr(\n            image_1, image_2, matcher_mf, matcher_loftr, matcher_sg, model_p2p, matcher_qta, segmodel, device)\n        sub = sub.append({'sample_id': f'{sample_id}', 'fundamental_matrix': f'{F_str}'}, ignore_index=True) \n        \n        # visualize\n        if i < 3:\n            drawMatches(cv2.imread(image_1), mkpts0, cv2.imread(image_2), mkpts1, inliers)\n    \n    return sub","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:27:45.455124Z","iopub.execute_input":"2022-06-06T08:27:45.455591Z","iopub.status.idle":"2022-06-06T08:27:45.478046Z","shell.execute_reply.started":"2022-06-06T08:27:45.455534Z","shell.execute_reply":"2022-06-06T08:27:45.477278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = inference(matcher_mf, matcher_loftr, matcher_sg, model_p2p, matcher_qta, CFG.mode, device, segmodel)\nsub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:27:45.479074Z","iopub.execute_input":"2022-06-06T08:27:45.479367Z","iopub.status.idle":"2022-06-06T08:28:00.946168Z","shell.execute_reply.started":"2022-06-06T08:27:45.479308Z","shell.execute_reply":"2022-06-06T08:28:00.945376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking Vaidation Score","metadata":{}},{"cell_type":"code","source":"if CFG.mode == 'val':\n    src = '../input/image-matching-challenge-2022/train'\n    scaling_dict = {}\n    with open(f'{src}/scaling_factors.csv') as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            if i == 0:\n                continue\n            scaling_dict[row[0]] = float(row[1])\n    \n    thresholds_q = np.linspace(1, 10, 10)\n    thresholds_t = np.geomspace(0.2, 5, 10)\n\n    print('--- Evaluate prediction ---')\n    maa, maa_per_scene, errors_dict_q, errors_dict_t = EvaluateSubmission('submission.csv', scaling_dict, thresholds_q, thresholds_t, src=src)\n    for scene, cur_maa in maa_per_scene.items():\n        print(f'Scene \"{scene}\" ({len(errors_dict_q[scene])} pairs), mAA={cur_maa:.05f}')\n    print()\n    print(f'Full dataset: mAA={maa:.05f}')\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T08:28:00.947592Z","iopub.execute_input":"2022-06-06T08:28:00.947965Z","iopub.status.idle":"2022-06-06T08:28:00.957253Z","shell.execute_reply.started":"2022-06-06T08:28:00.947921Z","shell.execute_reply":"2022-06-06T08:28:00.956491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}