{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\ndry_run = False\n!pip install ../input/kornialoftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install ../input/kornialoftr/kornia_moons-0.1.9-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-03T00:24:51.741284Z","iopub.execute_input":"2022-06-03T00:24:51.741717Z","iopub.status.idle":"2022-06-03T00:25:52.39184Z","shell.execute_reply.started":"2022-06-03T00:24:51.741605Z","shell.execute_reply":"2022-06-03T00:25:52.390706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nimport os\nimport csv\nimport random\nfrom glob import glob\nfrom tqdm import tqdm\nfrom collections import namedtuple\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport torch\nimport torchvision.transforms as transforms\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport kornia\nfrom kornia_moons.feature import *\nimport kornia as K\nimport kornia.feature as KF\nimport gc\nimport pydegensac\n\nimport sys\nimport time\n\nsys.path.append(\"../input/\")\nsys.path.append(\"../input/super-glue-pretrained-network\")\n\nfrom models.matching import Matching as Matching_SuperGlue\nfrom models.utils import (compute_pose_error, compute_epipolar_error,\n                          estimate_pose, make_matching_plot,\n                          error_colormap, AverageTimer, pose_auc, read_image,\n                          rotate_intrinsics, rotate_pose_inplane,\n                          scale_intrinsics)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-03T00:26:24.677227Z","iopub.execute_input":"2022-06-03T00:26:24.677982Z","iopub.status.idle":"2022-06-03T00:26:24.691025Z","shell.execute_reply.started":"2022-06-03T00:26:24.67793Z","shell.execute_reply":"2022-06-03T00:26:24.690304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check which GPUs I am assigned to\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n  print('and then re-execute this cell.')\nelse:\n  print(gpu_info)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:25:55.843649Z","iopub.execute_input":"2022-06-03T00:25:55.844192Z","iopub.status.idle":"2022-06-03T00:25:55.916806Z","shell.execute_reply.started":"2022-06-03T00:25:55.844152Z","shell.execute_reply":"2022-06-03T00:25:55.915594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## General Helper Functions","metadata":{}},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\ndef load_torch_image(device, fname=None, local_image=None, size=840.0):\n    # If the image is already in memory\n    if local_image is None:\n        img = cv2.imread(fname)\n    else:\n        img = np.copy(local_image)\n        \n    if size == -1:\n        scale = 1\n    else:\n        scale = float(size) / float(max(img.shape[0], img.shape[1]))\n    \n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    img = cv2.resize(img, (w, h))\n    img = K.image_to_tensor(img, False).float() /255.0\n    img = K.color.bgr_to_rgb(img)\n    \n    # the scale value here is the new_size / old_size, different from the original SuperGlue \n    return img.to(device), scale\n\ntest_samples_df = pd.DataFrame(test_samples, columns=[\"sample_id\", \"batch_id\", \"image_0_id\", \"image_1_id\"])\ntest_samples_df","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:25:55.919795Z","iopub.execute_input":"2022-06-03T00:25:55.92011Z","iopub.status.idle":"2022-06-03T00:25:55.955704Z","shell.execute_reply.started":"2022-06-03T00:25:55.920054Z","shell.execute_reply":"2022-06-03T00:25:55.954938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load SuperGlue","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nresize = [-1, ] # resize = [-1, ] means no resize\n# resize = 840\nresize_float = True\n\nconfig = {\n    \"superpoint\": {\n        # \"nms_radius\": 4,\n        \"nms_radius\": 4,\n        \"keypoint_threshold\": 0.005,\n        \"max_keypoints\": 2048\n    },\n    \"superglue\": {\n        \"weights\": \"outdoor\",\n        \"sinkhorn_iterations\": 150,\n        \"match_threshold\": 0.2,\n    }\n}\nmatcher_SG = Matching_SuperGlue(config).eval().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:25:55.95705Z","iopub.execute_input":"2022-06-03T00:25:55.957322Z","iopub.status.idle":"2022-06-03T00:25:59.859762Z","shell.execute_reply.started":"2022-06-03T00:25:55.95729Z","shell.execute_reply":"2022-06-03T00:25:59.859047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load LoFTR","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmatcher_LoFTR = KF.LoFTR(pretrained=None)\nmatcher_LoFTR.load_state_dict(torch.load(\"../input/kornialoftr/loftr_outdoor.ckpt\")['state_dict'])\n# matcher.load_state_dict(torch.load(\"../input/kornialoftr/epoch12-auc50.931-auc100.966-auc200.983.ckpt\")['state_dict'])\n# matcher.load_state_dict(torch.load(\"../input/kornialoftr/epoch3-auc100.960.ckpt\")['state_dict'])\nmatcher_LoFTR = matcher_LoFTR.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:25:59.860943Z","iopub.execute_input":"2022-06-03T00:25:59.861286Z","iopub.status.idle":"2022-06-03T00:26:00.888032Z","shell.execute_reply.started":"2022-06-03T00:25:59.861249Z","shell.execute_reply":"2022-06-03T00:26:00.887277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions to extract the keypoints and matches from different models","metadata":{}},{"cell_type":"code","source":"def get_keypoints_with_conf_LoFTR(image_1, image_2, matcher, img_resize=840.0, conf_th=[0.75, 0.5, 0.25, 0], num_keypoints=1000, take_all=False):\n                          \n    # the scale value here is the new_size / old_size, different from SuperGlue, to make it the same, take the inverse\n    image_1_tensor, scale_1 = load_torch_image(device, fname=None, local_image=image_1, size=img_resize)\n    image_2_tensor, scale_2 = load_torch_image(device, fname=None, local_image=image_2, size=img_resize)\n    scale_1 = float(1.0 / float(scale_1))\n    scale_2 = float(1.0 / float(scale_2))\n    \n    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1_tensor), \n                  \"image1\": K.color.rgb_to_grayscale(image_2_tensor)}\n    \n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n        \n    mkpts0_LoFTR = correspondences['keypoints0'].cpu().numpy()\n    mkpts1_LoFTR = correspondences['keypoints1'].cpu().numpy()\n    conf = correspondences['confidence'].cpu().numpy()\n    # print(\"initial number of LoFTR points: \" + str(len(mkpts0_LoFTR)))\n    \n    # This is usually for SE2-LoFTR\n    if take_all == True:\n        return mkpts0_LoFTR, mkpts1_LoFTR, np.mean(conf), 0, scale_1, scale_2\n    \n    mkpts0_LoFTR_conf_0 = mkpts0_LoFTR[conf > conf_th[0]]\n    mkpts1_LoFTR_conf_0 = mkpts1_LoFTR[conf > conf_th[0]]\n    mkpts0_LoFTR_conf_1 = mkpts0_LoFTR[conf > conf_th[1]]\n    mkpts1_LoFTR_conf_1= mkpts1_LoFTR[conf > conf_th[1]]\n    mkpts0_LoFTR_conf_2 = mkpts0_LoFTR[conf > conf_th[2]]\n    mkpts1_LoFTR_conf_2 = mkpts1_LoFTR[conf > conf_th[2]]\n    mkpts0_LoFTR_all = mkpts0_LoFTR[conf >= conf_th[3]]\n    mkpts1_LoFTR_all = mkpts1_LoFTR[conf >= conf_th[3]]\n    \n    # Use a progressive method to select the confidence threshold\n    #  If there are too many keypoints, take high confidence, otherwise take low\n    num_bin_1 = len(mkpts0_LoFTR_conf_0)\n    num_bin_2 = len(mkpts0_LoFTR_conf_1) - num_bin_1\n    num_bin_3 = len(mkpts0_LoFTR_conf_2) - num_bin_2 - num_bin_1\n    num_bin_4 = len(mkpts0_LoFTR_all) - num_bin_3 - num_bin_2 - num_bin_1\n    \n    largest_bin_index = np.argmax(np.array([num_bin_1, num_bin_2, num_bin_3, num_bin_4]))\n    conf_th_final = conf_th[largest_bin_index]\n    \n    mkpts0_LoFTR_final = mkpts0_LoFTR[conf > conf_th_final]\n    mkpts1_LoFTR_final = mkpts1_LoFTR[conf > conf_th_final]\n    conf_mean = np.mean(conf[conf > conf_th_final])\n    \n    if len(mkpts0_LoFTR_final) <= 7:\n        mkpts0_LoFTR_final = mkpts0_LoFTR_all\n        mkpts1_LoFTR_final = mkpts1_LoFTR_all\n        conf_mean = np.mean(conf)\n    \n    # If we don't use SE2-LoFTR, limit the number of matched keypoints\n    # Since experiments show that sometimes LoFTR can create an excessive amount of matching points\n    if len(mkpts0_LoFTR_final) > num_keypoints:\n        conf_final = conf[conf > conf_th_final]\n        conf_argsorted = np.argsort(conf_final)\n        selected_indices = conf_argsorted[-num_keypoints:]\n        mkpts0_LoFTR_final = mkpts0_LoFTR_final[selected_indices]\n        mkpts1_LoFTR_final = mkpts1_LoFTR_final[selected_indices]\n        \n    # print(\"final number of LoFTR points: \" + str(len(mkpts1_LoFTR_final)))\n    return mkpts0_LoFTR_final, mkpts1_LoFTR_final, conf_mean, conf_th_final, scale_1, scale_2\n\n\ndef get_keypoints_with_conf_SG(image_fpath_0, image_fpath_1, matcher, resize, resize_float, conf_th=[0.75, 0.5, 0.25, 0], take_all=False):\n    \n    # scale = original_size / new_size, different from the original SuperGlue. \n    image_0, inp_0, scales_0 = read_image(image_fpath_0, device, resize, 0, resize_float)\n    image_1, inp_1, scales_1 = read_image(image_fpath_1, device, resize, 0, resize_float)\n\n    input_dict = {\"image0\": inp_0, \"image1\": inp_1}\n\n    with torch.no_grad():\n        pred_SG = matcher(input_dict)\n        \n    pred_SG = {k: v[0].detach().cpu().numpy() for k, v in pred_SG.items()}\n    kpts0_SG, kpts1_SG = pred_SG[\"keypoints0\"], pred_SG[\"keypoints1\"]\n    # matches mask are different \"matches0\" and \"matches1\" since the number of keypoints are different\n    # but the valid keypoints after applying the mask will be the same (actually still different, probably a bug)\n    matches_mask_0_SG, conf_0 = pred_SG[\"matches0\"], pred_SG[\"matching_scores0\"]\n    \n    valid_0 = matches_mask_0_SG > -1\n    mkpts0_SG = kpts0_SG[valid_0]\n    mkpts1_SG = kpts1_SG[matches_mask_0_SG[valid_0]]\n    conf_0 = conf_0[valid_0]\n    conf = conf_0\n    \n    # print(\"initial number of SG points: \" + str(len(mkpts0_SG)))\n    # This is usually for SE2-LoFTR\n    if take_all == True:\n        return mkpts0_SG, mkpts1_SG, np.mean(conf), 0, scale_1, scale_2\n    \n    mkpts0_SG_conf_0 = mkpts0_SG[conf > conf_th[0]]\n    mkpts1_SG_conf_0 = mkpts1_SG[conf > conf_th[0]]\n    mkpts0_SG_conf_1 = mkpts0_SG[conf > conf_th[1]]\n    mkpts1_SG_conf_1= mkpts1_SG[conf > conf_th[1]]\n    mkpts0_SG_conf_2 = mkpts0_SG[conf > conf_th[2]]\n    mkpts1_SG_conf_2 = mkpts1_SG[conf > conf_th[2]]\n    mkpts0_SG_all = mkpts0_SG[conf >= conf_th[3]]\n    mkpts1_SG_all = mkpts1_SG[conf >= conf_th[3]]\n    \n    # Use a progressive method to select the confidence threshold\n    #  If there are too many keypoints, take high confidence, otherwise take low\n    num_bin_1 = len(mkpts0_SG_conf_0)\n    num_bin_2 = len(mkpts0_SG_conf_1) - num_bin_1\n    num_bin_3 = len(mkpts0_SG_conf_2) - num_bin_2 - num_bin_1\n    num_bin_4 = len(mkpts0_SG_all) - num_bin_3 - num_bin_2 - num_bin_1\n    \n    largest_bin_index = np.argmax(np.array([num_bin_1, num_bin_2, num_bin_3, num_bin_4]))\n    conf_th_final = conf_th[largest_bin_index]\n    \n    mkpts0_SG_final = mkpts0_SG[conf > conf_th_final]\n    mkpts1_SG_final = mkpts1_SG[conf > conf_th_final]\n    conf_mean = np.mean(conf[conf > conf_th_final])\n    \n    if len(mkpts0_SG_final) <= 7:\n        mkpts0_SG_final = mkpts0_SG_all\n        mkpts1_SG_final = mkpts1_SG_all\n        conf_mean = np.mean(conf)\n\n    # print(\"final number of SG points: \" + str(len(mkpts0_SG_final)))\n    return mkpts0_SG_final, mkpts1_SG_final, conf_mean, conf_th_final, scales_0, scales_1","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:28:50.520994Z","iopub.execute_input":"2022-06-03T00:28:50.521414Z","iopub.status.idle":"2022-06-03T00:28:50.547881Z","shell.execute_reply.started":"2022-06-03T00:28:50.521379Z","shell.execute_reply":"2022-06-03T00:28:50.547088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the dictionary for different scaling factors for different scenes","metadata":{}},{"cell_type":"code","source":"## Create a dictionary for scaling factors and pair's calibration data of different scenes\nsrc = '../input/image-matching-challenge-2022/train'\n\ndef create_scaling_dict(src):\n    scaling_dict = {}\n    with open(f'{src}/scaling_factors.csv') as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n            scaling_dict[row[0]] = float(row[1])\n\n    print(f'Scaling factors: {scaling_dict}')\n    return scaling_dict\n\ndef LoadCalibration(filename):\n    '''Load calibration data (ground truth) from the csv file.'''\n    \n    calib_dict = {}\n    with open(filename, 'r') as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n\n            camera_id = row[0]\n            K = np.array([float(v) for v in row[1].split(' ')]).reshape([3, 3])\n            R = np.array([float(v) for v in row[2].split(' ')]).reshape([3, 3])\n            T = np.array([float(v) for v in row[3].split(' ')])\n            calib_dict[camera_id] = Gt(K=K, R=R, T=T)\n    \n    return calib_dict\n\nscaling_dict = create_scaling_dict(src)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:26:12.70847Z","iopub.execute_input":"2022-06-03T00:26:12.708903Z","iopub.status.idle":"2022-06-03T00:26:12.714886Z","shell.execute_reply.started":"2022-06-03T00:26:12.708859Z","shell.execute_reply":"2022-06-03T00:26:12.714214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper functions to build the evaluation pipeline","metadata":{}},{"cell_type":"code","source":"# Some useful functions and definitions. You can skip this for now.\n\n# A named tuple containing the intrinsics (calibration matrix K) and extrinsics (rotation matrix R, translation vector T) for a given camera.\nGt = namedtuple('Gt', ['K', 'R', 'T'])\n\n# A small epsilon.\neps = 1e-15\n\ndef ReadCovisibilityData(filename):\n    covisibility_dict = {}\n    with open(filename) as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n            covisibility_dict[row[0]] = float(row[1])\n\n    return covisibility_dict\n\n\n# Project the keypoints' coordinates to the image plane \n# (i.e., to the normalized coordinate system as shown on a digital display)\ndef NormalizeKeypoints(keypoints, K):\n    C_x = K[0, 2]\n    C_y = K[1, 2]\n    f_x = K[0, 0]\n    f_y = K[1, 1]\n    keypoints = (keypoints - np.array([[C_x, C_y]])) / np.array([[f_x, f_y]])\n    return keypoints\n\n\ndef ComputeEssentialMatrix(F, K1, K2, kp1, kp2):\n    '''Compute the Essential matrix from the Fundamental matrix, given the calibration matrices. \n       Note that we ask participants to estimate F, i.e., without relying on known intrinsics.'''\n    \n    # Warning! Old versions of OpenCV's RANSAC could return multiple F matrices, encoded as a single matrix size 6x3 or 9x3, rather than 3x3.\n    # We do not account for this here, as the modern RANSACs do not do this:\n    # https://opencv.org/evaluating-opencvs-new-ransacs\n    assert F.shape[0] == 3, 'Malformed F?'\n\n    # Use OpenCV's recoverPose to solve the cheirality check:\n    # https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#gadb7d2dfcc184c1d2f496d8639f4371c0\n    E = np.matmul(np.matmul(K2.T, F), K1).astype(np.float64)\n    \n    kp1n = NormalizeKeypoints(kp1, K1)\n    kp2n = NormalizeKeypoints(kp2, K2)\n    num_inliers, R, T, mask = cv2.recoverPose(E, kp1n, kp2n)\n\n    return E, R, T\n\n\ndef ArrayFromCvKps(kps):\n    '''Convenience function to convert OpenCV keypoints into a simple numpy array.'''\n    return np.array([kp.pt for kp in kps])\n\n\ndef QuaternionFromMatrix(matrix):\n    '''Transform a rotation matrix into a quaternion.'''\n\n    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n    m00 = M[0, 0]\n    m01 = M[0, 1]\n    m02 = M[0, 2]\n    m10 = M[1, 0]\n    m11 = M[1, 1]\n    m12 = M[1, 2]\n    m20 = M[2, 0]\n    m21 = M[2, 1]\n    m22 = M[2, 2]\n\n    K = np.array([[m00 - m11 - m22, 0.0, 0.0, 0.0],\n              [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n              [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n              [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]])\n    K /= 3.0\n\n    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n    w, V = np.linalg.eigh(K)\n    q = V[[3, 0, 1, 2], np.argmax(w)]\n\n    if q[0] < 0:\n        np.negative(q, q)\n\n    return q\n\ndef DrawMatches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n    '''Draw keypoints and matches.'''\n    \n    composite, v_offset, h_offset = BuildCompositeImage(im1, im2, axis, margin, background)\n\n    # Draw all keypoints.\n    for coord_a, coord_b in zip(kp1, kp2):\n        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n    \n    # Draw matches, and highlight keypoints used in matches.\n    for idx_a, idx_b in matches:\n        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.line(composite,\n                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n                                   int(kp1[idx_a][1] + v_offset[0])]),\n                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n    return composite\n\n    \ndef ComputeErrorForOneExample(q_gt, T_gt, q, T, scale):\n    '''Compute the error metric for a single example.\n    The function returns two errors, over rotation and translation. \n    These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n    \n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t\n\n\ndef ComputeMaa(err_q, err_t, thresholds_q, thresholds_t):\n    '''Compute the mean Average Accuracy at different tresholds, for one scene.'''\n    \n    assert len(err_q) == len(err_t)\n    \n    acc, acc_q, acc_t = [], [], []\n    for th_q, th_t in zip(thresholds_q, thresholds_t):\n        acc += [(np.bitwise_and(np.array(err_q) < th_q, np.array(err_t) < th_t)).sum() / len(err_q)]\n        acc_q += [(np.array(err_q) < th_q).sum() / len(err_q)]\n        acc_t += [(np.array(err_t) < th_t).sum() / len(err_t)]\n\n    return np.mean(acc), np.array(acc), np.array(acc_q), np.array(acc_t)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:26:12.715999Z","iopub.execute_input":"2022-06-03T00:26:12.71625Z","iopub.status.idle":"2022-06-03T00:26:12.729327Z","shell.execute_reply.started":"2022-06-03T00:26:12.716214Z","shell.execute_reply":"2022-06-03T00:26:12.728549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Iterate over all scenes ","metadata":{}},{"cell_type":"code","source":"# Compute the metric for each scene, and then average it over all scenes.\n# Cap the number of image pairs for each scene to 50, and show one qualitative example per scene.\nshow_images = True\nnum_show_images = 10\nmax_pairs_per_scene = 10\nverbose = True\nsrc = '../input/image-matching-challenge-2022/train'\n\n# Use two different sets of thresholds over rotation and translation. Do not change this \n# -- these are the values used by the scoring back-end.\nthresholds_q = np.linspace(1, 10, 10)   # threshold for rotation values\nthresholds_t = np.geomspace(0.2, 5, 10) # threshold for translation values\n\n# Save the per-sample errors and the accumulated metric to dictionaries, for later inspection.\nerrors = {scene: {} for scene in scaling_dict.keys()}\nmAA = {scene: {} for scene in scaling_dict.keys()}\n\n# matcher_LoFTR = matcher_SE2_LoFTR\n\n# Instantiate the matcher.\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n\n# scene = 'colosseum_exterior'\nfor scene in scaling_dict.keys():\n    covisibility_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')\n    \n    pairs = [pair for pair, covis in covisibility_dict.items() if covis >= 0.1]\n    print(f'-- Processing scene \"{scene}\": found {len(pairs)} pairs \\\n            (will keep {min(len(pairs), max_pairs_per_scene)})', flush=True)\n    \n    # Subsample the pairs. Note that they are roughly sorted by difficulty (easy ones first), \n    # so we shuffle them beforehand: results would be misleading otherwise.\n    # random.shuffle(pairs)\n    #\n    # You can change the parameters here to include more image pairs for validation, \n    # for illustration purpose, I am only putting one pair for each scene here\n    pairs = pairs[0:1]\n    print(\"len(pairs): \" + str(len(pairs)))\n    \n    # Extract the images in these pairs (we don't need to load images we will not use).\n    pair_ids = []\n    for pair in pairs:\n        cur_ids = pair.split('-')\n        assert cur_ids[0] > cur_ids[1]\n        pair_ids += cur_ids\n    pair_ids = list(set(pair_ids))\n    \n    # Load ground truth data.\n    calib_dict = LoadCalibration(f'{src}/{scene}/calibration.csv')\n    \n    # Load images one by one (not in pairs)\n    images_path_dict = {}\n    for id in tqdm(pair_ids):\n        images_path_dict[id] = f'{src}/{scene}/images/{id}.jpg'\n    \n\n    # print(images_path_dict)\n    # Process the pairs.\n    for counter, pair in enumerate(pairs):\n        id1, id2 = pair.split('-')\n        # print(id1)\n        # Compute matches to get the keypoints' coordinates\n        image_fpath_1 = images_path_dict[id1]\n        image_fpath_2 = images_path_dict[id2]\n        image_1 = cv2.imread(image_fpath_1)\n        image_2 = cv2.imread(image_fpath_2)\n        image_1_tensor, scale = load_torch_image(device, fname=None, local_image=image_1, size=-1)\n        image_2_tensor, scale = load_torch_image(device, fname=None, local_image=image_2, size=-1)\n        \n        img1_max_dim = max(image_1.shape[0], image_1.shape[1])\n        img2_max_dim = max(image_2.shape[0], image_2.shape[1])\n        max_dim = max(img1_max_dim, img2_max_dim)\n        if max_dim > 1250:\n            max_dim = 1250\n        if max_dim < 750:\n            max_dim = 750\n            \n        input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1_tensor), \n                      \"image1\": K.color.rgb_to_grayscale(image_2_tensor)}\n\n        # First use LoFTR to get a coarse match (I only use one image size for illustration purpose)\n        conf_th = [0.75, 0.5, 0.25, 0]\n#         mkpts0_LoFTR, mkpts1_LoFTR, conf_mean_LoFTR, conf_th_LoFTR, scale_1_LoFTR, scale_2_LoFTR = \\\n#                                     get_keypoints_with_conf_LoFTR(image_1, image_2, matcher_LoFTR,\\\n#                                     img_resize=-1, conf_th=conf_th, take_all=False)  \n        \n#         mkpts0_LoFTR_resize1, mkpts1_LoFTR_resize1, conf_mean_LoFTR_resize1, conf_th_LoFTR_resize1,\\\n#                 scale_1_LoFTR_resize1, scale_2_LoFTR_resize1 = get_keypoints_with_conf_LoFTR(image_1, \\\n#                 image_2, matcher_LoFTR, img_resize=max_dim*1.2, conf_th=conf_th, take_all=False)\n\n        mkpts0_LoFTR_resize2, mkpts1_LoFTR_resize2, conf_mean_LoFTR_resize2, conf_th_LoFTR_resize2, \\\n                scale_1_LoFTR_resize2, scale_2_LoFTR_resize2 = get_keypoints_with_conf_LoFTR(image_1, \\\n                image_2, matcher_LoFTR, img_resize=840, conf_th=conf_th, take_all=False)\n        \n        \n        # Second use SuperGlue to get a coarse match (I only use one image size for illustration purpose))\n        mkpts0_SG, mkpts1_SG, conf_mean_SG, conf_th_SG, scale_1_SG, scale_2_SG = \\\n                        get_keypoints_with_conf_SG(image_fpath_1,image_fpath_2, matcher_SG, \\\n                        resize=[-1, ], resize_float=resize_float, conf_th=conf_th, take_all=False)\n\n#         mkpts0_SG_resize1, mkpts1_SG_resize1, conf_mean_SG_resize1, conf_th_SG_resize1, scale_1_SG_resize1, \\\n#                         scale_2_SG_resize1 = get_keypoints_with_conf_SG(image_fpath_1,image_fpath_2, matcher_SG, \\\n#                         resize=[max_dim*1.5, ], resize_float=resize_float, conf_th=conf_th, take_all=False)\n\n#         mkpts0_SG_resize2, mkpts1_SG_resize2, conf_mean_SG_resize2, conf_th_SG_resize2, scale_1_SG_resize2, \\\n#                         scale_2_SG_resize2 = get_keypoints_with_conf_SG(image_fpath_1,image_fpath_2, matcher_SG, \\\n#                         resize=[max_dim*0.6, ], resize_float=resize_float, conf_th=conf_th, take_all=False)\n        \n        # conf_th_mean_SG = np.mean([conf_th_SG, conf_th_SG_resize1, conf_th_SG_resize2])\n        \n        # mkpts0_LoFTR_ns = mkpts0_LoFTR * scale_1_LoFTR\n        # mkpts0_LoFTR_s1 = mkpts0_LoFTR_resize1 * scale_1_LoFTR_resize1\n        mkpts0_LoFTR_s2 = mkpts0_LoFTR_resize2 * scale_1_LoFTR_resize2\n        # mkpts1_LoFTR_ns = mkpts1_LoFTR * scale_2_LoFTR\n        # mkpts1_LoFTR_s1 = mkpts1_LoFTR_resize1 * scale_2_LoFTR_resize1\n        mkpts1_LoFTR_s2 = mkpts1_LoFTR_resize2 * scale_2_LoFTR_resize2\n        \n        mkpts0_SG_ns = mkpts0_SG * scale_1_SG\n        # mkpts0_SG_s1 = mkpts0_SG_resize1 * scale_1_SG_resize1\n        # mkpts0_SG_s2 = mkpts0_SG_resize2 * scale_1_SG_resize2\n        mkpts1_SG_ns = mkpts1_SG * scale_2_SG\n        # mkpts1_SG_s1 = mkpts1_SG_resize1 * scale_2_SG_resize1\n        # mkpts1_SG_s2 = mkpts1_SG_resize2 * scale_2_SG_resize2\n        \n        \n#         mkpts0_combined = np.concatenate((mkpts0_LoFTR_s1, mkpts0_LoFTR_s2, \\\n#                                           mkpts0_SG_ns, mkpts0_SG_s1, mkpts0_SG_s2), axis=0)\n#         mkpts1_combined = np.concatenate((mkpts1_LoFTR_s1, mkpts1_LoFTR_s2, \\\n#                                           mkpts1_SG_ns, mkpts1_SG_s1, mkpts1_SG_s2), axis=0)\n        \n        mkpts0_combined = np.concatenate((mkpts0_LoFTR_s2, mkpts0_SG_ns), axis=0)                 \n        mkpts1_combined = np.concatenate((mkpts1_LoFTR_s2, mkpts1_SG_ns), axis=0)                                      \n\n        # Get the F-matrix \n        if len(mkpts0_combined) > 7:\n            F, inliers = cv2.findFundamentalMat(mkpts0_combined, mkpts1_combined, cv2.USAC_MAGSAC, 0.2, 0.99999, 250000)\n#             F, inliers = pydegensac.findFundamentalMatrix(mkpts0_combined, mkpts1_combined, \\\n#                                          px_th=0.2, conf=0.99999, max_iters=800000, laf_consistensy_coef=0, \\\n#                                          error_type='symm_epipolar', symmetric_error_check=True, enable_degeneracy_check=True)  \n            inliers = inliers.squeeze() > 0  \n            assert F.shape == (3, 3), 'Malformed F?'\n            # F_dict[sample_id] = F  \n\n            print(\"number of inliers: \" + str(np.count_nonzero(inliers)))\n\n        else:\n            print(\"zero F matrix\")\n            # F_dict[sample_id] = np.zeros((3, 3))\n            # continue\n\n\n        # keypoints' coordinates after RANSAC\n        mkpts0_final = mkpts1_combined[inliers]\n        mkpts1_final = mkpts1_combined[inliers]\n\n        # Compute the essential matrix.\n        E_LoFTR, R_LoFTR, T_LoFTR = ComputeEssentialMatrix(F, calib_dict[id1].K, calib_dict[id2].K, mkpts0_final, mkpts1_final)\n        q_LoFTR = QuaternionFromMatrix(R_LoFTR)\n        T_LoFTR = T_LoFTR.flatten()\n\n        # Get the relative rotation and translation between these two cameras, given their R and T in the global reference frame.\n        # print(str(id1) + str(calib_dict[id1].T))\n        # print(str(id2) + str(calib_dict[id2].T))\n        R1_gt, T1_gt = calib_dict[id1].R, calib_dict[id1].T.reshape((3, 1))\n        R2_gt, T2_gt = calib_dict[id2].R, calib_dict[id2].T.reshape((3, 1))\n        dR_gt = np.dot(R2_gt, R1_gt.T)\n        dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n        q_gt = QuaternionFromMatrix(dR_gt)\n        q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n\n        # Compute the error for this example.\n        err_q_LoFTR, err_t_LoFTR = ComputeErrorForOneExample(q_gt, dT_gt, q_LoFTR, T_LoFTR, scaling_dict[scene])\n        errors[scene][pair] = [err_q_LoFTR, err_t_LoFTR]\n        \n        # torch.cuda.empty_cache()\n        gc.collect()\n        \n\n        # Plot the resulting matches and the pose error.\n        if verbose or (show_images and counter < num_show_images):\n            print(f'{pair}, err_q={(err_q_LoFTR):.02f} (deg), err_t={(err_t_LoFTR):.02f} (m)', flush=True)\n        if show_images and counter < num_show_images:\n            draw_LAF_matches(\n                KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0_combined).view(1,-1, 2),\n                                            torch.ones(mkpts0_combined.shape[0]).view(1,-1, 1, 1),\n                                            torch.ones(mkpts0_combined.shape[0]).view(1,-1, 1)),\n\n                KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1_combined).view(1,-1, 2),\n                                            torch.ones(mkpts1_combined.shape[0]).view(1,-1, 1, 1),\n                                            torch.ones(mkpts1_combined.shape[0]).view(1,-1, 1)),\n                torch.arange(mkpts1_combined.shape[0]).view(-1,1).repeat(1,2),\n                K.tensor_to_image(image_1_tensor.detach().cpu()),\n                K.tensor_to_image(image_2_tensor.detach().cpu()),\n                inliers,\n                draw_dict={'inlier_color': (0.2, 1, 0.2),\n                          'tentative_color': None, \n                          'feature_color': (0.2, 0.5, 1), 'vertical': False})\n\n    # Histogram the errors over this scene.\n    mAA[scene] = ComputeMaa([v[0] for v in errors[scene].values()], [v[1] \\\n                              for v in errors[scene].values()], thresholds_q, thresholds_t)\n    print()\n    print(f'Mean average Accuracy on \"{scene}\": {mAA[scene][0]:.05f}')\n    print()\n\nprint()\nprint('------- SUMMARY -------')\nprint()\nfor scene in scaling_dict.keys():\n    print(f'-- Mean average Accuracy on \"{scene}\": {mAA[scene][0]:.05f}')\nprint()\nprint(f'Mean average Accuracy on dataset: {np.mean([mAA[scene][0] for scene in mAA]):.05f}')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:26:12.730867Z","iopub.execute_input":"2022-06-03T00:26:12.731243Z","iopub.status.idle":"2022-06-03T00:26:12.746091Z","shell.execute_reply.started":"2022-06-03T00:26:12.731101Z","shell.execute_reply":"2022-06-03T00:26:12.74528Z"},"trusted":true},"execution_count":null,"outputs":[]}]}