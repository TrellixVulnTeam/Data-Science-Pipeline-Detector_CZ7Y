{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This notebook is one of my final submissions which had the best private score.\n### I also tried emsembling DKM and SE2-LoFTR under multiple conditions and multi-stage approaches,but unfortunately those did not work well for me","metadata":{}},{"cell_type":"code","source":"%%capture\ndry_run = False\n!pip install ../input/kornialoftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install ../input/kornialoftr/kornia_moons-0.1.9-py3-none-any.whl","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\n\nimport os\nimport csv\nimport random\nfrom glob import glob\nfrom tqdm import tqdm\nfrom collections import namedtuple\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport torch\nimport torchvision.transforms as transforms\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport kornia\nfrom kornia_moons.feature import *\nimport kornia as K\nimport kornia.feature as KF\nimport gc\nimport pydegensac\n\nimport sys\nimport time\n\nsys.path.append(\"../input/\")\nsys.path.append(\"../input/super-glue-pretrained-network\")\n\nfrom models.matching import Matching as Matching_SuperGlue\nfrom models.utils import (compute_pose_error, compute_epipolar_error,\n                          estimate_pose, make_matching_plot,\n                          error_colormap, AverageTimer, pose_auc, read_image,\n                          rotate_intrinsics, rotate_pose_inplane,\n                          scale_intrinsics)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-03T00:26:24.677227Z","iopub.execute_input":"2022-06-03T00:26:24.677982Z","iopub.status.idle":"2022-06-03T00:26:24.691025Z","shell.execute_reply.started":"2022-06-03T00:26:24.67793Z","shell.execute_reply":"2022-06-03T00:26:24.690304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check which GPUs I am assigned to\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n  print('and then re-execute this cell.')\nelse:\n  print(gpu_info)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:25:55.843649Z","iopub.execute_input":"2022-06-03T00:25:55.844192Z","iopub.status.idle":"2022-06-03T00:25:55.916806Z","shell.execute_reply.started":"2022-06-03T00:25:55.844152Z","shell.execute_reply":"2022-06-03T00:25:55.915594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## General Helper Functions","metadata":{}},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\ndef load_torch_image(device, fname=None, local_image=None, size=840.0):\n    # If the image is already in memory\n    if local_image is None:\n        img = cv2.imread(fname)\n    else:\n        img = np.copy(local_image)\n        \n    if size == -1:\n        scale = 1\n    else:\n        scale = float(size) / float(max(img.shape[0], img.shape[1]))\n    \n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    img = cv2.resize(img, (w, h))\n    img = K.image_to_tensor(img, False).float() /255.0\n    img = K.color.bgr_to_rgb(img)\n    \n    # the scale value here is the new_size / old_size, different from the original SuperGlue \n    return img.to(device), scale\n\ntest_samples_df = pd.DataFrame(test_samples, columns=[\"sample_id\", \"batch_id\", \"image_0_id\", \"image_1_id\"])\ntest_samples_df","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:25:55.919795Z","iopub.execute_input":"2022-06-03T00:25:55.92011Z","iopub.status.idle":"2022-06-03T00:25:55.955704Z","shell.execute_reply.started":"2022-06-03T00:25:55.920054Z","shell.execute_reply":"2022-06-03T00:25:55.954938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load SuperGlue","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nresize = [-1, ] # resize = [-1, ] means no resize\n# resize = 840\nresize_float = True\n\nconfig = {\n    \"superpoint\": {\n        # \"nms_radius\": 4,\n        \"nms_radius\": 4,\n        \"keypoint_threshold\": 0.005,\n        \"max_keypoints\": 2048\n    },\n    \"superglue\": {\n        \"weights\": \"outdoor\",\n        \"sinkhorn_iterations\": 160,\n        \"match_threshold\": 0.2,\n    }\n}\nmatcher_SG = Matching_SuperGlue(config).eval().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:25:55.95705Z","iopub.execute_input":"2022-06-03T00:25:55.957322Z","iopub.status.idle":"2022-06-03T00:25:59.859762Z","shell.execute_reply.started":"2022-06-03T00:25:55.95729Z","shell.execute_reply":"2022-06-03T00:25:59.859047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load LoFTR","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmatcher_LoFTR = KF.LoFTR(pretrained=None)\nmatcher_LoFTR.load_state_dict(torch.load(\"../input/kornialoftr/loftr_outdoor.ckpt\")['state_dict'])\nmatcher_LoFTR = matcher_LoFTR.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:25:59.860943Z","iopub.execute_input":"2022-06-03T00:25:59.861286Z","iopub.status.idle":"2022-06-03T00:26:00.888032Z","shell.execute_reply.started":"2022-06-03T00:25:59.861249Z","shell.execute_reply":"2022-06-03T00:26:00.887277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions to extract keypoints and mateches from the two models","metadata":{}},{"cell_type":"code","source":"def get_keypoints_with_conf_LoFTR(image_1, image_2, matcher, img_resize=840.0, conf_th=[0.75, 0.5, 0.25, 0], num_keypoints=1000, take_all=False):\n                          \n    # the scale value here is the new_size / old_size, different from SuperGlue, to make it the same, take the inverse\n    image_1_tensor, scale_1 = load_torch_image(device, fname=None, local_image=image_1, size=img_resize)\n    image_2_tensor, scale_2 = load_torch_image(device, fname=None, local_image=image_2, size=img_resize)\n    scale_1 = float(1.0 / float(scale_1))\n    scale_2 = float(1.0 / float(scale_2))\n    \n    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1_tensor), \n                  \"image1\": K.color.rgb_to_grayscale(image_2_tensor)}\n    \n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n        \n    mkpts0_LoFTR = correspondences['keypoints0'].cpu().numpy()\n    mkpts1_LoFTR = correspondences['keypoints1'].cpu().numpy()\n    conf = correspondences['confidence'].cpu().numpy()\n    # print(\"initial number of LoFTR points: \" + str(len(mkpts0_LoFTR)))\n    \n    if take_all == True:\n        return mkpts0_LoFTR, mkpts1_LoFTR, np.mean(conf), 0, scale_1, scale_2\n    \n    # Create bins of points according to confidence thresholds\n    mkpts0_LoFTR_conf_0 = mkpts0_LoFTR[conf > conf_th[0]]\n    mkpts1_LoFTR_conf_0 = mkpts1_LoFTR[conf > conf_th[0]]\n    mkpts0_LoFTR_conf_1 = mkpts0_LoFTR[conf > conf_th[1]]\n    mkpts1_LoFTR_conf_1= mkpts1_LoFTR[conf > conf_th[1]]\n    mkpts0_LoFTR_conf_2 = mkpts0_LoFTR[conf > conf_th[2]]\n    mkpts1_LoFTR_conf_2 = mkpts1_LoFTR[conf > conf_th[2]]\n    mkpts0_LoFTR_all = mkpts0_LoFTR[conf >= conf_th[3]]\n    mkpts1_LoFTR_all = mkpts1_LoFTR[conf >= conf_th[3]]\n    \n    # Use a progressive method to select the confidence threshold\n    #  If there are too many keypoints, take high confidence, otherwise take low\n    num_bin_1 = len(mkpts0_LoFTR_conf_0)\n    num_bin_2 = len(mkpts0_LoFTR_conf_1) - num_bin_1\n    num_bin_3 = len(mkpts0_LoFTR_conf_2) - num_bin_2 - num_bin_1\n    num_bin_4 = len(mkpts0_LoFTR_all) - num_bin_3 - num_bin_2 - num_bin_1\n    \n    largest_bin_index = np.argmax(np.array([num_bin_1, num_bin_2, num_bin_3, num_bin_4]))\n    conf_th_final = conf_th[largest_bin_index]\n    \n    mkpts0_LoFTR_final = mkpts0_LoFTR[conf > conf_th_final]\n    mkpts1_LoFTR_final = mkpts1_LoFTR[conf > conf_th_final]\n    conf_mean = np.mean(conf[conf > conf_th_final])\n    \n    if len(mkpts0_LoFTR_final) <= 7:\n        mkpts0_LoFTR_final = mkpts0_LoFTR_all\n        mkpts1_LoFTR_final = mkpts1_LoFTR_all\n        conf_mean = np.mean(conf)\n    \n    # Since experiments show that sometimes LoFTR can create an excessive amount of matching points\n    if len(mkpts0_LoFTR_final) > num_keypoints:\n        conf_final = conf[conf > conf_th_final]\n        conf_argsorted = np.argsort(conf_final)\n        selected_indices = conf_argsorted[-num_keypoints:]\n        print(np.min(selected_indices))\n        mkpts0_LoFTR_final = mkpts0_LoFTR_final[selected_indices]\n        mkpts1_LoFTR_final = mkpts1_LoFTR_final[selected_indices]\n        \n    print(\"final number of LoFTR points: \" + str(len(mkpts1_LoFTR_final)))\n    return mkpts0_LoFTR_final, mkpts1_LoFTR_final, conf_mean, conf_th_final, scale_1, scale_2\n\n\ndef get_keypoints_with_conf_SG(image_fpath_0, image_fpath_1, matcher, resize, resize_float, conf_th=[0.75, 0.5, 0.25, 0], take_all=False):\n    \n    # scale = original_size / new_size, different from the original SuperGlue. \n    image_0, inp_0, scales_0 = read_image(image_fpath_0, device, resize, 0, resize_float)\n    image_1, inp_1, scales_1 = read_image(image_fpath_1, device, resize, 0, resize_float)\n\n    input_dict = {\"image0\": inp_0, \"image1\": inp_1}\n\n    with torch.no_grad():\n        pred_SG = matcher(input_dict)\n        \n    pred_SG = {k: v[0].detach().cpu().numpy() for k, v in pred_SG.items()}\n    kpts0_SG, kpts1_SG = pred_SG[\"keypoints0\"], pred_SG[\"keypoints1\"]\n    # matches mask are different \"matches0\" and \"matches1\" since the number of keypoints are different\n    # but the valid keypoints after applying the mask will be the same (actually still different, probably a bug)\n    matches_mask_0_SG, conf_0 = pred_SG[\"matches0\"], pred_SG[\"matching_scores0\"]\n    \n    valid_0 = matches_mask_0_SG > -1\n    mkpts0_SG = kpts0_SG[valid_0]\n    mkpts1_SG = kpts1_SG[matches_mask_0_SG[valid_0]]\n    conf_0 = conf_0[valid_0]\n    conf = conf_0\n    \n    # print(\"initial number of SG points: \" + str(len(mkpts0_SG)))\n    if take_all == True:\n        return mkpts0_SG, mkpts1_SG, np.mean(conf), 0, scale_1, scale_2\n    \n    # Create bins of points according to confidence thresholds\n    mkpts0_SG_conf_0 = mkpts0_SG[conf > conf_th[0]]\n    mkpts1_SG_conf_0 = mkpts1_SG[conf > conf_th[0]]\n    mkpts0_SG_conf_1 = mkpts0_SG[conf > conf_th[1]]\n    mkpts1_SG_conf_1= mkpts1_SG[conf > conf_th[1]]\n    mkpts0_SG_conf_2 = mkpts0_SG[conf > conf_th[2]]\n    mkpts1_SG_conf_2 = mkpts1_SG[conf > conf_th[2]]\n    mkpts0_SG_all = mkpts0_SG[conf >= conf_th[3]]\n    mkpts1_SG_all = mkpts1_SG[conf >= conf_th[3]]\n    \n    # Use a progressive method to select the confidence threshold\n    #  If there are too many keypoints, take high confidence, otherwise take low\n    num_bin_1 = len(mkpts0_SG_conf_0)\n    num_bin_2 = len(mkpts0_SG_conf_1) - num_bin_1\n    num_bin_3 = len(mkpts0_SG_conf_2) - num_bin_2 - num_bin_1\n    num_bin_4 = len(mkpts0_SG_all) - num_bin_3 - num_bin_2 - num_bin_1\n    \n    largest_bin_index = np.argmax(np.array([num_bin_1, num_bin_2, num_bin_3, num_bin_4]))\n    conf_th_final = conf_th[largest_bin_index]\n    \n    mkpts0_SG_final = mkpts0_SG[conf > conf_th_final]\n    mkpts1_SG_final = mkpts1_SG[conf > conf_th_final]\n    conf_mean = np.mean(conf[conf > conf_th_final])\n    \n    if len(mkpts0_SG_final) <= 7:\n        mkpts0_SG_final = mkpts0_SG_all\n        mkpts1_SG_final = mkpts1_SG_all\n        conf_mean = np.mean(conf)\n\n    print(\"final number of SG points: \" + str(len(mkpts0_SG_final)))\n    return mkpts0_SG_final, mkpts1_SG_final, conf_mean, conf_th_final, scales_0, scales_1","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:28:50.520994Z","iopub.execute_input":"2022-06-03T00:28:50.521414Z","iopub.status.idle":"2022-06-03T00:28:50.547881Z","shell.execute_reply.started":"2022-06-03T00:28:50.521379Z","shell.execute_reply":"2022-06-03T00:28:50.547088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract keypoints with LoFTR and computer the F matrix","metadata":{}},{"cell_type":"code","source":"F_dict = {}\nimport time\nfrom matplotlib import pyplot as plt\n\nnum_kpts_LoFTR = 1000\nconf_th = [0.75, 0.5, 0.25, 0]\n\nfor i, row in enumerate(test_samples):\n    sample_id, batch_id, image_1_id, image_2_id = row\n    \n    # Load the images.\n    st = time.time()\n    image_fpath_1 = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n    image_fpath_2 = f'{src}/test_images/{batch_id}/{image_2_id}.png'\n    image_1 = cv2.imread(image_fpath_1)\n    image_2 = cv2.imread(image_fpath_2)\n    image_1_tensor, scale = load_torch_image(device, fname=None, local_image=image_1, size=-1)\n    image_2_tensor, scale = load_torch_image(device, fname=None, local_image=image_2, size=-1)\n\n    img1_max_dim = max(image_1.shape[0], image_1.shape[1])\n    img2_max_dim = max(image_2.shape[0], image_2.shape[1])\n    max_dim = max(img1_max_dim, img2_max_dim)\n    \n    # limit the image size, the input images shouldn't be too big or too small\n    if max_dim > 1250:\n        max_dim = 1250\n    if max_dim < 750:\n        max_dim = 750\n\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1_tensor), \n                  \"image1\": K.color.rgb_to_grayscale(image_2_tensor)}\n\n    # First use LoFTR to get a coarse match \n    mkpts0_LoFTR_resize2, mkpts1_LoFTR_resize2, conf_mean_LoFTR_resize2, conf_th_LoFTR_resize2, \\\n            scale_1_LoFTR_resize2, scale_2_LoFTR_resize2 = get_keypoints_with_conf_LoFTR(image_1, \\\n            image_2, matcher_LoFTR, img_resize=840, conf_th=conf_th, num_keypoints=num_kpts_LoFTR, take_all=False)\n    conf_th_mean_LoFTR = np.mean([conf_th_LoFTR_resize2])\n\n    # Second use SuperGlue to get a coarse match \n    mkpts0_SG, mkpts1_SG, conf_mean_SG, conf_th_SG, scale_1_SG, scale_2_SG = \\\n                    get_keypoints_with_conf_SG(image_fpath_1,image_fpath_2, matcher_SG, \\\n                    resize=[-1, ], resize_float=resize_float, conf_th=conf_th, take_all=False)\n\n    mkpts0_SG_resize1, mkpts1_SG_resize1, conf_mean_SG_resize1, conf_th_SG_resize1, scale_1_SG_resize1, \\\n                    scale_2_SG_resize1 = get_keypoints_with_conf_SG(image_fpath_1,image_fpath_2, matcher_SG, \\\n                    resize=[max_dim*1.6, ], resize_float=resize_float, conf_th=conf_th, take_all=False)\n\n    conf_th_mean_SG = np.mean([conf_th_SG, conf_th_SG_resize1])\n\n    # Map the keypoints back according to the image sizes\n    mkpts0_LoFTR_s2 = mkpts0_LoFTR_resize2 * scale_1_LoFTR_resize2\n    mkpts1_LoFTR_s2 = mkpts1_LoFTR_resize2 * scale_2_LoFTR_resize2\n\n    mkpts0_SG_ns = mkpts0_SG * scale_1_SG\n    mkpts0_SG_s1 = mkpts0_SG_resize1 * scale_1_SG_resize1\n    mkpts1_SG_ns = mkpts1_SG * scale_2_SG\n    mkpts1_SG_s1 = mkpts1_SG_resize1 * scale_2_SG_resize1\n\n\n    mkpts0_combined = np.concatenate((mkpts0_LoFTR_s2, mkpts0_SG_ns, mkpts0_SG_s1), axis=0)                                    \n    mkpts1_combined = np.concatenate((mkpts1_LoFTR_s2, mkpts1_SG_ns, mkpts1_SG_s1), axis=0)                            \n\n    # Get the F-matrix \n    if len(mkpts0_combined) > 7:\n        F, inliers = cv2.findFundamentalMat(mkpts0_combined, mkpts1_combined, cv2.USAC_MAGSAC, 0.2, 0.99999, 250000)\n        inliers = inliers.squeeze() > 0  \n        assert F.shape == (3, 3), 'Malformed F?'\n        F_dict[sample_id] = F  \n\n    else:\n        print(\"zero F matrix\")\n        F_dict[sample_id] = np.zeros((3, 3))\n        \n    gc.collect()\n    # torch.cuda.empty_cache()\n    \n    nd = time.time()   \n    \n    if (i < 3):\n        print(\"Running time: \", nd - st, \" s\")\n        draw_LAF_matches(\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0_combined).view(1,-1, 2),\n                                    torch.ones(mkpts0_combined.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts0_combined.shape[0]).view(1,-1, 1)),\n\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1_combined).view(1,-1, 2),\n                                    torch.ones(mkpts1_combined.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts1_combined.shape[0]).view(1,-1, 1)),\n        torch.arange(mkpts0_combined.shape[0]).view(-1,1).repeat(1,2),\n        K.tensor_to_image(image_1_tensor),\n        K.tensor_to_image(image_2_tensor),\n        inliers,\n        draw_dict={'inlier_color': (0.2, 1, 0.2),\n                   'tentative_color': None, \n                   'feature_color': (0.2, 0.5, 1), 'vertical': False})\n\n        \nwith open('submission.csv', 'w') as f:\n    f.write('sample_id,fundamental_matrix\\n')\n    for sample_id, F in F_dict.items():\n        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T00:28:51.904454Z","iopub.execute_input":"2022-06-03T00:28:51.904912Z","iopub.status.idle":"2022-06-03T00:29:01.130622Z","shell.execute_reply.started":"2022-06-03T00:28:51.904878Z","shell.execute_reply":"2022-06-03T00:29:01.129993Z"},"trusted":true},"execution_count":null,"outputs":[]}]}