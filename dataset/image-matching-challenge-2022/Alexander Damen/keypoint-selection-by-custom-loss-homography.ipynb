{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Example of keypoint selection using homography with a custom loss function\n\nThe idea is to select keypoints on both images that match well after applying homography between the keypoints. The points that match up nicely are used for calculating the fundamental matrix. I tested this approach and did not get better results than using the keypoints directly because of the power of random sample consensus (RANSAC) that is used in the OpenCV function that calculates the fundamental matrix. However the idea might still be usefull. Perhaps the best improvement can be gained by modifying the RANSAC (since the distance to epipolar lines is used as criterion, this can be modified as well).","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport skimage.io as io\nimport cv2\nimport os\nimport numpy as np\n\nsrc = '/kaggle/input/image-matching-challenge-2022/train/grand_place_brussels'\ncovisibility = pd.read_csv(os.path.join(src, \"pair_covisibility.csv\"))\nprint(\"Covisibility between the two pairs of images: {:.2f}\".format(covisibility['covisibility'][0]))\npair = covisibility['pair'][0].split('-')\nimg0 = io.imread(os.path.join(src, \"images\", pair[0]+'.jpg'))\nimg1 = io.imread(os.path.join(src, \"images\", pair[1]+'.jpg'))\nf_matrix = np.array(covisibility['fundamental_matrix'][0].split(\" \"), dtype=\"float\").reshape((3, 3))\nprint('Fundamental Matrix:')\nprint(f_matrix)\n\nfig, ax = plt.subplots(ncols=2)\nax[0].imshow(img0)\nax[1].imshow(img1)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:10:42.830947Z","iopub.execute_input":"2022-05-24T20:10:42.831224Z","iopub.status.idle":"2022-05-24T20:10:43.876911Z","shell.execute_reply.started":"2022-05-24T20:10:42.831138Z","shell.execute_reply":"2022-05-24T20:10:43.87614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use the SIFT detector to determine the keypoints, we will use the same parameters as used in the evaluation script that determines the leaderboard score. Found the FLANN matcher in one of the shared notebooks (don't know which one anymore).","metadata":{}},{"cell_type":"code","source":"img = cv2.imread(os.path.join(src, \"images\", pair[0]+'.jpg'))\nimg3 = cv2.imread(os.path.join(src, \"images\", pair[1]+'.jpg'))\n# Initiate SIFT detector\nsift = cv2.SIFT_create()\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute(img,None)\nkp2, des2 = sift.detectAndCompute(img3,None)\nimg2 = cv2.drawKeypoints(img, kp1, None, color=(255,0,0), flags=0)\nimg4 = cv2.drawKeypoints(img3, kp2, None, color=(255,0,0), flags=0)\n\nfig, ax = plt.subplots(ncols=2, figsize=(20, 10))\nax[0].imshow(img2)\nax[1].imshow(img4)\nfig.suptitle('SIFT keypoints on image')\n\nFLANN_INDEX_KDTREE = 1\nindex_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\nsearch_params = dict(checks = 50)\n\nflann = cv2.FlannBasedMatcher(index_params, search_params)\nmatches = flann.knnMatch(des1,des2,k=2)\n# store all the good matches as per Lowe's ratio test.\ngood = []\nfor m,n in matches:\n    if m.distance < 0.7*n.distance:\n        good.append(m)\n\nsrc_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\ndst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n\n# Calculate homography using the keypoints of both images.\nM, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n\nprint('Homography Matrix')\nprint(M)\n\nim_dst2 = cv2.warpPerspective(img, M, (img4.shape[1], img4.shape[0]))\n\nfig, ax = plt.subplots(ncols = 2, figsize=(20, 10))\nax[0].imshow(im_dst2)\nax[1].imshow(img3)\nfig.suptitle('Image after homography transform (OpenCV function)')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:10:43.878499Z","iopub.execute_input":"2022-05-24T20:10:43.879372Z","iopub.status.idle":"2022-05-24T20:10:46.589114Z","shell.execute_reply.started":"2022-05-24T20:10:43.87932Z","shell.execute_reply":"2022-05-24T20:10:46.587749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### FOLLOWING BLOCK OF CODE IS JUST COPIED FROM THE EVALUATION SCRIPT -> SKIP IT","metadata":{}},{"cell_type":"code","source":"\"\"\"\nEvaluate performance -- COPIED FROM EVALUATION CODE\n\"\"\"\n\nimport os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\nfrom copy import deepcopy\nfrom tqdm import tqdm\nimport random\n\n# Check that you're using a recent OpenCV version.\nassert cv2.__version__ > '4.5', 'Please use OpenCV 4.5 or later.'\n\n# Some useful functions and definitions. You can skip this for now.\n\n# A named tuple containing the intrinsics (calibration matrix K) and extrinsics (rotation matrix R, translation vector T) for a given camera.\nGt = namedtuple('Gt', ['K', 'R', 'T'])\n\n# A small epsilon.\neps = 1e-15\n\n# We use two different sets of thresholds over rotation and translation. Do not change this -- these are the values used by the scoring back-end.\nthresholds_q = np.linspace(1, 10, 10)\nthresholds_t = np.geomspace(0.2, 5, 10)\n\ndef LoadCalibration(filename):\n    '''Load calibration data (ground truth) from the csv file.'''\n    \n    calib_dict = {}\n    with open(filename, 'r') as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n\n            camera_id = row[0]\n            K = np.array([float(v) for v in row[1].split(' ')]).reshape([3, 3])\n            R = np.array([float(v) for v in row[2].split(' ')]).reshape([3, 3])\n            T = np.array([float(v) for v in row[3].split(' ')])\n            calib_dict[camera_id] = Gt(K=K, R=R, T=T)\n    \n    return calib_dict\n\n\ndef ReadCovisibilityData(filename):\n    covisibility_dict = {}\n    with open(filename) as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n            covisibility_dict[row[0]] = float(row[1])\n\n    return covisibility_dict\n\n\ndef NormalizeKeypoints(keypoints, K):\n    C_x = K[0, 2]\n    C_y = K[1, 2]\n    f_x = K[0, 0]\n    f_y = K[1, 1]\n    keypoints = (keypoints - np.array([[C_x, C_y]])) / np.array([[f_x, f_y]])\n    return keypoints\n\n\ndef ComputeEssentialMatrix(F, K1, K2, kp1, kp2):\n    '''Compute the Essential matrix from the Fundamental matrix, given the calibration matrices. Note that we ask participants to estimate F, i.e., without relying on known intrinsics.'''\n    \n    # Warning! Old versions of OpenCV's RANSAC could return multiple F matrices, encoded as a single matrix size 6x3 or 9x3, rather than 3x3.\n    # We do not account for this here, as the modern RANSACs do not do this:\n    # https://opencv.org/evaluating-opencvs-new-ransacs\n    assert F.shape[0] == 3, 'Malformed F?'\n\n    # Use OpenCV's recoverPose to solve the cheirality check:\n    # https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#gadb7d2dfcc184c1d2f496d8639f4371c0\n    E = np.matmul(np.matmul(K2.T, F), K1).astype(np.float64)\n    \n    kp1n = NormalizeKeypoints(kp1, K1)\n    kp2n = NormalizeKeypoints(kp2, K2)\n    num_inliers, R, T, mask = cv2.recoverPose(E, kp1n, kp2n)\n\n    return E, R, T\n\n\ndef ArrayFromCvKps(kps):\n    '''Convenience function to convert OpenCV keypoints into a simple numpy array.'''\n    \n    return np.array([kp.pt for kp in kps])\n\n\ndef QuaternionFromMatrix(matrix):\n    '''Transform a rotation matrix into a quaternion.'''\n\n    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n    m00 = M[0, 0]\n    m01 = M[0, 1]\n    m02 = M[0, 2]\n    m10 = M[1, 0]\n    m11 = M[1, 1]\n    m12 = M[1, 2]\n    m20 = M[2, 0]\n    m21 = M[2, 1]\n    m22 = M[2, 2]\n\n    K = np.array([[m00 - m11 - m22, 0.0, 0.0, 0.0],\n              [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n              [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n              [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]])\n    K /= 3.0\n\n    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n    w, V = np.linalg.eigh(K)\n    q = V[[3, 0, 1, 2], np.argmax(w)]\n\n    if q[0] < 0:\n        np.negative(q, q)\n\n    return q\n\n\ndef ExtractSiftFeatures(image, detector, num_features):\n    '''Compute SIFT features for a given image.'''\n    \n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    kp, desc = detector.detectAndCompute(gray, None)\n    return kp[:num_features], desc[:num_features]\n\n\ndef ComputeErrorForOneExample(q_gt, T_gt, q, T, scale):\n    '''Compute the error metric for a single example.\n    \n    The function returns two errors, over rotation and translation. These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n    \n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t\n\n\ndef ComputeMaa(err_q, err_t, thresholds_q, thresholds_t):\n    '''Compute the mean Average Accuracy at different tresholds, for one scene.'''\n    \n    assert len(err_q) == len(err_t)\n    \n    acc, acc_q, acc_t = [], [], []\n    for th_q, th_t in zip(thresholds_q, thresholds_t):\n        acc += [(np.bitwise_and(np.array(err_q) < th_q, np.array(err_t) < th_t)).sum() / len(err_q)]\n        acc_q += [(np.array(err_q) < th_q).sum() / len(err_q)]\n        acc_t += [(np.array(err_t) < th_t).sum() / len(err_t)]\n    return np.mean(acc), np.array(acc), np.array(acc_q), np.array(acc_t)\n\n# Load ground truth data.\nsrc = \"/kaggle/input/image-matching-challenge-2022/train\"\nscene = \"grand_place_brussels\"\ncalib_dict = LoadCalibration(f'{src}/{scene}/calibration.csv')\nscaling_dict = {}\nwith open(f'{src}/scaling_factors.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        scaling_dict[row[0]] = float(row[1])\n\ncalib_id1 = calib_dict[pair[0]]\ncalib_id2 = calib_dict[pair[1]]\nscale = scaling_dict[scene]\n\ndef compute_evaluation_one_scene(F, calib_id1, calib_id2, src, scene, id1, id2, scale, num_features=5000, contrastThreshold=-10000):\n    # Instantiate the matcher.\n    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n    # You may want to lower the detection threshold, as small images may not be able to reach the budget otherwise.\n    # Note that you may actually get more than num_features features, as a feature for one point can have multiple orientations (this is rare).\n    sift_detector = cv2.SIFT_create(num_features, contrastThreshold=contrastThreshold, edgeThreshold=-10000)\n    \n    # Compute descriptors between image pair\n    image1 = cv2.cvtColor(cv2.imread(f'{src}/{scene}/images/{id1}.jpg'), cv2.COLOR_BGR2RGB)\n    image2 = cv2.cvtColor(cv2.imread(f'{src}/{scene}/images/{id2}.jpg'), cv2.COLOR_BGR2RGB)\n    kp1, desc1 = ExtractSiftFeatures(image1, sift_detector, 2000)\n    kp2, desc2 = ExtractSiftFeatures(image2, sift_detector, 2000)\n    \"\"\"\n    for kp in kp2:\n        kp.pt = (kp.pt[0] + x_offset, kp.pt[1])\n    \"\"\"\n    \n    # Compute matches by brute force.\n    cv_matches = bf.match(desc1, desc2)\n    matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n    cur_kp_1 = ArrayFromCvKps([kp1[m[0]] for m in matches])\n    cur_kp_2 = ArrayFromCvKps([kp2[m[1]] for m in matches])\n    \n    # Filter matches with RANSAC.\n    _F, inlier_mask = cv2.findFundamentalMat(cur_kp_1, cur_kp_2, cv2.USAC_MAGSAC, 0.25, 0.99999, 10000)\n    inlier_mask = inlier_mask.astype(bool).flatten()\n\n    matches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\n    inlier_kp_1 = ArrayFromCvKps([kp1[m[0]] for m in matches_after_ransac])\n    inlier_kp_2 = ArrayFromCvKps([kp2[m[1]] for m in matches_after_ransac])\n    \n    \n    # Compute the essential matrix.\n    E, R, T = ComputeEssentialMatrix(F, calib_id1.K, calib_id2.K, inlier_kp_1, inlier_kp_2)\n    q = QuaternionFromMatrix(R)\n    T = T.flatten()\n    \n    # Get the relative rotation and translation between these two cameras, given their R and T in the global reference frame.\n    R1_gt, T1_gt = calib_id1.R, calib_id1.T.reshape((3, 1))\n    R2_gt, T2_gt = calib_id2.R, calib_id2.T.reshape((3, 1))\n    dR_gt = np.dot(R2_gt, R1_gt.T)\n    dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n    q_gt = QuaternionFromMatrix(dR_gt)\n    q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n\n    # Compute the error for this example.\n    err_q, err_t = ComputeErrorForOneExample(q_gt, dT_gt, q, T, scale)\n    print(\"Error rotation (degrees): {}\".format(err_q))\n    print(\"Error translation (meters): {}\".format(err_t))\n    return inlier_kp_1, inlier_kp_2\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:10:46.591195Z","iopub.execute_input":"2022-05-24T20:10:46.59153Z","iopub.status.idle":"2022-05-24T20:10:46.65861Z","shell.execute_reply.started":"2022-05-24T20:10:46.59149Z","shell.execute_reply":"2022-05-24T20:10:46.657624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compare accuracy of ground truth fundamental matrix and the one estimated using OpenCV's function on the FLANN matched SIFT keypoints\nAs we can see the ground truth indeed has an error in both rotation and translation less than 0.001, whereas the estimated one has a significantly higher error.","metadata":{}},{"cell_type":"code","source":"print(\"---- Ground Truth ----\")\n_a = compute_evaluation_one_scene(f_matrix, calib_id1, calib_id2, src, scene, pair[0], pair[1], scale)\nprint(\"---- OpenCV - SIFT + FLANN + MAGSAC\")\nF, inliers = cv2.findFundamentalMat(src_pts, dst_pts, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n_a = compute_evaluation_one_scene(F, calib_id1, calib_id2, src, scene, pair[0], pair[1], scale)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:10:46.661005Z","iopub.execute_input":"2022-05-24T20:10:46.661339Z","iopub.status.idle":"2022-05-24T20:10:48.656423Z","shell.execute_reply.started":"2022-05-24T20:10:46.661304Z","shell.execute_reply":"2022-05-24T20:10:48.654391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we calculate the homography using a custom loss function that is relatively robust to outliers.\nUsed a custom loss function based on the absolute distance between the points for homography and minimizing this. Least squares uses the quadratic and is more susceptible to outliers. The custom one should be more robust for outliers. As we can see the OpenCV fit is still better.","metadata":{}},{"cell_type":"code","source":"from scipy.optimize import minimize\nfrom scipy.optimize import basinhopping\n\ndef homography_transform_points(ikp, m_arr):\n    M = np.ones(9)\n    M[:-1] = m_arr\n    M = M.reshape((3, 3))\n    ikp_expanded = np.ones((3, ikp.shape[0]))\n    ikp_expanded[:2, :] = ikp.T\n    ikp_homography = np.matmul(M, ikp_expanded).T\n    return ikp_homography\n    \ndef loss(m_arr, src_pts, dst_pts):\n    src_ho = homography_transform_points(np.squeeze(src_pts), m_arr)\n    coords = np.vstack((src_ho[:, 0]/src_ho[:, 2], src_ho[:, 1]/src_ho[:, 2])).T\n    loss = np.sum(np.abs((coords - np.squeeze(dst_pts))))\n    return loss\n\ndef calc_homography_matrix(m_arr):\n    M = np.ones(9)\n    M[:-1] = m_arr\n    M = M.reshape((3, 3))\n    return M\n    \n\nres = minimize(loss, [1, 0, 0, 0, 1, 0, 0, 0], args=(src_pts, dst_pts), tol=1e-6)\nprint(calc_homography_matrix(res.x))\nminimizer_kwargs = {\"method\": \"BFGS\"}\n\n# Remove the redundant dimension from the keypoints\nsrc_pts = np.squeeze(src_pts)\ndst_pts = np.squeeze(dst_pts)\n\n\nikp1_ho = homography_transform_points(src_pts, res.x)\n# M was obtained by OpenCV homography as calculated in one of the previous cells.\nikp1_hom = homography_transform_points(src_pts, M.reshape(-1)[:-1])\nfig, ax = plt.subplots(ncols=2, figsize=(20, 10))\nax[0].set_title('Scatterplot of the keypoints over the full image')\nax[0].plot(dst_pts[:, 0], dst_pts[:, 1], '.')\nax[0].plot(ikp1_ho[:, 0]/ikp1_ho[:, 2], ikp1_ho[:, 1]/ikp1_ho[:, 2], '.')\nax[0].plot(ikp1_hom[:, 0]/ikp1_hom[:, 2], ikp1_hom[:, 1]/ikp1_hom[:, 2], '.')\nax[0].legend(['Keypoints Image 2', 'Custom Homography Keypoints Image 1', 'OpenCV Homography Keypoints Image 1'])\nax[1].set_title('Scatterplot of the keypoints over a zoomed in portion of the image')\nax[1].plot(dst_pts[:, 0], dst_pts[:, 1], '.')\nax[1].plot(ikp1_ho[:, 0]/ikp1_ho[:, 2], ikp1_ho[:, 1]/ikp1_ho[:, 2], '.')\nax[1].plot(ikp1_hom[:, 0]/ikp1_hom[:, 2], ikp1_hom[:, 1]/ikp1_hom[:, 2], '.')\nax[1].set_xlim([400, 600])\nax[1].set_ylim([800, 1000])\nax[1].legend(['Keypoints Image 2', 'Custom Homography Keypoints Image 1', 'OpenCV Homography Keypoints Image 1'])\n","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:10:48.658131Z","iopub.execute_input":"2022-05-24T20:10:48.658447Z","iopub.status.idle":"2022-05-24T20:10:49.271838Z","shell.execute_reply.started":"2022-05-24T20:10:48.658413Z","shell.execute_reply":"2022-05-24T20:10:49.270971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now we calculate the absolute delta between the points and keep the ones with a small absolute delta.\nNote that in my submission I decided to keep the 20 points with the smallest absolute delta, if there were less than 20 points all of them were kept.","metadata":{}},{"cell_type":"code","source":"#abs_delta = np.sum(np.abs(np.vstack((src_ho[:, 0]/src_ho[:, 2], src_ho[:, 1]/src_ho[:, 2])).T - np.squeeze(dst_pts)), axis=1)\nkey_ho = np.array([ikp1_ho[:, 0]/ikp1_ho[:, 2], ikp1_ho[:, 1]/ikp1_ho[:, 2]]).T\nabs_delta = np.sum(np.abs(key_ho - np.squeeze(dst_pts)), axis=1)\n\n\nfig, ax = plt.subplots(ncols=2, figsize=(20, 10))\nax[0].set_title('Histogram of the absolute delta of the keypoints after custom homography')\nax[0].hist(abs_delta, bins=100)\n\nsel_bool = abs_delta < 20\n\nax[1].set_title('Filtered points that have an absolute delta less than 20')\nax[1].plot(np.squeeze(dst_pts)[sel_bool, 0], np.squeeze(dst_pts)[sel_bool, 1], '.')\nax[1].plot(key_ho[sel_bool, 0], key_ho[sel_bool, 1], '.')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:10:49.273352Z","iopub.execute_input":"2022-05-24T20:10:49.273685Z","iopub.status.idle":"2022-05-24T20:10:49.734536Z","shell.execute_reply.started":"2022-05-24T20:10:49.273637Z","shell.execute_reply":"2022-05-24T20:10:49.733868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Next the custom homography is calculated based on the filtered points, the fit afterwards is much better.","metadata":{}},{"cell_type":"code","source":"src_pts_filtered = np.squeeze(src_pts)[sel_bool, :]\ndst_pts_filtered = np.squeeze(dst_pts)[sel_bool, :]\n\n\nres = minimize(loss, [1, 0, 0, 0, 1, 0, 0, 0], args=(src_pts_filtered, dst_pts_filtered), tol=1e-6)\n\nsrc_ho = homography_transform_points(np.squeeze(src_pts_filtered), res.x)\n\nplt.figure(figsize=(10, 10))\nplt.plot(np.squeeze(dst_pts_filtered)[:, 0], np.squeeze(dst_pts_filtered)[:, 1], '.')\nplt.plot(src_ho[:, 0]/src_ho[:, 2], src_ho[:, 1]/src_ho[:, 2], '.')\nplt.title('Custom Homography on Filtered Points')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:10:49.736138Z","iopub.execute_input":"2022-05-24T20:10:49.736469Z","iopub.status.idle":"2022-05-24T20:10:50.079811Z","shell.execute_reply.started":"2022-05-24T20:10:49.736422Z","shell.execute_reply":"2022-05-24T20:10:50.078807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now the errors in translation and rotation are calculated again for this image pair\nWe can see that the fit for rotation worsens but the fit for translation improves considerably.","metadata":{}},{"cell_type":"code","source":"print(\"---- Ground Truth ----\")\n_a = compute_evaluation_one_scene(f_matrix, calib_id1, calib_id2, src, scene, pair[0], pair[1], scale)\nprint(\"---- OpenCV - SIFT + FLANN + MAGSAC\")\nF, inliers = cv2.findFundamentalMat(src_pts, dst_pts, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n_a = compute_evaluation_one_scene(F, calib_id1, calib_id2, src, scene, pair[0], pair[1], scale)\nprint('---- Custom Homography Filtering - SIFT + FLANN')\nF_filtered, inliers = cv2.findFundamentalMat(src_pts_filtered, dst_pts_filtered, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n_a = compute_evaluation_one_scene(F_filtered, calib_id1, calib_id2, src, scene, pair[0], pair[1], scale)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:10:50.081379Z","iopub.execute_input":"2022-05-24T20:10:50.081727Z","iopub.status.idle":"2022-05-24T20:10:52.75947Z","shell.execute_reply.started":"2022-05-24T20:10:50.08168Z","shell.execute_reply":"2022-05-24T20:10:52.758417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Let's plot the homography warped image of the final solution (we can see the homography is quite nice)","metadata":{}},{"cell_type":"code","source":"im_dst3 = cv2.warpPerspective(img, calc_homography_matrix(res.x), (img4.shape[1], img4.shape[0]))\n\nfig, ax = plt.subplots(ncols = 2, figsize=(20, 10))\nax[0].imshow(im_dst3)\nax[1].imshow(img3)\nfig.suptitle('Image after homography transform (OpenCV function)')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T20:13:12.317516Z","iopub.execute_input":"2022-05-24T20:13:12.317946Z","iopub.status.idle":"2022-05-24T20:13:12.922951Z","shell.execute_reply.started":"2022-05-24T20:13:12.317906Z","shell.execute_reply":"2022-05-24T20:13:12.921914Z"},"trusted":true},"execution_count":null,"outputs":[]}]}