{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tutorial: Creating a baseline submission\n\nThis notebook shows you how to create and submit a submission with off-the-shelf algorithms from OpenCV.\n\nThe test set for this competition is hidden, and you score your solution by submitting the notebook. First, run the notebook with internet access on (right pane) and `dry_run=True`. Then you can set `dry_run=False`, toggle internet off, and submit the notebook for scoring using the \"submit\" button on the right pane.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-04T09:17:49.279548Z","iopub.execute_input":"2022-04-04T09:17:49.279803Z","iopub.status.idle":"2022-04-04T09:17:49.59657Z","shell.execute_reply.started":"2022-04-04T09:17:49.279736Z","shell.execute_reply":"2022-04-04T09:17:49.595964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If enabled, the notebook will return some feedback and draw images. Set to False before submitting.\ndry_run = True","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:17:49.598013Z","iopub.execute_input":"2022-04-04T09:17:49.59822Z","iopub.status.idle":"2022-04-04T09:17:49.601423Z","shell.execute_reply.started":"2022-04-04T09:17:49.598193Z","shell.execute_reply":"2022-04-04T09:17:49.600746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definitions.\n\ndef ExtractSiftFeatures(image, detector, num_features):\n    '''Compute SIFT features for a given image.'''\n    \n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    # Note that you may actually get more than num_features features, as a feature for one point can have multiple orientations (this is rare).    \n    return detector.detectAndCompute(gray, None)[:num_features]\n\n\ndef ArrayFromCvKps(kps):\n    '''Convenience function to convert OpenCV keypoints into a simple numpy array.'''\n    \n    return np.array([kp.pt for kp in kps])\n\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\ndef BuildCompositeImage(im1, im2, axis=1, margin=0, background=1):\n    '''Convenience function to stack two images with different sizes.'''\n    \n    if background != 0 and background != 1:\n        background = 1\n    if axis != 0 and axis != 1:\n        raise RuntimeError('Axis must be 0 (vertical) or 1 (horizontal')\n\n    h1, w1, _ = im1.shape\n    h2, w2, _ = im2.shape\n\n    if axis == 1:\n        composite = np.zeros((max(h1, h2), w1 + w2 + margin, 3), dtype=np.uint8) + 255 * background\n        if h1 > h2:\n            voff1, voff2 = 0, (h1 - h2) // 2\n        else:\n            voff1, voff2 = (h2 - h1) // 2, 0\n        hoff1, hoff2 = 0, w1 + margin\n    else:\n        composite = np.zeros((h1 + h2 + margin, max(w1, w2), 3), dtype=np.uint8) + 255 * background\n        if w1 > w2:\n            hoff1, hoff2 = 0, (w1 - w2) // 2\n        else:\n            hoff1, hoff2 = (w2 - w1) // 2, 0\n        voff1, voff2 = 0, h1 + margin\n    composite[voff1:voff1 + h1, hoff1:hoff1 + w1, :] = im1\n    composite[voff2:voff2 + h2, hoff2:hoff2 + w2, :] = im2\n\n    return (composite, (voff1, voff2), (hoff1, hoff2))\n\n\ndef DrawMatches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n    '''Draw keypoints and matches.'''\n    \n    composite, v_offset, h_offset = BuildCompositeImage(im1, im2, axis, margin, background)\n\n    # Draw all keypoints.\n    for coord_a, coord_b in zip(kp1, kp2):\n        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n    \n    # Draw matches, and highlight keypoints used in matches.\n    for idx_a, idx_b in matches:\n        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.line(composite,\n                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n                                   int(kp1[idx_a][1] + v_offset[0])]),\n                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n    return composite","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:17:49.603149Z","iopub.execute_input":"2022-04-04T09:17:49.603389Z","iopub.status.idle":"2022-04-04T09:17:49.630695Z","shell.execute_reply.started":"2022-04-04T09:17:49.603356Z","shell.execute_reply":"2022-04-04T09:17:49.630063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the pairs file.\n\nsrc = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n\nif dry_run:\n    for sample in test_samples:\n        print(sample)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:17:49.632428Z","iopub.execute_input":"2022-04-04T09:17:49.632733Z","iopub.status.idle":"2022-04-04T09:17:49.65159Z","shell.execute_reply.started":"2022-04-04T09:17:49.632696Z","shell.execute_reply":"2022-04-04T09:17:49.650896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_features = 8000\n\n# SIFT feature detector.\n# We lower the detection threshold to extract a \"fixed\" number of features -- small images may not be able to reach the budget otherwise.\ndetector = cv2.SIFT_create(num_features, contrastThreshold=-10000, edgeThreshold=-10000)\n\n# Brute-force matcher with bi-directionaly check.\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n\n# Compute this many samples, and fill the rest with random values, to generate a quick submission and check it works without waiting for a full run. Set to -1 to use all samples.\n# how_many_to_fill = 500\nhow_many_to_fill = -1\n\nF_dict = {}\nfor i, row in enumerate(test_samples):\n    sample_id, batch_id, image_1_id, image_2_id = row\n    \n    if how_many_to_fill >= 0 and i >= how_many_to_fill:\n        F_dict[sample_id] = np.random.rand(3, 3)\n        continue\n    \n    # Load the images.\n    image_1 = cv2.cvtColor(cv2.imread(f'{src}/test_images/{batch_id}/{image_1_id}.png'), cv2.COLOR_BGR2RGB)\n    image_2 = cv2.cvtColor(cv2.imread(f'{src}/test_images/{batch_id}/{image_2_id}.png'), cv2.COLOR_BGR2RGB)\n#     F_dict[sample_id] = np.random.rand(3, 3)\n#     continue\n    \n    # Extract features.\n    keypoints_1, descriptors_1 = ExtractSiftFeatures(image_1, detector, num_features)\n    keypoints_2, descriptors_2 = ExtractSiftFeatures(image_2, detector, num_features)\n    \n    # Compute matches.\n    cv_matches = bf.match(descriptors_1, descriptors_2)\n    \n    # Compute fundamental matrix.\n    cur_kp_1 = ArrayFromCvKps(keypoints_1)\n    cur_kp_2 = ArrayFromCvKps(keypoints_2)\n    matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n    F, inlier_mask = cv2.findFundamentalMat(cur_kp_1[matches[:, 0]], cur_kp_2[matches[:, 1]], cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, confidence=0.99999, maxIters=100000)\n    F_dict[sample_id] = F\n    \n    if dry_run:\n        matches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\n        im_inliers = DrawMatches(image_1, image_2, cur_kp_1, cur_kp_2, matches_after_ransac)\n        fig = plt.figure(figsize=(15, 15))\n        plt.title(f'{image_1_id}-{image_2_id}')\n        plt.imshow(im_inliers)\n        plt.axis('off')\n        plt.show()\n\nwith open('submission.csv', 'w') as f:\n    f.write('sample_id,fundamental_matrix\\n')\n    for sample_id, F in F_dict.items():\n        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')\n\n# !head submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:17:49.652576Z","iopub.execute_input":"2022-04-04T09:17:49.65285Z","iopub.status.idle":"2022-04-04T09:17:56.022993Z","shell.execute_reply.started":"2022-04-04T09:17:49.652825Z","shell.execute_reply":"2022-04-04T09:17:56.022215Z"},"trusted":true},"execution_count":null,"outputs":[]}]}