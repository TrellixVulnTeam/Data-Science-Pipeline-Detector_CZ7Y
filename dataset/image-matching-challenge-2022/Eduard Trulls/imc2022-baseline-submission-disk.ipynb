{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tutorial: Creating a submission with GPU dependencies (DISK)\n\nThis notebook shows you how to create and submit a submission with custom dependencies.\n\nFirst, follow the steps in [this notebook](https://www.kaggle.com/code/eduardtrulls/imc2022-dependencies/edit/run/91840821) and import the resulting \"dataset\" to this notebook. Your `input` folder should now contain `imc2022-dependencies` (see right pane).\n\nThe test set for this competition is hidden, and you score your solution by submitting the notebook. First, run the notebook with internet access on (right pane) and `dry_run=True`. Then you can set `dry_run=False`, toggle internet off, and submit the notebook for scoring using the \"submit\" button on the right pane.","metadata":{}},{"cell_type":"code","source":"dry_run = True","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-01T13:19:24.621639Z","iopub.execute_input":"2022-04-01T13:19:24.622146Z","iopub.status.idle":"2022-04-01T13:19:24.650605Z","shell.execute_reply.started":"2022-04-01T13:19:24.621981Z","shell.execute_reply":"2022-04-01T13:19:24.649736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -f /kaggle/input/imc2022-dependencies/wheels --no-index torch_dimcheck\n!pip install -f /kaggle/input/imc2022-dependencies/wheels --no-index torch_localize\n!pip install -f /kaggle/input/imc2022-dependencies/wheels --no-index unets\n!pip install -f /kaggle/input/imc2022-dependencies/wheels --no-index disk","metadata":{"execution":{"iopub.status.busy":"2022-04-01T13:19:24.652243Z","iopub.execute_input":"2022-04-01T13:19:24.652653Z","iopub.status.idle":"2022-04-01T13:20:49.479984Z","shell.execute_reply.started":"2022-04-01T13:19:24.652619Z","shell.execute_reply":"2022-04-01T13:20:49.478856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport imageio\nimport numpy as np\nfrom disk import DISK, Features\nimport torch\nimport torch.nn.functional as TorchFunctional\nfrom torch.utils.data import DataLoader\nfrom torch_dimcheck import dimchecked\nfrom tqdm import tqdm\nimport argparse\nfrom functools import partial\nimport h5py\nimport matplotlib.pyplot as plt\nfrom glob import glob\nimport csv\nimport cv2\n\nif not torch.cuda.is_available():\n    print('You may want to enable the GPU switch?')","metadata":{"execution":{"iopub.status.busy":"2022-04-01T13:20:49.481925Z","iopub.execute_input":"2022-04-01T13:20:49.482315Z","iopub.status.idle":"2022-04-01T13:20:51.883093Z","shell.execute_reply.started":"2022-04-01T13:20:49.482265Z","shell.execute_reply":"2022-04-01T13:20:51.882006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\ndef BuildCompositeImage(im1, im2, axis=1, margin=0, background=1):\n    '''Convenience function to stack two images with different sizes.'''\n    \n    if background != 0 and background != 1:\n        background = 1\n    if axis != 0 and axis != 1:\n        raise RuntimeError('Axis must be 0 (vertical) or 1 (horizontal')\n\n    h1, w1, _ = im1.shape\n    h2, w2, _ = im2.shape\n\n    if axis == 1:\n        composite = np.zeros((max(h1, h2), w1 + w2 + margin, 3), dtype=np.uint8) + 255 * background\n        if h1 > h2:\n            voff1, voff2 = 0, (h1 - h2) // 2\n        else:\n            voff1, voff2 = (h2 - h1) // 2, 0\n        hoff1, hoff2 = 0, w1 + margin\n    else:\n        composite = np.zeros((h1 + h2 + margin, max(w1, w2), 3), dtype=np.uint8) + 255 * background\n        if w1 > w2:\n            hoff1, hoff2 = 0, (w1 - w2) // 2\n        else:\n            hoff1, hoff2 = (w2 - w1) // 2, 0\n        voff1, voff2 = 0, h1 + margin\n    composite[voff1:voff1 + h1, hoff1:hoff1 + w1, :] = im1\n    composite[voff2:voff2 + h2, hoff2:hoff2 + w2, :] = im2\n\n    return (composite, (voff1, voff2), (hoff1, hoff2))\n\n\ndef DrawMatches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n    '''Draw keypoints and matches.'''\n    \n    composite, v_offset, h_offset = BuildCompositeImage(im1, im2, axis, margin, background)\n\n    # Draw all keypoints.\n    for coord_a, coord_b in zip(kp1, kp2):\n        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n    \n    # Draw matches, and highlight keypoints used in matches.\n    for idx_a, idx_b in matches:\n        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.line(composite,\n                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n                                   int(kp1[idx_a][1] + v_offset[0])]),\n                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n    \n    return composite","metadata":{"execution":{"iopub.status.busy":"2022-04-01T13:20:51.885423Z","iopub.execute_input":"2022-04-01T13:20:51.885713Z","iopub.status.idle":"2022-04-01T13:20:51.912941Z","shell.execute_reply.started":"2022-04-01T13:20:51.885679Z","shell.execute_reply":"2022-04-01T13:20:51.911857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the pairs file.\n\nsrc = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n\nif dry_run:\n    for sample in test_samples:\n        print(sample)","metadata":{"execution":{"iopub.status.busy":"2022-04-01T13:20:51.914162Z","iopub.execute_input":"2022-04-01T13:20:51.915054Z","iopub.status.idle":"2022-04-01T13:20:51.939557Z","shell.execute_reply.started":"2022-04-01T13:20:51.915002Z","shell.execute_reply":"2022-04-01T13:20:51.938171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pre-compute features. Code hastily copy-pasted with minor changes from:\n# https://github.com/cvlab-epfl/disk/blob/master/detect.py\n\n!rm -rf features\n\nclass Image:\n    def __init__(self, bitmap: ['C', 'H', 'W'], fname: str, orig_shape=None):\n        self.bitmap     = bitmap\n        self.fname      = fname\n        if orig_shape is None:\n            self.orig_shape = self.bitmap.shape[1:]\n        else:\n            self.orig_shape = orig_shape\n\n    def resize_to(self, shape):\n        return Image(\n            self._pad(self._interpolate(self.bitmap, shape), shape),\n            self.fname,\n            orig_shape=self.bitmap.shape[1:],\n        )\n\n    @dimchecked\n    def to_image_coord(self, xys: [2, 'N']) -> ([2, 'N'], ['N']):\n        f, _size = self._compute_interpolation_size(self.bitmap.shape[1:])\n        scaled = xys / f\n\n        h, w = self.orig_shape\n        x, y = scaled\n\n        mask = (0 <= x) & (x < w) & (0 <= y) & (y < h)\n\n        return scaled, mask\n\n    def _compute_interpolation_size(self, shape):\n        x_factor = self.orig_shape[0] / shape[0]\n        y_factor = self.orig_shape[1] / shape[1]\n\n        f = 1 / max(x_factor, y_factor)\n\n        if x_factor > y_factor:\n            new_size = (shape[0], int(f * self.orig_shape[1]))\n        else:\n            new_size = (int(f * self.orig_shape[0]), shape[1])\n\n        return f, new_size\n\n    @dimchecked\n    def _interpolate(self, image: ['C', 'H', 'W'], shape) -> ['C', 'h', 'w']:\n        _f, size = self._compute_interpolation_size(shape)\n        return TorchFunctional.interpolate(\n            image.unsqueeze(0),\n            size=size,\n            mode='bilinear',\n            align_corners=False,\n        ).squeeze(0)\n    \n    @dimchecked\n    def _pad(self, image: ['C', 'H', 'W'], shape) -> ['C', 'h', 'w']:\n        x_pad = shape[0] - image.shape[1]\n        y_pad = shape[1] - image.shape[2]\n\n        if x_pad < 0 or y_pad < 0:\n            raise ValueError(\"Attempting to pad by negative value\")\n\n        return TorchFunctional.pad(image, (0, y_pad, 0, x_pad))\n\n    \nclass SceneDataset:\n    def __init__(self, image_path, crop_size=(None, None)):\n        self.image_path = image_path\n        self.crop_size  = crop_size\n        self.names = [p for p in os.listdir(image_path) \\\n                      if p.endswith(args.image_extension)]\n\n    def __len__(self):\n        return len(self.names)\n\n    def __getitem__(self, ix):\n        name   = self.names[ix]\n        path   = os.path.join(self.image_path, name) \n        img    = np.ascontiguousarray(imageio.imread(path))\n        tensor = torch.from_numpy(img).to(torch.float32)\n\n        if len(tensor.shape) == 2: # some images may be grayscale\n            tensor = tensor.unsqueeze(-1).expand(-1, -1, 3)\n\n        bitmap              = tensor.permute(2, 0, 1) / 255.\n        extensionless_fname = os.path.splitext(name)[0]\n\n        image = Image(bitmap, extensionless_fname)\n\n        if self.crop_size != (None, None):\n            image = image.resize_to(self.crop_size)\n\n        return image\n\n    @staticmethod\n    def collate_fn(images):\n        bitmaps = torch.stack([im.bitmap for im in images], dim=0)\n        \n        return bitmaps, images\n\n\ndef extract(dataset, save_path):\n    dataloader = DataLoader(\n        dataset,\n        batch_size=1,\n        pin_memory=True,\n        collate_fn=dataset.collate_fn,\n        num_workers=4,\n    )\n\n    if args.mode == 'nms':\n        extract = partial(\n            model.features,\n            kind='nms',\n            window_size=args.window,\n            cutoff=0.,\n            n=args.n\n        )\n    else:\n        extract = partial(model.features, kind='rng')\n\n    os.makedirs(os.path.join(save_path), exist_ok=True)\n    keypoint_h5   = h5py.File(os.path.join(save_path, 'keypoints.h5'), 'w')\n    descriptor_h5 = h5py.File(os.path.join(save_path, 'descriptors.h5'), 'w')\n    if args.detection_scores:\n        score_h5 = h5py.File(os.path.join(save_path, 'scores.h5'), 'w')\n\n    pbar = tqdm(dataloader)\n    for bitmaps, images in pbar:\n        bitmaps = bitmaps.to(DEV, non_blocking=True)\n\n        with torch.no_grad():\n            try:\n                batched_features = extract(bitmaps)\n            except RuntimeError as e:\n                if 'U-Net failed' in str(e):\n                    msg = ('Please use input size which is multiple of 16 (or '\n                           'adjust the --height and --width flags to let this '\n                           'script rescale it automatically). This is because '\n                           'we internally use a U-Net with 4 downsampling '\n                           'steps, each by a factor of 2, therefore 2^4=16.')\n\n                    raise RuntimeError(msg) from e\n                else:\n                    raise\n\n        for features, image in zip(batched_features.flat, images):\n            features = features.to(CPU)\n\n            kps_crop_space = features.kp.T\n            kps_img_space, mask = image.to_image_coord(kps_crop_space)\n\n            keypoints   = kps_img_space.numpy().T[mask]\n            descriptors = features.desc.numpy()[mask]\n            scores      = features.kp_logp.numpy()[mask]\n\n            order = np.argsort(scores)[::-1]\n\n            keypoints   = keypoints[order]\n            descriptors = descriptors[order]\n            scores      = scores[order]\n\n            assert descriptors.shape[1] == args.desc_dim\n            assert keypoints.shape[1] == 2\n\n            if args.f16:\n                descriptors = descriptors.astype(np.float16)\n\n            keypoint_h5.create_dataset(image.fname, data=keypoints)\n            descriptor_h5.create_dataset(image.fname, data=descriptors)\n\n            if args.detection_scores:\n                score_h5.create_dataset(image.fname, data=scores)\n\n            pbar.set_postfix(n=keypoints.shape[0])\n    \n    \nparser = argparse.ArgumentParser()\nparser.add_argument(\n    '--height', default=None, type=int,\n    help='rescaled height (px). If unspecified, image is not resized in height dimension'\n)\nparser.add_argument(\n    '--width', default=None, type=int,\n    help='rescaled width (px). If unspecified, image is not resized in width dimension'\n)\nparser.add_argument(\n    '--image-extension', default='jpg', type=str,\n    help='This script ill process all files which match `image-path/*.{--image-extension}`'\n)\nparser.add_argument(\n    '--f16', action='store_true',\n    help='Store descriptors in fp16 (half precision) format'\n)\nparser.add_argument('--window', type=int, default=5, help='NMS window size')\nparser.add_argument(\n    '--n', type=int, default=None,\n    help='Maximum number of features to extract. If unspecified, the number is not limited'\n)\nparser.add_argument(\n    '--desc-dim', type=int, default=128,\n    help='descriptor dimension. Needs to match the checkpoint value.'\n)\nparser.add_argument(\n    '--mode', choices=['nms', 'rng'], default='nms',\n    help=('Whether to extract features using the non-maxima suppresion mode or '\n          'through training-time grid sampling technique')\n)\nparser.add_argument(\n     '--model_path', type=str, default='/kaggle/input/imc2022-dependencies/pretrained/disk-depth.pth',\n    help=\"Path to the model's .pth save file\"\n)\nparser.add_argument('--detection-scores', action='store_true')\n\n\n# Hacky copy-paste: parameters go here.\nargs = parser.parse_args('--n 1000 --window 9 --height 496 --width 896 --image-extension png'.split())\nprint(args)\n\nDEV = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCPU = torch.device('cpu')\nstate_dict = torch.load(args.model_path, map_location='cpu')\n\n# For compatibility with older model saves.\nif 'extractor' in state_dict:\n    weights = state_dict['extractor']\nelif 'disk' in state_dict:\n    weights = state_dict['disk']\nelse:\n    raise KeyError('Incompatible weight file!')\n\nmodel = DISK(window=args.window, desc_dim=128)\nmodel.load_state_dict(weights)\nmodel = model.to(DEV)\n\n# Extract features for every image in every folder.\nfor dataset_folder in glob('/kaggle/input/image-matching-challenge-2022/test_images/*'):\n    batch_id = dataset_folder.split('/')[-1]\n    print(f'Processing \"{dataset_folder}\"')\n    dataset = SceneDataset(dataset_folder, crop_size=(args.height, args.width))\n    described_samples = extract(dataset, f'features/{batch_id}')","metadata":{"execution":{"iopub.status.busy":"2022-04-01T13:20:51.941975Z","iopub.execute_input":"2022-04-01T13:20:51.94279Z","iopub.status.idle":"2022-04-01T13:21:16.380157Z","shell.execute_reply.started":"2022-04-01T13:20:51.942734Z","shell.execute_reply":"2022-04-01T13:21:16.378851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute this many samples, and fill the rest with random values, to generate a quick submission and check it works without waiting for a full run. Set to -1 to use all samples.\n# how_many_to_fill = 500\nhow_many_to_fill = -1\n\n# Brute-force matcher with bi-directionaly check.\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n\nF_dict = {}\nfor i, row in enumerate(test_samples):\n    sample_id, batch_id, image_1_id, image_2_id = row\n\n    if how_many_to_fill >= 0 and i >= how_many_to_fill:\n        F_dict[sample_id] = np.random.rand(3, 3)\n        continue\n\n    with h5py.File(f'features/{batch_id}/keypoints.h5', 'r') as kp_dict, h5py.File(f'features/{batch_id}/descriptors.h5', 'r') as desc_dict:\n        # Compute matches.\n        cv_matches = bf.match(desc_dict[image_1_id][()], desc_dict[image_2_id][()])\n        matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])        \n        \n        # Compute fundamental matrix.\n        cur_F, inlier_mask = cv2.findFundamentalMat(kp_dict[image_1_id][()][matches[:, 0]],\n                                                kp_dict[image_2_id][()][matches[:, 1]],\n                                                cv2.USAC_MAGSAC,\n                                                ransacReprojThreshold=0.5,\n                                                confidence=0.99999,\n                                                maxIters=10000)\n        F_dict[sample_id] = cur_F\n\n        if dry_run:\n            image_1 = cv2.cvtColor(cv2.imread(f'{src}/test_images/{batch_id}/{image_1_id}.png'), cv2.COLOR_BGR2RGB)\n            image_2 = cv2.cvtColor(cv2.imread(f'{src}/test_images/{batch_id}/{image_2_id}.png'), cv2.COLOR_BGR2RGB)\n            matches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\n\n            im_inliers = DrawMatches(image_1, image_2, kp_dict[image_1_id][()], kp_dict[image_2_id][()], matches_after_ransac)\n            fig = plt.figure(figsize=(15, 15))\n            plt.title(f'{image_1_id}-{image_2_id}')\n            plt.imshow(im_inliers)\n            plt.axis('off')\n            plt.show()\n\nwith open('submission.csv', 'w') as f:\n    f.write('sample_id,fundamental_matrix\\n')\n    for sample_id, F in F_dict.items():\n        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')\n\nif dry_run:\n    !cat submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-04-01T13:21:16.382191Z","iopub.execute_input":"2022-04-01T13:21:16.382746Z","iopub.status.idle":"2022-04-01T13:21:19.765905Z","shell.execute_reply.started":"2022-04-01T13:21:16.382704Z","shell.execute_reply":"2022-04-01T13:21:19.764693Z"},"trusted":true},"execution_count":null,"outputs":[]}]}