{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tutorial: Computing the competition metric on the training set\n\nThis notebook provides code to evaluate a submission file in csv format on the training set without digging into the internals (which are a bit complex).\n\nOnce you have a submission file, you can call `EvaluateSubmission(your_submission_file, scaling_dict, thresholds_q, thresholds_t)`. The scaling factors can be found in the training dataset. The last two parameters contain the thresholds over the rotation and translation errors: we recommend using the values in this notebook, which are the ones used in the competition.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport csv\nfrom collections import namedtuple\nfrom tqdm import tqdm\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-04T09:11:49.953385Z","iopub.execute_input":"2022-04-04T09:11:49.953942Z","iopub.status.idle":"2022-04-04T09:11:49.98298Z","shell.execute_reply.started":"2022-04-04T09:11:49.953819Z","shell.execute_reply":"2022-04-04T09:11:49.982092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Gt = namedtuple('Gt', ['K', 'R', 'T'])\neps = 1e-15\n\n\ndef ReadCovisibilityData(filename):\n    '''Read covisibility data from the csv file.'''\n\n    covisibility_dict = {}\n    F_dict = {}\n    with open(filename) as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n            covisibility_dict[row[0]] = float(row[1])\n            F_dict[row[0]] = np.array([float(v) for v in row[2].split(' ')])\n    return covisibility_dict, F_dict\n\n\ndef LoadCalibration(filename):\n    '''Load calibration data (ground truth) from the csv file.'''\n    \n    calib_dict = {}\n    with open(filename, 'r') as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n\n            camera_id = row[0]\n            K = np.array([float(v) for v in row[1].split(' ')]).reshape([3, 3])\n            R = np.array([float(v) for v in row[2].split(' ')]).reshape([3, 3])\n            T = np.array([float(v) for v in row[3].split(' ')])\n            calib_dict[camera_id] = Gt(K=K, R=R, T=T)\n    \n    return calib_dict\n\n\ndef QuaternionFromMatrix(matrix):\n    '''Transform a rotation matrix into a quaternion.'''\n\n    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n    m00 = M[0, 0]\n    m01 = M[0, 1]\n    m02 = M[0, 2]\n    m10 = M[1, 0]\n    m11 = M[1, 1]\n    m12 = M[1, 2]\n    m20 = M[2, 0]\n    m21 = M[2, 1]\n    m22 = M[2, 2]\n\n    K = np.array([[m00 - m11 - m22, 0.0, 0.0, 0.0],\n              [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n              [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n              [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]])\n    K /= 3.0\n\n    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n    w, V = np.linalg.eigh(K)\n    q = V[[3, 0, 1, 2], np.argmax(w)]\n\n    if q[0] < 0:\n        np.negative(q, q)\n\n    return q\n\n\ndef ComputeErrorForOneExample(q_gt, T_gt, q, T, scale):\n    '''Compute the error metric for a single example. The function returns two errors, over rotation and translation. These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n    \n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t\n\n\ndef ComputeMaa(err_q, err_t, thresholds_q, thresholds_t):\n    '''Compute the mean Average Accuracy at different tresholds, for one scene.'''\n    \n    assert len(err_q) == len(err_t)\n    \n    acc, acc_q, acc_t = [], [], []\n    for th_q, th_t in zip(thresholds_q, thresholds_t):\n        acc += [(np.bitwise_and(np.array(err_q) < th_q, np.array(err_t) < th_t)).sum() / len(err_q)]\n        acc_q += [(np.array(err_q) < th_q).sum() / len(err_q)]\n        acc_t += [(np.array(err_t) < th_t).sum() / len(err_t)]\n    return np.mean(acc), np.array(acc), np.array(acc_q), np.array(acc_t)\n\n\ndef ComputeFundamentalMatrix(K1, K2, R1, R2, T1, T2):\n    '''Compute the fundamental matrix, given intrinsics and extrinsics for two cameras.'''\n    dR = np.dot(R2, R1.T)\n    dT = (T2 - np.dot(dR, T1)).flatten()\n    A = np.dot(K1, np.dot(dR.T, dT))\n    C = np.array([[0, -A[2], A[1]], [A[2], 0, -A[0]], [-A[1], A[0], 0]])\n    return np.matmul(np.linalg.inv(K2).T, np.matmul(dR, np.matmul(K1.T, C)))\n\n\ndef DecomposeFundamentalMatrixWithIntrinsics(F, K1, K2):\n    '''Decompose the fundamental matrix into R and T, given the intrinsics.'''\n    \n    # This fundamentally reimplements this function: https://github.com/opencv/opencv/blob/be38d4ea932bc3a0d06845ed1a2de84acc2a09de/modules/calib3d/src/five-point.cpp#L742\n    # This is a pre-requisite of OpenCV's recoverPose: https://github.com/opencv/opencv/blob/be38d4ea932bc3a0d06845ed1a2de84acc2a09de/modules/calib3d/src/five-point.cpp#L559\n    # Instead of the cheirality check with correspondences, we keep and evaluate the different hypotheses downstream, and pick the best one.\n    # Note that our metric does not care about the sign of the translation vector, so we only need to evaluate the two rotation matrices.\n    E = np.matmul(K2.T, np.matmul(F, K1))\n\n    U, S, Vh = np.linalg.svd(E)\n    if np.linalg.det(U) < 0:\n        U *= -1\n    if np.linalg.det(Vh) < 0:\n        Vh *= -1\n\n    W = np.array([[0, 1, 0], [-1, 0, 0], [0, 0, 1]])\n    R_a = np.matmul(U, np.matmul(W, Vh))\n    R_b = np.matmul(U, np.matmul(W.T, Vh))\n    T = U[:, -1]\n\n    return R_a, R_b, T\n\n\ndef ComputeErrorForOneExample(q_gt, T_gt, q, T, scale):\n    '''Compute the error metric for a single example.\n    \n    The function returns two errors, over rotation and translation. These are combined at different thresholds by ComputeMaa, downstream, in order to compute the mean Average Accuracy.'''\n    \n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t\n\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\ndef EvaluateSubmission(prediction_file, scaling_dict, thresholds_q, thresholds_t):\n    '''Evaluate a prediction file against the ground truth.\n    \n    Note that only the subset of entries in the prediction file will be evaluated.'''\n    \n    # Load the predictions file.\n    predictions = {}\n    with open(prediction_file) as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n            predictions[row[0]] = np.array([float(v) for v in row[1].split(' ')]).reshape([3, 3])\n\n    # Extract a list of scenes from the predictions file. Note that there is a single dataset, so we do not keep track of it.\n    scenes = []\n    for prediction in predictions.keys():\n        dataset, scene, pair = prediction.split(';')\n        if scene not in scenes:\n            scenes += [scene]\n        \n    # Load the ground truth.\n    calib_dict = {}\n    for scene in scenes:\n        calib_dict[scene] = LoadCalibration(f'{src}/{scene}/calibration.csv')\n    \n    errors_dict_q = {scene: {} for scene in scenes}\n    errors_dict_t = {scene: {} for scene in scenes}\n    for prediction_key, F_predicted in tqdm(predictions.items()):\n        dataset, scene, pair = prediction_key.split(';')\n        image_id_1, image_id_2 = pair.split('-')\n\n        K1, R1_gt, T1_gt = calib_dict[scene][image_id_1].K, calib_dict[scene][image_id_1].R, calib_dict[scene][image_id_1].T.reshape((3, 1))\n        K2, R2_gt, T2_gt = calib_dict[scene][image_id_2].K, calib_dict[scene][image_id_2].R, calib_dict[scene][image_id_2].T.reshape((3, 1))\n\n        R_pred_a, R_pred_b, T_pred = DecomposeFundamentalMatrixWithIntrinsics(F_predicted, K1, K2)\n        q_pred_a = QuaternionFromMatrix(R_pred_a)\n        q_pred_b = QuaternionFromMatrix(R_pred_b)\n\n        dR_gt = np.dot(R2_gt, R1_gt.T)\n        dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n        q_gt = QuaternionFromMatrix(dR_gt)\n        q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n\n        # blah blah cheirality...\n        err_q_a, err_t_a = ComputeErrorForOneExample(q_gt, dT_gt, q_pred_a, T_pred, scaling_dict[scene])\n        err_q_b, err_t_b = ComputeErrorForOneExample(q_gt, dT_gt, q_pred_b, T_pred, scaling_dict[scene])\n        assert err_t_a == err_t_b\n        errors_dict_q[scene][pair] = min(err_q_a, err_q_b)\n        errors_dict_t[scene][pair] = err_t_a\n\n    # Aggregate the results by computing the final metric for each scene, and then averaging across all scenes.\n    maa_per_scene = {}\n    for scene in scenes:\n        maa_per_scene[scene], _, _, _ = ComputeMaa(list(errors_dict_q[scene].values()), list(errors_dict_t[scene].values()), thresholds_q, thresholds_t)\n    return np.mean(list(maa_per_scene.values())), maa_per_scene, errors_dict_q, errors_dict_t","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:11:49.987002Z","iopub.execute_input":"2022-04-04T09:11:49.987362Z","iopub.status.idle":"2022-04-04T09:11:50.042005Z","shell.execute_reply.started":"2022-04-04T09:11:49.987332Z","shell.execute_reply":"2022-04-04T09:11:50.041134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load per-scene scaling factors.\n\nsrc = '../input/image-matching-challenge-2022/train'\n\nscaling_dict = {}\nwith open(f'{src}/scaling_factors.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        scaling_dict[row[0]] = float(row[1])\n\nprint(f'Scaling factors: {scaling_dict}')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:11:50.04317Z","iopub.execute_input":"2022-04-04T09:11:50.043396Z","iopub.status.idle":"2022-04-04T09:11:50.065888Z","shell.execute_reply.started":"2022-04-04T09:11:50.04337Z","shell.execute_reply":"2022-04-04T09:11:50.065275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate a submission file with random values, and another with ground truth fundamental matrices, which are stored in the covisibility files.\n\n# Trim the pairs, as some scenes are extremely large otherwise.\nmax_num_pairs = 5000\n\nwith open('submission_random.csv', 'w') as f_random, open('submission_gt.csv', 'w') as f_gt:\n    f_random.write('sample_id,fundamental_matrix\\n')\n    f_gt.write('sample_id,fundamental_matrix\\n')\n\n    for scene in scaling_dict.keys():\n        covisibility_dict, F_gt_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')\n\n        # Let's remove pairs with a covisibility below 0.1. Note that the keys are roughly sorted by difficulty, in increasing order, so let's also shuffle before subsampling.\n        # Neither matters for the purposes of this exercise, but let's prevent mistakes down the line.\n        pairs = list([key for key, covis in covisibility_dict.items() if covis >= 0.1])\n        random.shuffle(pairs)\n        n = len(pairs)\n        pairs = pairs[:max_num_pairs]\n        print(f'Loading covisibility data for \"{scene}\"... kept {len(pairs)} out of {n} covisible pairs')\n\n        for i, pair in enumerate(pairs):\n            f_random.write(f'phototourism;{scene};{pair},{FlattenMatrix(np.random.rand(3, 3))}\\n')\n            f_gt.write(f'phototourism;{scene};{pair},{FlattenMatrix(F_gt_dict[pair])}\\n')\n\nprint('Done!')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:11:50.087339Z","iopub.execute_input":"2022-04-04T09:11:50.087758Z","iopub.status.idle":"2022-04-04T09:12:13.10405Z","shell.execute_reply.started":"2022-04-04T09:11:50.087719Z","shell.execute_reply":"2022-04-04T09:12:13.10316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds_q = np.linspace(1, 10, 10)\nthresholds_t = np.geomspace(0.2, 5, 10)\n\nprint('--- Processing a random submission ---')\nmaa, maa_per_scene, errors_dict_q, errors_dict_t = EvaluateSubmission('submission_random.csv', scaling_dict, thresholds_q, thresholds_t)\nfor scene, cur_maa in maa_per_scene.items():\n    print(f'Scene \"{scene}\" ({len(errors_dict_q[scene])} pairs): mAA={cur_maa:.05f}')\nprint()\nprint(f'Full dataset: mAA={maa:.05f}')\nprint()\n\nprint('--- Processing a ground truth submission ---')\nmaa, maa_per_scene, errors_dict_q, errors_dict_t = EvaluateSubmission('submission_gt.csv', scaling_dict, thresholds_q, thresholds_t)\nfor scene, cur_maa in maa_per_scene.items():\n    print(f'Scene \"{scene}\" ({len(errors_dict_q[scene])} pairs), mAA={cur_maa:.05f}')\nprint()\nprint(f'Full dataset: mAA={maa:.05f}')\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:12:13.105814Z","iopub.execute_input":"2022-04-04T09:12:13.106339Z","iopub.status.idle":"2022-04-04T09:13:22.409001Z","shell.execute_reply.started":"2022-04-04T09:12:13.106296Z","shell.execute_reply":"2022-04-04T09:13:22.408342Z"},"trusted":true},"execution_count":null,"outputs":[]}]}