{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tutorial: Loading training data and evaluating on it\n\nThe goal of this notebook is to help participants parse the training data, understand the problem, and show them how it is typically approached. It loads the training data and produces a solution using off-the-shelf features and algorithms available in [OpenCV](https://opencv.org/).","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\nfrom copy import deepcopy\nfrom tqdm import tqdm\nimport random\n\n# Check that you're using a recent OpenCV version.\nassert cv2.__version__ > '4.5', 'Please use OpenCV 4.5 or later.'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-04T09:07:26.012663Z","iopub.execute_input":"2022-04-04T09:07:26.012966Z","iopub.status.idle":"2022-04-04T09:07:26.018505Z","shell.execute_reply.started":"2022-04-04T09:07:26.012933Z","shell.execute_reply":"2022-04-04T09:07:26.017514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some useful functions and definitions. You can skip this for now.\n\n# A named tuple containing the intrinsics (calibration matrix K) and extrinsics (rotation matrix R, translation vector T) for a given camera.\nGt = namedtuple('Gt', ['K', 'R', 'T'])\n\n# A small epsilon.\neps = 1e-15\n\n\ndef ReadCovisibilityData(filename):\n    covisibility_dict = {}\n    with open(filename) as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n            covisibility_dict[row[0]] = float(row[1])\n\n    return covisibility_dict\n\n\ndef NormalizeKeypoints(keypoints, K):\n    C_x = K[0, 2]\n    C_y = K[1, 2]\n    f_x = K[0, 0]\n    f_y = K[1, 1]\n    keypoints = (keypoints - np.array([[C_x, C_y]])) / np.array([[f_x, f_y]])\n    return keypoints\n\n\ndef ComputeEssentialMatrix(F, K1, K2, kp1, kp2):\n    '''Compute the Essential matrix from the Fundamental matrix, given the calibration matrices. Note that we ask participants to estimate F, i.e., without relying on known intrinsics.'''\n    \n    # Warning! Old versions of OpenCV's RANSAC could return multiple F matrices, encoded as a single matrix size 6x3 or 9x3, rather than 3x3.\n    # We do not account for this here, as the modern RANSACs do not do this:\n    # https://opencv.org/evaluating-opencvs-new-ransacs\n    assert F.shape[0] == 3, 'Malformed F?'\n\n    # Use OpenCV's recoverPose to solve the cheirality check:\n    # https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#gadb7d2dfcc184c1d2f496d8639f4371c0\n    E = np.matmul(np.matmul(K2.T, F), K1).astype(np.float64)\n    \n    kp1n = NormalizeKeypoints(kp1, K1)\n    kp2n = NormalizeKeypoints(kp2, K2)\n    num_inliers, R, T, mask = cv2.recoverPose(E, kp1n, kp2n)\n\n    return E, R, T\n\n\ndef ArrayFromCvKps(kps):\n    '''Convenience function to convert OpenCV keypoints into a simple numpy array.'''\n    \n    return np.array([kp.pt for kp in kps])\n\n\ndef QuaternionFromMatrix(matrix):\n    '''Transform a rotation matrix into a quaternion.'''\n\n    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n    m00 = M[0, 0]\n    m01 = M[0, 1]\n    m02 = M[0, 2]\n    m10 = M[1, 0]\n    m11 = M[1, 1]\n    m12 = M[1, 2]\n    m20 = M[2, 0]\n    m21 = M[2, 1]\n    m22 = M[2, 2]\n\n    K = np.array([[m00 - m11 - m22, 0.0, 0.0, 0.0],\n              [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n              [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n              [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]])\n    K /= 3.0\n\n    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n    w, V = np.linalg.eigh(K)\n    q = V[[3, 0, 1, 2], np.argmax(w)]\n\n    if q[0] < 0:\n        np.negative(q, q)\n\n    return q\n\n\ndef ExtractSiftFeatures(image, detector, num_features):\n    '''Compute SIFT features for a given image.'''\n    \n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    kp, desc = detector.detectAndCompute(gray, None)\n    return kp[:num_features], desc[:num_features]\n\n\ndef ComputeErrorForOneExample(q_gt, T_gt, q, T, scale):\n    '''Compute the error metric for a single example.\n    \n    The function returns two errors, over rotation and translation. These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n    \n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t\n\n\ndef ComputeMaa(err_q, err_t, thresholds_q, thresholds_t):\n    '''Compute the mean Average Accuracy at different tresholds, for one scene.'''\n    \n    assert len(err_q) == len(err_t)\n    \n    acc, acc_q, acc_t = [], [], []\n    for th_q, th_t in zip(thresholds_q, thresholds_t):\n        acc += [(np.bitwise_and(np.array(err_q) < th_q, np.array(err_t) < th_t)).sum() / len(err_q)]\n        acc_q += [(np.array(err_q) < th_q).sum() / len(err_q)]\n        acc_t += [(np.array(err_t) < th_t).sum() / len(err_t)]\n    return np.mean(acc), np.array(acc), np.array(acc_q), np.array(acc_t)\n\n\ndef BuildCompositeImage(im1, im2, axis=1, margin=0, background=1):\n    '''Convenience function to stack two images with different sizes.'''\n    \n    if background != 0 and background != 1:\n        background = 1\n    if axis != 0 and axis != 1:\n        raise RuntimeError('Axis must be 0 (vertical) or 1 (horizontal')\n\n    h1, w1, _ = im1.shape\n    h2, w2, _ = im2.shape\n\n    if axis == 1:\n        composite = np.zeros((max(h1, h2), w1 + w2 + margin, 3), dtype=np.uint8) + 255 * background\n        if h1 > h2:\n            voff1, voff2 = 0, (h1 - h2) // 2\n        else:\n            voff1, voff2 = (h2 - h1) // 2, 0\n        hoff1, hoff2 = 0, w1 + margin\n    else:\n        composite = np.zeros((h1 + h2 + margin, max(w1, w2), 3), dtype=np.uint8) + 255 * background\n        if w1 > w2:\n            hoff1, hoff2 = 0, (w1 - w2) // 2\n        else:\n            hoff1, hoff2 = (w2 - w1) // 2, 0\n        voff1, voff2 = 0, h1 + margin\n    composite[voff1:voff1 + h1, hoff1:hoff1 + w1, :] = im1\n    composite[voff2:voff2 + h2, hoff2:hoff2 + w2, :] = im2\n\n    return (composite, (voff1, voff2), (hoff1, hoff2))\n\n\ndef DrawMatches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n    '''Draw keypoints and matches.'''\n    \n    composite, v_offset, h_offset = BuildCompositeImage(im1, im2, axis, margin, background)\n\n    # Draw all keypoints.\n    for coord_a, coord_b in zip(kp1, kp2):\n        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n    \n    # Draw matches, and highlight keypoints used in matches.\n    for idx_a, idx_b in matches:\n        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.line(composite,\n                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n                                   int(kp1[idx_a][1] + v_offset[0])]),\n                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n    return composite\n\n\ndef LoadCalibration(filename):\n    '''Load calibration data (ground truth) from the csv file.'''\n    \n    calib_dict = {}\n    with open(filename, 'r') as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n\n            camera_id = row[0]\n            K = np.array([float(v) for v in row[1].split(' ')]).reshape([3, 3])\n            R = np.array([float(v) for v in row[2].split(' ')]).reshape([3, 3])\n            T = np.array([float(v) for v in row[3].split(' ')])\n            calib_dict[camera_id] = Gt(K=K, R=R, T=T)\n    \n    return calib_dict","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:10:22.124447Z","iopub.execute_input":"2022-04-04T09:10:22.124762Z","iopub.status.idle":"2022-04-04T09:10:22.175515Z","shell.execute_reply.started":"2022-04-04T09:10:22.124728Z","shell.execute_reply":"2022-04-04T09:10:22.174645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory.\n\nsrc = '../input/image-matching-challenge-2022/train'\n\nval_scenes = []\nfor f in os.scandir(src):\n    if f.is_dir():\n        cur_scene = os.path.split(f)[-1]\n        print(f'Found scene \"{cur_scene}\"\" at {f.path}')\n        val_scenes += [cur_scene]","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:07:26.084835Z","iopub.execute_input":"2022-04-04T09:07:26.085532Z","iopub.status.idle":"2022-04-04T09:07:26.110405Z","shell.execute_reply.started":"2022-04-04T09:07:26.085493Z","shell.execute_reply":"2022-04-04T09:07:26.109498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Each scene in the validation set contains a list of images, poses, and pairs. Let's pick one and look at some images.\n\nscene = 'piazza_san_marco'\n\nimages_dict = {}\nfor filename in glob(f'{src}/{scene}/images/*.jpg'):\n    cur_id = os.path.basename(os.path.splitext(filename)[0])\n\n    # OpenCV expects BGR, but the images are encoded in standard RGB, so you need to do color conversion if you use OpenCV for I/O.\n    images_dict[cur_id] = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n    \nprint(f'Loaded {len(images_dict)} images.')\n\nnum_rows = 6\nnum_cols = 4\nf, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20), constrained_layout=True)\nfor i, key in enumerate(images_dict):\n    if i >= num_rows * num_cols:\n        break\n    cur_ax = axes[i % num_rows, i // num_rows]\n    cur_ax.imshow(images_dict[key])\n    cur_ax.set_title(key)\n    cur_ax.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:07:26.26461Z","iopub.execute_input":"2022-04-04T09:07:26.265043Z","iopub.status.idle":"2022-04-04T09:07:33.190937Z","shell.execute_reply.started":"2022-04-04T09:07:26.265009Z","shell.execute_reply":"2022-04-04T09:07:33.189876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Two images from the same scene may not always overlap.\n# The dataset contains co-visibility estimates that you can use to find pairs with more or less overlap.\n# We recommend using all pairs with a co-visibility estimate of 0.1 or larger.\n# For more details, please see Section 3.2 of the paper: https://arxiv.org/abs/2003.01587.\n\ncovisibility_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')\n\n# Let's look at easy pairs first, and difficult pairs later.\neasy_subset = [k for k, v in covisibility_dict.items() if v >= 0.7]\ndifficult_subset = [k for k, v in covisibility_dict.items() if v >= 0.1 and v < 0.2]\n\nfor i, subset in enumerate([easy_subset, difficult_subset]):\n    print(f'Pairs from an {\"easy\" if i == 0 else \"difficult\"} subset')\n    \n    for pair in subset[:4]:\n        # A pair string is simply two concatenated image IDs, separated with a hyphen.\n        image_id_1, image_id_2 = pair.split('-')\n\n        f, axes = plt.subplots(1, 2, figsize=(15, 10), constrained_layout=True)\n        axes[0].imshow(images_dict[image_id_1])\n        axes[0].set_title(image_id_1)\n        axes[1].imshow(images_dict[image_id_2])\n        axes[1].set_title(image_id_2)\n        for ax in axes:\n            ax.axis('off')\n        plt.show()\n\n    print()\n    print()\n\nfig = plt.figure(figsize=(15, 10), constrained_layout=True)\nplt.title('Covisibility histogram')\nplt.hist(list(covisibility_dict.values()), bins=10, range=[0, 1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:07:33.195429Z","iopub.execute_input":"2022-04-04T09:07:33.195734Z","iopub.status.idle":"2022-04-04T09:07:39.238535Z","shell.execute_reply.started":"2022-04-04T09:07:33.1957Z","shell.execute_reply":"2022-04-04T09:07:39.237432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The task is finding the relative geometry (rotation, translation) between the two cameras.\n# You can read more about epipolar geometry here: https://en.wikipedia.org/wiki/Epipolar_geometry\n\n# This problem is typically (but not always!) solved with sparse features.\n# Let's try using SIFT, a seminal work in computer vision (https://en.wikipedia.org/wiki/Scale-invariant_feature_transform).\n# No longer the state of the art, but still pretty solid!\n\nnum_features = 5000\n\n# You may want to lower the detection threshold, as small images may not be able to reach the budget otherwise.\n# Note that you may actually get more than num_features features, as a feature for one point can have multiple orientations (this is rare).\nsift_detector = cv2.SIFT_create(num_features, contrastThreshold=-10000, edgeThreshold=-10000)\n\nkeys = list(images_dict.keys())\nkeypoints, descriptors = ExtractSiftFeatures(images_dict[keys[0]], sift_detector, num_features)\nprint(f'Computed {len(keypoints)} features.')\n\n# Each local feature contains a keypoint (xy, possibly scale, possibly orientation) and a description vector (128-dimensional for SIFT).\nimage_with_keypoints = cv2.drawKeypoints(images_dict[keys[0]], keypoints, outImage=None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nfig = plt.figure(figsize=(15, 15))\nplt.imshow(image_with_keypoints)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:10:27.580642Z","iopub.execute_input":"2022-04-04T09:10:27.581336Z","iopub.status.idle":"2022-04-04T09:10:28.509245Z","shell.execute_reply.started":"2022-04-04T09:10:27.581301Z","shell.execute_reply":"2022-04-04T09:10:28.508516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can find correspondences by brute-force-matching local features between two images. Let's do this for an easy pair.\n\npair = easy_subset[0]\nimage_id_1, image_id_2 = pair.split('-')\nkeypoints_1, descriptors_1 = ExtractSiftFeatures(images_dict[image_id_1], sift_detector, 2000)\nkeypoints_2, descriptors_2 = ExtractSiftFeatures(images_dict[image_id_2], sift_detector, 2000)\n\n# For each descriptor on one image, find the closest descriptor on the other image.\n# With crossCheck=True we keep only bidirectional matches (i.e., two features are nearest neighbours from A to B and also from B to A).\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n\n# Compute matches.\ncv_matches = bf.match(descriptors_1, descriptors_2)\n\n# Convert keypoints and matches to something more human-readable.\ncur_kp_1 = ArrayFromCvKps(keypoints_1)\ncur_kp_2 = ArrayFromCvKps(keypoints_2)\nmatches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n\n# Plot the brute-force matches.\nim_matches = DrawMatches(images_dict[image_id_1], images_dict[image_id_2], cur_kp_1, cur_kp_2, matches)\nfig = plt.figure(figsize=(25, 25))\nplt.title('Matches before RANSAC')\nplt.imshow(im_matches)\nplt.axis('off')\nplt.show()\n\n# Notice that this includes many outliers. We can filter them with a state-of-the-art RANSAC algorithm. References:\n# * https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#ga59b0d57f46f8677fb5904294a23d404a\n# * https://opencv.org/evaluating-opencvs-new-ransacs\n\n# OpenCV gives us the Fundamental matrix after RANSAC, and a mask over the input matches. The solution is clearly much cleaner, even though it may still contain outliers.\n# This F is the prediction you'll submit to the contest.\nF, inlier_mask = cv2.findFundamentalMat(cur_kp_1[matches[:, 0]], cur_kp_2[matches[:, 1]], cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, confidence=0.99999, maxIters=10000)\n\nmatches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\nim_inliers = DrawMatches(images_dict[image_id_1], images_dict[image_id_2], cur_kp_1, cur_kp_2, matches_after_ransac)\nfig = plt.figure(figsize=(25, 25))\nplt.title('Matches before RANSAC')\nplt.imshow(im_inliers)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:10:35.052616Z","iopub.execute_input":"2022-04-04T09:10:35.053169Z","iopub.status.idle":"2022-04-04T09:10:37.418946Z","shell.execute_reply.started":"2022-04-04T09:10:35.053134Z","shell.execute_reply":"2022-04-04T09:10:37.417583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Is this any good? Let's load the ground truth.\n\ncalib_dict = LoadCalibration(f'{src}/{scene}/calibration.csv')\nprint(f'Loded ground truth data for {len(calib_dict)} images')\nprint()\n\n# One important caveat: the scenes were reconstructed from unstructured image collections using Structure-from-Motion (http://colmap.github.io), and are not up to \"real-world\" scale (i.e. meters, or inches).\n# We computed a scaling factor per scene to correct this. This is necessary to compute the metric correctly.\n\nscaling_dict = {}\nwith open(f'{src}/scaling_factors.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        scaling_dict[row[0]] = float(row[1])\n\nprint(f'Scaling factors: {scaling_dict}')\nprint()\n\n# We can compute the errors now. First, let's decompose the Fundamental matrix we just estimated. TODO explain why we do this.\ninlier_kp_1 = ArrayFromCvKps([kp for i, kp in enumerate(keypoints_1) if i in matches_after_ransac[:, 0]])\ninlier_kp_2 = ArrayFromCvKps([kp for i, kp in enumerate(keypoints_2) if i in matches_after_ransac[:, 1]])\nE, R, T = ComputeEssentialMatrix(F, calib_dict[image_id_1].K, calib_dict[image_id_2].K, inlier_kp_1, inlier_kp_2)\nq = QuaternionFromMatrix(R)\nT = T.flatten()\n\n# Get the ground truth relative pose difference for this pair of images.\nR1_gt, T1_gt = calib_dict[image_id_1].R, calib_dict[image_id_1].T.reshape((3, 1))\nR2_gt, T2_gt = calib_dict[image_id_2].R, calib_dict[image_id_2].T.reshape((3, 1))\ndR_gt = np.dot(R2_gt, R1_gt.T)\ndT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\nq_gt = QuaternionFromMatrix(dR_gt)\nq_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n\n# Given ground truth and prediction, compute the error for the example above.\nerr_q, err_t = ComputeErrorForOneExample(q_gt, dT_gt, q, T, scaling_dict[scene])\nprint(f'Pair \"{pair}, rotation_error={err_q:.02f} (deg), translation_error={err_t:.02f} (m)', flush=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:10:44.38316Z","iopub.execute_input":"2022-04-04T09:10:44.384197Z","iopub.status.idle":"2022-04-04T09:10:44.4746Z","shell.execute_reply.started":"2022-04-04T09:10:44.384145Z","shell.execute_reply":"2022-04-04T09:10:44.473644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's iterate over all the scenes now. Some are much larger than others -- note that the number of pairs increases quadratically with the number of images.\n# We compute the metric for each scene, and then average it over all scenes.\n# For a quick experiment, we cap the number of image pairs for each scene to 50, and show one qualitative example per scene.\n\nshow_images = True\nnum_show_images = 1\nmax_pairs_per_scene = 50\nverbose = True\n\n# We use two different sets of thresholds over rotation and translation. Do not change this -- these are the values used by the scoring back-end.\nthresholds_q = np.linspace(1, 10, 10)\nthresholds_t = np.geomspace(0.2, 5, 10)\n\n# Save the per-sample errors and the accumulated metric to dictionaries, for later inspection.\nerrors = {scene: {} for scene in scaling_dict.keys()}\nmAA = {scene: {} for scene in scaling_dict.keys()}\n\n# Instantiate the matcher.\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n\nfor scene in scaling_dict.keys():\n    # Load all pairs, find those with a co-visibility over 0.1, and subsample them.\n    covisibility_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')    \n    pairs = [pair for pair, covis in covisibility_dict.items() if covis >= 0.1]\n    \n    print(f'-- Processing scene \"{scene}\": found {len(pairs)} pairs (will keep {min(len(pairs), max_pairs_per_scene)})', flush=True)\n    \n    # Subsample the pairs. Note that they are roughly sorted by difficulty (easy ones first), so we shuffle them beforehand: results would be misleading otherwise.\n    random.shuffle(pairs)\n    pairs = pairs[:max_pairs_per_scene]\n    \n    # Extract the images in these pairs (we don't need to load images we will not use).\n    ids = []\n    for pair in pairs:\n        cur_ids = pair.split('-')\n        assert cur_ids[0] > cur_ids[1]\n        ids += cur_ids\n    ids = list(set(ids))\n    \n    # Load ground truth data.\n    calib_dict = LoadCalibration(f'{src}/{scene}/calibration.csv')\n    \n    # Load images and extract SIFT features.\n    images_dict = {}\n    kp_dict = {}\n    desc_dict = {}\n    print('Extracting features...')\n    for id in tqdm(ids):\n        images_dict[id] = cv2.cvtColor(cv2.imread(f'{src}/{scene}/images/{id}.jpg'), cv2.COLOR_BGR2RGB)\n        kp_dict[id], desc_dict[id] = ExtractSiftFeatures(images_dict[id], sift_detector, 2000)\n    print()\n    print(f'Extracted features for {len(kp_dict)} images (avg: {np.mean([len(v) for v in desc_dict.values()])})')\n\n    # Process the pairs.\n    max_err_acc_q_new = []\n    max_err_acc_t_new = []\n    for counter, pair in enumerate(pairs):\n        id1, id2 = pair.split('-')\n\n        # Compute matches by brute force.\n        cv_matches = bf.match(desc_dict[id1], desc_dict[id2])\n        matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n        cur_kp_1 = ArrayFromCvKps([kp_dict[id1][m[0]] for m in matches])\n        cur_kp_2 = ArrayFromCvKps([kp_dict[id2][m[1]] for m in matches])\n\n        # Filter matches with RANSAC.\n        F, inlier_mask = cv2.findFundamentalMat(cur_kp_1, cur_kp_2, cv2.USAC_MAGSAC, 0.25, 0.99999, 10000)\n        inlier_mask = inlier_mask.astype(bool).flatten()\n        \n        matches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\n        inlier_kp_1 = ArrayFromCvKps([kp_dict[id1][m[0]] for m in matches_after_ransac])\n        inlier_kp_2 = ArrayFromCvKps([kp_dict[id2][m[1]] for m in matches_after_ransac])\n\n        # Compute the essential matrix.\n        E, R, T = ComputeEssentialMatrix(F, calib_dict[id1].K, calib_dict[id2].K, inlier_kp_1, inlier_kp_2)\n        q = QuaternionFromMatrix(R)\n        T = T.flatten()\n\n        # Get the relative rotation and translation between these two cameras, given their R and T in the global reference frame.\n        R1_gt, T1_gt = calib_dict[id1].R, calib_dict[id1].T.reshape((3, 1))\n        R2_gt, T2_gt = calib_dict[id2].R, calib_dict[id2].T.reshape((3, 1))\n        dR_gt = np.dot(R2_gt, R1_gt.T)\n        dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n        q_gt = QuaternionFromMatrix(dR_gt)\n        q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n\n        # Compute the error for this example.\n        err_q, err_t = ComputeErrorForOneExample(q_gt, dT_gt, q, T, scaling_dict[scene])\n        errors[scene][pair] = [err_q, err_t]\n\n        # Plot the resulting matches and the pose error.\n        if verbose or (show_images and counter < num_show_images):\n            print(f'{pair}, err_q={(err_q):.02f} (deg), err_t={(err_t):.02f} (m)', flush=True)\n        if show_images and counter < num_show_images:\n            im_inliers = DrawMatches(images_dict[id1], images_dict[id2], ArrayFromCvKps(kp_dict[id1]), ArrayFromCvKps(kp_dict[id2]), matches_after_ransac)\n            fig = plt.figure(figsize=(25, 25))\n            plt.title(f'Inliers, \"{pair}\"')\n            plt.imshow(im_inliers)\n            plt.axis('off')\n            plt.show()\n            print()\n\n    # Histogram the errors over this scene.\n    mAA[scene] = ComputeMaa([v[0] for v in errors[scene].values()], [v[1] for v in errors[scene].values()], thresholds_q, thresholds_t)\n    print()\n    print(f'Mean average Accuracy on \"{scene}\": {mAA[scene][0]:.05f}')\n    print()\n\nprint()\nprint('------- SUMMARY -------')\nprint()\nfor scene in scaling_dict.keys():\n    print(f'-- Mean average Accuracy on \"{scene}\": {mAA[scene][0]:.05f}')\nprint()\nprint(f'Mean average Accuracy on dataset: {np.mean([mAA[scene][0] for scene in mAA]):.05f}')","metadata":{"execution":{"iopub.status.busy":"2022-04-04T09:10:48.605537Z","iopub.execute_input":"2022-04-04T09:10:48.606152Z","iopub.status.idle":"2022-04-04T09:19:58.80085Z","shell.execute_reply.started":"2022-04-04T09:10:48.606115Z","shell.execute_reply":"2022-04-04T09:19:58.799879Z"},"trusted":true},"execution_count":null,"outputs":[]}]}