{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Validation settings\n\nWe want to check the CV-LB correlation, so we need to carefully screen the data for validation.","metadata":{}},{"cell_type":"code","source":"# List of scenes you want to use for validation\nscene_list = [\"british_museum\", \"brandenburg_gate\", \"buckingham_palace\",\n \"colosseum_exterior\", \"grand_place_brussels\", \"lincoln_memorial_statue\",\n \"notre_dame_front_facade\", \"pantheon_exterior\", \"piazza_san_marco\",\n \"sacre_coeur\", \"sagrada_familia\", \"st_pauls_cathedral\", \"st_peters_square\",\n \"taj_mahal\", \"temple_nara_japan\", \"trevi_fountain\"]\n\ncfg = {\n    \"covisibility_thr_min\": 0.3, # Exclude low covisibility pairs\n    \"covisibility_thr_max\": 0.7, # Exclude high covisibility pairs\n    \"min_longest_edge\": 700, # Exclude small images\n    \"dZ_thr\": 100.0, # Exclude camera pairs that are far apart in the Z direction\n    \"rotation_thr\": 0.5, # Exclude images that are heavily tilted\n    \"max_num_pairs\": 500,\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:09:33.514176Z","iopub.execute_input":"2022-06-03T22:09:33.514845Z","iopub.status.idle":"2022-06-03T22:09:33.538482Z","shell.execute_reply.started":"2022-06-03T22:09:33.514753Z","shell.execute_reply":"2022-06-03T22:09:33.53776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Useful Functions\n\nThanks to the following Notebook.\n\nhttps://www.kaggle.com/code/eduardtrulls/imc2022-training-set-eval-one-function/notebook","metadata":{}},{"cell_type":"code","source":"import csv\nimport numpy as np\nimport os\nimport pandas as pd\n\nfrom collections import namedtuple\n\nGt = namedtuple('Gt', ['K', 'R', 'T'])\neps = 1e-15\n\ndef LoadCalibration(filename):\n    '''Load calibration data (ground truth) from the csv file.'''\n\n    calib_dict = {}\n    with open(filename, 'r') as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n\n            camera_id = row[0]\n            K = np.array([float(v) for v in row[1].split(' ')]).reshape([3, 3])\n            R = np.array([float(v) for v in row[2].split(' ')]).reshape([3, 3])\n            T = np.array([float(v) for v in row[3].split(' ')])\n            calib_dict[camera_id] = Gt(K=K, R=R, T=T)\n\n    return calib_dict\n\ndef DecomposeFundamentalMatrixWithIntrinsics(F, K1, K2):\n    '''Decompose the fundamental matrix into R and T, given the intrinsics.'''\n\n    # This fundamentally reimplements this function: https://github.com/opencv/opencv/blob/be38d4ea932bc3a0d06845ed1a2de84acc2a09de/modules/calib3d/src/five-point.cpp#L742\n    # This is a pre-requisite of OpenCV's recoverPose: https://github.com/opencv/opencv/blob/be38d4ea932bc3a0d06845ed1a2de84acc2a09de/modules/calib3d/src/five-point.cpp#L559\n    # Instead of the cheirality check with correspondences, we keep and evaluate the different hypotheses downstream, and pick the best one.\n    # Note that our metric does not care about the sign of the translation vector, so we only need to evaluate the two rotation matrices.\n    E = np.matmul(K2.T, np.matmul(F, K1))\n\n    U, S, Vh = np.linalg.svd(E)\n    if np.linalg.det(U) < 0:\n        U *= -1\n    if np.linalg.det(Vh) < 0:\n        Vh *= -1\n\n    W = np.array([[0, 1, 0], [-1, 0, 0], [0, 0, 1]])\n    R_a = np.matmul(U, np.matmul(W, Vh))\n    R_b = np.matmul(U, np.matmul(W.T, Vh))\n    T = U[:, -1]\n\n    return R_a, R_b, T\n\ndef QuaternionFromMatrix(matrix):\n    '''Transform a rotation matrix into a quaternion.'''\n\n    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n    m00 = M[0, 0]\n    m01 = M[0, 1]\n    m02 = M[0, 2]\n    m10 = M[1, 0]\n    m11 = M[1, 1]\n    m12 = M[1, 2]\n    m20 = M[2, 0]\n    m21 = M[2, 1]\n    m22 = M[2, 2]\n\n    K = np.array([[m00 - m11 - m22, 0.0, 0.0, 0.0],\n              [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n              [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n              [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]])\n    K /= 3.0\n\n    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n    w, V = np.linalg.eigh(K)\n    q = V[[3, 0, 1, 2], np.argmax(w)]\n\n    if q[0] < 0:\n        np.negative(q, q)\n\n    return q\n\n\ndef ComputeErrorForOneExample(q_gt, T_gt, q, T, scale):\n    '''Compute the error metric for a single example. The function returns two errors, over rotation and translation. These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n\n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t\n\ndef ComputeMaa(err_q, err_t, thresholds_q, thresholds_t):\n    '''Compute the mean Average Accuracy at different tresholds, for one scene.'''\n\n    assert len(err_q) == len(err_t)\n\n    acc, acc_q, acc_t = [], [], []\n    for th_q, th_t in zip(thresholds_q, thresholds_t):\n        acc += [(np.bitwise_and(np.array(err_q) < th_q, np.array(err_t) < th_t)).sum() / len(err_q)]\n        acc_q += [(np.array(err_q) < th_q).sum() / len(err_q)]\n        acc_t += [(np.array(err_t) < th_t).sum() / len(err_t)]\n    return np.mean(acc), np.array(acc), np.array(acc_q), np.array(acc_t)\n\ndef EvaluateSubmission(predictions, input_dir, scaling_dict, thresholds_q, thresholds_t, output_dir=None):\n    '''Evaluate a prediction file against the ground truth.\n\n    Note that only the subset of entries in the prediction file will be evaluated.'''\n\n    # Extract a list of scenes from the predictions file. Note that there is a single dataset, so we do not keep track of it.\n    scenes = []\n    for prediction in predictions.keys():\n        dataset, scene, pair = prediction.split(';')\n        if scene not in scenes:\n            scenes += [scene]\n\n    # Load the ground truth.\n    calib_dict = {}\n    for scene in scenes:\n        calib_dict[scene] = LoadCalibration(os.path.join(input_dir, scene, \"calibration.csv\"))\n\n    errors_dict_q = {scene: {} for scene in scenes}\n    errors_dict_t = {scene: {} for scene in scenes}\n    for prediction_key, F_predicted in predictions.items():\n        dataset, scene, pair = prediction_key.split(';')\n        image_id_1, image_id_2 = pair.split('-')\n\n        K1, R1_gt, T1_gt = calib_dict[scene][image_id_1].K, calib_dict[scene][image_id_1].R, calib_dict[scene][image_id_1].T.reshape((3, 1))\n        K2, R2_gt, T2_gt = calib_dict[scene][image_id_2].K, calib_dict[scene][image_id_2].R, calib_dict[scene][image_id_2].T.reshape((3, 1))\n\n        R_pred_a, R_pred_b, T_pred = DecomposeFundamentalMatrixWithIntrinsics(F_predicted, K1, K2)\n        q_pred_a = QuaternionFromMatrix(R_pred_a)\n        q_pred_b = QuaternionFromMatrix(R_pred_b)\n\n        dR_gt = np.dot(R2_gt, R1_gt.T)\n        dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n        q_gt = QuaternionFromMatrix(dR_gt)\n        q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n\n        # blah blah cheirality...\n        err_q_a, err_t_a = ComputeErrorForOneExample(q_gt, dT_gt, q_pred_a, T_pred, scaling_dict[scene])\n        err_q_b, err_t_b = ComputeErrorForOneExample(q_gt, dT_gt, q_pred_b, T_pred, scaling_dict[scene])\n        assert err_t_a == err_t_b\n        errors_dict_q[scene][pair] = min(err_q_a, err_q_b)\n        errors_dict_t[scene][pair] = err_t_a\n\n    # Aggregate the results by computing the final metric for each scene, and then averaging across all scenes.\n    maa_per_scene = {}\n    for scene in scenes:\n        maa_per_scene[scene], _, _, _ = ComputeMaa(list(errors_dict_q[scene].values()), list(errors_dict_t[scene].values()), thresholds_q, thresholds_t)\n\n    if output_dir is not None:\n        scene_list = []\n        pair_list = []\n        maa_list = []\n        for prediction_key, F_predicted in predictions.items():\n            dataset, scene, pair = prediction_key.split(';')\n            image_id_1, image_id_2 = pair.split('-')\n\n            e_dict_q = errors_dict_q[scene][pair]\n            e_dict_t = errors_dict_t[scene][pair]\n\n            maa_pair, _, _, _ = ComputeMaa([e_dict_q], [e_dict_t], thresholds_q, thresholds_t)\n\n            scene_list.append(scene)\n            pair_list.append(pair)\n            maa_list.append(maa_pair)\n\n        df = pd.DataFrame({\"scene\":scene_list, \"pair\":pair_list, \"maa\":maa_list})\n        outpur_csv = os.path.join(output_dir, \"validation_all.csv\")\n        df.to_csv(outpur_csv, index=False)\n\n    return np.mean(list(maa_per_scene.values())), maa_per_scene, errors_dict_q, errors_dict_t\n\ndef evaluate(input_dir, sample_id_list, fund_matrix_list):\n    thresholds_q = np.linspace(1, 10, 10)\n    thresholds_t = np.geomspace(0.2, 5, 10)\n\n    # Load per-scene scaling factors.\n    scaling_dict = {}\n    with open(os.path.join(input_dir, f\"scaling_factors.csv\")) as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n            scaling_dict[row[0]] = float(row[1])\n\n    predictions = {}\n    for sample_id, fundamental_matrix in zip(sample_id_list, fund_matrix_list):\n        predictions[sample_id] = np.array([float(v) for v in fundamental_matrix.split(' ')]).reshape([3, 3])\n\n    maa, _, _, _ = EvaluateSubmission(\n        predictions,\n        input_dir,\n        scaling_dict,\n        thresholds_q,\n        thresholds_t)\n\n    return maa\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:09:33.567387Z","iopub.execute_input":"2022-06-03T22:09:33.567655Z","iopub.status.idle":"2022-06-03T22:09:33.613555Z","shell.execute_reply.started":"2022-06-03T22:09:33.567631Z","shell.execute_reply":"2022-06-03T22:09:33.612897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n\nDefine here the model you want to validate.","metadata":{}},{"cell_type":"markdown","source":"## Feature matching (LoFTR model)\n\nLoFTR: Detector-Free Local Feature Matching with Transformers\n\nhttps://arxiv.org/abs/2104.00680\n\nhttps://github.com/zju3dv/LoFTR","metadata":{}},{"cell_type":"code","source":"!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:09:33.614918Z","iopub.execute_input":"2022-06-03T22:09:33.615679Z","iopub.status.idle":"2022-06-03T22:10:05.155658Z","shell.execute_reply.started":"2022-06-03T22:09:33.615643Z","shell.execute_reply":"2022-06-03T22:10:05.154683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport kornia as K\nimport kornia.feature as KF\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\nimport torch\n\nimport warnings\nwarnings.simplefilter('ignore', UserWarning)\n\ndef load_model_LoFTR(ckpt_path, device):\n    model = KF.LoFTR(pretrained=None)\n    model.load_state_dict(torch.load(ckpt_path)[\"state_dict\"])\n    model = model.to(device).eval()\n    return model\n\ndef _convert_image_loftr(img, img_size=-1, lrflip=False, ulflip=False):\n\n    # resize\n    if img_size > 0:\n        height, width = img.shape[:2]\n        scale = img_size / max(width, height)\n        w = int(width * scale + 0.5)\n        h = int(height * scale + 0.5)\n        if scale > 1.0:\n            interpolation=cv2.INTER_CUBIC\n        else:\n            interpolation=cv2.INTER_AREA\n        img = cv2.resize(img, (w, h), interpolation=interpolation)\n    else:\n        scale = 1.0\n\n    # crop\n    height, width = img.shape[:2]\n    img = img[:height//8*8, :width//8*8]\n\n    # flip\n    if lrflip:\n        img = cv2.flip(img, 1)\n    if ulflip:\n        img = cv2.flip(img, 0)\n\n    # convert\n    img = K.image_to_tensor(img, False).float() / 255.\n    img = K.color.bgr_to_rgb(img)\n\n    return img, scale\n\ndef matching_LoFTR(input_image_1, input_image_2, param):\n\n    img_size = param[\"img_size\"]\n    matcher = param[\"model\"]\n    device = param[\"device\"]\n\n    image_1_list = []\n    image_2_list = []\n    scale_1 = 1\n    scale_2 = 1\n    for lrflip in [False, True]:\n        for ulflip in [False]:\n            _image_1, _scale_1 = _convert_image_loftr(input_image_1, img_size=img_size, lrflip=lrflip, ulflip=ulflip)\n            _image_2, _scale_2 = _convert_image_loftr(input_image_2, img_size=img_size, lrflip=lrflip, ulflip=ulflip)\n            image_1_list.append(_image_1)\n            image_2_list.append(_image_2)\n            scale_1 = _scale_1\n            scale_2 = _scale_2\n\n    image_1 = torch.cat(image_1_list, dim=0)\n    image_2 = torch.cat(image_2_list, dim=0)\n\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1).to(device),\n                  \"image1\": K.color.rgb_to_grayscale(image_2).to(device)}\n\n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n\n    mkpts1 = correspondences[\"keypoints0\"].cpu().numpy()\n    mkpts2 = correspondences[\"keypoints1\"].cpu().numpy()\n    confidence = correspondences[\"confidence\"].cpu().numpy()\n    batch_indexes = correspondences[\"batch_indexes\"].cpu().numpy()\n\n    mkpts1_all = []\n    mkpts2_all = []\n    confidence_all = []\n    batch_id = 0\n    for lrflip in [False, True]:\n        for ulflip in [False]:\n\n            idx = batch_indexes == batch_id\n            _mkpts1 = mkpts1[idx]\n            _mkpts2 = mkpts2[idx]\n            _confidence = confidence[idx]\n            if lrflip:\n                for i in range(len(_mkpts1)):\n                    _mkpts1[i][0] = image_1.shape[3] - _mkpts1[i][0]\n                    _mkpts2[i][0] = image_2.shape[3] - _mkpts2[i][0]\n            if ulflip:\n                for i in range(len(_mkpts1)):\n                    _mkpts1[i][1] = image_1.shape[2] - _mkpts1[i][1]\n                    _mkpts2[i][1] = image_2.shape[2] - _mkpts2[i][1]\n\n            mkpts1_all.append(_mkpts1)\n            mkpts2_all.append(_mkpts2)\n            confidence_all.append(_confidence)\n\n            batch_id += 1\n\n    mkpts1 = np.concatenate(mkpts1_all, axis=0)\n    mkpts2 = np.concatenate(mkpts2_all, axis=0)\n    confidence = np.concatenate(confidence_all, axis=0)\n\n    mkpts1 /= scale_1\n    mkpts2 /= scale_2\n\n    return mkpts1, mkpts2, confidence\n\n# Weighted random sampling\n# https://github.com/Parskatt/DKM/blob/988ccbc1021459b807411c3eb683d0e3432b2a15/dkm/models/dkm.py#L629-L645\ndef points_sample(dense_matches_1, dense_matches_2, dense_confidence, num = 2000, relative_confidence_threshold = 0.0):\n    matches_1 = dense_matches_1\n    matches_2 = dense_matches_2\n    confidence = dense_confidence\n    relative_confidence = confidence/confidence.max()\n    matches_1, matches_2, confidence = (\n        matches_1[relative_confidence > relative_confidence_threshold],\n        matches_2[relative_confidence > relative_confidence_threshold],\n        confidence[relative_confidence > relative_confidence_threshold],\n    )\n    good_samples = np.random.choice(\n        np.arange(len(matches_1)),\n        size=min(num, len(confidence)),\n        replace=False,\n        p=confidence/np.sum(confidence),\n    )\n    return matches_1[good_samples], matches_2[good_samples], confidence[good_samples]","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:10:05.15806Z","iopub.execute_input":"2022-06-03T22:10:05.158476Z","iopub.status.idle":"2022-06-03T22:10:08.304192Z","shell.execute_reply.started":"2022-06-03T22:10:05.158434Z","shell.execute_reply":"2022-06-03T22:10:08.303122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Validation","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom tqdm.notebook import tqdm as tqdm\n\ninput_dir = \"/kaggle/input/image-matching-challenge-2022/train\"\ncsv_dir = \"/kaggle/input/imc2022-validation-csv\"\nweight_dir = \"/kaggle/input\"\noutput_dir = \"/kaggle/working\"\noutput_csv = \"submission.csv\"\n\ndevice = torch.device(\"cuda\")\n\nLoFTR_param = {\n    \"device\": device,\n    \"model\": load_model_LoFTR(os.path.join(weight_dir, \"kornia-loftr\", \"loftr_outdoor.ckpt\"), device),\n    \"img_size\": 840,\n}\nfindFMat_param = {\n    \"ransacReprojThreshold\": 0.15,\n    \"confidence\": 0.9999,\n    \"maxIters\": 10000,\n}\n\ntotal_time = 0\nsample_id_list_all = []\nfund_matrix_list_all = []\nfor scene in scene_list:\n\n    image_dir = os.path.join(input_dir, scene, \"images\")\n\n    # load validation.csv\n    df = pd.read_csv(os.path.join(csv_dir, f\"{scene}.csv\"))\n    # Exclude low covisibility pairs\n    df = df[(df[\"covisibility\"] > cfg[\"covisibility_thr_min\"]) & (df[\"covisibility\"] < cfg[\"covisibility_thr_max\"])]\n    # Exclude small images\n    df = df[df[\"min_longest_edge\"] > cfg[\"min_longest_edge\"]]\n    # Exclude camera pairs that are far apart in the Z direction\n    df = df[df[\"dZ\"] < cfg[\"dZ_thr\"]]\n    # Exclude images that are heavily tilted\n    df = df[(df[\"image_1_rotation\"] < cfg[\"rotation_thr\"]) & (df[\"image_2_rotation\"] < cfg[\"rotation_thr\"])]\n\n    # random sample\n    df = df.sample(n=min(len(df), cfg[\"max_num_pairs\"]), random_state=42)\n\n    sample_ids = df[\"sample_id\"].values\n    image_1_ids = df[\"image_1_id\"].values\n    image_2_ids = df[\"image_2_id\"].values\n\n    sample_id_list = []\n    fund_matrix_list = []\n    for sample_id, image_1_id, image_2_id in tqdm(zip(sample_ids, image_1_ids, image_2_ids), desc=scene, total=len(sample_ids), dynamic_ncols=True):\n        \n        start = time.time()\n        # 1. Preprocess(e.g. load image)\n        input_image_1 = cv2.imread(os.path.join(image_dir, f\"{image_1_id}.jpg\"))\n        input_image_2 = cv2.imread(os.path.join(image_dir, f\"{image_2_id}.jpg\"))\n\n        # 2. Inference(e.g. matcher)\n        mkpts1, mkpts2, confidence = matching_LoFTR(input_image_1, input_image_2, LoFTR_param)\n\n        # 3 . Postprocess(e.g. RANSAC)\n        mkpts1, mkpts2, _ = points_sample(mkpts1, mkpts2, confidence)\n        if len(mkpts1) > 7:\n            F, _ = cv2.findFundamentalMat(\n                mkpts1, mkpts2, cv2.USAC_MAGSAC,\n                findFMat_param[\"ransacReprojThreshold\"],\n                findFMat_param[\"confidence\"],\n                findFMat_param[\"maxIters\"])\n            if F.shape != (3, 3):\n                F = np.zeros((3, 3))\n        else:\n            F = np.zeros((3, 3))    \n            \n        sample_id_list.append(sample_id)\n        fund_matrix_list.append(FlattenMatrix(F))\n\n        end = time.time()\n        total_time = total_time + (end - start)\n\n    if sample_id_list_all == []:\n        sample_id_list_all = sample_id_list\n        fund_matrix_list_all = fund_matrix_list\n    else:\n        sample_id_list_all.extend(sample_id_list)\n        fund_matrix_list_all.extend(fund_matrix_list)\n\n# Evaluation\nmaa = evaluate(input_dir, sample_id_list_all, fund_matrix_list_all)\nprint(f'mAA={maa:.05f} (n={len(sample_id_list_all)}), elapsed time: {total_time/60.0:.2f} min -> {(total_time/len(sample_id_list_all)):.2f} sec/pair')","metadata":{"execution":{"iopub.status.busy":"2022-06-03T22:10:08.305838Z","iopub.execute_input":"2022-06-03T22:10:08.306694Z","iopub.status.idle":"2022-06-03T22:11:40.466638Z","shell.execute_reply.started":"2022-06-03T22:10:08.306654Z","shell.execute_reply":"2022-06-03T22:11:40.465585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}