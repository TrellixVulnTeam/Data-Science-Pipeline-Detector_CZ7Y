{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install Libs","metadata":{}},{"cell_type":"code","source":"dry_run = False\n!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n!pip install ../input/imc2022-submodules/e2cnn-0.2.1-py3-none-any.whl\n!pip install ../input/imc2022-submodules/einops-0.4.1-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-27T20:41:32.82374Z","iopub.execute_input":"2022-06-27T20:41:32.824331Z","iopub.status.idle":"2022-06-27T20:43:36.831822Z","shell.execute_reply.started":"2022-06-27T20:41:32.82421Z","shell.execute_reply":"2022-06-27T20:43:36.830644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import dependencies","metadata":{}},{"cell_type":"code","source":"import cv2\nimport kornia as K\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\n\nimport warnings\nwarnings.simplefilter('ignore', UserWarning)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:43:36.83418Z","iopub.execute_input":"2022-06-27T20:43:36.83462Z","iopub.status.idle":"2022-06-27T20:43:40.453451Z","shell.execute_reply.started":"2022-06-27T20:43:36.834579Z","shell.execute_reply":"2022-06-27T20:43:40.452324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"MASK_RATIO_VERTICAL = 0.1 # for vertical(portrait) photos\nMASK_RATIO_HORIZONTAL = 0.05 # for horizontal(landscape) photos\n\ndef bottom_mask(image):\n\n    height, width = image.shape[:2]\n    mask = np.ones((height, width), dtype=np.uint8)\n    through_mask = mask\n\n    # add bottom mask\n    if height >= width:\n        # vertical\n        cv2.rectangle(mask, pt1=(0, int(height*(1-MASK_RATIO_VERTICAL))), pt2=(width, height), color=(0), thickness=-1)\n    else:\n        # horizontal\n        cv2.rectangle(mask, pt1=(0, int(height*(1-MASK_RATIO_HORIZONTAL))), pt2=(width, height), color=(0), thickness=-1)\n\n    return mask, through_mask\n\ndef crop_image(img, rect):\n\n    xmin, ymin, xmax, ymax = rect\n    xmin = max(0, int(xmin*img.shape[1] - 0.5))\n    ymin = max(0, int(ymin*img.shape[0] - 0.5))\n    xmax = min(img.shape[1], int(xmax*img.shape[1] + 0.5))\n    ymax = min(img.shape[0], int(ymax*img.shape[0] + 0.5))\n\n    return img[ymin:ymax, xmin:xmax], (xmin, ymin, xmax, ymax)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:43:40.455094Z","iopub.execute_input":"2022-06-27T20:43:40.455987Z","iopub.status.idle":"2022-06-27T20:43:40.468751Z","shell.execute_reply.started":"2022-06-27T20:43:40.455947Z","shell.execute_reply":"2022-06-27T20:43:40.467784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image matching","metadata":{}},{"cell_type":"markdown","source":"## ROI estimation (DKM model)\n\nDeep Kernelized Dense Geometric Matching\n\nhttps://arxiv.org/abs/2202.00667\n\nhttps://github.com/Parskatt/DKM","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport shutil\nimport sys\nsys.path.append('../input/imc2022-submodules/DKM')\nfrom dkm import DKM\n\ndef load_model_DKM(ckpt_path, working_dir):\n\n    checkpoints_path = os.path.join(working_dir, \"checkpoints\")\n    if not os.path.exists(checkpoints_path):\n        os.makedirs(checkpoints_path)\n    if not os.path.exists(os.path.join(checkpoints_path, \"dkm_mega.pth\")):\n        shutil.copyfile(\n            os.path.join(ckpt_path, \"dkm_mega.pth\"),\n            os.path.join(checkpoints_path, \"dkm_mega.pth\"))\n\n    torch.hub.set_dir(working_dir)\n    model = DKM(pretrained=True, version=\"mega\")\n    return model\n\ndef convert_DKM_image(img, lrflip=False, ulflip=False):\n\n    # flip\n    if lrflip:\n        img = cv2.flip(img, 1)\n    if ulflip:\n        img = cv2.flip(img, 0)\n\n    # convert\n    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n\n    return img\n\ndef expand_rect(rect, scale):\n\n    x_center = (rect[0] + rect[2])/2\n    y_center = (rect[1] + rect[3])/2\n    w = (rect[2] - rect[0]) / 2\n    h = (rect[3] - rect[1]) / 2\n    w *= scale\n    h *= scale\n    rect = [max(0, x_center - w), max(0, y_center - h), min(1, x_center + w), min(1, y_center + h)]\n\n    return rect\n\ndef matching_DKM(input_image_1, input_image_2, DKM_param):\n\n    matcher = DKM_param[\"model\"]\n    scale = DKM_param[\"scale\"]\n    sample_num = DKM_param[\"sample_num\"]\n\n    _image_1 = convert_DKM_image(input_image_1)\n    _image_2 = convert_DKM_image(input_image_2)\n\n    # step1: dense matching\n    dense_matches, dense_certainty = matcher.match(_image_1, _image_2)\n    dense_certainty = dense_certainty.sqrt()\n\n    matches, confidence = (\n        dense_matches.reshape(-1, 4).cpu().numpy(),\n        dense_certainty.reshape(-1).cpu().numpy(),\n    )\n    mkpts1 = matches[:, :2]\n    mkpts2 = matches[:, 2:]\n\n    # step2: adaptive thresholding\n    relative_confidence = confidence/confidence.max()\n    relative_confidence_threshold = (cv2.threshold((relative_confidence*255.0).astype(np.uint8), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)[0]) / 255.0\n\n    idx = relative_confidence > relative_confidence_threshold\n    mkpts1 = mkpts1[idx]\n    mkpts2 = mkpts2[idx]\n    confidence = confidence[idx]\n\n    # coordinate transformation\n    # Note that matches are produced in the normalized grid [-1, 1] x [-1, 1]\n    mkpts1[:, 0] = ((mkpts1[:, 0] + 1)/2)\n    mkpts1[:, 1] = ((mkpts1[:, 1] + 1)/2)\n    mkpts2[:, 0] = ((mkpts2[:, 0] + 1)/2)\n    mkpts2[:, 1] = ((mkpts2[:, 1] + 1)/2)\n\n    # step3: ROI estimation\n    img_pair_rect = {\n        \"image1\": [\n            np.min(mkpts1[:, 0]), np.min(mkpts1[:, 1]),\n            np.max(mkpts1[:, 0]), np.max(mkpts1[:, 1])],\n        \"image2\": [\n            np.min(mkpts2[:, 0]), np.min(mkpts2[:, 1]),\n            np.max(mkpts2[:, 0]), np.max(mkpts2[:, 1])],\n    }\n\n    img_pair_rect[\"image1\"] = expand_rect(img_pair_rect[\"image1\"], scale)\n    img_pair_rect[\"image2\"] = expand_rect(img_pair_rect[\"image2\"], scale)\n\n    # step4: sparse matching\n    sparse_matches, sparse_certainty = matcher.sample(\n        dense_matches, dense_certainty,\n        num = sample_num,\n        relative_confidence_threshold = relative_confidence_threshold)\n\n    mkpts1 = sparse_matches[:, :2]\n    mkpts2 = sparse_matches[:, 2:]\n\n    h, w = input_image_1.shape[:2]\n    mkpts1[:, 0] = ((mkpts1[:, 0] + 1)/2) * w\n    mkpts1[:, 1] = ((mkpts1[:, 1] + 1)/2) * h\n\n    h, w = input_image_2.shape[:2]\n    mkpts2[:, 0] = ((mkpts2[:, 0] + 1)/2) * w\n    mkpts2[:, 1] = ((mkpts2[:, 1] + 1)/2) * h\n\n    confidence = sparse_certainty.reshape(-1)\n\n    return img_pair_rect, mkpts1, mkpts2, confidence","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:43:40.472745Z","iopub.execute_input":"2022-06-27T20:43:40.47328Z","iopub.status.idle":"2022-06-27T20:43:46.222918Z","shell.execute_reply.started":"2022-06-27T20:43:40.473228Z","shell.execute_reply":"2022-06-27T20:43:46.221924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature matching (LoFTR model)\n\nLoFTR: Detector-Free Local Feature Matching with Transformers\n\nhttps://arxiv.org/abs/2104.00680\n\nhttps://github.com/zju3dv/LoFTR","metadata":{}},{"cell_type":"code","source":"import kornia.feature as KF\ndef load_model_LoFTR(ckpt_path, device):\n    model = KF.LoFTR(pretrained=None)\n    model.load_state_dict(torch.load(ckpt_path)[\"state_dict\"])\n    model = model.to(device).eval()\n    return model\n\ndef convert_LoFTR_image(img, img_size=-1, lrflip=False, ulflip=False):\n\n    # resize\n    if img_size > 0:\n        height, width = img.shape[:2]\n        scale = img_size / max(width, height)\n        w = int(width * scale + 0.5)\n        h = int(height * scale + 0.5)\n        if scale > 1.0:\n            interpolation=cv2.INTER_CUBIC\n        else:\n            interpolation=cv2.INTER_AREA\n        img = cv2.resize(img, (w, h), interpolation=interpolation)\n    else:\n        scale = 1.0\n\n    # crop\n    height, width = img.shape[:2]\n    img = img[:height//8*8, :width//8*8]\n\n    # flip\n    if lrflip:\n        img = cv2.flip(img, 1)\n    if ulflip:\n        img = cv2.flip(img, 0)\n\n    # convert\n    img = K.image_to_tensor(img, False).float() / 255.\n    img = K.color.bgr_to_rgb(img)\n\n    return img, scale\n\ndef matching_loftr(input_image_1, input_image_2, mask_1, mask_2, LoFTR_param):\n\n    img_size = LoFTR_param[\"img_size\"]\n    matcher = LoFTR_param[\"model\"]\n    device = LoFTR_param[\"device\"]\n\n    image_1_list = []\n    image_2_list = []\n    scale_1 = 1\n    scale_2 = 1\n    for lrflip in [False, True]:\n        for ulflip in [False]:\n            _image_1, _scale_1 = convert_LoFTR_image(input_image_1, img_size=img_size, lrflip=lrflip, ulflip=ulflip)\n            _image_2, _scale_2 = convert_LoFTR_image(input_image_2, img_size=img_size, lrflip=lrflip, ulflip=ulflip)\n            image_1_list.append(_image_1)\n            image_2_list.append(_image_2)\n            scale_1 = _scale_1\n            scale_2 = _scale_2\n\n    image_1 = torch.cat(image_1_list, dim=0)\n    image_2 = torch.cat(image_2_list, dim=0)\n\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1).to(device),\n                  \"image1\": K.color.rgb_to_grayscale(image_2).to(device),\n                  \"mask0\": K.utils.image_to_tensor(mask_1).to(device),\n                  \"mask1\": K.utils.image_to_tensor(mask_2).to(device)}\n\n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n\n    mkpts1 = correspondences[\"keypoints0\"].cpu().numpy()\n    mkpts2 = correspondences[\"keypoints1\"].cpu().numpy()\n    confidence = correspondences[\"confidence\"].cpu().numpy()\n    batch_indexes = correspondences[\"batch_indexes\"].cpu().numpy()\n\n    mkpts1_all = []\n    mkpts2_all = []\n    confidence_all = []\n    batch_id = 0\n    for lrflip in [False, True]:\n        for ulflip in [False]:\n\n            idx = batch_indexes == batch_id\n            _mkpts1 = mkpts1[idx]\n            _mkpts2 = mkpts2[idx]\n            _confidence = confidence[idx]\n            if lrflip:\n                for i in range(len(_mkpts1)):\n                    _mkpts1[i][0] = image_1.shape[3] - _mkpts1[i][0]\n                    _mkpts2[i][0] = image_2.shape[3] - _mkpts2[i][0]\n            if ulflip:\n                for i in range(len(_mkpts1)):\n                    _mkpts1[i][1] = image_1.shape[2] - _mkpts1[i][1]\n                    _mkpts2[i][1] = image_2.shape[2] - _mkpts2[i][1]\n\n            mkpts1_all.append(_mkpts1)\n            mkpts2_all.append(_mkpts2)\n            confidence_all.append(_confidence)\n\n            batch_id += 1\n\n    mkpts1 = np.concatenate(mkpts1_all, axis=0)\n    mkpts2 = np.concatenate(mkpts2_all, axis=0)\n    confidence = np.concatenate(confidence_all, axis=0)\n\n    mkpts1 /= scale_1\n    mkpts2 /= scale_2\n\n    return mkpts1, mkpts2, confidence","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:43:46.224595Z","iopub.execute_input":"2022-06-27T20:43:46.224988Z","iopub.status.idle":"2022-06-27T20:43:46.251117Z","shell.execute_reply.started":"2022-06-27T20:43:46.224949Z","shell.execute_reply":"2022-06-27T20:43:46.25019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature matching (SE2-LoFTR model)\n\nA case for using rotation invariant features in state of the art feature matchers\n\nhttps://arxiv.org/abs/2204.10144\n\nhttps://github.com/georg-bn/se2-loftr","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/imc2022-submodules/se2-loftr\")\nfrom copy import deepcopy\nfrom src.loftr import LoFTR, default_cfg\n\ndef load_model_SE2LoFTR(ckpt_path, device):\n\n    _default_cfg = deepcopy(default_cfg)\n    _default_cfg['coarse']['temp_bug_fix'] = True\n    _default_cfg['backbone_type'] = 'E2ResNetFPN'\n    _default_cfg['resnetfpn']['nbr_rotations'] = 4\n    _default_cfg['resnetfpn']['e2_same_nbr_filters'] = False\n\n    model = LoFTR(config=_default_cfg)\n    model.load_state_dict(torch.load(ckpt_path, map_location=device)['state_dict'])\n    model = model.to(device).eval()\n\n    return model\n\ndef matching_se2loftr(input_image_1, input_image_2, SE2LoFTR_param):\n\n    img_size = SE2LoFTR_param[\"img_size\"]\n    matcher = SE2LoFTR_param[\"model\"]\n    device = SE2LoFTR_param[\"device\"]\n\n    image_1_list = []\n    image_2_list = []\n    scale_1 = 1\n    scale_2 = 1\n    for lrflip in [False, True]:\n        for ulflip in [False]:\n            _image_1, _scale_1 = convert_LoFTR_image(input_image_1, img_size=img_size, lrflip=lrflip, ulflip=ulflip)\n            _image_2, _scale_2 = convert_LoFTR_image(input_image_2, img_size=img_size, lrflip=lrflip, ulflip=ulflip)\n            image_1_list.append(_image_1)\n            image_2_list.append(_image_2)\n            scale_1 = _scale_1\n            scale_2 = _scale_2\n\n    image_1 = torch.cat(image_1_list, dim=0)\n    image_2 = torch.cat(image_2_list, dim=0)\n\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1).to(device),\n                  \"image1\": K.color.rgb_to_grayscale(image_2).to(device)}\n\n    with torch.no_grad():\n        matcher(input_dict)\n\n    mkpts1_all = []\n    mkpts2_all = []\n    confidence_all = []\n    batch_id = 0\n    for lrflip in [False, True]:\n        for ulflip in [False]:\n\n            # https://github.com/zju3dv/LoFTR/issues/58\n            _mkpts1 = input_dict[\"mkpts0_f\"][input_dict['m_bids'] == batch_id].cpu().numpy()\n            _mkpts2 = input_dict[\"mkpts1_f\"][input_dict['m_bids'] == batch_id].cpu().numpy()\n            _confidence = input_dict[\"mconf\"][input_dict['m_bids'] == batch_id].cpu().numpy()\n            if lrflip:\n                for i in range(len(_mkpts1)):\n                    _mkpts1[i][0] = image_1.shape[3] - _mkpts1[i][0]\n                    _mkpts2[i][0] = image_2.shape[3] - _mkpts2[i][0]\n            if ulflip:\n                for i in range(len(_mkpts1)):\n                    _mkpts1[i][1] = image_1.shape[2] - _mkpts1[i][1]\n                    _mkpts2[i][1] = image_2.shape[2] - _mkpts2[i][1]\n\n            mkpts1_all.append(_mkpts1)\n            mkpts2_all.append(_mkpts2)\n            confidence_all.append(_confidence)\n\n            batch_id += 1\n\n    mkpts1 = np.concatenate(mkpts1_all, axis=0)\n    mkpts2 = np.concatenate(mkpts2_all, axis=0)\n    confidence = np.concatenate(confidence_all, axis=0)\n\n    mkpts1 /= scale_1\n    mkpts2 /= scale_2\n\n    return mkpts1, mkpts2, confidence","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:43:46.252909Z","iopub.execute_input":"2022-06-27T20:43:46.253338Z","iopub.status.idle":"2022-06-27T20:43:46.729334Z","shell.execute_reply.started":"2022-06-27T20:43:46.253297Z","shell.execute_reply":"2022-06-27T20:43:46.727936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature matching (SGMNet model)\n\nLearning to Match Features with Seeded Graph Matching Network\n\nhttps://arxiv.org/abs/2108.08771\n\nhttps://github.com/vdvchen/SGMNet","metadata":{}},{"cell_type":"code","source":"import sys\nimport numpy as np\nsys.path.append(\"../input/imc2022-submodules/SGMNet\")\nfrom components import load_component\n\ndef load_model_SGMNet(ckpt_path):\n\n    extractor_cfg = {\n        \"name\": \"root\",\n        \"num_kpt\": 4000,\n        \"resize\": [-1],\n        \"det_th\": 0.00001,\n    }\n    extractor = load_component(\"extractor\", extractor_cfg[\"name\"], extractor_cfg)\n\n    matcher_cfg = {\n        \"name\": \"SGM\",\n        \"model_dir\": ckpt_path,\n        \"seed_top_k\": [256,256],\n        \"seed_radius_coe\": 0.01,\n        \"net_channels\": 128,\n        \"layer_num\": 9,\n        \"head\": 4,\n        \"seedlayer\": [0,6],\n        \"use_mc_seeding\": True,\n        \"use_score_encoding\": False,\n        \"conf_bar\": [1.11,0.1],\n        \"sink_iter\": [10,100],\n        \"detach_iter\": 1000000,\n        \"p_th\": 0.2,\n    }\n\n    SGM_matcher = load_component(\"matcher\", matcher_cfg[\"name\"], matcher_cfg)\n\n    model = {\n        \"extractor\": extractor,\n        \"matcher\": SGM_matcher,\n    }\n\n    return model\n\ndef extract_SGM(image1, image2, SGM_param):\n    extractor = SGM_param[\"model\"][\"extractor\"]\n\n    size1, size2 = np.flip(np.asarray(image1.shape[:2])), np.flip(np.asarray(image2.shape[:2]))\n    kpt1, desc1 = extractor.run(cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY))\n    kpt2, desc2 = extractor.run(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY))\n    test_data = {\"x1\":kpt1, \"x2\":kpt2, \"desc1\":desc1, \"desc2\":desc2, \"size1\":size1, \"size2\":size2}\n\n    return test_data\n\ndef matching_SGM(test_data, SGM_param):\n    matcher = SGM_param[\"model\"][\"matcher\"]\n\n    mkpts0, mkpts1 = matcher.run(test_data)\n\n    return mkpts0, mkpts1","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:43:46.731069Z","iopub.execute_input":"2022-06-27T20:43:46.732704Z","iopub.status.idle":"2022-06-27T20:43:46.854456Z","shell.execute_reply.started":"2022-06-27T20:43:46.732656Z","shell.execute_reply":"2022-06-27T20:43:46.853275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Postprocess","metadata":{}},{"cell_type":"markdown","source":"## NMS-based outlier pre-filtering\n\nEfficient adaptive non-maximal suppression algorithms for homogeneous spatial keypoint distribution\n\nhttps://www.researchgate.net/publication/323388062_Efficient_adaptive_non-maximal_suppression_algorithms_for_homogeneous_spatial_keypoint_distribution\n\nhttps://github.com/BAILOOL/ANMS-Codes","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append(\"../input/imc2022-submodules/ANMS-Codes/Python\")\nfrom ssc import ssc\n\ndef nms_core(w, h, mkpts, confidence, num_ret_points, tolerance=0.1):\n\n    keypoints = []\n    for pt1, conf in zip(mkpts, confidence):\n        keypoints.append(\n            cv2.KeyPoint(x=pt1[1], y=pt1[0], size=1, angle=0, response=conf, octave=0, class_id=0)\n        )\n\n    # keypoints should be sorted by strength in descending order\n    # before feeding to SSC to work correctly\n    keypoints = sorted(keypoints, key=lambda x: x.response, reverse=True)\n\n    selected_keypoints = ssc(keypoints, num_ret_points, tolerance, h, w)\n    if len(selected_keypoints) > num_ret_points:\n        selected_keypoints = np.random.choice(selected_keypoints, num_ret_points, replace=False)\n\n    mkpts_list = mkpts.tolist()\n    index_list = []\n    for kp in selected_keypoints:\n        x, y = kp.pt\n        index_list.append(mkpts_list.index([y, x]))\n\n    return index_list","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:43:46.856043Z","iopub.execute_input":"2022-06-27T20:43:46.856636Z","iopub.status.idle":"2022-06-27T20:43:46.877229Z","shell.execute_reply.started":"2022-06-27T20:43:46.856595Z","shell.execute_reply":"2022-06-27T20:43:46.876111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nms(image_1_width, image_1_height, mkpts1, mkpts2, confidence, num_ret_points):\n\n    if num_ret_points < len(mkpts1):\n        selected_index = nms_core(image_1_width, image_1_height, mkpts1, confidence, num_ret_points)\n        mkpts1 = mkpts1[selected_index]\n        mkpts2 = mkpts2[selected_index]\n        confidence = confidence[selected_index]\n\n    return mkpts1, mkpts2, confidence\n\ndef mkpts_filtering(mkpts1, mkpts2, confidence, param, image_1_width, image_1_height):\n\n    # confidence_thr\n    idx = confidence > param[\"conf_min_thr\"]\n    mkpts1 = mkpts1[idx]\n    mkpts2 = mkpts2[idx]\n    confidence = confidence[idx]\n\n    # non-maximum suppression\n    mkpts1, mkpts2, confidence = nms(\n        image_1_width, image_1_height,\n        mkpts1, mkpts2, confidence,\n        param[\"nms_ret_points\"])\n\n    return mkpts1, mkpts2, confidence\n\n# weighted random sampling\ndef points_sample(dense_matches_1, dense_matches_2, dense_confidence, num = 2000, relative_confidence_threshold = 0.0):\n    matches_1 = dense_matches_1\n    matches_2 = dense_matches_2\n    confidence = dense_confidence\n    relative_confidence = confidence/confidence.max()\n    matches_1, matches_2, confidence = (\n        matches_1[relative_confidence > relative_confidence_threshold],\n        matches_2[relative_confidence > relative_confidence_threshold],\n        confidence[relative_confidence > relative_confidence_threshold],\n    )\n    good_samples = np.random.choice(\n        np.arange(len(matches_1)),\n        size=min(num, len(confidence)),\n        replace=False,\n        p=confidence/np.sum(confidence),\n    )\n    return matches_1[good_samples], matches_2[good_samples], confidence[good_samples]\n\ndef matching_postprocess(args, params):\n\n    LoFTR_param = params[\"LoFTR\"]\n    SE2LoFTR_param = params[\"SE2LoFTR\"]\n    DKM_param = params[\"DKM\"]\n    findFMat_param = params[\"findFMat\"]\n\n    image_1_width = args[\"image_1_width\"]\n    image_1_height = args[\"image_1_height\"]\n    DKM_result = args[\"DKM\"]\n    LoFTR_result = args[\"LoFTR\"]\n    LoFTR_NC_result = args[\"LoFTR_NC\"]\n    SE2LoFTR_result = args[\"SE2LoFTR\"]\n    SGM_result = args[\"SGMNet\"]\n\n    # LoFTR concat\n    mkpts1_LoFTR = np.concatenate((LoFTR_result[\"mkpts1\"], LoFTR_NC_result[\"mkpts1\"]), axis=0)\n    mkpts2_LoFTR = np.concatenate((LoFTR_result[\"mkpts2\"], LoFTR_NC_result[\"mkpts2\"]), axis=0)\n    confidence_LoFTR = np.concatenate((LoFTR_result[\"confidence\"], LoFTR_NC_result[\"confidence\"]), axis=0)\n\n    #------------------------------------------------\n    # correspondence points filtering\n\n    # LoFTR\n    mkpts1_LoFTR, mkpts2_LoFTR, confidence_LoFTR = mkpts_filtering(\n        mkpts1_LoFTR, mkpts2_LoFTR, confidence_LoFTR,\n        LoFTR_param,\n        image_1_width, image_1_height)\n\n    # SE2-LoFTR\n    mkpts1_SE2LoFTR, mkpts2_SE2LoFTR, confidence_SE2LoFTR = mkpts_filtering(\n        SE2LoFTR_result[\"mkpts1\"],\n        SE2LoFTR_result[\"mkpts2\"],\n        SE2LoFTR_result[\"confidence\"],\n        SE2LoFTR_param,\n        image_1_width, image_1_height)\n\n    # correspondence points filtering\n    #------------------------------------------------\n\n    # concat\n    mkpts1 = np.concatenate((mkpts1_LoFTR, mkpts1_SE2LoFTR), axis=0)\n    mkpts2 = np.concatenate((mkpts2_LoFTR, mkpts2_SE2LoFTR), axis=0)\n    confidence = np.concatenate((confidence_LoFTR, confidence_SE2LoFTR), axis=0)\n\n    if SGM_result is not None:\n        # SGMNet boosting\n        mkpts1 = np.concatenate((mkpts1, SGM_result[\"mkpts1\"]), axis=0)\n        mkpts2 = np.concatenate((mkpts2, SGM_result[\"mkpts2\"]), axis=0)\n    else:\n        # weighted random sampling\n        mkpts1, mkpts2, _ = points_sample(\n            mkpts1, mkpts2, confidence, num = findFMat_param[\"input_num\"])\n\n    # When only a few percent of the usual number of correspondence points are found.\n    # Judging the scene to be extremely difficult to find sufficient correspondence points\n    # with the usual approach, search for correspondence points with DKM alone, not with an ensemble.\n    if len(mkpts1) < DKM_param[\"mkpts_num_thr\"]:\n        mkpts1, mkpts2, confidence = nms(\n            image_1_width, image_1_height,\n            DKM_result[\"mkpts1\"],\n            DKM_result[\"mkpts2\"],\n            DKM_result[\"confidence\"],\n            DKM_param[\"nms_ret_points\"])\n\n    return mkpts1, mkpts2","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:43:46.879007Z","iopub.execute_input":"2022-06-27T20:43:46.879402Z","iopub.status.idle":"2022-06-27T20:43:46.906232Z","shell.execute_reply.started":"2022-06-27T20:43:46.879365Z","shell.execute_reply":"2022-06-27T20:43:46.905125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline implementation","metadata":{}},{"cell_type":"code","source":"def preprocess(_args, params):\n\n    SGM_param = params[\"SGMNet\"]\n\n    #----------------------\n    # args_1\n    # Load image\n    if _args[\"args_1\"] is None:\n        result_1 = None\n    else:\n        args = _args[\"args_1\"]\n        image_dir = args[\"image_dir\"]\n        image_1_id = args[\"image_1_id\"]\n        image_2_id = args[\"image_2_id\"]\n        ext = args[\"ext\"]\n\n        # Load image\n        image_1 = cv2.imread(os.path.join(image_dir, f\"{image_1_id}.{ext}\"))\n        image_2 = cv2.imread(os.path.join(image_dir, f\"{image_2_id}.{ext}\"))\n        image_1_height, image_1_width = image_1.shape[:2]\n\n        # generate mask (bottom and through) before crop\n        mask_1, through_mask_1 = bottom_mask(image_1)\n        mask_2, through_mask_2 = bottom_mask(image_2)\n\n        # set result\n        result_1 = args\n        result_1[\"image_1\"] = image_1\n        result_1[\"image_2\"] = image_2\n        result_1[\"image_1_width\"] = image_1_width\n        result_1[\"image_1_height\"] = image_1_height\n        result_1[\"mask_1\"] = mask_1\n        result_1[\"mask_2\"] = mask_2\n        result_1[\"through_mask_1\"] = through_mask_1\n        result_1[\"through_mask_2\"] = through_mask_2\n    # args_1\n    #----------------------\n\n    #----------------------\n    # args_2\n    # pre-processing\n    if _args[\"args_2\"] is None:\n        result_2 = None\n    else:\n        args = _args[\"args_2\"]\n        image_1 = args[\"image_1\"]\n        image_2 = args[\"image_2\"]\n        through_mask_1 = args[\"through_mask_1\"]\n        through_mask_2 = args[\"through_mask_2\"]\n        img_pair_rect = args[\"img_pair_rect\"]\n\n        # crop image\n        image_1_crop, rect_1 = crop_image(image_1, img_pair_rect[\"image1\"])\n        image_2_crop, rect_2 = crop_image(image_2, img_pair_rect[\"image2\"])\n        # crop through mask\n        through_mask_1_crop, _ = crop_image(through_mask_1, img_pair_rect[\"image1\"])\n        through_mask_2_crop, _ = crop_image(through_mask_2, img_pair_rect[\"image2\"])\n\n        # SGM pre-processing\n        SGM_keypoints = extract_SGM(image_1_crop, image_2_crop, SGM_param)\n\n        # set result\n        result_2 = args\n        result_2[\"image_1_crop\"] = image_1_crop\n        result_2[\"image_2_crop\"] = image_2_crop\n        result_2[\"rect_1\"] = rect_1\n        result_2[\"rect_2\"] = rect_2\n        result_2[\"through_mask_1_crop\"] = through_mask_1_crop\n        result_2[\"through_mask_2_crop\"] = through_mask_2_crop\n        result_2[\"SGM_keypoints\"] = SGM_keypoints\n    # args_2\n    #----------------------\n\n    result = {\n        \"args_1\": result_1,\n        \"args_2\": result_2,\n    }\n\n    return result\n\ndef matching(_args, params):\n\n    LoFTR_param = params[\"LoFTR\"]\n    SE2LoFTR_param = params[\"SE2LoFTR\"]\n    DKM_param = params[\"DKM\"]\n    SGM_param = params[\"SGMNet\"]\n\n    #----------------------\n    # args_1\n    # Making ROI with DKM\n    if _args[\"args_1\"] is None:\n        result_1 = None\n    else:\n        args = _args[\"args_1\"]\n        image_1 = args[\"image_1\"]\n        image_2 = args[\"image_2\"]\n        mask_1 = args[\"mask_1\"]\n        mask_2 = args[\"mask_2\"]\n\n        # DKM based ROI generation\n        img_pair_rect, mkpts1_DKM, mkpts2_DKM, confidence_DKM = matching_DKM(image_1, image_2, DKM_param)\n        DKM_result = {\n            \"mkpts1\": mkpts1_DKM,\n            \"mkpts2\": mkpts2_DKM,\n            \"confidence\": confidence_DKM,\n        }\n\n        # set result\n        result_1 = args\n        result_1[\"DKM\"] = DKM_result\n        result_1[\"img_pair_rect\"] = img_pair_rect\n    # args_1\n    #----------------------\n\n    #----------------------\n    # args2\n    # Matching image pair\n\n    if _args[\"args_2\"] is None:\n        result_2 = None\n    else:\n        args = _args[\"args_2\"]\n        image_1 = args[\"image_1\"]\n        image_2 = args[\"image_2\"]\n        mask_1 = args[\"mask_1\"]\n        mask_2 = args[\"mask_2\"]\n        image_1_crop = args[\"image_1_crop\"]\n        image_2_crop = args[\"image_2_crop\"]\n        through_mask_1_crop = args[\"through_mask_1_crop\"]\n        through_mask_2_crop = args[\"through_mask_2_crop\"]\n        rect_1 = args[\"rect_1\"]\n        rect_2 = args[\"rect_2\"]\n        SGM_keypoints = args[\"SGM_keypoints\"]\n\n        # LoFTR\n        mkpts1_LoFTR, mkpts2_LoFTR, confidence_LoFTR = matching_loftr(image_1_crop, image_2_crop, through_mask_1_crop, through_mask_2_crop, LoFTR_param)    # crop back\n        mkpts1_LoFTR[:, 0] = mkpts1_LoFTR[:, 0] + rect_1[0]\n        mkpts1_LoFTR[:, 1] = mkpts1_LoFTR[:, 1] + rect_1[1]\n        mkpts2_LoFTR[:, 0] = mkpts2_LoFTR[:, 0] + rect_2[0]\n        mkpts2_LoFTR[:, 1] = mkpts2_LoFTR[:, 1] + rect_2[1]\n        LoFTR_result = {\n            \"mkpts1\": mkpts1_LoFTR,\n            \"mkpts2\": mkpts2_LoFTR,\n            \"confidence\": confidence_LoFTR,\n        }\n\n        # LoFTR no crop\n        mkpts1_LoFTR_NC, mkpts2_LoFTR_NC, confidence_LoFTR_NC = matching_loftr(image_1, image_2, mask_1, mask_2, LoFTR_param)\n        LoFTR_NC_result = {\n            \"mkpts1\": mkpts1_LoFTR_NC,\n            \"mkpts2\": mkpts2_LoFTR_NC,\n            \"confidence\": confidence_LoFTR_NC,\n        }\n\n        # SE2-LoFTR\n        mkpts1_SE2LoFTR, mkpts2_SE2LoFTR, confidence_SE2LoFTR = matching_se2loftr(image_1_crop, image_2_crop, SE2LoFTR_param)\n        # crop back\n        mkpts1_SE2LoFTR[:, 0] = mkpts1_SE2LoFTR[:, 0] + rect_1[0]\n        mkpts1_SE2LoFTR[:, 1] = mkpts1_SE2LoFTR[:, 1] + rect_1[1]\n        mkpts2_SE2LoFTR[:, 0] = mkpts2_SE2LoFTR[:, 0] + rect_2[0]\n        mkpts2_SE2LoFTR[:, 1] = mkpts2_SE2LoFTR[:, 1] + rect_2[1]\n        SE2LoFTR_result = {\n            \"mkpts1\": mkpts1_SE2LoFTR,\n            \"mkpts2\": mkpts2_SE2LoFTR,\n            \"confidence\": confidence_SE2LoFTR,\n        }\n\n        # When only a few dozen percent of the usual number of correspondence points are found.\n        # Boosting keypoints by incorporating methods that differ in tendency from those used\n        # in the previous stage of the ensemble. We used SGMNet as an additional ensemble.\n        if (len(mkpts1_LoFTR)+len(mkpts1_SE2LoFTR)) < SGM_param[\"mkpts_num_thr\"]:\n            mkpts1_SGM, mkpts2_SGM = matching_SGM(SGM_keypoints, SGM_param)\n            # crop back\n            mkpts1_SGM[:, 0] = mkpts1_SGM[:, 0] + rect_1[0]\n            mkpts1_SGM[:, 1] = mkpts1_SGM[:, 1] + rect_1[1]\n            mkpts2_SGM[:, 0] = mkpts2_SGM[:, 0] + rect_2[0]\n            mkpts2_SGM[:, 1] = mkpts2_SGM[:, 1] + rect_2[1]\n            SGM_result = {\n                \"mkpts1\": mkpts1_SGM,\n                \"mkpts2\": mkpts2_SGM,\n            }\n        else:\n            SGM_result = None\n\n        result_2 = args\n        result_2[\"LoFTR\"] = LoFTR_result\n        result_2[\"LoFTR_NC\"] = LoFTR_NC_result\n        result_2[\"SE2LoFTR\"] = SE2LoFTR_result\n        result_2[\"SGMNet\"] = SGM_result\n    # args2\n    #----------------------\n\n    result = {\n        \"args_1\": result_1,\n        \"args_2\": result_2,\n    }\n\n    return result\n\ndef postprocess(_args, params):\n\n    #----------------------\n    # args_1\n    if _args[\"args_1\"] is None:\n        result_1 = None\n    else:\n        result_1 = _args[\"args_1\"]\n    # args1\n    #----------------------\n\n    #----------------------\n    # args_2\n    # NMS and RANSAC\n    if _args[\"args_2\"] is None:\n        result_2 = None\n    else:\n        args = _args[\"args_2\"]\n\n        sample_id = args[\"sample_id\"]\n        # postprocess\n        mkpts1, mkpts2 = matching_postprocess(args, params)\n\n        # RANSAC\n        findFMat_param = params[\"findFMat\"]\n        if len(mkpts1) > 7:\n            F, inliers = cv2.findFundamentalMat(\n                mkpts1, mkpts2, cv2.USAC_MAGSAC,\n                findFMat_param[\"ransacReprojThreshold\"],\n                findFMat_param[\"confidence\"],\n                findFMat_param[\"maxIters\"])\n            if F.shape != (3, 3):\n                F = np.zeros((3, 3))\n        else:\n            F = np.zeros((3, 3))\n            inliers = None\n\n        result_2 = {\n            \"sample_id\": sample_id,\n            \"F\": FlattenMatrix(F),\n            \"inliers\": inliers,\n            \"mkpts1\": mkpts1,\n            \"mkpts2\": mkpts2,\n        }\n    # args_2\n    #----------------------\n\n    result = {\n        \"args_1\": result_1,\n        \"args_2\": result_2,\n    }\n\n    return result\n\nimport threading\nimport queue\n\ndef wrap_func_for_mt(func, params):\n    def wrap_func(queue_input, queue_output):\n        while True:\n            input = queue_input.get()\n            if input is None:\n                queue_output.put(None)\n                continue\n\n            result = func(input, params)\n\n            queue_output.put(result)\n\n    return wrap_func\n\ndef loop_proc(queues_input, queues_output, inputs):\n    for queue_input, input in zip(queues_input, inputs):\n        queue_input.put(input)\n\n    outputs = []\n    for queue_output in queues_output:\n        output = queue_output.get()\n        outputs.append(output)\n\n    return outputs\n\ndef prepare_multithreading(params):\n\n    # funcs to proc in pipeline\n    func_params = [\n        (preprocess, (params)),\n        (matching, (params)),\n        (postprocess, (params)),\n    ]\n    wrap_funcs = list(map(lambda func_param: wrap_func_for_mt(func_param[0], func_param[1]), func_params))\n\n    # prepare queues\n    queues_input = [queue.Queue() for _ in range(len(wrap_funcs))]\n    queues_output = [queue.Queue() for _ in range(len(wrap_funcs))]\n\n    # create Threads\n    threads = []\n    for wrap_func, queue_input, queue_output in zip(wrap_funcs, queues_input, queues_output):\n        t = threading.Thread(target=wrap_func, args=(queue_input, queue_output), daemon=True)\n        threads.append(t)\n\n    for t in threads:\n        t.start()\n\n    return queues_input, queues_output, len(wrap_funcs)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:43:46.911528Z","iopub.execute_input":"2022-06-27T20:43:46.913093Z","iopub.status.idle":"2022-06-27T20:43:46.958316Z","shell.execute_reply.started":"2022-06-27T20:43:46.913049Z","shell.execute_reply":"2022-06-27T20:43:46.957069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport kornia as K\nimport kornia.feature as KF\nimport matplotlib.pyplot as plt\nimport os\nimport torch\n\nfrom kornia_moons.feature import draw_LAF_matches\n\ndef draw_matching(image_dir, image_0_id, image_1_id, ext, mkpts0, mkpts1, inliers):\n\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\n    image_1 = cv2.imread(os.path.join(image_dir, f\"{image_0_id}.{ext}\"))\n    image_2 = cv2.imread(os.path.join(image_dir, f\"{image_1_id}.{ext}\"))\n    image_1 = K.image_to_tensor(image_1, False).float() /255.\n    image_2 = K.image_to_tensor(image_2, False).float() /255.\n    image_1 = K.color.bgr_to_rgb(image_1).to(device)\n    image_2 = K.color.bgr_to_rgb(image_2).to(device)\n\n    draw_LAF_matches(\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n        torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n        K.tensor_to_image(image_1),\n        K.tensor_to_image(image_2),\n        inliers,\n        draw_dict={'inlier_color': (0.2, 1, 0.2),\n                'tentative_color': None,\n                'feature_color': (0.2, 0.5, 1),\n                'vertical': False\n                },\n    )\n    return","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:43:46.960318Z","iopub.execute_input":"2022-06-27T20:43:46.960784Z","iopub.status.idle":"2022-06-27T20:43:46.980656Z","shell.execute_reply.started":"2022-06-27T20:43:46.960743Z","shell.execute_reply":"2022-06-27T20:43:46.979571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"import random\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef FlattenMatrix(M, num_digits=8):\n    \"\"\"Convenience function to write CSV files.\"\"\"\n    return \" \".join([f\"{v:.{num_digits}e}\" for v in M.flatten()])","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:43:46.98271Z","iopub.execute_input":"2022-06-27T20:43:46.983242Z","iopub.status.idle":"2022-06-27T20:43:46.996171Z","shell.execute_reply.started":"2022-06-27T20:43:46.983202Z","shell.execute_reply":"2022-06-27T20:43:46.994882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"input_dir = \"../input/image-matching-challenge-2022\"\nweight_dir = \"../input/imc2022-pretrained-weights\"\nworking_dir = \"/root/.cache/torch/hub\"\noutput_csv = \"submission.csv\"\n\nseed_everything(0)\ndevice = torch.device(\"cuda\")\next = \"png\"\n\n#--------------------------------------------------------------------------------\n# parameters\nDKM_param = {\n    \"model\": load_model_DKM(os.path.join(weight_dir, \"dkm\"), working_dir),\n    \"scale\": 1.5,\n    \"sample_num\": 10000,\n    \"nms_ret_points\": 3000,\n    \"mkpts_num_thr\": 300,\n}\nLoFTR_param = {\n    \"device\": device,\n    \"model\": load_model_LoFTR(os.path.join(weight_dir, \"kornia-loftr\", \"loftr_outdoor.ckpt\"), device),\n    \"img_size\": 840,\n    \"nms_ret_points\": 2000,\n    \"conf_min_thr\": 0.4,\n}\nSE2LoFTR_param = {\n    \"device\": device,\n    \"model\": load_model_SE2LoFTR(os.path.join(weight_dir, \"se2loftr\", \"4rot-big.ckpt\"), device),\n    \"img_size\": 840,\n    \"nms_ret_points\": 2000,\n    \"conf_min_thr\": 0.4,\n}\nSGM_param = {\n    \"model\": load_model_SGMNet(os.path.join(weight_dir, \"sgmnet\", \"sgm\", \"root\")),\n    \"mkpts_num_thr\": 5000,\n}\nfindFMat_param = {\n    \"input_num\": 3000,\n    \"ransacReprojThreshold\": 0.15,\n    \"confidence\": 0.9999,\n    \"maxIters\": 20000,\n}\n\nparams = {\n    \"LoFTR\": LoFTR_param,\n    \"SE2LoFTR\": SE2LoFTR_param,\n    \"DKM\": DKM_param,\n    \"SGMNet\": SGM_param,\n    \"findFMat\": findFMat_param,\n}\n# parameters\n#--------------------------------------------------------------------------------\n\ndf = pd.read_csv(os.path.join(input_dir, \"test.csv\"))\nsample_ids = df[\"sample_id\"].values\nbatch_ids = df[\"batch_id\"].values\nimage_1_ids = df[\"image_1_id\"].values\nimage_2_ids = df[\"image_2_id\"].values\n\n#-----------------\n# pipeline process\nqueues_input, queues_output, len_wrap_funcs = prepare_multithreading(params)\n\nsample_id_list = []\nfund_matrix_list = []\nidx = 0\nwhile len(sample_id_list) < len(sample_ids):\n\n    if idx >= len(sample_ids):\n        args_1 = None\n    else:\n        args_1 = {\n            \"sample_id\": sample_ids[idx],\n            \"image_dir\": os.path.join(input_dir, \"test_images\", batch_ids[idx]),\n            \"image_1_id\": image_1_ids[idx],\n            \"image_2_id\": image_2_ids[idx],\n            \"ext\": ext,\n        }\n\n    # start pipeline\n    if idx == 0:\n        args = {\n            \"args_1\": args_1,\n            \"args_2\": None,\n        }\n        init_inputs = [args] + [None]*(len_wrap_funcs - 1)  # [[], None, None, ...]\n        inputs = init_inputs\n    else:\n        if outputs[-1] is None:\n            args_2 = None\n        else:\n            args_2 = outputs[-1][\"args_1\"]\n\n        args = {\n            \"args_1\": args_1,\n            \"args_2\": args_2,\n        }\n        inputs = [args] + outputs[:-1]\n\n    outputs = loop_proc(queues_input, queues_output, inputs)\n    _result = outputs[-1]\n    if _result is None:\n        result = None\n    else:\n        result = _result[\"args_2\"]\n\n    if result is not None:\n        sample_id_list.append(result[\"sample_id\"])\n        fund_matrix_list.append(result[\"F\"])\n\n        # debug\n        if len(sample_id_list) <= 3:\n            print(f\"{len(sample_id_list)} / {len(sample_ids)} : {result['sample_id']}\")\n            # draw mkpts\n            batch_id = result[\"sample_id\"].split(\";\")[1]\n            image_pair = result[\"sample_id\"].split(\";\")[2]\n            image_1_id = image_pair.split(\"-\")[0]\n            image_2_id = image_pair.split(\"-\")[1]\n            draw_matching(\n                os.path.join(input_dir, \"test_images\", batch_id),\n                image_1_id, image_2_id, ext,\n                result[\"mkpts1\"], result[\"mkpts2\"], result[\"inliers\"])\n\n    idx = idx + 1\n# pipeline process\n#-----------------\n\ndf = pd.DataFrame({\"sample_id\":sample_id_list, \"fundamental_matrix\":fund_matrix_list})\ndf.to_csv(output_csv, index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T20:45:35.337839Z","iopub.execute_input":"2022-06-27T20:45:35.338311Z","iopub.status.idle":"2022-06-27T20:47:10.529818Z","shell.execute_reply.started":"2022-06-27T20:45:35.338275Z","shell.execute_reply":"2022-06-27T20:47:10.528625Z"},"trusted":true},"execution_count":null,"outputs":[]}]}