{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This notebook computes validation score on part of training set","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/\n!pip install kornia --no-index --find-links=file:///kaggle/input/imc2022-dependencies/pip/kornia/ --upgrade \n!pip install kornia_moons --no-index --find-links=file:///kaggle/input/imc2022-dependencies/pip/kornia_moons/ --no-deps  --upgrade \nprint('Done!')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-12T16:18:48.248898Z","iopub.execute_input":"2022-04-12T16:18:48.249498Z","iopub.status.idle":"2022-04-12T16:19:00.36566Z","shell.execute_reply.started":"2022-04-12T16:18:48.249403Z","shell.execute_reply":"2022-04-12T16:19:00.363981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install ../input/loftrutils/einops-0.4.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:19:00.36761Z","iopub.execute_input":"2022-04-12T16:19:00.367941Z","iopub.status.idle":"2022-04-12T16:19:27.738851Z","shell.execute_reply.started":"2022-04-12T16:19:00.367864Z","shell.execute_reply":"2022-04-12T16:19:27.737828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r ../input/imutils/imutils-0.5.3/ /\n!pip install /imutils-0.5.3/","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:19:27.741984Z","iopub.execute_input":"2022-04-12T16:19:27.742278Z","iopub.status.idle":"2022-04-12T16:19:57.593264Z","shell.execute_reply.started":"2022-04-12T16:19:27.742242Z","shell.execute_reply":"2022-04-12T16:19:57.592473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/loftrutils/LoFTR-master/LoFTR-master/')\nsys.path.append('../input/imc-utils/')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:19:57.59572Z","iopub.execute_input":"2022-04-12T16:19:57.596016Z","iopub.status.idle":"2022-04-12T16:19:57.601287Z","shell.execute_reply.started":"2022-04-12T16:19:57.595976Z","shell.execute_reply":"2022-04-12T16:19:57.60038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom collections import namedtuple\nimport cv2\nimport kornia as K\nimport kornia.feature as KF\nfrom kornia.feature.loftr import LoFTR\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport glob\nimport random\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nimport matplotlib.cm as cm\n\nfrom kornia_moons.feature import *\n\n# from src.loftr import LoFTR, default_cfg\nfrom src.utils.plotting import make_matching_figure","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:19:57.602659Z","iopub.execute_input":"2022-04-12T16:19:57.602922Z","iopub.status.idle":"2022-04-12T16:19:59.584525Z","shell.execute_reply.started":"2022-04-12T16:19:57.60289Z","shell.execute_reply":"2022-04-12T16:19:59.583413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = 'cuda:0'\nWEIGHT_PATH = '../input/loftrutils/outdoor_ds.ckpt'\nLONGEST_EDGE = 1280","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:19:59.589552Z","iopub.execute_input":"2022-04-12T16:19:59.589888Z","iopub.status.idle":"2022-04-12T16:19:59.59793Z","shell.execute_reply.started":"2022-04-12T16:19:59.589846Z","shell.execute_reply":"2022-04-12T16:19:59.597229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matcher = LoFTR(pretrained=None)\nmatcher.load_state_dict(torch.load(WEIGHT_PATH)['state_dict'])\nmatcher = matcher.to(DEVICE)\nmatcher.eval()\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:19:59.603509Z","iopub.execute_input":"2022-04-12T16:19:59.605797Z","iopub.status.idle":"2022-04-12T16:20:03.770494Z","shell.execute_reply.started":"2022-04-12T16:19:59.605757Z","shell.execute_reply":"2022-04-12T16:20:03.769715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\nsrc = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:20:03.771904Z","iopub.execute_input":"2022-04-12T16:20:03.772359Z","iopub.status.idle":"2022-04-12T16:20:03.781546Z","shell.execute_reply.started":"2022-04-12T16:20:03.772319Z","shell.execute_reply":"2022-04-12T16:20:03.780842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_images(ims):\n    \n    fig, axes = plt.subplots(3, 3, figsize=(20,20))\n    \n    for idx, img in enumerate(ims):\n        i = idx % 3 \n        j = idx // 3 \n        image = Image.open(img)\n        image = image.resize((300,300))\n        axes[i, j].imshow(image)\n        axes[i, j].set_title(img.split('/')[-1])\n\n    plt.subplots_adjust(wspace=0, hspace=.2)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:20:03.782977Z","iopub.execute_input":"2022-04-12T16:20:03.783248Z","iopub.status.idle":"2022-04-12T16:20:03.790445Z","shell.execute_reply.started":"2022-04-12T16:20:03.783215Z","shell.execute_reply":"2022-04-12T16:20:03.789728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imutils\ndef resize_keep_ratio(img, longest_size=LONGEST_EDGE):\n    height, width = img.shape[:2]\n    if np.maximum(height, width) <= longest_size: # no need to resize\n        return img\n    \n    if height >= width:\n        resized_img = imutils.resize(img, height=longest_size)\n    else:\n        resized_img = imutils.resize(img, width=longest_size)\n    return resized_img\n\ndef load_torch_image(fname):\n    img = cv2.imread(fname)\n    img = resize_keep_ratio(img)\n    img = cv2.resize(img, (img.shape[1]//8*8, img.shape[0]//8*8))  # input size should be divisible by 8\n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:20:03.794176Z","iopub.execute_input":"2022-04-12T16:20:03.794534Z","iopub.status.idle":"2022-04-12T16:20:03.804443Z","shell.execute_reply.started":"2022-04-12T16:20:03.794505Z","shell.execute_reply":"2022-04-12T16:20:03.803707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match(img_path0, img_path1, matcher, device=DEVICE):\n    img0 = load_torch_image(img_path0)\n    img1 = load_torch_image(img_path1)\n        \n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n    \n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n        \n    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n        \n    return mkpts0, mkpts1\n        \ndef get_F_matrix(mkpts0, mkpts1):\n\n    # Make sure we do not trigger an exception here.\n    if len(mkpts0) > 8:\n        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n\n        assert F.shape == (3, 3), 'Malformed F?'\n    else:\n        F = np.zeros((3, 3))\n\n    return F","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:20:03.805502Z","iopub.execute_input":"2022-04-12T16:20:03.807733Z","iopub.status.idle":"2022-04-12T16:20:03.817939Z","shell.execute_reply.started":"2022-04-12T16:20:03.807702Z","shell.execute_reply":"2022-04-12T16:20:03.817236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match_and_draw(img_path0, img_path1, matcher, device=DEVICE, drop_outliers=False):\n    \n    img0 = load_torch_image(img_path0)\n    img1 = load_torch_image(img_path1)\n    \n    print(img0.shape, img1.shape)\n    \n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n    \n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n        \n    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n    \n    if len(mkpts0) > 8:\n        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n\n        assert F.shape == (3, 3), 'Malformed F?'\n    else:\n        F = np.zeros((3, 3))\n            \n    if drop_outliers:\n        print(len(mkpts0))\n        mkpts0 = mkpts0[inliers.reshape(-1) > 0]\n        mkpts1 = mkpts1[inliers.reshape(-1) > 0]\n        inliers = inliers[inliers > 0]\n    \n        print(len(mkpts0))\n    \n    draw_LAF_matches(\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n        torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n        K.tensor_to_image(img0),\n        K.tensor_to_image(img1),\n        inliers,\n        draw_dict={'inlier_color': (0.2, 1, 0.2),\n                   'tentative_color': None, \n                   'feature_color': (0.2, 0.5, 1), 'vertical': False})\n    \n    del correspondences, input_dict\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:20:03.819206Z","iopub.execute_input":"2022-04-12T16:20:03.819604Z","iopub.status.idle":"2022-04-12T16:20:03.834587Z","shell.execute_reply.started":"2022-04-12T16:20:03.819566Z","shell.execute_reply":"2022-04-12T16:20:03.83317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_matching(samples, files):\n    for i in range(samples.shape[1]):\n        path0 = files[samples[0][i]]\n        path1 = files[samples[1][i]]\n        print(f'Matching: {path0} to {path1}')\n        match_and_draw(path0, path1, matcher)\n        plt.show()\n        \ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:20:03.836119Z","iopub.execute_input":"2022-04-12T16:20:03.836491Z","iopub.status.idle":"2022-04-12T16:20:03.84643Z","shell.execute_reply.started":"2022-04-12T16:20:03.836457Z","shell.execute_reply":"2022-04-12T16:20:03.845737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load per-scene scaling factors.\n\nsrc = '../input/image-matching-challenge-2022/train'\n\nscaling_dict = {}\nwith open(f'{src}/scaling_factors.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        scaling_dict[row[0]] = float(row[1])\n\nprint(f'Scaling factors: {scaling_dict}')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:20:03.847755Z","iopub.execute_input":"2022-04-12T16:20:03.848767Z","iopub.status.idle":"2022-04-12T16:20:03.865181Z","shell.execute_reply.started":"2022-04-12T16:20:03.84873Z","shell.execute_reply":"2022-04-12T16:20:03.864441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from imc_metric import EvaluateSubmission, ReadCovisibilityData, FlattenMatrix","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:20:03.866315Z","iopub.execute_input":"2022-04-12T16:20:03.867085Z","iopub.status.idle":"2022-04-12T16:20:03.877356Z","shell.execute_reply.started":"2022-04-12T16:20:03.867048Z","shell.execute_reply":"2022-04-12T16:20:03.876713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate on training set (sample)","metadata":{}},{"cell_type":"code","source":"max_num_pairs = 5000\n\nsample_ids = []\nfund_matrices = []\n\nfor scene in scaling_dict.keys():\n    covisibility_dict, F_gt_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')\n\n    # Let's remove pairs with a covisibility below 0.1. Note that the keys are roughly sorted by difficulty, in increasing order, so let's also shuffle before subsampling.\n    # Neither matters for the purposes of this exercise, but let's prevent mistakes down the line.\n    pairs = list([key for key, covis in covisibility_dict.items() if covis >= 0.1])\n    random.shuffle(pairs)\n    n = len(pairs)\n    pairs = pairs[:max_num_pairs]\n    print(f'Loading covisibility data for \"{scene}\"... kept {len(pairs)} out of {n} covisible pairs')\n    \n    for pair in tqdm(pairs):\n        image_0_id, image_1_id = pair.split('-')\n        img_path0 = f'{src}/{scene}/images/{image_0_id}.jpg'\n        img_path1 = f'{src}/{scene}/images/{image_1_id}.jpg'\n        mkpts0, mkpts1 = match(img_path0, img_path1, matcher)\n        F = get_F_matrix(mkpts0, mkpts1)\n        F_str = FlattenMatrix(F)\n\n        sample_ids.append(f'phototourism;{scene};{pair}')\n        fund_matrices.append(F_str)\n    #     break\n    \n#     break","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:21:06.307561Z","iopub.execute_input":"2022-04-12T16:21:06.307865Z","iopub.status.idle":"2022-04-12T16:27:46.487628Z","shell.execute_reply.started":"2022-04-12T16:21:06.307832Z","shell.execute_reply":"2022-04-12T16:27:46.486871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({'sample_id':sample_ids, 'fundamental_matrix':fund_matrices})\ndf.to_csv('train_pred.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T16:27:46.489458Z","iopub.execute_input":"2022-04-12T16:27:46.489693Z","iopub.status.idle":"2022-04-12T16:27:46.506152Z","shell.execute_reply.started":"2022-04-12T16:27:46.489665Z","shell.execute_reply":"2022-04-12T16:27:46.505518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thresholds_q = np.linspace(1, 10, 10)\nthresholds_t = np.geomspace(0.2, 5, 10)\n\nprint('--- Evaluate prediction ---')\nmaa, maa_per_scene, errors_dict_q, errors_dict_t = EvaluateSubmission('train_pred.csv', scaling_dict, thresholds_q, thresholds_t)\nfor scene, cur_maa in maa_per_scene.items():\n    print(f'Scene \"{scene}\" ({len(errors_dict_q[scene])} pairs), mAA={cur_maa:.05f}')\nprint()\nprint(f'Full dataset: mAA={maa:.05f}')\nprint()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}