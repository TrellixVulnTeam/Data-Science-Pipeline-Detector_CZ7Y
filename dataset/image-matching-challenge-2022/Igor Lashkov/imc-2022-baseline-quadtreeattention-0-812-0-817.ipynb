{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### This notebook is a part of [solution](https://www.kaggle.com/competitions/image-matching-challenge-2022/discussion/328805) presented by team of 6th place.\n\nImage size 1024 \n* w/o TTA Public/Private 0.812/0.817\n* with TTA (rot 15Â°) Public/Private 0.815/0.818\n\n#### Please, upvote if you find it useful!","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport pandas as pd\nimport sys, os, csv\nfrom PIL import Image\nimport cv2\nimport gc\nimport matplotlib.pyplot as plt\nimport torch\nfrom IPython import display\nimport PIL\nimport sys\nfrom pathlib import Path\nimport matplotlib.cm as cm\nimport torchvision","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-05T08:33:29.806927Z","iopub.execute_input":"2022-06-05T08:33:29.807409Z","iopub.status.idle":"2022-06-05T08:33:32.084802Z","shell.execute_reply.started":"2022-06-05T08:33:29.807237Z","shell.execute_reply":"2022-06-05T08:33:32.084023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:32.086353Z","iopub.execute_input":"2022-06-05T08:33:32.086603Z","iopub.status.idle":"2022-06-05T08:33:32.094584Z","shell.execute_reply.started":"2022-06-05T08:33:32.086567Z","shell.execute_reply":"2022-06-05T08:33:32.093537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dry_run = False","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:32.095912Z","iopub.execute_input":"2022-06-05T08:33:32.096395Z","iopub.status.idle":"2022-06-05T08:33:32.104297Z","shell.execute_reply.started":"2022-06-05T08:33:32.096357Z","shell.execute_reply":"2022-06-05T08:33:32.103269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Quadree attention","metadata":{}},{"cell_type":"code","source":"%%capture\n!cp -r /kaggle/input/quadtreeattention/ /kaggle/working/ # input folder is read only\n!cd /kaggle/working/quadtreeattention/QuadTreeAttention-master/QuadTreeAttention/ && pip install .\n\nsys.path.append('/kaggle/working/quadtreeattention/QuadTreeAttention-master/')\nsys.path.append('/kaggle/working/quadtreeattention/QuadTreeAttention-master/FeatureMatching/')\nsys.path.append('/kaggle/working/quadtreeattention/QuadTreeAttention-master/QuadTreeAttention/')","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:33:32.107172Z","iopub.execute_input":"2022-06-05T08:33:32.10759Z","iopub.status.idle":"2022-06-05T08:36:19.424808Z","shell.execute_reply.started":"2022-06-05T08:33:32.107546Z","shell.execute_reply":"2022-06-05T08:36:19.423711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install /kaggle/input/igla-py-wheels/loguru-0.5.3-py3-none-any.whl\n!pip install /kaggle/input/igla-py-wheels/einops-0.4.1-py3-none-any.whl\n!pip install /kaggle/input/igla-py-wheels/timm-0.4.12-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:36:19.42677Z","iopub.execute_input":"2022-06-05T08:36:19.427056Z","iopub.status.idle":"2022-06-05T08:37:47.446208Z","shell.execute_reply.started":"2022-06-05T08:36:19.427019Z","shell.execute_reply":"2022-06-05T08:37:47.445053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Init QuadTreeAttention","metadata":{}},{"cell_type":"code","source":"from FeatureMatching.src.config.default import get_cfg_defaults\nconfig = get_cfg_defaults()\n\n# INDOOT lofrt_ds_quadtree config\nconfig.LOFTR.MATCH_COARSE.MATCH_TYPE = 'dual_softmax'\nconfig.LOFTR.MATCH_COARSE.SPARSE_SPVS = False\nconfig.LOFTR.RESNETFPN.INITIAL_DIM = 128\nconfig.LOFTR.RESNETFPN.BLOCK_DIMS=[128, 196, 256]\nconfig.LOFTR.COARSE.D_MODEL = 256\nconfig.LOFTR.COARSE.BLOCK_TYPE = 'quadtree'\nconfig.LOFTR.COARSE.ATTN_TYPE = 'B'\nconfig.LOFTR.COARSE.TOPKS=[32, 16, 16]\nconfig.LOFTR.FINE.D_MODEL = 128\nconfig.TRAINER.WORLD_SIZE = 1 # 8\nconfig.TRAINER.CANONICAL_BS = 32\nconfig.TRAINER.TRUE_BATCH_SIZE = 1\n_scaling = 1\nconfig.TRAINER.ENABLE_PLOTTING = False\nconfig.TRAINER.SCALING = _scaling\nconfig.TRAINER.TRUE_LR = 1e-3 # 1e-4 config.TRAINER.CANONICAL_LR * _scaling\nconfig.TRAINER.WARMUP_STEP = 0 #math.floor(config.TRAINER.WARMUP_STEP / _scaling)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:37:47.447952Z","iopub.execute_input":"2022-06-05T08:37:47.448562Z","iopub.status.idle":"2022-06-05T08:37:47.486529Z","shell.execute_reply.started":"2022-06-05T08:37:47.448512Z","shell.execute_reply":"2022-06-05T08:37:47.485821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\nfrom FeatureMatching.src.utils.profiler import build_profiler\nfrom FeatureMatching.src.lightning.lightning_loftr import PL_LoFTR\n\n\n# lightning module\nqta_center_img_mask = False\nqta_max_img_size = 1056\nqta_torch_device = torch.device('cpu' if not torch.cuda.is_available() else 'cuda')\n\nqta_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndisable_ckpt = True\nqta_profiler_name = None # help='options: [inference, pytorch], or leave it unset\nqta_profiler = build_profiler(qta_profiler_name)\nqta_model = PL_LoFTR(config,\n                 pretrained_ckpt= \"../input/quadtreecheckpoints/outdoor_quadtree.ckpt\", # args.ckpt_path, from scratch atm\n                 profiler=qta_profiler)\nqta_matcher = qta_model.matcher\nqta_matcher.eval()\nqta_matcher.to(qta_device)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:49:05.698756Z","iopub.execute_input":"2022-06-05T08:49:05.699122Z","iopub.status.idle":"2022-06-05T08:49:06.095331Z","shell.execute_reply.started":"2022-06-05T08:49:05.699063Z","shell.execute_reply":"2022-06-05T08:49:06.094559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def load_loftr_image_origNEW(fname, img_w=480, img_h=832):\n    img0_raw = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n    if img_w > 0 and img_h > 0:\n        img0_raw = cv2.resize(img0_raw, (img_w, img_h))\n    img0 = torch.from_numpy(img0_raw)[None][None].cuda() / 255.\n    return img0\n\n\ndef load_resized_image(fname, max_image_size):\n    img = cv2.imread(fname)\n    if max_image_size == -1:\n        # no resize\n        return img, 1.0\n    scale = max_image_size / max(img.shape[0], img.shape[1]) \n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    img = cv2.resize(img, (w, h))\n    return img, scale\n\n\ndef scale_to_resized(mkpts0, mkpts1, scale1, scale2):\n    ### scale to original im size because we used max_image_size\n    # first point\n    mkpts0[:, 0] = mkpts0[:, 0] / scale1\n    mkpts0[:, 1] = mkpts0[:, 1] / scale1\n    \n    # second point\n    mkpts1[:, 0] = mkpts1[:, 0] / scale2\n    mkpts1[:, 1] = mkpts1[:, 1] / scale2\n    \n    return mkpts0, mkpts1\n\n\ndef put_img_on_disk(img, output_img_tag):\n    img_path_on_disk = f'/kaggle/working/{output_img_tag}.png'\n    cv2.imwrite(img_path_on_disk, img)\n    return img_path_on_disk","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:38:02.067346Z","iopub.execute_input":"2022-06-05T08:38:02.067613Z","iopub.status.idle":"2022-06-05T08:38:02.08062Z","shell.execute_reply.started":"2022-06-05T08:38:02.067579Z","shell.execute_reply":"2022-06-05T08:38:02.079958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_divide_size_smallest(im_size, coef):\n    # select size dividable by coef\n    if im_size % coef == 0:\n        # already dividable, just return original\n        return im_size\n    return round(((im_size / coef) + 0.5)) * coef\n\n\ndef add_zero_padding_two_img_same(img1, img2, div_coef=32):\n    img1_height, img1_width, img1_channels = img1.shape\n    img2_height, img2_width, img2_channels = img2.shape\n    \n    # fit both images on canvas\n    max_width = max(img1_width, img2_width)\n    max_height = max(img1_height, img2_height) \n    \n    # use own width and height for image with zero-padding\n    result1, offset1 = create_zero_padding_img(img1, max_width, max_height, img1_channels, div_coef)\n    result2, offset2 = create_zero_padding_img(img2, max_width, max_height, img2_channels, div_coef)\n    \n    return result1, result2, offset1, offset2\n\n\ndef create_zero_padding_img(img, max_im_width, max_im_height, channels, div_coef=32):\n    \n    # create new image of desired size and color (black) for padding\n    new_area_image_width = calc_divide_size_smallest(max_im_width, div_coef)\n    new_area_image_height = calc_divide_size_smallest(max_im_height, div_coef)\n    \n    im_height, im_width, im_channels = img.shape\n    if qta_center_img_mask:\n        x_offset = (new_area_image_width - im_width) // 2\n        y_offset = (new_area_image_height - im_height) // 2\n    else:\n        x_offset = 0\n        y_offset = 0\n    \n    im_right = x_offset + im_width # right of image corner where ends\n    im_bottom = y_offset + im_height # right of image corner where ends\n    \n    color = (0,0,0)\n    result = np.full((new_area_image_height, new_area_image_width, im_channels), color, dtype=np.uint8)\n    \n    # copy img image into center of result image\n    result[y_offset:im_bottom, x_offset:im_right] = img\n    \n    # return image and x,y of old image in a new image (frame)\n    return result, (x_offset, y_offset)\n\n\ndef unpad_matches(mkpts0, mkpts1, offset_point1, offset_point2):\n    offset_x1, offset_y1 = offset_point1\n    offset_x2, offset_y2 = offset_point2\n\n    # remove offeset\n    mkpts0[:, 0] = mkpts0[:, 0] - offset_x1\n    mkpts0[:, 1] = mkpts0[:, 1] - offset_y1\n    \n    mkpts1[:, 0] = mkpts1[:, 0] - offset_x2\n    mkpts1[:, 1] = mkpts1[:, 1] - offset_y2\n    return mkpts0, mkpts1","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:44:09.963795Z","iopub.execute_input":"2022-06-05T08:44:09.96408Z","iopub.status.idle":"2022-06-05T08:44:09.978517Z","shell.execute_reply.started":"2022-06-05T08:44:09.964047Z","shell.execute_reply":"2022-06-05T08:44:09.977745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inf_qta(image_fpath_1, image_fpath_2, max_image_size=qta_max_img_size, divide_coef=32):\n\n    # resize image if we need\n    img1_resized, scale1 = load_resized_image(image_fpath_1, max_image_size)\n    img2_resized, scale2 = load_resized_image(image_fpath_2, max_image_size)\n\n    \n    ### add padding -> use same image padding mask because models wants it\n    pad_img1, pad_img2, pad_offset_p1, pad_offset_p2 = add_zero_padding_two_img_same(img1_resized, img2_resized, divide_coef)\n    \n        \n    # save temporarily    \n    img1_disk_path = put_img_on_disk(pad_img1, 'qta_img1')\n    img2_disk_path = put_img_on_disk(pad_img2, 'qta_img2')\n    \n    \n    # load withr loftr    \n    gray_img_1 = load_loftr_image_origNEW(img1_disk_path, -1, -1)\n    gray_img_2 = load_loftr_image_origNEW(img2_disk_path, -1, -1)\n    \n    batch = {'image0': gray_img_1, 'image1': gray_img_2}\n    \n    # Inference\n    with torch.no_grad():\n        qta_matcher.eval()\n        qta_matcher.to(qta_device)\n        \n        qta_matcher(batch)\n        mkpts0 = batch['mkpts0_f'].cpu().numpy()\n        mkpts1 = batch['mkpts1_f'].cpu().numpy()\n        mconf = batch['mconf'].cpu().numpy()\n    \n    ### unpad matches\n    mkpts0, mkpts1 = unpad_matches(mkpts0, mkpts1, pad_offset_p1, pad_offset_p2)\n    \n    ### scale to original im size because we used max_image_size\n    mkpts0, mkpts1 = scale_to_resized(mkpts0, mkpts1, scale1, scale2)\n    \n    # cleanup\n    if os.path.exists(img1_disk_path): os.remove(img1_disk_path)\n    if os.path.exists(img2_disk_path): os.remove(img2_disk_path)\n    \n    return mkpts0, mkpts1, mconf","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:45:08.460912Z","iopub.execute_input":"2022-06-05T08:45:08.461178Z","iopub.status.idle":"2022-06-05T08:45:08.47125Z","shell.execute_reply.started":"2022-06-05T08:45:08.461145Z","shell.execute_reply":"2022-06-05T08:45:08.470465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Kornia for Vis","metadata":{}},{"cell_type":"code","source":"!pip install /kaggle/input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install /kaggle/input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n\nbase_loftr_github = '/kaggle/input/loftrutils/LoFTR-master/LoFTR-master'\nsys.path.append(base_loftr_github)\n!pip install /kaggle/input/igla-py-wheels/loguru-0.5.3-py3-none-any.whl\n!pip install /kaggle/input/igla-py-wheels/einops-0.4.1-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:38:02.137395Z","iopub.execute_input":"2022-06-05T08:38:02.137801Z","iopub.status.idle":"2022-06-05T08:40:00.153585Z","shell.execute_reply.started":"2022-06-05T08:38:02.137764Z","shell.execute_reply":"2022-06-05T08:40:00.152723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loftr","metadata":{}},{"cell_type":"code","source":"from src.loftr import LoFTR\nfrom src.utils.plotting import make_matching_figures\nfrom src.utils.comm import gather, all_gather\nfrom src.utils.misc import lower_config, flattenList\nfrom src.utils.profiler import PassThroughProfiler\n\nfrom src.loftr import default_cfg\nfrom src.config.default import get_cfg_defaults\nimport pytorch_lightning as pl\n\nimport kornia\nfrom kornia_moons.feature import *\nimport kornia as K\nimport kornia.feature as KF\n\n\ndef draw_img_match(img1, img2, mkpts0, mkpts1, inliers):\n    display_vertical = False#img1.shape[1]/img1.shape[0]>0.8 or img2.shape[1]/img2.shape[0]>0.8\n    \n    draw_LAF_matches(\n    KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n    KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n    torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n    K.tensor_to_image(img1),\n    K.tensor_to_image(img2),\n    inliers,\n    draw_dict={'inlier_color': (0.2, 1, 0.2),\n               'tentative_color': None, \n               'feature_color': (0.2, 0.5, 1), 'vertical': display_vertical})\n    \n\ndef load_torch_kornia_image(fname, device, max_image_size):\n    img,_ = load_resized_image(fname, max_image_size)\n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    orig_img_device = img.to(device)\n    return orig_img_device","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:40:00.15527Z","iopub.execute_input":"2022-06-05T08:40:00.155558Z","iopub.status.idle":"2022-06-05T08:40:00.178711Z","shell.execute_reply.started":"2022-06-05T08:40:00.155518Z","shell.execute_reply":"2022-06-05T08:40:00.177967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utils","metadata":{}},{"cell_type":"code","source":"def FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:40:00.181597Z","iopub.execute_input":"2022-06-05T08:40:00.182087Z","iopub.status.idle":"2022-06-05T08:40:00.189647Z","shell.execute_reply.started":"2022-06-05T08:40:00.18206Z","shell.execute_reply":"2022-06-05T08:40:00.188937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n\nif dry_run:\n    for sample in test_samples:\n        print(sample)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:40:00.192274Z","iopub.execute_input":"2022-06-05T08:40:00.19251Z","iopub.status.idle":"2022-06-05T08:40:00.210167Z","shell.execute_reply.started":"2022-06-05T08:40:00.192484Z","shell.execute_reply":"2022-06-05T08:40:00.209394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"F_dict = {}\nfor i, row in enumerate(test_samples):\n    sample_id, batch_id, image_1_id, image_2_id = row\n\n    image_fpath_1 = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n    image_fpath_2 = f'{src}/test_images/{batch_id}/{image_2_id}.png'\n\n    if dry_run:\n        st = time.time()\n        \n    mkps1,mkps2,mconf = inf_qta(image_fpath_1, image_fpath_2, max_image_size=qta_max_img_size, divide_coef=32)\n    \n    if len(mkps1) > 7:\n        rpt_F = 0.20\n        conf_F = 0.999999\n        m_iters_F = 200_000\n        F, inliers_F = cv2.findFundamentalMat(mkps1, mkps2, cv2.USAC_MAGSAC, rpt_F, conf_F, m_iters_F)\n\n        if dry_run:\n            print(\"Running time: \", time.time() - st, \" s\")\n            \n        \n        if F is None:\n            F_dict[sample_id] = np.zeros((3, 3))\n            continue\n        else:\n            F_dict[sample_id] = F\n            inliers = inliers_F\n    else:\n        F_dict[sample_id] = np.zeros((3, 3))\n        continue\n            \n\n    if dry_run and i < 3:\n        F_inliers_total = 0 if F is None else (inliers_F == 1).sum()\n        print('inliers F', F_inliers_total)\n        \n        orig_image_1 = load_torch_kornia_image(image_fpath_1, qta_torch_device, -1)\n        orig_image_2 = load_torch_kornia_image(image_fpath_2, qta_torch_device, -1)\n        draw_img_match(orig_image_1, orig_image_2, mkps1, mkps2, inliers)\n\n    try:\n        # cleanup\n        gc.collect()\n        torch.cuda.empty_cache()\n        del mkps1, mkps2\n    except Exception:\n        pass","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:48:33.257905Z","iopub.execute_input":"2022-06-05T08:48:33.258206Z","iopub.status.idle":"2022-06-05T08:48:48.194318Z","shell.execute_reply.started":"2022-06-05T08:48:33.258171Z","shell.execute_reply":"2022-06-05T08:48:48.193524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:40:00.724248Z","iopub.status.idle":"2022-06-05T08:40:00.725092Z","shell.execute_reply.started":"2022-06-05T08:40:00.724837Z","shell.execute_reply":"2022-06-05T08:40:00.724873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if dry_run is False:\n    !rm -rf  /kaggle/working/*","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:40:00.726298Z","iopub.status.idle":"2022-06-05T08:40:00.727052Z","shell.execute_reply.started":"2022-06-05T08:40:00.726816Z","shell.execute_reply":"2022-06-05T08:40:00.726842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('submission.csv', 'w') as f:\n    f.write('sample_id,fundamental_matrix\\n')\n    for sample_id, F in F_dict.items():\n        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')\n\nif dry_run:\n    !cat submission.csv","metadata":{"execution":{"iopub.status.busy":"2022-06-05T08:40:00.728174Z","iopub.status.idle":"2022-06-05T08:40:00.72873Z","shell.execute_reply.started":"2022-06-05T08:40:00.728496Z","shell.execute_reply":"2022-06-05T08:40:00.728523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}