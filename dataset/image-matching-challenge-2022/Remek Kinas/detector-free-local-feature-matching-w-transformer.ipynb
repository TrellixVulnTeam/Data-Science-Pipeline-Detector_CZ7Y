{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## LoFTR stands for Detector-Free Local Feature Matching with Transformers. \n\nIt is detailed described in [LoFTR: Detector-Free Local Feature Matching with Transformers](https://arxiv.org/pdf/2104.00680.pdf)(1). \n\nSource[1]: \nNovel method for local image feature matching. Instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. In contrast\nto dense methods that use a cost volume to search correspondences, we use self and cross attention layers in Transformer to obtain feature descriptors that are conditioned on both images. The global receptive field provided by Transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. The experiments on indoor and outdoor datasets show that LoFTR outperforms state-of-the-art methods by a large margin. LoFTR also ranks first on two public benchmarks of visual localization among the published methods. Code is available at  project page: https://zju3dv.github.io/loftr/\n\n\n<div align = \"center\"><img src=\"https://i.ibb.co/mygCR9n/LoFTR.png\"/></div>\n\n\nIn this notebook I will show you how use LoFTR using Kornia for matching whales features. Kornia is a differentiable library that allows classical computer vision to be integrated into deep learning models.\n\nIt consists of a set of routines and differentiable modules to solve generic computer vision problems. At its core, the package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions.\n\nWhy Kornia ?\nWith Kornia we fill the gap between classical and deep computer vision that implements standard and advanced vision algorithms for AI:\n\n* Computer Vision: Kornia fills the gap between Classical and Deep computer Vision.\n* Differentiable: We leverage the Computer Vision 2.0 paradigm.\n* Open Source: Our libraries and initiatives are always according to the community needs.\n* PyTorch: At our core we use PyTorch and its Autograd engine for its efficiency.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n\n!pip install git+https://github.com/kornia/kornia\n!pip install kornia_moons","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-04T20:03:26.950499Z","iopub.execute_input":"2022-04-04T20:03:26.950965Z","iopub.status.idle":"2022-04-04T20:04:13.825918Z","shell.execute_reply.started":"2022-04-04T20:03:26.950855Z","shell.execute_reply":"2022-04-04T20:04:13.825011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport kornia as K\nimport kornia.feature as KF\nfrom kornia.feature.loftr import LoFTR\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport glob\nimport random\n\nfrom kornia_moons.feature import *\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:09:37.764303Z","iopub.execute_input":"2022-04-04T20:09:37.764555Z","iopub.status.idle":"2022-04-04T20:09:37.769439Z","shell.execute_reply.started":"2022-04-04T20:09:37.764529Z","shell.execute_reply":"2022-04-04T20:09:37.76878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_images(ims):\n    \n    fig, axes = plt.subplots(3, 3, figsize=(20,20))\n    \n    for idx, img in enumerate(ims):\n        i = idx % 3 \n        j = idx // 3 \n        image = Image.open(img)\n        image = image.resize((300,300))\n        axes[i, j].imshow(image)\n        axes[i, j].set_title(img.split('/')[-1])\n\n    plt.subplots_adjust(wspace=0, hspace=.2)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:04:15.709045Z","iopub.execute_input":"2022-04-04T20:04:15.709282Z","iopub.status.idle":"2022-04-04T20:04:15.718256Z","shell.execute_reply.started":"2022-04-04T20:04:15.709251Z","shell.execute_reply":"2022-04-04T20:04:15.717435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_torch_image(fname):\n    img = K.image_to_tensor(cv2.imread(fname), False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:04:15.720464Z","iopub.execute_input":"2022-04-04T20:04:15.720889Z","iopub.status.idle":"2022-04-04T20:04:15.725997Z","shell.execute_reply.started":"2022-04-04T20:04:15.720855Z","shell.execute_reply":"2022-04-04T20:04:15.725244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match_and_draw(img_in1, img_in2):\n    img1 = load_torch_image(img_in1)\n    img2 = load_torch_image(img_in2)\n    matcher = LoFTR(pretrained='outdoor')\n    \n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img1), \n                  \"image1\": K.color.rgb_to_grayscale(img2)}\n    \n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n    \n    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n    H, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n    inliers = inliers > 0\n    \n    draw_LAF_matches(\n    KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n    KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n    torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n    K.tensor_to_image(img1),\n    K.tensor_to_image(img2),\n    inliers,\n    draw_dict={'inlier_color': (0.2, 1, 0.2),\n               'tentative_color': None, \n               'feature_color': (0.2, 0.5, 1), 'vertical': False})\n    return correspondences","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:04:15.72733Z","iopub.execute_input":"2022-04-04T20:04:15.727788Z","iopub.status.idle":"2022-04-04T20:04:15.740425Z","shell.execute_reply.started":"2022-04-04T20:04:15.727738Z","shell.execute_reply":"2022-04-04T20:04:15.739702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_matching(samples, files):\n    for i in range(samples.shape[1]):\n        image_1 = files[samples[0][i]]\n        image_2 = files[samples[1][i]]\n        print(f'Matching: {image_1} to {image_2}')\n        correspondences = match_and_draw(image_1, image_2)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:08:48.634513Z","iopub.execute_input":"2022-04-04T20:08:48.634808Z","iopub.status.idle":"2022-04-04T20:08:48.640028Z","shell.execute_reply.started":"2022-04-04T20:08:48.634757Z","shell.execute_reply":"2022-04-04T20:08:48.639254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SIMILARITY CHECK","metadata":{}},{"cell_type":"code","source":"brandenburg_gate_path =  '../input/image-matching-challenge-2022/train/brandenburg_gate/images/'\nbrandenburg_gate_files = [file for file in glob.glob(f'{brandenburg_gate_path}*.jpg')]\n\nplot_images(random.sample(brandenburg_gate_files, 9))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:09:06.348461Z","iopub.execute_input":"2022-04-04T20:09:06.349166Z","iopub.status.idle":"2022-04-04T20:09:08.502574Z","shell.execute_reply.started":"2022-04-04T20:09:06.349115Z","shell.execute_reply":"2022-04-04T20:09:08.501841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_samples = np.random.randint(len(brandenburg_gate_files), size=(2, 4))\n\nplot_matching(random_samples, brandenburg_gate_files)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:09:42.950545Z","iopub.execute_input":"2022-04-04T20:09:42.950818Z","iopub.status.idle":"2022-04-04T20:12:45.733522Z","shell.execute_reply.started":"2022-04-04T20:09:42.950785Z","shell.execute_reply":"2022-04-04T20:12:45.732838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sagrada_familia_path =  '../input/image-matching-challenge-2022/train/sagrada_familia/images/'\nsagrada_familia_files = [file for file in glob.glob(f'{sagrada_familia_path}*.jpg')]\n\nplot_images(random.sample(sagrada_familia_files, 9))","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:12:45.734833Z","iopub.execute_input":"2022-04-04T20:12:45.735166Z","iopub.status.idle":"2022-04-04T20:12:48.006366Z","shell.execute_reply.started":"2022-04-04T20:12:45.735136Z","shell.execute_reply":"2022-04-04T20:12:48.002916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_samples = np.random.randint(len(sagrada_familia_files), size=(2, 4))\n\nplot_matching(random_samples, sagrada_familia_files)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:12:48.007717Z","iopub.execute_input":"2022-04-04T20:12:48.008005Z","iopub.status.idle":"2022-04-04T20:16:36.469256Z","shell.execute_reply.started":"2022-04-04T20:12:48.00797Z","shell.execute_reply":"2022-04-04T20:16:36.468638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## DISSIMILARITY CHECK","metadata":{}},{"cell_type":"code","source":"samples = np.array([[0],[1]])\nfiles = [sagrada_familia_files[0], brandenburg_gate_files[0]]\n\nplot_matching(samples, files)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T20:17:04.0007Z","iopub.execute_input":"2022-04-04T20:17:04.001379Z","iopub.status.idle":"2022-04-04T20:17:46.699194Z","shell.execute_reply.started":"2022-04-04T20:17:04.001342Z","shell.execute_reply":"2022-04-04T20:17:46.69859Z"},"trusted":true},"execution_count":null,"outputs":[]}]}