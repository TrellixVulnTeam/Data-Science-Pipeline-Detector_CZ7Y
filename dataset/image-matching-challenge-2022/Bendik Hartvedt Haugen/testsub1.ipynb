{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***INSTALL LIBS*** ","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl\n!pip install git+https://github.com/kornia/kornia\n!pip install kornia_moons","metadata":{"execution":{"iopub.status.busy":"2022-06-06T09:57:00.675206Z","iopub.execute_input":"2022-06-06T09:57:00.675697Z","iopub.status.idle":"2022-06-06T09:59:13.226354Z","shell.execute_reply.started":"2022-06-06T09:57:00.675608Z","shell.execute_reply":"2022-06-06T09:59:13.22519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***IMPORT DEPENDENCIES***","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport torch\nimport matplotlib.pyplot as plt\nimport kornia\nfrom kornia_moons.feature import *\nimport kornia as K\nimport kornia.feature as KF\nimport kornia.feature.loftr as LoFTR\nimport gc\nimport pandas as pd\nimport glob\nimport random\n\nfrom PIL import Image\n","metadata":{"execution":{"iopub.status.busy":"2022-06-06T09:59:27.368371Z","iopub.execute_input":"2022-06-06T09:59:27.368741Z","iopub.status.idle":"2022-06-06T09:59:31.598179Z","shell.execute_reply.started":"2022-06-06T09:59:27.368708Z","shell.execute_reply":"2022-06-06T09:59:31.597114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***LOAD MODEL***\ndevice = torch.device('cuda')\nmatcher = KF.LoFTR(pretrained=None)\nmatcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\nmatcher = matcher.to(device).eval()","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda')\nmatcher = LoFTR.LoFTR(pretrained=None)\n#matcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\nmatcher.load_state_dict(torch.load(\"../input/trained/last.ckpt\")['state_dict'])\nmatcher = matcher.to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T10:00:46.991838Z","iopub.execute_input":"2022-06-06T10:00:46.992727Z","iopub.status.idle":"2022-06-06T10:00:51.85059Z","shell.execute_reply.started":"2022-06-06T10:00:46.992686Z","shell.execute_reply":"2022-06-06T10:00:51.849799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'\n\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\ndef load_torch_image(fname, device):\n    img = cv2.imread(fname)\n    scale = 840 / max(img.shape[0], img.shape[1]) \n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    img = cv2.resize(img, (w, h))\n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T10:00:54.566066Z","iopub.execute_input":"2022-06-06T10:00:54.566833Z","iopub.status.idle":"2022-06-06T10:00:54.579466Z","shell.execute_reply.started":"2022-06-06T10:00:54.566798Z","shell.execute_reply":"2022-06-06T10:00:54.578631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nF_dict = {}\nimport time\nfor i, row in enumerate(test_samples):\n    sample_id, batch_id, image_1_id, image_2_id = row\n    # Load the images.\n    st = time.time()\n    image_1 = load_torch_image(f'{src}/test_images/{batch_id}/{image_1_id}.png', device)\n    image_2 = load_torch_image(f'{src}/test_images/{batch_id}/{image_2_id}.png', device)\n    print(image_1.shape)\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(image_1), \n              \"image1\": K.color.rgb_to_grayscale(image_2)}\n\n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n        \n    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n    \n    if len(mkpts0) > 7:\n        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.200, 0.9999, 250000)\n        inliers = inliers > 0\n        assert F.shape == (3, 3), 'Malformed F?'\n        F_dict[sample_id] = F\n    else:\n        F_dict[sample_id] = np.zeros((3, 3))\n        continue\n    gc.collect()\n    nd = time.time()    \n    if (i < 3):\n        print(\"Running time: \", nd - st, \" s\")\n        draw_LAF_matches(\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n        torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n        K.tensor_to_image(image_1),\n        K.tensor_to_image(image_2),\n        inliers,\n        draw_dict={'inlier_color': (0.2, 1, 0.2),\n                   'tentative_color': None, \n                   'feature_color': (0.2, 0.5, 1), 'vertical': False})\n\nwith open('submission.csv', 'w') as f:\n    f.write('sample_id,fundamental_matrix\\n')\n    for sample_id, F in F_dict.items():\n        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-06-04T09:15:02.390329Z","iopub.execute_input":"2022-06-04T09:15:02.390703Z","iopub.status.idle":"2022-06-04T09:15:18.579363Z","shell.execute_reply.started":"2022-06-04T09:15:02.390671Z","shell.execute_reply":"2022-06-04T09:15:18.578443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_images(ims):\n    \n    fig, axes = plt.subplots(3, 3, figsize=(20,20))\n    \n    for idx, img in enumerate(ims):\n        i = idx % 3 \n        j = idx // 3 \n        image = Image.open(img)\n        image = image.resize((300,300))\n        axes[i, j].imshow(image)\n        axes[i, j].set_title(img.split('/')[-1])\n\n    plt.subplots_adjust(wspace=0, hspace=.2)\n    plt.show()\n    \n\ndef match_and_draw(img_in1, img_in2):\n    img1 = load_torch_image(img_in1, device)\n    img2 = load_torch_image(img_in2, device)\n\n\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img1), \n                  \"image1\": K.color.rgb_to_grayscale(img2)}\n    \n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n    \n    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n    H, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n    inliers = inliers > 0\n    \n    draw_LAF_matches(\n    KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n    KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n        \n        \n    torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n    K.tensor_to_image(img1),\n    K.tensor_to_image(img2),\n    inliers,\n    draw_dict={'inlier_color': (0.2, 1, 0.2),\n               'tentative_color': None, \n               'feature_color': (0.2, 0.5, 1), 'vertical': False})\n    return correspondences\n\n\n\ndef plot_matching(samples, files):\n    for i in range(samples.shape[1]):\n        image_1 = files[samples[0][i]]\n        image_2 = files[samples[1][i]]\n        print(f'Matching: {image_1} to {image_2}')\n        correspondences = match_and_draw(image_1, image_2)\n\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-06T10:01:02.127078Z","iopub.execute_input":"2022-06-06T10:01:02.127441Z","iopub.status.idle":"2022-06-06T10:01:02.146247Z","shell.execute_reply.started":"2022-06-06T10:01:02.12741Z","shell.execute_reply":"2022-06-06T10:01:02.145367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***SIMILARITY CHECKS ***","metadata":{}},{"cell_type":"code","source":"path =  '../input/image-matching-challenge-2022/train/trevi_fountain/images/'\ntrevi_fountain = [file for file in glob.glob(f'{path}*.jpg')]\n\nplot_images(random.sample(trevi_fountain, 9))","metadata":{"execution":{"iopub.status.busy":"2022-06-06T10:01:06.067659Z","iopub.execute_input":"2022-06-06T10:01:06.068038Z","iopub.status.idle":"2022-06-06T10:01:09.02063Z","shell.execute_reply.started":"2022-06-06T10:01:06.068005Z","shell.execute_reply":"2022-06-06T10:01:09.019777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples = np.random.randint(len(trevi_fountain), size=(2, 4))\n\nplot_matching(samples, trevi_fountain)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T10:01:32.089916Z","iopub.execute_input":"2022-06-06T10:01:32.090752Z","iopub.status.idle":"2022-06-06T10:02:01.453093Z","shell.execute_reply.started":"2022-06-06T10:01:32.090713Z","shell.execute_reply":"2022-06-06T10:02:01.452226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***DISSIMILARITY CHECK***","metadata":{}},{"cell_type":"code","source":"sagrada_familia_path =  '../input/image-matching-challenge-2022/train/sagrada_familia/images/'\nsagrada_familia_files = [file for file in glob.glob(f'{sagrada_familia_path}*.jpg')]\nsamples = np.array([[0],[1]])\nfiles = [sagrada_familia_files[0], trevi_fountain[0]]\n\nplot_matching(samples, files)","metadata":{"execution":{"iopub.status.busy":"2022-06-06T10:02:05.539839Z","iopub.execute_input":"2022-06-06T10:02:05.540625Z","iopub.status.idle":"2022-06-06T10:02:07.086395Z","shell.execute_reply.started":"2022-06-06T10:02:05.540587Z","shell.execute_reply":"2022-06-06T10:02:07.085668Z"},"trusted":true},"execution_count":null,"outputs":[]}]}