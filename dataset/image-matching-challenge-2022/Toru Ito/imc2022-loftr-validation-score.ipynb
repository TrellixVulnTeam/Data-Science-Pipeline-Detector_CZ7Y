{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Reference\n- https://www.kaggle.com/code/namgalielei/loftr-validation-score\n- https://www.kaggle.com/code/remekkinas/detector-free-local-feature-matching-w-transformer\n- https://www.kaggle.com/code/eduardtrulls/imc2022-training-set-eval-one-function","metadata":{}},{"cell_type":"markdown","source":"### This notebook computes validation score on part of training set","metadata":{}},{"cell_type":"code","source":"!pip install kornia --no-index --find-links=file:///kaggle/input/imc2022-dependencies/pip/kornia/ --upgrade \n!pip install kornia_moons --no-index --find-links=file:///kaggle/input/imc2022-dependencies/pip/kornia_moons/ --no-deps  --upgrade ","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-04-18T16:44:08.093793Z","iopub.execute_input":"2022-04-18T16:44:08.094296Z","iopub.status.idle":"2022-04-18T16:44:19.198604Z","shell.execute_reply.started":"2022-04-18T16:44:08.094205Z","shell.execute_reply":"2022-04-18T16:44:19.197739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nsys.path.append('../input/loftrutils/LoFTR-master/LoFTR-master/')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:44:19.200658Z","iopub.execute_input":"2022-04-18T16:44:19.20117Z","iopub.status.idle":"2022-04-18T16:44:19.205774Z","shell.execute_reply.started":"2022-04-18T16:44:19.201119Z","shell.execute_reply":"2022-04-18T16:44:19.20464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r ../input/imutils/imutils-0.5.3/ /\n!pip install /imutils-0.5.3/","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:44:19.207557Z","iopub.execute_input":"2022-04-18T16:44:19.207821Z","iopub.status.idle":"2022-04-18T16:44:49.541823Z","shell.execute_reply.started":"2022-04-18T16:44:19.207787Z","shell.execute_reply":"2022-04-18T16:44:49.540978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport glob\nimport gc\nimport random\n\nfrom collections import namedtuple\nfrom tqdm.notebook import tqdm\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nimport torch\nimport kornia as K\nimport kornia.feature as KF\nfrom kornia.feature.loftr import LoFTR\nfrom kornia_moons.feature import *\n\n# from src.loftr import LoFTR, default_cfg\nfrom src.utils.plotting import make_matching_figure","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:44:49.545155Z","iopub.execute_input":"2022-04-18T16:44:49.545775Z","iopub.status.idle":"2022-04-18T16:44:51.357504Z","shell.execute_reply.started":"2022-04-18T16:44:49.545744Z","shell.execute_reply":"2022-04-18T16:44:51.356762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_path = '/kaggle/input/image-matching-challenge-2022/'","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:44:51.358957Z","iopub.execute_input":"2022-04-18T16:44:51.359234Z","iopub.status.idle":"2022-04-18T16:44:51.363701Z","shell.execute_reply.started":"2022-04-18T16:44:51.359194Z","shell.execute_reply":"2022-04-18T16:44:51.363023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = 'cuda:0'\nWEIGHT_PATH = '../input/loftrutils/outdoor_ds.ckpt'\nLONGEST_EDGE = 1280","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:44:51.364939Z","iopub.execute_input":"2022-04-18T16:44:51.365741Z","iopub.status.idle":"2022-04-18T16:44:51.372597Z","shell.execute_reply.started":"2022-04-18T16:44:51.365705Z","shell.execute_reply":"2022-04-18T16:44:51.371765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"matcher = LoFTR(pretrained=None)\nmatcher.load_state_dict(torch.load(WEIGHT_PATH)['state_dict'])\nmatcher = matcher.to(DEVICE)\nmatcher.eval()\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:44:51.373875Z","iopub.execute_input":"2022-04-18T16:44:51.374203Z","iopub.status.idle":"2022-04-18T16:44:55.074107Z","shell.execute_reply.started":"2022-04-18T16:44:51.374161Z","shell.execute_reply":"2022-04-18T16:44:55.072589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import imutils\ndef resize_keep_ratio(img, longest_size=LONGEST_EDGE):\n    height, width = img.shape[:2]\n    if np.maximum(height, width) <= longest_size: # no need to resize\n        return img\n    \n    if height >= width:\n        resized_img = imutils.resize(img, height=longest_size)\n    else:\n        resized_img = imutils.resize(img, width=longest_size)\n    return resized_img\n\ndef load_torch_image(fname):\n    img = cv2.imread(fname)\n    img = resize_keep_ratio(img)\n    img = cv2.resize(img, (img.shape[1]//8*8, img.shape[0]//8*8))  # input size should be divisible by 8\n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:44:55.075459Z","iopub.execute_input":"2022-04-18T16:44:55.0757Z","iopub.status.idle":"2022-04-18T16:44:55.085868Z","shell.execute_reply.started":"2022-04-18T16:44:55.075666Z","shell.execute_reply":"2022-04-18T16:44:55.085185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match(img_path0, img_path1, matcher, device=DEVICE):\n    img0 = load_torch_image(img_path0)\n    img1 = load_torch_image(img_path1)\n        \n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n    \n    with torch.no_grad():\n        correspondences = matcher(input_dict)      \n        \n    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n        \n    # Make sure we do not trigger an exception here.\n    if len(mkpts0) > 8:\n        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.2, 0.999, 100000)\n\n        try:\n            assert F.shape == (3, 3), 'Malformed F?'\n        except:\n            F = np.zeros((3, 3))\n            \n    else:\n        F = np.zeros((3, 3))\n        \n    del correspondences, input_dict\n    del mkpts0, mkpts1\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return F","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:44:55.08695Z","iopub.execute_input":"2022-04-18T16:44:55.090208Z","iopub.status.idle":"2022-04-18T16:44:55.100093Z","shell.execute_reply.started":"2022-04-18T16:44:55.090172Z","shell.execute_reply":"2022-04-18T16:44:55.099417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def match_and_draw(img_path0, img_path1, matcher, device=DEVICE):\n    \n    img0 = load_torch_image(img_path0)\n    img1 = load_torch_image(img_path1)\n        \n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n    \n    with torch.no_grad():\n        correspondences = matcher(input_dict)    \n        \n    mkpts0 = correspondences['keypoints0'].cpu().numpy()\n    mkpts1 = correspondences['keypoints1'].cpu().numpy()\n    \n    if len(mkpts0) > 8:\n        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.2, 0.999, 100000)\n\n        assert F.shape == (3, 3), 'Malformed F?'\n    else:\n        F = np.zeros((3, 3))\n    \n    draw_LAF_matches(\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n        torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n        K.tensor_to_image(img0),\n        K.tensor_to_image(img1),\n        inliers,\n        draw_dict={'inlier_color': (0.2, 1, 0.2),\n                   'tentative_color': None, \n                   'feature_color': (0.2, 0.5, 1), 'vertical': False})\n    \n    del correspondences, input_dict\n    del mkpts0, mkpts1\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:44:55.101703Z","iopub.execute_input":"2022-04-18T16:44:55.102329Z","iopub.status.idle":"2022-04-18T16:44:55.116315Z","shell.execute_reply.started":"2022-04-18T16:44:55.102291Z","shell.execute_reply":"2022-04-18T16:44:55.115595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eps = 1e-15\n\ndef QuaternionFromMatrix(matrix):\n    '''Transform a rotation matrix into a quaternion.'''\n\n    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n    m00 = M[0, 0]\n    m01 = M[0, 1]\n    m02 = M[0, 2]\n    m10 = M[1, 0]\n    m11 = M[1, 1]\n    m12 = M[1, 2]\n    m20 = M[2, 0]\n    m21 = M[2, 1]\n    m22 = M[2, 2]\n\n    K = np.array([[m00 - m11 - m22, 0.0, 0.0, 0.0],\n              [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n              [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n              [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]])\n    K /= 3.0\n\n    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n    w, V = np.linalg.eigh(K)\n    q = V[[3, 0, 1, 2], np.argmax(w)]\n\n    if q[0] < 0:\n        np.negative(q, q)\n\n    return q\n\n\ndef ComputeErrorForOneExample(q_gt, T_gt, q, T, scale):\n    '''Compute the error metric for a single example. The function returns two errors, over rotation and translation. These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n    \n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t\n\n\ndef ComputeMaa(err_q, err_t, thresholds_q, thresholds_t):\n    '''Compute the mean Average Accuracy at different tresholds, for one scene.'''\n    \n    assert len(err_q) == len(err_t)\n    \n    acc, acc_q, acc_t = [], [], []\n    for th_q, th_t in zip(thresholds_q, thresholds_t):\n        acc += [(np.bitwise_and(np.array(err_q) < th_q, np.array(err_t) < th_t)).sum() / len(err_q)]\n        acc_q += [(np.array(err_q) < th_q).sum() / len(err_q)]\n        acc_t += [(np.array(err_t) < th_t).sum() / len(err_t)]\n    return np.mean(acc), np.array(acc), np.array(acc_q), np.array(acc_t)\n\n\ndef ComputeFundamentalMatrix(K1, K2, R1, R2, T1, T2):\n    '''Compute the fundamental matrix, given intrinsics and extrinsics for two cameras.'''\n    dR = np.dot(R2, R1.T)\n    dT = (T2 - np.dot(dR, T1)).flatten()\n    A = np.dot(K1, np.dot(dR.T, dT))\n    C = np.array([[0, -A[2], A[1]], [A[2], 0, -A[0]], [-A[1], A[0], 0]])\n    return np.matmul(np.linalg.inv(K2).T, np.matmul(dR, np.matmul(K1.T, C)))\n\n\ndef DecomposeFundamentalMatrixWithIntrinsics(F, K1, K2):\n    '''Decompose the fundamental matrix into R and T, given the intrinsics.'''\n    \n    # This fundamentally reimplements this function: https://github.com/opencv/opencv/blob/be38d4ea932bc3a0d06845ed1a2de84acc2a09de/modules/calib3d/src/five-point.cpp#L742\n    # This is a pre-requisite of OpenCV's recoverPose: https://github.com/opencv/opencv/blob/be38d4ea932bc3a0d06845ed1a2de84acc2a09de/modules/calib3d/src/five-point.cpp#L559\n    # Instead of the cheirality check with correspondences, we keep and evaluate the different hypotheses downstream, and pick the best one.\n    # Note that our metric does not care about the sign of the translation vector, so we only need to evaluate the two rotation matrices.\n    E = np.matmul(K2.T, np.matmul(F, K1))\n\n    U, S, Vh = np.linalg.svd(E)\n    if np.linalg.det(U) < 0:\n        U *= -1\n    if np.linalg.det(Vh) < 0:\n        Vh *= -1\n\n    W = np.array([[0, 1, 0], [-1, 0, 0], [0, 0, 1]])\n    R_a = np.matmul(U, np.matmul(W, Vh))\n    R_b = np.matmul(U, np.matmul(W.T, Vh))\n    T = U[:, -1]\n\n    return R_a, R_b, T\n\n\ndef ComputeErrorForOneExample(q_gt, T_gt, q, T, scale):\n    '''Compute the error metric for a single example.\n    \n    The function returns two errors, over rotation and translation. These are combined at different thresholds by ComputeMaa, downstream, in order to compute the mean Average Accuracy.'''\n    \n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t\n\n\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\ndef EvaluateSubmission(df):\n    '''Evaluate a prediction file against the ground truth.\n    \n    Note that only the subset of entries in the prediction file will be evaluated.'''\n    \n    thresholds_q = np.linspace(1, 10, 10)\n    thresholds_t = np.geomspace(0.2, 5, 10)\n    \n    scenes = df['scene'].unique()\n    errors_dict_q = {scene: {} for scene in scenes}\n    errors_dict_t = {scene: {} for scene in scenes}\n    \n    for i, row in df.iterrows():\n        F_predicted    = row.fundamental_matrix\n        scene          = row.scene\n        scaling_factor = row.scaling_factor\n        pair = row.image_id_1 + '-' + row.image_id_2\n\n        K1, R1_gt, T1_gt = row.K1, row.R1, row.T1\n        K2, R2_gt, T2_gt = row.K2, row.R2, row.T2\n\n        R_pred_a, R_pred_b, T_pred = DecomposeFundamentalMatrixWithIntrinsics(F_predicted, K1, K2)\n        q_pred_a = QuaternionFromMatrix(R_pred_a)\n        q_pred_b = QuaternionFromMatrix(R_pred_b)\n\n        dR_gt = np.dot(R2_gt, R1_gt.T)\n        dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n        q_gt = QuaternionFromMatrix(dR_gt)\n        q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n\n        # blah blah cheirality...\n        err_q_a, err_t_a = ComputeErrorForOneExample(q_gt, dT_gt, q_pred_a, T_pred, scaling_factor)\n        err_q_b, err_t_b = ComputeErrorForOneExample(q_gt, dT_gt, q_pred_b, T_pred, scaling_factor)\n        assert err_t_a == err_t_b\n        errors_dict_q[scene][pair] = min(err_q_a, err_q_b)\n        errors_dict_t[scene][pair] = err_t_a\n\n    # Aggregate the results by computing the final metric for each scene, and then averaging across all scenes.\n    maa_per_scene = {}\n    \n    for scene in scenes:\n        maa_per_scene[scene], _, _, _ = ComputeMaa(list(errors_dict_q[scene].values()), list(errors_dict_t[scene].values()), thresholds_q, thresholds_t)\n    \n    return np.mean(list(maa_per_scene.values())), maa_per_scene, errors_dict_q, errors_dict_t","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-18T16:46:09.096324Z","iopub.execute_input":"2022-04-18T16:46:09.096788Z","iopub.status.idle":"2022-04-18T16:46:09.165149Z","shell.execute_reply.started":"2022-04-18T16:46:09.096746Z","shell.execute_reply":"2022-04-18T16:46:09.164364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaling_df = pd.read_csv(f'{dataset_path}/train/scaling_factors.csv')\nscaling_df['scaling_factor'] = scaling_df['scaling_factor'].astype(float)\nscaling_df","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:46:09.839958Z","iopub.execute_input":"2022-04-18T16:46:09.840511Z","iopub.status.idle":"2022-04-18T16:46:09.858993Z","shell.execute_reply.started":"2022-04-18T16:46:09.840472Z","shell.execute_reply":"2022-04-18T16:46:09.858105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_df = pd.DataFrame()\nmax_num_pairs = 50\n\nfor scene in scaling_df['scene'].values:\n    pair_df = pd.read_csv(f'{dataset_path}/train/{scene}/pair_covisibility.csv')\n    pair_df['scene'] = scene\n    pair_df = pair_df[pair_df['covisibility']>0.1].copy()\n    pair_df.reset_index(drop=True, inplace=True)\n    \n    if len(pair_df) > max_num_pairs:\n        pair_df = pair_df.sample(n=max_num_pairs, random_state=42)\n        pair_df.reset_index(drop=True, inplace=True)\n    \n    pair_df['fundamental_matrix'] = pair_df['fundamental_matrix'].apply( lambda x: np.array([float(v) for v in x.split(' ')]).reshape([3, 3]) )\n    pair_df['image_id_1'] = pair_df['pair'].apply( lambda x: x.split('-')[0] )\n    pair_df['image_id_2'] = pair_df['pair'].apply( lambda x: x.split('-')[1] )\n    pair_df.drop( 'pair', axis=1, inplace=True )\n    \n    #calibration\n    calib_df = pd.read_csv(f'{dataset_path}/train/{scene}/calibration.csv')\n    calib_df['camera_intrinsics'] = calib_df['camera_intrinsics'].apply( lambda x: np.array([float(v) for v in x.split(' ')]).reshape([3, 3]) )\n    calib_df['rotation_matrix'] = calib_df['rotation_matrix'].apply( lambda x: np.array([float(v) for v in x.split(' ')]).reshape([3, 3]) )\n    calib_df['translation_vector'] = calib_df['translation_vector'].apply( lambda x: np.array([float(v) for v in x.split(' ')]).reshape([3, 1]) )\n\n    pair_df = pd.merge( pair_df, calib_df.rename( columns={'image_id':'image_id_1', 'camera_intrinsics':'K1', 'rotation_matrix':'R1', 'translation_vector':'T1'} ), on=['image_id_1'], how='left')\n    pair_df = pd.merge( pair_df, calib_df.rename( columns={'image_id':'image_id_2', 'camera_intrinsics':'K2', 'rotation_matrix':'R2', 'translation_vector':'T2'} ), on=['image_id_2'], how='left')\n\n    data_df = pd.concat( [data_df, pair_df], axis=0 )\n    data_df.reset_index(drop=True, inplace=True)\n    \n    del pair_df, calib_df\n    \ndata_df = pd.merge( data_df, scaling_df, on=['scene'], how='left')\ndata_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:46:54.792466Z","iopub.execute_input":"2022-04-18T16:46:54.79276Z","iopub.status.idle":"2022-04-18T16:46:59.382655Z","shell.execute_reply.started":"2022-04-18T16:46:54.792728Z","shell.execute_reply":"2022-04-18T16:46:59.381987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nprint('--- Processing a ground truth submission ---')\nmaa, maa_per_scene, errors_dict_q, errors_dict_t = EvaluateSubmission(data_df)\n\nfor scene, cur_maa in maa_per_scene.items():\n    print(f'Scene:{scene:25s} ({len(errors_dict_q[scene])} pairs), mAA={cur_maa:.05f}')\n    \nprint()\nprint(f'Full dataset: mAA={maa:.05f}')\n'''","metadata":{"execution":{"iopub.status.busy":"2022-04-18T16:46:59.384058Z","iopub.execute_input":"2022-04-18T16:46:59.385304Z","iopub.status.idle":"2022-04-18T16:47:01.862461Z","shell.execute_reply.started":"2022-04-18T16:46:59.385264Z","shell.execute_reply":"2022-04-18T16:47:01.860924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate on training set (sample)","metadata":{}},{"cell_type":"code","source":"F_list = []\n\nfor i, row in tqdm( data_df.iterrows(), total=len(data_df) ):\n    scene      = row.scene\n    image_id_1 = row.image_id_1\n    image_id_2 = row.image_id_2\n    \n    img_path1 = f'{dataset_path}/train/{scene}/images/{image_id_1}.jpg'\n    img_path2 = f'{dataset_path}/train/{scene}/images/{image_id_2}.jpg'\n    \n    F = match(img_path1, img_path2, matcher)\n    F_list.append(F)\n    \n    if i < 3:\n        match_and_draw(img_path1, img_path2, matcher)\n        \ndata_df['fundamental_matrix'] = F_list","metadata":{"execution":{"iopub.status.busy":"2022-04-16T15:40:37.309172Z","iopub.execute_input":"2022-04-16T15:40:37.310005Z","iopub.status.idle":"2022-04-16T15:41:42.493579Z","shell.execute_reply.started":"2022-04-16T15:40:37.309967Z","shell.execute_reply":"2022-04-16T15:41:42.492883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maa, maa_per_scene, errors_dict_q, errors_dict_t = EvaluateSubmission(data_df)\n\nfor scene, cur_maa in maa_per_scene.items():\n    print(f'Scene:{scene:25s} ({len(errors_dict_q[scene])} pairs), mAA={cur_maa:.05f}')\n    \nprint()\nprint(f'Full dataset: mAA={maa:.05f}')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T15:41:42.497783Z","iopub.execute_input":"2022-04-16T15:41:42.498303Z","iopub.status.idle":"2022-04-16T15:41:42.676626Z","shell.execute_reply.started":"2022-04-16T15:41:42.498259Z","shell.execute_reply":"2022-04-16T15:41:42.675789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}