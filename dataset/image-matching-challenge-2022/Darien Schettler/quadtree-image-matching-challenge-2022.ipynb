{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\">Image Matching Challenge 2022</h2>\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: DARIEN SCHETTLER</h5>\n\n<br>\n\n---\n\n<br>\n\n<center><div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üõë &nbsp; WARNING:</b><br><br><b>THIS IS A WORK IN PROGRESS</b><br>\n</div></center>\n\n\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üëè &nbsp; IF YOU FORK THIS OR FIND THIS HELPFUL &nbsp; üëè</b><br><br><b style=\"font-size: 22px; color: darkorange\">PLEASE UPVOTE!</b><br><br>This was a lot of work for me and while it may seem silly, it makes me feel appreciated when others like my work. üòÖ\n</div></center>\n\n\n","metadata":{"papermill":{"duration":0.120561,"end_time":"2021-11-06T21:15:09.611563","exception":false,"start_time":"2021-11-06T21:15:09.491002","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<p id=\"toc\"></p>\n\n<br><br>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\">TABLE OF CONTENTS</h1>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#background_information\">1&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">2&nbsp;&nbsp;&nbsp;&nbsp;SETUP</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">3&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#dataset_exploration\">4&nbsp;&nbsp;&nbsp;&nbsp;DATASET EXPLORATION & PREPROCESSING</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#model_baseline\">5&nbsp;&nbsp;&nbsp;&nbsp;BASELINE</a></h3>\n\n---","metadata":{"papermill":{"duration":0.085591,"end_time":"2021-11-06T21:15:09.78303","exception":false,"start_time":"2021-11-06T21:15:09.697439","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"imports\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: black;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>","metadata":{"papermill":{"duration":0.050527,"end_time":"2021-11-06T21:15:09.894476","exception":false,"start_time":"2021-11-06T21:15:09.843949","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import sys\n\n!cp -r /kaggle/input/einops .\n%cd /kaggle/working/einops\n!pip install -e . --no-index --no-deps\n%cd /kaggle/working\nsys.path.insert(0, \"/kaggle/working/einops\")\nimport einops\n\n!cp -r /kaggle/input/timmmaster .\n%cd /kaggle/working/timmmaster\n!pip install -e . --no-index --no-deps\n%cd /kaggle/working\nsys.path.insert(0, \"/kaggle/working/timmmaster\")\nimport timm\n\n!cp -r /kaggle/input/quad-tree-attention-image-matching-repo/* .\n%cd ./QuadTreeAttention\n!pip install -e . --no-index --no-deps\n%cd /kaggle/working\nsys.path.insert(0, \"/kaggle/working/FeatureMatching\")\nsys.path.insert(0, \"/kaggle/working/QuadTreeAttention\")\n\nprint(\"\\n... IMPORTS STARTING ...\\n\")\n\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t‚Äì NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t‚Äì SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom pandarallel import pandarallel; pandarallel.initialize();\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\n\n# # RAPIDS\n# import cudf, cupy, cuml\n# from cuml.neighbors import NearestNeighbors\n# from cuml.manifold import TSNE, UMAP\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport hashlib\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport json\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image, ImageEnhance\nimport matplotlib; print(f\"\\t\\t‚Äì MATPLOTLIB VERSION: {matplotlib.__version__}\");\nfrom matplotlib import animation, rc; rc('animation', html='jshtml')\nimport plotly\nimport PIL\nimport cv2\n\nimport plotly.io as pio\nprint(pio.renderers)\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nfrom copy import deepcopy\nimport torch\nimport matplotlib.cm as cm\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")","metadata":{"papermill":{"duration":162.144149,"end_time":"2021-11-06T21:17:52.087371","exception":false,"start_time":"2021-11-06T21:15:09.943222","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T19:06:45.271926Z","iopub.execute_input":"2022-04-12T19:06:45.272368Z","iopub.status.idle":"2022-04-12T19:09:27.799921Z","shell.execute_reply.started":"2022-04-12T19:06:45.272314Z","shell.execute_reply":"2022-04-12T19:09:27.799104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from src.loftr import LoFTR\nfrom src.config.default import get_cfg_defaults\nfrom src.loftr.utils.cvpr_ds_config import default_cfg\nfrom yacs.config import CfgNode as CN\nfrom configs.loftr.outdoor.loftr_ds_quadtree import cfg\n\ndef lower_config(yacs_cfg):\n    if not isinstance(yacs_cfg, CN):\n        return yacs_cfg\n    return {k.lower(): lower_config(v) for k, v in yacs_cfg.items()}\n\ncfg = lower_config(cfg)[\"loftr\"]\n\ntorch.set_grad_enabled(False)\n\nif torch.cuda.is_available():\n    device = 'cuda' \nelse:\n    raise RuntimeError(\"GPU is required to run this demo.\")\n    \n# Initialize LoFTR\nmatcher = LoFTR(config=cfg)\nmatcher.load_state_dict(torch.load(\"/kaggle/input/quadtree-outdoor-model-checkpoint/outdoor.ckpt\")['state_dict'])\nmatcher = matcher.eval().to(device=device)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:09:27.801686Z","iopub.execute_input":"2022-04-12T19:09:27.802158Z","iopub.status.idle":"2022-04-12T19:09:35.86882Z","shell.execute_reply.started":"2022-04-12T19:09:27.802117Z","shell.execute_reply":"2022-04-12T19:09:35.868015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">2.4 BASIC DATA DEFINITIONS & INITIALIZATIONS</h3>\n\n---\n","metadata":{"papermill":{"duration":0.090507,"end_time":"2021-11-06T21:17:53.624612","exception":false,"start_time":"2021-11-06T21:17:53.534105","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n\nprint(\"\\n... TRAIN META ...\\n\")\nDATA_DIR = \"/kaggle/input/image-matching-challenge-2022\"\nTRAIN_DIR = os.path.join(DATA_DIR, \"train\")\nSCALING_CSV = os.path.join(TRAIN_DIR, \"scaling_factors.csv\")\nscaling_df = pd.read_csv(SCALING_CSV)\nscale_map = scaling_df.groupby(\"scene\")[\"scaling_factor\"].first().to_dict()\n\n# 'british_museum', 'piazza_san_marco', \n# 'trevi_fountain', 'st_pauls_cathedral', \n# 'colosseum_exterior', 'buckingham_palace', \n# 'temple_nara_japan', 'sagrada_familia', \n# 'grand_place_brussels', 'pantheon_exterior', \n# 'notre_dame_front_facade', 'st_peters_square', \n# 'sacre_coeur', 'taj_mahal', \n# 'lincoln_memorial_statue', 'brandenburg_gate'\nTRAIN_SCENES = scaling_df.scene.unique().tolist()\n\ntrain_map = {} \nfor _s in tqdm(TRAIN_SCENES, total=len(TRAIN_SCENES)):\n    # Initialize    \n    train_map[_s] = {}\n    \n    # Image Stuff\n    train_map[_s][\"images\"] = sorted(glob(os.path.join(TRAIN_DIR, _s, \"images\", \"*.jpg\")))\n    train_map[_s][\"image_ids\"] = [_f_path[:-4].rsplit(\"/\", 1)[-1] for _f_path in train_map[_s][\"images\"]]\n    \n    # Calibration Stuff (CAL)\n    _tmp_cal_df = pd.read_csv(os.path.join(TRAIN_DIR, _s, \"calibration.csv\"))\n    _tmp_cal_df[\"image_path\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_cal_df[\"image_id\"]+\".jpg\"\n    train_map[_s][\"cal_df\"]=_tmp_cal_df.copy()\n        \n    # Pair Covisibility Stuff (PCO)\n    _tmp_pco_df = pd.read_csv(os.path.join(TRAIN_DIR, _s, \"pair_covisibility.csv\"))\n    _tmp_pco_df[\"image_id_1\"], _tmp_pco_df[\"image_id_2\"] = zip(*_tmp_pco_df.pair.apply(lambda x: x.split(\"-\")))\n    _tmp_pco_df[\"image_path_1\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_pco_df[\"image_id_1\"]+\".jpg\"\n    _tmp_pco_df[\"image_path_2\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_pco_df[\"image_id_2\"]+\".jpg\"\n    train_map[_s][\"pco_df\"] = _tmp_pco_df.copy()\n\n#cleanup\ndel _tmp_cal_df, _tmp_pco_df; gc.collect(); gc.collect();\n    \nprint(\"\\n... TEST META ...\\n\")\nTEST_IMG_DIR = os.path.join(DATA_DIR, \"test_images\")\nTEST_CSV = os.path.join(DATA_DIR, \"test.csv\")\ntest_df = pd.read_csv(TEST_CSV)\ntest_df[\"f_path_1\"] = TEST_IMG_DIR+\"/\"+test_df.batch_id+\"/\"+test_df.image_1_id+\".png\"\ntest_df[\"f_path_2\"] = TEST_IMG_DIR+\"/\"+test_df.batch_id+\"/\"+test_df.image_2_id+\".png\"\ndisplay(test_df)\n\nSS_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\nss_df = pd.read_csv(SS_CSV)\ndisplay(ss_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:09:35.870219Z","iopub.execute_input":"2022-04-12T19:09:35.870489Z","iopub.status.idle":"2022-04-12T19:09:52.133614Z","shell.execute_reply.started":"2022-04-12T19:09:35.870454Z","shell.execute_reply":"2022-04-12T19:09:52.132889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"helper_functions\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\" id=\"helper_functions\">\n    3&nbsp;&nbsp;HELPER FUNCTION & CLASSES&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{"papermill":{"duration":0.054893,"end_time":"2021-11-06T21:17:54.695576","exception":false,"start_time":"2021-11-06T21:17:54.640683","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    nested_list = [x if type(x) is list else [x,] for x in nested_list]\n    return [item for sublist in nested_list for item in sublist]\n\ndef plot_two_paths(f_path_1, f_path_2, _titles=None):\n    plt.figure(figsize=(20, 10))\n    \n    plt.subplot(1,2,1)\n    if _titles is None:\n        plt.title(f_path_1, fontweight=\"bold\")\n    else:\n        plt.title(_titles[0], fontweight=\"bold\")\n    plt.imshow(cv2.imread(f_path_1)[..., ::-1])\n    plt.axis(False)\n    \n    plt.subplot(1,2,2)\n    if _titles is None:\n        plt.title(f_path_2, fontweight=\"bold\")\n    else:\n        plt.title(_titles[1], fontweight=\"bold\")\n\n    plt.imshow(cv2.imread(f_path_2)[..., ::-1])\n    plt.axis(False)\n    \n    plt.tight_layout()\n    plt.show()\n    \ndef pco_random_plot(_df):\n    df_row = _df.sample(1).reset_index(drop=True)\n    plot_two_paths(\n        df_row.image_path_1[0],\n        df_row.image_path_2[0],\n        [f\"Image ID: {df_row.image_id_1} - COV={df_row.covisibility[0]}\",\n         f\"Image ID: {df_row.image_id_2} - COV={df_row.covisibility[0]}\",]\n    )\n    \ndef arr_from_str(_s):\n    return np.fromstring(_s, sep=\" \").reshape((-1,3)).squeeze()\n\ndef get_id2cal_map(_train_map):\n    _id2cal_map = {}\n    for _s in tqdm(TRAIN_SCENES, total=len(TRAIN_SCENES)):\n        for _, _row in _train_map[_s][\"cal_df\"].iterrows():\n            _img_id = _row[\"image_id\"]; _id2cal_map[_img_id]={}\n            _id2cal_map[_img_id][\"camera_intrinsics\"] = arr_from_str(_row[\"camera_intrinsics\"])\n            _id2cal_map[_img_id][\"rotation_matrix\"] = arr_from_str(_row[\"rotation_matrix\"])\n            _id2cal_map[_img_id][\"translation_vector\"] = arr_from_str(_row[\"translation_vector\"])\n            _id2cal_map[_img_id][\"image_path\"] = _row[\"image_path\"]\n    return _id2cal_map\n\nid2cal_map = get_id2cal_map(train_map)","metadata":{"papermill":{"duration":0.098071,"end_time":"2021-11-06T21:17:54.848036","exception":false,"start_time":"2021-11-06T21:17:54.749965","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-04-12T19:09:52.135871Z","iopub.execute_input":"2022-04-12T19:09:52.136314Z","iopub.status.idle":"2022-04-12T19:09:52.644583Z","shell.execute_reply.started":"2022-04-12T19:09:52.136275Z","shell.execute_reply":"2022-04-12T19:09:52.643789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The following functions are for feature identification, matrix manipulations, etc**","metadata":{}},{"cell_type":"code","source":"def extract_sift_features(rgb_image, detector, n_features):\n    \"\"\" \n    Helper Function to Compute SIFT features for a Given Image\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    # Convert RGB image to Grayscale\n    gray = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)\n    \n    # Run detector and retrieve only the top keypoints and descriptors \n    kp, desc = detector.detectAndCompute(gray, None)\n    \n    return kp[:n_features], desc[:n_features]\n\ndef extract_keypoints(rgb_image, detector):\n    \"\"\" \n    Helper Function to Compute SIFT features for a Given Image\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    # Convert RGB image to Grayscale\n    gray = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)\n    \n    # Run detector and retrieve the keypoints\n    return detector.detect(gray)\n\ndef build_composite_image(im1, im2, axis=1, margin=0, background=1):\n    \"\"\"\n    Convenience function to stack two images with different sizes.\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    \n    if background != 0 and background != 1:\n        background = 1\n    if axis != 0 and axis != 1:\n        raise RuntimeError('Axis must be 0 (vertical) or 1 (horizontal')\n\n    h1, w1, _ = im1.shape\n    h2, w2, _ = im2.shape\n\n    if axis == 1:\n        composite = np.zeros((max(h1, h2), w1 + w2 + margin, 3), dtype=np.uint8) + 255 * background\n        if h1 > h2:\n            voff1, voff2 = 0, (h1 - h2) // 2\n        else:\n            voff1, voff2 = (h2 - h1) // 2, 0\n        hoff1, hoff2 = 0, w1 + margin\n    else:\n        composite = np.zeros((h1 + h2 + margin, max(w1, w2), 3), dtype=np.uint8) + 255 * background\n        if w1 > w2:\n            hoff1, hoff2 = 0, (w1 - w2) // 2\n        else:\n            hoff1, hoff2 = (w2 - w1) // 2, 0\n        voff1, voff2 = 0, h1 + margin\n    composite[voff1:voff1 + h1, hoff1:hoff1 + w1, :] = im1\n    composite[voff2:voff2 + h2, hoff2:hoff2 + w2, :] = im2\n\n    return (composite, (voff1, voff2), (hoff1, hoff2))\n\n\ndef draw_cv_matches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n    \"\"\"\n    Draw keypoints and matches.\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    \n    composite, v_offset, h_offset = build_composite_image(im1, im2, axis, margin, background)\n\n    # Draw all keypoints.\n    for coord_a, coord_b in zip(kp1, kp2):\n        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n    \n    # Draw matches, and highlight keypoints used in matches.\n    for idx_a, idx_b in matches:\n        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.line(composite,\n                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n                                   int(kp1[idx_a][1] + v_offset[0])]),\n                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n    return composite\n\ndef normalize_keypoints(keypoints, K):\n    C_x = K[0, 2]\n    C_y = K[1, 2]\n    f_x = K[0, 0]\n    f_y = K[1, 1]\n    keypoints = (keypoints - np.array([[C_x, C_y]])) / np.array([[f_x, f_y]])\n    return keypoints\n\ndef compute_essential_matrix(F, K1, K2, kp1, kp2):\n    \"\"\"\n    Compute the Essential matrix from the Fundamental matrix, given the calibration matrices. \n    Note that we ask participants to estimate F, i.e., without relying on known intrinsics.\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    \n    # Warning! Old versions of OpenCV's RANSAC could return multiple F matrices, encoded as a single matrix size 6x3 or 9x3, rather than 3x3.\n    # We do not account for this here, as the modern RANSACs do not do this:\n    # https://opencv.org/evaluating-opencvs-new-ransacs\n    assert F.shape[0] == 3, 'Malformed F?'\n\n    # Use OpenCV's recoverPose to solve the cheirality check:\n    # https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#gadb7d2dfcc184c1d2f496d8639f4371c0\n    E = np.matmul(np.matmul(K2.T, F), K1).astype(np.float64)\n    \n    kp1n = normalize_keypoints(kp1, K1)\n    kp2n = normalize_keypoints(kp2, K2)\n    num_inliers, R, T, mask = cv2.recoverPose(E, kp1n, kp2n)\n\n    return E, R, T\n\ndef quaternion_from_matrix(matrix):\n    \"\"\"\n    Transform a rotation matrix into a quaternion\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \n    The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\\\n    \"\"\"\n\n    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n    m00 = M[0, 0]\n    m01 = M[0, 1]\n    m02 = M[0, 2]\n    m10 = M[1, 0]\n    m11 = M[1, 1]\n    m12 = M[1, 2]\n    m20 = M[2, 0]\n    m21 = M[2, 1]\n    m22 = M[2, 2]\n\n    K = np.array([\n        [m00 - m11 - m22, 0.0, 0.0, 0.0],\n        [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n        [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n        [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]\n    ])\n    K /= 3.0\n\n    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n    w, V = np.linalg.eigh(K)\n    q = V[[3, 0, 1, 2], np.argmax(w)]\n    if q[0] < 0:\n        np.negative(q, q)\n    return q\n\n\ndef compute_error_for_example(q_gt, T_gt, q, T, scale, eps=1e-15):\n    '''Compute the error metric for a single example.\n    \n    The function returns two errors, over rotation and translation. \n    These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n    \n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t\n\ndef quadtree_get_F_w_plot(img_path_1, img_path_2, matcher, resize_to=(640,480), n_iter=12_500, top_n=100):\n    _img_1 = cv2.resize(cv2.imread(img_path_1, cv2.IMREAD_GRAYSCALE), resize_to)\n    _img_2 = cv2.resize(cv2.imread(img_path_2, cv2.IMREAD_GRAYSCALE), resize_to)\n    img_1 = torch.from_numpy(_img_1)[None][None].cuda()/255.\n    img_2 = torch.from_numpy(_img_2)[None][None].cuda()/255.\n    batch = {'image0': img_1, 'image1': img_2}\n\n    with torch.no_grad():\n        matcher(batch)\n        kps_1 = batch['mkpts0_f'].cpu().numpy()\n        kps_2 = batch['mkpts1_f'].cpu().numpy()\n        m_conf = batch['mconf'].cpu().numpy()\n            \n    # Remove Low Confidence\n    kps_1 = kps_1[np.argsort(m_conf)][:top_n]\n    kps_2 = kps_2[np.argsort(m_conf)][:top_n]\n    m_conf = m_conf[np.argsort(m_conf)][:top_n]\n    \n    # Normalize confidence.\n    if len(m_conf)>0:\n        m_conf = (m_conf - m_conf.min()) / (m_conf.max()-m_conf.min()+1e-5)\n\n    print(\"\\n... BEFORE MAGSAC ...\\n\")\n    color = cm.afmhot(m_conf)\n    text = [\n        'LoFTR',\n        'Matches: {}'.format(len(kps_1)),\n    ]\n    fig = make_matching_figure(_img_1, _img_2, kps_1, kps_2, color, text=text, dpi=200,)\n    plt.show()\n    \n    F, inlier_mask = cv2.findFundamentalMat(kps_1, kps_2, cv2.USAC_MAGSAC, ransacReprojThreshold=0.50, confidence=0.999, maxIters=n_iter)\n\n    color = cm.afmhot(m_conf[np.where(inlier_mask==True)[0]])\n    text = [\n        'LoFTR',\n        'Matches: {}'.format(len(kps_1[np.where(inlier_mask==True)[0]])),\n    ]\n    \n    print(\"\\n... AFTER MAGSAC ...\\n\")\n    fig = make_matching_figure(_img_1, _img_2, kps_1[np.where(inlier_mask==True)[0]], kps_2[np.where(inlier_mask==True)[0]], color, text=text, dpi=200,)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:52:03.273966Z","iopub.execute_input":"2022-04-12T19:52:03.27435Z","iopub.status.idle":"2022-04-12T19:52:03.636631Z","shell.execute_reply.started":"2022-04-12T19:52:03.274313Z","shell.execute_reply":"2022-04-12T19:52:03.635054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"dataset_exploration\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\" id=\"dataset_exploration\">\n    4&nbsp;&nbsp;USE QUADTREE&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---\n\nTBD","metadata":{"papermill":{"duration":0.056443,"end_time":"2021-11-06T21:17:54.960518","exception":false,"start_time":"2021-11-06T21:17:54.904075","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacinga: 2px; color: black; background-color: #ffffff;\">4.1 LOOK AT A SINGLE EXAMPLE W/ COVISIBILITY & CALIBRATION INFORMATION</h3>\n\n---\n","metadata":{}},{"cell_type":"code","source":"from src.utils.plotting import make_matching_figure\n\n# Step 1 - Pick An Image Pair And Display\nDEMO_ROW = train_map[\"taj_mahal\"][\"pco_df\"].iloc[850]\nDEMO_IMG_ID_1 = DEMO_ROW.image_id_1\nDEMO_IMG_ID_2 = DEMO_ROW.image_id_2\nDEMO_PATH_1 = DEMO_ROW.image_path_1\nDEMO_PATH_2 = DEMO_ROW.image_path_2\n\ndemo_img_1 = cv2.imread(DEMO_PATH_1)[..., ::-1]\ndemo_img_2 = cv2.imread(DEMO_PATH_2)[..., ::-1]\n\n# FM= Fundamental Matrix\nDEMO_F = arr_from_str(DEMO_ROW.fundamental_matrix)\n\nplot_two_paths(DEMO_PATH_1, DEMO_PATH_2, _titles=[f\"IMAGE ID 1: {DEMO_IMG_ID_1}\", f\"IMAGE ID 2: {DEMO_IMG_ID_2}\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:44:16.159796Z","iopub.execute_input":"2022-04-12T19:44:16.16036Z","iopub.status.idle":"2022-04-12T19:44:16.930534Z","shell.execute_reply.started":"2022-04-12T19:44:16.160321Z","shell.execute_reply":"2022-04-12T19:44:16.929852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img0_raw = cv2.resize(cv2.imread(DEMO_PATH_1, cv2.IMREAD_GRAYSCALE), (640, 480))\nimg1_raw = cv2.resize(cv2.imread(DEMO_PATH_2, cv2.IMREAD_GRAYSCALE), (640, 480))\nimg0 = torch.from_numpy(img0_raw)[None][None].cuda() / 255.\nimg1 = torch.from_numpy(img1_raw)[None][None].cuda() / 255.\nbatch = {'image0': img0, 'image1': img1}\n\n# Inference with LoFTR and get prediction\nwith torch.no_grad():\n    matcher(batch)\n    mkpts0 = batch['mkpts0_f'].cpu().numpy()\n    mkpts1 = batch['mkpts1_f'].cpu().numpy()\n    mconf = batch['mconf'].cpu().numpy()\n    \n    # Sorted\n    mkpts0 = mkpts0[np.argsort(mconf)]\n    mkpts1 = mkpts1[np.argsort(mconf)]\n    mconf = mconf[np.argsort(mconf)]\n    \n    # Remove Low Confidence\n    mkpts0 = mkpts0[np.where(mconf>0.8)]\n    mkpts1 = mkpts1[np.where(mconf>0.8)]\n    mconf = mconf[np.where(mconf>0.8)]\n    \n    # Normalize confidence.\n    mconf = (mconf - mconf.min()) / (mconf.max()-mconf.min()+1e-5)\n    \ncolor = cm.afmhot(mconf)\ntext = [\n    'LoFTR',\n    'Matches: {}'.format(len(mkpts0)),\n]\n\nprint(\"\\n... BEFORE MAGSAC ...\\n\")\nfig = make_matching_figure(cv2.resize(demo_img_1, (640,480)), cv2.resize(demo_img_2, (640,480)), mkpts0, mkpts1, color, text=text, dpi=200, )","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:44:16.932301Z","iopub.execute_input":"2022-04-12T19:44:16.932738Z","iopub.status.idle":"2022-04-12T19:44:18.968626Z","shell.execute_reply.started":"2022-04-12T19:44:16.932702Z","shell.execute_reply":"2022-04-12T19:44:18.96789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###################################################################################################################\n################################################# GET INLIERS ONLY ################################################\n###################################################################################################################\nF, inlier_mask = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, ransacReprojThreshold=0.5, confidence=0.999, maxIters=50_000)\n###################################################################################################################\n\ncolor = cm.afmhot(mconf[np.where(inlier_mask==True)[0]])\ntext = [\n    'LoFTR',\n    'Matches: {}'.format(len(mkpts0[np.where(inlier_mask==True)[0]])),\n]\n\nprint(\"\\n... AFTER MAGSAC ...\\n\")\nfig = make_matching_figure(cv2.resize(demo_img_1, (640,480)), cv2.resize(demo_img_2, (640,480)), mkpts0[np.where(inlier_mask==True)[0]], mkpts1[np.where(inlier_mask==True)[0]], color, text=text, dpi=200, )","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:44:18.969711Z","iopub.execute_input":"2022-04-12T19:44:18.970082Z","iopub.status.idle":"2022-04-12T19:44:20.738765Z","shell.execute_reply.started":"2022-04-12T19:44:18.970047Z","shell.execute_reply":"2022-04-12T19:44:20.738046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _s in TRAIN_SCENES:\n    DEMO_ROW = train_map[_s][\"pco_df\"][(0.1<train_map[_s][\"pco_df\"].covisibility) & (train_map[_s][\"pco_df\"].covisibility<0.3)].sample(1).reset_index().iloc[0]\n    print(f\"\\n\\n\\n\\n{_s} - {DEMO_ROW.covisibility}\\n\")\n    quadtree_get_F_w_plot(DEMO_ROW.image_path_1, DEMO_ROW.image_path_2, matcher, resize_to=(640,480), n_iter=50_000)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:54:40.111266Z","iopub.execute_input":"2022-04-12T19:54:40.111529Z","iopub.status.idle":"2022-04-12T19:55:19.294437Z","shell.execute_reply.started":"2022-04-12T19:54:40.111483Z","shell.execute_reply":"2022-04-12T19:55:19.293113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"model_baseline\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: black; background-color: #ffffff;\" id=\"dataset_exploration\">\n    5&nbsp;&nbsp;MODEL BASELINE&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---\n\n**Let's create a wrapper function that can take an image pair and will return an approximate F matrix so we can submit using the basic techniques illustrated by the hosts in this notebook --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?rvi=1**","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:12:49.194824Z","iopub.execute_input":"2022-03-09T16:12:49.195617Z","iopub.status.idle":"2022-03-09T16:12:49.202167Z","shell.execute_reply.started":"2022-03-09T16:12:49.195577Z","shell.execute_reply":"2022-03-09T16:12:49.200859Z"}}},{"cell_type":"code","source":"def quadtree_get_F(img_path_1, img_path_2, matcher, resize_to=(640,480),top_n=500, n_iter=50_000):\n    _img_1 = cv2.resize(cv2.imread(img_path_1, cv2.IMREAD_GRAYSCALE), resize_to)\n    _img_2 = cv2.resize(cv2.imread(img_path_2, cv2.IMREAD_GRAYSCALE), resize_to)\n    img_1 = torch.from_numpy(_img_1)[None][None].cuda()/255.\n    img_2 = torch.from_numpy(_img_2)[None][None].cuda()/255.\n    batch = {'image0': img_1, 'image1': img_2}\n\n    with torch.no_grad():\n        matcher(batch)\n        kps_1 = batch['mkpts0_f'].cpu().numpy()\n        kps_2 = batch['mkpts1_f'].cpu().numpy()\n        m_conf = batch['mconf'].cpu().numpy()\n            \n    # Remove Low Confidence\n    kps_1 = kps_1[np.argsort(m_conf)][:top_n]\n    kps_2 = kps_2[np.argsort(m_conf)][:top_n]\n    m_conf = m_conf[np.argsort(m_conf)][:top_n]\n    \n    # Normalize confidence.\n    if len(m_conf)>0:\n        m_conf = (m_conf - m_conf.min()) / (m_conf.max()-m_conf.min()+1e-5)\n            \n    F, inlier_mask = cv2.findFundamentalMat(kps_1, kps_2, cv2.USAC_MAGSAC, ransacReprojThreshold=0.5, confidence=0.999, maxIters=n_iter)\n    return \" \".join([f\"{x:.15f}\" for x in F.flatten()])","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:58:04.909851Z","iopub.execute_input":"2022-04-12T19:58:04.910114Z","iopub.status.idle":"2022-04-12T19:58:04.920267Z","shell.execute_reply.started":"2022-04-12T19:58:04.910087Z","shell.execute_reply":"2022-04-12T19:58:04.919276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_test_image_paths_1 = test_df.f_path_1.tolist()\nall_test_image_paths_2 = test_df.f_path_2.tolist()\nss_df[\"fundamental_matrix\"] = [quadtree_get_F(_path_1, _path_2, matcher) for _path_1, _path_2 in zip(all_test_image_paths_1, all_test_image_paths_2)]\n\ndisplay(ss_df)\nss_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T19:58:05.17439Z","iopub.execute_input":"2022-04-12T19:58:05.174646Z","iopub.status.idle":"2022-04-12T19:58:05.981808Z","shell.execute_reply.started":"2022-04-12T19:58:05.174613Z","shell.execute_reply":"2022-04-12T19:58:05.981165Z"},"trusted":true},"execution_count":null,"outputs":[]}]}