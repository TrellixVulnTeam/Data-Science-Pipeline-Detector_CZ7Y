{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# First of all\n\nI did Titanic, but I had no idea what to do after that, and somehow drifted off thinking that a theme related to images would be interesting.\nAnyway, I'm going to join and try to get into the rags.\nI hope that the code will reassure beginners like myself that there are others at this level.\nI would like to start by commenting on the code provided by the organizer to understand it.\nThank you in advance.\n\nTranslated with www.DeepL.com/Translator (free version)","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple\nfrom copy import deepcopy\nfrom tqdm import tqdm\nimport random\n\n# Check that you're using a recent OpenCV version.\nassert cv2.__version__ > '4.5', 'Please use OpenCV 4.5 or later.'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-12T10:07:46.784413Z","iopub.execute_input":"2022-04-12T10:07:46.785357Z","iopub.status.idle":"2022-04-12T10:07:47.143628Z","shell.execute_reply.started":"2022-04-12T10:07:46.785255Z","shell.execute_reply":"2022-04-12T10:07:47.142869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some useful functions and definitions. You can skip this for now.\n\n# A named tuple containing the intrinsics (calibration matrix K) and \n# extrinsics (rotation matrix R, translation vector T) for a given camera.\nGt = namedtuple('Gt', ['K', 'R', 'T'])\n\n# A small epsilon.\neps = 1e-15","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:07:47.145158Z","iopub.execute_input":"2022-04-12T10:07:47.145396Z","iopub.status.idle":"2022-04-12T10:07:47.150323Z","shell.execute_reply.started":"2022-04-12T10:07:47.145368Z","shell.execute_reply":"2022-04-12T10:07:47.149583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ReadCovisibilityData processing details**","metadata":{}},{"cell_type":"markdown","source":"* Open the file specified by filename, using a comma as the delimiter.\n* csv.reader reads one line at a time.\n* enumerate(reader) extracts the index (i) and element (row) of reader at the same time. However, skip the first line because it is a header.\n* I wasn't sure about enumerate, so I looked at it in detail.\n* Contents of row when i = 1: ['65581481_3524160597-65217461_5742419027', '0.8', '4.54154565e-03 3.31353808e-01 -1.73462054e+02 -6.25478740e-01 -1.24804165e-01 1.28764805e+03 4.05247003e+02 -1.20859857e+03 -1.93872545e+05']\n* row[0] when i = 1: '65581481_3524160597-65217461_5742419027'\n* row[1] when i = 1: '0.8'\n* Okay, I see what you mean about row representing an element. I thought, no, it's just as written.\n* So finally, covisibility_dict at i = 1 would be of dictionary type {'65581481_3524160597-65217461_5742419027': 0.8}\n* You mean a function that repeats this for the number of rows to get the covisibility of all images I don't know what covisibility is.\n\nTranslated with www.DeepL.com/Translator (free version)","metadata":{}},{"cell_type":"code","source":"def ReadCovisibilityData(filename):\n    covisibility_dict = {}\n    with open(filename) as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n            covisibility_dict[row[0]] = float(row[1])\n\n    return covisibility_dict","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:07:47.151405Z","iopub.execute_input":"2022-04-12T10:07:47.151909Z","iopub.status.idle":"2022-04-12T10:07:47.161402Z","shell.execute_reply.started":"2022-04-12T10:07:47.151876Z","shell.execute_reply":"2022-04-12T10:07:47.160591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def NormalizeKeypoints(keypoints, K):\n    C_x = K[0, 2]\n    C_y = K[1, 2]\n    f_x = K[0, 0]\n    f_y = K[1, 1]\n    keypoints = (keypoints - np.array([[C_x, C_y]])) / np.array([[f_x, f_y]])\n    return keypoints\n\n\ndef ComputeEssentialMatrix(F, K1, K2, kp1, kp2):\n    '''Compute the Essential matrix from the Fundamental matrix, \n    given the calibration matrices. Note that we ask participants to estimate F, \n    i.e., without relying on known intrinsics.'''\n    \n    # Warning! Old versions of OpenCV's RANSAC could return multiple F matrices, \n    #encoded as a single matrix size 6x3 or 9x3, rather than 3x3.\n    # We do not account for this here, as the modern RANSACs do not do this:\n    # https://opencv.org/evaluating-opencvs-new-ransacs\n    assert F.shape[0] == 3, 'Malformed F?'\n\n    # Use OpenCV's recoverPose to solve the cheirality check:\n    # https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#gadb7d2dfcc184c1d2f496d8639f4371c0\n    E = np.matmul(np.matmul(K2.T, F), K1).astype(np.float64)\n    \n    kp1n = NormalizeKeypoints(kp1, K1)\n    kp2n = NormalizeKeypoints(kp2, K2)\n    num_inliers, R, T, mask = cv2.recoverPose(E, kp1n, kp2n)\n\n    return E, R, T\n\n\ndef ArrayFromCvKps(kps):\n    '''Convenience function to convert OpenCV keypoints into a simple numpy array.'''\n    \n    return np.array([kp.pt for kp in kps])\n\n\ndef QuaternionFromMatrix(matrix):\n    '''Transform a rotation matrix into a quaternion.'''\n\n    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n    m00 = M[0, 0]\n    m01 = M[0, 1]\n    m02 = M[0, 2]\n    m10 = M[1, 0]\n    m11 = M[1, 1]\n    m12 = M[1, 2]\n    m20 = M[2, 0]\n    m21 = M[2, 1]\n    m22 = M[2, 2]\n\n    K = np.array([[m00 - m11 - m22, 0.0, 0.0, 0.0],\n              [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n              [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n              [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]])\n    K /= 3.0\n\n    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n    w, V = np.linalg.eigh(K)\n    q = V[[3, 0, 1, 2], np.argmax(w)]\n\n    if q[0] < 0:\n        np.negative(q, q)\n\n    return q\n\n\ndef ExtractSiftFeatures(image, detector, num_features):\n    '''Compute SIFT features for a given image.'''\n    \n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    kp, desc = detector.detectAndCompute(gray, None)\n    return kp[:num_features], desc[:num_features]\n\n\ndef ComputeErrorForOneExample(q_gt, T_gt, q, T, scale):\n    '''Compute the error metric for a single example.\n    \n    The function returns two errors, over rotation and translation. \n    These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n    \n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t\n\n\ndef ComputeMaa(err_q, err_t, thresholds_q, thresholds_t):\n    '''Compute the mean Average Accuracy at different tresholds, for one scene.'''\n    \n    assert len(err_q) == len(err_t)\n    \n    acc, acc_q, acc_t = [], [], []\n    for th_q, th_t in zip(thresholds_q, thresholds_t):\n        acc += [(np.bitwise_and(np.array(err_q) < th_q, np.array(err_t) < th_t)).sum() / len(err_q)]\n        acc_q += [(np.array(err_q) < th_q).sum() / len(err_q)]\n        acc_t += [(np.array(err_t) < th_t).sum() / len(err_t)]\n    return np.mean(acc), np.array(acc), np.array(acc_q), np.array(acc_t)\n\n\ndef BuildCompositeImage(im1, im2, axis=1, margin=0, background=1):\n    '''Convenience function to stack two images with different sizes.'''\n    \n    if background != 0 and background != 1:\n        background = 1\n    if axis != 0 and axis != 1:\n        raise RuntimeError('Axis must be 0 (vertical) or 1 (horizontal')\n\n    h1, w1, _ = im1.shape\n    h2, w2, _ = im2.shape\n\n    if axis == 1:\n        composite = np.zeros((max(h1, h2), w1 + w2 + margin, 3), dtype=np.uint8) + 255 * background\n        if h1 > h2:\n            voff1, voff2 = 0, (h1 - h2) // 2\n        else:\n            voff1, voff2 = (h2 - h1) // 2, 0\n        hoff1, hoff2 = 0, w1 + margin\n    else:\n        composite = np.zeros((h1 + h2 + margin, max(w1, w2), 3), dtype=np.uint8) + 255 * background\n        if w1 > w2:\n            hoff1, hoff2 = 0, (w1 - w2) // 2\n        else:\n            hoff1, hoff2 = (w2 - w1) // 2, 0\n        voff1, voff2 = 0, h1 + margin\n    composite[voff1:voff1 + h1, hoff1:hoff1 + w1, :] = im1\n    composite[voff2:voff2 + h2, hoff2:hoff2 + w2, :] = im2\n\n    return (composite, (voff1, voff2), (hoff1, hoff2))\n\n\ndef DrawMatches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n    '''Draw keypoints and matches.'''\n    \n    composite, v_offset, h_offset = BuildCompositeImage(im1, im2, axis, margin, background)\n\n    # Draw all keypoints.\n    for coord_a, coord_b in zip(kp1, kp2):\n        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n    \n    # Draw matches, and highlight keypoints used in matches.\n    for idx_a, idx_b in matches:\n        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.line(composite,\n                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n                                   int(kp1[idx_a][1] + v_offset[0])]),\n                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n    return composite\n\n\ndef LoadCalibration(filename):\n    '''Load calibration data (ground truth) from the csv file.'''\n    \n    calib_dict = {}\n    with open(filename, 'r') as f:\n        reader = csv.reader(f, delimiter=',')\n        for i, row in enumerate(reader):\n            # Skip header.\n            if i == 0:\n                continue\n\n            camera_id = row[0]\n            K = np.array([float(v) for v in row[1].split(' ')]).reshape([3, 3])\n            R = np.array([float(v) for v in row[2].split(' ')]).reshape([3, 3])\n            T = np.array([float(v) for v in row[3].split(' ')])\n            calib_dict[camera_id] = Gt(K=K, R=R, T=T)\n    \n    return calib_dict","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:07:47.163214Z","iopub.execute_input":"2022-04-12T10:07:47.16359Z","iopub.status.idle":"2022-04-12T10:07:47.311082Z","shell.execute_reply.started":"2022-04-12T10:07:47.163515Z","shell.execute_reply":"2022-04-12T10:07:47.310046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* os.scandir(src): The list of directories directly under the directory of src is stored as an iterator.\n* f.is_dir: if f is the correct directory (as is)\n* os.path.split(f): splits a pathname into (head, tail) pairs, where tail is the end of the pathname and head is the part before it. One content is produced, ('...'). /input/image-matching-challenge-2022/train', 'british_museum'). os.path.split(f)[0] will extract the head part. os.path.split(f)[-1] will extract the tail part can be retrieved by writing os.path.split(f)[-1]. Here, [-1] is written to get the file name.\n\nTranslated with www.DeepL.com/Translator (free version)","metadata":{}},{"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory.\n\nsrc = '../input/image-matching-challenge-2022/train'\n\nval_scenes = []\nfor f in os.scandir(src):\n    if f.is_dir():\n        cur_scene = os.path.split(f)[-1]\n        print(f'Found scene \"{cur_scene}\"\" at {f.path}')\n        val_scenes += [cur_scene]","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:07:47.312035Z","iopub.execute_input":"2022-04-12T10:07:47.312267Z","iopub.status.idle":"2022-04-12T10:07:47.334351Z","shell.execute_reply.started":"2022-04-12T10:07:47.312241Z","shell.execute_reply":"2022-04-12T10:07:47.333564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"val_scenesの確認。フォルダ名がちゃんとありますね。","metadata":{}},{"cell_type":"code","source":"print(val_scenes)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:07:47.336939Z","iopub.execute_input":"2022-04-12T10:07:47.337314Z","iopub.status.idle":"2022-04-12T10:07:47.342088Z","shell.execute_reply.started":"2022-04-12T10:07:47.337268Z","shell.execute_reply":"2022-04-12T10:07:47.341433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **What we are doing in the first half of the for statement**","metadata":{}},{"cell_type":"markdown","source":"* os.path.splitxt(filename): gets filename as a tuple value split into non-extension and extension parts. os.path.splitxt(filename)[0] is the path + filename, os.path.splitxt( filename)[-1] is the extension.\n* os.path.basename(pathname): Extracts the filename portion at the end of pathname.\n* os.path.basename(os.path.splittext(filename)[0]): to get only the filename without the extension.\n* cv2.cvtColor: I heard that if you want to use OpenCV, you have to convert colors from standard RGB to BGR. I don't know what's in it, but okay.\n* The for statement in the first half creates a dictionary of images.\nThe images in piazza_san_macro show how many are in the folder.","metadata":{}},{"cell_type":"markdown","source":"# **What we are doing in the latter half of the for statement: displaying image samples in the num_rows rows num_cols columns**","metadata":{}},{"cell_type":"code","source":"# Each scene in the validation set contains a list of images, poses, and pairs. Let's pick one and look at some images.\n\nscene = 'piazza_san_marco'\n\nimages_dict = {}\nfor filename in glob(f'{src}/{scene}/images/*.jpg'):\n    cur_id = os.path.basename(os.path.splitext(filename)[0])\n\n    # OpenCV expects BGR, but the images are encoded in standard RGB, so you need to do color conversion if you use OpenCV for I/O.\n    images_dict[cur_id] = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n\nprint(f'Loaded {len(images_dict)} images.')\n\nnum_rows = 6\nnum_cols = 4\nf, axes = plt.subplots(num_rows, num_cols, figsize=(20, 20), constrained_layout=True)\nfor i, key in enumerate(images_dict):\n    if i >= num_rows * num_cols:\n        break\n    cur_ax = axes[i % num_rows, i // num_rows]\n    cur_ax.imshow(images_dict[key])\n    cur_ax.set_title(key)\n    cur_ax.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:07:47.342962Z","iopub.execute_input":"2022-04-12T10:07:47.34365Z","iopub.status.idle":"2022-04-12T10:07:54.868583Z","shell.execute_reply.started":"2022-04-12T10:07:47.343619Z","shell.execute_reply":"2022-04-12T10:07:54.867791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ReadCovisibilityData**","metadata":{}},{"cell_type":"markdown","source":"Open the file pair_covisibility.csv in the folder \"piazza_san_marco\" and for every image get the dictionary type {covisibility pair name: covisibility value} and store it in covisibility_dict covisibility_dict.","metadata":{}},{"cell_type":"markdown","source":"* The dictionary type is in the form {key: value}, and all keys and values can be obtained by using the method .items(), which is in the form [(key1, value1), (key2, value2), ... , (keyn, valuen)] in tuple form.\n* So this is how the dictionary type is used. I didn't know that.\n* The rest is using comprehension notation to get key(k) whose value(v) is greater than 0.7. I think I remember that if the covisibility is large, the two images are very similar, so it's easy to relate them? (hazy memory) sorry if I'm wrong.","metadata":{}},{"cell_type":"code","source":"# Two images from the same scene may not always overlap.\n# The dataset contains co-visibility estimates that you can use to find pairs with more or less overlap.\n# We recommend using all pairs with a co-visibility estimate of 0.1 or larger.\n# For more details, please see Section 3.2 of the paper: https://arxiv.org/abs/2003.01587.\n\ncovisibility_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')\n\n# Let's look at easy pairs first, and difficult pairs later.\neasy_subset = [k for k, v in covisibility_dict.items() if v >= 0.7]\ndifficult_subset = [k for k, v in covisibility_dict.items() if v >= 0.1 and v < 0.2]","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:07:54.870572Z","iopub.execute_input":"2022-04-12T10:07:54.871387Z","iopub.status.idle":"2022-04-12T10:07:54.924973Z","shell.execute_reply.started":"2022-04-12T10:07:54.87135Z","shell.execute_reply":"2022-04-12T10:07:54.924046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we look at easy_subset and difficult_subset.\nBoth are of type list, but seem to have different sizes.","metadata":{}},{"cell_type":"code","source":"print(len(easy_subset))\nprint(type(easy_subset))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:07:54.925997Z","iopub.execute_input":"2022-04-12T10:07:54.926206Z","iopub.status.idle":"2022-04-12T10:07:54.930934Z","shell.execute_reply.started":"2022-04-12T10:07:54.92618Z","shell.execute_reply":"2022-04-12T10:07:54.930114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(difficult_subset))\nprint(type(difficult_subset))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:07:54.932214Z","iopub.execute_input":"2022-04-12T10:07:54.932443Z","iopub.status.idle":"2022-04-12T10:07:54.942306Z","shell.execute_reply.started":"2022-04-12T10:07:54.932406Z","shell.execute_reply":"2022-04-12T10:07:54.941467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **The first for statement**","metadata":{}},{"cell_type":"markdown","source":"* [easy_subset, difficult_subset] can be written as a LIST with LIST as an element. It is like nesting.\n* The size of easy_subset is 8, and the size of difficult_subset is 534.\n* So you can put them in the same list, even though they are lists of different sizes. It seems strange, but I guess it is possible because both variables are the same as a single list type variable.\n* i moves from 0 to 1, subset will move from 0 to 7 when i=0, and from 0 to 533 when i=1.","metadata":{}},{"cell_type":"markdown","source":"# **Second for statement**","metadata":{}},{"cell_type":"markdown","source":"* Both easy_subset and difficult_subset pick up only 4 samples from 0-3rd.\n* Adjust the plots to fit nicely in figure using 1 row, 2 columns, 15x10 inches, constrained layout. Create such f and axes.\n* .split('-') would be an instruction to split the string with a hyphen. I haven't looked it up, but it's atmospheric.\n* imshow axes[0] with easy_subset and axes[1] with difficult_subset graph. Easy!\n* Finally, create a histogram for Covisibility, where bins is the number of bins in the graph, which means how many elements it is divided into.\n* So the histogram shows that there are overwhelmingly more images with smaller covisibilty. I wonder if this is an indication that the dataset has fewer corresponding parts in the image pairs.","metadata":{}},{"cell_type":"code","source":"for i, subset in enumerate([easy_subset, difficult_subset]):\n    print(f'Pairs from an {\"easy\" if i == 0 else \"difficult\"} subset')\n    \n    for pair in subset[:4]:\n        # A pair string is simply two concatenated image IDs, separated with a hyphen.\n        image_id_1, image_id_2 = pair.split('-')\n\n        f, axes = plt.subplots(1, 2, figsize=(15, 10), constrained_layout=True)\n        axes[0].imshow(images_dict[image_id_1])\n        axes[0].set_title(image_id_1)\n        axes[1].imshow(images_dict[image_id_2])\n        axes[1].set_title(image_id_2)\n        for ax in axes:\n            ax.axis('off')\n        plt.show()\n\n    print()\n    print()\n\nfig = plt.figure(figsize=(15, 10), constrained_layout=True)\nplt.title('Covisibility histogram')\nplt.hist(list(covisibility_dict.values()), bins=10, range=[0, 1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:07:54.943642Z","iopub.execute_input":"2022-04-12T10:07:54.944533Z","iopub.status.idle":"2022-04-12T10:08:00.822284Z","shell.execute_reply.started":"2022-04-12T10:07:54.944447Z","shell.execute_reply":"2022-04-12T10:08:00.821667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **About cv2.SIFT_creat function**","metadata":{}},{"cell_type":"markdown","source":"I found more information about it here. Thanks. (This sites are written by Japanese)　\n* https://qiita.com/tatsuya11bbs/items/225634d352de09206a8f\n* http://labs.eecs.tottori-u.ac.jp/sd/Member/oyamada/OpenCV/html/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html","metadata":{}},{"cell_type":"markdown","source":"# What is SIFT?","metadata":{}},{"cell_type":"markdown","source":"* Scale-Invariant Feature Transform by DeepL\n* Detects feature points and describes feature values\n* The algorithm is characterized by its robustness to scaling, rotation, and illumination change.","metadata":{}},{"cell_type":"markdown","source":"In other words, is it correct to say that it is a function that finds good features in an image and outputs them...?","metadata":{}},{"cell_type":"markdown","source":"# About the clock-like symbols in the output results","metadata":{}},{"cell_type":"markdown","source":"We looked at the clock-like circles with various colors on the output to see what they indicated.","metadata":{}},{"cell_type":"markdown","source":"* Circle size: Scale size of the feature point\n* Direction of the hands of a clock: Rotation angle of the feature point","metadata":{}},{"cell_type":"markdown","source":"It was written, \"What is a scale? What is a rotation angle? I was not sure. I looked into these questions and found that","metadata":{}},{"cell_type":"markdown","source":"* I want to create a function that can find feature points in a generic way for various image sizes.\n* However, even if the function can find image edges when the image size is small, when the image size is enlarged, the gradient becomes smaller and it becomes difficult to find edges.\n* Therefore, to make the algorithm robust against scaling (expansion and contraction), we would like to use a small window size for small images and a large window size for large images.\n* Specifically, they want to use Laplacian of Gaussian (LoG) with varying values of standard deviation σ. When σ is small, the algorithm will respond to small edges, and when σ is large, the algorithm will respond to large edges and return large values.\n* Consider the three dimensions of (x, y, σ) with scale σ added for a point (x, y) in the image and compute the gradient for σ.\n* By finding the point (x, y) that is the maximum in the gradient of σ, we can find the feature points that are edges in an image of any size.\n* Rotation angle is the direction in which the intensity and direction of the gradient in the neighborhood of the point where the gradient of σ reaches its maximum value (and 80% or more of the maximum value) is calculated.\n* In other words, the direction of the hands of a clock is the direction in which the gradient of the scale σ is greatest.","metadata":{}},{"cell_type":"markdown","source":"In summary.","metadata":{}},{"cell_type":"markdown","source":"* The point in the image where the small clock is drawn (the center point of the circle) has a characteristic point that is an edge in a small area.\n* The opposite is true for large clocks.\n* The direction of the clock hand indicates in which direction it is an edge (maximum gradient of σ).","metadata":{}},{"cell_type":"markdown","source":"I am sure there are some points that I have not researched or understand well enough, but please point out if I am wrong. (Or rather, I gave up halfway through because of the difficulty of the content)","metadata":{}},{"cell_type":"markdown","source":"# Confirmation of SIFT operation with home-made data","metadata":{}},{"cell_type":"markdown","source":"First, I wanted to see if it would work with my own photos, so I did it.","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nsample_src = '../input/sample7/IMG-2642_half2.jpg'\nsample_image = np.array(Image.open(sample_src))\n#image_sample = cv2.cvtColor(cv2.imread(sample_src), cv2.COLOR_BGR2RGB)\n\nnum_features = 5000\nsift_detector = cv2.SIFT_create(num_features, contrastThreshold=-10000, edgeThreshold=-10000)\nkeypoints, descriptors = ExtractSiftFeatures(sample_image, sift_detector, num_features)\n\nprint(f'Computed {len(keypoints)} features.')\n\nimage_with_keypoints = cv2.drawKeypoints(sample_image, keypoints, outImage=None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nfig = plt.figure(figsize=(15, 10))\nplt.imshow(image_with_keypoints)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:08:00.823626Z","iopub.execute_input":"2022-04-12T10:08:00.824574Z","iopub.status.idle":"2022-04-12T10:08:01.728196Z","shell.execute_reply.started":"2022-04-12T10:08:00.824527Z","shell.execute_reply":"2022-04-12T10:08:01.727546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perhaps because it is a high quality image taken with an iPhone, many feature points so small that they cannot be seen seem to have been detected.","metadata":{}},{"cell_type":"code","source":"# We can find correspondences by brute-force-matching local features between two images. Let's do this for an easy pair.\nsample1_src = '../input/sample7/IMG-2641_half2.jpg'\nsample2_src = '../input/sample7/IMG-2642_half2.jpg'\nsample1_image = np.array(Image.open(sample1_src))\nsample2_image = np.array(Image.open(sample2_src))\n\nkeypoints_1, descriptors_1 = ExtractSiftFeatures(sample1_image, sift_detector, num_features)\nkeypoints_2, descriptors_2 = ExtractSiftFeatures(sample2_image, sift_detector, num_features)\n\n# For each descriptor on one image, find the closest descriptor on the other image.\n# With crossCheck=True we keep only bidirectional matches (i.e., two features are nearest neighbours from A to B and also from B to A).\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n\n# Compute matches.\ncv_matches = bf.match(descriptors_1, descriptors_2)\n\n# Convert keypoints and matches to something more human-readable.\ncur_kp_1 = ArrayFromCvKps(keypoints_1)\ncur_kp_2 = ArrayFromCvKps(keypoints_2)\nmatches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n\n# Plot the brute-force matches.\nim_matches = DrawMatches(sample1_image, sample2_image, cur_kp_1, cur_kp_2, matches)\nfig = plt.figure(figsize=(25, 25))\nplt.title('Matches before RANSAC')\nplt.imshow(im_matches)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:08:01.729276Z","iopub.execute_input":"2022-04-12T10:08:01.729595Z","iopub.status.idle":"2022-04-12T10:08:03.849965Z","shell.execute_reply.started":"2022-04-12T10:08:01.729568Z","shell.execute_reply":"2022-04-12T10:08:03.848956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **RANSAC Algorithm**","metadata":{}},{"cell_type":"markdown","source":"It's very messy lol. I think there are some points, like the desk point, that are not supported, so it looks like this will be optimized using the next RANSAC.","metadata":{}},{"cell_type":"markdown","source":"Explanation of the RANSAC algorithm: I studied the following site. It was easy to understand.","metadata":{}},{"cell_type":"markdown","source":"* https://qiita.com/smurakami/items/14202a83bd13e55d4c09\n* http://www.sanko-shoko.net/note.php?id=rcpj\n* https://hkawabata.github.io/technical-note/note/ML/ransac.html","metadata":{}},{"cell_type":"markdown","source":"To quote the description.","metadata":{}},{"cell_type":"markdown","source":"1. randomly select a \"small\" number of samples from the data set, more than the number needed to determine the model\n2. derive an ad-hoc model from the \"few\" samples obtained, e.g., by the least-squares method\n3. fit the temporary model to the data, and if there are not so many outliers, add it to the \"correct model candidates\n4. repeat 2-3 several times\nAmong the obtained \"correct model candidates,\" the one that best matches the data is selected as the true model","metadata":{}},{"cell_type":"markdown","source":"So I felt that this is a robust estimation method that makes intuitive sense. I thought it was great that someone thought of this.","metadata":{}},{"cell_type":"code","source":"# Notice that this includes many outliers. We can filter them with a state-of-the-art RANSAC algorithm. References:\n# * https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#ga59b0d57f46f8677fb5904294a23d404a\n# * https://opencv.org/evaluating-opencvs-new-ransacs\n\n# OpenCV gives us the Fundamental matrix after RANSAC, and a mask over the input matches. The solution is clearly much cleaner, even though it may still contain outliers.\n# This F is the prediction you'll submit to the contest.\nF, inlier_mask = cv2.findFundamentalMat(cur_kp_1[matches[:, 0]], cur_kp_2[matches[:, 1]], cv2.USAC_MAGSAC, ransacReprojThreshold=3, confidence=0.99999, maxIters=10000)\n\nmatches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\nim_inliers = DrawMatches(sample1_image, sample2_image, cur_kp_1, cur_kp_2, matches_after_ransac)\nfig = plt.figure(figsize=(25, 25))\nplt.title('Matches after RANSAC')\nplt.imshow(im_inliers)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:08:03.851308Z","iopub.execute_input":"2022-04-12T10:08:03.851527Z","iopub.status.idle":"2022-04-12T10:08:04.674932Z","shell.execute_reply.started":"2022-04-12T10:08:03.8515Z","shell.execute_reply":"2022-04-12T10:08:04.673911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hmmm, I wonder if I'm getting correspondence. I was expecting a lot of correspondence lines to be drawn on the cup marks, but it doesn't look like they will be.","metadata":{}},{"cell_type":"markdown","source":"The keyboard in the back seems to correspond quite well. I didn't intend to, but the contrast is clear (large gradient) and it's linear, so I guess it was easy to tell.","metadata":{}},{"cell_type":"markdown","source":"Well, so much for the homemade data, let's look at the actual data set.","metadata":{}},{"cell_type":"code","source":"# The task is finding the relative geometry (rotation, translation) between the two cameras.\n# You can read more about epipolar geometry here: https://en.wikipedia.org/wiki/Epipolar_geometry\n\n# This problem is typically (but not always!) solved with sparse features.\n# Let's try using SIFT, a seminal work in computer vision (https://en.wikipedia.org/wiki/Scale-invariant_feature_transform).\n# No longer the state of the art, but still pretty solid!\n\nnum_features = 5000\n\n# You may want to lower the detection threshold, as small images may not be able to reach the budget otherwise.\n# Note that you may actually get more than num_features features, as a feature for one point can have multiple orientations (this is rare).\nsift_detector = cv2.SIFT_create(num_features, contrastThreshold=-10000, edgeThreshold=-10000)\n\nkeys = list(images_dict.keys())\nkeypoints, descriptors = ExtractSiftFeatures(images_dict[keys[0]], sift_detector, num_features)\nprint(f'Computed {len(keypoints)} features.')\n\n# Each local feature contains a keypoint (xy, possibly scale, possibly orientation) and a description vector (128-dimensional for SIFT).\nimage_with_keypoints = cv2.drawKeypoints(images_dict[keys[0]], keypoints, outImage=None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nfig = plt.figure(figsize=(15, 15))\nplt.imshow(image_with_keypoints)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:21:19.953467Z","iopub.execute_input":"2022-04-12T10:21:19.954031Z","iopub.status.idle":"2022-04-12T10:21:20.928173Z","shell.execute_reply.started":"2022-04-12T10:21:19.95399Z","shell.execute_reply":"2022-04-12T10:21:20.925807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We can find correspondences by brute-force-matching local features between two images. Let's do this for an easy pair.\n\npair = easy_subset[0]\nimage_id_1, image_id_2 = pair.split('-')\nkeypoints_1, descriptors_1 = ExtractSiftFeatures(images_dict[image_id_1], sift_detector, 2000)\nkeypoints_2, descriptors_2 = ExtractSiftFeatures(images_dict[image_id_2], sift_detector, 2000)\n\n# For each descriptor on one image, find the closest descriptor on the other image.\n# With crossCheck=True we keep only bidirectional matches (i.e., two features are nearest neighbours from A to B and also from B to A).\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n\n# Compute matches.\ncv_matches = bf.match(descriptors_1, descriptors_2)\n\n# Convert keypoints and matches to something more human-readable.\ncur_kp_1 = ArrayFromCvKps(keypoints_1)\ncur_kp_2 = ArrayFromCvKps(keypoints_2)\nmatches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n\n# Plot the brute-force matches.\nim_matches = DrawMatches(images_dict[image_id_1], images_dict[image_id_2], cur_kp_1, cur_kp_2, matches)\nfig = plt.figure(figsize=(25, 25))\nplt.title('Matches before RANSAC')\nplt.imshow(im_matches)\nplt.axis('off')\nplt.show()\n\n# Notice that this includes many outliers. We can filter them with a state-of-the-art RANSAC algorithm. References:\n# * https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#ga59b0d57f46f8677fb5904294a23d404a\n# * https://opencv.org/evaluating-opencvs-new-ransacs\n\n# OpenCV gives us the Fundamental matrix after RANSAC, and a mask over the input matches. The solution is clearly much cleaner, even though it may still contain outliers.\n# This F is the prediction you'll submit to the contest.\nF, inlier_mask = cv2.findFundamentalMat(cur_kp_1[matches[:, 0]], cur_kp_2[matches[:, 1]], cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, confidence=0.99999, maxIters=10000)\n\nmatches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\nim_inliers = DrawMatches(images_dict[image_id_1], images_dict[image_id_2], cur_kp_1, cur_kp_2, matches_after_ransac)\nfig = plt.figure(figsize=(25, 25))\nplt.title('Matches before RANSAC')\nplt.imshow(im_inliers)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:21:36.687891Z","iopub.execute_input":"2022-04-12T10:21:36.688275Z","iopub.status.idle":"2022-04-12T10:21:39.195186Z","shell.execute_reply.started":"2022-04-12T10:21:36.688233Z","shell.execute_reply":"2022-04-12T10:21:39.193904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What do you think? The roof looks like a pretty good correspondence, but the rest of the pattern seems subtle. It seems to me that it would be difficult to distinguish between the same pattern going on.","metadata":{}},{"cell_type":"markdown","source":"Next it looks like we will see the data of the answer. (ground truth means something like truthful answers, right?)","metadata":{}},{"cell_type":"code","source":"# Is this any good? Let's load the ground truth.\n\ncalib_dict = LoadCalibration(f'{src}/{scene}/calibration.csv')\nprint(f'Loded ground truth data for {len(calib_dict)} images')\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:23:21.35866Z","iopub.execute_input":"2022-04-12T10:23:21.358939Z","iopub.status.idle":"2022-04-12T10:23:21.382414Z","shell.execute_reply.started":"2022-04-12T10:23:21.358911Z","shell.execute_reply":"2022-04-12T10:23:21.381712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next it looks like you are calculating scaling_factor, I guess scaling factor is sigma. I see from the organizer's post that","metadata":{}},{"cell_type":"markdown","source":"The poses for each scene where reconstructed via Structure-from-Motion, and are only accurate up to a scaling factor. This file contains a scalar for each scene which can be used to convert them to meters. For code examples, please refer to this notebook.","metadata":{}},{"cell_type":"markdown","source":"It was written. You wrote that this scalar can convert each image to units of meters.","metadata":{}},{"cell_type":"markdown","source":"I guess I misunderstood you. I guess you are saying that by using this coefficient for the photos in the folder of each tourist attraction, it means that the pixel distance is directly converted to real world distance. No. I mean, it's written in the comments.","metadata":{}},{"cell_type":"code","source":"# One important caveat: the scenes were reconstructed from unstructured image collections using Structure-from-Motion (http://colmap.github.io), and are not up to \"real-world\" scale (i.e. meters, or inches).\n# We computed a scaling factor per scene to correct this. This is necessary to compute the metric correctly.\nscaling_dict = {}\nwith open(f'{src}/scaling_factors.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        scaling_dict[row[0]] = float(row[1])\n\nprint(f'Scaling factors: {scaling_dict}')\nprint()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T10:24:24.354753Z","iopub.execute_input":"2022-04-12T10:24:24.355055Z","iopub.status.idle":"2022-04-12T10:24:24.366534Z","shell.execute_reply.started":"2022-04-12T10:24:24.355023Z","shell.execute_reply":"2022-04-12T10:24:24.365675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm not sure if this is true, but I'll use it anyway.","metadata":{}},{"cell_type":"markdown","source":"I don't know what's going on, but when I run this program, it seems to have an error with the ground truth.","metadata":{}},{"cell_type":"code","source":"# We can compute the errors now. First, let's decompose the Fundamental matrix we just estimated. TODO explain why we do this.\ninlier_kp_1 = ArrayFromCvKps([kp for i, kp in enumerate(keypoints_1) if i in matches_after_ransac[:, 0]])\ninlier_kp_2 = ArrayFromCvKps([kp for i, kp in enumerate(keypoints_2) if i in matches_after_ransac[:, 1]])\nE, R, T = ComputeEssentialMatrix(F, calib_dict[image_id_1].K, calib_dict[image_id_2].K, inlier_kp_1, inlier_kp_2)\nq = QuaternionFromMatrix(R)\nT = T.flatten()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T11:24:14.074363Z","iopub.execute_input":"2022-04-12T11:24:14.076095Z","iopub.status.idle":"2022-04-12T11:24:14.113152Z","shell.execute_reply.started":"2022-04-12T11:24:14.076017Z","shell.execute_reply":"2022-04-12T11:24:14.112285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Well, as I recall, F is the fundamental matrix, E is the basic matrix (what's the difference between basic and fundamental?), and R and T are external camera parameters.","metadata":{}},{"cell_type":"markdown","source":"**Hmm, what's that? It just came out of nowhere!**","metadata":{}},{"cell_type":"markdown","source":"Quaternion (apparently read \"quaternion\")?　What is this?","metadata":{}},{"cell_type":"markdown","source":"# Quaternion: \"Attitude\" of a 3D object","metadata":{}},{"cell_type":"markdown","source":"It seems to mean. Here, \"posture\" seems to mean the direction in which an object is oriented? It seems that \"posture\" is an expression of how an object is oriented. It is difficult.","metadata":{}},{"cell_type":"markdown","source":"It seems that the quaternion is represented by a four-dimensional vector, with x, y, and z representing the direction of the axis, and the last θ representing the rotation of the axis, which is quicker to see in a diagram or something. The diagram and explanation in the link below was easy to understand.","metadata":{}},{"cell_type":"markdown","source":"https://www.acuity-inc.co.jp/pickups/knowhow/docs/20171225/","metadata":{}},{"cell_type":"markdown","source":"Well, I'm not sure what it is, but it means that you calculated the \"attitude\" from the camera's external parameter R. What is the posture? The camera's posture? Or the attitude of the object in the image? I don't know.","metadata":{}},{"cell_type":"markdown","source":"https://jpn.nec.com/rd/people/docs/doctoral_thesis_nakano.pdf","metadata":{}},{"cell_type":"markdown","source":"You can see it in the paper above.","metadata":{}},{"cell_type":"markdown","source":"> The rotation matrix R and the translation vector t, which describe the camera motion, are called extrinsic parameters. In Japanese, the rotation matrix is also called attitude and the translation vector is called position, and the two together are sometimes referred to as the camera position and attitude.","metadata":{}},{"cell_type":"markdown","source":"So, since it said that q was obtained from the camera rotation matrix R, this quaternion represents the \"camera attitude\"! I see!","metadata":{}},{"cell_type":"markdown","source":"All right, let's move on.","metadata":{}},{"cell_type":"code","source":"# Get the ground truth relative pose difference for this pair of images.\nR1_gt, T1_gt = calib_dict[image_id_1].R, calib_dict[image_id_1].T.reshape((3, 1))\nR2_gt, T2_gt = calib_dict[image_id_2].R, calib_dict[image_id_2].T.reshape((3, 1))\ndR_gt = np.dot(R2_gt, R1_gt.T)\ndT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\nq_gt = QuaternionFromMatrix(dR_gt)\nq_gt = q_gt / (np.linalg.norm(q_gt) + eps)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T11:36:55.396492Z","iopub.execute_input":"2022-04-12T11:36:55.39731Z","iopub.status.idle":"2022-04-12T11:36:55.407503Z","shell.execute_reply.started":"2022-04-12T11:36:55.397267Z","shell.execute_reply":"2022-04-12T11:36:55.406666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, it looks like the LoadCalibration function is storing the answer information from the calibration.csv file into the respective variables. Likewise, you said you are calculating the quaternion from R.","metadata":{}},{"cell_type":"markdown","source":"Then the ComputeErrorForOneExample function calculates the error between the attitude (quaternion q) and the ground truth of the translation vector t.","metadata":{}},{"cell_type":"code","source":"# Given ground truth and prediction, compute the error for the example above.\nerr_q, err_t = ComputeErrorForOneExample(q_gt, dT_gt, q, T, scaling_dict[scene])\nprint(f'Pair \"{pair}, rotation_error={err_q:.02f} (deg), translation_error={err_t:.02f} (m)', flush=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T11:43:18.593153Z","iopub.execute_input":"2022-04-12T11:43:18.59402Z","iopub.status.idle":"2022-04-12T11:43:18.601139Z","shell.execute_reply.started":"2022-04-12T11:43:18.593968Z","shell.execute_reply":"2022-04-12T11:43:18.600163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I didn't see any processing inside the function or anything, but that's okay ().","metadata":{}},{"cell_type":"markdown","source":"And you want to loop what we have done so far for all the SCENES.","metadata":{}},{"cell_type":"code","source":"# Let's iterate over all the scenes now. Some are much larger than others -- note that the number of pairs increases quadratically with the number of images.\n# We compute the metric for each scene, and then average it over all scenes.\n# For a quick experiment, we cap the number of image pairs for each scene to 50, and show one qualitative example per scene.\n\nshow_images = True\nnum_show_images = 1\nmax_pairs_per_scene = 50\nverbose = True\n\n# We use two different sets of thresholds over rotation and translation. Do not change this -- these are the values used by the scoring back-end.\nthresholds_q = np.linspace(1, 10, 10)\nthresholds_t = np.geomspace(0.2, 5, 10)\n\n# Save the per-sample errors and the accumulated metric to dictionaries, for later inspection.\nerrors = {scene: {} for scene in scaling_dict.keys()}\nmAA = {scene: {} for scene in scaling_dict.keys()}\n\n# Instantiate the matcher.\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:02:01.095297Z","iopub.execute_input":"2022-04-12T12:02:01.095633Z","iopub.status.idle":"2022-04-12T12:02:01.104233Z","shell.execute_reply.started":"2022-04-12T12:02:01.095583Z","shell.execute_reply":"2022-04-12T12:02:01.103439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems to instantiate Matcher by assigning various initial values, setting the Q and t thresholds, and preparing a dictionary to keep a history of errors. I don't understand that last part at all, so I'm going to go back and look at the function a bit.","metadata":{}},{"cell_type":"markdown","source":"http://labs.eecs.tottori-u.ac.jp/sd/Member/oyamada/OpenCV/html/py_tutorials/py_feature2d/py_matcher/py_matcher.html によると","metadata":{}},{"cell_type":"markdown","source":"> Brute-force matcher is simple. It computes the feature descriptor of a feature point in the first image and matches it with the feature values of all feature points in the second image based on some distance calculation. The feature point corresponding to the feature point with the smallest distance is returned as the matching result.","metadata":{}},{"cell_type":"markdown","source":"So it seems to be a function that matches the feature points of the first and second images by some distance calculation. I see.","metadata":{}},{"cell_type":"markdown","source":"The for statement from here on down is probably just shoving what we've done so far into the for minutes. I haven't looked at it.","metadata":{}},{"cell_type":"code","source":"for scene in scaling_dict.keys():\n    # Load all pairs, find those with a co-visibility over 0.1, and subsample them.\n    covisibility_dict = ReadCovisibilityData(f'{src}/{scene}/pair_covisibility.csv')    \n    pairs = [pair for pair, covis in covisibility_dict.items() if covis >= 0.1]\n    \n    print(f'-- Processing scene \"{scene}\": found {len(pairs)} pairs (will keep {min(len(pairs), max_pairs_per_scene)})', flush=True)\n    \n    # Subsample the pairs. Note that they are roughly sorted by difficulty (easy ones first), so we shuffle them beforehand: results would be misleading otherwise.\n    random.shuffle(pairs)\n    pairs = pairs[:max_pairs_per_scene]\n    \n    # Extract the images in these pairs (we don't need to load images we will not use).\n    ids = []\n    for pair in pairs:\n        cur_ids = pair.split('-')\n        assert cur_ids[0] > cur_ids[1]\n        ids += cur_ids\n    ids = list(set(ids))\n    \n    # Load ground truth data.\n    calib_dict = LoadCalibration(f'{src}/{scene}/calibration.csv')\n    \n    # Load images and extract SIFT features.\n    images_dict = {}\n    kp_dict = {}\n    desc_dict = {}\n    print('Extracting features...')\n    for id in tqdm(ids):\n        images_dict[id] = cv2.cvtColor(cv2.imread(f'{src}/{scene}/images/{id}.jpg'), cv2.COLOR_BGR2RGB)\n        kp_dict[id], desc_dict[id] = ExtractSiftFeatures(images_dict[id], sift_detector, 2000)\n    print()\n    print(f'Extracted features for {len(kp_dict)} images (avg: {np.mean([len(v) for v in desc_dict.values()])})')\n\n    # Process the pairs.\n    max_err_acc_q_new = []\n    max_err_acc_t_new = []\n    for counter, pair in enumerate(pairs):\n        id1, id2 = pair.split('-')\n\n        # Compute matches by brute force.\n        cv_matches = bf.match(desc_dict[id1], desc_dict[id2])\n        matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n        cur_kp_1 = ArrayFromCvKps([kp_dict[id1][m[0]] for m in matches])\n        cur_kp_2 = ArrayFromCvKps([kp_dict[id2][m[1]] for m in matches])\n\n        # Filter matches with RANSAC.\n        F, inlier_mask = cv2.findFundamentalMat(cur_kp_1, cur_kp_2, cv2.USAC_MAGSAC, 0.25, 0.99999, 10000)\n        inlier_mask = inlier_mask.astype(bool).flatten()\n        \n        matches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\n        inlier_kp_1 = ArrayFromCvKps([kp_dict[id1][m[0]] for m in matches_after_ransac])\n        inlier_kp_2 = ArrayFromCvKps([kp_dict[id2][m[1]] for m in matches_after_ransac])\n\n        # Compute the essential matrix.\n        E, R, T = ComputeEssentialMatrix(F, calib_dict[id1].K, calib_dict[id2].K, inlier_kp_1, inlier_kp_2)\n        q = QuaternionFromMatrix(R)\n        T = T.flatten()\n\n        # Get the relative rotation and translation between these two cameras, given their R and T in the global reference frame.\n        R1_gt, T1_gt = calib_dict[id1].R, calib_dict[id1].T.reshape((3, 1))\n        R2_gt, T2_gt = calib_dict[id2].R, calib_dict[id2].T.reshape((3, 1))\n        dR_gt = np.dot(R2_gt, R1_gt.T)\n        dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n        q_gt = QuaternionFromMatrix(dR_gt)\n        q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n\n        # Compute the error for this example.\n        err_q, err_t = ComputeErrorForOneExample(q_gt, dT_gt, q, T, scaling_dict[scene])\n        errors[scene][pair] = [err_q, err_t]\n\n        # Plot the resulting matches and the pose error.\n        if verbose or (show_images and counter < num_show_images):\n            print(f'{pair}, err_q={(err_q):.02f} (deg), err_t={(err_t):.02f} (m)', flush=True)\n        if show_images and counter < num_show_images:\n            im_inliers = DrawMatches(images_dict[id1], images_dict[id2], ArrayFromCvKps(kp_dict[id1]), ArrayFromCvKps(kp_dict[id2]), matches_after_ransac)\n            fig = plt.figure(figsize=(25, 25))\n            plt.title(f'Inliers, \"{pair}\"')\n            plt.imshow(im_inliers)\n            plt.axis('off')\n            plt.show()\n            print()\n\n    # Histogram the errors over this scene.\n    mAA[scene] = ComputeMaa([v[0] for v in errors[scene].values()], [v[1] for v in errors[scene].values()], thresholds_q, thresholds_t)\n    print()\n    print(f'Mean average Accuracy on \"{scene}\": {mAA[scene][0]:.05f}')\n    print()\n\nprint()\nprint('------- SUMMARY -------')\nprint()\nfor scene in scaling_dict.keys():\n    print(f'-- Mean average Accuracy on \"{scene}\": {mAA[scene][0]:.05f}')\nprint()\nprint(f'Mean average Accuracy on dataset: {np.mean([mAA[scene][0] for scene in mAA]):.05f}')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T12:02:48.845365Z","iopub.execute_input":"2022-04-12T12:02:48.845904Z","iopub.status.idle":"2022-04-12T12:11:37.794317Z","shell.execute_reply.started":"2022-04-12T12:02:48.84587Z","shell.execute_reply":"2022-04-12T12:11:37.793753Z"},"trusted":true},"execution_count":null,"outputs":[]}]}