{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Computer Vision Final Project\n## Spring 2022\n### Matthew Menzi\n\nReport and presentation at:\nhttps://drive.google.com/drive/folders/1OhXLYOmTM7ldXHgdVLO4I9yEmVxEkUIy?usp=sharing\n\nThe first portion of the notebook is installations, data loading, and helper functions. Second section is where more substantial code is.","metadata":{}},{"cell_type":"code","source":"# Installations \n\n# !pip install kornia\n!pip install kornia_moons\n# !pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install git+https://github.com/kornia/kornia\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-05T04:29:40.413461Z","iopub.execute_input":"2022-05-05T04:29:40.414295Z","iopub.status.idle":"2022-05-05T04:30:35.029998Z","shell.execute_reply.started":"2022-05-05T04:29:40.414154Z","shell.execute_reply":"2022-05-05T04:30:35.029007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r ../input/imutils/imutils-0.5.3/ /\n!pip install /imutils-0.5.3/","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:30:35.033527Z","iopub.execute_input":"2022-05-05T04:30:35.033748Z","iopub.status.idle":"2022-05-05T04:30:46.264022Z","shell.execute_reply.started":"2022-05-05T04:30:35.033721Z","shell.execute_reply":"2022-05-05T04:30:46.263086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Imports\n\nimport cv2\nimport numpy as np\nimport os\nimport pandas as pd\nimport gc\nimport tensorflow as tf\nimport random\nimport kornia as K\nimport kornia.feature as KF\nimport gc\nfrom kornia_moons.feature import *\nimport matplotlib.pyplot as plt\nimport torch\nimport pydegensac\nfrom kornia.feature.loftr import LoFTR\nfrom kornia.feature.hynet import HyNet\nfrom kornia.feature.keynet import KeyNetDetector\nimport time\nimport math\nfrom scipy.spatial import distance as dist\nimport imutils\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:30:46.266395Z","iopub.execute_input":"2022-05-05T04:30:46.266682Z","iopub.status.idle":"2022-05-05T04:30:52.44998Z","shell.execute_reply.started":"2022-05-05T04:30:46.266642Z","shell.execute_reply":"2022-05-05T04:30:52.449237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cpu')\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print (\"GPU mode\")\nelse:\n    print (\"CPU mode\")","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:30:52.452293Z","iopub.execute_input":"2022-05-05T04:30:52.452557Z","iopub.status.idle":"2022-05-05T04:30:52.511648Z","shell.execute_reply.started":"2022-05-05T04:30:52.45252Z","shell.execute_reply":"2022-05-05T04:30:52.509999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data imports\n\nDATA_DIR = \"/kaggle/input/image-matching-challenge-2022\"\n\nTRAIN_DIR = os.path.join(DATA_DIR, \"train\")\nSCALING_CSV = os.path.join(TRAIN_DIR, \"scaling_factors.csv\")\nscaling_df = pd.read_csv(SCALING_CSV)\nscale_map = scaling_df.groupby(\"scene\")[\"scaling_factor\"].first().to_dict()\n\nTRAIN_SCENES = scaling_df.scene.unique().tolist()\ntrain_map = {} \n\nfor _s in TRAIN_SCENES:\n    # Initialize    \n    train_map[_s] = {}\n    \n    # Image Stuff\n    train_map[_s][\"images\"] = sorted(tf.io.gfile.glob(os.path.join(TRAIN_DIR, _s, \"images\", \"*.jpg\")))\n    train_map[_s][\"image_ids\"] = [_f_path[:-4].rsplit(\"/\", 1)[-1] for _f_path in train_map[_s][\"images\"]]\n    \n    # Calibration Stuff (CAL)\n    _tmp_cal_df = pd.read_csv(os.path.join(TRAIN_DIR, _s, \"calibration.csv\"))\n    _tmp_cal_df[\"image_path\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_cal_df[\"image_id\"]+\".jpg\"\n    train_map[_s][\"cal_df\"]=_tmp_cal_df.copy()\n        \n    # Pair Covisibility Stuff (PCO)\n    _tmp_pco_df = pd.read_csv(os.path.join(TRAIN_DIR, _s, \"pair_covisibility.csv\"))\n    _tmp_pco_df[\"image_id_1\"], _tmp_pco_df[\"image_id_2\"] = zip(*_tmp_pco_df.pair.apply(lambda x: x.split(\"-\")))\n    _tmp_pco_df[\"image_path_1\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_pco_df[\"image_id_1\"]+\".jpg\"\n    _tmp_pco_df[\"image_path_2\"] = os.path.join(TRAIN_DIR, _s, \"images\")+\"/\"+_tmp_pco_df[\"image_id_2\"]+\".jpg\"\n    train_map[_s][\"pco_df\"] = _tmp_pco_df.copy()\n\n#cleanup\ndel _tmp_cal_df, _tmp_pco_df; gc.collect(); gc.collect();\ntorch.cuda.empty_cache()\n\n_cal_dfs = []\n_pco_dfs = []\n_s_dfs = []\nfor _s in TRAIN_SCENES:\n    _s_map = train_map[_s]\n    _s_dfs.append(pd.DataFrame({\n        \"scene\":[_s,]*len(_s_map[\"images\"]),\n        \"f_path\":_s_map[\"images\"],\n    }))\n    _cal_dfs.append(_s_map[\"cal_df\"])\n    _pco_dfs.append(_s_map[\"pco_df\"])\n\nall_pco_dfs = pd.concat(_pco_dfs).reset_index(drop=True)\nall_cal_dfs = pd.concat(_cal_dfs).reset_index(drop=True)\n\ntrain_df = pd.concat(_s_dfs).reset_index(drop=True)\ntrain_df.insert(0, \"image_id\", train_df.f_path.apply(lambda x: x[:-4].rsplit(\"/\", 1)[-1]))\ntrain_df = pd.merge(train_df, all_cal_dfs, on=\"image_id\").drop(columns=[\"image_path\",])\n\ncov_1_df = all_pco_dfs[[\"image_id_1\", \"covisibility\"]]\ncov_1_df.columns = [\"image_id\", \"covisibility\"]\ncov_2_df = all_pco_dfs[[\"image_id_2\", \"covisibility\"]]\ncov_2_df.columns = [\"image_id\", \"covisibility\"]\ncov_df = pd.concat([cov_1_df,cov_2_df])\nimg_id_2_cov = cov_df.groupby(\"image_id\")[\"covisibility\"].mean().to_dict()\ndel cov_1_df, cov_2_df, cov_df; gc.collect(); torch.cuda.empty_cache()\n\n# Add a column for average covisibility\n#    - i.e. For a given image, what is the average \n#           covisibility of all of it's respective pairs\ntrain_df.insert(2, \"mean_covisibility\", train_df.image_id.map(img_id_2_cov))\n\ntrain_df[\"fx\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[0])) # 0 = TL \ntrain_df[\"fy\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[4])) # 4 = C\ntrain_df[\"x0\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[2])) # 0 = TR\ntrain_df[\"y0\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[5])) # 4 = CR\ntrain_df[\"s\"] = train_df.camera_intrinsics.apply(lambda x: float(x.split()[1])) # 1 = TC\n\nif (train_df[\"fx\"]!=train_df[\"fy\"]).sum()==0:\n    print(\"All fx values (focal length x) are equal to the respective fy values (focal length y)... as expected\")\n    \nif train_df[\"s\"].sum()==0:\n    print(\"All skew values are 0... as expected\")\n\ntrain_df[\"t_x\"] = train_df.translation_vector.apply(lambda x: float(x.split()[0]))\ntrain_df[\"t_y\"] = train_df.translation_vector.apply(lambda x: float(x.split()[1]))\ntrain_df[\"t_z\"] = train_df.translation_vector.apply(lambda x: float(x.split()[2]))\n\ntrain_df[\"r1_1\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[0]))\ntrain_df[\"r2_1\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[1]))\ntrain_df[\"r3_1\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[2]))\ntrain_df[\"r1_2\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[3]))\ntrain_df[\"r2_2\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[4]))\ntrain_df[\"r3_2\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[5]))\ntrain_df[\"r1_3\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[6]))\ntrain_df[\"r2_3\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[7]))\ntrain_df[\"r3_3\"] = train_df.rotation_matrix.apply(lambda x: float(x.split()[8]))\n\nprint(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:30:52.513472Z","iopub.execute_input":"2022-05-05T04:30:52.514069Z","iopub.status.idle":"2022-05-05T04:31:10.95674Z","shell.execute_reply.started":"2022-05-05T04:30:52.51403Z","shell.execute_reply":"2022-05-05T04:31:10.955258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convenience function to stack two images with different sizes.\ndef build_composite_image(im1, im2, axis=1, margin=0, background=1):\n    \"\"\"\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    \n    if background != 0 and background != 1:\n        background = 1\n    if axis != 0 and axis != 1:\n        raise RuntimeError('Axis must be 0 (vertical) or 1 (horizontal')\n\n    h1, w1, _ = im1.shape\n    h2, w2, _ = im2.shape\n\n    if axis == 1:\n        composite = np.zeros((max(h1, h2), w1 + w2 + margin, 3), dtype=np.uint8) + 255 * background\n        if h1 > h2:\n            voff1, voff2 = 0, (h1 - h2) // 2\n        else:\n            voff1, voff2 = (h2 - h1) // 2, 0\n        hoff1, hoff2 = 0, w1 + margin\n    else:\n        composite = np.zeros((h1 + h2 + margin, max(w1, w2), 3), dtype=np.uint8) + 255 * background\n        if w1 > w2:\n            hoff1, hoff2 = 0, (w1 - w2) // 2\n        else:\n            hoff1, hoff2 = (w2 - w1) // 2, 0\n        voff1, voff2 = 0, h1 + margin\n    composite[voff1:voff1 + h1, hoff1:hoff1 + w1, :] = im1\n    composite[voff2:voff2 + h2, hoff2:hoff2 + w2, :] = im2\n\n    return (composite, (voff1, voff2), (hoff1, hoff2))\n\n#  Draw keypoints and matches.\ndef draw_cv_matches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n    \"\"\"\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    \n    composite, v_offset, h_offset = build_composite_image(im1, im2, axis, margin, background)\n\n    # Draw all keypoints.\n    for coord_a, coord_b in zip(kp1, kp2):\n        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n    \n    # Draw matches, and highlight keypoints used in matches.\n    for idx_a, idx_b in matches:\n        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.line(composite,\n                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n                                   int(kp1[idx_a][1] + v_offset[0])]),\n                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n    return composite\n\n# K = calibration matrix\ndef normalize_keypoints(keypoints, K):\n    C_x = K[0, 2]\n    C_y = K[1, 2]\n    f_x = K[0, 0]\n    f_y = K[1, 1]\n    keypoints = (keypoints - np.array([[C_x, C_y]])) / np.array([[f_x, f_y]])\n    return keypoints\n\n# F = fundamental matrix K1, K2 = calibration matrices kp1, kp2 = keypoints\ndef compute_essential_matrix(F, K1, K2, kp1, kp2):\n    \"\"\"\n    Compute the Essential matrix from the Fundamental matrix, given the calibration matrices. \n    Note that we ask participants to estimate F, i.e., without relying on known intrinsics.\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n\n    assert F.shape[0] == 3, 'Malformed F?'\n\n    # Use OpenCV's recoverPose to solve the cheirality check:\n    # https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#gadb7d2dfcc184c1d2f496d8639f4371c0\n    E = np.matmul(np.matmul(K2.T, F), K1).astype(np.float64)\n    \n    kp1n = normalize_keypoints(kp1, K1)\n    kp2n = normalize_keypoints(kp2, K2)\n    num_inliers, R, T, mask = cv2.recoverPose(E, kp1n, kp2n)\n\n    return E, R, T\n\ndef quaternion_from_matrix(matrix):\n    \"\"\"\n    Transform a rotation matrix into a quaternion\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \n    The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\\\n    \"\"\"\n\n    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n    m00 = M[0, 0]\n    m01 = M[0, 1]\n    m02 = M[0, 2]\n    m10 = M[1, 0]\n    m11 = M[1, 1]\n    m12 = M[1, 2]\n    m20 = M[2, 0]\n    m21 = M[2, 1]\n    m22 = M[2, 2]\n\n    K = np.array([\n        [m00 - m11 - m22, 0.0, 0.0, 0.0],\n        [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n        [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n        [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]\n    ])\n    K /= 3.0\n\n    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n    w, V = np.linalg.eigh(K)\n    q = V[[3, 0, 1, 2], np.argmax(w)]\n    if q[0] < 0:\n        np.negative(q, q)\n    return q\n\ndef compute_error_for_example(q_gt, T_gt, q, T, scale, eps=1e-15):\n    '''Compute the error metric for a single example.\n    \n    The function returns two errors, over rotation and translation. \n    These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n    \n    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n    q_norm = q / (np.linalg.norm(q) + eps)\n\n    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n    err_q = np.arccos(1 - 2 * loss_q)\n\n    # Apply the scaling factor for this scene.\n    T_gt_scaled = T_gt * scale\n    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n\n    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n\n    return err_q * 180 / np.pi, err_t\n\n# Compute mean average error\ndef ComputeMaa(err_q, err_t, thresholds_q, thresholds_t):\n    '''Compute the mean Average Accuracy at different tresholds, for one scene.'''\n    \n    assert len(err_q) == len(err_t)\n    \n    acc, acc_q, acc_t = [], [], []\n    for th_q, th_t in zip(thresholds_q, thresholds_t):\n        acc += [(np.bitwise_and(np.array(err_q) < th_q, np.array(err_t) < th_t)).sum() / len(err_q)]\n        acc_q += [(np.array(err_q) < th_q).sum() / len(err_q)]\n        acc_t += [(np.array(err_t) < th_t).sum() / len(err_t)]\n    return np.mean(acc), np.array(acc), np.array(acc_q), np.array(acc_t)\n\ndef resize_keep_ratio(img, longest_size=1500):\n    scale = 840 / max(img.shape[0], img.shape[1]) \n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    resized_img = cv2.resize(img, (w, h))\n    return resized_img\n\ndef resize(img, target_size=(640,480)):\n    resized_img = cv2.resize(img, target_size)\n    return resized_img\n\ndef load_torch_image(fname):\n    img = cv2.imread(fname)\n    img = resize_keep_ratio(img)\n#     img = cv2.resize(img, (img.shape[1]//8*8, img.shape[0]//8*8))  # input size should be divisible by 8\n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img\n\n#  Draw keypoints and matches.\ndef draw_cv_matches(im1, im2, kp1, kp2, matches, axis=1, margin=0, background=0, linewidth=2):\n    \"\"\"\n    REFERENCE --> https://www.kaggle.com/code/eduardtrulls/imc2022-training-data?scriptVersionId=92062607\n    \"\"\"\n    \n    composite, v_offset, h_offset = build_composite_image(im1, im2, axis, margin, background)\n\n    # Draw all keypoints.\n    for coord_a, coord_b in zip(kp1, kp2):\n        composite = cv2.drawMarker(composite, (int(coord_a[0] + h_offset[0]), int(coord_a[1] + v_offset[0])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n        composite = cv2.drawMarker(composite, (int(coord_b[0] + h_offset[1]), int(coord_b[1] + v_offset[1])), color=(255, 0, 0), markerType=cv2.MARKER_CROSS, markerSize=5, thickness=1)\n    \n    # Draw matches, and highlight keypoints used in matches.\n    for idx_a, idx_b in matches:\n        composite = cv2.drawMarker(composite, (int(kp1[idx_a, 0] + h_offset[0]), int(kp1[idx_a, 1] + v_offset[0])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.drawMarker(composite, (int(kp2[idx_b, 0] + h_offset[1]), int(kp2[idx_b, 1] + v_offset[1])), color=(0, 0, 255), markerType=cv2.MARKER_CROSS, markerSize=12, thickness=1)\n        composite = cv2.line(composite,\n                             tuple([int(kp1[idx_a][0] + h_offset[0]),\n                                   int(kp1[idx_a][1] + v_offset[0])]),\n                             tuple([int(kp2[idx_b][0] + h_offset[1]),\n                                   int(kp2[idx_b][1] + v_offset[1])]), color=(0, 0, 255), thickness=1)\n    return composite","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:31:10.958314Z","iopub.execute_input":"2022-05-05T04:31:10.958573Z","iopub.status.idle":"2022-05-05T04:31:11.013185Z","shell.execute_reply.started":"2022-05-05T04:31:10.958538Z","shell.execute_reply":"2022-05-05T04:31:11.012271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Key Aspects of Project\n\nThe goal of this notebook is not to provide a working submission to the contest, but to explore the options that Kornia has to offer for solving the problem addressed by the contest. This notebook will test different ways to set up a correspondence-finding pipeline and use information gleaned from the experiment to hopefully optimize an algorithm to be used for a submission.\n\nAll the code above this point was provided by the contest organizers. All of the code after this point was written by myself (Matthew Menzi).\n\n## 1. Candidate keypoint finder (e.g., SIFT)\n\nUsing Kornia, keypoints are \"patches\" of 32x32 pixels\n\n### Options:\n* SIFT basic keypoints\n* KeyNetDetector(pretrained=False, num_features=2048, keynet_conf=keynet_config['KeyNet_default_config'], ori_module=PassLAF(), aff_module=PassLAF())\n(Ended up not using KND because performance seemed worse at cursory glance and wasn't time to pursue it)\n\n## 2. Keypoint descriptor\n\n### Options:\n* DenseSIFTDescriptor computes SIFT descriptor densely over image (no keypoint finder needed)\n* SIFTDescriptor(patch_size=41, num_ang_bins=8, num_spatial_bins=4, rootsift=True, clipval=0.2) \n* MKDDescriptor(patch_size=32, kernel_type='concat', whitening='pcawt', training_batch='liberty', output_dims=128) Multiple Kernal location descriptors\n* HardNet(pretrained=False) computes HardNet descriptors\n* HardNet8 (better hardnet) \n* HyNet(pretrained=False, is_bias=True, is_bias_FRN=True, dim_desc=128, drop_rate=0.3, eps_l2_norm=1e-10) computes HyNet descriptors\n* TFeat(pretrained=False)\n* SOSNet(pretrained=False)\n(Didn't use MKDDescriptor because it was hard on the GPU)\n\n## 3. Keypoint matcher\n\n### Options:\n* match_nn(desc1, desc2, dm=None) finds nearest neighbors in desc2 for each vector in desc1\n* match_mnn(desc1, desc2, dm=None) finds mutual nearest neighbors in desc2 for each vector in desc1\n* match_snn(desc1, desc2, th=0.8, dm=None) The method satisfies first to second nearest neighbor distance <= th.\n* match_smnn(desc1, desc2, th=0.8, dm=None) \n* LoFTR\n\n## 4. Match selector\n* cv2.FindFundamentalMat\n* Performance for the cv2 MAGSAC option seems to be considered best\n","metadata":{}},{"cell_type":"code","source":"# Show pair of images with matches before RANSAC and matches after RANSAC \ndef display_example_from_batch(batch_df, loftr=False, resize=False):\n    idx = random.randint(1, batch_df.shape[0])\n    rowA = batch_df.iloc[idx]\n    if loftr:\n        centers1a = rowA['kp1']\n        centers2a = rowA['kp2']\n    else:\n        laf1a = rowA['kp1']\n        laf2a = rowA['kp2']\n        centers1a = KF.laf.get_laf_center(laf1a)[0]\n        centers2a = KF.laf.get_laf_center(laf2a)[0]\n    img1a = cv2.imread(rowA['img1'])\n    img2a = cv2.imread(rowA['img2'])\n    if resize:\n        img1a, img2a = resize_keep_ratio(img1a), resize_keep_ratio(img2a)\n    matches_beforeA = rowA['MatchIdxBefore']\n    matches_afterA = rowA['MatchIdxAfter']\n    beforeRansacA = draw_cv_matches(img1a, img2a, centers1a, centers2a, matches_beforeA)\n    afterRansacA = draw_cv_matches(img1a, img2a, centers1a, centers2a, matches_afterA)\n    rowB = batch_df.iloc[0]\n    if loftr:\n        centers1b = rowB['kp1']\n        centers2b = rowB['kp2']\n    else:\n        laf1b = rowB['kp1']\n        laf2b = rowB['kp2']\n        centers1b = KF.laf.get_laf_center(laf1b)[0]\n        centers2b = KF.laf.get_laf_center(laf2b)[0]\n    img1b = cv2.imread(rowB['img1'])\n    img2b = cv2.imread(rowB['img2'])\n    if resize:\n        img1b, img2b = resize_keep_ratio(img1b), resize_keep_ratio(img2b)\n    matches_beforeB = rowB['MatchIdxBefore']\n    matches_afterB = rowB['MatchIdxAfter']\n    beforeRansacB = draw_cv_matches(img1b, img2b, centers1b, centers2b, matches_beforeB)\n    afterRansacB = draw_cv_matches(img1b, img2b, centers1b, centers2b, matches_afterB)\n    plt.figure(figsize=(20, 10))\n    plt.subplot(2,2,1)\n    plt.imshow(beforeRansacB)\n    plt.subplot(2,2,3)\n    plt.imshow(afterRansacB)\n    plt.subplot(2,2,2)\n    plt.imshow(beforeRansacA)\n    plt.subplot(2,2,4)\n    plt.imshow(afterRansacA)\n    plt.tight_layout()\n    plt.show()\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:31:11.014681Z","iopub.execute_input":"2022-05-05T04:31:11.014945Z","iopub.status.idle":"2022-05-05T04:31:11.035726Z","shell.execute_reply.started":"2022-05-05T04:31:11.01491Z","shell.execute_reply":"2022-05-05T04:31:11.034781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load batch of image pairs to use for testing\n# All operations are done through a dataframe with pertinent info added as a column\n# as process iterates through following functions\n\ndef arr_from_str(_s):\n    return np.fromstring(_s, sep=\" \").reshape((-1,3)).squeeze()\n\n# Returns pair of images with GT fundamental matrix and covisibility\ndef get_pair(location, row_number):\n    row = train_map[location]['pco_df'].iloc[row_number]\n    path_1 = row.image_path_1\n    path_2 = row.image_path_2\n    covis = row.covisibility\n    img1 = path_1\n    img2 = path_2\n    fund = arr_from_str(row.fundamental_matrix)\n    return img1, img2, fund, covis\n\n# Returns intrinsics and extrinsics for image pair\ndef get_pair_info(location, row_number):\n    row = train_map[location]['pco_df'].iloc[row_number]\n    img_id_1 = row.image_id_1\n    img_id_2 = row.image_id_2\n    r1 = train_df.loc[train_df['image_id'] == img_id_1]\n    r2 = train_df.loc[train_df['image_id'] == img_id_2]\n    ci_1 = r1.iloc[0]['camera_intrinsics']\n    ci_2 = r2.iloc[0]['camera_intrinsics']\n    rm_1 = r1.iloc[0]['rotation_matrix']\n    rm_2 = r2.iloc[0]['rotation_matrix']\n    tv_1 = r1.iloc[0]['translation_vector']\n    tv_2 = r2.iloc[0]['translation_vector']\n    return arr_from_str(ci_1), arr_from_str(ci_2), arr_from_str(rm_1), arr_from_str(rm_2), arr_from_str(tv_1).reshape((3,1)), arr_from_str(tv_2).reshape((3,1))\n\n# Returns a batch of image pairs with n_per_loc images per location (min 16 images in batch) with min_covis minimum covisibility\ndef get_pair_batch(n_per_loc, min_covis=0.1):\n    dat = []\n    locs = scale_map.keys()\n    for loc in locs:\n        n_rows = train_map[loc]['pco_df'].loc[train_map[loc]['pco_df']['covisibility'] > min_covis].shape[0]\n        for i in range(n_per_loc):\n            row = random.randint(0, n_rows)\n            img1, img2, F, covis = get_pair(loc, row)\n            ci_1, ci_2, rm_1, rm_2, tv_1, tv_2 = get_pair_info(loc, row)\n            scale_factor = scale_map[loc]\n            dat.append([loc, scale_factor, covis, img1, img2, F, ci_1, ci_2, rm_1, rm_2, tv_1, tv_2])\n    batch_df = pd.DataFrame(dat, columns=['scene', 'scale', 'covis', 'img1', 'img2', 'F', 'ci_1', 'ci_2', 'rm_1', 'rm_2', 'tv_1', 'tv_2'])\n    del dat; gc.collect()\n    torch.cuda.empty_cache()\n    return batch_df\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:31:11.03733Z","iopub.execute_input":"2022-05-05T04:31:11.038057Z","iopub.status.idle":"2022-05-05T04:31:11.055282Z","shell.execute_reply.started":"2022-05-05T04:31:11.038018Z","shell.execute_reply":"2022-05-05T04:31:11.054541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get candidate lafs (keypoints) for image pairs\ndef extract_sift_lafs(rgb_image):\n    n_feat = 8_000\n    contrast_thresh = -10_000\n    edge_thresh = -10_000\n    \n    sift_detector = cv2.SIFT_create(n_feat, contrastThreshold=contrast_thresh, edgeThreshold=edge_thresh)\n    \n    gray = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)\n\n    keypoints = sift_detector.detect(gray)\n    \n    lafs = laf_from_opencv_SIFT_kpts(keypoints)\n    \n    return lafs\n\n# Get KeyNet candidate keypoints\ndef extract_KeyNet_keypoints(rgb_image, pre_tr):\n    with torch.no_grad():\n        detector = KF.KeyNetDetector(pretrained=pre_tr).eval().to(device)\n        gray = K.color.rgb_to_grayscale(K.image_to_tensor(rgb_image, False).float()/255.).to(device)\n        that = detector.forward(gray)[0]\n    return that\n\n# Get LAFs for batch of image pairs \ndef get_batch_lafs(batch_df, SIFT=True, resize=False):\n    keypoints1 = []\n    keypoints2 = []\n    for i in range(batch_df.shape[0]):\n        row = batch_df.iloc[i]\n        if SIFT:\n            img1, img2 = cv2.imread(row['img1']), cv2.imread(row['img2'])\n            if resize:\n                img1 = resize_keep_ratio(img1)\n                img2 = resize_keep_ratio(img2)\n            kp1 = extract_sift_lafs(img1)\n            kp2 = extract_sift_lafs(img2)\n            keypoints1.append(kp1.cpu())\n            keypoints2.append(kp2.cpu())\n            del img1, img2, kp1, kp2; gc.collect(), gc.collect(); torch.cuda.empty_cache()\n        else:\n            img1, img2 = cv2.imread(row['img1']), cv2.imread(row['img2'])\n            if resize:\n                img1 = resize_keep_ratio(img1)\n                img2 = resize_keep_ratio(img2)\n            kp1 = extract_KeyNet_keypoints(img1, True)\n            kp2 = extract_KeyNet_keypoints(img2, True)\n            keypoints1.append(kp1.cpu())\n            keypoints2.append(kp2.cpu())\n            del img1, img2, kp1, kp2; gc.collect(), gc.collect(); torch.cuda.empty_cache()\n        del row\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    new_batch_df = batch_df.copy()\n    new_batch_df['kp1'] = keypoints1\n    new_batch_df['kp2'] = keypoints2\n    new_batch_df['resized'] = [resize for i in range(batch_df.shape[0])]\n    return new_batch_df\n\n# Get descriptors for batch of image pairs\ndef get_batch_descriptors(batch_df, kornia_descriptor, resize=False):\n    desc1 = []\n    desc2 = []\n    times = []\n    with torch.no_grad():\n        kornia_descriptor.eval()\n        for i in range(batch_df.shape[0]):\n            t = time.process_time()\n            #print(f'Getting descriptors: iteration {i}')\n            row = batch_df.iloc[i]\n            img1, img2 = cv2.imread(row['img1']), cv2.imread(row['img2'])\n            if resize:\n                img1 = resize_keep_ratio(img1)\n                img2 = resize_keep_ratio(img2)\n            timg1 = K.color.rgb_to_grayscale(K.image_to_tensor(img1, False).float()/255.).to(device)\n            timg2 = K.color.rgb_to_grayscale(K.image_to_tensor(img2, False).float()/255.).to(device)\n            laf1 = row['kp1']\n            laf2 = row['kp2']\n            laf1, laf2 = laf1.to(device), laf2.to(device)\n            descs1 = KF.get_laf_descriptors(timg1, laf1, kornia_descriptor)\n            descs2 = KF.get_laf_descriptors(timg2, laf2, kornia_descriptor)\n            desc1.append(descs1.cpu())\n            desc2.append(descs2.cpu())\n            del img1, img2, timg1, timg2, row, descs1, descs2; gc.collect(); torch.cuda.empty_cache()\n            elapsed_time = time.process_time() - t\n            times.append(elapsed_time)\n    new_batch_df = batch_df.copy()\n    new_batch_df['descs1'] = desc1\n    new_batch_df['descs2'] = desc2\n    new_batch_df['desc_time'] = times\n    return new_batch_df\n\n# Get matches for batch of image pairs\ndef get_batch_matches(batch_df):\n    kp1before = []\n    kp2before = []\n    kp1after = []\n    kp2after = []\n    match_idx_before = []\n    match_idx_after = []\n    Fs = []\n    times = []\n    for i in range(batch_df.shape[0]):\n        t = time.process_time()\n        #print(f'Getting matches: iteration {i}')\n        row = batch_df.iloc[i]\n        desc1, desc2 = row['descs1'][0], row['descs2'][0]\n        laf1, laf2 = row['kp1'], row['kp2']\n        \n        dists, idxs = KF.match_smnn(desc1, desc2, 0.95)\n        \n        cv_matches = cv2_matches_from_kornia(dists, idxs)\n        centers1 = KF.laf.get_laf_center(laf1)[0]\n        centers2 = KF.laf.get_laf_center(laf2)[0]\n        kp_1_pos = np.float32([ list(centers1[m.queryIdx]) for m in cv_matches ])\n        kp_2_pos = np.float32([ list(centers2[m.trainIdx]) for m in cv_matches ])\n        matches_before = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n        F, inlier_mask = cv2.findFundamentalMat(kp_1_pos, kp_2_pos, \n                                    cv2.USAC_MAGSAC, ransacReprojThreshold=0.25, \n                                    confidence=0.99999, maxIters=10000)\n        matches_after = np.array([match for match, is_inlier in zip(matches_before, inlier_mask) if is_inlier])\n        kp_1_after = np.float32([ list(centers1[m[0]]) for m in matches_after])\n        kp_2_after = np.float32([ list(centers2[m[1]]) for m in matches_after])\n        Fs.append(F)\n        kp1before.append(kp_1_pos)\n        kp2before.append(kp_2_pos)\n        kp1after.append(kp_1_after)\n        kp2after.append(kp_2_after)\n        match_idx_before.append(matches_before)\n        match_idx_after.append(matches_after)\n        del row, desc1, desc2, laf1, laf2, dists, idxs, cv_matches, kp_1_pos, kp_2_pos, kp_1_after, kp_2_after, matches_before, F, inlier_mask, matches_after; \n        gc.collect(), torch.cuda.empty_cache()\n        elapsed_time = time.process_time() - t\n        times.append(elapsed_time)\n    new_batch_df = batch_df.copy()\n    new_batch_df['MatchesBefore1'] = kp1before\n    new_batch_df['MatchesBefore2'] = kp2before\n    new_batch_df['MatchesAfter1'] = kp1after\n    new_batch_df['MatchesAfter2'] = kp2after\n    new_batch_df['MatchIdxBefore'] = match_idx_before\n    new_batch_df['MatchIdxAfter'] = match_idx_after\n    new_batch_df['FoundF'] = Fs\n    new_batch_df['match_time'] = times\n    return new_batch_df\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:31:11.057784Z","iopub.execute_input":"2022-05-05T04:31:11.058235Z","iopub.status.idle":"2022-05-05T04:31:11.095856Z","shell.execute_reply.started":"2022-05-05T04:31:11.058077Z","shell.execute_reply":"2022-05-05T04:31:11.095054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LoFTR matcher\nWEIGHT_PATH_A = '../input/kornia-loftr/outdoor_ds.ckpt'\nWEIGHT_PATH_B = '../input/kornia-loftr/loftr_outdoor.ckpt'\n\n# LONGEST_EDGE = 1500\nLOFTR_A = LoFTR(pretrained=None)\nLOFTR_A.load_state_dict(torch.load(WEIGHT_PATH_A)['state_dict'])\nLOFTR_A = LOFTR_A.to(device)\nLOFTR_A.eval()\n\nLOFTR_B = LoFTR(pretrained=None)\nLOFTR_B.load_state_dict(torch.load(WEIGHT_PATH_B)['state_dict'])\nLOFTR_B = LOFTR_B.to(device)\nLOFTR_B.eval()\n\n# Function for LoFTR model, which bypasses the descriptor stage of the process\ndef loftr_match(batch_df, matcher, resize=False):\n    kp1before = []\n    kp2before = []\n    kp1after = []\n    kp2after = []\n    match_idx_before = []\n    match_idx_after = []\n    Fs = []\n    times = []\n    with torch.no_grad():\n        for i in range(batch_df.shape[0]):\n            t = time.process_time()\n            row = batch_df.iloc[i]\n            img1 = cv2.imread(row['img1'])\n            img2 = cv2.imread(row['img2'])\n            if resize:\n                img1 = resize_keep_ratio(img1)\n                img2 = resize_keep_ratio(img2)\n            img1 = K.color.rgb_to_grayscale(K.image_to_tensor(img1, False).float()/255.).to(device)\n            img2 = K.color.rgb_to_grayscale(K.image_to_tensor(img2, False).float()/255.).to(device)\n            input_dict = {\"image0\": img1, \"image1\": img2}\n            out = matcher(input_dict)\n            kp_1_before = out['keypoints0']\n            kp_2_before = out['keypoints1']\n            indexes = out['batch_indexes']\n            kp_1_before = kp_1_before.cpu().numpy()\n            kp_2_before = kp_2_before.cpu().numpy()\n            F, inlier_mask = cv2.findFundamentalMat(kp_1_before, kp_2_before, cv2.USAC_MAGSAC, 0.25, 0.999, 100000)\n            assert F.shape == (3, 3), 'Malformed F?'\n            kp_1_after = kp_1_before[inlier_mask.reshape(-1) > 0]\n            kp_2_after = kp_2_before[inlier_mask.reshape(-1) > 0]\n            matches_before = np.array([[i, i] for i in range(len(kp_1_before))])\n            matches_after = np.array([match for match, is_inlier in zip(matches_before, inlier_mask) if is_inlier])\n            Fs.append(F)\n            kp1before.append(kp_1_before)\n            kp2before.append(kp_2_before)\n            kp1after.append(kp_1_after)\n            kp2after.append(kp_2_after)\n            match_idx_before.append(matches_before)\n            match_idx_after.append(matches_after)\n            del row, img1, img2, input_dict, out, kp_1_before, kp_2_before, kp_1_after, kp_2_after, matches_before, F, inlier_mask, matches_after; \n            gc.collect(),torch.cuda.empty_cache()\n            elapsed_time = time.process_time() - t\n            times.append(elapsed_time)\n    new_batch_df = batch_df.copy()\n    new_batch_df['kp1'] = kp1before\n    new_batch_df['kp2'] = kp2before\n    new_batch_df['descs1'] = [0 for i in range(batch_df.shape[0])]\n    new_batch_df['descs2'] = [0 for i in range(batch_df.shape[0])]\n    new_batch_df['desc_time'] = [0 for i in range(batch_df.shape[0])]\n    new_batch_df['MatchesBefore1'] = kp1before\n    new_batch_df['MatchesBefore2'] = kp2before\n    new_batch_df['MatchesAfter1'] = kp1after\n    new_batch_df['MatchesAfter2'] = kp2after\n    new_batch_df['MatchIdxBefore'] = match_idx_before\n    new_batch_df['MatchIdxAfter'] = match_idx_after\n    new_batch_df['FoundF'] = Fs\n    new_batch_df['match_time'] = times\n    new_batch_df['resized'] = [resize for i in range(batch_df.shape[0])]\n    return new_batch_df","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:31:11.09885Z","iopub.execute_input":"2022-05-05T04:31:11.099037Z","iopub.status.idle":"2022-05-05T04:31:16.833394Z","shell.execute_reply.started":"2022-05-05T04:31:11.099014Z","shell.execute_reply":"2022-05-05T04:31:16.832582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Returns Mean Average Accuracy for batch \ndef get_score(batch_df):\n    errors_q = []\n    errors_t = []\n    thresholds_q = np.linspace(1, 10, 10)\n    thresholds_t = np.geomspace(0.2, 5, 10)\n        \n    for i in range(batch_df.shape[0]):\n        row = batch_df.iloc[i]\n        SCALE_FACTOR = row['scale']\n        inlier_kp_1 = np.array([row['MatchesAfter1']])\n        inlier_kp_2 = np.array([row['MatchesAfter2']])\n        F = row['FoundF']\n        E, R, T = compute_essential_matrix(F, row['ci_1'], row['ci_2'], inlier_kp_1, inlier_kp_2)\n        q = quaternion_from_matrix(R)\n        T = T.flatten()\n        R1_gt = row['rm_1']\n        R2_gt = row['rm_2']\n        T1_gt = row['tv_1']\n        T2_gt = row['tv_2']\n        dR_gt = np.dot(R2_gt, R1_gt.T)\n        dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n        q_gt = quaternion_from_matrix(dR_gt)\n        q_gt = q_gt / (np.linalg.norm(q_gt) + 1e-15)\n        err_q, err_t = compute_error_for_example(q_gt, dT_gt, q, T, SCALE_FACTOR)\n        errors_q.append(err_q)\n        errors_t.append(err_t)\n    maa, overall, rotation, translation = ComputeMaa(errors_q, errors_t, thresholds_q, thresholds_t)\n    print(f'Mean average accuracy: {maa}')\n\n# Returns MAA, rotation error, and translation error for a row\ndef get_row_score(row):\n    errors_q = []\n    errors_t = []\n    thresholds_q = np.linspace(1, 10, 10)\n    thresholds_t = np.geomspace(0.2, 5, 10)\n    SCALE_FACTOR = row['scale']\n    inlier_kp_1 = np.array([row['MatchesAfter1']])\n    inlier_kp_2 = np.array([row['MatchesAfter2']])\n    F = row['FoundF']\n    E, R, T = compute_essential_matrix(F, row['ci_1'], row['ci_2'], inlier_kp_1, inlier_kp_2)\n    q = quaternion_from_matrix(R)\n    T = T.flatten()\n    R1_gt = row['rm_1']\n    R2_gt = row['rm_2']\n    T1_gt = row['tv_1']\n    T2_gt = row['tv_2']\n    dR_gt = np.dot(R2_gt, R1_gt.T)\n    dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n    q_gt = quaternion_from_matrix(dR_gt)\n    q_gt = q_gt / (np.linalg.norm(q_gt) + 1e-15)\n    err_q, err_t = compute_error_for_example(q_gt, dT_gt, q, T, SCALE_FACTOR)\n    errors_q.append(err_q)\n    errors_t.append(err_t)\n    maa, overall, rotation, translation = ComputeMaa(errors_q, errors_t, thresholds_q, thresholds_t)\n    return maa, err_q, err_t\n    \n# Organizes info from batch dataframe and writes it to CSV\n# Pretty messy function but it gets the job done\n# This data will be analyzed in a different location\ndef stats_to_csv(descriptorName, pre_trained, batch_df, overwrite=False):\n    descriptor_name = []\n    pretrained = []\n    scene = []\n    maa = []\n    r_error = []\n    t_error = []\n    covis = []\n    size_dif = []\n    width1 = []\n    width2 = []\n    height1 = []\n    height2 = []\n    keypoints1 = []\n    keypoints2 = []\n    matches_before = []\n    matches_after = []\n    pct_kp_to_matches1 = []\n    pct_kp_to_matches2 = []\n    pct_m_after_ransac = []\n    descriptor_time = []\n    matcher_time = []\n    time_per_desc = []\n    time_per_match = []\n    intersect_h = []\n    intersect_m = []\n    intersect_a = []\n    correl_h = []\n    correl_m = []\n    correl_a = []\n    chsq_h = []\n    chsq_m = []\n    chsq_a = []\n    euc_h = []\n    euc_m = []\n    euc_a = []\n    cheb_h = []\n    cheb_m = []\n    cheb_a = []\n    resized = []\n    \n    for i in range(batch_df.shape[0]):\n        row = batch_df.iloc[i]\n        resized.append(row['resized'])\n        ma2, r_e, t_e = get_row_score(row)\n        img1 = cv2.imread(row['img1'])\n        img2 = cv2.imread(row['img2'])\n        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n        \n        \n        kps1 = len(list(row['kp1'][0]))\n        kps2 = len(list(row['kp2'][0]))\n        m_b = len(row['MatchIdxBefore'])\n        m_a = len(row['MatchIdxAfter'])\n        pct_kp_m1 = m_b / kps1\n        pct_kp_m2 = m_b / kps2\n        pct_m_a = m_a / m_b\n        d_t = row['desc_time']\n        m_t = row['match_time']\n        tpd = d_t / (kps1 + kps2) \n        tpm = m_t / m_b\n        \n        descriptor_name.append(descriptorName)\n        pretrained.append(pre_trained)\n        scene.append(row['scene'])\n        maa.append(ma2)\n        r_error.append(r_e)\n        t_error.append(t_e)\n        \n        covis.append(row['covis'])\n        size_dif.append(abs(1 - ((img1.shape[0] * img1.shape[1]) / (img2.shape[0] * img2.shape[1]))))\n        height1.append(img1.shape[0])\n        width1.append(img1.shape[1])\n        height2.append(img2.shape[0])\n        width2.append(img2.shape[1])\n        keypoints1.append(kps1)\n        keypoints2.append(kps2)\n        matches_before.append(m_b)\n        matches_after.append(m_a)\n        pct_kp_to_matches1.append(pct_kp_m1)\n        pct_kp_to_matches2.append(pct_kp_m2)\n        pct_m_after_ransac.append(pct_m_a)\n        descriptor_time.append(d_t)\n        matcher_time.append(m_t)\n        time_per_desc.append(tpd)\n        time_per_match.append(tpm)\n        \n        hist1 = cv2.calcHist([img1], [0], None, [16], [0, 256])\n        hist1 = hist1 / hist1.sum()\n        hist2 = cv2.calcHist([img2], [0], None, [16], [0, 256])\n        hist2 = hist2 / hist2.sum()\n        \n        gx1 = cv2.Sobel(img1, cv2.CV_32F, 1, 0, ksize=1)\n        gy1 = cv2.Sobel(img1, cv2.CV_32F, 0, 1, ksize=1)\n        mag1, angle1 = cv2.cartToPolar(gx1, gy1, angleInDegrees=True)\n        gx2 = cv2.Sobel(img2, cv2.CV_32F, 1, 0, ksize=1)\n        gy2 = cv2.Sobel(img2, cv2.CV_32F, 0, 1, ksize=1)\n        mag2, angle2 = cv2.cartToPolar(gx2, gy2, angleInDegrees=True)\n        \n        mhist1 = cv2.calcHist([mag1], [0], None, [16], [0, 256])\n        mhist1 = mhist1 / mhist1.sum()\n        mhist2 = cv2.calcHist([mag2], [0], None, [16], [0, 256])\n        mhist2 = mhist2 / mhist2.sum()\n        ahist1 = cv2.calcHist([angle1], [0], None, [16], [0, 360])\n        ahist1 = ahist1 / ahist1.sum()\n        ahist2 = cv2.calcHist([angle2], [0], None, [16], [0, 360])\n        ahist2 = ahist2 / ahist2.sum()\n        \n        intersect_h.append(cv2.compareHist(hist1, hist2, cv2.HISTCMP_INTERSECT))\n        intersect_m.append(cv2.compareHist(mhist1, mhist2, cv2.HISTCMP_INTERSECT))\n        intersect_a.append(cv2.compareHist(ahist1, ahist2, cv2.HISTCMP_INTERSECT))\n\n        correl_h.append(cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL))\n        correl_m.append(cv2.compareHist(mhist1, mhist2, cv2.HISTCMP_CORREL))\n        correl_a.append(cv2.compareHist(ahist1, ahist2, cv2.HISTCMP_CORREL))\n        \n        chsq_h.append(cv2.compareHist(hist1, hist2, cv2.HISTCMP_CHISQR))\n        chsq_m.append(cv2.compareHist(mhist1, mhist2, cv2.HISTCMP_CHISQR_ALT))\n        chsq_a.append(cv2.compareHist(ahist1, ahist2, cv2.HISTCMP_CHISQR))\n        euc_h.append(dist.euclidean(hist1, hist2))\n        euc_m.append(dist.euclidean(mhist1, mhist2))\n        euc_a.append(dist.euclidean(ahist1, ahist2))\n        cheb_h.append(dist.chebyshev(hist1, hist2))\n        cheb_m.append(dist.chebyshev(mhist1, mhist2))\n        cheb_a.append(dist.chebyshev(ahist1, ahist2))\n\n        \n    df = pd.DataFrame({'descriptor' : descriptor_name, 'resized' : resized, 'pretrained' : pretrained, 'scene': scene,\n                      'maa': maa, 'rotation error' : r_error, 'translation error' : t_error,\n                      'covisibility' : covis, 'size dif' : size_dif,\n                       'width 1' : width1, 'height 1' : height1, 'width 2' : width2, 'height 2':height2, \n                      'keypoints found 1' : keypoints1, 'keypoints found 2' : keypoints2,\n                      'matches before' : matches_before, 'matches after' : matches_after, \n                       '% KP as matches 1' : pct_kp_to_matches1, '% KP as matches 2' : pct_kp_to_matches2, \n                       '% matches after ransac' : pct_m_after_ransac,\n                       'descriptor time' : descriptor_time, 'matcher time' : matcher_time,\n                      'time per desc' : time_per_desc, 'time per match' : time_per_match,\n                      'inter': intersect_h, 'inter m': intersect_m, 'inter a' : intersect_a,\n                      'cor':correl_h, 'cor m' : correl_m, 'cor a':correl_a, 'chsq' : chsq_h,\n                      'chsq m' : chsq_m, 'chsq a':chsq_a, 'euc' : euc_h, 'euc m':euc_m,'euc a':euc_a,\n                      'cheb':cheb_h, 'cheb m':cheb_m, 'cheb a' : cheb_a})\n    if overwrite:\n        df.to_csv('/kaggle/working/model_stats.csv', mode='w')\n    else:  \n        df.to_csv('/kaggle/working/model_stats.csv', header=False, mode='a')\n        \n    del df,descriptor_name,pretrained,scene,maa,r_error,t_error,covis,size_dif,width1,width2,height1,height2,keypoints1,keypoints2,matches_before,matches_after,pct_kp_to_matches1,pct_kp_to_matches2,pct_m_after_ransac,descriptor_time,matcher_time,time_per_desc,time_per_match,intersect_h,intersect_m,intersect_a,correl_h,correl_m,correl_a,chsq_h,chsq_m,chsq_a,euc_h,euc_m,euc_a,cheb_h,cheb_m,cheb_a,resized\n    gc.collect()\n    torch.cuda.empty_cache()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:31:16.835206Z","iopub.execute_input":"2022-05-05T04:31:16.835724Z","iopub.status.idle":"2022-05-05T04:31:16.886847Z","shell.execute_reply.started":"2022-05-05T04:31:16.835686Z","shell.execute_reply":"2022-05-05T04:31:16.886107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Runs a matching process for a batch of lafs given a kornia descriptor\ndef run_test(lafs, descriptor, resize=False):\n    descriptors = get_batch_descriptors(lafs, descriptor, resize)\n    torch.cuda.empty_cache()\n    matches = get_batch_matches(descriptors)\n    torch.cuda.empty_cache()\n    get_score(matches)\n    return matches\n    del descriptors, matches; gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:31:16.888201Z","iopub.execute_input":"2022-05-05T04:31:16.888491Z","iopub.status.idle":"2022-05-05T04:31:16.900103Z","shell.execute_reply.started":"2022-05-05T04:31:16.888455Z","shell.execute_reply":"2022-05-05T04:31:16.899161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Descriptors\n# Loads the kornia descriptors that will be used\nSIFT = KF.SIFTDescriptor(patch_size=32, num_ang_bins=8, num_spatial_bins=4, rootsift=True, clipval=0.2).eval().to(device)\nMKD = KF.MKDDescriptor(patch_size=32, kernel_type='concat', whitening='pcawt', training_set='liberty', output_dims=128).eval().to(device)\nHardNet = KF.HardNet(True).eval().to(device)\nHardNet8 = KF.HardNet8(True).eval().to(device)\ntry: \n    HyNet = KF.hynet.HyNet(pretrained=True, is_bias=True, is_bias_FRN=True, dim_desc=128, drop_rate=0.3, eps_l2_norm=1e-10).eval().to(device)\nexcept:\n    HyNet = HyNet(pretrained=True, is_bias=True, is_bias_FRN=True, dim_desc=128, drop_rate=0.3, eps_l2_norm=1e-10).eval().to(device)\nTFeat = KF.TFeat(True).eval().to(device)\nSOSNet = KF.SOSNet(True).eval().to(device)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:31:16.901417Z","iopub.execute_input":"2022-05-05T04:31:16.90174Z","iopub.status.idle":"2022-05-05T04:31:23.644589Z","shell.execute_reply.started":"2022-05-05T04:31:16.901701Z","shell.execute_reply":"2022-05-05T04:31:23.643961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Loading a batch of image pairs\n# 8 images per scene for a total of 128 images in batch seems to be optimal batch size\n# There is little performance penalty vs 4 images per scene, but >8 uses too much memory\n\n# Getting LAFs for image pairs\n# Preloading LAFs saves time and memory\np_b = get_pair_batch(8, 0.1)\nlafs = get_batch_lafs(p_b)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:32:20.508471Z","iopub.execute_input":"2022-05-05T04:32:20.508808Z","iopub.status.idle":"2022-05-05T04:34:46.09269Z","shell.execute_reply.started":"2022-05-05T04:32:20.508775Z","shell.execute_reply":"2022-05-05T04:34:46.09191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Runs a matching process with SIFT descriptor\n# This could be considered the baseline\nsift = run_test(lafs, SIFT)\nstats_to_csv(\"SIFT\", \"True\", sift, overwrite=True)\ndisplay_example_from_batch(sift)\ndel sift\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:34:56.778006Z","iopub.execute_input":"2022-05-05T04:34:56.778278Z","iopub.status.idle":"2022-05-05T04:40:26.51651Z","shell.execute_reply.started":"2022-05-05T04:34:56.778249Z","shell.execute_reply":"2022-05-05T04:40:26.512437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# MKD kept breaking the GPU so it is commented out \n\n# %%time\n# mkd = run_test(lafs, MKD)\n# stats_to_csv(\"MKD\", \"True\", mkd, overwrite=False)\n# del mkd\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:40:26.518002Z","iopub.execute_input":"2022-05-05T04:40:26.518271Z","iopub.status.idle":"2022-05-05T04:40:26.521542Z","shell.execute_reply.started":"2022-05-05T04:40:26.518238Z","shell.execute_reply":"2022-05-05T04:40:26.520949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Runs a matching process with HardNet descriptor\nhardnet = run_test(lafs, HardNet)\nstats_to_csv(\"HardNet\", \"True\", hardnet, overwrite=False)\ndisplay_example_from_batch(hardnet)\ndel hardnet\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:40:26.522699Z","iopub.execute_input":"2022-05-05T04:40:26.523113Z","iopub.status.idle":"2022-05-05T04:48:52.469001Z","shell.execute_reply.started":"2022-05-05T04:40:26.523081Z","shell.execute_reply":"2022-05-05T04:48:52.468239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Runs a matching proces with HardNet8 descriptor\nhardnet8 = run_test(lafs, HardNet8)\nstats_to_csv(\"HardNet8\", \"True\", hardnet8, overwrite=False)\ndisplay_example_from_batch(hardnet8)\ndel hardnet8\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:48:52.470922Z","iopub.execute_input":"2022-05-05T04:48:52.471378Z","iopub.status.idle":"2022-05-05T04:57:25.28185Z","shell.execute_reply.started":"2022-05-05T04:48:52.471343Z","shell.execute_reply":"2022-05-05T04:57:25.281158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Runs a matching proces with HyNet descriptor\nhynet = run_test(lafs, HyNet)\nstats_to_csv(\"HyNet\", \"True\", hynet, overwrite=False)\ndisplay_example_from_batch(hynet)\ndel hynet\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T04:57:25.28307Z","iopub.execute_input":"2022-05-05T04:57:25.28347Z","iopub.status.idle":"2022-05-05T05:07:09.316863Z","shell.execute_reply.started":"2022-05-05T04:57:25.283434Z","shell.execute_reply":"2022-05-05T05:07:09.316254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Runs a matching proces with TFeat descriptor\ntfeat = run_test(lafs, TFeat)\nstats_to_csv(\"TFeat\", \"True\", tfeat, overwrite=False)\ndisplay_example_from_batch(tfeat)\ndel tfeat\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:07:09.318077Z","iopub.execute_input":"2022-05-05T05:07:09.319363Z","iopub.status.idle":"2022-05-05T05:12:39.708668Z","shell.execute_reply.started":"2022-05-05T05:07:09.319324Z","shell.execute_reply":"2022-05-05T05:12:39.707987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Runs a matching proces with SOSNet descriptor\nsosnet = run_test(lafs, SOSNet)\nstats_to_csv(\"SOSNet\", \"True\", sosnet, overwrite=False)\ndisplay_example_from_batch(sosnet)\ndel sosnet\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:12:39.712435Z","iopub.execute_input":"2022-05-05T05:12:39.712925Z","iopub.status.idle":"2022-05-05T05:21:30.016767Z","shell.execute_reply.started":"2022-05-05T05:12:39.712888Z","shell.execute_reply":"2022-05-05T05:21:30.016115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Runs a matching proces with LoFTR matcher\n# This could also be considered a baseline \"to beat\" because it is consistently highest performing\n# Goal of project is finding edge cases where other descriptors might be better and/or ways to tweak LoFTR\nloftra = loftr_match(lafs, LOFTR_A)\nstats_to_csv(\"LoFTR A\", \"True\", loftra, overwrite=False)\ndisplay_example_from_batch(loftra, loftr=True)\ndel loftra\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T05:36:38.119084Z","iopub.execute_input":"2022-05-05T05:36:38.119699Z","iopub.status.idle":"2022-05-05T05:42:06.718114Z","shell.execute_reply.started":"2022-05-05T05:36:38.119658Z","shell.execute_reply":"2022-05-05T05:42:06.717459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"/kaggle/working/model_stats.csv\"> Download File </a>","metadata":{}},{"cell_type":"markdown","source":"## ","metadata":{}}]}