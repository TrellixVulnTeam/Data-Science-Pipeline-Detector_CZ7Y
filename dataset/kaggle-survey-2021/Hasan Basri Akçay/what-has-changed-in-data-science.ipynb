{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![Image](https://sawaed19.net/wp-content/uploads/2021/01/700600p546EDNmainimg-process-change-management1.jpg)\n\n[Image Source](https://sawaed19.net/en/event/workshop-youth-for-change/)","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nHey, thanks for viewing my Kernel!\n\nIf you like my work, please, leave an upvote: it will be really appreciated and it will motivate me in offering more content to the Kaggle community ! 😊\n\nThe objective of this notebook is to explore the changes in data science over the years. Therefore, we worked on two differents dataset that are [kaggle_survey_2021](https://www.kaggle.com/c/kaggle-survey-2021) and [kaggle_survey_2020](https://www.kaggle.com/c/kaggle-survey-2020/overview). Kaggle survey 2020 has 39+ questions, 20,036 responses and survey 2021 has 42+ questions, 25,973 responses. \n\nThe notebook consists of 5 parts.\n1. Introduction\n2. Data Preparation\n3. Data Cleaning\n4. Data Analysis\n5. Conclusion\n\nIn the introduction part, we started simply importing libraries and datasets.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\n\n\nwarnings.simplefilter(\"ignore\")\nsns.set()\n\ndf21 = pd.read_csv('../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv')\ndf20 = pd.read_csv('../input/kaggle-survey-2020/kaggle_survey_2020_responses.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-30T12:29:17.914418Z","iopub.execute_input":"2021-12-30T12:29:17.915161Z","iopub.status.idle":"2021-12-30T12:29:21.364329Z","shell.execute_reply.started":"2021-12-30T12:29:17.91503Z","shell.execute_reply":"2021-12-30T12:29:21.363544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df20.shape, df21.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T12:29:21.36577Z","iopub.execute_input":"2021-12-30T12:29:21.366733Z","iopub.status.idle":"2021-12-30T12:29:21.372197Z","shell.execute_reply.started":"2021-12-30T12:29:21.366684Z","shell.execute_reply":"2021-12-30T12:29:21.371575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df20.columns)\nprint(df21.columns)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T12:29:21.373474Z","iopub.execute_input":"2021-12-30T12:29:21.373885Z","iopub.status.idle":"2021-12-30T12:29:21.384539Z","shell.execute_reply.started":"2021-12-30T12:29:21.373855Z","shell.execute_reply":"2021-12-30T12:29:21.383922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation\n\nIn this part, we created 3 functions that are used for simplification the datasets. \n\nIn the datasets, some questions have more than one column and function **group_cols** is used for grouping the questions. For example, Q24 is one group, and Q12_Part_1, Q12_Part_2, Q12_Part_3, Q12_OTHER are also one group. \n\nFunction **part_cols_convert** is written for converting the questions that have more than one column to one column. For instance, this function converts Q12_Part_1, Q12_Part_2, Q12_Part_3, Q12_OTHER to Q12 column. \n\nThe last function is **dict_preparation** that is used for matching the same question in 2020 and 2021. Of course in the datasets, some questions mean are the same but the questions are different. We solved that kind of problem with manual correction. For example, Q12 is \"Which types of specialized hardware do you use on a regular basis?  (Select all that apply) - Selected Choice - GPUs\" in 2020 and \"Which types of specialized hardware do you use on a regular basis?  (Select all that apply) - Selected Choice -  NVIDIA GPUs\" in 2021\n\nAfter all preparation, we combined  survey 2020 and 2021 by function **prepare_data**.","metadata":{}},{"cell_type":"code","source":"def group_cols(df):\n    cols = df.columns\n    \n    col_part = []\n    for col in cols:\n        if '_' in col:\n            col_part.append(col)\n    \n    cols_1 = list(set(cols) - set(col_part))\n    \n    temp_df = pd.DataFrame(col_part)\n    temp_df['question'] = temp_df[0].str.split('_').str[0]\n    temp_group = temp_df.groupby('question')[0]\n    \n    cols_2 = []\n    for name, group in temp_group:\n        if len(group) > 1:\n            cols_2.append(list(group.values))\n    \n    return list(cols_1 + cols_2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-30T12:29:21.386369Z","iopub.execute_input":"2021-12-30T12:29:21.386893Z","iopub.status.idle":"2021-12-30T12:29:21.397339Z","shell.execute_reply.started":"2021-12-30T12:29:21.386845Z","shell.execute_reply":"2021-12-30T12:29:21.396559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def part_cols_convert(df):\n    cols = df.columns\n    \n    col_part = []\n    for col in cols:\n        if '_' in col:\n            col_part.append(col)\n    \n    temp_df = pd.DataFrame(col_part)\n    temp_df['question'] = temp_df[0].str.split('_').str[0]\n    temp_group = temp_df.groupby('question')[0]\n    \n    cols_2 = []\n    for name, group in temp_group:\n        if len(group) > 1:\n            cols_2.append(list(group.values))\n    \n    part_df_list = []\n    for cols in cols_2:\n        part_df = pd.DataFrame()\n        new_col = cols[0].split('_')[0]\n        \n        values_list = []\n        for col in cols:\n            str_value = df.loc[0, col].split('-')[-1]\n            count_num = df[col].value_counts()[0]\n            values = [str_value for i in range(count_num)]\n            values_list.extend(values)\n        \n        part_df[new_col] = values_list\n        part_df_list.append(part_df)\n    \n    df_parts = pd.concat(part_df_list, 1)\n    return df_parts","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-30T12:29:21.398728Z","iopub.execute_input":"2021-12-30T12:29:21.398951Z","iopub.status.idle":"2021-12-30T12:29:21.410834Z","shell.execute_reply.started":"2021-12-30T12:29:21.398926Z","shell.execute_reply":"2021-12-30T12:29:21.410054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dict_preparation(question_2020, question_2021, df20, df21):\n    same_questions_dict = {}\n    question_mean_dict = {}\n\n    for c_20 in question_2020:\n        if type(c_20) is list:\n            c_20 = c_20[0]\n            question_mean_dict[c_20.split('_')[0]] = df20.loc[0, c_20]\n            #print('c_20:', c_20 , df20.loc[0, c_20])\n        else:\n            question_mean_dict[c_20] = df20.loc[0, c_20]\n        q_20 = df20.loc[0, c_20]\n\n        for c_21 in question_2021:\n            if type(c_21) is list:\n                c_21 = c_21[0]\n            q_21 = df21.loc[0, c_21]\n            #print('c_21:', c_21, q_21)\n            if q_21 == q_20:\n                if '_' in c_20:\n                    if '_' in c_21:\n                        same_questions_dict[c_20.split('_')[0]] = c_21.split('_')[0]\n                    else:\n                        same_questions_dict[c_20.split('_')[0]] = c_21\n                else:\n                    if '_' in c_21:\n                        same_questions_dict[c_20] = c_21.split('_')[0]\n                    else:\n                        same_questions_dict[c_20] = c_21\n                break\n    return same_questions_dict, question_mean_dict","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-30T12:29:21.412151Z","iopub.execute_input":"2021-12-30T12:29:21.412354Z","iopub.status.idle":"2021-12-30T12:29:21.423431Z","shell.execute_reply.started":"2021-12-30T12:29:21.412329Z","shell.execute_reply":"2021-12-30T12:29:21.422569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df20_parts = part_cols_convert(df20)\ndf21_parts = part_cols_convert(df21)\n\nquestion_2020 = group_cols(df20)\nquestion_2021 = group_cols(df21)\n\nsame_questions_dict, question_mean_dict = dict_preparation(question_2020, question_2021, df20, df21)\n\nprint(same_questions_dict)\nprint(question_mean_dict)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-30T12:29:21.42439Z","iopub.execute_input":"2021-12-30T12:29:21.426684Z","iopub.status.idle":"2021-12-30T12:29:23.299878Z","shell.execute_reply.started":"2021-12-30T12:29:21.426649Z","shell.execute_reply":"2021-12-30T12:29:23.299017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(question_2020)\nprint(question_2021)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-12-30T12:29:23.30121Z","iopub.execute_input":"2021-12-30T12:29:23.302015Z","iopub.status.idle":"2021-12-30T12:29:23.308658Z","shell.execute_reply.started":"2021-12-30T12:29:23.30197Z","shell.execute_reply":"2021-12-30T12:29:23.307786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from termcolor import colored\n\ndiff_questions_20_list = ['Q12_Part_1', 'Q27_A_Part_1', 'Q27_B_Part_1', 'Q28_A_Part_1', 'Q28_B_Part_1', 'Q36_Part_1']\ndiff_questions_21_list = ['Q12_Part_1', 'Q27_A_Part_1', 'Q27_B_Part_1', 'Q28', 'Q36_A_Part_1', 'Q36_B_Part_1']\nmore_questions_list = ['Q40_Part_1', 'Q41', 'Q42_Part_1']\n\nprint(colored('1) ', 'green'), df20.loc[0, 'Q12_Part_1'], ' - ', df21.loc[0, 'Q12_Part_1'])\nprint(colored('2) ', 'green'), df20.loc[0, 'Q27_A_Part_1'], ' - ', df21.loc[0, 'Q27_A_Part_1'])\nprint(colored('3) ', 'green'), df20.loc[0, 'Q27_B_Part_1'], ' - ', df21.loc[0, 'Q27_B_Part_1'])\n\nprint(colored('3.5) ', 'green'), df20.loc[0, 'Q26_A_Part_1'], ' - ', df21.loc[0, 'Q27_A_Part_1'])\n\nprint(colored('4) ', 'red'), df20.loc[0, 'Q28_A_Part_1'], ' - ', df21.loc[0, 'Q28'])\nprint(colored('5) ', 'red'), df20.loc[0, 'Q28_B_Part_1'], ' - ', df21.loc[0, 'Q28'])\nprint(colored('6) ', 'red'), df20.loc[0, 'Q36_Part_1'], ' - ', df21.loc[0, 'Q36_A_Part_1'])\nprint(colored('7) ', 'red'), df20.loc[0, 'Q36_Part_1'], ' - ', df21.loc[0, 'Q36_B_Part_1'])\n\nprint(colored('8) ', 'blue'), df21.loc[0, 'Q40_Part_1'])\nprint(colored('9) ', 'blue'), df21.loc[0, 'Q41'])\nprint(colored('10) ', 'blue'), df21.loc[0, 'Q42_Part_1'])\n\nsame_questions_dict['Q12'] = 'Q12'","metadata":{"execution":{"iopub.status.busy":"2021-12-30T12:29:23.310246Z","iopub.execute_input":"2021-12-30T12:29:23.311091Z","iopub.status.idle":"2021-12-30T12:29:23.333804Z","shell.execute_reply.started":"2021-12-30T12:29:23.311047Z","shell.execute_reply":"2021-12-30T12:29:23.333058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_exist_2020 = ['Q27', 'Q28', 'Q36']\nnot_exist_2021 = ['Q20', 'Q28', 'Q29', 'Q30', 'Q31', 'Q39']\n\nprint(colored('1) 2020 --- ', 'blue'), df20.loc[0, 'Q27_A_Part_1'])\nprint(colored('2) 2020 --- ', 'blue'), df20.loc[0, 'Q28_A_Part_1'])\nprint(colored('3) 2020 --- ', 'blue'), df20.loc[0, 'Q36_Part_1'])\nprint()\nprint(colored('1) 2021 --- ', 'red'), df21.loc[0, 'Q20'])\nprint(colored('2) 2021 --- ', 'red'), df21.loc[0, 'Q28'])\nprint(colored('3) 2021 --- ', 'red'), df21.loc[0, 'Q29_A_Part_1'])\nprint(colored('4) 2021 --- ', 'red'), df21.loc[0, 'Q30_A_Part_1'])\nprint(colored('5) 2021 --- ', 'red'), df21.loc[0, 'Q31_A_Part_1'])\nprint(colored('6) 2021 --- ', 'red'), df21.loc[0, 'Q39_Part_1'])\n\nsame_questions_dict['Q27'] = 'Q29'\nsame_questions_dict['Q28'] = 'Q31'\nsame_questions_dict['Q36'] = 'Q39'","metadata":{"execution":{"iopub.status.busy":"2021-12-30T12:29:23.337051Z","iopub.execute_input":"2021-12-30T12:29:23.337816Z","iopub.status.idle":"2021-12-30T12:29:23.351433Z","shell.execute_reply.started":"2021-12-30T12:29:23.337776Z","shell.execute_reply":"2021-12-30T12:29:23.350531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(same_questions_dict, df20, df21, df20_parts, df21_parts):\n    cols_20, cols_21 = [], []\n    part_cols_20, part_cols_21 = [], []\n    for key in same_questions_dict.keys():\n        if key in df20_parts.columns:\n            part_cols_20.append(key)\n            part_cols_21.append(same_questions_dict[key])\n        else:\n            cols_20.append(key)\n            cols_21.append(same_questions_dict[key])\n    \n    df20['years'] = 2020\n    df21['years'] = 2021\n    df20_parts['years'] = 2020\n    df21_parts['years'] = 2021\n    \n    cols_20.append('years')\n    cols_21.append('years')\n    part_cols_20.append('years')\n    part_cols_21.append('years')\n    \n    temp_df21 = df21[cols_21]\n    temp_df21.columns = cols_20\n    temp_df21_parts = df21_parts[part_cols_21]\n    temp_df21_parts.columns = part_cols_20\n    \n    df_20_21 = pd.concat([df20[cols_20].loc[1:, :], temp_df21.loc[1:, :]], join='outer')\n    df_part_20_21 = pd.concat([df20_parts[part_cols_20].loc[1:, :], temp_df21_parts.loc[1:, :]], join='outer')\n    \n    return df_20_21, df_part_20_21","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-30T12:29:23.352834Z","iopub.execute_input":"2021-12-30T12:29:23.353733Z","iopub.status.idle":"2021-12-30T12:29:23.365359Z","shell.execute_reply.started":"2021-12-30T12:29:23.35369Z","shell.execute_reply":"2021-12-30T12:29:23.364726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_20_21, df_part_20_21 = prepare_data(same_questions_dict, df20, df21, df20_parts, df21_parts)\n\nprint(df_20_21.shape)\nprint(df_part_20_21.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T12:29:23.366769Z","iopub.execute_input":"2021-12-30T12:29:23.367582Z","iopub.status.idle":"2021-12-30T12:29:23.580101Z","shell.execute_reply.started":"2021-12-30T12:29:23.367539Z","shell.execute_reply":"2021-12-30T12:29:23.579237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning\n\nData cleaning is one of the most importants part of data science. As with most datasets, this dataset needs data cleaning. According to my view, some answers were split like 'Product/Project Manager' to 'Program/Project Manager', 'Product Manager' and some answers have been fixed like PostgresSQL to PostgreSQL in 2021. In the below, we tried to match the same answers.","metadata":{}},{"cell_type":"code","source":"df_20_21_clean = df_20_21.copy()\ndf_part_20_21_clean = df_part_20_21.copy()\n\ndf_20_21_clean['Q6'] = df_20_21_clean['Q6'].str.replace('1-3 years', '1-2 years')\ndf_20_21_clean['Q30'] = df_20_21_clean['Q30'].str.replace('PostgresSQL', 'PostgreSQL')\ndf_20_21_clean['Q4'] = df_20_21_clean['Q4'].str.replace('Professional degree', 'Professional doctorate')\ndf_20_21_clean['Q5'] = df_20_21_clean['Q5'].str.replace('Program/Project Manager', 'Product/Project Manager')\ndf_20_21_clean['Q5'] = df_20_21_clean['Q5'].str.replace('Product Manager', 'Product/Project Manager')\ndf_20_21_clean['Q24'] = df_20_21_clean['Q24'].str.replace('$', '')\ndf_20_21_clean['Q24'] = df_20_21_clean['Q24'].str.replace('300,000-499,999', '300,000-500,000')\ndf_20_21_clean['Q24'] = df_20_21_clean['Q24'].str.replace('500,000-999,999', '> 500,000')\ndf_20_21_clean['Q24'] = df_20_21_clean['Q24'].str.replace('>1,000,000', '> 500,000')\ndf_20_21_clean['Q11'] = df_20_21_clean['Q11'].str.replace('A personal computer / desktop', 'A personal computer or laptop')\ndf_20_21_clean['Q11'] = df_20_21_clean['Q11'].str.replace('A laptop', 'A personal computer or laptop')\n\ndf_part_20_21_clean['Q10'] = df_part_20_21_clean['Q10'].str.replace('  Amazon Sagemaker Studio Notebooks ', '  Amazon Sagemaker Studio ')\ndf_part_20_21_clean['Q10'] = df_part_20_21_clean['Q10'].str.replace('\\n', '')\ndf_part_20_21_clean['Q10'] = df_part_20_21_clean['Q10'].str.replace(' Google Cloud Datalab Notebooks', ' Google Cloud Datalab')\ndf_part_20_21_clean['Q10'] = df_part_20_21_clean['Q10'].str.replace(' Google Cloud AI Platform Notebooks ', ' Google Cloud Notebooks (AI Platform / Vertex AI) ')\ndf_part_20_21_clean['Q29'] = df_part_20_21_clean['Q29'].str.replace('PostgresSQL', 'PostgreSQL')\ndf_part_20_21_clean['Q29'] = df_part_20_21_clean['Q29'].str.replace('\\n', '')\ndf_part_20_21_clean['Q29'] = df_part_20_21_clean['Q29'].str.replace(' Microsoft Azure SQL Database ', ' Microsoft Azure Data Lake Storage ')\ndf_part_20_21_clean['Q29'] = df_part_20_21_clean['Q29'].str.replace(' Microsoft Azure Cosmos DB ', ' Microsoft Azure Data Lake Storage ')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace('\\n', '')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace('(', '')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace(')', '')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace(' Automation of full ML pipelines e.g. Google AutoML, H2O Driverless AI', \n                                                        ' Automation of full ML pipelines AutoML')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace(' Automation of full ML pipelines e.g. Google Cloud AutoML, H2O Driverless AI', \n                                                        ' Automation of full ML pipelines Cloud AutoML')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace(' Automation of full ML pipelines e.g. Google AutoML, H20 Driverless AI', \n                                                        ' Automation of full ML pipelines AutoML')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace(' Automation of full ML pipelines e.g. Google Cloud AutoML, H20 Driverless AI', \n                                                        ' Automation of full ML pipelines Cloud AutoML')\ndf_part_20_21_clean['Q34'] = df_part_20_21_clean['Q34'].str.replace('  H20 Driverless AI  ', '  H2O Driverless AI  ')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('  Visual Studio / Visual Studio Code ', '  VisualStudio ')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('(', '')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace(')', '')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('  Visual Studio Code VSCode ', '  VisualStudio ')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('  Visual Studio ', '  VisualStudio ')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('  VisualStudio ', '  Visual Studio / Visual Studio Code ')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace(' Jupyter (JupyterLab, Jupyter Notebooks, etc) ', '  Jupyter Notebook')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('  Jupyter Notebook', ' Jupyter (JupyterLab, Jupyter Notebooks, etc) ')\ndf_part_20_21_clean['Q12'] = df_part_20_21_clean['Q12'].str.replace('  Google Cloud TPUs ', ' TPUs')\ndf_part_20_21_clean['Q12'] = df_part_20_21_clean['Q12'].str.replace('  NVIDIA GPUs ', ' GPUs')\ndf_part_20_21_clean['Q27'] = df_part_20_21_clean['Q27'].str.replace('  Amazon Elastic Container Service ', '  Amazon Elastic Compute Cloud (EC2) ')\ndf_part_20_21_clean['Q27'] = df_part_20_21_clean['Q27'].str.replace('  Microsoft Azure Container Instances ', '  Microsoft Azure Virtual Machines ')\n\n\n#print(df_20_21_clean['Q11'].unique())","metadata":{"_kg_hide-output":true,"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-30T12:29:23.581569Z","iopub.execute_input":"2021-12-30T12:29:23.581784Z","iopub.status.idle":"2021-12-30T12:29:25.927127Z","shell.execute_reply.started":"2021-12-30T12:29:23.581758Z","shell.execute_reply":"2021-12-30T12:29:25.926332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis\n\nIn this part, we plotted all questions for visual pieces of information. We created 2 functions. \n\nFunction **long_sentences_seperate** is used for visual editing. For instance, if a question or an answer text is so long for plotting, this function splits the text by adding '\\n' to the text.\n\nThe **barplot_all_cols** function is used for plotting all columns. For color, we selected the 'years' column.","metadata":{}},{"cell_type":"code","source":"def long_sentences_seperate(sentence, step=10):\n    try:\n        splittext = sentence.split(\" \")\n        for x in range(step, len(splittext), step):\n            splittext[x] = \"\\n\"+splittext[x].lstrip()\n        text = \" \".join(splittext)\n        return text\n    except:\n        return sentence","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-12-30T12:29:25.92833Z","iopub.execute_input":"2021-12-30T12:29:25.928605Z","iopub.status.idle":"2021-12-30T12:29:25.934711Z","shell.execute_reply.started":"2021-12-30T12:29:25.928575Z","shell.execute_reply":"2021-12-30T12:29:25.933541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def barplot_all_cols(df, question_mean_dict, df_cols, figsize=(24, 96)):\n    response_num_2020 = df20.shape[0]\n    response_num_2021 = df21.shape[0]\n    \n    ncols = 2\n    nrows = round(len(df_cols) / ncols)\n    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)\n    plt.subplots_adjust(hspace=0.3)\n    \n    index = 0\n    for row in range(nrows):\n        for col in range(ncols):\n            try:\n                col_name = df_cols[index]\n                question = question_mean_dict[col_name]\n                question = long_sentences_seperate(question, step=10)\n            except:\n                axes[row][col].set_visible(False)\n                continue\n            \n            if col_name == 'Q3':\n                selected_countries = df[col_name].value_counts(normalize=True).index[:10]\n                temp_df = df[df[col_name].isin(selected_countries)]\n                \n                temp_df = temp_df.groupby([col_name, 'years']).agg({col_name:'count'})\n                temp_df.columns = ['counts']\n                temp_df.reset_index(inplace=True)\n            else:\n                temp_df = df.groupby([col_name, 'years']).agg({col_name:'count'})\n                temp_df.columns = ['counts']\n                temp_df.reset_index(inplace=True)\n            \n            temp_df.loc[temp_df['years'] == 2020, 'counts_norm'] = temp_df.loc[temp_df['years'] == 2020, 'counts'] / response_num_2020\n            temp_df.loc[temp_df['years'] == 2021, 'counts_norm'] = temp_df.loc[temp_df['years'] == 2021, 'counts'] / response_num_2021\n            temp_df[col_name] = temp_df[col_name].apply(lambda x: long_sentences_seperate(x, step=4))\n            \n            ### Find The Order That Biggest Change to Lowest Change\n            count_df = temp_df[col_name].value_counts()\n            selected_values = list(count_df[count_df > 1].index)\n            clean_temp_df = temp_df[temp_df[col_name].isin(selected_values)]\n            changes_list = ((clean_temp_df.loc[clean_temp_df['years'] == 2021, 'counts'].values - clean_temp_df.loc[clean_temp_df['years'] == 2020, 'counts'].values) / \n                            clean_temp_df.loc[clean_temp_df['years'] == 2020, 'counts'].values)\n            change_twice_list = []\n            for value in changes_list:\n                change_twice_list.append(value)\n                change_twice_list.append(value)\n            clean_temp_df['change'] = change_twice_list\n            clean_temp_df.sort_values('change', inplace=True, ascending=False)\n            order_list = list(clean_temp_df[col_name].unique())\n            temp_df_unique = temp_df[col_name].unique()\n            diff_order = list(set(temp_df_unique) - set(order_list))\n            if len(diff_order) > 0:\n                order_list.extend(diff_order)\n            ###\n            \n            sns.barplot(data=temp_df, x='counts_norm', y=col_name, hue='years', order=order_list, ax=axes[row][col])\n            axes[row][col].set_title(question)\n            for p in axes[row][col].patches:\n                txt = str(p.get_width().round(3))\n                txt_x = p.get_width() \n                txt_y = p.get_y() + p.get_height() * 2 / 5\n                bar_color = p.get_facecolor()\n                try:\n                    if bar_color == (0.34705882352941175, 0.4588235294117645, 0.6411764705882353, 1.0):\n                        txt_count = str(round(p.get_width() * response_num_2020))\n                    elif bar_color == (0.7985294117647057, 0.536764705882353, 0.38970588235294135, 1.0):\n                        txt_count = str(round(p.get_width() * response_num_2021))\n                except:\n                    txt_count = 0\n                txt_count_y = p.get_y() + p.get_height() * 4 / 5\n                axes[row][col].text(txt_x,txt_y,txt,color=bar_color)\n                axes[row][col].text(txt_x,txt_count_y,txt_count,color=bar_color)\n            \n            index += 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-30T12:29:25.936048Z","iopub.execute_input":"2021-12-30T12:29:25.936259Z","iopub.status.idle":"2021-12-30T12:29:25.959489Z","shell.execute_reply.started":"2021-12-30T12:29:25.936232Z","shell.execute_reply":"2021-12-30T12:29:25.958762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DS_col = ['Q11', 'Q32', 'Q3', 'Q4', 'Q1', 'Q38', 'Q13', 'Q30', 'Q6', 'Q25', 'Q5', 'Q8']\n\nbarplot_all_cols(df_20_21_clean, question_mean_dict, DS_col)","metadata":{"execution":{"iopub.status.busy":"2021-12-30T12:29:25.960317Z","iopub.execute_input":"2021-12-30T12:29:25.960544Z","iopub.status.idle":"2021-12-30T12:29:34.905161Z","shell.execute_reply.started":"2021-12-30T12:29:25.960505Z","shell.execute_reply":"2021-12-30T12:29:34.904262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"barplot_all_cols(df_part_20_21_clean, question_mean_dict, df_part_20_21_clean.columns, figsize=(24, 192))","metadata":{"execution":{"iopub.status.busy":"2021-12-30T12:29:34.906612Z","iopub.execute_input":"2021-12-30T12:29:34.907127Z","iopub.status.idle":"2021-12-30T12:29:53.284397Z","shell.execute_reply.started":"2021-12-30T12:29:34.907086Z","shell.execute_reply":"2021-12-30T12:29:53.283414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nIn 2021, all usage of cloud computing increase. The most famous cloud computing tools are AWS, GCP, Microsoft Azure. The cloud computing process will be used more in 2022 ☁️.\n\nBusiness Intelligence Tools are also increasing. Microsoft Power BI increase 462 to 790 and Tableau increase 540 to 740. We can say Microsoft Power BI will be used much more than Tableau in the future.\n\nIn 2020, the most common age was 22–24 in Data Science. Now, it is 18–21. Welcome young Data Scientists 👋. Also 70+ increase from 76 to 128. The range of age of data scientists getting bigger. On the other hand, the answer that is \"I have never written code\" is decreased even if 18–21 age is increased in 2021. We can say \"Code age is decreased\" 🔥.\nThe number of Data Scientists increased around all of the worlds. Most increase in China 🌍.\n\nIn the usage of TPU, 2–5 times increase 2012 to 3405, 6–25 increase 424 to 947 and 25+ increase 272 to 612. We can say \"We will hear the name of TPU much more in 2022\". Usage of GPU decreased from 8309 to 8035 and TPUs increased 960 to 3451. In 2022 TPUs can be more used than GPUs 📱.\n\nUngraduated and Bachelor's degree increased but Professional doctorate decreased from 699 to 360. That is almost half. This situation can be caused by the Kaggle survey. Maybe data scientists that have Professional doctorates, stoped using Kaggle 📚.\n\nIn general, usage of all of the data products increased. Most increase in MySQL. In that article (What Are The Differences Between Data Scientists That Earn 500💲 And 225.000💲 Yearly?), it was also said that databases are so important for data scientists.\n\nIn general, all of the jobs increased but Business Analysts and Statisticians can be assumed to be unchanged. Now, we have a new job title that is Developer Relations/Advocacy 💼.\n\nIn, Hosted Notebook Products, Binder/JupyterHub decreased from 2072 to 1770 and Kaggle Notebooks increased 5991 to 9506, Colab Notebooks increased 6329 to 9792. The most increase is in Google Cloud Notebooks (AI Platform / Vertex AI) 📓.\nAll usage of data visualization libraries increased and the most increase is in Seaborn📊 8821 to 12586. In IDEs, all usages of IDEs increased but the most increase is in Visual Studio / Visual Studio Code 2445 to 14150. The second is in Jupyter 11210 to 21720.\n\nIn ML Frameworks, Tensorflow, Pytorch, and Xgboost all increased but the most increase in Xgboost 3935 to 5974, CatBoost 957 to 1512, and JAX 84 to 190. Also, we have new selections that are Huggingface and Pytorch Lightning. In Computer Vision, all usage of computer vision algorithms increased but the most increase is in CNN 2003 to 2740 and GAN 1092 to 1492. In Natural Language Processing (NLP), all usage of NLP increased but the most increase is in BERT 1428 to 2351.\n\nAnother question is about programing language recommendations. In this question, only Swift decreased. The most increase in SQL. In the use of programming language, Python🐍 is the most famous and the most increase in Javascript 2995 to 4332.\nIn the question that is about the most important part of work, the most percentage is in analyzing and understanding data to the product or business decisions 6420 to 9107. 35 percent of data scientists gave this answer and the most increase is in the \"None of these activities\" answer. There can be a new role that is not clear yet in data science.\n\nAutomated ML Tools are mostly used in ML pipelines in 2021. Probably it is still will be used ML pipelines in 2022. The most increase is in Databricks AutoML 948 to 1970 and Google Cloud AutoML 2839 to 5567. Also, Google AutoML is the most famous, and Amazon Sagemaker Autopilot, Azure Automate Machine Learning are new in Automated ML Tools ⚙️.\n\nIn the question of the course, the most increase is in Kaggle Learn Courses but this data is not trustable because the survey belongs to Kaggle. Other important courses are Certification Programs(AWS, Azure, GCP, etc) increased from 1076 to 1804 and LinkedIn Learning increased from 1617 to 2093. Also, in general, spending money for ml increased 📚.\n\nIn the question that is about sharing or deployment, the most famous tool is Github that increased 3434 to 4586. The most increase is in Streamlight 186 to 387, Kaggle 1878 to 3065, and Colab 1247 to 1848.\n\nIf you like my work, please, leave an upvote: it will be really appreciated and it will motivate me in offering more content to the Kaggle community ! 😊","metadata":{}}]}