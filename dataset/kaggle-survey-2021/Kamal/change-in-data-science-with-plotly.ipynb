{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis kernel depends on the data cleaning and preparation done by HASAN BASRI AKÃ‡AY in the notebook\n\nhttps://www.kaggle.com/hasanbasriakcay/what-has-changed-in-data-science/comments?kernelSessionId=84205371\n\n\n# Purpose \n\nThe objective of this notebook is to explore the changes in data science with the Plotly Express charts. Learn about the interfaces available with Plotly Express to render the data, in a more legible, friendly and usable manner. Also, find some interesting hacks that can help in making the chart rendering more fun, and rewarding.\n\n1. Introduction\n2. Data Preparation\n3. Data Cleaning\n*** All the above done by Hasan***\n\n4. Data rendering with Plotly\n","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nI believe that, one has to see the result before exploring how it was achieved. You will see the plots first. Then the conclusion. Rest of the codes will be hidden, which can be viewed and learned from.","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\nwarnings.simplefilter(\"ignore\")\nsns.set()\n\ndf21 = pd.read_csv('../input/kaggle-survey-2021/kaggle_survey_2021_responses.csv')\ndf20 = pd.read_csv('../input/kaggle-survey-2020/kaggle_survey_2020_responses.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T09:02:04.289496Z","iopub.execute_input":"2022-01-08T09:02:04.290142Z","iopub.status.idle":"2022-01-08T09:02:09.419918Z","shell.execute_reply.started":"2022-01-08T09:02:04.290027Z","shell.execute_reply":"2022-01-08T09:02:09.418747Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation\n\nIn this part, we created 3 functions that are used for simplification the datasets. \n\nIn the datasets, some questions have more than one column and function **group_cols** is used for grouping the questions. For example, Q24 is one group, and Q12_Part_1, Q12_Part_2, Q12_Part_3, Q12_OTHER are also one group. \n\nFunction **part_cols_convert** is written for converting the questions that have more than one column to one column. For instance, this function converts Q12_Part_1, Q12_Part_2, Q12_Part_3, Q12_OTHER to Q12 column. \n\nThe last function is **dict_preparation** that is used for matching the same question in 2020 and 2021. Of course in the datasets, some questions mean are the same but the questions are different. We solved that kind of problem with manual correction. For example, Q12 is \"Which types of specialized hardware do you use on a regular basis?  (Select all that apply) - Selected Choice - GPUs\" in 2020 and \"Which types of specialized hardware do you use on a regular basis?  (Select all that apply) - Selected Choice -  NVIDIA GPUs\" in 2021\n\nAfter all preparation, we combined  survey 2020 and 2021 by function **prepare_data**.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"def group_cols(df):\n    cols = df.columns # listing the columns\n    \n    col_part = [] # creating empty array\n    for col in cols: \n        if '_' in col: #This checks if the column is part of the questions\n            col_part.append(col) #Appending the column names to the array\n    \n    cols_1 = list(set(cols) - set(col_part)) #Removes the elements that are having underscores.\n    \n    temp_df = pd.DataFrame(col_part) #creating dataframe with the columns that are questions part\n    temp_df['question'] = temp_df[0].str.split('_').str[0]\n    temp_group = temp_df.groupby('question')[0] #This gives a groupby object, which can again be used \n    \n    cols_2 = []\n    for name, group in temp_group:\n        if len(group) > 1: # This checks if there are multiple values in the group. \n            cols_2.append(list(group.values)) #Then it creates, multi dimensional list\n    \n    return list(cols_1+cols_2)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T10:22:01.343982Z","iopub.execute_input":"2022-01-08T10:22:01.344301Z","iopub.status.idle":"2022-01-08T10:22:01.353546Z","shell.execute_reply.started":"2022-01-08T10:22:01.34427Z","shell.execute_reply":"2022-01-08T10:22:01.352483Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def part_cols_convert(df):\n    cols = df.columns\n    \n    col_part = []\n    for col in cols:\n        if '_' in col:\n            col_part.append(col) #till here same as group cols\n    \n    temp_df = pd.DataFrame(col_part)\n    temp_df['question'] = temp_df[0].str.split('_').str[0]\n    temp_group = temp_df.groupby('question')[0] #gives out a groupby object\n    \n    cols_2 = []\n    for name, group in temp_group:\n        if len(group) > 1:\n            cols_2.append(list(group.values)) # creates a column list\n    \n    part_df_list = []\n    for cols in cols_2:\n        part_df = pd.DataFrame()\n        new_col = cols[0].split('_')[0]#This will be the question number\n        \n        values_list = []\n        for col in cols:\n            str_value = df.loc[0, col].split('-')[-1]# this takes the default selection \n            count_num = df[col].value_counts()[0] #Counts the number of values in the column\n            values = [str_value for i in range(count_num)]\n            values_list.extend(values) # each of the list element is appended, as seperate element \n        \n        part_df[new_col] = values_list #list is populated under the new_col column name\n        part_df_list.append(part_df) #part_df is appended to the main part_df_list\n    \n    df_parts = pd.concat(part_df_list, 1)\n    return df_parts","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T10:22:05.07596Z","iopub.execute_input":"2022-01-08T10:22:05.076275Z","iopub.status.idle":"2022-01-08T10:22:05.089447Z","shell.execute_reply.started":"2022-01-08T10:22:05.076244Z","shell.execute_reply":"2022-01-08T10:22:05.088391Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dict_preparation(question_2020, question_2021, df20, df21):\n    same_questions_dict = {} #Creating empty dictionaries\n    question_mean_dict = {} #What is this dictionary for\n\n    for c_20 in question_2020: #iterates on the question series that is created from group cols\n        if type(c_20) is list:\n            c_20 = c_20[0]\n            question_mean_dict[c_20.split('_')[0]] = df20.loc[0, c_20]\n            #print('c_20:', c_20 , df20.loc[0, c_20])\n        else:\n            question_mean_dict[c_20] = df20.loc[0, c_20]\n        q_20 = df20.loc[0, c_20]\n\n        for c_21 in question_2021:\n            if type(c_21) is list:\n                c_21 = c_21[0]\n            q_21 = df21.loc[0, c_21]\n            #print('c_21:', c_21, q_21)\n            if q_21 == q_20:\n                if '_' in c_20:\n                    if '_' in c_21:\n                        same_questions_dict[c_20.split('_')[0]] = c_21.split('_')[0]\n                    else:\n                        same_questions_dict[c_20.split('_')[0]] = c_21\n                else:\n                    if '_' in c_21:\n                        same_questions_dict[c_20] = c_21.split('_')[0]\n                    else:\n                        same_questions_dict[c_20] = c_21\n                break\n    return same_questions_dict, question_mean_dict","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T10:22:11.066086Z","iopub.execute_input":"2022-01-08T10:22:11.066414Z","iopub.status.idle":"2022-01-08T10:22:11.079522Z","shell.execute_reply.started":"2022-01-08T10:22:11.066384Z","shell.execute_reply":"2022-01-08T10:22:11.078448Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df20_parts = part_cols_convert(df20)\ndf21_parts = part_cols_convert(df21)\n\nquestion_2020 = group_cols(df20) #This creates list of the questions headers that have questions with multiple parts\nquestion_2021 = group_cols(df21)\n\nsame_questions_dict, question_mean_dict = dict_preparation(question_2020, question_2021, df20, df21)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-01-08T10:22:17.896188Z","iopub.execute_input":"2022-01-08T10:22:17.89662Z","iopub.status.idle":"2022-01-08T10:22:19.597649Z","shell.execute_reply.started":"2022-01-08T10:22:17.896588Z","shell.execute_reply":"2022-01-08T10:22:19.596787Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from termcolor import colored\n\ndiff_questions_20_list = ['Q12_Part_1', 'Q27_A_Part_1', 'Q27_B_Part_1', 'Q28_A_Part_1', 'Q28_B_Part_1', 'Q36_Part_1']\ndiff_questions_21_list = ['Q12_Part_1', 'Q27_A_Part_1', 'Q27_B_Part_1', 'Q28', 'Q36_A_Part_1', 'Q36_B_Part_1']\nmore_questions_list = ['Q40_Part_1', 'Q41', 'Q42_Part_1']\n\nprint(colored('1) ', 'green'), df20.loc[0, 'Q12_Part_1'], ' - ', df21.loc[0, 'Q12_Part_1'])\nprint(colored('2) ', 'green'), df20.loc[0, 'Q27_A_Part_1'], ' - ', df21.loc[0, 'Q27_A_Part_1'])\nprint(colored('3) ', 'green'), df20.loc[0, 'Q27_B_Part_1'], ' - ', df21.loc[0, 'Q27_B_Part_1'])\n\nprint(colored('3.5) ', 'green'), df20.loc[0, 'Q26_A_Part_1'], ' - ', df21.loc[0, 'Q27_A_Part_1'])\n\nprint(colored('4) ', 'red'), df20.loc[0, 'Q28_A_Part_1'], ' - ', df21.loc[0, 'Q28'])\nprint(colored('5) ', 'red'), df20.loc[0, 'Q28_B_Part_1'], ' - ', df21.loc[0, 'Q28'])\nprint(colored('6) ', 'red'), df20.loc[0, 'Q36_Part_1'], ' - ', df21.loc[0, 'Q36_A_Part_1'])\nprint(colored('7) ', 'red'), df20.loc[0, 'Q36_Part_1'], ' - ', df21.loc[0, 'Q36_B_Part_1'])\n\nprint(colored('8) ', 'blue'), df21.loc[0, 'Q40_Part_1'])\nprint(colored('9) ', 'blue'), df21.loc[0, 'Q41'])\nprint(colored('10) ', 'blue'), df21.loc[0, 'Q42_Part_1'])\n\nsame_questions_dict['Q12'] = 'Q12'","metadata":{"execution":{"iopub.status.busy":"2022-01-08T09:02:11.365569Z","iopub.execute_input":"2022-01-08T09:02:11.365796Z","iopub.status.idle":"2022-01-08T09:02:11.390818Z","shell.execute_reply.started":"2022-01-08T09:02:11.365769Z","shell.execute_reply":"2022-01-08T09:02:11.390002Z"},"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"not_exist_2020 = ['Q27', 'Q28', 'Q36']\nnot_exist_2021 = ['Q20', 'Q28', 'Q29', 'Q30', 'Q31', 'Q39']\n\nprint(colored('1) 2020 --- ', 'blue'), df20.loc[0, 'Q27_A_Part_1'])\nprint(colored('2) 2020 --- ', 'blue'), df20.loc[0, 'Q28_A_Part_1'])\nprint(colored('3) 2020 --- ', 'blue'), df20.loc[0, 'Q36_Part_1'])\nprint()\nprint(colored('1) 2021 --- ', 'red'), df21.loc[0, 'Q20'])\nprint(colored('2) 2021 --- ', 'red'), df21.loc[0, 'Q28'])\nprint(colored('3) 2021 --- ', 'red'), df21.loc[0, 'Q29_A_Part_1'])\nprint(colored('4) 2021 --- ', 'red'), df21.loc[0, 'Q30_A_Part_1'])\nprint(colored('5) 2021 --- ', 'red'), df21.loc[0, 'Q31_A_Part_1'])\nprint(colored('6) 2021 --- ', 'red'), df21.loc[0, 'Q39_Part_1'])\n\nsame_questions_dict['Q27'] = 'Q29'\nsame_questions_dict['Q28'] = 'Q31'\nsame_questions_dict['Q36'] = 'Q39'","metadata":{"execution":{"iopub.status.busy":"2022-01-08T09:02:11.393633Z","iopub.execute_input":"2022-01-08T09:02:11.394333Z","iopub.status.idle":"2022-01-08T09:02:11.407227Z","shell.execute_reply.started":"2022-01-08T09:02:11.394281Z","shell.execute_reply":"2022-01-08T09:02:11.406612Z"},"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(same_questions_dict, df20, df21, df20_parts, df21_parts):\n    cols_20, cols_21 = [], []\n    part_cols_20, part_cols_21 = [], []\n    for key in same_questions_dict.keys():\n        if key in df20_parts.columns:\n            part_cols_20.append(key)\n            part_cols_21.append(same_questions_dict[key])\n        else:\n            cols_20.append(key)\n            cols_21.append(same_questions_dict[key])\n    \n    df20['years'] = 2020\n    df21['years'] = 2021\n    df20_parts['years'] = 2020\n    df21_parts['years'] = 2021\n    \n    cols_20.append('years')\n    cols_21.append('years')\n    part_cols_20.append('years')\n    part_cols_21.append('years')\n    \n    temp_df21 = df21[cols_21]\n    temp_df21.columns = cols_20\n    temp_df21_parts = df21_parts[part_cols_21]\n    temp_df21_parts.columns = part_cols_20\n    \n    df_20_21 = pd.concat([df20[cols_20].loc[1:, :], temp_df21.loc[1:, :]], join='outer')\n    df_part_20_21 = pd.concat([df20_parts[part_cols_20].loc[1:, :], temp_df21_parts.loc[1:, :]], join='outer')\n    \n    return df_20_21, df_part_20_21","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T10:22:41.45373Z","iopub.execute_input":"2022-01-08T10:22:41.454438Z","iopub.status.idle":"2022-01-08T10:22:41.465104Z","shell.execute_reply.started":"2022-01-08T10:22:41.45439Z","shell.execute_reply":"2022-01-08T10:22:41.464526Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_20_21, df_part_20_21 = prepare_data(same_questions_dict, df20, df21, df20_parts, df21_parts)\n\nprint(df_20_21.shape)\nprint(df_part_20_21.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T10:22:57.671134Z","iopub.execute_input":"2022-01-08T10:22:57.672037Z","iopub.status.idle":"2022-01-08T10:22:57.892826Z","shell.execute_reply.started":"2022-01-08T10:22:57.671999Z","shell.execute_reply":"2022-01-08T10:22:57.891829Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning\n\nData cleaning is one of the most importants part of data science. As with most datasets, this dataset needs data cleaning. According to my view, some answers were split like 'Product/Project Manager' to 'Program/Project Manager', 'Product Manager' and some answers have been fixed like PostgresSQL to PostgreSQL in 2021. In the below, we tried to match the same answers.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"df_20_21_clean = df_20_21.copy()\ndf_part_20_21_clean = df_part_20_21.copy()\n\ndf_20_21_clean['Q6'] = df_20_21_clean['Q6'].str.replace('1-3 years', '1-2 years')\ndf_20_21_clean['Q30'] = df_20_21_clean['Q30'].str.replace('PostgresSQL', 'PostgreSQL')\ndf_20_21_clean['Q4'] = df_20_21_clean['Q4'].str.replace('Professional degree', 'Professional doctorate')\ndf_20_21_clean['Q5'] = df_20_21_clean['Q5'].str.replace('Program/Project Manager', 'Product/Project Manager')\ndf_20_21_clean['Q5'] = df_20_21_clean['Q5'].str.replace('Product Manager', 'Product/Project Manager')\ndf_20_21_clean['Q24'] = df_20_21_clean['Q24'].str.replace('$', '')\ndf_20_21_clean['Q24'] = df_20_21_clean['Q24'].str.replace('300,000-499,999', '300,000-500,000')\ndf_20_21_clean['Q24'] = df_20_21_clean['Q24'].str.replace('500,000-999,999', '> 500,000')\ndf_20_21_clean['Q24'] = df_20_21_clean['Q24'].str.replace('>1,000,000', '> 500,000')\ndf_20_21_clean['Q11'] = df_20_21_clean['Q11'].str.replace('A personal computer / desktop', 'A personal computer or laptop')\ndf_20_21_clean['Q11'] = df_20_21_clean['Q11'].str.replace('A laptop', 'A personal computer or laptop')\ndf_20_21_clean['Q11'] = df_20_21_clean['Q11'].str.replace('A cloud computing platform (AWS, Azure, GCP, hosted notebooks, etc)', 'Cloud computing platform')\n\ndf_part_20_21_clean['Q10'] = df_part_20_21_clean['Q10'].str.replace('  Amazon Sagemaker Studio Notebooks ', '  Amazon Sagemaker Studio ')\ndf_part_20_21_clean['Q10'] = df_part_20_21_clean['Q10'].str.replace('\\n', '')\ndf_part_20_21_clean['Q10'] = df_part_20_21_clean['Q10'].str.replace(' Google Cloud Datalab Notebooks', ' Google Cloud Datalab')\ndf_part_20_21_clean['Q10'] = df_part_20_21_clean['Q10'].str.replace(' Google Cloud AI Platform Notebooks ', ' Google Cloud Notebooks (AI Platform / Vertex AI) ')\ndf_part_20_21_clean['Q29'] = df_part_20_21_clean['Q29'].str.replace('PostgresSQL', 'PostgreSQL')\ndf_part_20_21_clean['Q29'] = df_part_20_21_clean['Q29'].str.replace('\\n', '')\ndf_part_20_21_clean['Q29'] = df_part_20_21_clean['Q29'].str.replace(' Microsoft Azure SQL Database ', ' Microsoft Azure Data Lake Storage ')\ndf_part_20_21_clean['Q29'] = df_part_20_21_clean['Q29'].str.replace(' Microsoft Azure Cosmos DB ', ' Microsoft Azure Data Lake Storage ')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace('\\n', '')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace('(', '')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace(')', '')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace(' Automation of full ML pipelines e.g. Google AutoML, H2O Driverless AI', \n                                                        ' Automation of full ML pipelines AutoML')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace(' Automation of full ML pipelines e.g. Google Cloud AutoML, H2O Driverless AI', \n                                                        ' Automation of full ML pipelines Cloud AutoML')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace(' Automation of full ML pipelines e.g. Google AutoML, H20 Driverless AI', \n                                                        ' Automation of full ML pipelines AutoML')\ndf_part_20_21_clean['Q33'] = df_part_20_21_clean['Q33'].str.replace(' Automation of full ML pipelines e.g. Google Cloud AutoML, H20 Driverless AI', \n                                                        ' Automation of full ML pipelines Cloud AutoML')\ndf_part_20_21_clean['Q34'] = df_part_20_21_clean['Q34'].str.replace('  H20 Driverless AI  ', '  H2O Driverless AI  ')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('  Visual Studio / Visual Studio Code ', '  VisualStudio ')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('(', '')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace(')', '')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('  Visual Studio Code VSCode ', '  VisualStudio ')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('  Visual Studio ', '  VisualStudio ')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('  VisualStudio ', '  Visual Studio / Visual Studio Code ')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace(' Jupyter (JupyterLab, Jupyter Notebooks, etc) ', '  Jupyter Notebook')\ndf_part_20_21_clean['Q9'] = df_part_20_21_clean['Q9'].str.replace('  Jupyter Notebook', ' Jupyter (JupyterLab, Jupyter Notebooks, etc) ')\n#df_part_20_21_clean['Q12'] = df_part_20_21_clean['Q12'].str.replace('  Google Cloud TPUs ', ' TPUs')\n#df_part_20_21_clean['Q12'] = df_part_20_21_clean['Q12'].str.replace('  NVIDIA GPUs ', ' GPUs')\n#df_part_20_21_clean['Q27'] = df_part_20_21_clean['Q27'].str.replace('  Amazon Elastic Container Service ', '  Amazon Elastic Compute Cloud (EC2) ')\n#df_part_20_21_clean['Q27'] = df_part_20_21_clean['Q27'].str.replace('  Microsoft Azure Container Instances ', '  Microsoft Azure Virtual Machines ')\n\n\n#print(df_20_21_clean['Q11'].unique())","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T10:59:25.775435Z","iopub.execute_input":"2022-01-08T10:59:25.776608Z","iopub.status.idle":"2022-01-08T10:59:27.967816Z","shell.execute_reply.started":"2022-01-08T10:59:25.776547Z","shell.execute_reply":"2022-01-08T10:59:27.966786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis\n\nIn this part, we plotted all questions for visual pieces of information. We created 2 functions. \n\nFunction **long_sentences_seperate** is used for visual editing. For instance, if a question or an answer text is so long for plotting, this function splits the text by adding '\\n' to the text.\n\nThe **barplot_all_cols** function is used for plotting all columns. For color, we selected the 'years' column.","metadata":{"_kg_hide-input":true,"_kg_hide-output":true}},{"cell_type":"code","source":"import textwrap\ndef long_sentences_seperate(sentence, width=30):\n    try:\n        splittext = textwrap.wrap(sentence,width)\n        text = '<br>'.join(splittext)#whitespace is removed, and the sentence is joined\n        return text\n    except:\n        return sentence","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T11:37:17.853671Z","iopub.execute_input":"2022-01-08T11:37:17.855014Z","iopub.status.idle":"2022-01-08T11:37:17.861052Z","shell.execute_reply.started":"2022-01-08T11:37:17.854954Z","shell.execute_reply":"2022-01-08T11:37:17.860145Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepared_df(df, question_mean_dict, df_cols, figsize=(24, 96)):\n    response_num_2020 = df20.shape[0] #gets the number of rows\n    response_num_2021 = df21.shape[0] \n    \n    ncols = 2 \n    nrows = round(len(df_cols) / ncols) #intending to create 2 columns and corresponding no of rows.\n    counted_df = pd.DataFrame() #Initialising the empty dataframe to store the prepared data\n    index = 0\n    for row in range(nrows):\n        for col in range(ncols):\n            try:\n                col_name = df_cols[index]\n                question = question_mean_dict[col_name]\n                question = long_sentences_seperate(question,40)\n            except:\n                axes[row][col].set_visible(False) #if data is not available, then chart is invis\n                continue\n            \n            if col_name == 'Q3': #This Q3 is the country, all countries are not necessary\n                selected_countries = df[col_name].value_counts(normalize=True).index[:10]\n                temp_df = df[df[col_name].isin(selected_countries)] #Only selected top 10 countries \n                \n                temp_df = temp_df.groupby([col_name, 'years']).agg({col_name:'count'})\n                temp_df.columns = ['counts']\n                temp_df.reset_index(inplace=True) \n            else:\n                temp_df = df.groupby([col_name, 'years']).agg({col_name:'count'}) #Only take the count change\n                #Occurences of each answer choice is counted. If someone is selecting more than one S/W or \n                #choice is not considered.\n                temp_df.columns = ['counts']\n                temp_df.reset_index(inplace=True)\n            \n            temp_df.loc[temp_df['years'] == 2020, 'counts_norm'] = temp_df.loc[temp_df['years'] == 2020, 'counts'] / response_num_2020\n            temp_df.loc[temp_df['years'] == 2021, 'counts_norm'] = temp_df.loc[temp_df['years'] == 2021, 'counts'] / response_num_2021\n            temp_df['choices'] = temp_df[col_name].apply(lambda x: long_sentences_seperate(x, 25))\n            temp_df['quest_no'] = col_name #The question number is added for later use with Plotly\n            temp_df['question_asked'] = question\n            ### Find The Order That Biggest Change to Lowest Change\n            count_df = temp_df[col_name].value_counts() #Gets the value counts of the said column\n            selected_values = list(count_df[count_df > 1].index) #Consider only the values more than 1\n            clean_temp_df = temp_df[temp_df[col_name].isin(selected_values)] #df is created withe selected values\n            #Below we calculate the %change, and prioritize the max change\n            changes_list = ((clean_temp_df.loc[clean_temp_df['years'] == 2021, 'counts'].values - clean_temp_df.loc[clean_temp_df['years'] == 2020, 'counts'].values) / \n                            clean_temp_df.loc[clean_temp_df['years'] == 2020, 'counts'].values)\n            change_twice_list = [] # Why this?\n            for value in changes_list:\n                change_twice_list.append(value)\n                change_twice_list.append(value) #twice the values is appended to the list\n            clean_temp_df['change'] = change_twice_list\n            clean_temp_df.sort_values('change', inplace=True, ascending=False)\n            order_list = list(clean_temp_df[col_name].unique()) #the column names are ordered\n            temp_df_unique = temp_df[col_name].unique()\n            diff_order = list(set(temp_df_unique) - set(order_list))\n            if len(diff_order) > 0:\n                order_list.extend(diff_order)\n            ###\n            #print(temp_df.shape)\n            counted_df = pd.concat([counted_df,temp_df],join='outer')\n            #Taking out the clean_temp_df and concatenating to counted_df            \n            index += 1\n            \n    return counted_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-01-08T12:05:19.325281Z","iopub.execute_input":"2022-01-08T12:05:19.325654Z","iopub.status.idle":"2022-01-08T12:05:19.353608Z","shell.execute_reply.started":"2022-01-08T12:05:19.325617Z","shell.execute_reply":"2022-01-08T12:05:19.3528Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initiating the Plotly express charting helper function \ndef plot_helper(data_trial):\n    data_trial.years = data_trial.years.astype('category')\n\n    re_chart = px.bar(data_frame=data_trial,y='choices',x='counts',color='years',\n                      facet_col='question_asked',width= 900,facet_col_wrap=2,facet_col_spacing=0.25)\n\n    re_chart.update_yaxes(matches=None,automargin=False, #Automargin option lets us control, how much margin to allow\n                          tickfont=dict(family='Rockwell', color='black', size=10),\n                         title=None)\n    \n    re_chart.update_yaxes(showticklabels=True, col=2) #This adds the y-axis ticks\n\n    re_chart.update_xaxes(matches=None) #The matches None option allows to automatically modify the axis ticks\n\n    re_chart.update_layout(height = 1500,barmode='group',autosize=False,\n                           margin=dict(l=150, r=150, t=100, b=5),paper_bgcolor=\"LightSteelBlue\")\n    re_chart.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T12:04:37.349885Z","iopub.execute_input":"2022-01-08T12:04:37.350666Z","iopub.status.idle":"2022-01-08T12:04:37.362759Z","shell.execute_reply.started":"2022-01-08T12:04:37.350611Z","shell.execute_reply":"2022-01-08T12:04:37.36158Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DS_col_1 = ['Q11', 'Q32', 'Q3', 'Q4', 'Q1', 'Q38']\nDS_col_2 = ['Q13', 'Q30', 'Q6', 'Q25', 'Q5', 'Q8']\n\ntrial_1 = prepared_df(df_20_21_clean, question_mean_dict, DS_col_1)\ntrial_2 = prepared_df(df_20_21_clean, question_mean_dict, DS_col_2)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T12:05:29.341501Z","iopub.execute_input":"2022-01-08T12:05:29.342089Z","iopub.status.idle":"2022-01-08T12:05:29.710278Z","shell.execute_reply.started":"2022-01-08T12:05:29.342052Z","shell.execute_reply":"2022-01-08T12:05:29.709372Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reducing some of the very long ticks\ntrial_1.loc[trial_1.Q11 == 'A cloud computing platform (AWS, Azure, GCP, hosted notebooks, etc)','choices'] = 'A cloud computing platform'\ntrial_1.loc[trial_1.Q11 == 'A deep learning workstation (NVIDIA GTX, LambdaLabs, etc)','choices'] = 'A deep learning platform'\ntrial_2.loc[trial_2.choices.str.contains('Great'),'choices'] = 'UK'","metadata":{"execution":{"iopub.status.busy":"2022-01-08T12:05:29.712257Z","iopub.execute_input":"2022-01-08T12:05:29.712546Z","iopub.status.idle":"2022-01-08T12:05:29.721995Z","shell.execute_reply.started":"2022-01-08T12:05:29.712514Z","shell.execute_reply":"2022-01-08T12:05:29.721307Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#getting the data prepared for the rest of the questions, 2 PARTS\ntrial_part_1 = prepared_df(df_part_20_21_clean, question_mean_dict, df_part_20_21_clean.columns[:-9])\ntrial_part_2 = prepared_df(df_part_20_21_clean, question_mean_dict, df_part_20_21_clean.columns[9:-1])","metadata":{"execution":{"iopub.status.busy":"2022-01-08T12:05:29.723062Z","iopub.execute_input":"2022-01-08T12:05:29.723893Z","iopub.status.idle":"2022-01-08T12:05:30.472564Z","shell.execute_reply.started":"2022-01-08T12:05:29.723817Z","shell.execute_reply":"2022-01-08T12:05:30.471477Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_helper(trial_1)\nplot_helper(trial_2)\nplot_helper(trial_part_1)\nplot_helper(trial_part_2)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T12:05:30.474027Z","iopub.execute_input":"2022-01-08T12:05:30.47431Z","iopub.status.idle":"2022-01-08T12:05:31.601412Z","shell.execute_reply.started":"2022-01-08T12:05:30.474276Z","shell.execute_reply":"2022-01-08T12:05:31.600335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n### So is Plotly any use??? \n\nWhen I began my journey into the world of Data Visualisation, I began with Seaborn (still struggle a lot). Around that time, I was thinking about venturing into dash boards and started searching for the libraries in Python that support it. \n\n#### In came Plotly and Dash\n\nIt was instant love. The pleasure of working starts with our visualisation in our mind. The data of numbers in the hands of Plotly transforms easily into something very close to our visualisation.\n\n#### What we will learn???\n\nThere is simply two helper functions created on top of the data that was prepared. There were some additional columns included to make the plotly rendering easier.\n\ntextwrap library introduced by the plotly community here.\nhttps://community.plotly.com/t/wrap-long-text-in-title-in-dash/11419/2?u=plotkamal\nThis library solves the very long chart titles, when we use the questions are directly used.\n@Hasan had used a split function, by adding new line for Seaborn. Same cannot be used for plotly.\n\nplot_helper() function invokes the plotly bar chart. In the bar chart, there are couple of options that made the plots presentable.I am listing them here.\n    \nThis provides seperate Y-ticks for each of the facets, in same row.\n    update_yaxes(matches=None)\n    re_chart.update_yaxes(showticklabels=True, col=2)\n\nFollowing code decouples the margins from the plotly control, and lets you to set the margin\n\n    fig.update_layout(height = 1500,barmode='group',autosize=False,\n                           margin=dict(l=250, r=150, t=100, b=5),paper_bgcolor=\"LightSteelBlue\")\n    automargin=False option in update_yaxes() function\n\nAs the saying, the beauty is in the \"Eyes of the Beholder\", so is the data visualisation. Plotly lets us achieve it with lot of help from data preparation, and external helper libraries if searched. \n\nHaappppyyy Visualising...","metadata":{}}]}