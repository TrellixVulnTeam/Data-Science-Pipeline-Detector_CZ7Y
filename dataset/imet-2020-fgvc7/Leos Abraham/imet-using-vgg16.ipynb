{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tensorflow import keras\nimport os\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,MaxPool2D,Flatten,BatchNormalization,Dropout,GlobalMaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras import regularizers, optimizers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/imet-2020-fgvc7/train.csv\")\ntest = pd.read_csv(\"../input/imet-2020-fgvc7/sample_submission.csv\")\ntrain[\"id\"] = train[\"id\"] + '.png'\ntest[\"id\"] = test[\"id\"] + \".png\"\ntrain[\"attribute_ids\"] = train[\"attribute_ids\"].apply(lambda x:x.split())\n# valid = train.iloc[113701:]\n# train = train.iloc[:113701]\n# print(train.shape)\n# print(valid.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainGen = ImageDataGenerator(preprocessing_function=preprocess_input,width_shift_range=0.1,\n                             height_shift_range=0.1,\n                             zoom_range=0.2,rescale=1/255.0)\ntrain_data = trainGen.flow_from_dataframe(dataframe=train,directory=\"../input/imet-2020-fgvc7/train/\",\n                                          x_col=\"id\",y_col=\"attribute_ids\",\n                                          batch_size=128,seed=4,\n                                          shuffle=True,target_size=(224,224))\n# validGen = ImageDataGenerator(preprocessing_function=preprocess_input,width_shift_range=0.1,\n#                              height_shift_range=0.1,\n#                              zoom_range=0.2,rescale=1/255.0)\n# valid_data = validGen.flow_from_dataframe(dataframe=valid,directory=\"../input/imet-2020-fgvc7/train/\",\n#                                           x_col=\"id\",y_col=\"attribute_ids\",\n#                                           batch_size=128,seed=4,\n#                                           shuffle=True,target_size=(224,224))\ntestGen = ImageDataGenerator(preprocessing_function=preprocess_input,rescale=1/255.0)\ntest_data = testGen.flow_from_dataframe(dataframe=test,directory=\"../input/imet-2020-fgvc7/test/\",\n                                          x_col=\"id\",batch_size=1,seed=4,shuffle=False,\n                                          class_mode=None,target_size=(224,224))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vgg = VGG16(weights=None, include_top=True,pooling='max')\nx = vgg.layers[-13].output\n# x = GlobalMaxPooling2D()(x)\nx = Flatten()(x)\nx = Dense(units=1024,activation='relu')(x)\n# x = Dropout(rate=0.5)(x) \n# x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n# x = Dense(units=256,activation='relu')(x)\n# x = Dropout(rate=0.5)(x)\n# x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n# x = Dense(units=16,activation='relu')(x)\n# x = Dropout(rate=0.5)(x)\n# x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\nx = Dense(units=3471, activation=\"sigmoid\")(x)\nmodel = Model(inputs=vgg.input,outputs=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nopt = Adam(0.00001)\nmodel.compile(optimizer=opt, loss=keras.losses.binary_crossentropy, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_lrfn(lr_start=0.00001, lr_max=0.000075, \n               lr_min=0.000001, lr_rampup_epochs=20, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    \n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn\n\nlrfn = build_lrfn()\nlr_schedule = LearningRateScheduler(lrfn, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"STEP_SIZE_TRAIN=train_data.n//train_data.batch_size\n\nhist=model.fit(train_data,steps_per_epoch=STEP_SIZE_TRAIN,epochs=1)#,callbacks=[lr_schedule]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[0][0].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model\nmodel=load_model(\"model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(test_data,verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_bool = (result >0.2)\npredictions=[]\nlabels = train_data.class_indices\nlabels = dict((v,k) for k,v in labels.items())\nfor row in pred_bool:\n    l=[]\n    \n    for index,cls in enumerate(row):\n        if cls:\n            l.append(labels[index])\n    predictions.append(\" \".join(l))\n    \nfilenames=test_data.filenames\n\nresults = pd.DataFrame({\"id\":filenames,\"attribute_ids\":predictions})\nresults[\"id\"] = results[\"id\"].apply(lambda x:x.split(\".\")[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}