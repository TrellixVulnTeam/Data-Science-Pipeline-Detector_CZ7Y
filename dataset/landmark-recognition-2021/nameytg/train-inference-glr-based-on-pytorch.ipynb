{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os \nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:18.23934Z","iopub.execute_input":"2021-09-25T03:03:18.2399Z","iopub.status.idle":"2021-09-25T03:03:18.244409Z","shell.execute_reply.started":"2021-09-25T03:03:18.239863Z","shell.execute_reply":"2021-09-25T03:03:18.243571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Location of training file for our training dataset\n# train_file = \"../input/landmark-recognition-2021/train.csv\"\n# Reading file using pandas \"read_csv\" \n# data = pd.read_csv(train_file)\n# Exploring the dataset\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:18.246062Z","iopub.execute_input":"2021-09-25T03:03:18.246512Z","iopub.status.idle":"2021-09-25T03:03:18.254921Z","shell.execute_reply.started":"2021-09-25T03:03:18.246476Z","shell.execute_reply":"2021-09-25T03:03:18.254186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(data[\"landmark_id\"].max(), data[\"landmark_id\"].min(), data[\"landmark_id\"].nunique(), data[\"landmark_id\"].unique())","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:18.256297Z","iopub.execute_input":"2021-09-25T03:03:18.256586Z","iopub.status.idle":"2021-09-25T03:03:18.263516Z","shell.execute_reply.started":"2021-09-25T03:03:18.256552Z","shell.execute_reply":"2021-09-25T03:03:18.262821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# seems like our data containes label inconsistent so we need a mapping dictiory for mapping our target values associated with \n#our input_image\n\n# reverse_target_mapping = { i : value for i,value in enumerate(sorted(data[\"landmark_id\"].unique()))}","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:18.265754Z","iopub.execute_input":"2021-09-25T03:03:18.266021Z","iopub.status.idle":"2021-09-25T03:03:18.272316Z","shell.execute_reply.started":"2021-09-25T03:03:18.265988Z","shell.execute_reply":"2021-09-25T03:03:18.271502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reverse_target_mapping","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:18.273642Z","iopub.execute_input":"2021-09-25T03:03:18.273904Z","iopub.status.idle":"2021-09-25T03:03:18.283412Z","shell.execute_reply.started":"2021-09-25T03:03:18.273871Z","shell.execute_reply":"2021-09-25T03:03:18.282749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we are exchaning the keys and values in each other for mapping purpose\n# target_mapping = {value : key for key,value in zip(reverse_target_mapping.keys(), reverse_target_mapping.values())}","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:18.286569Z","iopub.execute_input":"2021-09-25T03:03:18.286825Z","iopub.status.idle":"2021-09-25T03:03:18.291758Z","shell.execute_reply.started":"2021-09-25T03:03:18.286802Z","shell.execute_reply":"2021-09-25T03:03:18.290785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sorting the value for concatnating our input image train file for processing\n# data.sort_values(inplace = True, by = \"id\")\n# Reseting the indices\n# data.reset_index(inplace = True)\n# droping the previous indices column , this can be done using reset index = True, but i forgot\n# data.drop(columns = [\"index\"] , inplace = True)\n\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:18.293297Z","iopub.execute_input":"2021-09-25T03:03:18.293587Z","iopub.status.idle":"2021-09-25T03:03:18.300014Z","shell.execute_reply.started":"2021-09-25T03:03:18.293554Z","shell.execute_reply":"2021-09-25T03:03:18.299343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data[\"landmark_id\"] = data[\"landmark_id\"].map(target_mapping)\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:18.301408Z","iopub.execute_input":"2021-09-25T03:03:18.301656Z","iopub.status.idle":"2021-09-25T03:03:18.307949Z","shell.execute_reply.started":"2021-09-25T03:03:18.301623Z","shell.execute_reply":"2021-09-25T03:03:18.307164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# filepath = \"../input/landmark-recognition-2021/train/\"\n# images = list()\n# with os.scandir(filepath) as entries_1:\n#     for entry_1 in entries_1:\n#         if not entry_1.is_file():\n#             with os.scandir(filepath + entry_1.name) as entries_2:\n#                 for entry_2 in entries_2:\n#                     if not entry_2.is_file():\n#                         with os.scandir(filepath + entry_1.name + '/' + entry_2.name) as entries_3:\n#                             for entry_3 in entries_3:\n#                                 if not entry_3.is_file():\n#                                     with os.scandir(filepath + entry_1.name + '/' + entry_2.name + '/' + entry_3.name) as entries_4:\n#                                         for entry_4 in entries_4:\n#                                             images.append(filepath + entry_1.name + '/' + entry_2.name + '/' + entry_3.name + '/' + entry_4.name)\n# print(len(images))\n#                 for image in images:\n#                     fileimages.append(filepath + entry.name + '/' + entry.name + '/' + image.name)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:18.309499Z","iopub.execute_input":"2021-09-25T03:03:18.309754Z","iopub.status.idle":"2021-09-25T03:03:18.316816Z","shell.execute_reply.started":"2021-09-25T03:03:18.309724Z","shell.execute_reply":"2021-09-25T03:03:18.316087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# series = pd.DataFrame(data = {\"true_label\" : target_mapping.keys(), \"mapped_label\" : target_mapping.values()})\n# print(series)\n# series.to_csv(\"./label_mapping.csv\")\n\n\n# data[\"input_file_loc\"] = sorted(images)\n# data.head()\n# data.to_csv('./train_target.csv', index=False)\ndata = pd.read_csv('../input/train-target/train_target.csv',usecols=[\"landmark_id\", \"input_file_loc\"])\n# data = data.drop(index=range(1,1000000), inplace=False)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:18.36198Z","iopub.execute_input":"2021-09-25T03:03:18.362193Z","iopub.status.idle":"2021-09-25T03:03:21.038978Z","shell.execute_reply.started":"2021-09-25T03:03:18.362159Z","shell.execute_reply":"2021-09-25T03:03:21.038216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.iloc[0, 1]","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:21.040679Z","iopub.execute_input":"2021-09-25T03:03:21.040887Z","iopub.status.idle":"2021-09-25T03:03:21.047609Z","shell.execute_reply.started":"2021-09-25T03:03:21.040864Z","shell.execute_reply":"2021-09-25T03:03:21.046417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.model_selection\nimport torchvision.datasets\nimport torchvision.transforms as transforms\nimport torch\nimport torchvision\nfrom PIL import Image\n\ntransform = transforms.Compose(\n    [transforms.Resize((28, 28)),\n     transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:21.049289Z","iopub.execute_input":"2021-09-25T03:03:21.049605Z","iopub.status.idle":"2021-09-25T03:03:26.304116Z","shell.execute_reply.started":"2021-09-25T03:03:21.04957Z","shell.execute_reply":"2021-09-25T03:03:26.303362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filepath = \"../input/landmark-recognition-2021/train/\"\n\n# Create the Torch Dataset object for our pandas dataframe\nclass GLRDataset(torch.utils.data.Dataset):\n    def __init__(self, data, root_dir, transform):\n        self.data = data\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        if self.root_dir == '../input/landmark-recognition-2021/train/':\n            img_name = self.data.iloc[idx, 1]\n            y_label = torch.tensor([self.data.iloc[idx, 0]])\n        elif self.root_dir == '../input/landmark-recognition-2021/test/':\n            img_name = self.data.iloc[idx, 0]\n        image = Image.open(img_name)\n\n        if self.transform:\n            image = self.transform(image)\n        if self.root_dir == '../input/landmark-recognition-2021/train/':\n            return (image, y_label)\n        elif self.root_dir == '../input/landmark-recognition-2021/test/':\n            return image","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:26.30554Z","iopub.execute_input":"2021-09-25T03:03:26.305817Z","iopub.status.idle":"2021-09-25T03:03:26.314769Z","shell.execute_reply.started":"2021-09-25T03:03:26.305785Z","shell.execute_reply":"2021-09-25T03:03:26.314084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = GLRDataset(data=data, root_dir=filepath, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:26.317353Z","iopub.execute_input":"2021-09-25T03:03:26.317871Z","iopub.status.idle":"2021-09-25T03:03:26.328871Z","shell.execute_reply.started":"2021-09-25T03:03:26.317832Z","shell.execute_reply":"2021-09-25T03:03:26.328087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=100, shuffle=True, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:26.332112Z","iopub.execute_input":"2021-09-25T03:03:26.332793Z","iopub.status.idle":"2021-09-25T03:03:26.338763Z","shell.execute_reply.started":"2021-09-25T03:03:26.332758Z","shell.execute_reply":"2021-09-25T03:03:26.337872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataiter = iter(train_loader)\nimages, labels = dataiter.next()\nprint(images.shape, labels.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:26.340163Z","iopub.execute_input":"2021-09-25T03:03:26.340441Z","iopub.status.idle":"2021-09-25T03:03:28.194393Z","shell.execute_reply.started":"2021-09-25T03:03:26.340407Z","shell.execute_reply":"2021-09-25T03:03:28.193654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport math\nimport torchvision.models as models\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n# model = EfficientNet.from_pretrained('efficientnet-b0')\n# model.to(device)\n\nimport math\nimport torch.utils.model_zoo as model_zoo\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\n\nclass CNN(nn.Module):\n\n    def __init__(self, block, layers, num_classes=81313):\n        self.inplanes = 64\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n\n        self.convtranspose1 = nn.ConvTranspose2d(512, 2048, kernel_size=3, stride=1, padding=1, output_padding=0,\n                                                 groups=1, bias=False, dilation=1)\n\n        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n\n        self.fclass = nn.Linear(2048, num_classes)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        # 新加层的forward\n        x = self.convtranspose1(x)\n        x = self.maxpool2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fclass(x)\n        return x\n\n\n    \n    \nresnet34 = models.resnet34(pretrained=False)\nresnet34.load_state_dict(torch.load('../input/resnet34/resnet34-333f7ec4.pth'))\n\nnet = CNN(BasicBlock, [3, 4, 6, 3])\nnet = net.to(device)\n\npretrained_dict = resnet34.state_dict()\nmodel_dict = net.state_dict()\n\npretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n\nmodel_dict.update(pretrained_dict)\n\nnet.load_state_dict(model_dict)\n\n# class GLRModel(nn.Module):\n#     def __init__(self):\n#         super(GLRModel, self).__init__()\n#         self.layer1 = nn.Sequential(\n#             # nn.BatchNorm2d(1280),\n#             nn.Conv2d(3, 20, 3),        #(100, 20, 222, 222)\n#             nn.BatchNorm2d(20),\n#             nn.Conv2d(20, 20, 3),       #(100, 20, 220, 220)\n#             nn.BatchNorm2d(20),\n#             nn.ReLU(),\n#             nn.AvgPool2d(2, 2))         #(100, 20, 110, 110)\n#         self.layer2 = nn.Sequential(\n#             nn.Conv2d(20, 5, 3),        #(100, 5, 108, 108)\n#             nn.BatchNorm2d(5),\n#             nn.ReLU(),\n#             nn.MaxPool2d(2, 2))         #(100, 5, 54, 54)\n#         self.pool = nn.Conv2d(5, 16, 1) #(100, 16, 54, 54)\n#         self.fc1 = nn.Linear(16 * 54 * 54, 81313)\n\n#     def forward(self, x):\n#         x = self.layer1(x)\n#         x = self.layer2(x)\n#         x = self.pool(x)\n#         x = x.reshape(100, -1)\n#         x = self.fc1(x)\n#         x = F.relu(x)\n#         # x = F.softmax(x, dim=1)\n#         return x\n\n    # net = CNN(BasicBlock, [3, 4, 6, 3])\n# net = net.to(device)\n# net = GLRModel()\n# net = net.to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(net.parameters(), lr=0.005)\n\n# optimizer = optim.RAdam(lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:28.197087Z","iopub.execute_input":"2021-09-25T03:03:28.197343Z","iopub.status.idle":"2021-09-25T03:03:38.229043Z","shell.execute_reply.started":"2021-09-25T03:03:28.197317Z","shell.execute_reply":"2021-09-25T03:03:38.228109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.autograd import Variable\n\n\n# for epoch in range(1):  # loop over the dataset multiple times\n\n#     running_loss = 0.0\n#     for i, data in enumerate(train_loader, 0):\n#         # get the inputs\n#         inputs, labels = data\n#         # wrap them in Variable\n#         inputs, labels = Variable(inputs.to(device)), Variable(labels.to(device))\n#         labels = labels.flatten()\n# #         print(labels.shape)\n# #         labels = Variable(inputs.view(20,))\n\n#         # forward + backward + optimize\n#         outputs = net(inputs)\n\n#         loss = criterion(outputs, labels)\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n\n#         # print statistics\n#         running_loss += loss.item()\n        \n#         if i % 200 == 199:    # print every 20 mini-batches\n#             print('[%d, %5d] loss: %.3f' %\n#                   (epoch + 1, i + 1, running_loss / 200))\n#             running_loss = 0.0\n# torch.save(net.state_dict(), './model_GLR_dict')","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:38.230569Z","iopub.execute_input":"2021-09-25T03:03:38.230841Z","iopub.status.idle":"2021-09-25T03:03:38.236512Z","shell.execute_reply.started":"2021-09-25T03:03:38.23081Z","shell.execute_reply":"2021-09-25T03:03:38.235597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net.eval()\nnet.load_state_dict(torch.load(\"../input/model-glr-dict/model_GLR_dict\"))","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:38.238093Z","iopub.execute_input":"2021-09-25T03:03:38.238399Z","iopub.status.idle":"2021-09-25T03:03:46.507968Z","shell.execute_reply.started":"2021-09-25T03:03:38.238366Z","shell.execute_reply":"2021-09-25T03:03:46.507258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get all images paths\ntest_images = []\nimages_id = []\nfor dirpath, dirnames, filenames in os.walk(\"../input/landmark-recognition-2021/test\"):\n    for filename in [f for f in filenames if f.endswith(\".jpg\")]:\n        test_images.append(os.path.join(dirpath, filename))\n        filename = os.path.splitext(filename)[0]\n        images_id.append(filename)\n\n\ntest_df = pd.DataFrame(test_images, columns=['input_file_loc']) \ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:46.509315Z","iopub.execute_input":"2021-09-25T03:03:46.509564Z","iopub.status.idle":"2021-09-25T03:03:54.712925Z","shell.execute_reply.started":"2021-09-25T03:03:46.509537Z","shell.execute_reply":"2021-09-25T03:03:54.712209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_id = sorted(images_id)\ntest_df[\"input_file_loc\"] = sorted(test_images)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:54.714099Z","iopub.execute_input":"2021-09-25T03:03:54.714421Z","iopub.status.idle":"2021-09-25T03:03:54.725371Z","shell.execute_reply.started":"2021-09-25T03:03:54.714387Z","shell.execute_reply":"2021-09-25T03:03:54.724648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_id[0:5]\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:54.726721Z","iopub.execute_input":"2021-09-25T03:03:54.726977Z","iopub.status.idle":"2021-09-25T03:03:54.739798Z","shell.execute_reply.started":"2021-09-25T03:03:54.726945Z","shell.execute_reply":"2021-09-25T03:03:54.739011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = GLRDataset(data=test_df, root_dir='../input/landmark-recognition-2021/test/', transform=transform)\ntest_loader = torch.utils.data.DataLoader(dataset=test, batch_size=100, shuffle=False, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:54.74397Z","iopub.execute_input":"2021-09-25T03:03:54.744193Z","iopub.status.idle":"2021-09-25T03:03:54.750727Z","shell.execute_reply.started":"2021-09-25T03:03:54.744167Z","shell.execute_reply":"2021-09-25T03:03:54.750041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataiter = iter(test_loader)\nimages = dataiter.next()\nprint(images.shape, test.__len__())","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:54.751902Z","iopub.execute_input":"2021-09-25T03:03:54.752752Z","iopub.status.idle":"2021-09-25T03:03:56.059786Z","shell.execute_reply.started":"2021-09-25T03:03:54.75272Z","shell.execute_reply":"2021-09-25T03:03:56.058984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_conf_list = list()\npredict_id_list = list()\n\nfor i, images in enumerate(test_loader):\n    images = images.to(device)\n    images = Variable(images)\n    outputs = net(images)\n    predict_id = torch.max(outputs.data, 1)[1].to(device).data.cpu().numpy()\n    predict_conf = F.softmax(outputs, dim=1)\n    predict_conf = torch.max(predict_conf.data, 1)[0].to(device).data.cpu().numpy()\n\n    predict_conf = np.array(predict_conf) \n    predict_conf_list.append(predict_conf)\n    predict_id = np.array(predict_id) \n    predict_id_list.append(predict_id)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:03:56.060897Z","iopub.execute_input":"2021-09-25T03:03:56.061176Z","iopub.status.idle":"2021-09-25T03:06:01.226225Z","shell.execute_reply.started":"2021-09-25T03:03:56.061128Z","shell.execute_reply":"2021-09-25T03:06:01.225513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict_id_list = np.reshape(predict_id_list, (1,))\nresults = list()\n\npre_id_list = list()\nfor id in predict_id_list:\n    for pre_id in id:\n            pre_id_list.append(pre_id)\n\npre_conf_list = list()\nfor id in predict_conf_list:\n    for pre_conf in id:\n            pre_conf_list.append(pre_conf)\n\n\nprint(len(pre_conf_list))\n\nfor i in range(10345):\n    results.append(str(pre_id_list[i]) + ' ' + str(pre_conf_list[i]))\nimages_id = pd.Series(images_id, name='id').astype(str)\nimages_Label = pd.Series(results, name='landmarks').astype(str)\n\ntest_df = pd.concat([images_id, images_Label], axis=1)\n\n# data[\"predict_id\"] = data[\"predict_id\"].map(reverse_target_mapping)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:06:01.227667Z","iopub.execute_input":"2021-09-25T03:06:01.227933Z","iopub.status.idle":"2021-09-25T03:06:01.276103Z","shell.execute_reply.started":"2021-09-25T03:06:01.227902Z","shell.execute_reply":"2021-09-25T03:06:01.275326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:06:01.27724Z","iopub.execute_input":"2021-09-25T03:06:01.277936Z","iopub.status.idle":"2021-09-25T03:06:01.315494Z","shell.execute_reply.started":"2021-09-25T03:06:01.2779Z","shell.execute_reply":"2021-09-25T03:06:01.314853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:06:01.316527Z","iopub.execute_input":"2021-09-25T03:06:01.316845Z","iopub.status.idle":"2021-09-25T03:06:01.320641Z","shell.execute_reply.started":"2021-09-25T03:06:01.31681Z","shell.execute_reply":"2021-09-25T03:06:01.319746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_df = pd.read_csv(\"../input/landmark-recognition-2021/sample_submission.csv\")\n# sample_df","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:06:01.321683Z","iopub.execute_input":"2021-09-25T03:06:01.322136Z","iopub.status.idle":"2021-09-25T03:06:01.330017Z","shell.execute_reply.started":"2021-09-25T03:06:01.322102Z","shell.execute_reply":"2021-09-25T03:06:01.329255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sample_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-25T03:06:01.331364Z","iopub.execute_input":"2021-09-25T03:06:01.331667Z","iopub.status.idle":"2021-09-25T03:06:01.408113Z","shell.execute_reply.started":"2021-09-25T03:06:01.331615Z","shell.execute_reply":"2021-09-25T03:06:01.407241Z"},"trusted":true},"execution_count":null,"outputs":[]}]}