{"cells":[{"metadata":{},"cell_type":"markdown","source":"__Competition Challenge__"},{"metadata":{},"cell_type":"markdown","source":"The challenge is to detect functional tissue units (FTUs) across different tissue preparation pipelines. An FTU is defined as a “three-dimensional block of cells centered around a capillary, such that each cell in this block is within diffusion distance from any other cell in the same block” (de Bono, 2013). The goal of this competition is the __implementation of a successful and robust glomeruli FTU detector.__"},{"metadata":{},"cell_type":"markdown","source":"__Competition Metric__"},{"metadata":{},"cell_type":"markdown","source":"The competition is evaluated on the mean Dice coefficient."},{"metadata":{},"cell_type":"markdown","source":"__Competition Rules__"},{"metadata":{},"cell_type":"markdown","source":"- CPU Notebook <= 9 hours run-time\n- GPU Notebook <= 9 hours run-time\n- TPUs will not be available for making submissions, however TPUs can be used for model training\n- Internet access disabled - Can be used for Training by not for Inference\n- Submission file must be named \"submission.csv\""},{"metadata":{"trusted":true},"cell_type":"code","source":"%env SM_FRAMEWORK=tf.keras\n!pip install -q segmentation-models==\"1.0.1\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os, json, re, math\n\nfrom tqdm import tqdm\nfrom glob import glob\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nimport tifffile as tiff\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\n\nfrom kaggle_datasets import KaggleDatasets\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\n\nfrom tensorflow.data import Dataset\nfrom tensorflow.keras.utils import get_custom_objects\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\nfrom tensorflow import keras\ntf.keras.backend.set_image_data_format('channels_last')\n\nimport segmentation_models as sm\n\nprint(os.listdir('/kaggle/input/'))\nprint(os.listdir('/kaggle/input/hubmap-kidney-segmentation/'))\n\nprint(\"Tensorflow version \" + tf.__version__)\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_dir = '/kaggle/input/hubmap-kidney-segmentation/'\nBACKBONE = 'efficientnetb7'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.listdir(base_dir + 'train/')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Number of train images: ', len(os.listdir(base_dir + 'train/')))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(base_dir + 'train.csv')\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta = pd.read_csv(base_dir + 'HuBMAP-20-dataset_information.csv')\nmeta","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Any Missing values in train and meta data?__"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Taking care of Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"meta.fillna(meta.mean(), inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Checking one json file__"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(os.path.join(base_dir, 'train/0486052bb-anatomical-structure.json')) as file:\n    ana = json.loads(file.read())\n    \nprint(json.dumps(ana))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Visualization__"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2)\n\nsns.distplot(meta['width_pixels'], ax = ax[0], kde = True, rug = True)\nax[0].axvline(np.mean(meta['width_pixels']), color = 'g', linestyle = '--')\nax[0].axvline(np.median(meta['width_pixels']), color = 'b', linestyle = '-')\nax[0].legend({'Median', 'Mean'})\n\nsns.distplot(meta['height_pixels'], ax = ax[1], kde = True, rug = True)\nax[1].axvline(np.mean(meta['height_pixels']), color = 'g', linestyle = '--')\nax[1].axvline(np.median(meta['height_pixels']), color = 'b', linestyle = '-')\nax[1].legend({'Median', 'Mean'})\n\nplt.suptitle('Distribution Plot of Pixel Width and Height')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2)\n\nsns.distplot(meta['bmi_kg/m^2'], ax = ax[0], kde = True, rug = True)\nax[0].axvline(np.mean(meta['bmi_kg/m^2']), color = 'g', linestyle = '--')\nax[0].axvline(np.median(meta['bmi_kg/m^2']), color = 'b', linestyle = '-')\nax[0].legend({'Median', 'Mean'})\n\nsns.distplot(meta['age'], ax = ax[1], kde = True, rug = True)\nax[1].axvline(np.mean(meta['age']), color = 'g', linestyle = '--')\nax[1].axvline(np.median(meta['age']), color = 'b', linestyle = '-')\nax[1].legend({'Median', 'Mean'})\n\nplt.suptitle('Distribution Plot of BMI and Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2)\n\nsns.distplot(meta['weight_kilograms'], ax = ax[0], kde = True, rug = True)\nax[0].axvline(np.mean(meta['weight_kilograms']), color = 'g', linestyle = '--')\nax[0].axvline(np.median(meta['weight_kilograms']), color = 'b', linestyle = '-')\nax[0].legend({'Median', 'Mean'})\n\nsns.distplot(meta['height_centimeters'], ax = ax[1], kde = True, rug = True)\nax[1].axvline(np.mean(meta['height_centimeters']), color = 'g', linestyle = '--')\nax[1].axvline(np.median(meta['height_centimeters']), color = 'b', linestyle = '-')\nax[1].legend({'Median', 'Mean'})\n\nplt.suptitle('Distribution Plot of Weight and Height')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2)\n\nsns.countplot(meta['sex'], ax = ax[0])\nsns.countplot(meta['race'], ax = ax[1])\nplt.suptitle('Count Plot of Sex and Race')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2)\n\nsns.countplot(meta['ethnicity'], ax = ax[0])\nsns.countplot(meta['race'], ax = ax[1])\nplt.suptitle('Count Plot of Ethnicity and Race')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2)\n\nsns.countplot(meta['sex'], ax = ax[0])\nsns.countplot(meta['laterality'], ax = ax[1])\nplt.suptitle('Count Plot of Sex and Laterality')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Helper Functions__"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\n\ndef mask2rle(img):\n    \n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n \ndef rle2mask(mask_rle, shape):\n    \n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (width,height) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T\n\ndef read_tiff(image, encoding_index, resize=None):\n    \n    '''\n    read tiff images and mask.\n    ----------------------------\n    \n    Arguments:\n    image -- tiff image\n    encoding_index -- corresponding tiff file encoding index.\n    \n    Returns:\n    tiff_image -- tiff image\n    tiff_mask -- segmentation mask\n    '''\n    \n    tiff_image = tiff.imread(os.path.join(base_dir, f'train/{image}.tiff'))\n    \n    if len(tiff_image.shape) == 5:\n        tiff_image = np.transpose(tiff_image.squeeze(), (1,2,0))\n        \n    tiff_mask = rle2mask(train['encoding'][encoding_index],\n                         (tiff_image.shape[1], tiff_image.shape[0]))\n    \n    print(f'Image Shape: {tiff_image.shape}')\n    print(f'Mask Shape: {tiff_mask.shape}')\n    \n    if resize:\n        rescaled = (tiff_image.shape[1] // resize, tiff_image.shape[0] // resize)\n        tiff_image = cv2.resize(tiff_image, rescaled)\n        tiff_mask = cv2.resize(tiff_mask, rescaled)\n\n    return tiff_image, tiff_mask\n\ndef read_test_tiff(image, resize=None):\n    \n    '''\n    read tiff images.\n    ----------------------------\n    \n    Arguments:\n    image -- tiff image\n    \n    Returns:\n    tiff_image -- tiff image\n    tiff_mask -- segmentation mask\n    '''\n    \n    tiff_image = tiff.imread(os.path.join(base_dir, f'test/{image}.tiff'))\n    \n    if len(tiff_image.shape) == 5:\n        tiff_image = np.transpose(tiff_image.squeeze(), (1,2,0))\n    \n    if resize:\n        rescaled = (tiff_image.shape[1] // resize, tiff_image.shape[0] // resize)\n        tiff_image = cv2.resize(tiff_image, rescaled)\n\n    return tiff_image\n\ndef plot(image, mask):\n    \n    '''\n    plot image and mask\n    ---------------------\n    \n    Arguments:\n    image -- tiff image \n    mask -- segmentation mask\n    \n    Returns:\n    matplotlib plot\n    '''\n    plt.figure(figsize = (15, 15))\n\n    # Image\n    plt.subplot(1, 3, 1)\n    plt.imshow(image)\n    plt.title(\"Image\", fontsize = 16)\n\n    # Mask\n    plt.subplot(1, 3, 2)\n    plt.imshow(mask)\n    plt.title(\"Image Mask\", fontsize = 16)\n\n    # Image + Mask\n    plt.subplot(1, 3, 3)\n    plt.imshow(image)\n    plt.imshow(mask, alpha=0.5)\n    plt.title(\"Image + Mask\", fontsize = 16);\n    \ndef plot_subset(image, mask, start_rh, end_rh, start_cw, end_cw):\n    \n    '''\n    plot image and mask\n    ---------------------\n    \n    Arguments:\n    image -- tiff image \n    mask -- segmentation mask\n    start_rh -- height start\n    end_rh -- height end\n    start_cw -- width start \n    end_cw -- width end\n    \n    Returns:\n    matplotlib plot\n    '''\n\n    # Figure size\n    plt.figure(figsize=(15, 15))\n\n    # subset image and mask\n    subset_image = image[start_rh:end_rh, start_cw:end_cw, :]\n    subset_mask = mask[start_rh:end_rh, start_cw:end_cw]\n\n    # Image\n    plt.subplot(1, 3, 1)\n    plt.imshow(subset_image)\n    plt.title(\"Zoomed Image\", fontsize=16)\n\n    # Mask\n    plt.subplot(1, 3, 2)\n    plt.imshow(subset_mask)\n    plt.title(\"Zoomed Image Mask\", fontsize=16)\n\n    # Image + Mask\n    plt.subplot(1, 3, 3)\n    plt.imshow(subset_image)\n    plt.imshow(subset_mask, alpha=0.5)\n    plt.title(\"Zoomed Image + Mask\", fontsize=16);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glob(base_dir + 'train/*.tiff'), glob(base_dir + 'test/*.tiff') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img, msk = read_tiff('2f6ecfcdf', 0, resize = None)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot(img, msk)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_subset(img, msk, 5000, 10000, 10000, 15000)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_subset(img, msk, 4000, 11000, 8000, 12000)\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__TPU Config__"},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    # detect and initialize tpu\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Connecting to tpu...')\n    print('device running at:', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    print('Initializing TPU...')\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    # instantiate a distribution strategy\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"TPU initialized\")\nelse:\n    print('Using deafualt strategy...')\n    strategy = tf.distribute.get_strategy()\n\nREPLICAS = strategy.num_replicas_in_sync\nprint(f\"REPLICAS:  {REPLICAS}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\nSEED = 42\nBATCH_SIZE = 8 * REPLICAS # ideal batch size is 128 per core. (128*8=1024)\nIMAGE_DIM = [512, 512]\nEPOCHS = 60\nLR = 5e-4\n\nGCS_PATH = KaggleDatasets().get_gcs_path('hubmap-512x512')\nprint(GCS_PATH)\n\n#Append train and mask to GCS PATH\nTIFF = tf.io.gfile.glob(str(GCS_PATH + '/train/*'))\nMASK = tf.io.gfile.glob(str(GCS_PATH + '/masks/*'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_TIFF = Dataset.from_tensor_slices(TIFF)\nTRAIN_MASK = Dataset.from_tensor_slices(MASK)\n\nTIFF_COUNT = tf.data.experimental.cardinality(TRAIN_TIFF).numpy()\nMASK_COUNT = tf.data.experimental.cardinality(TRAIN_MASK).numpy()\n\nprint('Training Data')\nprint(f'Total Tiff Images: {TIFF_COUNT}')\nprint(f'Total Masks: {MASK_COUNT}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for files in TRAIN_TIFF.take(5):\n    print(files.numpy())\nprint('\\n')\nfor files in TRAIN_MASK.take(5):\n    print(files.numpy())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image_and_mask(image, mask, augment = True):\n    \n    '''\n    decode image and mask in order to\n    feed data to TPU.\n    --------------------------------\n    \n    Arguments:\n    image -- patches of huge tiff file in png format.\n    mask -- patches of mask in png format.\n    augment -- apply augmentations on images and masks.\n    \n    Return:\n    image \n    mask\n    '''\n    \n    # load raw data as string\n    image = tf.io.read_file(image)\n    mask = tf.io.read_file(mask)\n    \n    image = tf.io.decode_png(image, channels = 3)  # convert compressed string to 3D uint8 tensor\n    mask = tf.io.decode_png(mask)  # convert compressed string to uinst8 tensor\n    \n    if augment:\n        \n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.flip_left_right(image)\n            mask = tf.image.flip_left_right(mask)\n            \n        if tf.random.uniform(()) > 0.4:\n            image = tf.image.flip_up_down(image)\n            mask = tf.image.flip_up_down(mask)\n            \n        if tf.random.uniform(()) > 0.5:\n            image = tf.image.rot90(image, k=1)\n            mask = tf.image.rot90(mask, k=1)\n            \n        if tf.random.uniform(()) > 0.45:\n            image = tf.image.random_saturation(image, 0.7, 1.3)\n            \n        if tf.random.uniform(()) > 0.45:\n            image = tf.image.random_contrast(image, 0.8, 1.2)\n    \n    image = tf.image.convert_image_dtype(image, tf.float32) # convert to floats in the [0,1] range\n    mask = tf.cast(mask, tf.float32)  # convert to floats 1. and 0.\n    \n    image = tf.image.resize(image, [*IMAGE_DIM])\n    image = tf.reshape(image, [*IMAGE_DIM, 3])  # reshaping image tensor\n    \n    mask = tf.image.resize(mask, [*IMAGE_DIM])\n    mask = tf.reshape(mask, [*IMAGE_DIM]) # reshaping mask tensor\n    \n    return image, mask\n\ndef generate_dataset(tiff, masks, batch_size = 16, shuffle = True):\n    \n    '''\n    generate batches of tf.Dataset\n    object\n    --------------------------------\n    \n    Arguments:\n    tiff -- tf.data.Dataset object (tf.Tensor)\n    mask -- tf.data.Dataset object (tf.Tensor)\n    batch_size -- batches of image, mask pair\n    shuffle -- generate train if True or validation data if False\n    \n    Return:\n    dataset - tf.data.Dataset dataset \n    '''\n    \n    \n    dataset = Dataset.zip((tiff, masks)) # create dataset by zipping (image, mask) into pair\n    dataset = dataset.map(decode_image_and_mask, num_parallel_calls = AUTO) # decode raw data coming from GCS bucket to valid image, mask pair \n    dataset = dataset.cache() # cache dataset preprocessing work that doesn't fit in memory\n    dataset = dataset.repeat() \n    \n    # shuffle while training else set to False\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size = 1000)\n        \n    dataset = dataset.batch(batch_size, drop_remainder = True) # generate batches of data\n    dataset = dataset.prefetch(buffer_size = AUTO) # fetch dataset while model is training\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = generate_dataset(TRAIN_TIFF, TRAIN_MASK, batch_size = BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Check image and mask size in the dataset__"},{"metadata":{"trusted":true},"cell_type":"code","source":"for img, msk in train_dataset.take(1):\n    image_batch, mask_batch = img, msk\n    print(\"Image shape: \", image_batch.numpy().shape)\n    print(\"Mask shape: \", mask_batch.numpy().shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Visualize images and masks__"},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(image_batch[0], mask_batch[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,16))\nfor i,(img, mask) in enumerate(zip(image_batch[:64], mask_batch[:64])):\n    plt.subplot(8, 8, i + 1)\n    plt.imshow(img, vmin = 0, vmax = 255)\n    plt.imshow(mask, alpha = 0.4)\n    plt.axis('off')\n    plt.subplots_adjust(wspace = None, hspace = None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Split dataset into train and validation sets__"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_img, valid_img, train_msk, valid_msk = train_test_split(TIFF, MASK, test_size = 0.2, random_state = 42)\nprint(len(train_img), len(train_msk))\nprint(len(valid_img), len(valid_msk))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_TIFF = Dataset.from_tensor_slices(train_img)\nTRAIN_MASK = Dataset.from_tensor_slices(train_msk)\n\nVALID_TIFF = Dataset.from_tensor_slices(valid_img)\nVALID_MASK = Dataset.from_tensor_slices(valid_msk)\n\nTRAIN_TIFF_CNT = tf.data.experimental.cardinality(TRAIN_TIFF).numpy()\nTRAIN_MASK_CNT = tf.data.experimental.cardinality(TRAIN_MASK).numpy()\n\nVALID_TIFF_CNT = tf.data.experimental.cardinality(VALID_TIFF).numpy()\nVALID_MASK_CNT = tf.data.experimental.cardinality(VALID_MASK).numpy()\n\nprint('Training Data Count')\nprint(f'Total Train Tiff Images: {TRAIN_TIFF_CNT}')\nprint(f'Total Train Masks: {TRAIN_MASK_CNT}')\n\nprint('Validation Data Count')\nprint(f'Total Valid Tiff Images: {VALID_TIFF_CNT}')\nprint(f'Total Valid Masks: {VALID_MASK_CNT}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"steps_per_epoch = tf.data.experimental.cardinality(TRAIN_TIFF).numpy() // BATCH_SIZE\nvalid_steps = tf.data.experimental.cardinality(VALID_TIFF).numpy() // BATCH_SIZE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = generate_dataset(TRAIN_TIFF, TRAIN_MASK, batch_size = BATCH_SIZE)\nvalid_dataset = generate_dataset(VALID_TIFF, VALID_MASK, batch_size = BATCH_SIZE)\ntrain_dataset, valid_dataset","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"__Model using Segemntation Models by Qubvel__"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = sm.Unet(BACKBONE)\n\noptimizer = 'adam'\nmodel.compile(optimizer = optimizer,\n              loss = tf.keras.losses.BinaryCrossentropy(),    \n              metrics = [sm.metrics.iou_score, 'accuracy'])\n\nearly = tf.keras.callbacks.EarlyStopping(monitor = 'val_iou_score', patience = 10, mode = 'max')\ncheck = tf.keras.callbacks.ModelCheckpoint(filepath = 'sm_unet.h5', monitor = 'val_iou_score', save_weights_only = True, \n                                       save_best_only = True, mode = 'max')\nreduce = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 10, \n                                              min_lr = 0.00001)\n#model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_dataset, \n                   epochs = EPOCHS, \n                   steps_per_epoch = steps_per_epoch, \n                   callbacks = [check, reduce], \n                   validation_data = valid_dataset, \n                   validation_steps = valid_steps, \n                   verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(history.history).plot(y = ['accuracy', 'val_accuracy'], logy = False)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(history.history).plot(y = ['loss', 'val_loss'], logy = False)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(history.history).plot(y = ['iou_score', 'val_iou_score'], logy = False)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"IOU_Score\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('sm_unet.h5')\nmodel.save('hubmap_sm.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}