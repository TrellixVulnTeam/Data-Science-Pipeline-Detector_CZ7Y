{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport pathlib, sys, os, random, time\nimport numba, cv2, gc, pickle\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm.notebook import tqdm\n\nimport albumentations as A","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import rasterio\nfrom rasterio.windows import Window","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as D\n\nimport torchvision\nfrom torchvision import transforms as T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def set_seeds(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seeds();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_PATH = '../input/hubmap-kidney-segmentation'\nEPOCHES = 35\nBATCH_SIZE = 32\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# used for converting the decoded image to rle mask\ndef rle_encode(im):\n    '''\n    im: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels = im.flatten(order = 'F')\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape=(256, 256)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape, order='F')\n\n@numba.njit()\ndef rle_numba(pixels):\n    size = len(pixels)\n    points = []\n    if pixels[0] == 1: points.append(0)\n    flag = True\n    for i in range(1, size):\n        if pixels[i] != pixels[i-1]:\n            if flag:\n                points.append(i+1)\n                flag = False\n            else:\n                points.append(i+1 - points[-1])\n                flag = True\n    if pixels[-1] == 1: points.append(size-points[-1]+1)    \n    return points\n\ndef rle_numba_encode(image):\n    pixels = image.flatten(order = 'F')\n    points = rle_numba(pixels)\n    return ' '.join(str(x) for x in points)\n\ndef make_grid(shape, window=256, min_overlap=32):\n    \"\"\"\n        Return Array of size (N,4), where N - number of tiles,\n        2nd axis represente slices: x1,x2,y1,y2 \n    \"\"\"\n    x, y = shape\n    nx = x // (window - min_overlap) + 1\n    x1 = np.linspace(0, x, num=nx, endpoint=False, dtype=np.int64)\n    x1[-1] = x - window\n    x2 = (x1 + window).clip(0, x)\n    ny = y // (window - min_overlap) + 1\n    y1 = np.linspace(0, y, num=ny, endpoint=False, dtype=np.int64)\n    y1[-1] = y - window\n    y2 = (y1 + window).clip(0, y)\n    slices = np.zeros((nx,ny, 4), dtype=np.int64)\n    \n    for i in range(nx):\n        for j in range(ny):\n            slices[i,j] = x1[i], x2[i], y1[j], y2[j]    \n    return slices.reshape(nx*ny,4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"identity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n\nclass HubDataset(D.Dataset):\n\n    def __init__(self, root_dir, transform,\n                 window=256, overlap=32, threshold = 100):\n        self.path = pathlib.Path(root_dir)\n        self.overlap = overlap\n        self.window = window\n        self.transform = transform\n        self.csv = pd.read_csv((self.path / 'train.csv').as_posix(),\n                               index_col=[0])\n        self.threshold = threshold\n        \n        \n        self.ids = {}\n        self.x, self.y = [], []\n        self.build_slices()\n        self.len = len(self.x)\n        self.as_tensor = T.Compose([\n            T.ToTensor(),\n            T.Normalize([0.625, 0.448, 0.688],\n                        [0.131, 0.177, 0.101]),\n        ])\n    \n    def build_slices(self):\n        self.masks = []\n        self.files = []\n        self.slices = []\n        count = 0\n        for i, filename in enumerate(self.csv.index.values):\n            tmp = []\n            filepath = (self.path /'train'/(filename+'.tiff')).as_posix()\n            self.files.append(filepath)\n            print(f'Transform-{filename}')\n            with rasterio.open(filepath, transform = identity) as dataset:\n                self.masks.append(rle_decode(self.csv.loc[filename, 'encoding'], dataset.shape))\n                slices = make_grid(dataset.shape, window=self.window, min_overlap=self.overlap)\n                \n                for slc in tqdm(slices, leave=False):\n                    x1,x2,y1,y2 = slc\n                    if self.masks[-1][x1:x2,y1:y2].sum() > self.threshold or np.random.randint(100) > 120:\n                        self.slices.append([i,x1,x2,y1,y2])\n                        \n                        image = dataset.read([1,2,3],\n                            window=Window.from_slices((x1,x2),(y1,y2)))\n                        \n#                         if image.std().mean() < 10:\n#                             continue\n                        \n                        # print(image.std().mean(), self.masks[-1][x1:x2,y1:y2].sum())\n                        image = np.moveaxis(image, 0, -1)\n                        self.x.append(image)\n                        self.y.append(self.masks[-1][x1:x2,y1:y2])\n                        tmp.append(count)\n                        count += 1\n            self.ids[i] = tmp\n        with open(\"folds.pkl\", \"wb\") as f:\n            pickle.dump(self.ids, f)\n    \n    # get data operation\n    def __getitem__(self, index):\n        image, mask = self.x[index], self.y[index]\n        augments = self.transform(image=image, mask=mask)\n        return self.as_tensor(augments['image']), augments['mask'][None]\n    \n    def __len__(self):\n        \"\"\"\n        Total number of samples in the dataset\n        \"\"\"\n        return self.len\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"WINDOW=1024\nMIN_OVERLAP=32\nNEW_SIZE=256\n\n# trfm = A.Compose([\n#     A.Resize(NEW_SIZE,NEW_SIZE),\n#     A.HorizontalFlip(p=0.5),\n#     A.VerticalFlip(p=0.5),\n    \n#     A.OneOf([\n#         A.RandomContrast(),\n#         A.RandomGamma(),\n#         A.RandomBrightness(),\n#         A.ColorJitter(brightness=0.07, contrast=0.07,\n#                    saturation=0.1, hue=0.1, always_apply=False, p=0.3),\n#         ], p=0.3),\n#     A.OneOf([\n#         A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n#         A.GridDistortion(),\n#         A.OpticalDistortion(distort_limit=2, shift_limit=0.5),\n#         ], p=0.0),\n#     A.ShiftScaleRotate(),\n# ])\n\n\ntrfm = A.Compose([\n    A.Resize(NEW_SIZE, NEW_SIZE),\n    A.HorizontalFlip(p=0.7),\n    A.VerticalFlip(p=0.7),\n    A.RandomRotate90(p=0.7),\n\n    A.IAAAdditiveGaussianNoise(p=0.2),\n    A.IAAPerspective(p=0.5),\n    \n    A.OneOf([\n        A.IAASharpen(p=1),\n        A.Blur(blur_limit=3, p=1),\n        A.MotionBlur(blur_limit=5),\n        A.MedianBlur(blur_limit=5),\n        A.GaussianBlur(blur_limit=5),\n        A.GaussNoise(var_limit=(5.0, 30.0)),\n        A.ISONoise(p=0.3),\n    ], p=0.7),\n    \n    A.OneOf([\n        A.RandomContrast(),\n        A.RandomGamma(),\n        A.RandomBrightness(),\n        A.ColorJitter(brightness=0.07, contrast=0.07,\n                      saturation=0.1, hue=0.1, always_apply=False, p=0.3),\n    ], p=0.7),\n    \n    A.OneOf([\n        A.HueSaturationValue(10,15,10, p=0.6),\n        A.CLAHE(clip_limit=2),\n        A.RandomBrightnessContrast(),\n    ], p=0.7),\n    \n    A.OneOf([\n        A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        A.GridDistortion(p=0.3),\n        A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=0.3),\n        A.IAAPiecewiseAffine(p=0.3),\n    ], p=0.7),\n\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.8),\n])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    model = torchvision.models.segmentation.fcn_resnet50(False)\n    \n    pth = torch.load(\"../input/pretrain-coco-weights-pytorch/fcn_resnet50_coco-1167a1af.pth\")\n    for key in [\"aux_classifier.0.weight\", \"aux_classifier.1.weight\", \"aux_classifier.1.bias\", \"aux_classifier.1.running_mean\", \"aux_classifier.1.running_var\", \"aux_classifier.1.num_batches_tracked\", \"aux_classifier.4.weight\", \"aux_classifier.4.bias\"]:\n        del pth[key]\n    \n    model.classifier[4] = nn.Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@torch.no_grad()\ndef validation(model, loader, loss_fn):\n    losses = []\n    model.eval()\n    for image, target in loader:\n        image, target = image.to(DEVICE), target.float().to(DEVICE)\n        output = model(image)['out']\n        loss = loss_fn(output, target)\n        losses.append(loss.item())\n        \n    return np.array(losses).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# deeplabv3_resnet101_coco-586e9e4e.pth  fcn_resnet50_coco-1167a1af.pth\n# deeplabv3_resnet50_coco-cd0a2569.pth   resnet101-5d3b4d8f.pth\n# fcn_resnet101_coco-7ecb50ca.pth        resnet50-19c8e357.pth\n# Copy pretrain weight for model to cache dir\n!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp ../input/pytorch-pretrained-models/resnet50-19c8e357.pth /root/.cache/torch/hub/checkpoints/\n!cp ../input/pretrain-coco-weights-pytorch/fcn_resnet50_coco-1167a1af.pth /root/.cache/torch/hub/checkpoints/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Table for results\nheader = r'''\n        Train | Valid\nEpoch |  Loss |  Loss | Time, m\n'''\n#          Epoch         metrics            time\nraw_line = '{:6d}' + '\\u2502{:7.3f}'*2 + '\\u2502{:6.2f}'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nimport torch\nimport itertools as it\nfrom torch.optim import Optimizer\nfrom collections import defaultdict\n\nclass Lookahead(Optimizer):\n    '''\n    PyTorch implementation of the lookahead wrapper.\n    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\n    '''\n    def __init__(self, optimizer,alpha=0.5, k=6,pullback_momentum=\"none\"):\n        '''\n        :param optimizer:inner optimizer\n        :param k (int): number of lookahead steps\n        :param alpha(float): linear interpolation factor. 1.0 recovers the inner optimizer.\n        :param pullback_momentum (str): change to inner optimizer momentum on interpolation update\n        '''\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        self.optimizer = optimizer\n        self.param_groups = self.optimizer.param_groups\n        self.alpha = alpha\n        self.k = k\n        self.step_counter = 0\n        assert pullback_momentum in [\"reset\", \"pullback\", \"none\"]\n        self.pullback_momentum = pullback_momentum\n        self.state = defaultdict(dict)\n\n        # Cache the current optimizer parameters\n        for group in self.optimizer.param_groups:\n            for p in group['params']:\n                param_state = self.state[p]\n                param_state['cached_params'] = torch.zeros_like(p.data)\n                param_state['cached_params'].copy_(p.data)\n\n    def __getstate__(self):\n        return {\n            'state': self.state,\n            'optimizer': self.optimizer,\n            'alpha': self.alpha,\n            'step_counter': self.step_counter,\n            'k':self.k,\n            'pullback_momentum': self.pullback_momentum\n        }\n\n    def zero_grad(self):\n        self.optimizer.zero_grad()\n\n    def state_dict(self):\n        return self.optimizer.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.optimizer.load_state_dict(state_dict)\n\n    def _backup_and_load_cache(self):\n        \"\"\"Useful for performing evaluation on the slow weights (which typically generalize better)\n        \"\"\"\n        for group in self.optimizer.param_groups:\n            for p in group['params']:\n                param_state = self.state[p]\n                param_state['backup_params'] = torch.zeros_like(p.data)\n                param_state['backup_params'].copy_(p.data)\n                p.data.copy_(param_state['cached_params'])\n\n    def _clear_and_load_backup(self):\n        for group in self.optimizer.param_groups:\n            for p in group['params']:\n                param_state = self.state[p]\n                p.data.copy_(param_state['backup_params'])\n                del param_state['backup_params']\n\n    def step(self, closure=None):\n        \"\"\"Performs a single Lookahead optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = self.optimizer.step(closure)\n        self.step_counter += 1\n\n        if self.step_counter >= self.k:\n            self.step_counter = 0\n            # Lookahead and cache the current optimizer parameters\n            for group in self.optimizer.param_groups:\n                for p in group['params']:\n                    param_state = self.state[p]\n                    p.data.mul_(self.alpha).add_(1.0 - self.alpha, param_state['cached_params'])  # crucial line\n                    param_state['cached_params'].copy_(p.data)\n                    if self.pullback_momentum == \"pullback\":\n                        internal_momentum = self.optimizer.state[p][\"momentum_buffer\"]\n                        self.optimizer.state[p][\"momentum_buffer\"] = internal_momentum.mul_(self.alpha).add_(\n                            1.0 - self.alpha, param_state[\"cached_mom\"])\n                        param_state[\"cached_mom\"] = self.optimizer.state[p][\"momentum_buffer\"]\n                    elif self.pullback_momentum == \"reset\":\n                        self.optimizer.state[p][\"momentum_buffer\"] = torch.zeros_like(p.data)\n\n        return loss\n\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        \n        self.degenerated_to_sgd = degenerated_to_sgd\n        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n            for param in params:\n                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n                    param['buffer'] = [[None, None, None] for _ in range(10)]\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = group['buffer'][int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    elif self.degenerated_to_sgd:\n                        step_size = 1.0 / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = -1\n                    buffered[2] = step_size\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n                    p.data.copy_(p_data_fp32)\n                elif step_size > 0:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n                    p.data.copy_(p_data_fp32)\n\n        return loss\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SoftDiceLoss(nn.Module):\n    def __init__(self, smooth=1., dims=(-2,-1)):\n\n        super(SoftDiceLoss, self).__init__()\n        self.smooth = smooth\n        self.dims = dims\n    \n    def forward(self, x, y):\n\n        tp = (x * y).sum(self.dims)\n        fp = (x * (1 - y)).sum(self.dims)\n        fn = ((1 - x) * y).sum(self.dims)\n        \n        dc = (2 * tp + self.smooth) / (2 * tp + fp + fn + self.smooth)\n        dc = dc.mean()\n\n        return 1 - dc\n\n    \nbce_fn = nn.BCEWithLogitsLoss()\ndice_fn = SoftDiceLoss()\n\ndef loss_fn(y_pred, y_true):\n    bce = bce_fn(y_pred, y_true)\n    dice = dice_fn(y_pred.sigmoid(), y_true)\n    return 0.8*bce+ 0.2*dice\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_fold(fold):\n    train_idx = []\n    valid_idx = []\n    with open(\"folds.pkl\", \"rb\") as f:\n        folds = pickle.load(f)\n    for i in range(8):\n        if i == fold:\n            valid_idx.extend(folds[i])\n        else:\n            train_idx.extend(folds[i])\n    return train_idx, valid_idx","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = HubDataset(DATA_PATH, window=WINDOW, overlap=MIN_OVERLAP, transform=trfm)\n\nfor fold in range(8):\n    train_idx, valid_idx = get_fold(fold)\n    print(f\"=======fold--{fold+1}=======\")\n    print(header)\n    \n    train_ds = D.Subset(ds, train_idx)\n    valid_ds = D.Subset(ds, valid_idx)\n\n    \n    # define training and validation data loaders\n    loader = D.DataLoader(\n        train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\n    vloader = D.DataLoader(\n        valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n    \n    model = get_model()\n    model.to(DEVICE)\n    \n    optimizer = RAdam(model.parameters(), lr=1e-3)\n    optimizer = Lookahead(optimizer=optimizer, k=5, alpha=0.5)\n\n    best_loss = 10\n    for epoch in range(1, EPOCHES+1):\n        losses = []\n        start_time = time.time()\n        model.train()\n        for image, target in loader:\n\n            image, target = image.to(DEVICE), target.float().to(DEVICE)\n            optimizer.zero_grad()\n            output = model(image)['out']\n            loss = loss_fn(output, target)\n            loss.backward()\n            optimizer.step()\n            losses.append(loss.item())\n        vloss = validation(model, vloader, loss_fn)\n        print(raw_line.format(epoch, np.array(losses).mean(), vloss,\n                                  (time.time()-start_time)/60**1))\n        losses = []\n\n        if vloss < best_loss:\n            best_loss = vloss\n            torch.save(model.state_dict(), f'fold{fold+1}_model_best.pth')\n    # del train set\n    del loader, vloader, train_ds, valid_ds, model\n    gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = get_model()\nmodel1.to(DEVICE)\nmodel1.load_state_dict(torch.load(f'fold1_model_best.pth'))\nmodel1.eval()\n                                                                          \nmodel2 = get_model()\nmodel2.to(DEVICE)\nmodel2.load_state_dict(torch.load(f'fold2_model_best.pth'))\nmodel2.eval()\n\nmodel3 = get_model()\nmodel3.to(DEVICE)\nmodel3.load_state_dict(torch.load(f'fold3_model_best.pth'))\nmodel3.eval()\n\nmodel4 = get_model()\nmodel4.to(DEVICE)\nmodel4.load_state_dict(torch.load(f'fold4_model_best.pth'))\nmodel4.eval()\n\nmodel5 = get_model()\nmodel5.to(DEVICE)\nmodel5.load_state_dict(torch.load(f'fold5_model_best.pth'))\nmodel5.eval()\n\nmodel6 = get_model()\nmodel6.to(DEVICE)\nmodel6.load_state_dict(torch.load(f'fold6_model_best.pth'))\nmodel6.eval()\n\nmodel7 = get_model()\nmodel7.to(DEVICE)\nmodel7.load_state_dict(torch.load(f'fold7_model_best.pth'))\nmodel7.eval()\n\nmodel8 = get_model()\nmodel8.to(DEVICE)\nmodel8.load_state_dict(torch.load(f'fold8_model_best.pth'))\nmodel8.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trfm = T.Compose([\n    T.ToPILImage(),\n    T.Resize(NEW_SIZE),\n    T.ToTensor(),\n    T.Normalize([0.625, 0.448, 0.688],\n                [0.131, 0.177, 0.101]),\n])\n\n\np = pathlib.Path(DATA_PATH)\n\nsubm = {}\n\n\nfor i, filename in enumerate(p.glob('test/*.tiff')):\n    print(f\"testing-{i+1}/5\")\n    dataset = rasterio.open(filename.as_posix(), transform = identity)\n    slices = make_grid(dataset.shape, window=WINDOW, min_overlap=MIN_OVERLAP)\n    preds = np.zeros(dataset.shape, dtype=np.uint8)\n    for (x1,x2,y1,y2) in slices:\n        image = dataset.read([1,2,3],\n                    window=Window.from_slices((x1,x2),(y1,y2)))\n        image = np.moveaxis(image, 0, -1)\n        image = trfm(image)\n        image = image.to(DEVICE)[None]\n        \n        pred = None\n        for model in [model1,model2,model3,model4,model5,model6,model7,model8]:\n            with torch.no_grad():\n                score = model(image)['out'][0][0]\n\n                score2 = model(torch.flip(image, [0, 3]))['out']\n                score2 = torch.flip(score2, [3, 0])[0][0]\n\n                score3 = model(torch.flip(image, [1, 2]))['out']\n                score3 = torch.flip(score3, [2, 1])[0][0]\n    \n                score_mean = (score + score2 + score3) \n                \n                if pred is None:\n                    pred = score_mean\n                else:\n                    pred = pred + score_mean\n        pred = pred / 24.0\n        score_sigmoid = pred.sigmoid().cpu().numpy()\n        score_sigmoid = cv2.resize(score_sigmoid, (WINDOW, WINDOW))\n\n        preds[x1:x2,y1:y2] = (score_sigmoid > 0.5).astype(np.uint8)\n            \n    subm[i] = {'id':filename.stem, 'predicted': rle_numba_encode(preds)}\n    del preds\n    gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rle2mask(mask_rle, shape=(1600,256)):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (width,height) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T\n\n\n#https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\ndef rle_encode_less_memory(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    This simplified method requires first and last pixel to be zero\n    '''\n    pixels = img.T.flatten()\n    \n    # This simplified method requires first and last pixel to be zero\n    pixels[0] = 0\n    pixels[-1] = 0\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    runs[1::2] -= runs[::2]\n    \n    return ' '.join(str(x) for x in runs)\n\ndef image_size_dict(img_id, x, y):\n    image_id = [thing[:-5] for thing in img_id]\n    x_y = [(x[i], y[i]) for i in range(0, len(x))]    \n    return dict(zip(image_id, x_y))\n\n\ndef global_shift_mask(maskpred1, y_shift, x_shift):\n    \"\"\"\n    applies a global shift to a mask by padding one side and cropping from the other\n    \"\"\"\n    if y_shift <0 and x_shift >=0:\n        maskpred2 = np.pad(maskpred1, [(0,abs(y_shift)), (abs(x_shift), 0)], mode='constant', constant_values=0)\n        maskpred3 = maskpred2[abs(y_shift):, :maskpred1.shape[1]]\n    elif y_shift >=0 and x_shift <0:\n        maskpred2 = np.pad(maskpred1, [(abs(y_shift),0), (0, abs(x_shift))], mode='constant', constant_values=0)\n        maskpred3 = maskpred2[:maskpred1.shape[0], abs(x_shift):]\n    elif y_shift >=0 and x_shift >=0:\n        maskpred2 = np.pad(maskpred1, [(abs(y_shift),0), (abs(x_shift), 0)], mode='constant', constant_values=0)\n        maskpred3 = maskpred2[:maskpred1.shape[0], :maskpred1.shape[1]]\n    elif y_shift < 0 and x_shift < 0:\n        maskpred2 = np.pad(maskpred1, [(0, abs(y_shift)), (0, abs(x_shift))], mode='constant', constant_values=0)\n        maskpred3 = maskpred2[abs(y_shift):, abs(x_shift):]\n    return maskpred3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndfpred = pd.DataFrame.from_dict(subm, orient='index')\nTARGET_ID = 'afa5e8098'\ny_shift = -40\nx_shift = -24\n\n#get image sizes \n\ndfinfo = pd.read_csv('../input/hubmap-kidney-segmentation/HuBMAP-20-dataset_information.csv')\n\nsize_dict = image_size_dict(dfinfo.image_file, dfinfo.width_pixels, dfinfo.height_pixels)  #dict which contains image sizes mapped to id's\nmask_shape = size_dict.get(TARGET_ID)\n\ntaridx = dfpred[dfpred['id']==TARGET_ID].index.values[0]  #row of TARGET_ID in dfpred\n\nmaskpred = rle2mask(dfpred.iloc[taridx]['predicted'], mask_shape)\n\nmaskpred1 = maskpred.copy()\nmaskpred1[maskpred1>0]=1\n\nmask_shifted = global_shift_mask(maskpred1, y_shift, x_shift)  #apply specified shift to mask\n\nnewrle = rle_encode_less_memory(mask_shifted)  #rle encode shifted mask\n\ndfpred.at[taridx, 'predicted'] = newrle\n\ndfsample = pd.read_csv('../input/hubmap-kidney-segmentation/sample_submission.csv')\n\nmydict = dict(zip(dfpred['id'], dfpred['predicted']))\n\ndfsample['predicted'] = dfsample['id'].map(mydict).fillna(dfsample['predicted'])\n\ndfsample = dfsample.replace(np.nan, '', regex=True)\n\ndfsample.to_csv('submission.csv',index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}