{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nimport torch.nn.functional as F\n\n__all__ = ['Res2Net', 'res2net50_v1b', 'res2net101_v1b', 'res2net50_v1b_26w_4s']\n\nmodel_urls = {\n    'res2net50_v1b_26w_4s': 'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net50_v1b_26w_4s-3cf99910.pth',\n    'res2net101_v1b_26w_4s': 'https://shanghuagao.oss-cn-beijing.aliyuncs.com/res2net/res2net101_v1b_26w_4s-0812c246.pth',\n}\n\n\nclass Bottle2neck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, baseWidth=26, scale=4, stype='normal'):\n        \"\"\" Constructor\n        Args:\n            inplanes: input channel dimensionality\n            planes: output channel dimensionality\n            stride: conv stride. Replaces pooling layer.\n            downsample: None when stride = 1\n            baseWidth: basic width of conv3x3\n            scale: number of scale.\n            type: 'normal': normal set. 'stage': first block of a new stage.\n        \"\"\"\n        super(Bottle2neck, self).__init__()\n\n        width = int(math.floor(planes * (baseWidth / 64.0)))\n        self.conv1 = nn.Conv2d(inplanes, width * scale, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(width * scale)\n\n        if scale == 1:\n            self.nums = 1\n        else:\n            self.nums = scale - 1\n        if stype == 'stage':\n            self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)\n        convs = []\n        bns = []\n        for i in range(self.nums):\n            convs.append(nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, bias=False))\n            bns.append(nn.BatchNorm2d(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n\n        self.conv3 = nn.Conv2d(width * scale, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stype = stype\n        self.scale = scale\n        self.width = width\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        for i in range(self.nums):\n            if i == 0 or self.stype == 'stage':\n                sp = spx[i]\n            else:\n                sp = sp + spx[i]\n            sp = self.convs[i](sp)\n            sp = self.relu(self.bns[i](sp))\n            if i == 0:\n                out = sp\n            else:\n                out = torch.cat((out, sp), 1)\n        if self.scale != 1 and self.stype == 'normal':\n            out = torch.cat((out, spx[self.nums]), 1)\n        elif self.scale != 1 and self.stype == 'stage':\n            out = torch.cat((out, self.pool(spx[self.nums])), 1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Res2Net(nn.Module):\n\n    def __init__(self, block, layers, baseWidth=26, scale=4, num_classes=1000):\n        self.inplanes = 64\n        super(Res2Net, self).__init__()\n        self.baseWidth = baseWidth\n        self.scale = scale\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 32, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 3, 1, 1, bias=False)\n        )\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.AvgPool2d(kernel_size=stride, stride=stride,\n                             ceil_mode=True, count_include_pad=False),\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=1, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample=downsample,\n                            stype='stage', baseWidth=self.baseWidth, scale=self.scale))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, baseWidth=self.baseWidth, scale=self.scale))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef res2net50_v1b(pretrained=False, **kwargs):\n    \"\"\"Constructs a Res2Net-50_v1b lib.\n    Res2Net-50 refers to the Res2Net-50_v1b_26w_4s.\n    Args:\n        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n    \"\"\"\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth=26, scale=4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['res2net50_v1b_26w_4s']))\n    return model\n\n\ndef res2net101_v1b(pretrained=False, **kwargs):\n    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n    Args:\n        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n    \"\"\"\n    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth=26, scale=4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n    return model\n\n\ndef res2net50_v1b_26w_4s(pretrained=False, **kwargs):\n    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n    Args:\n        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n    \"\"\"\n    model = Res2Net(Bottle2neck, [3, 4, 6, 3], baseWidth=26, scale=4, **kwargs)\n    if pretrained:\n        model_state = torch.load('./res2net50_v1b_26w_4s-3cf99910.pth')\n        model.load_state_dict(model_state)\n        # lib.load_state_dict(model_zoo.load_url(model_urls['res2net50_v1b_26w_4s']))\n    return model\n\n\ndef res2net101_v1b_26w_4s(pretrained=False, **kwargs):\n    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n    Args:\n        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n    \"\"\"\n    model = Res2Net(Bottle2neck, [3, 4, 23, 3], baseWidth=26, scale=4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['res2net101_v1b_26w_4s']))\n    return model\n\n\ndef res2net152_v1b_26w_4s(pretrained=False, **kwargs):\n    \"\"\"Constructs a Res2Net-50_v1b_26w_4s lib.\n    Args:\n        pretrained (bool): If True, returns a lib pre-trained on ImageNet\n    \"\"\"\n    model = Res2Net(Bottle2neck, [3, 8, 36, 3], baseWidth=26, scale=4, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['res2net152_v1b_26w_4s']))\n    return model","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, dilation=dilation, bias=False)\n        self.bn = nn.BatchNorm2d(out_planes)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass RFB_modified(nn.Module):\n    def __init__(self, in_channel, out_channel):\n        super(RFB_modified, self).__init__()\n        self.relu = nn.ReLU(True)\n        self.branch0 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n        )\n        self.branch1 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n            BasicConv2d(out_channel, out_channel, kernel_size=(1, 3), padding=(0, 1)),\n            BasicConv2d(out_channel, out_channel, kernel_size=(3, 1), padding=(1, 0)),\n            BasicConv2d(out_channel, out_channel, 3, padding=3, dilation=3)\n        )\n        self.branch2 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n            BasicConv2d(out_channel, out_channel, kernel_size=(1, 5), padding=(0, 2)),\n            BasicConv2d(out_channel, out_channel, kernel_size=(5, 1), padding=(2, 0)),\n            BasicConv2d(out_channel, out_channel, 3, padding=5, dilation=5)\n        )\n        self.branch3 = nn.Sequential(\n            BasicConv2d(in_channel, out_channel, 1),\n            BasicConv2d(out_channel, out_channel, kernel_size=(1, 7), padding=(0, 3)),\n            BasicConv2d(out_channel, out_channel, kernel_size=(7, 1), padding=(3, 0)),\n            BasicConv2d(out_channel, out_channel, 3, padding=7, dilation=7)\n        )\n        self.conv_cat = BasicConv2d(4*out_channel, out_channel, 3, padding=1)\n        self.conv_res = BasicConv2d(in_channel, out_channel, 1)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        x_cat = self.conv_cat(torch.cat((x0, x1, x2, x3), 1))\n\n        x = self.relu(x_cat + self.conv_res(x))\n        return x\n\n\nclass aggregation(nn.Module):\n    # dense aggregation, it can be replaced by other aggregation previous, such as DSS, amulet, and so on.\n    # used after MSF\n    def __init__(self, channel):\n        super(aggregation, self).__init__()\n        self.relu = nn.ReLU(True)\n\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.conv_upsample1 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample2 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample3 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample4 = BasicConv2d(channel, channel, 3, padding=1)\n        self.conv_upsample5 = BasicConv2d(2*channel, 2*channel, 3, padding=1)\n\n        self.conv_concat2 = BasicConv2d(2*channel, 2*channel, 3, padding=1)\n        self.conv_concat3 = BasicConv2d(3*channel, 3*channel, 3, padding=1)\n        self.conv4 = BasicConv2d(3*channel, 3*channel, 3, padding=1)\n        self.conv5 = nn.Conv2d(3*channel, 1, 1)\n\n    def forward(self, x1, x2, x3):\n        x1_1 = x1\n        x2_1 = self.conv_upsample1(self.upsample(x1)) * x2\n        x3_1 = self.conv_upsample2(self.upsample(self.upsample(x1))) \\\n               * self.conv_upsample3(self.upsample(x2)) * x3\n\n        x2_2 = torch.cat((x2_1, self.conv_upsample4(self.upsample(x1_1))), 1)\n        x2_2 = self.conv_concat2(x2_2)\n\n        x3_2 = torch.cat((x3_1, self.conv_upsample5(self.upsample(x2_2))), 1)\n        x3_2 = self.conv_concat3(x3_2)\n\n        x = self.conv4(x3_2)\n        x = self.conv5(x)\n\n        return x\n\n\nclass PraNet(nn.Module):\n    # res2net based encoder decoder\n    def __init__(self, channel=32):\n        super(PraNet, self).__init__()\n        # ---- ResNet Backbone ----\n        self.resnet = res2net50_v1b_26w_4s(pretrained=False)\n        # ---- Receptive Field Block like module ----\n        self.rfb2_1 = RFB_modified(512, channel)\n        self.rfb3_1 = RFB_modified(1024, channel)\n        self.rfb4_1 = RFB_modified(2048, channel)\n        # ---- Partial Decoder ----\n        self.agg1 = aggregation(channel)\n        # ---- reverse attention branch 4 ----\n        self.ra4_conv1 = BasicConv2d(2048, 256, kernel_size=1)\n        self.ra4_conv2 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n        self.ra4_conv3 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n        self.ra4_conv4 = BasicConv2d(256, 256, kernel_size=5, padding=2)\n        self.ra4_conv5 = BasicConv2d(256, 1, kernel_size=1)\n        # ---- reverse attention branch 3 ----\n        self.ra3_conv1 = BasicConv2d(1024, 64, kernel_size=1)\n        self.ra3_conv2 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra3_conv3 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra3_conv4 = BasicConv2d(64, 1, kernel_size=3, padding=1)\n        # ---- reverse attention branch 2 ----\n        self.ra2_conv1 = BasicConv2d(512, 64, kernel_size=1)\n        self.ra2_conv2 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra2_conv3 = BasicConv2d(64, 64, kernel_size=3, padding=1)\n        self.ra2_conv4 = BasicConv2d(64, 1, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.resnet.conv1(x)\n        x = self.resnet.bn1(x)\n        x = self.resnet.relu(x)\n        x = self.resnet.maxpool(x)      # bs, 64, 88, 88\n        # ---- low-level features ----\n        x1 = self.resnet.layer1(x)      # bs, 256, 88, 88\n        x2 = self.resnet.layer2(x1)     # bs, 512, 44, 44\n\n        x3 = self.resnet.layer3(x2)     # bs, 1024, 22, 22\n        x4 = self.resnet.layer4(x3)     # bs, 2048, 11, 11\n        x2_rfb = self.rfb2_1(x2)        # channel -> 32\n        x3_rfb = self.rfb3_1(x3)        # channel -> 32\n        x4_rfb = self.rfb4_1(x4)        # channel -> 32\n\n        ra5_feat = self.agg1(x4_rfb, x3_rfb, x2_rfb)\n        lateral_map_5 = F.interpolate(ra5_feat, scale_factor=8, mode='bilinear')    # NOTES: Sup-1 (bs, 1, 44, 44) -> (bs, 1, 352, 352)\n\n        # ---- reverse attention branch_4 ----\n        crop_4 = F.interpolate(ra5_feat, scale_factor=0.25, mode='bilinear')\n        x = -1*(torch.sigmoid(crop_4)) + 1\n        x = x.expand(-1, 2048, -1, -1).mul(x4)\n        x = self.ra4_conv1(x)\n        x = F.relu(self.ra4_conv2(x))\n        x = F.relu(self.ra4_conv3(x))\n        x = F.relu(self.ra4_conv4(x))\n        ra4_feat = self.ra4_conv5(x)\n        x = ra4_feat + crop_4\n        lateral_map_4 = F.interpolate(x, scale_factor=32, mode='bilinear')  # NOTES: Sup-2 (bs, 1, 11, 11) -> (bs, 1, 352, 352)\n\n        # ---- reverse attention branch_3 ----\n        crop_3 = F.interpolate(x, scale_factor=2, mode='bilinear')\n        x = -1*(torch.sigmoid(crop_3)) + 1\n        x = x.expand(-1, 1024, -1, -1).mul(x3)\n        x = self.ra3_conv1(x)\n        x = F.relu(self.ra3_conv2(x))\n        x = F.relu(self.ra3_conv3(x))\n        ra3_feat = self.ra3_conv4(x)\n        x = ra3_feat + crop_3\n        lateral_map_3 = F.interpolate(x, scale_factor=16, mode='bilinear')  # NOTES: Sup-3 (bs, 1, 22, 22) -> (bs, 1, 352, 352)\n\n        # ---- reverse attention branch_2 ----\n        crop_2 = F.interpolate(x, scale_factor=2, mode='bilinear')\n        x = -1*(torch.sigmoid(crop_2)) + 1\n        x = x.expand(-1, 512, -1, -1).mul(x2)\n        x = self.ra2_conv1(x)\n        x = F.relu(self.ra2_conv2(x))\n        x = F.relu(self.ra2_conv3(x))\n        ra2_feat = self.ra2_conv4(x)\n        x = ra2_feat + crop_2\n        lateral_map_2 = F.interpolate(x, scale_factor=8, mode='bilinear')   # NOTES: Sup-4 (bs, 1, 44, 44) -> (bs, 1, 352, 352)\n\n        return lateral_map_5, lateral_map_4, lateral_map_3, lateral_map_2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DIV_COEFFICIENT = 4\nDEVICE='cpu'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"net = PraNet()\nnet.load_state_dict(torch.load(\"../input/last-best-model/best_model_weights_0.4900_0.9474.save\", map_location=DEVICE))\nthreshold = 0.49\nnet = net.float().eval()\nnet.to(device=DEVICE);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport tifffile\nimport matplotlib.pyplot as plt\n\nTEST_PATH = \"../input/hubmap-kidney-segmentation/test\"\n\n#import json","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\ndef handle_by_crops(images, network, dsr_size, margin_size, device=DEVICE):\n    #print(images.shape)\n    height = images.shape[-2]  # numpy has opposite dimension order\n    width = images.shape[-1]\n    lines = []\n    network.to(device=device)\n    x_prev = 0\n    sigmoid = nn.Sigmoid()\n    for x in range(dsr_size, height+dsr_size - 2 * margin_size - 1, dsr_size - 2 * margin_size):\n        x = min(x, height)\n        line = []\n        y_prev = 0\n        for y in range(dsr_size, width+dsr_size - 2 * margin_size - 1, dsr_size - 2*margin_size):\n            y = min(y, width)\n            img_cropped = images[:,:,x-dsr_size:x, y - dsr_size: y]\n            img_cropped = img_cropped.to(device=device)\n            \n            \n            pred = network(img_cropped)\n            # the sum is the best to use (see notebook best_way_to_threshold)\n            mask = sigmoid(pred[0] + pred[1] + pred[2] + pred[3])  \n            mask = mask[:,0].detach().cpu().numpy()\n            if y - dsr_size != 0:\n                mask = mask[:,:,margin_size:]\n\n            if x - dsr_size!= 0:\n                mask = mask[:,margin_size:]\n\n            if y != width:\n                mask = mask[:,:,:-margin_size]\n            else:\n                mask = mask[:,:,y_prev-(y-dsr_size + margin_size):]\n\n\n            if x != height:\n                mask = mask[:,:-margin_size]\n            else:\n                mask = mask[:,x_prev-(x-dsr_size + margin_size):]\n            \n            #print(mask.shape)\n            #print()\n            mask = mask * 255\n            mask = mask.astype('uint8')\n            line.append(mask)\n            y_prev = y - margin_size\n\n\n        lines.append(np.concatenate(line,axis=2))\n        x_prev =  x - margin_size\n        \n    result_mask = np.concatenate(lines, axis=1)\n    #print(result_mask.shape)\n    return result_mask\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def mask2rle(pixels):\n#     '''\n#     img: numpy array, 1 - mask, 0 - background\n#     Returns run length as string formated\n#     '''\n    \n#     pixels = np.concatenate([[0], pixels, [0]])\n#     runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n#     runs[1::2] -= runs[::2]\n#     return ' '.join(str(x) for x in runs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images = [x[:-5] for x in os.listdir(TEST_PATH) if 'tiff' in x]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = int(255 * threshold)\nthreshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom PIL import Image\nimport cv2\nimport gc\nimport pickle\nImage.MAX_IMAGE_PIXELS = 1731207120\n\n\n#net.eval()\n\nimgs = []\nmasks = []\nfor i,image in enumerate(images):\n    print(image)\n    tiff_file_path = os.path.join(TEST_PATH, image + '.tiff')\n    anatomical_structure_file_path = os.path.join(TEST_PATH, image + '-anatomical-structure.json')\n    \n    # Open the file\n    original_image = tifffile.imread(tiff_file_path)\n    if len(original_image.shape) == 5:\n        original_image = original_image[0][0].transpose(1,2,0)\n\n    if original_image.shape[0] == 3: # Channels are first\n        original_image = original_image.transpose(1,2,0)\n    \n    print(original_image.shape)\n    original_height = original_image.shape[0]\n    original_width = original_image.shape[1]\n    height = original_height // DIV_COEFFICIENT  # numpy has height-then-width orientation\n    width = original_width // DIV_COEFFICIENT\n    original_image = np.array(Image.fromarray(original_image).resize((width, height)))\n    gc.collect()\n    original_image = original_image.transpose(2,0,1) / 255\n    original_image_torch = torch.from_numpy(original_image)[None].type(torch.FloatTensor)\n    \n    del original_image\n    gc.collect()\n    \n    mask_predicted = handle_by_crops(original_image_torch, net, 224*3, 112)\n    del original_image_torch\n    gc.collect()\n    mask_predicted = cv2.resize(mask_predicted[0], dsize=(original_width, original_height), interpolation=cv2.INTER_CUBIC)\n    #mask_predicted = Image.fromarray(mask_predicted[0])\n    #mask_predicted = mask_predicted.resize((original_width, original_height))\n    \n    gc.collect()\n    print(mask_predicted.shape)\n    mask_predicted = mask_predicted > threshold   # IT WILL BREAKS BECAUSE OF MEMORY HERE, THOUGH RIGHT BEFORE IT IS ONLY CONSUMED 3GB\n    gc.collect()\n    \n    #mask_predicted = np.array(mask_predicted)\n    #mask_predicted = np.array(Image.fromarray(mask_predicted[0]).resize((original_width, original_height)))\n    print(\"create mask\")\n    \n    mask_predicted = mask_predicted.T\n    gc.collect()\n    mask_predicted = mask_predicted.flatten()\n    gc.collect()\n    mask_predicted = np.concatenate([[0], mask_predicted, [0]])\n    print(\"concated\")\n    mask_predicted = np.where(mask_predicted[1:] != mask_predicted[:-1])[0] + 1\n    print(\"found\")\n    mask_predicted[1::2] -= mask_predicted[::2]\n    print(\"almost\")\n    rle_mask = ' '.join(str(x) for x in mask_predicted)\n    #return ' '.join(str(x) for x in runs)\n    print(\"done\")\n    imgs.append(image)\n    masks.append(rle_mask) \n    del mask_predicted, rle_mask\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#threshold","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#len(rle_mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mask_predicted.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#runs = np.where(mask_predicted[1:] != mask_predicted[:-1])[0] + 1\n#runs[1::2] -= runs[::2]\n#return ' '.join(str(x) for x in runs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nansw_df = pd.DataFrame()\nansw_df['id'] = imgs\nansw_df['predicted'] = masks\nansw_df.to_csv(\"./submission.csv\")\nansw_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import json\n\n# def get_coordinates_from_structure_file(structure_file_path, coef=DIV_COEFFICIENT):\n#     with open(structure_file_path, \"r\") as file:\n#         anatomical_structure_file = json.load(file)\n    \n#     lists_of_coordinates = []\n#     for structure in anatomical_structure_file:\n#         if 'properties' in structure:\n#             if 'classification' in structure['properties']:\n#                 if 'name' in structure['properties']['classification']:\n#                     name = structure['properties']['classification']['name']\n#                     if name == 'Cortex':\n#                         if 'geometry' in structure:\n#                             if 'coordinates' in structure['geometry']:\n#                                 coordinates = structure['geometry']['coordinates']\n#                                 if len(coordinates) == 1:\n#                                     coordinates = coordinates[0]\n\n#                                 x = [x[0]/coef for x in coordinates]\n#                                 y = [x[1]/coef for x in coordinates]\n#                                 coordinates = list(zip(x,y))\n#                                 lists_of_coordinates.append(coordinates)\n    \n#     return lists_of_coordinates","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# from PIL import Image\n# import gc\n# import pickle\n\n\n\n# def get_mask(mask_predicted, seg_map):\n#     return ((mask_predicted > threshold)*seg_map).astype('uint8')\n\n\n# Image.MAX_IMAGE_PIXELS = 1731207120\n\n# DIV_COEFFICIENT = 4\n\n# net.eval()\n\n# imgs = []\n# masks_masked_by_annotation = []\n# masks_no_masked = []\n# for i,image in enumerate(images):\n#     print(image)\n    \n#     tiff_file_path = os.path.join(TEST_PATH, image + '.tiff')\n#     anatomical_structure_file_path = os.path.join(TEST_PATH, image + '-anatomical-structure.json')\n#     coordinates = get_coordinates_from_structure_file(anatomical_structure_file_path, 1)\n    \n#     # Open the file\n#     original_image = tifffile.imread(tiff_file_path)\n#     if len(original_image.shape) == 5:\n#         original_image = original_image[0][0].transpose(1,2,0)\n        \n#     if original_image.shape[0] == 3: # Channels are first\n#         original_image = original_image.transpose(1,2,0)\n    \n#     print(original_image.shape)\n#     original_height = original_image.shape[0]\n#     original_width = original_image.shape[1]\n#     height = original_height // DIV_COEFFICIENT  # numpy has height-then-width orientation\n#     width = original_width // DIV_COEFFICIENT\n#     original_image = np.array(Image.fromarray(original_image).resize((width, height)))\n#     original_image = original_image.transpose(2,0,1) / 255\n#     original_image_torch = torch.from_numpy(original_image)[None].type(torch.FloatTensor)\n#     original_image  = None\n#     gc.collect()\n#     print(\"hi\")\n#     mask_predicted = handle_by_crops(original_image_torch, net, 224*3, 112)\n#     gc.collect()\n#     print(\"buy\")\n#     mask_predicted = mask_predicted[0]\n#     mask_predicted = np.array(Image.fromarray(mask_predicted).resize((original_width, original_height)))\n#     seg_map = make_seg_map(mask_predicted.shape, coordinates)\n#     mask_masked_by_annotation = get_mask(mask_predicted > threshold,seg_map)\n#     mask_no_masked = get_mask(mask_predicted > threshold,1.0)\n    \n#     imgs.append(image)\n#     masks_masked_by_annotation.append(mask2rle(mask_masked_by_annotation))\n#     masks_no_masked.append(mask2rle(mask_no_masked))\n#     #break\n#     gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# answ_df = pd.DataFrame()\n# answ_df['id'] = imgs\n# answ_df['predicted'] = masks_masked_by_annotation\n# answ_df.to_csv(\"./submission.csv\")\n# answ_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}