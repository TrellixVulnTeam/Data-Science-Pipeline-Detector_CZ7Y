{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport random\nimport math\nimport torch\nfrom rasterio.windows import Window\nfrom scipy.ndimage import rotate as sp_rotate\n\nfrom skimage import io, draw\nimport torchvision\nimport matplotlib.pyplot as plt\nimport dataclasses\nimport os\nimport rasterio\nfrom rasterio.mask import mask\nimport glob\nimport json\nimport gc\nfrom torch.utils.data import Dataset\n# https://www.kaggle.com/iafoss/256x256-images\nmean = np.array([0.65459856,0.48386562,0.69428385])\nstd = np.array([0.15167958,0.23584107,0.13146145])\n\n\ndef crop_center(img,cropx,cropy):\n    y,x,z = img.shape\n    startx = x//2-(cropx//2)\n    starty = y//2-(cropy//2)    \n    return img[starty:starty+cropy,startx:startx+cropx]\n\n        \nclass HuBMAPDatasetPreprocessing(Dataset):\n    def __init__(self, image, category='train', use_compressed=False, apply_jitter=False):\n        self.mask_folder = '/kaggle/input/hubmap-kidney-segmentation'\n        if use_compressed:\n            self.folder = '/kaggle/input/compressedhubmap'\n        else:\n            self.folder = '/kaggle/input/hubmap-kidney-segmentation'\n        self.use_compressed = use_compressed\n        self.image = image\n        self.category = category\n        self._file_ids = []\n        self.reduce = 4 if not use_compressed else 1\n        self._image = None\n        self.apply_jitter = apply_jitter\n        self._get_image()\n        if self._image.count != 3:\n            subdatasets = self._image.subdatasets\n            self.layers = []\n            if len(subdatasets) > 0:\n                for i, subdataset in enumerate(subdatasets, 0):\n                    self.layers.append(rasterio.open(subdataset))\n    def _read_geometry(self):\n        coords = [f['geometry'] for f in self._get_masks()]\n        if self.use_compressed:\n            for geom in coords:\n                for coord in geom['coordinates'][0]:\n                    coord[0] = coord[0] // 4\n                    coord[1] = coord[1] // 4\n                \n        return coords\n    def _get_image(self):\n        if self._image:\n            return self._image\n        if self.use_compressed:\n            file_path = os.path.join(self.folder, self.category, f'compressed-{self.image}.tiff')\n        else:\n            file_path = os.path.join(self.folder, self.category, f'{self.image}.tiff')\n\n        self._image = rasterio.open(file_path)\n        self.shape = self._image.shape\n        self.sz = 256 * self.reduce\n        self.pad0 = (self.sz - self.shape[0]%self.sz)\n        self.pad1 = (self.sz - self.shape[1]%self.sz)\n        self.n0max = (self.shape[0] + self.pad0)//self.sz\n        self.n1max = (self.shape[1] + self.pad1)//self.sz\n        if self.category == 'train':\n            self.mask = rasterio.mask.mask(self._image, self._read_geometry())[0] > 0\n        return self._image\n    \n    def close(self):\n        self._get_image().close()\n    \n    def _get_masks(self):\n        file_path = os.path.join(self.mask_folder, self.category, f'{self.image}.json')\n        return json.load(open(file_path))\n\n    def __del__(self):\n        self.close()\n        del self._image\n\n    def __len__(self):\n        return self.n0max * self.n1max\n\n    def __getitem__(self, idx):\n        image = self._get_image()\n        n0,n1 = idx//self.n1max, idx%self.n1max\n        x0,y0 = -self.pad0//2 + n0*self.sz, -self.pad1//2 + n1*self.sz\n        p00,p01 = max(0,x0), min(x0+self.sz,self.shape[0])\n        p10,p11 = max(0,y0), min(y0+self.sz,self.shape[1])\n\n        img = np.zeros((self.sz,self.sz,3),np.uint8)\n        if image.count == 3:\n            img[(p00-x0):(p01-x0),(p10-y0):(p11-y0)] = np.moveaxis(image.read([1,2,3],\n                window=Window.from_slices((p00,p01),(p10,p11))), 0, -1)\n        else:\n            for i,layer in enumerate(self.layers):\n                img[(p00-x0):(p01-x0),(p10-y0):(p11-y0),i] =\\\n                  layer.read(1,window=Window.from_slices((p00,p01),(p10,p11)))\n\n        if self.reduce != 1:\n            img = cv2.resize(img,(self.sz//self.reduce,self.sz//self.reduce),\n                             interpolation = cv2.INTER_AREA)\n        if self.category == 'train':\n            mask = np.zeros((3, self.sz,self.sz),np.uint8)\n\n            mask[:,(p00-x0):(p01-x0),(p10-y0):(p11-y0)] = self.mask[:, p00: p01, p10: p11]\n            mask = np.moveaxis(mask, 0, -1)\n            if self.reduce != 1:\n                mask = cv2.resize(mask, (self.sz//self.reduce,self.sz//self.reduce),\n                                 interpolation = cv2.INTER_AREA)\n        else:\n            mask = torch.zeros(1)\n\n        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n        h,s,v = cv2.split(hsv)\n        result_tensor = torch.from_numpy(img).permute(2,0,1).float()\n        #result_tensor = torch.from_numpy((img/255.0 - mean)/std).float()\n        result_tensor = torchvision.transforms.functional.normalize(result_tensor / 255, mean, std)\n        result_tensor = result_tensor.float()\n        if self.category == 'train' and self.apply_jitter is True:\n            #jitter = torchvision.transforms.ColorJitter(brightness=0.05)\n            p0 = random.random()\n            p1 = random.random()\n            p2 = random.random()\n            if p0 <= 0.7:\n                if p1 <= 0.3:\n                    result_tensor = torchvision.transforms.functional.hflip(result_tensor)\n                    mask = np.fliplr(mask)\n                if p2 <= 0.3:\n                    result_tensor = torchvision.transforms.functional.vflip(result_tensor)\n                    mask = np.flipud(mask)\n            else:\n                angle = random.randint(-20, 20)\n                result_tensor = torchvision.transforms.functional.rotate(result_tensor, angle, expand=True)\n                result_tensor = torchvision.transforms.functional.center_crop(result_tensor, (256, 256))\n                mask = sp_rotate(mask, angle)\n                x,y,z = mask.shape\n                crop = (x - 256) // 2\n                leftover = (x - 2 * crop) - 256\n                if crop != 0:\n                    mask = mask[crop+leftover:-crop, crop+leftover:-crop]\n            #result_tensor = jitter(result_tensor)\n\n        mask = mask > 0\n        if (s>40).sum() <= 1000 or img.sum() <= 1000:\n            return result_tensor, mask, -1\n\n        else: \n            return result_tensor, mask, idx","metadata":{"execution":{"iopub.status.busy":"2021-05-19T23:31:01.898026Z","iopub.execute_input":"2021-05-19T23:31:01.898364Z","iopub.status.idle":"2021-05-19T23:31:01.9341Z","shell.execute_reply.started":"2021-05-19T23:31:01.898332Z","shell.execute_reply":"2021-05-19T23:31:01.93301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchinfo\nfrom torchinfo import summary\n","metadata":{"execution":{"iopub.status.busy":"2021-05-19T23:31:01.961694Z","iopub.execute_input":"2021-05-19T23:31:01.961955Z","iopub.status.idle":"2021-05-19T23:31:08.249949Z","shell.execute_reply.started":"2021-05-19T23:31:01.96193Z","shell.execute_reply":"2021-05-19T23:31:08.24891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndataset = HuBMAPDatasetPreprocessing('0486052bb', 'train', apply_jitter=True, use_compressed=True)\n\nfig = plt.figure(figsize=(20, 12))\nf, plots = plt.subplots(2, 10)\nc = 0\nfor i in range(100):\n    sample, mask, idx = dataset[i]\n    sample = sample.permute(1,2,0)\n    \n    if idx == -1 or mask.sum() == 0:\n        continue\n    if c == 10:\n        break\n    plots[0][c].imshow(255. * sample.int().numpy())\n    plots[0][c].set_title(f'Sample #{c}')\n    mask = [np.array(m)*255 for m in mask]\n    plots[1][c].imshow(mask)\n    plots[1][c].set_title(f'Sample #{c}')\n    c += 1\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2021-05-19T23:31:08.254303Z","iopub.execute_input":"2021-05-19T23:31:08.254603Z","iopub.status.idle":"2021-05-19T23:31:08.264953Z","shell.execute_reply.started":"2021-05-19T23:31:08.254572Z","shell.execute_reply":"2021-05-19T23:31:08.264039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n\ndataset_test = HuBMAPDatasetPreprocessing('0486052bb', 'test', use_compressed=True)\nfig = plt.figure(figsize=(20, 12))\nf, plots = plt.subplots(2, 5)\nc = 0\nfor i in range(100):\n    sample, mask, idx = dataset_test[i]\n    sample = sample.permute(1,2,0)\n    if idx == -1:\n        continue\n    if c == 5:\n        break\n    print(sample)\n    plots[0][c].imshow(sample)\n    plots[0][c].set_title(f'Sample #{c}')\n    c += 1\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-05-19T23:31:08.268428Z","iopub.execute_input":"2021-05-19T23:31:08.268718Z","iopub.status.idle":"2021-05-19T23:31:08.276883Z","shell.execute_reply.started":"2021-05-19T23:31:08.268691Z","shell.execute_reply":"2021-05-19T23:31:08.275844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nfrom torch.utils import data\nimport torchvision\nfrom torchvision import transforms\nimport torchvision.transforms.functional as F\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.segmentation import fcn_resnet101\n\nimport json\n\n\n\n\n\nimport torch.nn.functional as F\n\n\"\"\" Parts of the U-Net model \"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        #self.skip_conf = nn.Sequential(\n        #    nn.Conv2d(in_channels//2,in_channels//2, kernel_size=3, padding=1),\n        #    nn.BatchNorm2d(in_channels//2),\n        #    nn.ReLU(inplace=True),\n        #)\n\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n\n    def forward(self, x1, x2):\n        #x2 = self.skip_conf(x2)\n        \n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n                        diffY // 2, diffY - diffY // 2])\n        x = torch.cat([x2, x1], dim=1)\n\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x.clone())\n\n\nclass Model(nn.Module):\n    \"\"\"The base class for the encoder-decoder architecture.\"\"\"\n    def __init__(self, n_channels=3, n_classes=1, bilinear=True, **kwargs):\n        super(Model, self).__init__(**kwargs)\n\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        factor = 2 if bilinear else 1\n\n        self.down3 = Down(256, 512)\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n        \n    def forward(self, X):\n        x1 = self.inc(X)\n\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return (logits)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T23:31:08.279089Z","iopub.execute_input":"2021-05-19T23:31:08.279579Z","iopub.status.idle":"2021-05-19T23:31:08.305345Z","shell.execute_reply.started":"2021-05-19T23:31:08.279538Z","shell.execute_reply":"2021-05-19T23:31:08.304393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nimport os\nimport numpy\nimport gc\nimport torchvision\nOUTPUT_PATH = '/kaggle/working/models'\n\ndef collate_fn(x):\n    x = filter(lambda x: x[-1] != -1, x)\n    data.dataloader.default_collate(list(x))\n\n#valset = ['e79de561c', 'cb2d976f4']\nVALSET = ['e79de561c', 'cb2d976f4']\n\ndef get_tiff_images():\n    input_folder = '/kaggle/input/hubmap-kidney-segmentation/train/*.tiff'\n    for images in glob.glob(input_folder):\n        bname = os.path.basename(images)\n        name = os.path.splitext(bname)[0]\n        if name not in VALSET:\n            dataset = HuBMAPDatasetPreprocessing(os.path.splitext(bname)[0], apply_jitter=False, use_compressed=True)\n            yield dataset\n\ndef get_val_set():\n    for image in VALSET:\n        dataset = HuBMAPDatasetPreprocessing(image, use_compressed=True)\n        yield dataset\n\n    \ndef dice_loss(pred,target, device):\n    numerator = 2 * torch.sum(pred.float() * target.float())\n    denominator = torch.sum(pred.float() + target.float())\n    #print(numerator, denominator)\n    return numerator / denominator\n\n\nimport collections\nclass Trainer:\n    def __init__(self):\n        self.is_cuda_available = torch.cuda.is_available()\n        if self.is_cuda_available:\n            dev = \"cuda:0\" \n        else:  \n            dev = \"cpu\"\n        print(\"Device\", dev)\n        self.device = torch.device(dev)\n        self.model = Model()\n        self.model.to(self.device)\n        self.epochs = 60\n        self._model_path = os.path.join(OUTPUT_PATH, 'model.pkl')\n        #self._original_model_path = os.path.join(OUTPUT_PATH, 'model.pkl')\n        self._original_model_path =  '../input/hubmapmodel/model (15).pkl' \n\n        #self._original_model_path = os.path.join(OUTPUT_PATH, 'model1.pkl') \n        self.optim = optim.Adam(self.model.parameters(), lr=0.0001)\n        self.eval_metrics = []\n        if os.path.exists(self._original_model_path):\n            if not self.is_cuda_available:\n                self._data_dict = torch.load(self._original_model_path, map_location=torch.device('cpu'))\n            else:\n                self._data_dict = torch.load(self._original_model_path)\n            self.start_positions = self._data_dict['epoch']\n            #self.loss = self._data_dict['loss']\n            #self.loss = FocalTverskyLoss(self.device, alpha=0.7, beta=0.3, gamma=0.75)\n            self.loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([5]).to(self.device))\n            #self.loss = lovasz_hinge\n            #self.optim.load_state_dict(self._data_dict['optimizer_state_dict'])\n            self.model.load_state_dict(self._data_dict['model_state_dict'])\n        else:\n            self.start_positions = 0\n            #self.loss = FocalTverskyLoss(self.device, alpha=0.7, beta=0.3, gamma=0.75)\n            self.loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([5]).to(self.device))\n            #self.loss = lovasz_hinge\n    \n    def evaluate(self, epoch, display=False):\n        val_set = get_val_set()\n        numerator, denominator = 0, 0\n        TP,FP,FN = 0, 0, 0\n        for ds in val_set:\n            print(f'started processing image {ds.image}')\n            loader = data.DataLoader(ds, 9, pin_memory=True)\n            \n            for batch_ndx, sample in enumerate(loader):\n                if not sample:\n                    continue\n                target, mask, idx = sample\n                target = target.to(self.device)\n                mask = mask.to(self.device)\n                mask = (mask[:,:,:,:1] > 0).squeeze()\n                res = self.model(target)\n                #print(res)\n                cnt = 0\n                result = (res > 0).squeeze()\n                for i in range(len(mask)):\n                    inputs = mask[i].int()\n                    targets = result[i].int()\n                    tp = (inputs * targets)    \n                    fp = ((1-targets) * inputs)\n                    fn = (targets * (1-inputs))\n                    \"\"\"\n                    if display is True and (tp.sum() > 0 and (fp.sum() / tp.sum() > 0.2 or fn.sum() / tp.sum() > 0.2)):\n                        _, subplots = plt.subplots(1, 3, figsize=(6,12))\n\n                        subplots[0].imshow(target[i].permute(1,2,0).cpu())\n                        subplots[1].imshow(res[i].detach().cpu().squeeze())\n                        \n                        display_predictions(tp, fp, fn, subplots[2])\n                    \"\"\"\n                    \n                    TP += tp.sum()\n                    FP += fp.sum()\n                    FN += fn.sum()\n                    numerator += 2 * torch.sum(mask[i].float() * result[i].float())\n                    denominator += torch.sum(mask[i].float() + result[i].float())\n\n                    \n                del mask, target, result, res\n                torch.cuda.empty_cache()\n                gc.collect()\n\n        print(f'epoch {epoch} evaluation', numerator/denominator)\n        print(f'epoch {epoch} TP={TP}, FP={FP}, FN={FN}')\n        self.eval_metrics.append(( numerator/denominator, TP, FP, FN))\n        del ds\n        gc.collect()\n\n    def train(self):\n        torch.autograd.set_detect_anomaly(True)\n\n        print(f'start position {self.start_positions}')\n        if not os.path.exists(OUTPUT_PATH):\n            os.makedirs(OUTPUT_PATH)\n        for i in range(self.start_positions, self.epochs):\n            print(f'Epoch {i}')\n\n            for dataset in get_tiff_images():\n                try:\n                    loader = data.DataLoader(dataset, 12, pin_memory=True)\n\n                    for batch_ndx, sample in enumerate(loader):\n                        self.optim.zero_grad()\n                        if not sample:\n                            continue\n                        target, mask, idx = sample\n                        mask = mask[:,:,:,:1] > 0\n\n                        result = self.model.forward(target.to(self.device)).to(self.device)\n                        #print(result)\n\n                        mask = mask.to(self.device).float().permute(0,3,1,2)\n                        loss_result = self.loss(result, mask)\n                        loss_result.backward()\n                        self.optim.step()\n\n\n                        del mask, target\n                        torch.cuda.empty_cache()\n                        gc.collect()\n\n                    print(f'data processed for {dataset.image}')\n                finally:\n                    del dataset, loader\n                    torch.cuda.empty_cache()\n                    gc.collect()\n            self.evaluate(i)\n            with open(os.path.join(OUTPUT_PATH, 'eval_metrics.csv'), 'w+') as fw:\n                fw.write('\\n'.join('\\t'.join(map(str, metric)) for metric in self.eval_metrics))\n            print(f\"saving epoch {i}\")\n            torch.save({\n                'epoch': i,\n                'model_state_dict': self.model.state_dict(),\n                'optimizer_state_dict': self.optim.state_dict(),\n                #'loss': self.loss,\n            }, os.path.join(OUTPUT_PATH, f'model_{i}.pkl'))\n            torch.save({\n                'epoch': i,\n                'model_state_dict': self.model.state_dict(),\n                'optimizer_state_dict': self.optim.state_dict(),\n                #'loss': self.loss,\n            }, self._model_path)\n\n                \ndef display_predictions(tp, fp, fn, subplot):\n    result = np.zeros(fp.shape)\n    result += tp.cpu().numpy() * 255\n    #result += fp.cpu().numpy() * 128\n    result += fn.cpu().numpy() * 64\n    subplot.imshow(result)","metadata":{"execution":{"iopub.status.busy":"2021-05-19T23:31:08.308372Z","iopub.execute_input":"2021-05-19T23:31:08.308761Z","iopub.status.idle":"2021-05-19T23:31:08.341799Z","shell.execute_reply.started":"2021-05-19T23:31:08.308724Z","shell.execute_reply":"2021-05-19T23:31:08.340909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntrainer = Trainer()\ntrainer.train()\n#trainer.evaluate(1, display=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-05-19T23:31:08.343282Z","iopub.execute_input":"2021-05-19T23:31:08.343748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport csv\nfrom skimage.color import rgb2grey\ndef collate_fn(x):\n    x = filter(lambda x: x[-1] != -1, x)\n    ls = list(x)\n    if ls:\n        return data.dataloader.default_collate(ls)\n    else:\n        return []\n\n\"\"\"    \n\ndef get_test_dataset():\n    input_folder = '/kaggle/input/hubmap-kidney-segmentation/train/*.tiff'\n    for images in glob.glob(input_folder):\n        print(images)\n        bname = os.path.basename(images)\n        dataset = HuBMAPDatasetPreprocessing(os.path.splitext(bname)[0], use_compressed=True)\n        yield dataset\n        return\n\"\"\"\ndef get_test_dataset():\n    submission_file = '../input/hubmap-kidney-segmentation/sample_submission.csv'\n    with open(submission_file) as sub_file:\n        reader = csv.reader(sub_file)\n\n        for image_id, _ in reader:\n            if image_id == 'id': continue\n            print(f\"openning {image_id}\")\n            dataset = HuBMAPDatasetPreprocessing(image_id, category='test')\n            yield dataset\n\n\n\ndef make_submission(model):\n    names, preds = [], []\n    is_cuda_available = torch.cuda.is_available()\n    if is_cuda_available:\n        dev = \"cuda:0\" \n    else:  \n        dev = \"cpu\"\n    model.to(dev)\n    for ds in get_test_dataset():\n        print(ds, len(ds))\n        loader = data.DataLoader(ds, 32, collate_fn=collate_fn)\n        mask = torch.zeros(len(ds),ds.sz,ds.sz,dtype=torch.int8)\n\n        for batch_num, data_ in enumerate(loader):\n            if not data_:\n                continue\n            print(f'processing batch {batch_num}')\n            image, _, idx = data_\n            \n            image = image.to(dev)\n            result = torch.nn.functional.upsample(model(image), scale_factor=ds.reduce, mode=\"bilinear\")\n            result = result.squeeze()\n            print(result.shape)\n\n            for i, ndx in enumerate(idx):\n                mask[ndx] = result[i] > 0\n            del result\n            gc.collect()\n            print(f'batch {batch_num} is done')\n\n            #if batch_num >= 20:\n            #    break\n        mask = mask.view(ds.n0max,ds.n1max,ds.sz,ds.sz).\\\n            permute(0,2,1,3).reshape(ds.n0max*ds.sz,ds.n1max*ds.sz)\n        mask = mask[ds.pad0//2:-(ds.pad0-ds.pad0//2) if ds.pad0 > 0 else ds.n0max*ds.sz,\n                    ds.pad1//2:-(ds.pad1-ds.pad1//2) if ds.pad1 > 0 else ds.n1max*ds.sz]\n        rle = rle_encode_less_memory(mask)\n        names.append(ds.image)\n        preds.append(rle)\n        del mask, ds\n        gc.collect()\n    df = pd.DataFrame({'id':names,'predicted':preds})\n    df.to_csv('submission.csv',index=False)\n\n        \ndef rle_encode_less_memory(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    This simplified method requires first and last pixel to be zero\n    '''\n    pixels = img.T.flatten()\n    \n    # This simplified method requires first and last pixel to be zero\n    pixels[0] = 0\n    pixels[-1] = 0\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    runs[1::2] -= runs[::2]\n    \n    return ' '.join(str(x) for x in runs)\n\nmodel = Model()\ndata_dict = torch.load('/kaggle/input/hubmapmodel/model (17).pkl')\nmodel.load_state_dict(data_dict['model_state_dict'])\n\n#make_submission(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom skimage import io\nimport matplotlib.pyplot as plt\nimage = io.imread('/kaggle/input/compressedhubmap/train/compressed-0486052bb.tiff')\n\"\"\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom skimage import io\nimport torch\nimport matplotlib.pyplot as plt\nsubimage = image[500:1500,2500:3500]\nf,subplots = plt.subplots(1, 3, figsize=(15, 45))\nsubplots[0].imshow(subimage)\nactivation = {} # dictionary to store the activation of a layer\ntorch.cuda.empty_cache()\n\ndef create_hook(name):\n    def hook(m, i, o):\n        # copy the output of the given layer\n        activation[name] = o\n\n    return hook\n\n#trainer = Trainer()\nmodel = Model()\ndata_dict = torch.load('../input/hubmapmodel/model (15).pkl', map_location=torch.device('cpu'))\nmodel.load_state_dict(data_dict['model_state_dict'])\n#model#.to('cuda:0')\ninput = torch.Tensor(subimage).permute(2,0,1).unsqueeze(0)#.to('cuda:0')\nprediction = model(input)\n\npred = prediction.detach().cpu().squeeze()\nsubplots[1].imshow(pred)\n#pred_2 = trainer.model(input).detach().cpu().squeeze()\ncounter = 0\n#print(pred_2)\nsubplots[2].imshow(pred > 0)\nmodel_weights = []\nconv_layers = []\n# append all the conv layers and their respective weights to the list\ndef rec(model):\n    global counter, model_weights, conv_layers\n    if type(model) in [nn.Conv2d]:\n        counter += 1\n        model_weights.append(model.weight)\n        conv_layers.append(model)\n        return\n    if type(model) == nn.Upsample:\n        conv_layers.append(model)\n    model_children = list(model.children())\n    for i in range(len(model_children)):\n        if type(model_children[i]) == nn.Sequential:\n            for j in range(len(model_children[i])):\n                rec(model_children[i][j])\n        else:\n            rec(model_children[i])\n\nrec(model)\nprint(f\"Total convolutional layers: {len(conv_layers)}\")\nprint(f\"Total layers: {len(model_weights)}\")\n# take a look at the conv layers and the respective weights\nfor weight, conv in zip(model_weights, conv_layers):\n    # print(f\"WEIGHT: {weight} \\nSHAPE: {weight.shape}\")\n    print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")\n    # visualize the first conv layer filters\n\n#plt.figure(figsize=(20, 17))\n#for i, filter in enumerate(model_weights[0]):\n    #plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)%%\n    #print(filter[0, :,:])\n    #plt.imshow(filter[0, :, :].detach(), cmap='gray')\n    #plt.axis('off')\n    #plt.savefig('filter.png')\n#plt.show()    \n# pass the image through all the layers\nresults = [conv_layers[0](input)]\ntry:\n    for i in range(1, len(conv_layers)):\n        # pass the result from the last layer to the next layer\n        res = conv_layers[i](results[-1])\n        print(f'calculated {i}, conv_layert: {conv_layers[i]}, result_shape: {res.shape}')\n        results.append(res)\nexcept:\n    pass\n# make a copy of the `results`\noutputs = results\n# visualize 64 features from each layer \n# (although there are more feature maps in the upper layers)\nfor num_layer in range(len(outputs)):\n    plt.figure(figsize=(30, 30))\n    layer_viz = outputs[num_layer][0, :, :, :]\n    layer_viz = layer_viz.data\n    print('size', layer_viz.size())\n    for i, filter in enumerate(layer_viz):\n        if i == 64: # we will visualize only 8x8 blocks from each layer\n            break\n        plt.subplot(8, 8, i + 1)\n        plt.imshow(filter, cmap='gray')\n        plt.axis(\"off\")\n    print(f\"Saving layer {num_layer} feature maps...\")\n    plt.savefig(f\"layer_{num_layer}.png\")\n    # plt.show()\n    plt.close()\n\"\"\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport csv\nimport os\nimport pandas as pd\ntest_csv = '../input/hubmapmodel/submission (3).csv'\noriginal_csv = '../input/hubmap-kidney-segmentation/train.csv'\nds_metadata = '../input/hubmap-kidney-segmentation/HuBMAP-20-dataset_information.csv'\n\nds_meta = pd.read_csv(ds_metadata)\ntest_value = pd.read_csv(test_csv)\noriginal_value = pd.read_csv(original_csv)\n\n\ndef rle_decode(mask_rle, shape):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape)\n\nimage_to_shape = {}\nimage_to_result = {}\nfor index, row in ds_meta.iterrows():\n    image_to_shape[os.path.splitext(row['image_file'])[0]] = (row['width_pixels'], row['height_pixels'])\n\nfor index, row in original_value.iterrows():\n    image_to_result[row['id']] = row['encoding']\n\nprint(image_to_shape.keys(), image_to_result.keys(), test_value)\nfor index, row in test_value.iterrows():\n    original = rle_decode(image_to_result[row['id']], image_to_shape[row['id']])\n    result = rle_decode(row['encoding'], image_to_shape[row['id']])\n\"\"\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}