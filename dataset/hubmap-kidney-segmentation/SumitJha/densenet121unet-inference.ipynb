{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport numba\nimport pathlib\nimport time\nimport rasterio\nfrom rasterio.windows import Window\nfrom rasterio.enums import Resampling\nimport pathlib\nfrom tqdm.notebook import tqdm\nimport cv2\nimport gc\nfrom skimage.measure import label, regionprops\nfrom skimage.morphology import disk\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom keras.models import Model\nfrom keras import layers\nfrom keras.layers import Activation\nfrom keras.layers import Dense\nfrom keras.layers import Input,Dropout,ZeroPadding2D\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Conv2D,Flatten,Conv2DTranspose,UpSampling2D,Concatenate\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras import backend as K\nfrom keras.applications import DenseNet121 as DenseNet121Keras\nimport time\nimport tensorflow as tf\n\n#DenseNet paper https://arxiv.org/pdf/1608.06993.pdf\ndef transition_block(x, reduction, name):\n    \"\"\"A Transition block.\n    A block which joins 2 dense-blocks, reduction\n    factor reduces the total number of feature maps\n    to be passed from DenseBlock 1 to 2\n    This model is called - DenseNet-C (compression = 0.5)\n\n    # Arguments\n        x: input tensor.\n        reduction: float, compression rate at transition layers.\n        name: string, block label.\n\n    # Returns\n        output tensor for the block.\n    \"\"\"\n    bn_axis =  3 if K.image_data_format() == 'channels_last' else 1\n    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n                                  name=name + '_bn')(x)\n    x = Activation('relu', name=name + '_relu')(x)\n    x = Conv2D(int(K.int_shape(x)[bn_axis] * reduction), 1,\n                      use_bias=False,kernel_initializer = 'he_normal',\n                      name=name + '_conv')(x)\n    x = AveragePooling2D(2, strides=2, name=name + '_pool')(x)\n    return x\n\ndef conv_block(x, growth_rate, name):\n    \"\"\"A building block for a dense block.\n\n    As per paper as dense block is having  X number of Conv blocks\n    Every Conv block will have [1x1-Conv then  3x3-Conv]\n    Every convolution   having BN+RELU prior to it\n\n    # Arguments\n        x: input tensor.\n        growth_rate: float, growth rate at dense layers.\n        name: string, block label.\n\n    # Returns\n        Output tensor for the block.\n    \"\"\"\n    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n    # 1x1-Convolution\n    x1 = BatchNormalization(axis=bn_axis,\n                            epsilon=1.001e-5,\n                            name=name + '_0_bn')(x)\n    x1 = Activation('relu', name=name + '_0_relu')(x1)\n    x1 = Conv2D(4 * growth_rate, 1,\n                use_bias=False,kernel_initializer = 'he_normal',\n                name=name + '_1_conv')(x1)\n\n    # 3x3-Convolution\n    x1 = BatchNormalization(axis=bn_axis, epsilon=1.001e-5,\n                                   name=name + '_1_bn')(x1)\n    x1 = Activation('relu', name=name + '_1_relu')(x1)\n    x1 = Conv2D(growth_rate, 3,\n                padding='same',\n                use_bias=False,kernel_initializer = 'he_normal',\n                name=name + '_2_conv')(x1)\n    x = Concatenate(axis=bn_axis, name=name + '_concat')([x, x1])\n    return x\n\ndef dense_block(x, blocks, name):\n    \"\"\"A dense block.\n    This block has - blocks number of Conv_blocks\n    # Arguments\n        x: input tensor.\n        blocks: integer, the number of building blocks.\n        name: string, block label.\n\n    # Returns\n        output tensor for the block.\n    \"\"\"\n    for i in range(blocks):\n        x = conv_block(x, growth_rate=32, name=name + '_block' + str(i + 1))\n    return x\n\n\ndef DenseNet(inputShape=(256, 256, 3), nClasses=2,\n                     pooling='avg',blocks = [6, 12, 24, 16],\n                     include_top=False):\n    img_input = Input(shape=inputShape)\n\n    with tf.name_scope('EntryBlock') as scope:\n        # define initial block\n        bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n        x = ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n        x = Conv2D(64, 7, strides=2, use_bias=False, kernel_initializer='he_normal', name='conv1/conv')(x)\n        x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n        x = Activation('relu', name='conv1/relu')(x)\n        x = ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n        x = MaxPooling2D(3, strides=2, name='pool1')(x)\n\n    with tf.name_scope('DenseBlock-1') as scope:\n        # denseblock1\n        x = dense_block(x, blocks[0], name='DB-1')\n        x = transition_block(x, 0.5, name='TB-1')\n\n    with tf.name_scope('DenseBlock-2') as scope:\n        # denseblock2\n        x = dense_block(x, blocks[1], name='DB-2')\n        x = transition_block(x, 0.5, name='TB-2')\n\n    with tf.name_scope('DenseBlock-3') as scope:\n        # denseblock3\n        x = dense_block(x, blocks[2], name='DB-3')\n        x = transition_block(x, 0.5, name='TB-3')\n\n    with tf.name_scope('DenseBlock-4') as scope:\n        # denseblock4\n        x = dense_block(x, blocks[3], name='DB-4')\n        x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='bn')(x)\n        x = Activation('relu', name='relu')(x)\n\n    with tf.name_scope('FC') as scope:\n        if include_top:\n            x = GlobalAveragePooling2D(name='avg_pool')(x)\n            act = 'sigmoid' if nClasses == 1 else 'softmax'\n            x = Dense(nClasses, activation=act, name='fc')(x)\n        else:\n            if pooling == 'avg':\n                x = GlobalAveragePooling2D(name='avg_pool')(x)\n            elif pooling == 'max':\n                x = GlobalMaxPooling2D(name='max_pool')(x)\n\n    model = Model(inputs=img_input, outputs=x)\n\n    return model\n\ndef DenseNet121(inputShape=(256, 256, 3), nClasses=2,\n                     pooling='avg',\n                     include_top=False):\n\n    return DenseNet(inputShape=(256, 256, 3), nClasses=2,\n                     pooling='avg',blocks = [6, 12, 24, 16],\n                     include_top=False)\n\ndef DenseNet169(inputShape=(256, 256, 3), nClasses=2,\n                     pooling='avg',\n                     include_top=False):\n\n    return DenseNet(inputShape=(256, 256, 3), nClasses=2,\n                     pooling='avg',blocks = [6, 12, 32, 32],\n                     include_top=False)\n\ndef DenseNet201(inputShape=(256, 256, 3), nClasses=2,\n                     pooling='avg',\n                     include_top=False):\n\n    return DenseNet(inputShape=(256, 256, 3), nClasses=2,\n                     pooling='avg',blocks = [6, 12, 48, 32],\n                     include_top=False)\n\n\ndef encoder_up_sample_block(input,nFilters,level,\n                  kernel_size=2,\n                  stride = 2,\n                  useConv2DTranspose = False):\n\n    if useConv2DTranspose:\n        x = Conv2DTranspose(filters=nFilters, kernel_size=kernel_size, strides=stride,\n                            name='Encoder_Up_' + str(level) + '_conv2dTrans_' + str(level))(input)\n    else:\n        x = UpSampling2D(size=kernel_size, interpolation='bilinear',name='Encoder_Up_' + str(level) + '_Up_' + str(level))(input)\n    return x\n\ndef conv_bn_act_block(x,\n                  nfilters = 64,\n                  num_conv_blocks=2,\n                  level = 0,\n                  kernel_size = 3,\n                  pad = 'same',\n                  conv_stride = 1,\n                  activation='relu',\n                  drop_out = True):\n\n    for i in range(num_conv_blocks):\n        x = Conv2D(filters=nfilters, kernel_size=kernel_size, strides=conv_stride, padding=pad,\n                   kernel_initializer='he_normal', name='Encoder_Level_{}_Conv_{}'.format(level,i+1))(x)\n        x = BatchNormalization(name='Encoder_Level_{}_BN_{}'.format(level,i+1))(x)\n        x = Activation(activation, name='Encoder_Level_{}_Act_{}'.format(level,i+1))(x)\n\n    # x = Conv2D(filters=nfilters, kernel_size=kernel_size, strides=conv_stride, padding=pad,\n    #            kernel_initializer='he_normal', name='Encoder_Level_{}_Conv_b'.format(level))(x)\n    # x = BatchNormalization(name='Encoder_Level_{}_BN_b'.format(level))(x)\n    # x = Activation(activation, name='Encoder_Level_{}_Act_b'.format(level))(x)\n\n    if drop_out:\n        x = Dropout(0.5)(x)\n\n    return x\n\ndef DenseNet121_UNET(inputShape=(256,256,3),nClasses=2,\n                     pooling = 'avg',\n                     drop_out = False,\n                     reduce_encoder_featuremap_in_decoder = False,\n                     reduction_factor = 0.5,\n                     include_top=False):\n\n    blocks = [6,12,24,16]\n    img_input = Input(shape=inputShape)\n\n    #define initial block\n    bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n    x = ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n    x = Conv2D(64, 7, strides=2, use_bias=False, kernel_initializer = 'he_normal',name='conv1/conv')(x)\n    x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='conv1/bn')(x)\n    x = Activation('relu', name='conv1/relu')(x)\n    conv1_unet = x\n    x = ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n    x = MaxPooling2D(3, strides=2, name='pool1')(x)\n\n    with tf.name_scope('DenseBlock-1') as scope:\n        # denseblock1\n        x = dense_block(x,blocks[0],name='DB-1')\n        conv2_unet = x\n        x = transition_block(x,0.5,name='TB-1')\n\n    with tf.name_scope('DenseBlock-2') as scope:\n        # denseblock2\n        x = dense_block(x, blocks[1], name='DB-2')\n        conv3_unet = x\n        x = transition_block(x, 0.5, name='TB-2')\n\n    with tf.name_scope('DenseBlock-3') as scope:\n        # denseblock3\n        x = dense_block(x, blocks[2], name='DB-3')\n        conv4_unet = x\n        x = transition_block(x, 0.5, name='TB-3')\n    with tf.name_scope('DenseBlock-4') as scope:\n        # denseblock4\n        x = dense_block(x, blocks[3], name='DB-4')\n        x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name='bn')(x)\n        conv5_unet = x\n\n    ''' Encoder '''\n    # up-level 1\n    nFilters = 512\n    level =1\n    with tf.name_scope('Encoder-Up-1') as scope:\n        up_out_1 = encoder_up_sample_block(conv5_unet, nFilters=nFilters, level=level, useConv2DTranspose=False)\n        up_out_1 = conv_bn_act_block(up_out_1, nfilters=nFilters, level='{}_1'.format(level), kernel_size=3,\n                                     pad='same', conv_stride=1, activation='relu', drop_out=drop_out)\n        if reduce_encoder_featuremap_in_decoder:\n            reqfliters = int(K.int_shape(conv4_unet)[bn_axis] * reduction_factor)\n            conv4_unet = conv_bn_act_block(conv4_unet, nfilters=reqfliters, num_conv_blocks=1,level='{}_reduce_enc_1x1'.format(level), kernel_size=1,\n                                         pad='same', conv_stride=1, activation='relu', drop_out=drop_out)\n\n        up_out_1 = Concatenate(axis=-1)([up_out_1,conv4_unet])\n        up_out_1 = conv_bn_act_block(up_out_1, nfilters = nFilters,level ='{}_2'.format(level),kernel_size = 3,\n                      pad = 'same',conv_stride = 1,activation='relu',drop_out = drop_out)\n\n    with tf.name_scope('Encoder-Up-2') as scope:\n        # up-level 2\n        nFilters = nFilters // 2 #256\n        level = level+1\n        up_out_2 = encoder_up_sample_block(up_out_1, nFilters=nFilters, level=level, useConv2DTranspose=False)\n        up_out_2 = conv_bn_act_block(up_out_2, nfilters=nFilters, level='{}_1'.format(level), kernel_size=3,\n                                     pad='same', conv_stride=1, activation='relu', drop_out=drop_out)\n        if reduce_encoder_featuremap_in_decoder:\n            reqfliters = int(K.int_shape(conv3_unet)[bn_axis] * reduction_factor)\n            conv3_unet = conv_bn_act_block(conv3_unet, nfilters=reqfliters, num_conv_blocks=1,level='{}_reduce_enc_1x1'.format(level), kernel_size=1,\n                                         pad='same', conv_stride=1, activation='relu', drop_out=drop_out)\n\n        up_out_2 = Concatenate(axis=-1)([up_out_2, conv3_unet])\n        up_out_2 = conv_bn_act_block(up_out_2, nfilters=nFilters, level='{}_2'.format(level), kernel_size=3,\n                                     pad='same', conv_stride=1, activation='relu', drop_out=drop_out)\n\n    with tf.name_scope('Encoder-Up-3') as scope:\n        # up-level 3\n        nFilters = nFilters // 2 #128\n        level = level + 1\n        up_out_3 = encoder_up_sample_block(up_out_2, nFilters=nFilters, level=level, useConv2DTranspose=False)\n        up_out_3 = conv_bn_act_block(up_out_3, nfilters=nFilters, level='{}_1'.format(level), kernel_size=3,\n                                     pad='same', conv_stride=1, activation='relu', drop_out=drop_out)\n        if reduce_encoder_featuremap_in_decoder:\n            reqfliters = int(K.int_shape(conv2_unet)[bn_axis] * reduction_factor)\n            conv2_unet = conv_bn_act_block(conv2_unet, nfilters=reqfliters, num_conv_blocks=1,level='{}_reduce_enc_1x1'.format(level), kernel_size=1,\n                                         pad='same', conv_stride=1, activation='relu', drop_out=drop_out)\n        up_out_3 = Concatenate(axis=-1)([up_out_3, conv2_unet])\n        up_out_3 = conv_bn_act_block(up_out_3, nfilters=nFilters, level='{}_2'.format(level), kernel_size=3,\n                                     pad='same', conv_stride=1, activation='relu', drop_out=drop_out)\n\n    with tf.name_scope('Encoder-Up-4') as scope:\n        # up-level 4\n        nFilters = nFilters // 2 #64\n        level = level + 1\n        up_out_4 = encoder_up_sample_block(up_out_3, nFilters=nFilters, level=level, useConv2DTranspose=False)\n        up_out_4 = conv_bn_act_block(up_out_4, nfilters=nFilters, level='{}_1'.format(level), kernel_size=3,\n                                     pad='same', conv_stride=1, activation='relu', drop_out=drop_out)\n        up_out_4 = Concatenate(axis=-1)([up_out_4, conv1_unet])\n        up_out_4 = conv_bn_act_block(up_out_4, nfilters=nFilters, level='{}_2'.format(level), kernel_size=3,\n                                     pad='same', conv_stride=1, activation='relu', drop_out=drop_out)\n\n    with tf.name_scope('Encoder-Up-5') as scope:\n        level = level + 1\n        up_out_5 = encoder_up_sample_block(up_out_4, nFilters=nFilters, level=level, useConv2DTranspose=False)\n        up_out_5 = conv_bn_act_block(up_out_5, nfilters=nFilters, level=level, kernel_size=3,\n                                     pad='same', conv_stride=1, activation='relu', drop_out=drop_out)\n\n    with tf.name_scope('FC') as scope:\n        act = 'sigmoid' if nClasses == 1 else 'softmax'\n        out = Conv2D(nClasses,kernel_size=1, strides=1, padding='same',activation=act,name='final_conv1x1_{}'.format(act))(up_out_5)\n\n    model = Model(inputs=img_input,outputs=out)\n\n    return model\n\nif False:\n    model = DenseNet121_UNET(inputShape=(512,512,3),nClasses=1,\n                             drop_out=True,\n                             reduce_encoder_featuremap_in_decoder=True,\n                             reduction_factor=0.5,\n                             include_top=False)\n    model.summary()\n    \n    print('loading weight from pretrained..')\n    modelKeras = DenseNet121Keras(input_shape=(512,512,3),include_top = False,weights='imagenet')\n    modelKeras.summary()\n    ts = time.time()\n    for i in range(2,len(modelKeras.layers)):\n        model.layers[i].set_weights(modelKeras.layers[i].get_weights())\n    print('Time taken to load and assign weights-',time.time()-ts)\n    model.summary()\n    print('Done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@numba.njit()\ndef rle_numba(pixels):\n    size = len(pixels)\n    points = []\n    if pixels[0] == 1: points.append(0)\n    flag = True\n    for i in range(1, size):\n        if pixels[i] != pixels[i-1]:\n            if flag:\n                points.append(i+1)\n                flag = False\n            else:\n                points.append(i+1 - points[-1])\n                flag = True\n    if pixels[-1] == 1: points.append(size-points[-1]+1)    \n    return points\n\ndef rle_numba_encode(image,img_size_hw):\n    if image.shape[0]!=shape[0] or image.shape[1] !=shape[1]:\n        image = cv2.resize(image,(img_size_hw[1],img_size_hw[0]),interpolation=cv2.INTER_AREA)\n    pixels = image.flatten(order = 'F')\n    points = rle_numba(pixels)\n    return ' '.join(str(x) for x in points)\n\ndef rle_encode_less_memory(img,shape):\n    ts = time.time()\n    if img.shape[0]!=shape[0] or img.shape[1] !=shape[1]:\n        print('resizing-rle generation')\n        img = cv2.resize(img,(shape[1],shape[0]),interpolation=cv2.INTER_AREA)\n        \n    pixels = img.T.flatten()\n    pixels[0] = 0\n    pixels[-1] = 0\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    runs[1::2] -= runs[::2]\n    res = ' '.join(str(x) for x in runs)\n    print('time for RLE conversion-',time.time()-ts)\n    return res\n\nidentity = rasterio.Affine(1, 0, 0, 0, 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_grid(shape, window=256, min_overlap=32):\n    \"\"\"\n        Return Array of size (N,4), where N - number of tiles,\n        2nd axis represente slices: x1,x2,y1,y2 \n    \"\"\"\n    x, y = shape\n    nx = x // (window - min_overlap) + 1\n    x1 = np.linspace(0, x, num=nx, endpoint=False, dtype=np.int64)\n\n    x1[-1] = x - window\n\n    x2 = (x1 + window).clip(0, x)\n\n    ny = y // (window - min_overlap) + 1\n    y1 = np.linspace(0, y, num=ny, endpoint=False, dtype=np.int64)\n    y1[-1] = y - window\n    y2 = (y1 + window).clip(0, y)\n    slices = np.zeros((nx,ny, 4), dtype=np.int64)\n\n    for i in range(nx):\n        for j in range(ny):\n            slices[i,j] = x1[i], x2[i], y1[j], y2[j]    \n    return slices.reshape(nx*ny,4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_tissue_mask(img,method='std',stdthd=20): #earlier 5\n    #need rgb for std\n    if method=='std':\n        mask = np.std(img,axis=-1) >stdthd\n    else:\n        #otsu\n        if np.ndim(img)==3:\n            img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n        thd = threshold_otsu(img)\n        mask = img < thd\n    return mask\n\ndef check_boundary_limit(yxpairArray,imgH,imgW):\n    mask = np.logical_and(yxpairArray[:,1] < imgH,yxpairArray[:,3]<imgW) #check H and W not going beyond image dims\n    #selectrow = mask.all(axis=1)\n    return yxpairArray[mask]\n    \ndef filterPoints(imgpath,point_sampling_level=4,patch_size=512,stride=448,min_percent_tissue_area=0.1):\n    iodataset = rasterio.open(imgpath, transform = identity)\n    x1,y1=0,0\n    x2,y2 = iodataset.height, iodataset.width\n    imgH, imgW = iodataset.height, iodataset.width\n    factor = 2 ** point_sampling_level\n    if True:\n        wsiImage = iodataset.read([1,2,3],window=Window.from_slices((x1,x2),(y1,y2)))\n        wsiImage = np.moveaxis(wsiImage, 0, -1)\n        rescaled = (wsiImage.shape[1] // factor, wsiImage.shape[0] // factor)\n        wsiImage_rsz = cv2.resize(wsiImage, rescaled, interpolation=cv2.INTER_LINEAR)\n        #wsiImage_gray = cv2.cvtColor(self.wsiImage, cv2.COLOR_RGB2GRAY) \n    else:\n        wsiImage_rsz = iodataset.read(\n                            out_shape=(dataset.count,\n                                int(dataset.height/factor),\n                                int(dataset.width/factor)),resampling=Resampling.bilinear)\n        wsiImage_rsz = np.moveaxis(wsiImage_rsz, 0, -1)\n        \n    valid_region_mask = get_tissue_mask(wsiImage_rsz,method='std')\n    H, W = valid_region_mask.shape\n    stride_mask_level = stride // factor\n    patch_mask_level = patch_size // factor\n    if True:\n        overlap_level = patch_mask_level - stride_mask_level\n        locations = make_grid(shape=(H,W),window=patch_mask_level,min_overlap=overlap_level )\n        area_thd = min_percent_tissue_area * patch_mask_level * patch_mask_level\n        yxpairList = []\n        for i in range(locations.shape[0]):\n            x1,x2,y1,y2 = np.int32(locations[i])\n            if np.sum(valid_region_mask[x1:x2, y1: y2]) > area_thd:\n                yxpairList.append([x1*factor,x2*factor,y1*factor, y2 * factor])\n            \n        del wsiImage_rsz\n        del valid_region_mask\n        del wsiImage\n        del locations\n    return np.reshape(yxpairList,[len(yxpairList),4])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def postprocessing(img):\n    from skimage.morphology import disk\n    kernel = disk(3)\n    #remove unwanted small detection\n    #self.stitch_image = cv2.erode(self.stitch_image,kernel,iterations=1)\n    #remove small white object- opening  ; Closing fills the hole in white image\n    stitch_image = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel,iterations=1)\n    return stitch_image\n\ndef processMask(maskFile,factor,fixThd=50):\n    H,W = maskFile.shape\n    resized = (W//factor,H//factor)\n    resized_mask = cv2.resize(maskFile,resized,interpolation=cv2.INTER_AREA)\n\n    kernel = disk(2)\n    resized_mask = cv2.morphologyEx(resized_mask, cv2.MORPH_OPEN, kernel, iterations=3)\n    #resized_mask = resized_mask >0\n    labelImg = label(resized_mask)\n    props = regionprops(labelImg)\n    filterImg = np.zeros_like(resized_mask)\n    for prp in props:\n        if prp.area  < 1 or prp.perimeter <1:\n            continue\n        if prp.area < fixThd: #original size = fixThd*factor (=8)\n            continue\n\n        mask = labelImg==prp.label\n        filterImg[mask] = 255\n        # id = prp.label\n        # area = prp.area\n        # solid = prp.solidity\n        # circulaity = 4 * area/(max(prp.perimeter,1))**2\n        # stats.append([id,area,solid,circulaity])\n\n    orig_mask = cv2.resize(filterImg, (W,H), interpolation=cv2.INTER_AREA)\n    del filterImg\n    return orig_mask","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_wight_path1 = '/kaggle/input/densenet121unet/weights-run1-19-val_acc-0.992-val_dice_loss-0.202.hdf5'\nmodel_wight_path2 = '/kaggle/input/densenet121unerbl/run2_boundary_loss.hdf5'\nmodel = DenseNet121_UNET(inputShape=(512,512,3),nClasses=1,\n                             drop_out=True,\n                             reduce_encoder_featuremap_in_decoder=True,\n                             reduction_factor=0.5,\n                             include_top=False)\nmodel.load_weights(model_wight_path2)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p = pathlib.Path('../input/hubmap-kidney-segmentation')\nsubm = {}\nimport time\nWINDOW = 1024;#512\nSTRIDE = 512\nMIN_OVERLAP = 256#128 # 1024-896 = 128\nNEW_SIZE = 1024\nTHRESHOLD = 0.25\nfold_models = [model] #train/aaa6a05c\nfor i, filename in tqdm(enumerate(p.glob('test/*.tiff')), \n                        total = len(list(p.glob('test/*.tiff')))):\n    \n    print(f'{i+1} Predicting {filename.stem}')\n    ts = time.time()\n    dataset = rasterio.open(filename.as_posix(), transform = identity)\n    slices = make_grid(dataset.shape, window=WINDOW, min_overlap=MIN_OVERLAP)\n    print('orig points',slices.shape)\n    if False:\n        slices = filterPoints(filename.as_posix(),point_sampling_level=4,\n                              patch_size=WINDOW,stride=STRIDE,\n                              min_percent_tissue_area=0.25)\n        print('total points b4 boundry check-',slices.shape)\n        slices = check_boundary_limit(slices,imgH=dataset.shape[0],imgW=dataset.shape[1])\n        print('total points after boundry check-',slices.shape)\n    preds = np.zeros(dataset.shape, dtype=np.uint8)\n    #print(preds.shape)\n    for (x1,x2,y1,y2) in tqdm(slices,total=slices.shape[0]):\n        image = dataset.read([1,2,3],\n                    window=Window.from_slices((x1,x2),(y1,y2)))\n        image = np.moveaxis(image, 0, -1)\n        image = cv2.resize(image, (NEW_SIZE, NEW_SIZE),interpolation = cv2.INTER_LINEAR)\n        #normlize\n        image = np.float32(image)/255.0\n        #image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n        image = np.expand_dims(image, 0)\n        \n#         pred = None\n#         for fold_model in fold_models:\n#             if pred is None:\n#                 pred = np.squeeze(fold_model.predict(image))\n#             else:\n#                 pred += np.squeeze(fold_model.predict(image))\n        \n#         pred = pred/len(fold_models)\n       \n        pred = np.squeeze(model.predict(image))\n        #print('pred',np.min(pred),np.max(pred))\n        #print('image',np.min(image),np.max(image))\n        pred = cv2.resize(pred, (WINDOW, WINDOW))\n        preds[x1:x2,y1:y2] += (pred > THRESHOLD).astype(np.uint8)\n    \n    preds = (preds > 0.5).astype(np.uint8)\n    #preds = postprocessing(preds)\n    preds = processMask(preds,factor=4,fixThd=1000)\n    preds = (preds > 0.5).astype(np.uint8)\n    if False:\n        basename = os.path.basename(filename)\n        first_name = basename.split('.')[0]\n        cv2.imwrite(f'./{first_name}_mask.png',preds*255)\n    subm[i] = {'id':filename.stem, 'predicted': rle_encode_less_memory(preds,shape=dataset.shape)}\n    #print(np.sum(preds))\n    print('Time taken',time.time()-ts)\n    del preds\n    gc.collect();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict(subm, orient='index')\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}