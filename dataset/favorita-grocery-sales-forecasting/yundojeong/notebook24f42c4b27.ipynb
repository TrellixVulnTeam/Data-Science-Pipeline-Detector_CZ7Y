{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install py7zr\nimport py7zr","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import py7zr\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        archive = py7zr.SevenZipFile(os.path.join(dirname, filename), mode='r')\n        archive.extractall(path=\"/kaggle/working\")\n        archive.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../working\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv('/kaggle/working/test.csv')\nholidays_events = pd.read_csv('/kaggle/working/holidays_events.csv')\nitems = pd.read_csv('/kaggle/working/items.csv')\noil = pd.read_csv('/kaggle/working/oil.csv')\nsample_submission = pd.read_csv('/kaggle/working/sample_submission.csv')\nstores = pd.read_csv('/kaggle/working/stores.csv')\ntransactions = pd.read_csv('/kaggle/working/transactions.csv')\ntrain = pd.read_csv('/kaggle/working/train.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Promotion"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# To solve the issue that there was low memory error or dtype error\n# train['onpromotion'].replace(False, 0)\n# train['onpromotion'].replace(True, 1)\n# train['onpromotion'].astype('float')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"promotiondata = train['onpromotion'].value_counts(sort=False, dropna=False)\npromotionarray = promotiondata.array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**To find out How many Promotions there were**"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (13,5))\nplt.bar(['NaN', 'OffPromotion', 'OnPromotion'], promotionarray, color = 'red')\nplt.xlabel(\"whether promotion On, Off or Nan\")\nplt.ylabel(\"Number of missing values in test data\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"promotionarray[2]/(promotionarray[1]+promotionarray[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To analysis and fill in null data at the onPromotion field\n# for i in test.index:\n#     if test.iloc[i].onpromotion == True :\n#         print(test.iloc[i].date)\n# TBD, kaggle memory issue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testpromotion = test['onpromotion'].value_counts(sort=False, dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testpromotionarray = testpromotion.array","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Location"},{"metadata":{"trusted":true},"cell_type":"code","source":"stores.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transactions.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"traindate = train['date'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transdate = transactions['date'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(traindate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.setdiff1d(traindate, transdate)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following the data we have calculated, we can see there were the gap between the number 1684 of train data set and the number 1682 of transactions data set. so we need to consider '2016-01-01, 03' might be ejected or deleted when merging the data set based on the date.\nThe next thing is to merge train data set and transaction data set and store data set based on the two columns, store number and date."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}