{"metadata":{"kernelspec":{"language":"python","name":"python3","display_name":"Python 3"},"language_info":{"nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.3","file_extension":".py","pygments_lexer":"ipython3","mimetype":"text/x-python"}},"nbformat_minor":1,"nbformat":4,"cells":[{"metadata":{"_uuid":"0040269d00cdb7f910368343b615af0916972195","_cell_guid":"481c640b-90d4-4f2f-a698-bd003b38fb7e"},"cell_type":"markdown","source":"# Objective\n[Heads or Tails](https://www.kaggle.com/headsortails/shopping-for-insights-favorita-eda) made a wonderful EDA in R two months ago and analyzed almost every aspects of the data  in Favorita competetion. No wonder he got more than 250 upvote with the kernel. For a beginner in Kaggle and a late paticipator in Favorita challenge, I felt astonished when I saw his beautiful plots and detailed analysis. I definitly want to go to the daRk side to enjoy all the fantastic features from ggplot2 library. However, since I already chose Python as my primary language in ML research, I just want to absorb all his ideas and skills in data analysis and visualization. What I can do is to reproduce his work in Python and practice my coding skills through this process. I found there are still many minor features I don't know how to express in the plots. Hope you can share your comments and methods after reading my kernel. Thanks."},{"metadata":{"_uuid":"5b654e5713b67bc8114258f44f7c0f63d018670b","_cell_guid":"a6ab4e5d-ef36-4bee-ab37-b3a2aabe0890"},"cell_type":"markdown","source":"## 1. Loading data"},{"metadata":{"_uuid":"c67563d59ca88f6990b23a21cc7027daea4b13a9","collapsed":true,"_cell_guid":"b91e7856-6f64-4c30-9fbe-4cc0d1bf3342"},"execution_count":null,"cell_type":"code","source":"import gc\nimport time\nimport numpy as np\nimport pandas as pd\n\nfrom scipy.sparse import csr_matrix, hstack\n\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nimport xgboost\nfrom time import time\nfrom sklearn.metrics import make_scorer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nfrom scipy.stats import norm\nimport random\nimport time\n","outputs":[]},{"metadata":{"_uuid":"36b1c57f029ce908aeb02756287b22d9e66ab9e9","_cell_guid":"dbcf4fec-891c-461a-9f02-c21c386ef5d6"},"execution_count":null,"cell_type":"code","source":"from subprocess import check_output\nprint (check_output(['ls', '../input']).decode('utf8'))","outputs":[]},{"metadata":{"_uuid":"a8fe31900089455ae9ed1dda378722b9d0ccd1bd","_cell_guid":"02ad20d2-0cf7-4500-9add-b52c66da0cf6"},"cell_type":"markdown","source":"The `train` dataset include more than 125 million lines. It takes forever to analyze them and plot. So I will only analyze 5% of them. "},{"metadata":{"_uuid":"2491d6f0fbe9b750c3728f01ddbf2ada47baa9e6","_cell_guid":"93d71d35-a377-40b6-81dd-c62a00512d6e"},"execution_count":null,"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")","outputs":[]},{"metadata":{"_uuid":"4af716e83c9a4ae96b8338eddb177ce56d45dcd9","_cell_guid":"70f78449-f6b7-4838-b676-78ad76139269"},"execution_count":null,"cell_type":"code","source":"train_sample = train.sample(frac = 0.05)\ntrain_sample.head()","outputs":[]},{"metadata":{"_uuid":"ec9e47cf615bfbaf5a16c222fa25119bd7a3ccf0","collapsed":true,"_cell_guid":"6abf47a5-d9ad-44e3-8096-f646f2d7aae8"},"execution_count":null,"cell_type":"code","source":"holiday_events = pd.read_csv('../input/holidays_events.csv')\nitems = pd.read_csv('../input/items.csv')\noil = pd.read_csv('../input/oil.csv')\nstores = pd.read_csv('../input/stores.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntransactions = pd.read_csv('../input/transactions.csv')","outputs":[]},{"metadata":{"_uuid":"c544d6d911e4f83576b3f4606e226a05f122fd49","_cell_guid":"a2ed282b-23d6-405e-b9ed-96ac52e0df3d"},"execution_count":null,"cell_type":"code","source":"print (holiday_events.head())\nprint (holiday_events.shape)\nprint (holiday_events.describe())","outputs":[]},{"metadata":{"_uuid":"2440dc14d3884c250fab8e62d53569f33293b186","_cell_guid":"85c20f95-2d3b-4b13-98d4-f99d53831803"},"execution_count":null,"cell_type":"code","source":"print (items.head())\nprint (items.shape)\nprint (items.describe())","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"print (len(items['family'].value_counts().index))\nprint (len(items['class'].value_counts().index))\nprint (items['perishable'].value_counts())","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From `item` dataset, we can find:\n    1. There are 4100 items.\n    2. These items can be classfied to 33 families (e.g. GROCERY I)\n    3. There are 337 classes\n    4. 3114 of them are not perishable, 986 of them are perishable"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"print (oil.head())\nprint (oil.shape)\nprint (oil.describe())","outputs":[]},{"metadata":{"scrolled":true},"execution_count":null,"cell_type":"code","source":"import datetime\ntrain_sample['date'] = train_sample['date'].apply(datetime.datetime.strptime, args = (\"%Y-%m-%d\",))\ntrain_sample.sort_values(by = 'date', inplace = True)\nprint (train_sample.head())\nprint (train_sample.shape)\nprint (train_sample.describe())\nprint (train_sample.isnull().sum())\n","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"train_sample['onpromotion'].fillna(value = 'missing', inplace = True)\n#train_sample['onpromotion'].isnull().sum()\nprint (train_sample['onpromotion'].value_counts())","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"print (test.head())\nprint (test.shape)\nprint (test.describe())\nprint (test.isnull().sum())","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"print (test['onpromotion'].value_counts())","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Individual feature visualisations\n\nIn this step we will get an visual overview of the data by plotting the individual feature distribution for each data set sepeartely."},{"metadata":{},"cell_type":"markdown","source":"## 3. Training data"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"daily_sale = pd.DataFrame()\ndaily_sale['count'] = train_sample['date'].value_counts()\ndaily_sale['date'] = train_sample['date'].value_counts().index\ndaily_sale = daily_sale.sort_values(by = 'date')\nprint (daily_sale.head(3))\nunit_sale = pd.DataFrame()\nunit_sale['count'] = train_sample[train['unit_sales']>0]['unit_sales'].value_counts()\nunit_sale['positive_unit_sales'] = train_sample[train['unit_sales']>0]['unit_sales'].value_counts().index\nunit_sale = unit_sale.sort_values(by = 'positive_unit_sales')","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"promotion_count = pd.DataFrame()\npromotion_count ['count'] = train_sample['onpromotion'].value_counts()","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize = (18,5))\nsns.set_style('darkgrid')\nax1.plot(daily_sale['date'], daily_sale['count'])\nax1.set_xlabel('date', fontsize=15)\nax1.set_ylabel('count', fontsize=15)\nax1.tick_params(labelsize=15)\nsns.barplot(x = promotion_count.index, y = promotion_count['count'],ax = ax2)\nax2.set_ylabel('count', fontsize = 15)\nax2.set_xlabel ('onpromotion',fontsize =15)\nax2.tick_params(labelsize = 15)\nax2.ticklabel_format(style = 'sci',scilimits = (0,0), axis = 'y')\nplt.subplot(1,3,3)\nplt.loglog(unit_sale['positive_unit_sales'], unit_sale['count'])\nplt.ylabel('count', fontsize = 15)\nplt.xlabel('positive_unit_sales', fontsize = 15)\nplt.xticks(fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.show()","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"store_count = pd.DataFrame()\nstore_count['count'] = train_sample['store_nbr'].value_counts().sort_index()","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (18, 3))\nsns.barplot(x = store_count.index, y = store_count['count'], ax = ax)\nax.set_ylabel('count', fontsize = 15)\nax.set_xlabel('store_nbr',fontsize = 15)\nax.tick_params(labelsize=15)","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"item_count = pd.DataFrame()\nitem_count['count'] = train_sample['item_nbr'].value_counts().sort_index()\nplt.plot(item_count.index)","outputs":[]},{"metadata":{"scrolled":true},"execution_count":null,"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (18, 3))\nsns.barplot(x = item_count.index, y = item_count['count'], ax = ax)\nax.set_ylabel('count', fontsize = 15)\nax.set_xlabel('item_nbr',fontsize = 15)\nax.tick_params(axis = 'x',which = 'both',top = 'off', bottom = 'off', labelbottom = 'off')","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"neg_unit_sale = pd.DataFrame()\nneg_unit_sale['unit_sales'] = (-train_sample[train_sample['unit_sales'] < 0]['unit_sales'])\nneg_unit_sale.head()","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (18, 5))\n#ax.set_xscale('log')\nnp.log(neg_unit_sale['unit_sales']).plot.hist(ax = ax, log = True,edgecolor = 'white', bins = 50)\nax.set_xlabel('neg_unit_sales (log10 scale)', fontsize=15)\nax.set_ylabel('count', fontsize=15)\nax.tick_params(labelsize=15)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Stores"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"city_count = pd.DataFrame()\ncity_count['count'] = stores['city'].value_counts().sort_index()\nfig, (ax1,ax2) = plt.subplots(1,2,figsize = (18, 4))\ng = sns.barplot(x = city_count.index, y = city_count['count'], ax = ax1)\nax1.set_ylabel('count', fontsize = 15)\nax1.set_xlabel('city',fontsize = 15)\nax1.tick_params(labelsize=15)\ng.set_xticklabels(city_count.index, rotation = 45)\nstate_count =pd.DataFrame()\nstate_count['count'] = stores['state'].value_counts().sort_index()\ng2 = sns.barplot(x = state_count.index, y = state_count['count'], ax = ax2)\nax2.set_ylabel('count', fontsize = 15)\nax2.set_xlabel('state',fontsize = 15)\nax2.tick_params(labelsize=15)\ng2.set_xticklabels(state_count.index, rotation = 45)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"type_count = pd.DataFrame()\ntype_count['count'] = stores['type'].value_counts().sort_index()\nfig, (ax1,ax2) = plt.subplots(1,2,figsize = (18, 4))\ng = sns.barplot(x = type_count.index, y = type_count['count'], ax = ax1)\nax1.set_ylabel('count', fontsize = 15)\nax1.set_xlabel('type',fontsize = 15)\nax.tick_params(labelsize=15)\ncluster_count = pd.DataFrame()\ncluster_count['count'] = stores['cluster'].value_counts().sort_index()\ng = sns.barplot(x = cluster_count.index, y = cluster_count['count'], ax = ax2)\nax2.set_ylabel('count', fontsize = 15)\nax2.set_xlabel('cluster',fontsize = 15)\nax2.tick_params(labelsize=15)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"import squarify\n","outputs":[]},{"metadata":{"scrolled":true},"execution_count":null,"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (16,8))\ngrouped_city = np.log1p(stores.groupby(['city']).count())\ngrouped_city['state'] = grouped_city.index\ncurrent_palette = sns.color_palette()\nsquarify.plot(sizes = grouped_city['store_nbr'], label = grouped_city.index, alpha = 0.8,color = current_palette)\nplt.rc('font', size = 25)\nplt.axis('off')","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For the treemap, I used squarify to show the `count` as areas of different `city` and `state`. I don't know how to plot sub treemap in a state square to illustrate  `city` count. If you know how to do it, please make a comment"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (16,8))\ngrouped_state = np.log1p(stores.groupby(['state']).count())\ngrouped_state['state'] = grouped_state.index\ncurrent_palette = sns.color_palette()\nsquarify.plot(sizes = grouped_state['store_nbr'], label = grouped_state.index, alpha = 0.8,color = current_palette)\nplt.rc('font', size = 20)\nplt.axis('off')","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"print (items.head())\nclass_count = pd.DataFrame()\nclass_count['count'] = items['class'].value_counts().sort_index()\nmost_freq_class = items['class'].value_counts().head()\nperish_count = items['perishable'].value_counts()\nfamily_count = items['family'].value_counts().sort_index()","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Items"},{"metadata":{"scrolled":true},"execution_count":null,"cell_type":"code","source":"fig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize = (18,3))\nsns.barplot(x = class_count.index, y = class_count['count'], ax = ax1)\nax1.set_ylabel('count', fontsize = 15)\nax1.set_xlabel('class',fontsize = 15)\nax1.tick_params(labelsize = 6, axis = 'x',which = 'both',top = 'off', bottom = 'off', labelbottom = 'off')\nsns.barplot(x = most_freq_class.index, y = most_freq_class.values, ax = ax2)\nax2.set_ylabel('n', fontsize = 15)\nax2.set_xlabel('class-most frequent',fontsize = 15)\nax2.tick_params(labelsize = 12)\nsns.barplot(x = perish_count.index, y = perish_count.values, ax = ax3)\nax3.set_ylabel('count', fontsize = 15)\nax3.set_xlabel('perish',fontsize = 15)\nax3.tick_params(labelsize = 12)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (18, 3))\ng = sns.barplot(x = family_count.index, y = family_count.values, ax = ax)\nax.set_ylabel('count', fontsize = 15)\nax.set_xlabel('family',fontsize = 15)\nax.tick_params(labelsize = 12)\ng.set_xticklabels(family_count.index, rotation = 60)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (16,8))\ngrouped_family = np.log1p(items.groupby(['family']).count().head(20))\ncurrent_palette = sns.color_palette()\nsquarify.plot(sizes = grouped_family['class'], label = grouped_family.index, alpha = 0.8,color = current_palette)\nplt.rc('font', size = 12)\nplt.axis('off')","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"med_trans = transactions.groupby(['date']).median()\nmed_trans['date'] = med_trans.index\nmed_trans['date'] = med_trans['date'].apply(datetime.datetime.strptime, args = (\"%Y-%m-%d\",))\nmed_trans.head()","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.4 Transactions\n\nI can reproduce Head and Tails' med_trans vs. date plot but the feature I missed is the trend curve. It seems very convinient and easy to fulfill in `R` with `geom_smooth` but I don't know how to do it in Python. Please let me know in the comment if you have a elegant solution. "},{"metadata":{"scrolled":true},"execution_count":null,"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (12,6))\nsns.set_style('darkgrid')\nplt.plot(med_trans['date'], med_trans['transactions'])\n#ax = sns.regplot(x='date', y='transactions', data = med_trans)\nax.set_xlabel('date', fontsize=15)\nax.set_ylabel('med_trans', fontsize=15)\nax.tick_params(labelsize=15)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since I don't know how to make smooth trend curve of time series plot, I don't have acess to the smoothed transations time series for each individual store. "},{"metadata":{},"execution_count":null,"cell_type":"code","source":"transactions['date'] = transactions['date'].apply(datetime.datetime.strptime, args = (\"%Y-%m-%d\",))\ngrouped_trans = transactions.groupby(['date','store_nbr']).sum().unstack()\ngrouped_trans.head()\nfig, ax = plt.subplots(figsize = (12,6))\ngrouped_trans.plot(ax = ax)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"## 2.5 Oil\n\nBelow is the plot of price over time together with its weekly changes (price - price 7 days later):"},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"oil['date'] = oil['date'].apply(datetime.datetime.strptime, args = (\"%Y-%m-%d\",))","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"import statsmodels.formula.api as sm\nregression = (sm.ols(formula=\"dcoilwtico ~ date \", data=oil).fit())","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"oil['trend'] = regression.predict(oil['date'])","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (18, 5))\nax.plot(oil['date'],oil['dcoilwtico'], label = 'oilprice')\nax.set\nax.set_xlabel('date', fontsize=15)\nax.set_ylabel('oilprice', fontsize=15)\nax.tick_params(labelsize=15)","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"lag7 = oil['dcoilwtico'] - oil['dcoilwtico'].shift(7)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (18, 5))\nax.plot(oil['date'],lag7, label = 'weekly variations in oil price')\nax.set\nax.set_xlabel('date', fontsize=15)\nax.set_ylabel('weekly variations', fontsize=15)\nax.tick_params(labelsize=15)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.6 Holidays"},{"metadata":{},"execution_count":null,"cell_type":"code","source":"holiday_events.head()","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"holiday_events['date'] = holiday_events['date'].apply(datetime.datetime.strptime, args = (\"%Y-%m-%d\",))","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"type_count = holiday_events['type'].value_counts().sort_index()\nlocale_count = holiday_events['locale'].value_counts().sort_index()","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2,figsize = (18, 4))\nsns.barplot(x = type_count.index, y = type_count.values, ax = ax1)\nax1.set_ylabel('count', fontsize = 15)\nax1.set_xlabel('type',fontsize = 15)\nsns.barplot(x = locale_count.index, y = locale_count.values, ax = ax2)\nax2.set_ylabel('count', fontsize = 15)\nax2.set_xlabel('locale',fontsize = 15)","outputs":[]},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"most_freq_descr = holiday_events['description'].value_counts().head(12).sort_index()\n#print (most_freq_descr)\ntransferred_count = holiday_events['transferred'].value_counts()","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2,figsize = (18, 4))\nsns.barplot(y = most_freq_descr.index, x = most_freq_descr.values, ax = ax1)\nax1.set_ylabel('Description-most frequent', fontsize = 15)\nax1.set_xlabel('Frequency',fontsize = 15)\nsns.barplot(x = transferred_count.index, y = transferred_count.values, ax = ax2)\nax2.set_ylabel('count', fontsize = 15)\nax2.set_xlabel('transferred',fontsize = 15)","outputs":[]},{"metadata":{},"execution_count":null,"cell_type":"code","source":"locale_name_count = holiday_events['locale_name'].value_counts().sort_index()\nlocale_name_count.head()\nfig, ax = plt.subplots(figsize = (18, 3))\ng = sns.barplot(x = locale_name_count.index, y = locale_name_count.values, ax = ax)\nax.set_ylabel('count', fontsize = 15)\nax.set_xlabel('locale_name',fontsize = 15)\nax.tick_params(labelsize = 12)\ng.set_xticklabels(locale_name_count.index, rotation = 45)","outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Summary\n\nI have completed the basic visualisation of datasets in the project. The following will be the analysis and plots combining different datasets together. The advantage of `ggplot` library in `R` is very obvious. It's easy to add legend, superposition of different plot, get smoothed trend curve and make plots pretty. "},{"metadata":{"collapsed":true},"execution_count":null,"cell_type":"code","source":"","outputs":[]}]}