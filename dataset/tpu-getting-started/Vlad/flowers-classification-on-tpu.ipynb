{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\nignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\ntrain_filenames = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords-jpeg-512*512/train/*.tfrec')\nraw_train_dataset = tf.data.TFRecordDataset(train_filenames, num_parallel_reads=AUTO)\n\nval_filenames = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords-jpeg-512*512/val/*.tfrec')\nraw_val_dataset = tf.data.TFRecordDataset(val_filenames, num_parallel_reads=AUTO)\n\ntest_filenames = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords-jpeg-512*512/test/*.tfrec')\nraw_test_dataset = tf.data.TFRecordDataset(test_filenames, num_parallel_reads=AUTO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfor raw_record in raw_test_dataset.take(1):\n  example = tf.train.Example()\n  example.ParseFromString(raw_record.numpy())\n#   print(example)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%config Completer.use_jedi = False\n\n#detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n\n\ntpu_strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_SIZE = (512, 512)\nAUTO = tf.data.experimental.AUTOTUNE\nbatch_size = 16*tpu_strategy.num_replicas_in_sync\n\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data , channels=3)\n#     image = tf.cast(image, tf.float32) / 255.0\n    image = tf.reshape(image, (*IMAGE_SIZE, 3))\n#     image = tf.image.random_flip_left_right(image)\n    return image\n\n\ndef parse_tfrecords(example):\n    features = {\n        'class': tf.io.FixedLenFeature([], tf.int64),\n        'image': tf.io.FixedLenFeature([], tf.string),\n#         'one_hot_class': tf.io.VarLenFeature(tf.float32)\n    }\n    example = tf.io.parse_single_example(example, features)\n    image = decode_image(example['image'])\n    return image, tf.cast(example['class'], tf.int32)\n\ndef parse_tfrecords_test(example):\n    features = {\n        'id': tf.io.FixedLenFeature([], tf.string),\n        'image': tf.io.FixedLenFeature([], tf.string),\n#         'one_hot_class': tf.io.VarLenFeature(tf.float32)\n    }\n    example = tf.io.parse_single_example(example, features)\n    image = decode_image(example['image'])\n    return image, example['id']\n\n\ndef augment(image, label):\n    return tf.image.random_flip_left_right(image), label\n\n\ndef load_dataset(filenames, labeled=True, as_supervised=True):\n    order = tf.data.Options()\n    order.experimental_deterministic = True\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    dataset = dataset.with_options(order)\n    \n    parse = parse_tfrecords if as_supervised else parse_tfrecords_test\n    return dataset.map(parse).map(augment, num_parallel_calls=AUTO)\n\n\ntrain_dataset = load_dataset(train_filenames)\ntrain_dataset = train_dataset.batch(batch_size).prefetch(AUTO)\n\nval_dataset = load_dataset(val_filenames)\nval_dataset = val_dataset.batch(batch_size).prefetch(AUTO)\n\ntest_dataset = load_dataset(test_filenames, as_supervised=False)\ntest_dataset = test_dataset.batch(batch_size).prefetch(AUTO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_one_flower(image, title):\n    plt.axis('off')\n    plt.imshow(image)\n    plt.title(title, fontsize=16)\n\n    \nimage, label = next(train_dataset.unbatch().as_numpy_iterator())\ndisplay_one_flower(image, label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image, label in train_dataset.take(2):\n    print(image.numpy().shape)\n    print(label.numpy().shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n#     pretrained_model = tf.keras.applications.Xception(input_shape=(*IMAGE_SIZE, 3), include_top=False)\n    img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.vgg16.preprocess_input(tf.cast(data, tf.float32)), input_shape=(*IMAGE_SIZE, 3))\n    pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False)\n    pretrained_model.trainable = False\n    model = tf.keras.Sequential([\n        img_adjust_layer,\n        pretrained_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(104, activation='softmax')\n    ])\n    model.compile(loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'], optimizer='adam')\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%config Completer.use_jedi = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def count_data_items(filenames):\n    basenames = [os.path.basename(path).split('-')[-1] for path in filenames]\n    images_counts = [int(basename.split('.')[0]) for basename in basenames]\n    return np.sum(images_counts)\n    \n\nn_train = count_data_items(train_filenames)\nn_val = count_data_items(val_filenames)\nn_test = count_data_items(test_filenames)\n\ntrain_steps = n_train // batch_size\n\nn_train, n_val, n_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 12\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005*tpu_strategy.num_replicas_in_sync\nrampup_epochs = 5\nsustain_epochs = 0\nexp_decay = .8\n\ndef lrfn(epoch):\n    if epoch < rampup_epochs:\n        return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n    else:\n        return (max_lr - min_lr)*exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n\nrang = np.arange(EPOCHS)\ny = [lrfn(x) for x in rang]\nplt.plot(rang, y)\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset.map(lambda i, l: print(l))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    train_dataset, validation_data=val_dataset, epochs=EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# history.history","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.mean(history.history['val_sparse_categorical_accuracy'][-5:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = model.predict_classes(test_dataset.map(lambda img, lab: img))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_ids = test_dataset.map(lambda img, lab: lab).unbatch()\ntest_ids = next(iter(img_ids.batch(n_test))).numpy().astype('U')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({'id': test_ids, 'label': prediction}).to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}