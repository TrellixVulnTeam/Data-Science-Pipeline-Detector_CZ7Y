{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport glob\nprint(os.listdir(\"../input/tpu-getting-started/tfrecords-jpeg-192x192/train\"))","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:37:16.884475Z","iopub.execute_input":"2021-06-26T14:37:16.885019Z","iopub.status.idle":"2021-06-26T14:37:16.914329Z","shell.execute_reply.started":"2021-06-26T14:37:16.884906Z","shell.execute_reply":"2021-06-26T14:37:16.913195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pdpipe\n!pip install kaggledatasets\n!pip install tensorflow_datasets\n\nimport seaborn as sns\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport pdpipe as pdp\nimport numpy as np\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom kaggle_datasets import KaggleDatasets","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:37:16.916044Z","iopub.execute_input":"2021-06-26T14:37:16.916442Z","iopub.status.idle":"2021-06-26T14:37:49.1521Z","shell.execute_reply.started":"2021-06-26T14:37:16.916402Z","shell.execute_reply":"2021-06-26T14:37:49.151283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turn on tpu\n# Detect TPU, return appropriate distribution strategy\nstrategy = tf.distribute.get_strategy() \n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:37:49.153679Z","iopub.execute_input":"2021-06-26T14:37:49.154186Z","iopub.status.idle":"2021-06-26T14:37:54.506549Z","shell.execute_reply.started":"2021-06-26T14:37:49.154156Z","shell.execute_reply":"2021-06-26T14:37:54.505443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path()\nGCS_DS_PATH\n\nGCS_PATH_SELECT = { # available image sizes\n    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n    512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n}\nGCS_PATH = GCS_PATH_SELECT[192]\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec') # predictions on this dataset should be submitted for the competition","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:37:54.508115Z","iopub.execute_input":"2021-06-26T14:37:54.508411Z","iopub.status.idle":"2021-06-26T14:37:55.049197Z","shell.execute_reply.started":"2021-06-26T14:37:54.508383Z","shell.execute_reply":"2021-06-26T14:37:55.048412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gsutil ls $GCS_PATH","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:37:55.050693Z","iopub.execute_input":"2021-06-26T14:37:55.051386Z","iopub.status.idle":"2021-06-26T14:37:58.862648Z","shell.execute_reply.started":"2021-06-26T14:37:55.051343Z","shell.execute_reply":"2021-06-26T14:37:58.861461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read train data\ntrain_filenames = TRAINING_FILENAMES\ntrain_dataset = tf.data.TFRecordDataset(train_filenames)\n\n# Read val data\nval_filenames = VALIDATION_FILENAMES\nval_dataset = tf.data.TFRecordDataset(val_filenames)\n\n# Read test dataset\ntest_filenames = TEST_FILENAMES\ntest_dataset = tf.data.TFRecordDataset(test_filenames)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:37:58.864357Z","iopub.execute_input":"2021-06-26T14:37:58.864681Z","iopub.status.idle":"2021-06-26T14:37:58.899384Z","shell.execute_reply.started":"2021-06-26T14:37:58.864647Z","shell.execute_reply":"2021-06-26T14:37:58.898338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for raw_record in train_dataset.take(1):\n    example = tf.train.Example()\n    example.ParseFromString(raw_record.numpy())\n    print(example)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:37:58.900882Z","iopub.execute_input":"2021-06-26T14:37:58.901281Z","iopub.status.idle":"2021-06-26T14:37:59.662636Z","shell.execute_reply.started":"2021-06-26T14:37:58.901242Z","shell.execute_reply":"2021-06-26T14:37:59.661666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a description of the features.\nfeature_description = {\n    \"class\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    \"id\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n    \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n}\n\ndef parse_labeled_data(example_proto):\n    # Parse the input `tf.train.Example` proto using the dictionary above.\n    parsed = tf.io.parse_single_example(example_proto, feature_description)\n    image = tf.image.decode_jpeg(parsed[\"image\"], channels=3)  # image format uint8 [0,255]\n    image = tf.reshape(image, [192, 192, 3]) # explicit size needed for TPU\n    return image, parsed[\"class\" ]\n\ndef parse_unlabeled_data(example_proto):\n    # Parse the input `tf.train.Example` proto using the dictionary above.\n    parsed = tf.io.parse_single_example(example_proto, feature_description)\n    image = tf.image.decode_jpeg(parsed[\"image\"], channels=3)  # image format uint8 [0,255]\n    image = tf.reshape(image, [192, 192, 3]) # explicit size needed for TPU\n    return image, parsed[\"id\"]\n\ntrain_dataset_2 = train_dataset.map(parse_labeled_data).batch(128).shuffle(128*100)\nval_dataset_2 = val_dataset.map(parse_labeled_data).batch(128).shuffle(128*100)\ntest_dataset_2 = test_dataset.map(parse_unlabeled_data).batch(128)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:37:59.66463Z","iopub.execute_input":"2021-06-26T14:37:59.664959Z","iopub.status.idle":"2021-06-26T14:37:59.851327Z","shell.execute_reply.started":"2021-06-26T14:37:59.664928Z","shell.execute_reply":"2021-06-26T14:37:59.850401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    # pretrained model\n    \n    base_model = tf.keras.applications.MobileNetV2(include_top=False, weights='imagenet')\n    \n    base_model.trainable = False\n\n    model = tf.keras.Sequential([\n        tf.keras.layers.experimental.preprocessing.Rescaling(1./255, input_shape=(192, 192, 3)),\n        base_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(256, activation='relu'),\n        tf.keras.layers.Dense(104, activation=\"softmax\")\n    ])\n    model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel.fit(train_dataset_2, epochs=10, validation_data=val_dataset_2)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:38:47.114325Z","iopub.execute_input":"2021-06-26T14:38:47.114683Z","iopub.status.idle":"2021-06-26T14:41:49.804401Z","shell.execute_reply.started":"2021-06-26T14:38:47.114654Z","shell.execute_reply":"2021-06-26T14:41:49.803674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images_ds = test_dataset_2.map(lambda image, idnum: image)\npredictions = np.argmax(model.predict(test_images_ds), axis=-1)\nprint(predictions)\n\nprint('Generating submission.csv file...')\ntest_ids_ds = test_dataset_2.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(np.size(predictions)))).numpy().astype('U') # all in one batch\ndata = {\n    \"id\": test_ids,\n    \"label\": predictions\n}\npd.DataFrame(data).to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-26T14:41:55.892508Z","iopub.execute_input":"2021-06-26T14:41:55.892888Z","iopub.status.idle":"2021-06-26T14:42:20.174393Z","shell.execute_reply.started":"2021-06-26T14:41:55.892853Z","shell.execute_reply":"2021-06-26T14:42:20.173601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}