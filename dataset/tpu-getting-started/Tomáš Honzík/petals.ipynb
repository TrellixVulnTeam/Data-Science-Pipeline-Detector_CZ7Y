{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.applications import DenseNet169\nfrom tensorflow.keras.applications import DenseNet201\nfrom tensorflow.keras.applications import ResNet101\nfrom tensorflow.keras.applications import ResNet152\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPool2D\nimport math\nimport re\nimport numpy as np\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nimport tensorflow.keras.backend as K\n\nprint(\"Tensorflow version \" + tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\nfrom kaggle_datasets import KaggleDatasets\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')\nprint(GCS_DS_PATH) # what do gcs paths look like?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\n# Detect TPU, return appropriate distribution strategy\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\n# Define the batch size. This will be 16 with TPU off and 128 (=16*8) with TPU on\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\nfrom kaggle_datasets import KaggleDatasets\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('tpu-getting-started')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\nIMAGE_SIZE = [224, 224]\nGCS_PATH = GCS_DS_PATH + '/tfrecords-jpeg-' + str(IMAGE_SIZE[0]) + 'x' + str(IMAGE_SIZE[1])\nAUTO = tf.data.experimental.AUTOTUNE\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train/*.tfrec')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/val/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test/*.tfrec') \n\nCLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport random\nimport tensorflow.keras.backend as K\n\n\ndeg_180 = tf.constant([180], dtype='float32')\n_1 = tf.constant([1], dtype='float32')\n_0 = tf.constant([0], dtype='float32')\nmat_shape = (3, 3)\n\n\ndef get_augment_matrix(rot=0.0, shift_x=0.0, shift_y=0.0, zoom_x=0.0, zoom_y=0.0):\n    \"\"\"\"\"\"\n    # Degrees to rad\n    rot = np.pi * (rot / deg_180)\n\n    # Rotation matrix\n    sin = tf.math.sin(rot)\n    cos = tf.math.cos(rot)\n    rot_mat = tf.concat([cos, sin, _0,\n                         -sin, cos, _0,\n                         _0, _0, _1], axis=0)\n    rot_mat = tf.reshape(rot_mat, mat_shape)\n\n    # Shift matrix\n    shift_mat = tf.concat([_1, _0, shift_y,\n                           _0, _1, shift_x,\n                           _0, _0, _1], axis=0)\n    shift_mat = tf.reshape(shift_mat, mat_shape)\n\n    # Zoom matrix\n    zoom_mat = tf.concat([_1 / (_1 + zoom_y), _0, _0,\n                          _0, _1 / (_1 + zoom_x), _0,\n                          _0, _0, _0], axis=0)\n    zoom_mat = tf.reshape(zoom_mat, mat_shape)\n\n    # Combine matrices by simple multiplication\n    return tf.matmul(tf.matmul(rot_mat, shift_mat), zoom_mat)\n\n\ndef augment_data(image, label):\n    \"\"\"\"\"\"\n    # Generate transformation parameters\n    rot = tf.random.uniform(shape=(1,), minval=-45, maxval=45, dtype='float32')\n    shift_x = tf.random.uniform(shape=(1,), minval=-50, maxval=50, dtype='float32')\n    shift_y = tf.random.uniform(shape=(1,), minval=-50, maxval=50, dtype='float32')\n    zoom = tf.random.normal([1], 0, 0.15, dtype='float32')\n\n    augment_mat = get_augment_matrix(rot, shift_x, shift_y, zoom, zoom)\n\n    # Image pixels coordinates\n    # - x and y vectors: so their stack makes cartesian product of pixel coordinates (all possible coordinate pair)\n    # - intercept vector: serves as supportive vector for shift operation (allows the operation in absence of addition)\n    x = tf.repeat(tf.range(IMAGE_SIZE[0] // 2, -IMAGE_SIZE[0] // 2, -1), IMAGE_SIZE[0])\n    y = tf.tile(tf.range(-IMAGE_SIZE[1] // 2, IMAGE_SIZE[1] // 2), [IMAGE_SIZE[1]])\n    intercept = tf.ones([IMAGE_SIZE[0] * IMAGE_SIZE[1]], dtype='int32')\n    coordinates = tf.stack([x, y, intercept])\n\n    # Find new pixel coordinates\n    # 1. Multiply coordinates and augmentation matrix\n    # 2. Remove coordinates exceeding the space of the image\n    # 3. Get rid of supportive vector and update the reference frame\n    coordinates = K.cast(K.dot(augment_mat, K.cast(coordinates, dtype='float32')), dtype='int32')\n    coordinates = K.clip(coordinates, -IMAGE_SIZE[0] // 2 + 1, IMAGE_SIZE[1] // 2)\n    coordinates = tf.stack([IMAGE_SIZE[0] // 2 - coordinates[0,], IMAGE_SIZE[1] // 2 - 1 + coordinates[1,]])\n\n    # Map the image to the new reference frame\n    image = tf.gather_nd(image, tf.transpose(coordinates))\n    image = tf.reshape(image, [IMAGE_SIZE[0], IMAGE_SIZE[1], 3])\n\n    return image, label\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\nimport math\nimport re\n\nimport numpy as np\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\n\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])  # explicit size needed for TPU\n    return image\n\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label  # returns a dataset of (image, label) pairs\n\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum  # returns a dataset of image(s)\n\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False  # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames,\n                                      num_parallel_reads=AUTO)  # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(\n        ignore_order)  # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    \n    # Augment the data\n    dataset = dataset.map(augment_data, num_parallel_calls=AUTO)\n    \n    dataset = dataset.repeat()  # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048, reshuffle_each_iteration=True)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)  # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec\n    # files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\n\ndef batch_to_numpy_images_and_labels(data):\n    images, labels = data\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n    if numpy_labels.dtype == object:  # binary string in this case,\n        # these are image ID strings\n        numpy_labels = [None for _ in enumerate(numpy_images)]\n    # If no labels, only image IDs, return None for labels (this is\n    # the case for test data)\n    return numpy_images, numpy_labels\n\n\ndef title_from_label_and_target(label, correct_label):\n    if correct_label is None:\n        return CLASSES[label], True\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\n\ndef display_one_flower(image, title, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    if len(title) > 0:\n        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize / 1.2), color='red' if red else 'black',\n                  fontdict={'verticalalignment': 'center'}, pad=int(titlesize / 1.5))\n    return (subplot[0], subplot[1], subplot[2] + 1)\n\n\ndef display_batch_of_images(databatch, predictions=None):\n    \"\"\"This will work with:\n    display_batch_of_images(images)\n    display_batch_of_images(images, predictions)\n    display_batch_of_images((images, labels))\n    display_batch_of_images((images, labels), predictions)\n    \"\"\"\n    # data\n    images, labels = batch_to_numpy_images_and_labels(databatch)\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n\n    # auto-squaring: this will drop data that does not fit into square\n    # or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images) // rows\n\n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot = (rows, cols, 1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE, FIGSIZE / cols * rows))\n    else:\n        plt.figure(figsize=(FIGSIZE / rows * cols, FIGSIZE))\n\n    # display\n    for i, (image, label) in enumerate(zip(images[:rows * cols], labels[:rows * cols])):\n        title = '' if label is None else CLASSES[label]\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE * SPACING / max(rows,\n                                                    cols) * 40 + 3  # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n\n    # layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n\n\ndef display_training_curves(training, validation, title, subplot):\n    if subplot % 10 == 1:  # set up the subplots on the first call\n        plt.subplots(figsize=(10, 10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model ' + title)\n    ax.set_ylabel(title)\n    # ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])\n    plt.grid()\n    \ndef display_set_of_images(images, labels, predictions=None):\n    # auto-squaring: this will drop data that does not fit into square\n    # or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images) // rows\n\n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.1\n    subplot = (rows, cols, 1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE, FIGSIZE / cols * rows))\n    else:\n        plt.figure(figsize=(FIGSIZE / rows * cols, FIGSIZE))\n\n    # display\n    for i, (image, label) in enumerate(zip(images[:rows * cols], labels[:rows * cols])):\n        title = '' if label is None else CLASSES[label]\n        correct = True\n        if predictions is not None:\n            title, correct = title_from_label_and_target(predictions[i], label)\n        dynamic_titlesize = FIGSIZE * SPACING / max(rows,\n                                                    cols) * 40 + 3  # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n\n    # layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0, hspace=0)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\n# Load datasets\nds_train = get_training_dataset()\nds_valid = get_validation_dataset(True)\nds_test = get_test_dataset(True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\n# Show examples of augmented images\nds_iter = iter(ds_train.unbatch().batch(20))\none_batch = next(ds_iter)\ndisplay_batch_of_images(one_batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# MODELS\n\ndef pretrainded_model(type: str, trainable=False):\n    with strategy.scope():\n        if type == 'VGG16':\n            backbone = VGG16(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n        elif type == 'VGG19':\n            backbone = VGG19(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n        elif type == 'DenseNet121':\n            backbone = DenseNet121(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n        elif type == 'DenseNet169':\n            backbone = DenseNet169(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n        elif type == 'DenseNet201':\n            backbone = DenseNet201(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n        elif type == 'ResNet101':\n            backbone = ResNet101(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n        elif type == 'ResNet152':\n            backbone = ResNet152(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n        elif type == 'ResNet50':\n            backbone = ResNet50(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n\n        backbone.trainable = trainable\n\n        model = Sequential([\n            backbone,\n            Flatten(),\n            Dense(512, activation='relu'),\n            Dropout(0.2),\n            Dense(512, activation='relu'),\n            Dropout(0.2),\n            Dense(512, activation='relu'),\n            Dropout(0.2),\n            Dense(512, activation='relu'),\n            Dropout(0.2),\n            tf.keras.layers.Dense(len(CLASSES), activation='softmax', use_bias=False)\n        ])\n\n    return model\n\n\ndef vgg16(trainable=False):\n    return pretrainded_model('VGG16', trainable)\n\n\ndef vgg19(trainable=False):\n    return pretrainded_model('VGG19', trainable)\n\n\ndef densenet121(trainable=False):\n    return pretrainded_model('DenseNet121', trainable)\n\n\ndef densenet169(trainable=False):\n    return pretrainded_model('DenseNet169', trainable)\n\n\ndef densenet201(trainable=False):\n    return pretrainded_model('DenseNet201', trainable)\n\n\ndef resNet101(trainable=False):\n    return pretrainded_model('ResNet101', trainable)\n\n\ndef resNet152(trainable=False):\n    return pretrainded_model('ResNet152', trainable)\n\n\ndef resNet50(trainable=False):\n    return pretrainded_model('ResNet50', trainable)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = densenet169()\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['sparse_categorical_accuracy']\n)\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\n# Define training epochs\nEPOCHS = 200\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n\n\nearly_stopping = EarlyStopping(monitor='val_loss',\n                               min_delta=0.0,\n                               patience=10,\n                               restore_best_weights=False)\n\n\nhistory = model.fit(\n        ds_train,\n        validation_data=ds_valid,\n        epochs=EPOCHS,\n        steps_per_epoch=STEPS_PER_EPOCH,\n        callbacks=[early_stopping],\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\ndisplay_training_curves(\n        history.history['loss'],\n        history.history['val_loss'],\n        'loss',\n        211,\n    )\ndisplay_training_curves(\n    history.history['sparse_categorical_accuracy'],\n    history.history['val_sparse_categorical_accuracy'],\n    'accuracy',\n    212,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Statistics\n\nreal_values = np.zeros((NUM_VALIDATION_IMAGES,))\nnp_images = np.zeros((NUM_VALIDATION_IMAGES,IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n\nbatch_size = 32\n\nds_iter = iter(ds_valid.unbatch().batch(batch_size))\n\nfor i in range(NUM_VALIDATION_IMAGES // batch_size):\n    one_batch = next(ds_iter)\n    images, lbl = one_batch\n    lbl = lbl.numpy()\n    images =images.numpy()\n    \n    real_values[i*batch_size:(i+1)*batch_size] = lbl\n    np_images[i*batch_size:(i+1)*batch_size, :, :, :] = images\n    \n\nest_values = np.argmax(model.predict(ds_valid), axis=-1)\n\nconf_mat = tf.math.confusion_matrix(est_values, real_values)\n\nplt.matshow(conf_mat)\n\n\nest_values = tf.cast(tf.one_hot(est_values, len(CLASSES)), 'int32')\nreal_values = tf.cast(tf.one_hot(real_values, len(CLASSES)), 'int32')\n\nerr = real_values - est_values\nerr = err.numpy()\nerr = np.argmax(err, axis=-1)\nerr =  [e for e in err if e > 0]\n\nreal_values = np.argmax(real_values, axis=-1)\n\nerr_count = np.zeros((len(CLASSES),))\nclass_count = np.zeros((len(CLASSES),))\n\nfor e in err:\n    err_count[e] += 1\n    \nfor r in real_values:\n    class_count[r] += 1\n\n\n\nrelative_err =  (err_count / class_count)\n\nleaderboard = np.argsort(-relative_err)\n\n\nworst_names = np.array(CLASSES)[leaderboard[:16]]\nwors_accuraci =  1.0 - relative_err[leaderboard[:16]]\n\nprint(relative_err)\nprint(worst_names)\n\nto_found = 16\nwrong_examples = []\nleaderboard = list(leaderboard[:16])\n\nfor i in range(NUM_VALIDATION_IMAGES):\n    if to_found < 1:\n        break\n    if real_values[i] in leaderboard:\n        wrong_examples.append(i)\n        to_found -= 1\n        leaderboard.remove(real_values[i])\n        \n\nwrong_images = np_images[wrong_examples, :, :, :]\nwrong_labels = real_values[wrong_examples]\n\ndisplay_set_of_images(wrong_images, wrong_labels)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np_images = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\nprint('Computing predictions...')\ntest_images_ds = ds_test.map(lambda image, idnum: image)\nprobabilities = model.predict(test_images_ds)\npredictions = np.argmax(probabilities, axis=-1)\nprint(predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Copied from https://www.kaggle.com/ryanholbrook/create-your-first-submission\n\nprint('Generating submission.csv file...')\n\n# Get image ids from test set and convert to unicode\ntest_ids_ds = ds_test.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n\n# Write the submission file\nnp.savetxt(\n    'submission.csv',\n    np.rec.fromarrays([test_ids, predictions]),\n    fmt=['%s', '%d'],\n    delimiter=',',\n    header='id,label',\n    comments='',\n)\n\n# Look at the first few predictions\n!head submission.csv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}