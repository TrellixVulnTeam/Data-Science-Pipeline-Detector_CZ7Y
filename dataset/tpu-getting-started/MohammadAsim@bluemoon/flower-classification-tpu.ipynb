{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom matplotlib import image\nfrom matplotlib import pyplot\nimport os\nimport cv2\nimport random\nimport concurrent.futures\nimport time\nimport sklearn\nprint(tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport tensorflow as tf\nBATCH_SIZE = 1024\nSHUFFLE_BUFFER = 2000\nNUM_CLASSES = 100\nIMAGE_SIZE = 224\ndef _parse_function(proto):\n    # define your tfrecord again. Remember that you saved your image as a string.\n    keys_to_features = {'image': tf.io.FixedLenFeature([], tf.string),\n                        \"class\": tf.io.FixedLenFeature([], tf.int64)}\n    # Load one example\n    parsed_features = tf.io.parse_single_example(proto, keys_to_features)\n    # Turn your saved image string into an array\n    image = decode_image(parsed_features['image'])\n    label = tf.cast(parsed_features['class'], tf.int32)\n    return image, label\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [IMAGE_SIZE, IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\ndef create_dataset(filepath):    \n    # This works with arrays as well\n    dataset = tf.data.TFRecordDataset(filepath)\n    # Maps the parser on every filepath in the array. You can set the number of parallel loaders here\n    dataset = dataset.map(_parse_function, num_parallel_calls=4)\n    # Set the number of datapoints you want to load and shuffle \n    dataset = dataset.shuffle(SHUFFLE_BUFFER)\n    # Set the batchsize\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n    # Create an iterator\n    iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n\n    # Create your tf representation of the iterator\n    image, label = iterator.get_next()\n    # Bring your picture back in shape\n    # Create a one hot array for your labels\n    label = tf.one_hot(label, NUM_CLASSES)\n    return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport tensorflow as tf\nfrom tensorflow.python import keras as keras\nfrom tensorflow.keras.applications import MobileNetV2\n\nPATH = '../input/tpu-getting-started/tfrecords-jpeg-224x224/train'\nfn = os.listdir(PATH)\nfilepath = [os.path.join(PATH, ele) for ele in fn]\n#Get your datatensors\ntrain_image, train_label = create_dataset(filepath)\n\nPATH = '../input/tpu-getting-started/tfrecords-jpeg-224x224/val'\nfn = os.listdir(PATH)\nfilepath = [os.path.join(PATH, ele) for ele in fn]\n#Get your datatensors\nval_image, val_label = create_dataset(filepath)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\n# This is the TPU initialization code that has to be at the beginning.\ntf.tpu.experimental.initialize_tpu_system(resolver)\nprint(\"All devices: \", tf.config.list_logical_devices('TPU'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 100\nrandom_rotation = tf.keras.layers.experimental.preprocessing.RandomRotation(3.142/2, seed=SEED)\nrandom_flip = tf.keras.layers.experimental.preprocessing.RandomFlip(mode=\"horizontal_and_vertical\", seed=SEED)\nrandom_zoom = tf.keras.layers.experimental.preprocessing.RandomZoom((0, 0.25), seed=SEED)\nrandom_translate = tf.keras.layers.experimental.preprocessing.RandomTranslation((-0, 0.25), (-0, 0.25), seed=SEED)\nwith tf.device('/cpu:0'):\n    train_dataset = tf.data.Dataset.from_tensor_slices((train_image, train_label)).batch(BATCH_SIZE)\n    train_dataset = train_dataset.prefetch(buffer_size = tf.data.AUTOTUNE).shuffle(100)\n    del train_image, train_label\n    val_dataset = tf.data.Dataset.from_tensor_slices((val_image, val_label)).batch(32)\n    val_dataset = val_dataset.prefetch(buffer_size = tf.data.AUTOTUNE).shuffle(100)\n    del val_image, val_label\ndef normalize(imgs, label):\n    imgs = random_rotation.call(imgs)\n    imgs = random_flip.call(imgs)\n    imgs = random_zoom.call(imgs)\n    imgs = random_translate.call(imgs)\n    return tf.cast(imgs, tf.float16)/255, label\ntrain_dataset = train_dataset.map(normalize, num_parallel_calls=4)\nval_dataset = val_dataset.map(normalize, num_parallel_calls=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strategy = tf.distribute.experimental.TPUStrategy(resolver)\nwith strategy.scope():\n    base_model = tf.keras.applications.MobileNetV2(include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\\\n                                                   weights='imagenet', pooling = 'max', alpha=1.3)\n    model = tf.keras.Sequential([\n        base_model,\n        tf.keras.layers.Dense(100, activation='softmax')\n    ])\n    optimizer = tf.keras.optimizers.Adam(0.001)\n    epoch_auc = tf.keras.metrics.AUC(num_thresholds=200)\nloss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\ntrain_loss_history = []\nval_loss_history = []\ndist_train_dataset = strategy.experimental_distribute_dataset(train_dataset)\ndist_val_dataset = strategy.experimental_distribute_dataset(val_dataset)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ndef compute_loss(labels, predictions):\n    per_example_loss = loss_object(labels, predictions)\n    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZE)\ndef compute_acc(labels, predictions):\n    return accuracy_score(labels, predictions)\ndef train_step(inputs):\n    images, labels = inputs\n    with tf.GradientTape() as tape:\n        logits = model(images, training=True)\n        loss_value = compute_loss(labels, logits)\n        epoch_auc.update_state(labels, logits)\n        auc = epoch_auc.result()\n    train_loss_history.append(loss_value)\n    grads = tape.gradient(loss_value, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    return loss_value, auc\n@tf.function\ndef distributed_train_step(dist_inputs):\n    per_replica_losses, per_replica_auc = strategy.run(train_step, args=(dist_inputs,))\n    loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n                         axis=None)\n    auc = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_auc,\n                         axis=None)\n    return loss, auc\ndef val_step(inputs):\n    images, labels = inputs\n    with tf.GradientTape() as tape:\n        logits = model(images, training=False)\n        loss_value = compute_loss(labels, logits)\n        epoch_auc.update_state(labels, logits)\n        auc = epoch_auc.result()\n    val_loss_history.append(loss_value)\n    return loss_value, auc\n@tf.function\ndef distributed_val_step(dist_inputs):\n    per_replica_losses, per_replica_auc = strategy.run(val_step, args=(dist_inputs,))\n    loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n                         axis=None)\n    auc = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_auc,\n                         axis=None)\n    return loss, auc\ndef train(epochs, verbose=1):\n    for epoch in range(epochs):\n        start = time.time()\n        i = 0\n        print ('\\nEpoch {}/{} '.format(epoch+1, epochs))\n        for data in dist_train_dataset:\n            loss, auc = distributed_train_step(data)\n            \n            percent = float(i+1) * 100 / len(train_dataset)\n            arrow   = '-' * int(percent/100 * 20 - 1) + '>'\n            spaces  = ' ' * (20 - len(arrow))\n            if(verbose):    \n                print('\\rTraining: [%s%s] %d %% - Training Loss: %f - Training AUC: %f'% (arrow, spaces, percent, loss, auc), end='', flush=True)\n            i += 1\n        if(not verbose):\n            print(' Epoch Loss: ', loss.numpy())\n        i = 0\n        if(verbose):\n            print(\" -\", int(time.time()-start), \"s\", end=\"\")\n            print()\n        start = time.time()\n        for data in dist_val_dataset:\n            loss, auc = distributed_val_step(data)\n            percent = float(i+1) * 100 / len(val_dataset)\n            arrow   = '-' * int(percent/100 * 20 - 1) + '>'\n            spaces  = ' ' * (20 - len(arrow))\n            if(verbose):    \n                print('\\rValidate: [%s%s] %d %% - Validation Loss: %f - Validation AUC: %f'% (arrow, spaces, percent, loss, auc), end='', flush=True)\n            i += 1\n        if(verbose):\n            print(\" -\", int(time.time()-start), \"s\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(50, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('./mobilenetv2.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}