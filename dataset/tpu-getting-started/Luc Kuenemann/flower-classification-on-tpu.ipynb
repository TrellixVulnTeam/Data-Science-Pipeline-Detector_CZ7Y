{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Initialization\n## Dependencies","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import re, math\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_datasets import KaggleDatasets\n\nprint(\"Tensorflow version \" + tf.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Detect TPU","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get data path","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"GCS_DS_PATH = KaggleDatasets().get_gcs_path() # you can list the bucket with \"!gsutil ls $GCS_DS_PATH\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Parameters","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Image and batching parameters\nIMAGE_SIZE = [512, 512, 3] # at this size, a GPU will run out of memory. Use the TPU\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nNUM_TRAINING_IMAGES = 12753\nNUM_TEST_IMAGES = 7382\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords-jpeg-%sx%s' % (IMAGE_SIZE[0], IMAGE_SIZE[1]) + '/train/*.tfrec') \nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords-jpeg-%sx%s' % (IMAGE_SIZE[0], IMAGE_SIZE[1]) + '/val/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '/tfrecords-jpeg-%sx%s' % (IMAGE_SIZE[0], IMAGE_SIZE[1]) + '/test/*.tfrec')\n\nCLASSES = [\n    'pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', \n    'wild geranium', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', \n    'globe thistle', 'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', \n    'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', \n    'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', \n    'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', \n    'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', \n    'carnation', 'garden phlox', 'love in the mist', 'cosmos',  'alpine sea holly', \n    'ruby-lipped cattleya', 'cape flower', 'great masterwort',  'siam tulip', \n    'lenten rose', 'barberton daisy', 'daffodil',  'sword lily', 'poinsettia', \n    'bolero deep blue',  'wallflower', 'marigold', 'buttercup', 'daisy', \n    'common dandelion', 'petunia', 'wild pansy', 'primula',  'sunflower', \n    'lilac hibiscus', 'bishop of llandaff', 'gaura',  'geranium', 'orange dahlia', \n    'pink-yellow dahlia', 'cautleya spicata',  'japanese anemone', 'black-eyed susan', \n    'silverbush', 'californian poppy',  'osteospermum', 'spring crocus', 'iris', \n    'windflower',  'tree poppy', 'gazania', 'azalea', 'water lily',  'rose', \n    'thorn apple', 'morning glory', 'passion flower',  'lotus', 'toad lily', \n    'anthurium', 'frangipani',  'clematis', 'hibiscus', 'columbine', 'desert-rose', \n    'tree mallow', 'magnolia', 'cyclamen ', 'watercress',  'canna lily', \n    'hippeastrum ', 'bee balm', 'pink quill',  'foxglove', 'bougainvillea', \n    'camellia', 'mallow',  'mexican petunia',  'bromelia', 'blanket flower', \n    'trumpet creeper',  'blackberry lily', 'common tulip', 'wild rose']\n\n# Training parameters\nWARMUP_EPOCHS = 20\nWARMUP_LEARNING_RATE = 3e-3\nTUNING_EPOCHS = 8\n#LR_TUNING = 3e-5\nLR_MIN = 1e-10\nLR_PEAK = 2e-4\nLR_RAMPUP_EPOCHS = 4\nLR_DECAY = 0.3\n\n# Random erasing parameters\nRE_PROBABILITY = 1\nRE_S_LOW = 0.1\nRE_S_HIGH = 0.6\nRE_RATIO = 0.3\n\n# Random cropping size\nCROP_MIN = tf.cast(IMAGE_SIZE[0]*0.6, dtype=tf.int32)\nCROP_MAX = tf.cast(IMAGE_SIZE[0]*0.95, dtype=tf.int32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loading\n\nThis data is loaded from Kaggle and automatically sharded to maximize parallelization.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_erasing(image, p=RE_PROBABILITY, sl=RE_S_LOW, sh=RE_S_HIGH, r1=RE_RATIO, r2=1./RE_RATIO):\n    \n    w = IMAGE_SIZE[0] # image width\n    h = IMAGE_SIZE[1] # image height\n    c = IMAGE_SIZE[2] # image channels\n    s = w*h # image area\n    \n    p1 = tf.random.uniform(shape=[], minval=0, maxval=1, dtype=tf.float32)\n    \n    erased_image = image\n    \n    # Chance of applying random erasing\n    if p1 <= p:\n        while tf.constant(True):\n            # Generate random rectangle\n            se = s*tf.random.uniform(shape=[], minval=sl, maxval=sh, dtype=tf.float32)\n            re = tf.random.uniform(shape=[], minval=r1, maxval=r2, dtype=tf.float32)\n            he = tf.cast((se*re)**0.5, tf.int32)\n            we = tf.cast((se/re)**0.5, tf.int32)\n            xe = tf.random.uniform(shape=[], minval=0, maxval=w, dtype=tf.int32)\n            ye = tf.random.uniform(shape=[], minval=0, maxval=h, dtype=tf.int32)\n            # If the rectangle fits\n            if (xe+we <= w) and (ye+he <= h):\n                # Generate blocking rectangle tensor of 0s\n                e = tf.zeros(shape=[we, he, c], dtype=tf.int32)\n                # Pad blocking rectangle on all 4 sides with 1s to get the full dimension tensor\n                e = tf.pad(e, [[xe,w-we-xe], [ye,h-he-ye], [0,0]], mode='CONSTANT', constant_values=1)\n                # Multiply padded erasure and image element-wise\n                erased_image = tf.math.multiply(tf.cast(image, dtype=tf.float32), tf.cast(e, dtype=tf.float32))\n                # Generate rectangle of white noise the same size as the blocking rectangle\n                r = tf.random.uniform(shape=[we, he, c], minval=0, maxval=1, dtype=tf.float32) # maxval is excluded of the range\n                # Pad the noisy rectangle on all 4 sides with 0s\n                r = tf.pad(r, [[xe,w-we-xe], [ye,h-he-ye], [0,0]], mode='CONSTANT', constant_values=0)\n                # Add the noisy rectangle and the occluded image\n                erased_image = erased_image + r\n                break\n                \n    return tf.cast(erased_image, image.dtype)\n\ndef augment_dataset(image, label):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = random_erasing(image)\n    crop_size = tf.random.uniform(shape=[], minval=CROP_MIN, maxval=CROP_MAX, dtype=tf.int32)\n    image = tf.image.random_crop(image, size=[crop_size, crop_size, IMAGE_SIZE[2]])\n    image = tf.image.resize(image, size=[IMAGE_SIZE[0], IMAGE_SIZE[1]])\n    return image, label","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\n\nfig, ax = plt.subplots()\n\nblank = tf.constant(1, shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], IMAGE_SIZE[2]), dtype=tf.float32)\nerasure = random_erasing(blank)\nax.imshow(np.asarray(erasure.numpy()))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, IMAGE_SIZE) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['class'], tf.int32)\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['id']\n    return image, idnum # returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.map(augment_dataset)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    #dataset = dataset.map(augment_dataset)\n    dataset = dataset.batch(BATCH_SIZE)\n    return dataset\n\ndef get_training_dataset_preview(ordered=True):\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    #dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\n\ntraining_dataset = get_training_dataset()\nvalidation_dataset = get_validation_dataset()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Training warm-up\nFirst, we import a pre-trained model from the Keras built-in models, and use transfer learning to train only our top layer.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():    \n    pretrained_model = tf.keras.applications.DenseNet201(\n        weights = 'imagenet',\n        include_top = False,\n        input_shape = IMAGE_SIZE)\n    pretrained_model.trainable = False # use transfer learning\n    \n    model = tf.keras.Sequential([\n        pretrained_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(104, activation = 'softmax')\n    ])\n\nwarmup_optimizer = tf.keras.optimizers.Adam(lr = WARMUP_LEARNING_RATE)\n\nmodel.compile(\n    optimizer = warmup_optimizer,\n    loss = 'sparse_categorical_crossentropy',\n    metrics = ['sparse_categorical_accuracy']\n)\nmodel.summary()\n\nwarmup_history = model.fit(\n    training_dataset,\n    steps_per_epoch = STEPS_PER_EPOCH,\n    epochs = WARMUP_EPOCHS,\n    validation_data = validation_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot accuracy during training\nplt.plot(warmup_history.history['sparse_categorical_accuracy'])\nplt.plot(warmup_history.history['val_sparse_categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# Plot loss during training\nplt.plot(warmup_history.history['loss'])\nplt.plot(warmup_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training fine-tuning\nNow that the top layer is train, we have a coherent model. We can unfreeze all previously frozen layers and try to fine-tune the training of the overall model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def scheduler(epoch):\n    lr = LR_MIN\n    if epoch <= LR_RAMPUP_EPOCHS:\n        lr = LR_MIN + epoch*(LR_PEAK-LR_MIN)/LR_RAMPUP_EPOCHS\n    else:\n        lr = LR_MIN + (LR_PEAK-LR_MIN)*LR_DECAY**(epoch - LR_RAMPUP_EPOCHS)\n    return lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in model.layers:\n    layer.trainable = True # Unfreeze layers\n\ntuning_optimizer = tf.keras.optimizers.Adam(lr = LR_PEAK)\n#tuning_optimizer = tf.keras.optimizers.Adam(lr = LR_TUNING)\n\nmodel.compile(\n    optimizer = tuning_optimizer,\n    loss = 'sparse_categorical_crossentropy',\n    metrics = ['sparse_categorical_accuracy']\n)\nmodel.summary()\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n\ntuning_history = model.fit(\n    training_dataset,\n    steps_per_epoch = STEPS_PER_EPOCH,\n    epochs = TUNING_EPOCHS,\n    validation_data = validation_dataset,\n    callbacks = lr_callback)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot accuracy during training\nplt.plot(tuning_history.history['sparse_categorical_accuracy'])\nplt.plot(tuning_history.history['val_sparse_categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# Plot loss during training\nplt.plot(tuning_history.history['loss'])\nplt.plot(tuning_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predictions on the test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_ds = get_test_dataset(ordered=True) # since we are splitting the dataset and iterating separately on images and ids, order matters.\n\nprint('Computing predictions...')\ntest_images_ds = test_ds.map(lambda image, idnum: image)\nprobabilities = model.predict(test_images_ds)\npredictions = np.argmax(probabilities, axis=-1)\nprint(predictions)\n\nprint('Generating submission.csv file...')\ntest_ids_ds = test_ds.map(lambda image, idnum: idnum).unbatch()\ntest_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\nnp.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}