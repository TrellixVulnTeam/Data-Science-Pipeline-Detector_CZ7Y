{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Analysis of Model flowerclass-efficientnetv2-2 2: with XAI LIME method\n\n### Goals\n\n* Apply LIME to explain decisions leading to model errors in `flowerclass-efficientnetv2-2-analysis2-imgvis` notebook","metadata":{}},{"cell_type":"code","source":"import math, re, os\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nprint(tf.__version__)\nprint(tfa.__version__)\n\nfrom flowerclass_read_tf_ds import get_datasets, display_batch_by_class, display_batch_of_images #, load_dataset, display_batch_of_images, batch_to_numpy_images_and_labels, display_one_flower\nimport tensorflow_hub as hub\nimport pandas as pd\nimport math\nimport plotly_express as px\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:48:39.713855Z","iopub.execute_input":"2022-03-23T01:48:39.71412Z","iopub.status.idle":"2022-03-23T01:48:39.722568Z","shell.execute_reply.started":"2022-03-23T01:48:39.714092Z","shell.execute_reply":"2022-03-23T01:48:39.721831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.test.gpu_device_name()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:48:39.724531Z","iopub.execute_input":"2022-03-23T01:48:39.725004Z","iopub.status.idle":"2022-03-23T01:48:39.741865Z","shell.execute_reply.started":"2022-03-23T01:48:39.72497Z","shell.execute_reply":"2022-03-23T01:48:39.741182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# I. Data prep, model Loading and Predictions with EfficientNetV2","metadata":{}},{"cell_type":"code","source":"image_size = 224\nbatch_size = 64","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:48:39.743128Z","iopub.execute_input":"2022-03-23T01:48:39.743548Z","iopub.status.idle":"2022-03-23T01:48:39.750641Z","shell.execute_reply.started":"2022-03-23T01:48:39.743511Z","shell.execute_reply":"2022-03-23T01:48:39.74984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102\nlen(class_names)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:48:39.752059Z","iopub.execute_input":"2022-03-23T01:48:39.75233Z","iopub.status.idle":"2022-03-23T01:48:39.765156Z","shell.execute_reply.started":"2022-03-23T01:48:39.752296Z","shell.execute_reply":"2022-03-23T01:48:39.764366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"effnet2_base = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_s/feature_vector/2\"","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:48:39.767627Z","iopub.execute_input":"2022-03-23T01:48:39.767907Z","iopub.status.idle":"2022-03-23T01:48:39.776Z","shell.execute_reply.started":"2022-03-23T01:48:39.767872Z","shell.execute_reply":"2022-03-23T01:48:39.775219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    effnet2_tfhub = tf.keras.Sequential([\n    # Explicitly define the input shape so the model can be properly\n    # loaded by the TFLiteConverter\n    tf.keras.layers.InputLayer(input_shape=(image_size, image_size,3)),\n    hub.KerasLayer(effnet2_base, trainable=False),\n    tf.keras.layers.Dropout(rate=0.2),\n    tf.keras.layers.Dense(104, activation='softmax')\n])\neffnet2_tfhub.build((None, image_size, image_size,3,)) #This is to be used for subclassed models, which do not know at instantiation time what their inputs look like.\n\n\neffnet2_tfhub.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:48:39.77723Z","iopub.execute_input":"2022-03-23T01:48:39.777488Z","iopub.status.idle":"2022-03-23T01:48:50.19454Z","shell.execute_reply.started":"2022-03-23T01:48:39.777455Z","shell.execute_reply":"2022-03-23T01:48:50.193864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_phase = 12\neffnet2_tfhub.load_weights(\"../input/flowerclass-efficientnetv2-2/training/\"+\"cp-\"+f\"{best_phase}\".rjust(4, '0')+\".ckpt\")","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:48:50.195778Z","iopub.execute_input":"2022-03-23T01:48:50.196276Z","iopub.status.idle":"2022-03-23T01:48:50.969716Z","shell.execute_reply.started":"2022-03-23T01:48:50.196237Z","shell.execute_reply":"2022-03-23T01:48:50.968997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:48:50.970922Z","iopub.execute_input":"2022-03-23T01:48:50.971346Z","iopub.status.idle":"2022-03-23T01:48:50.97625Z","shell.execute_reply.started":"2022-03-23T01:48:50.971298Z","shell.execute_reply":"2022-03-23T01:48:50.975164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ensure that validation data loader returns fixed order of elements.","metadata":{}},{"cell_type":"code","source":"ds_train, ds_valid, ds_test = get_datasets(BATCH_SIZE=batch_size, IMAGE_SIZE=(image_size, image_size), \n                                           RESIZE=None, tpu=False, with_id=True)\n\nimg_preds = []\nimg_labels = []\nimg_ids = []\nfor imgs, label, imgs_id in tqdm(ds_valid):\n    img_preds.append(effnet2_tfhub.predict(imgs, batch_size=batch_size))\n    img_labels.append(label.numpy())\n    img_ids.append(imgs_id.numpy())\n    \nimg_preds = np.concatenate([img_pred.argmax(1) for img_pred in img_preds])\nimg_labels = np.concatenate([img_label.argmax(1) for img_label in img_labels])\nimg_ids = np.concatenate([img_id for img_id in img_ids])\n","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:48:50.977887Z","iopub.execute_input":"2022-03-23T01:48:50.978134Z","iopub.status.idle":"2022-03-23T01:49:32.051612Z","shell.execute_reply.started":"2022-03-23T01:48:50.978101Z","shell.execute_reply":"2022-03-23T01:49:32.050921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_results = pd.DataFrame({'pred': img_preds, \"label\":img_labels, \"id\": img_ids})\nval_results['id'] = val_results['id'].apply(lambda txt: txt.decode())","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:49:32.053235Z","iopub.execute_input":"2022-03-23T01:49:32.053739Z","iopub.status.idle":"2022-03-23T01:49:32.06746Z","shell.execute_reply.started":"2022-03-23T01:49:32.053701Z","shell.execute_reply":"2022-03-23T01:49:32.066664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_results.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:49:32.068417Z","iopub.execute_input":"2022-03-23T01:49:32.0686Z","iopub.status.idle":"2022-03-23T01:49:32.084947Z","shell.execute_reply.started":"2022-03-23T01:49:32.068577Z","shell.execute_reply":"2022-03-23T01:49:32.084307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# II. Explaining model decisions","metadata":{}},{"cell_type":"code","source":"def get_images_by_ids(image_ids_search):\n    ds_train, ds_valid, ds_test = get_datasets(BATCH_SIZE=batch_size, IMAGE_SIZE=(image_size, image_size), \n                                               RESIZE=None, tpu=False, with_id=True)\n    \n    imgs_found = []\n    imgage_ids_found = []\n    labels_found = []\n    for imgs, labels, imgs_id in tqdm(ds_valid):\n        for img, img_id, label in zip(imgs, imgs_id, labels) :\n            if img_id in image_ids_search:\n                imgage_ids_found.append(img_id)\n                imgs_found.append(img)\n                labels_found.append(tf.argmax(label))\n                \n    return (tf.stack(imgs_found, 0), tf.cast(tf.concat(labels_found, 0), tf.int64)), imgage_ids_found","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:49:32.086105Z","iopub.execute_input":"2022-03-23T01:49:32.086434Z","iopub.status.idle":"2022-03-23T01:49:32.095739Z","shell.execute_reply.started":"2022-03-23T01:49:32.086399Z","shell.execute_reply":"2022-03-23T01:49:32.095072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lime\nfrom lime import lime_image\nfrom skimage.segmentation import mark_boundaries","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:49:32.098647Z","iopub.execute_input":"2022-03-23T01:49:32.098854Z","iopub.status.idle":"2022-03-23T01:49:32.459531Z","shell.execute_reply.started":"2022-03-23T01:49:32.098824Z","shell.execute_reply":"2022-03-23T01:49:32.458854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IIa). globe-flower predictions\n\nHere I dive deeper to understand a prediction for the globe-flower class analyzed in `flowerclass_efficientnetv2_2_analysis2_imgvis.ipynb`.\n","metadata":{}},{"cell_type":"markdown","source":"## FP Image ed3a59a35\n\nThe image for analysis has the id ed3a59a35.","metadata":{}},{"cell_type":"code","source":"image_id_investigate = \"ed3a59a35\"","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:49:32.462819Z","iopub.execute_input":"2022-03-23T01:49:32.463132Z","iopub.status.idle":"2022-03-23T01:49:32.467559Z","shell.execute_reply.started":"2022-03-23T01:49:32.463099Z","shell.execute_reply":"2022-03-23T01:49:32.46614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_found,  imgage_ids_found= get_images_by_ids([image_id_investigate])","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:49:32.469136Z","iopub.execute_input":"2022-03-23T01:49:32.469489Z","iopub.status.idle":"2022-03-23T01:49:39.241345Z","shell.execute_reply.started":"2022-03-23T01:49:32.469454Z","shell.execute_reply":"2022-03-23T01:49:39.240021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(batch_found[0][0].numpy())","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:53:25.598645Z","iopub.execute_input":"2022-03-23T01:53:25.599341Z","iopub.status.idle":"2022-03-23T01:53:25.832358Z","shell.execute_reply.started":"2022-03-23T01:53:25.599301Z","shell.execute_reply":"2022-03-23T01:53:25.831553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create perturbed sample. Perturb them by sampling from a Normal(0,1).\n\nDefault parameters:\n* kernel_width: width of kernel used in similarity measure to weight the surrounding data points when used in the surrogate model.\n* kernel: kernel type/function used in similarity measure. I used the default exponential here. After similarities (with `distance_metric` parameter below) between perturbed samples are calculated, we apply the kernel function to get the weights for the surrogate model. \n* feature_selection: how to set the number of features in the surrogate model (when calling `explain_instances` below). due to our default choice of 'auto' together with high `num_features` below features are selected as follows: select superpixels/features that have the highest product of (absolute) feature  weight * explained imagea pixels. This weight for features is derived from a Ridge model (`Ridge(alpha=0.01)`) trained on all perturbed instances. This feature selection step happens before the actual surrogate model (`model_regressor`  parameter) below is fit based on the selected subset of features. Method flow: segmentation approach to create superpixels/features > feature selection of super pixels > fit of the surrogate model on selected feeatures","metadata":{}},{"cell_type":"code","source":"explainer = lime_image.LimeImageExplainer(kernel_width=0.25, kernel=None, feature_selection='auto', random_state=42) # all params are defaults ","metadata":{"execution":{"iopub.status.busy":"2022-03-23T01:49:39.252918Z","iopub.execute_input":"2022-03-23T01:49:39.253167Z","iopub.status.idle":"2022-03-23T01:49:39.258975Z","shell.execute_reply.started":"2022-03-23T01:49:39.253133Z","shell.execute_reply":"2022-03-23T01:49:39.25819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create superpixels, create perturbed sampes. For each class (in `top_labels`) perform feature selection, fit surrogate model and get model coefficients.\n\nParameters:\n* labels: iterable with labels to be explained. as we chose top_labels parameter it is not used.\n* top_labels: choose the top n classes (highest probability). *note that the top_labels=12 was chosen to include the ground truth class 'lotus'.*\n* num_features: use default 100000 \n* num_samples: we use the default of 1000 samples in the neighbourhood of the out image as training data to train our surrogate model\n* distance_metric: consine (default) distance metric for the similarities between perturbed samples and image to be explained. in practice calculates similarity between perturbations only.\n* model_regressor: uses by default Ridge model (`sklearn.linear_model.Ridget(alpha=1)`), as the surrogate model. the model is trained on perturbed samples with weights given through the kernel function\n* segmentation_fn: quickshift segmentation algorithm from image based on skimage to create superpixels ([ref](https://scikit-image.org/docs/dev/api/skimage.segmentation.html?highlight=quickshift#skimage.segmentation.quickshift)). Image to be explained will be perturbed by turning on/off each superpixel with 50% probability to create`num_samples` samples.","metadata":{}},{"cell_type":"code","source":"%%time\nexplanation = explainer.explain_instance(image=batch_found[0][0].numpy().astype('double'), \n                                         classifier_fn=effnet2_tfhub.predict, \n                                         top_labels=12, hide_color=0, num_samples=1000,\n                                        random_seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:13:51.627471Z","iopub.execute_input":"2022-03-23T02:13:51.627753Z","iopub.status.idle":"2022-03-23T02:14:03.281743Z","shell.execute_reply.started":"2022-03-23T02:13:51.627719Z","shell.execute_reply":"2022-03-23T02:14:03.280869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As seen in the flowerclass_efficientnetv2_2_analysis2_imgvis notebook, the top prediction is globe-flower. The true class, tulip, is not available.","metadata":{}},{"cell_type":"code","source":"explanation.top_labels","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:14:03.287141Z","iopub.execute_input":"2022-03-23T02:14:03.287824Z","iopub.status.idle":"2022-03-23T02:14:03.295373Z","shell.execute_reply.started":"2022-03-23T02:14:03.287763Z","shell.execute_reply":"2022-03-23T02:14:03.294311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"[class_names[cl] for cl in explanation.top_labels]","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:14:03.29717Z","iopub.execute_input":"2022-03-23T02:14:03.2975Z","iopub.status.idle":"2022-03-23T02:14:03.306687Z","shell.execute_reply.started":"2022-03-23T02:14:03.297459Z","shell.execute_reply":"2022-03-23T02:14:03.305845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get pre-computed feature importance for superpixel of class selected (`explanation.top_labels[0]`) and display top 5 most important super pixels.\n\nParameters:\n* num_features: number of superpixels to include in explanation\n* positive_only: only take superpixels positively contributing\n* hide_rest: mark the non-explaining part of the image","metadata":{}},{"cell_type":"code","source":"temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:14:03.309108Z","iopub.execute_input":"2022-03-23T02:14:03.309415Z","iopub.status.idle":"2022-03-23T02:14:03.560005Z","shell.execute_reply.started":"2022-03-23T02:14:03.309378Z","shell.execute_reply":"2022-03-23T02:14:03.559338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Note that repeated execution of `explainer.explain_instance` above (without further changes) has lead to slight variation of the\nsuperpixels selected, despite of random_seed fixed.","metadata":{}},{"cell_type":"code","source":"temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:15:25.339318Z","iopub.execute_input":"2022-03-23T02:15:25.339579Z","iopub.status.idle":"2022-03-23T02:15:25.575295Z","shell.execute_reply.started":"2022-03-23T02:15:25.339549Z","shell.execute_reply":"2022-03-23T02:15:25.57464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> model focus mostly on lower end of the flower petals. The irrelevant background of gras is excluded.","metadata":{}},{"cell_type":"markdown","source":"increase the amount of top important superpixels from 5 to 10 to includes\nnow irrelevant background but also parts of the flower stem:","metadata":{}},{"cell_type":"code","source":"temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=10, hide_rest=True)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:15:27.995888Z","iopub.execute_input":"2022-03-23T02:15:27.996419Z","iopub.status.idle":"2022-03-23T02:15:28.233758Z","shell.execute_reply.started":"2022-03-23T02:15:27.996379Z","shell.execute_reply":"2022-03-23T02:15:28.233054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:15:30.30443Z","iopub.execute_input":"2022-03-23T02:15:30.305178Z","iopub.status.idle":"2022-03-23T02:15:30.557034Z","shell.execute_reply.started":"2022-03-23T02:15:30.305135Z","shell.execute_reply":"2022-03-23T02:15:30.556294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_colormap_explain(image, explanation, top_class):\n    '''plot segments of superpixels colored by the feature weight of the surrogate model'''\n\n    ind =  explanation.top_labels[top_class]\n\n    dict_heatmap = dict(explanation.local_exp[ind])\n    heatmap = np.vectorize(dict_heatmap.get)(explanation.segments) \n\n    fig, axes = plt.subplots(1, 2, figsize=(10,4))\n\n    axes[0].imshow(image)\n\n    img = axes[1].imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(), vmax = heatmap.max())\n    _ = plt.colorbar(img, ax=axes[1])\n    \nplot_colormap_explain(image=batch_found[0][0].numpy(), explanation=explanation, top_class=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:15:34.579732Z","iopub.execute_input":"2022-03-23T02:15:34.581409Z","iopub.status.idle":"2022-03-23T02:15:35.930225Z","shell.execute_reply.started":"2022-03-23T02:15:34.581361Z","shell.execute_reply":"2022-03-23T02:15:35.929538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The colormap indicates that hte surrogate model identifies the lower part of the petals as the most import part\nof the image for the globe-flower.","metadata":{}},{"cell_type":"markdown","source":"In the following I evaluate which superpixels are indicative of the class lotus which is the ground truth of the\nimage under investigation.","metadata":{}},{"cell_type":"code","source":"index_lotus_truth = 77","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:15:40.045382Z","iopub.execute_input":"2022-03-23T02:15:40.045833Z","iopub.status.idle":"2022-03-23T02:15:40.049662Z","shell.execute_reply.started":"2022-03-23T02:15:40.045777Z","shell.execute_reply":"2022-03-23T02:15:40.048856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntemp, mask = explanation.get_image_and_mask(index_lotus_truth, positive_only=True, num_features=5, hide_rest=True)\nplt.imshow(mark_boundaries(temp / 2 + 0.5, mask))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:15:41.668388Z","iopub.execute_input":"2022-03-23T02:15:41.669146Z","iopub.status.idle":"2022-03-23T02:15:41.888061Z","shell.execute_reply.started":"2022-03-23T02:15:41.669107Z","shell.execute_reply":"2022-03-23T02:15:41.887348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_colormap_explain(image=batch_found[0][0].numpy(), explanation=explanation, top_class=10)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T02:17:11.014624Z","iopub.execute_input":"2022-03-23T02:17:11.015394Z","iopub.status.idle":"2022-03-23T02:17:11.471295Z","shell.execute_reply.started":"2022-03-23T02:17:11.015348Z","shell.execute_reply":"2022-03-23T02:17:11.470606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> * A different part of the flower is indicative of the true class lotus compared to the predicted globe-flower class. This can help\nto get insights into why the image was wrongly predicted as globe-flower while for the class lotus only a small part of the image was deemed releveant.\n> * Also, the weights in the right image are much lower, then for the predicted class above","metadata":{}}]}