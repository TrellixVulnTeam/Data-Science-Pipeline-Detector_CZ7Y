{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n\n<br><center><img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/34547/logos/header.png?t=2022-02-15-22-37-27\" width=100%></center>\n\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">ü´Åü´Ä EDA ‚Äì HuBMAP+HPA ‚Äì Organ Segmentation ü´Äü´Å</h2>\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: DARIEN SCHETTLER</h5>\n\n<br>\n\n---\n\n<br>\n\n<center><div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üõë &nbsp; WARNING:</b><br><br><b>THIS IS A WORK IN PROGRESS</b><br>\n</div></center>\n\n\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">üëè &nbsp; IF YOU FORK THIS OR FIND THIS HELPFUL &nbsp; üëè</b><br><br><b style=\"font-size: 22px; color: darkorange\">PLEASE UPVOTE!</b><br><br>This was a lot of work for me and while it may seem silly, it makes me feel appreciated when others like my work. üòÖ\n</div></center>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<p id=\"toc\"></p>\n\n<br><br>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #BF2C58; background-color: #ffffff;\">TABLE OF CONTENTS</h1>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#background_information\">1&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">2&nbsp;&nbsp;&nbsp;&nbsp;SETUP</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">3&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#create_dataset\">4&nbsp;&nbsp;&nbsp;&nbsp;DATASET EXPLORATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#modelling\">5&nbsp;&nbsp;&nbsp;&nbsp;MODELLING</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#appendix\">9&nbsp;&nbsp;&nbsp;&nbsp;APPENDIX</a></h3>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"imports\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: #BF2C58;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>","metadata":{}},{"cell_type":"code","source":"print(\"\\n... IMPORTS STARTING ...\\n\")\n\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf; print(f\"\\t\\t‚Äì TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_hub as tfhub; print(f\"\\t\\t‚Äì TENSORFLOW HUB VERSION: {tfhub.__version__}\");\nimport tensorflow_addons as tfa; print(f\"\\t\\t‚Äì TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport tensorflow_io as tfio; print(f\"\\t\\t‚Äì TENSORFLOW I/O VERSION: {tfio.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t‚Äì NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t‚Äì SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom pandarallel import pandarallel; pandarallel.initialize();\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom scipy.spatial import cKDTree\n\n# # RAPIDS\n# import cudf, cupy, cuml\n# from cuml.neighbors import NearestNeighbors\n# from cuml.manifold import TSNE, UMAP\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport hashlib\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport json\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image, ImageEnhance\nimport matplotlib; print(f\"\\t\\t‚Äì MATPLOTLIB VERSION: {matplotlib.__version__}\");\nfrom matplotlib import animation, rc; rc('animation', html='jshtml')\nimport plotly\nimport PIL\nimport cv2\n\nimport plotly.io as pio\nprint(pio.renderers)\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-28T16:16:13.543813Z","iopub.execute_input":"2022-06-28T16:16:13.545025Z","iopub.status.idle":"2022-06-28T16:16:25.527252Z","shell.execute_reply.started":"2022-06-28T16:16:13.544871Z","shell.execute_reply":"2022-06-28T16:16:25.526216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<a id=\"background_information\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #BF2C58; background-color: #ffffff;\" id=\"background_information\">1&nbsp;&nbsp;BACKGROUND INFORMATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">1.1 BASIC COMPETITION INFORMATION</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">PRIMARY TASK DESCRIPTION</b>\n\nIn this competition, you‚Äôll <mark><b>identify and segment functional tissue units</b></mark> **(FTUs)** across <mark><b>five</b></mark> human organs.\n* You'll build your model using a dataset of tissue section images, with the best submissions segmenting FTUs as accurately as possible.\n* The five human organs we are segmenting are:\n    * **Prostate**\n    * **Spleen**\n    * **Lung**\n    * **Kidney**\n    * **Large Intestine**\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">BASIC BACKGROUND INFORMATION</b>\n\nWhen you think of ‚Äúlife hacks,‚Äù normally you‚Äôd imagine productivity techniques. But how about the kind that helps you understand your body at a molecular level? It may be possible! Researchers must first determine the function and relationships among the 37 trillion cells that make up the human body. A better understanding of our cellular composition could help people live healthier, longer lives.\n\nA previous <b><a href=\"https://www.kaggle.com/c/hubmap-kidney-segmentation\">Kaggle</a></b> competition aimed to annotate cell population neighborhoods that perform an organ‚Äôs main physiologic function, also called functional tissue units (FTUs). Manually annotating FTUs (e.g., glomeruli in kidney or alveoli in the lung) is a time-consuming process. In the average kidney, there are over 1 million glomeruli FTUs. While there are existing cell and FTU segmentation methods, we want to push the boundaries by building algorithms that generalize across different organs and are robust across different dataset differences.\n\nThe <b><a href=\"https://hubmapconsortium.org/\">Human BioMolecular Atlas Program (HuBMAP)</a></b> is working to create a <b><a href=\"https://www.nature.com/articles/s41556-021-00788-6\">Human Reference Atlas</a></b> at the cellular level. Sponsored by the National Institutes of Health (NIH), HuBMAP and Indiana University‚Äôs Cyberinfrastructure for Network Science Center (CNS) have partnered with institutions across the globe for this endeavor. A major partner is the <a href=\"https://www.proteinatlas.org/\"><b>Human Protein Atlas (HPA)</b></a>, a Swedish research program aiming to map the protein expression in human cells, tissues, and organs, funded by the Knut and Alice Wallenberg Foundation.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">VISUAL EXPLANATION</b>\n\nTBD\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">COMPETITION IMPACT STATEMENT</b>\n\nIf successful, you'll help accelerate the world‚Äôs understanding of the relationships between cell and tissue organization. With a better idea of the relationship of cells, researchers will have more insight into the function of cells that impact human health. Further, the Human Reference Atlas constructed by HuBMAP will be freely available for use by researchers and pharmaceutical companies alike, potentially improving and prolonging human life.\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">COMPETITION HOST INFORMATION</b>\n\n<br><b>PRIMARY SPONSORS</b>\n\n<img src=\"https://storage.googleapis.com/kaggle-media/competitions/Hubmap/kaggle-teams.png\"><br>\n\n<br><b>SECONDARY SPONSORS</b>\n* Google\n* Genentech\n* Indiana University\n\n<br><br>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">1.2 COMPETITION EVALUATION</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL EVALUATION INFORMATION</b>\n\nThis competition is evaluated on the mean <b><a href=\"https://radiopaedia.org/articles/dice-similarity-coefficient#:~:text=The%20Dice%20similarity%20coefficient%2C%20also,between%20two%20sets%20of%20data.\">Dice Coefficient</a></b>. \n\nThe <b><a href=\"https://radiopaedia.org/articles/dice-similarity-coefficient#:~:text=The%20Dice%20similarity%20coefficient%2C%20also,between%20two%20sets%20of%20data.\">Dice Coefficient</a></b> can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n\n$$\n\\frac{2 * |X \\cap Y|}{|X| + |Y|}\n$$\n\n* Where $X$ is the predicted set of pixels and $Y$ is the ground truth. \n* The Dice coefficient is defined to be $1$ when both $X$ and $Y$ are empty. \n* The leaderboard score is the mean of the <b><a href=\"https://radiopaedia.org/articles/dice-similarity-coefficient#:~:text=The%20Dice%20similarity%20coefficient%2C%20also,between%20two%20sets%20of%20data.\">Dice Coefficients</a></b> for each image in the test set.\n\n<br>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">SUBMISSION FILE INFORMATION</b>\n\nIn order to reduce the submission file size, our metric uses **run-length encoding** on the pixel values.  \n* Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length\n* E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n* Note that, at the time of encoding, the mask should be **binary**\n    * The masks for all objects in an image are joined into a single large mask\n    * The value of **0** should indicate pixels that are not **masked**\n    * The value of **1** will indicate pixels that are **masked**.\n\nThe competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\n<mark><b>Note that, at the time of encoding, the mask should be binary, meaning the masks for all objects in an image are joined into a single large mask. A value of 0 should indicate pixels that are not masked, and a value of 1 will indicate pixels that are masked.</b></mark>\n\n<br>\n\nThe file should contain a header and have the following format:\n\n```html\nid,rle\n1,1 1 5 1\n2,1 1\n3,1 1\n4,1 5 2 17\netc.\n```\n\n<br><font color=\"red\"><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">IS THIS A CODE COMPETITION?</b></font>\n\n<font color=\"red\" style=\"font-size: 30px\"><b>YES</b></font>\n\n<br>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">1.3 DATASET OVERVIEW</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">GENERAL INFORMATION</b>\n\n<b><mark>The goal of this competition is to identify the locations of each functional tissue unit (FTU) in biopsy slides from several different organs.</mark></b>. \n\nThe training **<mark>annotations are provided as RLE-encoded masks</mark>**, and the images are in **<mark>16-bit</mark>**, **<mark>grayscale</mark>**, **<mark>PNG format</mark>**.\n\nThe underlying data includes <b>imagery from different sources</b> prepared with <b>different protocols</b> at a <b>variety of resolutions</b>, reflecting typical challenges for working with medical data.\n\nThis competition uses data from two different consortia, the <b><a href=\"https://www.nature.com/articles/s41556-021-00788-6\">Human Reference Atlas (HPA)</a></b> and <b><a href=\"https://hubmapconsortium.org/\">Human BioMolecular Atlas Program (HuBMAP)</a></b>. \n\nThe data is sourced as following:\n* The training dataset consists of data from public HPA data\n* the public test set is a combination of private HPA data **and HuBMAP data**\n* the private test set contains **only HuBMAP data**. \n\n<br>\n\n**Adapting models to function properly when presented with data that was prepared using a different protocol will be one of the core challenges of this competition. While this is expected to make the problem more difficult, developing models that generalize is a key goal of this endeavor.**\n\n<br>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">FILE INFORMATION</b>\n\n**[`train|test.csv`]**\n\n<ul>\n<li><code>id</code> - The image ID.</li>\n<li><code>organ</code> - The organ that the biopsy sample was taken from.</li>\n<li><code>data_source</code> - Whether the image was provided by Hubamp or HPA.</li>\n<li><code>img_height</code> - The height of the image in pixels.</li>\n<li><code>img_width</code> - The width of the image in pixels.</li>\n<li><code>pixel_size</code> - The height/width of a single pixel from this image in micrometers. All HPA images have a pixel size of 0.4 ¬µm. For Hubmap imagery the pixel size is 0.5 ¬µm for kidney, 0.2290 ¬µm for large intestine, 0.7562 ¬µm for lung, 0.4945 ¬µm for spleen, and 6.263 ¬µm for prostate.</li>\n<li><code>tissue_thickness</code> - The thickness of the biopsy sample in micrometers. All HPA images have a thickness of 4 ¬µm. The Hubmap samples have tissue slice thicknesses 10 ¬µm for kidney, 8 ¬µm for large intestine, 4 ¬µm for spleen, 5 ¬µm for lung, and 5 ¬µm for prostate.</li>\n<li><code>rle</code> - The target column. A run length encoded copy of the annotations. Provided for the training set only.</li>\n<li><code>age</code> - The patient's age in years. Provided for the training set only.</li>\n<li><code>sex</code> - The sex of the patient. Provided for the training set only.</li>\n</ul>\n\n<br>\n\n**[`sample_submission.csv`]**\n\n<ul>\n<li><code>id</code> - The image ID.</li>\n<li><code>rle</code> - A run length encoded mask of the FTUs in the image.</li>\n</ul>\n\n<br>\n\n**[`train|test_images`]**\n \nThe images\n* Expect roughly **550 images in the hidden test set**. \n* All images used have **at least one FTU**.\n* All tissue data used in this competition is **from healthy donors that pathologists identified as pathologically unremarkable tissue**.\n* HPA details:\n    * All HPA images are **3000 x 3000 pixels** with a **tissue area within the image around 2500 x 2500 pixels**. \n    * HPA samples were stained with antibodies visualized with 3,3'-diaminobenzidine (DAB) and counterstained with hematoxylin. \n* HuBMAP details:\n    * The Hubmap images range in size from **4500x4500** down to **160x160 pixels**. \n    * HuBMAP images were prepared using Periodic acid-Schiff (PAS)/hematoxylin and eosin (H&E) stains. \n\n<br>\n\n**[`train_annotations`]**\n\nThe annotations \n* Provided in the format of **points that define the boundaries of the polygon masks of the FTUs**\n\n<br>\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<a id=\"setup\"></a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #BF2C58; background-color: #ffffff;\" id=\"setup\">2&nbsp;&nbsp;SETUP&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a></h1>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">2.1 ACCELERATOR DETECTION</h3>\n\n---\n\nIn order to use **`TPU`**, we use **`TPUClusterResolver`** for the initialization which is necessary to connect to the remote cluster and initialize cloud TPUs. Let's go over two important points\n\n1. When using TPU on Kaggle, you don't need to specify arguments for **`TPUClusterResolver`**\n2. However, on **G**oogle **C**ompute **E**ngine (**GCE**), you will need to do the following:\n\n<br>\n\n```python\n# The name you gave to the TPU to use\nTPU_WORKER = 'my-tpu-name'\n\n# or you can also specify the grpc path directly\n# TPU_WORKER = 'grpc://xxx.xxx.xxx.xxx:8470'\n\n# The zone you chose when you created the TPU to use on GCP.\nZONE = 'us-east1-b'\n\n# The name of the GCP project where you created the TPU to use on GCP.\nPROJECT = 'my-tpu-project'\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER, zone=ZONE, project=PROJECT)\n```\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- Although the Tensorflow documentation says it is the <b>project name</b> that should be provided for the argument <b><code>`project`</code></b>, it is actually the <b>Project ID</b>, that you should provide. This can be found on the GCP project dashboard page.<br>\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCES:</b><br><br>\n    - <a href=\"https://www.tensorflow.org/guide/tpu#tpu_initialization\"><b>Guide - Use TPUs</b></a><br>\n    - <a href=\"https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver\"><b>Doc - TPUClusterResolver</b></a><br>\n\n</div>","metadata":{}},{"cell_type":"code","source":"print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU/GPU ...\")\n    # Yield the default distribution strategy in Tensorflow\n    #   --> Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\n# What Is a Replica?\n#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n#    --> Each replica is essentially a copy of the training graph that is run on each core and \n#        trains a mini-batch containing 1/8th of the overall batch size\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n\nprint(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:25.529108Z","iopub.execute_input":"2022-06-28T16:16:25.529432Z","iopub.status.idle":"2022-06-28T16:16:25.545024Z","shell.execute_reply.started":"2022-06-28T16:16:25.529402Z","shell.execute_reply":"2022-06-28T16:16:25.543663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">2.2 COMPETITION DATA ACCESS</h3>\n\n---\n\nTPUs read data must be read directly from **G**oogle **C**loud **S**torage **(GCS)**. Kaggle provides a utility library ‚Äì¬†**`KaggleDatasets`** ‚Äì which has a utility function **`.get_gcs_path`** that will allow us to access the location of our input datasets within **GCS**.<br><br>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìå &nbsp; TIPS:</b><br><br>- If you have multiple datasets attached to the notebook, you should pass the name of a specific dataset to the <b><code>`get_gcs_path()`</code></b> function. <i>In our case, the name of the dataset is the name of the directory the dataset is mounted within.</i><br><br>\n</div>","metadata":{}},{"cell_type":"code","source":"print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    DATA_DIR = KaggleDatasets().get_gcs_path('hubmap-organ-segmentation')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\nelse:\n    # Local path to training and validation images\n    DATA_DIR = \"/kaggle/input/hubmap-organ-segmentation\"\n    save_locally = None\n    load_locally = None\n\nprint(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:25.548063Z","iopub.execute_input":"2022-06-28T16:16:25.548454Z","iopub.status.idle":"2022-06-28T16:16:25.566054Z","shell.execute_reply.started":"2022-06-28T16:16:25.548422Z","shell.execute_reply":"2022-06-28T16:16:25.565026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">2.3 LEVERAGING XLA OPTIMIZATIONS</h3>\n\n---\n\n\n**XLA** (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes. **The results are improvements in speed and memory usage**.\n\n<br>\n\nWhen a TensorFlow program is run, all of the operations are executed individually by the TensorFlow executor. Each TensorFlow operation has a precompiled GPU/TPU kernel implementation that the executor dispatches to.\n\nXLA provides us with an alternative mode of running models: it compiles the TensorFlow graph into a sequence of computation kernels generated specifically for the given model. Because these kernels are unique to the model, they can exploit model-specific information for optimization.<br><br>\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üõë &nbsp; WARNING:</b><br><br>- XLA can not currently compile functions where dimensions are not inferrable: that is, if it's not possible to infer the dimensions of all tensors without running the entire computation<br>\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìå &nbsp; NOTE:</b><br><br>- XLA compilation is only applied to code that is compiled into a graph (in <b>TF2</b> that's only a code inside <b><code>tf.function</code></b>).<br>- The <b><code>jit_compile</code></b> API has must-compile semantics, i.e. either the entire function is compiled with XLA, or an <b><code>errors.InvalidArgumentError</code></b> exception is thrown)\n</div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">üìñ &nbsp; REFERENCE:</b><br><br>    - <a href=\"https://www.tensorflow.org/xla\"><b>XLA: Optimizing Compiler for Machine Learning</b></a><br>\n</div>","metadata":{}},{"cell_type":"code","source":"print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:25.568263Z","iopub.execute_input":"2022-06-28T16:16:25.568554Z","iopub.status.idle":"2022-06-28T16:16:25.574757Z","shell.execute_reply.started":"2022-06-28T16:16:25.568527Z","shell.execute_reply":"2022-06-28T16:16:25.57378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">2.4 BASIC DATA DEFINITIONS & INITIALIZATIONS</h3>\n\n---\n","metadata":{}},{"cell_type":"code","source":"print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n\n# Open the training dataframe and display the initial dataframe\nTRAIN_IMAGES_DIR = os.path.join(DATA_DIR, \"train_images\")\nTRAIN_LABELS_DIR = os.path.join(DATA_DIR, \"train_annotations\")\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\ntrain_df = pd.read_csv(TRAIN_CSV)\n\n# Get all training images\nall_train_images = glob(os.path.join(TRAIN_IMAGES_DIR, \"*.tiff\"), recursive=True)\nall_train_labels = glob(os.path.join(TRAIN_LABELS_DIR, \"*.json\"), recursive=True)\n\nprint(\"\\n... ORIGINAL TRAINING DATAFRAME... \\n\")\ndisplay(train_df)\n\nTEST_IMAGES_DIR = os.path.join(DATA_DIR, \"test_images\")\nTEST_CSV = os.path.join(DATA_DIR, \"test.csv\")\ntest_df = pd.read_csv(TEST_CSV)\n\nSS_CSV   = os.path.join(DATA_DIR, \"sample_submission.csv\")\nss_df = pd.read_csv(SS_CSV)\n\n# Get all testing images if there are any\nall_test_images = glob(os.path.join(TEST_IMAGES_DIR, \"*.tiff\"), recursive=True)\n\nprint(\"\\n\\n\\n... ORIGINAL SUBMISSION DATAFRAME... \\n\")\ndisplay(ss_df)\n\n# For debugging purposes when the test set hasn't been substituted we will know\nDEBUG=len(ss_df)==0\n\n# if DEBUG:\n#     TEST_DIR = TRAIN_DIR\n#     all_test_images = all_train_images\n#     ss_df = train_df.iloc[:10]\n#     ss_df = ss_df[[\"id\", \"class\"]]\n#     ss_df[\"predicted\"] = \"\"\n    \n#     print(\"\\n\\n\\n... DEBUG SUBMISSION DATAFRAME... \\n\")\n#     display(ss_df)\n\ndef rgb2hex(rgb_tuple):\n    r,g,b = rgb_tuple\n    def clamp(x): \n        return max(0, min(x, 255))\n    return \"#{0:02x}{1:02x}{2:02x}\".format(clamp(r), clamp(g), clamp(b))\nORGANS = ['kidney', 'largeintestine', 'lung', 'prostate', 'spleen']\n_COLOURS = [(230, 0, 73), (11, 180, 255), (80, 233, 145), (230, 216, 0), (155, 25, 245)]\nO2C_MAP = {_o:_c for _o,_c in zip(ORGANS, _COLOURS)}\nO2C_HEX_MAP = {_o:rgb2hex(_c) for _o,_c in O2C_MAP.items()}\n\nprint(f\"\\n\\n\\n... ARE WE DEBUGGING: {DEBUG}... \\n\")\n\nprint(\"\\n... BASIC DATA SETUP FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:25.57599Z","iopub.execute_input":"2022-06-28T16:16:25.577056Z","iopub.status.idle":"2022-06-28T16:16:26.168351Z","shell.execute_reply.started":"2022-06-28T16:16:25.577003Z","shell.execute_reply":"2022-06-28T16:16:26.167308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">2.5 UPDATE DATAFRAMES WITH ACCESSIBLE EXTRA INFORMATION</h3>\n\n---\n\nI wrapped the logic in a preprocessing function but also went through step by step so people could validate if they so wished\n\n**NOTE:** \n","metadata":{}},{"cell_type":"code","source":"sex_2_int = {\"Male\":0, \"Female\":1}\nint_2_sex = {v:k for k,v in sex_2_int.items()}\nage_divisor = 100.0\n\ntrain_df[\"sex\"] = train_df[\"sex\"].map(sex_2_int)\ntrain_df[\"age\"] = train_df[\"age\"]/age_divisor\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:26.170009Z","iopub.execute_input":"2022-06-28T16:16:26.170443Z","iopub.status.idle":"2022-06-28T16:16:26.207214Z","shell.execute_reply.started":"2022-06-28T16:16:26.170399Z","shell.execute_reply":"2022-06-28T16:16:26.206161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_map = {int(x[:-5].rsplit(\"/\", 1)[-1]):x for x in all_train_images}\ntrain_lbl_map = {int(x[:-5].rsplit(\"/\", 1)[-1]):x for x in all_train_labels}\ntest_img_map = {int(x[:-5].rsplit(\"/\", 1)[-1]):x for x in all_test_images}\n\ntrain_df.insert(3, \"img_path\", train_df[\"id\"].map(train_img_map))\ntrain_df.insert(4, \"lbl_path\", train_df[\"id\"].map(train_lbl_map))\ntest_df.insert(3, \"img_path\", test_df[\"id\"].map(test_img_map))\n\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:26.208392Z","iopub.execute_input":"2022-06-28T16:16:26.208709Z","iopub.status.idle":"2022-06-28T16:16:26.24996Z","shell.execute_reply.started":"2022-06-28T16:16:26.208682Z","shell.execute_reply":"2022-06-28T16:16:26.248963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"helper_functions\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #BF2C58; background-color: #ffffff;\" id=\"helper_functions\">\n    3&nbsp;&nbsp;HELPER FUNCTION & CLASSES&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n# modified from: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns: \n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    # Split the string by space, then convert it into a integer array\n    s = np.array(mask_rle.split(), dtype=int)\n\n    # Every even value is the start, every odd value is the \"run\" length\n    starts = s[0::2] - 1\n    lengths = s[1::2]\n    ends = starts + lengths\n\n    # The image image is actually flattened since RLE is a 1D \"run\"\n    if len(shape)==3:\n        h, w, d = shape\n        img = np.zeros((h * w, d), dtype=np.float32)\n    else:\n        h, w = shape\n        img = np.zeros((h * w,), dtype=np.float32)\n\n    # The color here is actually just any integer you want!\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n        \n    # Don't forget to change the image back to the original shape\n    return img.reshape(shape).T\n\n# https://www.kaggle.com/namgalielei/which-reshape-is-used-in-rle\ndef rle_decode_top_to_bot_first(mask_rle, shape):\n    \"\"\" TBD\n    \n    Args:\n        mask_rle (str): run-length as string formated (start length)\n        shape (tuple of ints): (height,width) of array to return \n    \n    Returns:\n        Mask (np.array)\n            - 1 indicating mask\n            - 0 indicating background\n\n    \"\"\"\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape((shape[1], shape[0]), order='F').T  # Reshape from top -> bottom first\n\n# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\ndef rle_encode(img):\n    \"\"\" TBD\n    \n    Args:\n        img (np.array): \n            - 1 indicating mask\n            - 0 indicating background\n    \n    Returns: \n        run length as string formated\n    \"\"\"\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    return [item for sublist in nested_list for item in sublist]\n\ndef load_json_to_dict(json_path):\n    \"\"\" tbd \"\"\"\n    with open(json_path) as json_file:\n        data = json.load(json_file)\n    return data\n\ndef tf_decode_tiff(img_path, to_numpy=False, to_rgb=False):\n    img = tf.io.read_file(img_path)\n    img = tfio.experimental.image.decode_tiff(img)\n    \n    # Optionals\n    if to_rgb: img = tfio.experimental.color.rgba_to_rgb(img)\n    if to_numpy: img = img.numpy()\n        \n    return img","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:26.251429Z","iopub.execute_input":"2022-06-28T16:16:26.251744Z","iopub.status.idle":"2022-06-28T16:16:26.271014Z","shell.execute_reply.started":"2022-06-28T16:16:26.251718Z","shell.execute_reply":"2022-06-28T16:16:26.26961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"create_dataset\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #BF2C58; background-color: #ffffff;\" id=\"create_dataset\">\n    4&nbsp;&nbsp;DATASET EXPLORATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">4.0 LOOK AT A SINGLE EXAMPLE PRIOR TO INVESTIGATION</h3>\n\n---\n\nWe simply do this to make sure everything is where it should be and we understand the basics of how to access all the relevant data.\n\nWe will wrap this basic exploration functionality as single function to allow for easy examination of any passed identifier","metadata":{}},{"cell_type":"code","source":"def get_overlay(img_path, organ, rle_str, img_shape, _alpha=0.999, _beta=0.35, _gamma=0):\n    _img = tf_decode_tiff(img_path, to_numpy=True, to_rgb=True).astype(np.float32)\n    _seg_rgb = (np.stack([rle_decode(rle_str, shape=img_shape, color=1),]*3, axis=-1)*O2C_MAP[organ]).astype(np.float32)\n    seg_overlay = cv2.addWeighted(src1=_img, alpha=_alpha, \n                                  src2=_seg_rgb, beta=_beta, gamma=_gamma)\n    return seg_overlay/255.\n\ndef examine_id(df, ex_id=None, plot_overlay=True, print_meta=True, plot_original=False, plot_segmentation=False, _figsize=(20,20)):\n    \"\"\" Wrapper function to allow for easy visual exploration of an example \"\"\"\n    if ex_id is None:\n        ex_id = df[\"id\"].sample(1).values[0]\n        print(f\"\\n... NO ID GIVEN... RANDOM ID CHOSEN: ID={ex_id} ...\")\n    \n    print(f\"\\n... ID ({ex_id}) EXPLORATION STARTED ...\\n\\n\")\n    demo_ex = df[df[\"id\"]==ex_id].squeeze()\n\n    if print_meta:\n        print(f\"\\n... WITH id=`{ex_id}` (organ=`{demo_ex['organ']}`)  ‚Äì  WE HAVE THE FOLLOWING DEMO EXAMPLE TO WORK FROM ... \\n\\n\")\n        display(demo_ex.to_frame())\n\n    if plot_original:\n        print(f\"\\n\\n... ORIGINAL IMAGE PLOT ...\\n\")\n        plt.figure(figsize=_figsize)\n        plt.imshow(tf_decode_tiff(demo_ex[\"img_path\"], to_numpy=True, to_rgb=True))\n        plt.title(f\"Original RGB Image For ID: {ex_id}\", fontweight=\"bold\")\n        plt.axis(False)\n        plt.show()\n\n    if plot_segmentation:\n        print(f\"\\n\\n... SEGMENTATION MASK PLOT ({demo_ex['organ']})...\\n\")\n        plt.figure(figsize=_figsize)\n        plt.imshow((np.stack([rle_decode(demo_ex.rle, shape=(demo_ex.img_width, demo_ex.img_height), color=1),]*3, axis=-1)*O2C_MAP[demo_ex.organ]).astype(np.float32))\n        plt.title(f\"Segmentation Mask ({demo_ex.organ})\", fontweight=\"bold\")\n        plt.axis(False)\n        plt.show()\n\n    if plot_overlay:\n        print(f\"\\n\\n... IMAGE WITH RGB SEGMENTATION MASK OVERLAY ({demo_ex['organ']}) ...\\n\")\n        seg_overlay = get_overlay(demo_ex.img_path, demo_ex.organ, demo_ex.rle, img_shape=(demo_ex.img_width, demo_ex.img_height))\n\n        plt.figure(figsize=_figsize)\n        plt.imshow(seg_overlay)\n        plt.title(f\"Segmentation Overlay id=`{ex_id}` (organ=`{demo_ex['organ']}`)\", fontweight=\"bold\")\n        handles = [Rectangle((0,0),1,1, color=(*[__c/255. for __c in _c], 0.5)) for _c in _COLOURS]\n        labels = ORGANS\n        plt.legend(handles,labels)\n        plt.axis(False)\n        plt.show()\n\n    print(\"\\n\\n... SINGLE ID EXPLORATION FINISHED ...\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:26.273395Z","iopub.execute_input":"2022-06-28T16:16:26.274007Z","iopub.status.idle":"2022-06-28T16:16:26.302276Z","shell.execute_reply.started":"2022-06-28T16:16:26.273953Z","shell.execute_reply":"2022-06-28T16:16:26.300598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for _o in ORGANS:\n    print(f\"\\n\\n... RANDOM {_o.upper()} TRAIN EXAMPLE ...\\n\\n\")\n    examine_id(train_df[train_df.organ==_o])\n\nprint(\"\\n\\n\\n\\n... TEST IMAGE ...\\n\\n\")\nexamine_id(test_df, plot_overlay=False, plot_original=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:26.305457Z","iopub.execute_input":"2022-06-28T16:16:26.306112Z","iopub.status.idle":"2022-06-28T16:16:49.216199Z","shell.execute_reply.started":"2022-06-28T16:16:26.306071Z","shell.execute_reply":"2022-06-28T16:16:49.215053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">4.1 INVESTIGATE THE FREQUENCY OF VARIOUS ORGAN SEGMENTATION EXAMPLES</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n\nWe can observe the following distribution:\n* **kidney**\n    * **99** images\n* **prostate**\n    * **93** images\n* **large intestine**\n    * **58** images\n* **spleen**\n    * **53** images\n* **lung**\n    * **48** images\n    \nAll of the organ types are represented fairly well within our data, with **kidney** and **prostate** being about twice as common as the other three organ types: **large intestine**, **spleen**, and **lung**","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(train_df, \"organ\", color_discrete_map=O2C_HEX_MAP, color=\"organ\", title=\"<b>Number of Segmentation Masks Per Organ Type</b>\",)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:49.217664Z","iopub.execute_input":"2022-06-28T16:16:49.21805Z","iopub.status.idle":"2022-06-28T16:16:50.161106Z","shell.execute_reply.started":"2022-06-28T16:16:49.218006Z","shell.execute_reply":"2022-06-28T16:16:50.160012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">4.2 INVESTIGATE THE IMAGE SIZES</h3>\n\n---\n\nIt's observable that not all images have the same size... however, given that, there is not that much variation between image sizes:\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n* Globally, we can see that most of the images are dominated by a single size $(3000 \\times 3000)$\n* All the other sizes (19) only have 1 or 2 occurences each.\n* All image sizes are square\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OTHER INFORMATION</b>\n\n* Expect roughly 550 images in the hidden test set. \n* All HPA images are 3000 x 3000 pixels with a tissue area within the image around 2500 x 2500 pixels. \n* The Hubmap images range in size from 4500x4500 down to 160x160 pixels.\n\n<br>","metadata":{}},{"cell_type":"code","source":"fig = px.scatter(train_df.drop_duplicates(subset=[\"img_width\", \"img_height\"]), x=\"img_width\", y=\"img_height\", \n                 size=train_df.groupby([\"img_width\", \"img_height\"])[\"id\"].transform(\"count\").iloc[train_df.drop_duplicates(subset=[\"img_width\", \"img_height\"]).index], \n                 color=\"(\"+train_df.drop_duplicates(subset=[\"img_width\", \"img_height\"])[\"img_width\"].astype(str)+\",\"+train_df.drop_duplicates(subset=[\"img_width\", \"img_height\"])[\"img_height\"].astype(str)+\")\", \n                 title=\"<b>Bubble Chart Showing The Various Image Sizes</b>\",\n                 labels={\"color\":\"<b>Size Legend</b>\", \n                         \"size\":\"<b>Number Of Observations</b>\",\n                         \"img_height\":\"<b>Image Height (pixels)</b>\",\n                         \"img_width\":\"<b>Image Width (pixels)</b>\"},\n                 size_max=150)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:50.162477Z","iopub.execute_input":"2022-06-28T16:16:50.162799Z","iopub.status.idle":"2022-06-28T16:16:50.34535Z","shell.execute_reply.started":"2022-06-28T16:16:50.16277Z","shell.execute_reply":"2022-06-28T16:16:50.344209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">4.3 INVESTIGATE AGE</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n* The age distribution is skewed to be 50+\n* The distribution of examples with **large intestine** segmented is much higher among the oldest demographic\n* The general distribution of organ segmentation types across age groups appears relatively stratified (with the previously noted exception)\n\n<br>","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(train_df, train_df[\"age\"]*age_divisor, color_discrete_map=O2C_HEX_MAP, color=\"organ\", \n                   title=\"<b>Number of Segmentation Masks Per Age Bin by Organ Type </b>\",\n                   labels={\"organ\":\"<b>Organ Legend</b>\", \n                         \"count\":\"<b>Number Of Observations</b>\",\n                         \"x\":\"<b>Age</b>\"}, text_auto=True,\n                  )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:50.346885Z","iopub.execute_input":"2022-06-28T16:16:50.347314Z","iopub.status.idle":"2022-06-28T16:16:50.434528Z","shell.execute_reply.started":"2022-06-28T16:16:50.347269Z","shell.execute_reply":"2022-06-28T16:16:50.433487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">4.4 INVESTIGATE SEX</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n* Women do not have a prostate and as such there are no prostate examples where gender is Female.\n* Kidney's seem skewed towards the Male gender\n* All other organ's appear evenly distributed\n\n<br>","metadata":{}},{"cell_type":"code","source":"fig = px.histogram(train_df, train_df[\"sex\"], color_discrete_map=O2C_HEX_MAP, color=\"organ\", \n                   title=\"<b>Examples by Gender (0=Male, 1=Female) and Organ Type</b>\",\n                   labels={\"organ\":\"<b>Organ Legend</b>\", \n                         \"count\":\"<b>Number Of Observations</b>\",\n                         \"sex\":\"<b>Sex</b>\"},text_auto=True, barmode=\"group\",\n                  )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:16:50.436159Z","iopub.execute_input":"2022-06-28T16:16:50.436471Z","iopub.status.idle":"2022-06-28T16:16:50.522584Z","shell.execute_reply.started":"2022-06-28T16:16:50.436441Z","shell.execute_reply":"2022-06-28T16:16:50.521499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">4.5 INVESTIGATE FTUS ACROSS DIFFERENT ORGANS</h3>\n\n---\n\nHere is a zoom sequence from the human body to the single-cell level for the kidney. \n* **Notice that FTU is one level higher than cells.**\n\n<center><img src=\"https://i.ibb.co/wRySj9z/results-5-1.png\"></center>\n\n<br>\n\nFTUs in the organs presented in this competition are: \n* **glomeruli** in the **kidney** ‚Äì (a)\n* **crypt** in the **large intestine**  ‚Äì (b)\n* **alveolus** in the **lung**  ‚Äì (c)\n* **glandular acinus** in the **prostate**  ‚Äì (d)\n* **white pulp** in the **spleen**  ‚Äì (e)\n\n<center><img src=\"https://i.ibb.co/ZX0ZYYV/results-7-1.png\"></center>\n\n<br>\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n* TBD","metadata":{}},{"cell_type":"code","source":"def crop_ftu_polygons(img, cnts, pad=5):\n    def get_crop_from_cnt(_img, _cnt, _pad):\n        x1,y1,w,h = cv2.boundingRect(np.array(_cnt))\n        x2,y2 = x1+w+_pad, y1+h+_pad\n        x1, y1 = max(0, x1-_pad), max(0, y1-_pad)\n        return _img[y1:y2, x1:x2]\n    ftu_crops = [get_crop_from_cnt(img, cnt, pad) for cnt in cnts]\n    return ftu_crops    \n        \ndef plot_ftus(ftu_crops, n_cols=4, height_per_row=7, max_crops=50):\n    ftu_crops = ftu_crops[:max_crops]\n    n_rows = int(np.ceil(len(ftu_crops)/n_cols))\n    \n    plt.figure(figsize=(20, n_rows*height_per_row))\n    for i, ftu_crop in enumerate(ftu_crops):\n        plt.subplot(n_rows, n_cols, i+1)\n        plt.imshow(ftu_crop)\n        plt.title(f\"FTU #{i+1}  ‚Äì‚Äì  SHAPE={ftu_crop.shape[:-1]}\", fontweight=\"bold\")\n        plt.axis(False)\n        \n    plt.tight_layout()\n    plt.show()\n\nMAX_N_FTU = 50\nN_EX = 10\n\nftu_crop_map = {}\nfor _organ in ORGANS:\n    sub_df = train_df[train_df.organ==_organ].sample(N_EX)\n    imgs = [cv2.imread(img_path)[..., ::-1] for img_path in sub_df.img_path.values]\n    cnts = [load_json_to_dict(json_path) for json_path in sub_df.lbl_path.values]\n    ftu_crops = flatten_l_o_l([crop_ftu_polygons(img, _cnts, pad=0) for img, _cnts in zip(imgs, cnts)])[:MAX_N_FTU]\n    ftu_crop_map[_organ] = ftu_crops.copy()\n    print(f\"\\n\\n\\n... DISPLAYING {len(ftu_crops)} {_organ} FTU CROPS ...\\n\")\n    plot_ftus(ftu_crops)\n\n# def get_ftus(df, organ, k=25, pad=25, one_per_img=False, data_source=\"HPA\", do_resize=False, resize_to=(128,128)):\n#     pass","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:21:59.184235Z","iopub.execute_input":"2022-06-28T16:21:59.185503Z","iopub.status.idle":"2022-06-28T16:22:56.2919Z","shell.execute_reply.started":"2022-06-28T16:21:59.18543Z","shell.execute_reply":"2022-06-28T16:22:56.290581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">4.6 AVERAGE FTU SIZE (BOUNDING-RECT) BY ORGAN</h3>\n\n---\n\nSince it would take too much processing to comb through all the available images and respective FTUs we will look at the previously obtained sampling of crops (bounding-rects) and the respective dimensions therewithin.\n\nAs we know all the HPA images have a pixel resolution (size) of 0.4¬µm we can therefore determine the real dimensions of the bounding rectangles around each FTU.\n\nNote dimensions are represented as (HEIGHTxWIDTH) unless otherwise communicated\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n* **Kidney**\n    * **Actual** FTU size is on-average SQUARE (most images aren't square but it averages as such) at **~157x155¬µm**\n    * **HPA** FTU size in pixels will be **~392x389px**\n    * **HuBMAP** FTU size in pixels will be **~314x310px**\n* **Large Intestine**\n    * **Actual** FTU size is on-average RECTANGULAR (1Hx2W) (images are mostly horizontal but occasionally vertical or square) at **~60x131¬µm**\n    * **HPA** FTU size in pixels will be **~150x328px**\n    * **HuBMAP** FTU size in pixels will be **~262x572px**\n* **Lung**\n    * **Actual** FTU size is on-average SQUARE (images aren't square but average to mostly square) at **~90x80¬µm**\n    * **HPA** FTU size in pixels will be **~225x200px**\n    * **HuBMAP** FTU size in pixels will be **~119x106px**\n* **Prostate**\n    * **Actual** FTU size is on-average SQUARE (images aren't square but average to mostly square) at **~68x77¬µm**\n    * **HPA** FTU size in pixels will be **~172x193px**\n    * **HuBMAP** FTU size in pixels will be **~11x13px**\n* **Spleen**\n    * **Actual** FTU size is on-average RECTANGULAR (2Hx1W) at **~407x197¬µm**\n    * **HPA** FTU size in pixels will be **~1018x493px**\n    * **HuBMAP** FTU size in pixels will be **~823x399px**\n\n<br>","metadata":{}},{"cell_type":"code","source":"ftu_shape_map = {_organ:[c_map.shape[:-1] for c_map in crop_maps] for _organ, crop_maps in ftu_crop_map.items()}\nftu_area_map = {_organ:[c_map.shape[0]*c_map.shape[1] for c_map in crop_maps] for _organ, crop_maps in ftu_crop_map.items()}\n\norgan_ftu_df = pd.DataFrame({\"organ\":ORGANS})\norgan_ftu_df[\"average_area\"] = organ_ftu_df.organ.apply(lambda x: np.array(ftu_area_map[x]).mean())\norgan_ftu_df[\"average_area_¬µm\"] = organ_ftu_df[\"average_area\"]*0.4**2\norgan_ftu_df[\"average_height\"] = organ_ftu_df.organ.apply(lambda x: np.array(ftu_shape_map[x][0]).mean())\norgan_ftu_df[\"average_width\"] = organ_ftu_df.organ.apply(lambda x: np.array(ftu_shape_map[x][1]).mean())\norgan_ftu_df[\"average_shape_¬µm\"] = (\"(\"+(organ_ftu_df[\"average_height\"]*0.4).round(2).astype(str)+\",\"+(organ_ftu_df[\"average_width\"]*0.4).round(2).astype(str)+\")\").apply(ast.literal_eval)\norgan_ftu_df","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:45:38.359544Z","iopub.execute_input":"2022-06-28T16:45:38.360036Z","iopub.status.idle":"2022-06-28T16:45:38.390048Z","shell.execute_reply.started":"2022-06-28T16:45:38.359998Z","shell.execute_reply":"2022-06-28T16:45:38.389262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">4.7 WHY DO WE HAVE JSON FILES AND RLE?</h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">OBSERVATIONS</b>\n* The RLE is the entire binary mask as a simple compressed string\n* The JSON contains the description of the masks as a collection (list) of polygons representing the respective masks.\n    * A polygon is represented as a collection (list) of vertices.\n    * A vertex is a point represented by a pair of values ‚Äì‚Äì the x,y position\n    * If you took these polygons and plotted them alongside the RLE decoded mask, they SHOULD be the same. I think it's simply another representation for us to use.\n\n<br>","metadata":{}},{"cell_type":"code","source":"# Use first row as example --> ex_id=10044\nex_row = train_df[train_df.id==10044]\n\nprint(\"\\n... First ‚Äì Plot the image and the RLE overlapped ontop ...\\n\")\nexamine_id(ex_row)\n\nprint(\"\\n\\n\\n... Second ‚Äì¬†Load the .json file and display mask polygons ...\\n\")\nex_poly = load_json_to_dict(ex_row.lbl_path.values[0])\nraw_img = np.array(cv2.imread(ex_row.img_path[0])[..., ::-1])\nimg = np.zeros_like(raw_img)\ncolor_space = np.linspace(235, 20, len(ex_poly), dtype=int)\nfor i, ex_p in enumerate(ex_poly):\n    img = cv2.fillPoly(img, [np.expand_dims(np.array(ex_p, np.int32), axis=0)], color=(int(color_space[i]),245,245))\nplt.figure(figsize=(20,20))\nplt.title(\"Mask Polygons\", fontweight=\"bold\")\nplt.imshow(img)\nplt.show()\n\nprint(\"\\n\\n\\n Lastly ‚Äì¬†Compare mask polygons to rle decoded mask\")\nrle_mask = rle_decode(ex_row.rle.values[0], (ex_row.img_width.values[0], ex_row.img_height.values[0]))\njson_mask = np.where(img>0, 1, 0)[..., 0].astype(np.float32)\ndiff_mask = np.abs(rle_mask-json_mask).astype(np.uint8)\n\nplt.figure(figsize=(20,20))\nplt.title(f\"Number of Non-Equal Pixels = {(rle_mask!=json_mask).sum()}\", fontweight=\"bold\")\nplt.imshow(diff_mask, cmap=\"gray\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:17:44.269274Z","iopub.execute_input":"2022-06-28T16:17:44.269659Z","iopub.status.idle":"2022-06-28T16:17:50.982719Z","shell.execute_reply.started":"2022-06-28T16:17:44.269628Z","shell.execute_reply":"2022-06-28T16:17:50.981803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**VISUALIZE DAB STAIN AND HEMATOXYLIN COUNTERSTAIN**\n* DAB is brown and present at locations of enzymatic activity\n* HEMATOXYLIN is light blue when found within cells\n* HEMATOXYLIN is dark blue when found within nuclei","metadata":{"_kg_hide-output":true,"_kg_hide-input":true}},{"cell_type":"code","source":"# original_prostate_img = cv2.imread(train_df.img_path[0])[..., ::-1]\n# resized_prostate_img = cv2.resize(original_prostate_img, (int(3000*(0.4/6.263)),int(3000*(0.4/6.263))))\n\n# plt.figure(figsize=(20,40))\n\n# plt.subplot(2,1,1)\n# plt.imshow(original_prostate_img)\n# plt.title(\"Original Prostate Image (3000x3000) ‚Äì 0.4\", fontweight=\"bold\")\n\n# plt.subplot(2,1,2)\n# plt.imshow(resized_prostate_img)\n# plt.title(\"Resized Prostate Image (191x191) ‚Äì 6.263\", fontweight=\"bold\")\n\n# plt.savefig(\"comparison.png\", dpi=300)\n# plt.show()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-28T16:17:50.984076Z","iopub.execute_input":"2022-06-28T16:17:50.984925Z","iopub.status.idle":"2022-06-28T16:17:50.989917Z","shell.execute_reply.started":"2022-06-28T16:17:50.984884Z","shell.execute_reply":"2022-06-28T16:17:50.988885Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"modelling\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #BF2C58; background-color: #ffffff;\" id=\"modelling\">\n    5&nbsp;&nbsp;MODELLING&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">5.1 ALL ONES WITH SOME INTELLIGENCE</h3>\n\n---\n\nWe simply submit a mask made up of ones in a circular shape in the centre of the respective image.\n\n<br>","metadata":{}},{"cell_type":"code","source":"rles = []\nSMART_CIRCLE_FRAC = 0.85\n\nfor i, (img_w, img_h) in enumerate(zip(test_df[\"img_width\"], test_df[\"img_height\"])):\n    tmp_img = np.zeros((img_w, img_h))\n    tmp_img = cv2.circle(tmp_img, (int(np.round(img_w/2)), int(np.round(img_h/2))), int(np.round((img_w/2)*SMART_CIRCLE_FRAC)), 1, -1)\n    rles.append(rle_encode(tmp_img))\n    if i==0:\n        plt.figure(figsize=(10,10))\n        plt.imshow(rle_decode(rles[0], (img_w, img_h)))\n        plt.title(\"Example of Smart Circle Mask\", fontweight=\"bold\")\n        plt.show()\n\nss_df[\"rle\"] = rles\nss_df = ss_df[[\"id\", \"rle\"]]\nss_df.to_csv(\"submission.csv\", index=False)\ndisplay(ss_df)","metadata":{"execution":{"iopub.status.busy":"2022-06-28T16:17:50.991598Z","iopub.execute_input":"2022-06-28T16:17:50.992566Z","iopub.status.idle":"2022-06-28T16:17:51.877938Z","shell.execute_reply.started":"2022-06-28T16:17:50.992518Z","shell.execute_reply":"2022-06-28T16:17:51.876691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<br>\n\n\n<a id=\"appendix\"></a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #BF2C58; background-color: #ffffff;\" id=\"appendix\">\n    9&nbsp;&nbsp;APPENDIX&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;</a>\n</h1>\n\n---\n\nThis section is mostly if not entirely copied from various sources listed below:\n* <a href=\"https://www.kaggle.com/code/yashvrdnjain/background-information/notebook\"><b>Yashvardhan Jain's Background Info Notebook</b></a>","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">9.1  HUMAN BIOMOLECULAR ATLAS PROGRAM (HUBMAP)</h3>\n\n---\n\nHuBMAP aims to create an open, global atlas of the healthy human body at the cellular level, see the [**HuBMAP Consortium**](https://hubmapconsortium.org/about/) and [**Visible Human Massive Open Online Course (VHMOOC)**](https://expand.iu.edu/browse/sice/cns/courses/hubmap-visible-human-mooc). \n\nOne component of this overarching goal is to identify medically relevant functional tissue units (FTUs) within whole slide microscopy images of human tissues. Once these FTUs are detected, information on size, shape, variability in number and location within the tissue samples can be used to help build a spatially accurate and semantically explicit model of the human body.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">9.2  HUMAN PROTEIN ATLAS (HPA)</h3>\n\n---\n\nThe [**Human Protein Atlas**](https://www.proteinatlas.org/) is a Swedish-based program initiated in 2003 with the aim to map all the human proteins in cells, tissues, and organs using an integration of various omics technologies, including antibody-based imaging, mass spectrometry-based proteomics, transcriptomics, and systems biology. All the data in the knowledge resource is open access to allow scientists both in academia and industry to freely access the data for exploration of the human proteome. The images for this competition are from the [**Tissue Atlas**](https://www.proteinatlas.org/humanproteome/tissue), which maps the protein expression profile across human tissues.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">9.3  FUNCTIONAL TISSUE UNITS (FTUs)</h3>\n\n---\n\nBernard de Bono and his team coined the term ‚Äúfunctional tissue unit‚Äù to support tissue modelling (**[de Bono, 2013](https://www.ncbi.nlm.nih.gov/pubmed/24103658)**) as:\n> ‚Äú... a three-dimensional block of cells centered around a capillary, such that each cell in this block is within diffusion distance from any other cell in the same block‚Äù\n\nFor the purposes of Human Reference Atlas (**[B√∂rner, 2021](https://www.nature.com/articles/s41556-021-00788-6)**, **[Godwin, 2021](https://www.biorxiv.org/content/10.1101/2021.11.09.467810v1)**, **[B√∂rner, 2021](https://www.biorxiv.org/content/10.1101/2021.12.30.474265v1)**) construction and usage, **we define an FTU as the smallest level of tissue organization (i.e., a cell population neighborhood) that performs the organ‚Äôs major physiological function**.  \n* One example of an FTU is the glomerulus found in the outer layer of kidney tissue known as the cortex, which in humans has an area of about 800 mm2 and average depth of about 9 mm (**[Mounier-Vehier, 2002](https://doi.org/10.1046/j.1523-1755.2002.00167.x)**). \n* Glomeruli consist of capillaries that facilitate filtration of waste products out of blood. \n* Normal glomeruli typically range from 100-350 Œºm in diameter with a roughly spherical shape (**[Kannan, 2019](https://www.kireports.org/article/S2468-0249(19))**). \n\n<br>\n\nRefer to Fig. 1 for a zoom sequence from the human body to single-cell level for the kidney.\n\n<center><img src=\"https://i.ibb.co/wRySj9z/results-5-1.png\" width=90%></center>\n\n<br>\n\nSimilarly, FTUs in other organs presented in this competition are: crypt in the large intestine, alveolus in the lung, glandular acinus in the prostate, and white pulp in the spleen, see Fig. 2.\n\n<center><img src=\"https://i.ibb.co/ZX0ZYYV/results-7-1.png\" width=90%></center>\n\n<br>","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">9.4  TERMINOLOGY</h3>\n\n---\n\n* **Tissue Preservation**: \n    * Different types of tissue preservation techniques are used to maintain the integrity of tissue for downstream biomolecular assays analysis.\n* **Fresh Frozen Tissue Preservation**: \n    * Fresh frozen tissue is frozen in liquid nitrogen (-190¬∞C) within 30-60 minutes after surgical excision; this type of preservation has been the method of choice for transcriptomics and immunohistochemistry ([Robbe, 2018](https://doi.org/10.1038/gim.2017.241), [Stoeckli, 2007](https://doi.org/10.1016/j.ijms.2006.10.007)).\n* **Formalin Fixed, Paraffin Embedded (FFPE) Tissue Preservation**: \n    * FFPE tissue is the preferred method for clinical pathology samples for histology assessment ([Bass, 2014](https://doi.org/10.5858/arpa.2013-0691-RA)).\n* **Periodic acid-Schiff (PAS) Stain Microscopy**: \n    * [PAS](https://en.wikipedia.org/wiki/Periodic_acid%E2%80%93Schiff_stain) is a histology stain that detects complex sugars in tissue sections.\n* **Hematoxylin and Eosin (H&E) Stain Microscopy**: \n    * [H&E](https://en.wikipedia.org/wiki/H%26E_stain) stain is one of the principal tissue stains used in histology.\n* **IHC staining**: \n    * [Immunohistochemical](https://en.wikipedia.org/wiki/Immunohistochemistry) staining is accomplished with antibodies that recognize the target antigen.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">9.5  DATASETS</h3>\n\n---\n\nThe Ground Truth segmentations were manually created and further reviewed by Subject Matter Experts (SMEs) for quality assurance.\n\n* **HPA Data** comprises images of 1 mm diameter tissue microarray cores stained with antibodies visualized with 3,3'-diaminobenzidine (DAB) and counterstained with hematoxylin.\n* **HuBMAP Data** comprises Periodic acid-Schiff (PAS)/hematoxylin and eosin (H&E) stain images of human tissue.","metadata":{}},{"cell_type":"markdown","source":"<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #BF2C58; background-color: #ffffff;\">9.6  RELATED WORK</h3>\n\n---\n\n* Leah L. Godwin, Yingnan Ju, Naveksha Sood, Yashvardhan Jain, Ellen M. Quardokus, Andreas Bueckle, Teri Longacre, Aaron Horning, Yiing Lin, Edward D. Esplin, John W. Hickey, Michael P. Snyder, N. Heath Patterson, Jeffrey M. Spraggins, Katy B√∂rner. ‚ÄúRobust and generalizable segmentation of human functional tissue units‚Äù; bioRxiv 2021.11.09.467810; doi: https://doi.org/10.1101/2021.11.09.467810\n* Sheehan, Susan M., and Ron Korstanje. 2018. ‚ÄúAutomatic Glomerular Identification and Quantification of Histological Phenotypes Using Image Analysis and Machine Learning.‚Äù American Journal of Physiology - Renal Physiology 315 (6): F1644‚Äì51. https://doi.org/10.1152/ajprenal.00629.2017.\n* Gallego, Jaime, Anibal Pedraza, Samuel Lopez, Georg Steiner, Lucia Gonzalez, Arvydas Laurinavicius, and Gloria Bueno. 2018. ‚ÄúGlomerulus Classification and Detection Based on Convolutional Neural Networks.‚Äù Journal of Imaging 4 (1): 20. https://doi.org/10.3390/jimaging4010020.\n* Govind, Darshana, Brandon Ginley, Brendon Lutnick, John E. Tomaszewski, and Pinaki Sarder. 2018. ‚ÄúGlomerular Detection and Segmentation from Multimodal Microscopy Images Using a Butterworth Band-Pass Filter.‚Äù In Medical Imaging 2018: Digital Pathology, 10581:1058114. International Society for Optics and Photonics. https://doi.org/10.1117/12.2295446.\n* Bukowy, John D., Alex Dayton, Dustin Cloutier, Anna D. Manis, Alexander Staruschenko, Julian H. Lombard, Leah C. Solberg Woods, Daniel A. Beard, and Allen W. Cowley. 2018. ‚ÄúRegion-Based Convolutional Neural Nets for Localization of Glomeruli in Trichrome-Stained Whole Kidney Sections.‚Äù Journal of the American Society of Nephrology 29 (8): 2081‚Äì88. https://doi.org/10.1681/ASN.2017111210.\n* Kannan, Shruti, Laura A. Morgan, Benjamin Liang, McKenzie G. Cheung, Christopher Q. Lin, Dan Mun, Ralph G. Nader, et al. 2019. ‚ÄúSegmentation of Glomeruli Within Trichrome Images Using Deep Learning.‚Äù Kidney International Reports 4 (7): 955‚Äì62. https://doi.org/10.1016/j.ekir.2019.04.008.\n* Nassim Bouteldja, Barbara M. Klinkhammer, Roman D. B√ºlow, Patrick Droste, Simon W. Otten, Saskia Freifrau von Stillfried, Julia Moellmann, Susan M. Sheehan, Ron Korstanje, Sylvia Menzel, Peter Bankhead, Matthias Mietsch, Charis Drummer, Michael Lehrke, Rafael Kramann, J√ºrgen Floege, Peter Boor, Dorit Merhof. ‚ÄúDeep Learning‚ÄìBased Segmentation and Quantification in Experimental Kidney Histopathology‚Äù; JASN Jan 2021 32 (1) 52-68; https://doi.org/10.1681/ASN.2020050597\n* Brandon Ginley, Brendon Lutnick, Kuang-Yu Jen, Agnes B. Fogo, Sanjay Jain, Avi Rosenberg, Vighnesh Walavalkar, Gregory Wilding, John E. Tomaszewski, Rabi Yacoub, Giovanni Maria Rossi, Pinaki Sarder.‚ÄúComputational Segmentation and Classification of Diabetic Glomerulosclerosis‚Äù; JASN Oct 2019, 30 (10) 1953-1967; https://doi.org/10.1681/ASN.2018121259","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}