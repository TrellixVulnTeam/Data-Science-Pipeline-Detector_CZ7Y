{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"text-align: center; font-family: Verdana; font-size: 32px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: #468282; background-color: #ffffff;\">HuBMAP + HPA - Hacking the Human Body</h1>\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">Segment multi-organ functional tissue units</h2>\n\n<img src=\"https://storage.googleapis.com/kaggle-competitions/kaggle/34547/logos/header.png\"> \n\n> # ðŸ“ŒIntroduction: \n>> A lot of segmentation competition is getting hosted in kaggle recently. This competition is very similar to the previous segmentation competitions, eg: [Sartorius CIS](https://www.kaggle.com/competitions/sartorius-cell-instance-segmentation/overview), and ongoing [UW-Madison GI Tract Image Segmentation](https://www.kaggle.com/competitions/uw-madison-gi-tract-image-segmentation) also felt similar [but no so much], from the POV of medical data, segmentation problem statement. You might start with different models used in those above competitions, and learn a few frameworks used like MMdet, detectron2 etc. The evaluation metric is mean Dice coefficient, don't really know why mean?[maybe because mean over all the segments found in a single image]. But other than that Dice coefficient is a very popular metric for image segmentation. if you don't know, you might want to check out this [NB](https://www.kaggle.com/code/yerramvarun/understanding-dice-coefficient). Use models like Unet, SegNet, Enet etc. type encoder decoder model for baseline, then move on to more complex models and pre-processing and post-processing techniques. Follow the augmentations used in the previous competitions, and do trial and error for fitting those augmentations to the model, or come up with some new one. \n\n>> Data for this competition comes from two different consortiums, the Human Protein Atlas (HPA) and Human BioMolecular Atlas Program (HUBMAP). As mentioned in the Data tab, one of the main challenges of this competition will be adapting models to function properly when presented with data collected using a different protocol. Because among the three datasets, the training set contains data from public HPAs, the public test set is a combination of private HPAs and HuBMAP data, and the private test set contains only HuBMAP data. Image resolution is high, though the number of training images are very small(351).\n\n>> I plan to publish three different parts which will include data prep, model training, model inference. Its a very basic version of the NB, will try to improve over time.","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-22T21:35:44.375098Z","iopub.execute_input":"2022-06-22T21:35:44.375558Z","iopub.status.idle":"2022-06-22T21:35:44.382234Z","shell.execute_reply.started":"2022-06-22T21:35:44.375508Z","shell.execute_reply":"2022-06-22T21:35:44.381246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DIR = \"../input/hubmap-organ-segmentation\"\ntrain_df = pd.read_csv(os.path.join(DIR,\"train.csv\"))\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:22:25.757598Z","iopub.execute_input":"2022-06-22T20:22:25.758338Z","iopub.status.idle":"2022-06-22T20:22:26.160981Z","shell.execute_reply.started":"2022-06-22T20:22:25.758296Z","shell.execute_reply":"2022-06-22T20:22:26.16018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T22:31:48.770414Z","iopub.execute_input":"2022-06-22T22:31:48.77083Z","iopub.status.idle":"2022-06-22T22:31:48.801085Z","shell.execute_reply.started":"2022-06-22T22:31:48.770797Z","shell.execute_reply":"2022-06-22T22:31:48.799834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# source: https://www.kaggle.com/code/julian3833/sartorius-starter-torch-mask-r-cnn-lb-0-273 w/ a bit change\ndef rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape).T","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:58:42.524468Z","iopub.execute_input":"2022-06-22T20:58:42.524885Z","iopub.status.idle":"2022-06-22T20:58:42.533236Z","shell.execute_reply.started":"2022-06-22T20:58:42.524854Z","shell.execute_reply":"2022-06-22T20:58:42.532011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking the training data:","metadata":{}},{"cell_type":"code","source":"# train_df[\"rle\"].iloc[0]\n\nfor i in np.random.choice(200,5):\n    rle_img = rle_decode(train_df[\"rle\"].iloc[i],(train_df[\"img_height\"].iloc[i],train_df[\"img_width\"].iloc[i]))\n    img_dir = os.path.join(\"../input/hubmap-organ-segmentation/train_images\" , str(train_df[\"id\"].iloc[i]) + '.tiff')\n#     print(img_dir)\n    img = plt.imread(img_dir)\n\n    plt.figure(figsize=(16,18))\n    plt.subplot(1,2,1)\n    plt.imshow(img)\n    plt.title(f\"id: {i} image\")\n\n    plt.subplot(1,2,2)\n    plt.imshow(rle_img)\n    plt.title(f\"id: {i} mask\");","metadata":{"execution":{"iopub.status.busy":"2022-06-22T21:53:04.92597Z","iopub.execute_input":"2022-06-22T21:53:04.926727Z","iopub.status.idle":"2022-06-22T21:53:20.300601Z","shell.execute_reply.started":"2022-06-22T21:53:04.926679Z","shell.execute_reply":"2022-06-22T21:53:20.297944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparing `train_annotations` folder data with RLE: ","metadata":{}},{"cell_type":"markdown","source":"- Reading the RLE from `.csv` and creating the mask.","metadata":{}},{"cell_type":"code","source":"i = 0\nrle_img = rle_decode(train_df[\"rle\"].iloc[i],(train_df[\"img_height\"].iloc[i],train_df[\"img_width\"].iloc[i]))\nimg_dir = os.path.join(\"../input/hubmap-organ-segmentation/train_images\" , str(train_df[\"id\"].iloc[i]) + '.tiff')\n#     print(img_dir)\nimg = plt.imread(img_dir)\n\nplt.figure(figsize=(16,18))\nplt.subplot(1,2,1)\nplt.imshow(img)\n\nplt.subplot(1,2,2)\nplt.imshow(rle_img);","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:59:25.284613Z","iopub.execute_input":"2022-06-22T20:59:25.284977Z","iopub.status.idle":"2022-06-22T20:59:27.620277Z","shell.execute_reply.started":"2022-06-22T20:59:25.284946Z","shell.execute_reply":"2022-06-22T20:59:27.619009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Reading the polygon ploints from json.","metadata":{}},{"cell_type":"code","source":"with open(\"../input/hubmap-organ-segmentation/train_annotations/10044.json\") as rle_json:\n    data = json.load(rle_json)\n    \nprint(data.__len__())","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:59:13.976202Z","iopub.execute_input":"2022-06-22T20:59:13.97676Z","iopub.status.idle":"2022-06-22T20:59:14.518743Z","shell.execute_reply.started":"2022-06-22T20:59:13.976717Z","shell.execute_reply":"2022-06-22T20:59:14.517446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = np.zeros((3000,3000))\nfor i in range(len(data)):\n    image = cv2.fillPoly(image, pts = [np.array(data[i])], color =(255,255,255))\n\nplt.figure(figsize=(16,18))\nplt.subplot(1,2,1)\nplt.imshow(img)\n\nplt.subplot(1,2,2)\nplt.imshow(image);","metadata":{"execution":{"iopub.status.busy":"2022-06-22T20:59:54.018762Z","iopub.execute_input":"2022-06-22T20:59:54.019338Z","iopub.status.idle":"2022-06-22T20:59:56.103012Z","shell.execute_reply.started":"2022-06-22T20:59:54.019293Z","shell.execute_reply":"2022-06-22T20:59:56.101555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Both are same","metadata":{}},{"cell_type":"markdown","source":"# Saving the masks in folder:","metadata":{}},{"cell_type":"code","source":"folder1 = \"/kaggle/working/train_masks\"\nfolder2 = \"/kaggle/working/train_masks_np\"\n\nif not os.path.isdir(folder1):\n    os.mkdir(folder1)\n    \nif not os.path.isdir(folder2):\n    os.mkdir(folder2)\n    \n    \nfor i in tqdm(range(len(train_df))):\n    rle_img = rle_decode(train_df[\"rle\"].iloc[i],(train_df[\"img_height\"].iloc[i],train_df[\"img_width\"].iloc[i]))\n    f_name1 = os.path.join(folder1, str(train_df[\"id\"].iloc[i])+'.png')\n    f_name2 = os.path.join(folder2, str(train_df[\"id\"].iloc[i])+'.npy')\n\n    cv2.imwrite(f_name1,rle_img)\n    np.save(f_name2, rle_img)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-22T21:35:52.798459Z","iopub.execute_input":"2022-06-22T21:35:52.799579Z","iopub.status.idle":"2022-06-22T21:36:46.385369Z","shell.execute_reply.started":"2022-06-22T21:35:52.799523Z","shell.execute_reply":"2022-06-22T21:36:46.38422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(np.load(os.path.join(folder2,\"10610.npy\")))","metadata":{"execution":{"iopub.status.busy":"2022-06-22T21:46:56.670567Z","iopub.execute_input":"2022-06-22T21:46:56.671004Z","iopub.status.idle":"2022-06-22T21:46:57.742773Z","shell.execute_reply.started":"2022-06-22T21:46:56.670961Z","shell.execute_reply":"2022-06-22T21:46:57.741649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(plt.imread(os.path.join(folder1,\"10610.png\")))","metadata":{"execution":{"iopub.status.busy":"2022-06-22T21:47:01.525103Z","iopub.execute_input":"2022-06-22T21:47:01.525474Z","iopub.status.idle":"2022-06-22T21:47:02.577173Z","shell.execute_reply.started":"2022-06-22T21:47:01.525442Z","shell.execute_reply":"2022-06-22T21:47:02.576331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.listdir(\"./train_masks_np\").__len__(), os.listdir(\"./train_masks\").__len__()","metadata":{"execution":{"iopub.status.busy":"2022-06-22T21:47:18.574565Z","iopub.execute_input":"2022-06-22T21:47:18.574941Z","iopub.status.idle":"2022-06-22T21:47:18.58351Z","shell.execute_reply.started":"2022-06-22T21:47:18.57491Z","shell.execute_reply":"2022-06-22T21:47:18.582429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> # â­• WORK IN PROGRESS ! ! !\n<p align=\"center\">\n<img src=\"https://media.giphy.com/media/xThuWu82QD3pj4wvEQ/giphy.gif\" width=\"300\">\n</p>","metadata":{}}]}