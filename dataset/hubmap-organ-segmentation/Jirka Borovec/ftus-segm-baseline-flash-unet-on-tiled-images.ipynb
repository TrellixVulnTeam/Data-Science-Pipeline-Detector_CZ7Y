{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FTUs‚öïÔ∏èSegm: EDAüîé & baseline Lightning‚ö°Flash on tiled images\n\nThis is derived from Flash docs and paralele competition: https://www.kaggle.com/code/jirkaborovec/tract-segm-eda-flash-deeplab-albumentation","metadata":{}},{"cell_type":"code","source":"!pip uninstall -y torchtext\n# !pip install -q --upgrade torch torchvision\n!mkdir -p frozen_packages\n!cp ../input/starter-flash-semantic-segmentation/frozen_packages/* frozen_packages/\n!cp ../input/ftus-segm-eda-viewer/frozen_packages/* frozen_packages/\n!pip install -q \"lightning-flash[image]\" \"torchmetrics<0.8\" --no-index --find-links frozen_packages/\n!pip install -q -U timm segmentation-models-pytorch --no-index --find-links frozen_packages/\n!pip install -q 'kaggle-image-segmentation' --no-index --find-links frozen_packages/\n\n! pip list | grep -e torch -e lightning\n! nvidia-smi -L","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-29T22:35:30.765385Z","iopub.execute_input":"2022-06-29T22:35:30.766125Z","iopub.status.idle":"2022-06-29T22:37:15.462678Z","shell.execute_reply.started":"2022-06-29T22:35:30.766029Z","shell.execute_reply":"2022-06-29T22:37:15.461487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading dataset\n\nIn this case we are using generated segmentation mask exported in this dataset: https://www.kaggle.com/datasets/jirkaborovec/hacking-the-human-body-annotation-masks\n\nand generated from following EDA kernel: https://www.kaggle.com/code/jirkaborovec/ftus-segm-eda-export-rle-mask","metadata":{}},{"cell_type":"code","source":"import os, glob\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nDATASET_FOLDER = \"/kaggle/input/hubmap-organ-segmentation\"\nANNOT_DATASET = \"/kaggle/input/hacking-the-human-body-annotation-masks\"\npath_csv = os.path.join(DATASET_FOLDER, \"train.csv\")\ndf_train = pd.read_csv(path_csv)\ndisplay(df_train.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:37:15.465317Z","iopub.execute_input":"2022-06-29T22:37:15.465872Z","iopub.status.idle":"2022-06-29T22:37:15.807254Z","shell.execute_reply.started":"2022-06-29T22:37:15.465826Z","shell.execute_reply":"2022-06-29T22:37:15.806153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv(os.path.join(DATASET_FOLDER, \"test.csv\"))\n\ndisplay(df_test.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:37:15.808769Z","iopub.execute_input":"2022-06-29T22:37:15.809146Z","iopub.status.idle":"2022-06-29T22:37:15.831244Z","shell.execute_reply.started":"2022-06-29T22:37:15.809109Z","shell.execute_reply":"2022-06-29T22:37:15.83012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ls = glob.glob(os.path.join(DATASET_FOLDER, 'test_images', '*'))\nWITH_SUBMISSION = len(ls) > 1\n\nfor fname in ls[:2]:\n    plt.imshow(plt.imread(fname))","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:37:15.835908Z","iopub.execute_input":"2022-06-29T22:37:15.836216Z","iopub.status.idle":"2022-06-29T22:37:16.750459Z","shell.execute_reply.started":"2022-06-29T22:37:15.836189Z","shell.execute_reply":"2022-06-29T22:37:16.749416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make a grid/tiles","metadata":{}},{"cell_type":"code","source":"!mkdir -p /kaggle/temp/images\n!mkdir -p /kaggle/temp/masks","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:37:16.751651Z","iopub.execute_input":"2022-06-29T22:37:16.752888Z","iopub.status.idle":"2022-06-29T22:37:18.373479Z","shell.execute_reply.started":"2022-06-29T22:37:16.752831Z","shell.execute_reply":"2022-06-29T22:37:18.371851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\n\ndef tile_image(p_img, folder, size: int = 1024) -> list:\n    w = h = size\n    im = np.array(Image.open(p_img))\n    # https://stackoverflow.com/a/47581978/4521646\n    tiles = [im[i:(i + h), j:(j + w), ...] for i in range(0, im.shape[0], h) for j in range(0, im.shape[1], w)]\n    idxs = [(i, (i + h), j, (j + w)) for i in range(0, im.shape[0], h) for j in range(0, im.shape[1], w)]\n    name, _ = os.path.splitext(os.path.basename(p_img))\n    files = []\n    for k, tile in enumerate(tiles):\n        if tile.shape[:2] != (h, w):\n            tile_ = tile\n            tile = np.zeros_like(tiles[0])\n            tile[:tile_.shape[0], :tile_.shape[1], ...] = tile_\n        p_img = os.path.join(folder, f\"{name}_{k:02}.png\")\n        Image.fromarray(tile).save(p_img)\n        files.append(p_img)\n    return files, idxs\n\n\ntiles_img, _ = tile_image(\"../input/hubmap-organ-segmentation/train_images/12233.tiff\", \"/kaggle/temp/images\", size=1024)\ntiles_seg, idxs = tile_image(\"../input/hacking-the-human-body-annotation-masks/train_masks/12233.png\", \"/kaggle/temp/masks\", size=1024)\n\n!ls -lh /kaggle/temp/images\n!ls -lh /kaggle/temp/masks","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-06-29T22:37:18.378559Z","iopub.execute_input":"2022-06-29T22:37:18.379233Z","iopub.status.idle":"2022-06-29T22:37:24.189036Z","shell.execute_reply.started":"2022-06-29T22:37:18.379194Z","shell.execute_reply":"2022-06-29T22:37:24.187764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Show the image tiles with segmentations","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom skimage import color\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(9, 9))\nfor i, (p_img, p_seg) in enumerate(zip(tiles_img, tiles_seg)):\n    img = plt.imread(p_img)\n    mask = np.array(Image.open(p_seg))\n    axes[i // 3, i % 3].imshow(color.label2rgb(mask, img, bg_label=0, bg_color=(1.,1.,1.), alpha=0.25))\n    axes[i // 3, i % 3].set_axis_off()\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:37:24.190759Z","iopub.execute_input":"2022-06-29T22:37:24.191452Z","iopub.status.idle":"2022-06-29T22:37:29.578551Z","shell.execute_reply.started":"2022-06-29T22:37:24.191411Z","shell.execute_reply":"2022-06-29T22:37:29.577604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Back recosntruction","metadata":{}},{"cell_type":"code","source":"tiles = [np.array(Image.open(p_seg)) for p_seg in tiles_seg]\nim = plt.imread(\"../input/hubmap-organ-segmentation/train_images/12233.tiff\")\nseg = np.zeros(im.shape[:2], dtype=np.uint8)\nfor tile, (i1, i2, j1, j2) in zip(tiles, idxs):\n    i2 = min(i2, im.shape[0])\n    j2 = min(j2, im.shape[1])\n    seg[i1:i2, j1:j2] = tile[:(i2 - i1), :(j2 - j1)]\nplt.imshow(seg)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:37:29.580328Z","iopub.execute_input":"2022-06-29T22:37:29.581007Z","iopub.status.idle":"2022-06-29T22:37:30.621235Z","shell.execute_reply.started":"2022-06-29T22:37:29.580968Z","shell.execute_reply":"2022-06-29T22:37:30.620203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Process dataset","metadata":{}},{"cell_type":"code","source":"from tqdm.auto import tqdm\nfrom joblib import Parallel, delayed\n\nTILE_SIZE = 1024\n\nfor dir_source, dir_target in [\n    (os.path.join(DATASET_FOLDER, 'train_images'), \"/kaggle/temp/images\"),\n    (os.path.join(ANNOT_DATASET, 'train_masks'), \"/kaggle/temp/masks\"),\n]:\n    ls = glob.glob(os.path.join(dir_source, '*'))\n    _= Parallel(n_jobs=3)(\n        delayed(tile_image)(p_img, dir_target, size=TILE_SIZE) for p_img in tqdm(ls)\n    )","metadata":{"execution":{"iopub.status.busy":"2022-06-29T22:37:30.622857Z","iopub.execute_input":"2022-06-29T22:37:30.623458Z","iopub.status.idle":"2022-06-29T22:53:59.173132Z","shell.execute_reply.started":"2022-06-29T22:37:30.623413Z","shell.execute_reply":"2022-06-29T22:53:59.172045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lightning‚ö°Flash & Unet++\n\nlets follow the Semantinc segmentation example: https://lightning-flash.readthedocs.io/en/stable/reference/semantic_segmentation.html","metadata":{}},{"cell_type":"code","source":"import torch\n\nimport flash\nimport numpy as np\nfrom flash.core.data.utils import download_data\nfrom flash.image import SemanticSegmentation, SemanticSegmentationData","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-29T22:53:59.177756Z","iopub.execute_input":"2022-06-29T22:53:59.178077Z","iopub.status.idle":"2022-06-29T22:54:12.477076Z","shell.execute_reply.started":"2022-06-29T22:53:59.178048Z","shell.execute_reply":"2022-06-29T22:54:12.47586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Create the DataModule","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = (384, 384)\n\ndatamodule = SemanticSegmentationData.from_folders(\n    train_folder=\"/kaggle/temp/images\",\n    train_target_folder=\"/kaggle/temp/masks\",\n    val_split=0.01 if WITH_SUBMISSION else 0.2,\n    predict_folder=os.path.join(DATASET_FOLDER, 'test_images'),\n    transform_kwargs=dict(image_size=IMAGE_SIZE),\n    num_classes=2,\n    batch_size=12,\n    num_workers=2,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:18:34.763327Z","iopub.execute_input":"2022-06-29T23:18:34.7638Z","iopub.status.idle":"2022-06-29T23:18:34.823224Z","shell.execute_reply.started":"2022-06-29T23:18:34.763764Z","shell.execute_reply":"2022-06-29T23:18:34.82149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axarr = plt.subplots(ncols=2, nrows=datamodule.batch_size, figsize=(8, 4 * datamodule.batch_size))\n\nfor batch in datamodule.train_dataloader():\n    for i in range(len(batch['input'])):\n        segm = batch['target'][i].numpy()\n        img = np.rollaxis(batch['input'][i].cpu().numpy(), 0, 3)\n        axarr[i, 0].imshow(img)\n        seg = axarr[i, 1].imshow(segm, vmin=0, vmax=1)\n        plt.colorbar(seg, ax=axarr[i, 1])\n    break","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:18:34.82521Z","iopub.execute_input":"2022-06-29T23:18:34.825496Z","iopub.status.idle":"2022-06-29T23:18:43.500722Z","shell.execute_reply.started":"2022-06-29T23:18:34.82546Z","shell.execute_reply":"2022-06-29T23:18:43.499745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Build the task","metadata":{}},{"cell_type":"code","source":"from pprint import pprint\n\npprint(SemanticSegmentation.available_heads())\npprint(SemanticSegmentation.available_backbones()['unetplusplus'])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-29T23:18:43.502765Z","iopub.execute_input":"2022-06-29T23:18:43.503412Z","iopub.status.idle":"2022-06-29T23:18:43.513173Z","shell.execute_reply.started":"2022-06-29T23:18:43.503371Z","shell.execute_reply":"2022-06-29T23:18:43.512281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import segmentation_models_pytorch as smp\n\nmodel = SemanticSegmentation(\n    backbone=\"efficientnet-b4\",\n    head=\"unetplusplus\",\n    pretrained=False,\n    optimizer=\"Adamax\",\n    learning_rate=0.05,\n    lr_scheduler=(\"StepLR\", {\"step_size\": 1500}),\n    # loss_fn=smp.losses.DiceLoss(mode='binary'),\n    num_classes=datamodule.num_classes,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:18:43.515971Z","iopub.execute_input":"2022-06-29T23:18:43.516579Z","iopub.status.idle":"2022-06-29T23:18:43.820269Z","shell.execute_reply.started":"2022-06-29T23:18:43.516544Z","shell.execute_reply":"2022-06-29T23:18:43.81921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Create the trainer and finetune the model","metadata":{}},{"cell_type":"code","source":"import pytorch_lightning as pl\n\ntrainer = flash.Trainer(\n    max_epochs=20 if WITH_SUBMISSION else 10,\n    logger=pl.loggers.CSVLogger(save_dir='logs/'),\n    gpus=torch.cuda.device_count(),\n    precision=16 if torch.cuda.is_available() else 32,\n    accumulate_grad_batches=8,\n    gradient_clip_val=0.01,\n    limit_train_batches=1.0 if WITH_SUBMISSION else 0.5,\n    limit_val_batches=1.0 if WITH_SUBMISSION else 0.5,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T23:18:43.822066Z","iopub.execute_input":"2022-06-29T23:18:43.822436Z","iopub.status.idle":"2022-06-29T23:18:43.834553Z","shell.execute_reply.started":"2022-06-29T23:18:43.822398Z","shell.execute_reply":"2022-06-29T23:18:43.833202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\n# Train the model\ntrainer.finetune(model, datamodule=datamodule, strategy=\"no_freeze\")\n\n# Save the model!\ntrainer.save_checkpoint(\"semantic_segmentation_model.pt\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-29T23:18:43.836225Z","iopub.execute_input":"2022-06-29T23:18:43.836624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Show training progress","metadata":{}},{"cell_type":"code","source":"import seaborn as sn\n\nmetrics = pd.read_csv(f'{trainer.logger.log_dir}/metrics.csv')\ndel metrics[\"step\"]\nmetrics.set_index(\"epoch\", inplace=True)\n# display(metrics.dropna(axis=1, how=\"all\").head())\ng = sn.relplot(data=metrics, kind=\"line\")\nplt.gcf().set_size_inches(12, 4)\nplt.grid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Segment a few images!","metadata":{}},{"cell_type":"code","source":"sample_imgs = tiles_img[:5]\n\ndm = SemanticSegmentationData.from_files(\n    predict_files=sample_imgs,\n    transform_kwargs=dict(image_size=IMAGE_SIZE),\n    batch_size=3,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from itertools import chain\n\nnrows = max(2, len(sample_imgs))\nfig, axarr = plt.subplots(ncols=3, nrows=nrows, figsize=(15, 5 * nrows))\n\npreds = trainer.predict(model, datamodule=dm)\npreds = list(chain(*preds))\nfor i, pred in enumerate(preds):\n    # print(pred.keys())\n    img = np.rollaxis(pred['input'].cpu().numpy(), 0, 3)\n    print(img.dtype, img.min(), img.max())\n    axarr[i, 0].imshow(img)\n    for j, seg in enumerate(pred['preds'].cpu().numpy()):\n        p = axarr[i, j + 1].imshow(seg, vmin=-10, vmax=10)\n        plt.colorbar(p, ax=axarr[i, j + 1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference üî•","metadata":{}},{"cell_type":"code","source":"model = SemanticSegmentation.load_from_checkpoint(\n    \"semantic_segmentation_model.pt\"\n)\ntest_images = glob.glob(os.path.join(DATASET_FOLDER, \"test_images\", \"*.tiff\"))\nprint(f\"images: {len(test_images)}\")\n\n!rm /kaggle/temp/images/*\n!rm /kaggle/temp/masks/*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom itertools import chain\nfrom kaggle_imsegm.mask import rle_encode\nfrom torch.utils.data import DataLoader\n# from skimage.transform import rescale, resize\n\ndf_test['pixel_size'] =  df_test['pixel_size'].fillna(0.4)\n\npreds = []\nfor _, row in df_test.iterrows():\n    scale = row[\"pixel_size\"] / 0.4\n    test_img = os.path.join(DATASET_FOLDER, \"test_images\", f\"{row['id']}.tiff\")\n    im = plt.imread(test_img)\n    \n    # perform scaling on level tiles as the input is scaled to the CNN input size anyway\n    tiles_img, idxs = tile_image(test_img, \"/kaggle/temp/images\", size=int(TILE_SIZE / scale))\n    dm = SemanticSegmentationData.from_files(\n        predict_files=tiles_img,\n        # predict_transform=SemanticSegmentationInputTransform,\n        transform_kwargs=dict(image_size=IMAGE_SIZE),\n        num_classes=2,\n        batch_size=3,\n        num_workers=2,\n    )\n    pred = trainer.predict(model, datamodule=dm, output=\"labels\")\n    pred = list(chain(*pred))\n    \n    seg = np.zeros(im.shape[:2], dtype=np.uint8)\n    for tile, (i1, i2, j1, j2) in zip(pred, idxs):\n        i2 = min(i2, im.shape[0])\n        j2 = min(j2, im.shape[1])\n        seg[i1:i2, j1:j2] = np.array(tile, dtype=np.uint8)[:(i2 - i1), :(j2 - j1)]\n    # seg = resize(seg * 255, img.shape[:2], order=0) / 255\n    \n    rle = rle_encode(seg.T) if np.sum(seg) > 1 else {}\n    name, _ = os.path.splitext(os.path.basename(test_img))\n    preds.append({\"id\": row['id'], \"rle\": rle.get(1, \"\")})\n\ndf_pred = pd.DataFrame(preds)\ndisplay(df_pred[df_pred[\"rle\"] != \"\"].head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Finalize submissions","metadata":{}},{"cell_type":"code","source":"df_ssub = pd.read_csv(os.path.join(DATASET_FOLDER, \"sample_submission.csv\"))\ndel df_ssub['rle']\ndf_pred = df_ssub.merge(df_pred, on='id')\n\ndf_pred[['id', 'rle']].to_csv(\"submission.csv\", index=False)\n\n!head submission.csv","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]}]}