{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style=\"font-family: Verdana; font-size: 28px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #CCCCFF; color: black;\"><center><br>HuBMAP + HPA üëÄ: Exploratory Data Analysis üî¨</center></h1>\n                                                      \n<center><img src = \"https://storage.googleapis.com/kaggle-competitions/kaggle/34547/logos/header.png?t=2022-02-15-22-37-27\" width = \"1000\" height = \"500\"/></center>   \n\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: NGHI HUYNH and YUAN HONG</h5>","metadata":{}},{"cell_type":"markdown","source":"<p id=\"toc\"></p>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #CCCCFF; color: black;\" role=\"tab\" aria-controls=\"home\"><center><br>CONTENTS</center></h2>\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 16px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\"><a href=\"#background\">0&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 16px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\"><a href=\"#imports\">1&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 16px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\"><a href=\"#img_viz\">2&nbsp;&nbsp;&nbsp;&nbsp;IMAGE + MASK VISUALIZATION</a></h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 16px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: black; background-color: #ffffff;\"><a href=\"#metadata\">3&nbsp;&nbsp;&nbsp;&nbsp;METADATA ANALYSIS</a></h3>\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:#F08080; border:0; color:black' role=\"tab\" aria-controls=\"home\"><center><br>If you find this notebook useful, do give us an upvote, it motivates us a lot.<br><br> This notebook is still a work in progress. Keep checking for further developments!üòä</center></h3>","metadata":{}},{"cell_type":"markdown","source":"\n<a id=\"background\"></a>\n\n<h2 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #CCCCFF; color: black;\" id=\"background\"><center><br>0. BACKGROUND INFORMATION <a href=\"#toc\">&#10514;</a></center> </h2>\n\n# Task description:\n\nIn this competition, we'll **identify and segment functional tissue units (FTUs)** accross **five** human organs:\n\n* Prostate\n* Spleen\n* Lung\n* Kidney\n* Large Intestine\n\nThe challenge in this competition is to build algorithms that **generalize**:\n* across different **organs** and\n* across different **dataset** differences\n\n=> This is a **semantic segmentation** problem.\n\n# Data description:\n\nThey only release public *Human Protein Atlas (HPA)* data for the training dataset. However, they will release private *HPA* data and *Human BioMolecular Atlas Program (HuBMAP)* for their public test set. For the private test set, they only use *HuBMAP* data.\n\n**File Information**:\n1. `train|test.csv`:\n* `id` - The image ID.\n* `organ` - The organ that the biopsy sample was taken from.\n* `data_source` - Whether the image was provided by Hubamp or HPA.\n* `img_height` - The height of the image in pixels.\n* `img_width` - The width of the image in pixels.\n* `pixel_size` - The height/width of a single pixel from this image in micrometers. All HPA images have a pixel size of 0.4 ¬µm. For Hubmap imagery the pixel size is 0.5 ¬µm for kidney, 0.2290 ¬µm for large intestine, 0.7562 ¬µm for lung, 0.4945 ¬µm for spleen, and 6.263 ¬µm for prostate.\n* `tissue_thickness` - The thickness of the biopsy sample in micrometers. All HPA images have a thickness of 4 ¬µm. The Hubmap samples have tissue slice thicknesses 10 ¬µm for kidney, 8 ¬µm for large intestine, 4 ¬µm for spleen, 5 ¬µm for lung, and 5 ¬µm for prostate.\n* `rle` - The target column. A run length encoded copy of the annotations. Provided for the training set only.\n* `age` - The patient's age in years. Provided for the training set only.\n* `sex` - The sex of the patient. Provided for the training set only.\n2. `train_annotations`: provided in the format of **points that define the boundaries of the polygon masks of the FTUs**\n3. `train|test_images`: the images:\n* Expect roughly 550 images in the hidden test set.\n* All images used have at least one FTU.\n* All tissue data used in this competition is from healthy donors that pathologists identified as pathologically unremarkable tissue.\n* HPA details:\n    * All HPA images are 3000 x 3000 pixels with a tissue area within the image around 2500 x 2500 pixels.\n    * HPA samples were stained with antibodies visualized with 3,3'-diaminobenzidine (DAB) and counterstained with hematoxylin.\n* HuBMAP details:\n    * The Hubmap images range in size from 4500x4500 down to 160x160 pixels.\n    * HuBMAP images were prepared using Periodic acid-Schiff (PAS)/hematoxylin and eosin (H&E) stains.\n4. `sample_submission.csv`:\n* `id`-the image ID\n* `rle`-a run length encoded mask of the FTUs in the image\n\n# Evaluation metric:\n\nThis competition is evaluated on the mean [Dice coefficient](https://radiopaedia.org/articles/dice-similarity-coefficient#:~:text=The%20Dice%20similarity%20coefficient%2C%20also,between%20two%20sets%20of%20data.). The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n\n$$\\frac{2‚àó|ùëã‚à©ùëå|}{|ùëã|+|ùëå|}$$\n\nwhere \n* X is the predicted set of pixels and Y is the ground truth. \n* The Dice coefficient is defined to be 1 when both X and Y are empty. \n\n**Note**: metric is to judge the performance of the model, whereas loss function is to optimize the model.\n\nIn this case, our metric is the **mean Dice coefficient**, and we can use different loss functions like \n* **Dice Loss**\n* **Jaccard Loss**\n* **BCE Loss**\n* **Lovasz Loss**\n* **Tversky Loss**\n\nto optimize our models\n\n# Submission file format\n\nTo reduce the submission file size, the metric uses run-length encoding on the pixel values.\n\nInstead of submitting an exhaustive list of indices for our segmentation, we will submit pairs of values that contain a start position and a run length\n\nE.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\nNote that, at the time of encoding, the mask should be **binary**\n* The masks for all objects in an image are joined into a single large mask\n* The value of 0 should indicate pixels that are not masked\n* The value of 1 will indicate pixels that are masked.\n\n# Methods:\n\n1. Overview:\n    * **Run-length encoding (RLE)**: a form of lossless data compression.\n    \n    Since we already have the RLE masks, we don't need to use the annotations from `.json` file.\n    * **Given**: images (`.tiff`), masks in RLE (we need to convert RLE to binary mask before feeding to our models)\n    * **Predict**: masks then convert to RLE for submission\n2. Data processing:\n    * Resize\n    * Normalize\n3. Data augmentation:\n4. Baseline model: \n    * UNET\n\n5. Testing model:\n    * UNET + pretrained model from previous competition","metadata":{}},{"cell_type":"markdown","source":"<a id=\"imports\"></a>\n\n<h2 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #CCCCFF; color: black;\" id=\"imports\"><center><br>1. IMPORTS <a href=\"#toc\">&#10514;</a></center> </h2>","metadata":{}},{"cell_type":"code","source":"import os \nimport glob\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\nimport plotly\nfrom plotly import tools\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.offline as pyo\nimport plotly.io as pio\nimport plotly.graph_objects as go\n#pio.templates.default = 'plotly_white'\nsns.set_theme(style=\"dark\")\nimport cv2\nimport tifffile as tiff\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:25:39.9172Z","iopub.execute_input":"2022-06-27T21:25:39.918138Z","iopub.status.idle":"2022-06-27T21:25:41.575875Z","shell.execute_reply.started":"2022-06-27T21:25:39.917895Z","shell.execute_reply":"2022-06-27T21:25:41.574491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Global config","metadata":{}},{"cell_type":"code","source":"class config:\n    BASE_PATH = Path(\"../input/hubmap-organ-segmentation/\")\n    TRAIN_CSV_PATH = BASE_PATH / \"train.csv\"\n    TRAIN_IMAGES_PATH = BASE_PATH / \"train_images/\"\n    TRAIN_ANNOTATIONS_PATH = BASE_PATH / \"train_annotations/\" # Not needed","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:26:40.973712Z","iopub.execute_input":"2022-06-27T21:26:40.974228Z","iopub.status.idle":"2022-06-27T21:26:40.980042Z","shell.execute_reply.started":"2022-06-27T21:26:40.974177Z","shell.execute_reply":"2022-06-27T21:26:40.979086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load datasets","metadata":{}},{"cell_type":"markdown","source":"## Train dataset","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(config.TRAIN_CSV_PATH)\ndata.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-27T21:26:44.04893Z","iopub.execute_input":"2022-06-27T21:26:44.049407Z","iopub.status.idle":"2022-06-27T21:26:44.252222Z","shell.execute_reply.started":"2022-06-27T21:26:44.049364Z","shell.execute_reply":"2022-06-27T21:26:44.250958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train images\n\nLoad a single image from `train_images`","metadata":{}},{"cell_type":"code","source":"id_ = 10274\nimg = tiff.imread(str(config.TRAIN_IMAGES_PATH/ f\"{id_}.tiff\"))\nprint(img.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:26:46.880894Z","iopub.execute_input":"2022-06-27T21:26:46.882077Z","iopub.status.idle":"2022-06-27T21:26:46.932708Z","shell.execute_reply.started":"2022-06-27T21:26:46.882029Z","shell.execute_reply":"2022-06-27T21:26:46.931604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To enhance image contrast, we apply a method from the [LAB color space](https://www.xrite.com/blog/lab-color-space)\n\nLAB color space expresses color variations across three channels. One channel for brightness and two channels for color:\n\n* L-channel: representing lightness in the image\n* a-channel: representing change in color between red and green\n* b-channel: representing change in color between yellow and blue\n\nWe perform adaptive histogram equalization on the L-channel, and convert the image back to RGB color space.","metadata":{}},{"cell_type":"code","source":"# https://stackoverflow.com/questions/39308030/how-do-i-increase-the-contrast-of-an-image-in-python-opencv\n# Enhance image contrast\nsample = img.copy()\nlab= cv2.cvtColor(sample, cv2.COLOR_BGR2LAB)\nl_channel, a, b = cv2.split(lab)\n# Applying CLAHE to L-channel\n# feel free to try different values for the limit and grid size:\nclahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(12,12))\ncl = clahe.apply(l_channel)\n\n# merge the CLAHE enhanced L-channel with the a and b channel\nlimg = cv2.merge((cl,a,b))\n\n# Converting image from LAB Color model to BGR color space\nenhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n\nplt.figure(figsize=(10, 10))\n# Stacking the original image with the enhanced image\nresult = np.hstack((img, enhanced_img))\nplt.title('Image Enhancement\\n [Left]: Original [Right]: Enhanced', fontsize=14)\nplt.imshow(result)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:26:49.943859Z","iopub.execute_input":"2022-06-27T21:26:49.944249Z","iopub.status.idle":"2022-06-27T21:26:52.78237Z","shell.execute_reply.started":"2022-06-27T21:26:49.944216Z","shell.execute_reply":"2022-06-27T21:26:52.781154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot image","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:26:55.996376Z","iopub.execute_input":"2022-06-27T21:26:55.996768Z","iopub.status.idle":"2022-06-27T21:26:57.394376Z","shell.execute_reply.started":"2022-06-27T21:26:55.996737Z","shell.execute_reply":"2022-06-27T21:26:57.393225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train annotations","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/paulorzp/rle-functions-run-length-encode-decode\ndef mask2rle(img): # encoder\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n \ndef rle2mask(mask_rle, shape=(1600,256)): # decoder\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (width,height) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:27:01.387482Z","iopub.execute_input":"2022-06-27T21:27:01.387885Z","iopub.status.idle":"2022-06-27T21:27:01.401355Z","shell.execute_reply.started":"2022-06-27T21:27:01.387851Z","shell.execute_reply":"2022-06-27T21:27:01.400214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = rle2mask(data[data[\"id\"]==id_][\"rle\"].values[0], (img.shape[1], img.shape[0]))\nmask.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:27:04.011327Z","iopub.execute_input":"2022-06-27T21:27:04.011765Z","iopub.status.idle":"2022-06-27T21:27:04.035133Z","shell.execute_reply.started":"2022-06-27T21:27:04.01173Z","shell.execute_reply":"2022-06-27T21:27:04.034224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot mask","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.imshow(mask, cmap='seismic', alpha=0.7)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:27:07.697086Z","iopub.execute_input":"2022-06-27T21:27:07.6975Z","iopub.status.idle":"2022-06-27T21:27:08.994989Z","shell.execute_reply.started":"2022-06-27T21:27:07.697463Z","shell.execute_reply":"2022-06-27T21:27:08.994004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"img_viz\"></a>\n\n<h2 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #CCCCFF; color: black;\" id=\"img_viz\"><center><br>2. IMAGE + MASK VISUALIZATION <a href=\"#toc\">&#10514;</center> </h2>","metadata":{}},{"cell_type":"markdown","source":"# Overlay image and mask","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.imshow(enhanced_img)\nplt.imshow(mask, cmap='seismic', alpha=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:27:17.355125Z","iopub.execute_input":"2022-06-27T21:27:17.355596Z","iopub.status.idle":"2022-06-27T21:27:19.855262Z","shell.execute_reply.started":"2022-06-27T21:27:17.355555Z","shell.execute_reply":"2022-06-27T21:27:19.854115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot mask on enhanced image\ndef plot_mask(image, mask, image_id):\n    plt.figure(figsize=(10, 10))\n    \n    lab= cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n    l_channel, a, b = cv2.split(lab)\n    # Applying CLAHE to L-channel\n    # feel free to try different values for the limit and grid size:\n    clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(12,12))\n    cl = clahe.apply(l_channel)\n\n    # merge the CLAHE enhanced L-channel with the a and b channel\n    limg = cv2.merge((cl,a,b))\n\n    # Converting image from LAB Color model to BGR color space\n    enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n    \n    \n    # subplot(nrows, ncols, index)\n    plt.subplot(2, 2, 1)\n    plt.imshow(image)\n    plt.title(f\"Image {image_id}\", fontsize=14)\n\n    plt.subplot(2, 2, 2)\n    plt.imshow(enhanced_img)\n    plt.title(f\"Enhanced Image {image_id}\", fontsize=14)\n    \n    plt.subplot(2, 2, 3)\n    plt.imshow(mask, cmap=\"seismic\", alpha =0.8)\n    plt.title(f\"Mask\", fontsize=14)    \n    \n    plt.subplot(2, 2, 4)\n    plt.imshow(enhanced_img)\n    plt.imshow(mask, cmap=\"seismic\", alpha=0.5)\n    plt.title(f\"Enhanced Image {image_id} + mask\", fontsize=14)    \n    \n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:27:41.027045Z","iopub.execute_input":"2022-06-27T21:27:41.028101Z","iopub.status.idle":"2022-06-27T21:27:41.040071Z","shell.execute_reply.started":"2022-06-27T21:27:41.028055Z","shell.execute_reply":"2022-06-27T21:27:41.03894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_mask(img, mask, id_)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:27:43.949655Z","iopub.execute_input":"2022-06-27T21:27:43.950101Z","iopub.status.idle":"2022-06-27T21:27:50.107708Z","shell.execute_reply.started":"2022-06-27T21:27:43.950063Z","shell.execute_reply":"2022-06-27T21:27:50.106453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Batches of images + masks grouped by organs\n\n* Prostate\n* Spleen\n* Lung\n* Kidney\n* Large Intestine","metadata":{}},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"code","source":"def read_image_mask(df, img_id):\n    image = tiff.imread(str(config.TRAIN_IMAGES_PATH/ f\"{img_id}.tiff\"))\n    \n    mask = rle2mask(\n        df[df[\"id\"] == img_id][\"rle\"].values[0], \n        (image.shape[1], image.shape[0])\n    )\n    return image, mask","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:27:50.109767Z","iopub.execute_input":"2022-06-27T21:27:50.110157Z","iopub.status.idle":"2022-06-27T21:27:50.118044Z","shell.execute_reply.started":"2022-06-27T21:27:50.110122Z","shell.execute_reply":"2022-06-27T21:27:50.116283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def image_info(df, img_id):\n    organ = df[df['id'] == img_id]['organ'].values[0]\n    pixel_size = df.loc[df['id'] == img_id]['pixel_size'].values[0]\n    tissue_thickness = df.loc[df['id'] == img_id]['tissue_thickness'].values[0]\n    age = df.loc[df['id'] == img_id]['age'].values[0]\n    sex = df.loc[df['id'] == img_id]['sex'].values[0]\n    image, mask_ = read_image_mask(df, img_id)\n    print('\\033[1m' + 'Case study : {}'.format(img_id) + '\\033[0m')\n    print('\\n------------------------------\\n')\n    print('\\033[1m' + 'General info: \\n' + '\\033[0m')\n    print('\\033[1m' + 'Organ: ' '\\033[0m' + f'{organ}')   \n    print('\\033[1m' + 'Age: ' '\\033[0m' + f'{age}')\n    print('\\033[1m' + 'Sex: ' '\\033[0m' + f'{sex}')\n    print('\\n------------------------------\\n')\n    print('\\033[1m' + 'Image + Mask info: \\n' + '\\033[0m')\n    print('\\033[1m' + 'Pixel size: ' '\\033[0m' + f'{pixel_size}')\n    print('\\033[1m' + 'Tissue thickness: ' '\\033[0m' + f'{tissue_thickness}')\n    print('\\033[1m' + 'Image shape: ' '\\033[0m' + f'{image.shape}')\n    print('\\033[1m' + 'Mask shape: ' '\\033[0m' + f'{mask_.shape}')\n    plot_mask(image, mask_, img_id)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:27:51.801059Z","iopub.execute_input":"2022-06-27T21:27:51.802316Z","iopub.status.idle":"2022-06-27T21:27:51.814035Z","shell.execute_reply.started":"2022-06-27T21:27:51.802265Z","shell.execute_reply":"2022-06-27T21:27:51.812828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prostate (Only male)","metadata":{}},{"cell_type":"code","source":"df_prostate = data.loc[data['organ']=='prostate']\ndf_prostate.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:27:54.274558Z","iopub.execute_input":"2022-06-27T21:27:54.27502Z","iopub.status.idle":"2022-06-27T21:27:54.296103Z","shell.execute_reply.started":"2022-06-27T21:27:54.274977Z","shell.execute_reply":"2022-06-27T21:27:54.294814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_info(df_prostate, 10044)\nimage_info(df_prostate, 10274)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:27:56.589966Z","iopub.execute_input":"2022-06-27T21:27:56.590403Z","iopub.status.idle":"2022-06-27T21:28:09.223296Z","shell.execute_reply.started":"2022-06-27T21:27:56.590367Z","shell.execute_reply":"2022-06-27T21:28:09.221155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Spleen","metadata":{}},{"cell_type":"code","source":"df_spleen = data.loc[data['organ']=='spleen']\ndf_spleen.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:28:09.226218Z","iopub.execute_input":"2022-06-27T21:28:09.226733Z","iopub.status.idle":"2022-06-27T21:28:09.254295Z","shell.execute_reply.started":"2022-06-27T21:28:09.226687Z","shell.execute_reply":"2022-06-27T21:28:09.253032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_info(df_spleen, 2279)\nimage_info(df_spleen, 10610)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:28:09.255719Z","iopub.execute_input":"2022-06-27T21:28:09.256219Z","iopub.status.idle":"2022-06-27T21:28:21.284805Z","shell.execute_reply.started":"2022-06-27T21:28:09.256178Z","shell.execute_reply":"2022-06-27T21:28:21.283857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lung","metadata":{}},{"cell_type":"code","source":"df_lung = data.loc[data['organ']=='lung']\ndf_lung.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:28:21.287211Z","iopub.execute_input":"2022-06-27T21:28:21.288269Z","iopub.status.idle":"2022-06-27T21:28:21.307709Z","shell.execute_reply.started":"2022-06-27T21:28:21.288222Z","shell.execute_reply":"2022-06-27T21:28:21.306379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_info(df_lung, 10488)\nimage_info(df_lung, 11629)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:28:21.309376Z","iopub.execute_input":"2022-06-27T21:28:21.309702Z","iopub.status.idle":"2022-06-27T21:28:32.980643Z","shell.execute_reply.started":"2022-06-27T21:28:21.309672Z","shell.execute_reply":"2022-06-27T21:28:32.979515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Kidney","metadata":{}},{"cell_type":"code","source":"df_kidney = data.loc[data['organ']=='kidney']\ndf_kidney.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:28:32.982191Z","iopub.execute_input":"2022-06-27T21:28:32.982574Z","iopub.status.idle":"2022-06-27T21:28:33.003084Z","shell.execute_reply.started":"2022-06-27T21:28:32.982538Z","shell.execute_reply":"2022-06-27T21:28:33.001651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_info(df_kidney, 1157)\nimage_info(df_kidney, 11497)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:28:33.004618Z","iopub.execute_input":"2022-06-27T21:28:33.005033Z","iopub.status.idle":"2022-06-27T21:28:45.251587Z","shell.execute_reply.started":"2022-06-27T21:28:33.004995Z","shell.execute_reply":"2022-06-27T21:28:45.250496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Large intestine","metadata":{}},{"cell_type":"code","source":"df_LI = data.loc[data['organ']=='largeintestine']\ndf_LI.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:28:45.253248Z","iopub.execute_input":"2022-06-27T21:28:45.253932Z","iopub.status.idle":"2022-06-27T21:28:45.276851Z","shell.execute_reply.started":"2022-06-27T21:28:45.253892Z","shell.execute_reply":"2022-06-27T21:28:45.275748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_info(df_LI, 1168)\nimage_info(df_LI, 11662)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:28:45.278449Z","iopub.execute_input":"2022-06-27T21:28:45.279197Z","iopub.status.idle":"2022-06-27T21:28:56.99119Z","shell.execute_reply.started":"2022-06-27T21:28:45.279151Z","shell.execute_reply":"2022-06-27T21:28:56.989983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"metadata\"></a>\n\n<h2 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #CCCCFF; color: black;\" id=\"metadata\"><center><br>3. METADATA ANALYSIS <a href=\"#toc\">&#10514;</center> </h2>","metadata":{}},{"cell_type":"markdown","source":"# Statistical Description","metadata":{}},{"cell_type":"code","source":"def EDA(df):\n    \n    print('\\033[1m' +'EXPLORATORY DATA ANALYSIS :'+ '\\033[0m\\n')\n    print('\\033[1m' + 'Shape of the data (rows, columns):' + '\\033[0m')\n    print(df.shape, \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'All columns from the dataframe :' + '\\033[0m')\n    print(df.columns, \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Datatypes and Missing values:' + '\\033[0m')\n    print(df.info(), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    for col in df.columns:\n        print('\\033[1m' + 'Unique values in {} :'.format(col) + '\\033[0m',len(data[col].unique()))\n    print('\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Summary statistics for the data :' + '\\033[0m')\n    print(df.describe(include='all'), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n        \n    print('\\033[1m' + 'Memory used by the data :' + '\\033[0m')\n    print(df.memory_usage(), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Number of duplicate values :' + '\\033[0m')\n    print(df.duplicated().sum())\n          \nEDA(data)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:30:02.327068Z","iopub.execute_input":"2022-06-27T21:30:02.327518Z","iopub.status.idle":"2022-06-27T21:30:02.470263Z","shell.execute_reply.started":"2022-06-27T21:30:02.327482Z","shell.execute_reply":"2022-06-27T21:30:02.468845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data visualization","metadata":{}},{"cell_type":"markdown","source":"## Univariate visualization of categorical variables (sex, organ)\n","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/toomuchsauce/mental-health-plotly-interactive-viz\ncolumns = ['organ','sex']\ndf = data[columns]\n\nbuttons = []\ni = 0\nvis = [False] * 4\n\nfor col in df.columns:\n    vis[i] = True\n    buttons.append({'label' : col,\n             'method' : 'update',\n             'args'   : [{'visible' : vis},\n             {'title'  : col}] })\n    i+=1\n    vis = [False] * 4\n\nfig = go.Figure()\n\nfor col in df.columns:\n    fig.add_trace(go.Pie(\n             values = df[col].value_counts(),\n             labels = df[col].value_counts().index,\n             title = dict(text = 'Distribution of {}'.format(col),\n                          font = dict(size=18, family = 'monospace'),\n                          ),\n             hole = 0.5,\n             hoverinfo='label+percent',))\n\nfig.update_traces(hoverinfo='label+percent',\n                  textinfo='label+percent',\n                  textfont_size=12,\n                  opacity = 0.8,\n                  showlegend = False,\n                  marker = dict(colors = sns.color_palette('Pastel1').as_hex(),\n                              line=dict(color='#000000', width=1)))\n              \n\nfig.update_layout(margin=dict(t=0, b=0, l=0, r=0),\n                  updatemenus = [dict(\n                        type = 'dropdown',\n                        x = 1.15,\n                        y = 0.85,\n                        showactive = True,\n                        active = 0,\n                        buttons = buttons)],\n                 annotations=[\n                             dict(text = \"<b>Choose<br>Column<b> : \",\n                             showarrow=False,\n                             x = 1.06, y = 0.92, yref = \"paper\", align = \"left\")])\n\nfor i in range(1,2):\n    fig.data[i].visible = False\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:30:05.482835Z","iopub.execute_input":"2022-06-27T21:30:05.483296Z","iopub.status.idle":"2022-06-27T21:30:05.569697Z","shell.execute_reply.started":"2022-06-27T21:30:05.483254Z","shell.execute_reply":"2022-06-27T21:30:05.568421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Density chart with Age and Sex","metadata":{}},{"cell_type":"code","source":"g = sns.FacetGrid(data,hue='sex', height=4, aspect=4)\ng = g.map(sns.kdeplot,'age',shade=True,alpha=0.4)                      \ng.add_legend()\ng.set(xlim=(20, 84), xticks=np.arange(20,88,4))\ng.fig.suptitle('Density between Age and Sex', y=1.05)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:30:14.982349Z","iopub.execute_input":"2022-06-27T21:30:14.98282Z","iopub.status.idle":"2022-06-27T21:30:15.554056Z","shell.execute_reply.started":"2022-06-27T21:30:14.982754Z","shell.execute_reply":"2022-06-27T21:30:15.552846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(15,8))\ng = sns.histplot(data=data, x=\"age\", hue=\"sex\", multiple=\"dodge\", color='sex', bins='fd')\ng.set(title='Distribution of Age by Gender')\ng.set(xlim=(20, 84), xticks=np.arange(20,88,4))\nfor p in g.patches:\n    x, w, h = p.get_x(), p.get_width(), p.get_height()\n    if h > 0:\n        g.text(x + w / 2, h, f'{h}\\n', ha='center', va='center', size=11)\ng.margins(y=0.07)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:33:47.258439Z","iopub.execute_input":"2022-06-27T21:33:47.25947Z","iopub.status.idle":"2022-06-27T21:33:47.729301Z","shell.execute_reply.started":"2022-06-27T21:33:47.259395Z","shell.execute_reply":"2022-06-27T21:33:47.727853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of organ type by gender","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(15,8))\ng = sns.histplot(data=data, x=\"sex\", hue=\"organ\", multiple=\"dodge\", color='organ', bins='fd')\ng.set(title='Distribution of Organ Type by Gender')\nfor p in g.patches:\n    x, w, h = p.get_x(), p.get_width(), p.get_height()\n    if h > 0:\n        g.text(x + w / 2, h, f'{h}\\n', ha='center', va='center', size=11)\ng.margins(y=0.07)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:33:56.486638Z","iopub.execute_input":"2022-06-27T21:33:56.487291Z","iopub.status.idle":"2022-06-27T21:33:56.830087Z","shell.execute_reply.started":"2022-06-27T21:33:56.487231Z","shell.execute_reply":"2022-06-27T21:33:56.82895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution of age by organ type","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(15,10))\ng = sns.histplot(data=data, x=\"age\", hue=\"organ\", multiple=\"stack\", color='organ', bins='fd')\ng.set(xlim=(20, 86), xticks=np.arange(20,88,4))\ng.set(title='Distribution of Age by Organ Type')\nfor p in g.patches:\n    x, y, w, h = p.get_x(), p.get_y(), p.get_width(), p.get_height()\n    if h > 0:\n        g.text(x + w / 2, y + h / 2, f'{h:.0f}\\n', ha='center', va='center', size=11)\ng.margins(y=0.07)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T21:34:01.876089Z","iopub.execute_input":"2022-06-27T21:34:01.876538Z","iopub.status.idle":"2022-06-27T21:34:02.968217Z","shell.execute_reply.started":"2022-06-27T21:34:01.876498Z","shell.execute_reply":"2022-06-27T21:34:02.967083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}