{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install optuna","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport time\npd.set_option('display.max_columns', 100)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.metrics import f1_score, recall_score, precision_score, classification_report\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class PreProcess:\n    \n    def __init__(self):\n        \n        self.df_otto_train = pd.read_csv('/kaggle/input/otto-group-product-classification-challenge/train.csv')\n        self.df_otto_test = pd.read_csv('/kaggle/input/otto-group-product-classification-challenge/test.csv')\n\n    def __call__(self):\n        \n        conv = lambda x: int(x.replace('Class_', '')) - 1\n        self.df_otto_train['target2'] = self.df_otto_train.target.map(conv)\n        \n        x = self.df_otto_train.drop(['id', 'target', 'target2'], axis=1).values\n        y = self.df_otto_train.target2.values\n        \n        x_test_id = self.df_otto_test.id.values\n        x_test = self.df_otto_test.drop(['id'], axis=1).values\n        \n        x_train, x_valid, t_train, t_valid = train_test_split(x, y, test_size=0.2, shuffle=True)\n        \n        _, _, _, counts = np.unique(t_train, return_index=True, return_inverse=True, return_counts=True)\n        loss_weight = (1 / torch.Tensor(counts)) * (x_train.shape[0]/len(np.unique(t_train)))\n        loss_weight = loss_weight.to(device)\n        \n        return x_train, x_valid, t_train, t_valid, x_test, x_test_id, loss_weight","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MLP(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(MLP, self).__init__()\n        \n        n_layers = 5\n        layers = []\n        features = [522, 1006, 1304, 706, 851]\n        p = [0.1287716033668473, 0.08497812991732616, 0.19230277862176845, 0.09567140996248122, 0.15776969001259095]\n        \n        in_features = in_dim\n        for i in range(n_layers):\n            out_features = features[i]\n            layers.append(nn.Linear(in_features, out_features))\n            layers.append(nn.BatchNorm1d(out_features))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(p[i]))\n            \n            in_features = out_features\n            \n        layers.append(nn.Linear(in_features, out_dim))\n        layers.append(nn.LogSoftmax(dim=-1))  \n        \n        self.network = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        \n        return self.network(x)\n    \nclass MLP_tune(nn.Module):\n    def __init__(self, trial, in_dim, out_dim):\n        super(MLP_tune, self).__init__()\n        \n        n_layers = trial.suggest_int(\"n_layers\", 5, 10)\n        layers = []\n        \n        in_features = in_dim\n        for i in range(n_layers):\n            out_features = trial.suggest_int(\"n_units_l{}\".format(i), in_dim, in_dim*20)\n            layers.append(nn.Linear(in_features, out_features))\n            layers.append(nn.BatchNorm1d(out_features))\n            layers.append(nn.ReLU())\n            p = trial.suggest_float(\"dropout_l{}\".format(i), 0.0, 0.5)\n            layers.append(nn.Dropout(p))\n            \n            in_features = out_features\n            \n        layers.append(nn.Linear(in_features, out_dim))\n        layers.append(nn.LogSoftmax(dim=-1))  \n        \n        self.network = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        \n        return self.network(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Train:\n    \n    def __init__(self):\n        \n        self.x_train, self.x_valid, self.t_train, self.t_valid, self.x_test, self.x_test_id, self.loss_weight = PreProcess()()\n    \n    def train(self, n_epochs, lr, weight_decay):\n               \n        in_dim = self.x_train.shape[1]\n        out_dim = len(np.unique(self.t_train))\n        \n        mlp = MLP(in_dim, out_dim).to(device)\n        \n#         optimizer = optim.Adam(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n        optimizer = optim.RMSprop(mlp.parameters(), lr=lr, weight_decay=weight_decay)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n        \n        criterion = nn.NLLLoss(weight=self.loss_weight)  # Negative Log Liklihood Loss\n        # criterion = nn.NLLLoss()  # Negative Log Liklihood Loss\n\n        ds_train = TensorDataset(torch.tensor(self.x_train).float(), torch.tensor(self.t_train))\n        ds_valid = TensorDataset(torch.tensor(self.x_valid).float(), torch.tensor(self.t_valid))\n        self.ds_test = TensorDataset(torch.tensor(self.x_test).float())\n\n        loader_train = DataLoader(ds_train, batch_size=256, shuffle=True)\n        loader_valid = DataLoader(ds_valid, batch_size=256, shuffle=True)\n        \n        start_time = time.time()\n\n        for epoch in range(n_epochs):\n            losses_train = []\n            losses_valid = []\n            preds_train = []\n            preds_valid = []\n            trues_train = []\n            trues_valid = []\n\n            mlp.train()\n            for x, t in loader_train:\n\n                true = t.tolist()\n                trues_train.extend(true)\n\n                mlp.zero_grad()\n\n                x = x.to(device)\n                t = t.to(device)\n\n                y = mlp.forward(x)\n\n                loss = criterion(y, t.long())\n\n                loss.backward()\n\n                optimizer.step()\n\n                pred = y.argmax(1).tolist()\n                preds_train.extend(pred)\n\n                losses_train.append(loss.tolist())\n\n            mlp.eval()\n            for x, t in loader_valid:\n\n                true = t.tolist()\n                trues_valid.extend(true)\n\n                x = x.to(device)\n                t = t.to(device)\n\n                y = mlp.forward(x)\n\n                loss = criterion(y, t.long())\n\n                pred = y.argmax(1).tolist()\n                preds_valid.extend(pred)\n\n                losses_valid.append(loss.tolist())\n\n            print('EPOCH: {}, Train [Loss: {:.3f}, F1: {:.3f}, R: {:.3f}, P: {:.3f}], Valid [Loss: {:.3f}, F1: {:.3f}, R: {:.3f}, P: {:.3f}]'.format(\n                epoch,\n                np.mean(losses_train),\n                f1_score(trues_train, preds_train, average='weighted'),\n                recall_score(trues_train, preds_train, average='weighted'),\n                precision_score(trues_train, preds_train, average='weighted'),\n                np.mean(losses_valid),\n                f1_score(trues_valid, preds_valid, average='weighted'),\n                recall_score(trues_valid, preds_valid, average='weighted'),\n                precision_score(trues_valid, preds_valid, average='weighted')\n            ))\n\n            scheduler.step(np.mean(losses_valid))\n\n        # # State Dict save\n        # torch.save(mlp.state_dict(), './{}/{}/cat{}_state_dict'.format(self.valid_year, self.valid_month, pred_layer))\n\n        elapsed_time = time.time() - start_time\n        print (\"elapsed_time:{:.1f}\".format(elapsed_time / 60) + \"[min]\")\n        \n        return mlp\n    \n    def test(self, mlp):\n\n        loader_test = DataLoader(self.ds_test, batch_size=256, shuffle=False)\n\n        preds_test = []\n\n        mlp.eval()\n        for x in loader_test:\n\n            x = x[0].to(device)\n\n            y = mlp.forward(x)\n            y = y.to('cpu').detach().numpy()\n            y = np.exp(y)\n\n            preds_test.extend(y)\n        \n        return preds_test\n    \n    def submit(self, preds_test):\n        \n        df_sample_submission = pd.read_csv('/kaggle/input/otto-group-product-classification-challenge/sampleSubmission.csv')\n        df_submission = pd.DataFrame(columns = df_sample_submission.columns)\n        df_submission.id = self.x_test_id\n        df_submission[df_submission.columns[1:]] = preds_test\n        df_submission.to_csv('submission.csv', index=False)\n        \n        return\n    \n    def get_optimizer(self, trial, model):\n        \n        optimizer_names = ['Adam', 'MomentumSGD', 'rmsprop']\n        optimizer_name = trial.suggest_categorical('optimizer', optimizer_names)\n        weight_decay = trial.suggest_loguniform('weight_decay', 1e-10, 1e-3)\n        \n        if optimizer_name == optimizer_names[0]:\n            adam_lr = trial.suggest_loguniform('adam_lr', 1e-5, 1e-1)\n            optimizer = optim.Adam(model.parameters(), lr=adam_lr, weight_decay=weight_decay)\n        elif optimizer_name == optimizer_names[1]:\n            momentum_sgd_lr = trial.suggest_loguniform('momentum_sgd_lr', 1e-5, 1e-1)\n            optimizer = optim.SGD(model.parameters(), lr=momentum_sgd_lr, momentum=0.9, weight_decay=weight_decay)\n        else:\n            rmsprop_lr = trial.suggest_loguniform('rmsprop_lr', 1e-5, 1e-1)\n            optimizer = optim.RMSprop(model.parameters(), lr=rmsprop_lr, weight_decay=weight_decay)\n\n        return optimizer\n    \n\n    def objective(self, trial):\n        \n        self.x_train, self.x_valid, self.t_train, self.t_valid, self.x_test, self.x_test_id, self.loss_weight = PreProcess()()\n        \n        in_dim = self.x_train.shape[1]\n        out_dim = len(np.unique(self.t_train))\n\n        n_epochs = 100\n\n        mlp = MLP_tune(trial, in_dim, out_dim).to(device)\n\n        optimizer = self.get_optimizer(trial, mlp)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n\n        criterion = nn.NLLLoss(weight=self.loss_weight)  # Negative Log Liklihood Loss\n        # criterion = nn.NLLLoss()  # Negative Log Liklihood Loss\n        \n        ds_train = TensorDataset(torch.tensor(self.x_train).float(), torch.tensor(self.t_train))\n        ds_valid = TensorDataset(torch.tensor(self.x_valid).float(), torch.tensor(self.t_valid))\n\n        loader_train = DataLoader(ds_train, batch_size=256, shuffle=True)\n        loader_valid = DataLoader(ds_valid, batch_size=256, shuffle=True)\n\n        start_time = time.time()\n\n        for epoch in range(n_epochs):\n            losses_train = []\n            losses_valid = []\n            preds_train = []\n            preds_valid = []\n            trues_train = []\n            trues_valid = []\n\n            mlp.train()\n            for x, t in loader_train:\n\n                true = t.tolist()\n                trues_train.extend(true)\n\n                mlp.zero_grad()\n\n                x = x.to(device)\n                t = t.to(device)\n\n                y = mlp.forward(x)\n\n                loss = criterion(y, t.long())\n\n                loss.backward()\n\n                optimizer.step()\n\n                pred = y.argmax(1).tolist()\n                preds_train.extend(pred)\n\n                losses_train.append(loss.tolist())\n\n            mlp.eval()\n            for x, t in loader_valid:\n\n                true = t.tolist()\n                trues_valid.extend(true)\n\n                x = x.to(device)\n                t = t.to(device)\n\n                y = mlp.forward(x)\n\n                loss = criterion(y, t.long())\n\n                pred = y.argmax(1).tolist()\n                preds_valid.extend(pred)\n\n                losses_valid.append(loss.tolist())\n\n            print('EPOCH: {}, Train [Loss: {:.3f}, F1: {:.3f}, R: {:.3f}, P: {:.3f}], Valid [Loss: {:.3f}, F1: {:.3f}, R: {:.3f}, P: {:.3f}]'.format(\n                epoch,\n                np.mean(losses_train),\n                f1_score(trues_train, preds_train, average='weighted'),\n                recall_score(trues_train, preds_train, average='weighted'),\n                precision_score(trues_train, preds_train, average='weighted'),\n                np.mean(losses_valid),\n                f1_score(trues_valid, preds_valid, average='weighted'),\n                recall_score(trues_valid, preds_valid, average='weighted'),\n                precision_score(trues_valid, preds_valid, average='weighted')\n            ))\n\n            scheduler.step(np.mean(losses_valid))\n\n#             metric = f1_score(trues_valid, preds_valid, average='weighted')\n            metric = np.mean(losses_valid)\n            trial.report(metric, epoch)\n\n            # Handle pruning based on the intermediate value.\n            if trial.should_prune():\n                raise optuna.exceptions.TrialPruned()\n\n        elapsed_time = time.time() - start_time\n        print (\"elapsed_time:{:.1f}\".format(elapsed_time / 60) + \"[min]\")\n        \n        return metric","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run():\n    \n    n_cross_valid = 30\n    preds_test_all = []\n    for n in range(n_cross_valid):   \n    \n        n_epochs = 100\n        lr = 7.560243948481283e-05\n        weight_decay = 0.0005896154168582327\n        train = Train()\n        mlp = train.train(n_epochs, lr, weight_decay)\n        preds_test = train.test(mlp)\n        preds_test_all.append(preds_test)\n    \n    preds_test = np.mean(preds_test_all, axis=0)\n#     preds_test = preds_test.argmax(1).tolist()\n#     preds_test = np.identity(9)[preds_test]\n    train.submit(list(preds_test))\n\nrun()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tune hyper parameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"def tune():\n    \n    train = Train()\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(train.objective, n_trials=1000)\n\n    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n    complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n\n    print(\"Study statistics: \")\n    print(\"  Number of finished trials: \", len(study.trials))\n    print(\"  Number of pruned trials: \", len(pruned_trials))\n    print(\"  Number of complete trials: \", len(complete_trials))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: \", trial.value)\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n        \n# tune()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}