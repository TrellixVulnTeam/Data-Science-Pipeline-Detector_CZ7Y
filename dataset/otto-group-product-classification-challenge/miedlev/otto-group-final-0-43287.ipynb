{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\npd.options.display.max_columns = 500\npd.options.display.max_rows = 500\n\nimport matplotlib\nmatplotlib.rc(\"font\", family = \"AppleGothic\")\nmatplotlib.rc(\"axes\", unicode_minus = False)\n\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats(\"retina\")","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/otto-group-product-classification-challenge/train.csv\",  index_col = \"id\")\n\nprint(train.shape)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/otto-group-product-classification-challenge/test.csv\",  index_col = \"id\")\n\nprint(test.shape)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!conda install -c conda-forge -y lightgbm","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# explortory"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train 확인결과 이는 feature_name을 선별적으로 선택하는 것이 아니라 전체를 종합적으로 사용한 뒤 modeling해야하는 문제이다\n# 따라 데이터분석본다는 정확한 예측모형을 만드는게 더 중요하다고 볼수 있음","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 절대적인 수치는 class_2가 제일 많고 그다음 class_6이 많다 \n# 가장 많은 데이터가 무엇인지 파악하는 차원에서 보는 것이고 이것이 label_name을 결정하는데 큰 영향을 주지는 않는다\n\nsns.countplot(x = \"target\", data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 가장 수가 많은 class_2를 일부 뽑아서 어떤 feat이 가장 많은지 확인해본다\n# 결과 밑처럼 feat_14 / feat_40 / feat_15 순으로 숫자가 많은 것을 알 수 있다\n\n# 하지만 여기서는 이런식으로 독립적인 분석을 진행하면 안된다\n# feat_14가 합산을 했을 때 가장 큰 숫자이긴 하지만, 이외에 feat_9의 경우 일정 큰 수가 있는 경우 이것도 class_2의 결과로 이어지는 모습을 보여준다\n# 이는 일정수의 feat별로 연관성을 가져 독립성이 아닌, 연속성을 띈 데이터라고 봐야한다 \n\n# 아마 0,1,2,3,4,5와 같은 single product를 어느쪽에서는 feat1로 , 어느쪽은 feat2로 보았다는 이야기이고 \n# 따라서 이러한 상관관계를 연결할 수 있는 모델을 찾아야함을 보여준다 \n\ntrain_feat_2 = train[train[\"target\"] == \"Class_2\"]\ntrain_feat_2 = train_feat_2.drop(\"target\", axis = 1)\ntrain_feat_2 = train_feat_2.T\ntrain_feat_2[\"Total\"] = train_feat_2.sum(axis = 1)\ntrain_feat_2\n\ntrain_feat_2.groupby(train_feat_2.index)[\"Total\"].sum().reset_index().\\\nsort_values(by = \"Total\", ascending = False).head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각 feat별로 상관관계를 파악하여 어느정도 연관성이 있는지 추측해본다. \n# 여기서 알 수 있는 사실은 feature_name을 진행할 때 전 feat를 사용해야 한다는 것이다 \n# 각 feat가 어느정도 연관성을 가지고 서로에게 영향을 주며 움직이고 있음을 알 수 있고 확고하게 영향을 미치는 것은 적다는 점을 알 수 있기에 \n# 이 문제가 log_loss를 통해 문제를 풀어야하는 이유를 설명해줄수 있다고 본다\n\n# 주목할 점은 feat_4와 feat_19\n# 보통 correspond는 -1~1사이의 값을 나타내는데 이들은 각기 다른 feat와의 상관관계가 거의 다 이를 넘어선다\n# 이 이야기는 이 두가지 feat는 다른 feat에 영향을 주거나 받지 않고 독자적으로 움직일 가능성이 높다는 이야기가 된다\n# 아마 이 둘은 독자적으로 class를 찾아갈 가능성이 있다\n# feature_name 구할때는 필요한 정보가 될 수 있음으로(독립성이 있어서 확실한 class를 보장할 가능성이 크다) 그대로 담는다\n\ntrain_correlation = train.corr()\ntrain_correlation","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_correlation[[\"feat_4\"]].sort_values(by = \"feat_4\", ascending = False).head()#.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# preparation"},{"metadata":{},"cell_type":"markdown","source":"### Select  [feature_names, label_name]"},{"metadata":{"trusted":true},"cell_type":"code","source":"# label_name을 제외한 전부의 컬럼을 feature_name화 한다\n\nlabel_name = \"target\"\nfeature_names = train.columns.difference([label_name])\n\nx_train = train[feature_names]\ny_train = train[label_name]\nx_test = test[feature_names]\n\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## coefficient of correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 적용 전 hyperparameter에서 어떠한 상관관계가 있는지 분석해본다 \n# tree를 어느정도 쳐야하는지, 숫자는 대략적으로 얼마나 되는지 확인하여 후에 적용할 수 있도록 한다 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# holdout validation을 활용하여 데이터를 분산해서 확인한다 \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\nx_train_holdout, x_test_holdout, y_train_holdout, y_test_holdout = \\\ntrain_test_split(x_train, y_train, test_size = 0.3, random_state = 42)\n\nprint(x_train_holdout.shape)\nprint(x_test_holdout.shape)\nprint(y_train_holdout.shape)\nprint(y_test_holdout.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# DecisionTreeClassifier로 대략적으로 조사를 했을때 70%정도의 정확성을 보여준다 \n\nfrom sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state = 42)\nmodel.fit(x_train_holdout, y_train_holdout)\n\ny_train_predict = model.predict(x_train_holdout)\ny_test_predict = model.predict(x_test_holdout)\n\ntrain_accuracy = (y_train_predict == y_train_holdout).mean()\ntest_accuracy = (y_test_predict == y_test_holdout).mean()\n\ntrain_accuracy, test_accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# max_depth을 조사했을때 대부분의 model과 마찬가지로 max_depth가 일정수가 증가할때 score가 좋아진다 \n# 이를 통해 후에 적용시 max_depth이 어느정도의 크기가 되어있어야 한다는 것을 알 수 있다\n\nmax_depth_list = range(30,51)\nhyperparameter = []\n\nfor max_depth in max_depth_list:\n   \n    model = DecisionTreeClassifier(random_state = 42, max_depth = max_depth)\n    model.fit(x_train_holdout, y_train_holdout)\n\n    y_train_predict = model.predict(x_train_holdout)\n    y_test_predict = model.predict(x_test_holdout)\n\n    train_accuracy = (y_train_predict == y_train_holdout).mean()\n    test_accuracy = (y_test_predict == y_test_holdout).mean()\n    \n    print(f\"max_depth = {max_depth}, train = {train_accuracy :.6f}, test = {test_accuracy:.6f}\") \n    # hyperparameter.append({\"max_depth\" : max_depth, \"train\" : train_accuracy, \"test\" :test_accuracy})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### model_selection"},{"metadata":{"trusted":true},"cell_type":"code","source":"# model을 적합성을 평가하기 위해 log_loss를 적용해야 한다.(문제의 조건임)\n# log_loss의 최적조건을 맞출 수 있는 model을 선정해야함으로 이를 구하기 위해 \n# LGBMClassifier / RandomForestClassifer 두가지를 hold-out으로 비교하여 점수가 더 좋은 모델을 선정하는 작업을 진행한다 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nmodel_first = LGBMClassifier(boosting_type='gbdt')\nmodel_first","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmodel_second = RandomForestClassifier()\nmodel_second","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGBMClassifier로 log_loss 측정시 prediction은 0.51로 측정되고\n# RandomForestClassifier로 log_loss 측정시 prediction은 1.49~ 1.51로 측정이 된다\n# log_loss는 0 ~ 1 사이의 값을 주로 나타내고 이 범위를 벗어나면 무한대로 측정, 즉 오차가 심한결과를 가져오는데 \n# RandomForestClassifier의 값이 많이 어긋난다는 것을 보여준다.\n# 즉 이 문제에서는 Gradient Boosting을 통해 bias를 조절하는 작업을 진행해야 함을 보여준다 \n\n\n# LGBMClassifier로 log_loss 측정\nmodel_first.fit(x_train_holdout, y_train_holdout)\ny_test_predict = model_first.predict_proba(x_test_holdout)\n\nprediction_LGBM = log_loss(y_test_holdout, y_test_predict)\n\n\n# RandomForestClassifier로 log_loss 측정\nmodel_second.fit(x_train_holdout, y_train_holdout)\ny_test_predict = model_second.predict_proba(x_test_holdout)\n\nprediction_Random = log_loss(y_test_holdout, y_test_predict)\n\n\n\n# 최종결과\nprint(\"LGBMClassifier is :\", prediction_LGBM)\nprint(\"RandomForest is :\", prediction_Random)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### model optimization process"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradient Boosting안에 있는 hyperparameters들을 조절하는 작업을 진행한다 \n# boosting_type을 dart, gbdt 둘중 한개를 선택해야 하는데 \n# 보통은 dart가 더 좋다고 하나 여기서는 gbdt의 점수가 더 높다(default가정)\n# 따라서 gbdt로 진행한다\n\nfrom lightgbm import LGBMClassifier\nmodel = LGBMClassifier(boosting_type = \"gbdt\")\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameters 중 중요한 몇개들을 별도 세팅을 진행해준다 \n# n_estimators / learning_rate가 제일 중요한 것이니 별도 세팅 진행해주고\n# 구역을 나누어주는 max_bin, 가지수 결정하는 num_leaves\n# random하게 서치하는데 필요한 colsample_bytree, subsample, subsample_freq, min_child_samples 정리한다\n\n# 대략 몇개의 숫자를 넣고 돌려본다 맞는지 확인체크\n\nrandom_search = 10\n\nfor number in range(random_search):\n    \n    n_estimators = np.random.randint(1, 10)\n    learning_rate = 10 ** -np.random.uniform(0, 1)\n    max_bin = np.random.randint(2, 500)\n    num_leaves = np.random.randint(10, 300)\n    min_child_samples = np.random.randint(2, 300)\n    colsample_bytree = np.random.uniform(0.1, 1)\n    subsample = np.random.uniform(0.4, 1)\n    \n    model = LGBMClassifier(n_estimators = n_estimators,\n                           learning_rate = learning_rate,\n                           max_bin = max_bin,\n                           num_leaves = num_leaves,\n                           min_child_samples = min_child_samples,\n                           colsample_bytree = colsample_bytree,\n                           subsample = subsample,\n                           subsample_freq = 1,\n                           n_jobs = -1,\n                           random_state = 42)\n    \n    model.fit(x_train_holdout, y_train_holdout)\n    y_test_predict = model.predict_proba(x_test_holdout) \n    score = log_loss(y_test_holdout, y_test_predict)  \n    \n    print(number, score)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# [random_search]\n# basic = 100 / final = 10 \n\n# [n_estimators]\n# 이 둘의 상관관계를 파악하기 위해 n_estimators를 1000,2000 해본다\n\n# [learning_rate]\n# learning_rate = 10 ** -np.random.uniform(1, 10)\n# final learning_rate = 10 ** -np.random.uniform(0.9, 3) \n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import log_loss\n\nrandom_search = 10\nhyperparameter_list= []\nearly_stopping_rounds = 10\n\nfor loop in range(random_search):\n    \n    # n_estimators = np.random.randint(1000, 3000)\n    n_estimators = np.random.randint(10, 30)\n    # learning_rate = 10 ** -np.random.uniform(1, 10)\n    learning_rate = 10 ** -np.random.uniform(0.9, 3)\n    max_bin = np.random.randint(2, 500)\n    num_leaves = np.random.randint(10, 300)\n    min_child_samples = np.random.randint(2, 300)\n    colsample_bytree = np.random.uniform(0.1, 1)\n    subsample = np.random.uniform(0.4, 1)\n    reg_alpha = 10 ** -np.random.uniform(1, 10)\n    reg_lambda = 10 ** -np.random.uniform(1, 15)\n    \n    model = LGBMClassifier(n_estimators = n_estimators,\n                           learning_rate = learning_rate,\n                           max_bin = max_bin,\n                           num_leaves = num_leaves,\n                           min_child_samples = min_child_samples,\n                           colsample_bytree = colsample_bytree,\n                           subsample = subsample,\n                           subsample_freq = 1,\n                           n_jobs = -1,\n                           random_state = 42)\n    \n    \n    model.fit(x_train_holdout, y_train_holdout, eval_set = [(x_test_holdout, y_test_holdout)],\n              early_stopping_rounds = early_stopping_rounds, verbose = 0)\n    y_test_predict = model.predict_proba(x_test_holdout)\n    \n          # model.best_score_[\"valid_0\"]['multi_logloss']\n    score = log_loss(y_test_holdout, y_test_predict)\n    \n    hyperparameter = {\"score\" : score, \"learning_rate\" : learning_rate,\n                     \"max_bin\" : max_bin,\n                     \"num_leaves\" : num_leaves,\n                     \"min_child_samples\" : min_child_samples,\n                     \"colsample_bytree\" : colsample_bytree,\n                     \"subsample\" : subsample,\n                     \"min_child_samples\" : min_child_samples,\n                     \"reg_alpha\" : reg_alpha,\n                     \"reg_lambda\" : reg_lambda}\n    \n    hyperparameter_list.append(hyperparameter)\n    \n    print(f\"score = {score:.6f}, n_estimators = {n_estimators},learning = {learning_rate:.6f},\\\n    max_bin = {max_bin}, num_leaves = {num_leaves}, subsample = {subsample:.6f},\\\n    colsample_bytree = {colsample_bytree}, min_child_samples = {min_child_samples}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_list = pd.DataFrame.from_dict(hyperparameter_list)\nfinal_list = final_list.sort_values(by = \"score\", ascending = True)\nfinal_list.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.best_score_[\"valid_0\"]['multi_logloss']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 결과(result) \n\n# 1. <n_estimators = 1,000인 경우 다음과 같이  score가 낮은 모습을 보인다>\n# score = 0.448931, n_estimators = 1000,learning = 0.021010,    max_bin = 38, num_leaves = 235, subsample = 0.560212\n# score = 0.453496, n_estimators = 1000,learning = 0.022231,    max_bin = 118, num_leaves = 291, subsample = 0.963156\n# score = 0.455559, n_estimators = 1000,learning = 0.032189,    max_bin = 382, num_leaves = 70, subsample = 0.561532\n# score = 0.457469, n_estimators = 1000,learning = 0.018955,    max_bin = 94, num_leaves = 205, subsample = 0.733559\n\n# 2. <n_estimators = 2,000인 경우 다음과 같이  score가 낮은 모습을 보인다>\n# score = 0.452683, n_estimators = 2000,learning = 0.011920,    max_bin = 387, num_leaves = 46, subsample = 0.652579,\n# score = 0.454024, n_estimators = 2000,learning = 0.013580,    max_bin = 281, num_leaves = 270, subsample = 0.757202\n# score = 0.455261, n_estimators = 2000,learning = 0.011020,    max_bin = 492, num_leaves = 179, subsample = 0.676981,\n# score = 0.455836, n_estimators = 2000,learning = 0.011370,    max_bin = 9, num_leaves = 281, subsample = 0.716674,\n\n# n_estimators가 1000인 경우 0.02XXX , n_estimators가 2000인 경우 0.01XXX에서 강한 점수를 보였다 \n# 이는 n_estimators별로 특정위치의 learning_rate를 가질때 최대의 효과를 가진다고 봐야한다. \n\n# 3. <n_estimators = 1,000~3,000인 경우 다음과 같이  score가 낮은 모습을 보인다>\n# score = 0.447990, n_estimators = 1917,learning = 0.008747,    max_bin = 257, num_leaves = 138, subsample = 0.683348,\n# score = 0.450463, n_estimators = 1850,learning = 0.013366,    max_bin = 355, num_leaves = 269, subsample = 0.638961,\n# score = 0.451037, n_estimators = 2196,learning = 0.009747,    max_bin = 17, num_leaves = 265, subsample = 0.900356,\n# score = 0.453210, n_estimators = 2800,learning = 0.005284,    max_bin = 61, num_leaves = 242, subsample = 0.491945,\n\n#  3번의 score = 0.447990","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Outperform"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nmodel = LGBMClassifier(boosting_type = \"gbdt\",\n                       n_estimators = 1917,\n                       learning_rate = 0.008747,\n                       max_bin = 257,\n                       min_child_samples = 118,\n                       colsample_bytree = 0.784344330044348,\n                       num_leaves = 138, \n                       subsample = 0.683348,\n                       subsample_freq = 1,\n                       n_jobs = -1, \n                       random_state = 42)\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.fit(x_train, y_train)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}