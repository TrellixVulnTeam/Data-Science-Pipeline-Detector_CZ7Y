{"cells":[{"metadata":{"_uuid":"574da2e6e40215fe5c44a30c68a4cb7bce8ba2e0"},"cell_type":"markdown","source":"CONCLUSIONS AT THE END. VERY QUITE INTERESTING NOTEBOOK HOWEVER."},{"metadata":{"_uuid":"48a725e10bba82257cafb8f8c5e5a46a42794e59"},"cell_type":"markdown","source":"This is inspired in the Otto product clasification Challenge, and on this kernel: [https://www.kaggle.com/pratik2901/otto-product](http://)\nThe goal is to classify the products with a random forest and then compare the results with other methods. KNN, SVM, etc, just to play and learn a bit. NO CROSS VALIDATION IN THE RANDOM FOREST FOR SIMPLICITY. IN THE OTHERS I DO IT."},{"metadata":{"trusted":true,"_uuid":"58ceae43a059f8f4907bc69726f22b8cc24374a8"},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import shuffle\n\nprint(\"All imports OK\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17fe86586d36960e07cef874070852b20b7fdcff"},"cell_type":"markdown","source":"**READ DATA**"},{"metadata":{"trusted":true,"_uuid":"bb22fe535da6b058a712cb4ab82eadd3eb717277"},"cell_type":"code","source":"%%time\n#As usual, I get confused with the kaggle input. Just ../input/train_file.csv !!\n#There is no target column in the test file, so I need to split my train and use part to validate\ntrain=pd.read_csv(\"../input/train.csv\")\ntest=pd.read_csv(\"../input/test.csv\")\nprint(\"Everything looks fine so far\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f686fca36e8be13621f933814c64fc774280cfc8"},"cell_type":"markdown","source":"Now, it is possible to use the training data \"as is\" or I can shuffle the rows randomly. If it's not necessary to shuffle, the result won't change, but if the Otto people didn't produce the data with a random order it may impact the clasification. I'll do it because I can.\n\nALSO, A MATRIX PLOT IN THIS CASE WOULD BE HUGE, TOO MANY VARIABLES."},{"metadata":{"trusted":true,"_uuid":"31ecade95961b116094cca158d662223dec09a7c"},"cell_type":"code","source":"train=shuffle(train)\ntrain.head()\n#Notice the id is index+1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4db6304930d1209b21e2b3fd8b4d6abfb7b57fa1"},"cell_type":"markdown","source":"Now I´ll se how many products I have per class"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"19f825f47028f75fb142cd2c11e7826294e1e04b"},"cell_type":"code","source":"#I want at the beginning of the bar plot: class 1, then 2, etc.\nordered=sorted(set(train['target']))\n\nsns.countplot(x='target', data=train, order=ordered);\n\nplt.title(\"Number of products per class\");\nplt.figure();\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5d45589259847a0094258933d10992fbdbe172a"},"cell_type":"code","source":"#check that there are no missing values\nprint(train.isnull().sum().sum())\n#Double sum, to sum over all the list of rows. If it is not 0, remove on of the .sum() and find where they NaNs are.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf7a273e1c630cc1b0d747060b519c25eb903920"},"cell_type":"markdown","source":"Ok, there are many producs of class 2 and 6. I am not sure that will be problematic, just that other classes may not be classified as accurately.\n\nBesides, scikitlearn doesn't like class_1 or so as labels. Need encoding and in this case I'll do only label encoding. One-hot encoding adds dimensions and I'll do it at some point in the very near future"},{"metadata":{"trusted":true,"_uuid":"48fd6c10f4ca79af4293eda7cec91ffab2ef1b82","scrolled":true},"cell_type":"code","source":"X=train\nY=train.target\nX=X.drop(['target','id'], axis=1)\n\nfrom sklearn import preprocessing\nLE=preprocessing.LabelEncoder()\nLE.fit_transform(Y.values.tolist())\nLabels=LE.transform(Y.astype(str))\nprint(LE.classes_)\nprint(Labels)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ab605930810a2e281cddeb291312bef6949662a"},"cell_type":"markdown","source":"Since I had a look at the head of the train/test files, and they are very sparse, I am sure I can remove some features while not losing too much info, because they don't contribute to the model too much. Like an mp3 or jpg encoding. This also reduces computation time, which will be significant with large datasets."},{"metadata":{"_uuid":"f5a43bf5be5f7d42f777010742dd1f73bbe2da95"},"cell_type":"markdown","source":"RFE is recursive feature elimination. scikitlearn does a lot of things easilty for me, so let's use it. For more basic info:\n\n[https://medium.com/@aneesha/recursive-feature-elimination-with-scikit-learn-3a2cbdf23fb7](http://)\n\nand\n\n[https://www.kaggle.com/arthurtok/feature-ranking-rfe-random-forest-linear-models](http://)"},{"metadata":{"trusted":true,"_uuid":"0fc99932f532558f24a1ac61f007d5474ba8af07"},"cell_type":"code","source":"%%time\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\n\nn_feat=30 #I´ll play a bit with this value and leave the one I like the most (45,30,15)\n#The selection takes the same amount of time with 1 feature or with 100, it ranks all of htem and then selects the best ones\n#45 is a lot but better than 30, you lose too much info. I'll put 30 for speed.\n\nclasif=RandomForestClassifier()\nrfe=RFE(clasif,n_feat)\n\nfit=rfe.fit(X, Labels)\n\nprint(\"Num features: %d\" %fit.n_features_)\nprint(\"Selected features: %s\" %fit.support_)\nprint(\"Feature ranking: %s\" %fit.ranking_) #Best is 1, worse is increasing\n\nfeatures=[]\n\n#Now I´ll do a loop. If my feature is retained (==True), I keep it.\nfor i,j in zip(X.columns,fit.support_): #zip iterates on two lists in parallell, not only in one.\n    if j==True:\n        features.append(str(i))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"81159e8c636a131e6822ddd4b819282018603674"},"cell_type":"markdown","source":"I know which features to keep and now I will classify them. Train and test with the RFC\n\n**FULL DETAILS OF TUNING THE RFC AND ITS PARAMETERS HERE:**\n\n[https://medium.com/@taplapinger/tuning-a-random-forest-classifier-1b252d1dde92](http://)\n\n[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](http://)"},{"metadata":{"trusted":true,"_uuid":"fe7259ec363862d6b506126a6ed70f298bb4c05b"},"cell_type":"code","source":"%%time\n\nX_RFC=X[features]\nX_RFC.head() #As always, to check\nfrom sklearn.model_selection import train_test_split\ntrain_X,val_X,train_y,val_y=train_test_split(X_RFC,Y,random_state=13)\n\n#Now I train my model with my data (No splitting necessary, it was already split in train and test files)\n\nmodel_rfc=RandomForestClassifier(n_jobs=1, max_depth=13, random_state=17)\n#I can add more parameters, but the default should be ok in this set\n#I didn´t touch Y, X_smt I will keep on changing\nmodel_rfc.fit(train_X,train_y)\n\npred_RFC=model_rfc.predict(val_X)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"285b46170d1f2d9641c562e396b8ce51fde207fe"},"cell_type":"markdown","source":"Now I evaluate my model. This is not a numerical regressor, so MAE and RMSE are a bit difficult to understand in this case. I need accuracy_score and confusion matrix"},{"metadata":{"trusted":true,"_uuid":"ab97c370ebc002f36fa907fab01b22d66daaf699"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score,confusion_matrix\n\nacc_RFC=np.round(accuracy_score(val_y,pred_RFC),4)\nprint(\"Accuracy RFC is: \", acc_RFC)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4631e265f1854a9284036579c431f5b5a738a84"},"cell_type":"markdown","source":"Not bad, 77% accurate. Let's see what is the modal category, in case I'm not predicting very much."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"61fd2a58b522096301eae74abac02ba29cd4e8cf"},"cell_type":"code","source":"mode=train_y.mode()\nls=[mode for i in range(len(val_y))]\n#ls\nacc_mode=np.round(accuracy_score(val_y,ls),4)\nprint(\"Accuracy With mode is: \", acc_mode)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be084320ffc57d29a60c57af30313237ec981a6c"},"cell_type":"markdown","source":"My random forest predicts quite well. Let´s do the confusion matrix and the AOC curve\n\nBibliography for CM here: [https://www.python-course.eu/confusion_matrix.php](http://)"},{"metadata":{"trusted":true,"_uuid":"95efb714fb7ad68991fe553a0a25a3cb98db0d03"},"cell_type":"code","source":"labels = [sorted(train.target.unique())]\nlabels\ncm = confusion_matrix(val_y, pred_RFC)\nprint(cm)\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cddc34fc49f55aa34492ff2e48af7f702c828925"},"cell_type":"markdown","source":"My CM is not great, but not too bad. You can see that CM(0,0) is blue, which is more or less bad, but there were not too many elements in class 1. Whites are NaNs."},{"metadata":{"trusted":true,"_uuid":"4e214e38169f25a8e24629c63d69c84117d511c7"},"cell_type":"code","source":"cm_N = cm/cm.sum(axis=1)\nprint(cm_N)\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(111)\ncax_N = ax.matshow(cm_N)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax_N)\nax.set_xticklabels([''] + labels)\n#ax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e80b84f364dba04c425e0e0852796881e2c6c9e5"},"cell_type":"markdown","source":"sklearn does not support multiclass ROC/AUC metrics, so it is necessary to make it myself.\n\nFOR REFERENCE\n* PRECISION = DEFINITION IN ELECTRICAL PROTECTIONS. HOW MANY TRUE POSITIVES OVER TOTAL POSITIVES ------>1 IF MY CLASIFFIER/PROTECTION WORKS PROPERLY\n\n* RECALL= TRUE POSITIVES/(TRUE POSITIVES+FALSE NEGATIVES). THIS HAD ANOTHER NAME IN ELECTRICAL PROTECTIONS BUT I DON'T REMEMBER IT. BASICALLY, I WANT FALSE NEGATIVES TO BE ZERO, SO ------>1\n\nFOR MULTICLASS, I SUM OVER COLUMNS (AXIS =1) FOR RECALL AND I SUM OVER ROWS FOR PRECISION (AXIS=0)\n\n"},{"metadata":{"trusted":true,"_uuid":"c4cd4c1d2d77e8d3ea0f843bc9b8ee39b5edca37"},"cell_type":"code","source":"recall_RFC=np.diag(cm)/np.sum(cm, axis=1)\n#recall_RFC\nprecision_RFC=np.diag(cm)/np.sum(cm, axis=0)\n#precision_RFC","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"525113a6ce1eb7da49f6b1dc44676a80e5e53c95"},"cell_type":"markdown","source":"Now, I can either average everything normally or do the weighted average, which makes more sense when the classes are unbalanced. However, I dropped the target"},{"metadata":{"trusted":true,"_uuid":"5da4c56c154232f6b81bb9ff0478ed4672b65cab"},"cell_type":"code","source":"#Calculate the weights\n\ntotal_prod=len(train_X.index)\ntotal_classes=train.groupby('target').size() #size() gives a list, count() gives a df\n#total_classes\nweights=0.75*total_classes/total_prod\n#The 0.75 is because I´m doing the weights with the FULL train set, not with the train. The proportions should be the same, as you would expect if the splitting is random\n#Then when I divide by the training set, result is > 1, which I correct with 0.75. It´s not a great solution, but it´s a solution.\n#Anyway, the numbers are all irrelevant, this is not a real case.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc82f9d2c1978e3d633c41873a0156e54a13d694"},"cell_type":"code","source":"av_recall_RFC=np.multiply(recall_RFC,weights).sum() #each*weight, and then sum all\nav_prec_RFC=np.multiply(precision_RFC,weights).sum()\n\nprint(\"The average recall of the RFC is: \", av_recall_RFC)\nprint(\"The average precision of the RFC is: \", av_prec_RFC)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3993e3174d230587b18507d958da7389b21e61ba"},"cell_type":"markdown","source":"That means my recall (TP/TP+FN) has a 27% error when my precision (TP/TP+FP) is 75%. I get too many false negatives and false positives. Let's see the F1 score (valor F in \"espanish\"). Here I will set a value of $\\beta=1$ because in this case I care as much about precision as about recall. Otherwise I adjust it accordingly.\n[https://en.wikipedia.org/wiki/F1_score](http://)"},{"metadata":{"trusted":true,"_uuid":"bf25ef2fececb2f8b896046bdea46fe016e3f371"},"cell_type":"code","source":"beta=1\nF1_RFC=(1+beta)*(av_recall_RFC*av_prec_RFC)/(beta**2*av_prec_RFC+av_recall_RFC)\nprint(\"The F1 score for the RFC is: \",F1_RFC)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61a716bfc3e5f500b441d0390012eb8c87219c18"},"cell_type":"markdown","source":"Now I´ll implement a kNN classifier for the multiclass problem. I USE CROSS VALIDATION. I don't do train_test_split because it was done previously, same as with label encoding."},{"metadata":{"trusted":true,"_uuid":"1107d22f1e69e3e72f7ad80d0e3f07a300040dcb"},"cell_type":"code","source":"%%time\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score,recall_score\n#I do with 3 neighbors, then I do a loop with many for cross val\n\nknn=KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\nknn.fit(train_X,train_y)\npred_KNN_3=knn.predict(val_X)\n#For completeness, I use the built in metrics, later I'll calculate the same thing as before, for consistency\nacc_KNN_3=accuracy_score(val_y, pred_KNN_3)\nprint(\"The accuracy score from sklearn for KNN3 is: \",acc_KNN_3)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff45fa297e0144e38a6d025d668610743b048718"},"cell_type":"markdown","source":"75% percent \"pseudo-accuracy\". Wow, this is powerful but very slow. I'll do a loop for cross-validation and find the best K. Try not to run this loop so much, it's a very slow loop although it's 10 iterations for K and 5 cross-validations. THIS TAKES SOME TIME AND A CRAZY AMOUNT OF MEMORY, COMPARED TO OTHER CLASSIFIERS. Logical, to be honest."},{"metadata":{"trusted":true,"_uuid":"2a21e8f77fa786ce8c2915548338f8833d79bb62"},"cell_type":"code","source":"%%time\n\nfrom sklearn.model_selection import cross_val_score\nvalues_K = list(range(1,15)) #More than 10 neighbours is a bit too much\n\n# empty list for scores\ncv_scores = []\n\n#I use accuracy as a metric. I have to choose something and then I'll do a proper fitting and cm and all that with the optimum.\n\n\n# 5-fold cross validation\nfor k in values_K:\n    knn = KNeighborsClassifier(n_neighbors=k,n_jobs=-1)\n    scores = cross_val_score(knn, train_X, train_y, cv=5, scoring='accuracy')\n    cv_scores.append(scores.mean())\n\nprint(\"Finished!\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d2821c5b9cac9a2dde334bb1540b4ba6c82cfc8"},"cell_type":"markdown","source":"Let's print the score for every K. The best one is for 9 neighbors. I may have to do a few more, but the iterations take very long. I won't plot the scatter because there are 9 classes, that's a bit much."},{"metadata":{"trusted":true,"_uuid":"7d120dd405da28ea1d7f846583138269df4a4524"},"cell_type":"code","source":"print(\"The optimum number of neighbors is: \", cv_scores.index(max(cv_scores))+1) #Because Index starts at 0!!!\nplt.plot(values_K, cv_scores)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Accuracy score')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6dabd255e7a7233e4502bf29a45247a00b6cdac"},"cell_type":"code","source":"%%time\n\nmodel_knn=KNeighborsClassifier(n_neighbors=9,n_jobs=-1)\nmodel_knn.fit(train_X,train_y)\n\npred_KNN_9=model_knn.predict(val_X)\n#Now I do everything with confusion matrix, as before.\n\nprint(\"Trained and predicted\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39e46c12c24add2e5847bdd12181953430832f68"},"cell_type":"code","source":"cm_knn = confusion_matrix(val_y, pred_KNN_9)\nprint(cm_knn)\nfig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(cm_knn)\nplt.title('Confusion matrix of the classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4f29f1ab81e245b3dd4094bd443e43d4894d5a14"},"cell_type":"code","source":"recall_KNN=np.diag(cm_knn)/np.sum(cm_knn, axis=1)\n#recall_RFC\nprecision_KNN=np.diag(cm_knn)/np.sum(cm_knn, axis=0)\n#precision_RFC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"595126d52a7b033d3a632851dbc9486969672a43"},"cell_type":"code","source":"av_recall_KNN=np.multiply(recall_KNN,weights).sum() #each*weight, and then sum all\nav_precision_KNN=np.multiply(precision_KNN,weights).sum()\n\nprint(\"The average recall of the RFC is: \", av_recall_KNN)\nprint(\"The average precision of the RFC is: \", av_precision_KNN)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f38b7ffcea7a3b05557b127e9b511a19fda53fb"},"cell_type":"markdown","source":"Now let's do a gradient boosted decision tree. THIS IS NOT A RANDOM FOREST. An RF builds many trees and combines them (average for example). GBT is a SEQUENTIAL set of trees. Build 1, Improve! (tune param) and then build a second one. Improve the second one, and so on. IT'S SEQUENTIAL, NOT PARALLELL. In this case, it will be much slower, but perhaps better in accuracy. Again, I don't need to split. I did it previously."},{"metadata":{"_uuid":"8258a1d7fea618fa8e4b8fb1f5801fff65bb5a6a"},"cell_type":"markdown","source":"THE NEXT CELL TAKES VERY LONG, ~7 min."},{"metadata":{"trusted":true,"_uuid":"fc030a701c023fb01739b59a599d46be4df3dabe"},"cell_type":"code","source":"%%time\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nmodel_GBDT=GradientBoostingClassifier(loss='deviance', learning_rate=0.15, n_estimators=100, subsample=1, max_depth=8, random_state=13) #No n_jobs\nmodel_GBDT.fit(train_X,train_y)\npred_GBDT=model_GBDT.predict(val_X)\n#print(\"Model fitted and predictions done.\")\n#train_y.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56383a48f1c428014c18a523f856300d245d2876"},"cell_type":"code","source":"cm_GBDT = confusion_matrix(val_y, pred_GBDT)\n#print(cm_GBDT)\n#fig = plt.figure(figsize=(20,10))\n#ax = fig.add_subplot(111)\n#cax = ax.matshow(cm_GBDT)\n#plt.title('Confusion matrix of the classifier')\n#fig.colorbar(cax)\n#ax.set_xticklabels([''] + labels)\n#ax.set_yticklabels([''] + labels)\n#plt.xlabel('Predicted')\n#plt.ylabel('True')\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67819ad6cb31f32ad639cdfc4a762372fc6f688e"},"cell_type":"code","source":"recall_GBDT=np.diag(cm_GBDT)/np.sum(cm_GBDT, axis=1)\n#recall_RFC\nprecision_GBDT=np.diag(cm_GBDT)/np.sum(cm_GBDT, axis=0)\n#precision_RFC\n\nav_recall_GBDT=np.multiply(recall_GBDT,weights).sum() #each*weight, and then sum all\nav_precision_GBDT=np.multiply(precision_GBDT,weights).sum()\n\nprint(\"The average recall of the RFC is: \", av_recall_GBDT)\nprint(\"The average precision of the RFC is: \", av_precision_GBDT)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"896198964e3d9c713291e4f07c815ae9017e3c48"},"cell_type":"markdown","source":"The GBDT is the best so far. Let's see a (few) SVMs. Basically, linear, polynomial low-high order, rbf. KEEP IN MIND THE SVM ARE VERY BAD FOR SCALING, THEY RUN IN O(N^2) FOR N SAMPLES. IN PRACTICE MORE THAN 10-20 K SAMPLES ARE NOT VERY CLASSIFIABLE.\n[https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](http://)"},{"metadata":{"_uuid":"dd20416893e5d0bb9dc796bbabdf2d99161f7f0f"},"cell_type":"markdown","source":"I'm going to save work and do a few at the same time, then compare."},{"metadata":{"trusted":true,"_uuid":"828814031c5273b672a5741cc5a077924cd80e80"},"cell_type":"code","source":"%%time\n\nfrom sklearn import svm\n\nmodel_SVC_lin=svm.SVC(C=1, kernel='linear')\nmodel_SVC_lin.fit(train_X,train_y)\npred_SVC_lin=model_SVC_lin.predict(val_X)\n\ncm_SVC_lin = confusion_matrix(val_y, pred_SVC_lin)\n\nrecall_SVC_lin=np.diag(cm_SVC_lin)/np.sum(cm_SVC_lin, axis=1)\n#recall_RFC\nprecision_SVC_lin=np.diag(cm_SVC_lin)/np.sum(cm_SVC_lin, axis=0)\n#precision_RFC\n\nav_recall_SVC_lin=np.multiply(recall_SVC_lin,weights).sum() #each*weight, and then sum all\nav_precision_SVC_lin=np.multiply(precision_SVC_lin,weights).sum()\n\nprint(\"The average recall of the SVC with linear kernel is: \", av_recall_SVC_lin)\nprint(\"The average precision of the SVC with linear kernel is: \", av_precision_SVC_lin)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7f9ef12d515cc76fcb9b93fcefa3cf954a9fb2b"},"cell_type":"markdown","source":"Ok, this is not great, but similar to the other ones. Which is not surprising, it's a linear clasification and can be a bit coarse."},{"metadata":{"trusted":true,"_uuid":"0e222a71d7f5f0926cee0bf1aaa56c4c44fe49cd"},"cell_type":"code","source":"%%time\n#polynomial. I'll use degree 3-5 and decide manually\n\n\nmodel_SVC_p=svm.SVC(C=1, kernel='poly', degree=3)\nmodel_SVC_p.fit(train_X,train_y)\npred_SVC_p=model_SVC_p.predict(val_X)\n\ncm_SVC_p = confusion_matrix(val_y, pred_SVC_p)\n\nrecall_SVC_p=np.diag(cm_SVC_p)/np.sum(cm_SVC_p, axis=1)\n#recall_RFC\nprecision_SVC_p=np.diag(cm_SVC_p)/np.sum(cm_SVC_p, axis=0)\n#precision_RFC\n\nav_recall_SVC_p=np.multiply(recall_SVC_p,weights).sum() #each*weight, and then sum all\nav_precision_SVC_p=np.multiply(precision_SVC_p,weights).sum()\n\nprint(\"The average recall of the SVC of degree 3 is: \", av_recall_SVC_p)\nprint(\"The average precision of the SVC of degree 3 is: \", av_precision_SVC_p)\n\n#third degree: recall/prec = 0.751/0.751\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b21e72fa228d13bc021a9dc985bbaa5feebedb53"},"cell_type":"code","source":"#%%time\n#polynomial. I'll use degree 3-5 and decide manually\n\n\n#model_SVC_p=svm.SVC(C=1, kernel='poly', degree=5)\n#model_SVC_p.fit(train_X,train_y)\n#pred_SVC_p=model_SVC_p.predict(val_X)\n\n#cm_SVC_p = confusion_matrix(val_y, pred_SVC_p)\n\n#recall_SVC_p=np.diag(cm_SVC_p)/np.sum(cm_SVC_p, axis=1)\n#recall_RFC\n#precision_SVC_p=np.diag(cm_SVC_p)/np.sum(cm_SVC_p, axis=0)\n#precision_RFC\n\n#av_recall_SVC_p=np.multiply(recall_SVC_p,weights).sum() #each*weight, and then sum all\n#av_precision_SVC_p=np.multiply(precision_SVC_p,weights).sum()\n\n#print(\"The average recall of the SVC of degree 5 is: \", av_recall_SVC_p)\n#print(\"The average precision of the SVC of degree 5 is: \", av_precision_SVC_p)\n\n#third degree: recall/prec = 0.751/0.751","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a46f0bd4566a0f2daba1b1785ce2696245b8c75"},"cell_type":"code","source":"%%time\n#RBF\n\nmodel_SVC_rbf=svm.SVC(C=1, kernel='rbf')\nmodel_SVC_rbf.fit(train_X,train_y)\npred_SVC_rbf=model_SVC_rbf.predict(val_X)\n\ncm_SVC_rbf = confusion_matrix(val_y, pred_SVC_rbf)\n\nrecall_SVC_rbf=np.diag(cm_SVC_rbf)/np.sum(cm_SVC_rbf, axis=1)\n#recall_RFC\nprecision_SVC_rbf=np.diag(cm_SVC_p)/np.sum(cm_SVC_rbf, axis=0)\n#precision_RFC\n\nav_recall_SVC_rbf=np.multiply(recall_SVC_rbf,weights).sum() #each*weight, and then sum all\nav_precision_SVC_rbf=np.multiply(precision_SVC_rbf,weights).sum()\n\nprint(\"The average recall of the SVC with an rbf kernel is: \", av_recall_SVC_rbf)\nprint(\"The average precision of the SVC with an rbf kernel is: \", av_precision_SVC_rbf)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c38c42fce1e1786f7253feccc8c6fed85efa495"},"cell_type":"markdown","source":"I'm not going to do a plot of times because I already took too long with this thing. However, SVM with linear kernel is ~5 min and GDBT is ~8 min. However, SVC with 3rd degree kernel is 23 min!!!!! I didn't bother to try the 5th degree anyway."},{"metadata":{"trusted":true,"_uuid":"57d3bebabf8e5cbf0f705b83b1c56b54434831a0"},"cell_type":"code","source":"F1_RFC=round((1+beta)*(av_recall_RFC*av_prec_RFC)/(beta**2*av_prec_RFC+av_recall_RFC),4)\nprint(\"The F1 score for the RFC is: \",F1_RFC)\n\nF1_KNN_9=round((1+beta)*(av_recall_KNN*av_precision_KNN)/(beta**2*av_precision_KNN+av_recall_KNN),4)\nprint(\"The F1 score for the KNN with 9 neighbors is: \",F1_KNN_9)\n\nF1_GBDT=round((1+beta)*(av_recall_GBDT*av_precision_GBDT)/(beta**2*av_precision_GBDT+av_recall_GBDT),4)\nprint(\"The F1 score for the GBDT is: \",F1_GBDT)\n\n\nF1_SVC_lin=round((1+beta)*(av_recall_SVC_lin*av_precision_SVC_lin)/(beta**2*av_precision_SVC_lin+av_recall_SVC_lin),4)\nprint(\"The F1 score for the SVM with linear kernel is: \",F1_SVC_lin)\n\nF1_SVC_p=round((1+beta)*(av_recall_SVC_p*av_precision_SVC_p)/(beta**2*av_precision_SVC_p+av_recall_SVC_p),4)\nprint(\"The F1 score for the SVM with 3rd degree kernel is: \",F1_SVC_p)\n\nF1_SVC_rbf=round((1+beta)*(av_recall_SVC_rbf*av_precision_SVC_rbf)/(beta**2*av_precision_SVC_rbf+av_recall_SVC_rbf),4)\nprint(\"The F1 score for the SVM with 3rd degree kernel is: \",F1_SVC_rbf)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22d78820db0b696e452700eda4af30c97d62b46e"},"cell_type":"markdown","source":"Conclusion: GBDT is the best in precision. Significantly higher than the other ones. Now, with RFE I reduce times and memory, but of course I'll lose some info. Can be a good strategy to play with and then do a finer model. Avoid SVM with mid to big datasets, especially if you need to try a different polynomial model. Perhaps rbf is better than polynomial, or linear. Depends on the data. Random forest is still the fastest one because it creates the trees in a parallell way, while GDBT are sequential."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}