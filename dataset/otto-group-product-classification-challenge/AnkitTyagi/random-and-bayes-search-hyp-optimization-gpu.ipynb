{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n!pip install scikit-optimize==0.8.1\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport xgboost as xg\nfrom collections import Counter\n!pip install kneed\n# kneed is not installed in kaggle. uncomment the above line.\nfrom kneed import KneeLocator\nimport matplotlib.pyplot as plt\nfrom functools import partial\nfrom skopt import space, gp_minimize","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Loading the dataset and data preprocessing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reading train dataset in the environment.\ndataset_pd = pd.read_csv(\"/kaggle/input/otto-group-product-classification-challenge/train.csv\", index_col = 0)\nprint(dataset_pd.shape)\n# Reading test dataset in the environment.\ndataset_pd2 = pd.read_csv(\"/kaggle/input/otto-group-product-classification-challenge/test.csv\", index_col = 0)\nprint(dataset_pd2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a predictor matrix (removing the response variable column)\ndataset_train = dataset_pd.values\nX = dataset_train[:,0:93] # Predictors\ny = dataset_train[:,93] # Response \n\n# XGBoost do not take a categorical variable as input. We can use LabelEncoder to assign labels to categorical variables.\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y)\nlabel_encoder_y = label_encoder.transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"XGBoost Model with default settings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running a XGBoost with default settings.\nmodel = xg.XGBClassifier(nthreads = -1)\nmodel.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyper parameter Optimization using Random Search","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier = xg.XGBClassifier(n_thread = 6, tree_method='gpu_hist')\n# Defining the parameter grid for the Random Search.\nparam_grid = {\n    \"n_estimators\" : np.arange(100, 1000, 100),\n    \"max_depth\" : np.arange(1, 20, 2),\n    \"colsample_bytree\": np.arange(0.5,1, 0.1),\n    \"learning_rate\" : [0.0001, 0.001, 0.01, 0.1],\n    \"criterion\": [\"gini\",'entropy']\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomizedSearchCV(estimator = classifier,\n                          param_distributions = param_grid,\n                          n_iter = 10,\n                          scoring = \"accuracy\",\n                          verbose = 10,\n                          n_jobs = -1,\n                          cv = 5)\nmodel.fit(X, label_encoder_y)\nmodel.best_score_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.best_estimator_.get_params())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's fit a model using above paramters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)\n\nclassifier = xg.XGBClassifier(n_thread = 6, tree_method='gpu_hist', \n                              n_estimators = 600, \n                              max_depth = 5, \n                              colsample_bytree = 0.8,\n                              learning_rate = 0.1,\n                              criterion = \"entropy\")\nclassifier.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, classifier.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, classifier.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Making a submission using this model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test = dataset_pd2.values\n\nclassifier = xg.XGBClassifier(n_thread = 6, tree_method='gpu_hist', \n                              n_estimators = 600, \n                              max_depth = 5, \n                              colsample_bytree = 0.8,\n                              learning_rate = 0.1,\n                              criterion = \"entropy\")\nclassifier.fit(X, label_encoder_y)\n\nprediction_sub = classifier.predict(dataset_test)\n\n#dataset_pd2[\"prediction\"] = prediction_sub\nX_sub = np.array(prediction_sub).reshape(-1,1)\nonehot_encoder = OneHotEncoder(sparse = False)\nsubmission_file = onehot_encoder.fit_transform(X_sub)\n\nsubmission_file_df = pd.DataFrame(submission_file, \n                                  columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6',\n                                            'Class_7','Class_8','Class_9'], index = dataset_pd2.index)\n\n\nsubmission_file_df.to_csv(\"submission_otto_ver2.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Trying Bayesian Optimization using using Gaussian Process","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimize function for gp_minimize\ndef optimize(params, param_names, x, y):\n    params = dict(zip(param_names, params))\n    model = xg.XGBClassifier(**params)\n    kf = StratifiedKFold(n_splits = 5)\n    accuracies = []\n    for idx in kf.split(X = x, y = y):\n        train_idx , test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        \n        xtest = x[test_idx]\n        ytest = y[test_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xtest)\n        fold_acc = accuracy_score(ytest, preds)\n        accuracies.append(fold_acc)\n    \n    return -1.0 * np.mean(accuracies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Parameter Space for XGBoost\nparam_space = [\n    space.Integer(3,15, name = 'max_depth'),\n    space.Integer(100, 600, name = 'n_estimators'),\n    space.Categorical(['gini', 'entropy'], name = 'criterion'),\n    space.Real(0.01,1, prior = 'uniform', name = 'colsample_bytree'),\n    space.Real(0.001,1, prior = 'uniform', name = 'learning_rate') \n]\nparam_names = [\n    \"max_depth\",\n    \"n_estimators\",\n    \"criterion\",\n    \"colsample_bytree\",\n    \"learning_rate\"\n]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Optimization Function\noptimization_function = partial(\n    optimize,\n    param_names = param_names,\n    x = X,\n    y = label_encoder_y\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = gp_minimize(optimization_function,\n                    dimensions = param_space,\n                    n_calls = 10,\n                    n_random_starts = 10,\n                    verbose = 10, \n                    n_jobs = -1\n)\nprint(dict(zip(param_names, result.x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Let's fit a model using above paramters.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)\n\nclassifier = xg.XGBClassifier(n_thread = 6, tree_method='gpu_hist', \n                              n_estimators = 171, \n                              max_depth = 12, \n                              colsample_bytree = 0.9444262241947871,\n                              learning_rate = 0.253008978,\n                              criterion = \"entropy\")\nclassifier.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, classifier.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, classifier.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test = dataset_pd2.values\n\nclassifier = xg.XGBClassifier(n_thread = 6, tree_method='gpu_hist', \n                              n_estimators = 171, \n                              max_depth = 12, \n                              colsample_bytree = 0.9444262241947871,\n                              learning_rate = 0.253008978,\n                              criterion = \"entropy\")\nclassifier.fit(X, label_encoder_y)\n\nprediction_sub = classifier.predict(dataset_test)\n\n#dataset_pd2[\"prediction\"] = prediction_sub\nX_sub = np.array(prediction_sub).reshape(-1,1)\nonehot_encoder = OneHotEncoder(sparse = False)\nsubmission_file = onehot_encoder.fit_transform(X_sub)\n\nsubmission_file_df = pd.DataFrame(submission_file, \n                                  columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6',\n                                            'Class_7','Class_8','Class_9'], index = dataset_pd2.index)\n\n\nsubmission_file_df.to_csv(\"submission_otto_ver2.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}