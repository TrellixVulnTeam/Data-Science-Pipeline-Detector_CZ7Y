{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport xgboost as xg\nfrom collections import Counter\n!pip install kneed\n# kneed is not installed in kaggle. uncomment the above line.\nfrom kneed import KneeLocator\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Loading the dataset and data preprocessing","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Reading train dataset in the environment.\ndataset_pd = pd.read_csv(\"/kaggle/input/otto-group-product-classification-challenge/train.csv\", index_col = 0)\nprint(dataset_pd.shape)\n# Reading test dataset in the environment.\ndataset_pd2 = pd.read_csv(\"/kaggle/input/otto-group-product-classification-challenge/test.csv\", index_col = 0)\nprint(dataset_pd2.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a predictor matrix (removing the response variable column)\ndataset_train = dataset_pd.values\nX = dataset_train[:,0:93] # Predictors\ny = dataset_train[:,93] # Response \n\n# XGBoost do not take a categorical variable as input. We can use LabelEncoder to assign labels to categorical variables.\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y)\nlabel_encoder_y = label_encoder.transform(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Model with default settings","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running a XGBoost with default settings.\nmodel = xg.XGBClassifier()\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a confusion matrix \nprint(confusion_matrix(y_test, model.predict(X_test)))\nprint(classification_report(y_test, model.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running a XGBoost with less column sample.\nmodel = xg.XGBClassifier(colsample_bytree = 0.5)\nmodel.fit(X_train, y_train)\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Counter(y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Studying the feature importance and selecting the top variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Storing the feature importance matrix\nfeature_imp = pd.DataFrame(model.feature_importances_, \n                           index = dataset_pd.drop('target', axis = 1).columns, columns = ['imp'])\nfeature_imp.sort_values(by = 'imp', ascending = False, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating accuracy considering different threshold for feature importance.\nnum = []\nscore = []\nfor thresh in model.feature_importances_:\n    selection = SelectFromModel(model, threshold = thresh, prefit = True)\n    Select_X_train = selection.transform(X_train)\n    selection_model = xg.XGBClassifier()\n    selection_model.fit(Select_X_train, y_train)\n    Select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(Select_X_test)\n    num.append(Select_X_train.shape[1])\n    score.append(accuracy_score(y_test, y_pred)* 100)\n    print(\"Thresh: %.3f, n = %d, Accuracy: %.2f%%\" % (thresh, Select_X_train.shape[1], accuracy_score(y_test, y_pred)* 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Storing the accuracy table for different threshold and then plotting it.\naccuracy_table = pd.DataFrame({'params' : num, 'accuracy' : score})\naccuracy_table.sort_values(by = 'accuracy', ascending = False, inplace = True)\nplt.plot(range(93), accuracy_table['accuracy'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Storing the accuracy table for different threshold and then plotting it.\naccuracy_table.sort_values(by = 'params', inplace = True)\nplt.plot(range(93), accuracy_table['accuracy'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can find the elbow using KneeLocator.\nkl = KneeLocator(range(1, 94), accuracy_table['accuracy'], curve=\"concave\", direction=\"increasing\")\nkl.elbow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can select 25 top variables and then fit the model again.\nfeature_imp[:25].index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Selecting the top 25 variables.\ndata_top25 = dataset_pd[feature_imp[:25].index]\nX_top25 = data_top25.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X_top25, label_encoder_y, test_size = 0.33, random_state = 7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Running a XGBoost with default settings with only top 25 variables.\nmodel = xg.XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Hyperparameter Tuning\n\nNumber of trees and size of the tree.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Search for number of trees\nmodel = xg.XGBClassifier(n_thread = -1)\nn_estimators = range(100, 500, 50)\n#max_depth = [2,4,6,8]\nparam_grid = dict(n_estimators = n_estimators)\nkfold = StratifiedKFold(n_splits = 8, shuffle = True, random_state = 7)\ngrid_search = GridSearchCV(model, param_grid, scoring = \"neg_log_loss\", n_jobs = -1, cv = kfold, verbose = 3)\nresult = grid_search.fit(X_top25, label_encoder_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best paramter is %s \" % result.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Mean score for all the paramters tested\npd.DataFrame({\"params\": result.cv_results_['params'], \"mean_score\": result.cv_results_['mean_test_score'],\n             \"std_score\": result.cv_results_['std_test_score']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.errorbar(n_estimators, result.cv_results_['mean_test_score'], yerr = result.cv_results_['std_test_score'])\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"Log Loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Search for learning rate\nmodel = xg.XGBClassifier()\nlearning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\nparam_grid = dict(learning_rate = learning_rate, n_estimators = [150])\nkfold = StratifiedKFold(n_splits = 8, shuffle = True, random_state = 7)\ngrid_search = GridSearchCV(model, param_grid, scoring = \"neg_log_loss\", n_jobs = -1, cv = kfold, verbose = 1)\nresult = grid_search.fit(X, label_encoder_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Best paramter is %s \" % result.best_params_)\nprint(\"Best score is %f\" % result.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame({\"params\": result.cv_results_['params'], \"mean_score\": result.cv_results_['mean_test_score'],\n             \"std_score\": result.cv_results_['std_test_score']})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.errorbar(learning_rate, result.cv_results_['mean_test_score'], yerr = result.cv_results_['std_test_score'])\nplt.xlabel(\"Learning_rate\")\nplt.ylabel(\"Log Loss\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we have optimized 2 paramters, now we will try to build a XGBoost using these new hyperparamters.\n\nFirst we will try with all the variables and then with top 25 variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a predictor matrix (removing the response variable column)\ndataset_train = dataset_pd.values\nX = dataset_train[:,0:93] # Predictors\ny = dataset_train[:,93] # Response \n\n# XGBoost do not take a categorical variable as input. We can use LabelEncoder to assign labels to categorical variables.\nlabel_encoder = LabelEncoder()\nlabel_encoder = label_encoder.fit(y)\nlabel_encoder_y = label_encoder.transform(y)\n\n# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X, label_encoder_y, test_size = 0.33, random_state = 7)\n\n# Running a XGBoost with less column sample.\nmodel = xg.XGBClassifier(n_estimators = 150, learning_rate = 0.2)\nmodel.fit(X_train, y_train)\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X_top25, label_encoder_y, test_size = 0.33, random_state = 7)\n\n# Running a XGBoost with less column sample.\nmodel = xg.XGBClassifier(n_estimators = 150, learning_rate = 0.2)\nmodel.fit(X_train, y_train)\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train and test split of the data\nX_train, X_test, y_train, y_test = train_test_split(X_top25, label_encoder_y, test_size = 0.33, random_state = 7)\n\n# Running a XGBoost with less column sample.\nmodel = xg.XGBClassifier(n_estimators = 150, learning_rate = 0.2, colsample_bytree = 0.7)\nmodel.fit(X_train, y_train)\n# Check the accuracy of the model on train and test dataset.\naccuracy_train = accuracy_score(y_train, model.predict(X_train))\nprint(\"Accuracy on train dataset %.2f%%\" % (accuracy_train * 100))\n\naccuracy_test = accuracy_score(y_test, model.predict(X_test))\nprint(\"Accuracy on test dataset %.2f%%\" % (accuracy_test * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_test = dataset_pd2.values\n# Selecting the top 25 variables.\ndata_top25_test = dataset_pd2[feature_imp[:25].index]\ndataset_test = data_top25_test.values\nprediction_sub = model.predict(dataset_test)\n\n#dataset_pd2[\"prediction\"] = prediction_sub\nX_sub = np.array(prediction_sub).reshape(-1,1)\nonehot_encoder = OneHotEncoder(sparse = False)\nsubmission_file = onehot_encoder.fit_transform(X_sub)\n\nsubmission_file_df = pd.DataFrame(submission_file, \n                                  columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6',\n                                            'Class_7','Class_8','Class_9'], index = dataset_pd2.index)\n\nsubmission_file_df.to_csv(\"submission_otto_ver2.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}