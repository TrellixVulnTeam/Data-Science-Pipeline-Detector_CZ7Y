{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Dropout, ReLU, Input\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"../input/otto-group-product-classification-challenge/train.csv\")\ntest = pd.read_csv(\"../input/otto-group-product-classification-challenge/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(_input_shape, _num_classes):\n    model = Sequential()\n    model.add(Dense(512, input_dim=_input_shape, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(512, input_dim=_input_shape, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(512, input_dim=_input_shape, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(512, activation='relu'))\n    model.add(Dense(_num_classes, activation=\"softmax\"))\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(_train, _test):\n    drop_cols = [\"id\"]\n    target_col = \"target\"\n    feat_cols = [col for col in train.columns if col not in drop_cols + [target_col]]\n\n    data = pd.concat([_train, _test]).reset_index()\n    data_scale = StandardScaler().fit_transform(data[feat_cols])\n    data = pd.concat([data[drop_cols + [target_col]], pd.DataFrame(data_scale)], axis=1)\n\n    _train = data[~data[target_col].isnull()].reset_index(drop=True)\n    _test = data[data[target_col].isnull()].reset_index(drop=True)\n\n    _target = _train[target_col]\n    _train.drop(columns=drop_cols + [target_col], inplace=True)\n    _test.drop(columns=drop_cols + [target_col], inplace=True)\n\n    _target = LabelEncoder().fit_transform(target)\n\n    input_shapes = _train.shape[1]\n    num_classes = np.unique(_target).size\n\n    del data\n    gc.collect()\n    return _train, _test, _target","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"drop_cols = [\"id\"]\ntarget_col = \"target\"\nfeat_cols = [col for col in train.columns if col not in drop_cols + [target_col]]\n\ndata = pd.concat([train, test]).reset_index()\ndata_scale = StandardScaler().fit_transform(data[feat_cols])\ndata = pd.concat([data[drop_cols + [target_col]], pd.DataFrame(data_scale)], axis=1)\n\ntrain = data[~data[target_col].isnull()].reset_index(drop=True)\ntest = data[data[target_col].isnull()].reset_index(drop=True)\n\ntarget = train[target_col]\ntrain.drop(columns=drop_cols + [target_col], inplace=True)\ntest.drop(columns=drop_cols + [target_col], inplace=True)\n\ntarget = LabelEncoder().fit_transform(target)\n\ninput_shapes = train.shape[1]\nnum_classes = np.unique(target).size\n\ndel data\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\nBATCH_SIZE = 512\nNFOLDS = 5\nRANDOM_STATE = 871972\n\nfolds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, \n                        random_state=RANDOM_STATE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test = np.zeros((len(test), 9))\noof = np.zeros((len(train), 9))\nscore = 0\n\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(train, y=target)):\n    print('Fold', fold_n)\n    X_train, X_valid = train.iloc[train_index], train.iloc[valid_index]\n    y_train, y_valid = target[train_index], target[valid_index]\n    model = build_model(input_shapes, num_classes)\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    history = model.fit(train, target, epochs=EPOCHS, batch_size=BATCH_SIZE)\n    y_pred_valid = model.predict(X_valid)\n    oof[valid_index] = y_pred_valid\n    score += log_loss(y_valid, y_pred_valid)\n    y_pred_test += model.predict(test) / NFOLDS\nprint('valid logloss average:', score / NFOLDS, log_loss(target, oof))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submit = pd.read_csv(\"../input/otto-group-product-classification-challenge/sampleSubmission.csv\")\nsubmit = pd.concat([sample_submit[['id']], pd.DataFrame(y_pred_test)], axis = 1)\nsubmit.columns = sample_submit.columns\nsubmit.to_csv('submit.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}