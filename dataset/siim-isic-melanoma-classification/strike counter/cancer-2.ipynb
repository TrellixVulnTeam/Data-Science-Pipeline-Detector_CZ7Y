{"cells":[{"metadata":{"_uuid":"3181182e-05ee-49fe-80bf-dca3dacabe15","_cell_guid":"3e00ae03-c0fa-4f5b-8d29-ac3f316d5e84","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport re\nimport tensorflow.keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c8ffe3c-91cf-424f-94fc-2595e2d4d4d8","_cell_guid":"294c3404-ffa2-498c-85dc-b88fef3723dd","trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\n#REPLICAS = 8\nprint(f'REPLICAS: {REPLICAS}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e9583c6-3da2-4885-8a2b-2774bf6b0152","_cell_guid":"9c0a194c-75d8-45c5-87b9-f2966e4fec13","trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/siim-isic-melanoma-classification/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/siim-isic-melanoma-classification/test.csv\")\nsample_sub = pd.read_csv(\"/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0243c269-d6f0-48ac-b9f4-c08e954cc2ee","_cell_guid":"72247b6f-5ffa-4c6e-9a90-68f1fff993aa","trusted":true},"cell_type":"code","source":"FOLDS = 5\nIMG_SIZE = 512\nBATCH_SIZE = 32\nEPOCHS = 12","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17fee8a8-abca-4238-a32d-4e858aff1d0d","_cell_guid":"3aabc096-7b13-48cd-955f-d5a6180d3d25","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nGCS_PATH2 = KaggleDatasets().get_gcs_path(\"isic2019-512x512\")\nGCS_PATH1 = KaggleDatasets().get_gcs_path(\"melanoma-512x512\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76438a85-3b84-4ef6-9374-46b73741b79a","_cell_guid":"eadf5f02-4a06-464e-9d99-9aa295aa9e22","trusted":true},"cell_type":"code","source":"train_filenames1 = tf.io.gfile.glob(GCS_PATH1 + '/train*.tfrec')\ntrain_filenames2 = tf.io.gfile.glob(GCS_PATH2 + '/train*.tfrec')\ntest_filenames = tf.io.gfile.glob(GCS_PATH1 + '/test*.tfrec')\ndata_filenames = train_filenames1 + train_filenames2","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebb63a99-2a13-4ec3-a1a0-cdda1cb4bd19","_cell_guid":"17034159-df49-4205-baf5-13b9cf093448","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_filenames, valid_filenames = train_test_split(data_filenames, test_size=0.2, random_state=0, shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcaf0c47-6b2f-45be-82bd-da794579e4ed","_cell_guid":"33827990-faf5-42a7-827d-3f403a1ee78d","trusted":true},"cell_type":"code","source":"def read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"target\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = example['image']\n    label = example['target']\n    return image, label # returns a dataset of (image, label) pairs\n\ndef read_unlabeled_tfrecord(example, return_image_name=True):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = example['image']\n    idnum = example['image_name']\n    return image, idnum if return_image_name else 0\n\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fe1e5d9-b8ee-4091-b148-dd2a389b9323","_cell_guid":"80624f00-801f-4e68-95aa-c91fccdba00e","trusted":true},"cell_type":"code","source":" def decode_image(image_data, augment=False):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n    if augment:\n        image = tf.image.random_flip_left_right(image)\n        image = tf.image.random_flip_up_down(image)\n        image = tf.image.random_saturation(image, 0, 2)\n        image = tf.image.rot90(image)\n    image = tf.reshape(image, [IMG_SIZE,IMG_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef get_training_dataset():\n    dataset = tf.data.TFRecordDataset(train_filenames, num_parallel_reads=AUTO)\n    dataset = dataset.repeat() \n    dataset = dataset.shuffle(1024*8)\n    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=True), imgname_or_label), num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE * REPLICAS)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_val_dataset():\n    dataset = tf.data.TFRecordDataset(valid_filenames, num_parallel_reads=AUTO)\n    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=False), imgname_or_label), num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE * REPLICAS)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(ordered=False):\n    dataset = tf.data.TFRecordDataset(test_filenames, num_parallel_reads=AUTO)\n    dataset = dataset.repeat() \n    dataset = dataset.map(lambda example: read_unlabeled_tfrecord(example, False), num_parallel_calls=AUTO)\n    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=False), imgname_or_label), num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE * REPLICAS)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4f863c4-edd9-4899-8e30-dcea479a9047","_cell_guid":"4888c185-915b-47ca-8e6c-b829d906f195","trusted":true},"cell_type":"code","source":"def get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * REPLICAS * batch_size\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cc14ba6-5475-4ea7-a00e-a01a00817b84","_cell_guid":"0899616c-ea13-41a0-a5a4-b9d6f1489081","trusted":true},"cell_type":"code","source":"def build_model(dim=512):\n    inp = tf.keras.layers.Input(shape=(dim,dim,3))\n    base = efn.EfficientNetB0(input_shape=(dim,dim,3),weights='imagenet',include_top=False)\n    x = base(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n    model = tf.keras.Model(inputs=inp,outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \n    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f69323b-5a65-405d-9f02-e18f3ef62802","_cell_guid":"4ecd03b6-1aa9-435b-abf6-2c59b5b1ff77","trusted":true},"cell_type":"code","source":"!pip install yapl==0.1.2 efficientnet > /dev/null","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9a2ae1d-94a7-4bb1-af93-be6422e4fbc7","_cell_guid":"33d5424f-73cf-4066-9cf5-64ebf34281d7","trusted":true},"cell_type":"code","source":"#model 3\nimport efficientnet.tfkeras as efn\ninput_shape = (512, 512, 3)\ndef create_model():\n    model = tf.keras.Sequential([\n        efn.EfficientNetB5(\n                        input_shape=input_shape,\n                        weights='imagenet',\n                        include_top=False\n                    ),\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39ae51cf-2501-4578-9108-27e1ddd38474","_cell_guid":"ab030cef-84cd-4f46-8efd-e6d38d42213a","trusted":true},"cell_type":"code","source":"# with strategy.scope():\n#     model = create_model()\n# optimizer = tf.keras.optimizers.Adam(lr=0.001)\n# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n# model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd0a7473-5ccc-4928-9275-4cb2cf2d7a12","_cell_guid":"1e06d619-c225-456e-87ac-44e8760ea3a3","trusted":true},"cell_type":"code","source":"import math\ncallbacks = [get_lr_callback(BATCH_SIZE)] \nstep_per_epoch = count_data_items(train_filenames)/BATCH_SIZE//REPLICAS","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62b8191e-9cea-43f6-8b36-dcaba3e23a2d","_cell_guid":"521239d2-bfaa-4cc4-ad82-1ef8687feb9a","trusted":true},"cell_type":"code","source":"# warmup_history = model.fit(get_training_dataset(), \n#                            steps_per_epoch=step_per_epoch, \n#                            validation_data=get_val_dataset(),\n#                             epochs=1, \n#                             verbose=1, callbacks=callbacks).history","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"462d0bb3-593d-47a8-8eda-a4d0e028c1dd","_cell_guid":"e46d17ad-7b28-4710-8449-295004ec1482","trusted":true},"cell_type":"code","source":"test_dataset = get_test_dataset(ordered=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6d36085-fb79-4c4c-ab89-4acbf957312c","_cell_guid":"6e2d0700-7802-4968-a410-54ae7202f53a","trusted":true},"cell_type":"code","source":"# test_images_ds = test_dataset.map(lambda image, idnum: image)\n# ct_test = count_data_items(test_filenames)\n# STEPS = ct_test/BATCH_SIZE/4/REPLICAS\n# probabilities = model.predict(test_images_ds, steps=STEPS).flatten()\n# print(probabilities)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bedb4f59-b091-481f-87f0-f8604f20ca85","_cell_guid":"b432a48b-16b9-4d32-8300-2b9b2e488470","trusted":true},"cell_type":"code","source":"# test_imgs = test_dataset.map(lambda images, ids: images)\n# img_ids_ds = test_dataset.map(lambda images, ids: ids).unbatch()\n\n# img_ids = []\n# for coutner, ids in enumerate(img_ids_ds):\n#     if coutner%500 == 0:\n#         print(coutner)\n#     img_ids.append(ids.numpy())\n\n# img_ids = np.array(img_ids).astype('U')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e11592d2-3966-4c01-bf08-943c151f3388","_cell_guid":"3304f6b3-779b-48ef-ae4e-7d43618c2c61","trusted":true},"cell_type":"code","source":"# sample_sub = sample_sub.set_index(\"image_name\").transpose().reindex(columns=list(img_ids)).transpose()\n# sample_sub[\"target\"] = probabilities\n# sample_sub.to_csv(\"submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KFOLD","execution_count":null},{"metadata":{"_uuid":"69c5cff9-868b-45a8-9fea-a915358826a3","_cell_guid":"5cd296f5-a363-43e5-b23f-feb94c59ee43","trusted":true},"cell_type":"code","source":"def get_dataset(filename, shuffle=False, repeat=False, return_img_name=False, augment=False, labeled=False, batch_size=32, dim=512):\n    dataset = tf.data.TFRecordDataset(filename, num_parallel_reads=AUTO)\n    if repeat:\n        dataset = dataset.repeat()\n    if shuffle:\n        dataset = dataset.shuffle(1024*8)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        dataset = dataset.with_options(opt)\n    if labeled:\n        dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    else:\n         dataset = dataset.map(lambda example: read_unlabeled_tfrecord(example, return_img_name), num_parallel_calls=AUTO)\n    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=augment), imgname_or_label), num_parallel_calls=AUTO)\n    dataset = dataset.batch(batch_size * REPLICAS)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_filenames)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"227a2015-8a2b-47d3-9b6d-1820dd096a68","_cell_guid":"26c7d397-4491-4240-bdce-7910d87d977f","trusted":true},"cell_type":"code","source":"FOLDS = 5\nIMG_SIZE = 512\nBATCH_SIZE = 32\nEPOCHS = 12\nweights = 1/6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\nkfold = KFold(n_splits=FOLDS, shuffle=True, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TTA = 11\npreds = np.zeros((count_data_items(test_filenames),1))\nfor fold, (train_file, valid_file) in enumerate(kfold.split(data_filenames)):\n    print(fold)\n    training_dataset = get_dataset([data_filenames[x] for x in train_file], augment=True, shuffle=True, repeat=True, labeled=True)\n    valid_dataset = get_dataset([data_filenames[x] for x in valid_file], augment=True, shuffle=False, repeat=False, labeled=False)\n    K.clear_session()\n    with strategy.scope():\n        model = create_model()\n    if fold == 3:\n        print(\"ok\")\n    optimizer = tf.keras.optimizers.Adam(lr=0.001)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    callbacks = [get_lr_callback(BATCH_SIZE)] \n    steps_per_epoch = count_data_items([data_filenames[x] for x in train_file])/BATCH_SIZE//REPLICAS\n    sv = tf.keras.callbacks.ModelCheckpoint('fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n    callbacks.append(sv)\n    warmup_history = model.fit(training_dataset, steps_per_epoch=steps_per_epoch, validation_data=valid_dataset, epochs=8, verbose=1, callbacks=callbacks).history\n    if fold == 3:\n        print(\"whatup\")\n\n\n    test_dataset = get_dataset(test_filenames, augment=True, repeat=True, batch_size=BATCH_SIZE*4)\n    ct_valid = count_data_items(test_filenames)\n    if fold == 3:\n        print(\"hello\")\n    STEPS = TTA * ct_valid/32/4/REPLICAS\n    pred = model.predict(test_dataset,steps=STEPS,verbose=1)[:TTA*ct_valid,]\n    preds[:, 0] += np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = np.concatenate(preds)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = preds/5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ds = get_dataset(test_filenames, augment=False, repeat=False,\n                 labeled=False, return_img_name=True)\n\nimage_names = np.array([img_name.numpy().decode(\"utf-8\") \n                        for img, img_name in iter(ds.unbatch())])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame(dict(image_name=list(image_names), target=preds))\nsubmission = submission.sort_values('image_name')\nsubmission = submission.set_index(\"image_name\")\nsubmission.to_csv('submission.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n# train_datagen = ImageDataGenerator(\n#         shear_range=0.1,\n#         zoom_range=0.1,\n#         horizontal_flip=True,\n#         rotation_range=10.,\n#         fill_mode='reflect',\n#         width_shift_range = 0.1, \n#         height_shift_range = 0.1)\n# train_datagen.flow(get_dataset(train_filenames, augment=False, shuffle=False, repeat=True, labeled=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# train_file, valid_file = train_test_split(train_filenames, test_size=0.5, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training_dataset = get_dataset(train_file, augment=True, shuffle=True, repeat=True, labeled=True)\n# valid_dataset = get_dataset(valid_file, augment=True, shuffle=False, repeat=True, labeled=False, batch_size=32*5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TTA = 10\n# oof_pred = []\n# test_dataset = get_dataset(test_filenames, augment=True, repeat=True, batch_size=BATCH_SIZE*4)\n# ct_valid = count_data_items(test_filenames)\n# STEPS = TTA * ct_valid/32/4/REPLICAS\n# pred = model.predict(test_dataset,steps=STEPS,verbose=1)       \n# preds = pred[:TTA*ct_valid,]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# preds.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# oof_pred.append( np.mean(preds.reshape((ct_valid,TTA),order='F'),axis=1) ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.zeros((count_data_items(test_filenames),1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sample_sub)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}