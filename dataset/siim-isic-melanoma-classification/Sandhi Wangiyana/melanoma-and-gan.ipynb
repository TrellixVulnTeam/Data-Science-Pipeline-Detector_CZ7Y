{"cells":[{"metadata":{},"cell_type":"markdown","source":"# GAN to assist in Melanoma Detection\n\n[melanoma competition](https://www.kaggle.com/c/siim-isic-melanoma-classification/data)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import glob\nimport os\nimport time\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nprint(tf.__version__)\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom IPython import display","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Why use GAN?\n\n* Dataset target classes are highly imbalance, only 1.76% of malignant\n\n## Interesting topics to try\n\n1. GAN to generate additional malignant class images\n2. Using VAEs to train anomaly detection from benign (non lethal) lesion\n3. GAN to generate SR and additional attention based cropping for train a better lesion classifier and detector","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# asign some paths\ntrain_csv_path = '../input/siim-isic-melanoma-classification/train.csv'\ntest_csv_path = '../input/siim-isic-melanoma-classification/test.csv'\nimage_path = '../input/siim-isic-melanoma-classification/jpeg/train/'\n\n# read the csv data using pandas\ntrain_df = pd.read_csv(train_csv_path)\ntest_df = pd.read_csv(test_csv_path)\n\nprint(\"unique values in column 'target': {}\".format(list(train_df['target'].unique())))\ntarget_dis = list(train_df['target'].value_counts())\nbenign_per = target_dis[0]/sum(target_dis)\nprint(\"target count distribution: {}\".format(target_dis))\nprint(\"benign percentage: {:.2f}% vs malignant: {:.2f}%\".format(benign_per*100, (1-benign_per)*100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. DCGAN to generate Malignant images\n\nSource\n* [code](https://www.tensorflow.org/tutorials/generative/dcgan) \n* [GAN-based Synthetic Medical Image Augmentation](https://arxiv.org/pdf/1803.01229.pdf)\n\nDataset\n* [JPEG 128x128](https://www.kaggle.com/cdeotte/jpeg-melanoma-128x128) melanoma from Chris Deotte\n\nProblems:\n* Generated images are nowhere near the input images\n* Maybe needs some augmentation. [Augmentation in GAN](https://arxiv.org/pdf/2006.05338v1.pdf) ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect and initialize TPU (ignore if using GPU)\n# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#     print('Device:', tpu.master())\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     # set distribution strategy\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# except:\n#     strategy = tf.distribute.get_strategy()\n# print('Number of replicas:', strategy.num_replicas_in_sync)\n\n# # Use these params if using TPU\n# IMAGE_SIZE = [128, 128]  # used for reshaping\n# AUTOTUNE = tf.data.experimental.AUTOTUNE\n# GCS_PATH = KaggleDatasets().get_gcs_path('melanoma-128x128')  # store dataset to gcs buckets for the TPU to access in cloud\n# BATCH_SIZE = 16 * strategy.num_replicas_in_sync","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess Image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"path_tfrec = '../input/melanoma-128x128/'\npath_jpg = '../input/jpeg-melanoma-128x128/train/'\nIMAGE_SIZE = [128, 128]\n\nmalignant = train_df[train_df[\"target\"] == 1]  # list of malignant images\n\ndef preprocess_X():  # load the images into memory\n    X = []\n    for img in malignant.image_name.values:\n        img_name = path_jpg + img + '.jpg'\n        i = tf.keras.preprocessing.image.load_img(img_name) #color_mode='grayscale')\n        i = tf.keras.preprocessing.image.img_to_array(i)\n#         i = preprocess_input(i)  # normalize to range of 0-1\n        i = (i-127.5)/127.5\n        X.append(i)\n    return np.array(X)  # convert to numpy array","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = preprocess_X()\nX.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Display preprocessed image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_img(arr):\n    i = tf.keras.preprocessing.image.array_to_img(arr)\n    plt.imshow(i, cmap='gray')\n\nplt.figure(figsize=(7,7))\nfor i in range(9):\n    plt.subplot(3,3, i+1)\n    display_img(X[i])  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Increase training data with Augmentations\ndef augmentation_pipeline(image):\n    image = tf.image.random_flip_left_right(image)\n#     image = tf.image.resize(image, IMAGE_RESIZE)\n    return image\n\n# Simple dataset processing with batch and shuffle\ndef get_dataset():\n    ds = tf.data.Dataset.from_tensor_slices(X)\n#     ds = ds.map(augmentation_pipeline)\n    ds = ds.shuffle(BUFFER_SIZE)\n    ds = ds.batch(BATCH_SIZE)\n    return ds\n\ndef make_generator_model():\n    model = tf.keras.Sequential()   # dense unit is configured to match soon tobe reshaped layer\n    model.add(layers.Dense(32*32*256, use_bias=False, input_shape=(noise_dim,)))  # starts with 1D array, input is noise array of 100\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    # 32x32 bcz there's 2 conv2D. 128/2/2=32\n    model.add(layers.Reshape((32, 32, 256)))\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n\n    return model\n\ndef make_discriminator_model():\n    model = tf.keras.Sequential()   # basic binary classification model\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[128, 128, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model\n\n# This method returns a helper function to compute cross entropy loss (prob between 0 and 1)\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n# Loss function\ndef discriminator_loss(real_output, fake_output):\n    # ones_like creates array of ones with similar shape as the input array\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n\n# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n\n        for image_batch in dataset:\n            train_step(image_batch)\n\n        # Produce images for the GIF as we go\n        display.clear_output(wait=True)\n        generate_and_save_images(generator,\n                                 epoch + 1,\n                                 seed)\n\n        # Save the model every 15 epochs\n        if (epoch + 1) % 15 == 0:\n            checkpoint.save(file_prefix = checkpoint_prefix)\n\n        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n        # Generate after the final epoch\n        display.clear_output(wait=True)\n        generate_and_save_images(generator, epochs, seed)\n        \ndef generate_and_save_images(model, epoch, test_input):\n    # Notice `training` is set to False.\n    # This is so all layers run in inference mode (batchnorm).\n    predictions = model(test_input, training=False)  # same as num_examples_to_generate\n    fig = plt.figure(figsize=(12,12))\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(3, 3, i+1)\n        plt.imshow(predictions[i, :, :, :] * 127.5 + 127.5, cmap='gray')\n        plt.axis('off')\n\n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# hyperparameters\nBUFFER_SIZE = 584  # the same as number of melanoma images\nBATCH_SIZE = 128  # from 128\nEPOCHS = 100  # from 50\nnoise_dim = 100  # from 100\nnum_examples_to_generate = 9\n\n# We will reuse this seed overtime (so it's easier to visualize progress in the animated GIF)\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = get_dataset()  # batch and shuffle dataset\ngenerator = make_generator_model()  # create generator model\ndiscriminator = make_discriminator_model()  # create D model\n\n# create optimizers\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n\n# create callbacks\ncheckpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(train_dataset, EPOCHS)  # start training!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(train_dataset, EPOCHS)  # start training!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Below are version seperated by cells","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def augmentation_pipeline(image):\n    image = tf.image.random_flip_left_right(image)\n#     image = tf.image.resize(image, IMAGE_RESIZE)\n    return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple dataset processing with batch and shuffle\ndef get_dataset():\n    ds = tf.data.Dataset.from_tensor_slices(X)\n#     ds = ds.map(augmentation_pipeline)\n    ds = ds.shuffle(BUFFER_SIZE)\n    ds = ds.batch(BATCH_SIZE)\n    return ds\n    \ntrain_dataset = get_dataset()\n# inspect a batch\nn_batch = 0\nfor i in train_dataset:\n    n_batch += 1\nprint(f\"num of batch: {n_batch}, shape of each batch: {i.shape}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Generator and Discriminator","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_generator_model():\n    model = tf.keras.Sequential()   # dense unit is configured to match soon tobe reshaped layer\n    model.add(layers.Dense(32*32*256, use_bias=False, input_shape=(noise_dim,)))  # starts with 1D array, input is noise array of 100\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    # 32x32 bcz there's 2 conv2D. 128/2/2=32\n    model.add(layers.Reshape((32, 32, 256)))\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n\n    return model\n\n# create the generator\ngenerator = make_generator_model()\ngenerator.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Display an image generated from noise (G still not trained yet)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"noise = tf.random.normal([1, noise_dim])  # outputs random values from normal dist. to a certain array shape\ngenerated_image = generator(noise, training=False)  # interesting, doesn't need .fit .predict or anything\n\nplt.imshow(generated_image[0, :, :, :]*255)#, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_discriminator_model():\n    model = tf.keras.Sequential()   # basic binary classification model\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[128, 128, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model\n\n# create D\ndiscriminator = make_discriminator_model()\nprint(discriminator.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let the untrained D predict that generated image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"decision = discriminator(generated_image)\nprint(decision)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define Loss and Optimizer","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This method returns a helper function to compute cross entropy loss (prob between 0 and 1)\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Discriminator Loss\nmeasures how well D distinguish real and fake images. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def discriminator_loss(real_output, fake_output):\n    # ones_like creates array of ones with similar shape as the input array\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Generator Loss\nMeasures how well G can trick D. If G is performing well, D will classify fake images as 1 (real)\nthe discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s. Here, we will compare the discriminators decisions on the generated images to an array of 1s.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Optimizers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(1e-4)  # but here they use the same Adam anyway\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create callbacks","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Defining training loop\n\nThe training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fake images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Notice the use of `tf.function`\n# This annotation causes the function to be \"compiled\".\n@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n        real_output = discriminator(images, training=True)\n        fake_output = discriminator(generated_images, training=True)\n\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n\n        for image_batch in dataset:\n            train_step(image_batch)\n\n        # Produce images for the GIF as we go\n        display.clear_output(wait=True)\n        generate_and_save_images(generator,\n                                 epoch + 1,\n                                 seed)\n\n        # Save the model every 15 epochs\n        if (epoch + 1) % 15 == 0:\n            checkpoint.save(file_prefix = checkpoint_prefix)\n\n        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n        # Generate after the final epoch\n        display.clear_output(wait=True)\n        generate_and_save_images(generator, epochs, seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_and_save_images(model, epoch, test_input):\n    # Notice `training` is set to False.\n    # This is so all layers run in inference mode (batchnorm).\n    predictions = model(test_input, training=False)  # same as num_examples_to_generate\n    fig = plt.figure(figsize=(12,12))\n\n    for i in range(predictions.shape[0]):\n        plt.subplot(3, 3, i+1)\n        plt.imshow(predictions[i, :, :, :] * 127.5 + 127.5, cmap='gray')\n        plt.axis('off')\n\n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train(train_dataset, EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. GAN for anomali detection\n\n* Anomali detection in Alzheimer Disease with GAN\n* SHOW RESULTS\n\n* Also other research that has anomali results: HERE HERE and HERE\n* Very effective when positive samples are rare, it's also a how doctors learn to classify\n* But in melanoma, is it really effective? since the difference between benign and malignant images can sometimes be **very subtle**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 2.1 Papers on anomali detection\n\n* [Lesion detection in Brain MRI with constrained adversarial auto-encoder](https://arxiv.org/pdf/1806.04972.pdf)\n* ","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"benign = train_df[train_df[\"target\"] == 0]\nmalignant = train_df[train_df[\"target\"] == 1]\n\ndef show_img(target, n=16):\n    img_name = target.image_name.values\n    ex_img = np.random.choice(img_name, n)  # grab n number of images\n    plt.figure(figsize=(15,15))\n    for i in range(n):\n        plt.subplot(4, 4, i + 1)\n        img = plt.imread(image_path + ex_img[i]+'.jpg')\n        plt.imshow(img, cmap='gray')\n        plt.axis('off')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"show_img(benign)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"show_img(malignant)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}