{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport cv2\nimport gc\nfrom keras.utils import to_categorical\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(dataset):\n\n    dataset['sex'].fillna(\"no sex\", inplace = True)\n    dataset['age_approx'].fillna(0, inplace = True)\n    dataset['anatom_site_general_challenge'].fillna(\"NA\", inplace = True)\n    dataset = dataset.replace(to_replace = ['male'], value = 0)\n    dataset = dataset.replace(to_replace = ['female'], value = 1)\n    dataset = dataset.replace(to_replace = ['no sex'], value = 2)\n    dataset = dataset.replace(to_replace = ['torso'], value = 0)\n    dataset = dataset.replace(to_replace = ['lower extremity'], value = 1)\n    dataset = dataset.replace(to_replace = ['upper extremity'], value = 2)\n    dataset = dataset.replace(to_replace = ['head/neck'], value = 3)\n    dataset = dataset.replace(to_replace = ['NA'], value = 4)\n    dataset = dataset.replace(to_replace = ['palms/soles'], value = 5)\n    dataset = dataset.replace(to_replace = ['oral/genital'], value = 6)\n    \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = preprocessing(pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/test.csv'))\ndf_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"IMAGE_HEIGHT = 300\nIMAGE_WIDTH = 300\nTOTAL_SAMPLES = 1440","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 8\nSHUFFLE_SIZE = TOTAL_SAMPLES\nSTEPS_PER_EPOCH = int(TOTAL_SAMPLES/BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _parse_function_train(proto):\n    # define your tfrecord again. Remember that you saved your image as a string.\n    keys_to_features = {'image_raw': tf.io.FixedLenFeature([], tf.string),\n                        'target': tf.io.FixedLenFeature([], tf.int64),\n                        'sex': tf.io.FixedLenFeature([], tf.int64),\n                        'age_approx': tf.io.FixedLenFeature([], tf.float32),\n                       'anatom_site_general_challenge': tf.io.FixedLenFeature([],tf.int64)}\n    \n    # Load one example\n    parsed_features = tf.io.parse_single_example(proto, keys_to_features)\n    \n    image_shape = tf.stack([IMAGE_HEIGHT,IMAGE_WIDTH,3])\n\n    parsed_features['image_raw'] = tf.io.decode_jpeg(parsed_features['image_raw'], 3)#, fixed_length = 256*256*3)\n    \n    parsed_features['image_raw'] = tf.reshape(parsed_features['image_raw'], image_shape)\n        \n    parsed_features['image_raw'] = tf.image.random_flip_left_right(parsed_features['image_raw'])\n    \n    parsed_features['image_raw'] = tf.image.random_flip_up_down(parsed_features['image_raw'])\n    \n    parsed_features['image_raw'] = tf.image.adjust_saturation(parsed_features['image_raw'],4)\n        \n    return parsed_features['image_raw'], parsed_features['sex'], parsed_features['age_approx'], parsed_features['anatom_site_general_challenge'],parsed_features['target']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def dataset_fetch (filenames, isTrain):\n    \n    dataset = tf.data.TFRecordDataset(filenames)\n    \n    if(isTrain == True):\n        dataset = dataset.repeat()\n\n    dataset = dataset.map(_parse_function_train)\n        \n    dataset = dataset.shuffle(SHUFFLE_SIZE)\n\n    dataset = dataset.batch(BATCH_SIZE)\n        \n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n            \n    return dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_dataset = dataset_fetch('../input/melanoma-tfrecord/train(2).tfrecords',True)\ntraining_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validation_dataset = dataset_fetch('../input/melanoma-classification-eda/validation.tfrecords',True)\nvalidation_dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fetch_data(iterator):\n    while True:\n        image,sex,age,site,target = iterator.get_next()\n        target = to_categorical(target)\n        yield ([image,age],target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fetch_validation_data(val_iterator):\n    while True:\n        image,sex,age,site,target = val_iterator.get_next()\n        target = to_categorical(target)\n        yield ([image,age],target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iterator_train = iter(training_dataset)\ntraining_dataset_final = fetch_data(iterator_train)\n\nprint(training_dataset_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iterator_val = iter(validation_dataset)\nvalidation_dataset_final = fetch_validation_data(iterator_val)\n\nprint(validation_dataset_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_input = tf.keras.layers.Input(shape = (IMAGE_HEIGHT,IMAGE_WIDTH,3))\nx1 = tf.keras.applications.Xception(weights = 'imagenet', include_top = False)(image_input)\nx1 = tf.keras.layers.MaxPooling2D((2,2))(x1)\nx1 = tf.keras.layers.Flatten()(x1)\nx1 = tf.keras.layers.Dense(128, activation = 'relu')(x1)\nx1 = tf.keras.layers.Dropout(0.2)(x1)\n# x1 = tf.keras.layers.Dense(128, activation = 'relu')(x1)\n# x1 = tf.keras.layers.Dropout(0.2)(x1)\nimage = tf.keras.layers.Dense(32, activation = 'relu',kernel_regularizer = tf.keras.regularizers.l2())(x1)\n\n\nage_input = tf.keras.layers.Input(shape = (1))\n# x2 = tf.keras.layers.Dense(128,activation = 'relu')(age_input)\nx2 = tf.keras.layers.Dense(64, activation = 'relu')(age_input)\nx2 = tf.keras.layers.Dropout(0.2)(x2)\nage = tf.keras.layers.Dense(32, activation = 'relu',kernel_regularizer = tf.keras.regularizers.l2())(x2)\n\n# gender_input = tf.keras.layers.Input(shape = (1))\n# # x3 = tf.keras.layers.Dense(128,activation = 'relu')(gender_input)\n# x3 = tf.keras.layers.Dense(64, activation = 'relu')(gender_input)\n# x3 = tf.keras.layers.Dropout(0.2)(x3)\n# gender = tf.keras.layers.Dense(32, activation = 'relu',kernel_regularizer = tf.keras.regularizers.l2())(x3)\n\n# site_input = tf.keras.layers.Input(shape = (1))\n# # x4 = tf.keras.layers.Dense(128,activation = 'relu')(site_input)\n# x4 = tf.keras.layers.Dense(64, activation = 'relu')(site_input)\n# x4 = tf.keras.layers.Dropout(0.2)(x4)\n# site = tf.keras.layers.Dense(32, activation = 'relu',kernel_regularizer = tf.keras.regularizers.l2())(x4)\n\nmerge1 = tf.keras.layers.concatenate([image,age])\n\nop = tf.keras.layers.Dense(64, activation = 'relu')(merge1)\nop = tf.keras.layers.Dropout(0.3)(op)\n# op = tf.keras.layers.Dense(64, activation = 'relu')(op)\nop = tf.keras.layers.Dense(16, activation = 'relu',kernel_regularizer = tf.keras.regularizers.l2())(op)\noutput_final = tf.keras.layers.Dense(2, activation = 'softmax')(op)\n\nmodel = tf.keras.models.Model(inputs = [image_input,age_input], outputs = output_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile( optimizer=tf.keras.optimizers.Adamax(),\n    loss='binary_crossentropy',\n    metrics=['accuracy'],)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_lrfn(lr_start=0.00001, lr_max=0.0001, \n               lr_min=0.000001, lr_rampup_epochs=20, \n               lr_sustain_epochs=0, lr_exp_decay=.8):\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn\n\nlrfn = build_lrfn()\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(training_dataset_final,\n          epochs = 50, \n          #validation_data = validation_dataset_final,validation_steps = 150,\n         steps_per_epoch = STEPS_PER_EPOCH)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_list = np.array([], dtype = 'float32')\nprint(predictions_list)\nfor row in df_test.iterrows():\n    im_test = cv2.imread('/kaggle/input/siim-isic-melanoma-classification/jpeg/test/' + row[1]['image_name'] + '.jpg')\n    im_resize = np.reshape(cv2.resize(im_test, (IMAGE_HEIGHT, IMAGE_WIDTH)), (1,IMAGE_HEIGHT,IMAGE_WIDTH,3))\n    predictions_list = np.append(predictions_list,\n                                 \n                                 model.predict([im_resize,\n#                                                 np.reshape(row[1]['sex'],(1)),\n                                                np.reshape(row[1]['age_approx'],(1)),\n#                                                 np.reshape(row[1]['anatom_site_general_challenge'],(1)),\n                                               ]\n                                              )[0][1])\n\n    print(predictions_list.shape, end = \"\\r\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_elements, counts_elements = np.unique(predictions_list, return_counts=True)\n\nprint(np.asarray((unique_elements, counts_elements)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_image_name = df_test['image_name'].to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.DataFrame({\"image_name\":test_image_name, \"target\":predictions_list})\nsample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\",index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}