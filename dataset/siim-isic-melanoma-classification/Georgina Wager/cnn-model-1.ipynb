{"cells":[{"metadata":{},"cell_type":"markdown","source":"CNN Model 1 "},{"metadata":{"trusted":true},"cell_type":"code","source":"## Reproducibility\n## Random seed given\nimport random ## import the random library\nrandom.seed(10) ## Setting the seed to get the same answer no matter how many times and who runs the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Downloading specific libraries\nimport numpy as np ## Library that enables linear functions \nimport pandas as pd ##  # Enables data processing\nimport glob ## returns an array of filenames that match a pattern\nimport cv2 ## helps add labels to image classifications\nimport matplotlib.pyplot as plt ## library for producing figures\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img ## importing image processing packages from \n## keras\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout ## Import libraries for model building ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Read the train csv file\ntrain_dir='/kaggle/input/siim-isic-melanoma-classification/jpeg/train/' ## assigning a name to the location of the train images\ntrain=pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/train.csv') ## assigning a name to the location of the CSV file\n\n## Read the test csv file\ntest_dir='/kaggle/input/siim-isic-melanoma-classification/jpeg/test/' ## assigning a name to the location of the test images\ntest=pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/test.csv') ## assigning a name to the location of the CSV file ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Finding the unique patient ids from train csv file\nprint(f\"The total patient ids are {train['patient_id'].count()}, from those the unique ids are {train['patient_id'].value_counts().shape[0]} \")\n\n## Finding the unique patient ids from test csv file\nprint(f\"The total patient ids are {test['patient_id'].count()}, from those the unique ids are {test['patient_id'].value_counts().shape[0]} \")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['path'] = train_dir + train.image_name + \".jpg\" ## adding the location of the image to the row for the train data set\ntrain.head() ## showing the first 5 lines of the train data set, note the \"path\" coloumn \n\ntest['path'] = test_dir + test.image_name + \".jpg\"  ## adding the location of the image to the row for the test data set\ntest.head()  ## showing the first 5 lines of the test data set, note the \"path\" coloumn ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Class Distribution\ntrain.target.value_counts() ## Count the number of images that were classified as malinnent or non malignent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_0=train[train['target']==0].sample(1000) ## produce a data frame using 1000 images from the train data set where the target equals zero\ndf_1=train[train['target']==1] ## produce a data frame using all the images from the test data set where the target equals 584\ntrain=pd.concat([df_0,df_1]) ## create a new dataset using the smaller training data set\ntrain=train.reset_index() ## making sure the new \"train\" data set is being used for the model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape ## how many observations and variables are in the training set being used for the model ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head() ## First 5 rows of the new train set  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will resize the given images to 150 x 150 size images for faster processing\nIMG_DIM = (150, 150) ## changing the image dimensions ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split ## importing the train test split function \nX_train, X_val, y_train, y_val = train_test_split(train, train.target, test_size=0.2, random_state=42) ## taking 20% of the training data set ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_files = X_train.path ## Image path for the training data set\nval_files = X_val.path ## Image path for the validation data set\n\ntrain_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files] ## load images using load_img function from keras \n## preprocessing using the target_size function \nvalidation_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in val_files] ## using the img_to_array will tranform the loaded image to an array\n\ntrain_imgs = np.array(train_imgs) ## converting the list of arrays to array for the training dataset \ntrain_labels = y_train\n\nvalidation_imgs = np.array(validation_imgs) ## converting the list of arrays to array for the validation dataset \nval_labels = y_val\n\n\nprint('Train dataset shape:', train_imgs.shape, \n      '\\tValidation dataset shape:', validation_imgs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Scale Images\n## scale each image with pixel values between (0, 255) to values between (0, 1) because deep learning models work really\n## well with small input values.\ntrain_imgs_scaled = train_imgs.astype('float32')\n\nvalidation_imgs_scaled  = validation_imgs.astype('float32')\n\n# divide the pixels by 255 to scale the pixels between 0 and 1\ntrain_imgs_scaled /= 255\nvalidation_imgs_scaled /= 255\n\nprint(train_imgs[0].shape)\n\narray_to_img(train_imgs[0]) ## using the array_to_img function will convert the given array to image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# setup basic configuration\nbatch_size = 30 ## indicating the total number of images passed to the model per iteration\nnum_classes = 2 \nepochs = 30 ## establishing the training time \ninput_shape = (150, 150, 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random ## import the random library\nrandom.seed(10) ## Setting the seed to get the same answer no matter how many times and who runs the model\n\nfrom keras.models import Sequential ## importing the sequential library\nfrom keras import optimizers ## importing optimizers\n\nmodel = Sequential() ## creating and instance of Sequential\n\nmodel.add(Conv2D(16, kernel_size=(3, 3), activation='relu', \n                 input_shape=input_shape))\n# Pooling layer used here will select the largest values on the feature maps and use these as inputs to subsequent layers\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n\n# another set of Convolutional & Max Pooling layers\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\n# Finally the Dense Layer\nmodel.add(Dense(512, activation='relu'))\n# sigmoid function here will help perform binary classification\nmodel.add(Dense(1, activation='sigmoid'))\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=optimizers.RMSprop(),\n              metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(x=train_imgs_scaled, y=train_labels,\n                    validation_data=(validation_imgs_scaled, val_labels),\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('CNN Model 1', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = list(range(1,31))\nax1.plot(epoch_list, history.history['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, history.history['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 31, 5))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, history.history['loss'], label='Train Loss')\nax2.plot(epoch_list, history.history['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 31, 5))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The model ran shows that there is a level of overfitting, the train accuracy continues to increase until it gets to 100%, the validation accuracy begins to fall of at 70-75%. The train loss declined at around 75% right down to zero, while the validation loss continues to increase. Overfitting is a result of not enough images to train the model."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}