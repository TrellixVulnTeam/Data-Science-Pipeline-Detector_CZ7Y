{"cells":[{"metadata":{},"cell_type":"markdown","source":"Welcome. In this notebook I shall give a detailed overview of the popular tabular approach that a lot of people that are using. I used Dieter's notebook as a baseline and I will give you an overview of what this actually is.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"### Import libraries\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras.applications.densenet import preprocess_input, DenseNet201\n## define params\ntrain_df = pd.read_csv('../input/siim-isic-melanoma-classification/train.csv')\nimg_size = 256\nbatch_size = 64","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">> **Summary of above cell:**\n+ It defines inputs and basic parameters such as batch size and image size.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"ids = train_df['image_name'].values\n# Takes the id of each image\nn_batches = len(ids) // batch_size + 1\n# Number of batches = length of ids divided by batch size + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">> **Summary of above cell:**\n+ Defines ids (list of images)\n+ Sets number of batches.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Resize to square\ndef resize_to_square(im):\n    # Old size\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(img_size)/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    # We use this new size to resize images\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    # Delta width is the change in width\n    ## Same for delta height\n    top, bottom = delta_h//2, delta_h-(delta_h//2)\n    left, right = delta_w//2, delta_w-(delta_w//2)\n    ## Define top and bottom dim\n    ## Define left and right too\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    ## Square image\n    return new_im\n\ndef load_image(path, ids):\n    image = cv2.imread(f'{path}{ids}-1.jpg') # read new image\n    new_image = resize_to_square(image) # resize to square\n    new_image = preprocess_input(new_image) # now preprocess inputs for DenseNet\n    return new_image","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">> Summary of above cell:\n+ Resizes to square:\n    + This defines all the necessary parameters for a resize (the dimensions and length of images(\n+ Loads image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\n\n# input image\ninp = Input((256,256,3))\n# DenseNet model\nbackbone = DenseNet121(input_tensor = inp, include_top = False)\n# To make sure we do not load the full thing\n# we load the densenet output\nx = backbone.output\n# Make the output smaller (from 1024 output params)\nx = GlobalAveragePooling2D()(x)\n# Expands dimensions (very useful)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n# Finally pools to 4\nx = AveragePooling1D(4)(x)\n# final output\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">> Summary of above cell:\n+ Defines model with DenseNet\n+ Post-processes input from 1024","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features = {}\nfor b in tqdm_notebook(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"../input/siim-isic-melanoma-classification/train_images/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,ids in enumerate(batch_pets):\n        features[ids] = batch_preds[i]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":">> Summary of above cell:\n+ Applies our model to extract features\n+ Predicts the features\n+ Creats a dataframe for our features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_feats = pd.DataFrame.from_dict(features, orient='index')\ntrain_feats.to_csv('train_feats.csv') # convert to csv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}