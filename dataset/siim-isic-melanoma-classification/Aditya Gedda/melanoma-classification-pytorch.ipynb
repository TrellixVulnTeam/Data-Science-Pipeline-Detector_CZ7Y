{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nimport os\nimport glob\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as model\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor \nimport cv2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyDataSet(torch.utils.data.Dataset):\n\n    def __init__(self, image_path, targets, transforms=None):\n        self.image_path = image_path\n        self.targets = targets\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.image_path)\n\n    def __getitem__(self, item):\n        image = self.image_path[item]\n        targets = self.targets[item]\n        #img = Image.open(image)\n        #img = np.array(img)\n        #print(img.shape)\n        img = cv2.imread(image,cv2.cv2.IMREAD_COLOR)\n        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        \n        if self.transforms is not None:\n            sample = {'image':img}\n            sample = self.transforms(**sample)\n            img = sample['image']\n            #img = self.transforms(img)\n            #print(img.shape)\n        else:\n            img = np.transpose(img, (2, 0, 1))\n        return torch.tensor(img, dtype=torch.float), torch.tensor(targets, dtype=torch.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fold(fold):\n    training_path = \"../input/melanoma-custom/Data/Data/Train_512\"\n    df = pd.read_csv('../input/melanoma-custom/train_Kfold.csv')\n\n    df_train = df[df['k-fold'] != fold].reset_index(drop=True)\n    df_valid = df[df['k-fold'] == fold].reset_index(drop=True)\n\n    train_images = list(df_train.image_name)\n    train_images = [os.path.join(training_path,i+'.jpg') for i in train_images]\n    train_targets = df_train.target.values\n\n    valid_images = list(df_valid.image_name)\n    valid_images = [os.path.join(training_path,i+'.jpg') for i in valid_images]\n    valid_targets = df_valid.target.values\n\n    train_transform = A.Compose([\n        A.RandomRotate90(),\n        A.HorizontalFlip(),\n        A.VerticalFlip(),\n        A.RGBShift(r_shift_limit=40),\n        A.MultiplicativeNoise(p=1.0),\n        A.RandomBrightness(0.1),\n        A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ToTensor()\n    ])\n\n    valid_transform = A.Compose([\n        A.RandomRotate90(),\n        A.RandomBrightness(0.1),\n        A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ToTensor()\n    ])\n\n    train_dataset = MyDataSet(train_images, train_targets, train_transform)\n    valid_dataset = MyDataSet(valid_images, valid_targets, valid_transform)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=256,shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=256, shuffle=False)\n\n    return train_loader, valid_loader,valid_targets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Resnet_152(nn.Module):\n  def __init__(self,model):\n    super(Resnet_152, self).__init__()\n    self.model = model\n    self.ext = nn.Sequential(\n        nn.ReLU(),\n        nn.Linear(100,1)\n    )\n\n  def forward(self,images):\n    out = self.model(images)\n    out = self.ext(out)\n    return out\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntorch.cuda.empty_cache()\n\nprint(device)\n\nmodel = model.resnet152(pretrained=True)\n\nfor param in model.parameters():\n    param.requires_grad=False\n\n\nin_features = model.fc.in_features\nmodel.fc = nn.Linear(in_features, 100)\n\n\nmymodel = Resnet_152(model)\n\nmymodel = mymodel.to(device)\ncriteria = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.AdamW(mymodel.parameters(), lr=0.0001,weight_decay=0.0001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, mode='max',verbose=True)\n\n#es = EarlyStopping(patience=5, mode=\"max\")\n\nfor i in range(5):\n    print('Fold={}'.format(i))\n    tr_loader, val_loader,targets = train_fold(i)\n    epochs = 10\n    for j in range(epochs):\n        loss_arr = []\n        mymodel.train()\n        for data in tr_loader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            del data\n            optimizer.zero_grad()\n            output = mymodel(images)\n            #print(output)\n            loss = criteria(output, labels.unsqueeze(1))\n            loss.backward()\n            optimizer.step()\n            loss_arr.append(loss.item())\n            del images,labels\n        print(\"epoch={},loss={}\".format(j, sum(loss_arr)/len(loss_arr)))\n        mymodel.eval()\n        final_predictions = []\n        for val_data in val_loader:\n            val_images, val_labels = val_data\n            val_images, val_labels = val_images.to(device), val_labels.to(device)\n            del  val_data\n            with torch.no_grad():\n                val_output = mymodel(val_images)\n                #proba, pred = torch.max(val_output.data, 1)\n                #print(val_output)\n                val_output = torch.sigmoid(val_output)\n                pred = val_output.cpu()\n                #final_predictions.extend(pred)\n                final_predictions.append(pred)\n                del val_images, val_labels\n        #predictions = np.array(final_predictions)\n        predictions = np.vstack(final_predictions).ravel()\n        k=roc_auc_score(targets, predictions)\n        #l=accuracy_score(targets, predictions)\n        print('val_auc_acore={}'.format(k))\n        scheduler.step(k)\n    torch.save(mymodel.state_dict(), 'resnet152_{}.pth'.format(i))   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Testing\ndf = pd.read_csv(\"../input/melanoma-custom/test.csv\")\ntest_path = \"../input/melanoma-custom/Test_512/Test_512\"\ntest_images = list(df['image_name'])\ntest_images = [os.path.join(test_path,i+'.jpg') for i in test_images]\ntest_transform = A.Compose([\n    A.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n    ToTensor()\n])\ntargets = np.zeros(len(test_images))\ntest_dataset = MyDataSet(test_images, targets , test_transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nmymodel.eval()\nfinal_predictions = []\nfor test_data in test_loader:\n    test_images,test_labels = test_data\n    test_images = test_images.to(device)\n    with torch.no_grad():\n        test_output = mymodel(test_images)\n        #proba, pred = torch.max(val_output.data, 1)\n        #print(val_output)\n        test_output = torch.sigmoid(test_output)\n        pred = test_output.cpu()\n        #final_predictions.extend(pred)\n        final_predictions.append(pred)\n        del test_images,test_data\n#predictions = np.array(final_predictions)\npredictions = np.vstack(final_predictions).ravel()\nsample = pd.read_csv(\"../input/siim-isic-melanoma-classification/sample_submission.csv\")\nsample.loc[:, \"target\"] = predictions\nsample.to_csv(\"submission.csv\", index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}