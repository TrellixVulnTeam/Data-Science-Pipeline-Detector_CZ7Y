{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-16T20:48:14.645177Z","iopub.execute_input":"2022-03-16T20:48:14.645459Z","iopub.status.idle":"2022-03-16T20:48:14.651034Z","shell.execute_reply.started":"2022-03-16T20:48:14.645429Z","shell.execute_reply":"2022-03-16T20:48:14.650068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Thinking About Different Options**\nHere we will explore some potentially useful features for our model.\n\nSome ideas:\n1. Is it possible to get individual player stats for each season?\n    - cumulative stats prior to the start of each game would be ideal because it would be independent of the team id\n    - possible pitfall is that we must predict the entire tournament prior to its start\n    - this would be less history dependent (less overfitting to the past?)\n    - this does not currently seem possible (outside of top 100 players per category)\n    - maybe can be used as a feature (i.e. # of players in top 100 for each category)\n2. Some useful outside data can be found here: https://www.teamrankings.com/ncb/rankings/\n    - try building a random forest model using all the different kinds of ratings, only\n    - need to download data to notebook\n3. Develop new features for each team for each season leading up to the tournament\n    - win percentage\n    - field goal percentage\n    - 3-point percentage\n    - free throw percentage\n    - offensive rebounds\n    - defensive rebounds\n    - assists\n    - turnovers (assist/turnover ratio seems relevant)\n    - steals\n    - blocks\n    - personal fouls\n4. Do the same thing as 3. but use the team rank in each of these categories instead\n    - i.e. rank 1 Eastern Ky. vs rank 9 Alabama in 3-pt FG attempts\n    - data source https://www.ncaa.com/stats/basketball-men/d1/current/team/625\n5. Develop features based on $n$ previous games\n    - this is the idea that recent performance is most important in determining who wins\n    - for each game, features are computed for both teams based on previous $n$ games (need to handle season overlap)\n    - this might be bad for a tournament since you won't know how a team plays throughout the tournament, only the last games of the season are available\n    - game data is available in `MRegularSeasonDetailedResults.csv`\n    \nCurrent Plan:\n- Create a dataframe that has the features described below of each team indexed by (team, season)\n- For the year to be predicted, we will weight the results of the previous $n$ years by exponential decay factor $\\lambda$. I.e. for season Y\n    $$F_{Y} = (1 - \\lambda) F_{Y} + (1 - \\lambda)\\lambda F_{Y - 1} + (1 - \\lambda)\\lambda^2 F_{Y - 2} + \\cdots = \\sum_{t = 0}^n (1 - \\lambda) \\lambda^t F_{Y - t}$$\n- Build a random forest (or xgboost) model and cross-validate to select hyperparameters\n- List of features (per team per season):\n    - \"AverageRank\": average ordinal ranking\n    - \"PPG\": points per game\n    - \"W-L%\": Win percentage\n    - \"SOS\": strength of schedule\n    - \"FTr\": free throw attempt rate\n    - \"3PAr\": 3-point attempt rate\n    - \"TRB%\": Total rebound percentage\n    - \"AST%\": Total assist percentage\n    - \"BLK%\": Total block percentage\n    - \"eFG%\": Effective field goal percentage\n    - \"TOV%\": Turnover percentage\n    - \"FT/FGA\": Free throw per field goal attempt\n\nData Source: https://www.sports-reference.com/cbb/seasons/ or https://www.kaggle.com/thomashopkins32/mncaa-additional-data-2022","metadata":{}},{"cell_type":"code","source":"STAGE_1_DIR = '/kaggle/input/mens-march-mania-2022/MDataFiles_Stage1/'\nSEASON_DIR = '/kaggle/input/mncaa-additional-data-2022/'\nteams_df = pd.read_csv(STAGE_1_DIR + 'MTeams.csv')\nteams_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T20:48:14.652694Z","iopub.execute_input":"2022-03-16T20:48:14.653008Z","iopub.status.idle":"2022-03-16T20:48:14.68188Z","shell.execute_reply.started":"2022-03-16T20:48:14.652974Z","shell.execute_reply":"2022-03-16T20:48:14.68105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's get into cleaning the data! Note: there was a weird encoding error when reading `MTeamSpellings.csv`. Adding `ecoding='cp1252'` seems to fix it.","metadata":{}},{"cell_type":"markdown","source":"## Processing Data","metadata":{}},{"cell_type":"code","source":"import glob\n\n\ndef compute_average_rank(teamID, season, rankings_df=None):\n    if rankings_df is None:\n        rankings_df = pd.read_csv(STAGE_1_DIR + 'MMasseyOrdinals.csv')\n    ranks = rankings_df[(rankings_df.TeamID == teamID) & (rankings_df.Season == season)]\n    return ranks.OrdinalRank.mean()\n\n\ndef clean_season_data(dataframe, season, teamIDs=None, rankings=None):\n    ''' Cleans the data by removing rows, filling in NA values, etc. '''\n    print(f'Processing data for {season} season')\n    # drop columns that we do not need\n    dataframe = dataframe.drop(['Rk', 'G', 'W', 'L', 'SRS', 'Unnamed: 8', 'W.1',\n                                'L.1', 'Unnamed: 11', 'W.2', 'L.2', 'Unnamed: 14',\n                                'W.3', 'L.3', 'Unnamed: 17', 'Tm.', 'Opp.',\n                                'Unnamed: 20', 'Pace', 'ORtg', 'TS%', 'STL%',\n                                'ORB%'], axis=1)\n    # convert team names to team id\n    dataframe.School = dataframe.School.apply(lambda x: x.lower().replace('ncaa', '').strip())\n    if teamIDs is None:\n        teamIDs = pd.read_csv(STAGE_1_DIR + 'MTeamSpellings.csv', encoding='cp1252')\n    dfIDs = []\n    ranks = []\n    for i, sch in enumerate(dataframe.School):\n        team = teamIDs[teamIDs.TeamNameSpelling.str.lower() == sch]\n        if len(team) == 0:\n            # some entries were not in the data (see table below)\n            if sch == 'purdue-fort wayne':\n                schid = 1236\n            elif sch == 'cal state long beach':\n                schid = np.nan\n            elif sch == 'kansas city':\n                schid = 1282\n            elif sch == 'st. thomas (mn)':\n                schid = 1472\n            else:\n                schid = int(input(sch))\n        else:\n            schid = int(team.TeamID.iloc[0])\n        dfIDs.append(schid)\n        ranks.append(compute_average_rank(schid, int(season),\n                                          rankings_df=rankings))\n    dataframe['ID'] = dfIDs\n    \n    # add season column\n    season_data = [int(season)] * len(dataframe)\n    dataframe['Season'] = season_data\n    \n    # add rank column\n    dataframe['Rank'] = ranks\n    \n    return dataframe\n\n\ndef process_season_data(datadir):\n    ''' Reads in the data, cleans, and returns a dataframe '''\n    data = glob.glob(datadir + '*.csv')\n    teamIDs = pd.read_csv(STAGE_1_DIR + 'MTeamSpellings.csv', encoding='cp1252')\n    rankings_df = pd.read_csv(STAGE_1_DIR + 'MMasseyOrdinals.csv')\n    dataframes = []\n    for d in data:\n        df = pd.read_csv(d, header=1)\n        df = clean_season_data(df, '20' + d.rstrip('.csv')[-2:],\n                               teamIDs=teamIDs, rankings=rankings_df)\n        dataframes.append(df)\n    return pd.concat(dataframes)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T20:48:14.683586Z","iopub.execute_input":"2022-03-16T20:48:14.683828Z","iopub.status.idle":"2022-03-16T20:48:14.701381Z","shell.execute_reply.started":"2022-03-16T20:48:14.683799Z","shell.execute_reply":"2022-03-16T20:48:14.700422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the additional data I gathered for my model, some of the names are not present in the `MTeamSpellings.csv` so we have to manually search for them.\n\nHere is a list of ones that I found so far:\n\n|Team Name | ID|\n|------------|---------------|\n|purdue-fort wayne | 1236|\n|cal state long beach | NA|\n|kansas city | 1282|\n| st. thomas (mn) | 1472 |","metadata":{}},{"cell_type":"code","source":"team_season_stats = process_season_data(SEASON_DIR)\nteam_season_stats.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T20:48:14.702694Z","iopub.execute_input":"2022-03-16T20:48:14.702944Z","iopub.status.idle":"2022-03-16T20:50:19.087525Z","shell.execute_reply.started":"2022-03-16T20:48:14.702915Z","shell.execute_reply":"2022-03-16T20:50:19.086753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see some statistics about all of the teams across all of the seasons. Some interesting ones are the features with high standard deviation (`std`). Assists and strength of schedule (SOS) seem to have the greatest deviation. The standard deviation among free throw and 3-point attempt rate is low indicating that all of the teams are similar in the number of attempts for these categories.","metadata":{}},{"cell_type":"code","source":"team_season_stats.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T20:50:19.089416Z","iopub.execute_input":"2022-03-16T20:50:19.089678Z","iopub.status.idle":"2022-03-16T20:50:19.139895Z","shell.execute_reply.started":"2022-03-16T20:50:19.089648Z","shell.execute_reply":"2022-03-16T20:50:19.139073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With that out of the way, we can get to building our model. Every time we encounter a (season, team, team) trio in the training data, we will query the `team_season_stats` data frame to get each team's statistics over the past $n$ seasons. The features for a team in a particular season will be a weighted sum of the previous $n$ seasons with a weight factor that decays exponentially. If you are familiar with reinforcement learning, this is similar to the idea of $n$-step TD returns.\n\nWe will then concatenate the two feature vectors and use them as input for our model.\n\nThere are some interesting considerations to make about the training/validation split:\n- Should we only train using the Regular Season games and validate on Tournament games?\n- Should we use a random set split derived from Regular Season, Tournament, and Conference games?\n    - For stage 1, we cannot use Tournament games from 2016-2021 to train since this is what we will submit as predictions\n    - For stage 2, this option looks good\n- Is there data leakage in our feature space?\n    - If we only use data from previous seasons, I don't think this will be an issue.\n    \nCurrent Plan:\n- Train the model on all regular season and tournament games\n- Concatenate season stats (maybe try difference?)\n- If a game is in regular season, feature vector is created only from previous seasons\n- If a game is a tournament game, feature vector includes the regular season that occurred right before it in the same year\n- We don't have team stats from before 2003, so we are restricted to training on seasons after 2003.","metadata":{}},{"cell_type":"code","source":"def get_features(stats_df, team_id, season, n=4, discount=0.5):\n    '''\n    Computes the feature vector of the team over the past n seasons\n    \n    Parameters\n    ----------\n    stats_df : pd.DataFrame\n        Table that maps (team_id, season) to feature vector\n    team_id : str\n        The unique identifier for the team\n    season : int\n        Current season\n    n : int\n        Number of seasons to average over the past\n    discount : float\n        Exponential decay factor to apply to averaging\n    \n    Returns\n    -------\n    features : np.array\n        The team's features averaged over the previous n seasons.\n    '''\n    team_and_seasons = stats_df[(stats_df.ID == team_id) &\n                                (season - n <= stats_df.Season) &\n                                (stats_df.Season < season)]\n    sorted_feature_table = team_and_seasons.sort_values('Season', ascending=False)\n    ordered_features = sorted_feature_table[\n        sorted_feature_table.columns.difference(['School', 'ID', 'Season'])]\n    discount_vector = np.array([(1 - discount) * (discount ** t) for t in range(n)])\n    if len(ordered_features) < n:\n        discount_vector = discount_vector[:len(ordered_features)]\n    return ordered_features.multiply(discount_vector, axis='rows').sum(axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T20:50:19.141079Z","iopub.execute_input":"2022-03-16T20:50:19.141473Z","iopub.status.idle":"2022-03-16T20:50:19.148823Z","shell.execute_reply.started":"2022-03-16T20:50:19.141444Z","shell.execute_reply":"2022-03-16T20:50:19.148173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stage 1 Data Preparation","metadata":{}},{"cell_type":"code","source":"def create_lower_id_won_feature(full_data):\n    lower_id_won = []\n    for i, row in full_data.iterrows():\n        if row.WTeamID < row.LTeamID:\n            lower_id_won.append(1)\n        else:\n            lower_id_won.append(0)\n    full_data['LowerIDWon'] = lower_id_won\n\nRNG = 32\ntournament_games = pd.read_csv(STAGE_1_DIR + 'MNCAATourneyCompactResults.csv')\ntournament_games = tournament_games[tournament_games.Season > 2003]\n# must do this to get averaging over current season as well\ntournament_games.Season = tournament_games.Season + 1\n\nseason_games = pd.read_csv(STAGE_1_DIR + 'MRegularSeasonCompactResults.csv')\nseason_games = season_games[season_games.Season > 2003]\n\nfull_train_tournament = tournament_games[tournament_games.Season < 2016 + 1]\nfull_test_tournament = tournament_games[tournament_games.Season >= 2016 + 1]\nfull_train_season = season_games[season_games.Season < 2016]\nfull_test_season = season_games[season_games.Season >= 2016]\nfull_train = pd.concat([full_train_season, full_train_tournament])\nfull_test = pd.concat([full_test_season, full_test_tournament])\ncreate_lower_id_won_feature(full_train)\ncreate_lower_id_won_feature(full_test)\n\nfull_train_shuffled = full_train.sample(frac=1, random_state=RNG)\nfull_test_shuffled = full_test.sample(frac=1, random_state=RNG)\n\nfull_train_shuffled.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T20:50:19.149963Z","iopub.execute_input":"2022-03-16T20:50:19.150176Z","iopub.status.idle":"2022-03-16T20:50:25.552362Z","shell.execute_reply.started":"2022-03-16T20:50:19.150148Z","shell.execute_reply":"2022-03-16T20:50:25.551617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_matchup_to_features(team_id1, team_id2, season, stats_df, **kwargs):\n    if team_id1 < team_id2:\n        lower = team_id1\n        higher = team_id2\n    else:\n        lower = team_id2\n        higher = team_id1\n    lower_id_features = get_features(stats_df, lower, season, **kwargs)\n    # lower_id_features.index = [c + '.L' for c in lower_id_features.index]\n    higher_id_features = get_features(stats_df, higher, season, **kwargs)\n    # higher_id_features.index = [c + '.R' for c in higher_id_features.index]\n    new_row = lower_id_features - higher_id_features\n    return new_row","metadata":{"execution":{"iopub.status.busy":"2022-03-16T20:50:25.553675Z","iopub.execute_input":"2022-03-16T20:50:25.553879Z","iopub.status.idle":"2022-03-16T20:50:25.559212Z","shell.execute_reply.started":"2022-03-16T20:50:25.553853Z","shell.execute_reply":"2022-03-16T20:50:25.558639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can precompute all of the training and testing data.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef precompute_matchup_to_features(id_table, stats_df, **kwargs):\n    new_rows = []\n    for i, row in tqdm(id_table.iterrows(), total=len(id_table)):\n        if row.WTeamID < row.LTeamID:\n            lower = row.WTeamID\n            higher = row.LTeamID\n        else:\n            lower = row.LTeamID\n            higher = row.WTeamID\n        lower_id_features = get_features(stats_df, lower, row.Season, **kwargs)\n        # lower_id_features.index = [c + '.L' for c in lower_id_features.index]\n        higher_id_features = get_features(stats_df, higher, row.Season, **kwargs)\n        # higher_id_features.index = [c + '.R' for c in higher_id_features.index]\n        new_row = lower_id_features - higher_id_features\n        new_rows.append(new_row)\n    return pd.DataFrame(new_rows)\n        \n\nN = 4\nDISCOUNT = 0.5\nX_train_ids = full_train_shuffled.loc[:, ['WTeamID', 'LTeamID', 'Season']]\ny_train = full_train_shuffled.loc[:, 'LowerIDWon']\nX_test_ids = full_test_shuffled.loc[:, ['WTeamID', 'LTeamID', 'Season']]\ny_test = full_test_shuffled.loc[:, 'LowerIDWon']\nX_train = precompute_matchup_to_features(X_train_ids, team_season_stats, n=N,\n                                    discount=DISCOUNT)\nX_test = precompute_matchup_to_features(X_test_ids, team_season_stats, n=N,\n                                   discount=DISCOUNT)\nprint(X_train.head())\nprint(y_train.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-16T20:50:25.560302Z","iopub.execute_input":"2022-03-16T20:50:25.560522Z","iopub.status.idle":"2022-03-16T20:59:53.687277Z","shell.execute_reply.started":"2022-03-16T20:50:25.560494Z","shell.execute_reply":"2022-03-16T20:59:53.686327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stage 1 Model Building","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators=500, random_state=RNG, criterion='entropy')\nclassifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T20:59:53.689098Z","iopub.execute_input":"2022-03-16T20:59:53.689409Z","iopub.status.idle":"2022-03-16T21:02:39.411928Z","shell.execute_reply.started":"2022-03-16T20:59:53.689367Z","shell.execute_reply":"2022-03-16T21:02:39.4109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:02:39.41535Z","iopub.execute_input":"2022-03-16T21:02:39.415703Z","iopub.status.idle":"2022-03-16T21:02:44.855149Z","shell.execute_reply.started":"2022-03-16T21:02:39.415662Z","shell.execute_reply":"2022-03-16T21:02:44.854354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stage 1 Make and Submit Predictions","metadata":{}},{"cell_type":"code","source":"stage_1_submission = pd.read_csv(STAGE_1_DIR + 'MSampleSubmissionStage1.csv')\n\nfor i, row in tqdm(stage_1_submission.iterrows(), total=len(stage_1_submission)):\n    season_str, low_id_str, high_id_str = row.ID.split('_')\n    season = int(season_str)\n    low_id = int(low_id_str)\n    high_id = int(high_id_str)\n    x = transform_matchup_to_features(low_id, high_id, season, team_season_stats, n=N,\n                                      discount=DISCOUNT).to_numpy().reshape(1, -1)\n    y = classifier.predict_proba(x)[0]\n    stage_1_submission.iloc[i, 1] = y[1]","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:02:44.85641Z","iopub.execute_input":"2022-03-16T21:02:44.856654Z","iopub.status.idle":"2022-03-16T21:12:09.120418Z","shell.execute_reply.started":"2022-03-16T21:02:44.856624Z","shell.execute_reply":"2022-03-16T21:12:09.119647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stage_1_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:12:09.122118Z","iopub.execute_input":"2022-03-16T21:12:09.122559Z","iopub.status.idle":"2022-03-16T21:12:09.133422Z","shell.execute_reply.started":"2022-03-16T21:12:09.122515Z","shell.execute_reply":"2022-03-16T21:12:09.132626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stage_1_submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:12:09.134948Z","iopub.execute_input":"2022-03-16T21:12:09.135202Z","iopub.status.idle":"2022-03-16T21:12:09.188399Z","shell.execute_reply.started":"2022-03-16T21:12:09.135171Z","shell.execute_reply":"2022-03-16T21:12:09.187751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"STAGE_2_DIR = '/kaggle/input/mens-march-mania-2022/MDataFiles_Stage2/'\nRNG = 32\ntournament_games = pd.read_csv(STAGE_2_DIR + 'MNCAATourneyCompactResults.csv')\ntournament_games = tournament_games[tournament_games.Season > 2003]\n# must do this to get averaging over current season as well\ntournament_games.Season = tournament_games.Season + 1\n\nseason_games = pd.read_csv(STAGE_2_DIR + 'MRegularSeasonCompactResults.csv')\nseason_games = season_games[season_games.Season > 2003]\n\nfull_train_tournament = tournament_games\nfull_train_season = season_games\nfull_train = pd.concat([full_train_season, full_train_tournament])\ncreate_lower_id_won_feature(full_train)\n\nfull_train_shuffled = full_train.sample(frac=1, random_state=RNG)\n\nfull_train_shuffled.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:12:09.189615Z","iopub.execute_input":"2022-03-16T21:12:09.189864Z","iopub.status.idle":"2022-03-16T21:12:15.73867Z","shell.execute_reply.started":"2022-03-16T21:12:09.189832Z","shell.execute_reply":"2022-03-16T21:12:15.737718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = 4\nDISCOUNT = 0.5\nX_train_ids = full_train_shuffled.loc[:, ['WTeamID', 'LTeamID', 'Season']]\ny_train = full_train_shuffled.loc[:, 'LowerIDWon']\nX_test_ids = full_test_shuffled.loc[:, ['WTeamID', 'LTeamID', 'Season']]\ny_test = full_test_shuffled.loc[:, 'LowerIDWon']\nX_train = precompute_matchup_to_features(X_train_ids, team_season_stats, n=N,\n                                    discount=DISCOUNT)\nX_test = precompute_matchup_to_features(X_test_ids, team_season_stats, n=N,\n                                   discount=DISCOUNT)\nprint(X_train.head())\nprint(y_train.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:12:15.739795Z","iopub.execute_input":"2022-03-16T21:12:15.74Z","iopub.status.idle":"2022-03-16T21:25:32.946135Z","shell.execute_reply.started":"2022-03-16T21:12:15.739974Z","shell.execute_reply":"2022-03-16T21:25:32.944444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators=500, random_state=RNG, criterion='entropy')\nclassifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:25:32.947163Z","iopub.execute_input":"2022-03-16T21:25:32.947388Z","iopub.status.idle":"2022-03-16T21:30:12.48545Z","shell.execute_reply.started":"2022-03-16T21:25:32.947358Z","shell.execute_reply":"2022-03-16T21:30:12.484491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier.feature_importances_","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:49:14.288888Z","iopub.execute_input":"2022-03-16T21:49:14.289181Z","iopub.status.idle":"2022-03-16T21:49:14.626365Z","shell.execute_reply.started":"2022-03-16T21:49:14.289149Z","shell.execute_reply":"2022-03-16T21:49:14.625595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.columns","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:49:32.038119Z","iopub.execute_input":"2022-03-16T21:49:32.038941Z","iopub.status.idle":"2022-03-16T21:49:32.044597Z","shell.execute_reply.started":"2022-03-16T21:49:32.038893Z","shell.execute_reply":"2022-03-16T21:49:32.043747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stage_2_submission = pd.read_csv(STAGE_2_DIR + 'MSampleSubmissionStage2.csv')\n\nfor i, row in tqdm(stage_2_submission.iterrows(), total=len(stage_2_submission)):\n    season_str, low_id_str, high_id_str = row.ID.split('_')\n    season = int(season_str)\n    low_id = int(low_id_str)\n    high_id = int(high_id_str)\n    x = transform_matchup_to_features(low_id, high_id, season, team_season_stats, n=N,\n                                      discount=DISCOUNT).to_numpy().reshape(1, -1)\n    y = classifier.predict_proba(x)[0]\n    stage_2_submission.iloc[i, 1] = y[1]","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:30:12.486898Z","iopub.execute_input":"2022-03-16T21:30:12.487208Z","iopub.status.idle":"2022-03-16T21:32:07.517985Z","shell.execute_reply.started":"2022-03-16T21:30:12.487165Z","shell.execute_reply":"2022-03-16T21:32:07.517086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stage_2_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:32:07.519191Z","iopub.execute_input":"2022-03-16T21:32:07.519501Z","iopub.status.idle":"2022-03-16T21:32:07.52971Z","shell.execute_reply.started":"2022-03-16T21:32:07.519457Z","shell.execute_reply":"2022-03-16T21:32:07.528836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stage_2_submission.to_csv('submission2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:32:07.531424Z","iopub.execute_input":"2022-03-16T21:32:07.532381Z","iopub.status.idle":"2022-03-16T21:32:07.551079Z","shell.execute_reply.started":"2022-03-16T21:32:07.532332Z","shell.execute_reply":"2022-03-16T21:32:07.550126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"id_df = pd.read_csv(STAGE_2_DIR + 'MTeams.csv')\nprint('If these two teams are playing each other, the one on the left has this chance of winning the matchup...')\nfor i, row in stage_2_submission.iterrows():\n    season_str, low_id_str, high_id_str = row.ID.split('_')\n    season = int(season_str)\n    lower = int(low_id_str)\n    higher = int(high_id_str)\n    lower_team_name = id_df[id_df.TeamID == lower].TeamName.iloc[0]\n    higher_team_name = id_df[id_df.TeamID == higher].TeamName.iloc[0]\n    print(f'{lower_team_name} vs. {higher_team_name}: {row.Pred}')","metadata":{"execution":{"iopub.status.busy":"2022-03-16T21:32:07.554726Z","iopub.execute_input":"2022-03-16T21:32:07.554957Z","iopub.status.idle":"2022-03-16T21:32:10.449094Z","shell.execute_reply.started":"2022-03-16T21:32:07.554929Z","shell.execute_reply":"2022-03-16T21:32:10.448092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}