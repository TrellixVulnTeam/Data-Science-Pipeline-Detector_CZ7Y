{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Intro\n\nI would love it if someone lands a medal just tweaking this notebook. Don't forget to mention @verracodeguacas at the end if you do.\n\nThe notebook roughly follows the guidelines of @raddar - Darius Baru≈°auskas as shown in his youtube presentation: https://www.youtube.com/watch?v=KmhGNc7gcCM&t=18s&ab_channel=Kaggle\n\nHe won the women's version of this tournament in 2018 using these ideas. I implemented in python but it won't be exactly the same because I added my own stuff. I basically changed some of the features, adapted signals that work for men's, and modified his \"quality\" measure to something that made more sense to me.\n\nI dedicated the last few cells to create overrides - In case you like or dislike some particular university just for the hell of it. You can do that.\n\nHere's the original R code if anybody wants to replicate: https://github.com/fakyras/ncaa_women_2018/blob/master/win_ncaa.R","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom scipy.interpolate import UnivariateSpline\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport collections\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\npd.set_option(\"display.max_column\", 999)\nprint(os.listdir(\"../input\"))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-15T02:51:07.335983Z","iopub.execute_input":"2022-03-15T02:51:07.336412Z","iopub.status.idle":"2022-03-15T02:51:18.100407Z","shell.execute_reply.started":"2022-03-15T02:51:07.336373Z","shell.execute_reply":"2022-03-15T02:51:18.09908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preparation. \n\nA lot of this has to do with duplicating the data. Each game is seen once from the winner's and once from the loser's perspective. Watch the video to understand better what this is about.","metadata":{"_uuid":"20a1490f4d5e0c7916ed661855497604980d2925"}},{"cell_type":"code","source":"tourney_results = pd.read_csv('../input/mens-march-mania-2022/MDataFiles_Stage2/MNCAATourneyDetailedResults.csv')\nseeds = pd.read_csv('../input/mens-march-mania-2022/MDataFiles_Stage2/MNCAATourneySeeds.csv')\nregular_results = pd.read_csv('../input/mens-march-mania-2022/MDataFiles_Stage2/MRegularSeasonDetailedResults.csv')\n\nregular_results['WEFFG'] = regular_results['WFGM'] / regular_results['WFGA']\nregular_results['WEFFG3'] = regular_results['WFGM3'] / regular_results['WFGA3']\nregular_results['WDARE'] = regular_results['WFGM3'] / regular_results['WFGM']\nregular_results['WTOQUETOQUE'] = regular_results['WAst'] / regular_results['WFGM']\n\nregular_results['LEFFG'] = regular_results['LFGM'] / regular_results['LFGA']\nregular_results['LEFFG3'] = regular_results['LFGM3'] / regular_results['LFGA3']\nregular_results['LDARE'] = regular_results['LFGM3'] / regular_results['LFGM']\nregular_results['LTOQUETOQUE'] = regular_results['LAst'] / regular_results['LFGM']\nregular_results.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(df):\n    dfswap = df[['Season', 'DayNum', 'LTeamID', 'LScore', 'WTeamID', 'WScore', 'WLoc', 'NumOT', \n    'LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', \n    'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']]\n\n    dfswap.loc[df['WLoc'] == 'H', 'WLoc'] = 'A'\n    dfswap.loc[df['WLoc'] == 'A', 'WLoc'] = 'H'\n    df.columns.values[6] = 'location'\n    dfswap.columns.values[6] = 'location'    \n      \n    df.columns = [x.replace('W','T1_').replace('L','T2_') for x in list(df.columns)]\n    dfswap.columns = [x.replace('L','T1_').replace('W','T2_') for x in list(dfswap.columns)]\n\n    output = pd.concat([df, dfswap]).reset_index(drop=True)\n    output.loc[output.location=='N','location'] = '0'\n    output.loc[output.location=='H','location'] = '1'\n    output.loc[output.location=='A','location'] = '-1'\n    output.location = output.location.astype(int)\n    \n    output['PointDiff'] = output['T1_Score'] - output['T2_Score']\n    \n    return output","metadata":{"_uuid":"512dcd34de61fd5e61cd08eccc1c9560362f4178","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regular_data = prepare_data(regular_results)\ntourney_data = prepare_data(tourney_results)","metadata":{"_uuid":"6a7bf779e16fe27aa89633ff78f89d452794aa84","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering!","metadata":{"_uuid":"6dbbc08907e17be459f936ed59e6f50a6dade35c"}},{"cell_type":"code","source":"regular_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Choose the features that you want. Either because you know basketball or use some feature engineering!","metadata":{}},{"cell_type":"code","source":"boxscore_cols = ['T1_Score', 'T2_Score', \n        'T1_FGM', 'T1_FGA', 'T1_FGM3', 'T1_FGA3', 'T1_FTM', 'T1_FTA', 'T1_OR', 'T1_DR', 'T1_Ast', 'T1_TO', 'T1_Stl', 'T1_Blk', 'T1_PF', \n        'T2_FGM', 'T2_FGA', 'T2_FGM3', 'T2_FGA3', 'T2_FTM', 'T2_FTA', 'T2_OR', 'T2_DR', 'T2_Ast', 'T2_TO', 'T2_Stl', 'T2_Blk', 'T2_PF', \n        'PointDiff']\n\nboxscore_cols = [\n        'T1_FGM', 'T1_FGA', 'T1_OR', 'T1_Ast', 'T1_TO', 'T1_Stl', 'T1_PF', 'T1_FTM', 'T2_FTM', 'T2_FGM', 'T2_FGA', \n        'T2_OR', 'T2_Ast', 'T2_TO', 'T2_Stl', 'T2_Blk', 'T1_Score', 'T2_Score', 'PointDiff',\n        'T1_EFFG', 'T1_EFFG3', 'T1_DARE', 'T1_TOQUETOQUE', 'T2_EFFG', 'T2_EFFG3', 'T2_DARE', 'T2_TOQUETOQUE']\n\nboxscore_cols = ['T1_Score', 'T2_Score', \n        'T1_FGM', 'T1_FGA', 'T1_FGM3', 'T1_FGA3', 'T1_FTM', 'T1_FTA', 'T1_OR', 'T1_DR', 'T1_Ast', 'T1_TO', 'T1_Stl', 'T1_Blk', 'T1_PF', \n        'T2_FGM', 'T2_FGA', 'T2_FGM3', 'T2_FGA3', 'T2_FTM', 'T2_FTA', 'T2_OR', 'T2_DR', 'T2_Ast', 'T2_TO', 'T2_Stl', 'T2_Blk', 'T2_PF', \n        'T1_EFFG', 'T1_EFFG3', 'T1_DARE', 'T1_TOQUETOQUE', 'T2_EFFG', 'T2_EFFG3', 'T2_DARE', 'T2_TOQUETOQUE']\n\n# After my analysis\n#boxscore_cols = ['PointDiff', 'T1_Blk', 'T2_Blk', 'T1_Ast', 'T2_Ast', 'T1_Stl', 'T2_Stl', 'T1_FGA', \n#                 'T2_FGA', 'T1_FGM', 'T2_FGM', 'T1_DR', 'T2_DR', 'T1_Score', 'T2_Score']\n\n\n# Choose a function to aggregate\nfuncs = [np.mean]","metadata":{"_uuid":"ca9f902b70fce0de0db21be2e9f047b233e4d65c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The idea is to be able to take a picture of the teams right before the tournament","metadata":{}},{"cell_type":"code","source":"season_statistics = regular_data.groupby([\"Season\", 'T1_TeamID'])[boxscore_cols].agg(funcs).reset_index()\nseason_statistics.columns = [''.join(col).strip() for col in season_statistics.columns.values]\n#Make two copies of the data\nseason_statistics_T1 = season_statistics.copy()\nseason_statistics_T2 = season_statistics.copy()\n\nseason_statistics_T1.columns = [\"T1_\" + x.replace(\"T1_\",\"\").replace(\"T2_\",\"opponent_\") for x in list(season_statistics_T1.columns)]\nseason_statistics_T2.columns = [\"T2_\" + x.replace(\"T1_\",\"\").replace(\"T2_\",\"opponent_\") for x in list(season_statistics_T2.columns)]\nseason_statistics_T1.columns.values[0] = \"Season\"\nseason_statistics_T2.columns.values[0] = \"Season\"","metadata":{"_uuid":"bac629b77123932c43842a10dbbe1ac6e6d5871b","execution":{"iopub.status.busy":"2022-02-19T06:20:33.860243Z","iopub.execute_input":"2022-02-19T06:20:33.860865Z","iopub.status.idle":"2022-02-19T06:20:34.04211Z","shell.execute_reply.started":"2022-02-19T06:20:33.860817Z","shell.execute_reply":"2022-02-19T06:20:34.041307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We don't have the box score statistics in the prediction bank. So drop it.","metadata":{}},{"cell_type":"code","source":"tourney_data = tourney_data[['Season', 'DayNum', 'T1_TeamID', 'T1_Score', 'T2_TeamID' ,'T2_Score']]\ntourney_data.head()","metadata":{"_uuid":"632f6a9c61b60ed1ea391aa784589356a228c429","execution":{"iopub.status.busy":"2022-02-19T06:21:03.230257Z","iopub.execute_input":"2022-02-19T06:21:03.230975Z","iopub.status.idle":"2022-02-19T06:21:03.243368Z","shell.execute_reply.started":"2022-02-19T06:21:03.230937Z","shell.execute_reply":"2022-02-19T06:21:03.242385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tourney_data = pd.merge(tourney_data, season_statistics_T1, on = ['Season', 'T1_TeamID'], how = 'left')\ntourney_data = pd.merge(tourney_data, season_statistics_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n# Notice that there are Team 1 statistics, team 1 opponent's statistics, team 2 statistics and team 2 opponent statistics\ntourney_data.head()","metadata":{"_uuid":"b695d847f457c01b16ca4649d5219fb326ee180e","execution":{"iopub.status.busy":"2022-02-19T06:21:11.309842Z","iopub.execute_input":"2022-02-19T06:21:11.310239Z","iopub.status.idle":"2022-02-19T06:21:11.373842Z","shell.execute_reply.started":"2022-02-19T06:21:11.310211Z","shell.execute_reply":"2022-02-19T06:21:11.372917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cut the opponent columns that I don't want\n#opplist = [opp for opp in tourney_data.columns if '_opponent_' in opp]\n#todelete = [opp for opp in opplist if 'Blk' not in opp]\n#tourney_data.drop(todelete, axis = 1, inplace = True)\n#tourney_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T06:21:19.267803Z","iopub.execute_input":"2022-02-19T06:21:19.26807Z","iopub.status.idle":"2022-02-19T06:21:19.271977Z","shell.execute_reply.started":"2022-02-19T06:21:19.268043Z","shell.execute_reply":"2022-02-19T06:21:19.271336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Raddar likes to modify some stuff in the last two weeks before the tournament. I would rather not touch this, but I leave it commented out if you believe in Raddar and want to replicate. Also, a lot of people will copy-paste these notebooks and submit, so if you actually read this. You might as well change some stuff and score something different from the crowd. Your choice!","metadata":{}},{"cell_type":"code","source":"# These statistics are created because in the last 2 weeks some stuff may happen (injuries just before the tournament and such)\n#last14days_stats_T1 = regular_data.loc[regular_data.DayNum>118].reset_index(drop=True)\n#last14days_stats_T1['win'] = np.where(last14days_stats_T1['PointDiff']>0,1,0)\n#last14days_stats_T1 = last14days_stats_T1.groupby(['Season','T1_TeamID'])['win'].mean().reset_index(name='T1_win_ratio_14d')\n\n#last14days_stats_T2 = regular_data.loc[regular_data.DayNum>118].reset_index(drop=True)\n#last14days_stats_T2['win'] = np.where(last14days_stats_T2['PointDiff']<0,1,0)\n#last14days_stats_T2 = last14days_stats_T2.groupby(['Season','T2_TeamID'])['win'].mean().reset_index(name='T2_win_ratio_14d')\n\n#tourney_data = pd.merge(tourney_data, last14days_stats_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n#tourney_data = pd.merge(tourney_data, last14days_stats_T2, on = ['Season', 'T2_TeamID'], how = 'left')","metadata":{"_uuid":"bbe5b07505a16c7fa38e486c11089dd68122ae56","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the teams that make it to the tournament and see how they do with respect to the others\nregular_season_effects = regular_data[['Season','T1_TeamID','T2_TeamID','PointDiff']].copy()\nregular_season_effects['T1_TeamID'] = regular_season_effects['T1_TeamID'].astype(str)\nregular_season_effects['T2_TeamID'] = regular_season_effects['T2_TeamID'].astype(str)\nregular_season_effects['win'] = np.where(regular_season_effects['PointDiff']>0,1,0)\nmarch_madness = pd.merge(seeds[['Season','TeamID']],seeds[['Season','TeamID']],on='Season')\nmarch_madness.columns = ['Season', 'T1_TeamID', 'T2_TeamID']\nmarch_madness.T1_TeamID = march_madness.T1_TeamID.astype(str)\nmarch_madness.T2_TeamID = march_madness.T2_TeamID.astype(str)\nregular_season_effects = pd.merge(regular_season_effects, march_madness, on = ['Season','T1_TeamID','T2_TeamID'])\nregular_season_effects.shape","metadata":{"_uuid":"0119bb397cea0028aeb4247034ef6096e738becf","execution":{"iopub.status.busy":"2022-02-19T06:23:28.037469Z","iopub.execute_input":"2022-02-19T06:23:28.03775Z","iopub.status.idle":"2022-02-19T06:23:29.022358Z","shell.execute_reply.started":"2022-02-19T06:23:28.037721Z","shell.execute_reply":"2022-02-19T06:23:29.021418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Team Quality","metadata":{}},{"cell_type":"markdown","source":"This is the team quality measure. The most important part. Watch the youtube presentation at this timepoint to understand what it is: https://youtu.be/KmhGNc7gcCM?t=2279 it is a measure of team strenght.\n\nWarning: I changed it and it's not exactly the same quality measure mentioned in the youtube video. Consider changing this!","metadata":{}},{"cell_type":"code","source":"def normalize_column(values):\n  themean = np.mean(values)\n  thestd = np.std(values)\n  norm = (values - themean)/(thestd) \n  return(pd.DataFrame(norm))\n\ndef team_quality(season):\n    formula = 'win~-1+T1_TeamID+T2_TeamID'\n    glm = sm.GLM.from_formula(formula=formula, \n                              data=regular_season_effects.loc[regular_season_effects.Season==season,:], \n                              family=sm.families.Binomial()).fit()\n    quality = pd.DataFrame(glm.params).reset_index()\n    quality.columns = ['TeamID','quality']\n    quality['Season'] = season\n    quality['quality'] = normalize_column(quality['quality'])\n    quality['quality'] = np.exp(quality['quality'])\n    quality = quality.loc[quality.TeamID.str.contains('T1_')].reset_index(drop=True)\n    quality['TeamID'] = quality['TeamID'].apply(lambda x: x[10:14]).astype(int)\n    print(quality['quality'].mean(), quality['quality'].std())\n    return quality","metadata":{"_uuid":"4d9b04a51db542edb705c94ed145d90af2ad2f15","execution":{"iopub.status.busy":"2022-02-19T06:28:31.547338Z","iopub.execute_input":"2022-02-19T06:28:31.54826Z","iopub.status.idle":"2022-02-19T06:28:31.562277Z","shell.execute_reply.started":"2022-02-19T06:28:31.548199Z","shell.execute_reply":"2022-02-19T06:28:31.561491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is metric to measure the team's strength, in this case, this is a logistic regression and we\n# the coefficients\nglm_quality = pd.concat([team_quality(2003),\n                         team_quality(2004),\n                         team_quality(2005),\n                         team_quality(2006),\n                         team_quality(2007),\n                         team_quality(2008),\n                         team_quality(2009),\n                         team_quality(2010),\n                         team_quality(2011),\n                         team_quality(2012),\n                         team_quality(2013),\n                         team_quality(2014),\n                         team_quality(2015),\n                         team_quality(2016),\n                         team_quality(2017),\n                         team_quality(2018),\n                         team_quality(2019),\n                         team_quality(2021)]).reset_index(drop=True)","metadata":{"_uuid":"d805268806a47d1af7d4d614809e883819847df9","execution":{"iopub.status.busy":"2022-02-19T06:28:34.489885Z","iopub.execute_input":"2022-02-19T06:28:34.490422Z","iopub.status.idle":"2022-02-19T06:28:41.506009Z","shell.execute_reply.started":"2022-02-19T06:28:34.490386Z","shell.execute_reply":"2022-02-19T06:28:41.505101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glm_quality_T1 = glm_quality.copy()\nglm_quality_T2 = glm_quality.copy()\nglm_quality_T1.columns = ['T1_TeamID','T1_quality','Season']\nglm_quality_T2.columns = ['T2_TeamID','T2_quality','Season']","metadata":{"_uuid":"b67f4e36dd77a6d7b781f33572e19c2a90e8f184","execution":{"iopub.status.busy":"2022-02-19T06:28:45.722031Z","iopub.execute_input":"2022-02-19T06:28:45.722326Z","iopub.status.idle":"2022-02-19T06:28:45.729026Z","shell.execute_reply.started":"2022-02-19T06:28:45.722293Z","shell.execute_reply":"2022-02-19T06:28:45.728079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tourney_data = pd.merge(tourney_data, glm_quality_T1, on = ['Season', 'T1_TeamID'], how = 'left')\ntourney_data = pd.merge(tourney_data, glm_quality_T2, on = ['Season', 'T2_TeamID'], how = 'left')\ntourney_data['T1_quality'].fillna(0.2, inplace = True)\ntourney_data['T2_quality'].fillna(0.2, inplace = True)\ntourney_data.T2_quality.isnull().sum()","metadata":{"_uuid":"1b667442750b3fd5dd24c41d75e3bb7a0ed28f8a","execution":{"iopub.status.busy":"2022-02-19T06:28:58.268963Z","iopub.execute_input":"2022-02-19T06:28:58.269588Z","iopub.status.idle":"2022-02-19T06:28:58.288414Z","shell.execute_reply.started":"2022-02-19T06:28:58.269549Z","shell.execute_reply":"2022-02-19T06:28:58.287516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seeds['seed'] = seeds['Seed'].apply(lambda x: int(x[1:3]))\nseeds.head()","metadata":{"_uuid":"f2ce28123dfc563a8b1fb58f98401c927e2eb7c2","execution":{"iopub.status.busy":"2022-02-19T06:29:01.331563Z","iopub.execute_input":"2022-02-19T06:29:01.331858Z","iopub.status.idle":"2022-02-19T06:29:01.34729Z","shell.execute_reply.started":"2022-02-19T06:29:01.331827Z","shell.execute_reply":"2022-02-19T06:29:01.346494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seeds_T1 = seeds[['Season','TeamID','seed']].copy()\nseeds_T2 = seeds[['Season','TeamID','seed']].copy()\nseeds_T1.columns = ['Season','T1_TeamID','T1_seed']\nseeds_T2.columns = ['Season','T2_TeamID','T2_seed']","metadata":{"_uuid":"e2d6a4e33b062401088f472b698cc9cabff3a612","execution":{"iopub.status.busy":"2022-02-19T06:29:02.483047Z","iopub.execute_input":"2022-02-19T06:29:02.483778Z","iopub.status.idle":"2022-02-19T06:29:02.490488Z","shell.execute_reply.started":"2022-02-19T06:29:02.483744Z","shell.execute_reply":"2022-02-19T06:29:02.489913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tourney_data = pd.merge(tourney_data, seeds_T1, on = ['Season', 'T1_TeamID'], how = 'left')\ntourney_data = pd.merge(tourney_data, seeds_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n#Optional but not relevant\ntourney_data[\"Seed_diff\"] = tourney_data[\"T1_seed\"] - tourney_data[\"T2_seed\"]","metadata":{"_uuid":"3ce355578d9881346602388e2df329cafd1f24e7","execution":{"iopub.status.busy":"2022-02-19T06:29:03.868398Z","iopub.execute_input":"2022-02-19T06:29:03.868959Z","iopub.status.idle":"2022-02-19T06:29:03.882677Z","shell.execute_reply.started":"2022-02-19T06:29:03.868924Z","shell.execute_reply":"2022-02-19T06:29:03.88184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's add the massey ordinals to this thing! ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nmassey = pd.read_csv('../input/mens-march-mania-2022/MDataFiles_Stage2/MMasseyOrdinals_thruDay128.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-15T03:07:28.985626Z","iopub.execute_input":"2022-03-15T03:07:28.985951Z","iopub.status.idle":"2022-03-15T03:07:31.47668Z","shell.execute_reply.started":"2022-03-15T03:07:28.98592Z","shell.execute_reply":"2022-03-15T03:07:31.475587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RANKINGS AVAILABLE\nmassey[massey.RankingDayNum == 128].SystemName.unique()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T03:29:05.060943Z","iopub.execute_input":"2022-02-25T03:29:05.061625Z","iopub.status.idle":"2022-02-25T03:29:05.117605Z","shell.execute_reply.started":"2022-02-25T03:29:05.061583Z","shell.execute_reply":"2022-02-25T03:29:05.117022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also add POM ranks, gagarin, and many more.","metadata":{}},{"cell_type":"code","source":"bagofRanks = dict()\n#oldtoconsider = ['WLK']\ntrafalgars = ['WLK', 'SAG', 'POM', 'COL', 'DOL', 'MOR', 'RTH', 'WOL', 'ATP', 'EMK', 'DWH', 'AP']\nfor traf in trafalgars:\n    bagofRanks[traf] = massey[(massey['SystemName']==traf) & (massey['RankingDayNum']==128)]\n    traf_T1 = bagofRanks[traf][['Season','TeamID','OrdinalRank']].copy()\n    traf_T2 = bagofRanks[traf][['Season','TeamID','OrdinalRank']].copy()\n    traf_T1.columns = ['Season','T1_TeamID','T1_OR_' + traf]\n    traf_T2.columns = ['Season','T2_TeamID','T2_OR_' + traf]\n    tourney_data = pd.merge(tourney_data, traf_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n    tourney_data = pd.merge(tourney_data, traf_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n    tourney_data[traf + \"_diff\"] = tourney_data[\"T1_OR_\" + traf] - tourney_data[\"T2_OR_\" + traf]\n    tourney_data.drop([\"T2_OR_\" + traf], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T06:31:34.207933Z","iopub.execute_input":"2022-02-19T06:31:34.208407Z","iopub.status.idle":"2022-02-19T06:31:40.023226Z","shell.execute_reply.started":"2022-02-19T06:31:34.208348Z","shell.execute_reply":"2022-02-19T06:31:40.022393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time to build some models!","metadata":{"_uuid":"c2ba59b62b09fc5b5514a2cca4ca5408e9a2b71a"}},{"cell_type":"code","source":"# The descriptive feature is the score, not the winner\ny = tourney_data['T1_Score'] - tourney_data['T2_Score']\ny.describe()","metadata":{"_uuid":"9981fb89482e120adc3c014b1607c160383081f8","execution":{"iopub.status.busy":"2022-02-19T06:31:05.090089Z","iopub.execute_input":"2022-02-19T06:31:05.090461Z","iopub.status.idle":"2022-02-19T06:31:05.106327Z","shell.execute_reply.started":"2022-02-19T06:31:05.090415Z","shell.execute_reply":"2022-02-19T06:31:05.10538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Last chance to drop a couple of features:\ntourney_data.drop(['T1_OR_POM', 'T1_OR_RTH', 'T1_OR_WLK', 'T1_OR_COL', 'T1_OR_WOL', 'T1_OR_MOR'], axis = 1, inplace = True)\n# Drop own efficiency and OR - Curiously the opponent efficiency IS important. - Because we effectively damage it?\ntourney_data.drop(['T1_EFFGmean', 'T2_EFFGmean', 'T1_ORmean', 'T2_ORmean'], axis = 1, inplace = True)\n# This opponent data just seems to always be insignificant\ntourney_data.drop(['T1_opponent_Stlmean', 'T2_opponent_Stlmean', 'T1_opponent_Astmean', 'T2_opponent_Astmean', 'T1_opponent_Scoremean', 'T2_opponent_Scoremean', 'T1_opponent_FGMmean', 'T2_opponent_FGMmean'], axis = 1, inplace = True)\nfeatures = tourney_data.columns[6:]\n# Drop the next ones from the features but not from the dataframe\nfeatures.drop(['T2_seed'])\nlen(features)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T06:31:20.793621Z","iopub.execute_input":"2022-02-19T06:31:20.79393Z","iopub.status.idle":"2022-02-19T06:31:21.174339Z","shell.execute_reply.started":"2022-02-19T06:31:20.793897Z","shell.execute_reply":"2022-02-19T06:31:21.173108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = tourney_data[features].values\ndtrain = xgb.DMatrix(X, label = y)","metadata":{"_uuid":"a0632be8ffefd40903c86e266a3848ef39d3571a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here's just a feature importance idea that I didn't like","metadata":{}},{"cell_type":"code","source":"# #Run the feature experiment to see their importance\n# from sklearn.model_selection import train_test_split \n# from sklearn.ensemble import RandomForestClassifier\n# X = tourney_data[features] \n# X['random_1'] = np.random.normal(0.0, 1.0, X.shape[0]) \n# X['random_2'] = np.random.normal(0.0, 1.0, X.shape[0]) \n# X['random_3'] = np.random.normal(0.0, 1.0, X.shape[0]) \n# def imp_df(column_names, importances):\n#     df = pd.DataFrame({'feature':column_names, 'feature_importance': importances}).sort_values('feature_importance', ascending = False).reset_index(drop = True)\n#     return(df)\n\n# myfeatures = dict() \n# for f in list(features) + ['random_1', 'random_2', 'random_3']: \n#     myfeatures[f] = list()\n    \n# for md in range(5,9): \n#     for n_estimators in [50, 55, 65, 75, 100]: \n#         for rs in range(6): \n#             clf = RandomForestClassifier(max_depth=md,n_estimators=n_estimators, random_state=rs) \n#             clf.fit(X, y) \n#             perm = PermutationImportance(clf, cv = None, refit = False, n_iter = 10).fit(X, y) \n#             perm_imp_eli5 = imp_df(X.columns, perm.feature_importances_) \n#             for c, f in enumerate([i for i in perm_imp_eli5['feature']]): \n#                 myfeatures[f].append(c) \n#             print('where is:', md, n_estimators, rs) \n#             print([i for i in perm_imp_eli5['feature']])\n                    \n# for f in list(features) + ['random_1', 'random_2', 'random_3']: \n#     print(f, myfeatures[f], max(myfeatures[f]), np.mean(myfeatures[f]), min(myfeatures[f]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss function\n\nThis is the objective loss function provided to xgboost. This was created by raddar but there's not really much to it. Notice that it's smooth and convex and that's all I care about.","metadata":{}},{"cell_type":"code","source":"def cauchyobj(preds, dtrain):\n    labels = dtrain.get_label()\n    c = 5000 \n    x =  preds-labels    \n    grad = x / (x**2/c**2+1)\n    hess = -c**2*(x**2-c**2)/(x**2+c**2)**2\n    return grad, hess","metadata":{"_uuid":"1816ebbcb0c348919e7ceeb0fd6509e8601ad567","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param = {} \n#param['objective'] = 'reg:linear'\nparam['eval_metric'] =  'mae'\nparam['booster'] = 'gbtree'\nparam['eta'] = 0.02 #recommend change to ~0.02 for final run. Higher when debugging.\nparam['subsample'] = 0.35\nparam['colsample_bytree'] = 0.7\nparam['num_parallel_tree'] = 3 #recommend 10. Write 3 for debugging.\nparam['min_child_weight'] = 40\nparam['gamma'] = 10\nparam['max_depth'] =  3\nparam['silent'] = 1\n\nprint(param)","metadata":{"scrolled":true,"_uuid":"7b726f9fa289643e02c5b4eb837443d83f00a831","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_cv = []\nrepeat_cv = 4 # recommend 10 for final submission. Smaller for debugging.\n\nfor i in range(repeat_cv): \n    print(f\"Fold repeater {i}\")\n    xgb_cv.append(\n        xgb.cv(\n          params = param,\n          dtrain = dtrain,\n          obj = cauchyobj,\n          num_boost_round = 3000,\n          folds = KFold(n_splits = 5, shuffle = True, random_state = i),\n          early_stopping_rounds = 25,\n          verbose_eval = 50\n        )\n    )","metadata":{"_uuid":"ac7c1d166298eeb3c06d884ecb7874a5ef618d27","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iteration_counts = [np.argmin(x['test-mae-mean'].values) for x in xgb_cv]\nval_mae = [np.min(x['test-mae-mean'].values) for x in xgb_cv]\niteration_counts, val_mae","metadata":{"_uuid":"e76e9a3c2e209243950fcb0b0ec28cc1c729b40f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This is to get out-of-fold predictions\noof_preds = []\nfor i in range(repeat_cv):\n    print(f\"Fold repeater {i}\")\n    preds = y.copy()\n    kfold = KFold(n_splits = 5, shuffle = True, random_state = i)    \n    for train_index, val_index in kfold.split(X,y):\n        dtrain_i = xgb.DMatrix(X[train_index], label = y[train_index])\n        dval_i = xgb.DMatrix(X[val_index], label = y[val_index])  \n        model = xgb.train(\n              params = param,\n              dtrain = dtrain_i,\n              num_boost_round = iteration_counts[i],\n              verbose_eval = 50\n        )\n        preds[val_index] = model.predict(dval_i)\n    oof_preds.append(np.clip(preds,-19,19))","metadata":{"_uuid":"005e51b486d6853ede2091bf7dd36bfecc340542","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_df = pd.DataFrame({\"pred\":oof_preds[0], \"label\":np.where(y>0,1,0)})\nplot_df[\"pred_int\"] = plot_df[\"pred\"].astype(int)\nplot_df = plot_df.groupby('pred_int')['label'].mean().reset_index(name='average_win_pct')\n\nplt.figure()\nplt.plot(plot_df.pred_int,plot_df.average_win_pct)","metadata":{"_uuid":"27281822aa36656d962d855d1791914b8d51a0e6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fit some beautiful splines to it.","metadata":{}},{"cell_type":"code","source":"spline_model = []\n\nfor i in range(repeat_cv):\n    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n    dat = sorted(dat, key = lambda x: x[0])\n    datdict = {}\n    for k in range(len(dat)):\n        datdict[dat[k][0]]= dat[k][1]\n        \n    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n    spline_fit = spline_model[i](oof_preds[i])\n    \n    print(f\"logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\") ","metadata":{"_uuid":"80131eb61294dd14c9a4b84f59b54f173d37a605","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_df = pd.DataFrame({\"pred\":oof_preds[0], \"label\":np.where(y>0,1,0), \"spline\":spline_model[0](oof_preds[0])})\nplot_df[\"pred_int\"] = (plot_df[\"pred\"]).astype(int)\nplot_df = plot_df.groupby('pred_int')['spline','label'].mean().reset_index()\n\nplt.figure()\nplt.plot(plot_df.pred_int,plot_df.spline)\nplt.plot(plot_df.pred_int,plot_df.label)","metadata":{"_uuid":"5c5313989cfa1fb2cfa57e4ddb1e2a22ab290423","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spline_model = []\n\nfor i in range(repeat_cv):\n    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n    dat = sorted(dat, key = lambda x: x[0])\n    datdict = {}\n    for k in range(len(dat)):\n        datdict[dat[k][0]]= dat[k][1]\n    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n    spline_fit = spline_model[i](oof_preds[i])\n    spline_fit = np.clip(spline_fit,0.025,0.975)\n    \n    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\") ","metadata":{"_uuid":"2b06344b5c968cfcd5c5840db36db184de311f96","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spline_model = []\n\nfor i in range(repeat_cv):\n    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n    dat = sorted(dat, key = lambda x: x[0])\n    datdict = {}\n    for k in range(len(dat)):\n        datdict[dat[k][0]]= dat[k][1]\n    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n    spline_fit = spline_model[i](oof_preds[i])\n    spline_fit = np.clip(spline_fit,0.02,0.98)\n    spline_fit[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16)] = 1.0\n    spline_fit[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1)] = 0.0\n    \n    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\") ","metadata":{"_uuid":"701351545f58a26cbd8afe55196df383f570517d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's just check some upsets for fun (and to understand what's going on). Can you risk making some crazy bets? - Explained in the video. Again, the idea is that low seeds rarely lose, so you may want to override some values.","metadata":{}},{"cell_type":"code","source":"#looking for upsets\npd.concat(\n    [tourney_data[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16) & (tourney_data.T1_Score < tourney_data.T2_Score)],\n     tourney_data[(tourney_data.T1_seed==2) & (tourney_data.T2_seed==15) & (tourney_data.T1_Score < tourney_data.T2_Score)],\n     tourney_data[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1) & (tourney_data.T1_Score > tourney_data.T2_Score)],\n     tourney_data[(tourney_data.T1_seed==15) & (tourney_data.T2_seed==2) & (tourney_data.T1_Score > tourney_data.T2_Score)]]\n)   \n\n#https://en.wikipedia.org/wiki/NCAA_Division_I_Women%27s_Basketball_Tournament_upsets","metadata":{"_uuid":"575ae5e165fcde8fecec21f0df0d27fd8fa0726a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spline_model = []\n\nfor i in range(repeat_cv):\n    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n    dat = sorted(dat, key = lambda x: x[0])\n    datdict = {}\n    for k in range(len(dat)):\n        datdict[dat[k][0]]= dat[k][1]\n    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n    spline_fit = spline_model[i](oof_preds[i])\n    spline_fit = np.clip(spline_fit,0.025,0.975)\n    spline_fit[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n    spline_fit[(tourney_data.T1_seed==2) & (tourney_data.T2_seed==15) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n    spline_fit[(tourney_data.T1_seed==3) & (tourney_data.T2_seed==14) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n    spline_fit[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n    spline_fit[(tourney_data.T1_seed==15) & (tourney_data.T2_seed==2) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n    spline_fit[(tourney_data.T1_seed==14) & (tourney_data.T2_seed==3) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n    \n    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\") ","metadata":{"_uuid":"3aad32289e9d47005b85793d952555104b0e45ad","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_cv = []\nspline_model = []\n\nfor i in range(repeat_cv):\n    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n    dat = sorted(dat, key = lambda x: x[0])\n    datdict = {}\n    for k in range(len(dat)):\n        datdict[dat[k][0]]= dat[k][1]\n    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n    spline_fit = spline_model[i](oof_preds[i])\n    spline_fit = np.clip(spline_fit,0.02,0.98)\n    spline_fit[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n    spline_fit[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n    spline_fit[(tourney_data.T1_seed==2) & (tourney_data.T2_seed==15) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n    spline_fit[(tourney_data.T1_seed==15) & (tourney_data.T2_seed==2) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n    \n    val_cv.append(pd.DataFrame({\"y\":np.where(y>0,1,0), \"pred\":spline_fit, \"season\":tourney_data.Season}))\n    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\") \n    \nval_cv = pd.concat(val_cv)\nval_cv.groupby('season').apply(lambda x: log_loss(x.y, x.pred))","metadata":{"_uuid":"afc43d2f93dac2fe257e670f952c02e74a0b0bdb","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission time!","metadata":{"_uuid":"0c06e55e0ed14fd6c3c625e01aca69ae873d74c3"}},{"cell_type":"code","source":"sub = pd.read_csv('../input/mens-march-mania-2022/MDataFiles_Stage2/MSampleSubmissionStage2.csv')\nsub.head()","metadata":{"_uuid":"840525feee6f9406554c9b5170a39b815ce5ba22","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub[\"Season\"] = sub[\"ID\"].apply(lambda x: x[0:4]).astype(int)\nsub[\"T1_TeamID\"] = sub[\"ID\"].apply(lambda x: x[5:9]).astype(int)\nsub[\"T2_TeamID\"] = sub[\"ID\"].apply(lambda x: x[10:14]).astype(int)\nsub.shape","metadata":{"_uuid":"f2fb2bbe182ac6d6da98cd5c0086a3089988a62a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.merge(sub, season_statistics_T1, on = ['Season', 'T1_TeamID'])\nsub = pd.merge(sub, season_statistics_T2, on = ['Season', 'T2_TeamID'])\nprint(sub.shape)\nsub = pd.merge(sub, glm_quality_T1, on = ['Season', 'T1_TeamID'], how = 'left') # This is because some teams didn't face off in the regular season\nsub = pd.merge(sub, glm_quality_T2, on = ['Season', 'T2_TeamID'], how = 'left')\nprint(sub.shape)\nsub = pd.merge(sub, seeds_T1, on = ['Season', 'T1_TeamID'])\nsub = pd.merge(sub, seeds_T2, on = ['Season', 'T2_TeamID'])\nprint(sub.shape)\n#sub = pd.merge(sub, last14days_stats_T1, on = ['Season', 'T1_TeamID'])\n#sub = pd.merge(sub, last14days_stats_T2, on = ['Season', 'T2_TeamID'])\n#print(sub.shape)\nsub[\"Seed_diff\"] = sub[\"T1_seed\"] - sub[\"T2_seed\"]\nsub.shape\nfor traf in trafalgars:\n    traf_T1 = bagofRanks[traf][['Season','TeamID','OrdinalRank']].copy()\n    traf_T2 = bagofRanks[traf][['Season','TeamID','OrdinalRank']].copy()\n    traf_T1.columns = ['Season','T1_TeamID','T1_OR_' + traf]\n    traf_T2.columns = ['Season','T2_TeamID','T2_OR_' + traf]\n    sub = pd.merge(sub, traf_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n    sub = pd.merge(sub, traf_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n    sub[traf + \"_diff\"] = sub[\"T1_OR_\" + traf] - sub[\"T2_OR_\" + traf]\nsub.shape","metadata":{"_uuid":"861d6c4c12a094804f9cf0ea2c14c0c5136d164c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()\nprint(sub.T2_quality.isnull().sum())\nsub['T1_quality'].fillna(0.2, inplace = True)\nsub['T2_quality'].fillna(0.2, inplace = True)\nsub.T2_quality.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xsub = sub[features].values\ndtest = xgb.DMatrix(Xsub)","metadata":{"_uuid":"cb1627abe53f9db9fdb5be2e45a739e0f78ede83","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_models = []\nfor i in range(repeat_cv):\n    print(f\"Fold repeater {i}\")\n    sub_models.append(\n        xgb.train(\n          params = param,\n          dtrain = dtrain,\n          num_boost_round = int(iteration_counts[i] * 1.05),\n          verbose_eval = 50\n        )\n    )","metadata":{"_uuid":"a700a993c601d9692f2374f11c8eadb234979086","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_preds = []\nfor i in range(repeat_cv):\n    sub_preds.append(np.clip(spline_model[i](np.clip(sub_models[i].predict(dtest),-30,30)),0.025,0.975))\n    \nsub[\"Pred\"] = pd.DataFrame(sub_preds).mean(axis=0)\n#sub[\"Pred\"] /= (2*sub[\"Pred\"].mean())\nsub.loc[sub['Pred'] > 0.5, \"Pred\"] *= 1.00 # This is calibrated by trial and error.\nsub.loc[sub['Pred'] < 0.5, \"Pred\"] /= 1.00\nprint(sub['Pred'].mean())\nsub[\"Pred\"] = np.clip(sub[\"Pred\"], 0.03, 0.97)\n","metadata":{"_uuid":"01757a38a0ad222dec31bf062ceee7cfb61a6b16","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Are you feeling lucky? Try to override some results.","metadata":{}},{"cell_type":"code","source":"#sub.loc[(sub.T1_seed==2) & (sub.T2_seed==15), 'Pred'] = 0.99\n#sub.loc[(sub.T1_seed==3) & (sub.T2_seed==14), 'Pred'] = 0.99\n#sub.loc[(sub.T1_seed==15) & (sub.T2_seed==2), 'Pred'] = 0.01\n#sub.loc[(sub.T1_seed==14) & (sub.T2_seed==3), 'Pred'] = 0.01\n#sub.loc[(sub.T1_seed==1) & (sub.T2_seed==16), 'Pred'] = 0.99\n#sub.loc[(sub.T1_seed==16) & (sub.T2_seed==1), 'Pred'] = 0.01\nsub[['ID','Pred']].to_csv(\"finalsubmission.csv\", index = None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tourney_results2018 = pd.read_csv('../input/NCAA_2018_Solution_Womens.csv')\n#tourney_results2018 = tourney_results2018[tourney_results2018.Pred!=-1].reset_index(drop=True)\n#tourney_results2018.columns = ['ID', 'label']\n#tourney_results2018 = pd.merge(tourney_results2018, sub, on = 'ID')\n#log_loss(tourney_results2018.label, tourney_results2018.Pred)","metadata":{"_uuid":"ce959e232dd5c58f5748e74e7a5fe0c5d2d6ef6f","trusted":true},"execution_count":null,"outputs":[]}]}