{"cells":[{"metadata":{},"cell_type":"markdown","source":"# imports"},{"metadata":{"_uuid":"993abe0b-2561-4abc-a6a2-b83d8dd4c846","_cell_guid":"16a3df23-8416-46b5-8661-c52345005b6d","trusted":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nimport numpy as np\nimport joblib\nimport logging\nimport transformers\nimport sys\nimport torch.nn as nn\nimport gc;\nimport h5py\nfrom scipy import stats\nfrom collections import OrderedDict, namedtuple\nfrom torch.optim import lr_scheduler\nfrom transformers import (\n    AdamW, get_linear_schedule_with_warmup, get_constant_schedule, \n    XLMRobertaTokenizer, XLMRobertaModel, XLMRobertaConfig,\n)\nfrom sklearn import metrics, model_selection\nfrom tqdm.autonotebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\n# load the data\n\ntrain1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\", usecols=[\"comment_text\", \"toxic\"])\ntrain2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\", usecols=[\"comment_text\", \"toxic\"])\ntrain2.toxic = train2.toxic.round().astype(int)\n\ndf_valid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\ntest = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\nsub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')\n\ndf_train = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=99937, random_state=0), # hacked to make train_data size divisible by bs;\n])\n\ndel train1, train2\ngc.collect(); gc.collect();\nprint(df_train.shape, df_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ntqdm.pandas()\ndf_train[\"comment_text\"] = df_train[\"comment_text\"].progress_apply(lambda x: \" \" + \" \".join(str(x).split()))\ndf_valid[\"comment_text\"] = df_valid[\"comment_text\"].progress_apply(lambda x: \" \" + \" \".join(str(x).split()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom joblib import Parallel, delayed\ntokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n\ndef regular_encode(texts, tokenizer=tokenizer, maxlen=128):\n    enc_di = tokenizer.encode_plus(\n        str(texts[0]),\n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids']), np.array(enc_di[\"attention_mask\"]), texts[1]\n\nrows = zip(df_train['comment_text'].values.tolist(), df_train.toxic.values.tolist())\nx_train = Parallel(n_jobs=4, backend='multiprocessing')(delayed(regular_encode)(row) for row in tqdm(rows))\n\nrows = zip(df_valid['comment_text'].values.tolist(), df_valid.toxic.values.tolist())\nx_valid = Parallel(n_jobs=4, backend='multiprocessing')(delayed(regular_encode)(row) for row in tqdm(rows))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.save(\"x_train_tokenized\", x_train)\nnp.save(\"x_valid_tokenized\", x_valid);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.array(x_train).shape, np.array(x_valid).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy\n\na = numpy.memmap('train.mymemmap', dtype='int32', mode='w+', shape=(2, np.array(x_train).shape[0], 128))\nfor idx in tqdm(range(np.array(x_train).shape[0])):\n    a[0][idx] = np.array(x_train[idx][0], dtype=np.int32)\n    a[1][idx] = np.array(x_train[idx][1], dtype=np.int32)\ndel a;\n\na = numpy.memmap('train_targets.mymemmap', dtype='int32', mode='w+', shape=(np.array(x_train).shape[0],))\nfor idx in tqdm(range(np.array(x_train).shape[0])):\n    a[idx] = np.array(x_train[idx][2], dtype=np.int32)\ndel a;\n\na = numpy.memmap('valid.mymemmap', dtype='int32', mode='w+', shape=(2, np.array(x_valid).shape[0], 128))\nfor idx in tqdm(range(np.array(x_valid).shape[0])):\n    a[0][idx] = np.array(x_valid[idx][0], dtype=np.int32)\n    a[1][idx] = np.array(x_valid[idx][1], dtype=np.int32)\ndel a\n\na = numpy.memmap('valid_targets.mymemmap', dtype='int32', mode='w+', shape=(np.array(x_valid).shape[0],))\nfor idx in tqdm(range(np.array(x_valid).shape[0])):\n    a[idx] = np.array(x_valid[idx][2], dtype=np.int32)\ndel a;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyIterableDataset_v1(torch.utils.data.IterableDataset):\n    \n    def __init__(self, batch_size):\n        \n        self.data = np.memmap(\"valid.mymemmap\", shape=(2, 8000, 128), mode=\"r\", dtype=\"int32\")\n        self.target = np.memmap(\"valid_targets.mymemmap\", shape=(8000,), mode=\"r\", dtype=\"int32\")\n        self.batch_size = batch_size\n    \n    def __iter__(self):\n        # memmap contains input_ids, masks, targets\n        return iter(zip(np.array(self.data[0]), np.array(self.data[1]), np.array(self.target)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iterable_dataset = MyIterableDataset_v1(batch_size = my_batch_size)\nloader = torch.utils.data.DataLoader(iterable_dataset, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for batch in tqdm(loader):\n    print(batch)\n    break","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}