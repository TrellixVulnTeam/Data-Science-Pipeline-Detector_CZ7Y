{"cells":[{"metadata":{},"cell_type":"markdown","source":"# LSTM with pre-trained GloVe embeddings \nOnly using one of the training sets. Relies on translated test and validation data from @bamps53 and @kashnitsky"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# # Load packages\n\n# Ignore warnings\nimport warnings\n\ndef warn(*args, **kwargs):\n    pass\n\nwarnings.warn = warn\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\n\nimport keras\nfrom keras import *\nfrom keras import layers\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom keras.models import Model\nfrom keras.preprocessing import *\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import Callback\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom io import StringIO","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"class RocCallback(Callback):\n    def __init__(self,training_data,validation_data):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.x_val = validation_data[0]\n        self.y_val = validation_data[1]\n    \n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_train = self.model.predict_proba(self.x)\n        roc_train = roc_auc_score(self.y, y_pred_train)\n        y_pred_val = self.model.predict_proba(self.x_val)\n        roc_val = roc_auc_score(self.y_val, y_pred_val)\n        print('\\rroc-auc_train: %s - roc-auc_val: %s' % (str(round(roc_train,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n\n\n    \nPLOT_FONT_SIZE = 10    #font size for axis of plots\n\n#define helper function for confusion matrix\n\ndef displayConfusionMatrix(confusionMatrix):\n    \"\"\"Confusion matrix plot\"\"\"\n    \n    confusionMatrix = np.transpose(confusionMatrix)\n    \n    ## calculate class level precision and recall from confusion matrix\n    precisionLow = round((confusionMatrix[0][0] / (confusionMatrix[0][0] + confusionMatrix[0][1]))*100, 1)\n    precisionHigh = round((confusionMatrix[1][1] / (confusionMatrix[1][0] + confusionMatrix[1][1]))*100, 1)\n    recallLow = round((confusionMatrix[0][0] / (confusionMatrix[0][0] + confusionMatrix[1][0]))*100, 1)\n    recallHigh = round((confusionMatrix[1][1] / (confusionMatrix[0][1] + confusionMatrix[1][1]))*100, 1)\n\n    ## show heatmap\n    plt.imshow(confusionMatrix, interpolation='nearest',cmap=plt.cm.Blues,vmin=0, vmax=100)\n    \n    ## axis labeling\n    xticks = np.array([0,1])\n    plt.gca().set_xticks(xticks)\n    plt.gca().set_yticks(xticks)\n    plt.gca().set_xticklabels([\"Not Toxic \\n Recall=\" + str(recallLow), \"Toxic \\n Recall=\" + str(recallHigh)], fontsize=PLOT_FONT_SIZE)\n    plt.gca().set_yticklabels([\"Not Toxic \\n Precision=\" + str(precisionLow), \"Toxic \\n Precision=\" + str(precisionHigh)], fontsize=PLOT_FONT_SIZE)\n    plt.ylabel(\"Predicted Class\", fontsize=PLOT_FONT_SIZE)\n    plt.xlabel(\"Actual Class\", fontsize=PLOT_FONT_SIZE)\n        \n    ## add text in heatmap boxes\n    addText(xticks, xticks, confusionMatrix)\n    \ndef addText(xticks, yticks, results):\n    \"\"\"Add text in the plot\"\"\"\n    for i in range(len(yticks)):\n        for j in range(len(xticks)):\n            text = plt.text(j, i, results[i][j], ha=\"center\", va=\"center\", color=\"white\", size=PLOT_FONT_SIZE) ### size here is the size of text inside a single box in the heatmap","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmetize_data(data,field):\n    cleaned_texts = []\n    for text in data[field]: # Loop through the tokens (the words or symbols) \n        cleaned_text = text.lower()  # Convert the text to lower case\n        cleaned_text = ' '.join([word for word in cleaned_text.split() if word not in stopset])  # Keep only words that are not stopwords.\n        cleaned_text = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='n') for word in cleaned_text.split()])  # Keep each noun's lemma.\n        cleaned_text = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='v') for word in cleaned_text.split()])  # Keep each verb's lemma.\n        cleaned_text = re.sub(\"[^a-zA-Z]\",\" \", cleaned_text)  # Remove numbers and punctuation.\n        cleaned_text = ' '.join(cleaned_text.split())  # Remove white space.\n        cleaned_texts.append(cleaned_text) \n    data['cleanText'] = cleaned_texts","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('stopwords')\nnltk.download('wordnet')\n\nwordnet_lemmatizer = WordNetLemmatizer()\nstopset = list(set(stopwords.words('english')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load training data 1\ntrain_comment=pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')\nlemmetize_data(train_comment,'comment_text')\ntrain_comment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load validation data 1\nvalidGoogle=pd.read_csv('../input/val-en-df/validation_en.csv')\nlemmetize_data(validGoogle,'comment_text_en')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load validation data 2\nvalidYandex=pd.read_csv('../input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_valid_translated.csv')\nlemmetize_data(validYandex,'translated')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# append, when we calculate AUC it will reflect the average\nvalid = validGoogle.append(validYandex)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load testing data (Translated via Google)\ntest_google=pd.read_csv('../input/test-en-df/test_en.csv')\nlemmetize_data(test_google,'content_en')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load testing data (Translated via Yandex)\ntest_yandex=pd.read_csv('../input/jigsaw-multilingual-toxic-test-translated/jigsaw_miltilingual_test_translated.csv')\nlemmetize_data(test_yandex,'translated')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotCases(data):\n    cases_count = data.value_counts(dropna=False)\n\n    # Plot  results \n    plt.figure(figsize=(6,6))\n    sns.barplot(x=cases_count.index, y=cases_count.values)\n    plt.ylabel('Texts', fontsize=12)\n    plt.xticks(range(len(cases_count.index)), ['Not', 'Toxic'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels = train_comment['toxic']\nvalid_labels = valid['toxic']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotCases(train_labels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the numbers here are doubled from the append above\nplotCases(valid_labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LSTM with pretrained"},{"metadata":{},"cell_type":"markdown","source":"Prepare data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lemmetize\ntrain_texts = train_comment['cleanText']\n\nvalid_texts = valid['cleanText']\n\ntest_textsGoogle = test_google['cleanText']\ntest_textsYandex = test_yandex['cleanText']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define vocabulary size (you can tune this parameter and evaluate model performance)\nVOCABULARY_SIZE = 15000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create input feature arrays\ntokenizer = Tokenizer(num_words=VOCABULARY_SIZE)\ntokenizer.fit_on_texts(train_texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert words into word ids\nmeanLengthTrain = np.mean([len(item.split(\" \")) for item in train_texts])\nmeanLengthValid = np.mean([len(item.split(\" \")) for item in valid_texts])\nmeanLengthTestGoogle = np.mean([len(item.split(\" \")) for item in test_textsGoogle])\nmeanLengthTestYandex = np.mean([len(item.split(\" \")) for item in test_textsYandex])\n\nprint('Average length - Train:',meanLengthTrain,'Valid:',meanLengthValid,'TestGoogle:',meanLengthTestGoogle,'TestYandex:',meanLengthTestYandex)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SENTENCE_LENGTH = int(meanLengthTrain + 20) # we let a text go 20 words longer than the mean text length (you can also tune this parameter).\n\n# Convert train, validation, and test text into lists with word ids\ntrainFeatures = tokenizer.texts_to_sequences(train_texts)\ntrainFeatures = pad_sequences(trainFeatures, MAX_SENTENCE_LENGTH, padding='post')\ntrainLabels = train_labels.values\n\nvalidFeatures = tokenizer.texts_to_sequences(valid_texts)\nvalidFeatures = pad_sequences(validFeatures, MAX_SENTENCE_LENGTH, padding='post')\nvalidLabels = valid_labels.values\n\ntestFeaturesGoogleLSTMwith = tokenizer.texts_to_sequences(test_textsGoogle)\ntestFeaturesGoogleLSTMwith = pad_sequences(testFeaturesGoogleLSTMwith, MAX_SENTENCE_LENGTH, padding='post')\n\ntestFeaturesYandexLSTMwith = tokenizer.texts_to_sequences(test_textsYandex)\ntestFeaturesYandexLSTMwith = pad_sequences(testFeaturesYandexLSTMwith, MAX_SENTENCE_LENGTH, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from: https://www.kaggle.com/bertcarremans/using-word-embeddings-for-sentiment-analysis/data\nEMBEDDING_FILE='../input/glove-twitter/glove.twitter.27B.25d.txt'\nemb_dict = {}\nglove = open(EMBEDDING_FILE)\nfor line in glove:\n    values = line.split()\n    word = values[0]\n    vector = np.asarray(values[1:], dtype='float32')\n    emb_dict[word] = vector\nglove.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from: https://www.kaggle.com/bertcarremans/using-word-embeddings-for-sentiment-analysis/data\nembedding_matrix = np.zeros((VOCABULARY_SIZE, 25))\n\nfor w, i in tokenizer.word_index.items():\n    # The word_index contains a token for all words of the training data so we need to limit that\n    if i < VOCABULARY_SIZE:\n        vect = emb_dict.get(w)\n        # Check if the word from the training data occurs in the GloVe word embeddings\n        # Otherwise the vector is kept with only zeros\n        if vect is not None:\n            embedding_matrix[i] = vect\n    else:\n        break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Hyperparameters for model tuning\nLEARNING_RATE = 0.001\nBATCH_SIZE = 128\nEPOCHS = 9","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#LSTM\nLSTMwith = Sequential()\n\n# We use pre-trained embeddings from GloVe. These are fed in as a layer of our network and the weights do not update during the training process.\nLSTMwith.add(Embedding(input_dim=VOCABULARY_SIZE, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False, input_length=len(trainFeatures[0])))\nLSTMwith.add(Bidirectional(LSTM(24)))\nLSTMwith.add(Dropout(0.6))\nLSTMwith.add(Dense(12, activation='relu'))\nLSTMwith.add(Dropout(0.5))\nLSTMwith.add(Dense(1, activation='sigmoid'))\n            \noptimizer = optimizers.Adam(lr=LEARNING_RATE)\nLSTMwith.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n\nprint(LSTMwith.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ratio of non-toxic to toxic in training:\n200000/25000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have a class imbalance, upweight the toxic comments\nclass_weights = {0: 1,\n                 1: 8}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc = RocCallback(training_data=(trainFeatures, trainLabels),\n                  validation_data=(validFeatures, validLabels))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train model\nstart = time.time()    \nhistory = LSTMwith.fit(trainFeatures, trainLabels, validation_data = (validFeatures, validLabels), batch_size=BATCH_SIZE, epochs=EPOCHS, class_weight=class_weights, callbacks=[roc])\nprint(\"Training took %d seconds\" % (time.time() - start))  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_valid_LSTMwith = pd.DataFrame(LSTMwith.predict(validFeatures))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(validLabels, pred_valid_LSTMwith)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"validPred = pred_valid_LSTMwith\npred_valid_binary = round(validPred)\n\nconfusionMatrix = None\nconfusionMatrix = confusion_matrix(validLabels, pred_valid_binary)\n\nplt.rcParams['figure.figsize'] = [3, 3] ## plot size\ndisplayConfusionMatrix(confusionMatrix)\nplt.title(\"Confusion Matrix\", fontsize=PLOT_FONT_SIZE)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# make test predictions (average both translations)\npredictionsGoogle = pd.DataFrame(LSTMwith.predict(testFeaturesGoogleLSTMwith))\npredictionsYandex = pd.DataFrame(LSTMwith.predict(testFeaturesYandexLSTMwith))\npredictions = (predictionsGoogle+predictionsYandex)/2\npredictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prep for submission\nsample = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\")\nsample['toxic'] = predictions\nsample","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make submission\nsample.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}