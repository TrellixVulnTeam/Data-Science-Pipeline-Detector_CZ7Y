{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport git\nimport os\n\ndirectory = \"abhi\"\nparent_dir = \"/kaggle/working\"\n\ntry:\n    BASE_PATH = os.path.join(parent_dir, directory)  \n    os.mkdir(BASE_PATH) \n    print(\"Directory '%s' created\" %directory) \nexcept:\n    pass\n\ngit.Git(BASE_PATH).clone(\"https://github.com/abhisha1991/w251_hw6\")\nprint(\"Repository is created\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# steps taken from here: https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-python\n\n!pip install ibm-cos-sdk\nimport boto3\nfrom botocore.client import Config\nimport ibm_boto3\nfrom ibm_botocore.client import Config, ClientError\n\n# Constants for IBM COS values - BAD PRACTICE TO EXPOSE THE VALUES\nCOS_ENDPOINT = \"https://s3.eu-de.cloud-object-storage.appdomain.cloud\"\nCOS_API_KEY_ID = \"<<REDACTED>>\"\nCOS_SERVICE_CRN = \"crn:v1:bluemix:public:iam-identity::a/f3de9bdab4ee4727b5f81c87efa6d530::serviceid:ServiceId-2ef85b26-fc98-4444-bc26-2af926723bfa\"\nCOS_AUTH_ENDPOINT = \"https://iam.bluemix.net/oidc/token\"\nCOS_RESOURCE_CRN = \"crn:v1:bluemix:public:cloud-object-storage:global:a/f3de9bdab4ee4727b5f81c87efa6d530:1f545981-7f91-47a0-8437-6b4c581e31e2::\"\nBUCKET_NAME = \"abhihw6bucket\"\n\n# Create resource\ncos = ibm_boto3.resource(\"s3\",\n    ibm_api_key_id=COS_API_KEY_ID,\n    ibm_service_instance_id=COS_RESOURCE_CRN,\n    ibm_auth_endpoint=COS_AUTH_ENDPOINT,\n    config=Config(signature_version=\"oauth\"),\n    endpoint_url=COS_ENDPOINT\n)\n\n'''\n# Create client \ncos = ibm_boto3.client(\"s3\",\n    ibm_api_key_id=COS_API_KEY_ID,\n    ibm_service_instance_id=COS_SERVICE_CRN,\n    config=Config(signature_version=\"oauth\"),\n    endpoint_url=COS_ENDPOINT\n)\n'''\n\ndef get_bucket_item_list(bucket_name):\n    print(\"Retrieving bucket contents from: {0}\".format(bucket_name))\n    try:\n        files = cos.Bucket(bucket_name).objects.all()\n        for file in files:\n            print(\"Item: {0} ({1} bytes).\".format(file.key, file.size))\n    except ClientError as be:\n        print(\"CLIENT ERROR: {0}\\n\".format(be))\n    except Exception as e:\n        print(\"Unable to retrieve bucket contents: {0}\".format(e))\n    return files\n\ndef get_item(bucket_name, item_name):\n    print(\"Retrieving item from bucket: {0}, key: {1}\".format(bucket_name, item_name))\n    try:\n        file = cos.Object(bucket_name, item_name).get()\n        # print(\"File Contents: {0}\".format(file[\"Body\"].read()))\n    except ClientError as be:\n        print(\"CLIENT ERROR: {0}\\n\".format(be))\n    except Exception as e:\n        print(\"Unable to retrieve file contents: {0}\".format(e))\n    return file[\"Body\"].read()\n\nall_files = get_bucket_item_list(BUCKET_NAME)\nfor file in all_files:\n    if file.size > 0:\n        file_content = get_item(BUCKET_NAME, file.key)\n        # Download this file locally\n        d = BASE_PATH + \"/\" + '/'.join(file.key.split('/')[0:-1])\n        if not os.path.exists(d):\n            os.makedirs(d)\n        f = open(BASE_PATH + \"/\" + file.key, \"wb\")\n        f.write(file_content)\n        f.close()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# taking the rest from here - we need to load the model and run it\n# https://www.kaggle.com/abhishek/pytorch-bert-inference\n\nimport sys\npackage_dir = \"../input/ppbert/pytorch-pretrained-bert/pytorch-pretrained-BERT\"\nsys.path.append(package_dir)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport torch.utils.data\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nfrom pytorch_pretrained_bert import BertConfig\n\nwarnings.filterwarnings(action='once')\n# we have to comment out the below because the notebook is running on kaggle's server which unfortunately doesnt allow for cuda training in its free version\n# device = torch.device('cuda')\n# instead we define the device as CPU based\ndevice = torch.device('cpu')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nBATCH_SIZE = 32\nBERT_MODEL_PATH = BASE_PATH + \"/\" + \"vm_output/data/uncased_L-12_H-768_A-12\"\n\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nbert_config = BertConfig(BERT_MODEL_PATH + \"/bert_config.json\")\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv(BASE_PATH + \"/vm_output/data/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype(str) \n\n# Unfortunately, since we are evaluating on a cpu based backend and cuda is disabled, doing inference for 3000+ test batches (when batch size = 32)\n# is going to take over 10 hours! Instead, we consider only the top n rows which get processed in batches of 32\ntest_df = test_df[:128]\n\n\nX_test = convert_lines(test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), MAX_SEQUENCE_LENGTH, tokenizer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = BertForSequenceClassification(bert_config, num_labels=1)\nmodel.load_state_dict(torch.load(BASE_PATH + \"/vm_output/bert_pytorch.bin\", map_location=device))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_preds = np.zeros((len(X_test)))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.long))\ntest_loader = torch.utils.data.DataLoader(test, batch_size=32, shuffle=False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n    test_preds[i * 32:(i + 1) * 32] = pred[:, 0].detach().cpu().squeeze().numpy()\n\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame.from_dict({\n    'id': test_df['id'],\n    'prediction': test_pred\n})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission[10:30]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}