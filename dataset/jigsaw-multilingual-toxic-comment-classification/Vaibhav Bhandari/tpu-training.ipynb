{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Code to setup pytorch in kaggle. Found in a comment\n!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nfrom collections import OrderedDict, namedtuple\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport joblib\n\nimport logging\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule#AdamW is an adaptive optimizer\nimport sys\nfrom sklearn import metrics, model_selection\n\n#Import torch_xla for running pytorch on a TPU\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\n\n#Bert base uncased is used here. Uncased means that the text has been lowercased and it is the base model(not large model) of BERT\n#Transformers are pre built architectures for NLP by HuggingFace\n#BERT stands for Bidirectional Encoder Representations from Transformers. It trains bidirectional representations of text from left and right sides simultaneously.\n#BERT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n\nclass BERTBaseUncased(nn.Module):\n    def __init__(self, bert_path):\n        super(BERTBaseUncased, self).__init__()\n        self.bert_path = bert_path#Taking the pretrained model from TRANSFORMERS and defining the path(which is in config file)\n        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n        self.bert_drop = nn.Dropout(0.3)#Has a dropout 0f 0.3, that is 30% of the input tensors are zeroed out\n        self.out = nn.Linear(768 * 2, 1)#We get a vector of size 768*2, one 768 for mean pooling, one 768 for max pooling\n\n    def forward(self,ids,mask,token_type_ids):\n        o1, _ = self.bert(ids,attention_mask=mask,token_type_ids=token_type_ids)#Here the underscore represnts that we dont need the second ouotput in the forward propogation step\n        apool = torch.mean(o1, 1)#Both of these will be vectors of size 768 as the out layer is a vector of size 768(self.out)\n        mpool, _ = torch.max(o1, 1)\n        cat = torch.cat((apool, mpool), 1)#We concatenate both the poolings with axis 1\n        bo = self.bert_drop(cat)\n        p2 = self.out(bo)\n        return p2\n\nclass BERTDatasetTraining:\n    def __init__(self, comment_text, targets, tokenizer, max_length):\n        self.comment_text = comment_text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.comment_text)\n\n    def __getitem__(self, item):\n        comment_text = str(self.comment_text[item])\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(#from Hugging Face Tokenizers that encodes first and second string, but here there is no second string, so its None\n            comment_text,            \n            None,\n            add_special_tokens=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        padding_length = self.max_length - len(ids)\n        \n        ids = ids + ([0] * padding_length)#We padd it on the right for BERT\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n        }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mx = BERTBaseUncased(bert_path=\"../input/bert-base-multilingual-uncased/\")#The BERT Base Uncased model loaded, after adding it to the kaggle input data\ndf_train1 = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\", usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\ndf_train2 = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\", usecols=[\"comment_text\", \"toxic\"]).fillna(\"none\")\ndf_train_full = pd.concat([df_train1, df_train2], axis=0).reset_index(drop=True)\ndf_train = df_train_full.sample(frac=1).reset_index(drop=True).head(200000)\n\ndf_valid = pd.read_csv('../input/jigsaw-multilingual-toxic-comment-classification/validation.csv', usecols=[\"comment_text\", \"toxic\"])\n\n#df_train = pd.concat([df_train, df_valid], axis=0).reset_index(drop=True)\n#df_train = df_train.sample(frac=1).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _run():\n    def loss_fn(outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))#Simple loss function inbuilt\n\n    def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n        model.train()\n        #Go through all the batches inside Data Loader\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            targets = d[\"targets\"]\n\n            #Put all the values to the device you are using\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            optimizer.zero_grad()#Clears the gradients of all optimized\n            outputs = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n            loss = loss_fn(outputs, targets)# Calculate batch loss based on CrossEntropy\n            if bi % 10 == 0:\n                xm.master_print(f'bi={bi}, loss={loss}')#Prints the loss for each batch index of multiples of 10\n\n            loss.backward()#Back Propogation\n            xm.optimizer_step(optimizer)#Optimizer function but should be called like this if using TPU\n            if scheduler is not None:\n                scheduler.step()#Needed to change the learning rate everytime\n\n    def eval_loop_fn(data_loader, model, device):\n        model.eval()\n        fin_targets = []\n        fin_outputs = []\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n            targets_np = targets.cpu().detach().numpy().tolist()\n            outputs_np = outputs.cpu().detach().numpy().tolist()\n            fin_targets.extend(targets_np)#Adds element to the list and extends it\n            fin_outputs.extend(outputs_np)    \n\n        return fin_outputs, fin_targets\n\n    \n    MAX_LEN = 192\n    TRAIN_BATCH_SIZE = 64\n    EPOCHS = 2\n\n    tokenizer = transformers.BertTokenizer.from_pretrained(\"../input/bert-base-multilingual-uncased/\", do_lower_case=True)\n\n    train_targets = df_train.toxic.values\n    valid_targets = df_valid.toxic.values\n\n    train_dataset = BERTDatasetTraining(#calling Constructor of class BERTDatasetTraining \n        comment_text=df_train.comment_text.values,\n        targets=train_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(#We have to use distributedSampler for TPUs which distributes the data over multiple cores\n          train_dataset,\n          num_replicas=xm.xrt_world_size(),#World size if the number of cores being used\n          rank=xm.get_ordinal(),#Its the rank of current process in num_replicas.Retrieves the replication ordinal of the current process.The ordinals range from 0 to xrt_world_size() minus 1.\n          shuffle=True)#If true (default), sampler will shuffle the indices\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True, #the drop_last argument drops the last non-full batch of each worker’s dataset replica.As each core creates a dataset replica for itself, if there are no equal batch sizes, it will crash\n        num_workers=1\n    )\n#All validation functions are similar to training functions\n    valid_dataset = BERTDatasetTraining(\n        comment_text=df_valid.comment_text.values,\n        targets=valid_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=16,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n\n    #device = torch.device(\"cuda\")\n    #device = torch.device(\"cpu\")\n    device = xm.xla_device()#To recognize device as TPU\n    model = mx.to(device)\n\n    param_optimizer = list(model.named_parameters())# Get the list of named parameters\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']# Specify parameters where weight decay shouldn't be applied\n    optimizer_grouped_parameters = [# Define two sets of parameters: those with weight decay, and those without\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    lr = 3e-5 * xm.xrt_world_size()#Learning Rate\n    #lr = 0.4 * 1e-5 * xm.xrt_world_size()#Learning Rate\n    num_train_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE / xm.xrt_world_size() * EPOCHS)#Steps will be divided by batch size and the number of cores being used\n    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)# Instantiate AdamW optimizer with our two sets of parameters, and a learning rate\n    scheduler = get_linear_schedule_with_warmup(#Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period.\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])#ParallelLoader loads the training data onto each device\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)#Call the train loop function\n\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])#Do same for validation data\n        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n        xm.save(model.state_dict(), \"model.bin\")#Saves the file as model.bin in output files\n        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)#Prints the AUC score as that is the one used for the competition\n        xm.master_print(f'AUC = {auc}')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}