{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as pylab\nimport seaborn as sns\n\nimport re\nimport keras\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, GlobalAveragePooling1D, concatenate\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n#rom keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint \nfrom keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n\ntest_data = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\")\ntest_data.columns = ['id','comment_text','lang']\nvalidation_data = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#target_columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain_data.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(pd.isnull(train_data).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dataset in [train_data, test_data]:\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'ll', ' will'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'ve', ' have'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('don\\'t', ' do not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('dont', ' do not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('aren\\'t', ' are not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('won\\'t', ' will not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('wont', ' will not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('can\\'t', ' cannot'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('cant', ' cannot'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('shan\\'t', ' shall not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('shant', ' shall not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'m', ' am'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"doesn't\", \"does not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"doesnt\", \"does not\"))                                                      \n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didn't\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didnt\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"hasn't\", \"has not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"hasnt\", \"has not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"haven't\", \"have not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"havent\", \"have not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"wouldn't\", \"would not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didn't\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didnt\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"it's\" , \"it is\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"that's\" , \"that is\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"weren't\" , \"were not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"werent\" , \"were not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(' u ', ' you '))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(' U ', ' you '))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: re.sub('[\\(\\)\\\"\\t_\\n.,:=!@#$%^&*-/[\\]?|1234567890â€”]', ' ', x).strip())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nplt.figure(figsize=(7,7))\nplt.title('Correlation of Features & Targets',y=1.05,size=13)\nsns.heatmap(train_data[target_columns].astype(float).corr(),linewidths=0.2,vmax=1.0,square=True,annot=True)\nplt.show()\n\"\"\"\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Y = train_data[target_columns]\nY = train_data['toxic']\nY","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_features = 20000\nmax_length = 100\nembed_size = 300\nbatch_size = 1024\nepochs = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nTokenization\n\"\"\"\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(train_data['comment_text'])\n\ntrain_tokenized = tokenizer.texts_to_sequences(train_data['comment_text'])\ntest_tokenized = tokenizer.texts_to_sequences(test_data['comment_text'])\n\nX = pad_sequences(train_tokenized, maxlen=max_length)\nX_ = pad_sequences(test_tokenized, maxlen=max_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nEmbedding Matrix\n\"\"\"\nembedding_index = {}\nwith open(\"/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\", encoding='utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        embedding_index[values[0]] = np.asarray(values[1:], dtype='float32')\n\nword_index = tokenizer.word_index\nnum_words = min(max_features, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features:\n        continue\n\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inp = Input(shape=(max_length,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(GRU(128, return_sequences=True, dropout=0.1,recurrent_dropout=0.1))(x)\nx = Conv1D(64, kernel_size = 3, padding = \"valid\", activation=\"relu\")(x)\n\nx = concatenate([GlobalAveragePooling1D()(x), GlobalMaxPool1D()(x)])\n\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-8)\nmodel.fit(X, Y, batch_size=batch_size, epochs=epochs, validation_split=0.1,\n              callbacks=[reduce_lr])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sumbission_file = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')\n#sumbission_file = sumbission_file.drop('toxic',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub = model.predict(X_)\n#cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n\"\"\"\nfor i in cols:\n    sumbission_file[i]=\"\"\n\"\"\"\n\n\nsumbission_file['toxic'] = sub\nsumbission_file.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}