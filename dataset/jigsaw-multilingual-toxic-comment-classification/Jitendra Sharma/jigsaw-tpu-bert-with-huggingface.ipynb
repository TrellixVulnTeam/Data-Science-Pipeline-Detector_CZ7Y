{"cells":[{"metadata":{"_uuid":"1cbef360-309f-4569-9ed4-f2a14284a5d0","_cell_guid":"63e75482-a733-4222-b5d6-50c7a2cbd4f0","trusted":true},"cell_type":"markdown","source":"## About this notebook\n\n*[Jigsaw Multilingual Toxic Comment Classification](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification)* is the 3rd annual competition organized by the Jigsaw team. It follows *[Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)*, the original 2018 competition, and *[Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)*, which required the competitors to consider biased ML predictions in their new models. This year, the goal is to use english only training data to run toxicity predictions on many different languages, which can be done using multilingual models, and speed up using TPUs.\n\nMany awesome notebooks has already been made so far. Many of them used really cool technologies like [Pytorch XLA](https://www.kaggle.com/theoviel/bert-pytorch-huggingface-starter). This notebook instead aims at constructing a **fast, concise, reusable, and beginner-friendly model scaffold**. It will focus on the following points:\n* **Using Tensorflow and Keras**: Tensorflow is a powerful framework, and Keras makes the training process extremely easy to understand. This is especially good for beginners to learn how to use TPUs, and for experts to focus on the modelling aspect.\n* **Huggingface's transformers library**: [This library](https://huggingface.co/transformers/) is extremely popular, so using this let you easily integrate the end result into your ML pipelines, and can be easily reused for your other projects.\n* **Multilingual DistilBERT**: DistilBERT is **2 times faster and 25% lighter** than multilingual BERT base, all while retaining **92% of its performance**. This model let you quickly experiments with different ideas, and when you are ready for the real thing, just change two lines of code to use `bert-base-multilingual-cased`.\n* **Blazing fast tokenization**: [Huggingface's `tokenizers`](https://github.com/huggingface/tokenizers/tree/master/bindings/python) is order of magnitude faster than the default BERT tokenizer, since it is written in Rust, and uses a Python interface.\n* **Native TPU usage**: The TPU usage is abstracted using the native `strategy` that was created using Tensorflow's `tf.distribute.experimental.TPUStrategy`. This avoids getting too much into the lower-level aspect of TPU management.\n* **Subset of the data**: Instead of using the entire dataset, we will only use the 2018 subset of the data available, which makes this much faster, all while achieving a respectable accuracy.\n\n\n### References\n* Original Author: [@xhlulu](https://www.kaggle.com/xhlulu/)\n* Original notebook: [Link](https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras)"},{"metadata":{"_uuid":"b8bfe409-b717-455b-a2f0-d8cd07992ed3","_cell_guid":"4e044074-0b37-4ec6-8383-948414e1222b","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0519f9dc-8da6-4809-af36-144a1debd905","_cell_guid":"456ba035-b96d-4aeb-b0b3-6241ebbdc6c0","trusted":true},"cell_type":"markdown","source":"## Helper Functions"},{"metadata":{"_uuid":"317be182-115c-43f5-8bf9-91150a05ab85","_cell_guid":"a4a3d67f-66fc-4ba4-98fc-f579cbebf4af","trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ea3039c-23a9-4ed5-95e7-a13eb2098477","_cell_guid":"6c3b2835-00f5-4e9f-b96e-69d1caac5258","trusted":true},"cell_type":"code","source":"def build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ee35ede-9770-44ed-8cf5-8c6ba8c6afb1","_cell_guid":"7925a833-1b52-4480-9f1d-8b680312117c","trusted":true},"cell_type":"markdown","source":"## TPU Configs"},{"metadata":{"_uuid":"b9075c62-7984-4144-8911-6b0e762435c3","_cell_guid":"fab6950d-4a07-47fa-91a8-fe23b8a7a0f3","trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40bc923c-a51d-4320-b5fa-ce25bc24707c","_cell_guid":"01bfb237-5ec8-444b-953c-6d9675a81c3c","trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\n# Configuration\nEPOCHS = 3\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b1710ef-0add-48ee-b777-9aa1b3e3e88b","_cell_guid":"8e940bf9-931b-4a19-8786-633173227ae2","trusted":true},"cell_type":"markdown","source":"## Create fast tokenizer"},{"metadata":{"_uuid":"167282cf-da65-4872-b6f8-31a2653b6d5a","_cell_guid":"700c31d0-2cc8-43af-90c3-65876dba2dd2","trusted":true},"cell_type":"code","source":"# First load the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\nfast_tokenizer","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d8e9a89c-9794-47bc-86dd-ecd0ce6f6f78","_cell_guid":"17980434-5e22-4f8d-936b-98f6032ece92","trusted":true},"cell_type":"markdown","source":"## Load text data into memory"},{"metadata":{"_uuid":"0a0ab321-a283-4309-ba8d-79d17169d75d","_cell_guid":"2d426701-17e0-4eb0-ac18-a27f51738375","trusted":true},"cell_type":"code","source":"train1 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\ntrain2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\ntrain2.toxic = train2.toxic.round().astype(int)\n\nvalid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\ntest = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\nsub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"088b9e10-974e-448c-9b94-c8905127929b","_cell_guid":"c238c1dd-302d-42ed-a7c4-46c8653552d5","trusted":true},"cell_type":"code","source":"# Combine train1 with a subset of train2\ntrain = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=150000, random_state=0)\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['comment_text'] = train['comment_text'].apply(lambda x:clean_text(x))\nvalid['comment_text'] = valid['comment_text'].apply(lambda x:clean_text(x))\ntest['content'] = test['content'].apply(lambda x:clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f82329f1-c6a9-4aad-95e4-fa495441dc13","_cell_guid":"f36d2caf-a870-4f1b-8f96-5333ad8fed9e","trusted":true},"cell_type":"code","source":"x_train = fast_encode(train.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nx_valid = fast_encode(valid.comment_text.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nx_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ny_train = train.toxic.values\ny_valid = valid.toxic.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e2f8252-9f98-44c1-b865-0069506efa93","_cell_guid":"2445e113-5517-456e-b63d-7f8bc3af0adb","trusted":true},"cell_type":"markdown","source":"## Build datasets objects"},{"metadata":{"_uuid":"bac9a265-a3fe-439a-8609-3205dcb7d531","_cell_guid":"06118524-8d1c-4e1f-b602-f04f5935c8eb","trusted":true},"cell_type":"code","source":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10a438fa-8adb-46df-a1a2-04ee4e9bc93f","_cell_guid":"dab98cb4-5b07-4e13-b0c7-c59912c7882e","trusted":true},"cell_type":"markdown","source":"## Load model into the TPU"},{"metadata":{"_uuid":"189fcae5-a320-49dc-b345-994de42c94ba","_cell_guid":"b1f7f1d0-abf6-4869-b44c-0ce8599e733f","trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = (transformers.TFDistilBertModel.from_pretrained('distilbert-base-multilingual-cased'))\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c66910d-8b33-4fba-b073-b9445f0ce38f","_cell_guid":"c5339e9a-ad19-4b36-969c-4fee2dc2c52d","trusted":true},"cell_type":"markdown","source":"## Train Model"},{"metadata":{"_uuid":"fd2d162b-4d4a-4cca-b081-982f914dda72","_cell_guid":"4ad894c1-fde7-419d-922b-a9f69631d969","trusted":true},"cell_type":"markdown","source":"First, we train on the subset of the training set, which is completely in English."},{"metadata":{"_uuid":"9d22983a-1657-41c4-b6d5-4b02726f6bd3","_cell_guid":"c2c4ce2f-01ed-49be-8128-d5265711e542","trusted":true},"cell_type":"code","source":"n_steps = x_train.shape[0] // BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05f035cf-a206-410d-a424-6e757de22496","_cell_guid":"c5e93a10-7ef3-4a0a-85cb-3964e8a81aef","trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n# Plot training & validation accuracy values\nplt.plot(train_history.history['accuracy'])\nplt.plot(train_history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"910ceffa-53fd-4d04-9f59-513af1370b70","_cell_guid":"a1b358fc-10e1-40c8-8107-e24d46acf7cc","trusted":true},"cell_type":"markdown","source":"Now that we have pretty much saturated the learning potential of the model on english only data, we train it for one more epoch on the `validation` set, which is significantly smaller but contains a mixture of different languages."},{"metadata":{"_uuid":"890c9bf6-06ca-4d9f-9449-045c14e5e085","_cell_guid":"b605d198-3f0b-46f6-8774-c24d9ca4a6b6","trusted":true},"cell_type":"code","source":"n_steps = x_valid.shape[0] // BATCH_SIZE\ntrain_history_2 = model.fit(valid_dataset.repeat(), steps_per_epoch=n_steps, epochs=EPOCHS*2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"911e1462-ffdc-43bb-9f05-b97a5c37ff1e","_cell_guid":"4fb8c23b-3410-4197-aeec-8758b8577d28","trusted":true},"cell_type":"code","source":"# Plot training & validation accuracy values\nplt.plot(train_history_2.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eee5b777-6396-4233-b7a5-269ca2f319d6","_cell_guid":"c7c9d7b3-91ca-4050-b75d-5b77aaf44c4c","trusted":true},"cell_type":"markdown","source":"## Submission"},{"metadata":{"_uuid":"fcbf332d-25eb-4dbc-9d56-78a5a56a473f","_cell_guid":"0e3c7aaf-94a2-4082-bf2b-1edf797bf0f4","trusted":true},"cell_type":"code","source":"sub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57b75db7-0968-4a02-9d3c-9dbbe1c840f9","_cell_guid":"7dd9d699-4101-4c43-bdca-1ea360dd4581","trusted":true},"cell_type":"code","source":"","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}