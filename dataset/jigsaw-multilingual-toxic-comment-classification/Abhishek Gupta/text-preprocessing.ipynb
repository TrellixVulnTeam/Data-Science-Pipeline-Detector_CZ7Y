{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install googletrans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install translate","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfile = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nfrom googletrans import Translator\nfrom bs4 import BeautifulSoup\nimport string\nimport re\nimport json\n\nclass PreprocessData():\n    def __init__(self,file_name):\n        self.f_name = pd.read_csv(file_name)\n        self.ps = PorterStemmer()\n        self.translator = Translator()\n        self.stop_words = set(stopwords.words('english')) \n        self.cnt=0\n    \n    def getFile(self):\n        return self.f_name\n    \n    def decontract_word(self,data):\n        decont = re.sub(r\"won't\", \"will not\", data)\n        decont = re.sub(r\"can\\'t\", \"can not\", decont)\n        decont = re.sub(r\"n\\'t\", \" not\", decont)\n        decont = re.sub(r\"\\'re\", \" are\", decont)\n        decont = re.sub(r\"\\'s\", \" is\", decont)\n        decont = re.sub(r\"\\'d\", \" would\", decont)\n        decont = re.sub(r\"\\'ll\", \" will\", decont)\n        decont = re.sub(r\"\\'t\", \" not\", decont)\n        decont = re.sub(r\"\\'ve\", \" have\", decont)\n        decont = re.sub(r\"\\'m\", \" am\", decont)\n        return decont\n    \n    def clean_punct(self,data):\n        try:\n            new_str = [char for char in data if char not in string.punctuation]\n            sent = ''.join(new_str)\n            sent = re.sub(r\"http\\S+\", \"\", sent)\n            sent = BeautifulSoup(sent, 'lxml').get_text()\n            sent = self.decontract_word(sent)\n            sent = re.sub(\"\\S*\\d\\S*\", \"\", sent).strip()\n            sent = re.sub('[^A-Za-z]+', ' ', sent)\n            sent = [word.lower() for word in sent.split() if word.lower() not in self.stop_words]\n            return ' '.join(sent)\n        except Exception as e:\n            print(e)\n            return \"unknown\"\n    \n    def translate_and_clean_data(self,data):\n        try:\n            \n            data = self.translator.translate(data,dest='en').text\n            data = self.clean_punct(data)\n            print(str(self.cnt)+\" \"+data)\n#             print(str(self.cnt)+\" \"+self.translator.detect(data).lang)\n            self.cnt+=1\n            return data\n        except Exception as e:\n            print(e)\n            return \"unknown\"\n    \n    def get_translated_clean_text(self,col_name):\n        self.f_name[col_name] = self.f_name[col_name].map(lambda x: re.sub('\\\\n',' ',str(x)))\n        self.f_name[col_name] = self.f_name[col_name].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n        self.f_name[col_name] = self.f_name[col_name].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n        self.f_name[col_name] = self.f_name[col_name].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n#         self.f_name[col_name] = self.f_name[col_name].map(self.translate_and_clean_data)\n        \n    def clean_text(self,col_name):\n        self.f_name[col_name] = self.f_name[col_name].map(self.clean_punct)\n        self.f_name[col_name] = self.f_name[col_name].map(lambda x: re.sub('\\\\n',' ',str(x)))\n        self.f_name[col_name] = self.f_name[col_name].map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n        self.f_name[col_name] = self.f_name[col_name].map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n        self.f_name[col_name] = self.f_name[col_name].map(lambda x: re.sub(\"(http://.*?\\s)|(http://.*)\",'',str(x)))\n    \n    def save_csv_file(self,name):\n        self.f_name.to_csv(\"pre_proc_file/\"+name,index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == \"__main__\":\n    pp1 = PreprocessData(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n    pp2 = PreprocessData(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\")\n    pp3 = PreprocessData(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = pp1.getFile()\nprint(dt['comment_text'].head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp1.clean_text(\"comment_text\")\ndt = pp1.getFile()\nprint(dt['comment_text'].head(15))\n# pp1.save_csv_file(\"train_set.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp2.get_translated_clean_text('comment_text')\ndt = pp2.getFile()\nprint(dt['comment_text'].head(10))\n# pp2.save_csv_file(\"valid_set.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp3.get_translated_clean_text('content')\ndt = pp3.getFile()\nprint(dt['content'].head(10))\n# pp2.save_csv_file(\"test_set.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir pre_proc_file","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}