{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as pylab\nimport seaborn as sns\n\nimport re\nimport keras\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, GlobalAveragePooling1D, concatenate\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n#rom keras.callbacks import CSVLogger, ReduceLROnPlateau, ModelCheckpoint \nfrom keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-18T15:18:43.048803Z","iopub.execute_input":"2021-11-18T15:18:43.049356Z","iopub.status.idle":"2021-11-18T15:18:47.98326Z","shell.execute_reply.started":"2021-11-18T15:18:43.049324Z","shell.execute_reply":"2021-11-18T15:18:47.982139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n\ntest_data = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\")\ntest_data.columns = ['id','comment_text','lang']\nvalidation_data = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:18:47.985916Z","iopub.execute_input":"2021-11-18T15:18:47.986248Z","iopub.status.idle":"2021-11-18T15:18:51.601126Z","shell.execute_reply.started":"2021-11-18T15:18:47.986209Z","shell.execute_reply":"2021-11-18T15:18:51.600195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:18:51.602809Z","iopub.execute_input":"2021-11-18T15:18:51.606172Z","iopub.status.idle":"2021-11-18T15:18:51.635354Z","shell.execute_reply.started":"2021-11-18T15:18:51.606126Z","shell.execute_reply":"2021-11-18T15:18:51.634305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(pd.isnull(train_data).sum())","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:18:51.63716Z","iopub.execute_input":"2021-11-18T15:18:51.637675Z","iopub.status.idle":"2021-11-18T15:18:51.725887Z","shell.execute_reply.started":"2021-11-18T15:18:51.637633Z","shell.execute_reply":"2021-11-18T15:18:51.724664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in [train_data, test_data]:\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'ll', ' will'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'ve', ' have'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('don\\'t', ' do not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('dont', ' do not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('aren\\'t', ' are not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('won\\'t', ' will not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('wont', ' will not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('can\\'t', ' cannot'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('cant', ' cannot'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('shan\\'t', ' shall not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('shant', ' shall not'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace('\\'m', ' am'))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"doesn't\", \"does not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"doesnt\", \"does not\"))                                                      \n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didn't\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didnt\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"hasn't\", \"has not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"hasnt\", \"has not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"haven't\", \"have not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"havent\", \"have not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"wouldn't\", \"would not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didn't\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"didnt\", \"did not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"it's\" , \"it is\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace( \"that's\" , \"that is\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"weren't\" , \"were not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(\"werent\" , \"were not\"))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(' u ', ' you '))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: x.replace(' U ', ' you '))\n    dataset['comment_text'] = dataset['comment_text'].apply(lambda x: re.sub('[\\(\\)\\\"\\t_\\n.,:=!@#$%^&*-/[\\]?|1234567890â€”]', ' ', x).strip())","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:18:51.729649Z","iopub.execute_input":"2021-11-18T15:18:51.730171Z","iopub.status.idle":"2021-11-18T15:19:02.994899Z","shell.execute_reply.started":"2021-11-18T15:18:51.730125Z","shell.execute_reply":"2021-11-18T15:19:02.994017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = train_data['toxic']\nY","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:19:02.997189Z","iopub.execute_input":"2021-11-18T15:19:02.997477Z","iopub.status.idle":"2021-11-18T15:19:03.009511Z","shell.execute_reply.started":"2021-11-18T15:19:02.997436Z","shell.execute_reply":"2021-11-18T15:19:03.008612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_features = 20000\nmax_length = 220\nembed_size = 300\nbatch_size = 1024\nepochs = 15","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:19:03.01113Z","iopub.execute_input":"2021-11-18T15:19:03.011917Z","iopub.status.idle":"2021-11-18T15:19:03.021177Z","shell.execute_reply.started":"2021-11-18T15:19:03.011844Z","shell.execute_reply":"2021-11-18T15:19:03.019899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(validation_data['comment_text'])\n\ntrain_tokenized = tokenizer.texts_to_sequences(validation_data['comment_text'])\ntest_tokenized = tokenizer.texts_to_sequences(test_data['comment_text'])\n\nX = pad_sequences(train_tokenized, maxlen=max_length)\nX_ = pad_sequences(test_tokenized, maxlen=max_length)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:19:03.022764Z","iopub.execute_input":"2021-11-18T15:19:03.023817Z","iopub.status.idle":"2021-11-18T15:19:10.460815Z","shell.execute_reply.started":"2021-11-18T15:19:03.023774Z","shell.execute_reply":"2021-11-18T15:19:10.45989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_index = {}\nwith open(\"/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\", encoding='utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        embedding_index[values[0]] = np.asarray(values[1:], dtype='float32')\n\nword_index = tokenizer.word_index\nnum_words = min(max_features, len(word_index) + 1)\nembedding_matrix = np.zeros((num_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features:\n        continue\n\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:19:10.463283Z","iopub.execute_input":"2021-11-18T15:19:10.463768Z","iopub.status.idle":"2021-11-18T15:23:40.557742Z","shell.execute_reply.started":"2021-11-18T15:19:10.463727Z","shell.execute_reply":"2021-11-18T15:23:40.556793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = Input(shape=(max_length,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(GRU(128, return_sequences=True, dropout=0.1,recurrent_dropout=0.1))(x)\nx = Conv1D(64, kernel_size = 3, padding = \"valid\", activation=\"relu\")(x)\n\nx = concatenate([GlobalAveragePooling1D()(x), GlobalMaxPool1D()(x)])\n\nx = Conv1D(32, kernel_size = 3, padding = \"valid\", activation=\"relu\")(x)\n\nx = concatenate([GlobalAveragePooling1D()(x), GlobalMaxPool1D()(x)])\n\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:23:40.559456Z","iopub.execute_input":"2021-11-18T15:23:40.559815Z","iopub.status.idle":"2021-11-18T15:23:44.330607Z","shell.execute_reply.started":"2021-11-18T15:23:40.559738Z","shell.execute_reply":"2021-11-18T15:23:44.327995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=1e-8)\nmodel.fit(X, Y, batch_size=batch_size, epochs=epochs, validation_split=0.1,\n              callbacks=[reduce_lr])","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:23:44.3318Z","iopub.status.idle":"2021-11-18T15:23:44.332224Z","shell.execute_reply.started":"2021-11-18T15:23:44.332017Z","shell.execute_reply":"2021-11-18T15:23:44.332041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sumbission_file = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:23:44.333433Z","iopub.status.idle":"2021-11-18T15:23:44.33427Z","shell.execute_reply.started":"2021-11-18T15:23:44.333893Z","shell.execute_reply":"2021-11-18T15:23:44.333929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = model.predict(X_)\n#cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n\nsumbission_file['toxic'] = sub\nsumbission_file.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:23:44.337342Z","iopub.status.idle":"2021-11-18T15:23:44.338214Z","shell.execute_reply.started":"2021-11-18T15:23:44.337834Z","shell.execute_reply":"2021-11-18T15:23:44.337869Z"},"trusted":true},"execution_count":null,"outputs":[]}]}