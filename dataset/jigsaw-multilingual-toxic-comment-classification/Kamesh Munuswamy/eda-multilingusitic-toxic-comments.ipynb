{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Implementation of XLM-Roberta and DistilBert Transformer models for classification of Multilinguistic Toxic Comments"},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":"**Importing required Libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip\n!pip install -q textstat","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport gc\nimport os\nimport time\nimport math\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom datetime import date\nfrom transformers import *\nfrom sklearn.metrics import *\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport re\nimport folium\nimport textstat\nfrom scipy import stats\nfrom colorama import Fore, Back, Style, init\n\nimport math\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport random\nimport networkx as nx\nfrom pandas import Timestamp\n\nfrom PIL import Image\nfrom IPython.display import SVG\nfrom keras.utils import model_to_dot\n\nimport requests\nfrom IPython.display import HTML\n\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ntqdm.pandas()\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport transformers\nimport tensorflow as tf\n\nfrom tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n\nfrom tensorflow.keras.models import Model\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow.keras.optimizers import Adam\nfrom tokenizers import BertWordPieceTokenizer\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Embedding\nfrom tensorflow.keras.layers import LSTM, GRU, Conv1D, SpatialDropout1D\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.constraints import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.regularizers import *\n\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer,\\\n                                            CountVectorizer,\\\n                                            HashingVectorizer\n\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer  \n\nimport nltk\nfrom textblob import TextBlob\n\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nstopword=set(STOPWORDS)\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Importing required datasets from .CSV files**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = pd.read_csv(\"../input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n\n\ntrain2 = pd.read_csv(\"/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\")\ntrain2.toxic = train2.toxic.round().astype(int)\n\ntrain3 = pd.read_csv('../input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es-cleaned.csv')\n\n\nvalid = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\ntest = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')\nsub = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')\n\n\ntoxic = len(train2[['comment_text', 'toxic']].query('toxic==1'))\n# Combine train1 with a subset of train2\ntrain_cat = pd.concat([\n    train1[['comment_text', 'toxic']],\n    train2[['comment_text', 'toxic']].query('toxic==1'),\n    \n    train3[['comment_text', 'toxic']].query('toxic==0'),\n    train3[['comment_text', 'toxic']].query('toxic==1'),   \n    train2[['comment_text', 'toxic']].query('toxic==0').sample(n=(toxic+(toxic//3)), random_state=101)\n]).sample(n=600000).reset_index(drop=True) #restricting data to 600,000 records due to memory issue\n\ntest_data = test\ntrain_data = train_cat\ntrain_data_o  = train_cat\n\nmaxlen = 192","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_data))\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head() ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Checking for data imbalance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_o['toxic'].value_counts() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nsns.countplot(train_data_o['toxic'])\nplt.title('Target on training data')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Making Class Balance (Toxic and Non-toxic)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Class count\ncount_toxic_0, count_toxic_1 = train_data_o.toxic.value_counts()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Divide by class for train data\ndf_toxic_0 = train_data_o[train_data_o['toxic'] == 0]\ndf_toxic_1 = train_data_o[train_data_o['toxic'] == 1] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \ndf_toxic_0_under = df_toxic_0.sample(count_toxic_1)\ntrain_data_df = pd.concat([df_toxic_0_under, df_toxic_1], axis=0)\n\nprint('Random under-sampling:')\nprint(train_data_df.toxic.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bar graph of Balanced Class**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nsns.countplot(train_data_df['toxic'])\nplt.title('Target on training data')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of words\ntrain_data['num_words'] = train_data['comment_text'].apply(lambda x: len(str(x).split()))\n#df_test['num_words'] = df_test['question_text'].apply(lambda x: len(str(x).split()))\n\n# Number of capital_letters\ntrain_data['num_capital_let'] = train_data['comment_text'].apply(lambda x: len([c for c in str(x) if c.isupper()]))\n#df_test['num_capital_let'] = df_test['question_text'].apply(lambda x: len([c for c in str(x) if c.isupper()]))\n\n# Number of special characters\ntrain_data['num_special_char'] = train_data['comment_text'].str.findall(r'[^a-zA-Z0-9 ]').str.len()\n#df_test['num_special_char'] = df_test['question_text'].str.findall(r'[^a-zA-Z0-9 ]').str.len()\n\n# Number of unique words\ntrain_data['num_unique_words'] = train_data['comment_text'].apply(lambda x: len(set(str(x).split())))\n#df_test['num_unique_words'] = df_test['question_text'].apply(lambda x: len(set(str(x).split())))\n\n# Number of numerics\ntrain_data['num_numerics'] = train_data['comment_text'].apply(lambda x: sum(c.isdigit() for c in x))\n#df_test['num_numerics'] = df_test['question_text'].apply(lambda x: sum(c.isdigit() for c in x))\n\n# Number of characters\ntrain_data['num_char'] = train_data['comment_text'].apply(lambda x: len(str(x)))\n#df_test['num_char'] = df_test['question_text'].apply(lambda x: len(str(x)))\n\n# Number of stopwords\ntrain_data['num_stopwords'] = train_data['comment_text'].apply(lambda x: len([c for c in str(x).lower().split() if c in STOPWORDS]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_boxplot(_x, _y, _data, _title):\n    sns.boxplot(x=_x, y=_y, data=_data)\n    plt.grid(True)\n    #plt.tick_params(axis='x', which='major', labelsize=15)\n    plt.title(_title,fontsize=17)\n    plt.xlabel(_x, fontsize=10)\n\n# Boxplot: Number of words\nplt.subplot(2, 3, 1)\ndisplay_boxplot('toxic', 'num_words', train_data, 'No. of words in each class')\n\n# Boxplot: Number of chars\nplt.subplot(2, 3, 2)\ndisplay_boxplot('toxic', 'num_char', train_data, 'Number of characters in each class')\n\n# Boxplot: Number of unique words\nplt.subplot(2, 3, 3)\ndisplay_boxplot('toxic', 'num_unique_words', train_data, 'Number of unique words in each class')\n\n# Boxplot: Number of special characters\nplt.subplot(2, 3, 4)\ndisplay_boxplot('toxic', 'num_special_char', train_data, 'No. of special characters in each class')\n\n# Boxplot: Number of stopwords\nplt.subplot(2, 3, 5)\ndisplay_boxplot('toxic', 'num_stopwords', train_data, 'Number of stopwords in each class')\n\n# Boxplot: Number of capital letters\nplt.subplot(2, 3, 6)\ndisplay_boxplot('toxic', 'num_capital_let', train_data, 'No. of capital letters in each class')\n\n\nplt.subplots_adjust(right=3.0)\nplt.subplots_adjust(top=2.0)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA and Data visualization analysis"},{"metadata":{},"cell_type":"markdown","source":"**Wordcloud of all comments**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def nonan(x):\n    if type(x) == str:\n        return x.replace(\"\\n\", \"\")\n    else:\n        return \"\"\n\ntext = ' '.join([nonan(abstract) for abstract in train_data[\"comment_text\"]])\nwordcloud = WordCloud(max_font_size=None, background_color='black', collocations=False,\n                      width=1200, height=1000).generate(text)\nfig = px.imshow(wordcloud)\nfig.update_layout(title_text='Common words in comments')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def new_len(x):\n    if type(x) is str:\n        return len(x.split())\n    else:\n        return 0\n\ntrain_data[\"comment_words\"] = train_data[\"comment_text\"].apply(new_len)\nnums = train_data.query(\"comment_words != 0 and comment_words < 200\").sample(frac=0.1)[\"comment_words\"]\nfig = ff.create_distplot(hist_data=[nums],\n                         group_labels=[\"All comments\"],\n                         colors=[\"coral\"])\n\nfig.update_layout(title_text=\"Comment words\", xaxis_title=\"Comment words\", template=\"simple_white\", showlegend=False)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Negative sentiment**"},{"metadata":{"trusted":true},"cell_type":"code","source":"def polarity(x):\n    if type(x) == str:\n        return SIA.polarity_scores(x)\n    else:\n        return 1000\n    \nSIA = SentimentIntensityAnalyzer()\ntrain_data[\"polarity\"] = train_data[\"comment_text\"].progress_apply(polarity)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(go.Histogram(x=[pols[\"neg\"] for pols in train_data[\"polarity\"] if pols[\"neg\"] != 0], marker=dict(\n            color='seagreen')\n    ))\n\nfig.update_layout(xaxis_title=\"Negativity sentiment\", title_text=\"Negativity sentiment\", template=\"simple_white\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Negativity vs. Toxicity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"negativity\"] = train_data[\"polarity\"].apply(lambda x: x[\"neg\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nums_1 = train_data.sample(frac=0.1).query(\"toxic == 1\")[\"negativity\"]\nnums_2 = train_data.sample(frac=0.1).query(\"toxic == 0\")[\"negativity\"]\n\nfig = ff.create_distplot(hist_data=[nums_1, nums_2],\n                         group_labels=[\"Toxic\", \"Non-toxic\"],\n                         colors=[\"darkorange\", \"dodgerblue\"], show_hist=False)\n\nfig.update_layout(title_text=\"Negativity vs. Toxicity\", xaxis_title=\"Negativity\", template=\"simple_white\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Positive Sentiment**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(go.Histogram(x=[pols[\"pos\"] for pols in train_data[\"polarity\"] if pols[\"pos\"] != 0], marker=dict(\n            color='indianred')\n    ))\n\nfig.update_layout(xaxis_title=\"Positivity sentiment\", title_text=\"Positivity sentiment\", template=\"simple_white\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Positivity vs Toxicity**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"positivity\"] = train_data[\"polarity\"].apply(lambda x: x[\"pos\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nums_1 = train_data.sample(frac=0.1).query(\"toxic == 1\")[\"positivity\"]\nnums_2 = train_data.sample(frac=0.1).query(\"toxic == 0\")[\"positivity\"]\n\nfig = ff.create_distplot(hist_data=[nums_1, nums_2],\n                         group_labels=[\"Toxic\", \"Non-toxic\"],\n                         colors=[\"darkorange\", \"dodgerblue\"], show_hist=False)\n\nfig.update_layout(title_text=\"Positivity vs. Toxicity\", xaxis_title=\"Positivity\", template=\"simple_white\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Readability "},{"metadata":{},"cell_type":"markdown","source":"**Flesch reading ease**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"flesch_reading_ease\"] = train_data[\"comment_text\"].progress_apply(textstat.flesch_reading_ease)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(go.Histogram(x=train_data.query(\"flesch_reading_ease > 0\")[\"flesch_reading_ease\"], marker=dict(\n            color='darkorange')\n    ))\n\nfig.update_layout(xaxis_title=\"Flesch reading ease\", title_text=\"Flesch reading ease\", template=\"simple_white\")\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Non-toxic vs. Toxic**"},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_mask=np.array(Image.open(\"../input/imagesforkernal/safe-zone.png\"))\nclean_mask=clean_mask[:,:,1]\n\nsubset = train_data.query(\"toxic == 0\")\ntext = subset.comment_text.values\nwc = WordCloud(background_color=\"black\",max_words=2000,mask=clean_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(7.5, 7.5))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Clean Comments\", fontsize=16)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()\n\nclean_mask=np.array(Image.open(\"../input/imagesforkernal/swords.png\"))\nclean_mask=clean_mask[:,:,1]\n\nsubset = train_data.query(\"toxic == 1\")\ntext = subset.comment_text.values\nwc = WordCloud(background_color=\"black\",max_words=2000,mask=clean_mask,stopwords=stopword)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(7.5, 7.5))\nplt.axis(\"off\")\nplt.title(\"Words frequented in Toxic Comments\", fontsize=16)\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17), alpha=0.98)\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}