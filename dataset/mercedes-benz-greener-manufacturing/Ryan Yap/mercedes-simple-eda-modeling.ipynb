{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Outline of This Notebook\n1. ### EDA - Exploratory Data Analysis\n    1. [Quick Peak](#Quick-Peak)\n    1. [Quantitative and Qualitative Data](#Quantitative-and-Qualitative-Data)\n    1. [Visualization on Binary Data](#Visualization-on-Binary-Data)\n    1. [Missing Values](#Missing-Values)\n    \n1. ### Data Processing\n    1. [Normalization of Dependant Variable - \"y\"](#Normalization-of-Dependant-Variable---\"y\")\n    1. [Removal of One Unique Values](#Removal-of-One-Unique-Values)\n    1. [Eliminating Biased Feature](#Eliminating-Biased-Feature)\n    1. [Split The Data Back into Train and Test Dataset](#Split-The-Data-Back-into-Train-and-Test-Dataset)\n    \n1. ### Model Building\n    1. [Train_test_split](#Train_test_split)\n    1. [GridSearchCV Best Parameters for Below Models](#GridSearchCV-Best-Parameters-for-Below-Models)\n    1. [Evaluation the Optimized Estimators](Evaluation-the-Optimized-Estimators)\n    1. [Visualization of Estimator's CV score](Visualization-of-Estimator's-CV-score)\n    \n    \n## [Submission](#Submission)","metadata":{}},{"cell_type":"markdown","source":"### Quick Peak","metadata":{}},{"cell_type":"code","source":"## import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport warnings\nwarnings.filterwarnings(\"ignore\") ## ignore warnings","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## import train dataset\ndf_train = pd.read_csv(\"../input/mercedes-benz-greener-manufacturing/train.csv.zip\")\n\n## Quick peak on train dataset\nprint(\"Shape of Train Dataset : {}\".format(df_train.shape))\ndf_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## import test dataset\ndf_test = pd.read_csv(\"../input/mercedes-benz-greener-manufacturing/test.csv.zip\")\n\n## Quick peak on test dataset\nprint(\"Shape of Test Dataset : {}\".format(df_test.shape))\ndf_test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Drop ID column\ndf_train = df_train.drop(\"ID\",axis=1)\ndf_test = df_test.drop(\"ID\",axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Quantitative and Qualitative Data","metadata":{}},{"cell_type":"code","source":"## Quantitative data\nquantitative_data = [feat for feat in df_train if df_train[feat].dtype != np.object]\n\nprint(\"A total of {} quantitative columns\".format(len(quantitative_data)))\nprint(quantitative_data)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Two types of Quantitative data:\n\n1. Continuous data: Numerical data that can take on any values. A great example would be the the height of a person. Donald is 6 foot or 182.88cm tall.\n\n1. Discrete data: Numerical data that has specific values. A great example would be the number of dogs. The number of dogs are counted as 1 dog, 2 dogs, 3 dogs. There is no such thing as 0.5 dog.\n","metadata":{}},{"cell_type":"code","source":"## Continuous data\ncontinuous_feats = [feat for feat in quantitative_data if df_train[feat].nunique() > 25]\n\ncontinuous_feats","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our targer variable is a continuous data.","metadata":{}},{"cell_type":"code","source":"## Discrete data\ndiscrete_feats = [feat for feat in quantitative_data if df_train[feat].nunique() <= 25]\n\ndiscrete_feats","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Binary data****: Numerical data or categorical data that has two unique values represents one of two conceptually opposed values. Eg. \"Success\" or \"Failure\" (in categorical data), \"1\" or \"0\" (in numerical data).\n\n****One Unique Value Column****: Any column in the dataset that contain only the same value. One unique value column will not help to your model to differentiate between two different labels instead it can even negatively affect your model by creating bias in the data.\n\n\n\nDetermine whether the discrete features consists of binary data or one unique value column","metadata":{}},{"cell_type":"code","source":"## Binary Data\nbinary_feats = [feat for feat in discrete_feats if df_train[feat].nunique() == 2]\n\n## One_unique_feats\none_uni_feats = [feat for feat in discrete_feats if df_train[feat].nunique() == 1]\n\nprint(\"Number of Binary column: {}\".format(len(binary_feats)))\nprint(\"Number of One unique column: {}\".format(len(one_uni_feats)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization on Binary Data","metadata":{}},{"cell_type":"code","source":"zero_value = []\none_value = []\n\nfor col in binary_feats:\n    zero_value.append(df_train[col].value_counts(normalize=True).loc[0])\n    one_value.append(df_train[col].value_counts(normalize=True).loc[1])\n\ncount = len(binary_feats)\nidx = np.arange(count)\nbar_width = 0.5\n\nplt.figure(figsize=(6,125))\np1 = plt.barh(idx,zero_value,bar_width,color=\"red\")\np2 = plt.barh(idx,one_value,bar_width,left=zero_value, color=\"black\")\nplt.yticks(idx,binary_feats)\nplt.legend((p1[0], p2[0]), ('Zero count', 'One Count'))\nplt.show()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Qualitative Data\nqualitative_data = [feat for feat in df_train if df_train[feat].dtype == np.object]\nprint(\"A total of {} qualitative columns\".format(len(qualitative_data)))\nprint(qualitative_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's a total of 8 qualitative and 368 quantitative features in this dataset","metadata":{}},{"cell_type":"code","source":"## Visualization of qualitative columns\nfor col in qualitative_data:\n    fig, ax = plt.subplots(1,1,figsize=(16,5))\n    sns.boxplot(df_train[col],df_train[\"y\"],ax=ax)\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n    plt.show()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values ","metadata":{}},{"cell_type":"code","source":"## Assign to Dataframe\ndf_check = pd.DataFrame()\ndf_check[\"Missing Values\"] = df_train.isnull().sum()\ndf_check[\"Number of Unique Values\"] = df_train.nunique()\n\n#Have a look on the dataframe\nprint(\"Presence of missing values = {}\".format(any(df_check[\"Missing Values\"] > 0)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Extract the dependant variable \"y\" out and drop it from the quantitative features.","metadata":{}},{"cell_type":"code","source":"## extraction of dependant variable\ny = df_train[\"y\"]\n\n## drop\ndf_train = df_train.drop(\"y\",axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Processing","metadata":{}},{"cell_type":"code","source":"# Create a new column \"train\" for easier for separate the data later\ndf_train[\"train\"] = 1 # 1 indicates train data)\ndf_test[\"train\"] = 0 # 0 indicates test data)\n# Combination of both test and train\ndf_all = pd.concat([df_train,df_test],axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Normalization of Dependant Variable - \"y\"","metadata":{}},{"cell_type":"code","source":"## Let's have a look on \"y\" normal distribution\nplt.figure(figsize=(12,9))\nsns.distplot(y)\nplt.title(\"Before Normalization\")\nplt.show()\nprint(\"Skewness: {}\".format(y.skew()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Normalize dependant variable\ny_norm = np.log1p(y)\n\n## Visualization\nplt.figure(figsize=(12,9))\nsns.distplot(y_norm)\nplt.title(\"After Normalization\")\nplt.show()\nprint(\"Skewness: {}\".format(y.skew()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a = [f for f in df_all if df_all[f].nunique() == 1]\n# print(a)\n# a = []\n# for f in df_all:\n#     if df_all[f].nunique() == 1:\n#         a.append(f)\n# print(a)\n# df_all[\"X1\"].nunique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Removal of One Unique Values\n","metadata":{}},{"cell_type":"code","source":"## Shape of dataset before the removal of one unique feats\nprint(\"Shape of the whole dataset before the removal: {}\".format(df_all.shape))\n\n## Removal of one unique feats\none_uni_feats = [feat for feat in df_all.columns if df_all[feat].nunique() == 1]\ndf_all = df_all.drop(one_uni_feats,axis=1)\n\n## Shape of dataset after the removal of one unique feats\nprint(\"Shape of the whole dataset after the removal: {}\".format(df_all.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Eliminating Biased Feature\nFeature with biased data would negatively affect a model.","metadata":{}},{"cell_type":"code","source":"threshold = 0.9 ## A unique value that consist more 90% of the feature is consider as biased\n\n## Indentify biased features \nbias_feat = []\nfor col in df_all:\n    feat = df_all[col].value_counts(normalize=True,dropna=False).values[0] > 0.9\n    if feat == True:\n        bias_feat.append(col)\n\n## Remove them\ndf_all.drop(bias_feat,axis=1)\n\n## Let's check on the shape of the whole dataset\nprint(\"Shape: {}\".format(df_all.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label Encode Qualitative Data\n","metadata":{}},{"cell_type":"code","source":"##Label encode qualitative data\nfrom sklearn.preprocessing import LabelEncoder #Library for LabelEncoding\nlbl_encoder = LabelEncoder()\n\nfor col in qualitative_data:\n    encoded = lbl_encoder.fit_transform(df_all[col])\n    df_all[col] = encoded","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split The Data Back into Train and Test Dataset","metadata":{}},{"cell_type":"code","source":"# df_train data\ndf_train = df_all.loc[df_all[\"train\"]== 1,:]\ndf_train.drop(\"train\",axis=1,inplace=True)\n\n# df_test data\ndf_test = df_all.loc[df_all[\"train\"]== 0,:] \ndf_test.drop(\"train\",axis=1,inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building\n### Train_test_split\nThe dataset are split into X_train,X_test, y_train, y_test","metadata":{}},{"cell_type":"code","source":"## import necessary package\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n## Split the data into train and test set\nX_train, X_test, y_train, y_test =  train_test_split(df_train,y,test_size=0.33,random_state=42)\n\n\n## Check on the dataset shape\nprint(\"Shapes: \", X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GridSearchCV Best Parameters for Below Models\nSearching for the optimal predefined hyperparameters for each individual estimator.\n1. XGBoost Regressor\n1. ExtraTreeRegressor\n1. GradientBoostingRegressor","metadata":{}},{"cell_type":"code","source":"## import models\nfrom sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Create a list for store each individual best estimator\nbest_estimators = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### XGBRegressor","metadata":{}},{"cell_type":"code","source":"## parameters\nparams = {\n    \"learning_rate\": [.1, .5],\n    \"colsample_bytree\": [.3,.5,],\n    \"max_depth\": [2, 4],\n    \"alpha\": [3, 5],\n    \"subsample\": [.5],\n    \"n_estimators\": [30, 70],\n    \"random_state\" : [42]\n}\n\n## XGBoost Regressor\nXGBR =  XGBRegressor()\nXGBR_grid = GridSearchCV(XGBR, params, scoring='r2', cv=7, n_jobs=-1)\nXGBR_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(XGBR_grid.best_params_))\nprint(\"Best score: {}\".format(XGBR_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"XGBoostR\",XGBR_grid.best_estimator_])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### ExtraTreesRegressor","metadata":{}},{"cell_type":"code","source":"## Parameters\nparams = {\n    \"max_depth\": [\"None\",10],\n    \"max_features\": [\"auto\",.3, .4],\n    \"min_samples_leaf\": [3,7],\n    \"min_samples_split\": [2, 4],\n    \"n_estimators\": [50, 100],\n    \"random_state\" : [42]\n}\n\n## ExtraTreesRegressor\nETR = ExtraTreesRegressor()\nETR_grid = GridSearchCV(ETR, params, scoring='r2', cv=7, n_jobs=-1)\nETR_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(ETR_grid.best_params_))\nprint(\"Best score: {}\".format(ETR_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"ExtraTreesR\",ETR_grid.best_estimator_])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### GradientBoostingRegressor","metadata":{}},{"cell_type":"code","source":"## Parameters\nparams = {\n    \"max_depth\": [2, 3],\n    \"max_features\": [\"auto\",0.3],\n    \"min_samples_leaf\": [1,3],\n    \"min_samples_split\": [2, 5],\n    \"n_estimators\": [50, 100],\n    \"random_state\" : [42],\n    \"tol\" : [0.0001,0.01]\n}\n\n## GradientBoostingRegressor\nGBR = GradientBoostingRegressor()\nGBR_grid = GridSearchCV(GBR, params, scoring='r2', cv=7, n_jobs=-1)\nGBR_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(GBR_grid.best_params_))\nprint(\"Best score: {}\".format(GBR_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"GradientBoostR\",GBR_grid.best_estimator_])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation the Optimized Estimators","metadata":{}},{"cell_type":"code","source":"## import necessary libraries for evaluation\nfrom sklearn.model_selection import cross_val_score,KFold\n\n## create an empty dataframe to store estimator's cross_validation_score\nevaluate = pd.DataFrame(columns=[\"model\",\"std\",\"cv_mean\",\"cv_median\"])#note: \"cv\" for mean cv & \"cv_all\" for later visualization use\n\nfor name,estimator in best_estimators:\n    kfold = KFold(n_splits=10,random_state=42,shuffle=True)\n    cv = cross_val_score(estimator,X_train,y_train,cv=kfold,n_jobs=-1,scoring=\"r2\")\n    \n    row = evaluate.shape[0]\n    evaluate.loc[row,\"model\"] = name\n    evaluate.loc[row,\"cv_mean\"] = round(cv.mean(),3)\n    evaluate.loc[row,\"cv_median\"] = round(np.median(cv),3)\n    evaluate.loc[row,\"std\"] = \"+/- {}\".format(round(cv.std(),4))\n    \n    evaluate = evaluate.sort_values(\"cv_mean\",ascending=False)\n\nevaluate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualization of Estimator's CV score\nVisualize the each estimator mean in bar charts","metadata":{}},{"cell_type":"code","source":"## Visualization\nfig,ax = plt.subplots(1,1,figsize=(12,9))\nbar = sns.barplot(evaluate[\"model\"],evaluate[\"cv_mean\"],ax=ax,palette=sns.color_palette(\"rocket\"))\nfor rec in bar.patches:\n    height = rec.get_height()\n    ax.text(rec.get_x() + rec.get_width()/2, height *1.02, height,ha=\"center\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We should be choosing the XGBoostRegressor estimator. Now let's train estimator with the whole training dataset and make prediction.","metadata":{}},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"## Optimal XGBRegressor Estimator\nXBGR = XGBR_grid.best_estimator_\nXBGR.fit(df_train,y)\n\n## Submission\n\nsubmission = pd.read_csv(\"../input/mercedes-benz-greener-manufacturing/sample_submission.csv.zip\")\nsubmission.iloc[:,1] = XBGR.predict(df_test)\n\nsubmission.to_csv('submission', index=False)\n\n## Print out the first five row of the submission\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}