{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\nI am not a data scientist but an engineer/researcher in the manufacturing domain. I work with the data here and reflect myself from a production engineering perspective. Any comments or advice are welcome!","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, cross_val_score, cross_validate\nfrom xgboost import XGBRegressor, plot_importance\nfrom statistics import mean\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/mercedes-benz-greener-manufacturing/train.csv.zip')\ntest = pd.read_csv('../input/mercedes-benz-greener-manufacturing/test.csv.zip')\nprint(data.shape, test.shape)\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Exploring the dataset \n### 1.1 Basic checkup on & Cleaning","metadata":{}},{"cell_type":"code","source":"print(data.dtypes.value_counts())\ndata.isnull().sum().sort_values(ascending=False) # no missing value!\ntest.isnull().sum().sort_values(ascending=False) # no missing value for test data as well!\n\n# cardinality=1 columns: 12 columns in data and 5 columns in test\ndata_one_cardinality_columns = [column for column in data.columns if data[column].nunique()==1]\ntest_one_cardinality_columns = [column for column in test.columns if test[column].nunique()==1]\n\none_cardinality_columns = data_one_cardinality_columns + test_one_cardinality_columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I could drop these one cardinality columns but I do not know the data well yet. So I will consider it later.","metadata":{}},{"cell_type":"code","source":"sns.boxplot(data.y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Takt time for car assembly lines are often around 60 sec but not sure for the premium cars like Mercedes. If this y values reflect the reality, the tak time may be longer than 60 sec and there may be two or more test stations at the end of the line. Anyhow, one data point over 250 sec seems very strange. I can remove this data point.  ","metadata":{}},{"cell_type":"code","source":"data_o = data[(data['y'] <= 200)]\nsns.boxplot(data_o.y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Facets overview from Google. I like this one for inital data exploration.","metadata":{}},{"cell_type":"code","source":"!pip install facets-overview","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Create the feature stats for the datasets and stringify it.\nimport base64\nfrom facets_overview.generic_feature_statistics_generator import GenericFeatureStatisticsGenerator\n\ngfsg = GenericFeatureStatisticsGenerator()\nproto = gfsg.ProtoFromDataFrames([{'name': 'train', 'table': data}])\nprotostr = base64.b64encode(proto.SerializeToString()).decode(\"utf-8\")\n\n### Display the facets overview visualization for this data\nfrom IPython.core.display import display, HTML\n\nHTML_TEMPLATE = \"\"\"\n        <script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script>\n        <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\" >\n        <facets-overview id=\"elem\"></facets-overview>\n        <script>\n          document.querySelector(\"#elem\").protoInput = \"{protostr}\";\n        </script>\"\"\"\nhtml = HTML_TEMPLATE.format(protostr=protostr)\ndisplay(HTML(html))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Some reflection on the data\n* I understand they are list of car features. Eight categorical and the rest is binary. \n* No particular outliears and such things due to this. Clean and simple dataset.\n* Since the input data is car features, the quality of the data should be always good. Otherwise they can not build cars! \n* Many features have low variance e.g. 99.8% is 0. Not sure they will contribute to the model training.\n* The description of the dataset says \"dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing\". This is a bit difficult for me to understand. Does the permutation matter for the test time or is it combination of features instead? The latter makes more sense to me but maybe I misunderstand something....","metadata":{}},{"cell_type":"markdown","source":"## 2. Data preprocessing","metadata":{}},{"cell_type":"code","source":"y = data_o.y\nX = data_o.drop(['ID','y'], axis=1)\nX_submission = test.drop('ID', axis=1)\n\n# Concatenate X and X_submission before applying auto OneHotEncoder\nX_con = pd.concat([X, X_submission], axis=0)\n\n# Apply auto OneHotEncoder \nX_con_ohe = pd.get_dummies(X_con)\n\n# Now deviding back to train and test data \nX_ohe = X_con_ohe[:len(X)]\nX_submission_ohe = X_con_ohe[len(X):]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Run some estimaters as baselines\n* Most of the manufcaturing related relational data work very well with ensemble trees. I have not seen so far other models such as liner regression or neural network have beaten them... \n* R2 is used for socoring but mae is more informative in this test bench case. So I use mae as well.\n\n### 3.1 XGboost","metadata":{}},{"cell_type":"code","source":"xgb= XGBRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n\nfit_params = {\"early_stopping_rounds\": 5, \"eval_set\": [(X_ohe, y)]}\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nxgb_scores = cross_validate(xgb, X_ohe, y, scoring=['neg_mean_absolute_error','r2'], cv=cv, n_jobs=-1, \n                            verbose=1, fit_params=fit_params, return_estimator=True)\n\nprint('mae:',abs(xgb_scores['test_neg_mean_absolute_error'].mean()))\nprint('r2:',xgb_scores['test_r2'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(max_depth=4, n_estimators=5, random_state=42)\n# evaluate model\ncv = KFold(n_splits=10, shuffle=True, random_state=42)\nrf_scores = cross_validate(rf, X_ohe, y, scoring=['neg_mean_absolute_error','r2'], cv=cv, n_jobs=-1, return_estimator=True)\n\nrf_base_mae = abs(rf_scores['test_neg_mean_absolute_error'].mean())\nrf_base_r2 = rf_scores['test_r2'].mean()\n\nprint('mae:',rf_base_mae)\nprint('r2:',rf_base_r2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 XGboost HavlingRandamizedSeaerchCV\nThis is not necessary but I just wanted to experiement how this turns out.","metadata":{}},{"cell_type":"code","source":"from sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingRandomSearchCV\nfrom scipy.stats import uniform, randint\n\nxgb_model = XGBRegressor(n_estimators=200)   \n\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"subsample\": uniform(0.6, 0.4)}\n\nfit_params = {\n    \"early_stopping_rounds\": 5,\n    \"eval_set\": [(X_ohe, y)]}\n\nxgb_halvsearch = HalvingRandomSearchCV(xgb_model,                   \n                param_distributions=params, resource='n_estimators', \n                max_resources=40, random_state=42, cv=5, scoring='r2',\n                verbose=1, n_jobs=-1, return_train_score=True)\n\nxgb_halvsearch.fit(X_ohe, y, **fit_params)\nprint('best r2 score:',xgb_halvsearch.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.4 Reflection on running some estimators\n* Random Forest is quick and generates ok result. So I will use this further.\n* 5 sec deviation from the target on average is probably ok, condering the current mean and variation of the target.\n* A few 0.01 points increase in r2 only affects a few 0.1 seconds improvement for the accuracy (mae). This is very little considering that the average test cycles is around 100 sec, and such small time reduction is easily diminished by other factors in production.\n* So it makes less sense in spending hours to improve the scores...","metadata":{}},{"cell_type":"markdown","source":"## 4. Explore with SHAP","metadata":{}},{"cell_type":"code","source":"import shap\n\nestimater = rf_scores['estimator'][1]   # I use the one generating the score close to the average....\n\nexplainer = shap.explainers.Tree(estimater)\nshap_values = explainer(X_ohe)\n\nshap.summary_plot(shap_values, X_ohe)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So several features, especialy when they are true, contributes to the inference.","metadata":{}},{"cell_type":"code","source":"# Check mean shap value for each column\nshap_mean = np.abs(shap_values.values).mean(axis=0)\nshap_mean_columns = pd.Series(shap_mean, index=X_ohe.columns)\n#shap_mean_columns.value_counts().sort_index()\nshap_mean_columns.sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So there are quite many features having very low shap values. It may be a good idea to remove them.","metadata":{}},{"cell_type":"markdown","source":"## 5. Feature selection\n### 5.1 Feature selection based on mean SHAP values","metadata":{}},{"cell_type":"code","source":"# creat column list that has mean shap is higher 0.01 =>about 31 columns\nshap_incl_columns = shap_mean_columns[shap_mean_columns.values>0.01].index.to_list()\nprint(shap_incl_columns)\nX_ohe_fs = X_ohe[shap_incl_columns]\nX_ohe_fs.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Recursive Feature Elimination and Cross-Validated selection (RFECV)","metadata":{}},{"cell_type":"code","source":"# I borrowed the code from Dmitriy K. thanks!\nfrom sklearn.feature_selection import RFECV\n\nselector = RFECV(estimater, step = 1, cv=5, n_jobs=-1,verbose=1, scoring='r2')\nselector.fit(X_ohe_fs, y)\n\nprint(selector.grid_scores_)\n\nrfecv_features = [f for f, s in zip(X_ohe_fs, selector.support_) if s]\nprint('selected features:', rfecv_features)\n\nX_ohe_rfecv = X_ohe[rfecv_features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.3 Compare results after the feature selection","metadata":{}},{"cell_type":"code","source":"rf = RandomForestRegressor(max_depth=4, n_estimators=5, random_state=42)\ncv = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# Score with all features\nprint('mae_all_data:',rf_base_mae)\nprint('r2_all_data:',rf_base_r2)\n\n# Score with features with mean shap value >0.01 (31 columns)\nrf_scores1 = cross_validate(rf, X_ohe_fs, y, scoring=['neg_mean_absolute_error','r2'], cv=cv, n_jobs=-1, return_estimator=True)\nprint('mae_shap_features:', abs(rf_scores1['test_neg_mean_absolute_error'].mean()))\nprint('r2_shap_features:', rf_scores1['test_r2'].mean())\n\n# Score with RFECV features (5 columns)\nrf_scores2 = cross_validate(rf, X_ohe_rfecv, y, scoring=['neg_mean_absolute_error','r2'], cv=cv, n_jobs=-1, return_estimator=True)\nprint('mae_rfecv_features:', abs(rf_scores2['test_neg_mean_absolute_error'].mean()))\nprint('r2_rfecv_features:', rf_scores2['test_r2'].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.4 Reflection on feature selection\n* Reduced features show a slightly better results.\n* Accuracy wise, Shap_features or Rfecv_features does not make difference.\n* I use shap features here. If the features are those requiring maintenace for instance sensor values, then I would use fewer features. But this case the data should be very stable...\n* Inference time of the trained model is short enough, since the car features should be decided before the actual manufacturing and takt time is much longer than that time.","metadata":{}},{"cell_type":"markdown","source":"## 6. Use RF with shap features for the final model\n### 6.1 Map the difference between the target and predicted\nI learned from experience that just looking at aggregated statistical data such as means can be risky especially in manufacturing. It is good the check how the inference looks like against each target.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfinal_model = RandomForestRegressor(max_depth=4, n_estimators=5, random_state=42)\nfinal_model.fit(X_ohe_fs,y)\n\n# make prediction and calcurate the difference betweeen y and prediction on each dataset\nX_train, X_test, y_train, y_test = train_test_split(X_ohe_fs, y,shuffle=False, test_size=0.25)\npredicted_y = pd.Series(final_model.predict(X_test))\npredicted_y.index = X_test.index\ndif = abs(predicted_y-y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DataFrame with columns y, predicted_y, difference\ncompare = y_test.to_frame().join(predicted_y.to_frame(name='predicted_y'))\ncompare = compare.join(dif.to_frame(name='abs_dif'))\n\n# Not necessarily lineplot but I can see the difference easily.\nsns.set_theme(context='notebook', style='darkgrid')\nplt.figure(figsize=(24, 6.5))\nsns.lineplot(data=compare.iloc[250:350,0:2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the difference\nplt.figure(figsize=(24, 6.5))\nsns.lineplot(data=compare.iloc[250:350,2]) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(data=compare, x=y_test)\nsns.displot(data=compare, x=predicted_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6.2 Reflection\n * The final model generally follows well to the target but does not predict well for the longer test cycle. But the purpose is to reduce the test cycle time. So this discrepancy is not so important either.\n * Distribution plot of the predicted values shows that the model traces the characteristic of the target distribution. ","metadata":{}},{"cell_type":"markdown","source":"## 7. submission","metadata":{}},{"cell_type":"code","source":"X_submission_ohe_fs = X_submission_ohe[shap_incl_columns]\n\npredict = final_model.predict(X_submission_ohe_fs)\nsubmission = pd.DataFrame({'ID': test.ID, 'y': predict})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}