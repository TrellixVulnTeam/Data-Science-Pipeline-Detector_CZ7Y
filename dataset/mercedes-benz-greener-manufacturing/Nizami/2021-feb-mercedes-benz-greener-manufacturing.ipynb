{"cells":[{"metadata":{},"cell_type":"markdown","source":"Dear readers, this is latest version of Mercedec Benz Greener Manufacturing Notebook. You can run the below codes on Python, without changin anything. I copied this notebook from [Podsyp](http://www.kaggle.com/podsyp/mercedes-benz-greener-manufacturing). It is very well written and easy to understand. I'll try to touch some points as well. The result is 0.55259"},{"metadata":{},"cell_type":"markdown","source":"# 0. Starts"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Import"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_summary as ps\n\n# Data processing, metrics and modeling\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold, train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LassoCV, RidgeCV, HuberRegressor, ElasticNetCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\n\n\nfrom scipy.stats import norm\n\n# Lgbm\nimport lightgbm as lgb\n\n# Support warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since there are many number of features, we have to extend it."},{"metadata":{},"cell_type":"markdown","source":"# 2. Options"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\n\nfolder = '/kaggle/input/mercedes-benz-greener-manufacturing/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Read CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(folder + 'train.csv.zip')\ntest_df = pd.read_csv(folder + 'test.csv.zip')\nsub_df = pd.read_csv(folder + 'sample_submission.csv.zip')\n\nprint('train_df: ', train_df.shape)\nprint('test_df: ', test_df.shape)\nprint('sub_df: ', sub_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to see which variables are categorical and which ones are numerical, later we will transfer categorical variables to numerical."},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs_train = ps.DataFrameSummary(train_df)\nprint('categoricals: ', dfs_train.categoricals.tolist())\nprint('numerics: ', dfs_train.numerics.tolist())\ndfs_train.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_cols = dfs_train.categoricals.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfs_test = ps.DataFrameSummary(test_df)\nprint('categoricals: ', dfs_test.categoricals.tolist())\nprint('numerics: ', dfs_test.numerics.tolist())\ndfs_test.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Get target"},{"metadata":{"trusted":true},"cell_type":"code","source":"ps.DataFrameSummary(train_df[['y']]).summary().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.distplot(train_df['y'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['y'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Let's logarithm the value of Y"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['y'] = np.log(train_df['y'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ps.DataFrameSummary(train_df[['y']]).summary().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.distplot(train_df['y'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['y'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Drop outliers by percentile"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[(train_df['y'] > np.percentile(train_df['y'], 0.5)) & (train_df['y'] < np.percentile(train_df['y'], 99.5))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.distplot(train_df['y'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['y'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['y']\ntrain_df.drop(['y'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# VERY BAD So, as we can see the distribution is not normal.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have some features (X0-X8) which are categorical, we need to trasnfer them into numerical using Mean Target Encoding. As we see our target feature is not a dummy variable, so we can not use dummy option. For more information about MTE read this [link](http://medium.com/@shailypa/target-encoding-cd3e9c14fcc)"},{"metadata":{},"cell_type":"markdown","source":"## 6. Make mean target encoding for categorical feature\n\nLet us consider the above table (A simple binary classification). \n\n$$ MeanTargetEnc_i = {((GlobalMean * C) + (Mean_i * Size)) \\over (C + Size)} $$\n\nInstead of finding the mean of the targets, we can also focus on median and other statistical correlationsâ€¦.These are broadly called target encodings"},{"metadata":{"trusted":true},"cell_type":"code","source":"class MeanEncoding(BaseEstimator):\n    \"\"\"   In Mean Encoding we take the number \n    of labels into account along with the target variable \n    to encode the labels into machine comprehensible values    \"\"\"\n    \n    def __init__(self, feature, C=0.1):\n        self.C = C\n        self.feature = feature\n        \n    def fit(self, X_train, y_train):\n        \n        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n        \n        self.global_mean = df.target.mean()\n        mean = df.groupby('feature').target.mean()\n        size = df.groupby('feature').target.size()\n        \n        self.encoding = (self.global_mean * self.C + mean * size) / (self.C + size)\n    \n    def transform(self, X_test):\n        \n        X_test[self.feature] = X_test[self.feature].map(self.encoding).fillna(self.global_mean).values\n        \n        return X_test\n    \n    def fit_transform(self, X_train, y_train):\n        \n        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n        \n        self.global_mean = df.target.mean()\n        mean = df.groupby('feature').target.mean()\n        size = df.groupby('feature').target.size()\n        self.encoding = (self.global_mean * self.C + mean * size) / (self.C + size)\n        \n        X_train[self.feature] = X_train[self.feature].map(self.encoding).fillna(self.global_mean).values\n        \n        return X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in cat_cols:\n    me = MeanEncoding(f, C=0.99)\n    me.fit(train_df, y)\n    train_df = me.transform(train_df)\n    test_df = me.transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Cluster stratify split"},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KMeans(n_clusters=2, random_state=13)\nkm.fit(pd.DataFrame(y))\ny_clust = km.predict(pd.DataFrame(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(y_clust).value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val, y_train_clust, y_val_clust = train_test_split(\n    train_df, y, pd.Series(y_clust), \n    test_size=0.25,\n    stratify=y_clust,\n    random_state=777\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train_clust.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 8. Scailing"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_sc = pd.DataFrame(scaler.transform(X_train))\nX_val_sc = pd.DataFrame(scaler.transform(X_val))\ntest_df_sc = pd.DataFrame(scaler.transform(test_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 9. Visualize our dateset"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\npca.fit(X_train_sc)\ntrain_pca_transformed = pca.transform(X_train_sc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nplt.scatter(train_pca_transformed[:, 0], train_pca_transformed[:, 1], c=y_train_clust);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 10. Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = LassoCV(max_iter=9999)\nlasso.fit(X_train_sc, y_train)\nlasso_train_pred = lasso.predict(X_train_sc)\nlasso_val_pred = lasso.predict(X_val_sc)\nprint('train', metrics.r2_score(y_train, lasso_train_pred), 'val', metrics.r2_score(y_val, lasso_val_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge = RidgeCV()\nridge.fit(X_train_sc, y_train)\nridge_train_pred = ridge.predict(X_train_sc)\nridge_val_pred = ridge.predict(X_val_sc)\nprint('train', metrics.r2_score(y_train, ridge_train_pred), 'val', metrics.r2_score(y_val, ridge_val_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enet = ElasticNetCV()\nenet.fit(X_train_sc, y_train)\nenet_train_pred = enet.predict(X_train_sc)\nenet_val_pred = enet.predict(X_val_sc)\nprint('train', metrics.r2_score(y_train, enet_train_pred), 'val', metrics.r2_score(y_val, enet_val_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"huber = HuberRegressor(alpha=0.05)\nhuber.fit(X_train_sc, y_train)\nhuber_train_pred = huber.predict(X_train_sc)\nhuber_val_pred = huber.predict(X_val_sc)\nprint('train', metrics.r2_score(y_train, huber_train_pred), 'val', metrics.r2_score(y_val, huber_val_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(n_estimators=5)\nrf.fit(X_train_sc, y_train)\nrf_train_pred = rf.predict(X_train_sc)\nrf_val_pred = rf.predict(X_val_sc)\nprint('train', metrics.r2_score(y_train, rf_train_pred), 'val', metrics.r2_score(y_val, rf_val_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see from the results random forest regression model performed better than others."},{"metadata":{},"cell_type":"markdown","source":"# 11. predict"},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df['y'] = np.round(np.exp(lasso.predict(test_df_sc)), 4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_df.to_csv('sub.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}