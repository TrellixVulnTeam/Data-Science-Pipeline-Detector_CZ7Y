{"cells":[{"metadata":{},"cell_type":"markdown","source":"***As a Mechanical Engineer working in working in an automotive domain I know how time consuming vechile testing can be. The process consists of building the prototype car, instrumenting it and then running the required tests . The major bottle neck in car testing occurs during instrumention phase which requires to de-assemble the car ,fit the required recording instruments and then re-assemble the car.*** <br>\n\n**Another bottle neck during testing is also the avaliablity of testing equipments such as drive cells required to run the test.** <br>\n\n**All this factors results in man-hours wasteage and a increased development time in the vechile development program. This adds and over-head cost to the company.** <br>\n\n**The Mercedes-Benz greeener manufacturing challenge on Kaggle provides one such case .In this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing***<br>","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import the required library \n\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport os \nimport zipfile\nimport glob \n\nimport tensorflow as tf \nimport tensorflow_addons as tfa\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers \nfrom tensorflow.keras.models import Model,load_model\nfrom keras.callbacks import Callback\nimport ml_metrics\n\nfrom sklearn import preprocessing \n\n%matplotlib inline \nplt.style.use('fivethirtyeight')\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Unzip and Import the dataset ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\npath ='/kaggle/input/mercedes-benz-greener-manufacturing'\n\nworking_path='/kaggle/working'\n\n\nif (os.getcwd()!=path):\n    os.chdir(path)\n    \n#Uzip the data \n\nfor file in glob.glob('*.zip'):\n    with zipfile.ZipFile(os.path.join(path,file), 'r') as zip_ref:\n        zip_ref.extractall(working_path)\n\n\nos.chdir(working_path)\n\n# Import the dataset \n\n\ndf_train =pd.read_csv('./train.csv')\n\ndf_test=pd.read_csv('./test.csv')\n\ndf_submission=pd.read_csv('./sample_submission.csv')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 150)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looks like the data is quite less just 4K samples and lot of columns ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.shape, df_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dtypes=pd.DataFrame({'col':df_train.columns,'dtypes':df_train.dtypes}).reset_index(drop=True)\nnp.transpose(df_dtypes[:400])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So in this dataframe we have : \n\n* 8 object columns \n* y as float \n* x10 to x17 as int64 ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets see the distribution of y variable \nplt.figure(figsize=(15,8))\nplt.subplot(1, 2, 1)\nplt.scatter(range(df_train.shape[0]), np.sort(df_train.y.values));\nplt.xlabel('index', fontsize=12);\nplt.ylabel('y', fontsize=12);\n\nplt.subplot(1,2,2)\nplt.hist(df_train.y,bins=15)\nplt.xlim(40,180)\nplt.xlabel('y', fontsize=12);\nplt.ylabel('counts', fontsize=12);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets correct the outlier point before we begin the model building ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train=df_train[df_train.y<=180]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Looks like there are no Null values in the train dataframe ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lets Explore the testing dataframe ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From EDA of various notebooks we know that all int variables are between 0 and 1 <br>\n\nLets combine test and train datasets for feature engineering <br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['test']=0\ndf_test['test']=1\n\ndata=pd.concat([df_train,df_test],axis=0)\n\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets seperate the int features \n\ninterger_columns=[]\n\nfor col in data.columns:\n    if col not in ['X0','X1','X2','X3','X4','X5','X6','X8']:\n        interger_columns.append(col)\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data[interger_columns]\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col=[c for c in data.columns if c not in ['y','test','ID']]\n\n# Feature engineering \n\ndata['sum']=data[col].sum(axis=1)\ndata['mean']=data[col].mean(axis=1)\ndata['median']=data[col].median(axis=1)\ndata['skew']=data[col].skew(axis=1)\ndata['kurt']=data[col].kurtosis(axis=1)\ndata['mode']=data[col].mode(axis=1)\n\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets split the data back  into test and train set \n\ntrain =data[data['test']!=1]\ntest =data[data['test']==1]\n\n\ntest.shape, train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing\n\nX=train.drop(columns=['ID','y'],axis=1)\ny=train['y']\n\n\nfrom sklearn import model_selection\nX_train,X_val,y_train,y_val=model_selection.train_test_split(X,y,shuffle=True,test_size=0.1,random_state=101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scale the Continous Variable \n\nnormalize_col=['sum','skew','kurt']\n\nfor col in normalize_col:\n    scaler=preprocessing.MinMaxScaler()\n    scaler.fit(X_train[col].values.reshape(-1,1))\n    X_train.loc[:,col]=scaler.transform(X_train[col].values.reshape(-1,1))\n    X_val.loc[:,col]=scaler.transform(X_val[col].values.reshape(-1,1))\n    test.loc[:,col]=scaler.transform(test[col].values.reshape(-1,1))\n    \n# Scaling Response variable \n\ny_scaler=preprocessing.MinMaxScaler()\ny_scaler.fit(y_train.values.reshape(-1,1))\ny_train=y_scaler.transform(y_train.values.reshape(-1,1))\ny_val=y_scaler.transform(y_val.values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ridge Regression on Interger Features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn import linear_model\nreg = linear_model.RidgeCV(alphas=(0.001,0.01,0.1,0.3,0.003,1,5))\n\nreg.fit(X_train,y_train)\npred_reg=reg.predict(X_val)\n\n\nfrom sklearn import metrics \n\n\n#Actually not the right way to score but just wanted to use max data in RidgeCV model for traning . Completely leaving out val set from training gives R2 score ~0.59\nprint('The R2 score for Ridge Regression is {}'.format(metrics.r2_score(y_val,pred_reg)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Lasso Regression on Integer Features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso=linear_model.Lasso(alpha=0.001,random_state=101)\n\nlasso.fit(X_train,y_train)\n\npred_lasso=lasso.predict(X_val)\n\nprint('The R2 score for Lasso Regression is {}'.format(metrics.r2_score(y_val,pred_lasso)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Bayesian Regression on Integer Features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"br=linear_model.BayesianRidge()\n\nbr.fit(X_train,y_train)\n\npred_bayesian=br.predict(X_val)\n\nprint('The R2 score for Bayesian Regression is {}'.format(metrics.r2_score(y_val,pred_bayesian)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest on Interger Features \n\n### Below cell is  RandomGridsearch ran for getting the parameters of random forest ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#from sklearn.model_selection import RandomizedSearchCV\n#from sklearn import ensemble \n\n#params = {\n\n#'n_estimators': [50,100,150,200,250],\n\n## Number of features to consider at every split\n#'max_features' : ['auto', 'sqrt'],\n\n## Maximum number of levels in tree\n#'max_depth' :[5,10,15,20,25],\n\n## Minimum number of samples required to split a node\n#'min_samples_split' : [2, 5, 10],\n\n## Minimum number of samples required at each leaf node\n#'min_samples_leaf': [1, 2, 4],\n\n# Method of selecting samples for training each tree\n#'bootstrap' : [True, False],\n    \n#'criterion':['mse', 'mae']\n    \n#}\n\n#from sklearn.metrics import r2_score, make_scorer\n#r2_scorer = make_scorer(r2_score)\n\n\n#rf = ensemble.RandomForestRegressor()\n\n#folds = 5\n#param_comb = 20\n\n#kfold = model_selection.KFold(n_splits=folds, shuffle = True, random_state = 101)\n\n#random_search = RandomizedSearchCV(rf, param_distributions=params, n_iter=param_comb, scoring=r2_scorer, n_jobs=1, cv=kfold.split(X,y), verbose=5, random_state=101,refit=True )\n\n#random_search.fit(X, y)\n\n#print(\"The best score is {}\".format(random_search.best_score_ ))\n\n#print('/n')\n\n#print ('The best paramerts are {}'.format(random_search.best_estimator_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# From running the random search CV above  we get the following values \n\nfrom sklearn import ensemble\n\nrf = ensemble.RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=4,n_estimators=50,random_state=101)\nrf.fit(X_train,y_train)\n\nrf_predict=rf.predict(X_val)\nmetrics.r2_score(y_val,rf_predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Result Prediction Using , Lasso, Ridge , Bayesian and RF on Integer Columns ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Result Prediction with Lasso and Ridge \n\n\ntest.drop('y',axis=1,inplace=True)\n\ntest_Ridge=reg.predict(test.drop('ID',axis=1))\ntest_Lasso=lasso.predict(test.drop('ID',axis=1))\ntest_Bayesian=br.predict(test.drop('ID',axis=1))\ntest_random_forest=rf.predict(test.drop('ID',axis=1))\n\ntest['Ridge_Prediction']=test_Ridge\ntest['Lasso_Prediction']=test_Lasso\ntest['Bayesian_Prediction']=test_Bayesian\ntest['Random_Prediction']=test_random_forest\n\ntest.loc[:,'y']=test[['Ridge_Prediction', 'Lasso_Prediction','Bayesian_Prediction','Random_Prediction']].mean(axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using Stacking option ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Stacking all Validation dataset prediction into a dataframe \ndf_stacking=pd.DataFrame(np.column_stack([y_val,pred_lasso,pred_reg,pred_bayesian,rf_predict]),\n                         columns=['y','lasso','Ridge','Bayesian','rf'])\n\n#All X_Test set predictions using all the above 4 models this will be later multiplied with the weights of the stacking model to get the final model \ndf_stacking_test=pd.DataFrame(np.column_stack([test.ID,test_Lasso,test_Ridge,test_Bayesian,test_random_forest]),\n                              columns=['ID','lasso','Ridge','Bayesian','rf'])\n\nfor col in df_stacking.columns:\n    df_stacking.loc[:,col]=y_scaler.inverse_transform(df_stacking[col].values.reshape(-1,1))\n\nfor col in ['lasso','rf','Ridge','Bayesian']:\n    df_stacking_test.loc[:,col]=y_scaler.inverse_transform(df_stacking_test[col].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Using a Simple Linear Regression as the Stacking model ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_stack=linear_model.LinearRegression()\nlr_stack.fit(df_stacking[['lasso','rf','Ridge','Bayesian']].values,df_stacking['y'].values)\n\ndf_stacking_test.loc[:,'y']=lr_stack.predict(df_stacking_test[['lasso','rf','Ridge','Bayesian']])\ndf_stacking_test[['ID','y']].to_csv('/kaggle/working/Stacking_Integer.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGbost with Entire Dataset ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions=test.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['test']=0\ndf_test['test']=1\n\ndata=pd.concat([df_train,df_test],axis=0)\n\n\ncol=[c for c in data.columns if c not in ['y','test','ID','X0','X1','X2','X3','X4','X5','X6','X8']]\ndata['sum']=data[col].sum(axis=1)\ndata['mean']=data[col].mean(axis=1)\ndata['median']=data[col].median(axis=1)\ndata['skew']=data[col].skew(axis=1)\ndata['kurt']=data[col].kurtosis(axis=1)\ndata['mode']=data[col].mode(axis=1)\n\n\nfor col in ['X0','X1','X2','X3','X4','X5','X6','X8']:\n    lbl_XG=preprocessing.LabelEncoder()\n    data.loc[:,col]=lbl_XG.fit_transform(data[col].values.reshape(-1,1))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=data[data.test!=1]\ntest=data[data.test==1]\n\ntrain.drop(columns='test',axis=1,inplace=True)\ntest.drop(columns='test',axis=1,inplace=True)\n\nX=train.drop(columns=['y','ID'],axis=1)\ny=train.y\n\nX_test=test.drop('y',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfrom sklearn import model_selection\n\nX_train,X_val,y_train,y_val=model_selection.train_test_split(X,y,shuffle=True,random_state=101,test_size=0.1)\n\nnormalize_col=['sum','skew','kurt']\n\nfrom sklearn import preprocessing\n\n\nfor col in normalize_col:\n    scaler=preprocessing.MinMaxScaler()\n    scaler.fit(X_train[col].values.reshape(-1,1))\n    X_train.loc[:,col]=scaler.transform(X_train[col].values.reshape(-1,1))\n    X_val.loc[:,col]=scaler.transform(X_val[col].values.reshape(-1,1))\n    X_test.loc[:,col]=scaler.transform(X_test[col].values.reshape(-1,1))\n    \n    \n# Scaling Response variable \ny_scaler=preprocessing.MinMaxScaler()\ny_scaler.fit(y_train.values.reshape(-1,1))\ny_train=y_scaler.transform(y_train.values.reshape(-1,1))\ny_val=y_scaler.transform(y_val.values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost With Random Search CV ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# A parameter grid for XGBoost\n\n### uncomment and run the below cell to run the random search CV ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#y_db=np.concatenate((y_train,y_val))\n#X_db=pd.concat([X_train,X_val])\n\n\n\n#params = {\n#        'learning_rate':[0.01,0.1,1],\n#        'n_estimators':[50,100,150,200,250],\n#        'min_child_weight': [1, 5, 10],\n#        'gamma': [0.5, 1, 1.5, 2, 5],\n#        'subsample': [0.6, 0.8, 1.0],\n#        'colsample_bytree': [0.6, 0.8, 1.0],\n#        'max_depth': [5,10,15],\n#        'reg_lambda':[0.5,1]\n#        }\n\n#xgb = xgb.XGBRegressor(objective ='reg:squarederror',\\\n#                    silent=False, nthread=1)\n\n#folds = 5\n#param_comb = 20\n\n#from sklearn.metrics import r2_score, make_scorer\n#r2_scorer = make_scorer(r2_score)\n\n\n#kfold = model_selection.KFold(n_splits=folds, shuffle = True, random_state = 101)\n\n#random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring=r2_scorer, n_jobs=1, cv=kfold.split(X_db,y_db), verbose=5, random_state=101,refit=True )\n\n#random_search.fit(X_db, y_db)\n\n\n#print(\"The best score is {}\".format(random_search.best_score_ ))\n\n#print('/n')\n\n#print ('The best paramerts are {}'.format(random_search.best_estimator_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The result of the above parameter grid is below ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Bestparams = {<br>\n        'learning_rate':[1],<br>\n        'n_estimators':[50],<br>\n        'min_child_weight': [5],\n        'gamma': [ 2],<br>\n        'subsample': [ 1.0],\n        'colsample_bytree': [0.6],<br>\n        'max_depth': [15],<br>\n        'reg_lambda':[0.5,1]<br>\n        }<br>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#From a previous Random Search CV run \n\nxgb=xgb.XGBRegressor(objective ='reg:squarederror',n_estimators=50,learning_rate=1,min_child_weight=5,gamma=2,\n                     colsample_by_tree=0.6,max_depth=15,reg_lambda=0.75,subsample=1)\n\nxgb.fit(X_train,y_train)\n\n\nX_test['XGB_predict']=xgb.predict(X_test.drop(\"ID\",axis=1))\n\npredictions['XGB_predictions']=X_test['XGB_predict']\n\nX_test['XGB_predict']=y_scaler.inverse_transform(X_test['XGB_predict'].values.reshape(-1,1))\n\nX_test_final=X_test[['ID','XGB_predict']]\n\n\n# Private LB score for just XGboost 0.53881\nX_test_final.to_csv('/kaggle/working/XG_boost_solution.csv',index=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest with Entire Dataset variables ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import ensemble\n\nrfc = ensemble.RandomForestRegressor(max_depth=10, max_features='sqrt', min_samples_leaf=4,n_estimators=50)\nrfc.fit(X_train,y_train)\n\nrfc_predict=rfc.predict(X_val)\n\nmetrics.r2_score(y_val,rfc_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions['Random_forest_entire']=rfc.predict(X_test.drop(columns=['ID','XGB_predict'],axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Private LB score =0.54222\n\npredictions['y1']=predictions[['Ridge_Prediction','Lasso_Prediction','Random_Prediction','XGB_predictions','Bayesian_Prediction']].mean(axis=1)\npredictions['y1']=y_scaler.inverse_transform(predictions['y1'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Private LB score =0.54324\n\npredictions['y2']=((0.40*predictions['XGB_predictions']+0.25*predictions['Lasso_Prediction']+0.25*predictions['Ridge_Prediction']+0.10*predictions['Random_Prediction']))\npredictions['y2']=y_scaler.inverse_transform(predictions['y2'].values.reshape(-1,1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Private LB score = 0.54453 Best Score of the notebook \npredictions['y3']=(0.4*y_scaler.inverse_transform(predictions['XGB_predictions'].values.reshape(-1,1))+ 0.6*df_stacking_test['y'].values.reshape(-1,1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Private LB score =0.54357\n\npredictions['y4']=(0.25*y_scaler.inverse_transform(predictions['XGB_predictions'].values.reshape(-1,1))+ 0.5*df_stacking_test['y'].values.reshape(-1,1)+\\\n                    0.25*y_scaler.inverse_transform(predictions['Random_forest_entire'].values.reshape(-1,1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Private LB score =0.54091\n\npredictions['y5']=(0.15*predictions['XGB_predictions']+0.30*predictions['Lasso_Prediction']+0.30*predictions['Ridge_Prediction']+0.10*predictions['Random_Prediction']\\\n                    +0.15*predictions['Random_forest_entire'])\n\npredictions['y5']=y_scaler.inverse_transform(predictions['y5'].values.reshape(-1,1))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Export the combination you need ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#predictions[['ID','y3']].to_csv('/kaggle/working/Stacking_XGboost_Random_forest.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://images-wixmp-ed30a86b8c4ca887773594c2.wixmp.com/f/a5a71474-025c-4ffe-b8c1-373c30b8bd6c/dc6kiet-017f562b-03e8-4636-97da-44a8df70b589.jpg/v1/fill/w_1024,h_587,q_75,strp/that_s_all_folks_space_jam_by_toon1990_dc6kiet-fullview.jpg?token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJ1cm46YXBwOiIsImlzcyI6InVybjphcHA6Iiwib2JqIjpbW3siaGVpZ2h0IjoiPD01ODciLCJwYXRoIjoiXC9mXC9hNWE3MTQ3NC0wMjVjLTRmZmUtYjhjMS0zNzNjMzBiOGJkNmNcL2RjNmtpZXQtMDE3ZjU2MmItMDNlOC00NjM2LTk3ZGEtNDRhOGRmNzBiNTg5LmpwZyIsIndpZHRoIjoiPD0xMDI0In1dXSwiYXVkIjpbInVybjpzZXJ2aWNlOmltYWdlLm9wZXJhdGlvbnMiXX0.Jb5O8VlFxU3vIZOYOsU5ICuht58Igo2Ss1rro97ArYw)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Useful Resource/References \n\nhttps://medium.com/@songxia.sophia/two-machine-learning-algorithms-to-predict-xgboost-neural-network-with-entity-embedding-caac68717dea <br>\n\nhttps://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526<br>\n\nhttps://gdcoder.com/entity-embeddings-of-categorical-variables-in-neural-networks/<br>\n\nhttps://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Book%20Recommendation%20System.ipynb<br>\n\nXGboost tutorial :https://www.datacamp.com/community/tutorials/xgboost-in-python<br>\n\nRandomSearchCV on XGboost : https://www.kaggle.com/tilii7/hyperparameter-grid-search-with-xgboost<br>","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}