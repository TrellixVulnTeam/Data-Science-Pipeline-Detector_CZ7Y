{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Mercedes-Benz 데이터셋","metadata":{}},{"cell_type":"markdown","source":"- 많은 공정 변수가 포함되어 있는 데이터셋을 통해서, 차원의 저주 문제를 해결해보고 여러가지 전처리 기법들을 적용해봅니다.","metadata":{}},{"cell_type":"markdown","source":"## 1. 라이브러리, 데이터 불러오기","metadata":{}},{"cell_type":"code","source":"# 데이터분석 4종 세트\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:11.757057Z","iopub.execute_input":"2022-06-29T07:51:11.757605Z","iopub.status.idle":"2022-06-29T07:51:11.764051Z","shell.execute_reply.started":"2022-06-29T07:51:11.757562Z","shell.execute_reply":"2022-06-29T07:51:11.763003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# flag variables\nremove_outlier = True\nencoding = 'OH' # 'OD', 'OH'\nfeature_reducing = 2   # 'PCA':0 / 'correlation':1 / 'importance':2 (optional)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:11.911713Z","iopub.execute_input":"2022-06-29T07:51:11.912157Z","iopub.status.idle":"2022-06-29T07:51:11.917503Z","shell.execute_reply.started":"2022-06-29T07:51:11.912124Z","shell.execute_reply":"2022-06-29T07:51:11.916028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터를 불러옵니다.\ntrain = pd.read_csv('../input/mercedes-benz-greener-manufacturing/train.csv.zip')\ntest = pd.read_csv('../input/mercedes-benz-greener-manufacturing/test.csv.zip')\nprint(train.shape, test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:12.080877Z","iopub.execute_input":"2022-06-29T07:51:12.081315Z","iopub.status.idle":"2022-06-29T07:51:12.408945Z","shell.execute_reply.started":"2022-06-29T07:51:12.081281Z","shell.execute_reply":"2022-06-29T07:51:12.407389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. EDA","metadata":{}},{"cell_type":"markdown","source":"#### 찾은 특징들\n\n\n1. 결측치 : 없음\n\n\n2. dtype이 object인 column : X0 ~ X8까지 8개. (categorical feature)\n\n> -> 어떻게 처리할지 고민해야함. (Ordinal Encoding VS One-Hot Encoding)\n\n> -> categorical feature들은 종류 정보들이 알파벳으로 되어있으며(anomynized) 이 정보들 대비 target값의 차이가 있는지 확인.\n(특별하게 관련 없음)\n\n> -> binary feature들중에서 0만 가지고 있는 column들이 있음.\n\n> -> 정보가 충분하지 않다고 판단(target value와의 관련성 0) 삭제.\n\n\n3. target distribution\n-> train data에 180을 넘는 데이터가 하나 있음. 이 데이터를 outlier라고 생각하고 제거.","metadata":{}},{"cell_type":"code","source":"# 1. 결측치 체크 --> train, test 데이터 모두 결측치가 존재하지 않음.\n\ntrain.loc[:, train.isnull().any()]   # 결측치를 포함하고 있는 column의 모든 row를 출력.\ntrain.columns[train.isnull().any()]  # 결측치를 포함하고 있는 모든 column을 출력.\ntrain[train.isnull().any(axis=1)]    # 결측치를 포함하고 있는 모든 row를 출력.\n\ntest[test.isnull().any(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:12.411748Z","iopub.execute_input":"2022-06-29T07:51:12.412147Z","iopub.status.idle":"2022-06-29T07:51:12.46039Z","shell.execute_reply.started":"2022-06-29T07:51:12.412114Z","shell.execute_reply":"2022-06-29T07:51:12.458989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. dtype이 object인 column 확인 (categorical feature 찾기)\n#cat_features = [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]\ncat_features = train.columns[train.dtypes == 'object']\ncat_features","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:12.462593Z","iopub.execute_input":"2022-06-29T07:51:12.463635Z","iopub.status.idle":"2022-06-29T07:51:12.473507Z","shell.execute_reply.started":"2022-06-29T07:51:12.463574Z","shell.execute_reply":"2022-06-29T07:51:12.472064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categorical feature들 확인\nfor col in cat_features:\n    plt.figure(figsize=(12, 6))\n    sns.countplot(data=train, x=col) # X0 ~ X8\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:12.476844Z","iopub.execute_input":"2022-06-29T07:51:12.477421Z","iopub.status.idle":"2022-06-29T07:51:15.421645Z","shell.execute_reply.started":"2022-06-29T07:51:12.477366Z","shell.execute_reply":"2022-06-29T07:51:15.420284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2-2. binary features\n## TO-DO : binary feature들에 대해서, 모두 0 또는 모두 1을 가지고 있는 column들을 찾아서 제거!\n\n# 1) 전체 column에서 ID, y, cat_feature를 제외하는 방법.\nbinary_features = train.columns.drop(['ID', 'y'] + list(cat_features))\n\n# 2) 전체 column에 dtypes이 int인 column을 뽑는 방법.\nbinary_features = train.columns[train.dtypes == 'int64'].drop(\"ID\")\n\n# 모두 0이거나 모두 1인 column들.\ndrop_cols = binary_features[(train[binary_features].sum() == 0) | (train[binary_features].sum() == len(train))]\ndrop_cols","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:15.424514Z","iopub.execute_input":"2022-06-29T07:51:15.425508Z","iopub.status.idle":"2022-06-29T07:51:15.455767Z","shell.execute_reply.started":"2022-06-29T07:51:15.425453Z","shell.execute_reply":"2022-06-29T07:51:15.454496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. target distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(data=train, x=\"y\", bins=100)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:15.457098Z","iopub.execute_input":"2022-06-29T07:51:15.457403Z","iopub.status.idle":"2022-06-29T07:51:15.938243Z","shell.execute_reply.started":"2022-06-29T07:51:15.457372Z","shell.execute_reply":"2022-06-29T07:51:15.93713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if remove_outlier:\n    y_limit = 175\n    train = train.drop(index=train[train.y > y_limit].index)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:15.941008Z","iopub.execute_input":"2022-06-29T07:51:15.941976Z","iopub.status.idle":"2022-06-29T07:51:15.9556Z","shell.execute_reply.started":"2022-06-29T07:51:15.941939Z","shell.execute_reply":"2022-06-29T07:51:15.953971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. 전처리","metadata":{}},{"cell_type":"markdown","source":"#### 결측치 처리\n\n- 비어있는 값들이 있는 column은 어떻게 처리할까요?\n\n\n> 결측치 없음!","metadata":{}},{"cell_type":"code","source":"# 결측치가 있는 column\npass","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:15.957477Z","iopub.execute_input":"2022-06-29T07:51:15.958061Z","iopub.status.idle":"2022-06-29T07:51:15.963274Z","shell.execute_reply.started":"2022-06-29T07:51:15.958024Z","shell.execute_reply":"2022-06-29T07:51:15.961897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# binary_feature들 중에서 필요없는 feature들 제거\ntrain = train.drop(columns=drop_cols)\ntest  = test.drop(columns=drop_cols)\nprint(train.shape, test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:15.965224Z","iopub.execute_input":"2022-06-29T07:51:15.966743Z","iopub.status.idle":"2022-06-29T07:51:15.988376Z","shell.execute_reply.started":"2022-06-29T07:51:15.966659Z","shell.execute_reply":"2022-06-29T07:51:15.987333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### feature 구분\n\n- X0 ~ X8 : categorical feature\n\n- other features : binary feature(0 / 1)","metadata":{}},{"cell_type":"code","source":"# categorical feature encoding\n# 1) Ordinal Encoding\n\n# type_map = {'a':0, 'b':1, 'c':2, 'd':3, 'e':4, 'f':5, 'g':6}\n# train.X3 = train.X3.map(type_map)  # custom mapping\n\n# 전처리를 위해서 train, test 데이터를 합친 데이터를 하나 생성.\ntotal = pd.concat([train, test])\n\nif encoding == 'OD':\n    for col in cat_features:\n        total[col] = pd.factorize(total[col])[0]\n    \n\n# 2) One-hot Encoding\nelif encoding == \"OH\":\n    total = pd.get_dummies(data=total, columns=cat_features)\n\n    \ntrain = total[:len(train)] # 4209 \ntest = total[len(train):].drop(columns=\"y\") # 4209\nprint(train.shape, test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:15.991453Z","iopub.execute_input":"2022-06-29T07:51:15.992069Z","iopub.status.idle":"2022-06-29T07:51:16.118875Z","shell.execute_reply.started":"2022-06-29T07:51:15.992014Z","shell.execute_reply":"2022-06-29T07:51:16.117736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### feature engineering\n\n1. PCA(Principal Component Analysis)\n\n\n2. Correlation\n\n\n3. VIF\n\n\n4. (OPTIONAL) feature importance","metadata":{}},{"cell_type":"code","source":"# 1. PCA를 이용하여 차원 감소 시키기\nif feature_reducing == 0:\n    from sklearn.decomposition import PCA\n\n    #pca = PCA(n_components=15, random_state=42)   # 15차원으로 변환해주세요.\n    pca = PCA(n_components=0.95, random_state=42) # 원본 데이터의 정보를 95% 보존하는 차원으로 변환해주세요.\n    pca_X = pca.fit_transform(train.drop(columns=[\"ID\", 'y']))\n\n    pca_columns = [f\"PC{i}\" for i in range(1, pca_X.shape[1]+1)]\n    X = pd.DataFrame(data=pca_X, columns=pca_columns)\n    display(X)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:16.120629Z","iopub.execute_input":"2022-06-29T07:51:16.121195Z","iopub.status.idle":"2022-06-29T07:51:16.130099Z","shell.execute_reply.started":"2022-06-29T07:51:16.121146Z","shell.execute_reply":"2022-06-29T07:51:16.128759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. correlation을 이용하여 서로 비슷한 정보를 가지는 피처 제거하기\nif feature_reducing == 1:\n    threshold = 0.9\n    except_cols = []\n    remove_cols = []\n\n    corr = train.drop(columns=[\"ID\", 'y']).corr()\n\n    for col in corr: # X0 -> X385\n\n        if col in except_cols:\n            continue\n\n        except_cols.append(col)\n        row = np.abs(corr.loc[col])   # 절대값을 취해서 양/음수를 없앰. (크기만 비교하겠다)\n        condition1 = (row >= threshold)\n        condition2 = ~corr.columns.isin(except_cols)  # 전체 column중에 except_cols에 없는 column들.\n\n        temp = row[condition1 & condition2].index  # 위의 두 조건을 모두 만족하는 column들 (v)\n        except_cols = except_cols + list(temp)\n        remove_cols = remove_cols + list(temp)\n\n    print(\"Number of columns to be removed : \", len(remove_cols))\n    \n    X = train.drop(columns=[\"ID\", 'y'] + remove_cols)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:16.132293Z","iopub.execute_input":"2022-06-29T07:51:16.133043Z","iopub.status.idle":"2022-06-29T07:51:16.144467Z","shell.execute_reply.started":"2022-06-29T07:51:16.133003Z","shell.execute_reply":"2022-06-29T07:51:16.143194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # 3. VIF 구현하기\n# from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# # VIF_i = 1 / (1 - R^2_i)\n\n# X = train.drop(columns=[\"ID\", 'y'])\n\n# vifs = [variance_inflation_factor(X, idx) for idx in range(len(X.columns))]\n# vif_df = pd.DataFrame({\"feature_name\":X.columns, \"VIF\":vifs})\n# vif_df.sort_values(by=\"VIF\", ascending=False)\n\n# # vifs = [variance_inflation_factor(X, idx) for idx in range(8)]\n# # vif_df = pd.DataFrame({\"feature_name\":X.columns[:8], \"VIF\":vifs})\n# # vif_df.sort_values(by=\"VIF\", ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:16.148755Z","iopub.execute_input":"2022-06-29T07:51:16.149583Z","iopub.status.idle":"2022-06-29T07:51:16.156844Z","shell.execute_reply.started":"2022-06-29T07:51:16.149544Z","shell.execute_reply":"2022-06-29T07:51:16.155242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # VIF가 threshold를 넘는 경우에 해당 column을 제거.\n# threshold = 10\n# temp_cols = vif_df.loc[vif_df.VIF >= threshold, \"feature_name\"].values\n\n# X = X.drop(columns=temp_cols)","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:16.160838Z","iopub.execute_input":"2022-06-29T07:51:16.161393Z","iopub.status.idle":"2022-06-29T07:51:16.168417Z","shell.execute_reply.started":"2022-06-29T07:51:16.161353Z","shell.execute_reply":"2022-06-29T07:51:16.16708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4. (OPTIONAL) feature importance를 기준으로 중요한 feature 뽑기\nif feature_reducing == 2:\n    X = train.drop(columns=[\"ID\", 'y'])\n    y = train.y\n\n    from sklearn.ensemble import RandomForestRegressor\n\n    reg = RandomForestRegressor(n_estimators=100,\n                               max_depth=15,\n                               max_features=0.7)\n    reg.fit(X, y)\n\n    importances = reg.feature_importances_\n    feature_names = X.columns\n    indices = np.argsort(importances)[::-1][:15] # feature importance가 높은 Feature들의 index\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=feature_names[indices], y=importances[indices])\n    plt.show()\n    \n    X = X[feature_names[indices]] # feature importance가 높은 15개를 선정.","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:16.170131Z","iopub.execute_input":"2022-06-29T07:51:16.170452Z","iopub.status.idle":"2022-06-29T07:51:22.688746Z","shell.execute_reply.started":"2022-06-29T07:51:16.17042Z","shell.execute_reply":"2022-06-29T07:51:22.686873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 전처리가 완료된 X, y를 세팅합니다!","metadata":{}},{"cell_type":"code","source":"X_train = X\ny_train = train.y","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:22.690692Z","iopub.execute_input":"2022-06-29T07:51:22.691098Z","iopub.status.idle":"2022-06-29T07:51:22.696454Z","shell.execute_reply.started":"2022-06-29T07:51:22.691062Z","shell.execute_reply":"2022-06-29T07:51:22.695242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-06-29T07:51:22.698459Z","iopub.execute_input":"2022-06-29T07:51:22.699026Z","iopub.status.idle":"2022-06-29T07:51:22.728972Z","shell.execute_reply.started":"2022-06-29T07:51:22.698986Z","shell.execute_reply":"2022-06-29T07:51:22.727277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}