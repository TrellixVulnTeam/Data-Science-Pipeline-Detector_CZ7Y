{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 이번 프로젝트의 흐름","metadata":{}},{"cell_type":"markdown","source":"1. 간단한 전처리만 수행한 데이터(결측치, categorical feature encoding)를 바로 Random Forest에 돌린 결과를 baseline으로 선정.\n(Linear Regression은 사용하지 않았습니다. 왜냐면 categorical feature와 binary feature가 많은 경우에 동작하지 않을 것을 어느정도 예상했기 때문입니다)\n\n\n2. 3가지 feature engineering 방법을 사용합니다.\n- 우선 categorical feature는 모두 one-hot encoding을 합니다.\n\n\n> 2-1. Correlation check (threshold는 0.9로 세팅)\n\n> 2-2. feature importance가 0.1을 넘는 feature만 선택\n\n> 2-3. PCA로 90%를 보존하는 차원으로 차원감소\n\n\n\n3. 해당 모델마다 동일한 범위 내에서 optuna로 hyper-parameter tuning을 수행하여 성능을 비교합니다. 비교하여 성능이 높은 feature engineering 기법을 고릅니다.\n\n> train-validation split은 9:1로 합니다.\n\n\n4. LightGBM으로 모델을 변경 후, optuna로 hyper-parameter tuning을 열심히 돌립니다.\n\n\n5. 최종 모델 선정","metadata":{}},{"cell_type":"markdown","source":"## 1. 라이브러리, 데이터 불러오기","metadata":{}},{"cell_type":"code","source":"# 데이터분석 4종 세트\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 모델들, 성능 평가\n# (저는 일반적으로 정형데이터로 머신러닝 분석할 때는 이 2개 모델은 그냥 돌려봅니다. 특히 RF가 테스트하기 좋습니다.)\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm.sklearn import LGBMRegressor\n\n# KFold(CV), partial : optuna를 사용하기 위함\nfrom sklearn.model_selection import KFold\nfrom functools import partial\n\n# hyper-parameter tuning을 위한 라이브러리, optuna\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:47.613041Z","iopub.execute_input":"2022-06-03T06:08:47.613587Z","iopub.status.idle":"2022-06-03T06:08:49.315259Z","shell.execute_reply.started":"2022-06-03T06:08:47.61346Z","shell.execute_reply":"2022-06-03T06:08:49.314151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# flag setting\nfeature_reducing = \"feature_importance\" # \"correlation\" / \"feature_importance\" / \"PCA\"","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:49.317203Z","iopub.execute_input":"2022-06-03T06:08:49.31791Z","iopub.status.idle":"2022-06-03T06:08:49.321848Z","shell.execute_reply.started":"2022-06-03T06:08:49.317867Z","shell.execute_reply":"2022-06-03T06:08:49.321103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터를 불러옵니다.\ntrain = pd.read_csv(\"../input/mercedes-benz-greener-manufacturing/train.csv.zip\")\ntest = pd.read_csv(\"../input/mercedes-benz-greener-manufacturing/test.csv.zip\")\ntrain","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:49.32293Z","iopub.execute_input":"2022-06-03T06:08:49.323807Z","iopub.status.idle":"2022-06-03T06:08:49.696424Z","shell.execute_reply.started":"2022-06-03T06:08:49.323765Z","shell.execute_reply":"2022-06-03T06:08:49.69547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. EDA","metadata":{}},{"cell_type":"code","source":"# y의 분포\nplt.figure(figsize=(12, 6))\nsns.histplot(data=train, x=\"y\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:49.697797Z","iopub.execute_input":"2022-06-03T06:08:49.69822Z","iopub.status.idle":"2022-06-03T06:08:50.077286Z","shell.execute_reply.started":"2022-06-03T06:08:49.698183Z","shell.execute_reply":"2022-06-03T06:08:50.076157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 찾은 특징들\n\n\n1. 결측치 : 없음\n\n\n2. dtype이 object인 column : X0 ~ X8까지 8개. (categorical feature)\n\n> -> 어떻게 처리할지 고민해야함. (Ordinal Encoding VS One-Hot Encoding)\n\n> -> categorical feature들은 종류 정보들이 알파벳으로 되어있으며(anomynized) 이 정보들 대비 target값의 차이가 있는지 확인.\n(특별하게 관련 없음)\n\n> -> binary feature들중에서 0만 가지고 있는 column들이 있음.\n\n> -> 정보가 충분하지 않다고 판단(target value와의 관련성 0) 삭제.\n\n\n3. target distribution\n-> train data에 180을 넘는 데이터가 하나 있음. 이 데이터를 outlier라고 생각하고 제거.","metadata":{}},{"cell_type":"markdown","source":"### 3. 전처리","metadata":{}},{"cell_type":"markdown","source":"#### 결측치 처리","metadata":{}},{"cell_type":"code","source":"# 결측치가 있는 column\ntrain[train.isnull().any(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:50.07979Z","iopub.execute_input":"2022-06-03T06:08:50.080249Z","iopub.status.idle":"2022-06-03T06:08:50.104023Z","shell.execute_reply.started":"2022-06-03T06:08:50.080208Z","shell.execute_reply":"2022-06-03T06:08:50.10305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[test.isnull().any(axis=1)]","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:50.105524Z","iopub.execute_input":"2022-06-03T06:08:50.10588Z","iopub.status.idle":"2022-06-03T06:08:50.129465Z","shell.execute_reply.started":"2022-06-03T06:08:50.10585Z","shell.execute_reply":"2022-06-03T06:08:50.128481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### feature 구분\n\n- X0 ~ X8 : categorical feature\n\n- other features : binary feature(0 / 1)","metadata":{}},{"cell_type":"code","source":"categorical_features = train.columns[2:10]\ncategorical_features","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:50.131013Z","iopub.execute_input":"2022-06-03T06:08:50.13138Z","iopub.status.idle":"2022-06-03T06:08:50.138094Z","shell.execute_reply.started":"2022-06-03T06:08:50.131349Z","shell.execute_reply":"2022-06-03T06:08:50.137249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = train.columns[10:]\ntemp","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:50.139332Z","iopub.execute_input":"2022-06-03T06:08:50.139826Z","iopub.status.idle":"2022-06-03T06:08:50.151786Z","shell.execute_reply.started":"2022-06-03T06:08:50.139792Z","shell.execute_reply":"2022-06-03T06:08:50.150831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"card1 = train.columns[train.nunique() == 1]\ncard1","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:50.153892Z","iopub.execute_input":"2022-06-03T06:08:50.154383Z","iopub.status.idle":"2022-06-03T06:08:50.210594Z","shell.execute_reply.started":"2022-06-03T06:08:50.154337Z","shell.execute_reply":"2022-06-03T06:08:50.209458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"binary_features = np.setdiff1d(temp, card1)\nprint(\"%d features - %d features = %d binary features\" % (len(temp), len(card1), len(binary_features)))","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:50.212336Z","iopub.execute_input":"2022-06-03T06:08:50.213189Z","iopub.status.idle":"2022-06-03T06:08:50.221933Z","shell.execute_reply.started":"2022-06-03T06:08:50.213116Z","shell.execute_reply":"2022-06-03T06:08:50.22005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### feature engineering\n\n1. Correlation\n\n\n2. feature importance\n\n\n3. PCA","metadata":{}},{"cell_type":"code","source":"# feature engineering을 위해 tempX, y 생성\ntotal = pd.concat([train, test])\nsplit_point = len(train)\ntotal_OHE = pd.get_dummies(data=total, columns=categorical_features)\ny = train.y\ntempX = total_OHE.drop(columns=[\"ID\", \"y\"])\ntempX = tempX.drop(columns=card1)\ntrainX = tempX[:split_point]\ntestX = tempX[split_point:]\nprint(trainX.shape, testX.shape, y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:50.223332Z","iopub.execute_input":"2022-06-03T06:08:50.22457Z","iopub.status.idle":"2022-06-03T06:08:50.368676Z","shell.execute_reply.started":"2022-06-03T06:08:50.224524Z","shell.execute_reply":"2022-06-03T06:08:50.367563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainX # sponge","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:50.370327Z","iopub.execute_input":"2022-06-03T06:08:50.370965Z","iopub.status.idle":"2022-06-03T06:08:50.395767Z","shell.execute_reply.started":"2022-06-03T06:08:50.370919Z","shell.execute_reply":"2022-06-03T06:08:50.394422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testX","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:50.397558Z","iopub.execute_input":"2022-06-03T06:08:50.397912Z","iopub.status.idle":"2022-06-03T06:08:50.424343Z","shell.execute_reply.started":"2022-06-03T06:08:50.397879Z","shell.execute_reply":"2022-06-03T06:08:50.423459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. correlation\n\n# 중복정보가 있는 column 제거하기 위해 상관계수를 확인해봅니다.\ndef remove_collinearity(X, threshold):\n    \"\"\"\n    X : feature matrix\n    threshold : 다중공선성을 제거할 column을 고르는 기준 값. [0, 1]\n    \"\"\"\n    \n    corr = X.corr()\n    candidate_cols = []\n    \n    for x in corr.iterrows():\n        idx, row = x[0], x[1] # decoupling tuple\n        # 해당 row는 이미 처리가 되어서 볼 필요가 없다.\n        if idx in candidate_cols:\n            continue\n        #print(row[row > 0.7].index[1:])\n        candidates = row[row > threshold].index[1:]\n\n        # 자기 자신을 제외하고 threshold를 넘는 column이 있다면,\n        if len(candidates) != 0:\n            for col in candidates:\n                candidate_cols.append(col)           \n    \n    return candidate_cols\n\ndef find_feature_importance(X, model, show_plot):\n\n    feat_names = X.columns.values\n    importances = model.feature_importances_\n    std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n    indices = np.argsort(importances)[::-1][:20]\n\n    plt.figure(figsize=(12,12))\n    plt.title(\"Feature importances\")\n    plt.bar(range(len(indices)), importances[indices], color=\"r\", align=\"center\")\n    plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\n    plt.xlim([-1, len(indices)])\n    plt.show()\n    \n    important_features = X.columns[importances >= 0.005]\n    return important_features\n    \ndef apply_PCA(X, show_plot):\n    from sklearn.decomposition import PCA\n    # training data와 test data를 모두 PCA를 이용하여 차원 감소를 수행합니다.\n    pca = PCA(n_components=0.90) # 원래 데이터의 90%를 보존하는 차원.\n    pca_090 = pca.fit(X) # 학습 및 변환\n    reduced_X = pca_090.transform(X)\n    print(reduced_X.shape)\n    \n    if show_plot:\n        labels = [f\"PC{x}\" for x in range(1, reduced_X.shape[1]+1)]\n        pca_090_variance = np.round(pca_090.explained_variance_ratio_.cumsum()*100, decimals=1)\n        plt.figure(figsize=(25,5))\n        plt.bar(x=range(1, len(pca_090_variance)+1), height=pca_090_variance, tick_label=labels)\n\n        plt.xticks(rotation=90, color='indigo', size=15)\n        plt.yticks(rotation=0, color='indigo', size=15)\n        plt.title('Scree Plot',color='tab:orange', fontsize=25)\n        plt.xlabel('Principal Components', {'color': 'tab:orange', 'fontsize':15})\n        plt.ylabel('Cumulative percentage of explained variance ', {'color': 'tab:orange', 'fontsize':15})\n        plt.show()\n        \n        X_train_pca_df = pd.DataFrame(reduced_X, columns=labels)\n        display(X_train_pca_df)\n\n    return pca_090, X_train_pca_df","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:50.428206Z","iopub.execute_input":"2022-06-03T06:08:50.428564Z","iopub.status.idle":"2022-06-03T06:08:50.447222Z","shell.execute_reply.started":"2022-06-03T06:08:50.428532Z","shell.execute_reply":"2022-06-03T06:08:50.446329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 적용\nif feature_reducing == \"correlation\":\n    threshold = 0.7\n    correlated_features = remove_collinearity(trainX, threshold)\n    correlated_features = set(correlated_features) # 중복 제거\n    print(\"%d Correlation features over %.2f\" % (len(correlated_features), threshold))\n    \n    X = trainX.drop(columns=correlated_features)\n    print(X.shape)\n    \nelif feature_reducing == \"feature_importance\":\n    show_plot = True\n    model = RandomForestRegressor(max_features=\"sqrt\", n_jobs=-1, random_state=0xC0FFEE)\n    model.fit(trainX, y)\n    important_features = find_feature_importance(trainX, model, show_plot)\n    X = trainX[important_features]\n    print(X.shape)\n    \nelif feature_reducing == \"PCA\":\n    show_plot = True\n    pca_model, X = apply_PCA(trainX, show_plot)\n    print(X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:50.448519Z","iopub.execute_input":"2022-06-03T06:08:50.448865Z","iopub.status.idle":"2022-06-03T06:08:51.829389Z","shell.execute_reply.started":"2022-06-03T06:08:50.448825Z","shell.execute_reply":"2022-06-03T06:08:51.828479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. 학습 데이터 분할","metadata":{}},{"cell_type":"code","source":"# 첫번째 테스트용으로 사용하고, 실제 학습시에는 K-Fold CV를 사용합니다.\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0xC0FFEE)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:51.830785Z","iopub.execute_input":"2022-06-03T06:08:51.831376Z","iopub.status.idle":"2022-06-03T06:08:51.840947Z","shell.execute_reply.started":"2022-06-03T06:08:51.831339Z","shell.execute_reply":"2022-06-03T06:08:51.839693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. 학습 및 평가 (debugging 용도)","metadata":{}},{"cell_type":"code","source":"print(\"\\nFitting Random Forest...\")\nmodel = RandomForestRegressor(max_features='sqrt', n_jobs=-1)\nmodel.fit(X_train, y_train)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-03T06:08:51.842502Z","iopub.execute_input":"2022-06-03T06:08:51.843488Z","iopub.status.idle":"2022-06-03T06:08:52.208285Z","shell.execute_reply.started":"2022-06-03T06:08:51.843449Z","shell.execute_reply":"2022-06-03T06:08:52.207212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metric은 그때마다 맞게 바꿔줘야 합니다.\nfrom sklearn.metrics import r2_score\nevaluation_metric = r2_score","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:52.209517Z","iopub.execute_input":"2022-06-03T06:08:52.209861Z","iopub.status.idle":"2022-06-03T06:08:52.214085Z","shell.execute_reply.started":"2022-06-03T06:08:52.209831Z","shell.execute_reply":"2022-06-03T06:08:52.213096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Prediction\")\npred_train = model.predict(X_train)\npred_test = model.predict(X_test)\n\n\ntrain_score = evaluation_metric(y_train, pred_train)\ntest_score = evaluation_metric(y_test, pred_test)\n\nprint(\"Train Score : %.4f\" % train_score)\nprint(\"Test Score : %.4f\" % test_score)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:52.215551Z","iopub.execute_input":"2022-06-03T06:08:52.216211Z","iopub.status.idle":"2022-06-03T06:08:52.438746Z","shell.execute_reply.started":"2022-06-03T06:08:52.216161Z","shell.execute_reply":"2022-06-03T06:08:52.437616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Hyper-parameter Tuning","metadata":{}},{"cell_type":"markdown","source":"- optuna를 갈아넣습니다!","metadata":{}},{"cell_type":"code","source":"!pip install plotly","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:08:52.440253Z","iopub.execute_input":"2022-06-03T06:08:52.441064Z","iopub.status.idle":"2022-06-03T06:09:03.340934Z","shell.execute_reply.started":"2022-06-03T06:08:52.441025Z","shell.execute_reply":"2022-06-03T06:09:03.339657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For Regression\n\ndef optimizer(trial, X, y, K):\n    # 조절할 hyper-parameter 조합을 적어줍니다.\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n    max_depth = trial.suggest_int(\"max_depth\", 8, 30)\n    max_features = trial.suggest_categorical(\"max_features\", ['auto', 'sqrt', 'log2'])\n    \n    \n    # 원하는 모델을 지정합니다, optuna는 시간이 오래걸리기 때문에 저는 보통 RF로 일단 테스트를 해본 뒤에 LGBM을 사용합니다.\n    model = RandomForestRegressor(n_estimators=n_estimators,\n                                  max_depth=max_depth,\n                                  max_features=max_features,\n                                  n_jobs=-1,\n                                  random_state=0xC0FFEE)\n    \n    \n    # K-Fold Cross validation을 구현합니다.\n    folds = KFold(n_splits=K)\n    scores = []\n    \n    for train_idx, val_idx in folds.split(X, y):\n        X_train = X.iloc[train_idx, :]\n        y_train = y.iloc[train_idx]\n        \n        X_val = X.iloc[val_idx, :]\n        y_val = y.iloc[val_idx]\n        \n        model.fit(X_train, y_train)\n        preds = model.predict(X_val)\n        score = evaluation_metric(y_val, preds)\n        scores.append(score)\n    \n    \n    # K-Fold의 평균 loss값을 돌려줍니다.\n    return np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:03.342851Z","iopub.execute_input":"2022-06-03T06:09:03.343237Z","iopub.status.idle":"2022-06-03T06:09:03.354465Z","shell.execute_reply.started":"2022-06-03T06:09:03.343202Z","shell.execute_reply":"2022-06-03T06:09:03.353206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K = 5 # Kfold 수\nopt_func = partial(optimizer, X=X_train, y=y_train, K=K)\n\nrf_study = optuna.create_study(study_name=\"RF\", direction=\"maximize\") # regression task에서 R^2를 최대화!\nrf_study.optimize(opt_func, n_trials=15)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:03.356087Z","iopub.execute_input":"2022-06-03T06:09:03.356483Z","iopub.status.idle":"2022-06-03T06:09:44.918388Z","shell.execute_reply.started":"2022-06-03T06:09:03.356449Z","shell.execute_reply":"2022-06-03T06:09:44.917233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optuna가 시도했던 모든 실험 관련 데이터\nrf_study.trials_dataframe()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:44.919729Z","iopub.execute_input":"2022-06-03T06:09:44.920108Z","iopub.status.idle":"2022-06-03T06:09:44.94995Z","shell.execute_reply.started":"2022-06-03T06:09:44.920074Z","shell.execute_reply":"2022-06-03T06:09:44.948613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best Score: %.4f\" % rf_study.best_value) # best score 출력\nprint(\"Best params: \", rf_study.best_trial.params) # best score일 때의 하이퍼파라미터들","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:44.951381Z","iopub.execute_input":"2022-06-03T06:09:44.951898Z","iopub.status.idle":"2022-06-03T06:09:44.959338Z","shell.execute_reply.started":"2022-06-03T06:09:44.951859Z","shell.execute_reply":"2022-06-03T06:09:44.958046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 실험 기록 시각화\noptuna.visualization.plot_optimization_history(rf_study)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:44.961402Z","iopub.execute_input":"2022-06-03T06:09:44.961784Z","iopub.status.idle":"2022-06-03T06:09:45.03163Z","shell.execute_reply.started":"2022-06-03T06:09:44.96175Z","shell.execute_reply":"2022-06-03T06:09:45.030448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyper-parameter들의 중요도\noptuna.visualization.plot_param_importances(rf_study)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:45.033257Z","iopub.execute_input":"2022-06-03T06:09:45.033621Z","iopub.status.idle":"2022-06-03T06:09:45.40261Z","shell.execute_reply.started":"2022-06-03T06:09:45.033591Z","shell.execute_reply":"2022-06-03T06:09:45.401454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. 테스트 및 제출 파일 생성","metadata":{}},{"cell_type":"code","source":"final_rf_model = RandomForestRegressor(n_estimators=rf_study.best_trial.params[\"n_estimators\"],\n                                 max_depth=rf_study.best_trial.params[\"max_depth\"],\n                                 max_features=rf_study.best_trial.params[\"max_features\"])\n\nfinal_rf_model.fit(X, y) # finalize model","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:45.404312Z","iopub.execute_input":"2022-06-03T06:09:45.404843Z","iopub.status.idle":"2022-06-03T06:09:45.797017Z","shell.execute_reply.started":"2022-06-03T06:09:45.404788Z","shell.execute_reply":"2022-06-03T06:09:45.795954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testX","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:45.798184Z","iopub.execute_input":"2022-06-03T06:09:45.798533Z","iopub.status.idle":"2022-06-03T06:09:45.825578Z","shell.execute_reply.started":"2022-06-03T06:09:45.798503Z","shell.execute_reply":"2022-06-03T06:09:45.824453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 적용\nif feature_reducing == \"correlation\":\n    test = testX.drop(columns=correlated_features)\n    print(X.shape)\n    \nelif feature_reducing == \"feature_importance\":\n    test = testX[important_features]\n    print(X.shape)\n    \nelif feature_reducing == \"PCA\":\n    test = pca_model.transform(testX)\n    print(X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:45.827066Z","iopub.execute_input":"2022-06-03T06:09:45.827488Z","iopub.status.idle":"2022-06-03T06:09:45.835783Z","shell.execute_reply.started":"2022-06-03T06:09:45.827451Z","shell.execute_reply":"2022-06-03T06:09:45.834844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = final_rf_model.predict(test)\nprediction","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:45.836809Z","iopub.execute_input":"2022-06-03T06:09:45.837366Z","iopub.status.idle":"2022-06-03T06:09:45.919793Z","shell.execute_reply.started":"2022-06-03T06:09:45.837323Z","shell.execute_reply":"2022-06-03T06:09:45.91852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/mercedes-benz-greener-manufacturing/sample_submission.csv.zip\")\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:45.9214Z","iopub.execute_input":"2022-06-03T06:09:45.921962Z","iopub.status.idle":"2022-06-03T06:09:45.943394Z","shell.execute_reply.started":"2022-06-03T06:09:45.921905Z","shell.execute_reply":"2022-06-03T06:09:45.942002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission[\"y\"] = prediction\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:45.944997Z","iopub.execute_input":"2022-06-03T06:09:45.94552Z","iopub.status.idle":"2022-06-03T06:09:45.960584Z","shell.execute_reply.started":"2022-06-03T06:09:45.945467Z","shell.execute_reply":"2022-06-03T06:09:45.959376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.reset_index(drop=True).to_csv(f\"rf_submission_{feature_reducing}.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:09:45.962862Z","iopub.execute_input":"2022-06-03T06:09:45.963412Z","iopub.status.idle":"2022-06-03T06:09:45.991306Z","shell.execute_reply.started":"2022-06-03T06:09:45.963358Z","shell.execute_reply":"2022-06-03T06:09:45.990234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 9. LightGBM으로 변경!","metadata":{}},{"cell_type":"markdown","source":"Reference : https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html","metadata":{}},{"cell_type":"code","source":"# For Regression\n\ndef optimizer(trial, X, y, K):\n    \n    import os\n    \n    param = {\n        'objective': 'regression', # 회귀\n        'verbose': 0, \n        'max_depth': trial.suggest_int('max_depth', 8, 20),\n        'learning_rate': trial.suggest_loguniform(\"learning_rate\", 1e-8, 1e-2),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_loguniform('subsample', 0.4, 1),\n    }\n\n    model = LGBMRegressor(**param, n_jobs=os.cpu_count())\n    \n    # K-Fold Cross validation을 구현합니다.\n    folds = KFold(n_splits=K)\n    scores = []\n    \n    for train_idx, val_idx in folds.split(X, y):\n        X_train = X.iloc[train_idx, :]\n        y_train = y.iloc[train_idx]\n        \n        X_val = X.iloc[val_idx, :]\n        y_val = y.iloc[val_idx]\n        \n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=25)\n        preds = model.predict(X_val)\n        score = evaluation_metric(y_val, preds)\n        scores.append(score)\n    \n    \n    # K-Fold의 평균 loss값을 돌려줍니다.\n    return np.mean(scores)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:15:56.553884Z","iopub.execute_input":"2022-06-03T06:15:56.554436Z","iopub.status.idle":"2022-06-03T06:15:56.566163Z","shell.execute_reply.started":"2022-06-03T06:15:56.554393Z","shell.execute_reply":"2022-06-03T06:15:56.565075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K = 5 # Kfold 수\nopt_func = partial(optimizer, X=X_train, y=y_train, K=K)\n\nlgbm_study = optuna.create_study(study_name=\"LGBM\", direction=\"maximize\") # regression task에서 R^2를 최대화!\nlgbm_study.optimize(opt_func, n_trials=15)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:15:58.375698Z","iopub.execute_input":"2022-06-03T06:15:58.376569Z","iopub.status.idle":"2022-06-03T06:16:45.83039Z","shell.execute_reply.started":"2022-06-03T06:15:58.376526Z","shell.execute_reply":"2022-06-03T06:16:45.829465Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optuna가 시도했던 모든 실험 관련 데이터\nlgbm_study.trials_dataframe()","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:10:32.946358Z","iopub.execute_input":"2022-06-03T06:10:32.948718Z","iopub.status.idle":"2022-06-03T06:10:32.979308Z","shell.execute_reply.started":"2022-06-03T06:10:32.948655Z","shell.execute_reply":"2022-06-03T06:10:32.978208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best Score: %.4f\" % lgbm_study.best_value) # best score 출력\nprint(\"Best params: \", lgbm_study.best_trial.params) # best score일 때의 하이퍼파라미터들","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:10:32.981027Z","iopub.execute_input":"2022-06-03T06:10:32.982227Z","iopub.status.idle":"2022-06-03T06:10:32.989974Z","shell.execute_reply.started":"2022-06-03T06:10:32.982171Z","shell.execute_reply":"2022-06-03T06:10:32.988756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 실험 기록 시각화\noptuna.visualization.plot_optimization_history(lgbm_study)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:10:32.992062Z","iopub.execute_input":"2022-06-03T06:10:32.992954Z","iopub.status.idle":"2022-06-03T06:10:33.02251Z","shell.execute_reply.started":"2022-06-03T06:10:32.992895Z","shell.execute_reply":"2022-06-03T06:10:33.020081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyper-parameter들의 중요도\noptuna.visualization.plot_param_importances(lgbm_study)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:10:33.026167Z","iopub.execute_input":"2022-06-03T06:10:33.028921Z","iopub.status.idle":"2022-06-03T06:10:33.437539Z","shell.execute_reply.started":"2022-06-03T06:10:33.028689Z","shell.execute_reply":"2022-06-03T06:10:33.436573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trial = lgbm_study.best_trial\ntrial_params = trial.params\n\nfinal_lgb_model = LGBMRegressor(**trial_params)\nfinal_lgb_model.fit(X, y) # finalize model","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:10:33.439451Z","iopub.execute_input":"2022-06-03T06:10:33.44027Z","iopub.status.idle":"2022-06-03T06:10:34.072139Z","shell.execute_reply.started":"2022-06-03T06:10:33.440221Z","shell.execute_reply":"2022-06-03T06:10:34.071245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PCA 적용\nif feature_reducing == \"correlation\":\n    test = testX.drop(columns=correlated_features)\n    print(X.shape)\n    \nelif feature_reducing == \"feature_importance\":\n    test = testX[important_features]\n    print(X.shape)\n    \nelif feature_reducing == \"PCA\":\n    test = pca_model.transform(testX)\n    print(X.shape)\n    \nprediction = final_lgb_model.predict(test)\nsubmission[\"y\"] = prediction\ndisplay(submission)\nsubmission.reset_index(drop=True).to_csv(f\"lgbm_submission_{feature_reducing}.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-03T06:10:34.080048Z","iopub.execute_input":"2022-06-03T06:10:34.081467Z","iopub.status.idle":"2022-06-03T06:10:34.202158Z","shell.execute_reply.started":"2022-06-03T06:10:34.08139Z","shell.execute_reply":"2022-06-03T06:10:34.201217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}