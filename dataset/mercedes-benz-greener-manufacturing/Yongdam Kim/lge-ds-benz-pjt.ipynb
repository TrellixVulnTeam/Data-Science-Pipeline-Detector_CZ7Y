{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Machine Learning 프로젝트 수행을 위한 코드 구조화 ","metadata":{}},{"cell_type":"markdown","source":"- ML project를 위해서 사용하는 템플릿 코드를 만듭니다.\n\n1. **필요한 라이브러리와 데이터를 불러옵니다.**\n\n\n2. **EDA를 수행합니다.** 이 때 EDA의 목적은 풀어야하는 문제를 위해서 수행됩니다.\n\n\n3. **전처리를 수행합니다.** 이 때 중요한건 **feature engineering**을 어떻게 하느냐 입니다.\n\n\n4. **데이터 분할을 합니다.** 이 때 train data와 test data 간의 분포 차이가 없는지 확인합니다.\n\n\n5. **학습을 진행합니다.** 어떤 모델을 사용하여 학습할지 정합니다. 성능이 잘 나오는 GBM을 추천합니다.\n\n\n6. **hyper-parameter tuning을 수행합니다.** 원하는 목표 성능이 나올 때 까지 진행합니다. 검증 단계를 통해 지속적으로 **overfitting이 되지 않게 주의**하세요.\n\n\n7. **최종 테스트를 진행합니다.** 데이터 분석 대회 포맷에 맞는 submission 파일을 만들어서 성능을 확인해보세요.","metadata":{}},{"cell_type":"markdown","source":"## 1. 라이브러리, 데이터 불러오기","metadata":{}},{"cell_type":"code","source":"# 데이터분석 4종 세트\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 모델들, 성능 평가\n# (저는 일반적으로 정형데이터로 머신러닝 분석할 때는 이 2개 모델은 그냥 돌려봅니다. 특히 RF가 테스트하기 좋습니다.)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm.sklearn import LGBMRegressor\n\n# 상관관계 분석, VIF : 다중공선성 제거\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# KFold(CV), partial : optuna를 사용하기 위함\nfrom sklearn.model_selection import KFold\nfrom functools import partial\n\n# hyper-parameter tuning을 위한 라이브러리, optuna\nimport optuna","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:26.104595Z","iopub.execute_input":"2022-06-23T02:42:26.10521Z","iopub.status.idle":"2022-06-23T02:42:27.924294Z","shell.execute_reply.started":"2022-06-23T02:42:26.105069Z","shell.execute_reply":"2022-06-23T02:42:27.923267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# flag setting\nfeature_reducing = True","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:27.926911Z","iopub.execute_input":"2022-06-23T02:42:27.928343Z","iopub.status.idle":"2022-06-23T02:42:27.934076Z","shell.execute_reply.started":"2022-06-23T02:42:27.928276Z","shell.execute_reply":"2022-06-23T02:42:27.932653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 데이터를 불러옵니다.\ntrain = pd.read_csv(\"../input/mercedes-benz-greener-manufacturing/train.csv.zip\")\ntest = pd.read_csv('../input/mercedes-benz-greener-manufacturing/test.csv.zip')","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:27.93563Z","iopub.execute_input":"2022-06-23T02:42:27.93602Z","iopub.status.idle":"2022-06-23T02:42:28.271351Z","shell.execute_reply.started":"2022-06-23T02:42:27.935986Z","shell.execute_reply":"2022-06-23T02:42:28.270243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. EDA","metadata":{}},{"cell_type":"markdown","source":"- 데이터에서 찾아야 하는 기초적인 내용들을 확인합니다.\n\n\n- class imbalance, target distribution, outlier, correlation을 확인합니다.","metadata":{}},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:28.273241Z","iopub.execute_input":"2022-06-23T02:42:28.273612Z","iopub.status.idle":"2022-06-23T02:42:28.30689Z","shell.execute_reply.started":"2022-06-23T02:42:28.273577Z","shell.execute_reply":"2022-06-23T02:42:28.305943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## On your Own\n# 1. 데이터 크기 확인\ntrain.info() # 4209 x 378","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:28.30959Z","iopub.execute_input":"2022-06-23T02:42:28.310172Z","iopub.status.idle":"2022-06-23T02:42:28.342219Z","shell.execute_reply.started":"2022-06-23T02:42:28.310134Z","shell.execute_reply":"2022-06-23T02:42:28.340889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# 2. 결측치 체크 : column 방향으로 먼저 체크.\ntrain.columns[train.isnull().any()]  # 결측치 없음.\ntrain[train.isnull().any(axis=1)]  # 결측치 없음.","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:28.343778Z","iopub.execute_input":"2022-06-23T02:42:28.344162Z","iopub.status.idle":"2022-06-23T02:42:28.377432Z","shell.execute_reply.started":"2022-06-23T02:42:28.344122Z","shell.execute_reply":"2022-06-23T02:42:28.375942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:28.380398Z","iopub.execute_input":"2022-06-23T02:42:28.381606Z","iopub.status.idle":"2022-06-23T02:42:28.413812Z","shell.execute_reply.started":"2022-06-23T02:42:28.381548Z","shell.execute_reply":"2022-06-23T02:42:28.412492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3. dtype이 object인 column 체크. : categorical feature(가 될 가능성이 높은) 체크.\ncat_features = train.columns[train.dtypes == 'object']  # dtype이 object인 Column을 뽑음.\ntrain.loc[:, train.dtypes == 'object']   # dtype이 object인 모든 데이터를 뽑음.\n\ntrain.columns[10:] # 앞에 10개를 제외한 나머지 모든 column\n# 전체 column에서 \"ID\", \"y\", cat_features를 제외한 나머지 column\nbinary_features = list(set(train.columns) - set(cat_features) - {\"ID\", \"y\"})\n#binary_features = np.setdiff1d(train.columns, cat_features)\n\n# 순서를 안바꾸고, train.columns에서 ID, y, cat_feature를 제외한 나머지를 filtering하고 싶음.\n# 3-1. ID, y, cat_features를 하나로 합침.\narr = np.hstack([[\"ID\", \"y\"], cat_features])\n\n# 3-2. 위에서 만든 column들에 포함이 안되면이라는 조건 생성.\nmask = ~train.columns.isin(arr)\n\n# 3-3. masking\nbinary_features = train.columns[mask]","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:28.416399Z","iopub.execute_input":"2022-06-23T02:42:28.41735Z","iopub.status.idle":"2022-06-23T02:42:28.430666Z","shell.execute_reply.started":"2022-06-23T02:42:28.417288Z","shell.execute_reply":"2022-06-23T02:42:28.429445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4. target distribution : target value의 분포 체크.\nplt.figure(figsize=(12, 6))\nsns.histplot(data=train, x=\"y\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:28.432691Z","iopub.execute_input":"2022-06-23T02:42:28.433576Z","iopub.status.idle":"2022-06-23T02:42:28.820494Z","shell.execute_reply.started":"2022-06-23T02:42:28.433519Z","shell.execute_reply":"2022-06-23T02:42:28.818888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 5. 상관관계 분석 : correlation matrix + heatmap\ncorr = train.drop(columns=\"ID\").corr()\nplt.figure(figsize=(24, 24))\nsns.heatmap(data=corr, cmap=\"Blues\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:28.822089Z","iopub.execute_input":"2022-06-23T02:42:28.822694Z","iopub.status.idle":"2022-06-23T02:42:33.310387Z","shell.execute_reply.started":"2022-06-23T02:42:28.822652Z","shell.execute_reply":"2022-06-23T02:42:33.309081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이런 식으로 여러가지 그래프를 그려가며, 데이터에 대한 인사이트를 얻습니다!","metadata":{}},{"cell_type":"code","source":"## TO-DO - EDA, 이전 코드 복습!\n\nfor col in cat_features:\n    plt.figure(figsize=(12, 8))\n    sns.countplot(data=train, x=col)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:33.31207Z","iopub.execute_input":"2022-06-23T02:42:33.312484Z","iopub.status.idle":"2022-06-23T02:42:36.168633Z","shell.execute_reply.started":"2022-06-23T02:42:33.312442Z","shell.execute_reply":"2022-06-23T02:42:36.167386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## binary features들 중에서 모든 값이 동일한 feature는 학습에 도움이 되지 않기 때문에, 제거합니다.\ndrop_cols = []\nfor col in binary_features:\n    if train[col].nunique() == 1: # 해당 column에 전부 다 0이거나 1인 column들.\n        print(col)\n        drop_cols.append(col)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:36.170193Z","iopub.execute_input":"2022-06-23T02:42:36.170716Z","iopub.status.idle":"2022-06-23T02:42:36.207677Z","shell.execute_reply.started":"2022-06-23T02:42:36.170676Z","shell.execute_reply":"2022-06-23T02:42:36.206127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# binary_features에서 column별 합이 0이거나 전체 길이(4209)인 column들을 추출합니다.\nbinary_features[(train[binary_features].sum() == 0) | (train[binary_features].sum() == len(train))]","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:36.209317Z","iopub.execute_input":"2022-06-23T02:42:36.209807Z","iopub.status.idle":"2022-06-23T02:42:36.237487Z","shell.execute_reply.started":"2022-06-23T02:42:36.209755Z","shell.execute_reply":"2022-06-23T02:42:36.236146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. 전처리","metadata":{}},{"cell_type":"markdown","source":"#### 불필요한 column 처리","metadata":{}},{"cell_type":"code","source":"train = train.drop(columns=[\"ID\"] + drop_cols)\ntest = test.drop(columns=[\"ID\"] + drop_cols)\nprint(train.shape, test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:36.243729Z","iopub.execute_input":"2022-06-23T02:42:36.244799Z","iopub.status.idle":"2022-06-23T02:42:36.262761Z","shell.execute_reply.started":"2022-06-23T02:42:36.244746Z","shell.execute_reply":"2022-06-23T02:42:36.261779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Categorical Feature encoding\n\n- Ordinal Encoding VS One-Hot encoding\n\n\n- pandas VS sklearn","metadata":{}},{"cell_type":"code","source":"# pandas로 ordinal encoding\nencoding = \"ODE\"\n\n# train과 test 데이터를 한번에 처리하기 위해서, 두 데이터를 합쳐서 전처리를 수행합니다. (모든 category에 대한 학습이 필요함.)\ntotal = pd.concat([train, test])\n\nif encoding == 'ODE':\n    for f in cat_features:\n        total[f] = pd.factorize(total[f])[0]\n\n# pandas로 one-hot encoding\nelif encoding == 'OHE':\n    total = pd.get_dummies(data=total, columns=cat_features)\n    \ntrain = total[:len(train)]\ntest = total[len(train):].drop(columns='y')","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:36.264526Z","iopub.execute_input":"2022-06-23T02:42:36.265208Z","iopub.status.idle":"2022-06-23T02:42:36.31249Z","shell.execute_reply.started":"2022-06-23T02:42:36.265165Z","shell.execute_reply":"2022-06-23T02:42:36.311243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:36.315742Z","iopub.execute_input":"2022-06-23T02:42:36.316747Z","iopub.status.idle":"2022-06-23T02:42:36.345446Z","shell.execute_reply.started":"2022-06-23T02:42:36.316688Z","shell.execute_reply":"2022-06-23T02:42:36.343988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:36.347622Z","iopub.execute_input":"2022-06-23T02:42:36.348143Z","iopub.status.idle":"2022-06-23T02:42:36.374757Z","shell.execute_reply.started":"2022-06-23T02:42:36.348091Z","shell.execute_reply":"2022-06-23T02:42:36.373022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 다중공선성 처리","metadata":{}},{"cell_type":"code","source":"# 중복정보가 있는 column 제거하기 위해 상관계수를 확인해봅니다.\n\n\ncorr = train.drop(columns='y').corr()\ncorr\n\n# X0 <--> X2, X12, X356\n# X1 <--> ...\n# X2 (skip)\n# X3 \n...\n# X385 <--> ...","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-23T02:42:36.376945Z","iopub.execute_input":"2022-06-23T02:42:36.377887Z","iopub.status.idle":"2022-06-23T02:42:37.91508Z","shell.execute_reply.started":"2022-06-23T02:42:36.37782Z","shell.execute_reply":"2022-06-23T02:42:37.913784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"threshold = 0.7\nexcept_cols = []  # 이전에 이미 체크한 column들. (제거 대상으로 안봐도 되는 column들.)\nremove_cols = []  # 상관관계가 threshold를 넘어서 제거할 column들.\n\nfor col in corr:\n    if col in except_cols:  # 이미 체크를 한 column이라 지나갑니다.\n        continue\n    except_cols.append(col) # 같은 Column은 무조건 1이라서, 제외.\n    row = np.abs(corr.loc[col]) # correlation matrix의 row 하나\n    condition1 = row >= threshold  # 상관계수를 넘기는 column들.\n    condition2 = ~corr.columns.isin(except_cols) # 이전에 이미 봤기 때문에 넘어가도 되는 column.\n    temp = row[condition1 & condition2].index\n    except_cols = except_cols + list(temp)  # threshold를 넘긴 column들이라서 제외.\n    remove_cols = remove_cols + list(temp)  # 이전에 등장하지 않았던 새로운 Column들.","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:37.916973Z","iopub.execute_input":"2022-06-23T02:42:37.917364Z","iopub.status.idle":"2022-06-23T02:42:38.090385Z","shell.execute_reply.started":"2022-06-23T02:42:37.917327Z","shell.execute_reply":"2022-06-23T02:42:38.089117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(remove_cols)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:38.092325Z","iopub.execute_input":"2022-06-23T02:42:38.092845Z","iopub.status.idle":"2022-06-23T02:42:38.100112Z","shell.execute_reply.started":"2022-06-23T02:42:38.092767Z","shell.execute_reply":"2022-06-23T02:42:38.09859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VIF(Variance Inflation Factor)를 이용하여 다중공선성(서로 상관이 높은) column들을 제거합니다.\n# VIF가 1이라면, 다른 feature와 전혀 상관관계가 없고 그 때의 R^2는 0입니다.\n\n## 여기서는 Ordinal encoding이 된 X0 ~ X8까지만 체크합니다.\n\n#vifs = [variance_inflation_factor(train.drop(columns='y'), idx) for idx in range(len(train.columns)-1)]\nvifs = [variance_inflation_factor(train.drop(columns='y'), idx) for idx in range(8)]\n\n#vif_df = pd.DataFrame({\"Features\" : train.drop(columns='y').columns, \"VIF\" : vifs})\nvif_df = pd.DataFrame({\"Features\" : train.drop(columns='y').columns[:8], \"VIF\" : vifs})\nvif_df.sort_values(by=\"VIF\", ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:38.102448Z","iopub.execute_input":"2022-06-23T02:42:38.102937Z","iopub.status.idle":"2022-06-23T02:42:44.328221Z","shell.execute_reply.started":"2022-06-23T02:42:38.102885Z","shell.execute_reply":"2022-06-23T02:42:44.326636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# VIF가 threshold를 넘기는 feature들을 제거합니다.\nthreshold = 10 # <-> R2 score = 0.9\nvif_df[vif_df.VIF >= threshold].Features.values\n# skip.","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:44.33047Z","iopub.execute_input":"2022-06-23T02:42:44.331512Z","iopub.status.idle":"2022-06-23T02:42:44.346683Z","shell.execute_reply.started":"2022-06-23T02:42:44.331449Z","shell.execute_reply":"2022-06-23T02:42:44.344015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### feature extraction\n\n- 차원의 저주를 해결하거나, 데이터의 feature 조합을 이용하는 새로운 feature를 생성할 때, PCA를 사용합니다.\n\n- 분석에 사용할 feature를 선택하는 과정도 포함합니다.","metadata":{}},{"cell_type":"code","source":"# PCA 적용\nif feature_reducing:\n    from sklearn.decomposition import PCA\n    \n    #pca = PCA(n_components=15)   # 15차원으로 변환\n    pca = PCA(n_components=0.95)  # 원본 데이터의 정보를 90% 보존하는 차원으로 변환\n    train_pca = pca.fit_transform(train.drop(columns='y'))\n    print(train_pca)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:44.349039Z","iopub.execute_input":"2022-06-23T02:42:44.350318Z","iopub.status.idle":"2022-06-23T02:42:44.757857Z","shell.execute_reply.started":"2022-06-23T02:42:44.35022Z","shell.execute_reply":"2022-06-23T02:42:44.756497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_columns = [f\"PC{i+1}\" for i in range(train_pca.shape[1])]\npca_df = pd.DataFrame(data=train_pca, columns=pca_columns)\npca_df","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:44.760161Z","iopub.execute_input":"2022-06-23T02:42:44.761143Z","iopub.status.idle":"2022-06-23T02:42:44.809013Z","shell.execute_reply.started":"2022-06-23T02:42:44.761084Z","shell.execute_reply":"2022-06-23T02:42:44.807194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. 학습 데이터 분할","metadata":{}},{"cell_type":"code","source":"# 첫번째 테스트용으로 사용하고, 실제 학습시에는 K-Fold CV를 사용합니다.\nfrom sklearn.model_selection import train_test_split\n\nif feature_reducing:\n    X = pca_df\nelse:\n    X = train.drop(columns=['y'] + remove_cols)  # 학습에 사용하지 않을 column들을 drop.\ny = train.y\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0xC0FFEE)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:44.818802Z","iopub.execute_input":"2022-06-23T02:42:44.824194Z","iopub.status.idle":"2022-06-23T02:42:44.843873Z","shell.execute_reply.started":"2022-06-23T02:42:44.824066Z","shell.execute_reply":"2022-06-23T02:42:44.842273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:44.854207Z","iopub.execute_input":"2022-06-23T02:42:44.859584Z","iopub.status.idle":"2022-06-23T02:42:44.867533Z","shell.execute_reply.started":"2022-06-23T02:42:44.859354Z","shell.execute_reply":"2022-06-23T02:42:44.866021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. 학습 및 평가","metadata":{}},{"cell_type":"code","source":"# 간단하게 LightGBM 테스트\n# 적당한 hyper-parameter 조합을 두었습니다. (항상 best는 아닙니다. 예시입니다.)\n\nparam_grid = {\n    \"max_bin\" : 20,\n    \"learning_rate\" : 0.0025,\n    \"objective\" : \"regression\",\n    \"boosting_type\" : \"gbdt\",\n    \"metric\" : \"mae\",\n    \"sub_feature\" : 0.345,\n    \"bagging_fraction\" : 0.85,\n    \"bagging_freq\" : 40,\n    \"num_leaves\" : 512,\n    \"min_data\" : 500,\n    \"min_hessian\" : 0.05,\n    \"verbose\" : 2,\n    \"feature_fraction_seed\" : 2,\n    \"bagging_seed\" : 3\n}\n\nmodel = LGBMRegressor(**param_grid)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:44.871051Z","iopub.execute_input":"2022-06-23T02:42:44.873045Z","iopub.status.idle":"2022-06-23T02:42:44.886237Z","shell.execute_reply.started":"2022-06-23T02:42:44.872964Z","shell.execute_reply":"2022-06-23T02:42:44.884193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\nFitting LightGBM...\")\nmodel.fit(X_train, y_train)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-23T02:42:44.889889Z","iopub.execute_input":"2022-06-23T02:42:44.890631Z","iopub.status.idle":"2022-06-23T02:42:45.259662Z","shell.execute_reply.started":"2022-06-23T02:42:44.89057Z","shell.execute_reply":"2022-06-23T02:42:45.258448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# metric은 그때마다 맞게 바꿔줘야 합니다.\nfrom sklearn.metrics import r2_score\n#from sklearn.metrics import mean_squared_error\n\nevaluation_metric = r2_score","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:45.264378Z","iopub.execute_input":"2022-06-23T02:42:45.265745Z","iopub.status.idle":"2022-06-23T02:42:45.27193Z","shell.execute_reply.started":"2022-06-23T02:42:45.265682Z","shell.execute_reply":"2022-06-23T02:42:45.27066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Prediction\")\npred_train = model.predict(X_train)\npred_test = model.predict(X_test)\n\n\ntrain_score = evaluation_metric(y_train, pred_train)\ntest_score = evaluation_metric(y_test, pred_test)\n\nprint(\"Train Score : %.4f\" % train_score)\nprint(\"Test Score : %.4f\" % test_score)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:42:45.273551Z","iopub.execute_input":"2022-06-23T02:42:45.274077Z","iopub.status.idle":"2022-06-23T02:42:45.369686Z","shell.execute_reply.started":"2022-06-23T02:42:45.274024Z","shell.execute_reply":"2022-06-23T02:42:45.368337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Hyper-parameter Tuning","metadata":{}},{"cell_type":"markdown","source":"> GridSearchCV","metadata":{}},{"cell_type":"markdown","source":"** LightGBM의 hyperparameter **\n\n[Official Documentation] https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html \n\n[Blog 1] https://smecsm.tistory.com/133\n\n[Blog 2] https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\n\n[Blog 3] https://nurilee.com/2020/04/03/lightgbm-definition-parameter-tuning/","metadata":{}},{"cell_type":"code","source":"# GridSearchCV를 이용하여 가장 좋은 성능을 가지는 모델을 찾아봅시다. (이것은 첫번째엔 선택입니다.)\n# Lightgbm은 hyper-parameter의 영향을 많이 받기 때문에, 저는 보통 맨처음에 한번 정도는 가볍게 GCV를 해봅니다.\n# 성능 향상이 별로 없다면, lightgbm으로 돌린 대략적인 성능이 이 정도라고 생각하면 됩니다.\n# 만약 성능 향상이 크다면, 지금 데이터는 hyper-parameter tuning을 빡빡하게 하면 성능 향상이 많이 이끌어 낼 수 있습니다.\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"max_depth\" : [8, 16, None],\n    \"n_estimators\" : [100, 300, 500],\n    \"max_bin\" : [20],\n    \"learning_rate\" : [0.001, 0.0025, 0.003],\n    \"objective\" : [\"regression\"],\n    \"boosting_type\" : [\"gbdt\"],\n    \"metric\" : [\"mae\"],\n    \"sub_feature\" : [0.345],\n    \"bagging_fraction\" : [0.7, 0.75, 0.85],\n    \"bagging_freq\" : [40],\n    \"num_leaves\" : [256, 512],\n    \"min_data\" : [500],\n    \"verbose\" : [-1], # 필수\n    \"min_hessian\" : [0.05],\n    \"feature_fraction_seed\" : [2],\n    \"bagging_seed\" : [3]\n}  # 3 x 3 x 3 x 3 x 2 = 162 ---> GridSearch가 탐색하는 Hyper-parameter 조합 수.\n\n# CV(Cross-Validation) : KFold(K등분)\ngcv = GridSearchCV(estimator=model, param_grid=param_grid, cv=5,\n                  n_jobs=-1, verbose=1)\n\ngcv.fit(X_train, y_train)  # 5 x 162 = 810.\nprint(\"Best Estimator : \", gcv.best_estimator_) ## 162개의 hyper-parameter 조합 중에, 5개의 서로 다른 데이터로 평가한 평균 성능이 가장 좋은 모델.","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-06-23T02:42:45.371691Z","iopub.execute_input":"2022-06-23T02:42:45.380684Z","iopub.status.idle":"2022-06-23T02:43:19.834664Z","shell.execute_reply.started":"2022-06-23T02:42:45.380611Z","shell.execute_reply":"2022-06-23T02:43:19.833334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Prediction with Best Estimator\")\ngcv_pred_train = gcv.predict(X_train)\ngcv_pred_test = gcv.predict(X_test)\n\ngcv_train_score = evaluation_metric(y_train, gcv_pred_train)\ngcv_test_score = evaluation_metric(y_test, gcv_pred_test)\n\nprint(\"Train R2 Score : %.4f\" % gcv_train_score)\nprint(\"Test R2 Score : %.4f\" % gcv_test_score)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:19.839909Z","iopub.execute_input":"2022-06-23T02:43:19.841884Z","iopub.status.idle":"2022-06-23T02:43:19.896094Z","shell.execute_reply.started":"2022-06-23T02:43:19.841819Z","shell.execute_reply":"2022-06-23T02:43:19.895057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Performance Gain\") # 이걸로 성능 향상 확인.\nprint(\"in train : \", -(train_score - gcv_train_score))\nprint(\"in test : \", -(test_score - gcv_test_score))","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:19.901198Z","iopub.execute_input":"2022-06-23T02:43:19.904483Z","iopub.status.idle":"2022-06-23T02:43:19.912507Z","shell.execute_reply.started":"2022-06-23T02:43:19.904422Z","shell.execute_reply":"2022-06-23T02:43:19.911209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> optuna를 사용해봅시다 !","metadata":{}},{"cell_type":"code","source":"def optimizer(trial, X, y, K):\n    # 조절할 hyper-parameter 조합을 적어줍니다.\n#     n_estimators = trial.suggest_int('n_estimators', 100, 200)\n#     max_depth = trial.suggest_int('max_depth', 4, 8)\n#     #max_features = trial.suggest_categorical('max_features', ['sqrt', 'auto', 'log2'])\n#     max_features = trial.suggest_float('max_features', 0.5, 0.8)\n    \n    \n#     # 원하는 모델을 지정합니다, optuna는 시간이 오래걸리기 때문에 저는 보통 RF로 일단 테스트를 해본 뒤에 LGBM을 사용합니다.\n#     model = RandomForestRegressor(n_estimators=n_estimators,\n#                                  max_depth=max_depth,\n#                                  max_features=max_features)\n\n    max_depth = trial.suggest_int('max_depth', 5, 20)\n    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n    max_bin = trial.suggest_int('max_bin', 20, 100)\n    learning_rate = trial.suggest_float('learning_rate', 0.001, 0.01)\n    \n    model = LGBMRegressor(max_depth=max_depth,\n                          n_estimators=n_estimators,\n                          max_bin=max_bin,\n                          learning_rate=learning_rate)\n    \n    \n    # K-Fold Cross validation을 구현합니다.\n    folds = KFold(n_splits=K)\n    losses = []\n    \n    for train_idx, val_idx in folds.split(X, y):\n        X_train = X.iloc[train_idx, :]\n        y_train = y.iloc[train_idx]\n        \n        X_val = X.iloc[val_idx, :]\n        y_val = y.iloc[val_idx]\n        \n        model.fit(X_train, y_train)\n        preds = model.predict(X_val)\n        loss = evaluation_metric(y_val, preds)\n        losses.append(loss)\n    \n    \n    # K-Fold의 평균 loss값을 돌려줍니다.\n    return np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:19.914093Z","iopub.execute_input":"2022-06-23T02:43:19.914494Z","iopub.status.idle":"2022-06-23T02:43:19.933669Z","shell.execute_reply.started":"2022-06-23T02:43:19.914455Z","shell.execute_reply":"2022-06-23T02:43:19.932343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"K = 5 # Kfold 수\nopt_func = partial(optimizer, X=X_train, y=y_train, K=K)\n\nstudy = optuna.create_study(study_name='LGBM', direction=\"maximize\") # 최소/최대 어느 방향의 최적값을 구할 건지. # R2 score는 커질수록 좋음.\nstudy.optimize(opt_func, n_trials=1000)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:19.935515Z","iopub.execute_input":"2022-06-23T02:43:19.936005Z","iopub.status.idle":"2022-06-23T02:43:26.505133Z","shell.execute_reply.started":"2022-06-23T02:43:19.935954Z","shell.execute_reply":"2022-06-23T02:43:26.504062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optuna가 시도했던 모든 실험 관련 데이터\nstudy.trials_dataframe()","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:26.510249Z","iopub.execute_input":"2022-06-23T02:43:26.512986Z","iopub.status.idle":"2022-06-23T02:43:26.539667Z","shell.execute_reply.started":"2022-06-23T02:43:26.512924Z","shell.execute_reply":"2022-06-23T02:43:26.53813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best Score: %.4f\" % study.best_value) # best score 출력\nprint(\"Best params: \", study.best_trial.params) # best score일 때의 하이퍼파라미터들","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:26.543735Z","iopub.execute_input":"2022-06-23T02:43:26.544152Z","iopub.status.idle":"2022-06-23T02:43:26.553604Z","shell.execute_reply.started":"2022-06-23T02:43:26.544115Z","shell.execute_reply":"2022-06-23T02:43:26.552368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 실험 기록 시각화\noptuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:26.555777Z","iopub.execute_input":"2022-06-23T02:43:26.556778Z","iopub.status.idle":"2022-06-23T02:43:26.613715Z","shell.execute_reply.started":"2022-06-23T02:43:26.556694Z","shell.execute_reply":"2022-06-23T02:43:26.612341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyper-parameter들의 중요도\noptuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:26.615484Z","iopub.execute_input":"2022-06-23T02:43:26.61587Z","iopub.status.idle":"2022-06-23T02:43:26.93511Z","shell.execute_reply.started":"2022-06-23T02:43:26.615833Z","shell.execute_reply":"2022-06-23T02:43:26.933339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 7. 테스트 및 제출 파일 생성\n\n- prediction using test data\n\n\n1. test data를 train data와 같은 방식으로 전처리\n\n\n2. 학습한 모델로 test data 예측\n\n\n3. submission 파일에 예측 결과를 저장\n\n\n4. submission 파일 제출","metadata":{}},{"cell_type":"code","source":"# 3. PCA\npca_test = pca.transform(test)\nX_test = pd.DataFrame(columns=pca_columns, data=pca_test)\nX_test","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:26.937572Z","iopub.execute_input":"2022-06-23T02:43:26.938175Z","iopub.status.idle":"2022-06-23T02:43:27.000552Z","shell.execute_reply.started":"2022-06-23T02:43:26.938125Z","shell.execute_reply":"2022-06-23T02:43:26.999105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1) optuna에서 찾은 best hyper-parameter 조합으로 학습 후, 예측.  (v)\n# 2) optuna가 학습을 할 때, best hyper-parameter를 찾았던 모델을 가져와서 에측.\n# model = RandomForestRegressor(n_estimators=study.best_trial.params[\"n_estimators\"],\n#                                  max_depth=study.best_trial.params[\"max_depth\"],\n#                                  max_features=study.best_trial.params[\"max_features\"])\n\n    \nmodel = LGBMRegressor(max_depth=study.best_trial.params['max_depth'],\n                      n_estimators=study.best_trial.params['n_estimators'],\n                      max_bin=study.best_trial.params['max_bin'],\n                      learning_rate=study.best_trial.params['learning_rate'])\n\n\nmodel.fit(X, y) # 전체 데이터로 학습.  ## finalize\npreds = model.predict(X_test)\npreds","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:27.013808Z","iopub.execute_input":"2022-06-23T02:43:27.015124Z","iopub.status.idle":"2022-06-23T02:43:27.280796Z","shell.execute_reply.started":"2022-06-23T02:43:27.015024Z","shell.execute_reply":"2022-06-23T02:43:27.279646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.shape, preds.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:27.285643Z","iopub.execute_input":"2022-06-23T02:43:27.286468Z","iopub.status.idle":"2022-06-23T02:43:27.296961Z","shell.execute_reply.started":"2022-06-23T02:43:27.286422Z","shell.execute_reply":"2022-06-23T02:43:27.296002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/mercedes-benz-greener-manufacturing/sample_submission.csv.zip') # submission을 생성합니다.\nsubmission['y'] = preds\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:27.298308Z","iopub.execute_input":"2022-06-23T02:43:27.298852Z","iopub.status.idle":"2022-06-23T02:43:27.333649Z","shell.execute_reply.started":"2022-06-23T02:43:27.298816Z","shell.execute_reply":"2022-06-23T02:43:27.332354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.reset_index(drop=True).to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-23T02:43:27.338163Z","iopub.execute_input":"2022-06-23T02:43:27.338591Z","iopub.status.idle":"2022-06-23T02:43:27.364556Z","shell.execute_reply.started":"2022-06-23T02:43:27.338555Z","shell.execute_reply":"2022-06-23T02:43:27.363479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}