{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9f88a0a0-b4ac-9b4f-45d9-a203066ac51a"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Any results you write to the current directory are saved as output.\ntrain = pd.read_csv('../input/train.csv')\ntrain['Src'] = \"train\"\ntest = pd.read_csv('../input/test.csv')\ntest['Src'] = \"test\"\ncomb = pd.concat([train,test],0)\n\nprint (\"Stage0 shape: {0}\".format(comb.shape))\n#STEP1: Dummy set encoding of the categorical fields\ncatfields = []\nframes = [comb]\n\nfor c in comb.drop(['ID','y','Src'],1).columns:\n    if train[c].dtype == 'object':\n        catfields.append(c)\n        tempdf = pd.get_dummies(comb[c],prefix=c)\n        frames.append(tempdf)\n        \ncomb2 = pd.concat(frames,axis=1).drop(catfields, 1)\nprint (\"Stage1 Shape: {0}\".format(comb2.shape))\n\n#STEP2: Remove fields with little or no information\n#TFPE: Too few posititve entries (proportion)\ntfpe = 0.05\n\nproblem_fields = []\nfor c in comb2.drop(['ID','y','Src'],1).columns:\n    uniq = len(np.unique(comb2[c]))\n    mv = comb2[c].mean()\n    if ((uniq == 1) or (mv < tfpe)): problem_fields.append(c)\n\ncomb3 = comb2.drop(problem_fields, 1)\nprint (\"Stage3 Shape: {0}\".format(comb3.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b907adb-a41a-bdef-78c3-4c03db4a1e11"},"outputs":[],"source":"from sklearn.decomposition import PCA\npca2 = PCA(n_components=35)\npca2_results = pca2.fit_transform(comb3.drop(['ID','y','Src'],1))\n\n\neigvals = pca2.explained_variance_\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=(8,5))\nsing_vals = np.arange(len(eigvals)) + 1\nplt.plot(sing_vals, eigvals, 'ro-', linewidth=2)\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Eigenvalue')\nleg = plt.legend(['Eigenvalues from PCA'], loc='best', borderpad=0.3, \n shadow=False, prop=matplotlib.font_manager.FontProperties(size='small'),\n markerscale=0.4)\nleg.get_frame().set_alpha(0.4)\nleg.draggable(state=True)\nplt.show()\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"46451bcd-99aa-71e9-df10-734313e35527"},"source":"To eye, around 16 components appears to be about right for this data."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"95b2ae9c-cca2-65a4-ef14-70708f13c60b"},"outputs":[],"source":"#Stage 4: Apply the PCA results\npcacols = ['ID','y','Src']\nfor i in range(16):\n    cn = 'pca' + str(i)\n    comb3[cn]=pca2_results[:,i]\n    pcacols.append(cn)\n\ncomb4 = comb3[pcacols]\nprint (\"Stage4 Shape: {0}\".format(comb4.shape))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ef5bf3b7-1391-6337-4ad1-1b2880b40521"},"outputs":[],"source":"\"\"\"\nProposed Steps for the regression model\nBox Cox model to assess if transformatins required\nBuild initial model\nCalculate the pseudo cooks distance and look for over influential cases\nVIF, Although we have orthogonal PCA so should not be a problem\ntest regression assumptions:\n    Linear relationship (look at box cox results).\n    Multivariate normality.\n    No or little multicollinearity.\n    No auto-correlation.\n    Homoscedasticity.\nBuild final model\n\"\"\"\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled = scaler.fit_transform(comb4[pcacols[-16:]]) + 0.001\nscaleddf = pd.DataFrame(scaled)                              \n\nbclst = ['ID','y','Src']\nlmbdalst = []\nfor p in range(16):\n    X , ll = stats.boxcox(scaleddf[p])\n    lmbdalst.append(ll)\n    a = 'pca_bc' + str(p)\n    bclst.append(a)\n    comb4[a] = X \nprint (lmbdalst)\ncomb5 = comb4[bclst]\n\nprint (\"Stage5 Shape: {0}\".format(comb5.shape))\nprint (comb5.columns)\n\nx1=comb5['pca_bc14'].head(250)\nx2=comb4['pca14'].head(250)\ny=comb5['y'].head(250)\n\nfig = plt.figure()\nax1 = fig.add_subplot(121)\nax1.scatter(x1, y)\nax2 = fig.add_subplot(122)\nax2.scatter(x2, y)"},{"cell_type":"markdown","metadata":{"_cell_guid":"111f94c2-de9d-d0a9-13d7-2f6a6f17676f"},"source":"Lambdas are not very large, so box-cox is not really doing much in this case\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bbb92a7d-1417-0271-19e6-4e8d94de9597"},"outputs":[],"source":"#Split back out the training and test set\ntrain = comb5[comb4['Src'] == 'train'].drop('Src',1)\ntest = comb5[comb4['Src'] == 'test'].drop('Src',1)\n\nprint (\"Stage5a; Train Shape: {0}, Test Shape: {1}\".format(train.shape,test.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"633b78ec-1e72-9f08-a277-8981369b35ac"},"source":"Note: We will not be splitting the training data up further into training and validation sets as we are aiming to build a statistical model so, our validation of the model will be through testing the assumptions underpinning a linear regression model\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0a51a224-db98-f897-96ea-949ebad1cb8e"},"outputs":[],"source":"#Evaluate an initial model\n\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n\n#Generate the model string\nst = \"y ~ \"\nfor f in bclst[-16:]:  st = st + f + \" + \"\n    \n#Fit the model\nmodel = ols(st[:-3], train).fit()\n\n#Print the summary\nprint(model.summary())\n\nprint(\"\\nparameter estimates:\")\nprint(model._results.params)\n#analysis of variance on fitted linear model\nanova_results = anova_lm(model)\nprint('\\nANOVA results')\nprint(anova_results)\n#analysis of outliers\nscore = model.outlier_test()\noutliers = (i for i,t in enumerate(score[\"bonf(p)\"]) if t < 0.6 )\noutlierslist = (list(outliers))\nprint ('\\nOutliers: ', len(outlierslist))\nprint (outlierslist)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"f3862e6e-2372-71d4-2445-0965bfc42823"},"source":"Durbin-Watson Shows no sign of auto-correlation in the model\n\nJarque-Bera Test shows we have no sign that the data is not multivariate normal\n\nCondition number is borderline, might want to consider fewer components"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f727bc93-55fc-fdc5-9a6e-43d892cca692"},"outputs":[],"source":"#And some regression plots to help us evaluate the model\n%matplotlib inline\nimport statsmodels.api as sm\n#Influence Plot\n#--------------\nfig, ax = plt.subplots(figsize=(12,8))\nfig = sm.graphics.influence_plot(model, ax=ax, criterion=\"cooks\")\n\n#This confirms what we saw in above bonf(p) list >> we are happy with out outlier list\n\n#Homoscedasticity\n#-------------\nyhat = model.fittedvalues\nehat = model.resid\nfig, ax1 = plt.subplots(figsize=(12,8))\nax1.scatter(yhat, ehat)\n\n#Shows some evidence of homoscedasticity, but not severe enough to worry about\n#More evidence of the outlier;"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1eabf8f-8737-80f4-1828-80943326abed"},"outputs":[],"source":"#Build a Second model.\n#Not much work required, outliers mainly\n\ntrain2 = train.drop(train.index[outlierslist])\n\n#Fit the model\nmodel2 = ols(st[:-3], train2).fit()\n\n#Print the summary\nprint(model2.summary())\n\nprint(\"\\nparameter estimates:\")\nprint(model2._results.params)\n#analysis of variance on fitted linear model\nanova_results = anova_lm(model2)\nprint('\\nANOVA results')\nprint(anova_results)\n#analysis of outliers\nscore = model2.outlier_test()\noutliers = (i for i,t in enumerate(score[\"bonf(p)\"]) if t < 0.05 )\noutlierslist2 = (list(outliers))\nprint ('\\nOutliers: ', len(outlierslist2))\nprint (outlierslist2)\nyhat = model2.fittedvalues\nehat = model2.resid\nfig, ax1 = plt.subplots(figsize=(12,8))\nax1.scatter(yhat, ehat)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ec17542-e498-ca99-c911-a9f4a629cabf"},"outputs":[],"source":"#Final Build\n\ntrain3 = train2.drop(train.index[outlierslist2])\n\n#Fit the model\nmodel3 = ols(st[:-3], train3).fit()\n\n#Print the summary\nprint(model3.summary())\n\nprint(\"\\nparameter estimates:\")\nprint(model3._results.params)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79190bed-5de7-1339-ac86-fc72ac4aa0f9"},"outputs":[],"source":"#Score up the test set \n\ny_pred = model3.predict(test)\noutput = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\noutput.to_csv('RH_SUB1_Baseline.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1b086963-a6fb-dcf4-1cf9-16ea7265f3d7"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}