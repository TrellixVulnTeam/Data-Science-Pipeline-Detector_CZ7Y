{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2bdcaf35-4cb5-b404-d8d6-6926db172f00"},"source":"First: the columns X0-X8\n------------------------\n\nwe try to find if there is a match between those data , lets construct an artificial key value, and look if there is a match between train - test\n\n - the length of test and train matches perfectly, thats probably a balancing\n-linking on those artifical  key we don't match, so we can't use it to transfer the time\n\n**- although its clear that the ID's are not matching...in that sence they simply splitted the database in two parts and have the stats at hand**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"09406dcd-c4a7-10af-646f-ed09b90b4aa5"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\ndef grouper (l0,l1,l2,l3,l4,l5,l6,l8):\n    gr=l0+'0 '+l1+'1 '+l2+'2 '+l3+'3 '+l4+'4 '+l5+'5 '+l6+'6 '+l8+'8 '\n    return gr\n    \n\ndf_train = pd.read_csv('../input/train.csv')\nprint('Size of training set: {} rows and {} columns'.format(*df_train.shape))\nprint('TRAIN',df_train.head(7))\nprint('record 252',df_train.iloc[252][['ID','X0','X1','X2','X3','X4','X5','X6','X8']])\n# sort smallest first\ndf_train=df_train.sort_values(by='y', axis=0)  \nbins= [c for c in df_train.columns if 'X' in c]\nprint('Number of features TRAIN: {}'.format(len(bins)))\n\n# grouping all types together\ndf_train['keys']=df_train[['X0','X1','X2','X3','X4','X5','X6','X8']].apply(lambda x: grouper(*x), axis=1)  \ndf_train_gr=df_train.groupby(by='keys', axis=0).mean()\n#print(df_train_gr.sort_values(by='y',axis=0))\n\n\n\n#reading test___________________________\ndf_test = pd.read_csv('../input/test.csv')\nprint('Size of  TEST: {} rows and {} columns'.format(*df_test.shape))\nprint('Test',df_test.head(7))\nprint('record 252 in test',df_test.iloc[251][['ID','X0','X1','X2','X3','X4','X5','X6','X8']])\nprint('record 252 in test',df_test.ix[df_test['ID']==504][['ID','X0','X1','X2','X3','X4','X5','X6','X8']])\n# grouping all types together\ndf_test['keys']=df_test[['X0','X1','X2','X3','X4','X5','X6','X8']].apply(lambda x: grouper(*x), axis=1)  \n\n\n#merge two databases____on artificial keys_________________________\nresult = pd.merge(df_test,df_train, how='left', on='keys', left_on=None, right_on=None,\n         left_index=False, right_index=False, sort=True,\n         suffixes=['_te', '_tr'], copy=True, indicator=False)\nprint(result.shape)\n\n# Benching with 1-1 linking > gives NO  >1-1 link\nsub = pd.DataFrame()\nsub['ID'] = result.ID_te\nsub['y'] = result.y\nsub2 = sub.groupby(by='ID',axis=0).min()\nprint('transfer time estimate through link on key', sub2.T)\n\nresult = pd.merge(df_test,df_train, how='left', on='ID',suffixes=['_te', '_tr'], copy=True, indicator=False)\n\nprint(result[['keys_tr','keys_te','ID']].head())\nprint(result.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"55499458-66f7-e979-7fdf-fcae20e36ed9"},"source":"CORRELATION ?\n-------------\n\nYes there is correlation between the data\n\nThe correlation of the Y-time versus the data can be very positive, and negative...\nthe negative correlated parameters like X127 will reduce the time spend on the car-tests...\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ebab082-9879-000f-cded-38d0e81e72fa"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\ncorrtest=bins+['y']\n#correlation matrix\ncorrm=df_train[corrtest].corr()\nplt.imshow(corrm,\n           interpolation='nearest', cmap=cm.gist_rainbow)\nplt.show()\n#some rows have no data\n#print(np.diag(corrm))\nprint(len(corrm))\n#sort on y\ncorry=corrm[len(corrm)-1:len(corrm)]\ncorry=corry.T.sort_values(by='y',axis=0)\n\nax = corry.plot(kind='bar', title =\"Y\", figsize=(15, 10), legend=True, fontsize=12)\n# so the uncorrelated to y are the time gaining components...\nax = corry[0:30].plot(kind='bar', title =\"Y\", figsize=(15, 10), legend=True, fontsize=12)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1b335493-a97b-a8b6-2400-729dd8beef15"},"source":"Is there on average a difference between the X-columns in the database TEST versus TRAIN ?\n========================================================================\n\nvisually there is no big difference\nso fe the X127 that is negative correlated will help nearly the most to reduce the time\nand the data learned from the training set should be useable for the test set"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69273053-7845-77ad-3b20-c5d07331b9a0"},"outputs":[],"source":"from numpy import corrcoef, sum, log, arange\nusable_columns = list(set(df_train.columns) - set(['ID', 'y','X0', 'X6', 'X3', 'X4', 'X8', 'X2', 'keys', 'X5', 'X1']))\n\ntrainstats=df_train[usable_columns].describe().T\n\nprint(trainstats.head())\ntrainstats['count']=trainstats['count']/4209\ntrainstatsort=trainstats.sort_values(by='mean',axis=0)\n\nteststats=df_test[usable_columns].describe().T\nprint(teststats.head())\nteststats['count']=teststats['count']/4209\nteststatsort=teststats.sort_values(by='mean',axis=0)\n\nstats=pd.merge(trainstats, teststats, how='inner', on=None, left_on=None, right_on=None,\n         left_index=True, right_index=True, sort=False,\n         suffixes=('_tr', '_te'), copy=True, indicator=False)\nstatssort=stats.sort_values(by='mean_tr')\n#print(stats)\n\nstatssort[['mean_tr','mean_te']].tail(50).plot(kind='bar', title =\"Y\", figsize=(15, 10), legend=True, fontsize=12)\n\nvcovm=np.cov(df_train[usable_columns].T,df_train['y'].T)\ndf_vcm=pd.DataFrame(vcovm)\ndiagonalen=pd.DataFrame(np.diag(df_vcm))\n\nstatssort[['std_tr','std_te']].tail(50).plot(kind='bar', title =\"Y\", figsize=(15, 10), legend=True, fontsize=12)"},{"cell_type":"markdown","metadata":{"_cell_guid":"977195ed-d903-9760-1116-a8dbb493c9ab"},"source":"Lets treat the artificial key column as a tfidf txt..\n----\nwe found a big difference in paired values between train and test"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"624df0ab-5cf5-8ef1-b851-28bc48d97548"},"outputs":[],"source":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer( ngram_range=(1,2))\ntfidftest=tfidf.fit_transform(df_train['keys'].append(df_test['keys']))\n\nprint(tfidf)\n\n#tfidf2 = TfidfVectorizer( ngram_range=(1,2))\n#tfidf2.fit_transform(df_test['keys'])\n#print(tfidf2)\n\nwordnr = pd.DataFrame(pd.Series(list(tfidf.vocabulary_.keys()), index=tfidf.vocabulary_.values()),columns=['woord']) # #print(words)  #woordenboek !\nwords=np.asarray(wordnr.woord)\n\n#wordnr2 = pd.DataFrame(pd.Series(list(tfidf2.vocabulary_.keys()), index=tfidf2.vocabulary_.values()),columns=['woord']) # #print(words)  #woordenboek !\n#words2=np.asarray(wordnr2.woord)\n\n#are there words not existing in the test ?\n#print([w for w in words2 if w not in words])\n#alot of new unique combinations\n\ndf_train[df_train['keys'].str.contains('az0 s1 as2')]\ndf_test[df_test['keys'].str.contains('az0 s1 as2')]\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"19e8065e-761a-0587-ffbd-7402382d9fa9"},"source":"Find singulars from the keys\n----------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f5aa550-cdb0-8966-ee3a-0206f23628d0"},"outputs":[],"source":"from sklearn.decomposition import TruncatedSVD\n\n\nprint('svd')     \nsvd = TruncatedSVD(n_components=50, n_iter=30, random_state=42)\ntempi=pd.DataFrame(svd.fit_transform(tfidf.transform(df_train['keys'])))\ntempi.rename(columns=lambda x: str(x)+'_i', inplace=True) #nog eens zoeken omcolumns te renamen\nprint('before',df_train.shape)\ndf_train=df_train.join(tempi,how='inner')\nprint('tempi',tempi.shape)\nprint('joined with tempi',df_train.shape)\n\n\nprint('svd2')     \nsvd2 = TruncatedSVD(n_components=50, n_iter=30, random_state=42)\ntemp2=pd.DataFrame(svd2.fit_transform(tfidf.transform(df_test['keys'])))\ntemp2.rename(columns=lambda x: str(x)+'_i', inplace=True) #nog eens zoeken omcolumns te renamen\ndf_test=df_test.join(temp2,how='inner')\nprint('test before',df_test.shape)\nprint('temp2',temp2.shape)\nprint('joined with temp2',df_test.shape)"},{"cell_type":"markdown","metadata":{"_cell_guid":"cb579250-2c63-283c-0da3-022de2c1c067"},"source":"X314\n---\nmost positive correlated with y, separates clearly the time\nit's one of the most powerfull separators, or when this option is installed it takes alot of time to check...\nits an option that is installed in 30% of the cars... thats sounds like fe manual versus automatic, or benzin versus gasoil"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ad6c187-ba38-68af-ecd8-a3b378b7ab2b"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set()\nsns.pairplot(df_train[['X314','X261','X263','0_i','1_i','2_i','3_i','4_i','5_i','X232','X29','X279','y']], hue='X314')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"761565ca-d2f8-f188-aefe-4fdb2c51d24b"},"outputs":[],"source":"Option X279 is a huge timegainer, its installed in a minority of carrs"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"68d6b771-ea35-e4cd-d054-08b027c66bc2"},"outputs":[],"source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set()\nsns.pairplot(df_train[['X314','X261','X263','0_i','1_i','2_i','3_i','4_i','5_i','X232','X29','X279','y']], hue='X279')"},{"cell_type":"markdown","metadata":{"_cell_guid":"fd945a3e-f731-211e-3305-0ed97569b79d"},"source":"XGB\n---"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9eab8798-e45e-dfd6-3eda-03042d32e5a3"},"outputs":[],"source":"y_train = df_train['y'].values\n\nusable_columns = list(set(df_train.columns) - set(['ID', 'y','X0', 'X6', 'X3', 'X4', 'X8', 'X2', 'keys', 'X5', 'X1']))\nx_train = df_train[usable_columns]\nx_test = df_test[usable_columns]\n\nimport xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=4242)\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix(x_test)\n\nparams = {}\nparams['objective'] = 'reg:gamma'\nparams['eta'] = 0.02\nparams['max_depth'] = 16\n\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nclf = xgb.train(params, d_train, 2000, watchlist, early_stopping_rounds=10, feval=xgb_r2_score, maximize=True, verbose_eval=10)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cd4d2bfc-c4fe-2954-b506-f5f95d3519eb"},"outputs":[],"source":"from sklearn.preprocessing import LabelEncoder\n\nfor c in df_train.columns:\n    if df_train[c].dtype == 'object':\n        lbl = LabelEncoder() \n        lbl.fit(list(df_train[c].values) + list(df_test[c].values)) \n        df_train[c] = lbl.transform(list(df_train[c].values))\n        df_test[c] = lbl.transform(list(df_test[c].values))\n        \nprint(df_train)\ndf_norm = (df_train - df_train.mean()) / (df_train.max() - df_train.min())\nprint(df_norm)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}