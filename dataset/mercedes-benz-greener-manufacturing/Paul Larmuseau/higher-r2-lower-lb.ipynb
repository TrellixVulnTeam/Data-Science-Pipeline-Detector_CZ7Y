{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"eb93dea8-a1de-3b81-deab-1261e8ed89a8"},"source":"imho we are doing a kind of internal organisational 'timemanagement'  where people are monitored...\nanyone has an explanation  how R2 are related with LB score ?\n\nbtw: the better we are clustering the R2 the less efficient we become iMHO."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"149e94b4-3c85-737f-6f9e-9d5531e19f72"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# read datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntest['y'] = 102  # to make append possible\nprint\ny_train = train[\"y\"]\ntotaal= train.append(test)\n#yx0_train = totaal[['y','X0']]\n#print(yx0_train.groupby('X0').mean().sort('y'))\n#temp=yx0_train.groupby('X0').mean().sort('y')\n#templ=temp.index\n#print(templ)\n#totaal['X0'].replace(to_replace=templ, value=[x for x in range(0,len(templ))], inplace=True, method='pad', axis=1)\nprint(totaal.head())\n# process columns, apply LabelEncoder to categorical features\nfor c in totaal.columns:\n    if totaal[c].dtype == 'object':\n        tempt = totaal[['y',c]]\n        temp=tempt.groupby(c).mean().sort('y')\n        templ=temp.index\n        print(templ)\n        aant=len(templ)\n        train[c].replace(to_replace=templ, value=[x/aant for x in range(0,aant)], inplace=True, method='pad', axis=1)\n        test[c].replace(to_replace=templ, value=[x/aant for x in range(0,aant)], inplace=True, method='pad', axis=1)\n         #test[c] = lbl.transform(list(test[c].values))\n\n# shape  \nprint(train.head())\nprint(test.head())\nprint('Shape train: {}\\nShape test: {}'.format(train.shape, test.shape))\n\n\n##Add decomposed components: PCA / ICA etc.\nfrom sklearn.decomposition import PCA, FastICA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation,FactorAnalysis,MiniBatchDictionaryLearning\nfrom sklearn.cluster import MiniBatchKMeans\nn_comp = 2\n#nmf\nnmf = NMF(n_components=n_comp, random_state=1,          alpha=.1, l1_ratio=.5)\nnmf_results_train=nmf.fit_transform(train.drop(['y'], axis=1))#,y=y_train)  #R2 +Y 0.44 #R2-Y 0.44\nnmf_results_test=nmf.transform(test.drop(['y'], axis=1))\n\n\n#lda\nlda = LatentDirichletAllocation(n_topics=n_comp, max_iter=5,                                learning_method='online',                                learning_offset=50.,                                random_state=0).fit(train.drop([\"y\"], axis=1),y_train)\nlda_results_train = lda.fit_transform(train.drop([\"y\"], axis=1),y_train ) #R2 +Y 0.40\nlda_results_test = lda.transform(test.drop(['y'], axis=1))\n\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=42)\ntsvd_results_train = tsvd.fit_transform(train.drop(['y'], axis=1) ) #,y=y_train) #R2 +Y=0.47\ntsvd_results_test = tsvd.transform(test.drop(['y'], axis=1))\n\n# PCA\npca = PCA(n_components=n_comp, random_state=42)\npca2_results_train = pca.fit_transform(train.drop(['y'], axis=1))#,y=y_train) #R2 +Y 0.467\npca2_results_test = pca.transform(test.drop(['y'], axis=1))\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=42)\nica2_results_train = ica.fit_transform(train.drop(['y'], axis=1) ) #,y=y_train) #R2+y=0.439\nica2_results_test = ica.transform(test.drop(['y'], axis=1))\n\n#FA\nfa =FactorAnalysis(n_components=n_comp)\nfa2_results_train = fa.fit_transform(train.drop([\"y\"], axis=1)) #,y=y_train) #R2 +y 0.412\nfa2_results_test = fa.transform(test.drop(['y'], axis=1))\n\n#MDB\nmdb=MiniBatchDictionaryLearning(n_components=n_comp, alpha=0.1,n_iter=50, batch_size=3,random_state=42)\nmdb_results_train = mdb.fit_transform(train.drop(['y'], axis=1)) #,y=y_train) #R2 +y 0.469\nmdb_results_test = mdb.transform(test.drop(['y'], axis=1))\n\n#mbk\nmbk=MiniBatchKMeans(n_clusters=n_comp, tol=1e-3, batch_size=20,max_iter=50, random_state=42)\nmbk_results_train = mbk.fit_transform(train.drop(['y'], axis=1)) #,y=y_train) #R2 +y 0.437\nmbk_results_test = mbk.transform(test.drop(['y'], axis=1))\n\n# Append decomposition components to datasets\nfor i in range(1, n_comp+1):\n    train['mbk_' + str(i)] = mbk_results_train[:,i-1]\n    test['mbk_' + str(i)] = mbk_results_test[:, i-1]\n    \n    train['mdb_' + str(i)] = mdb_results_train[:,i-1]\n    test['mdb_' + str(i)] = mdb_results_test[:, i-1]\n     \n    train['fa_' + str(i)] = fa2_results_train[:,i-1]\n    test['fa_' + str(i)] = fa2_results_test[:, i-1]\n    \n    train['pca_' + str(i)] = pca2_results_train[:,i-1]\n    test['pca_' + str(i)] = pca2_results_test[:, i-1]\n    \n    train['ica_' + str(i)] = ica2_results_train[:,i-1]\n    test['ica_' + str(i)] = ica2_results_test[:, i-1]\n    \n    train['tsvd_' + str(i)] = tsvd_results_train[:,i-1]\n    test['tsvd_' + str(i)] = tsvd_results_test[:, i-1]\n\n    train['lda_' + str(i)] = lda_results_train[:,i-1]\n    test['lda_' + str(i)] = lda_results_test[:, i-1]    \n\n    train['nmf_' + str(i)] = nmf_results_train[:,i-1]\n    test['nmf_' + str(i)] = nmf_results_test[:, i-1]        \n    \n\n\nprint('Shape with PCA train: {}\\nShape test: {}'.format(train.shape, test.shape))\n\ny_mean = np.mean(y_train)\n\nprint(test.head())\n\n### Regressor\nimport xgboost as xgb\n\n# prepare dict of params for xgboost to run with\nxgb_params = {\n    'n_trees': 500, \n    'eta': 0.0025,\n    'max_depth': 4,\n    'subsample': 0.95,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': y_mean, # base prediction = mean(target)\n    'silent': 1\n}\n\n\n# form DMatrices for Xgboost training\ndtrain = xgb.DMatrix(train.drop('y', axis=1), y_train)\ndtest = xgb.DMatrix(test.drop('y', axis=1))\n\n\nnum_boost_rounds = 1500\n# train model\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\nfig, ax = plt.subplots(figsize=(12,15))\nxgb.plot_importance(model, height=0.8, ax=ax, max_num_features=30)\nplt.show()\n\n# check f2-score (to get higher score - increase num_boost_round in previous cell)\nfrom sklearn.metrics import r2_score\nprint(r2_score(model.predict(dtrain), dtrain.get_label()))\n\n# make predictions and save results\ny_pred = model.predict(dtest)\noutput = pd.DataFrame({'id': test['ID'].astype(np.int32), 'y': y_pred})\n\n\nplt.figure(figsize=(12,8))\nsns.distplot(output.y.values, bins=50, kde=False)\nplt.xlabel('Predicted AVG Time on Test platform', fontsize=12)\nplt.show()\n    \noutput.to_csv('submission_baseLine.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}