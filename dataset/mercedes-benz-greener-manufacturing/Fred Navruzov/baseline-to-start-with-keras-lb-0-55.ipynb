{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"fa31842e-9387-e376-50bd-ee1040a8aeb6"},"source":"Hi, Kagglers!\n\nHereafter I will try to publish **some basic approaches to climb up the Leaderboard**\n\n**Competition goal**\n\nIn this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench.\n<br>Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. <br>Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimlerâ€™s standards. \n\n**The Notebook offers basic Keras skeleton to start with**\n\n### Stay tuned, this notebook will be updated on a regular basis\n**P.s. Upvotes and comments would let me update it faster and in a more smart way :)**"},{"cell_type":"markdown","metadata":{"_cell_guid":"f374fecd-0331-b78b-b0d5-d8a674b501ef"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"00245f2e-d3a7-4084-9a5f-09083382cad1"},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# preprocessing/decomposition\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.decomposition import PCA, FastICA, FactorAnalysis, KernelPCA\n\n# keras \nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# model evaluation\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n# supportive models\nfrom sklearn.ensemble import GradientBoostingRegressor\n# feature selection (from supportive model)\nfrom sklearn.feature_selection import SelectFromModel\n\n# to make results reproducible\nseed = 42 # was 42"},{"cell_type":"markdown","metadata":{"_cell_guid":"3f096068-d22d-829f-34e1-3aa220725168"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1078c0db-29e2-318a-6181-cc8ecdfc37cb"},"outputs":[],"source":"# Read datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# save IDs for submission\nid_test = test['ID'].copy()\n\n# glue datasets together\ntotal = pd.concat([train, test], axis=0)\nprint('initial shape: {}'.format(total.shape))\n\n# binary indexes for train/test set split\nis_train = ~total.y.isnull()\n\n# find all categorical features\ncf = total.select_dtypes(include=['object']).columns\n\n# make one-hot-encoding convenient way - pandas.get_dummies(df) function\ndummies = pd.get_dummies(\n    total[cf],\n    drop_first=False # you can set it = True to ommit multicollinearity (crucial for linear models)\n)\n\nprint('oh-encoded shape: {}'.format(dummies.shape))\n\n# get rid of old columns and append them encoded\ntotal = pd.concat(\n    [\n        total.drop(cf, axis=1), # drop old\n        dummies # append them one-hot-encoded\n    ],\n    axis=1 # column-wise\n)\n\nprint('appended-encoded shape: {}'.format(total.shape))\n\n# recreate train/test again, now with dropped ID column\ntrain, test = total[is_train].drop(['ID'], axis=1), total[~is_train].drop(['ID', 'y'], axis=1)\n\n# drop redundant objects\ndel total\n\n# check shape\nprint('\\nTrain shape: {}\\nTest shape: {}'.format(train.shape, test.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"485c2995-f1a6-4f6f-e59e-64cf7d65b9db"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bd78230e-8075-a9a2-eedf-d40a5be61d0e"},"outputs":[],"source":"# Calculate additional features: dimensionality reduction components\nn_comp=10 # was 10\n\n# uncomment to scale data before applying decompositions\n# however, all features are binary (in [0,1] interval), i don't know if it's worth something\ntrain_scaled = train.drop('y', axis=1).copy()\ntest_scaled = test.copy()\n'''\nss = StandardScaler()\nss.fit(train.drop('y', axis=1))\n\ntrain_scaled = ss.transform(train.drop('y', axis=1).copy())\ntest_scaled = ss.transform(test.copy())\n'''\n\n# PCA\npca = PCA(n_components=n_comp, random_state=seed)\npca2_results_train = pca.fit_transform(train_scaled)\npca2_results_test = pca.transform(test_scaled)\n\n# ICA\nica = FastICA(n_components=n_comp, random_state=42)\nica2_results_train = ica.fit_transform(train_scaled)\nica2_results_test = ica.transform(test_scaled)\n\n# Append it to dataframes\nfor i in range(1, n_comp+1):\n    train['pca_' + str(i)] = pca2_results_train[:,i-1]\n    test['pca_' + str(i)] = pca2_results_test[:, i-1]\n    \n    train['ica_' + str(i)] = ica2_results_train[:,i-1]\n    test['ica_' + str(i)] = ica2_results_test[:, i-1]\n   "},{"cell_type":"markdown","metadata":{"_cell_guid":"af5f7ca0-ff7a-a4aa-6865-b332ca19cfbc"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c99937b6-e3c1-4514-a2d6-1cedf0d5d5ff"},"outputs":[],"source":"# create augmentation by feature importances as additional features\nt = train['y']\ntr = train.drop(['y'], axis=1)\n\n# Tree-based estimators can be used to compute feature importances\nclf = GradientBoostingRegressor(\n                max_depth=4, \n                learning_rate=0.005, \n                random_state=seed, \n                subsample=0.95, \n                n_estimators=200\n)\n\n# fit regressor\nclf.fit(tr, t)\n\n# df to hold feature importances\nfeatures = pd.DataFrame()\nfeatures['feature'] = tr.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\n\n# select best features\nmodel = SelectFromModel(clf, prefit=True)\ntrain_reduced = model.transform(tr)\n\n\ntest_reduced = model.transform(test.copy())\n\n# dataset augmentation\ntrain = pd.concat([train, pd.DataFrame(train_reduced)], axis=1)\ntest = pd.concat([test, pd.DataFrame(test_reduced)], axis=1)\n\n# check new shape\nprint('\\nTrain shape: {}\\nTest shape: {}'.format(train.shape, test.shape))"},{"cell_type":"markdown","metadata":{"_cell_guid":"ad81cc31-cd26-83cf-4df6-75742836c39e"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bbd46fc6-2fac-4e2d-0492-1a6e4ec45df6"},"outputs":[],"source":"# define custom R2 metrics for Keras backend\nfrom keras import backend as K\n\ndef r2_keras(y_true, y_pred):\n    SS_res =  K.sum(K.square( y_true - y_pred )) \n    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"},{"cell_type":"markdown","metadata":{"_cell_guid":"88c418a9-ed38-45d3-cdd8-7225bcbabd1b"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"31424416-a7c5-6aa0-9c5a-8412bb9e9819"},"outputs":[],"source":"# base model architecture definition\ndef model():\n    model = Sequential()\n    #input layer\n    model.add(Dense(input_dims, input_dim=input_dims))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    # hidden layers\n    model.add(Dense(input_dims))\n    model.add(BatchNormalization())\n    model.add(Activation(act_func))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims//2))\n    model.add(BatchNormalization())\n    model.add(Activation(act_func))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims//4, activation=act_func))\n    \n    # output layer (y_pred)\n    model.add(Dense(1, activation='linear'))\n    \n    # compile this model\n    model.compile(loss='mean_squared_error', # one may use 'mean_absolute_error' as alternative\n                  optimizer='adam',\n                  metrics=[r2_keras] # you can add several if needed\n                 )\n    \n    # Visualize NN architecture\n    print(model.summary())\n    return model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2c64b578-8b9a-1519-3262-379c63c2abb7"},"outputs":[],"source":"# initialize input dimension\ninput_dims = train.shape[1]-1\n\n#activation functions for hidden layers\nact_func = 'tanh' # could be 'relu', 'sigmoid', ...\n\n# make np.seed fixed\nnp.random.seed(seed)\n\n# initialize estimator, wrap model in KerasRegressor\nestimator = KerasRegressor(\n    build_fn=model, \n    nb_epoch=100, \n    batch_size=20,\n    verbose=1\n)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b256a8e3-2af9-142a-3e91-dd85dbb4c806"},"outputs":[],"source":"# X, y preparation\nX, y = train.drop('y', axis=1).values, train.y.values\nprint(X.shape)\n\n# X_test preparation\nX_test = test\nprint(X_test.shape)\n\n# train/validation split\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X, \n    y, \n    test_size=0.2, \n    random_state=seed\n)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7fe6fa28-eddf-9afb-ec8c-59c2bedc9e0a"},"outputs":[],"source":"# define path to save model\nimport os\nmodel_path = 'keras_model.h5'\n\n# prepare callbacks\ncallbacks = [\n    EarlyStopping(\n        monitor='val_loss', \n        patience=10, # was 10\n        verbose=1),\n    \n    ModelCheckpoint(\n        model_path, \n        monitor='val_loss', \n        save_best_only=True, \n        verbose=0)\n]\n\n# fit estimator\nestimator.fit(\n    X_tr, \n    y_tr, \n    epochs=10, # increase it to 20-100 to get better results\n    validation_data=(X_val, y_val),\n    verbose=2,\n    callbacks=callbacks,\n    shuffle=True\n)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0346a9f2-10bc-ad1a-93eb-337cb644fee3"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d5365903-19fd-ca65-dae9-caaa82b23cb5"},"outputs":[],"source":"# if best iteration's model was saved then load and use it\nif os.path.isfile(model_path):\n    estimator = load_model(model_path, custom_objects={'r2_keras': r2_keras})\n\n# check performance on train set\nprint('MSE train: {}'.format(mean_squared_error(y_tr, estimator.predict(X_tr))**0.5)) # mse train\nprint('R^2 train: {}'.format(r2_score(y_tr, estimator.predict(X_tr)))) # R^2 train\n\n# check performance on validation set\nprint('MSE val: {}'.format(mean_squared_error(y_val, estimator.predict(X_val))**0.5)) # mse val\nprint('R^2 val: {}'.format(r2_score(y_val, estimator.predict(X_val)))) # R^2 val\npass"},{"cell_type":"markdown","metadata":{"_cell_guid":"fb113fea-c0e7-c3de-5cde-bc78a019a28d"},"source":""},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"80255a67-494c-c7ae-2a4d-a98fb08cbbc1"},"outputs":[],"source":"# predict results\nres = estimator.predict(X_test.values).ravel()\n\n# create df and convert it to csv\noutput = pd.DataFrame({'id': id_test, 'y': res})\noutput.to_csv('keras-baseline.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b83d560f-075d-a89a-a09b-335b2b269e40","collapsed":true},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}