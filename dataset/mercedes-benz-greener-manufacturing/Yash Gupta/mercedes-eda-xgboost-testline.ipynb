{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mercedes-Benz Greener Manufacturing\n\nWelcome to a new competition! This time from Mercedes-Benz - our job is to predict how long a car on a production line will take to pass the testing phase. This is a classical regression problem, and we're evaluated with the R2 metric. Let's take a look at the data we're given:","metadata":{"_cell_guid":"ba22a66a-2099-c52e-7017-82c1f081b902"}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\npal = sns.color_palette()\n\nprint('# File sizes')\nfor f in os.listdir('../input'):\n    if 'zip' not in f:\n        print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')","metadata":{"_cell_guid":"45578147-cb22-cd2a-ccc7-1b81c82e137b","execution":{"iopub.status.busy":"2021-11-10T07:50:57.05325Z","iopub.execute_input":"2021-11-10T07:50:57.054209Z","iopub.status.idle":"2021-11-10T07:50:57.909452Z","shell.execute_reply.started":"2021-11-10T07:50:57.054098Z","shell.execute_reply":"2021-11-10T07:50:57.90866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, a much smaller dataset than what we've been used to recently. No images here! :)\nWe're given a single train and test csv, indicating that the data should also be pretty simple to play with.\n\nTime to load it into memory!\n## Training set","metadata":{"_cell_guid":"2ed913f1-75ad-868b-ee5e-aec7bf264039"}},{"cell_type":"code","source":"df_train = pd.read_csv('../input/mercedes-benz-greener-manufacturing/train.csv.zip')\nprint('Size of training set: {} rows and {} columns'.format(*df_train.shape))\ndf_train.head()","metadata":{"_cell_guid":"ba151bcc-e94e-3ff4-d213-4e80e7b4d956","execution":{"iopub.status.busy":"2021-11-10T07:51:35.894512Z","iopub.execute_input":"2021-11-10T07:51:35.895387Z","iopub.status.idle":"2021-11-10T07:51:36.08433Z","shell.execute_reply.started":"2021-11-10T07:51:35.895348Z","shell.execute_reply":"2021-11-10T07:51:36.083483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just from this, we can see that our training data is made up of just 4000 rows, but has 400 seemingly anonymised features inside. As well as this, we are given an ID (which is not equal to the row number, this could be significant) and the target value, which is the number of seconds taken.\n\nLet's start off by looking at the distribution of the target value:","metadata":{"_cell_guid":"23fe3955-073a-681b-b90f-759363267b04"}},{"cell_type":"code","source":"y_train = df_train['y'].values\nplt.figure(figsize=(15, 5))\nplt.hist(y_train, bins=20)\nplt.xlabel('Target value in seconds')\nplt.ylabel('Occurences')\nplt.title('Distribution of the target value')\n\nprint('min: {} max: {} mean: {} std: {}'.format(min(y_train), max(y_train), y_train.mean(), y_train.std()))\nprint('Count of values above 180: {}'.format(np.sum(y_train > 200)))","metadata":{"_cell_guid":"8aa66a00-591c-7cff-cc88-051bfcaa40c5","execution":{"iopub.status.busy":"2021-11-10T07:51:42.400628Z","iopub.execute_input":"2021-11-10T07:51:42.400943Z","iopub.status.idle":"2021-11-10T07:51:42.75082Z","shell.execute_reply.started":"2021-11-10T07:51:42.400907Z","shell.execute_reply":"2021-11-10T07:51:42.75023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we have a pretty standard distribution here, which is centred around almost exactly 100. Nothing special to note here, except there is a single outlier at 265 seconds where every other value is below 180.\n\nThe fact that our ID is not equal to the row ID seems to suggest that the train and test sets were randomly sampled from the same dataset, which could have some special order to it, for example a time series. Let's take a look at how this target value changes over time in order to understand whether we're given time series data.\n","metadata":{"_cell_guid":"290f8db2-9db3-565a-2175-9640b8a100c5"}},{"cell_type":"code","source":"plt.figure(figsize=(15, 5))\nplt.plot(y_train)\nplt.xlabel('Row ID')\nplt.ylabel('Target value')\nplt.title('Change in target value over the dataset')\nplt.show()\n\nplt.figure(figsize=(15, 5))\nplt.plot(y_train[:100])\nplt.xlabel('Row ID')\nplt.ylabel('Target value')\nplt.title('Change in target value over the dataset (first 100 samples)')\nprint()","metadata":{"_cell_guid":"d6abf382-b443-f5ff-0d07-da607cf43916","execution":{"iopub.status.busy":"2021-11-10T07:51:42.752024Z","iopub.execute_input":"2021-11-10T07:51:42.752507Z","iopub.status.idle":"2021-11-10T07:51:43.203937Z","shell.execute_reply.started":"2021-11-10T07:51:42.752481Z","shell.execute_reply":"2021-11-10T07:51:43.203187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At first glance, there doesn't seem to be anything overly suspicious here - looks like how a random sort would. I might take a closer look later but for now let's move on to the features.\n\n## Feature analysis","metadata":{"_cell_guid":"c9bdaae4-2276-0b59-1da9-3558098ff9aa"}},{"cell_type":"code","source":"cols = [c for c in df_train.columns if 'X' in c]\nprint('Number of features: {}'.format(len(cols)))\n\nprint('Feature types:')\ndf_train[cols].dtypes.value_counts()","metadata":{"_cell_guid":"85c914e4-162a-e3be-ec91-1e687b8222e2","execution":{"iopub.status.busy":"2021-11-10T07:51:43.205357Z","iopub.execute_input":"2021-11-10T07:51:43.205576Z","iopub.status.idle":"2021-11-10T07:51:43.224675Z","shell.execute_reply.started":"2021-11-10T07:51:43.20555Z","shell.execute_reply":"2021-11-10T07:51:43.223886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So out of all our features, we are given 8 object (likely a string) variables, 368 integer variables. What about the cardinality of our features?","metadata":{"_cell_guid":"ca12b871-17d9-be23-be6b-7cca0ec45312"}},{"cell_type":"code","source":"counts = [[], [], []]\nfor c in cols:\n    typ = df_train[c].dtype\n    uniq = len(np.unique(df_train[c]))\n    if uniq == 1: counts[0].append(c)\n    elif uniq == 2 and typ == np.int64: counts[1].append(c)\n    else: counts[2].append(c)\n\nprint('Constant features: {} Binary features: {} Categorical features: {}\\n'.format(*[len(c) for c in counts]))\n\nprint('Constant features:', counts[0])\nprint('Categorical features:', counts[2])","metadata":{"_cell_guid":"33c6cb00-4390-077a-7576-6f33dd6a1eb3","execution":{"iopub.status.busy":"2021-11-10T07:51:43.432516Z","iopub.execute_input":"2021-11-10T07:51:43.433127Z","iopub.status.idle":"2021-11-10T07:51:43.509111Z","shell.execute_reply.started":"2021-11-10T07:51:43.433085Z","shell.execute_reply":"2021-11-10T07:51:43.50831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, we have 12 features which only have a single value in them - these are pretty useless for supervised algorithms, and should probably be dropped (unless you want to use them for anomaly detection in case a different value appears in the test set)\n\nThe rest of our dataset is made up of many binary features, and a few categorical features.","metadata":{"_cell_guid":"9e04780c-9f66-98d7-f4f6-8904aec93f40"}},{"cell_type":"code","source":"binary_means = [np.mean(df_train[c]) for c in counts[1]]\nbinary_names = np.array(counts[1])[np.argsort(binary_means)]\nbinary_means = np.sort(binary_means)\n\nfig, ax = plt.subplots(1, 3, figsize=(12,30))\nax[0].set_ylabel('Feature name')\nax[1].set_title('Mean values of binary variables')\nfor i in range(3):\n    names, means = binary_names[i*119:(i+1)*119], binary_means[i*119:(i+1)*119]\n    ax[i].barh(range(len(means)), means, color=pal[2])\n    ax[i].set_xlabel('Mean value')\n    ax[i].set_yticks(range(len(means)))\n    ax[i].set_yticklabels(names, rotation='horizontal')\nplt.show()","metadata":{"_cell_guid":"90384b82-46ec-7c44-7067-e9c4b0f7bb82","execution":{"iopub.status.busy":"2021-11-10T07:51:43.77572Z","iopub.execute_input":"2021-11-10T07:51:43.775993Z","iopub.status.idle":"2021-11-10T07:51:47.812314Z","shell.execute_reply.started":"2021-11-10T07:51:43.775962Z","shell.execute_reply":"2021-11-10T07:51:47.811791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in counts[2]:\n    value_counts = df_train[c].value_counts()\n    fig, ax = plt.subplots(figsize=(10, 5))\n    plt.title('Categorical feature {} - Cardinality {}'.format(c, len(np.unique(df_train[c]))))\n    plt.xlabel('Feature value')\n    plt.ylabel('Occurences')\n    plt.bar(range(len(value_counts)), value_counts.values, color=pal[1])\n    ax.set_xticks(range(len(value_counts)))\n    ax.set_xticklabels(value_counts.index, rotation='vertical')\n    plt.show()","metadata":{"_cell_guid":"664cd1be-2890-dcc8-efc9-2a9b25446102","execution":{"iopub.status.busy":"2021-11-10T07:51:47.813498Z","iopub.execute_input":"2021-11-10T07:51:47.813818Z","iopub.status.idle":"2021-11-10T07:51:50.260454Z","shell.execute_reply.started":"2021-11-10T07:51:47.813791Z","shell.execute_reply":"2021-11-10T07:51:50.259738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost Starter\nNow that we know the outline of what the data's made up of, we can make a simple model on it. Time to bring out XGBoost!","metadata":{"_cell_guid":"5bfa3800-9d93-ac90-f10b-6043d5512dbf"}},{"cell_type":"code","source":"df_test = pd.read_csv('../input/mercedes-benz-greener-manufacturing/test.csv.zip')\n\nusable_columns = list(set(df_train.columns) - set(['ID', 'y']))\n\ny_train = df_train['y'].values\nid_test = df_test['ID'].values\n\nx_train = df_train[usable_columns]\nx_test = df_test[usable_columns]\n\nfor column in usable_columns:\n    cardinality = len(np.unique(x_train[column]))\n    if cardinality == 1:\n        x_train.drop(column, axis=1) # Column with only one value is useless so we drop it\n        x_test.drop(column, axis=1)\n    if cardinality > 2: # Column is categorical\n        mapper = lambda x: sum([ord(digit) for digit in x])\n        x_train[column] = x_train[column].apply(mapper)\n        x_test[column] = x_test[column].apply(mapper)\n        \nx_train.head()","metadata":{"_cell_guid":"a5438fb4-6d7c-b453-a7ac-107420bbf771","execution":{"iopub.status.busy":"2021-11-10T07:54:47.777564Z","iopub.execute_input":"2021-11-10T07:54:47.77839Z","iopub.status.idle":"2021-11-10T07:54:48.249277Z","shell.execute_reply.started":"2021-11-10T07:54:47.778355Z","shell.execute_reply":"2021-11-10T07:54:48.248432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=4242)\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix(x_test)\n\nparams = {}\nparams['objective'] = 'reg:linear'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\n\nclf = xgb.train(params, d_train, 1000, watchlist, early_stopping_rounds=50, feval=xgb_r2_score, maximize=True, verbose_eval=10)","metadata":{"_cell_guid":"835e9ec6-d432-d4dd-6a97-bae3c62f205f","execution":{"iopub.status.busy":"2021-11-10T07:55:08.659212Z","iopub.execute_input":"2021-11-10T07:55:08.65948Z","iopub.status.idle":"2021-11-10T07:55:17.957606Z","shell.execute_reply.started":"2021-11-10T07:55:08.65945Z","shell.execute_reply":"2021-11-10T07:55:17.95701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_test = clf.predict(d_test)\n\nsub = pd.DataFrame()\nsub['ID'] = id_test\nsub['y'] = p_test\nsub.to_csv('xgb.csv', index=False)","metadata":{"_cell_guid":"efd8259e-9b42-e97e-67aa-5da3767c8452","execution":{"iopub.status.busy":"2021-11-10T07:55:17.959434Z","iopub.execute_input":"2021-11-10T07:55:17.960137Z","iopub.status.idle":"2021-11-10T07:55:17.998268Z","shell.execute_reply.started":"2021-11-10T07:55:17.960096Z","shell.execute_reply":"2021-11-10T07:55:17.997311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.head()","metadata":{"_cell_guid":"0de9e211-12a6-6b73-ee97-959c10eefb57","execution":{"iopub.status.busy":"2021-11-10T07:55:24.558642Z","iopub.execute_input":"2021-11-10T07:55:24.559125Z","iopub.status.idle":"2021-11-10T07:55:24.568703Z","shell.execute_reply.started":"2021-11-10T07:55:24.559079Z","shell.execute_reply":"2021-11-10T07:55:24.567897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks for reading my EDA! :)\n\n**If you have any questions or suggestions feel free to leave a comment - and please upvote if this helped you!**","metadata":{"_cell_guid":"1a97bce5-f4b2-f40c-6d70-693c8858787d"}}]}