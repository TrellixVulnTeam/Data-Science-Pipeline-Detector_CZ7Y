{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# basic imports\nimport pandas as pd\nimport numpy as np\n\n# plotting libraries and magics\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# garbage collector\nimport gc\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\n# modeling requirements\nfrom sklearn.model_selection import train_test_split # to split the data into train and validation sets\nfrom sklearn.metrics import r2_score # eval metric for this competetion\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# enabling multiple outputs for Jupyter cells\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity='all'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the data\ntrain = pd.read_csv('../input/mercedes-benz-greener-manufacturing/train.csv', index_col='ID')\ntest = pd.read_csv('../input/mercedes-benz-greener-manufacturing/test.csv', index_col='ID')\nsub = pd.read_csv('../input/mercedes-benz-greener-manufacturing/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train.head(3)\n# test.head(3)\n# sub.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's first get our target extracted\ny = train['y']\n# y.head(3)\n\n# drop the target from the train set \ntrain.drop('y', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_num_cat_cols(df):\n    \"\"\"\n    Returns two lists, one for categorical variables and one for numeric variables\n    \"\"\"\n    cat_vars = [col for col in df.columns if df[col].dtype == 'object']\n    num_vars = [col for col in df.columns if df[col].dtype != 'object']\n    \n    return cat_vars, num_vars","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # let's see what's the missing values look like\n# missing_data(train)\n# # wow! No missing values!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# let's see how many categorical and how many numeric variables we have\ncat_vars, num_vars = get_num_cat_cols(train)\nprint('Categorical variables: ', cat_vars)\nprint('Numeric variables: ', num_vars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see what the numeric data looks like\ntrain[num_vars].head()\n\n# we could add a new feature which is the count of 1's in the num_vars per row\ntrain['1_count'] = train[num_vars].sum(axis=1)\ntest['1_count'] = test[num_vars].sum(axis=1)\ntrain.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's also get the percentage of 1's to total numeric columns\ntrain['1_count_percent'] = (train[num_vars].sum(axis=1) * 100) / len(num_vars)\ntest['1_count_percent'] = (test[num_vars].sum(axis=1) * 100) / len(num_vars)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see what kind of data is present in the columns\ncols_with_less_data = [c for c in num_vars if train[c].nunique()<2]\n# for c in cols_with_less_data:\n#     train[c].unique()\n\n# these rows only have zeroes, that's not helping us gain any information, let's drop them - that reduces 12 columns.\ntrain.drop(cols_with_less_data, axis=1, inplace=True)\ntest.drop(cols_with_less_data, axis=1, inplace=True)\n# train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check the categorical variables\n[print('There are {1} unique values in {0} column'.format(c, train[c].nunique())) for c in cat_vars];","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X4 has only 4 values. let's see what they are\ntrain.X4.unique() # could be label encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X3 has only 7 values. let's see what they are\ntrain.X3.unique() # could be label encoded","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check X6\ntrain.X6.unique() # could be label encoded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"They all are alphabets - already label encoded in a way.\nI wonder what the variables with higher than 26 as a count have as values. Let's see."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check X2\ntrain.X2.unique() # could be label encoded","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"They are just a sequence of alphabets. Will ordering them do anything?\nAlso, does a category that starts with 'a' (like a, at, av, as, aq etc.) fall under the same larger category? Or just a way of sequencing them?"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's get the num_cols once again since we removed some of them\ncat_vars, num_vars = get_num_cat_cols(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# # help(sns.heatmap)\n# plt.figure(figsize=(15, 5))\n# sns.heatmap(train_without_trgt[num_vars].corr());","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ugh! Not a pretty heatmap. Let's see what we can do.\nLet's label encode the cat columns and make a quick and dirty model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def label_encode_columns(df):\n    \"\"\"\n    Given a dataframe, this will label encode all the categorical columns\n    \"\"\"\n    cat_cols, _ = get_num_cat_cols(df)\n    le = LabelEncoder()\n    for c in cat_cols:\n        le.fit(df[c])\n        df[c] = le.transform(df[c])\n    \n    return df\n\n# posibly, implement inplace=True functionality too","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"train = label_encode_columns(train)\ntest = label_encode_columns(test)\n\ntrain[cat_vars].head()\ntest[cat_vars].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's split the train and test set\ntrain_x, val_x, train_y, val_y = train_test_split(train, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's gather up a quick Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"important_features = ['X314','X315','X119','X118','X263','X136','X29','X279','X5','X232','X76','X54','X189','X47','X104','X8','1_count_percent','1_count','X2',\n                       'X3','X275','X65','X26','X6','X127','X1','X267','X311','X283','X0','X77','X341','X287','X13','X241','X46','X162','X117','X82','X105']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# params = {n_estimators: [100, 200, 400, 500],\n#           max_depth: [3, 4, 5],\n          \n#          }\n\n# for now, let's run it with default param values. We'll tune things later.\nreg = RandomForestRegressor(n_estimators = 500, \n                            max_depth = 5,\n                            random_state=42)\nreg.fit(train_x[important_features], train_y);\n\nval_pred = reg.predict(val_x[important_features])\n\n# now, let's check the R2 score\nr2_score(val_y, val_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # let's see what features are important and what are junk - we'll get rid of the junk to maybe make the performance better\n# plt.figure(figsize=(20, 10))\n# feat_importances = pd.Series(reg.feature_importances_, index=train.columns);\n# feat_importances.nlargest(40).plot(kind='barh');\n\n# important_features = list(feat_importances.nlargest(40).index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['y'] = reg.predict(test[important_features])\nsub.to_csv('quick_and_dirty_Random_forest.csv', index=False);","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}