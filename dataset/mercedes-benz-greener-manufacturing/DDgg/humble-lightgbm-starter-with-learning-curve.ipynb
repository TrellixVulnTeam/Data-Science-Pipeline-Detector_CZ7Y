{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"303e2a34-94c6-ce38-0d3c-0575333cf2e6"},"source":"**Example for LightGBM**, see https://github.com/Microsoft/LightGBM  \nvery fast, run time is around 2 seconds locally (old 2012 CPU)   \nand 16 seconds Kaggle cloud (without grid search at the end),     \nincludes learning curve, parameter importance plot, grid search and parameter tuning  \n\nFull code for copy-paste in local python editor here:  \nhttps://www.kaggle.com/tobikaggle/humble-lightgbm-starter"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"276a448d-9693-d6a8-f128-75528d3ef9a4"},"outputs":[],"source":"# -----------------------------------------------------------------------------\n# LightGBM regression example\n# __author__ = \"DDgg\"\n# https://www.kaggle.com/c/mercedes-benz-greener-manufacturing\n# -----------------------------------------------------------------------------\nimport numpy as np\nimport lightgbm as lgb\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9533aa90-607b-4138-619a-addd80ed4e6b"},"outputs":[],"source":"# PCA -------------------------------------------------------------------------\n# add to see skewness\n# -----------------------------------------------------------------------------\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c8a36d04-20a5-e765-7998-6e08378256ef"},"outputs":[],"source":"# data imnport \n# fork of forks from https://www.kaggle.com/jaybob20/starter-xgboost\n# Any results you write to the current directory are saved as output.\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n#pca_3D_plot(test)\n\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder() \n        lbl.fit(list(train[c].values) + list(test[c].values)) \n        train[c] = lbl.transform(list(train[c].values))\n        test[c] =  lbl.transform(list(test[c].values))\n        \ny_train = train[\"y\"]\ny_mean = np.mean(y_train)\ntrain.drop('y', axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a0605670-ee2b-047f-21ce-0403c1cb0d4c"},"outputs":[],"source":"# split into training and validation set\n# the data has a number of outliers, so the validation size needs\n# to be large enough plus cross-validation is needed\nX_train, X_valid, y_train, y_valid = train_test_split(\n        train, y_train, test_size=0.2, random_state=12345)\n\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n# to record eval results for plotting\nevals_result = {} \n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"461c4b33-9382-db5d-3e0c-35346b267492"},"outputs":[],"source":"# The rmse of prediction is: 7.75675312274\n# specify your configurations as a dict\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'l2'},\n    'num_leaves': 20,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'min_data_in_leaf':4,\n     #'min_sum_hessian_in_leaf': 5,\n    'verbose':10\n}\n\nprint('Start training...')\n\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=200,\n                valid_sets=[lgb_train, lgb_valid],\n                evals_result=evals_result,\n                verbose_eval=10,\n                early_stopping_rounds=50)\n\n# print('\\nSave model...')\n# save model to file\n# gbm.save_model('model.txt')\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"59e279d9-6aa4-99ab-a298-16c074b374c9"},"outputs":[],"source":"print('Start predicting...')\n# predict\ny_pred = gbm.predict(X_valid, num_iteration=gbm.best_iteration)\n\n# eval rmse\nprint('\\nThe rmse of prediction is:', mean_squared_error(y_valid, y_pred) ** 0.5)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"06d079d1-1ea3-4010-d925-f2f3e03ea7b4"},"outputs":[],"source":"# print feature names\nprint('\\nFeature names:', gbm.feature_name())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20605ac7-757e-e981-788e-005b64d51428"},"outputs":[],"source":"print('\\nCalculate feature importances...')\n\n# feature importances\nprint('Feature importances:', list(gbm.feature_importance()))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a908d647-ba89-6294-7683-b47ebfe0a9c4"},"outputs":[],"source":"# -------------------------------------------------------\nprint('Plot metrics during training...')\nax = lgb.plot_metric(evals_result, metric='l2')\nplt.show()\n# -------------------------------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b7f99ab7-d46d-5633-0244-51005734aee0"},"outputs":[],"source":"print('Plot feature importances...')\nax = lgb.plot_importance(gbm, max_num_features=10)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9903944a-0884-592e-d1b3-7b49428bc6f5"},"outputs":[],"source":"print('\\nPredicting test set...')\ny_pred = gbm.predict(test, num_iteration=gbm.best_iteration)\n\n# y_pred = model.predict(dtest)\noutput = pd.DataFrame({'id': test['ID'], 'y': y_pred})\noutput.to_csv('submit-lightgbm.csv', index=False)\n\nprint(\"Finished.\")\n# -----------------------------------------------------------------------------"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d62e1d4-9da3-11bd-6fef-855c42d1154e"},"outputs":[],"source":"# -----------------------------------------------------------------------------\n# Grid search example // uncomment block if needed\n# The parameters estimated here need to be fed into the upper code\n# can also be rearranged for demo purposes only\n# once this part is executed the ipython notebook has to be rerun\n# in external pyton editor the code section can be executed alone\n\nfrom sklearn.model_selection import GridSearchCV\nestimator = lgb.LGBMRegressor()\n\n# get possible parameters\nestimator.get_params().keys()\n\n# fill parameters ad libitum\nparam_grid = {\n'num_leaves': [20, 30],    \n'learning_rate': [0.01, 0.1],\n#     'n_estimators': [],\n#     'colsample_bytree' :[],\n#     'min_split_gain' :[],\n#     'subsample_for_bin' :[],\n#     'max_depth' :[],\n#     'subsample' :[], \n#     'reg_alpha' :[], \n#     'max_drop' :[], \n#     'gaussian_eta' :[], \n#     'drop_rate' :[], \n#     'silent' :[], \n#     'boosting_type' :[], \n#     'min_child_weight' :[], \n#     'skip_drop' :[], \n#     'learning_rate' :[], \n#     'fair_c' :[], \n#     'seed' :[], \n#     'poisson_max_delta_step' :[], \n#     'subsample_freq' :[], \n#     'max_bin' :[], \n#     'n_estimators' :[], \n#     'nthread' :[], \n#     'min_child_samples' :[], \n#     'huber_delta' :[], \n#     'use_missing' :[], \n#     'uniform_drop' :[], \n#     'reg_lambda' :[], \n#     'xgboost_dart_mode' :[], \n#     'objective'\n}\n\n\ngbm = GridSearchCV(estimator, param_grid)\n\ngbm.fit(X_train, y_train)\n\n# list them\nprint('Best parameters found by grid search are:', gbm.best_params_)\n"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}