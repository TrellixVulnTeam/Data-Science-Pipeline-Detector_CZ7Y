{"cells":[{"metadata":{"_cell_guid":"a7bb5427-807a-5f9d-7e5b-da9ba7d53bb7"},"cell_type":"markdown","source":"### 回归问题 (regression)\nIn this notebook, let us explore the dataset that is given for this competition.\n\n**Objective:**\n\nThis dataset contains an anonymized set of variables that describe different Mercedes cars. The ground truth is labeled 'y' and represents the time (in seconds) that the car took to pass testing. \n\nLet us first import the necessary modules."},{"metadata":{"_cell_guid":"3f860dba-f833-a61b-cbf4-318d578a0c61","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport xgboost as xgb\n# R2 决定系数（拟合优度）,模型越好：r2→1,模型越差：r2→0\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\ncolor = sns.color_palette()\n\n%matplotlib inline\n\npd.options.mode.chained_assignment = None  # default='warn'\npd.options.display.max_columns = 999\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"54d57ad3-5ac6-817e-7a1f-beaa71461c67","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \", train_df.shape)\nprint(\"Test shape : \", test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8cdace2e-2bdf-0645-9748-1e3b04a0a13b"},"cell_type":"markdown","source":"Wow the number of rows are small with 388 columns. We should try not to overfit :)\n\nLet us look at the top few rows."},{"metadata":{"_cell_guid":"a48c0537-c1b8-1c25-2184-441ec15c327a","trusted":true},"cell_type":"code","source":"\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7b74ea12-170b-c5e2-c3b1-bcbeb3137963"},"cell_type":"markdown","source":"**Target Variable:**\n\n\"y\" is the variable we need to predict. So let us do some analysis on this variable first."},{"metadata":{"_cell_guid":"143d232c-6b56-369d-9e19-10e65acd0190","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\n# 散点图\nplt.scatter(range(train_df.shape[0]), np.sort(train_df.y.values))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b6ade60-58f6-3446-a3d8-da1c5c53e1b7"},"cell_type":"markdown","source":"Seems like a single data point is well above the rest. \n\nNow let us plot the distribution graph."},{"metadata":{"_cell_guid":"146e9999-be25-458e-f7bd-5276a782b9c1","trusted":true},"cell_type":"code","source":"ulimit = 180\n# loc 选取特定行和列的切片，可以使用 boolean array，这里只切片了行\ntrain_df['y'].loc[train_df['y']>ulimit] = ulimit\n\nplt.figure(figsize=(12,8))\nsns.distplot(train_df.y.values, bins=50, kde=False)\nplt.xlabel('y value', fontsize=12)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d8de9470-d597-ca46-e091-6c457f0fc5cc"},"cell_type":"markdown","source":"Now let us have a look at the data type of all the variables present in the dataset."},{"metadata":{"_cell_guid":"7b5f8ed8-51a1-a3cf-1dfb-53d61dbc0a3d","trusted":true},"cell_type":"code","source":"# count 数据类型\ndtype_df = train_df.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b8d4880-f4ce-ebf4-8e50-b0f483101815"},"cell_type":"markdown","source":"> So majority of the columns are integers with 8 categorical columns and 1 float column (target variable)"},{"metadata":{"_cell_guid":"83a063e8-d79c-59c0-23c0-a239e7f4f282","trusted":true},"cell_type":"code","source":"# 这些 object 都是 str\ndtype_df.iloc[:10,:]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fdfc5ba6-b5c1-675f-1544-9b583136f403"},"cell_type":"markdown","source":"X0 to X8 are the categorical columns.\n\n**Missing values:**\n\nLet us now check for the missing values."},{"metadata":{"_cell_guid":"c0cf97e0-470b-fbeb-0b1d-5f289c09c628","trusted":true},"cell_type":"code","source":"# 检查 null 数据\nmissing_df = train_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.loc[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\nmissing_df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f78e003a-3ec2-5260-88f0-fd71246fbeda"},"cell_type":"markdown","source":"Good to see that there are no missing values in the dataset :) \n\n**Integer Columns Analysis:**"},{"metadata":{"_cell_guid":"fc836ed7-5f52-ce86-c287-8756c15dda5d","trusted":true},"cell_type":"code","source":"# 分析 integer 类型的列的数值特点，发现发部分都是 0-1 数据，还有些是毫无意义的 0\nunique_values_dict = {}\nfor col in train_df.columns:\n    if col not in [\"ID\", \"y\", \"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        # unique 将 list 变为 set\n        unique_value = str(np.sort(train_df[col].unique()).tolist())\n        tlist = unique_values_dict.get(unique_value, [])\n        tlist.append(col)\n        unique_values_dict[unique_value] = tlist[:]\nfor unique_val, columns in unique_values_dict.items():\n    print(\"Columns containing the unique values : \",unique_val)\n    print(columns)\n    print(\"--------------------------------------------------\")\n        ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4921ec55-24ee-f263-20ab-2b832efda29a"},"cell_type":"markdown","source":"So all the integer columns are binary with some columns have only one unique value 0. Possibly we could exclude those columns in our modeling activity.\n\nNow let us explore the categorical columns present in the dataset."},{"metadata":{"_cell_guid":"e501e9f6-989d-9691-75df-2334c4063780","trusted":true},"cell_type":"code","source":"# 使用 sns 统计类别列的特征\nvar_name = \"X0\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\n# Draw a scatterplot where one variable is categorical.\nsns.stripplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"93658ab4-1ab1-2a52-3fcd-633c8e21bea8","trusted":true},"cell_type":"code","source":"var_name = \"X4\"\ncol_order = np.sort(train_df[var_name].unique()).tolist()\nplt.figure(figsize=(12,6))\nsns.violinplot(x=var_name, y='y', data=train_df, order=col_order)\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3ea34b89-bfb0-51af-815a-7f596a519fe8"},"cell_type":"markdown","source":"**Binary Variables:**\n\nNow we can look into the binary variables. There are quite a few of them as we have seen before. Let us start with getting the number of 0's and 1's in each of these variables."},{"metadata":{"_cell_guid":"ea8dfe62-e5d0-d053-30eb-8e1ce8ab6146","trusted":true},"cell_type":"code","source":"zero_count_list = []\none_count_list = []\ncols_list = unique_values_dict['[0, 1]']\nfor col in cols_list:\n    zero_count_list.append((train_df[col]==0).sum())\n    one_count_list.append((train_df[col]==1).sum())\n\nN = len(cols_list)\nind = np.arange(N)\nwidth = 0.35\n\nplt.figure(figsize=(6,100))\n# Make a horizontal bar plot.\np1 = plt.barh(ind, zero_count_list, width, color='red')\np2 = plt.barh(ind, one_count_list, width, left=zero_count_list, color=\"blue\")\nplt.yticks(ind, cols_list)\nplt.legend((p1[0], p2[0]), ('Zero count', 'One Count'))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"498165ce-53a9-eef2-2c13-00581b8afe88"},"cell_type":"markdown","source":"**ID variable:**\n\nOne more important thing we need to look at it is ID variable. This will give an idea of how the splits are done across train and test (random or id based) and also to help see if ID has some potential prediction capability (probably not so useful for business)\n\nLet us first see how the 'y' variable changes with ID variable."},{"metadata":{"_cell_guid":"d6d90a0b-59f9-37c6-52d1-11409079622a","trusted":true},"cell_type":"code","source":"var_name = \"ID\"\nplt.figure(figsize=(12,6))\n# Plot data and a linear regression model fit.\n# scatter_kws are the additional keywords to scatter\n# alpha is Proportional opacity of the points\nsns.regplot(x=var_name, y='y', data=train_df, scatter_kws={'alpha':0.5})\nplt.xlabel(var_name, fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of y variable with \"+var_name, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76faf1c9-18e4-b646-77ad-36ac20b51d5b"},"cell_type":"markdown","source":"There seems to be a slight decreasing trend with respect to ID variable. Now let us see how the IDs are distributed across train and test."},{"metadata":{"_cell_guid":"a02130ef-4776-ec34-16e6-08210c05f465","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,10))\ntrain_df['eval_set'] = \"train\"\ntest_df['eval_set'] = \"test\"\nfull_df = pd.concat([train_df[[\"ID\",\"eval_set\"]], test_df[[\"ID\",\"eval_set\"]]], axis=0)\n\nplt.figure(figsize=(12,6))\nsns.violinplot(x=\"eval_set\", y='ID', data=full_df)\nplt.xlabel(\"eval_set\", fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title(\"Distribution of ID variable with evaluation set\", fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29a903cb-cf55-1c67-3ee5-409aee86317a"},"cell_type":"markdown","source":"Seems like a random split of ID variable between train and test samples.\n\n**Important Variables:**\n\nNow let us run xgboost model to get the important variables."},{"metadata":{"_cell_guid":"eeda9aeb-05b0-cdf7-bafc-825c8e9a9abd","trusted":true},"cell_type":"code","source":"for f in [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X8\"]:\n        # encode the labels to normalized numerical representations with\n        # value between 0 and n_classes-1\n        # LabelEncoder can be used to normalize labels.\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train_df[f].values)) \n        train_df[f] = lbl.transform(list(train_df[f].values))\n\ntrain_y = train_df['y']\n# drop 表示去掉\ntrain_x = train_df.drop([\"ID\", \"y\", \"eval_set\"] + \n                        ['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293',\n                         'X297', 'X330', 'X347'], axis=1)\n\n# 将 train_X 划分为 train test\n# 参数stratify： 依据标签y，按原数据y中各类比例，分配给train和test，\n# 使得train和test中各类数据的比例与原数据集一样。\ntrain_X, test_X, train_Y, test_Y = train_test_split(train_x, train_y, test_size=0.3, \n                                                    # stratify=train_y,\n                                                    shuffle=True, random_state=1)\n\n# Thanks to anokas for this #\ndef xgb_r2_score(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'r2', r2_score(labels, preds)\n\nxgb_params = {\n    # 'gbtree'： 表示采用xgboost (默认值)\n    # 'gblinear'： 表示采用线性模型。\n    # 'gblinear' 使用带l1,l2 正则化的线性回归模型作为基学习器。因为boost 算法是一个线性叠加的过程，\n    # 而线性回归模型也是一个线性叠加的过程。因此叠加的最终结果就是一个整体的线性模型，xgboost \n    # 最后会获得这个线性模型的系数。\n    # 'dart'： 表示采用dart booster\n    'booster': 'gbtree',\n    # 也称作学习率。默认为 0.3 。范围为 [0,1]\n    'eta': 0.05,\n    #  也称作最小划分损失min_split_loss。 它刻画的是：对于一个叶子节点，当对它采取划分之后，\n    # 损失函数的降低值的阈值。\n    # 如果大于该阈值，则该叶子节点值得继续划分\n    # 如果小于该阈值，则该叶子节点不值得继续划分\n    # 该值越大，则算法越保守（尽可能的少划分）。默认值为 0\n    'gamma': 0.,\n    # 每棵子树的最大深度。其取值范围为， 0 表示没有限制，默认值为6。\n    # 该值越大，则子树越复杂；值越小，则子树越简单。\n    'max_depth': 6,\n    # 对训练样本的采样比例。取值范围为 (0,1]，默认值为 1 。\n    # 如果为 0.5， 表示随机使用一半的训练样本来训练子树。它有助于缓解过拟合。\n    'subsample': 0.7,\n    # 构建子树时，对特征的采样比例。取值范围为 (0,1]， 默认值为 1。\n    # 如果为 0.5， 表示随机使用一半的特征来训练子树。它有助于缓解过拟合。\n    # 要依据特征个数来判断\n    'colsample_bytree': 0.7,\n    # 目标函数的选择要根据问题确定，如果是回归问题 ，一般是 reg:linear ,\n    # reg:logistic , count:poisson 如果是分类问题，一般是binary:logistic ,rank:pairwise\n    # 多分类：'objective': 'multi:softmax', 配合 'num_class': 3,\n    'objective': 'reg:linear',\n    # silent： 如果为 0（默认值），则表示打印运行时的信息；如果为 1，\n    # 则表示silent mode（ 不打印这些信息）\n    'silent': 0,\n    # nthread： 指定了运行时的并行线程的数量。如果未设定该参数，则默认值为可用的最大线程数。\n    # lambda： L2 正则化系数（基于weights的正则化），默认为 1。 该值越大则模型越简单\n    # alpha： L1 正则化系数（基于weights的正则化），默认为 0。 该值越大则模型越简单\n    # tree_method： 指定了构建树的算法，可以为下列的值：（默认为'auto' )\n    # 'auto'： 使用启发式算法来选择一个更快的tree_method：\n    # 对于小的和中等的训练集，使用exact greedy 算法分裂节点\n    # 对于非常大的训练集，使用近似算法分裂节点\n    # 旧版本在单机上总是使用exact greedy 分裂节点\n    # 'exact'： 使用exact greedy 算法分裂节点\n    # 'approx'： 使用近似算法分裂节点\n    # 'hist'： 使用histogram 优化的近似算法分裂节点（比如使用了bin cacheing 优化）\n    # 'gpu_exact'： 基于GPU 的exact greedy 算法分裂节点\n    # 'gpu_hist'： 基于GPU 的histogram 算法分裂节点\n    'tree_method': 'auto',\n    # early_stopping_rounds：一个整数，表示早停参数。\n    # 如果在early_stopping_rounds 个迭代步内，验证集的验证误差没有下降，则训练停止。\n    # 该参数要求evals 参数至少包含一个验证集。如果evals 参数包含了多个验证集，则使用最后的一个。\n    # 返回的模型是最后一次迭代的模型（而不是最佳的模型）。\n    # 如果早停发生，则模型拥有三个额外的字段：\n    # .best_score： 最佳的分数\n    # .best_iteration： 最佳的迭代步数\n    # .best_ntree_limit： 最佳的子模型数量\n    'eval_metric': ['rmse'],\n}\n# 调参思路\n# 设置num_round 足够大（比如100000），以至于你能发现每一个round 的验证集预测结果，\n# 如果在某一个round后 validation set 的预测误差上升了，你就可以停止掉正在运行的程序了。\n# 然后开始逐个调参了。\n# 首先调整max_depth ,通常max_depth 这个参数与其他参数关系不大，初始值设置为10，找到一个最好的误差值，\n# 然后就可以调整参数与这个误差值进行对比。比如调整到8，如果此时最好的误差变高了，那么下次就调整到12；\n# 如果调整到12,误差值比10 的低，那么下次可以尝试调整到15.\n# 在找到了最优的max_depth之后，可以开始调整subsample,初始值设置为1，然后调整到0.8 \n# 如果误差值变高，下次就调整到0.9，如果还是变高，就保持为1.0\n# 接着开始调整min_child_weight , 方法与上面同理\n# 再接着调整colsample_bytree\n# 经过上面的调整，已经得到了一组参数，这时调整eta 到0.05，然后让程序运行来得到一个最佳的num_round,\n# (在 误差值开始上升趋势的时候为最佳 )\n\n\n# 这里 feature_names 表示 Set names for features.\n# DMatrix is a internal data structure that used by XGBoost which is optimized for both memory\n# efficiency and training speed.\ndtrain = xgb.DMatrix(train_X, train_Y, feature_names=train_X.columns.values)\ndval = xgb.DMatrix(test_X, test_Y, feature_names=test_X.columns.values)\nwatchlist = [(dtrain,'train'),(dval,'val')]\n# feval is a Custom evaluation function\nmodel = xgb.train(params=xgb_params, dtrain=dtrain, num_boost_round=200,\n                  evals=watchlist,\n                  # 一个布尔值或者整数。\n                  # 如果为True，则evalutation metric  将在每个boosting stage 打印出来\n                  # 如果为一个整数，则evalutation metric  将在每隔verbose_eval个boosting stage 打印出来。\n                  # 另外最后一个boosting stage，以及早停的boosting stage 的 evalutation metric  也会被打印\n                  verbose_eval=20,\n                  feval=xgb_r2_score, maximize=True)\n# test\n# y_pred = model.predict(dtest)\n\n# plot the important features #\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8f798d1d-e529-5567-a95b-4089583395ed"},"cell_type":"markdown","source":"Categorical occupy the top spots followed by binary variables. \n\nLet us also build a Random Forest model and check the important variables."},{"metadata":{"_cell_guid":"91e69192-2e77-46af-4cba-540dc50d3f1f","trusted":true},"cell_type":"code","source":"from sklearn import ensemble\nmodel = ensemble.RandomForestRegressor(n_estimators=200, max_depth=10,\n                                       min_samples_leaf=4, max_features=0.2,\n                                       n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_Y)\nfeat_names = train_x.columns.values\n\npred_test = model.predict(test_X)\nplt.figure(figsize=(8,6))\nwidth = 50\nstart = np.random.random_integers(0, pred_test.size - width)\nend = start + width\n# 散点图\nplt.scatter(range(width), pred_test[start:end], alpha=0.5, s=20, color='blue')\nplt.scatter(range(width), test_Y[start:end], alpha=0.5, s=20, color='red')\nplt.bar(x=range(width), height=pred_test[start:end]-test_Y[start:end], width=0.4, alpha=0.4,\n        bottom=test_Y[start:end], color='green')\nplt.xlabel('index', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.show()\nprint('Test results of RandomForest: R2 socre:', r2_score(test_Y, pred_test))\n\n## plot the importances ##\nimportances = model.feature_importances_\n# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8d04f529-6d5e-84fb-c6ad-7f3375d64d1e"},"cell_type":"markdown","source":"Quite a few differences in the important variables between xgboost and random forest. Not sure why though.!"},{"metadata":{},"cell_type":"markdown","source":"## Advanced Techniques: PCA and Stacked Modeles"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import TruncatedSVD\n# lightgbm 是 MS 开发的类似 xgboost 的框架\nimport lightgbm as lgb\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n# convert catagory variables to numberical variables\nfor c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(list(train[c].values))\n        test[c] = lbl.transform(list(test[c].values))\ntrain_y=train['y'] \ntrain.drop(['y', 'X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289',\n                    'X290', 'X293', 'X297', 'X330', 'X347'],inplace=True,axis=1)\ntest.drop(['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289',\n                    'X290', 'X293', 'X297', 'X330', 'X347'],inplace=True,axis=1)\n# convert object variables into one-hot variables\ncombine=pd.concat([train,test])\ncolumns=['X0', 'X1','X2','X3','X4','X5','X6','X8']\nfor column in columns:\n    temp=pd.get_dummies(pd.Series(combine[column]))\n    print('cnovert categorical variable to one-hot dummy/indicator variables for {} with shape:'.format(\n        column), temp.shape)\n    combine=pd.concat([combine,temp],axis=1)\n    combine= combine.drop([column], axis=1)\n\ntrain=combine[:train.shape[0]]\ntest=combine[train.shape[0]:]\n\n# assert train.columns.shape[0] == len(set(train.columns.values)), 'error need uniqueify'\n\n# 把上述 one-hot 出来的重复的 column 名称 uniquify\ndef df_column_uniquify(df):\n    df_columns = df.columns\n    new_columns = []\n    for item in df_columns:\n        counter = 0\n        newitem = item\n        while newitem in new_columns:\n            counter += 1\n            newitem = \"{}_{}\".format(item, counter)\n        new_columns.append(newitem)\n    df.columns = new_columns\n    return df\n\ntrain = df_column_uniquify(train)  \ntest = df_column_uniquify(test)   \n# train['y'] = y\nassert train.columns.shape[0] == len(set(train.columns.values)), 'error after uniqueify'\n\n# drop 表示去掉\n# train_x = train_df.drop([\"ID\"], axis=1)\ntrain_X, test_X, train_y, test_Y = train_test_split(train[list(set(test.columns) - {'y', 'ID'})], train_y, test_size=0.3, \n                                                    # stratify=train_y,\n                                                    shuffle=True, random_state=1)\nassert 'y' not in list(train_X.columns)\nassert 'y' not in list(test_X.columns)\n# train_ = train_X\n# train_['y'] = train_y\n# test_ = test_X\n# 打乱顺序\n# train_ = train_.sample(frac=1,random_state=420)\n\n# Reduce dimensionality\nn_comp = 20\ncol = list(test.columns)\ncol.remove('ID')\nreduced_dim_cols = []\n# tSVD\ntsvd = TruncatedSVD(n_components=n_comp, random_state=420)\ntsvd_results_train = tsvd.fit_transform(train_X[col])\ntsvd_results_test = tsvd.transform(test_X[col])\n# PCA\npca = PCA(n_components=n_comp, random_state=420)\npca2_results_train = pca.fit_transform(train_X[col])\npca2_results_test = pca.transform(test_X[col])\n# ICA\n# tol is A positive scalar giving the tolerance at which the un-mixing matrix is considered to have converged.\nica = FastICA(n_components=n_comp, tol=0.03, random_state=420)\nica2_results_train = ica.fit_transform(train_X[col])\nica2_results_test = ica.transform(test_X[col])\n# GRP\ngrp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\ngrp_results_train = grp.fit_transform(train_X[col])\ngrp_results_test = grp.transform(test_X[col])\n# SRP\nsrp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\nsrp_results_train = srp.fit_transform(train_X[col])\nsrp_results_test = srp.transform(test_X[col])\n# Autoencoder\nncol = len(col)\n# print(set(train_X.columns) - (set(col) & set(train_X.columns)))\ninput_dim = Input(shape = (ncol, ))\n# Encoder Layers\nencoded1 = Dense(300, activation = 'relu')(input_dim)\nencoded2 = Dense(150, activation = 'relu')(encoded1)\nencoded4 = Dense(50, activation = 'relu')(encoded2)\nencoded5 = Dense(25, activation = 'relu')(encoded4)\nencoded6 = Dense(n_comp, activation = 'relu')(encoded5)\n# Decoder Layers\ndecoded1 = Dense(25, activation = 'relu')(encoded6)\ndecoded2 = Dense(50, activation = 'relu')(decoded1)\ndecoded4 = Dense(150, activation = 'relu')(decoded2)\ndecoded5 = Dense(300, activation='relu')(decoded4)\ndecoded6 = Dense(ncol, activation = 'sigmoid')(decoded5)\n# Combine Encoder and Deocder layers\nautoencoder = Model(inputs = input_dim, outputs = decoded6)\n# Compile the Model\nautoencoder.compile(optimizer = 'adadelta', loss = 'binary_crossentropy')\nautoencoder.fit(train_X[col], train_X[col], nb_epoch = 10, batch_size = 32, shuffle = False,\n                validation_data = (test_X[col], test_X[col]))\nencoder = Model(inputs = input_dim, outputs = encoded6)\nautoencoder_results_train = encoder.predict(train_X[col])\nautoencoder_results_test = encoder.predict(test_X[col])\nfor i in range(1, n_comp + 1):\n        reduced_dim_cols.append('tsvd_' + str(i))\n        train_X['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n        test_X['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n        reduced_dim_cols.append('pca_' + str(i))\n        train_X['pca_' + str(i)] = pca2_results_train[:, i - 1]\n        test_X['pca_' + str(i)] = pca2_results_test[:, i - 1]\n        reduced_dim_cols.append('ica_' + str(i))\n        train_X['ica_' + str(i)] = ica2_results_train[:, i - 1]\n        test_X['ica_' + str(i)] = ica2_results_test[:, i - 1]\n        reduced_dim_cols.append('grp_' + str(i))\n        train_X['grp_' + str(i)] = grp_results_train[:, i - 1]\n        test_X['grp_' + str(i)] = grp_results_test[:, i - 1]\n        reduced_dim_cols.append('srp_' + str(i))\n        train_X['srp_' + str(i)] = srp_results_train[:, i - 1]\n        test_X['srp_' + str(i)] = srp_results_test[:, i - 1]\n        reduced_dim_cols.append('ae_' + str(i))\n        train_X['ae_' + str(i)] = autoencoder_results_train[:, i - 1]\n        test_X['ae_' + str(i)] = autoencoder_results_test[:, i - 1]\n\n\ndef get_lgb_stack_data(params,rounds,train,col,label,test):\n    ID = []\n    train = train.reset_index(drop=True)\n    kf = KFold(n_splits=5,shuffle=False)\n    i=0\n    R2_Score = []\n    RMSE = []\n    for train_index, test_index in kf.split(train):\n        print(\"Training \"+str(i+1)+' Fold')\n        X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n        y_train, y_test = label.iloc[train_index],label.iloc[test_index]\n        train_lgb=lgb.Dataset(X_train[col],y_train)\n        model = lgb.train(params,train_lgb,num_boost_round=rounds)\n        pred = model.predict(X_test[col])\n        X_test['label'] = list(y_test)\n        X_test['predicted'] = pred\n        r2 = r2_score(y_test,pred)\n        rmse = MSE(y_test,pred)**0.5\n        print('R2 Scored of Fold '+str(i+1)+' is '+str(r2))\n        R2_Score.append(r2)\n        RMSE.append(rmse)\n        print('RMSE of Fold '+str(i+1)+' is '+str(rmse))\n        if i==0:\n            Final = X_test\n        else:\n            Final = Final.append(X_test,ignore_index=True)\n        i+=1\n    lgb_train_ = lgb.Dataset(train[col],label)\n    print('Start Training')\n    model_ = lgb.train(params,lgb_train_,num_boost_round=rounds)\n    Final_pred = model_.predict(test[col])\n    Final_pred = pd.DataFrame({'y':Final_pred})\n    print('Calculating In-Bag R2 Score')\n    print(r2_score(label, model.predict(train[col])))\n    print('Calculating Out-Bag R2 Score')\n    print(np.mean(R2_Score))\n    print('Calculating In-Bag RMSE')\n    print(MSE(label, model.predict(train[col]))**0.5)\n    print('Calculating Out-Bag RMSE')\n    print(np.mean(RMSE))\n    return Final,Final_pred\n\ndef get_sklearn_stack_data(model,train,col,label,test):\n    ID = []\n    R2_Score = []\n    RMSE = []\n    train = train.reset_index(drop=True)\n    kf = KFold(n_splits=5,shuffle=False)\n    i=0\n    for train_index, test_index in kf.split(train):\n        print(\"Training \"+str(i+1)+' Fold')\n        X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n        y_train, y_test = label.iloc[train_index],label.iloc[test_index]\n        model.fit(X_train[col],y_train)\n        pred = model.predict(X_test[col])\n        X_test['label'] = list(y_test)\n        X_test['predicted'] = pred\n        r2 = r2_score(y_test,pred)\n        rmse = MSE(y_test,pred)**0.5\n        print('R2 Scored of Fold '+str(i+1)+' is '+str(r2))\n        R2_Score.append(r2)\n        RMSE.append(rmse)\n        print('RMSE of Fold '+str(i+1)+' is '+str(rmse))\n        if i==0:\n            Final = X_test\n        else:\n            Final = Final.append(X_test,ignore_index=True)\n        i+=1\n    print('Start Training')\n    model.fit(train[col],label)\n    Final_pred = model.predict(test[col])\n    Final_pred = pd.DataFrame({'y':Final_pred})\n    print('Calculating In-Bag R2 Score')\n    print(r2_score(label, model.predict(train[col])))\n    print('Calculating Out-Bag R2 Score')\n    print(np.mean(R2_Score))\n    print('Calculating In-Bag RMSE')\n    print(MSE(label, model.predict(train[col]))**0.5)\n    print('Calculating Out-Bag RMSE')\n    print(np.mean(RMSE))\n    return Final,Final_pred\n\n\ndef get_xgb_stack_data(params,rounds,train,col,label,test):\n    ID = []\n    train = train.reset_index(drop=True)\n    kf = KFold(n_splits=5,shuffle=False)\n    i=0\n    R2_Score = []\n    RMSE = []\n    for train_index, test_index in kf.split(train):\n        print(\"Training \"+str(i+1)+' Fold')\n        X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n        y_train, y_test = label.iloc[train_index],label.iloc[test_index]\n        dtrain = xgb.DMatrix(X_train[col],y_train)\n        dtest = xgb.DMatrix(X_test[col])\n        model = xgb.train(params,dtrain,num_boost_round=rounds)\n        pred = model.predict(dtest)\n        X_test['label'] = list(y_test)\n        X_test['predicted'] = pred\n        r2 = r2_score(y_test,pred)\n        rmse = MSE(y_test,pred)**0.5\n        print('R2 Scored of Fold '+str(i+1)+' is '+str(r2))\n        R2_Score.append(r2)\n        RMSE.append(rmse)\n        print('RMSE of Fold '+str(i+1)+' is '+str(rmse))\n#         ID.append(X_test['ID'])\n        if i==0:\n            Final = X_test\n        else:\n            Final = Final.append(X_test,ignore_index=True)\n        i+=1\n    dtrain_ = xgb.DMatrix(train[col],label)\n    dtest_ = xgb.DMatrix(test[col])\n    print('Start Training')\n    model_ = xgb.train(params,dtrain_,num_boost_round=rounds)\n    Final_pred = model_.predict(dtest_)\n    Final_pred = pd.DataFrame({ # 'ID':test['ID'],\n        'y':Final_pred})\n    print('Calculating In-Bag R2 Score')\n    print(r2_score(dtrain_.get_label(), model.predict(dtrain_)))\n    print('Calculating Out-Bag R2 Score')\n    print(np.mean(R2_Score))\n    print('Calculating In-Bag RMSE')\n    print(MSE(dtrain_.get_label(), model.predict(dtrain_))**0.5)\n    print('Calculating Out-Bag RMSE')\n    print(np.mean(RMSE))\n    return Final,Final_pred\n\n\n# There are 3 models that we'd like to stack\ncol = list(test_X.columns)\n# only with reduced additional features\n# col = reduced_dim_cols\n# col.remove('eval_set')\n## Input 1: GBDT\n# n_estimators is The number of boosting stages to perform.\n# Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\ngb1 = GradientBoostingRegressor(n_estimators=1000,max_features=0.95,learning_rate=0.005,max_depth=4)\ngb1_train,gb1_test = get_sklearn_stack_data(gb1,train_X,col,train_y,test_X)\n## Input2: Lasso\n# las1 = Lasso(alpha=5,random_state=42)\n# las1_train,las1_test = get_sklearn_stack_data(las1,train_,col,train_['y'],test_)\ny_mean = np.mean(train_y)\nparams = {\n    'eta': 0.005,\n    'max_depth': 2,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': y_mean, # base prediction = mean(target)\n    'silent': 1\n}\nxgb_train, xgb_test = get_xgb_stack_data(params,800,train_X,col,train_y,test_X)\n## Input 3: LGB\nparams = {\n            'objective': 'regression',\n            'metric': 'rmse',\n            'boosting': 'gbdt',\n            'learning_rate': 0.0045 , #small learn rate, large number of iterations\n            'verbose': 0,\n            'num_iterations': 500,\n            'bagging_fraction': 0.95,\n            'bagging_freq': 1,\n            'bagging_seed': 42,\n            'feature_fraction': 0.95,\n            'feature_fraction_seed': 42,\n            'max_bin': 100,\n            'max_depth': 3,\n            'num_rounds': 800,\n            'sparse_threshold': 1.0,\n#             'device': 'gpu',\n#             'gpu_platform_id': 0,\n#             'gpu_device_id': 0\n        }\nlgb_train, lgb_test = get_lgb_stack_data(params,800,train_X,col,train_y,test_X)\n\n# Now we use xgboost to stack them\n\nstack_train = gb1_train[['label','predicted']]\nstack_train.columns=['label','gbdt']\nstack_train['lgb']=lgb_train['predicted']\n# stack_train['las'] = las1_train['predicted']\nstack_train['xgb'] = xgb_train['predicted']\n\nstack_test = gb1_test[['y']]\nstack_test.columns=['gbdt']\nstack_test['lgb']=lgb_test['y']\n# stack_test['las'] = las1_test['y']\nstack_test['xgb'] = xgb_test['y']\n\n## Meta Model: xgb\ny_mean = np.mean(train_y)\n\ncol = list(stack_test.columns)\n\nparams = {\n    'eta': 0.005,\n    'max_depth': 2,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': y_mean, # base prediction = mean(target)\n    'silent': 1\n}\n\nprint(col)\ndtrain = xgb.DMatrix(stack_train[col], stack_train['label'])\ndtest = xgb.DMatrix(stack_test[col])\n\nmodel = xgb.train(params,dtrain, num_boost_round=900)\npred_1 = model.predict(dtest)\n\n# Original XGBoost\ntrain_ = train_X\ntrain_['y'] = train_y\ntest_ = test_X\n# 打乱顺序\ntrain_ = train_.sample(frac=1, random_state=420)\ncol = list(test_X.columns)\n# only with reduced additional features\n# col = reduced_dim_cols\n\nxgb_params = {\n        'n_trees': 520, \n        'eta': 0.0045,\n        'max_depth': 4,\n        'subsample': 0.93,\n        'objective': 'reg:linear',\n        'eval_metric': 'rmse',\n        'base_score': y_mean, # base prediction = mean(target)\n        'silent': True,\n        'seed': 42,\n#         'tree_method': 'gpu_hist',\n    }\ndtrain = xgb.DMatrix(train_.drop('y', axis=1)[col], train_.y)\ndtest = xgb.DMatrix(test_[col])\n    \nnum_boost_rounds = 1250\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\ny_pred = model.predict(dtest)\n\n# Average Two Solutions\n\npred_test_average = 0.70*y_pred + 0.30*pred_1\n\n# Calculate the results\nprint('Test results of Stacked Models: R2 socre:', r2_score(test_Y, pred_test_average))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":1}