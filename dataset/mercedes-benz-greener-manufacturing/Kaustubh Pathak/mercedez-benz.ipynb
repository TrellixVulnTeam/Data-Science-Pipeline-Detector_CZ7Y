{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Tools\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import r2_score\nfrom sklearn.random_projection import GaussianRandomProjection\nfrom sklearn.random_projection import SparseRandomProjection\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_list = train.select_dtypes(include=['object']).columns\ndisplay(train[object_list].sample(10).T)\nfor f in object_list:\n    print('Unique in column ',f,' is -> ',len(train[f].unique()))\nfloat_list = train.select_dtypes(include=['float64']).columns\ndisplay(train[float_list].sample(10).T)\nint_list = train.select_dtypes(include=['int64']).columns\none_columns=[]\nfor f in int_list:\n    if len(train[f].unique())==1:\n        one_columns.append(f)\ntrain.drop(columns = one_columns , inplace = True)\ntest.drop(columns = one_columns , inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for f in object_list:\n    le = LabelEncoder()\n    le.fit(list(train[f].values) + list(test[f].values))\n    train[f] = le.transform(list(train[f].values))\n    test[f]  = le.transform(list(test[f].values))\nY = train['y']\ntrain.drop(columns=['y'] , inplace = True , axis=1)\ncombine = pd.concat([train ,  test])\n# object_list = object_list[1:]\nfor f in object_list:\n    temp = pd.get_dummies(combine[f])\n    combine = pd.concat([combine,temp] , axis =1)\n#     combine = combine.drop([f] , axis=1)\ntrain=combine[:train.shape[0]]\ntest=combine[train.shape[0]:] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)\nprint(Y.shape)\ntrain_columns = train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def df_column_uniquify(df):\n    df_columns = df.columns\n    new_columns = []\n    for item in df_columns:\n        counter = 0\n        newitem = item\n        while newitem in new_columns:\n            counter += 1\n            newitem = \"{}_{}\".format(item, counter)\n        new_columns.append(newitem)\n    df.columns = new_columns\n    return df\n\ntrain = df_column_uniquify(train)  \ntest = df_column_uniquify(test)   \ntrain['y']=Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_col = list(test.drop(columns=object_list).columns)\ndisplay(train.head())\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* magic = train [['ID' , 'X0' , 'y']]\n* magic = magic.groupby(['X0'])['y'].mean()\n* magic = pd.DataFrame({'X0' : magic.index , 'magic' : list(magic)})\n* magic_mean = magic['magic'].mean()\n* print(magic.head(10))\n* print(magic.shape)\n* print(magic_mean)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_additional_features(train , test , ID = False):\n    col = list(test.columns)\n    if ID!= True:\n        col.remove('ID')\n    n_comp = 12\n    #TSVD\n    tsvd = TruncatedSVD(n_components = n_comp  , random_state = 98)\n    tsvd_result_train = tsvd.fit_transform(train[col])\n    tsvd_result_test = tsvd.transform(test[col])\n    #PCA\n    pca = PCA(n_components = n_comp , random_state = 98)\n    pca_result_train = pca.fit_transform(train[col])\n    pca_result_test = pca.transform(test[col])\n    #FICA\n    ica = FastICA(n_components =n_comp , random_state = 98)\n    ica_result_train = ica.fit_transform(train[col])\n    ica_result_test = ica.transform(test[col])\n    #GRP\n    grp = GaussianRandomProjection(n_components = n_comp , random_state = 98)\n    grp_result_train = grp.fit_transform(train[col])\n    grp_result_test = grp.transform(test[col])\n    #SRP\n    srp = SparseRandomProjection(n_components = n_comp , random_state = 98 , dense_output =True )\n    srp_result_train = srp.fit_transform(train[col])\n    srp_result_test = srp.transform(test[col])\n    for i in range(1,n_comp+1):\n        train['tsvd_' + str(i)] = tsvd_result_train[:, i - 1]\n        test['tsvd_' + str(i)] = tsvd_result_test[:, i - 1]\n        train['pca_' + str(i)] = pca_result_train[:, i - 1]\n        test['pca_' + str(i)] = pca_result_test[:, i - 1]\n        train['ica_' + str(i)] = ica_result_train[:, i - 1]\n        test['ica_' + str(i)] = ica_result_test[:, i - 1]\n        train['grp_' + str(i)] = grp_result_train[:, i - 1]\n        test['grp_' + str(i)] = grp_result_test[:, i - 1]\n        train['srp_' + str(i)] = srp_result_train[:, i - 1]\n        test['srp_' + str(i)] = srp_result_test[:, i - 1]\n    return train ,test\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lgb_data(train , test , col , label , params ,rounds):\n    i=0\n    RMSE = []\n    R2_Score = []\n    ID = []\n    kf = KFold(n_splits = 5 , shuffle = False)\n    train = train.reset_index(drop = True)\n    for train_index , test_index in kf.split(train):\n        train_x , test_x = train.iloc[train_index, :] ,test.iloc[test_index ,:]\n        train_y , test_y = label.iloc[train_index] , label.iloc[test_index] \n        train_lgb = lgb.Dataset(train_x[col] , train_y)\n        model = lgb.train(params , train_lgb , num_boost_round = rounds)\n        pred = model.predict(test_x[col])\n        test_x['label'] = list(test_y)\n        test_x['predicted'] = pred\n        r2 = r2_score(test_y , pred)\n        rmse = MSE(test_y  ,pred)**0.5\n        print('R2 Scored of Fold '+str(i+1)+' is '+str(r2))\n        R2_Score.append(r2)\n        RMSE.append(rmse)\n        print('RMSE of Fold '+str(i+1)+' is '+str(rmse))\n        ID.append(test_x['ID'])\n        if i==0:\n            Final = test_x\n        else:\n            Final = Final.append(test_x,ignore_index=True)\n        i+=1\n    lgb_train = lgb.Dataset(train[col], label)\n    model = lgb.train(params , lgb_train , num_boost_round = rounds)\n    pred = model.predict(test[col])\n    lgb.plot_importance(model, max_num_features = 20)\n    Final_pred = pd.DataFrame({'ID': test['ID'] ,'y':pred})\n    print('Out of Bag R2 Score')\n    print(np.mean(r2))\n    print('Out of Bag RMSE')\n    print(np.mean(RMSE))\n    return Final , Final_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_sklearn_data(train , test , model , label ,col):\n    ID = []\n    RMSE = []\n    R2_Score = []\n    i=0\n    train =train.reset_index(drop = True)\n    kf = KFold(n_splits = 5 , shuffle = False)\n    for train_index , test_index in kf.split(train):\n        train_x , test_x = train.iloc[train_index,:] , train.iloc[test_index,:]\n        train_y , test_y = label.iloc[train_index] , label.iloc[test_index]\n        model.fit(train_x[col],train_y)\n        pred = model.predict(test_x[col])\n        test_x['label'] = list(test_y)\n        test_x['predicted'] = pred\n        r2 = r2_score(test_y , pred)\n        rmse = MSE(test_y , pred)**0.5\n        print('R2 Scored of Fold '+str(i+1)+' is '+str(r2))\n        R2_Score.append(r2)\n        RMSE.append(rmse)\n        print('RMSE of Fold '+str(i+1)+' is '+str(rmse))\n        ID.append(test_x['ID'])\n        if i==0:\n            Final = test_x\n        else:\n#             Final = pd.concat([Final , test_x] , axis=0 ) \n            Final = Final.append(test_x,ignore_index=True)\n        i+=1\n    model.fit(train[col] , label)\n    Final_pred = model.predict(test[col])  \n    Final_pred = pd.DataFrame({'ID':test['ID'] , 'y':Final_pred})\n    print('In of Bag R2 Score')\n    print(r2_score(label , model.predict(train[col])))\n    print('Out of Bag R2 Score')\n    print(np.mean(R2_Score))\n    print('In of Bag MSE Score')\n    print(MSE(label , model.predict(train[col])))\n    print('Out of Bag MSE Score')\n    print(np.mean(RMSE))\n    return Final , Final_pred , model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def magic_function(train ,test , columns):\n    for col in columns:\n        magic = train[['ID' , col , 'y']]\n        magic = magic.groupby([col])['y'].mean()\n        col_name = str(col)+'_mean'\n        magic = pd.DataFrame({col: magic.index , col_name:list(magic)})\n        magic_mean = magic[col_name].mean()\n        train = train.merge(magic , how = 'left' , on = col)\n        test = test.merge(magic , how = 'left' , on = col)\n        test[col_name] = test[col_name].fillna(magic_mean)\n    return train , test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = magic_function(train , test , object_list)\ntrain.sample(5)\ntrain_new , test_new = get_additional_features(train.drop(columns = object_list) ,test.drop(columns = object_list))\ntrain_new = train_new.sample(frac = 1 , random_state = 98)\ntrain_new.sample(10)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_corr = train_new[original_col].corr().abs()*100\ntrain_corr = train_corr.where(np.triu(np.ones(train_corr.shape)).astype(np.bool))\ntrain_corr.values[[np.arange(train_corr.shape[0])]*2] = np.nan\nprint(train_corr.shape)\n# display(train_corr.tail(20))\ncounter =0\ncolumns_drop =[]\ntrain_corr_matrix = train_corr.values\nfor i in range(1 , len(original_col) ,1 ):\n    for j in range(i , len(original_col) , 1):\n        if train_corr_matrix[i][j] >= 95:\n            counter+=1\n            columns_drop.append(train_corr.columns[j])\n            if counter%20 ==0:\n                print('Comman Columns pair reached ... ' , counter)\nprint(' Total Common Pair Found .... ',counter)\ncolumns_drop = list(set(columns_drop))\n# counter =0\n# for i in original_col:\n#     for j in original_col:\n#         if train_corr.loc[[i],[j]].values ==100:\n#             counter+=1\n#             if counter%10==0:\n#                 print('Comman Columns pair ...',counter)\n# print(counter)\n# train_corr = train_corr.unstack() \n# train_corr = pd.DataFrame(train_corr)\n# # display(train_corr.head(10))\n# train_corr = train_corr.reset_index()\n# train_corr.columns = [['Row' , 'Column' , 'Value']]\n# train_corr.dropna(inplace=True)\n# # train_corr = train_corr.sort_values(by=['Value'] ,ascending = False)\n# # train_corr.loc[train_corr['Value']>60]\n# train_corr.reset_index(drop= True , inplace = True)\n# train_corr.sample(10)\n# train_corr['Value']\n# train_corr = pd.DataFrame({'Row':train_corr['Row'].values , 'Column':train_corr['Column'].values ,'Value':  train_corr['Value'].values} )\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new.drop(columns=columns_drop , inplace = True )\ntest_new.drop(columns = columns_drop , inplace = True)\ncol = list(test_new.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\ngb1 = GradientBoostingRegressor(n_estimators = 1000 , max_features= 0.95, learning_rate = 0.95 , max_depth = 4)\ngb1_train , gb1_test  , model = get_sklearn_data(train_new , test_new , gb1  , train_new['y'] , col)\nimportances = model.feature_importances_\ndataframe = pd.DataFrame({'col':col , 'importance':importances})\ndataframe = dataframe.sort_values(by=['importance'] ,ascending = False)\ndataframe['importance_ratio'] = dataframe['importance']/dataframe['importance'].max()*100\ndataframe = dataframe.head(25)\nplt.figure(figsize=(18,6))\nplt.barh(dataframe['col'], dataframe['importance_ratio'], color='orange' , align='center' ,linewidth =30 )\nplt.xticks(rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nlasso1 = Lasso(alpha = 5 , random_state = 98)\nlasso_train , lasso_test , model = get_sklearn_data(train_new , test_new , lasso1 , train_new['y'] , col)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nrfr = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=4, max_features=0.2, n_jobs=-1, random_state=0)\nrfr_train , rfr_test , model = get_sklearn_data(train_new , test_new , rfr , train_new['y'] , col)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = model.feature_importances_\ndataframe = pd.DataFrame({'col':col , 'importance':importances})\ndataframe = dataframe.sort_values(by=['importance'] ,ascending = False)\ndataframe['importance_ratio'] = dataframe['importance']/dataframe['importance'].max()*100\ndataframe = dataframe.head(25)\ndataframe['col'] = dataframe['col'].apply(lambda x:  str(x))\nplt.figure(figsize=(18,6))\nplt.barh(dataframe['col'], dataframe['importance_ratio'], color='orange' , align='center' ,linewidth =30 )\nplt.xticks(rotation=30)\nplt.show()\nprint(dataframe.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nparams = {\n            'objective': 'regression',\n            'metric': 'rmse',\n            'boosting': 'gbdt',\n            'learning_rate': 0.0045 ,\n            'verbose': 0,\n            'num_iterations': 500,\n            'bagging_fraction': 0.95,\n            'bagging_freq': 1,\n            'bagging_seed': 42,\n            'feature_fraction': 0.95,\n            'feature_fraction_seed': 42,\n            'max_bin': 100,\n            'max_depth': 3,\n            'num_rounds': 800\n        }\nlgb_train, lgb_test = get_lgb_data(train_new, test_new , col , train_new['y'] , params , 800)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_new , test_new = get_additional_features(train.drop(columns = object_list) ,test.drop(columns = object_list) , ID = True)\ntrain_new = train_new.sample(frac = 1 , random_state = 98)\ntrain_new.drop(columns=columns_drop , inplace = True )\ntest_new.drop(columns = columns_drop , inplace = True)\ncol = list(test_new.columns)\ny_mean = np.mean(train_new['y'])\nxgb_params = {\n        'n_trees': 520, \n        'eta': 0.0045,\n        'max_depth': 4,\n        'subsample': 0.93,\n        'eval_metric': 'rmse',\n        'base_score': y_mean, \n        'silent': True,\n        'seed': 42,\n    }\ndtrain = xgb.DMatrix(train_new[col], train_new.y)\ndtrain_test = xgb.DMatrix(train_new[col])\ndtest = xgb.DMatrix(test_new[col])\n    \nnum_boost_rounds = 1250\nmodel = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\ny_pred = model.predict(dtest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stack_train =  gb1_train[['label' , 'predicted']]\nstack_train.columns = [['label','gbrp']]\nstack_train['lgb'] = lgb_train['predicted']\nstack_train['lasso'] = lasso_train['predicted'] \n# stack_train['xgb'] = list(y_pred_train)\n# stack_train['rfr'] = rfr_train['predicted']\n\nstack_test = gb1_test[['ID' , 'y']]\nstack_test.columns = [['ID' , 'gbrp']]\nstack_test['lgb'] = lgb_test['y']\nstack_test['lasso'] = lasso_test['y']\n# stack_test['xgb'] = list(y_pred)\n# stack_test['rfr'] = rfr_test['y']\nstack_test = stack_test.drop(['ID'] ,axis = 1)\ncol =  list(stack_test.columns)\n\n\n\nparams = {\n    'eta': 0.005,\n    'max_depth': 2,\n    'objective': 'reg:linear',\n    'eval_metric': 'rmse',\n    'base_score': y_mean,\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(stack_train[col], stack_train['label'])\ndtest = xgb.DMatrix(stack_test[col])\n\nxgb_cvalid = xgb.cv(params, dtrain, num_boost_round=2000, early_stopping_rounds=20, verbose_eval=50, show_stdv=True,seed=42)\nxgb_cvalid[['train-rmse-mean', 'test-rmse-mean']].plot()\nprint('Performance does not improve from '+str(len(xgb_cvalid))+' rounds')\nmodel = xgb.train(params,dtrain,num_boost_round =900)\npred_1 = model.predict(dtest)\nxgb.plot_importance(model, max_num_features = 3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Average = 0.70*y_pred + 0.30*pred_1\n# Average = np.expm1(Average)\nsub = pd.DataFrame({'ID':test['ID'],'y':Average})\nsub1 = pd.DataFrame({'ID':test['ID'],'y':y_pred})\nsub2 = pd.DataFrame({'ID':test['ID'],'y':pred_1})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub.to_csv('Fifth_submission.csv',index=False)\n# sub1.to_csv('Sixth_submission.csv',index=False)\nsub2.to_csv('12th_submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* With Using log Give Score of 0.53154\n* Following the next step with using direct and submission result give 0.53137\n* Not Much differnce so going to use direct with this time using ID In second XGBoost predictions 0.53826\n* Using Forth submission and Using magic function to increased accruracy is -> 0.54651\n* from fifth submission we are using the randomforest along with other algo -> 0.54411(using random Forest is not working fine)\n* for sixth we are just going to use y_pred that is single xgb ->0.53014\n* for seventh we are going to use pred_1 that is combination of algo's ->0.55520\n#### This Prove that we should use combination of multiple algos and not single XGB as it significantly reduces the accuracy. Now we can either remove random forest from algorithms as it significantly reduce accuracy\n* for eighth submission removed random_forest_regressor giving accuracy of 0.55799 (Significant improvement) and in private leaderboard give accuracy of 0.54239\n* for Ninth Submission i removed duplicate columns with correlation greater than or equal to 99% incresing the accuracy to 0.56105 and private LB to 0.54331\n* for 10th submission i used 95% accuracy to remove high corr columns 0.55770 with private LB is \n* for 11 th submission i used xbg in Combination of predictions is 0.45207 with private LB is 0.49288\n* for 12th submission i will remove columns before to create new columns by dimension reduction"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}