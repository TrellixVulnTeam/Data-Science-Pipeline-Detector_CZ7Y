{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"935b4ed8-9402-0e5e-2f71-aa93918087a0"},"source":"Hi\n\n You can find below a simple net with 1 hidden layers and averaging of 6. It's perfomance changes with different types of activation Units, batch size, number of epochs and the data you put it (mostly) Play around with them\n\nThe basic feature generation has been made\n\nThere is also a checkpoint but Umberto has already published a net with that  (https://www.kaggle.com/umbertogriffo/deep-learning-0-55326-for-now/comments#latest-189753)\n\nThe best result from net was about 56,2 - but i think it can achieve a little more\n\nTry it"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bfa853b1-55e3-a3b6-5382-43e3239f83cc"},"outputs":[],"source":"import numpy as np\nfrom sklearn.metrics import r2_score\nfrom sklearn.cross_validation import train_test_split\nimport pandas as pd\nimport subprocess\nfrom scipy.sparse import csr_matrix, hstack\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers.advanced_activations import PReLU"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b255ba5d-b447-ebd6-5db5-faaf86dc62ac"},"outputs":[],"source":"test = pd.read_csv('../input/test.csv')\ntrain = pd.read_csv('../input/train.csv')\nsub = pd.read_csv('../input/sample_submission.csv')\ny = train['y']\nID = test['ID']\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1d0a403a-99e7-9eb7-50d4-ecf4723fab5c"},"outputs":[],"source":"train.drop(['y','ID'],axis=1,inplace= True)\ntest.drop(['ID'],axis=1,inplace= True)\nntrain = train.shape[0]\nxtr_te = pd.concat((train, test), axis = 0)\nsparse_data = []\ncategories = ['X0', 'X1', 'X2', 'X3', 'X4','X5', 'X6', 'X8']\nfor f in categories:\n    dummy = pd.get_dummies(xtr_te[f].astype('category'))\n    tmp = csr_matrix(dummy)\n    sparse_data.append(tmp)\nxtr_te_cat = hstack(sparse_data, format = 'csr')\nxtr_te_cat = xtr_te_cat.todense()\ndrop = []\nfor i in train.columns:\n    if len(train[i].unique()) <2:\n        print(len(train[i].unique())), i\n        drop.append(i)\nxtr_te.drop(categories,axis=1,inplace=True)\nxtr_te.drop(drop,axis=1,inplace=True)\nxtr_te = np.array(xtr_te)\nxtr_te = np.concatenate((xtr_te_cat,xtr_te),axis =1)\nxtrain = xtr_te[:ntrain,:]\nxtest = xtr_te[ntrain:,:]\nfrom sklearn import preprocessing\n\nxtr_te = np.array(xtr_te)\nxtr_te = preprocessing.minmax_scale(xtr_te)\nxtrain = xtr_te[:ntrain,:]\nxtest = xtr_te[ntrain:,:]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d09f9d48-33c7-0570-da6e-e50e7548848b"},"outputs":[],"source":"X_train_1,X_test_1,y_train_1,y_test_1 = train_test_split(xtrain,y,test_size=0.4)\nX_train_2,X_test_2,y_train_2,y_test_2 = train_test_split(xtrain,y,test_size=0.3)\nX_train_3,X_test_3,y_train_3,y_test_3 = train_test_split(xtrain,y,test_size=0.3)\nX_train_4,X_test_4,y_train_4,y_test_4 = train_test_split(xtrain,y,test_size=0.3)\nX_train_5,X_test_5,y_train_5,y_test_5 = train_test_split(xtrain,y,test_size=0.3)\nX_train_6,X_test_6,y_train_6,y_test_6 = train_test_split(xtrain,y,test_size=0.3)\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2aa11dbd-8dc0-d1e4-4837-0cf68dc9a7a8"},"outputs":[],"source":"# play around with parameters\nbatch__size = 128 # try 32,64,16\nnumber_of_epochs = 10 #change it to 10000 to find out the best set of weights\nden = 125 # try different one"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1279cd7-e6bb-9cda-b065-6b982c2edd99"},"outputs":[],"source":"from keras.callbacks import ModelCheckpoint\nimport h5py\ncheckpointer = ModelCheckpoint(filepath=\"weights_model_1.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\ndef nn_model():\n    model = Sequential()\n    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, init = 'he_normal'))\n    model.compile(loss = 'mse', optimizer = 'adadelta')\n    return(model)\nmodel_1 = nn_model()\nmodel_1.fit(X_train_1, y_train_1, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n                 callbacks=[checkpointer],validation_data=(X_test_1,y_test_1))\n\nv1 = model_1.evaluate(X_test_1,y_test_1)\nprint (v1,v1**0.5)\n\nfrom keras.callbacks import ModelCheckpoint\nimport h5py\ncheckpointer = ModelCheckpoint(filepath=\"weights_model_2.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\ndef nn_model():\n    model = Sequential()\n    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, init = 'he_normal'))\n    model.compile(loss = 'mse', optimizer = 'adadelta')\n    return(model)\nmodel_2 = nn_model()\nmodel_2.fit(X_train_2, y_train_2, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n                 callbacks=[checkpointer],validation_data=(X_test_2,y_test_2))\n\nv2 = model_2.evaluate(X_test_2,y_test_2)\n\nprint (v2,v2**0.5)\n\nfrom keras.callbacks import ModelCheckpoint\nimport h5py\ncheckpointer = ModelCheckpoint(filepath=\"weights_model_3.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\ndef nn_model():\n    model = Sequential()\n    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, init = 'he_normal'))\n    model.compile(loss = 'mse', optimizer = 'adadelta')\n    return(model)\nmodel_3 = nn_model()\nmodel_3.fit(X_train_3, y_train_3, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n                 callbacks=[checkpointer],validation_data=(X_test_3,y_test_3))\n\nv3 = model_3.evaluate(X_test_3,y_test_3)\n\nprint (v3,v3**0.5)\n\nfrom keras.callbacks import ModelCheckpoint\nimport h5py\ncheckpointer = ModelCheckpoint(filepath=\"weights_model_4.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\ndef nn_model():\n    model = Sequential()\n    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, init = 'he_normal'))\n    model.compile(loss = 'mse', optimizer = 'adadelta')\n    return(model)\nmodel_4 = nn_model()\nmodel_4.fit(X_train_4, y_train_4, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n                 callbacks=[checkpointer],validation_data=(X_test_4,y_test_4))\n\nv4 = model_4.evaluate(X_test_4,y_test_4)\nprint (v4,v4**0.5)\n\nfrom keras.callbacks import ModelCheckpoint\nimport h5py\ncheckpointer = ModelCheckpoint(filepath=\"weights_model_5.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\ndef nn_model():\n    model = Sequential()\n    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, init = 'he_normal'))\n    model.compile(loss = 'mse', optimizer = 'adadelta')\n    return(model)\nmodel_5 = nn_model()\nmodel_5.fit(X_train_5, y_train_5, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n                 callbacks=[checkpointer],validation_data=(X_test_5,y_test_5))\n\nv5 = model_5.evaluate(X_test_5,y_test_5)\nprint (v5,v5**0.5)\n\nfrom keras.callbacks import ModelCheckpoint\nimport h5py\ncheckpointer = ModelCheckpoint(filepath=\"weights_model_6.hdf5\", verbose=0, save_best_only=True,monitor='val_loss')\ndef nn_model():\n    model = Sequential()\n    model.add(Dense(den, input_dim = xtrain.shape[1], init = 'he_normal',activation='tanh'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, init = 'he_normal'))\n    model.compile(loss = 'mse', optimizer = 'adadelta')\n    return(model)\nmodel_6 = nn_model()\nmodel_6.fit(X_train_6, y_train_6, nb_epoch=number_of_epochs,verbose=0,batch_size=batch__size,\n                 callbacks=[checkpointer],validation_data=(X_test_6,y_test_6))\n\nv6 = model_6.evaluate(X_test_6,y_test_6)\nprint (v6,v6**0.5)\n\nprint ((v1+v2+v3+v4+v5+v6)/6, 'mse final',(v1**0.5+v2**0.5+v3**0.5+v4**0.5+v5**0.5+v6**0.5)/6,'as rmse final')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f96cd14d-ba26-37b2-bf2b-a565e176cd89"},"outputs":[],"source":"model_1.load_weights(\"weights_model_1.hdf5\")\nmodel_2.load_weights(\"weights_model_2.hdf5\")\nmodel_3.load_weights(\"weights_model_3.hdf5\")\nmodel_4.load_weights(\"weights_model_4.hdf5\")\nmodel_5.load_weights(\"weights_model_5.hdf5\")\nmodel_6.load_weights(\"weights_model_6.hdf5\")\n\nr1 = r2_score(y_test_1,model_1.predict(X_test_1))\nr2 = r2_score(y_test_2,model_2.predict(X_test_2))\nr3 = r2_score(y_test_3,model_3.predict(X_test_3))\nr4 = r2_score(y_test_4,model_4.predict(X_test_4))\nr5 = r2_score(y_test_5,model_5.predict(X_test_5))\nr6 = r2_score(y_test_6,model_6.predict(X_test_6))\n\nprint (r1,r2,r3,r4,r5,r6)\nprint ((r1+r2+r3+r4+r5+r6)/6.0)\n\n\n\npred1 = model_1.predict(xtest)\npred2 = model_2.predict(xtest)\npred3 = model_3.predict(xtest)\npred4 = model_4.predict(xtest)\npred5 = model_5.predict(xtest)\npred6 = model_6.predict(xtest)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"170369fc-409d-4590-c72d-ca8e1cf22aa2"},"outputs":[],"source":"pred_final2 = (pred1+pred2+pred3+pred4+pred5+pred6)/6.0\nsub['y'] = pred_final2\nsub.to_csv('output.csv',index=None)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"618b8802-092f-88cc-a530-edc47526beff"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}