{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement: Refer [Mercedes-Benz Greener Manufacturing](https://www.kaggle.com/c/mercedes-benz-greener-manufacturing) \n\n## **Objective:** To Reduce the time a Mercedes-Benz spends on the test bench","metadata":{}},{"cell_type":"code","source":"# import the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n%matplotlib inline\n\n# import the library to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the train dataset\ntrain_original = pd.read_csv('../input/mercedes-benz-greener-manufacturing/train.csv.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display the data\ntrain_original.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the train dataset\ntrain_original.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**\n > 1. There are 4209 data points and 378 features in the dataset.\n > 2. It is observed that there are more number of features with binary values. Hence sparsity exists in the train data.\n > 3. Target feature 'y' from the training data is the time that cars spend on the test bench.","metadata":{}},{"cell_type":"code","source":"# check the info of the train data\ntrain_original.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**.\n> 1. The train dataset has one feature with float64 datatype which is target (Y) features\n> 2. The train dataset has 369 features with int64 datatype which are features having binary values (0 and 1)\n> 3. The train dataset has 8 features with object datatype which are features having categorical data","metadata":{}},{"cell_type":"code","source":"# First,check the original variance of all the features in the train dataset and store it to new object\ntrain_original_var=pd.DataFrame(train_original.var(axis=0),columns=['Variance'])\ntrain_original_var","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Second, define a function to remove the features of train dataset with zero variance \ndef features_zero_var(df):\n    df_original_var=pd.DataFrame(df.var(axis=0),columns=['Variance']) \n    return((df_original_var[df_original_var.Variance==0]))\n# Add the object train_original to the function. i.e., features_zero_var(train_original)\n# Firstly, the variance with respect to each features in the train_original dataset will be stored in the object df_original_var.\n# Index contains the feature names and the corresponding variances will be displayed in the column with name 'Variance'  \n# Lastly, it returns only the features of object 'df_original_var' having variance = 0\n# df_original_var is named because it will be applicable to both train and test data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the function to return the train dataset features having zero variance.\nfeatures_zero_var(train_original)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> Above listed features have zero variance.","metadata":{}},{"cell_type":"code","source":"# Remove the features with 0 variance from train dataset and store the data in the new object\ntrain_original_modified= train_original.drop(columns=train_original_var[train_original_var.Variance==0].index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the modified train dataset after removing the features having zero variance\ntrain_original_modified.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the original and modified shape of the train dataset\nprint('Shape of original train dataset is:', train_original.shape)\nprint('\\nShape of modified train dataset after removing features having zero variance is:', train_original_modified.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> The modified train dataset contains 366 features, which means the 12 features with zero variance from the original dataset is removed.","metadata":{}},{"cell_type":"code","source":"# Save the modified train dataset\ntrain_original_modified.to_csv('train_original_modified.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Similarly remove the zero variance features from the test dataset ","metadata":{}},{"cell_type":"code","source":"#import the test dataset\ntest_original = pd.read_csv('../input/mercedes-benz-greener-manufacturing/test.csv.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display the data\ntest_original.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the data\ntest_original.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**\n> 1. There are 4209 data points and 377 features in the test dataset.\n> 2. Number of features in the test dataset is 377 because Target feature 'y' is missing in the test dataset compared to train dataset.","metadata":{}},{"cell_type":"code","source":"# First,check the original variance of all the features in the test dataset and store it to new object\ntest_original_var=pd.DataFrame(test_original.var(axis=0),columns=['Variance'])\ntest_original_var","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the function to return the test dataset features having zero variance.\nfeatures_zero_var(test_original)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> 1. Above listed features have zero variance.\n> 2. However, since test dataset is not considered for training and only used for testing, we can remove the same features of train dataset in the test dataset as well.\n> 3. This will ensure the same size and shape of the train and test dataset","metadata":{}},{"cell_type":"code","source":"# In test dataset, remove the same features of train dataset having 0 variance.\ntest_original_modified= test_original.drop(columns=['X11', 'X93', 'X107','X233', 'X235', 'X268', 'X289', 'X290', 'X293','X297','X330','X347'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the modified test dataset after removing the features having zero variance\ntest_original_modified.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the original and modified shape of the test dataset\nprint('Shape of original test dataset is:', test_original.shape)\nprint('\\nShape of modified test dataset after removing features having zero variance is:', test_original_modified.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> 1. The modified test dataset contains 365 features. This is because, we had removed the same 12 features having zero variance in the train dataset.\n> 2. The modified test dataset has 365 features, whereas modified train dataset had 366 features. This is because target feature 'y' is missing  in the train dataset","metadata":{}},{"cell_type":"code","source":"# Save the modified test dataset\ntest_original_modified.to_csv('test_original_modified.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Check the null and unique values in the train dataset","metadata":{}},{"cell_type":"code","source":"# Check the null values in the train dataset\nprint('The sum of null values in the train dataset is:', train_original_modified.isnull().any().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** \n> There are no null values in the train dataset.","metadata":{}},{"cell_type":"code","source":"# Check the unique values in the train dataset\n# unique() function includes the missing value \n# nunique() function excludes the missing value as the default parameter is dropna=True\n# Since there are no missing values in the train and test dataset, we can use nunique()\ntrain_original_modified_UV=pd.DataFrame(train_original_modified.nunique(),columns=['Unique_Values'])\ntrain_original_modified_UV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> 1. It is observed that all the values in ID's are unique. In the provided dataaset, the ID represents the unique car configuration. So, this feature must be ignored for training as it will not make any sense to the prediction. \n> 2. 'y' feature is the target feature. \n> 3. Features X0,X1,X2,X3,X4,X5,X6,X8 are the categorical features which must be converted to numerical values/one hot encoded values.\n> 4. All the features after X8 are having binary values.","metadata":{}},{"cell_type":"code","source":"# Print the train dataset features unique values where the values = 2 and values>2.\nprint('Train Features with unique values greater than 2 are as follows:\\n',train_original_modified_UV[train_original_modified_UV.Unique_Values>2].unstack())\nprint('Test Features with unique values equal to 2 are as follows:\\n',train_original_modified_UV[train_original_modified_UV.Unique_Values==2].unstack())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:**\n> Out of 366 train dataset features, 10 features are having greater than 2 unique values and remaining features are having only 2 unique values (0 and 1). ","metadata":{}},{"cell_type":"markdown","source":"### Similarly check the  null and unique values in the  test dataset ","metadata":{}},{"cell_type":"code","source":"# Check the null values in the test dataset\nprint('The sum of null values in the test dataset is:', test_original_modified.isnull().any().sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** \n> There are no null values in the test dataset.","metadata":{}},{"cell_type":"code","source":"# Check the unique values in the test dataset\ntest_original_modified_UV=pd.DataFrame(test_original_modified.nunique(),columns=['Unique_Values'])\ntest_original_modified_UV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> 1. It is observed that all the valus in ID's are unique.In the provided dataaset, the ID represents the unique car configuration. So, this feature must be ignored for testing as it will not make any sense to the prediction.\n> 2. Features X0,X1,X2,X3,X4,X5,X6,X8 are the categorical features which must be converted to numerical values/one hot encoded values.\n> 3. All the features after X8 are having binary values.","metadata":{}},{"cell_type":"code","source":"# Print the test dataset features unique values where the values = 2 and values >2.\nprint('Test Features with unique values greater than 2 are as follows:\\n',test_original_modified_UV[test_original_modified_UV.Unique_Values>2].unstack())\nprint('Test Features with unique values equal to 2 are as follows:\\n',test_original_modified_UV[test_original_modified_UV.Unique_Values==2].unstack())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:**\n> Out of 365 test dataset features, 9 features are having greater than 2 unique values and remaining features are having only 2 unique values (0 and 1). ","metadata":{}},{"cell_type":"markdown","source":"## Apply the label encoder/one hot encoding  for the train dataset","metadata":{}},{"cell_type":"code","source":"# Before applying label encoder, separate the ID and 'y' features from the train dataset\n##\n##Drop the columns 'ID' and 'y' and store the data into new object train_X_check and verify the shape\ntrain_X_check = train_original_modified.drop(columns = ['ID','y'])\n# train_X_check to verify how the one hot encoding works for the features with multi categorical variables \ntrain_X_check.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# perform label encoder/one hot encoding for the categories features of train_X_check\n##\n# Import the required library\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the function to apply the label encoder for the categories features of train_X_check\ndef label_encoder(df,x):\n    features_cat=df.select_dtypes(include='object').columns # select only the features with datatype Object\n    le=LabelEncoder() # instantiate the label encoder\n    for i in features_cat:\n        x[i]=le.fit_transform(x[i]) # Fit,transform and replace with label encoded data for the existing data in the object datype columns of train_X_check","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the function to apply label encoder for the train_X_check data\nlabel_encoder(train_original_modified,train_X_check)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# verify the train_X dataset\ntrain_X_check.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**\n> 1. It is observed from the above table that, label encoder is applied to categorial features of train_X_check data, the encoded labels are not binary (0 and 1) since the features has 4 or more different categories.\n> \n> \n> 2. Example: X5 feature has 29 unique categories. So, when label encoder is applied, the categories will be replaced with 28 unique values starting from 0 to 28.\n> \n> \n> 3. This may impact the accuracy level.\n> \n> \n> 4. Inorder to fix this issue, the following procedure is followed here [[Reference](https://www.youtube.com/watch?v=6WDFfaYtN6s)]\n>     - Identify the top 10 most frequent categories from each feature. \n>     \n>     - Perform one hot encoding only for the top 10 most frequent categories.\n>     \n>     - All Top 10 most frequent categories will be considered as '1' and all the remaining categories will be considered as '0' in each feature.\n>     \n>     - By performing above 3 steps ensures only binary values (0 and 1) in all the features","metadata":{}},{"cell_type":"code","source":"# Identify the top 10 most frequent categories of features X0,X1,X2,X3,X4,X5,X6,X8\n\n#X0\ntrain_original_modified.X0.value_counts().sort_values(ascending=False).head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a list for the top 10 most frequent catergories of feature X0\ntop_10_X0 = [x for x in train_original_modified.X0.value_counts().sort_values(ascending=False).head(10).index]\ntop_10_X0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a funtion to peform one hot encoding for the top 10 most frequent categories of features\ndef one_hot_top10(df,feature,top10_categories):\n    for category in top10_categories:\n        df[feature+'_'+category]=np.where(train_original_modified[feature]==category,1,0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Call the function to perform one hot encoding on feature X0\none_hot_top10(train_original_modified,'X0',top_10_X0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# verify the train dataset after applying one hot encoding for the feature X0\ntrain_original_modified.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# verify for one feature if it contains only binary value\ntrain_original_modified.X0_z.unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:**\n> 1. It is evident from the above table that 10 new features are created with only binary values, for the top 10 most frequent categories of feature X0. \n> \n> 2. Hence the total columns/features are increased from 366 to 376.\n> \n> 3. Similarly perform one hot encoding for the top 10 frequent categories of remaining categorical features.","metadata":{}},{"cell_type":"code","source":"# create a list for the top 10 most frequent categories of features X1, X2,X3,X4,X5,X6 and X8\n# Call the function to perform one hot encoding on features X1, X2,X3,X4,X5,X6 and X8 \n# verify the train dataset after applying one hot encoding for the features X1, X2,X3,X4,X5,X6 and X8 \n\n# X1 \ntop_10_X1 = [x for x in train_original_modified.X1.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X1',top_10_X1)\ntrain_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X2\ntop_10_X2 = [x for x in train_original_modified.X2.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X2',top_10_X2)\ntrain_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X3\ntop_10_X3 = [x for x in train_original_modified.X3.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X3',top_10_X3)\ntrain_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> In case X3, only 7 columns were added since there are only 7 unique categories in in this feature.","metadata":{}},{"cell_type":"code","source":"# X4\ntop_10_X4 = [x for x in train_original_modified.X4.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X4',top_10_X4)\ntrain_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> In case X4, only 4 columns were added since there are only 4 unique categories in in this feature.","metadata":{}},{"cell_type":"code","source":"# X5\ntop_10_X5 = [x for x in train_original_modified.X5.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X5',top_10_X5)\ntrain_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X6\ntop_10_X6 = [x for x in train_original_modified.X6.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X6',top_10_X6)\ntrain_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X8\ntop_10_X8 = [x for x in train_original_modified.X8.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X8',top_10_X8)\ntrain_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** \n> label encoder/one hot encoding is successfully applied to the train dataset","metadata":{}},{"cell_type":"code","source":"# Store the data set in train_original_modified to new object train_original_modified_OHE\n# drop the columns which are not required after performing one hot encoding\ntrain_original_modified_OHE = train_original_modified.drop(columns=['X0','X1','X2','X3','X4','X5','X6','X8'])\n# OHE means One hot encoded data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the modified one hot encoded train dataset\ntrain_original_modified_OHE.to_csv('train_original_modified_OHE.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the modified one hot encoded train dataset\ntrain_original_modified_OHE.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** \n> The shape of the train dataset is reduced to 429 from 437 since 8 features were dropped after performing one hot encoding","metadata":{}},{"cell_type":"markdown","source":"## Apply the label encoder/one hot encoding for the test dataset similar to the train dataset","metadata":{}},{"cell_type":"code","source":"# create a list for the top 10 most frequent catergories of features X0, X1, X2,X3,X4,X5,X6 and X8\n# Call the function to perform one hot encoding on features X0,X1,X2,X3,X4,X5,X6 and X8 \n# verify the train dataset after applying one hot encoding for the features X0,X1,X2,X3,X4,X5,X6 and X8 \n\n# X0\ntop_10_test_X0 = [x for x in test_original_modified.X0.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X0',top_10_test_X0)\ntest_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X1\ntop_10_test_X1 = [x for x in test_original_modified.X1.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X1',top_10_test_X1)\ntest_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X2\ntop_10_test_X2 = [x for x in test_original_modified.X2.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X2',top_10_test_X2)\ntest_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X3\ntop_10_test_X3 = [x for x in test_original_modified.X3.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X3',top_10_test_X3)\ntest_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> In case X3, only 7 columns were added as the maximum categorial variable for this feature is only 7.","metadata":{}},{"cell_type":"code","source":"# X4\ntop_10_test_X4 = [x for x in test_original_modified.X4.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X4',top_10_test_X4)\ntest_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> In case X4, only 4 columns were added as the maximum categorial variable for this feature is only 4.","metadata":{}},{"cell_type":"code","source":"# X5\ntop_10_test_X5 = [x for x in test_original_modified.X5.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X5',top_10_test_X5)\ntest_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X6\ntop_10_test_X6 = [x for x in test_original_modified.X6.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X6',top_10_test_X6)\ntest_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X8\ntop_10_test_X8 = [x for x in test_original_modified.X8.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X8',top_10_test_X8)\ntest_original_modified.head(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** \n> label encoder/one hot encoding is successfully applied to the test dataset","metadata":{}},{"cell_type":"code","source":"# Store the data set in test_original_modified to new object test_original_modified_OHE\n# drop the columns which are not required after performing one hot encoding\ntest_original_modified_OHE = test_original_modified.drop(columns=['X0','X1','X2','X3','X4','X5','X6','X8'])\n# OHE means One hot encoded data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the modified one hot encoded train dataset\ntest_original_modified_OHE.to_csv('test_original_modified_OHE.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape of the modified one hot encoded train dataset\ntest_original_modified_OHE.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** \n> The shape of the train dataset is reduced to 428 from 436 since 8 features were dropped after performing one hot encoding","metadata":{}},{"cell_type":"markdown","source":"## Perform dimensionality reduction.","metadata":{}},{"cell_type":"code","source":"# Before performing dimensional reduction aka. principal component anlysis (PCA), seperate the following:\n# features 'ID' and 'y' from train_original_modified_OHE dataset and store it in the new object\n# features 'ID' from test_original_modified_OHE.shape and store it in the new object","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store the 'ID' values into the new object train_ID and test_ID and verify the shape\n\n# Train dataset\ntrain_ID = train_original_modified_OHE.ID\nprint('The shape of the ID feature in train dataset is:',train_ID.shape)\n\n# Test dataset\ntest_ID = test_original_modified_OHE.ID\nprint('\\nThe shape of the ID feature in test dataset is:',test_ID.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store the remaining values into the new object train_X and test_X and verify the shape\n\n# Train dataset\ntrain_X = train_original_modified_OHE.drop(columns = ['ID','y'])\nprint('The shape of the final train dataset is:',train_X.shape)\n\n# Test dataset\ntest_X = test_original_modified_OHE.drop(columns = ['ID'])\nprint('\\nThe shape of the final test dataset is:',test_X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store the 'y' values into the new object train_y and verify the shape\ntrain_y=train_original_modified_OHE.y\nprint('The shape of the target feature of train dataset is:',train_y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> 1. Successfully separated the 'ID' and target feature 'y' from train dataset.\n> 2. Successfully separated the 'ID' feature from test dataset.\n> 3. Also, observed that shape of the train and test data are same.It is therefore good to proceed further.","metadata":{}},{"cell_type":"code","source":"# save these datasets to csv \ntrain_ID.to_csv('train_ID.csv',index=False)\ntest_ID.to_csv('test_ID.csv',index=False)\ntrain_X.to_csv('train_X.csv',index=False)\ntest_X.to_csv('test_X.csv',index=False)\ntrain_y.to_csv('train_y.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import the required PCA library\nfrom sklearn.decomposition import PCA","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Before performing PCA, the data needs to centered and scaled\n# After centring, the average value for each train and train features will be 0\n# After scaling, the standard deviation for each feature will be 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** \n> Since all the features are having 0 and 1, there is no need to standardize.","metadata":{}},{"cell_type":"code","source":"# create a PCA object (instantiate)\npca=PCA()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and transform the train dataset\n# transform the test dataset\n\n# fit the train dataset\npca_fit_train_X = pca.fit(train_X)\n\n# transform the  train dataset\npca_fit_transform_train_X = pca_fit_train_X.transform(train_X)\n\n# transform the test dataset\npca_transform_test_X = pca.transform(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:**\n\n> 1. In the fit step, loading scores and variation of each principal components are calculated\n> 2. fit method is used only for the train dataset as the test dataset will only learn from the train dataset.\n> 3. In the transform step, cordinates for the PCA plot are generated based on the loading scores and scaled data(we have not scaled the data since all features are having 0 and 1 values).\n> 4. test dataset is only transformed. This is because when we feed the test dataset to the algorithm,it will learn the loading scores and variation of each principal components from the train dataset and predict the outcome.   ","metadata":{}},{"cell_type":"code","source":"# visualize the train_X dataset using scree plot to see how many principal components should go into the final plot.\n# Calculate the percentage of variation of each principal components\n# Assuming we have two principal components PC1 and PC2, then  \n# explained_variance_ratio for PC1 = (Variation for PC1/ (Total variation ie (PC1+PC2)))*100\n##\npca_train_X_variation = np.round(pca_fit_train_X.explained_variance_ratio_.cumsum()*100,decimals=1)\n##\n# cumsum() is used to display PC's variation with respect to cumulative percentage","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the cumulative percentage of expalined variance\npca_train_X_variation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> 1. By looking into the array of elements, It is observed that among 427 Principal components, ~90% of variation of the data in train_X dataset is explained by only first 72 principal components . \n> 2. lets validate by visualizing scree plot for the first 72 pricipal components ","metadata":{}},{"cell_type":"code","source":"# create a PCA object again (instantiate) by considering first 72 principal components\npca_72=PCA(n_components=72)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit and transform the train dataset\n# transform the test dataset\n\n# fit the train dataset\npca_fit_train_X_72 = pca_72.fit(train_X)\n\n# transform the train dataset\npca_fit_transform_train_X_72 = pca_fit_train_X_72.transform(train_X)\n\n# transform the test dataset\npca_transform_test_X_72 = pca_72.transform(test_X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## check the variation for the first 72 principal components \npca_train_X_variation_72 = np.round(pca_fit_train_X_72.explained_variance_ratio_.cumsum()*100,decimals=1)\npca_train_X_variation_72\n##","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# assign labels for each PC's as PC1,2,etc., for visulaization in scree plot \n##\nlabels = ['PC' + str(x) for x in range(1, len(pca_train_X_variation_72)+1)]\n##\n# All the first 72 features of train_X data set will be labelled as PC1, PC2, .....+ PC72\n# Eg: feature X10 will be labelled as PC1, etc., \n# Here X10 will be the first feature since we had one hot encoded and dropped the below listed categorical features\n# X0\tX1\tX2\tX3\tX4\tX5\tX6\tX8\n# After one hot encoding, the newly created columns/features will get automatically moved to the end.\n# Hence the first PC1 will be the feature X10 and so on.....","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate the scree plot\n## \nplt.figure(figsize=(25,5))\n##\nplt.bar(x=range(1, len(pca_train_X_variation_72)+1), height=pca_train_X_variation_72,tick_label=labels)\n##\nplt.xticks(rotation=90, color='indigo', size=15)\nplt.yticks(rotation=0, color='indigo', size=15)\n##\n##################\nplt.title('Scree Plot',color='tab:orange', fontsize=25)\n###################\n##\nplt.xlabel('Principal Components', {'color': 'tab:orange', 'fontsize':15})\nplt.ylabel('Cumulative percentage of explained variance ', {'color': 'tab:orange', 'fontsize':15})\n##","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:**\n> Above scree plot shows that considering first 72 principal components should be sufficient to represent the train_X dataset","metadata":{}},{"cell_type":"code","source":"# Draw the 2D PCA plot by considering only PC1 and PC2\n# PCA plot is to visualize how the data is spread across the origin with new coordinates, based on the loading scores and scaling.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"####\n# Put the new coordinates created by pca_fit_transform_train_X_72 into matrix\n# Rows are the observations (X) and columns are the Principal components (Y)\n####\npca_fit_transform_train_X_72_df = pd.DataFrame(pca_fit_transform_train_X_72,columns=labels )\n#####\n# verify the first 2 rows of data with new coordinates\npca_fit_transform_train_X_72_df.head(2)\n#####","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw the 2D PCA plot for PC1 and PC2\n\n##\n## Removing the cumsum() from the earlier expalained ratio calculation\npca_train_X_variation_72_Nocumsum = np.round(pca_fit_train_X_72.explained_variance_ratio_*100,decimals=1)\n##\nplt.title('PCA Plot',color='tab:orange', fontsize=20)\n##\nplt.scatter(pca_fit_transform_train_X_72_df.PC1, pca_fit_transform_train_X_72_df.PC2)\n##\nplt.xticks(rotation=90, color='indigo', size=15)\nplt.yticks(rotation=0, color='indigo', size=15)\n##\nplt.xlabel('PC1 - {0}%'.format(pca_train_X_variation_72_Nocumsum[0]), {'color': 'tab:orange', 'fontsize':15});\nplt.ylabel('PC2 - {0}%'.format(pca_train_X_variation_72_Nocumsum[1]), {'color': 'tab:orange', 'fontsize':15});\n##\n## The principal components are zero-indexed, So, PC1=[0], PC2=[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments:** \n> 1. Above PCA plot shows that how the data is spread along X-axis(PC1) and Y-axis (PC2).\n> 2. 11.9% variance of the data is explained by PC1 and 8.2 % of data is explained by PC2\n> 3. Similarly we visulaize how the data is spread among other pricipal components as well","metadata":{}},{"cell_type":"code","source":"# print the loading scores\n# Loading scores explains the proportion of each observation with respect to each principal components\n#\n#Lets check only for the PC1\n#\nloading_scores = pd.Series(pca_72.components_[0])\n#\n#\n#sort the loading scores based on absolute value\nsorted_loading_scores=loading_scores.abs().sort_values(ascending=False)\n#\n# display only the top 10 loading scores\nsorted_loading_scores[0:10]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the minimum and maximum loading scores of PC1\nprint(sorted_loading_scores.min())\nprint(sorted_loading_scores.max())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**:\n> 1. It can be concluded from the above loading scores that, almost all the observations of the train datasets plays a role in separating the Principal components PC1\n> 2. Example: The 175th observation has a 1 unit long vector consisting of the following:\n>    - 0.191403 * PC1 +.......+ Xn * PCn \n>    - 0.191403 is the proportion of 175th observation for PC1 \n>    - This unit vector is called singular vector or eigen vector for PC1\n> 3. similarly the loading scores will be calculated for PC2 as pca_72.components_[1], etc","metadata":{}},{"cell_type":"code","source":"# From PCA, the final train and test datasets are as follows\n\n#train data\npca_fit_transform_train_X_72.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test data\npca_transform_test_X_72.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train label\ntrain_y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train ID\ntrain_ID.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ID.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predict test_data using XGBoost.","metadata":{}},{"cell_type":"code","source":"# Before predicting the test values, lets check the target variable train_y for any outliers\n# If present, the value will be replaced with median values\n# Using boxplot to identify the outliers\nplt.boxplot(train_y);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: Outliers are observed in the target variable train_y","metadata":{}},{"cell_type":"code","source":"# Print the 50th percentile value which is the median\nprint(train_y.quantile(0.50)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the 95th percentile value \nprint(train_y.quantile(0.95)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Replace the outlier with median values\ntrain_y = np.where(train_y > 120.80600000000001, 99.15, train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify again with box_plot after replacing the outliers with median values\nplt.boxplot(train_y);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the shape again   \ntrain_y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments**: \n> 1. It is evident from the box plot that outliers are replaced with median values in the target variable train_y\n> 2. Also, there is no change in the shape of the target variable. Hence its good to go with further steps","metadata":{}},{"cell_type":"code","source":"# import the required libraries\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score,cross_val_predict\n# Since objective is to predict continuous variable we use XGBregressor\nfrom xgboost import XGBRegressor","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation metrics for regression\n### Mean Absolute Error, Mean Squared Error and R2\n# We will use R2 in this case\n# R2 is also known as Coefficient of Determination\n# It gives the percentage variation in 'y' (test time) explained by 'X'variables\n# or,it gives the percentage of data points that fall within the regression line\n# R2= (1-SSR/SST) \n# SSR- Sum of square residual; SST- Sum of squares total\n# R2 value should be between 0 to 1\n# -R2 valve indicates the worst model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the XGBoost parameters\nprint(XGBRegressor())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the Regressor\n# specifying random_state ensures same result if we run the model multiple times\n# Objective will be automatically set to ''reg:squarederror'\nxgb_reg = xgb.XGBRegressor() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To find best XGBoost Parameters\nparams={ 'learning_rate'   : [0.01,0.05,0.1,1] ,\n         'max_depth'       : [2,3,5,10],\n         'min_child_weight': [ 0, 1, 3],\n         'n_estimators'    : [100,150,200,500],\n         'gamma'           : [1e-2,1e-3,0,0.1,0.01,0.5,1],\n         'colsample_bytree': [0.1,0.5,0.7,1],\n         'subsample'       : [0.2,0.3,0.5,1],\n         'reg_lambda'      : [0,1,10],\n         'reg_alpha'       : [1e-5,1e-3,1e-1,1,1e1] \n        }\n## Explainations of the parameters\n# 'max_depth' - Maximum depth of trees (default = 6, range: [0,∞])\n# 'Learning rate'(eta) - scaling the tree by learning rate predicts the output in smaller steps closer to the actual value.\n# 'reg_lambda' - L1 regularization parameter on weights to avoid overfit\n#  'reg_alpha' - L1 regularization parameter on weights to avoid overfit\n# 'gamma' - Minimum loss reduction required to make a further partition on a leaf node of the tree (pruning)\n# 'min_child_weight' - default =1. If the weights of each leaf is less than the min_child weight, then ramove the leaf\n# So weights of the each leaf is > min_child_weight\n# 'colsample_bytree': It is the subsample ratio of columns when constructing each tree.\n# 'Subsample' is the  ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. This will prevent overfitting. \n# Subsampling will occur once in every boosting iteration.\n# 'n_estimators' is the number of trees","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Optimize the Hyperparameter using RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Random search of parameters with 10 fold cross validation\n# Improve the predictions using cross validation to optimize the parameters\nRandom_Search=RandomizedSearchCV (xgb_reg,params,cv=10, scoring='r2', return_train_score=True, n_jobs=-1,verbose=1) \n# cv=10 - Number of folds in a `(Stratified)KFold`","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the training set to the Randon_Search to obtain the best estimators and parameters.\nRandom_Search.fit(pca_fit_transform_train_X_72,train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the best estimator\nRandom_Search.best_estimator_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Print the best parameters\nRandom_Search.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Instantiate the XGBoost classifier with the best estimators and parameters\nxgb_reg=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.5, gamma=0.01, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=2,\n             min_child_weight=3, missing=None, monotone_constraints='()',\n             n_estimators=500, n_jobs=2, num_parallel_tree=1, random_state=0,\n             reg_alpha=0.1, reg_lambda=10, scale_pos_weight=1, subsample=0.5,\n             tree_method='exact', validate_parameters=1, verbosity=None)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the r2 score of the model using Number of folds in a `(Stratified)KFold` cv=10\nr2_Score = cross_val_score(xgb_reg,pca_fit_transform_train_X_72,train_y,scoring='r2',cv=10)\nr2_Score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the mean r2_score\nprint('r2_score of the model with cross validation is:',round(r2_Score.mean(),2))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Comments** \n> 1. Since r2_score with cross validation is: 0.62 or 62 % which is between 50 to 100%. Hence, its good to proceed with the prediction of time the car takes to pass testing using test data.\n> 2. This means the model explains 62% variability of the target variable (y) around its mean.","metadata":{}},{"cell_type":"code","source":"# Fit the training data\nxgb_reg.fit(pca_fit_transform_train_X_72,train_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict the time taken by car to pass testing using test dataset\nX_test_pred = xgb_reg.predict(pca_transform_test_X_72)\nX_test_pred ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the predicted value (time) in the form of table\ndf_test_pred = pd.DataFrame({'ID': test_ID, 'y': X_test_pred})\n# Print the first 10 predicted values\ndf_test_pred.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the predicted time values \ndf_test_pred.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Conclusion**:\n>  For a given dataset, XGBoost Regressor algorithm with cross validation results in R2 score of **0.62**.","metadata":{}},{"cell_type":"markdown","source":"**Scope of Future Work:**\n> 1.  I have conisdered 72 principal components for the study using principal component analysis(PCA). Features can be further reduced by using different dimensionality reduction techniques [[Reference](https://thenewstack.io/3-new-techniques-for-data-dimensionality-reduction-in-machine-learning/)].\n> 2.  I have used only XGBoost Regressor algorithm for predicting the time, a Mercedes-Benz spends on the test bench. R2 score can be further improved by using other regressor algorithms with hyperparameter tuning and cross validation. \n> 3. As the number of features are more, Deep learning techniques can also be used for the study. ","metadata":{}},{"cell_type":"markdown","source":"**References:**\n> 1. Dataset and problem statement: https://www.kaggle.com/c/mercedes-benz-greener-manufacturing \n> 2. https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion\n> 3. https://medium.com/swlh/greener-manufacturing-with-machine-learning-6ec77d0e7a91\n> 4. How to Perform One Hot Encoding for Multi Categorical Variables https://www.youtube.com/watch?v=6WDFfaYtN6s","metadata":{}}]}