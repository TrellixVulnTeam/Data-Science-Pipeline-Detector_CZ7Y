{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"5c75d654-23bb-0519-f895-8e08ec3bd220"},"source":"**Easy loading & cleaning of the data.**\n\nBig thanks to SRK, on whose scripts data loading part."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6dd15425-782c-67b7-38b7-edfc4461238e"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport gc\n\n\nfrom sklearn.cross_validation import KFold\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing\n\nfrom scipy.sparse import csr_matrix, hstack\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"eec3354c-2ea7-abf3-26d6-843ee0adbfc6"},"outputs":[],"source":"numerical_cols = ['ncodpers', 'age', 'antiguedad', 'renta']\n\n\nfeature_cols = ['ind_actividad_cliente', \n                \"ind_empleado\", \"pais_residencia\" ,\"sexo\" , \"ind_nuevo\", \n                 \"nomprov\", \"segmento\", 'indrel', 'tiprel_1mes', 'indresi', 'indext',\n               'conyuemp', 'indfall', 'canal_entrada']\n\ndtype_list = {'ind_cco_fin_ult1': 'float16', 'ind_deme_fin_ult1': 'float16', 'ind_aval_fin_ult1': 'float16', 'ind_valo_fin_ult1': 'float16', 'ind_reca_fin_ult1': 'float16', 'ind_ctju_fin_ult1': 'float16', 'ind_cder_fin_ult1': 'float16', 'ind_plan_fin_ult1': 'float16', 'ind_fond_fin_ult1': 'float16', 'ind_hip_fin_ult1': 'float16', 'ind_pres_fin_ult1': 'float16', 'ind_nomina_ult1': 'float16', 'ind_cno_fin_ult1': 'float16', 'ncodpers': 'int64', 'ind_ctpp_fin_ult1': 'float16', 'ind_ahor_fin_ult1': 'float16', 'ind_dela_fin_ult1': 'float16', 'ind_ecue_fin_ult1': 'float16', 'ind_nom_pens_ult1': 'float16', 'ind_recibo_ult1': 'float16', 'ind_deco_fin_ult1': 'float16', 'ind_tjcr_fin_ult1': 'float16', 'ind_ctop_fin_ult1': 'float16', 'ind_viv_fin_ult1': 'float16', 'ind_ctma_fin_ult1': 'float16'}\ntarget_cols = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1']\n\ndata_path = \"../input/\"\ntrain_file = data_path + \"train_ver2.csv\"\ntest_file = data_path + \"test_ver2.csv\"\ntrain_size = 13647309\nnrows = 2000000 # change this value to read more rows from train\nstart_index = train_size - nrows"},{"cell_type":"markdown","metadata":{"_cell_guid":"eaf9bb8d-38ed-325f-cb87-b63260ee2805"},"source":"**Loader class:**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f67c163c-776d-5714-b25e-bf1deda6e7eb"},"outputs":[],"source":"class SantanderLoader(object):\n    \n    def __init__(self, train, test):\n        self.train_file = train\n        self.test_file = test\n        \n        \n    def load_categorical(self, feature_cols ):\n        \n        start_index = train_size - nrows\t\n        \n        for ind, col in enumerate(feature_cols):\n            print(col)\n            train = pd.read_csv(self.train_file, usecols=[col])\n            test = pd.read_csv(self.test_file, usecols=[col])\n            train.fillna(-1, inplace=True)\n            test.fillna(-1, inplace=True)\n            if train[col].dtype == \"object\":\n                le = LabelEncoder()\n                le.fit(list(train[col].values) + list(test[col].values))\n                temp_train_X = le.transform(list(train[col].values)).reshape(-1,1)[start_index:,:]\n                temp_test_X = le.transform(list(test[col].values)).reshape(-1,1)\n            else:\n                temp_train_X = np.array(train[col]).reshape(-1,1)[start_index:,:]\n                temp_test_X = np.array(test[col]).reshape(-1,1)\n            if ind == 0:\n                train_X = temp_train_X.copy()\n                test_X = temp_test_X.copy()\n            else:\n                train_X = np.hstack([train_X, temp_train_X])\n                test_X = np.hstack([test_X, temp_test_X])\n            print(train_X.shape, test_X.shape)\n        del train\n        del test\n        print (\"Categorical features loaded.\")\n        return train_X, test_X\n        \n        \n    def load_numeric(self, numeric_cols ):\n        \n        start_index = train_size - nrows\t\n        \n        for ind, col in enumerate(numerical_cols):\n            print(col)\n            train = pd.read_csv(self.train_file, usecols=[col])\n            test = pd.read_csv(self.test_file, usecols=[col])\n            if train[col].dtype == \"object\":\n                temp_train_X = pd.to_numeric(train[col], 'coerce').fillna(-1).astype('float64').reshape(-1,1)[start_index:,:]\n                temp_test_X = pd.to_numeric(test[col], 'coerce').fillna(-1).astype('float64').reshape(-1,1)\n            else:\n                temp_train_X = np.array(pd.to_numeric(train[col], 'coerce').fillna(-999).astype('float64')).reshape(-1,1)[start_index:,:]\n                temp_test_X = np.array(pd.to_numeric(test[col], 'coerce').fillna(-999).astype('float64')).reshape(-1,1)\n            if ind == 0:\n                train_X_f = temp_train_X.copy()\n                test_X_f = temp_test_X.copy()\n            else:\n                train_X_f = np.hstack([train_X_f, temp_train_X])\n                test_X_f = np.hstack([test_X_f, temp_test_X])\n        print (\"Numeric features loaded.\")\n        return train_X_f, test_X_f\n    \n    \n    def load_dates(self, ):\n        \n        start_index = train_size - nrows\t\n        \n        train_X_d = pd.read_csv(self.train_file, usecols = ['fecha_dato', 'fecha_alta'], nrows = nrows)\n        test_X_d = pd.read_csv(self.test_file, usecols = ['fecha_dato', 'fecha_alta'])\n\n        print (\"Date features loaded\")\n        return train_X_d, test_X_d\n    \n    \n    def stack_features(self, cats, nums, dates):\n        \n        cats_df = pd.DataFrame(cats)\n        nums_df = pd.DataFrame(nums)\n        dates_df = pd.DataFrame(dates)\n        \n        stacked = pd.concat((cats_df, nums_df, dates_df), axis = 1)\n        print (\"Columns stacked\")\n        return stacked\n    \n    \n    def name_columns(self, data, set_index = False):\n        \n        #df = pd.DataFrame(data)\n        data.columns = ['ind_actividad_cliente', \n                \"ind_empleado\", \"pais_residencia\" ,\"sexo\" , \"ind_nuevo\", \n                 \"nomprov\", \"segmento\", 'indrel', 'tiprel_1mes', 'indresi', 'indext',\n               'conyuemp', 'indfall', 'canal_entrada',\n                   'ncodpers', 'age', 'antiguedad', 'renta', \n                   'fecha_dato', 'fecha_alta']\n        \n        if set_index:\n            data.set_index(['ncodpers'], inplace = True)\n            print (\"Index set to ncodpers\")\n        \n        print (\"Columns named\")\n        return data\n    \n    \n    def to_csv(self, data):\n        \n        data.to_csv('data.csv', index = False)\n        print (\"File exported to csv\")\n        \n        \n    def label_loader(self, drop_date = False, set_index = False):\n        \n        full_y = pd.read_csv(self.train_file, usecols=['fecha_dato'] + ['ncodpers'] + target_cols, \n                             dtype=dtype_list, nrows = nrows)\n        full_y.fillna(0, inplace = True)\n        \n        if drop_date:\n            full_y.drop(['fecha_dato'], axis = 1, inplace = True)\n            print (\"Date dropped\")\n            \n        if set_index:\n            full_y.set_index(['ncodpers'], inplace = True)\n            print (\"Index set to ncodpers\")\n            \n        \n        print (\"Labels loaded\")\n        return full_y"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1a32eb8-ad95-79a0-6be6-96ea7d01ab01"},"outputs":[],"source":"loader = SantanderLoader(train_file, test_file)\n\ntrain1, test1 = loader.load_categorical(feature_cols)\ntrain2, test2 = loader.load_numeric(numerical_cols)\ntrain3, test3 = loader.load_dates()\n\nstacked_train = loader.stack_features(train1, train2, train3)\nnamed_train = loader.name_columns(stacked_train)\nstacked_test = loader.stack_features(test1, test2, test3)\nnamed_test = loader.name_columns(stacked_test)\n\n# loader.to_csv(named_train) <- if you'd like to save the output for further processing,\n# without needing to load the raw data each time.\n\nprint (named_train.info(memory_usage = True))\n\ndel train1, test1, train2, test2, train3, test3, stacked_train, stacked_test\ngc.collect()\n\ny = loader.label_loader()\nprint (y.head())\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"02dc18ba-5595-860e-a19a-c5b96e5674ec"},"source":"**Cleaner class:**"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16498238-5e79-8d4a-4bae-ec64de7c0dc4"},"outputs":[],"source":"class SantanderCleaner(object):\n    \n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def month(self, year, month):\n        \n        monthly_data = self.data[self.data['fecha_dato'] == \"201{}-0{}-28\".format(year, month) ]\n        monthly_labels = self.labels[self.labels['fecha_dato'] == \"201{}-0{}-28\".format(year, month) ]\n        \n        print (\"Month {} from year {} data shape: \".format(year, month), monthly_data.shape)\n        return monthly_data, monthly_labels\n    \n    def get_same(self, data1, data2):\n        \n        data_1st = data1[data1.ncodpers.isin(data2.ncodpers.values)]\n        data_2nd = data2[data2.ncodpers.isin(data1.ncodpers.values)]\n        \n        print (\"Shape when having same clients: \", data_1st.shape)\n        return data_1st, data_2nd\n    \n    def prepare_to_train(self, data1, labels, ncod_delete = True):\n        \n        if ncod_delete == True:\n            data2 = data1.drop(['ncodpers', 'fecha_dato', 'fecha_alta'], 1)\n            labels1 = labels.drop(['ncodpers', 'fecha_dato'], 1)\n            labels1 = labels1.astype(int)\n        else:\n            data2 = data1.drop(['fecha_dato', 'fecha_alta'], 1)\n            labels1 = labels.drop(['fecha_dato'], 1)\n            labels1 = labels1.astype(int)\n\n        print (\"Final data shape: \", data2.shape, \"Final labels shape:\", labels1.shape)\n        return data2, labels1\n    \n\n    def impute_missing(self, data1):\n\n        imputer_cat = preprocessing.Imputer(-1, 'most_frequent', 0)\n        imputer_renta = preprocessing.Imputer(-999, 'mean', 0)\n        \n        renta = data1.iloc[:, 16:]\n        cat_data = data1.iloc[:, :16]\n\n        cat_data = imputer_cat.fit_transform(cat_data)\n        renta = imputer_renta.fit_transform(renta)\n\n        imputed_data = pd.DataFrame(np.hstack([cat_data, renta]))\n\n        print (\"Data Imputed\")\n        return imputed_data\n\n    def numerical_scale(self, data1):\n\n        scaler = RobustScaler()\n        data1.loc[:,16]= scaler.fit_transform(data1.loc[:,16])\n\n        print (\"Numerical data scaled\")\n        return data1\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9a03dd13-4697-64e8-d154-1ade3cb01987"},"outputs":[],"source":"training = SantanderCleaner(named_train, y)\n\njanuary, january_y = training.month(5, 1)\nfebruary, february_y = training.month(5, 2)\n\njanuary, january_y  = training.prepare_to_train(january, january_y )\nfebruary, february_y = training.prepare_to_train(february, february_y)\n\njanuary = training.impute_missing(january)\nfebruary = training.impute_missing(february)\n\njanuary = training.numerical_scale(january)\nfebruary = training.numerical_scale(february)\n\n\n# For validation\n\nmarch, march_y = training.month(5, 3)\nmarch, march_y = training.prepare_to_train(march, march_y)\nmarch = training.impute_missing(march)\nmarch = training.numerical_scale(march)\n\n#del full_df_train, full_y\n#gc.collect()\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"356d3e6b-f0b0-4455-6f0e-37d556ad45e1"},"outputs":[],"source":"test = SantanderCleaner(named_test, _)\ntest_data = named_test.drop(['ncodpers', 'fecha_dato', 'fecha_alta'], 1)\ntest_data = test.impute_missing(test_data)\ntest_data = test.numerical_scale(test_data)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1124723b-5f95-b8da-09f4-414fab1f9287"},"outputs":[],"source":"train = pd.concat([january, february], axis = 0)\nlabels = pd.concat([january_y, february_y], axis = 0)\n\ntrain2 = train.iloc[:100000, :]\nlabels2 = labels.iloc[:100000, :]\n\nX_train, X_val, y_train, y_val = train_test_split(train2, labels2, test_size = 0.2, \n                                                  random_state = 669)\n\nprint ( type(labels2) )"},{"cell_type":"markdown","metadata":{"_cell_guid":"e73720d6-fcbd-f299-602c-3580391d740a"},"source":"XGB is here rather in order to show how it can be used for the dataset, than a classifier, based on which submission should be sent."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2554ccdb-37f5-da4d-82b4-97b049f7e2b3"},"outputs":[],"source":"from sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom xgboost import XGBClassifier\n\nnb_classes = labels.shape[1]\n\nparams = {\n    'n_estimators': 10,\n    'max_depth': 10,\n}\n\nxgbc = XGBClassifier(**params)\n\n\nova_xgbc = OneVsRestClassifier(xgbc)\nova_xgbc.fit(X_train, y_train)\nova_preds = ova_xgbc.predict(X_val)\n\n\n\nF1 = f1_score(y_val, ova_preds, average = \"macro\")\nprint(\"F1 score: \", F1 )"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e48cf7c6-b175-b216-0f5b-27f6e22072c2"},"outputs":[],"source":"last_instance_df = y.drop_duplicates('ncodpers', keep='last')\nlast_instance_df = last_instance_df.drop(['fecha_dato'], 1)\n\npreds = ova_xgbc.predict(test_data)\nprint (\"Test set predictions done.\", '\\n')\nprint (\"Shape of test predictions: \", preds.shape, '\\n')\n\n\n\nprint(\"Getting last instance dict..\", '\\n')\nlast_instance_df = last_instance_df.fillna(0).astype('int')\ncust_dict = {}\ntarget_cols = np.array(target_cols)\nfor ind, row in last_instance_df.iterrows():\n    cust = row['ncodpers']\n    used_products = set(target_cols[np.array(row[1:])==1])\n    cust_dict[cust] = used_products\n\n    \nprint(\"Creating submission..\")\npreds = np.argsort(preds, axis=1)\npreds = np.fliplr(preds)\ntest_id = np.array(pd.read_csv(test_file, usecols=['ncodpers'])['ncodpers'])\nfinal_preds = []\nfor ind, pred in enumerate(preds):\n    cust = test_id[ind]\n    top_products = target_cols[pred]\n    used_products = cust_dict.get(cust,[])\n    new_top_products = []\n    for product in top_products:\n        if product not in used_products:\n            new_top_products.append(product)\n        if len(new_top_products) == 7:\n            break\n    final_preds.append(\" \".join(new_top_products))\n\nlen(final_preds[0])\nlen(final_preds)\nfinal_preds[0]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bc1ce93d-e916-1bd0-f860-905a35cccc36"},"outputs":[],"source":"out_df = pd.DataFrame({'ncodpers':test_id, 'added_products':final_preds})\nout_df.to_csv('XGBoostClassifier.csv', index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}