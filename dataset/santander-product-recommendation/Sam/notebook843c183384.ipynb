{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0,"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"d31e7dbe-2e65-ae39-9e62-b9bac1dc301c","_active":false},"source":"Step 1. Load training data\nStep 2. Data cleansing of train data\n\tData format\n\tFill empty cells\n\tSeparate age and change outliners\n\tDelete tipodom and cod_prov. \n        Add month\n        Add livetime\nStep 3\tCreate output variable\nStep 4\tDefine X as Input and Y as Output\nStep 5 Define performance metric\nStep 6 Shuffle and split the data into training and testing subsets\nStep 7 Set up model: Decision tree and train it\nStep 8 Test model accuracy\nStep 9 Load real testing data\nStep 10 Data cleansing of testing data\n\tData format\n\tFill empty cells\n\tSeparate age and change outliners\n\tDelete tipodom and cod_prov. \nAdd month\nStep 11 Let the model predict on the testing data\nStep 12 Create submission file","outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"14a13c2b-0f4b-79da-ae2f-049d2913a9cb","_active":false},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%pylab inline\npylab.rcParams['figure.figsize'] = (10, 6)\nlimit_rows   = 7000000\ndf           = pd.read_csv(\"../input/train_ver2.csv\",dtype={\"sexo\":str,\n                                                    \"ind_nuevo\":str,\n                                                    \"ult_fec_cli_1t\":str,\n                                                    \"indext\":str, \"segmento\":str}, nrows=limit_rows)\nunique_ids   = pd.Series(df[\"ncodpers\"].unique())\nlimit_people = 1e4\nunique_id    = unique_ids.sample(n=limit_people)\ndf           = df[df.ncodpers.isin(unique_id)]\n#First convert the dates. There's fecha_dato, the row-identifier date, and fecha_alta, the date #that the customer joined.\ndf[\"fecha_dato\"] = pd.to_datetime(df[\"fecha_dato\"],format=\"%Y-%m-%d\")\ndf[\"fecha_alta\"] = pd.to_datetime(df[\"fecha_alta\"],format=\"%Y-%m-%d\")\n# let's add a month column. \ndf[\"month\"] = pd.DatetimeIndex(df[\"fecha_dato\"]).month\ndf[\"age\"]   = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n#Let's separate the “age” distribution and move the outliers to the mean of the closest one.\ndf.loc[df.age < 18,\"age\"]  = df.loc[(df.age >= 18) & (df.age <= 30),\"age\"].mean(skipna=True)\ndf.loc[df.age > 100,\"age\"] = df.loc[(df.age >= 30) & (df.age <= 100),\"age\"].mean(skipna=True)\ndf[\"age\"].fillna(df[\"age\"].mean(),inplace=True)\ndf[\"age\"]                  = df[\"age\"].astype(int)\n#Next ind_nuevo, which indicates whether a customer is new or not. Let's see if we can fill in #missing values by looking how many months of history these customers have.\nmonths_active = df.loc[df[\"ind_nuevo\"].isnull(),:].groupby(\"ncodpers\", sort=False).size()\n#Looks like these are all new customers, so replace accordingly.\ndf.loc[df[\"ind_nuevo\"].isnull(),\"ind_nuevo\"] = 1\n#Now antiguedad\ndf.antiguedad = pd.to_numeric(df.antiguedad,errors=\"coerce\")\ndf.loc[df.antiguedad.isnull(),\"antiguedad\"] = df.antiguedad.min()\ndf.loc[df.antiguedad <0, \"antiguedad\"]\n#Some entries don't have the date they joined the company. Just give them something in the #middle of the pack\ndates=df.loc[:,\"fecha_alta\"].sort_values().reset_index()\nmedian_date = int(np.median(dates.index.values))\ndf.loc[df.fecha_alta.isnull(),\"fecha_alta\"] = dates.loc[median_date,\"fecha_alta\"]\ndf.loc[df.indrel.isnull(),\"indrel\"] = 1\ndf.drop([\"tipodom\",\"cod_prov\"],axis=1,inplace=True)\ndf.loc[df.ind_actividad_cliente.isnull(),\"ind_actividad_cliente\"] = \\\ndf[\"ind_actividad_cliente\"].median()\ndf.loc[df.nomprov==\"CORU\\xc3\\x91A, A\",\"nomprov\"] = \"CORUNA, A\"\ndf.loc[df.nomprov.isnull(),\"nomprov\"] = \"UNKNOWN\"\ndf.loc[df.renta.isnull(),\"renta\"] = 0\ndf.loc[df.ind_nomina_ult1.isnull(), \"ind_nomina_ult1\"] = 0\ndf.loc[df.ind_nom_pens_ult1.isnull(), \"ind_nom_pens_ult1\"] = 0\nstring_data = df.select_dtypes(include=[\"object\"])\nmissing_columns = [col for col in string_data if string_data[col].isnull().any()]\ndf.loc[df.indfall.isnull(),\"indfall\"] = \"N\"\ndf.loc[df.tiprel_1mes.isnull(),\"tiprel_1mes\"] = \"A\"\ndf.tiprel_1mes = df.tiprel_1mes.astype(\"category\")\n# As suggested by @StephenSmith\nmap_dict = { 1.0  : \"1\",\n            \"1.0\" : \"1\",\n            \"1\"   : \"1\",\n            \"3.0\" : \"3\",\n            \"P\"   : \"P\",\n            3.0   : \"3\",\n            2.0   : \"2\",\n            \"3\"   : \"3\",\n            \"2.0\" : \"2\",\n            \"4.0\" : \"4\",\n            \"4\"   : \"4\",\n            \"2\"   : \"2\"}\n\ndf.indrel_1mes.fillna(\"P\",inplace=True)\ndf.indrel_1mes = df.indrel_1mes.apply(lambda x: map_dict.get(x,x))\ndf.indrel_1mes = df.indrel_1mes.astype(\"category\")\nunknown_cols = [col for col in missing_columns if col not in [\"indfall\",\"tiprel_1mes\",\"indrel_1mes\"]]\nfor col in unknown_cols:\n    df.loc[df[col].isnull(),col] = \"UNKNOWN\"\n# let’s add a customer lifetime column in months\ndf[\"lifetime\"] = (df.fecha_dato - df.fecha_alta).astype('timedelta64[M]')\n#Convert the feature columns into integer values\nfeature_cols = df.iloc[:1,].filter(regex=\"ind_+.*ult.*\").columns.values\nfor col in feature_cols:\n    df[col] = df[col].astype(int)\n#Now for the main event. To study trends in customers adding or removing services, I will #create a label for each product and month that indicates whether a customer added, dropped #or maintained that service in that billing cycle. I will do this by assigning a numeric id to each #unique time stamp, and then matching each entry with the one from the previous month. The #difference in the indicator value for each product then gives the desired value.\nunique_months = pd.DataFrame(pd.Series(df.fecha_dato.unique()).sort_values()).reset_index(drop=True)\nunique_months[\"month_id\"] = pd.Series(range(1,1+unique_months.size)) \n# start with month 1, not 0 to match what we already have\nunique_months[\"month_next_id\"] = 1 + unique_months[\"month_id\"]\nunique_months.rename(columns={0:\"fecha_dato\"},inplace=True)\ndf = pd.merge(df,unique_months,on=\"fecha_dato\")\n#Now I'll build a function that will convert differences month to month into a meaningful label. #Each month, a customer can either maintain their current status with a particular product, add #it, or drop it.\ndef status_change(x):\n    diffs = x.diff().fillna(0)# first occurrence will be considered Maintained, \n    #which is a little lazy. A better way would be to check if \n    #the earliest date was the same as the earliest we have in the dataset\n    #and consider those separately. Entries with earliest dates later than that have \n    #joined and should be labeled as \"Added\"\n    label = [\"Added\" if i==1 \\\n         else \"Dropped\" if i==-1 \\\n         else \"Maintained\" for i in diffs]\n    return label\ndf.loc[:, feature_cols] = df.loc[:, [i for i in feature_cols]+[\"ncodpers\"]].groupby(\"ncodpers\").transform(status_change)\ndf = pd.melt(df, id_vars   = [col for col in df.columns if col not in feature_cols],\n            value_vars= [col for col in feature_cols])\ndf = df.loc[df.value!=\"Maintained\",:]\n#df.head()\ndf2 = df.copy()\ndf2 = df2.loc[df2.value==\"Added\",:]\n\n\n#Feature columns all except fecha_dato, fecha_alta, tipodom and cod_prov. Add month\nfeatures = [\"ncodpers\",\"ind_empleado\",\"pais_residencia\",\"sexo\",\"age\",\"ind_nuevo\",\"antiguedad\",\"indrel\",\"ult_fec_cli_1t\",\"indrel_1mes\",\"tiprel_1mes\",\"indresi\",\"indext\",\"conyuemp\",\"canal_entrada\",\"indfall\",\"nomprov\",\"ind_actividad_cliente\",\"renta\",\"segmento\",\"lifetime\",\"month\"]\nX1 = df[features]\nX2 = df2[features]\n#labels = [\"variable\",\"value\"]\n#Y = df[labels]\ny1 = df[\"value\"]\ny2 = df2[\"variable\"]\ndel df, df2"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16760009-8055-e34d-cc4b-d7ea8aba1a15","_active":false},"outputs":[],"source":"#Jetzt geht es zu den wirklichen Testdaten\ndf           = pd.read_csv(\"../input/test_ver2.csv\",dtype={\"sexo\":str,\n                                                    \"ind_nuevo\":str,\n                                                    \"ult_fec_cli_1t\":str,\n                                                    \"indext\":str}, nrows=limit_rows)\nunique_ids   = pd.Series(df[\"ncodpers\"].unique())\nlimit_people = 1e4\nunique_id    = unique_ids.sample(n=limit_people)\ndf           = df[df.ncodpers.isin(unique_id)]\n#First convert the dates. There's fecha_dato, the row-identifier date, and fecha_alta, the date #that the customer joined.\ndf[\"fecha_dato\"] = pd.to_datetime(df[\"fecha_dato\"],format=\"%Y-%m-%d\")\ndf[\"fecha_alta\"] = pd.to_datetime(df[\"fecha_alta\"],format=\"%Y-%m-%d\")\n# let’s add a customer lifetime column in months\ndf[\"lifetime\"] = (df.fecha_dato - df.fecha_alta).astype('timedelta64[M]')\n# let's add a month column. \ndf[\"month\"] = pd.DatetimeIndex(df[\"fecha_dato\"]).month\ndf[\"age\"]   = pd.to_numeric(df[\"age\"], errors=\"coerce\")\n#Let's separate the “age” distribution and move the outliers to the mean of the closest one.\ndf.loc[df.age < 18,\"age\"]  = df.loc[(df.age >= 18) & (df.age <= 30),\"age\"].mean(skipna=True)\ndf.loc[df.age > 100,\"age\"] = df.loc[(df.age >= 30) & (df.age <= 100),\"age\"].mean(skipna=True)\ndf[\"age\"].fillna(df[\"age\"].mean(),inplace=True)\ndf[\"age\"]                  = df[\"age\"].astype(int)\n#Next ind_nuevo, which indicates whether a customer is new or not. Let's see if we can fill in #missing values by looking how many months of history these customers have.\nmonths_active = df.loc[df[\"ind_nuevo\"].isnull(),:].groupby(\"ncodpers\", sort=False).size()\n#Looks like these are all new customers, so replace accordingly.\ndf.loc[df[\"ind_nuevo\"].isnull(),\"ind_nuevo\"] = 1\n#Now antiguedad\ndf.antiguedad = pd.to_numeric(df.antiguedad,errors=\"coerce\")\ndf.loc[df.antiguedad.isnull(),\"antiguedad\"] = df.antiguedad.min()\ndf.loc[df.antiguedad <0, \"antiguedad\"]\n#Some entries don't have the date they joined the company. Just give them something in the #middle of the pack\ndates=df.loc[:,\"fecha_alta\"].sort_values().reset_index()\nmedian_date = int(np.median(dates.index.values))\ndf.loc[df.fecha_alta.isnull(),\"fecha_alta\"] = dates.loc[median_date,\"fecha_alta\"]\ndf.loc[df.indrel.isnull(),\"indrel\"] = 1\ndf.drop([\"tipodom\",\"cod_prov\"],axis=1,inplace=True)\ndf.loc[df.ind_actividad_cliente.isnull(),\"ind_actividad_cliente\"] = \\\ndf[\"ind_actividad_cliente\"].median()\ndf.loc[df.nomprov==\"CORU\\xc3\\x91A, A\",\"nomprov\"] = \"CORUNA, A\"\ndf.loc[df.nomprov.isnull(),\"nomprov\"] = \"UNKNOWN\"\ndf.loc[df.renta.isnull(),\"renta\"] = 0\n#df.loc[df.ind_nomina_ult1.isnull(), \"ind_nomina_ult1\"] = 0\n#df.loc[df.ind_nom_pens_ult1.isnull(), \"ind_nom_pens_ult1\"] = 0\nstring_data = df.select_dtypes(include=[\"object\"])\nmissing_columns = [col for col in string_data if string_data[col].isnull().any()]\ndf.loc[df.indfall.isnull(),\"indfall\"] = \"N\"\ndf.loc[df.tiprel_1mes.isnull(),\"tiprel_1mes\"] = \"A\"\ndf.tiprel_1mes = df.tiprel_1mes.astype(\"category\")\n# As suggested by @StephenSmith\nmap_dict = { 1.0  : \"1\",\n            \"1.0\" : \"1\",\n            \"1\"   : \"1\",\n            \"3.0\" : \"3\",\n            \"P\"   : \"P\",\n            3.0   : \"3\",\n            2.0   : \"2\",\n            \"3\"   : \"3\",\n            \"2.0\" : \"2\",\n            \"4.0\" : \"4\",\n            \"4\"   : \"4\",\n            \"2\"   : \"2\"}\n\ndf.indrel_1mes.fillna(\"P\",inplace=True)\ndf.indrel_1mes = df.indrel_1mes.apply(lambda x: map_dict.get(x,x))\ndf.indrel_1mes = df.indrel_1mes.astype(\"category\")\nunknown_cols = [col for col in missing_columns if col not in [\"indfall\",\"tiprel_1mes\",\"indrel_1mes\"]]\nfor col in unknown_cols:\n    df.loc[df[col].isnull(),col] = \"UNKNOWN\"\n#Convert the feature columns into integer values\nfeature_cols = df.iloc[:1,].filter(regex=\"ind_+.*ult.*\").columns.values\nfor col in feature_cols:\n    df[col] = df[col].astype(int)\n\n\nunique_months = pd.DataFrame(pd.Series(df.fecha_dato.unique()).sort_values()).reset_index(drop=True)\nunique_months[\"month_id\"] = pd.Series(range(1,1+unique_months.size)) \n# start with month 1, not 0 to match what we already have\nunique_months[\"month_next_id\"] = 1 + unique_months[\"month_id\"]\nunique_months.rename(columns={0:\"fecha_dato\"},inplace=True)\ndf = pd.merge(df,unique_months,on=\"fecha_dato\")\n\n\n \nX_Test = df[features]\nX_Test.head()\ndel df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"33292cb3-674d-82ed-02c5-0955ef9246a0","_active":false},"outputs":[],"source":"X = pd.concat([X1,X_Test], axis=0)\nX1_rows = X1.shape[0]\n\ndef preprocess_features(X):\n    ''' Preprocesses the data and converts non-numeric binary variables into binary (0/1) variables. Converts categorical variables into dummy variables. '''\n    # Initialize new output DataFrame\n    output = pd.DataFrame(index = X.index)\n    # Investigate each feature column for the data\n    for col, col_data in X.iteritems():\n        # If data type is non-numeric, replace all yes/no values with 1/0\n        if col_data.dtype == object:\n            col_data = col_data.replace(['yes', 'no'], [1, 0])\n        # If data type is categorical, convert to dummy variables\n        if col_data.dtype == object:\n            # Example: 'school' => 'school_GP' and 'school_MS'\n            col_data = pd.get_dummies(col_data, prefix = col)  \n        # Collect the revised columns\n        output = output.join(col_data)\n    return output\n\nX = preprocess_features(X)\nX1 = X.iloc[:X1_rows, :] \nX_Test = X.iloc[X1_rows:, :]\n\nX2 = preprocess_features(X2)\ny1 = y1.replace([\"Added\", \"Dropped\"], [1, 0])\ny2 = y2.replace(['ind_cco_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_pres_fin_ult1',\n       'ind_reca_fin_ult1', 'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1',\n       'ind_viv_fin_ult1', 'ind_nomina_ult1', 'ind_nom_pens_ult1',\n       'ind_recibo_ult1'], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"da067323-605a-96eb-3adc-39032a6e149c","_active":false},"outputs":[],"source":"from sklearn.metrics import f1_score\ndef performance_metric(y_true, y_predict):\n    \"\"\" Calculates and returns the performance score between \n        true and predicted values based on the metric chosen. \"\"\"\n    score = f1_score(y_true, y_predict)\n    return score\n\n\n# TODO: Import 'train_test_split'\nfrom sklearn.cross_validation import train_test_split\n\n#Shuffle and split the data into training and testing subsets\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.20, random_state=42)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"23c5e58d-f7d3-f156-7348-35c967a39b4d","_active":false},"outputs":[],"source":"from sklearn.metrics import make_scorer\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.cross_validation import ShuffleSplit\n\ndef fit_model(X, y):\n    \"\"\" Performs grid search over the 'max_depth' parameter for a \n        decision tree model trained on the input data [X, y]. \"\"\"\n    \n    # Create cross-validation sets from the training data\n    cv_sets = ShuffleSplit(X.shape[0], n_iter = 10, test_size = 0.20, random_state = 0)\n\n    # Create a decision tree regressor object\n    classifier = DecisionTreeClassifier()\n#classifier = RandomForestClassifier()\n\n    # Create a dictionary for the parameter 'max_depth' with a range from 1 to 10\n    #params = [{'max_depth': range(1,11)}]\n    params = [{'max_depth': [2,3,4,5,6]}]\n \n    # Transform 'performance_metric' into a scoring function using 'make_scorer' \n    scoring_fnc = make_scorer(performance_metric)\n\n    # Create the grid search object\n    grid = GridSearchCV(classifier, params, cv = cv_sets)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(X, y)\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_\n\n\nmod1 = fit_model(X1_train, y1_train)\ny1_predict = mod1.predict(X1_test)\n\n\nmod1.get_params()['max_depth']\nperformance_metric(y1_test, y1_predict)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2bf0f249-1ee7-7aac-1075-2549dce770bc","_active":false},"outputs":[],"source":"mod1"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3578b6d5-8206-2cf3-8a88-4f686e9afda1","_active":false},"outputs":[],"source":"mod1.feature_importances_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4708301-0454-dc5a-def2-715de860929a","_active":false},"outputs":[],"source":"X1_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b751df4-cc65-e4e6-abe5-9380b802e642","_active":false},"outputs":[],"source":"importances = mod1.feature_importances_\nindices = np.argsort(importances)[::-1]"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7ac2f605-3009-4767-2d6f-f49742b4c006","_active":false},"outputs":[],"source":"indices"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c7ff17a-9b40-a476-8a5c-b54714d46d2b","_active":false},"outputs":[],"source":"names=X1_train.columns\nimp,names=zip(*sorted(zip(importances,names)))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7980e07b-70cc-dcf2-0bff-c4964c9e934a","_active":false},"outputs":[],"source":"for f in range(X1_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c5f2a5c4-b162-e2d1-89aa-12cdc27c2a8f","_active":false},"outputs":[],"source":"X1_train.iloc[:,263]"}]}