{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"/kaggle/input/santander-product-recommendation/test_ver2.csv.zip\",encoding=\"latin1\", compression=\"zip\")\ntest.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/santander-product-recommendation/sample_submission.csv.zip\",encoding=\"latin1\", compression=\"zip\")\nsample.head(100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/santander-product-recommendation/train_ver2.csv.zip\",encoding=\"latin1\", compression=\"zip\",nrows=10000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check all features' values to see exceptions and errors on data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"for ft in df.columns:\n    print(ft,\" : \",df[ft].unique(),\" : \",len(df[ft].unique()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. ult_fec_cli_1t, : Last date as primary customer (if he isn't at the end of the month) -> deleting feature\n1. conyuemp, : Spouse index. 1 if the customer is spouse of an employee                  -> filling with 0\n1. renta, : Gross income of the household                                                -> (filling with AVG value)\n1. canal_entrada : channel used by the customer to join                                  -> can't fill, delete feature or nans \n1. nomprov and cod_prov is null at same way                                              -> drop nan   \n1. tiprel_1mes and indrel_1mes at same way                                               -> drop nan \n\nfeatures contains a lot of nan variables. We can drop these features or fill nan values with any logic.    ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* ncodpers              -> customer id, REMAIN SAME\n* ind_empleado          -> 5 classes and a NAN ,ohe\n* pais_residencia       -> 118 classes and a NAN, ohe\n* sexo                  -> 2 classes and a NAN, ohe\n* age                   -> have value 'NA' divide by 100 OR apply binning \n* fecha_alta            -> have NAN, date of beginning the journey of customer in bank\n* ind_nuevo             -> 1 if customer is new and a NAN -> remain same\n* antiguedad            -> the time customer is a customer, have NA and extreme numbers -99999 -> min max scale OR binning\n* indrel                -> 2 classes 1, 9 and a NAN  \n* ult_fec_cli_1t        -> date, have NAN\n* indrel_1mes           -> 5 classes, 1,2,3,4,5,P , different type of values [1.0 nan 3.0 '1.0' '1' 'P' .. etc]  \n* tiprel_1mes           -> 4 classes, ohe, have NAN and 'N'\n* indresi               -> 2 classes, ohe, have NAN\n* indext                -> 2 classes, ohe, have NAN\n* conyuemp              -> 2 classes, ohe, have NAN (it means if it is a spurse of an employee)\n* canal_entrada         -> 162 classes,have NAN, ohe or DIFFERENT ENCODING\n* indfall               -> is dead, 2 classes (have NAN values), ohe\n* tipodom               -> 2 classes, 1 or NAN, convert na to 0\n* cod_prov              -> 53 classes with a NAN, [\"52.\",\"4.\"...]\n* nomprov               -> province name, 53 classes, ohe OR DIFFERENT ENCODING\n* ind_actividad_cliente -> 0 or 1 or NAN, delete NAN?\n* renta                 -> income, scale OR binning\n* segmento              -> 3 classes and NAN, ohe, delete NAN?\n* ind_nomina_ult1       -> 0, 1 or NAN\n* ind_nom_pens_ult1     -> 0, 1 or NAN\n* others                -> REMAIN SAME  ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Handling NANs\n* ind_empleado          -> fill with 'N' or delete\n* pais_residencia       -> drop nan\n* sexo                  -> drop nan\n* age                   -> fill with most popular value \n* fecha_alta            -> delete, we mustn't use that feature anyway, we have antiguedad\n* ind_nuevo             -> fill NAN with 1(new customer)\n* antiguedad            -> fill 'NA' and negative numbers to 1\n* indrel                -> fill NAN to 99 OR DELETE FEATURE   \n* indresi               -> drop nan\n* indext                -> drop nan\n* indfall               -> fill nan as dead ('S')\n* tipodom               -> fill nan to 0\n* ind_actividad_cliente -> fill nan to 0 (not active) or drop  \n* segmento              -> drop nan\n* ind_nomina_ult1       -> drop nan\n* ind_nom_pens_ult1     -> drop nan","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### My Data Processing and Binning Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def processData(df,getDummies=True):\n    data = df.copy()\n    data.drop(columns=[\"ult_fec_cli_1t\",\"canal_entrada\"],inplace=True)\n    data[\"conyuemp\"] = data[\"conyuemp\"].fillna('N')\n    data[\"renta\"] = data[\"renta\"].fillna(data[\"renta\"].mean())\n    #Filling nan vlaues \n    def is_float(string):\n      try:\n        return float(string) or float(string)==0  \n      except:  # String is not a number\n        return False\n\n    data[\"ind_empleado\"] = data[\"ind_empleado\"].fillna('N')\n\n    data[\"age\"] = data[\"age\"].replace(' ', '', regex=True)\n    data[\"age\"] = data[\"age\"].replace('.', '')\n\n    data[\"age\"] = data[\"age\"].replace('NA',np.nan)\n    data[\"age\"] = data[\"age\"].astype(float)\n\n    data[\"ind_nuevo\"] = data[\"ind_nuevo\"].fillna(1)\n\n\n    data[\"antiguedad\"] = data[\"antiguedad\"].replace(' ', '', regex=True)\n    data[\"antiguedad\"] = data.loc[:,\"antiguedad\"].replace(\"NA\",1)\n    data[\"antiguedad\"] = data[\"antiguedad\"].astype(int)\n    data.loc[data.antiguedad<0,\"antiguedad\"] = 1    \n    data[\"indfall\"] = data[\"indfall\"].fillna('S')\n    data[\"tipodom\"] = data[\"tipodom\"].fillna(0)\n    data[\"ind_actividad_cliente\"] = data[\"ind_actividad_cliente\"].fillna(0)\n    data.drop(columns=[\"indrel\",\"fecha_alta\",\"nomprov\"],inplace=True)\n    data = data.dropna()    \n    if(getDummies):\n        data = pd.get_dummies(data, columns=['indresi','indext','conyuemp','indfall','sexo',\n                                         'pais_residencia','ind_empleado','tiprel_1mes',\n                                        \"segmento\"],drop_first=True)\n    return data\ndef dataBin(df, getDummies=True):\n    binned = df.copy()\n    binned[\"renta\"] = pd.qcut(binned[\"renta\"], 3, labels=[\"low\", \"mid\", \"high\"])\n    binned[\"age\"] = pd.cut(binned[\"age\"], [0, 40, 80,200], labels=[\"low\",\"mid\",\"high\"])\n    binned[\"antiguedad\"] = pd.cut(binned[\"antiguedad\"], [0, 50, 150,250], labels=[\"low\",\"mid\",\"high\"])\n    if(getDummies):\n        binned = pd.get_dummies(binned, columns=[\"renta\",\"age\",\"antiguedad\",\"cod_prov\"],drop_first=True)\n    return binned\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\ndata = processData(df)\nsns.set_style('whitegrid')\n#Binning on renta, age, antiguedad\ndata['age'].plot(kind='hist')\nplt.show()\ndata['renta'].plot(kind='hist')\nplt.show()\n\ndata['antiguedad'].plot(kind='hist')\nplt.show()\n\n#age can be binned: 0-40, 40-80, 80+\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['antiguedad'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Binning to renta, age and antiguedad","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"binned= dataBin(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ItemNames","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ItemNames = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1',]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get Ratings By Sale Count","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"itemcols = ItemNames\nitemcols.append(\"ncodpers\") \n\n#binned\nUserItemMatrix = df[itemcols]\nUserItemMatrix = UserItemMatrix.groupby(\"ncodpers\").sum() \nUserItemMatrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Set Ratings Dictionary ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings_dict = {\n    \"item\": [],\n    \"user\": [],\n    \"rating\": [],\n}\nUserItemMatrix \n\nindexes = UserItemMatrix.index\nitems = UserItemMatrix.columns\nfor i in range(len(UserItemMatrix)):\n    for j in range(len(items)):\n        rating = UserItemMatrix.iloc[i,j]\n        \n        if(rating>0):\n            ratings_dict[ \"item\" ].append(items[j])\n            ratings_dict[ \"user\" ].append(indexes[i])\n            ratings_dict[\"rating\"].append(rating)\n        else:\n            ratings_dict[ \"item\" ].append(items[j])\n            ratings_dict[ \"user\" ].append(indexes[i])\n            ratings_dict[\"rating\"].append(0)#UserItemMatrixCrop.iloc[:,j].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fine Tuning and Using SVD Collabrative Filtering algorithm using Scikit-Suprise ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise import Reader, Dataset\n\nmax_rating = UserItemMatrix.max().values.max()\ndfRatings = pd.DataFrame(ratings_dict)\nreader = Reader(rating_scale=(0, max_rating))\n\ndata = Dataset.load_from_df(dfRatings[[\"user\", \"item\", \"rating\"]], reader)\n\nfrom surprise import SVD\nfrom surprise.model_selection import GridSearchCV\n\nparam_grid = {\n    \"n_epochs\": [1, 30],\n    \"lr_all\": [0.002, 0.003],\n    \"reg_all\": [0.4]\n}\ngs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=3,refit=True)\ngs.fit(data)\n\nprint(gs.best_score[\"rmse\"])\nprint(gs.best_params[\"rmse\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from surprise.model_selection import KFold\nfrom surprise import accuracy\nfrom surprise import KNNBasic\n\nkf = KFold(n_splits=5)\nsim_options = {'name': 'cosine',\n               'user_based': True  # compute  similarities between items\n               }\nalgo = SVD()\n\nfor trainset, testset in kf.split(data):\n\n    # train and test algorithm.\n    algo.fit(trainset)\n    predictions = algo.test(testset)\n    # Compute and print Root Mean Squared Error\n    accuracy.mae(predictions, verbose=True)\n    accuracy.rmse(predictions, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"testPersons = test[\"ncodpers\"]\nsubmission = {\"ncodpers\":[],\"added_products\":[]}\nfor personid in testPersons.values:\n    preds = \"\"\n    for itemName in ItemNames:\n        pred= gs.predict(personid,itemName)\n        prob = pred[3]/max_rating\n        if(prob>0.05):\n            preds+= \" \" + itemName if preds != \"\" else itemName\n    submission[\"ncodpers\"].append(personid)\n    submission[\"added_products\"].append(preds)        \nSubmissiondf = pd.DataFrame(data=submission)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Submissiondf.set_index(\"ncodpers\")\nSubmissiondf.to_csv(\"sub7MData5e-2.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TURICREATE AND INCLUDING USER INFORMATIONS","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Drop date and items to get user information","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"featuresNotUserInfo = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1',\"fecha_dato\"]\ndata = processData(df.drop(columns=featuresNotUserInfo),getDummies = False)\ndata = dataBin(data,getDummies= False)\nuserInfo = data.groupby(\"ncodpers\").last()\nuserInfo = userInfo.reset_index()\n\nuserInfo.rename(columns={\"ncodpers\":\"user_id\"},inplace=True)\nuserInfo = userInfo.to_dict()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create and Train Recommender System","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install turicreate\nimport turicreate as tc\n\nSF_userInfo = tc.SFrame(userInfo)\n#!pip install turicreate\nturiDict = {}\nturiDict[\"item_id\"] = ratings_dict[\"item\"]\nturiDict[\"user_id\"] = ratings_dict[\"user\"]\nturiDict[\"rating\"] = ratings_dict[\"rating\"]\nactions  = tc.SFrame(turiDict)\ntraining_data, validation_data = tc.recommender.util.random_split_by_user(actions)\nmodel = tc.recommender.create(training_data,target='rating',user_data=SF_userInfo)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save recommender Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"recommendations.model\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check Item Similarity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ItemNames = ['ind_ahor_fin_ult1', 'ind_aval_fin_ult1', 'ind_cco_fin_ult1',\n       'ind_cder_fin_ult1', 'ind_cno_fin_ult1', 'ind_ctju_fin_ult1',\n       'ind_ctma_fin_ult1', 'ind_ctop_fin_ult1', 'ind_ctpp_fin_ult1',\n       'ind_deco_fin_ult1', 'ind_deme_fin_ult1', 'ind_dela_fin_ult1',\n       'ind_ecue_fin_ult1', 'ind_fond_fin_ult1', 'ind_hip_fin_ult1',\n       'ind_plan_fin_ult1', 'ind_pres_fin_ult1', 'ind_reca_fin_ult1',\n       'ind_tjcr_fin_ult1', 'ind_valo_fin_ult1', 'ind_viv_fin_ult1',\n       'ind_nomina_ult1', 'ind_nom_pens_ult1', 'ind_recibo_ult1',]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.get_similar_items(ItemNames, k=24)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"collapsed":true},"cell_type":"code","source":"testPersons = test[\"ncodpers\"]\n#Recommendation skorları\nrecommendations = model.recommend(testPersons.values)\nsubmission = {\"ncodpers\":[],\"added_products\":[]}\n\n#0.9 thresholddakilerin üstü alınır\nusers = recommendations[recommendations[\"score\"]>0.9]\n\n#Test dosyasındak her user için, öneri skorlarında o user id'ye denk gelen itemleri ekle\nfor id in testPersons.values:\n    submission[\"ncodpers\"].append(id)\n    itemsOfUser = users[users[\"user_id\"]==id][\"item_id\"]\n    \n    #userların alacağı tüm itemleri boşluk ile ayırıp string haline getir\n    itemString = \"\"\n    for item in itemsOfUser:\n        itemString += \" \"+item\n        \n    print(id,\" : \",itemString)    \n    submission[\"added_products\"].append(itemString)  \nSubmissiondf = pd.DataFrame(data=submission)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Submissiondf.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Submissiondf.set_index(\"ncodpers\")\nSubmissiondf.to_csv(\"turicreate.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}