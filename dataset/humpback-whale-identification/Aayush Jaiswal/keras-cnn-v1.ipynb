{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mplimg\nfrom matplotlib.pyplot import imshow\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom keras import layers\nfrom keras.preprocessing import image\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout\nfrom keras.models import Model\n\nimport keras.backend as K\nfrom keras.models import Sequential\n\nimport warnings\nwarnings.simplefilter(\"ignore\", category=DeprecationWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/humpback-whale-identification/train.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepareImages(data, m, dataset):\n    print(\"Preparing images\")\n    X_train = np.zeros((m, 224, 224, 3))\n    count = 0\n    \n    for fig in data['Image']:\n        #load images into images of size 100x100x3\n        img = image.load_img(\"../input/humpback-whale-identification/\"+dataset+\"/\"+fig, target_size=(224, 224, 3))\n        x = image.img_to_array(img)\n        x = preprocess_input(x)\n\n        X_train[count] = x\n        if (count%500 == 0):\n            print(\"Processing image: \", count+1, \", \", fig)\n        count += 1\n    \n    return X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_labels(y):\n    values = np.array(y)\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values)\n    # print(integer_encoded)\n\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n    # print(onehot_encoded)\n\n    y = onehot_encoded\n    # print(y.shape)\n    return y, label_encoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y, label_encoder = prepare_labels(train_df['Id'])\ny.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = prepareImages(train_df, train_df.shape[0], \"train\")\nX /= 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = Sequential()\n\nmodel1.add(Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0', input_shape = (224, 224, 3)))\nmodel1.add(BatchNormalization(axis = 3, name = 'bn0'))\nmodel1.add(Activation('relu'))\nmodel1.add(MaxPooling2D((2, 2), name='max_pool_0'))\n\nmodel1.add(Conv2D(64, (5, 5), strides = (2, 2), name=\"conv1\"))\nmodel1.add(BatchNormalization(axis = 3, name = 'bn2'))\nmodel1.add(Activation('relu'))\nmodel1.add(AveragePooling2D((2, 2), name='avg_pool'))\n\nmodel1.add(Conv2D(128, (3, 3), strides = (1,1), name=\"conv2\"))\nmodel1.add(Conv2D(128, (1, 1), strides = (1,1), name=\"conv3\"))\nmodel1.add(BatchNormalization(axis = 3, name = 'bn3'))\nmodel1.add(Activation('relu'))\nmodel1.add(MaxPooling2D((2, 2), name='max_pool_2'))\n\nmodel1.add(Flatten())\n\nmodel1.add(Dense(500, activation=\"relu\", name='rl'))\nmodel1.add(Dropout(0.8))\n\nmodel1.add(Dense(y.shape[1], activation='softmax', name='sm'))\n\nmodel1.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\nmodel1.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history1 = model1.fit(X, y, epochs=50, batch_size=128, validation_split=0.30)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history1.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# summarize history for accuracy\nplt.plot(history1.history['accuracy'])\nplt.plot(history1.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history1.history['loss'])\nplt.plot(history1.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test = os.listdir(\"../input/humpback-whale-identification/test/\")\nprint(len(test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = ['Image']\ntest_df = pd.DataFrame(test, columns=col)\ntest_df['Id'] = ''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = prepareImages(test_df, test_df.shape[0], \"test\")\nX_test /= 255","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = model1.predict(np.array(X_test), verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i, pred in enumerate(predictions):\n    test_df.loc[i, 'Id'] = ' '.join(label_encoder.inverse_transform(pred.argsort()[-5:][::-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head(10)\ntest_df.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test, label_encoder = prepare_labels(test_df['Id'])\ny_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.evaluate(X_test, y_test)\nprint()\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import base64\nimport pandas as pd\nfrom IPython.display import HTML\n\ndef create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):\n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ndf = pd.DataFrame(data = [[1,2],[3,4]], columns=['Col 1', 'Col 2'])\ncreate_download_link(test_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential() \n  \n# 1st Convolutional Layer \nmodel.add(Conv2D(filters = 96, input_shape = (224, 224, 3),  \n            kernel_size = (11, 11), strides = (4, 4),  \n            padding = 'valid')) \nmodel.add(Activation('relu')) \n# Max-Pooling  \nmodel.add(MaxPooling2D(pool_size = (2, 2), \n            strides = (2, 2), padding = 'valid')) \n# Batch Normalisation \nmodel.add(BatchNormalization()) \n  \n# 2nd Convolutional Layer \nmodel.add(Conv2D(filters = 256, kernel_size = (11, 11),  \n            strides = (1, 1), padding = 'valid')) \nmodel.add(Activation('relu')) \n# Max-Pooling \nmodel.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2),  \n            padding = 'valid')) \n# Batch Normalisation \nmodel.add(BatchNormalization()) \n  \n# 3rd Convolutional Layer \nmodel.add(Conv2D(filters = 384, kernel_size = (3, 3),  \n            strides = (1, 1), padding = 'valid')) \nmodel.add(Activation('relu')) \n# Batch Normalisation \nmodel.add(BatchNormalization()) \n  \n# 4th Convolutional Layer \nmodel.add(Conv2D(filters = 384, kernel_size = (3, 3),  \n            strides = (1, 1), padding = 'valid')) \nmodel.add(Activation('relu')) \n# Batch Normalisation \nmodel.add(BatchNormalization()) \n  \n# 5th Convolutional Layer \nmodel.add(Conv2D(filters = 256, kernel_size = (3, 3),  \n            strides = (1, 1), padding = 'valid')) \nmodel.add(Activation('relu')) \n# Max-Pooling \nmodel.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2),  \n            padding = 'valid')) \n# Batch Normalisation \nmodel.add(BatchNormalization()) \n  \n# Flattening \nmodel.add(Flatten()) \n  \n# 1st Dense Layer \nmodel.add(Dense(4096, input_shape = (224*224*3, ))) \nmodel.add(Activation('relu')) \n# Add Dropout to prevent overfitting \nmodel.add(Dropout(0.4)) \n# Batch Normalisation \nmodel.add(BatchNormalization()) \n  \n# 2nd Dense Layer \nmodel.add(Dense(4096)) \nmodel.add(Activation('relu')) \n# Add Dropout \nmodel.add(Dropout(0.4)) \n# Batch Normalisation \nmodel.add(BatchNormalization()) \n  \n# Output Softmax Layer \nmodel.add(Dense(5005)) \nmodel.add(Activation('softmax')) \n\nmodel.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history1 = model.fit(X, y, epochs=50, batch_size=128, validation_split=0.30)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}