{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This code builds 1000 dimension product embeddings from images by passing each image through a pre-trained VGG16 CNN from Keras Applications API.\nEmbeddings are stored in a pickle file found here https://www.kaggle.com/datasets/mohammedobeidat/hm-product-image-embeddings\nIt takes 11 hours to finish the process**","metadata":{}},{"cell_type":"markdown","source":"**To see how these embeddings are used to find similar items please head to \nhttps://www.kaggle.com/code/mohammedobeidat/finding-similar-items-with-image-embeddings-knn**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\nimport numpy as np\nimport tensorflow as tf\nmodel = VGG16(weights='imagenet')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-15T07:28:05.305713Z","iopub.execute_input":"2022-05-15T07:28:05.306111Z","iopub.status.idle":"2022-05-15T07:28:17.192019Z","shell.execute_reply.started":"2022-05-15T07:28:05.306001Z","shell.execute_reply":"2022-05-15T07:28:17.19097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\npaths = []\nids = []\nfor dirname, _, filenames in os.walk('../input/h-and-m-personalized-fashion-recommendations/images'):\n    for filename in filenames:\n        paths.append(os.path.join(dirname, filename))\n        ids.append(filename[:1])","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:38:44.258944Z","iopub.execute_input":"2022-05-13T05:38:44.259608Z","iopub.status.idle":"2022-05-13T05:40:17.282214Z","shell.execute_reply.started":"2022-05-13T05:38:44.25954Z","shell.execute_reply":"2022-05-13T05:40:17.281331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\nwith open('paths.pickle', 'wb') as f:\n    pickle.dump(paths, f)\n    \nwith open('ids.pickle', 'wb') as f:\n    pickle.dump(ids, f)","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:40:17.284377Z","iopub.execute_input":"2022-05-13T05:40:17.284914Z","iopub.status.idle":"2022-05-13T05:40:17.337761Z","shell.execute_reply.started":"2022-05-13T05:40:17.284865Z","shell.execute_reply":"2022-05-13T05:40:17.336614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embeds = []\n# labels = []\nembeds = open('embeds.pickle', 'wb')\nlabels = open('labels.pickle', 'wb')\n\nfor path in paths:\n    try:\n        img = image.load_img(path, target_size=(224, 224))\n        x = image.img_to_array(img)\n        x = np.expand_dims(x, axis=0)\n        x = preprocess_input(x)\n        embed = model.predict(x)\n        label = decode_predictions(embed, top=1)[0][0][1]\n\n    #     embeds.append(embed)\n    #     labels.append(label)\n\n        pickle.dump(embed, embeds)\n        pickle.dump(label, labels)\n        \n    except e:\n        print(e)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-13T05:42:29.457784Z","iopub.execute_input":"2022-05-13T05:42:29.45821Z","iopub.status.idle":"2022-05-13T05:42:30.798805Z","shell.execute_reply.started":"2022-05-13T05:42:29.458173Z","shell.execute_reply":"2022-05-13T05:42:30.798045Z"},"trusted":true},"execution_count":null,"outputs":[]}]}