{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Content-Based Image Filtering for Recommendation\n\nIn this notebook we are going to use Content-based filtering with images. Content-based filtering is a type of recommender system that guesses what a user may like based on that user's activity.\n\nIn the [H&M Personalized Fashion Recommendation Challenge](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations) we were given images to (almost) all articles. Thus, we can find and recommend those articles that are most similar - in terms of images - to the user's already purchased items. The general idea is to (1) create a feature vector for each image and (2) use these feature vectors to find similar images.\n\nOverall, the result of content-based image filtering is poor (public score: 0.004). But I think it might be interesting to see how content-based image filtering works. Further, I'm thinking about combining this approach with collaborative filtering and other content-based filtering methods (such as an NLP based recommendation).\n\n## <a id=\"Content\">Table of Content</a>\n[<span>1. Load Data</span>](#First)  \n[<span>2. Find Nearest Neighbors</span>](#Second)  \n[<span>3. Content-Based Image Filtering</span>](#Third)  \n[<span>4. Submission</span>](#Fourth)  ","metadata":{}},{"cell_type":"markdown","source":"### Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nimport random\nimport warnings\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torchvision import *\nfrom PIL import Image\nfrom sklearn.neighbors import NearestNeighbors\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:26:22.330893Z","iopub.execute_input":"2022-02-16T07:26:22.331712Z","iopub.status.idle":"2022-02-16T07:26:25.258949Z","shell.execute_reply.started":"2022-02-16T07:26:22.331573Z","shell.execute_reply":"2022-02-16T07:26:25.257755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Settings","metadata":{}},{"cell_type":"code","source":"PATH = Path(\"../input/h-and-m-personalized-fashion-recommendations/\")\nROOT_PATH = Path(\"../input/\")\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:27:16.359429Z","iopub.execute_input":"2022-02-16T07:27:16.359801Z","iopub.status.idle":"2022-02-16T07:27:16.364238Z","shell.execute_reply.started":"2022-02-16T07:27:16.359767Z","shell.execute_reply":"2022-02-16T07:27:16.36363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span id=\"First\">1. Load Data</span>\n\n### 1.1 Load Articles\n\nBased on the *articles.csv* file, I created an *articles_extended.csv* file with an additional column called *img_paths* which contains the path to each article's image. You can find it [here](https://www.kaggle.com/oberfink/articles-extended). </br>\n\nNote that some articles [miss corresponding images](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations/discussion/307064). We will drop such articles.","metadata":{}},{"cell_type":"code","source":"articles_df = pd.read_csv(ROOT_PATH / 'articles-extended/articles_extended.csv', dtype={'article_id': str})\narticles_df.dropna(subset=[\"img_paths\"], inplace=True)  # Drop articles without image\narticles_df.reset_index(inplace=True, drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:27:51.304531Z","iopub.execute_input":"2022-02-16T07:27:51.304833Z","iopub.status.idle":"2022-02-16T07:27:52.667119Z","shell.execute_reply.started":"2022-02-16T07:27:51.304802Z","shell.execute_reply":"2022-02-16T07:27:52.66601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 Load Transaction Data\n\nAs noted in this [notebook](https://www.kaggle.com/hengzheng/time-is-our-best-friend), filtering on date and only picking summer months as the test period is one week after 22nd september (I found that in the case of content-based image filter it is really useful!)","metadata":{}},{"cell_type":"code","source":"transactions_train_df = pd.read_csv(PATH / 'transactions_train.csv', dtype={'article_id': str})\ntransactions_train_df['t_dat'] = pd.to_datetime(transactions_train_df['t_dat'])\ntransactions_train_df = transactions_train_df.loc[transactions_train_df['t_dat']>=datetime(2020, 9, 7)]\ntransactions_train_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:27:58.515404Z","iopub.execute_input":"2022-02-16T07:27:58.515696Z","iopub.status.idle":"2022-02-16T07:29:22.54728Z","shell.execute_reply.started":"2022-02-16T07:27:58.515667Z","shell.execute_reply":"2022-02-16T07:29:22.545826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 1.3 Load Feature Vectors\n\nIn another notebook, I used a pre-trained ResNet50 to calculate a feature vector for each article's image </br>\n\nThe idea behind the feature extraction is the following: </br>\n\n<u>Model:</u></br>\nWe take a pre-trained CNN such as ResNet50 and remove the final output layer (the one which is responsible for predicting different classes such as dogs, cats etc.) as we do not intent to use the model as a classifier. Subsequently, the final layer of the model is a convolutional layer. Finally, as we want to produce a feature vector for each image, we have to reshape or flatten the output. In the case of a ResNet50 the size of the reshaped feature vector is 2048.\n\n<u>Feature Extraction:</u></br>\nUsing the model described above, we generate a feature vector for each article's image by simply forward-propagating each image through the model. As you might image, feature extraction for over 100,000 articles takes quite some time. Thus, I saved the feature vector for each image in this [feature_matrix.pt](https://www.kaggle.com/oberfink/fashionfeaturematrix) file. Note that the indicies match the indices of articles_df.\n\nIf you are interested in the full feature extraction notebook let me know and I'll share it.","metadata":{}},{"cell_type":"code","source":"# Just for illustration purposes. This code creates the model for feature extraction based on a ResNet50\ndef FashionModel():\n    from torchvision.models import resnet50\n    \n    resnet = resnet50(pretrained=True)\n    model = torch.nn.Sequential(*(list(resnet.children())[:-1]))\n    for param in model.parameters():\n        param.requires_grad = False\n    model.eval()\n    model.to(device)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:29:22.549966Z","iopub.execute_input":"2022-02-16T07:29:22.550267Z","iopub.status.idle":"2022-02-16T07:29:22.558047Z","shell.execute_reply.started":"2022-02-16T07:29:22.550238Z","shell.execute_reply":"2022-02-16T07:29:22.557153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_matrix = torch.load(ROOT_PATH / 'fashionfeaturematrix/feature_matrix.pt', map_location=torch.device('cpu'))\nfeature_matrix = feature_matrix.numpy()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:30:34.341687Z","iopub.execute_input":"2022-02-16T07:30:34.341976Z","iopub.status.idle":"2022-02-16T07:30:35.376695Z","shell.execute_reply.started":"2022-02-16T07:30:34.341944Z","shell.execute_reply":"2022-02-16T07:30:35.375814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span id=\"Second\">2. Find Nearest Neighbors</span>\n\nAfter extracting the feature vector for each image, we can use these vectors to find most similar articles using sklearn's [NearestNeighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html) method. For each article, we want to find its 12 most similar articles. In fact, we want to find its 13 most similar articles because the method always returns the article itself.","metadata":{}},{"cell_type":"code","source":"# Fit the model\nn_neighbors = 13\nneigh = NearestNeighbors(n_neighbors=n_neighbors, metric=\"cosine\")\nneigh.fit(feature_matrix)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:30:47.617775Z","iopub.execute_input":"2022-02-16T07:30:47.618255Z","iopub.status.idle":"2022-02-16T07:30:47.829378Z","shell.execute_reply.started":"2022-02-16T07:30:47.618223Z","shell.execute_reply":"2022-02-16T07:30:47.828383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For each article (image) find the most similar articles (images)\n_, indices = neigh.kneighbors(feature_matrix)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:30:53.440273Z","iopub.execute_input":"2022-02-16T07:30:53.441199Z","iopub.status.idle":"2022-02-16T07:41:02.630677Z","shell.execute_reply.started":"2022-02-16T07:30:53.441138Z","shell.execute_reply":"2022-02-16T07:41:02.629825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dictionary which stores the 13 most similar articles for each image.\nnearest_neighbor_dictionary = {}\nfor article_id, nearest_neighbor_ids in zip(articles_df[\"article_id\"].values, indices):\n    nearest_neighbor_dictionary[article_id] = nearest_neighbor_ids","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:41:02.63227Z","iopub.execute_input":"2022-02-16T07:41:02.632501Z","iopub.status.idle":"2022-02-16T07:41:02.724976Z","shell.execute_reply.started":"2022-02-16T07:41:02.63246Z","shell.execute_reply":"2022-02-16T07:41:02.724237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 Examples","metadata":{}},{"cell_type":"code","source":"transformation = transforms.Compose([transforms.Resize((512, 512)),\n                                     transforms.ToTensor(),\n                                    ])\n\ndef get_image(path):\n    pil_image = Image.open(path)\n    img = transformation(pil_image)\n    return img.permute(1, 2, 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:41:02.726136Z","iopub.execute_input":"2022-02-16T07:41:02.726378Z","iopub.status.idle":"2022-02-16T07:41:02.732883Z","shell.execute_reply.started":"2022-02-16T07:41:02.726348Z","shell.execute_reply":"2022-02-16T07:41:02.731898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index = 1\ndistances, indices = neigh.kneighbors(feature_matrix[index].reshape(1, -1))","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:41:02.735059Z","iopub.execute_input":"2022-02-16T07:41:02.735784Z","iopub.status.idle":"2022-02-16T07:41:03.78352Z","shell.execute_reply.started":"2022-02-16T07:41:02.735748Z","shell.execute_reply":"2022-02-16T07:41:03.782305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(5,5))\nimg = get_image(articles_df.iloc[index,:].img_paths)\nax.set_xticks([])\nax.set_yticks([])\nax.imshow(img, cmap='gray')","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:41:03.785272Z","iopub.execute_input":"2022-02-16T07:41:03.785672Z","iopub.status.idle":"2022-02-16T07:41:04.114523Z","shell.execute_reply.started":"2022-02-16T07:41:03.785619Z","shell.execute_reply":"2022-02-16T07:41:04.113552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 5, figsize=(20,10))\nfor i in range(5):\n    idx = indices[0][i]\n    ax[i].set_xticks([])\n    ax[i].set_yticks([])\n    img_path = articles_df.iloc[idx,:].img_paths\n    img = get_image(img_path)\n    ax[i].imshow(img, cmap='gray')\n    ax[i].set_title(distances[0][i], fontsize = 14)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:41:04.115983Z","iopub.execute_input":"2022-02-16T07:41:04.116293Z","iopub.status.idle":"2022-02-16T07:41:05.096302Z","shell.execute_reply.started":"2022-02-16T07:41:04.116253Z","shell.execute_reply":"2022-02-16T07:41:05.095429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span id=\"Third\">3. Content-Based Image Filtering</span>","metadata":{}},{"cell_type":"markdown","source":"### 3.1 Customer Transactions\n\nFor each customer, determine all the articles which have been purchased is the past","metadata":{}},{"cell_type":"code","source":"customer_transactions = transactions_train_df.groupby(by=\"customer_id\")['article_id'].agg(list).reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:41:30.475778Z","iopub.execute_input":"2022-02-16T07:41:30.476079Z","iopub.status.idle":"2022-02-16T07:41:32.42916Z","shell.execute_reply.started":"2022-02-16T07:41:30.476049Z","shell.execute_reply":"2022-02-16T07:41:32.42857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2 Dummy Recommendation\n\nAdditionally to the content-based image filtering recommendation, we need another strategy for customers who have not purchased any item yet. For now, we will use a \"dummy recommendation\" based on recommending the 12 most often purchased articles.","metadata":{}},{"cell_type":"code","source":"most_bought_articles = list((transactions_train_df['article_id'].value_counts()).index)[:12]\nmost_bought_articles = ' '.join(most_bought_articles)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:41:32.430884Z","iopub.execute_input":"2022-02-16T07:41:32.431394Z","iopub.status.idle":"2022-02-16T07:41:32.556461Z","shell.execute_reply.started":"2022-02-16T07:41:32.431348Z","shell.execute_reply":"2022-02-16T07:41:32.555637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.3 Make Recommendations","metadata":{}},{"cell_type":"code","source":"def add_neighbors(article_ids):\n    nearest_articles = []\n    for article_id in article_ids:\n        try:\n            nearest_indices = nearest_neighbor_dictionary[article_id][1:]\n            nearest_articles.extend([x for x in articles_df.iloc[nearest_indices, 0].to_list() if x not in article_ids])\n        except:\n            continue\n    nearest_articles = list(set(nearest_articles))\n    if len(nearest_articles) > 12:\n        nearest_articles = random.sample(nearest_articles, 12)\n    elif len(nearest_articles) < 12:\n        nearest_articles.extend(random.sample(most_bought_articles, 12 - len(nearest_articles)))\n    return nearest_articles","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:41:33.599162Z","iopub.execute_input":"2022-02-16T07:41:33.599444Z","iopub.status.idle":"2022-02-16T07:41:33.606794Z","shell.execute_reply.started":"2022-02-16T07:41:33.599411Z","shell.execute_reply":"2022-02-16T07:41:33.605942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_transactions[\"nearest_article_ids\"] = customer_transactions.apply(lambda row: add_neighbors(row[\"article_id\"]), axis=1)\ncustomer_transactions[\"nearest_article_ids\"] = customer_transactions[\"nearest_article_ids\"].apply(lambda x: \" \".join(x))","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:41:34.070115Z","iopub.execute_input":"2022-02-16T07:41:34.070404Z","iopub.status.idle":"2022-02-16T07:43:20.597063Z","shell.execute_reply.started":"2022-02-16T07:41:34.070371Z","shell.execute_reply":"2022-02-16T07:43:20.596093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <span id=\"Third\">4. Submission</span>","metadata":{}},{"cell_type":"code","source":"submission_df = pd.read_csv(PATH / 'sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:45:15.279537Z","iopub.execute_input":"2022-02-16T07:45:15.279855Z","iopub.status.idle":"2022-02-16T07:45:20.421269Z","shell.execute_reply.started":"2022-02-16T07:45:15.279821Z","shell.execute_reply":"2022-02-16T07:45:20.420623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df = submission_df[[\"customer_id\"]].merge(customer_transactions[[\"customer_id\", \"nearest_article_ids\"]], on='customer_id', how=\"left\")\nsubmission_df.columns = [\"customer_id\", \"prediction\"]\n#For each customer who has not purchased an article yet, recommend to buy the most most often purchased articles.\nsubmission_df.fillna(most_bought_articles, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:45:21.145871Z","iopub.execute_input":"2022-02-16T07:45:21.146625Z","iopub.status.idle":"2022-02-16T07:45:22.64471Z","shell.execute_reply.started":"2022-02-16T07:45:21.146585Z","shell.execute_reply":"2022-02-16T07:45:22.643839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df.to_csv(\"submission_content_based_image_filtering.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T07:46:28.409718Z","iopub.execute_input":"2022-02-16T07:46:28.410617Z","iopub.status.idle":"2022-02-16T07:46:41.462575Z","shell.execute_reply.started":"2022-02-16T07:46:28.410576Z","shell.execute_reply":"2022-02-16T07:46:41.461558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**\"What day is it?\"** asked Pooh. </br>\n**\"It's today\"** squeaked Piglet. </br>\n**\"My favorite day\"** said Pooh. </br>\n\nHappy kaggleing.","metadata":{}}]}