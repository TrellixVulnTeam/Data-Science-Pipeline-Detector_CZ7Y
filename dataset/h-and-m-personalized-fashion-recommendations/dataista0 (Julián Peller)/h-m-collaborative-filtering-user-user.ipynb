{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# H&M - Collaborative Filtering: User-user\n![](https://storage.googleapis.com/kaggle-competitions/kaggle/31254/logos/header.png)\n\n## A user-user collaborative filtering (UUCF) model for the competition [H&M Personalized Fashion Recommendations](https://www.kaggle.com/c/h-and-m-personalized-fashion-recommendations).\n\n\nA UUCF (user-user collaborative filtering) is one of the most common Recommender System. The core idea behind it is as follow: \n* For a given user (`u`), we will search for similar users. Let's say, the 10 most similar users to `u`. This is `u`'s neighborhood. \n* We search for the most popular items in this neighborhood of users similar to `u`, `I`\n* We check the items that are popular in the group of users similar to `u`, that `u` hasn't purchased yet.\n* We recommend those items to `u`\n\nNow, we need to operationalize two things here: \n* What does it mean that 2 users are similar?\n* What does it mean that an item is popular in a group of users?\n\nFor the similarity among users, we will just compare their purchases. The overlap of their purchased items is the similarity of the users. With this idea, we want to capture users with similar consumption patterns.\n\nFor the popularity of items in a group, there are more simple and more complex definitions. We can just take the number of purchases a given product has in a group and rank all the possible products with that metric. Or we can consider the similarity to the original user `u` and weight the purchases depending on who was purchasing. That way, purchases of users that are more similar to `u` will have a stronger impact on the overall ranking of products.\n\n---\n\n**In version 20 I have removed Heng's \"time is our best friend\" model as the fallback model for cold start and put a simple 12-most popular one in its place. Heng's baseline seems to be performing much better than my UUCF so the reporting a `0.019` score is actually a `-0.001` against doing nothing. I want to see the result of the UUCF by itself. I was expecting to outperform the baseline with a subset of users under the UUCF regime, but I couldn't till now. The next steps in order to make UUCF work would be to have a validation strategy and extract the hyperparameters of the code in order to find the optimal combination. Hope someone can make the UUCF perform better (I know it can).**\n\nIf I can obtain a score that surpasses the `0.02`, using UUCF + Heng's baseline, I will share it for sure.\n\n\n\n# Please, _DO_ upvote if you find this kernel useful or interesting!","metadata":{}},{"cell_type":"markdown","source":"# Imports and prepare data","metadata":{}},{"cell_type":"code","source":"import time\nimport numpy as np\nimport pandas as pd\n\nimport multiprocessing as mp\nfrom multiprocessing import Pool\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:02:47.603583Z","iopub.execute_input":"2022-02-15T20:02:47.604032Z","iopub.status.idle":"2022-02-15T20:02:47.629735Z","shell.execute_reply.started":"2022-02-15T20:02:47.603911Z","shell.execute_reply":"2022-02-15T20:02:47.629093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = '../input/h-and-m-personalized-fashion-recommendations/'\ncsv_train = f'{base_path}transactions_train.csv'\ncsv_sub = f'{base_path}sample_submission.csv'\ncsv_users = f'{base_path}customers.csv'\ncsv_items = f'{base_path}articles.csv'\n\ndf = pd.read_csv(csv_train, dtype={'article_id': str}, parse_dates=['t_dat'])\ndf_sub = pd.read_csv(csv_sub)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-02-15T20:02:47.633327Z","iopub.execute_input":"2022-02-15T20:02:47.633669Z","iopub.status.idle":"2022-02-15T20:04:05.672911Z","shell.execute_reply.started":"2022-02-15T20:02:47.633635Z","shell.execute_reply":"2022-02-15T20:04:05.671271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create mapping from ids to incremental integers and viceversa\n\n* We will reserve the term `user_id` as a integer from 0 and `customer_id` for the original id.\n* The same goes with `item_id` and `article_id`","metadata":{}},{"cell_type":"code","source":"dfu = pd.read_csv(csv_users)\ndfi = pd.read_csv(csv_items, dtype={'article_id': str})\n\nALL_USERS = dfu['customer_id'].unique().tolist()\nALL_ITEMS = dfi['article_id'].unique().tolist()\n\nuser_to_customer_map = {user_id: customer_id for user_id, customer_id in enumerate(ALL_USERS)}\ncustomer_to_user_map = {customer_id: user_id for user_id, customer_id in enumerate(ALL_USERS)}\n\nitem_to_article_map = {item_id: article_id for item_id, article_id in enumerate(ALL_ITEMS)}\narticle_to_item_map = {article_id: item_id for item_id, article_id in enumerate(ALL_ITEMS)}\n\ndel dfu, dfi","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:04:05.675404Z","iopub.execute_input":"2022-02-15T20:04:05.675645Z","iopub.status.idle":"2022-02-15T20:04:13.309999Z","shell.execute_reply.started":"2022-02-15T20:04:05.675614Z","shell.execute_reply":"2022-02-15T20:04:13.30914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['user_id'] = df['customer_id'].map(customer_to_user_map)\ndf['item_id'] = df['article_id'].map(article_to_item_map)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:04:13.311243Z","iopub.execute_input":"2022-02-15T20:04:13.311473Z","iopub.status.idle":"2022-02-15T20:04:27.661189Z","shell.execute_reply.started":"2022-02-15T20:04:13.311445Z","shell.execute_reply":"2022-02-15T20:04:27.660394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build model","metadata":{}},{"cell_type":"markdown","source":"# Configuration parameters","metadata":{}},{"cell_type":"markdown","source":"Since UUCF is very computationally expensive, we will apply it only to a small subset of users. The hope is that, for those users, the recommendation are better than the other models.\n\nWe will reduce the data from 2 fronts:\n* Keep only the most recent history. That is `START_DATE`.\n* Keep only users with at least `MINIMUM_PURCHASES`","metadata":{}},{"cell_type":"code","source":"N_SIMILAR_USERS = 30\n\nMINIMUM_PURCHASES = 3\n\nSTART_DATE = '2020-08-21'\n\nDROP_PURCHASED_ITEMS = False\n\nDROP_USER_FROM_HIS_NEIGHBORHOOD = False\n\nTEST_RUN = False\n\nTEST_SIZE = 1000","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:04:27.66354Z","iopub.execute_input":"2022-02-15T20:04:27.663815Z","iopub.status.idle":"2022-02-15T20:04:27.669247Z","shell.execute_reply.started":"2022-02-15T20:04:27.663761Z","shell.execute_reply":"2022-02-15T20:04:27.667672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def flatten(l):\n    \"\"\" Flatten a list of lists\"\"\"\n    return [item for sublist in l for item in sublist]\n\ndef compare_vectors(v1, v2):\n    \"\"\"Compare lists of purchased product for two given users\n    v1 stands for the \"vector representation for user 1\", which is a list of the purchases of u1\n    \n    Returns:\n        A value between 0 and 1 (similarity)\n    \"\"\"\n    intersection = len(set(v1) & set(v2))\n    denominator = np.sqrt(len(v1) * len(v2))\n    return intersection / denominator\n\ndef get_similar_users(u, v, dfh):\n    \"\"\"\n    Get the N_SIMILAR_USERS most similar users to the given one with their similarity score\n    Arguments:\n        u: the user_id, \n        v:  the \"vector\" representation of the user (list of item_id)\n        dfh : the \"history of transaccions\" dataframe\n        \n    Returns:\n        tuple of lists ([similar user_id], [similarity scores])\n    \"\"\"\n    similar_users = dfh.apply(lambda v_other: compare_vectors(v, v_other)).sort_values(ascending=False).head(N_SIMILAR_USERS + 1)\n    \n    if DROP_USER_FROM_HIS_NEIGHBORHOOD:\n        similar_users = similar_users[similar_users.index != u]\n        \n    return similar_users.index.tolist(), similar_users.tolist()\n\ndef get_items(u, v, dfh):\n    \"\"\" Get the recommend items for a given users\n    \n    It will:\n        1) Get similar users for the given user\n        2) Obtain all the items those users purchased\n        3) Rank them using the similarity scores of the user that purchased them\n        4) Return the 12 best ranked\n    \n    Arguments:\n        u: the user_id, \n        v:  the \"vector\" representation of the user (list of item_id)\n        dfh : the \"history of transaccions\" dataframe\n        \n    Returns:\n        list of item_id of lenght at most 12\n    \"\"\"\n    global i, n\n    \n    users, scores = get_similar_users(u, v, dfh)\n    df_nn = pd.DataFrame({'user': users, 'score': scores})\n    df_nn['items'] = df_nn.apply(lambda row: dfh.loc[row.user], axis=1)\n    df_nn['weighted_items'] = df_nn.apply(lambda row: [(item, row.score) for item in row['items']], axis=1)\n\n    recs = pd.DataFrame(flatten(df_nn['weighted_items'].tolist()), columns=['item', 'score']).groupby('item')['score'].sum().sort_values(ascending=False)\n    if DROP_PURCHASED_ITEMS:\n        recs = recs[~recs.index.isin(v)]\n    # Keep the first 12 and get the item_ids\n    i +=1\n    if i % 200 == 0:\n        pid = mp.current_process().pid\n        print(f\"[PID {pid:>2d}] Finished {i:3d} / {n:5d} - {i/n*100:3.0f}%\")\n    return recs.head(12).index.tolist()\n\ndef get_items_chunk(user_ids: np.array, dfh: pd.DataFrame):\n    \"\"\" Call get_item for a list of user_ids\n    \n    Arguments:\n        user_ids: list of user_id, \n        dfh: the \"history of transaccions\" dataframe\n        \n    Returns:\n        pd.Series with index user_id and list of item_id (recommendations) as value\n    \"\"\"\n    global i, n\n    i = 0\n    \n    n = len(user_ids)\n    pid = mp.current_process().pid\n    print(f\"[PID {pid:>2d}] Started working with {n:5d} users\")\n    \n    df_user_vectors = pd.DataFrame(dfh.loc[user_ids]).reset_index()\n    df_user_vectors['recs'] = df_user_vectors.apply(lambda row: get_items(row.user_id, row.item_id, dfh), axis=1)\n    return df_user_vectors.set_index('user_id')['recs']\n\ndef get_recommendations(users: list, dfh: pd.DataFrame):\n    \"\"\"\n    Obtained recommendation for the users using transaccion dfh in a parallelized manner\n    \n    Call get_items_chunk in a \"smart\" multiprocessing fashion\n    \n    Arguments:\n        users: list of user_id\n        dfh: the \"history of transaccions\" dataframe\n    \n    Returns:\n        pd.DataFrame with index user_id and list of item_id (recommendations) as value\n    \n    \"\"\"\n    time_start = time.time()\n    \n    # Split into approximately evenly sized chunks\n    # We will send just one batch to each CPU \n    user_chunks = np.array_split(users, mp.cpu_count())\n    \n    f = partial(get_items_chunk, dfh=dfh)\n    with Pool(mp.cpu_count()) as p:\n        res = p.map(f, user_chunks)\n    \n    df_rec = pd.DataFrame(pd.concat(res))\n\n    elapsed = (time.time() - time_start) / 60\n    print(f\"Finished get_recommendations({len(users)}). It took {elapsed:5.2f} mins\")\n    return df_rec\n\n\ndef uucf(df, start_date=START_DATE):\n    \"\"\" Entry point for the UUCF model. \n    \n    Receive the original transactions_train.csv and a start_date and gets UUCF recommendations\n    \n    The model will not cover the full list of users, but just a subset of them.\n    \n    It will provide recommendations for users with at least MINIMUM_PURCHASES after start_date.\n    It might return less than 12 recs per user.\n    \n    An ad-hoc function for filling these gaps should be used downstream.\n    (See fill functionality right below)\n    \n    \n    Arguments:\n        df: The raw dataframe from transactions_train.csv\n        start_date: a date\n        \n    Returns:\n        a submission-like pd.DataFrame with columns [customer_id, prediction]\n        'prediction' is a list and not a string though\n    \n    \"\"\"\n    df_small = df[df['t_dat'] > start_date]\n    print(f\"Kept data from {start_date} on. Total rows: {len(df_small)}\")\n    \n    # H stands for \"Transaction history\"\n    # dfh is a series of user_id => list of item_id (the list of purchases in order)\n    dfh = df_small.groupby(\"user_id\")['item_id'].apply(lambda items: list(set(items)))\n    dfh = dfh[dfh.str.len() >= MINIMUM_PURCHASES]\n    if TEST_RUN:\n        print(\"WARNING: TEST_RUN is True. It will be a toy execution.\")\n        dfh = dfh.head(TEST_SIZE)\n    \n    users = dfh.index.tolist()\n    n_users = len(users)\n    print(f\"Total users in the time frame with at least {MINIMUM_PURCHASES}: {n_users}\")\n    \n    df_rec = get_recommendations(users, dfh)\n    df_rec['customer_id'] = df_rec.index.map(user_to_customer_map)\n    df_rec['prediction'] = df_rec['recs'].map(lambda l: [item_to_article_map[i] for i in l])\n    \n    # Submission ready dataframe\n    df_rec.reset_index(drop=True)[['customer_id', 'prediction']]\n    return df_rec ","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:04:27.670895Z","iopub.execute_input":"2022-02-15T20:04:27.671425Z","iopub.status.idle":"2022-02-15T20:04:27.737581Z","shell.execute_reply.started":"2022-02-15T20:04:27.67138Z","shell.execute_reply":"2022-02-15T20:04:27.736992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_recs = uucf(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:04:27.738696Z","iopub.execute_input":"2022-02-15T20:04:27.739103Z","iopub.status.idle":"2022-02-15T20:04:39.478289Z","shell.execute_reply.started":"2022-02-15T20:04:27.73907Z","shell.execute_reply":"2022-02-15T20:04:39.477461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fill the remainder with another model\nWe will use [Heng Zheng](https://www.kaggle.com/hengzheng)'s [time is our best friend v2](https://www.kaggle.com/hengzheng/time-is-our-best-friend-v2/) which is simple and the best performing public model as of today.\n\nI have created a dataset with the submission file from that notebook [here](https://www.kaggle.com/julian3833/heng-zhengs-time-is-our-best-friend-v2-submission).","metadata":{}},{"cell_type":"code","source":"csv_fill = '../input/h-m-content-based-12-most-popular-items-0-007/submission.csv'\ndf_fill = pd.read_csv(csv_fill)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:04:39.479577Z","iopub.execute_input":"2022-02-15T20:04:39.47985Z","iopub.status.idle":"2022-02-15T20:04:44.344634Z","shell.execute_reply.started":"2022-02-15T20:04:39.479822Z","shell.execute_reply":"2022-02-15T20:04:44.343766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def drop_duplicates(seq):\n    \"\"\" Remove duplicates of a given sequence keeping order\"\"\"\n    seen = set()\n    seen_add = seen.add\n    return [x for x in seq if not (x in seen or seen_add(x))]\n\ndef fill_row(row):\n    uucf = row['prediction_uucf']\n    fill = row['prediction_fill'].split()\n    new_list = drop_duplicates(uucf + fill)[:12]\n    return ' '.join(new_list)\n\n\ndef fill(df_recs, df_fill):\n    df_recs['len'] = df_recs['prediction'].str.len()\n    df_recs = pd.merge(df_fill, df_recs, how='left', on='customer_id', suffixes=('_fill', '_uucf'))\n    \n    \n    # No recs from UUCF at all: use the fallback model \n    df_recs.loc[df_recs['prediction_uucf'].isnull(), 'prediction'] = df_recs['prediction_fill']\n\n\n    # Full UUCF recommendation\n    mask = df_recs['prediction_uucf'].notnull() & (df_recs['len'] == 12)\n    df_recs.loc[mask, 'prediction'] = df_recs['prediction_uucf']\n\n\n    # Fill with another model. Not enough recs from UUCF\n    fill_mask = df_recs['prediction_uucf'].notnull() & (df_recs['len'] < 12)\n    df_recs.loc[fill_mask, 'prediction'] = df_recs[fill_mask].apply(fill_row, axis=1)\n    return df_recs.drop(['prediction_uucf', 'prediction_fill', 'len', 'recs'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:04:44.345814Z","iopub.execute_input":"2022-02-15T20:04:44.346038Z","iopub.status.idle":"2022-02-15T20:04:44.356241Z","shell.execute_reply.started":"2022-02-15T20:04:44.34601Z","shell.execute_reply":"2022-02-15T20:04:44.355468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill with another model\ndf_sub = fill(df_recs, df_fill)\ndf_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:04:44.357221Z","iopub.execute_input":"2022-02-15T20:04:44.357953Z","iopub.status.idle":"2022-02-15T20:04:46.252325Z","shell.execute_reply.started":"2022-02-15T20:04:44.357924Z","shell.execute_reply":"2022-02-15T20:04:46.251552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:04:46.253639Z","iopub.execute_input":"2022-02-15T20:04:46.254431Z","iopub.status.idle":"2022-02-15T20:04:46.26274Z","shell.execute_reply.started":"2022-02-15T20:04:46.25439Z","shell.execute_reply":"2022-02-15T20:04:46.261979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submit\ndf_sub.to_csv(\"uucf.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-15T20:04:46.26453Z","iopub.execute_input":"2022-02-15T20:04:46.264882Z","iopub.status.idle":"2022-02-15T20:04:58.828449Z","shell.execute_reply.started":"2022-02-15T20:04:46.264844Z","shell.execute_reply":"2022-02-15T20:04:58.827828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Please, _DO_ upvote if you find this kernel useful or interesting!","metadata":{}}]}