{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Summary:**\n* 1. Retrain the model using the latest four weeks of data (train_weeks: [1, 2, 3, 4]  -> [0, 1, 2, 3])\n> LB: 0.0181 -> 0.0197\n* 2. Add bn.\n> LB: 0.0197\n* 3. Solve the user cold start problem with popular recommendations (using [time-decaying-popularity-benchmark](https://www.kaggle.com/code/mayukh18/time-decaying-popularity-benchmark-0-0216), train_weeks: [0, 1, 2, 3, 4]).\n> LB: 0.0197 -> 0.0211\n","metadata":{}},{"cell_type":"code","source":"import gc\nimport numpy as np\nimport pandas as pd\n\nis_debug = False\ndf = pd.read_csv(\"../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv\", \n                 dtype={\"article_id\": str})\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:09:19.598646Z","iopub.execute_input":"2022-04-02T04:09:19.598968Z","iopub.status.idle":"2022-04-02T04:10:20.763842Z","shell.execute_reply.started":"2022-04-02T04:09:19.598889Z","shell.execute_reply":"2022-04-02T04:10:20.763151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"t_dat\"] = pd.to_datetime(df[\"t_dat\"])\ndf[\"t_dat\"].max()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:10:20.765605Z","iopub.execute_input":"2022-04-02T04:10:20.765882Z","iopub.status.idle":"2022-04-02T04:10:26.55902Z","shell.execute_reply.started":"2022-04-02T04:10:20.765844Z","shell.execute_reply":"2022-04-02T04:10:26.558294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"active_articles = df.groupby(\"article_id\")[\"t_dat\"].max().reset_index()\nactive_articles = active_articles[active_articles[\"t_dat\"] >= \"2019-09-01\"].reset_index()\nactive_articles.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:10:26.560284Z","iopub.execute_input":"2022-04-02T04:10:26.560536Z","iopub.status.idle":"2022-04-02T04:10:31.443967Z","shell.execute_reply.started":"2022-04-02T04:10:26.560502Z","shell.execute_reply":"2022-04-02T04:10:31.443216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[df[\"article_id\"].isin(active_articles[\"article_id\"])].reset_index(drop=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:10:31.44596Z","iopub.execute_input":"2022-04-02T04:10:31.44616Z","iopub.status.idle":"2022-04-02T04:10:38.163894Z","shell.execute_reply.started":"2022-04-02T04:10:31.446134Z","shell.execute_reply":"2022-04-02T04:10:38.163195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"week\"] = (df[\"t_dat\"].max() - df[\"t_dat\"]).dt.days // 7\ndf[\"week\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:10:38.165246Z","iopub.execute_input":"2022-04-02T04:10:38.165561Z","iopub.status.idle":"2022-04-02T04:10:39.680723Z","shell.execute_reply.started":"2022-04-02T04:10:38.165523Z","shell.execute_reply":"2022-04-02T04:10:39.680022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n\narticle_ids = np.concatenate([[\"placeholder\"], np.unique(df[\"article_id\"].values)])\n\nle_article = LabelEncoder()\nle_article.fit(article_ids)\ndf[\"article_id\"] = le_article.transform(df[\"article_id\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:10:39.682088Z","iopub.execute_input":"2022-04-02T04:10:39.682379Z","iopub.status.idle":"2022-04-02T04:11:53.608675Z","shell.execute_reply.started":"2022-04-02T04:10:39.682298Z","shell.execute_reply":"2022-04-02T04:11:53.60781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WEEK_HIST_MAX = 5\n\ndef create_dataset(df, week):\n    hist_df = df[(df[\"week\"] > week) & (df[\"week\"] <= week + WEEK_HIST_MAX)]\n    hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list, \"week\": list}).reset_index()\n    hist_df.rename(columns={\"week\": 'week_history'}, inplace=True)\n    \n    target_df = df[df[\"week\"] == week]\n    target_df = target_df.groupby(\"customer_id\").agg({\"article_id\": list}).reset_index()\n    target_df.rename(columns={\"article_id\": \"target\"}, inplace=True)\n    target_df[\"week\"] = week\n    \n    return target_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n\n# val_weeks = [0]\ntrain_weeks = [i for i in range(WEEK_HIST_MAX)]\n\n# val_df = pd.concat([create_dataset(df, w) for w in val_weeks]).reset_index(drop=True)\ntrain_df = pd.concat([create_dataset(df, w) for w in train_weeks]).reset_index(drop=True)\nif is_debug:\n    train_df = train_df.sample(n = 10000)\n\ndel df\nprint(gc.collect())\n\ntrain_articles = train_df['customer_id'].unique().tolist()\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:11:53.609916Z","iopub.execute_input":"2022-04-02T04:11:53.610764Z","iopub.status.idle":"2022-04-02T04:14:01.255948Z","shell.execute_reply.started":"2022-04-02T04:11:53.610714Z","shell.execute_reply":"2022-04-02T04:14:01.255177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torch\nfrom tqdm import tqdm\n\nclass HMDataset(Dataset):\n    def __init__(self, df, seq_len, is_test=False):\n        self.df = df.reset_index(drop=True)\n        self.seq_len = seq_len\n        self.is_test = is_test\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        \n        if self.is_test:\n            target = torch.zeros(2).float()\n        else:\n            target = torch.zeros(len(article_ids)).float()\n            for t in row.target:\n                target[t] = 1.0\n            \n        article_hist = torch.zeros(self.seq_len).long()\n        week_hist = torch.ones(self.seq_len).float()\n        \n        \n        if isinstance(row.article_id, list):\n            if len(row.article_id) >= self.seq_len:\n                article_hist = torch.LongTensor(row.article_id[-self.seq_len:])\n                week_hist = (torch.LongTensor(row.week_history[-self.seq_len:]) - row.week)/WEEK_HIST_MAX/2\n            else:\n                article_hist[-len(row.article_id):] = torch.LongTensor(row.article_id)\n                week_hist[-len(row.article_id):] = (torch.LongTensor(row.week_history) - row.week)/WEEK_HIST_MAX/2\n                \n        return article_hist, week_hist, target\n    \n# HMDataset(val_df, 64)[1]","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:14:01.257672Z","iopub.execute_input":"2022-04-02T04:14:01.258321Z","iopub.status.idle":"2022-04-02T04:14:02.677273Z","shell.execute_reply.started":"2022-04-02T04:14:01.258272Z","shell.execute_reply":"2022-04-02T04:14:02.676545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_lr(optimizer, epoch):\n    if epoch < 1:\n        lr = 5e-5\n    elif epoch < 6:\n        lr = 1e-3\n    elif epoch < 9:\n        lr = 1e-4\n    else:\n        lr = 1e-5\n\n    for p in optimizer.param_groups:\n        p['lr'] = lr\n    return lr\n    \ndef get_optimizer(net):\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=3e-4, betas=(0.9, 0.999),\n                                 eps=1e-08)\n    return optimizer","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:14:02.678989Z","iopub.execute_input":"2022-04-02T04:14:02.679225Z","iopub.status.idle":"2022-04-02T04:14:02.687127Z","shell.execute_reply.started":"2022-04-02T04:14:02.679189Z","shell.execute_reply":"2022-04-02T04:14:02.686221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass HMModel(nn.Module):\n    def __init__(self, article_shape):\n        super(HMModel, self).__init__()\n        \n        self.article_emb = nn.Embedding(article_shape[0], embedding_dim=article_shape[1])\n        \n        self.article_likelihood = nn.Parameter(torch.zeros(article_shape[0]), requires_grad=True)\n        self.top = nn.Sequential(nn.Conv1d(3, 8, kernel_size=1), nn.LeakyReLU(),nn.BatchNorm1d(8),\n                                 nn.Conv1d(8, 32, kernel_size=1), nn.LeakyReLU(),nn.BatchNorm1d(32),\n                                 nn.Conv1d(32, 8, kernel_size=1), nn.LeakyReLU(),nn.BatchNorm1d(8),\n                                 nn.Conv1d(8, 1, kernel_size=1),nn.LeakyReLU(),)\n        \n    def forward(self, inputs):\n        article_hist, week_hist = inputs[0], inputs[1]\n        x = self.article_emb(article_hist)\n        x = F.normalize(x, dim=2)\n        \n        x = x@F.normalize(self.article_emb.weight).T\n        \n        x, indices = x.max(axis=1)\n        x = x.clamp(1e-3, 0.999)\n        x = -torch.log(1/x - 1)\n        \n        max_week = week_hist.unsqueeze(2).repeat(1, 1, x.shape[-1]).gather(1, indices.unsqueeze(1).repeat(1, week_hist.shape[1], 1))\n        max_week = max_week.mean(axis=1).unsqueeze(1)\n        \n        x = torch.cat([x.unsqueeze(1), max_week,\n                       self.article_likelihood[None, None, :].repeat(x.shape[0], 1, 1)], axis=1)\n        \n        x = self.top(x).squeeze(1)\n        return x\n    \n    \nmodel = HMModel((len(le_article.classes_), 512))\nmodel = model.cuda()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:14:02.693086Z","iopub.execute_input":"2022-04-02T04:14:02.693612Z","iopub.status.idle":"2022-04-02T04:14:05.700496Z","shell.execute_reply.started":"2022-04-02T04:14:02.693573Z","shell.execute_reply":"2022-04-02T04:14:05.699664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\n\ndef calc_map(topk_preds, target_array, k=12):\n    metric = []\n    tp, fp = 0, 0\n    \n    for pred in topk_preds:\n        if target_array[pred]:\n            tp += 1\n            metric.append(tp/(tp + fp))\n        else:\n            fp += 1\n            \n    return np.sum(metric) / min(k, target_array.sum())\n\ndef read_data(data):\n    return tuple(d.cuda() for d in data[:-1]), data[-1].cuda()\n\n\ndef validate(model, val_loader, k=12):\n    model.eval()\n    \n    tbar = tqdm(val_loader, file=sys.stdout)\n    \n    maps = []\n    \n    with torch.no_grad():\n        for idx, data in enumerate(tbar):\n            inputs, target = read_data(data)\n\n            logits = model(inputs)\n\n            _, indices = torch.topk(logits, k, dim=1)\n\n            indices = indices.detach().cpu().numpy()\n            target = target.detach().cpu().numpy()\n\n            for i in range(indices.shape[0]):\n                maps.append(calc_map(indices[i], target[i]))\n        \n    \n    return np.mean(maps)\n\nSEQ_LEN = 16\n\nBS = 256\nNW = 8\n\n# val_dataset = HMDataset(val_df, SEQ_LEN)\n# val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=NW,\n#                           pin_memory=False, drop_last=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:14:05.73869Z","iopub.execute_input":"2022-04-02T04:14:05.73942Z","iopub.status.idle":"2022-04-02T04:14:05.755266Z","shell.execute_reply.started":"2022-04-02T04:14:05.739377Z","shell.execute_reply":"2022-04-02T04:14:05.754546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dice_loss(y_pred, y_true):\n    y_pred = y_pred.sigmoid()\n    intersect = (y_true*y_pred).sum(axis=1)\n    \n    return 1 - (intersect/(intersect + y_true.sum(axis=1) + y_pred.sum(axis=1))).mean()\n\n\ndef train(model, train_loader, epochs, val_loader = None):\n    np.random.seed(SEED)\n    \n    optimizer = get_optimizer(model)\n    scaler = torch.cuda.amp.GradScaler()\n\n    criterion = torch.nn.BCEWithLogitsLoss()\n    \n    for e in range(epochs):\n        model.train()\n        tbar = tqdm(train_loader, file=sys.stdout)\n        \n        lr = adjust_lr(optimizer, e)\n        \n        loss_list = []\n\n        for idx, data in enumerate(tbar):\n            inputs, target = read_data(data)\n\n            optimizer.zero_grad()\n            \n            with torch.cuda.amp.autocast():\n                logits = model(inputs)\n                loss = criterion(logits, target) + dice_loss(logits, target)\n            \n            \n            #loss.backward()\n            scaler.scale(loss).backward()\n            #optimizer.step()\n            scaler.step(optimizer)\n            scaler.update()\n            \n            loss_list.append(loss.detach().cpu().item())\n            \n            avg_loss = np.round(100*np.mean(loss_list), 4)\n\n            tbar.set_description(f\"Epoch {e+1} Loss: {avg_loss} lr: {lr}\")\n            \n#         val_map = validate(model, val_loader)\n\n        log_text = f\"Epoch {e+1}\\nTrain Loss: {avg_loss}\"\n            \n#         print(log_text)\n        \n        #logfile = open(f\"models/{MODEL_NAME}_{SEED}.txt\", 'a')\n        #logfile.write(log_text)\n        #logfile.close()\n    return model\n\n\nMODEL_NAME = \"exp001\"\nSEED = 0\n\ntrain_dataset = HMDataset(train_df, SEQ_LEN)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=NW,\n                          pin_memory=False, drop_last=True)\n\nmodel = train(model, train_loader, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:14:05.757218Z","iopub.execute_input":"2022-04-02T04:14:05.75792Z","iopub.status.idle":"2022-04-02T04:15:51.524576Z","shell.execute_reply.started":"2022-04-02T04:14:05.757879Z","shell.execute_reply":"2022-04-02T04:15:51.523067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv').drop(\"prediction\", axis=1)\ntest_df = test_df.loc[test_df['customer_id'].isin(train_articles)] # Predict only for trained customers\nprint(test_df.shape)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:15:51.526439Z","iopub.status.idle":"2022-04-02T04:15:51.527388Z","shell.execute_reply.started":"2022-04-02T04:15:51.527101Z","shell.execute_reply":"2022-04-02T04:15:51.527134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_test_dataset(test_df):\n    week = -1\n    test_df[\"week\"] = week\n    \n    hist_df = df[(df[\"week\"] > week) & (df[\"week\"] <= week + WEEK_HIST_MAX)]\n    hist_df = hist_df.groupby(\"customer_id\").agg({\"article_id\": list, \"week\": list}).reset_index()\n    hist_df.rename(columns={\"week\": 'week_history'}, inplace=True)\n    \n    \n    return test_df.merge(hist_df, on=\"customer_id\", how=\"left\")\n\ntest_df = create_test_dataset(test_df)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:15:51.529029Z","iopub.status.idle":"2022-04-02T04:15:51.52994Z","shell.execute_reply.started":"2022-04-02T04:15:51.529664Z","shell.execute_reply":"2022-04-02T04:15:51.529693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = HMDataset(test_df, SEQ_LEN, is_test=True)\ntest_loader = DataLoader(test_ds, batch_size=BS, shuffle=False, num_workers=NW,\n                          pin_memory=False, drop_last=False)\n\n\ndef inference(model, loader, k=12):\n    model.eval()\n    \n    tbar = tqdm(loader, file=sys.stdout)\n    \n    preds = []\n    \n    with torch.no_grad():\n        for idx, data in enumerate(tbar):\n            inputs, target = read_data(data)\n\n            logits = model(inputs)\n\n            _, indices = torch.topk(logits, k, dim=1)\n\n            indices = indices.detach().cpu().numpy()\n            target = target.detach().cpu().numpy()\n\n            for i in range(indices.shape[0]):\n                preds.append(\" \".join(list(le_article.inverse_transform(indices[i]))))\n        \n    \n    return preds\n\n\ntest_df[\"prediction\"] = inference(model, test_loader)\ntest_df = test_df[['customer_id', 'prediction']]","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:15:51.531464Z","iopub.status.idle":"2022-04-02T04:15:51.532375Z","shell.execute_reply.started":"2022-04-02T04:15:51.532101Z","shell.execute_reply":"2022-04-02T04:15:51.532129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Merge with Time Decaying Popularity Benchmark [0.0216]\nsub_df = pd.read_csv('../input/time-decaying-popularity-benchmark-0-0216/submissions.csv')\nsub_df = sub_df.loc[~sub_df['customer_id'].isin(train_articles)]\n\nsub_df = pd.concat([sub_df, test_df])\nsub_df = sub_df.sort_values('customer_id')","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:15:51.533891Z","iopub.status.idle":"2022-04-02T04:15:51.534841Z","shell.execute_reply.started":"2022-04-02T04:15:51.534543Z","shell.execute_reply":"2022-04-02T04:15:51.534571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_df.to_csv(\"submission.csv\", index=False, columns=[\"customer_id\", \"prediction\"])","metadata":{"execution":{"iopub.status.busy":"2022-04-02T04:15:51.53638Z","iopub.status.idle":"2022-04-02T04:15:51.537307Z","shell.execute_reply.started":"2022-04-02T04:15:51.537037Z","shell.execute_reply":"2022-04-02T04:15:51.537066Z"},"trusted":true},"execution_count":null,"outputs":[]}]}