{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# H&M Comppetition so easy Baseline  \nI did [EDA](https://www.kaggle.com/code/kaicho0504/h-m-eda-2022-4-20), I think popular product type is't much different in each year. In addition, I assume popular products in the season are sold a lot. Therefore I use data during 2018, 2019 and 2020 September.  \n\n1. Count kind of popular products by using past data(2018, 2019) and select cadidate product type.  \n\n2. Count popular products by using close target period data(2020 September) and select recommended product to users that product type is included cadidate.  ","metadata":{"id":"XGoKLIl0l6lr"}},{"cell_type":"markdown","source":"## Import Library  ","metadata":{"id":"7gY6Zt0Hl3eG"}},{"cell_type":"code","source":"import os\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","metadata":{"id":"AUOGQK-plpe6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data  ","metadata":{"id":"9FC13e3NqKz0"}},{"cell_type":"code","source":"DIR = \"../input/h-and-m-personalized-fashion-recommendations\"\narticles = pd.read_csv(os.path.join(DIR, \"articles.csv\"))\ncustomers = pd.read_csv(os.path.join(DIR, \"customers.csv\"))\ntransactions = pd.read_csv(os.path.join(DIR, \"transactions_train.csv\"))\nsubmission = pd.read_csv(os.path.join(DIR, \"sample_submission.csv\"))\n\nprint(f\"artiles data shape: {articles.shape}\")\nprint(f\"cusomters data shape: {customers.shape}\")\nprint(f\"transactions data shape: {transactions.shape}\")\nprint(f\"sample submission shape: {submission.shape}\")\n\ndisplay(articles.head())\ndisplay(customers.head())\ndisplay(transactions.head())\ndisplay(submission.head())","metadata":{"id":"cJSr8jbEl1I0","outputId":"385d7b24-e7a3-4568-f0db-c67aaf4bfa37"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing data  \n**add columns**  \n- yeear  \n- month  \n- day  \n- binning age  \n\n**merge data**\n- transactions + product_type_name(articles) + bin_age(customers)  \n\n**extract data**\n1. transacsions in 2018 September.  \n2. transactions in 2019 September.  \n3. transactions in 2020 September.  ","metadata":{"id":"Hh5VshKGqJ-f"}},{"cell_type":"code","source":"# add columns to transactions\ntransactions[\"t_dat\"] = pd.to_datetime(transactions[\"t_dat\"])\n\n# add year, month, day\ntransactions[\"year\"] = transactions[\"t_dat\"].dt.year\ntransactions[\"month\"] = transactions[\"t_dat\"].dt.month\ntransactions[\"day\"] = transactions[\"t_dat\"].dt.day\n\n# add binning age\nbins = [i for i in range(10, 101, 10)]\nlabels = [i for i in range(1, len(bins))]\n\ncustomers[\"bin_age\"] = pd.cut(customers[\"age\"], bins=bins, labels=labels)\n\n# merge data\ntransactions = transactions.merge(articles[[\"article_id\", \"product_type_name\"]],\n                                  on=\"article_id\")\ntransactions = transactions.merge(customers[[\"customer_id\", \"bin_age\"]],\n                                  on=\"customer_id\")\n\n# extract data\ntransactions_2018sep = transactions.query(\"year == 2018 and month == 9\").copy().reset_index(drop=True)\ntransactions_2019sep = transactions.query(\"year == 2019 and month == 9\").copy().reset_index(drop=True)\ntransactions_2020sep = transactions.query(\"year == 2020 and month == 9\").copy().reset_index(drop=True)\n\n# to save memory\ndel transactions\n\ndisplay(transactions_2018sep)\ndisplay(transactions_2019sep)\ndisplay(transactions_2020sep)","metadata":{"id":"vuvf0aW_qFWo","outputId":"019a273d-e07e-4de0-d545-354b947ebdef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select cadidate product type by using 2018 and 2019 data\n# count sold product type\nprod_count_2018 = transactions_2018sep[\"product_type_name\"].value_counts()\nprod_count_2019 = transactions_2019sep[\"product_type_name\"].value_counts()\n\n# initilize dataframe to store count data\nprod_count = pd.DataFrame(\n    index=[2018, 2019],\n    columns=articles[\"product_type_name\"].unique()\n)\nprod_count = prod_count.fillna(0)\n\n# assign count values to dataframe\nfor year, df in zip([2018, 2019], [prod_count_2018, prod_count_2019]):\n    for prod in df.index:\n        prod_count.loc[year, prod] = df.loc[prod]\n\ndisplay(prod_count)","metadata":{"id":"swUYJJI3q0M5","outputId":"69476f0d-928f-41d2-d2d2-274ab534da61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract product type top 12 in each year\nprod_count_2018 = prod_count.loc[2018]\nprod_count_2019 = prod_count.loc[2019]\n\n# descending sort\nindices_2018 = prod_count_2018.values.argsort()[::-1]\nindices_2019 = prod_count_2019.values.argsort()[::-1]\n\n# extract cadidate product names and values\ncadidate_2018 = prod_count_2018.index[indices_2018][:12]\nvalues_2018 = prod_count_2018.loc[cadidate_2018]\n\ncadidate_2019 = prod_count_2019.index[indices_2019][:12]\nvalues_2019 = prod_count_2019.loc[cadidate_2019]\n\nprint(\"2018 cadidate and values\")\nfor prod in cadidate_2018:\n    print(f\"{prod}: {values_2018.loc[prod]}\")\n\nprint(\"\\n2019 cadidate and values\")\nfor prod in cadidate_2019:\n    print(f\"{prod}: {values_2019.loc[prod]}\")","metadata":{"id":"lJVrxQxbukUP","outputId":"a351e735-b98e-4bc1-acd2-d6e99942d27c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate candidates and eliminate duplication\ncadidate = set(cadidate_2018.tolist() + cadidate_2019.tolist())\n\n# add 2018 values and 2019 values\nprod_cadidate = pd.DataFrame(\n    data=(prod_count_2018[cadidate].values+prod_count_2019[cadidate].values).reshape(-1, 1),\n    index=cadidate,\n    columns=[\"num of sold product\"],\n)\n\n# descending sort to prioritize\nprod_cadidate = prod_cadidate.sort_values(by=\"num of sold product\", ascending=False)\ncadidate = prod_cadidate.index.tolist()\n\nprint(cadidate)\ndisplay(prod_cadidate)","metadata":{"id":"kx_u3_s2xOwm","outputId":"3b3f1efd-5446-4de7-b8d6-debe0a06b959"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select recommended product  \nProducts that the user bought recently will be not bought by the user(same user) so should not recoomend same products.  \n\nFirst, extract the product selling the most in each product type.  ","metadata":{"id":"4yNZLESay3uX"}},{"cell_type":"code","source":"# list to store top product\ntop_products = []\n\nfor prod in cadidate:\n    top_product_count = transactions_2020sep.query(\"product_type_name == @prod\")[\"article_id\"].value_counts()\n    top_products.append(top_product_count.index[0])\n\n    # display number of other product(top5)\n    print(f\"prod:\")\n    for i in range(5):\n        print(f\"{top_product_count.index[i]}: {top_product_count.iloc[i]}\")\n\n    print(\"\\n\")\n\nprint(top_products)","metadata":{"id":"Xc0-qpcl2H-z","outputId":"e92e03c3-761c-4d77-a454-7f62980a9a2e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is not by far the best selling product, so I should select some cadidate in each product type.  ","metadata":{"id":"t5PmypHx0xmd"}},{"cell_type":"code","source":"# condatenate article id and transform to str\ntop_products_str = \" \".join(np.array(top_products)[:12].astype(str))\nsubmission[\"prediction\"] = top_products_str\ndisplay(submission)","metadata":{"id":"DZbFGOin31Zj","outputId":"d5c821cc-0538-4b56-a898-ef6796352326"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Discussion  \nI don't expected to get good score by this baseline at first, but kaggle is a big community so I think someone will help us and can discovery good method with us.  \n\nMy thought and what I'll do after are following.  \n- I tried to apply similar this method each customers, but it take much execution time more than 1 month.  So I'll classification customers and apply this method.  \n- I saw using LGBMRanker of scikit-learn in other user notebook, I'll try to use this model and improve score.  \n- I did'nt use image data and natural language data, but we make features of these data to use CNN model, ViT, or BERT etc. I think how to use these features. We must consider execution time because this dataset is so big.  \n\nThank you for watching my notebook and I'm forward to your advice!","metadata":{"id":"mgR4rHy68ED5"}},{"cell_type":"code","source":"","metadata":{"id":"Ww_uKfYwDMrc"},"execution_count":null,"outputs":[]}]}