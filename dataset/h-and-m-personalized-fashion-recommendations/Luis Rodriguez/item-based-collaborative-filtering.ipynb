{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime\nfrom tqdm import tqdm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preparing the data","metadata":{}},{"cell_type":"markdown","source":"We need to restrict the data respect to a minimum transaction date. In that way, we reduce the dimensionality of the problem and we get rid of transactions that are not important in terms of the time decaying popularity.","metadata":{}},{"cell_type":"markdown","source":"Also, we are getting rid of articles that have not been bought enough. (Minimum 10 purchases are required)","metadata":{}},{"cell_type":"code","source":"transactions = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv', dtype={'article_id':str})\ntransactions.drop(['sales_channel_id', 'price'], inplace=True, axis=1)\ntransactions['bought'] = 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_date = datetime.datetime(2020,9,1)\n# Filter transactions by date\ntransactions[\"t_dat\"] = pd.to_datetime(transactions[\"t_dat\"])\ntransactions = transactions.loc[transactions[\"t_dat\"] >= start_date]\n\n# Filter transactions by number of an article has been bought\narticle_bought_count = transactions[['article_id', 't_dat']].groupby('article_id').count().reset_index().rename(columns={'t_dat': 'count'})\nmost_bought_articles = article_bought_count[article_bought_count['count']>10]['article_id'].values\ntransactions = transactions[transactions['article_id'].isin(most_bought_articles)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Due to the big amount of items, we can not consider the whole matrix in order to train. Therefore, we need to generate some negative samples: transactions that have never occured.","metadata":{}},{"cell_type":"code","source":"# Generate negative samples\nnp.random.seed(0)\n\nnegative_samples = pd.DataFrame({\n    'article_id': np.random.choice(transactions.article_id.unique(), transactions.shape[0]),\n    'customer_id': np.random.choice(transactions.customer_id.unique(), transactions.shape[0]),\n    'bought': np.zeros(transactions.shape[0])\n})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Defining the model","metadata":{}},{"cell_type":"markdown","source":"Model will be based on recommendations computed through the time decaying popularity and the most similar items to those items bought the most times by each user. Similarity among items is computed through cosine distance.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\n\nclass ItemBased_RecSys:\n    ''' Collaborative filtering using a custom sim(u,u'). '''\n\n    def __init__(self, positive_transactions, negative_transactions, num_components=10):\n        ''' Constructor '''\n        self.positive_transactions = positive_transactions\n        self.transactions = pd.concat([positive_transactions, negative_transactions])\n        self.customers = self.transactions.customer_id.values\n        self.articles = self.transactions.article_id.values\n        self.bought = self.transactions.bought.values\n        self.num_components = num_components\n\n        self.customer_id2index = {c: i for i, c in enumerate(np.unique(self.customers))}\n        self.article_id2index = {a: i for i, a in enumerate(np.unique(self.articles))}\n        \n    def __sdg__(self):\n        for idx in tqdm(self.training_indices):\n            # Get the current sample\n            customer_id = self.customers[idx]\n            article_id = self.articles[idx]\n            bought = self.bought[idx]\n\n            # Get the index of the user and the article\n            customer_index = self.customer_id2index[customer_id]\n            article_index = self.article_id2index[article_id]\n\n            # Compute the prediction and the error\n            prediction = self.predict_single(customer_index, article_index)\n            error = (bought - prediction) # error\n            \n            # Update latent factors in terms of the learning rate and the observed error\n            self.customers_latent_matrix[customer_index] += self.learning_rate * \\\n                                    (error * self.articles_latent_matrix[article_index] - \\\n                                     self.lmbda * self.customers_latent_matrix[customer_index])\n            self.articles_latent_matrix[article_index] += self.learning_rate * \\\n                                    (error * self.customers_latent_matrix[customer_index] - \\\n                                     self.lmbda * self.articles_latent_matrix[article_index])\n                \n                \n    def fit(self, n_epochs=10, learning_rate=0.001, lmbda=0.1):\n        ''' Compute the matrix factorization R = P x Q '''\n        self.learning_rate = learning_rate\n        self.lmbda = lmbda\n        n_samples = self.transactions.shape[0]\n        \n        # Initialize latent matrices\n        self.customers_latent_matrix = np.random.normal(scale=1., size=(len(np.unique(self.customers)), self.num_components))\n        self.articles_latent_matrix = np.random.normal(scale=1., size=(len(np.unique(self.articles)), self.num_components))\n\n        for epoch in range(n_epochs):\n            print('Epoch: {}'.format(epoch))\n            self.training_indices = np.arange(n_samples)\n            \n            # Shuffle training samples and follow stochastic gradient descent\n            np.random.shuffle(self.training_indices)\n            self.__sdg__()\n\n    def predict_single(self, customer_index, article_index):\n        ''' Make a prediction for an specific user and article '''\n        prediction = np.dot(self.customers_latent_matrix[customer_index], self.articles_latent_matrix[article_index])\n        prediction = np.clip(prediction, 0, 1)\n        \n        return prediction\n\n    def default_recommendation(self):\n        ''' Calculate time decaying popularity '''\n        # Calculate time decaying popularity. This leads to items bought more recently having more weight in the popularity list.\n        # In simple words, item A bought 5 times on the first day of the train period is inferior than item B bought 4 times on the last day of the train period.\n        self.positive_transactions['pop_factor'] = self.positive_transactions['t_dat'].apply(lambda x: 1/(datetime.datetime(2020,9,23) - x).days)\n        transactions_by_article = self.positive_transactions[['article_id', 'pop_factor']].groupby('article_id').sum().reset_index()\n        return transactions_by_article.sort_values(by='pop_factor', ascending=False)['article_id'].values[:12]\n\n\n    def predict(self, customers):\n        ''' Make recommendations '''\n        recommendations = []\n\n        # Compute similarity matrix (cosine)\n        similarity_matrix = cosine_similarity(self.articles_latent_matrix, self.articles_latent_matrix, dense_output=False)\n\n        # Convert similarity matrix into a matrix containing the 12 most similar items' index for each item\n        similarity_matrix = np.argsort(similarity_matrix, axis=1)\n        similarity_matrix = similarity_matrix[:, -12:]\n\n        # Get default recommendation (time decay popularity)\n        default_recommendation = self.default_recommendation()\n\n        # Group articles by user and articles to compute the number of times each article has been bought by each user\n        transactions_by_customer = self.positive_transactions[['customer_id', 'article_id', 'bought']].groupby(['customer_id', 'article_id']).count().reset_index()\n        most_bought_article = transactions_by_customer.loc[transactions_by_customer.groupby('customer_id').bought.idxmax()]['article_id'].values\n\n        # Make predictions\n        for customer in tqdm(customers):\n            try:\n                rec_aux1 = []\n                rec_aux2 = []\n                aux = []\n\n                # Retrieve the most bought article by customer\n                user_most_bought_article_id = most_bought_article[self.customer_id2index[customer]]\n\n                # Using the similarity matrix, get the 6 most similar articles\n                rec_aux1 = self.articles[similarity_matrix[self.article_id2index[user_most_bought_article_id]]]\n                # Return the half of the default recommendation\n                rec_aux2 = default_recommendation\n\n                # Merge half of both recommendation lists\n                for rec_idx in range(6):\n                    aux.append(rec_aux2[rec_idx])\n                    aux.append(rec_aux1[rec_idx])\n\n                recommendations.append(' '.join(aux))\n            except:\n                # Return the default recommendation\n                recommendations.append(' '.join(default_recommendation))\n        \n        return pd.DataFrame({\n            'customer_id': customers,\n            'prediction': recommendations,\n        })","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"markdown","source":"Define your hyperparameters and fit the model. Take into account that there are more customizable parameters in the data processing section.","metadata":{}},{"cell_type":"code","source":"rec = ItemBased_RecSys(transactions, negative_samples, num_components=1000)\nrec.fit(n_epochs=20)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generating a submission","metadata":{}},{"cell_type":"code","source":"customers = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv').customer_id.unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recommendations = rec.predict(customers)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recommendations.to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(recommendations)","metadata":{},"execution_count":null,"outputs":[]}]}