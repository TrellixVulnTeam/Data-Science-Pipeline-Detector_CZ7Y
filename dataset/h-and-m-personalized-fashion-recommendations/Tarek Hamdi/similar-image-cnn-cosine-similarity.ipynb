{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://miro.medium.com/max/1400/1*gYe2FMr9lKys2Wmo1v-s7A.jpeg)","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.applications.xception import Xception,preprocess_input\nimport tensorflow as tf\nfrom keras.preprocessing import image\nfrom keras.layers import Input\nfrom keras.backend import reshape\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\nimport random \nfrom tqdm import tqdm_notebook as tqdm\nimport numba \nfrom numba import cuda ","metadata":{"execution":{"iopub.status.busy":"2022-02-17T18:37:40.01971Z","iopub.execute_input":"2022-02-17T18:37:40.020416Z","iopub.status.idle":"2022-02-17T18:37:46.157279Z","shell.execute_reply.started":"2022-02-17T18:37:40.020316Z","shell.execute_reply":"2022-02-17T18:37:46.156545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Loading and Preprocessing of Data","metadata":{}},{"cell_type":"code","source":"images_dir = '../input/h-and-m-personalized-fashion-recommendations/images'","metadata":{"execution":{"iopub.status.busy":"2022-02-17T18:37:46.158918Z","iopub.execute_input":"2022-02-17T18:37:46.159178Z","iopub.status.idle":"2022-02-17T18:37:46.163554Z","shell.execute_reply.started":"2022-02-17T18:37:46.159144Z","shell.execute_reply":"2022-02-17T18:37:46.16289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1=pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/sample_submission.csv')\ndf2=pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv', converters={'article_id': str})\narticle_id=df2['article_id'].tolist()\nsorter=df1['customer_id'].tolist()\ndf=pd.merge(df1, df2, on='customer_id', how =\"left\")\ndf3=df[['customer_id', 'article_id', 'prediction']]\ndf3=df3[df3['customer_id'].isin(df1['customer_id'].tolist())]\n\ndfs = df3.set_index('customer_id')\ndfs=dfs.loc[sorter]\ndfs = dfs.reset_index()\ndfs = dfs.drop_duplicates(subset=['customer_id'], keep='first')\ndfs = dfs.reset_index()\ndel dfs['index']\ndfs['customer_id'].tolist() == df1['customer_id'].tolist()\ndel df, df1, df2, df3","metadata":{"execution":{"iopub.status.busy":"2022-02-17T18:37:46.164631Z","iopub.execute_input":"2022-02-17T18:37:46.165239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getImagePaths(path):\n    \"\"\"\n    Function to Combine Directory Path with individual Image Paths\n    \n    parameters: path(string) - Path of directory\n    returns: image_names(string) - Full Image Path\n    \"\"\"\n    image_names = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            fullpath = os.path.join(dirname, filename)\n            image_names.append(fullpath)\n    return image_names\n\ndef preprocess_img(img_path):\n    dsize = (256,256)\n    new_image=cv2.imread(img_path)\n    new_image=cv2.resize(new_image,dsize,interpolation=cv2.INTER_NEAREST)  \n    new_image=np.expand_dims(new_image,axis=0)\n    new_image=preprocess_input(new_image)\n    return new_image\n\ndef load_data():\n    output=[]\n    images_id=[]\n    output=random.sample(getImagePaths(images_dir), 2000)\n    images_id=[s.split('/')[-1][0] for s in output]\n    return output, images_id","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Defining model and extracting feature for all the training data","metadata":{}},{"cell_type":"code","source":"def model():\n    model=Xception(weights='imagenet',include_top=False)\n    for layer in model.layers:\n        layer.trainable=False\n        #model.summary()\n    return model\n\ndef feature_extraction(image_data,model):\n    features=model.predict(image_data)\n    features=np.array(features)\n    features=features.flatten()\n    return features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Finding the similar image through cosine similarity","metadata":{}},{"cell_type":"code","source":"def result_vector_cosine(model,feature_vector,new_img):\n    new_feature = model.predict(new_img)\n    new_feature = np.array(new_feature)\n    new_feature = new_feature.flatten()\n    N_result = 1\n    nbrs = NearestNeighbors(n_neighbors=N_result, metric=\"cosine\").fit(feature_vector)\n    distances, indices = nbrs.kneighbors([new_feature])\n    return(indices)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Result\n## Vector Cosine","metadata":{}},{"cell_type":"code","source":"# I will try the first 100k point until I will find a way to make code run in multiprocessing. \n# Rest of values will be chosen randomly\n\n\ndef input_show(data):\n    plt.title(\"Query Image\")\n    plt.imshow(data)\n\ndef show_result(data,result):\n    v=[]\n    for i in range(0,1):\n        index_result=result[0][i]\n        v.append(data[index_result].split('/')[-1].split('.')[0])\n    return ' '.join(v)\n        \ndef get_dataframe():\n    return dfs\nglobal dfs\n\ndef main():  \n    dfs=get_dataframe()\n    prediction=[]\n    features=[]\n    output, images_id = load_data()\n    main_model=model()\n    #Limiting the data for training\n    for i in output:\n        new_img=preprocess_img(i)\n        features.append(feature_extraction(new_img,main_model))\n    feature_vec = np.array(features)\n    \n    i=0\n    for p, tq in zip(get_dataframe()['article_id'].tolist(), tqdm(range(len(get_dataframe()['article_id'].tolist())))):\n        if i<=16000:\n            try:\n                path= '../input/h-and-m-personalized-fashion-recommendations/images/' + str(p)[:3] + '/'+ str(p) +'.jpg'\n                result=result_vector_cosine(main_model,feature_vec,preprocess_img(path))\n                prediction.append(show_result(output,result))\n            except:\n                prediction.append(' '.join(random.sample(article_id, 12)))\n                pred=' '.join(random.sample(article_id, 12))\n        elif i>16000:\n            pred=' '.join(random.sample(article_id, 12))\n            prediction.append(pred)\n        i+=1\n            \n    dfs['prediction']=prediction\n    dfs=dfs[['customer_id', 'prediction']]\n    dfs.to_csv('submission', index=False)\n    \n           \n\nif __name__=='__main__':\n    main()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}