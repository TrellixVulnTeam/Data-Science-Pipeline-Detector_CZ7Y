{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# What is happening at the end of October every year?\n\n>While trying to apply different logic to get a better recommendation system, I figured out that every year in October we have a spike in sales data. Yeah, actually we have several other spikes as well, but since we need to build a recommender system for the end of September, I guess October spikes are rather interesting to have a look at.\n\n>We are not given any information if H&M has a special promotion at these times or they are due to the start of seasonal sales. Nevertheless, I would recommend keeping in mind these spikes if you are building a model which relies on the most popular (sold) items. Here, previous years' data for the same season would be quite helpful.  ","metadata":{}},{"cell_type":"code","source":"# necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-04T22:31:31.064179Z","iopub.execute_input":"2022-03-04T22:31:31.065031Z","iopub.status.idle":"2022-03-04T22:31:31.070102Z","shell.execute_reply.started":"2022-03-04T22:31:31.064989Z","shell.execute_reply":"2022-03-04T22:31:31.069241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data\n# since we are interested in only sales data over time, we will read only those columns.\n# also we set index directly to datetime index, which would be easier to have a daily sum of sales later\ntransactions = pd.read_csv('../input/h-and-m-personalized-fashion-recommendations/transactions_train.csv', index_col='t_dat', usecols=['t_dat', 'price'], parse_dates=['t_dat'], infer_datetime_format=True)\ntransactions.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T22:45:51.518666Z","iopub.execute_input":"2022-03-04T22:45:51.519008Z","iopub.status.idle":"2022-03-04T22:46:39.960567Z","shell.execute_reply.started":"2022-03-04T22:45:51.518972Z","shell.execute_reply":"2022-03-04T22:46:39.959657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pandas resample() function is quite usefull to deal with timeseries data\n# we will get daily sum of price column and plot it for two year\ntransactions['price'].resample('1D').sum().plot(figsize=(20, 6), alpha=0.6)\nplt.title('Daily sales data for 2 years', fontsize=18, color='r');","metadata":{"execution":{"iopub.status.busy":"2022-03-04T22:46:52.164966Z","iopub.execute_input":"2022-03-04T22:46:52.165336Z","iopub.status.idle":"2022-03-04T22:46:53.658227Z","shell.execute_reply.started":"2022-03-04T22:46:52.165295Z","shell.execute_reply":"2022-03-04T22:46:53.657309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Obviously, we have several spikes, especially in October 2019. Let's have a closer look at these spikes for 2019 and 2018.**","metadata":{}},{"cell_type":"code","source":"# apply similar technique as above but for the data before 30-10-2018\ntransactions['price'].resample('1D').sum().loc[:'2018-10-30'].plot(figsize=(20, 6), alpha=0.6)\nplt.title('Daily sales data for OCTOBER of 2018', fontsize=18, color='r');","metadata":{"execution":{"iopub.status.busy":"2022-03-04T22:49:49.816411Z","iopub.execute_input":"2022-03-04T22:49:49.817313Z","iopub.status.idle":"2022-03-04T22:49:50.682278Z","shell.execute_reply.started":"2022-03-04T22:49:49.817221Z","shell.execute_reply":"2022-03-04T22:49:50.681332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply similar technique as above but for the data between 15-09-2019 to 30-10-2019\ntransactions['price'].resample('1D').sum().loc['2019-09-15':'2019-10-30'].plot(figsize=(20, 6), alpha=0.6)\nplt.title('Daily sales data for OCTOBER of 2019', fontsize=18, color='r');","metadata":{"execution":{"iopub.status.busy":"2022-03-04T22:51:47.650491Z","iopub.execute_input":"2022-03-04T22:51:47.65077Z","iopub.status.idle":"2022-03-04T22:51:48.565794Z","shell.execute_reply.started":"2022-03-04T22:51:47.650742Z","shell.execute_reply":"2022-03-04T22:51:48.56486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ending date of dataset\ntransactions.index.max()","metadata":{"execution":{"iopub.status.busy":"2022-03-04T22:54:40.430468Z","iopub.execute_input":"2022-03-04T22:54:40.430914Z","iopub.status.idle":"2022-03-04T22:54:40.437416Z","shell.execute_reply.started":"2022-03-04T22:54:40.430882Z","shell.execute_reply":"2022-03-04T22:54:40.436541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Surprise! Surprise!\n**H&M asked us to predict items a customer will buy in the next 7-day period after the training time period. Well, as we have seen above, both in 2018 and 2019, the end of September is a vibrant period. So in your model construction, particularly, if you rely on the most popular items sold in the last weeks, I would strongly recommend keeping in mind that last weeks' popular products may not work ideally. Better have a look at last years' same period.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> if you liked this observation and think it is helpful, please upvote it!\nor if you have comments, please leave them below!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}