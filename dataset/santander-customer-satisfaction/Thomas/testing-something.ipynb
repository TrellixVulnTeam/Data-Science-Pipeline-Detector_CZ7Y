{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"white\", color_codes=True)\n\ntrain = pd.read_csv(\"../input/train.csv\") \n#test = pd.read_csv(\"../input/test.csv\") \n\n#remove static data,\nremove = []\nfor col in train.columns:\n    if train[col].std() == 0:\n        remove.append(col)\n        \n#print(remove)\n#for x in remove:\n#    print(x,train[x].describe()) # yes these are all just 0s\ntrain.drop(remove, axis=1, inplace=True)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#fit a rf to the data\nfrom sklearn.ensemble import RandomForestClassifier\n\nX = train.drop(['ID','TARGET'], axis=1)\ny = train['TARGET']\nclf = RandomForestClassifier().fit(X,y)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#find the important features\nimp = clf.feature_importances_\nbest = sorted(imp, reverse=True)\nfor x in best[:10]:\n    print (x, X.columns[np.where(imp == x)].values[0])\nmost_important = X.columns[np.argmax(imp)]\nX[most_important].describe()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# 116 values in column var3 are -999999\n# var3 is suspected to be the nationality of the customer\n# -999999 would mean that the nationality of the customer is unknown\n#train['var3'].hist()\ntrain.loc[train.var3==-999999].shape"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Replace -999999 in var3 column with most common value 2 \n# See https://www.kaggle.com/cast42/santander-customer-satisfaction/debugging-var3-999999\n# for details\ntrain = train.replace(-999999,2)\ntrain.loc[train.var3==-999999].shape"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#train.var3.max()\n#train.var3.min()\ntrain.var3.hist(bins=100)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train[train.var3!=2].var3.hist(bins=100)"},{"cell_type":"markdown","metadata":{},"source":"# Var38"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# var38 is important according to XGBOOST\n# see https://www.kaggle.com/cast42/santander-customer-satisfaction/xgboost-with-early-stopping/files\n# Also RFC thinks var38 is important\n# see https://www.kaggle.com/tks0123456789/santander-customer-satisfaction/data-exploration/notebook\n# so far I have not seen a guess what var38 may be about\ntrain.var38.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# How is var38 looking when customer is unhappy ?\ntrain.loc[train['TARGET']==1, 'var38'].describe()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Histogram for var 38 is not normal distributed\nfig, ax = plt.subplots()\ntrain.var38.hist(ax=ax, bins=1000, bottom=0.1)\nax.set_yscale('log')\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Histogram for var 38 is not normal distributed\nfig, ax = plt.subplots()\ntrain.loc[train['TARGET']==1, 'var38'].hist(ax=ax, bins=1000, bottom=0.1)\n#ax.set_yscale('log')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Histogram for var 38 is not normal distributed\nfig, ax = plt.subplots()\ntrain.loc[train['TARGET']==0, 'var38'].hist(ax=ax, bins=1000, bottom=0.1)\n#ax.set_yscale('log')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train.var38.hist(bins=1000)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# where is the spike between 11 and 12  in the log plot ?\ntrain.var38.map(np.log).mode()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# What are the most common values for var38 ?\ntrain.var38.value_counts()"},{"cell_type":"markdown","metadata":{},"source":"the value 117310.979016 appears 14868"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# what is we exclude the most common value\ntrain.loc[~np.isclose(train.var38, 117310.979016), 'var38'].value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Look at the distribution\ntrain.loc[~np.isclose(train.var38, 117310.979016), 'var38'].map(np.log).hist(bins=100)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"dist38 = train.loc[~np.isclose(train.var38, 117310.979016), 'var38']"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"fig, ax = plt.subplots()\nd = dist38.map(np.log);\nfrom scipy.stats import norm\nmu, std = norm.fit(d);\nplt.hist(d, bins=25, normed=True, alpha=0.6, color='g')\n\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, std)\nplt.plot(x, p, 'b', linewidth=2)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Above plot suggest we split up var38 into two variables\n# var38mc == 1 when var38 has the most common value and 0 otherwise\n# logvar38 is log transformed feature when var38mc is 0, zero otherwise\ntrain['var38mc'] = np.isclose(train.var38, 117310.979016)\ntrain['logvar38'] = train.loc[~train['var38mc'], 'var38'].map(np.log)\ntrain.loc[train['var38mc'], 'logvar38'] = 0"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Check for nan's\nprint('Number of nan in var38mc', train['var38mc'].isnull().sum())\nprint('Number of nan in logvar38',train['logvar38'].isnull().sum())"},{"cell_type":"markdown","metadata":{},"source":"# var15"},{"cell_type":"markdown","metadata":{},"source":"The most important feature for XGBoost is var15. According to [a Kaggle form post](https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/19291/data-dictionary/110414#post110414)\n    var15 is the age of the customerLets explore var15"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train['var15'].describe()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Looks more normal, plot the histogram\ntrain['var15'].hist(bins=100)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"fig, ax = plt.subplots()\n#d = train['var15'].map(np.log);\nd = train['var15']\nfrom scipy.stats import norm\nmu, std = norm.fit(d);\nn, bins, patches = plt.hist(d, bins=100, normed=True, alpha=0.6, color='g')\np = norm.pdf(bins, mu, std)\nplt.plot(bins, p, 'b', linewidth=2)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"d = train['var15']#.map(np.log)\nfrom scipy.stats import expon\nfloc_d = 21\nmu, std = expon.fit(d, floc=floc_d);\nplt.hist(d, bins=100, normed=True, alpha=0.6, color='g')\n\nxmin, xmax = plt.xlim()\nprint (xmin, xmax)\n#xmin, xmax = plt.xlim(d)\nx = np.linspace(xmin, xmax, 100)\np = expon.pdf(x, mu, std)\nplt.plot(x, p, 'b', linewidth=2)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's look at the density of the age of happy/unhappy customers\nsns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"var15\") \\\n   .add_legend()\nplt.title('Unhappy customers are slightly older')\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"# Explore the interaction between var15 (age) and var38"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train, hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"var38\", \"var15\") \\\n   .add_legend()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train, hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"logvar38\", \"var15\") \\\n   .add_legend()\nplt.ylim([0,120]) # Age must be positive ;-)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Exclude most common value for var38 \nsns.FacetGrid(train[~train.var38mc], hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"logvar38\", \"var15\") \\\n   .add_legend()\nplt.ylim([0,120])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# What is distribution of the age when var38 has it's most common value ?\nsns.FacetGrid(train[train.var38mc], hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"var15\") \\\n   .add_legend()"},{"cell_type":"markdown","metadata":{},"source":"# Select the most important features"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"X = train.iloc[:,:-1]\ny = train.TARGET\n\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_classif,chi2\nfrom sklearn.preprocessing import Binarizer, scale\n\n# First select features based on chi2 and f_classif\np = 3\n\nX_bin = Binarizer().fit_transform(scale(X))\nselectChi2 = SelectPercentile(chi2, percentile=p).fit(X_bin, y)\nselectF_classif = SelectPercentile(f_classif, percentile=p).fit(X, y)\n\nchi2_selected = selectChi2.get_support()\nchi2_selected_features = [ f for i,f in enumerate(X.columns) if chi2_selected[i]]\nprint('Chi2 selected {} features {}.'.format(chi2_selected.sum(),\n   chi2_selected_features))\nf_classif_selected = selectF_classif.get_support()\nf_classif_selected_features = [ f for i,f in enumerate(X.columns) if f_classif_selected[i]]\nprint('F_classif selected {} features {}.'.format(f_classif_selected.sum(),\n   f_classif_selected_features))\nselected = chi2_selected & f_classif_selected\nprint('Chi2 & F_classif selected {} features'.format(selected.sum()))\nfeatures = [ f for f,s in zip(X.columns, selected) if s]\nprint (features)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Make a dataframe with the selected features and the target variable\nX_sel = train[features+['TARGET']]"},{"cell_type":"markdown","metadata":{},"source":"# var36"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# var38 (important for XGB and RFC is not selected but var36 is. Let's explore\nX_sel['var36'].value_counts()"},{"cell_type":"markdown","metadata":{},"source":"var38 is most of the times 99 or [0,1,2,3]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's plot the density in function of the target variabele\nsns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"var36\") \\\n   .add_legend()"},{"cell_type":"markdown","metadata":{},"source":"In above plot we see that the density of unhappy custormers is lower when var36 is not 99"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# var36 in function of var38 (most common value excluded) \nsns.FacetGrid(train[~train.var38mc], hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"var36\", \"logvar38\") \\\n   .add_legend()\n"},{"cell_type":"markdown","metadata":{},"source":"Let's seperate that in two plots"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train[(~train.var38mc) & (train.var36 < 4)], hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"var36\", \"logvar38\") \\\n   .add_legend()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's plot the density in function of the target variabele, when var36 = 99\nsns.FacetGrid(train[(~train.var38mc) & (train.var36 ==99)], hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"logvar38\") \\\n   .add_legend()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.pairplot(train[['var15','var36','logvar38','TARGET']], hue=\"TARGET\", size=2, diag_kind=\"kde\")"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train[['var15','var36','logvar38','TARGET']].boxplot(by=\"TARGET\", figsize=(12, 6))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# A final multivariate visualization technique pandas has is radviz\n# Which puts each feature as a point on a 2D plane, and then simulates\n# having each sample attached to those points through a spring weighted\n# by the relative value for that feature\nfrom pandas.tools.plotting import radviz\nradviz(train[['var15','var36','logvar38','TARGET']], \"TARGET\")"},{"cell_type":"markdown","metadata":{},"source":"# now look at all 8 features together"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"features"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"radviz(train[features], \"TARGET\")"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.pairplot(train[features], hue=\"TARGET\", size=2, diag_kind=\"kde\")"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}