{"cells":[{"metadata":{},"cell_type":"markdown","source":"## [Tutorial] Santander Customer Satisfaction 산탄데르 은행 고객 만족 예측\n### 책 <파이썬 머신러닝 완벽 마스터> 필사 코드입니다."},{"metadata":{},"cell_type":"markdown","source":"---"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/santander-customer-satisfaction/train.csv',\n                encoding = 'latin-1')\nprint('dataset shape: ', df.shape)\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label 값인 target 속성 값 분포 알아보기\nprint(df['TARGET'].value_counts())\nunsatisfied_cnt = df[df['TARGET'] == 1].TARGET.count()\ntotal_cnt = df.TARGET.count()\nprint('unsatisfied 비율은 {0:.2f}'.format((unsatisfied_cnt / total_cnt)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 분포를 살펴볼 때 `var3` 변수의 min 값에서 이상치 발견\n# 또한 `ID` feature는 단순 식별자이기 때문에 삭제\n\ndf['var3'].replace(-999999, df['var3'].mode()[0], inplace=True) # 최빈값으로 대체\ndf.drop('ID', axis=1, inplace=True)\n\n# feature와 label set 분리\nX_features = df.iloc[:, :-1]\ny_label = df.iloc[:, -1]\nprint('feature data shape: {0}'.format(X_features.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_features,\n                                                   y_label,\n                                                   test_size = .2, \n                                                   random_state = 0)\n\ntrain_cnt = y_train.count()\ntest_cnt = y_test.count()\nprint('train set shape: {0}, test set shape: {1}'.format(X_train.shape, X_test.shape))\n\nprint('train set label 값 분포 비율')\nprint(y_train.value_counts() / train_cnt)\nprint('test set label 값 분포 비율')\nprint(y_test.value_counts() / test_cnt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"학습과 테스트 데이터 세트 모두 `TARGET`의 값의 분포가 원본 데이터와 유사하게 전체 데이터의 4% 정도로 불만족 값으로 만들어졌습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# xgboost 학습 모델을 생성하고 예측 결과를 ROC AUC로 평가\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\n\nxgb = XGBClassifier(n_estimators = 500,\n                   random_state = 156)\n\nxgb.fit(X_train, y_train,\n       early_stopping_rounds = 100,\n       eval_metric = 'auc', \n       eval_set = [(X_train, y_train), (X_test, y_test)])\n\nxgb_roc_score = roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1],\n                             average = 'macro')\nprint('ROC AUC: {0: 4f}'.format(xgb_roc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"테스트 데이터 세트로 예측 시 ROC AUC는 약 84% 입니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# XGboost hyper-parameter 튜닝 수행\nfrom sklearn.model_selection import GridSearchCV\n\n# hyper-parameter test의 수행 속도를 향상시키기 위해 n_estimator를 100으로 감소\nxgb = XGBClassifier(n_estimator = 100)\n\nparams = {\n    'max_depth' : [5, 7],\n    'min_child_weight' : [1, 3],\n    'colsample_bytree' : [0.5, 0.75]\n}\n\ngridcv = GridSearchCV(xgb, param_grid = params, cv = 3)\ngridcv.fit(X_train, y_train,\n          early_stopping_rounds = 30,\n          eval_metric = 'auc', \n          eval_set = [(X_train, y_train), (X_test, y_test)])\nprint('GridSearchCV 최적 파라미터: ', gridcv.best_params_)\n\nxgb_roc_score = roc_auc_score(y_test,\n                             gridcv.predict_proba(X_test)[:, 1],\n                             average = 'macro')\nprint('ROC AUC: {0:.4f}'.format(xgb_roc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이전 예제에서 hyper-parameter를 적용한 이후 84.18%로 조금 개선되었습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 이전에 수정한 파라미터에 더해 다른 파라미터 수정을 진행\nxgb = XGBClassifier(n_estimator = 1000,\n                   random_state = 156, \n                   learning_rate = .02,\n                   max_depth = 7,\n                   min_child_weight = 3,\n                   colsample_bytree = .75,\n                   reg_alpha = .03)\n\nxgb.fit(X_train, y_train,\n       early_stopping_rounds = 200,\n       eval_metric = 'auc', \n       eval_set = [(X_train, y_train), (X_test, y_test)])\n\nxgb_roc_score = roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1],\n                             average = 'macro')\n\nprint('ROC AUC: {0:.4f}'.format(xgb_roc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"하이퍼 파라미터를 추가하여 수정하니 84.2% 일부 향상하였습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature importance 확인하기\nfrom xgboost import plot_importance\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\nplot_importance(xgb, ax=ax,\n               max_num_features = 20, \n               height = 0.4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LightGBM 모델을 이용하여 학습하기"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nlgbm = LGBMClassifier(n_estimator = 500)\nevals = [(X_test, y_test)]\n\nlgbm.fit(X_train, y_train,\n        early_stopping_rounds = 100,\n        eval_metric = 'auc',\n        eval_set = evals,\n        verbose = True)\nlgbm_roc_score = roc_auc_score(y_test, lgbm.predict_proba(X_test)[:, 1],\n                               average = 'macro')\nprint('ROC AUC: {0:.4f}'.format(lgbm_roc_score))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 이전에 xgboost보다 값이 감소 \n# hyper-parameter 조정 진행\n\nlgbm = LGBMClassifier(n_estimators = 200)\n\nparams = {\n    'num_leaves' : [32 ,64],\n    'max_depth' : [128, 160],\n    'min_child_samples' : [60, 100],\n    'subsample' : [0.8, 1]\n}\n\ngridcv = GridSearchCV(lgbm, param_grid = params, cv = 3)\ngridcv.fit(X_train, y_train,\n          early_stopping_rounds = 30,\n          eval_metric = 'auc',\n          eval_set = [(X_train, y_train), (X_test, y_test)])\nprint('GridSearchCV 최적 파라미터: ', gridcv.best_params_)\nlgbm_roc_score = roc_auc_score(y_test, gridcv.predict_proba(X_test)[:, 1],\n                              average = 'macro')\nprint('ROC AUC: {0:.4f}'.format(lgbm_roc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ROC AUC값이 84.42%로 향상된 것을 확인할 수 있습니다. 최적 파라미터를 적용하여 재학습 후, 측정 결과를 도출해 보겠습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMClassifier(n_estimator = 1000,\n                     num_leaves = 32,\n                     sumbsample = 0.8,\n                     min_child_samples = 100,\n                     max_depth = 128)\nevals = [(X_test, y_test)]\nlgbm.fit(X_train, y_train,\n        early_stopping_rounds = 100,\n        eval_metric = 'auc',\n        eval_set = evals,\n        verbose = True)\n\nlgbm_roc_score = roc_auc_score(y_test, lgbm.predict_proba(X_test)[:, 1],\n                               average = 'macro')\nprint('ROC AUC: {0:.4f}'.format(lgbm_roc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"최적 값은 84.42%로 나타납니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"# 제출하기\ntest = pd.read_csv('../input/santander-customer-satisfaction/test.csv', encoding = 'latin-1')\ntest_id = test['ID']\ntest.drop('ID', axis=1, inplace = True)\ntest.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = lgbm.predict_proba(test)[:, 1]\n\nsubmission = pd.DataFrame({'ID' : test_id, 'TARGET' : pred})\nsubmission.to_csv('submission.csv', index=False)\n\nprint('completed!')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}