{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).\ndecode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from itertools import combinations\nfrom sklearn import cross_validation as cv\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom numpy import array, array_equal \nfrom itertools import combinations\nimport pandas as pd\nfrom numpy import array, array_equal\nimport numpy as np\n\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def write_file(prediction, file_name='submission.csv'):\n    global test_dataset\n    submit = pd.DataFrame(index=test_dataset.index, data=prediction, columns=['TARGET'])\n    submit.head()\n    submit.to_csv(file_name, header=True)\n    print('Results written to file \"{}\".'.format(file_name))\n\ndef identify_constant_features(dataframe):\n    count_uniques = dataframe.apply(lambda x: len(x.unique()))\n    constants = count_uniques[count_uniques == 1].index.tolist()\n    return constants\n\ndef identify_equal_features(dataframe):\n    features_to_compare = list(combinations(dataframe.columns.tolist(),2))\n    equal_features = []\n    for compare in features_to_compare:\n        is_equal = array_equal(dataframe[compare[0]],dataframe[compare[1]])\n        if is_equal:\n            equal_features.append(list(compare))\n    return equal_features\n\ndef print_shapes():\n    global train_dataset\n    global test_dataset\n    print('Train: {}\\nTest: {}'.format(train_dataset.shape, test_dataset.shape))\n\n\ndef write_results(prediction, file_name='submission.csv'):\n    global test_dataset\n    submit = pd.DataFrame(index=test_dataset.index, data=prediction, columns=['TARGET'])\n    submit.head()\n    submit.to_csv(file_name, header=True)\n    print('Results written to file \"{}\".'.format(file_name))\n\ndef features_uncorrelated(threshold=0.8):\n    correlated = array(correlation_matrix[\n        correlation_matrix.abs() > threshold].index.tolist())\n    return list(set(X.columns.tolist())- set(correlated[:,1]))\n\ndef modelfit(alg, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n    #Source: http://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n    \n    #if not isinstance(dtrain, xgb.DMatrix):\n    #    raise TypeError('Parameter dtrain: expected DMatrix type, got {} type.'.format(type(dtrain)))\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain['TARGET'])\n\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n    \n    #Perform cross-validation:\n    if performCV:\n        cv_score = cv.cross_val_score(alg, dtrain[predictors], dtrain['TARGET'], cv=cv_folds, scoring='roc_auc')\n    \n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['TARGET'].values, dtrain_predictions))\n    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['TARGET'], dtrain_predprob))\n    \n    if performCV:\n        print(\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n        \n    #Print Feature Importance:\n    if printFeatureImportance:\n        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n        feat_imp.plot(kind='bar', title='Feature Importances')\n        plt.ylabel('Feature Importance Score')\n\n  "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"%%time\n\ntrain_dataset = pd.read_csv('../input/train.csv', index_col='ID')\ntest_dataset = pd.read_csv('../input/test.csv', index_col='ID')\n\n##\n## Drop the constant features\nconstant_features_train = identify_constant_features(train_dataset)\nprint('There were {} constant features in TRAIN dataset.'.format(\n        len(constant_features_train)))\ntrain_dataset.drop(constant_features_train, axis=1, inplace=True)\ntest_dataset.drop(constant_features_train, axis=1, inplace=True)\n\n##\n## Drop equal features\nequal_features_train = identify_equal_features(train_dataset)\n\nprint('There were {} pairs of equal features in TRAIN dataset.'.format(len(equal_features_train)))\n\nfeatures_to_drop = array(equal_features_train)[:,1]\ntrain_dataset.drop(features_to_drop, axis=1, inplace=True)\ntest_dataset.drop(features_to_drop, axis=1, inplace=True)\n\n##\n## Clean\n\n# The var3 is outlier\n#var3_outlier = -999999\n#train_dataset.var3.replace(var3_outlier, 0, inplace=True)\n#test_dataset.var3.replace(var3_outlier, 0, inplace=True)\n\n##\n## Define the variables model.\ny_name = 'TARGET'\nfeature_names = train_dataset.columns.tolist()\nfeature_names.remove(y_name)\n\nX = train_dataset[feature_names]\ny = train_dataset[y_name]\n\nX_submit = test_dataset[feature_names]\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train_dataset.shape, test_dataset.shape, X.shape, y.shape, X_submit.shape\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"%%time \n## Use a sklearn GBM\nfrom sklearn import ensemble\ngbm = ensemble.GradientBoostingClassifier(random_state=1)\nmodelfit(gbm, train_dataset, feature_names)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"%%time \n\n## Use a sklearn GBM\nfrom sklearn import ensemble\nxgbmodel = xgb.XGBClassifier(seed=1)\nmodelfit(xgbmodel, train_dataset, feature_names)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# cv.cross_val_score(xgb.XGBClassifier(), X_train, y_train, cv=3, scoring=score_metric)\n\n# Scores:  0.80735352,  0.83863037,  0.83129632"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"%%time\n\ndtrain = xgb.DMatrix(X_train, label=y_train, feature_names=X_train.columns.tolist())\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"%%time\nparams = {'objective':'binary:logistic' }\nxgb.cv(params, \n       dtrain, \n       metrics='auc',\n      nfolds=3)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"cv.cro"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}