{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nprint(check_output([\"ls\", \"../\"]).decode(\"utf8\"))\nprint(check_output([\"ls\", \".\"]).decode(\"utf8\"))\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\nimport matplotlib.pyplot as plt\nimport matplotlib\nmatplotlib.use(\"Agg\") #Needed to save figures\nfrom sklearn import cross_validation\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\n\ntraining = pd.read_csv(\"../input/train.csv\", index_col=0)\ntest = pd.read_csv(\"../input/test.csv\", index_col=0)\n\nprint(training.shape)\nprint(test.shape)\n\n# Replace -999999 in var3 column with most common value 2 \n# See https://www.kaggle.com/cast42/santander-customer-satisfaction/debugging-var3-999999\n# for details\ntraining = training.replace(-999999,2)\n\n\n# Replace 9999999999 with NaN\n# See https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/19291/data-dictionary/111360#post111360\n# training = training.replace(9999999999, np.nan)\n# training.dropna(inplace=True)\n# Leads to validation_0-auc:0.839577\n\nX = training.iloc[:,:-1]\ny = training.TARGET\n\n# Add zeros per row as extra feature\nX['n0'] = (X == 0).sum(axis=1)\n# # Add log of var38\n# X['logvar38'] = X['var38'].map(np.log1p)\n# # Encode var36 as category\n# X['var36'] = X['var36'].astype('category')\n# X = pd.get_dummies(X)\n\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_classif,chi2\nfrom sklearn.preprocessing import Binarizer, scale\n\np = 86 # 308 features validation_1-auc:0.848039\np = 80 # 284 features validation_1-auc:0.848414\np = 77 # 267 features validation_1-auc:0.848000\np = 75 # 261 features validation_1-auc:0.848642\n# p = 73 # 257 features validation_1-auc:0.848338\n# p = 70 # 259 features validation_1-auc:0.848588\n# p = 69 # 238 features validation_1-auc:0.848547\n# p = 67 # 247 features validation_1-auc:0.847925\n# p = 65 # 240 features validation_1-auc:0.846769\n# p = 60 # 222 features validation_1-auc:0.848581\n\nX_bin = Binarizer().fit_transform(scale(X))\nselectChi2 = SelectPercentile(chi2, percentile=p).fit(X_bin, y)\nselectF_classif = SelectPercentile(f_classif, percentile=p).fit(X, y)\n\nchi2_selected = selectChi2.get_support()\nchi2_selected_features = [ f for i,f in enumerate(X.columns) if chi2_selected[i]]\nprint('Chi2 selected {} features {}.'.format(chi2_selected.sum(),\n   chi2_selected_features))\nf_classif_selected = selectF_classif.get_support()\nf_classif_selected_features = [ f for i,f in enumerate(X.columns) if f_classif_selected[i]]\nprint('F_classif selected {} features {}.'.format(f_classif_selected.sum(),\n   f_classif_selected_features))\nselected = chi2_selected & f_classif_selected\nprint('Chi2 & F_classif selected {} features'.format(selected.sum()))\nfeatures = [ f for f,s in zip(X.columns, selected) if s]\nprint (features)\n\nX_sel = X[features]\n\nX_train, X_test, y_train, y_test = \\\n  cross_validation.train_test_split(X_sel, y, random_state=1301, stratify=y, test_size=0.3)\n\n# xgboost parameter tuning with p = 75\n# recipe: https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/forums/t/19083/best-practices-for-parameter-tuning-on-models/108783#post108783\n\nratio = float(np.sum(y == 1)) / np.sum(y==0)\n# Initial parameters for the parameter exploration\n# clf = xgb.XGBClassifier(missing=9999999999,\n#                 max_depth = 10,\n#                 n_estimators=1000,\n#                 learning_rate=0.1, \n#                 nthread=4,\n#                 subsample=1.0,\n#                 colsample_bytree=0.5,\n#                 min_child_weight = 5,\n#                 scale_pos_weight = ratio,\n#                 seed=4242)\n\n# gives : validation_1-auc:0.845644\n# max_depth=8 -> validation_1-auc:0.846341\n# max_depth=6 -> validation_1-auc:0.845738\n# max_depth=7 -> validation_1-auc:0.846504\n# subsample=0.8 -> validation_1-auc:0.844440\n# subsample=0.9 -> validation_1-auc:0.844746\n# subsample=1.0,  min_child_weight=8 -> validation_1-auc:0.843393\n# min_child_weight=3 -> validation_1-auc:0.848534\n# min_child_weight=1 -> validation_1-auc:0.846311\n# min_child_weight=4 -> validation_1-auc:0.847994\n# min_child_weight=2 -> validation_1-auc:0.847934\n# min_child_weight=3, colsample_bytree=0.3 -> validation_1-auc:0.847498\n# colsample_bytree=0.7 -> validation_1-auc:0.846984\n# colsample_bytree=0.6 -> validation_1-auc:0.847856\n# colsample_bytree=0.5, learning_rate=0.05 -> validation_1-auc:0.847347\n# max_depth=8 -> validation_1-auc:0.847352\n# learning_rate = 0.07 -> validation_1-auc:0.847432\n# learning_rate = 0.2 -> validation_1-auc:0.846444\n# learning_rate = 0.15 -> validation_1-auc:0.846889\n# learning_rate = 0.09 -> validation_1-auc:0.846680\n# learning_rate = 0.1 -> validation_1-auc:0.847432\n# max_depth=7 -> validation_1-auc:0.848534\n# learning_rate = 0.05 -> validation_1-auc:0.847347\n# \n\nclf = xgb.XGBClassifier(missing=9999999999,\n                max_depth = 7,\n                n_estimators=1000,\n                learning_rate=0.1, \n                nthread=4,\n                subsample=1.0,\n                colsample_bytree=0.5,\n                min_child_weight = 3,\n                scale_pos_weight = ratio,\n                seed=4242)\n                \nclf.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"auc\",\n        eval_set=[(X_train, y_train), (X_test, y_test)])\n        \nprint('Overall AUC:', roc_auc_score(y, clf.predict_proba(X_sel, ntree_limit=clf.best_iteration)[:,1]))\n\ntest['n0'] = (test == 0).sum(axis=1)\n# test['logvar38'] = test['var38'].map(np.log1p)\n# # Encode var36 as category\n# test['var36'] = test['var36'].astype('category')\n# test = pd.get_dummies(test)\nsel_test = test[features]    \ny_pred = clf.predict_proba(sel_test, ntree_limit=clf.best_iteration)\n\nsubmission = pd.DataFrame({\"ID\":test.index, \"TARGET\":y_pred[:,1]})\nsubmission.to_csv(\"submission.csv\", index=False)\n\nmapFeat = dict(zip([\"f\"+str(i) for i in range(len(features))],features))\nts = pd.Series(clf.booster().get_fscore())\n#ts.index = ts.reset_index()['index'].map(mapFeat)\nts.sort_values()[-15:].plot(kind=\"barh\", title=(\"features importance\"))\n\nfeatp = ts.sort_values()[-15:].plot(kind='barh', x='feature', y='fscore', legend=False, figsize=(6, 10))\nplt.title('XGBoost Feature Importance')\nfig_featp = featp.get_figure()\nfig_featp.savefig('feature_importance_xgb.png', bbox_inches='tight', pad_inches=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}