{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import os\nimport sys\nimport pandas as pd\nimport pickle as pkl\nimport numpy as np\nimport gc\nfrom pandas.core.common import array_equivalent\nfrom sklearn.cluster import k_means\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import euclidean_distances\n\nprint('Modules loaded')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#--read data\nprint('Reading data')\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nprint('Original shapes:', train.shape,test.shape)\n#--"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#--drop ID and TARGET\ntrain.drop('ID',axis=1,inplace=True)\ntestIds = test.ID\ntest.drop('ID',axis=1,inplace=True)\ntrainY = train.TARGET\ntrain.drop('TARGET',axis=1,inplace=True)\nprint('Shapes after dropping ID and TARGET:', train.shape,trainY.shape,test.shape)\n#pkl.dump(testIds,open('testIds.pkl','wb'))\n#--"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#--add zero count per row--\ndef zero_count(x):\n    return sum(x==0)\ntrain['zero_count'] = train.apply(zero_count,axis=1)\ntest['zero_count'] = test.apply(zero_count,axis=1)\nprint('Shapes after adding zero_count:', train.shape,trainY.shape,test.shape)\n#---"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#--remove constant vars--\nconstant_vars = list(train.columns[train.apply(pd.Series.nunique) == 1])\nprint('No. of constant vars in train:', len(constant_vars))\ntrain.drop(constant_vars,axis=1,inplace=True)\ntest.drop(constant_vars,axis=1,inplace=True)\nprint('Shapes after dropping constant vars in train:', train.shape,trainY.shape,test.shape)\n#--"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#--drop duplicate columns--\ndef duplicate_columns(frame):\n    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n    dups = []\n\n    for t, v in groups.items():\n\n        cs = frame[v].columns\n        vs = frame[v]\n        lcs = len(cs)\n\n        for i in range(lcs):\n            ia = vs.iloc[:,i].values\n            for j in range(i+1, lcs):\n                ja = vs.iloc[:,j].values\n                if array_equivalent(ia, ja):\n                    dups.append(cs[i])\n                    break\n\n    return dups\ndup_cols = duplicate_columns(train)\nprint('No. of duplicate cols:', len(dup_cols))\ntrain = train.drop(dup_cols, axis=1)\ntest = test.drop(dup_cols, axis=1)\nprint('Shapes after dropping duplicate columns:', train.shape,trainY.shape,test.shape)\n#--"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"corr = train.corr()\nprint('Correlation mat is calculated')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"n_clusters = 140\nw_ss_arr = []\nkm_arr = []"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def get_w_ss(centroids,idx):\n    w_ss = 0\n    for uidx in np.unique(idx):\n        d = euclidean_distances(corr[idx==uidx],[list(centroids[uidx])])\n        w_ss = w_ss+np.sum(np.square(np.squeeze(d)))\n    return w_ss\n\nfor i in range(140,n_clusters+1):\n    print('Clustering:', i)\n    km = k_means(corr,i)\n    w_ss_arr.append(get_w_ss(km[0],km[1]))\n    km_arr.append(km)\n\nplt.plot(range(140,n_clusters+1),w_ss_arr)\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#---\nidxs = km_arr[9][1] #---n_clusters = 139 seems to be the point of saturation on the scree plot\n\ncol_idx = np.arange(train.shape[1])\nrearranged_col_idx = []\n\nfor unique_idx in np.unique(idxs):\n    rearranged_col_idx = rearranged_col_idx + list(col_idx[idxs==unique_idx])\n\n#train = train.ix[:,rearranged_col_idx]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train = train.ix[:,np.array(train.columns)[rearranged_col_idx]]\ncorr = train.corr()\n\nprint('Re-arranged correlation matrix calculated')\n\n#plot heatmap\nsns.set(context=\"paper\", font=\"monospace\")\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr, vmax=.8, square=True)\nf.tight_layout()"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}