{"cells":[{"cell_type":"markdown","metadata":{},"source":"The most important feature for XGBoost is var15. According to [a Kaggle form post](https://www.kaggle.com/c/santander-customer-satisfaction/forums/t/19291/data-dictionary/110414#post110414)\n    var15 is the age of the customer. Let's explore var15"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train['var15'].describe()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"#Looks more normal, plot the histogram\ntrain['var15'].hist(bins=100);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's look at the density of the age of happy/unhappy customers\nsns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"var15\") \\\n   .add_legend()\nplt.title('Unhappy customers are slightly older');"},{"cell_type":"markdown","metadata":{},"source":"# saldo_var30"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train.saldo_var30.hist(bins=100)\nplt.xlim(0, train.saldo_var30.max());"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# improve the plot by making the x axis logarithmic\ntrain['log_saldo_var30'] = train.saldo_var30.map(np.log)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's look at the density of the age of happy/unhappy customers for saldo_var30\nsns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"log_saldo_var30\") \\\n   .add_legend();"},{"cell_type":"markdown","metadata":{},"source":"# Explore the interaction between var15 (age) and var38"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train, hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"var38\", \"var15\") \\\n   .add_legend();"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train, hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"logvar38\", \"var15\") \\\n   .add_legend()\nplt.ylim([0,120]); # Age must be positive ;-)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Exclude most common value for var38 \nsns.FacetGrid(train[~train.var38mc], hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"logvar38\", \"var15\") \\\n   .add_legend()\nplt.ylim([0,120]);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# What is distribution of the age when var38 has it's most common value ?\nsns.FacetGrid(train[train.var38mc], hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"var15\") \\\n   .add_legend();"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# What is density of n0 ?\nsns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"n0\") \\\n   .add_legend()\nplt.title('Unhappy customers have a lot of features that are zero');"},{"cell_type":"markdown","metadata":{},"source":"# Select the most important features"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import f_classif,chi2\nfrom sklearn.preprocessing import Binarizer, scale\n\n# First select features based on chi2 and f_classif\np = 3\n\nX_bin = Binarizer().fit_transform(scale(X))\nselectChi2 = SelectPercentile(chi2, percentile=p).fit(X_bin, y)\nselectF_classif = SelectPercentile(f_classif, percentile=p).fit(X, y)\n\nchi2_selected = selectChi2.get_support()\nchi2_selected_features = [ f for i,f in enumerate(X.columns) if chi2_selected[i]]\nprint('Chi2 selected {} features {}.'.format(chi2_selected.sum(),\n   chi2_selected_features))\nf_classif_selected = selectF_classif.get_support()\nf_classif_selected_features = [ f for i,f in enumerate(X.columns) if f_classif_selected[i]]\nprint('F_classif selected {} features {}.'.format(f_classif_selected.sum(),\n   f_classif_selected_features))\nselected = chi2_selected & f_classif_selected\nprint('Chi2 & F_classif selected {} features'.format(selected.sum()))\nfeatures = [ f for f,s in zip(X.columns, selected) if s]\nprint (features)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Make a dataframe with the selected features and the target variable\nX_sel = train[features+['TARGET']]"},{"cell_type":"markdown","metadata":{},"source":"# var36"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"X_sel['var36'].value_counts()"},{"cell_type":"markdown","metadata":{},"source":"var36 is most of the times 99 or [0,1,2,3]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's plot the density in function of the target variabele\nsns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"var36\") \\\n   .add_legend()\nplt.title('If var36 is 0,1,2 or 3 => less unhappy customers');"},{"cell_type":"markdown","metadata":{},"source":"In above plot we see that the density of unhappy custormers is lower when var36 is not 99"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# var36 in function of var38 (most common value excluded) \nsns.FacetGrid(train[~train.var38mc], hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"var36\", \"logvar38\") \\\n   .add_legend();"},{"cell_type":"markdown","metadata":{},"source":"Let's seperate that in two plots"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train[(~train.var38mc) & (train.var36 < 4)], hue=\"TARGET\", size=10) \\\n   .map(plt.scatter, \"var36\", \"logvar38\") \\\n   .add_legend()\nplt.title('If var36==0, only happy customers');"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's plot the density in function of the target variabele, when var36 = 99\nsns.FacetGrid(train[(~train.var38mc) & (train.var36 ==99)], hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"logvar38\") \\\n   .add_legend();"},{"cell_type":"markdown","metadata":{},"source":"# num_var5"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train.num_var5.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train[train.TARGET==1].num_var5.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train[train.TARGET==0].num_var5.value_counts()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(plt.hist, \"num_var5\") \\\n   .add_legend();"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.FacetGrid(train, hue=\"TARGET\", size=6) \\\n   .map(sns.kdeplot, \"num_var5\") \\\n   .add_legend();"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.pairplot(train[['var15','var36','logvar38','TARGET']], hue=\"TARGET\", size=2, diag_kind=\"kde\");"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train[['var15','var36','logvar38','TARGET']].boxplot(by=\"TARGET\", figsize=(12, 6));"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# A final multivariate visualization technique pandas has is radviz\n# Which puts each feature as a point on a 2D plane, and then simulates\n# having each sample attached to those points through a spring weighted\n# by the relative value for that feature\nfrom pandas.tools.plotting import radviz\nradviz(train[['var15','var36','logvar38','TARGET']], \"TARGET\");"},{"cell_type":"markdown","metadata":{},"source":"# now look at all 8 features together"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"features"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"radviz(train[features+['TARGET']], \"TARGET\");"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"sns.pairplot(train[features+['TARGET']], hue=\"TARGET\", size=2, diag_kind=\"kde\");"},{"cell_type":"markdown","metadata":{},"source":"# Correlations"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"cor_mat = X.corr()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"f, ax = plt.subplots(figsize=(15, 12))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(cor_mat,linewidths=.5, ax=ax);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"cor_mat = X_sel.corr()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"f, ax = plt.subplots(figsize=(15, 12))\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(cor_mat,linewidths=.5, ax=ax);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# only important correlations and not auto-correlations\nthreshold = 0.7\nimportant_corrs = (cor_mat[abs(cor_mat) > threshold][cor_mat != 1.0]) \\\n    .unstack().dropna().to_dict()\nunique_important_corrs = pd.DataFrame(\n    list(set([(tuple(sorted(key)), important_corrs[key]) \\\n    for key in important_corrs])), columns=['attribute pair', 'correlation'])\n# sorted by absolute value\nunique_important_corrs = unique_important_corrs.ix[\n    abs(unique_important_corrs['correlation']).argsort()[::-1]]\nunique_important_corrs"},{"cell_type":"markdown","metadata":{},"source":"# Clusters "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Recipe from https://github.com/mgalardini/python_plotting_snippets/blob/master/notebooks/clusters.ipynb\nimport matplotlib.patches as patches\nfrom scipy.cluster import hierarchy\nfrom scipy.stats.mstats import mquantiles\nfrom scipy.cluster.hierarchy import dendrogram, linkage"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Correlate the data\n# also precompute the linkage\n# so we can pick up the \n# hierarchical thresholds beforehand\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\n\n# scale to mean 0, variance 1\ntrain_std = pd.DataFrame(scale(X_sel))\ntrain_std.columns = X_sel.columns\nm = train_std.corr()\nl = linkage(m, 'ward')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Plot the clustermap\n# Save the returned object for further plotting\nmclust = sns.clustermap(m,\n               linewidths=0,\n               cmap=plt.get_cmap('RdBu'),\n               vmax=1,\n               vmin=-1,\n               figsize=(14, 14),\n               row_linkage=l,\n               col_linkage=l)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Threshold 1: median of the\n# distance thresholds computed by scipy\nt = np.median(hierarchy.maxdists(l))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Plot the clustermap\n# Save the returned object for further plotting\nmclust = sns.clustermap(m,\n               linewidths=0,\n               cmap=plt.get_cmap('RdBu'),\n               vmax=1,\n               vmin=-1,\n               figsize=(12, 12),\n               row_linkage=l,\n               col_linkage=l)\n\n# Draw the threshold lines\nmclust.ax_col_dendrogram.hlines(t,\n                               0,\n                               m.shape[0]*10,\n                               colors='r',\n                               linewidths=2,\n                               zorder=1)\nmclust.ax_row_dendrogram.vlines(t,\n                               0,\n                               m.shape[0]*10,\n                               colors='r',\n                               linewidths=2,\n                               zorder=1)\n\n# Extract the clusters\nclusters = hierarchy.fcluster(l, t, 'distance')\nfor c in set(clusters):\n    # Retrieve the position in the clustered matrix\n    index = [x for x in range(m.shape[0])\n             if mclust.data2d.columns[x] in m.index[clusters == c]]\n    # No singletons, please\n    if len(index) == 1:\n        continue\n\n    # Draw a rectangle around the cluster\n    mclust.ax_heatmap.add_patch(\n        patches.Rectangle(\n            (min(index),\n             m.shape[0] - max(index) - 1),\n                len(index),\n                len(index),\n                facecolor='none',\n                edgecolor='r',\n                lw=3)\n        )\n\nplt.title('Cluster matrix')\n\npass"},{"cell_type":"markdown","metadata":{},"source":"For clustering with more features, have a look at: [https://www.kaggle.com/cast42/santander-customer-satisfaction/correlation-pairs](https://www.kaggle.com/cast42/santander-customer-satisfaction/correlation-pairs)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}