{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nfrom sklearn.feature_selection import VarianceThreshold\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_columns = None\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (16,7)\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_curve,roc_auc_score,precision_score,recall_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scikitplot.helpers import binary_ks_curve\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import validation_curve\nfrom category_encoders import OneHotEncoder,TargetEncoder\nfrom yellowbrick.model_selection import RFECV\nimport shap","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-02T12:23:05.629518Z","iopub.execute_input":"2022-05-02T12:23:05.629869Z","iopub.status.idle":"2022-05-02T12:23:09.019202Z","shell.execute_reply.started":"2022-05-02T12:23:05.629784Z","shell.execute_reply":"2022-05-02T12:23:09.018356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Read dataframe","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/santander-customer-satisfaction/train.csv\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:09.022673Z","iopub.execute_input":"2022-05-02T12:23:09.022916Z","iopub.status.idle":"2022-05-02T12:23:11.969528Z","shell.execute_reply.started":"2022-05-02T12:23:09.022889Z","shell.execute_reply":"2022-05-02T12:23:11.968623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining features\n\nAqui estamos dividindo o dataframe em 3 grupos:\n\n#### 1 - Colunas de identificação\n\n- Colunas que representam chaves ID \n\n#### 2 Target\n\n- Coluna que identifica a target do problema\n\n#### 3 Variáveis dependentes/explicativas\n\n- Aqui são todas as features disponiveis, esse grupo foi dividido em 2 subgrupos que são variaveis categoricas e variaveis continuas","metadata":{"execution":{"iopub.status.busy":"2022-03-20T16:30:51.39061Z","iopub.execute_input":"2022-03-20T16:30:51.391052Z","iopub.status.idle":"2022-03-20T16:30:51.402822Z","shell.execute_reply.started":"2022-03-20T16:30:51.39102Z","shell.execute_reply":"2022-03-20T16:30:51.402172Z"}}},{"cell_type":"code","source":"id_columns = ['ID']\ntarget_column = ['TARGET']\n\nnum_vars = df_train.select_dtypes(include=['float64','int64'])\ncat_vars = df_train.select_dtypes(include=['object'])\n\nprint('initial numerical vars =',len(num_vars.columns))\nprint('initial categorical vars =',len(cat_vars.columns))\n\ny = df_train[target_column]","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:11.970539Z","iopub.execute_input":"2022-05-02T12:23:11.970763Z","iopub.status.idle":"2022-05-02T12:23:12.060525Z","shell.execute_reply.started":"2022-05-02T12:23:11.970739Z","shell.execute_reply":"2022-05-02T12:23:12.059635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dropping null features if exists","metadata":{}},{"cell_type":"code","source":"def drop_nulls(df,threshold,num_features):\n    missing = pd.DataFrame({'types':df.filter(num_features).dtypes, 'percentual_nulo': df.filter(num_features).isna().sum()/len(df.filter(num_features))})\n    missing = missing[missing['percentual_nulo'] > threshold].sort_values(by='percentual_nulo',ascending=False)\n    print(missing)\n    deletar = list(missing[missing['percentual_nulo'] > threshold].index)\n    print('number of null features = ',len(deletar))\n    df_drop = df.drop(columns= deletar,axis=1)\n    return df_drop\n\ndf_train_null = drop_nulls(df_train,0.5,num_features = num_vars )","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:12.063307Z","iopub.execute_input":"2022-05-02T12:23:12.063528Z","iopub.status.idle":"2022-05-02T12:23:12.46337Z","shell.execute_reply.started":"2022-05-02T12:23:12.063502Z","shell.execute_reply":"2022-05-02T12:23:12.462395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dimensionality reduction (Variance threshould, multi correlation)\n\nCom intuito de minimizar o custo de altas dimensões, ajudando a diminuir a complexidade computacional de todos algoritmos que serão criados e diminuir a chance de problemas de sobreajuste devido ao alto numero de dimensoes iremos fazer uma análise para diminuir o espaco de possibilidades. Todo processo foi divididos nessas etapas:\n\n- Eliminação de variaveis constantes ou que possuam pouca variação (variance threshould)\n- Eliminação de multicolinearidade em variaveis continuas\n- Eliminação de variaveis categoricas com mais de 10 níveis ","metadata":{}},{"cell_type":"markdown","source":"### Número total de features disponíveis ","metadata":{}},{"cell_type":"code","source":"print('Número total de features =', len(df_train_null.columns))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:12.464407Z","iopub.execute_input":"2022-05-02T12:23:12.464617Z","iopub.status.idle":"2022-05-02T12:23:12.46875Z","shell.execute_reply.started":"2022-05-02T12:23:12.464592Z","shell.execute_reply":"2022-05-02T12:23:12.46822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop categorical feature with many values","metadata":{}},{"cell_type":"code","source":"def drop_categorical_with_many_values(df,categorical_features, threshold):\n    df.filter(cat_vars).nunique().sort_values(ascending=False).head(50).plot(kind='bar')\n    condition = df.filter(cat_vars).nunique().sort_values(ascending=False).reset_index()[0]>threshold\n    list_to_drop = df.filter(cat_vars).nunique().sort_values(ascending=False).reset_index()[condition]\n    print('number of feauters to delete = ',len(list_to_drop))\n    df_train_null_drop = df.drop(columns = list_to_drop['index'])\n    return df_train_null_drop\n\n# df_train_null_drop = drop_categorical_with_many_values(df = df_train_null\n#                                                        ,categorical_features = cat_vars\n#                                                        , threshold = 10)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:12.46967Z","iopub.execute_input":"2022-05-02T12:23:12.469873Z","iopub.status.idle":"2022-05-02T12:23:12.48168Z","shell.execute_reply.started":"2022-05-02T12:23:12.469829Z","shell.execute_reply":"2022-05-02T12:23:12.480952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Number of features after treatment","metadata":{}},{"cell_type":"code","source":"df_train_null_drop = df_train_null.copy()\nprint('Número total de features =', len(df_train_null_drop.columns))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:12.482928Z","iopub.execute_input":"2022-05-02T12:23:12.483227Z","iopub.status.idle":"2022-05-02T12:23:12.605845Z","shell.execute_reply.started":"2022-05-02T12:23:12.483191Z","shell.execute_reply":"2022-05-02T12:23:12.605269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dropping duplicates columns","metadata":{}},{"cell_type":"code","source":"# Checking if there is any duplicated column\nremove = []\ncols = df_train_null_drop.columns\nfor i in range(len(cols)-1):\n    column = df_train_null_drop[cols[i]].values\n    for j in range(i+1,len(cols)):\n        if np.array_equal(column, df_train_null_drop[cols[j]].values):\n            remove.append(cols[j])\n\n\n# If yes, than they will be dropped here\ndf_train_null_drop = df_train_null_drop.drop(remove, axis = 1)\nprint('Número total de features =', len(df_train_null_drop.columns))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:12.606875Z","iopub.execute_input":"2022-05-02T12:23:12.607069Z","iopub.status.idle":"2022-05-02T12:23:20.535253Z","shell.execute_reply.started":"2022-05-02T12:23:12.607046Z","shell.execute_reply":"2022-05-02T12:23:20.534333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Split in train and test\n\n- Foram usados 25% dos dados para teste e 75% para treino\n- O split foi feito de forma estratificada para preservarmos a distribuição da target em treino e teste","metadata":{}},{"cell_type":"code","source":"x = df_train_null_drop.fillna(0)\n\n\n#Split em treino e teste \nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.25,random_state=42,stratify=y)\n\nprint(\"number of rows in train data = \",len(x_train))\nprint('---------------------------')\nprint(y_train.value_counts().reset_index())\nprint('---------------------------')\nprint(\"number of rows in test data = \",len(x_test))\nprint('---------------------------')\nprint(y_test.value_counts().reset_index())","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:20.536477Z","iopub.execute_input":"2022-05-02T12:23:20.537194Z","iopub.status.idle":"2022-05-02T12:23:21.135205Z","shell.execute_reply.started":"2022-05-02T12:23:20.537154Z","shell.execute_reply":"2022-05-02T12:23:21.134276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining some function (Outliers treatment, variance threshould and multi correlation)\n\n### Clip de outliers\n- Para usarmos as técnicas de agrupamento fizemos o tratamento de outliers, pois o algoritmo escolhido para clusterização é sensivel a dados ruidos\n\n### Variance threshould\n\n- Foi utilizado o variance threshould nas variaveis continuas para tirar variaveis constantes ou que possuam uma pequena variação essa estratégia tem o intuito de não deixar que pequenas variações impactem no modelo, reduzindo a chance de um sobreajuste/overfitting e além disso diminuir o número de dimensões para diminuirmos o custo computacional dos próximos passos\n\n### Eliminação de features correlacionadas\n\n- Foi retirado a correlação entre as variaveis continuas para que possamos diminuir o custo computacional dos próximos passos","metadata":{}},{"cell_type":"code","source":"def clip_outliers(df,drop_columns,q_min,q_max):\n    \n    num_vars_name = df.select_dtypes(include=['float64']).columns     \n    for i in num_vars_name:\n        \n        #get max and min quantile\n        min_value = df.loc[:,i].quantile(q_min)\n        max_value = df.loc[:,i].quantile(q_max)\n\n        #replace values with max and min quantile value\n        df.loc[:,i] = np.where(df.loc[:,i] < min_value, min_value,df.loc[:,i])\n        df.loc[:,i] = np.where(df.loc[:,i] > max_value, min_value,df.loc[:,i])\n        \n    return df\n\ndef variance_threshold(df,threshold):\n    vt = VarianceThreshold(threshold=threshold)\n\n    vt.fit(df)\n\n    mask = vt.get_support()\n\n    num_vars_reduced = df.iloc[:, mask]\n    return num_vars_reduced\n\ndef correlation(df, threshold):\n    col_corr = set() # Set of all the names of deleted columns\n    corr_matrix = df.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n                colname = corr_matrix.columns[i] # getting the name of column\n                col_corr.add(colname)\n                if colname in df.columns:\n                    del df[colname] # deleting the column from the dataset\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:21.137796Z","iopub.execute_input":"2022-05-02T12:23:21.138026Z","iopub.status.idle":"2022-05-02T12:23:21.146742Z","shell.execute_reply.started":"2022-05-02T12:23:21.137998Z","shell.execute_reply":"2022-05-02T12:23:21.146116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Aplplying variance threshould, drop correlation features and clip the outliers","metadata":{}},{"cell_type":"code","source":"num_vars_vt = variance_threshold(x_train.filter(num_vars),threshold = 0.01)\nnum_vars_vt_corr = correlation(x_train.filter(num_vars_vt), threshold = 0.7)\n\n#Scaler feature for clustering analyses\nscaler = StandardScaler()\nscaler.fit(x_train.filter(num_vars_vt_corr))\n\n#Apply in train and test\nx_train.loc[:,num_vars_vt_corr.columns] = scaler.transform(x_train.loc[:,num_vars_vt_corr.columns])\nx_test.loc[:,num_vars_vt_corr.columns] = scaler.transform(x_test.loc[:,num_vars_vt_corr.columns])\n\n           \n#Select important features\nx_train = x_train.filter(list(num_vars_vt_corr.columns)+list(cat_vars)+list(id_columns)).fillna(0)\nx_test  =  x_test.filter(list(num_vars_vt_corr.columns)+list(cat_vars)+list(id_columns)).fillna(0)\n\n\n#Reduce outliers for clustering analyses\nx_train = clip_outliers(x_train,drop_columns=id_columns,q_min=0.05,q_max=0.95)\nx_test = clip_outliers(x_test,drop_columns=id_columns,q_min=0.05,q_max=0.95)\n\n\nprint('Número total de features =', len(x_train.columns))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:21.147463Z","iopub.execute_input":"2022-05-02T12:23:21.147668Z","iopub.status.idle":"2022-05-02T12:23:36.337424Z","shell.execute_reply.started":"2022-05-02T12:23:21.147643Z","shell.execute_reply":"2022-05-02T12:23:36.336616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### One hot enconder\n\n- Aqui todas as variáveis categoricas que sobraram do step de retirada de cardionalidade serão transformadas em dummy 0 e 1","metadata":{}},{"cell_type":"code","source":"### Convert categoricals features to string\n# d = dict.fromkeys(x_train.select_dtypes(np.int64).columns, 'str')\n# x_train =x_train.astype(d)\n# x_test = x_test.astype(d)\n\n# ##Fitting a oneHotEnconder\n# enc = OneHotEncoder().fit(x_train.drop(columns=id_columns))\n\n# x_train  = enc.transform(x_train.drop(columns=id_columns))\n# x_test = enc.transform(x_test.drop(columns=id_columns))\n\nprint(x_train.shape,x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:36.33879Z","iopub.execute_input":"2022-05-02T12:23:36.339006Z","iopub.status.idle":"2022-05-02T12:23:36.343668Z","shell.execute_reply.started":"2022-05-02T12:23:36.33898Z","shell.execute_reply":"2022-05-02T12:23:36.342854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Recursive Feature elimination (Feature selection)\n\nCom intuito de selecionar as variaveis que melhor irão nos ajudar a discriminar nosso problema iremos utilizar um método chamado recursive feature elimination. esse método funciona da seguinte forma:\n\n- 1) Treina um modelo com todas as features\n- 2) Elimina as features com feature_importances_ menores\n- 3) Retreina um novo modelo com as features restantes\n- 4) Repete passo 2 e 3\n- 5) Avalia o número de features selecionadas versus a métrica de sucesso do seu modelo\n\nAqui escolhemos o algoritmo de Random forest e a métrica de avaliação a ROC AUC","metadata":{}},{"cell_type":"code","source":"fs_model = RandomForestClassifier(max_depth=6,n_jobs=-1,n_estimators=100,class_weight='balanced')\n\n# Instantiate RFECV visualizer with a linear Random forest classifier\nvisualizer = RFECV(fs_model,scoring='roc_auc',cv=3,step=0.1)\n\n# Fit the data to the visualizer\nvisualizer.fit(x_train, y_train) \n\n# Finalize and render the figure\nvisualizer.show()           ","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:23:36.344983Z","iopub.execute_input":"2022-05-02T12:23:36.345501Z","iopub.status.idle":"2022-05-02T12:35:55.948316Z","shell.execute_reply.started":"2022-05-02T12:23:36.34547Z","shell.execute_reply":"2022-05-02T12:35:55.947734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Select the best features","metadata":{}},{"cell_type":"code","source":"print('Optimal number of features :', visualizer.n_features_)\nbest_features = list(x_train.columns[visualizer.support_])\nprint('features selecionadas: ', best_features)\n\nx_train = x_train[best_features]\nx_test = x_test[best_features]","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:35:55.949375Z","iopub.execute_input":"2022-05-02T12:35:55.950206Z","iopub.status.idle":"2022-05-02T12:35:55.959947Z","shell.execute_reply.started":"2022-05-02T12:35:55.95016Z","shell.execute_reply":"2022-05-02T12:35:55.959323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clustering analysis\n\nO objetivo dessa frente é segmentar o conjunto de dados, encontrando padrões escondidos na nossa base, a ideia é que pontos dentro do mesmo grupo sejam parecidos entre si **(coesão interna)** e que pontos de grupos distintos sejam diferentes **(separabilidade)**\n\nPara conseguirmos esse tipo de agrupamento podemos utilizar 3 tipos de agrupamento que são:\n\n- Particionais\n- hierarquicos\n- Baseado em densidade\n\nAqui escolhemos um método particional que é o algoritmo de kmeans que funcionará da seguinte forma:\n\n- 1) Escolha K centroides\n- 2) Atribua cada ponto ao K centroide mais próximo\n- 3) Recalcula o centro do centroide com base na média dos pontos do cluster\n- 4) Repete-se o passo 2 e 3 até que se atinja um número pré definido de iterações ou um limiar mínimo de mudanças no centroide que mudem a função de custo\n\nUm dos problemas desse tipo de agrupamento particional é que temos que definir (antes de rodar) o número ideial de clusters e como não sabemos o número ideal de clusters que melhor irar separar os dados iremos fazer algumas iterações para verificar o número de K ótimo","metadata":{}},{"cell_type":"markdown","source":"### Defining the best number of K (cluster)\n\nPara definirmos o melhor número de clusters para o nosso agrupamento vamos utilizar 3 métricas que são:\n\n**1) Método Elbow**\n\nElbow é um método visual para estimarmos o número ótimo de clusters, basicamente rodamos o Kmeans multiplas vezes para K diferentes e plotamos o inertia **(soma das distancias ao quadrado de cada ponto ao seu respectivo cluster)**.\nO único problema desse método é que estamos olhando apenas o conceito de **coesa interna**, ou seja, estamos escolhendo um K que de certa forma minimiza a distancia intra cluster, proem não estamos olhando o quesito de **separabilidade** (máximizar a distancia entre clusters)\n\n**2) Silhouette Score**\n\nNesse método iremos usar os dois conceitos mencionados acima coesao e separabilidade, por fim queremos um cluster que minimize as distancias intra clusters (coesao) e maximize a distancia entre clusters (separbilidade). O silhouete score trabalha com esses dois conceitos, utilizando a propria inertia calculada pela funcao de custo do kmeans e a distancia entre o ponto e os pontos do clusters mais proximo ao seu. a formula fica:\n\na = distancia intra cluster / inertia \nb = distancia entre cluster (vizinho mais próximo)\n\nsilhouette = (b - a) / Max(a,b)\n\nUm ponto de atenção é que para clusters diferentes de esféricos e com caracteristicas de densidade a silhouette perde o sentido, pois tem forte relação a métrica de distancia usada para calcular o A e B e no geral usa-se a distância euclidiana que gera clusters globulares\n\n**3) Davies Bouldin score**\n\nO método de Davies bouldin é baseado na razão entre distancia intra cluster e distancia entre clusters. A diferença emrelação ao silhouette é que esse método constuma ser mais custoso por considerar a distancia entre clusters e não só a distancia entre o cluster vizinho mais próximo \n","metadata":{}},{"cell_type":"markdown","source":"### Kmeans with elbow method and silhouette score\n\n- Aqui foi testado o kmeans direto com sua função de custo para termos uma ideia inicia de número cluster ótimo","metadata":{}},{"cell_type":"code","source":"min = 3\nmax = 8\nwcss = []\nsilhouette= []\n\ntrain_kmeans = x_train.select_dtypes(include=\"float64\")\n\nfor i in range(min, max):\n    \n    ##Training a kmeans model\n    model = KMeans(n_clusters = i, random_state = 42)\n    model.fit(train_kmeans)\n    \n    #Scoring\n    pred = model.predict(train_kmeans)\n    \n    #Get silhouette score\n    score = silhouette_score(train_kmeans, pred)\n    \n    # inertia method returns wcss for that model\n    wcss.append(model.inertia_)\n    print('Silhouette Score for k = {}: {:<.3f}'.format(i, score))\n    \nplt.figure(figsize=(10,max))\nsns.lineplot(range(min, max), wcss,marker='o',color='red')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:35:55.961194Z","iopub.execute_input":"2022-05-02T12:35:55.96163Z","iopub.status.idle":"2022-05-02T12:39:06.906105Z","shell.execute_reply.started":"2022-05-02T12:35:55.961595Z","shell.execute_reply":"2022-05-02T12:39:06.905096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Kmeans with hyperopt to optimize silhouette score\n\n- Aqui será ajustado a função de custo do hyperopt, mas inicialmente foi utilizado o silhouette score para termos clusters coesos e com alta separabilidade","metadata":{}},{"cell_type":"code","source":"from hyperopt import fmin, tpe, hp\ndef objective(params):\n    params = {'n_clusters': int(params['n_clusters'])\n              ,'n_init': int(params['n_init'])\n              ,'max_iter': int(params['max_iter'])}\n    \n    model = KMeans(random_state = 42, **params).fit(train_kmeans)\n    pred = model.predict(train_kmeans)\n    silhouette = silhouette_score(train_kmeans, pred)\n    score = (1-silhouette)\n    print(\"Silhouette {:.3f} params {}\".format(silhouette , params))\n    return score\n\nspace = {\n    'n_clusters': hp.quniform('n_clusters', 3, 10, 1),\n    'n_init': hp.quniform('max_depth', 10, 30, 2),\n    \"max_iter\": hp.quniform('max_iter', 300, 1000, 50)\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:39:06.907985Z","iopub.execute_input":"2022-05-02T12:39:06.908255Z","iopub.status.idle":"2022-05-02T12:42:29.916136Z","shell.execute_reply.started":"2022-05-02T12:39:06.908225Z","shell.execute_reply":"2022-05-02T12:42:29.915028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing Gaussian mix models\n\nAqui iremos testar um outro método de clusterização que é o GMM (Gaussian mixture models), é um modelo que tem como arte solucionar um dos problemas do Kmeans que só detectam clusters **globulares**, o GMM é uma generalização do kmeans, porem ao inves de estimar os centroides iremos estimar **3 parametros** que são:\n- **Média**\n- **Matriz de covariancia**\n- **pesos das gausianas** (ou probabilidade de um ponto tenha sido gerado por uma gausiana k)\n\nO algoritmo simular ao kmeans utiliza-se de **4 grandes passos**:\n\n- 1) Inicialização dos parametros (média, covariancia e pesos)\n- 2) Expectation (Calcula o valor esperado para a log da verossimelhanca com base em variável latente)\n- 3) Maximization (Recalcula os paramétros com intuito de maximizar a log da verossimelhança)\n- 4) Repete-se o passo 2 e 3 até que se atinja um número pré definido de iterações ou um limiar mínimo de mudanças na log verossimilhanca\n\nDiferente do Kmeans onde a saída é uma partição rigida, aqui temos um modelo cujo a **saida é probabilistica**, então temos uma probabilidade a um ponto X pertencer a um cluster K, alem disso, devido a estimativa da matriz de covariancia o GMM produz clusters com diferentes formatos como elipses, fato que não ocorre no Kmeans. Outro fato interessante é a **função de custo** do GMM está relacionada a maximização da log verossimilhança, ou seja, queremos que aproximizar a distribuição das gausianas da distribuição final que gerou os dados. Apesar de todas as vantagens o GMM acaba sendo **mais custoso computacionalmente**, pois nas etapas 2 e 3 (EM) é necessário calcular a matriz inversa de covariancia o que deixa o modelo com uma complexidade O(K.N.D³).\n\n**Principais parâmetros**\n\nAlém dos parâmetros conhecidos do kmeans, aqui temos o **covariance type** que irá ditar o formato dos nossos clusters com as opções full, tied, diag e spherical. O **init params** que está relacionado a inicialização dos 3 parâmetros que pode ser randomico ou até pelo proprio kmeans\n\n**CUIDADO** - Caso tenhamos clusters diferentes de globulares no gmm, métricas como silhouette perdem o sentido","metadata":{}},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\n\nmin = 3\nmax = 8\nsilhouette= []\n\ntrain_gmm = x_train.select_dtypes(include=\"float64\")\n\nfor i in range(min,max):\n    gmm = GaussianMixture(n_components = i, random_state = 42).fit(train_gmm) \n    pred=gmm.predict(train_gmm)\n    score = silhouette_score(train_gmm, pred)\n    print('Silhouette Score for k = {}: {:<.3f}'.format(i, score))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:42:29.91781Z","iopub.execute_input":"2022-05-02T12:42:29.91866Z","iopub.status.idle":"2022-05-02T12:45:43.047117Z","shell.execute_reply.started":"2022-05-02T12:42:29.918616Z","shell.execute_reply":"2022-05-02T12:45:43.04621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing the same pipeline of hyperopt in GMM\n\nIntuito de encontrar a maior silhouetta interandoa alguns hiperparametros para os proximos passos iremos iterar no covariance type para verificarmos se temos um formato melhor de covariancia que se adequa aos dados e iremos iterar a inicialização entre randomica ou iniciar com a saída de um kmeans","metadata":{}},{"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\nfrom hyperopt import fmin, tpe, hp\ndef objective(params):\n    params = {'n_components': int(params['n_components'])\n              ,'n_init': int(params['n_init'])\n              ,'max_iter': int(params['max_iter'])}\n\n    model = GaussianMixture(random_state = 42, **params).fit(train_gmm) \n    pred = model.predict(train_gmm)\n    silhouette = silhouette_score(train_gmm, pred)\n    score = (1-silhouette)\n    print(\"Silhouette {:.3f} params {}\".format(silhouette , params))\n    return score\n\nspace = {\n    'n_components': hp.quniform('n_components', 3, 10, 1),\n    'n_init': hp.quniform('max_depth', 10, 30, 2),\n    'max_iter': hp.quniform('max_iter', 300, 1000, 50),\n}\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:45:43.048893Z","iopub.execute_input":"2022-05-02T12:45:43.049209Z","iopub.status.idle":"2022-05-02T12:57:48.325973Z","shell.execute_reply.started":"2022-05-02T12:45:43.049168Z","shell.execute_reply":"2022-05-02T12:57:48.32541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### InterclusterDistance \n\nÉ usado para termos de forma visual a separação de clusters por traz desse método é rodado um PCA com 2 componentes e por fim é plotado esses dois componentes em volta dos K clusters que definirmos","metadata":{}},{"cell_type":"code","source":"from yellowbrick.cluster import InterclusterDistance\nmodel = KMeans(n_clusters = 4 , random_state = 42)\nmodel.fit(train_kmeans)\n\n# Instantiate the clustering model and visualizer\nvisualizer = InterclusterDistance(model)\n\nvisualizer.fit(train_kmeans)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:57:48.327018Z","iopub.execute_input":"2022-05-02T12:57:48.327641Z","iopub.status.idle":"2022-05-02T12:57:50.115594Z","shell.execute_reply.started":"2022-05-02T12:57:48.327608Z","shell.execute_reply":"2022-05-02T12:57:50.114877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scoring\nx_train['cluster'] = model.predict(x_train.filter(train_kmeans.columns))\nx_test['cluster'] = model.predict(x_test.filter(train_kmeans.columns))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:57:50.117284Z","iopub.execute_input":"2022-05-02T12:57:50.117711Z","iopub.status.idle":"2022-05-02T12:57:50.150384Z","shell.execute_reply.started":"2022-05-02T12:57:50.117663Z","shell.execute_reply":"2022-05-02T12:57:50.149589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation curve to diagnostic bias and variance in hiperparameters\n\nPara implementarmos um modelo de classificação iremos investigar como a mudança de um parametro altera a performance do modelo, com isso iremos ter um range melhor para inserirmos nas buscas otimizadas. Além disso, essemétodo nos ajudará enxergar a relação vies e variancia de cada hiperparametro que iremos otimizar","metadata":{}},{"cell_type":"code","source":"def plot_validation_curve(x,y,modelo,parametro,param_range,metrica):\n\n    # Calculate accuracy on training and test set using range of parameter values\n    train_scores, test_scores = validation_curve(modelo, \n                                                 x, \n                                                 y, \n                                                 param_name=parametro, \n                                                 param_range=param_range,\n                                                 cv=3, \n                                                 scoring=metrica, \n                                                 n_jobs=-1)\n\n\n    # Calculate mean and standard deviation for training set scores\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n\n    # Calculate mean and standard deviation for test set scores\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    # Plot mean accuracy scores for training and test sets\n    plt.plot(param_range, train_mean, label=\"Training score\", color=\"red\")\n    plt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"blue\")\n\n    # Plot accurancy bands for training and test sets\n    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\n    plt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gray\")\n\n    # Create plot\n    plt.title(\"Validation Curve With Random Forest\")\n    plt.xlabel(\"Parameter\")\n    plt.ylabel(\"Roc Auc Score\")\n    plt.tight_layout()\n    plt.ylim(ymin=0.6)\n    plt.legend(loc=\"best\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:57:50.151631Z","iopub.execute_input":"2022-05-02T12:57:50.151863Z","iopub.status.idle":"2022-05-02T12:57:50.161447Z","shell.execute_reply.started":"2022-05-02T12:57:50.15183Z","shell.execute_reply":"2022-05-02T12:57:50.160865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### N estimators","metadata":{}},{"cell_type":"code","source":"plot_validation_curve(x = x_train,\n                      y = y_train.values.reshape(-1,),\n                      modelo = RandomForestClassifier(class_weight= 'balanced' ,max_depth=5),\n                      parametro = \"n_estimators\",\n                      param_range = np.arange(50, 300, 50),\n                      metrica = \"roc_auc\")","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:57:50.162386Z","iopub.execute_input":"2022-05-02T12:57:50.16327Z","iopub.status.idle":"2022-05-02T12:58:11.79904Z","shell.execute_reply.started":"2022-05-02T12:57:50.163221Z","shell.execute_reply":"2022-05-02T12:58:11.798071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Max_depth","metadata":{}},{"cell_type":"code","source":"plot_validation_curve(x = x_train,\n                      y = y_train.values.reshape(-1,),\n                      modelo = RandomForestClassifier(class_weight= 'balanced'),\n                      parametro = \"max_depth\",\n                      param_range = np.arange(3, 10, 1),\n                      metrica = \"roc_auc\")","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:58:11.800653Z","iopub.execute_input":"2022-05-02T12:58:11.80093Z","iopub.status.idle":"2022-05-02T12:58:31.217184Z","shell.execute_reply.started":"2022-05-02T12:58:11.800898Z","shell.execute_reply":"2022-05-02T12:58:31.216028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Min sample_split","metadata":{}},{"cell_type":"code","source":"plot_validation_curve(x = x_train,\n                      y = y_train.values.reshape(-1,),\n                      modelo = RandomForestClassifier(class_weight= 'balanced' ,max_depth=5),\n                      parametro = \"min_samples_split\",\n                      param_range =  np.arange(0, 100, 15),\n                      metrica = \"roc_auc\")","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:58:31.218235Z","iopub.execute_input":"2022-05-02T12:58:31.218426Z","iopub.status.idle":"2022-05-02T12:58:46.033901Z","shell.execute_reply.started":"2022-05-02T12:58:31.218403Z","shell.execute_reply":"2022-05-02T12:58:46.032998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Min samples leaf","metadata":{}},{"cell_type":"code","source":"plot_validation_curve(x = x_train,\n                      y = y_train.values.reshape(-1,),\n                      modelo = RandomForestClassifier(class_weight= 'balanced' ,max_depth=5),\n                      parametro = \"min_samples_leaf\",\n                      param_range = np.arange(10, 150, 25),\n                      metrica = \"roc_auc\")","metadata":{"execution":{"iopub.status.busy":"2022-05-02T12:58:46.03544Z","iopub.execute_input":"2022-05-02T12:58:46.03572Z","iopub.status.idle":"2022-05-02T12:59:00.632871Z","shell.execute_reply.started":"2022-05-02T12:58:46.035688Z","shell.execute_reply":"2022-05-02T12:59:00.631938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Hyperopt with Random forest exploring the best result of the validation curve output\n\n- Com base no learning curve é possvel dimensionar melhor a busca de hiperparametros no hyperopt ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']), 'max_depth': int(params['max_depth'])}\n    clf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', **params)\n    score = cross_val_score(clf, x_test, y_test, scoring='roc_auc', cv=StratifiedKFold()).mean()\n    print(\"ROC {:.3f} params {}\".format(score, params))\n    score = (1-score)\n    return score\n\nspace = {\n    'n_estimators': hp.quniform('n_estimators', 50, 500, 25),\n    'max_depth': hp.quniform('max_depth', 3, 8, 1),\n    \"max_features\": ['sqrt','log2'],\n    \"min_samples_split\" : hp.quniform('min_samples_split',10,200,10)\n}\n\nbest_rf = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:09:59.989474Z","iopub.execute_input":"2022-05-02T13:09:59.990658Z","iopub.status.idle":"2022-05-02T13:10:27.81924Z","shell.execute_reply.started":"2022-05-02T13:09:59.990594Z","shell.execute_reply":"2022-05-02T13:10:27.818262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing Smote Oversampling in a Random forest","metadata":{}},{"cell_type":"code","source":"from imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import StratifiedKFold\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']), 'max_depth': int(params['max_depth'])}\n    steps = [('over', SMOTE()), ('model', RandomForestClassifier(n_jobs=-1, **params))]\n    clf = Pipeline(steps=steps)\n    score = cross_val_score(clf, x_test, y_test, scoring='roc_auc', cv=StratifiedKFold()).mean()\n    print(\"ROC {:.3f} params {}\".format(score, params))\n    score = (1-score)\n    return score\n\nspace = {\n    'n_estimators': hp.quniform('n_estimators', 50, 1000, 10),\n    'max_depth': hp.quniform('max_depth', 3, 9, 1),\n    \"max_features\": ['sqrt','log2'],\n    \"min_samples_split\" : hp.quniform('min_samples_split',50,200,10)\n}\n\nbest_ = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5)\n\nprint('best parametes', best)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:00:43.643788Z","iopub.execute_input":"2022-05-02T13:00:43.644126Z","iopub.status.idle":"2022-05-02T13:02:19.606491Z","shell.execute_reply.started":"2022-05-02T13:00:43.64408Z","shell.execute_reply":"2022-05-02T13:02:19.605696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing a Boosting Algorithm (Xgboost with hiperparameter tuning hyperopt)","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nxgb.set_config(verbosity=0)\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import StratifiedKFold\ndef objective(params):\n    params = {'max_depth': int(params['max_depth'])\n              , 'gamma': int(params['gamma'])\n              , 'reg_alpha': int(params['reg_alpha'])\n              , 'reg_lambda': int(params['reg_lambda'])\n              , 'colsample_bytree': int(params['colsample_bytree'])\n              , 'min_child_weight': int(params['min_child_weight'])\n              , 'n_estimators': int(params['n_estimators'])\n             }\n\n    clf = xgb.XGBClassifier(n_jobs=-1,**params)\n    score = cross_val_score(clf,  x_test, y_test, scoring='roc_auc', cv=StratifiedKFold()).mean()\n    print(\"ROC {:.3f} params {}\".format(score, params))\n    score = (1-score)\n    return score\n\nspace={'max_depth': hp.quniform(\"max_depth\", 3, 8, 1),\n        'gamma': hp.uniform ('gamma', 0,0.5),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,100,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 20, 1),\n        'n_estimators': hp.quniform('n_estimators', 50, 500, 25),\n    }\n\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=5)\n\nprint('best parametes', best)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:07:29.633667Z","iopub.execute_input":"2022-05-02T13:07:29.634048Z","iopub.status.idle":"2022-05-02T13:07:53.461016Z","shell.execute_reply.started":"2022-05-02T13:07:29.634012Z","shell.execute_reply":"2022-05-02T13:07:53.459947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the best results from the Random forest","metadata":{}},{"cell_type":"code","source":"print(\"Start training the best model\")\nmodel = RandomForestClassifier(n_jobs=-1\n                                      , class_weight='balanced'\n                                      , max_depth = int(best_rf['max_depth'])\n                                      , min_samples_split = int(best_rf['min_samples_split'])\n                                      , n_estimators = int(best_rf['n_estimators'])).fit(x_train,y_train)\nprint(\"Scoring the train data\")\n#Scoring the best model in train dataset\npredict_train_entire = model.predict(x_train)\nproba_train_entire = model.predict_proba(x_train)[:,1]\n\nprint(\"Scoring the test data\")\n#Scoring the best model in test dataset\npredict_test_entire = model.predict(x_test)\nproba_test_entire = model.predict_proba(x_test)[:,1]\n\nprint(\"Getting metrics\")\n# calculate scores\nauc_train = roc_auc_score(y_train, proba_train_entire)\nauc_test = roc_auc_score(y_test, proba_test_entire )\n\n# calculate roc curves\nfpr_train, tpr_train, _ = roc_curve(y_train, proba_train_entire)\nfpr_test, tpr_test, _ = roc_curve(y_test, proba_test_entire)\n\n# plot the roc curve for the model\npyplot.plot(fpr_train, tpr_train, linestyle='--', label='Train')\npyplot.plot(fpr_test, tpr_test, linestyle='--', label='Test')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n# show the legend\npyplot.legend()\n# show the plot\npyplot.show()\n\n# summarize scores\nprint('ROC AUC Train =%.3f' % (auc_train))\nprint('ROC AUC Test =%.3f' % (auc_test))\n\nprint('-----------------------------------------------------')\n\nks_stat_train = binary_ks_curve(y_train, predict_train_entire)[3]\nprint('ks train =',ks_stat_train)\n\nks_stat_test = binary_ks_curve(y_test, predict_test_entire)[3]\nprint('ks test =',ks_stat_test)\n\nprint('-----------------------------------------------------')\n\nrecall_train = recall_score(y_train, predict_train_entire)\nprint('recall train =',recall_train)\n\nrecall_test = recall_score(y_test, predict_test_entire)\nprint('recall test =',recall_test)\n\nprint('-----------------------------------------------------')\nprint(\"------------------------\")\nprint(\"Calculate decil in train\")\nprint(\"------------------------\")\n\navg_tgt = y_train.sum()/len(y_train)\ndf_data = x_train.copy()\nX_data = df_data.copy()\ndf_data['Actual'] = y_train\ndf_data['Predict']= model.predict(X_data)\ny_Prob = pd.DataFrame(model.predict_proba(X_data))\ndf_data['Prob_1']=list(y_Prob[1])\ndf_data.sort_values(by=['Prob_1'],ascending=False,inplace=True)\ndf_data.reset_index(drop=True,inplace=True)\ndf_data['Decile']=pd.qcut(df_data.index,5,labels=False)\noutput_df = pd.DataFrame()\ngrouped = df_data.groupby('Decile',as_index=False)\noutput_df['Qtd']=grouped.count().Actual\noutput_df['Sum_Target']=grouped.sum().Actual\noutput_df['Per_Target'] = (output_df['Sum_Target']/output_df['Sum_Target'].sum())*100\noutput_df['Per_Acum_Target'] = output_df.Per_Target.cumsum()\noutput_df['Max_proba']=grouped.max().Prob_1\noutput_df['Min_proba']=grouped.min().Prob_1\noutput_df[\"Per_Pop\"] = (output_df[\"Qtd\"]/len(y_train))*100\noutput_df[\"Lift\"] = output_df[\"Per_Acum_Target\"]/output_df.Per_Pop.cumsum()\noutput_df= output_df.drop(columns='Per_Pop')\nprint(round(output_df,3))\n\nprint(\"------------------------\")\nprint(\"Calculate decil in test\")\nprint(\"------------------------\")\nAvg_tgt = y_test.sum()/len(y_test)\ndf_data = x_test.copy()\nX_data = df_data.copy()\ndf_data['Actual'] = y_test\ndf_data['Predict']= model.predict(X_data)\ny_Prob = pd.DataFrame(model.predict_proba(X_data))\ndf_data['Prob_1']=list(y_Prob[1])\ndf_data.sort_values(by=['Prob_1'],ascending=False,inplace=True)\ndf_data.reset_index(drop=True,inplace=True)\ndf_data['Decile']=pd.qcut(df_data.index,5,labels=False)\noutput_df = pd.DataFrame()\ngrouped = df_data.groupby('Decile',as_index=False)\noutput_df['Qtd']=grouped.count().Actual\noutput_df['Sum_Target']=grouped.sum().Actual\noutput_df['Per_Target'] = (output_df['Sum_Target']/output_df['Sum_Target'].sum())*100\noutput_df['Per_Acum_Target'] = output_df.Per_Target.cumsum()\noutput_df['Max_proba']=grouped.max().Prob_1\noutput_df['Min_proba']=grouped.min().Prob_1\noutput_df[\"Per_Pop\"] = (output_df[\"Qtd\"]/len(y_test))*100\noutput_df[\"Lift\"] = output_df[\"Per_Acum_Target\"]/output_df.Per_Pop.cumsum()\noutput_df= output_df.drop(columns='Per_Pop')\nprint(round(output_df,3))","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:10:35.598644Z","iopub.execute_input":"2022-05-02T13:10:35.598941Z","iopub.status.idle":"2022-05-02T13:10:40.010823Z","shell.execute_reply.started":"2022-05-02T13:10:35.598908Z","shell.execute_reply":"2022-05-02T13:10:40.009788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Showing the best discrimination threshould","metadata":{}},{"cell_type":"code","source":"from yellowbrick.classifier import discrimination_threshold\nvisualizer =  discrimination_threshold(model,X = x_train,y = y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:10:59.814523Z","iopub.execute_input":"2022-05-02T13:10:59.814849Z","iopub.status.idle":"2022-05-02T13:12:41.699044Z","shell.execute_reply.started":"2022-05-02T13:10:59.81481Z","shell.execute_reply":"2022-05-02T13:12:41.698157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Print the error of the model","metadata":{}},{"cell_type":"code","source":"from yellowbrick.classifier import ClassPredictionError\n\n# Instantiate the classification model and visualizer\nvisualizer = ClassPredictionError(\n    estimator = model\n)\n\n# Fit the training data to the visualizer\nvisualizer.fit(x_train, y_train['TARGET'])\n\n# Evaluate the model on the test data\nvisualizer.score(x_test, y_test['TARGET'])\n\n# Draw visualization\nvisualizer.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:12:41.700776Z","iopub.execute_input":"2022-05-02T13:12:41.701033Z","iopub.status.idle":"2022-05-02T13:12:42.373584Z","shell.execute_reply.started":"2022-05-02T13:12:41.701002Z","shell.execute_reply":"2022-05-02T13:12:42.372718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Showing the features importance based ina shape values","metadata":{}},{"cell_type":"code","source":"x_test_shap = x_test.sample(500)\nexplainer = shap.Explainer(model.predict, x_test_shap)\nshap_values = explainer(x_test_shap)\nshap.plots.bar(shap_values)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:22:20.997092Z","iopub.execute_input":"2022-05-02T13:22:20.997392Z","iopub.status.idle":"2022-05-02T13:22:51.217201Z","shell.execute_reply.started":"2022-05-02T13:22:20.997357Z","shell.execute_reply":"2022-05-02T13:22:51.216306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Summary plot","metadata":{}},{"cell_type":"code","source":"shap.summary_plot(shap_values)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T13:22:51.218888Z","iopub.execute_input":"2022-05-02T13:22:51.219115Z","iopub.status.idle":"2022-05-02T13:22:51.776146Z","shell.execute_reply.started":"2022-05-02T13:22:51.21909Z","shell.execute_reply":"2022-05-02T13:22:51.775169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Make a submission","metadata":{}},{"cell_type":"code","source":"# making predctions on the test dataset (df_test), from Kaggle, with the selected features and optimized parameters\ncolunas = x_test.columns\ndf_teste = pd.read_csv(\"../input/santander-customer-satisfaction/test.csv\").filter(colunas)\n\nproba_test = model.predict_proba(df_teste)[:,1]\n# saving the result into a csv file to be uploaded into Kaggle late subimission \n# https://www.kaggle.com/c/santander-customer-satisfaction/submit\nsub = pd.Series(proba_test, index = df_test['ID'], \nname = 'TARGET')\nsub.to_csv('data/df_test_predictions.csv')","metadata":{},"execution_count":null,"outputs":[]}]}