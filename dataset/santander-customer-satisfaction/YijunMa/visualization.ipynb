{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nsns.set(style=\"white\", color_codes=True)\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import gc\nimport xgboost as xgb\nfrom scipy.sparse import csr_matrix\nfrom sklearn.metrics import log_loss, roc_auc_score\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# read data\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Clean duplicated features\nremove = []\nc = train.columns\nfor i in range(len(c)-1):\n    v = train[c[i]].values\n    for j in range(i+1, len(c)):\n        if np.array_equal(v, train[c[j]].values):\n            remove.append(c[j])\n\ntrain.drop(remove, axis=1, inplace=True)\ntest.drop(remove, axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# remove features with 0 variance\nremove = []\nfor col in train.columns:\n    if train[col].std() == 0:\n        remove.append(col)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# common feature engineering on Kaggle forum\nfeatures = train.columns[1:-1]\ntrain.insert(1, 'SumZeros', (train[features] == 0).astype(int).sum(axis=1))\ntest.insert(1, 'SumZeros', (test[features] == 0).astype(int).sum(axis=1))\n\ntrain.drop(remove, axis=1, inplace=True)\ntest.drop(remove, axis=1, inplace=True)\nfeatures = train.columns[1:-1]\npca = PCA(n_components=2)\nx_train_projected = pca.fit_transform(normalize(train[features], axis=0))\nx_test_projected = pca.transform(normalize(test[features], axis=0))\ntrain.insert(1, 'PCAOne', x_train_projected[:, 0])\ntrain.insert(1, 'PCATwo', x_train_projected[:, 1])\ntest.insert(1, 'PCAOne', x_test_projected[:, 0])\ntest.insert(1, 'PCATwo', x_test_projected[:, 1])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# There are 309 features in total, we are definitely not going to visualize them all\n# How to find the important features?\nfeatures = train.columns[1:-1]\nfeatures"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# My way is to run a fast and dirty XGB, and see what are the most important contributors\n# The assumption is XGB will show underlying if then rules\n# Chi2 and other might works\n# but as far as I know, it cannot reveal if then relationship between features\n# Let me know if there is any other ways.\n\n# The following code is from:\n# https://www.kaggle.com/scirpus/santander-customer-satisfaction/python-xgb-lb-41047/discussion\n# Change a bit to make it faster\n\nsplit = 3 # 3 fold is faster than 10 fold\nskf = StratifiedKFold(train.TARGET.values,\n                      n_folds=split,\n                      shuffle=False,\n                      random_state=42)\n\ntrain_preds = None\ntest_preds = None\nvisibletrain = blindtrain = train\nindex = 0\nprint('Change num_rounds to 350')\nnum_rounds = 30\nparams = {}\nparams[\"objective\"] = \"binary:logistic\"\nparams[\"eta\"] = 0.3 # larger learning rate\nparams[\"subsample\"] = 0.2\nparams[\"colsample_bytree\"] = 0.7\nparams[\"silent\"] = 0\nparams[\"max_depth\"] = 5\nparams[\"min_child_weight\"] = 6\nparams[\"eval_metric\"] = \"auc\"\nparams[\"gamma\"] = 0\nfor train_index, test_index in skf:\n    visibletrain = train.iloc[train_index]\n    blindtrain = train.iloc[test_index]\n    dvisibletrain = \\\n    xgb.DMatrix(csr_matrix(visibletrain[features]),\n                visibletrain.TARGET.values,\n                silent=True)\n    dblindtrain = \\\n    xgb.DMatrix(csr_matrix(blindtrain[features]),\n                blindtrain.TARGET.values,\n                silent=True)\n    watchlist = [(dblindtrain, 'eval'), (dvisibletrain, 'train')]\n    clf = xgb.train(params, dvisibletrain, num_rounds,\n                    evals=watchlist, early_stopping_rounds=50,\n                    verbose_eval=False)\n\n    blind_preds = clf.predict(dblindtrain)\n    \n    index = index+1\n    del visibletrain\n    del blindtrain\n    del dvisibletrain\n    del dblindtrain\n    gc.collect()\n    dfulltrain = \\\n    xgb.DMatrix(csr_matrix(train[features]),\n                train.TARGET.values,\n                silent=True)\n    dfulltest = \\\n    xgb.DMatrix(csr_matrix(test[features]),\n                silent=True)\n    if(train_preds is None):\n        train_preds = clf.predict(dfulltrain)\n        test_preds = clf.predict(dfulltest)\n    else:\n        train_preds *= clf.predict(dfulltrain)\n        test_preds *= clf.predict(dfulltest)\n        del dfulltrain\n        del dfulltest\n        #  del clf # we need the clf to extract useful features\n        gc.collect()\n\n    train_preds = np.power(train_preds, 1./index)\n    test_preds = np.power(test_preds, 1./index)\n    print(\"\")\n    print(\"mean AUC: %s +/- %s;\" % (np.mean(AUCs),np.std(AUCs)))\n    print('Average ROC:', roc_auc_score(train.TARGET.values, train_preds))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Extract the useful features\n# Sort by the contribution to the XGB\na = pd.DataFrame([clf.get_fscore().keys(),clf.get_fscore().values()]).T\na['featureID'] = a[0].apply(lambda x: int(x[1:]))\na['Feature'] = a.featureID.apply(lambda x: features[x])\na.columns = ['x','Imp','y',\"feature\"]\na = a[['feature',\"Imp\"]].sort(\"Imp\",ascending = False)\na.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Let's plot the graph!\n# select two features as x and y, plot it as scatter plot\n# black is TARGET == 1; white is TARGET == 0\nsubset = train.copy().sort('TARGET',ascending = True) # rank TARGET == 1 to the bottom,\n                                                      # so it will plot at the front,\n                                                      # and won't be blocked by TARGET == 0\nk = 0\nfor i in range(len(a.feature.values)-1):\n    for j in range(i+1,len(a.feature.values)):\n        k += 1 # you may comment this in your local ipynb\n        if (k == 604): # you may comment this in your local ipynb\n            subset.plot(kind = 'scatter', x = a.feature.values[i], y = a.feature.values[j], c = subset.TARGET)\n            plt.title(str(k)+\": \" + a.feature.values[i] + \" -- \" +a.feature.values[j])\n            plt.show()\n        if (k > 604): # you may comment this in your local ipynb\n            break # you may comment this in your local ipynb"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Interesting to fin that num_var35 is negatively correlated to SumZeros\n# lots of other similar relationships among different features"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}