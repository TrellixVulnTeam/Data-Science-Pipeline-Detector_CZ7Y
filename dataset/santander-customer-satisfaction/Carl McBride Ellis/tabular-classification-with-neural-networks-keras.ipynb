{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular binary classification with neural networks: keras\nHere we create a fully-connected artificial neural network based on the kaggle [\"Intro to Deep Learning\"](https://www.kaggle.com/learn/intro-to-deep-learning) course written by [Ryan Holbrook](https://www.kaggle.com/ryanholbrook). We shall be using [Keras](https://keras.io/), the python deep learning API.\nFor our data we shall be using the [Santander Customer Satisfaction](https://www.kaggle.com/c/santander-customer-satisfaction) dataset.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_rows', None)\nimport numpy  as np\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:27.914898Z","iopub.execute_input":"2021-10-04T07:09:27.915517Z","iopub.status.idle":"2021-10-04T07:09:34.065926Z","shell.execute_reply.started":"2021-10-04T07:09:27.915401Z","shell.execute_reply":"2021-10-04T07:09:34.064658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in the csv data using pandas \ntrain  = pd.read_csv('../input/santander-customer-satisfaction/train.csv',index_col=0)\ntest   = pd.read_csv('../input/santander-customer-satisfaction/test.csv', index_col=0)\nsample = pd.read_csv('../input/santander-customer-satisfaction/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:34.067785Z","iopub.execute_input":"2021-10-04T07:09:34.068234Z","iopub.status.idle":"2021-10-04T07:09:40.01339Z","shell.execute_reply.started":"2021-10-04T07:09:34.068188Z","shell.execute_reply":"2021-10-04T07:09:40.012549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first thing we shall do is take a look at what types of data we have to train with. Categorical features can be handled via [embedding](https://www.fast.ai/2018/04/29/categorical-embeddings/).","metadata":{}},{"cell_type":"code","source":"train.dtypes.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:40.015388Z","iopub.execute_input":"2021-10-04T07:09:40.016009Z","iopub.status.idle":"2021-10-04T07:09:40.031196Z","shell.execute_reply.started":"2021-10-04T07:09:40.015966Z","shell.execute_reply":"2021-10-04T07:09:40.030054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we have no strings, so we shall not be using the aforementioned embedding. However, we do have a large number of integer columns. Let us take a look at how many different values each of these integer columns have (this is a long list, so it has been hidden. Click on \"Show hidden\" to take a look)","metadata":{}},{"cell_type":"code","source":"train.select_dtypes(include=['int64']).nunique()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-04T07:09:40.032707Z","iopub.execute_input":"2021-10-04T07:09:40.033156Z","iopub.status.idle":"2021-10-04T07:09:40.381729Z","shell.execute_reply.started":"2021-10-04T07:09:40.033123Z","shell.execute_reply":"2021-10-04T07:09:40.380679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see that a good many of the integer features have one single value. Such columns have zero variance and thus have no predictive value, so we shall drop these columns from the train, as well as the test data to maintain consistency","metadata":{}},{"cell_type":"code","source":"features_to_drop = train.nunique()\nfeatures_to_drop = features_to_drop.loc[features_to_drop.values==1].index\n# now drop these columns from both the training and the test datasets\ntrain = train.drop(features_to_drop,axis=1)\ntest  = test.drop(features_to_drop,axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:40.383138Z","iopub.execute_input":"2021-10-04T07:09:40.383718Z","iopub.status.idle":"2021-10-04T07:09:40.900444Z","shell.execute_reply.started":"2021-10-04T07:09:40.383672Z","shell.execute_reply":"2021-10-04T07:09:40.899409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us check for any missing values, as these do not sit well with neural networks","metadata":{}},{"cell_type":"code","source":"train.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:40.901748Z","iopub.execute_input":"2021-10-04T07:09:40.90205Z","iopub.status.idle":"2021-10-04T07:09:40.933597Z","shell.execute_reply.started":"2021-10-04T07:09:40.90202Z","shell.execute_reply":"2021-10-04T07:09:40.932613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"wonderful, there are no missing values at all. Now let us divide the training data into the features and the target","metadata":{}},{"cell_type":"code","source":"X = train.iloc[:,:-1]\ny = train['TARGET']","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:40.935148Z","iopub.execute_input":"2021-10-04T07:09:40.935614Z","iopub.status.idle":"2021-10-04T07:09:41.005327Z","shell.execute_reply.started":"2021-10-04T07:09:40.935569Z","shell.execute_reply":"2021-10-04T07:09:41.004122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check to see whether the data is [highly imbalanced or not](https://www.kaggle.com/carlmcbrideellis/classification-how-imbalanced-is-imbalanced)","metadata":{}},{"cell_type":"code","source":"y.value_counts().to_frame().T","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:41.00902Z","iopub.execute_input":"2021-10-04T07:09:41.009582Z","iopub.status.idle":"2021-10-04T07:09:41.031168Z","shell.execute_reply.started":"2021-10-04T07:09:41.009533Z","shell.execute_reply":"2021-10-04T07:09:41.030042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"indeed there is a large class imbalance, in view of this we shall resample the minority class using [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) (Synthetic Minority Over-sampling Technique) from the [imbalanced-learn](https://imbalanced-learn.org/stable/) library","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nX_resampled, y_resampled = SMOTE().fit_resample(X, y)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:41.033015Z","iopub.execute_input":"2021-10-04T07:09:41.033336Z","iopub.status.idle":"2021-10-04T07:09:47.451969Z","shell.execute_reply.started":"2021-10-04T07:09:41.033303Z","shell.execute_reply":"2021-10-04T07:09:47.451019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let us take a look","metadata":{}},{"cell_type":"code","source":"y_resampled.value_counts().to_frame().T","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:47.453125Z","iopub.execute_input":"2021-10-04T07:09:47.453576Z","iopub.status.idle":"2021-10-04T07:09:47.466864Z","shell.execute_reply.started":"2021-10-04T07:09:47.453545Z","shell.execute_reply":"2021-10-04T07:09:47.465737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"much better. \n\nAs this is a large dataset we shall only use 50% of the data for training, and 20% for validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, \n                                                  train_size=0.5,\n                                                  test_size=0.2, \n                                                  random_state=42, \n                                                  shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:47.468044Z","iopub.execute_input":"2021-10-04T07:09:47.468329Z","iopub.status.idle":"2021-10-04T07:09:47.813193Z","shell.execute_reply.started":"2021-10-04T07:09:47.468302Z","shell.execute_reply":"2021-10-04T07:09:47.812427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"also, neural networks like to have data all in the same range, for example [0,1] to improve stability. We shall do this using the [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\nNote: One should really remove any outliers before doing this as a single outlier can have a large influence on the results","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler  = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_val   = scaler.transform(X_val)\ntest    = scaler.transform(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:47.814202Z","iopub.execute_input":"2021-10-04T07:09:47.814678Z","iopub.status.idle":"2021-10-04T07:09:48.432203Z","shell.execute_reply.started":"2021-10-04T07:09:47.814643Z","shell.execute_reply":"2021-10-04T07:09:48.431343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are now ready to create our neural network, note that we insert a [dropout layer](https://keras.io/api/layers/regularization_layers/dropout/) as a form of regularization which will  help reduce overfitting by randomly setting (here 30%) of the input unit values to zero.","metadata":{}},{"cell_type":"code","source":"model = keras.Sequential(\n    [\n        keras.layers.Dense(units=9, activation=\"relu\", input_shape=(X_train.shape[-1],) ),\n        # randomly delete 30% of the input units below\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(units=9, activation=\"relu\"),\n        # the output layer, with a single neuron\n        keras.layers.Dense(units=1, activation=\"sigmoid\"),\n    ]\n)\n\n# save the initial weights for later\ninitial_weights = model.get_weights()","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:48.433298Z","iopub.execute_input":"2021-10-04T07:09:48.433716Z","iopub.status.idle":"2021-10-04T07:09:48.543994Z","shell.execute_reply.started":"2021-10-04T07:09:48.433685Z","shell.execute_reply":"2021-10-04T07:09:48.542793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let us take a look at our model","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:48.545175Z","iopub.execute_input":"2021-10-04T07:09:48.545484Z","iopub.status.idle":"2021-10-04T07:09:48.555047Z","shell.execute_reply.started":"2021-10-04T07:09:48.545456Z","shell.execute_reply":"2021-10-04T07:09:48.55272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"or create a more aesthetically pleasing representation via","metadata":{}},{"cell_type":"code","source":"keras.utils.plot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:48.556475Z","iopub.execute_input":"2021-10-04T07:09:48.556829Z","iopub.status.idle":"2021-10-04T07:09:49.007912Z","shell.execute_reply.started":"2021-10-04T07:09:48.556798Z","shell.execute_reply":"2021-10-04T07:09:49.006689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We shall use the [Adam](https://keras.io/api/optimizers/adam/) (Adaptive Moment Estimation) optimizer, a form of [Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD). For more on the subject see the review [\"*An overview of gradient descent optimization algorithms*\"](https://arxiv.org/pdf/1609.04747.pdf), written by Sebastian Ruder.\nAs our problem is binary classification our [loss function](https://en.wikipedia.org/wiki/Loss_function) will be the [binary cross entropy](https://keras.io/api/losses/probabilistic_losses/). Finally, as per the [competition evaluation criteria](https://www.kaggle.com/c/santander-customer-satisfaction/overview/evaluation), we shall calculate the area under the curve (AUC) of the [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).","metadata":{}},{"cell_type":"code","source":"learning_rate = 0.001\n\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n              loss=\"binary_crossentropy\", \n              metrics=keras.metrics.AUC()\n             )","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:49.009637Z","iopub.execute_input":"2021-10-04T07:09:49.009988Z","iopub.status.idle":"2021-10-04T07:09:49.0412Z","shell.execute_reply.started":"2021-10-04T07:09:49.009955Z","shell.execute_reply":"2021-10-04T07:09:49.040057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we shall now train the model, providing 1000 rows of training data at a  time (`batch_size`), the whole process repeated 500 times (these are the `epochs`).\nIf the `batch_size` is  large, it will take more `epochs` for the neural net to converge. For an interesting article on the subject see [\"*Effect of Batch Size on Neural Net Training*\"](https://medium.com/deep-learning-experiments/effect-of-batch-size-on-neural-net-training-c5ae8516e57).","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_train, y_train, \n          epochs=500, \n          batch_size=1000, \n          validation_data=(X_val, y_val),\n          verbose=0)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:09:49.042548Z","iopub.execute_input":"2021-10-04T07:09:49.042863Z","iopub.status.idle":"2021-10-04T07:13:40.838805Z","shell.execute_reply.started":"2021-10-04T07:09:49.042832Z","shell.execute_reply":"2021-10-04T07:13:40.837847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Take a look at how the training went, checking for either [overfitting or underfitting](https://www.kaggle.com/ryanholbrook/overfitting-and-underfitting), by plotting the so-called *learning curves*","metadata":{}},{"cell_type":"code","source":"logs = pd.DataFrame(history.history)\n\nplt.figure(figsize=(14, 4))\nplt.subplot(1, 2, 1)\nplt.plot(logs.loc[5:,\"loss\"], lw=2, label='training loss')\nplt.plot(logs.loc[5:,\"val_loss\"], lw=2, label='validation loss')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(logs.loc[5:,\"auc\"], lw=2, label='training ROC AUC score')\nplt.plot(logs.loc[5:,\"val_auc\"], lw=2, label='validation ROC AUC score')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"ROC AUC\")\nplt.legend(loc='lower right')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-04T07:15:04.000577Z","iopub.execute_input":"2021-10-04T07:15:04.001112Z","iopub.status.idle":"2021-10-04T07:15:04.326509Z","shell.execute_reply.started":"2021-10-04T07:15:04.00108Z","shell.execute_reply":"2021-10-04T07:15:04.325536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we can see that towards the end the validation ROC AUC score has leveled off and is not improving. We can also see that despite the validation ROC AUC score stagnating, the training ROC AUC score is still steadily rising; this is an indication that we are overfitting. Ideally we would like to watch out for this and call a halt to the training so as not to waste time and CPU/GPU. To do this we can set up a \"callback\":","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    min_delta = 0.0002, # minimium amount of change to count as an improvement\n    patience  = 20,     # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:13:41.20075Z","iopub.execute_input":"2021-10-04T07:13:41.201171Z","iopub.status.idle":"2021-10-04T07:13:41.205935Z","shell.execute_reply.started":"2021-10-04T07:13:41.201128Z","shell.execute_reply":"2021-10-04T07:13:41.205193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us delete our old training and start anew. We do this by restoring the initial weights of our neural network:","metadata":{}},{"cell_type":"code","source":"model.set_weights(initial_weights)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:13:41.207099Z","iopub.execute_input":"2021-10-04T07:13:41.207589Z","iopub.status.idle":"2021-10-04T07:13:41.221811Z","shell.execute_reply.started":"2021-10-04T07:13:41.207557Z","shell.execute_reply":"2021-10-04T07:13:41.220728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train, \n          epochs=500, \n          batch_size=1000, \n          validation_data=(X_val, y_val),\n          verbose=0,\n          # add in our early stopping callback\n          callbacks=[early_stopping]\n        )","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:13:41.223149Z","iopub.execute_input":"2021-10-04T07:13:41.223445Z","iopub.status.idle":"2021-10-04T07:14:37.582095Z","shell.execute_reply.started":"2021-10-04T07:13:41.223418Z","shell.execute_reply":"2021-10-04T07:14:37.580975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logs = pd.DataFrame(history.history)\n\nplt.figure(figsize=(14, 4))\nplt.subplot(1, 2, 1)\nplt.plot(logs.loc[5:,\"loss\"], lw=2, label='training loss')\nplt.plot(logs.loc[5:,\"val_loss\"], lw=2, label='validation loss')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.subplot(1, 2, 2)\nplt.plot(logs.loc[5:,\"auc\"], lw=2, label='training ROC AUC score')\nplt.plot(logs.loc[5:,\"val_auc\"], lw=2, label='validation ROC AUC score')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"ROC AUC\")\nplt.legend(loc='lower right')\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-10-04T07:14:37.583655Z","iopub.execute_input":"2021-10-04T07:14:37.584071Z","iopub.status.idle":"2021-10-04T07:14:37.913907Z","shell.execute_reply.started":"2021-10-04T07:14:37.584029Z","shell.execute_reply":"2021-10-04T07:14:37.911623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we have now finished training our neural network. Now let us use our model on the `test` data to predict the values, when it comes to neural networks the prediction stage is more commonly referred to  as *inference*:","metadata":{}},{"cell_type":"code","source":"sample['TARGET'] = model.predict(test)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:14:37.916868Z","iopub.execute_input":"2021-10-04T07:14:37.917172Z","iopub.status.idle":"2021-10-04T07:14:39.65093Z","shell.execute_reply.started":"2021-10-04T07:14:37.917144Z","shell.execute_reply":"2021-10-04T07:14:39.649731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now write out a `submission.csv` file for submission to the competition for scoring. The scores for this model are given at the top of this notebook. As a reference point the winning score for this competition had a Private Leaderboard score of `0.82907`.","metadata":{}},{"cell_type":"code","source":"sample.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-04T07:14:39.652751Z","iopub.execute_input":"2021-10-04T07:14:39.65316Z","iopub.status.idle":"2021-10-04T07:14:39.901978Z","shell.execute_reply.started":"2021-10-04T07:14:39.653124Z","shell.execute_reply":"2021-10-04T07:14:39.901102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Related reading\nIt is well known that, when it comes to tabular data, neural networks have a difficult time competing with the likes of [XGBoost](https://www.kaggle.com/carlmcbrideellis/very-simple-xgboost-regression). However, progress is constantly being made in that respect, and the following papers make for some very interesting reading\n* [\"*Neural Oblivious Decision Ensembles for Deep Learning on Tabular Data*\"](https://arxiv.org/pdf/1909.06312.pdf) (2019)\n* [\"*TabNet: Attentive Interpretable Tabular Learning*\"](https://arxiv.org/pdf/1908.07442.pdf) (2020)\n* [\"*Tabular Data: Deep Learning is Not All You Need*\"](https://arxiv.org/pdf/2106.03253.pdf) (2021)\n* [\"*Regularization is all you Need: Simple Neural Nets can Excel on Tabular Data*\"](https://arxiv.org/pdf/2106.11189.pdf) (2021)\n* [\"*Deep Neural Networks and Tabular Data: A Survey*\"](https://arxiv.org/pdf/2110.01889.pdf) (2021)","metadata":{}}]}