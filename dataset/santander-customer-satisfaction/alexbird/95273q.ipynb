{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport itertools\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\ntraining = pd.read_csv(\"../input/train.csv\", index_col=0)\ntest = pd.read_csv(\"../input/test.csv\", index_col=0)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\ndef principal_component_analysis(x_train):\n\n    \"\"\"\n    Principal Component Analysis (PCA) identifies the combination\n    of attributes (principal components, or directions in the feature space)\n    that account for the most variance in the data.\n\n    Let's calculate the 2 first principal components of the training data,\n    and then create a scatter plot visualizing the training data examples\n    projected on the calculated components.\n    \"\"\"\n\n    # Extract the variable to be predicted\n    y_train = x_train[\"TARGET\"]\n    x_train = x_train.drop(labels=\"TARGET\", axis=1)\n    classes = np.sort(np.unique(y_train))\n    labels = [\"Satisfied customer\", \"Unsatisfied customer\"]\n\n    # Normalize each feature to unit norm (vector length)\n    x_train_normalized = normalize(x_train, axis=0)\n    \n    # Run PCA\n    pca = PCA(n_components=2)\n    x_train_projected = pca.fit_transform(x_train_normalized)\n\n    # Visualize\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(1, 1, 1)\n    colors = [(0.0, 0.63, 0.69), 'black']\n    markers = [\"o\", \"D\"]\n    for class_ix, marker, color, label in zip(\n            classes, markers, colors, labels):\n        ax.scatter(x_train_projected[np.where(y_train == class_ix), 0],\n                   x_train_projected[np.where(y_train == class_ix), 1],\n                   marker=marker, color=color, edgecolor='whitesmoke',\n                   linewidth='1', alpha=0.9, label=label)\n        ax.legend(loc='best')\n    plt.title(\n        \"Scatter plot of the training data examples projected on the \"\n        \"2 first principal components\")\n    plt.xlabel(\"Principal axis 1 - Explains %.1f %% of the variance\" % (\n        pca.explained_variance_ratio_[0] * 100.0))\n    plt.ylabel(\"Principal axis 2 - Explains %.1f %% of the variance\" % (\n        pca.explained_variance_ratio_[1] * 100.0))\n    plt.show()\n\n    plt.savefig(\"pca.pdf\", format='pdf')\n    plt.savefig(\"pca.png\", format='png')\n\n\ndef remove_feat_constants(data_frame):\n    # Remove feature vectors containing one unique value,\n    # because such features do not have predictive value.\n    print(\"\")\n    print(\"Deleting zero variance features...\")\n    # Let's get the zero variance features by fitting VarianceThreshold\n    # selector to the data, but let's not transform the data with\n    # the selector because it will also transform our Pandas data frame into\n    # NumPy array and we would like to keep the Pandas data frame. Therefore,\n    # let's delete the zero variance features manually.\n    n_features_originally = data_frame.shape[1]\n    selector = VarianceThreshold()\n    selector.fit(data_frame)\n    # Get the indices of zero variance feats\n    feat_ix_keep = selector.get_support(indices=True)\n    orig_feat_ix = np.arange(data_frame.columns.size)\n    feat_ix_delete = np.delete(orig_feat_ix, feat_ix_keep)\n    # Delete zero variance feats from the original pandas data frame\n    data_frame = data_frame.drop(labels=data_frame.columns[feat_ix_delete],\n                                 axis=1)\n    # Print info\n    n_features_deleted = feat_ix_delete.size\n    print(\"  - Deleted %s / %s features (~= %.1f %%)\" % (\n        n_features_deleted, n_features_originally,\n        100.0 * (np.float(n_features_deleted) / n_features_originally)))\n    return data_frame\n\n\ndef remove_feat_identicals(data_frame):\n    # Find feature vectors having the same values in the same order and\n    # remove all but one of those redundant features.\n    print(\"\")\n    print(\"Deleting identical features...\")\n    n_features_originally = data_frame.shape[1]\n    # Find the names of identical features by going through all the\n    # combinations of features (each pair is compared only once).\n    feat_names_delete = []\n    for feat_1, feat_2 in itertools.combinations(\n            iterable=data_frame.columns, r=2):\n        if np.array_equal(data_frame[feat_1], data_frame[feat_2]):\n            feat_names_delete.append(feat_2)\n    feat_names_delete = np.unique(feat_names_delete)\n    # Delete the identical features\n    data_frame = data_frame.drop(labels=feat_names_delete, axis=1)\n    n_features_deleted = len(feat_names_delete)\n    print(\"  - Deleted %s / %s features (~= %.1f %%)\" % (\n        n_features_deleted, n_features_originally,\n        100.0 * (np.float(n_features_deleted) / n_features_originally)))\n    return data_frame"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print(training.shape)\nprint(test.shape)\n#print(dir(training))\n\n\nprint(training.info())\nremove = []\nfor col in training.columns:\n    if training[col].std() == 0:\n        remove.append(col)\ntraining.drop(remove, axis=1, inplace=True)\nprint(training.info())"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"X = training.iloc[:, :-1]\ny = training.TARGET\n\ny.value_counts() / float(y.size)\ntraining['TARGET'].std()\n\nif __name__ == \"__main__\":\n    x_train = pd.read_csv(filepath_or_buffer=\"../input/train.csv\",\n                          index_col=0, sep=',')\n    x_train = remove_feat_constants(x_train)\n    x_train = remove_feat_identicals(x_train)\n    principal_component_analysis(x_train)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}