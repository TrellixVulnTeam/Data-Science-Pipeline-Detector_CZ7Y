{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 만족하지 못한 고객은 1이고 만족한 고객은 0<br>\n## 고객이 만족스럽지 않은 고객일 확률을 예측","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# seaborn 라이브러리 세팅\nplt.style.use('seaborn')   # matplot 기본 그림 말고 seaborn 그림 스타일 사용\nsns.set(font_scale=2.5)    # 폰트 사이즈 2.5로 고정\n\n# null 데이터를 시각화하여 보여주는 라이브러리\nimport missingno as msno   \n\n# 오류 무시하는 코드 \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplot 라이브러리 사용해 시각화한 뒤 show했을 때 새로운 창이 아닌 노트북에서 바로 확인 가능하도록\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. 데이터 셋 확인","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/santander-customer-satisfaction/train.csv')\ntest = pd.read_csv('../input/santander-customer-satisfaction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()\n\n# dtypes를 통해 모든 피처가 숫자형임을 알 수 있음\n# null값 없음","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()\n# var3열의 min -9999는 NaN이나 특정 예외 값을 -9999로 변환한 것 같음","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### var3값 확인","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.var3.value_counts()[:10])\n# -999999값이 116개 존재\n# var3은 숫자형이고, 다른 값에 비해 편차가 심하므로 가장 값이 많은 2로 변환","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### var3의 -999999를 가장 많은 값인 2로 변환","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train['var3'].replace(-999999, 2, inplace=True)\ntest['var3'].replace(-999999, 2, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### ID피처는 불필요할 것 같아 드롭","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('ID',axis=1 , inplace=True)\ntest.drop('ID',axis=1 , inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. 타겟 레이블 확인","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 타겟 레이블의 분포도 확인\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 이진 분류인 경우 타겟값의 분류가 불균형한지 아닌지 확인해야 함\nprint(train['TARGET'].value_counts())  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 불만족인 데이터 건수\nunsatisfied= train[train['TARGET'] == 1].TARGET.count()\nprint(unsatisfied)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 전체 데이터 건수\ntotal= train.TARGET.count()\nprint(total)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 불만족 데이터와 전체 데이터 건수의 비율 확인\nprint(unsatisfied / total)\n# 불만족 비율 4%","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. 모델링 및 평가\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 학습 데이터 셋과 테스트 데이터 셋으로 분리","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('TARGET', axis=1)  # TARGET을 제외한 피처들\nY = train['TARGET']   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 비대칭한 데이터이므로 타겟값이 학습과 검증 데이터에 골고루 분포되었는지 확인","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_count = y_train.count()\nprint(y_train.value_counts()/train_count)\n# 타겟값 불만족이 원본 데이터와 유사하게 4%유지","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_count = y_val.count()\nprint(y_val.value_counts()/val_count)\n# 타겟값 불만족이 원본 데이터와 유사하게 4%유지","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.1 결정트리","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dt = DecisionTreeClassifier(random_state=11)\n\n# 학습\ndt.fit(X_train , y_train)\n\n# 예측\ndt_pred = dt.predict(X_val)\n\n# 평가\ndt_roc = roc_auc_score(y_val, dt_pred)\nprint('ROC AUC: {0:.4f}'.format(dt_roc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GridSearchCV로 하이퍼 파라미터 튜닝","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 하이퍼 파라미터 설정\nparameters = {'max_depth':[2,3,5,10],'min_samples_split':[2,3,5], 'min_samples_leaf':[1,5,8]} \n\n# 하이퍼 파라미터를 5개의 train, val로 나누어 테스트 수행 설정\ngrid_dt = GridSearchCV(dt, param_grid = parameters, scoring = 'accuracy', cv=5, verbose=1 , refit = True)  #  verbose: 얼마나 자세히 정보를 표시할 것인가 0,1,2로 나눠짐\n\n# 튜닝된 하이퍼 파라미터로 학습\ngrid_dt.fit(X_train, y_train)\n\n# 최고 성능을 낸 하이퍼 파라미터 값과 그때의 평가값 저장\nprint('GridSearchCV 최적 하이퍼 파라미터:', grid_dt.best_params_)       # 최적 하이퍼 파라미터\nprint('GridSearchCV 최고 정확도:{0:.4f}'.format(grid_dt.best_score_))  # 최적 하이퍼 파라미터일 때 정확도","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GridSearchCV로 얻은 최적 하이퍼 파리미터를 적용하여 학습/예측/평가 재수행","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# refit = True로 최적 하이퍼 파라미터 미리 학습하여 best_estimator_로 저장됨(별도로 fit할 필요없음)\ndt1= grid_dt.best_estimator_   \n\n# 재예측\ndt1_pred = dt1.predict(X_val)   \n\n# 재평가\ndt1_roc = roc_auc_score(y_val , dt1_pred)\nprint('ROC AUC:{0:.4f}'.format(dt1_roc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.2 랜덤포레스트","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(random_state=11)\n\n# 학습\nrf.fit(X_train , y_train)\n\n# 예측\nrf_pred = rf.predict(X_val)\n\n# 평가\nrf_roc = roc_auc_score(y_val ,rf_pred)\nprint('AUC_ROC: {0:.4f}'.format(rf_roc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GridSearchCV로 하이퍼 파라미터 튜닝\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 하이퍼 파라미터 설정\nparameters = {'n_estimators':[10], 'max_depth' : [6, 8, 10, 12], 'min_samples_leaf' : [8, 12, 18 ],'min_samples_split' : [8, 16, 20]}\n\n# 하이퍼 파라미터를 2개의 train, val로 나누어 테스트 수행 설정\ngrid_rf = GridSearchCV(rf, param_grid = parameters , cv=2)     # 이번에는 refit 안해봄\n\n# 튜닝된 하이퍼 파라미터로 학습\ngrid_rf.fit(X_train, y_train)\n\n# 최고 성능을 낸 하이퍼 파라미터 값과 그때의 평가값 저장\nprint('AUC_ROC:\\n', grid_rf.best_params_)              # 최적 하이퍼 파라미터\nprint('AUC_ROC: {0:.4f}'.format(grid_rf.best_score_))  # 최적 하이퍼 파라미터일 때 정확도","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GridSearchCV로 얻은 최적 하이퍼 파리미터를 적용하여 학습/예측/평가 재수행","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 최적 하이퍼 파라미터 적용\nrf1 = RandomForestClassifier(n_estimators=10, max_depth=6, min_samples_leaf=8, min_samples_split=8, random_state=0)\n\n# 재학습\nrf1.fit(X_train , y_train)    # refit 안했으므로 fit도 수행\n\n# 재예측\nrf1_pred = rf1.predict(X_val)\n\n# 재평가\nprint(roc_auc_score(y_val , rf1_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.3 XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(n_estimators=500, random_state=156)\n\n# 평가 데이터 세트는 앞에서 분리한 테스트 데이터 세트 이용 -> 1이 4%밖에 없어서 테스트 세트 이용해 검증  \nevals = [(X_train, y_train), (X_val, y_val)]\n\n# 학습\nxgb.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=evals, verbose=0)\n                                                       \n# 예측 및 평가\nxgb_roc_score = roc_auc_score(y_val, xgb.predict_proba(X_val)[:,1],average='macro')\nprint('ROC AUC: {0:.4f}'.format(xgb_roc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GridSearchCV로 하이퍼 파라미터 튜닝","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 수행시간 조절을 위해 100으로 줄임\nxgb = XGBClassifier(n_estimators=100)\n\n# 하이퍼 파라미터 설정\nparameters = {'max_depth':[5, 7] , 'min_child_weight':[1,3] ,'colsample_bytree':[0.5, 0.75] }    # 칼럼을 샘플링해서 적용(칼럼이 많으므로 조절)\n\n# 평가 데이터 세트는 앞에서 분리한 테스트 데이터 세트 이용 -> 1이 4%밖에 없어서 테스트 세트 이용해 검증  \nevals = [(X_train, y_train), (X_val, y_val)]\n\n# 하이퍼 파라미터의 수행속도를 향상시키기 위해 cv설정 안함\ngrid_xgb = GridSearchCV(xgb, param_grid=parameters)\n\n# 튜닝된 하이퍼 파라미터로 학습\ngrid_xgb.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=\"auc\", eval_set=evals, verbose =0)\n\n# 예측 및 평가\nxgb_roc_score = roc_auc_score(y_val, grid_xgb.predict_proba(X_val)[:,1], average='macro')\nprint('GridSearchCV 최적 파라미터:',grid_xgb.best_params_) \nprint('ROC AUC: {0:.4f}'.format(xgb_roc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GridSearchCV로 얻은 최적 하이퍼 파리미터를 적용하여 학습/예측/평가 재수행","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# n_estimators는 1000으로 증가시키고, learning_rate=0.02로 감소, reg_alpha=0.03으로 추가\nxgb1 = XGBClassifier(n_estimators=1000, random_state=156, learning_rate=0.02, max_depth=5,min_child_weight=3, colsample_bytree=0.5, reg_alpha=0.03)\n\n# 재학습\nxgb1.fit(X_train, y_train, early_stopping_rounds=200, eval_metric=\"auc\",eval_set=[(X_train, y_train), (X_val, y_val)],  verbose =0)\n\n# 재예측 및 재평가                                                                                                                        \nxgb1_roc_score = roc_auc_score(y_val, xgb1.predict_proba(X_val)[:,1],average='macro')\nprint('ROC AUC: {0:.4f}'.format(xgb1_roc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### feaure_importances_사용하여 피처들의 중요도 확인","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import plot_importance\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig, ax = plt.subplots(1,1,figsize=(10,8))\nplot_importance(xgb1, ax=ax , max_num_features=20,height=0.4)\n\n# 칼럼이름이 나오는 이유는 numpy가 아닌 dataframe으로 했기 때문","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3.4 LightGBM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm = LGBMClassifier(n_estimators=500)\n\nevals = [(X_val, y_val)]\n\n# 학습\nlgbm.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=evals, verbose=0)\n\n# 예측 및 평가\nlgbm_roc_score = roc_auc_score(y_val, lgbm.predict_proba(X_val)[:,1],average='macro')\nprint('ROC AUC: {0:.4f}'.format(lgbm_roc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GridSearchCV로 하이퍼 파라미터 튜닝","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# 하이퍼 파라미터 테스트의 수행 속도를 향상시키기 위해 n_estimators를 100으로 감소\nlgbm = LGBMClassifier(n_estimators=200)\n\nparameters = {'num_leaves': [32, 64 ],'max_depth':[128, 160],'min_child_samples':[60, 100],'subsample':[0.8, 1]}\n\n\n# 수행속도위해 cv저장 안함\ngrid_lgbm = GridSearchCV(lgbm, param_grid=parameters)\n\n# 학습\ngrid_lgbm.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=\"auc\", eval_set=[(X_train, y_train), (X_val, y_val)],verbose=0)\n\n# 예측 및 평가\nlgbm_roc_score = roc_auc_score(y_val,grid_lgbm.predict_proba(X_val)[:,1],average='macro')\n\nprint('GridSearchCV 최적 파라미터:', grid_lgbm.best_params_)\nprint('ROC AUC: {0:.4f}'.format(lgbm_roc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### GridSearchCV로 얻은 최적 하이퍼 파리미터를 적용하여 학습/예측/평가 재수행","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"lgbm1 = LGBMClassifier(n_estimators=1000, num_leaves=32, sumbsample=0.8, min_child_samples=100,max_depth=128)\n\nevals = [(X_val, y_val)]\n\nlgbm1.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=evals, verbose=0)\n\nlgbm1_roc_score = roc_auc_score(y_val, lgbm1.predict_proba(X_val)[:,1],average='macro')\nprint('ROC AUC: {0:.4f}'.format(lgbm1_roc_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. 결과 제출","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/santander-customer-satisfaction/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = xgb1.predict(test)  # 실제 예측","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission['TARGET'] = prediction  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv('submission.csv', index = False)  # 캐글 커널 서버에 csv파일 저장","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}