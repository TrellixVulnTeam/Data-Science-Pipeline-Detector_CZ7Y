{"cells":[{"metadata":{"_uuid":"9c9e48e4634de684f2c8e897becddf6af888030a"},"cell_type":"markdown","source":"# Constant Features\nConstant features are those that show the same value, just one value, for all the observations of the dataset. This is, the same value for all the rows of the dataset. These features provide no information that allows a machine learning model to discriminate or predict a target. Identifying and removing constant features. \nTo identify constant features, we can use the VarianceThreshold function from sklearn."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport collections\nimport os\nprint(os.listdir(\"../input\"))\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import roc_auc_score,classification_report,confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier","execution_count":196,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e141843d928c8e2048475207d67f2796987009e1","collapsed":true},"cell_type":"code","source":"#Load the train dataset. It contain more then 76000 records. Lets load 10000 records only to make things fast.\ndf=pd.read_csv('../input/santander-customer-satisfaction/train.csv',nrows=10000)\ndf.shape","execution_count":81,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f071b9983a83f06375033dc983eff7811c06c33","collapsed":true},"cell_type":"code","source":"df.info()","execution_count":82,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"447b6596b40539c2c66e2629194c51ff4953d554","collapsed":true},"cell_type":"code","source":"#Find the missing data in the columns\n[col for col in df.columns if df[col].isnull().sum()>0]\n#After executing the above code we found that there is no missing data.","execution_count":83,"outputs":[]},{"metadata":{"_uuid":"2eea72a67ec093dbe997ddaebc1793b3f507d3fc"},"cell_type":"markdown","source":"**It is good practice to select the features by examining only the training set to avoid overfitting.**"},{"metadata":{"trusted":true,"_uuid":"1b9085c7112bb1d575e6143efc9f0d0497682bb5","collapsed":true},"cell_type":"code","source":"# separate dataset into train and test\nX_train, X_test, Y_train, Y_test = train_test_split(df.drop(labels=['TARGET'], axis=1),df['TARGET'],test_size=0.3,random_state=0)\n#Shape of training set and test set.\nX_train.shape, X_test.shape\n\n# I keep a copy of the dataset with all the variables\n# to measure the performance of machine learning models\n# at the end of the notebook\nX_train_org=X_train.copy()\nX_test_org=X_test.copy()","execution_count":124,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f0aad48bb71ea594676c81a7af1d53b0abaa750a"},"cell_type":"markdown","source":"**Using Variance Threshold**<br>\nVariance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples."},{"metadata":{"trusted":true,"_uuid":"7fd4683b32f37821ceadebeb2f5b5adf79f0f4a3","collapsed":true},"cell_type":"code","source":"varModel=VarianceThreshold(threshold=0) #Setting variance threshold to 0 which means features that have same value in all samples.\nvarModel.fit(X_train)","execution_count":125,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"49fb4d565bee3a47df23a7fccbe245f907d86ea3","collapsed":true},"cell_type":"code","source":"constArr=varModel.get_support()\nconstArr\n#get_support() return True and False value for each feature.\n#True: Not a constant feature\n#False: Constant feature(It contains same value in all samples.)","execution_count":126,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74007db68bc18ba837fb80e41c6bb835c0d552b4","collapsed":true},"cell_type":"code","source":"#To find total number of constant and non constant features we will be using collections.Counter function.\ncollections.Counter(constArr)\n#Non Constant feature:284\n#Constant feature: 86","execution_count":127,"outputs":[]},{"metadata":{"_uuid":"8ff3053abfb4cc71a3e7b4cc9084fd73623e44e5"},"cell_type":"markdown","source":"We can see there are 86  features/columns having constant value. This mean they have same value in all samples. Lets proof that, by selecting some of the constant features and print out value counts. We can also use unique method."},{"metadata":{"trusted":true,"_uuid":"407ed2d4e0b98e017e53cad1eb1c21e90270bd22","collapsed":true},"cell_type":"code","source":"#Print out constant feature name\nconstCol=[col for col in X_train.columns if col not in X_train.columns[constArr]]\nconstCol","execution_count":128,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f4dcb028724eee2e04c4e7774432491f5ef4507","collapsed":true},"cell_type":"code","source":"print(X_train['ind_var2_0'].value_counts())\nprint(X_train['ind_var13_medio'].value_counts())\nprint(X_train['ind_var27'].value_counts())","execution_count":129,"outputs":[]},{"metadata":{"_uuid":"dbc8e0a716b6deb2ea2ff39c009774f414a11991"},"cell_type":"markdown","source":"Constant features do not play  any role in predicting the result. So we will remove it from our training set and test set. Transform will remove all the constant columns from training set and test set but we will not use it because it will transform a dataframe to numpy array. We are going to use same training set and test set for other feasture selection as well. So will drop the constant columns from tables."},{"metadata":{"trusted":true,"_uuid":"840b96003cf0f1c7c41573697757d83b6a2f9d4d","collapsed":true},"cell_type":"code","source":"print('Shape before drop-->',X_train.shape, X_test.shape)\n#X_train=varModel.transform(X_train)\n#X_test=varModel.transform(X_test)\nX_train.drop(columns=constCol,axis=1,inplace=True)\nX_test.drop(columns=constCol,axis=1,inplace=True)\nprint('Shape after drop-->',X_train.shape, X_test.shape)","execution_count":130,"outputs":[]},{"metadata":{"_uuid":"35f1d2e8b3bd6a3054e4e23e80fbc2a20c5208e6"},"cell_type":"markdown","source":"# Quasi-Constant Features \nQuasi-constant features are those that show the same value for the great majority of the observations of the dataset. Mostly we do not consider these features in prediting the result.\nTo identify Quasi constant features, we can use the VarianceThreshold function from sklearn. We will be using the same training set and test set."},{"metadata":{"_uuid":"6a76d27ae5b4fa08f11ad30a10a2e9475c003426"},"cell_type":"markdown","source":"**Using Variance Threshold**<br>\nVariance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples."},{"metadata":{"trusted":true,"_uuid":"cf252e838df3d716a769376b8e3267fe99ed0e99","collapsed":true},"cell_type":"code","source":"#Create variance threshold model\nquasiModel=VarianceThreshold(threshold=0.01) #It will search for the features having 99% of same value in all samples.\nquasiModel.fit(X_train)","execution_count":131,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2e3f9d99936cf928b6c8b9b13d5c5130dad2eba0","collapsed":true},"cell_type":"code","source":"quasiArr=quasiModel.get_support()\nquasiArr\n#get_support() return True and False value for each feature.\n#True: Not a quasi constant feature\n#False: Quasi constant feature(It contains 99% same value in all samples.)","execution_count":132,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7a17b769989493d43d30b223ee8bc19228a12cb","collapsed":true},"cell_type":"code","source":"#To find total number of quasi constant and non quasi constant features we will be using collections.Counter function.\ncollections.Counter(quasiArr)\n#Non quasi Constant feature:241\n#Quasi constant feature: 43","execution_count":133,"outputs":[]},{"metadata":{"_uuid":"2084e203c144adbe5016546633f53d9edd58f769"},"cell_type":"markdown","source":"We can see there are 43  features/columns having quasi  constant value. This mean they have 99% same value in all samples. Lets proof that, by selecting some of the quasi constant features and print out value counts."},{"metadata":{"trusted":true,"_uuid":"6eef7950e6c602cd3dba386155ac22e252a66ec7","collapsed":true},"cell_type":"code","source":"#Print out quasi constant feature name\nquasiCols=[col for col in X_train.columns if col not in X_train.columns[quasiArr]]\nquasiCols","execution_count":134,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef5658c3355c21f6a76fe0b6d9d96601ded647ec","collapsed":true},"cell_type":"code","source":"totalSampleCount=len(X_train)\nprint(X_train['num_aport_var33_ult1'].value_counts()/totalSampleCount)\nprint(X_train['num_var29'].value_counts()/totalSampleCount)\nprint(X_train['num_venta_var44_ult1'].value_counts()/totalSampleCount)","execution_count":135,"outputs":[]},{"metadata":{"_uuid":"0e5b6a2e5930ce289517f10c9c8ebf1396d2c2ae"},"cell_type":"markdown","source":"We can see here more than 99% observation show one value 0. Therefore, there features are almost constant. Lets remove it from training set and test set."},{"metadata":{"trusted":true,"_uuid":"ba5b9c638392d3eca5f7178d9adf93cff90f538b","collapsed":true},"cell_type":"code","source":"print('Shape before drop-->',X_train.shape, X_test.shape)\nX_train.drop(columns=quasiCols,axis=1,inplace=True)\nX_test.drop(columns=quasiCols,axis=1,inplace=True)\nprint('Shape after drop-->',X_train.shape, X_test.shape)","execution_count":136,"outputs":[]},{"metadata":{"_uuid":"324c138f0a8605293e8fa6757954e3053f1fcff0"},"cell_type":"markdown","source":"# Duplicated Features\nOften datasets contain one or more features that show the same values across all the observations. This means that both features are in essence identical. In addition, it is not unusual to introduce duplicated features after performing one hot encoding of categorical variables, particularly when using several highly cardinal variables. <br>\nNote: Finding duplicated features is a computationally costly operation in Python, therefore depending on the size of your dataset, you might not always be able to perform it."},{"metadata":{"trusted":true,"_uuid":"94ebd3423d192c21c13f1a4cef44fe3d7991d3c9","collapsed":true},"cell_type":"code","source":"#The method will find the duplicate columns and return name of duplicated columns in an array\ndef duplicateColumns(data):\n    dupliCols=[]\n    for i in range(0,len(data.columns)):\n        col1=data.columns[i]\n        for col2 in data.columns[i+1:]:\n            if data[col1].equals(data[col2]):\n                dupliCols.append(col1+','+col2)\n    return dupliCols","execution_count":137,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cfba6b397795cc5d80f1c9f7ec7944a144d7bb7","collapsed":true},"cell_type":"code","source":"duplCols=duplicateColumns(X_train)\nduplCols","execution_count":138,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a39c582b7782144ef25d089ef95ee25b32151e68","collapsed":true},"cell_type":"code","source":"print('Total Duplicated columns',len(duplCols))","execution_count":139,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f2a47760bede93bbd914ed746f145593ea56fe3","collapsed":true},"cell_type":"code","source":"#Lets verify the columns are Identical or not.\nX_train[['ind_var1_0','ind_var40_0']]","execution_count":140,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54c5a7d6a981f2a9ff68c0709b41ac0c88bd85b2","collapsed":true},"cell_type":"code","source":"#Get the duplicate column names\ndCols=[col.split(',')[1] for col in duplCols]\ndCols","execution_count":141,"outputs":[]},{"metadata":{"_uuid":"6cb9ba00596a50b4f410814f3377e201d8e4ac1f"},"cell_type":"markdown","source":"Remove the duplicat columns from training set and test set."},{"metadata":{"trusted":true,"_uuid":"786a0b0d733475373fb0ac4787ef2fc99b7c1c13","collapsed":true},"cell_type":"code","source":"#Find the count of unique columns\nlen(set(dCols))","execution_count":142,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82212e5f5ad66c8134fcbf867edd716b58a2f8a5","collapsed":true},"cell_type":"code","source":"print('Shape of our data before applying filter technique-->',df.shape)\nprint('Shape before droping duplicate columns-->',X_train.shape, X_test.shape)\nX_train=X_train.drop(columns=dCols,axis=1)\nX_test=X_test.drop(columns=dCols,axis=1)\nprint('Shape after droping duplicate columns-->',X_train.shape, X_test.shape)\n\n# I keep a copy of the dataset except constant and duplicated variables\n# to measure the performance of machine learning models\n# at the end of the notebook\nX_train_fil=X_train.copy()\nX_test_fil=X_test.copy()","execution_count":143,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8d45c391692f11338c9fed6e3d0029722b13eefb"},"cell_type":"markdown","source":"**As you can see after applying Constant, Quasi-Constant and Duplicate filter method we have remove 150 features(371-221=150) from our training set and test. Lets do some more filtration.**"},{"metadata":{"_uuid":"fc431eddf588bbff426694f7b33b483b31785470"},"cell_type":"markdown","source":"# Correlation\nIts a process of establishing a relationship or connection between two or more feature. Good feature subsets contain features highly correlated with the target, yet uncorrelated to each other. \n\nI will be using House Prices dataset to show correlation between columns and target."},{"metadata":{"_uuid":"b06acbbe32689a8f2506dcfa61c9f4d936040255","trusted":true,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":144,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ac8209ab616d38eea2c80190c6a2ed7e864a5885"},"cell_type":"code","source":"houseDf=pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')","execution_count":145,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb8de2581e893bc1221872f0049bbd7e8dc535c4","collapsed":true},"cell_type":"code","source":"houseDf.head()","execution_count":146,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f2502439d548895c097489505567afc3ab3ca2c6","collapsed":true},"cell_type":"code","source":"houseDf.info()","execution_count":147,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a08bd7c0d73be51a99c026740b338f7c84fa317","collapsed":true},"cell_type":"code","source":"#Currently I will be dealling with numerical columns only.\ncolType = ['int64','float64']\n#Select the columns which are either int64 or float64.\nnumCols=list(houseDf.select_dtypes(include=colType).columns)\n#Assigning numerical columns from df to data variable. We can use the same variable as well.\ndata=houseDf[numCols]","execution_count":148,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f2ebc2dd1eb4d85bb49ff52ef06d5ca7f8978b5","collapsed":true},"cell_type":"code","source":"data.shape","execution_count":149,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46320f9a8c90a373cdf4d0d147540f3aad5aa1c1","collapsed":true},"cell_type":"code","source":"#Check if there is any missing data.\ndata.isnull().sum().max()","execution_count":150,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ac3a050a3be540dc78c75adfc5a92a87d133551","collapsed":true},"cell_type":"code","source":"#Filling missing data\ndata.fillna(0,axis=1,inplace=True)","execution_count":151,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbbff8f608d0ff9f81a8c3be0fc7c77fbdc83b82","collapsed":true},"cell_type":"code","source":"#Re-check if there is any missing data.\ndata.isnull().sum().max()","execution_count":152,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0adc83b5526c18119e19105703bccd9ee2d17f3c"},"cell_type":"code","source":"#Split our data in training and test set.\nx_train,x_test,y_train,y_test=train_test_split(data.drop('SalePrice',axis=1),data['SalePrice'],test_size=.2,random_state=1)","execution_count":153,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"773240fcf5a76af722c6c4adc88d02e8f102d9a3","collapsed":true},"cell_type":"code","source":"# visualise correlated features\n# I will build the correlation matrix, which examines the \n# correlation of all features (for all possible feature combinations)\n# and then visualise the correlation matrix using seaborn\nplt.figure(figsize=(20,15))\nsns.heatmap(x_train.corr())\nplt.show()","execution_count":154,"outputs":[]},{"metadata":{"_uuid":"25b2bea225126cdac1d7ab3d0df544ab121e6396"},"cell_type":"markdown","source":"In the plot above, the light squares correspond to highly correlated features (>0.75). We can see that there are quite a few. The diagonal represents the correlation of a feature with itself, therefore the value is 1."},{"metadata":{"_uuid":"2613e31b3be8bc91520d5534b81a21061944a0bb"},"cell_type":"markdown","source":"### Brute force approach"},{"metadata":{"trusted":true,"_uuid":"b06642da1216f92eea36a231dd0fb68cb4187e26","collapsed":true},"cell_type":"code","source":"def correlation(dataset,threshold):\n    col_corr=set() # set will contains unique values.\n    corr_matrix=dataset.corr() #finding the correlation between columns.\n    for i in range(len(corr_matrix.columns)): #number of columns\n        for j in range(i):\n            if abs(corr_matrix.iloc[i,j])>threshold: #checking the correlation between columns.\n                colName=corr_matrix.columns[i] #getting the column name\n                col_corr.add(colName) #adding the correlated column name heigher than threshold value.\n    return col_corr #returning set of column names\ncol=correlation(x_train,0.75)\nprint('Correlated columns:',col)","execution_count":155,"outputs":[]},{"metadata":{"_uuid":"2b362bf422549eefe65f4bd45935fe5c5018d15a"},"cell_type":"markdown","source":"We can see that 3 features are highly correlated with other features in the training set. Currently we are dealing with a small dataset, that's why we have only 3 highly correlated features. Lets try to find out the correlated features in santander customer satisfaction database. "},{"metadata":{"trusted":true,"_uuid":"3f71cb0d368028da4cd09385b9c8baf4ef6c4b68","collapsed":true},"cell_type":"code","source":"#X_train is train dataset for Santander database.\nscol=correlation(X_train,0.8)\nprint('Correlated columns:',scol)\nprint(len(scol))","execution_count":156,"outputs":[]},{"metadata":{"_uuid":"7d11089dacce98b9e48378b2765f5f7c5b6dc7be"},"cell_type":"markdown","source":"**As you can see it here, there are 134 features heighly correlated with each othre.**\nCorrelated features in general doesn't improve model preformance most of the time. So its better to remove the correlated features. It makes the learning algorithm faster.\nDue to the curse of dimensionality, less features usually mean high improvement in term of speed."},{"metadata":{"_uuid":"cea3e6e428c72e51d08e2ee32181207c12539a5b"},"cell_type":"markdown","source":"### Lets remove correlated features from Santander database."},{"metadata":{"trusted":true,"_uuid":"6027042427af2fed154e32997ffc5913af6c1576","collapsed":true},"cell_type":"code","source":"print('Shape of our data before applying filter technique-->',df.shape)\nprint('Shape before droping duplicate columns-->',X_train.shape, X_test.shape)\nX_train=X_train.drop(columns=scol,axis=1)\nX_test=X_test.drop(columns=scol,axis=1)\nprint('Shape after droping duplicate columns-->',X_train.shape, X_test.shape)","execution_count":157,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0eac022e643fd7281dba8cb80660439e78fbcad9"},"cell_type":"code","source":"# create a function to build random forests and compare performance in train and test set\ndef RandomForest(X_train, X_test, y_train, y_test):\n    rf = RandomForestClassifier(n_estimators=200, random_state=1, max_depth=4)\n    rf.fit(X_train, y_train)\n    print('Train set')\n    pred = rf.predict_proba(X_train)\n    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n    print('Test set')\n    pred = rf.predict_proba(X_test)\n    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","execution_count":206,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"537d1986e37cdff4ec2c1ff03ccdf4cc1745e537","collapsed":true},"cell_type":"code","source":"# original dataset result\nRandomForest(X_train_org.drop(labels=['ID'], axis=1),\n                  X_test_org.drop(labels=['ID'], axis=1),\n                  Y_train, Y_test)","execution_count":202,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9e4c4b4a4806e24567a3109432da4545a9226b9","collapsed":true},"cell_type":"code","source":"#Result after applying basic filter method on dataset.\nRandomForest(X_train_fil.drop(labels=['ID'], axis=1),\n                  X_test_fil.drop(labels=['ID'], axis=1),\n                  Y_train, Y_test)","execution_count":203,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24f9803d1b506187c9d8016b7313f23e7d3531bc","collapsed":true},"cell_type":"code","source":"#Result after removing correlated features from filtered dataset.\nRandomForest(X_train.drop(labels=['ID'], axis=1),\n                  X_test.drop(labels=['ID'], axis=1),\n                  Y_train, Y_test)","execution_count":204,"outputs":[]},{"metadata":{"_uuid":"28c16995618d3e72abd27ec10cd76310cde6a541"},"cell_type":"markdown","source":"#### We can see after applying Constaint, Quasi Constant, Duplicate and Correlated filter features methods, we have removed **284** (original feature count = 371 - feature count after appling filter methods = 87) features and the performance of the model is also improved(0.7581 vs 7619)."},{"metadata":{"_uuid":"4e83ecb20d4aad557edbdb32d9b34df005bbab7b"},"cell_type":"markdown","source":"**Please checkout [Feature Selection Main Page](https://www.kaggle.com/raviprakash438/feature-selection-technique-in-machine-learning)**"},{"metadata":{"_uuid":"c284c2128e72e87d8a539ffb0141542c184f474f"},"cell_type":"markdown","source":"***Please share your comments,likes or dislikes so that I can improve the post.***"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6117aef96eb2fe980c93693bdfff9cb6e6a729e8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}