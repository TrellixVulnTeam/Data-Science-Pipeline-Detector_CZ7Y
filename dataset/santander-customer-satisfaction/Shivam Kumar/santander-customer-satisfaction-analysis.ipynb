{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.linear_model import LogisticRegression\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]\n    #                           [2/3, 4/7]]\n\n    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]\n    #                           [3/7, 4/7]]\n    # sum of row elements = 1\n    \n    B =(C/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C/C.sum(axis=0)) = [[1/4, 2/6],\n    #                      [3/4, 4/6]] \n    \n    labels = [0,1]\n    # representing A in heatmap format\n    print(\"-\"*20, \"Confusion matrix\", \"-\"*20)\n    #plt.figure(figsize=(20,7))\n    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    print(\"-\"*20, \"Precision matrix (Columm Sum=1)\", \"-\"*20)\n    #plt.figure(figsize=(20,7))\n    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    \n    # representing B in heatmap format\n    print(\"-\"*20, \"Recall matrix (Row sum=1)\", \"-\"*20)\n    #plt.figure(figsize=(20,7))\n    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"analyse = train.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see how many features have same value till 75 percentile ?\n# last row is TARGET, so don't count it\ntest_1 = analyse[analyse['min']==analyse['75%']].iloc[:-1]\nprint('Total features which have same value till 75 percentile is {0} out of {1}'.format(test_1.shape[0], train.shape[1]-2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the duplicated columns\ntrain = train.T.drop_duplicates().T\nprint('after dropping duplicated columns, total number of features remain = ', train.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the columns and the total number of unique values\nfor col in train.columns:\n    if(col=='TARGET'):\n        continue\n    print(col, ' : ', len(train[col].unique()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# as we can see there are many columns only contain two distinct values\n# let's check how many are there\n\ntake_col = []\nfor col in train.columns:\n    if(col=='TARGET'):\n        continue\n    l = len(train[col].unique())\n    if(l<=2):\n        take_col.append([col, len(train[col].unique())])\n        \nprint('Total number of columns contain only two or less unique value = ', len(take_col))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's check whether these columns combinely give any relevant information by univariate analysis\n# we will apply logistic regression model on these features and compare with random model\n# if logistic regression will give better result than we can say these features gives some good information\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"take_col = [x[0] for x in take_col]\nx_train, x_test, y_train, y_test=train_test_split(train[take_col], train['TARGET'], test_size=0.3) #splitting the data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# total datapoints in y_test\ny_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ****Random model****"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we need to generate 9 numbers and the sum of numbers should be 1\n# one solution is to genarate 9 numbers and divide each of the numbers by their sum\n# ref: https://stackoverflow.com/a/18662466/4084039\ntrain_data_len = x_train.shape[0]\ntest_data_len = x_test.shape[0]\n\n# we create a output array that has exactly same size as the CV data\ntrain_predicted_y = np.zeros((train_data_len,2))\nfor i in range(train_data_len):\n    rand_probs = np.random.rand(1,2)\n    train_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Cross Validation Data using Random Model\",log_loss(y_train,train_predicted_y, eps=1e-15))\n\n\n# Test-Set error.\n#we create a output array that has exactly same as the test data\ntest_predicted_y = np.zeros((test_data_len,2))\nfor i in range(test_data_len):\n    rand_probs = np.random.rand(1,2)\n    test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test,test_predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(test_predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ****Logistic Regression****"},{"metadata":{"trusted":true},"cell_type":"code","source":"tscv=TimeSeriesSplit(n_splits=10)\npenalty=['l2']\nparam={'C': [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10], 'penalty':penalty}\nclf=LogisticRegression()\nclf=GridSearchCV(estimator=clf, param_grid=param, cv=tscv,  n_jobs=1, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"start=datetime.datetime.now()\nprint(start)\nclf.fit(x_train,y_train)\n#pickle.dump(clf,open('bow_unigram.p','wb'))\nend=datetime.datetime.now()\nprint('duration = ',(end-start))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c=clf.best_estimator_.get_params()['C']\npenalty=clf.best_estimator_.get_params()['penalty']\nprint('best C=',c)\nprint('best penalty=',penalty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=clf.predict(x_test)\nplot_confusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can say that random model works better than logstic regression. So these 86 sparse features we can drop it.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's analyse more**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')\ntest = pd.read_csv('/kaggle/input/santander-customer-satisfaction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_duplicated_columns(train):\n    train = train.T.drop_duplicates().T\n    #test = test.T.drop_duplicates().T\n    return (train)\n\ndef drop_duplicated_rows(train, test):\n    train = train.drop_duplicates()\n    test = test.drop_duplicates()\n    return (train, test)\n\ndef drop_sparse_columns(train, test):\n    take_col = []\n    for col in train.columns:\n        if(col=='TARGET'):\n            continue\n        l = len(train[col].unique())\n        if(l<=2):\n            take_col.append(col)\n        \n    train.drop(take_col, axis=1, inplace=True)\n    col = train.columns.to_list()\n    col.remove('TARGET')\n    test = test[col]\n    \n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = drop_duplicated_columns(train)\ntrain, test = drop_duplicated_rows(train, test)\ntrain, test = drop_sparse_columns(train, test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will see the columns and their percentile\nfor c in train.columns:\n    percentile = []\n    x = train[c].values\n    for i in range(101):\n        percentile.append(str(np.percentile(x, i)))\n    print(c)\n    print(','.join(percentile))\n    print('***************************')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def unwanted_columns(train):\n    remove_col = []\n    for c in train.columns:\n        percentile = []\n        x = train[c].values\n        if((np.percentile(x, 99.9)==0) & (min(x)>=-1)):\n            remove_col.append(c)\n    train.drop(remove_col, axis=1, inplace=True)    \n    return train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = unwanted_columns(train)\ntrain.reset_index(drop=True, inplace=True)\nmin_var3 = train.loc[train['var3']>(-999999), 'var3'].min()\ntrain.loc[train['var3']==(-999999), 'var3'] = min_var3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# on test data\ncol = train.columns.to_list()\ncol.remove('TARGET')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = test[col]\ntest.loc[test['var3']==(-999999), 'var3'] = min_var3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets do some feature engineering","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = train.columns.to_list()\ncol.remove('TARGET')\ncol.remove(\"ID\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca = PCA(n_components=2)\nx_train_pca = pca.fit_transform(normalize(train[col]))\nx_test_pca = pca.transform(normalize(test[col]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.insert(1, 'PCA1', x_train_pca[:, 0])\ntrain.insert(1, 'PCA2', x_train_pca[:, 1])\ntest.insert(1, 'PCA1', x_test_pca[:, 0])\ntest.insert(1, 'PCA2', x_test_pca[:, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom tqdm import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for ncl in tqdm(range(2,11)):\n    cls = KMeans(n_clusters=ncl)\n    cls.fit_predict(train[col].values)\n    train['kmeans_cluster'+str(ncl)] = cls.predict(train[col].values)\n    test['kmeans_cluster'+str(ncl)] = cls.predict(test[col].values)\n    #flist_kmeans.append('kmeans_cluster'+str(ncl))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"col = train.columns.to_list()\ncol.remove('TARGET')\ncol.remove('ID')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}