{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f95caa90-9894-48b8-9d03-6411363dc2fd"},"outputs":[],"source":"#-*- coding: UTF-8 -*-\n# xgboost+lr,excited!!!!!!\nimport time\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport xgboost as xgb\nfrom sklearn.cross_validation import StratifiedKFold\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.feature_selection import SelectFromModel, VarianceThreshold\nimport xgboost as xgb\nfrom sklearn.cross_validation import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(\"start time: %s\" % time.ctime())\ncheck_time = time.time()\ntime_name = str(time.ctime())\nfile_time = \"_\".join(time_name.split()[:3])\nGBDT_FILE = 'gbdt_imbalance_%s.txt' % file_time    #tree node file\noriginal_seed = 1729\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\nprint('Load data...')\ntrain = pd.read_csv(\"../input/train.csv\")       #update\ntrain_id = train['ID'].values\ntarget = train['TARGET'].values\n#train = train.drop(['ID','TARGET'],axis=1)\n\ntest = pd.read_csv(\"../input/test.csv\")          #update\ntest_id = test['ID'].values\n#test = test.drop(['ID'],axis=1)\nprint('load data complete, train records count: <<<<%s, test records count: <<<<%s, train columns count: <<<<%s, test columns count: <<<<%s, time: <<<<%s' % (train.shape[0], test.shape[0], train.shape[1], test.shape[1], round(((time.time() - check_time)/60),2)))\ncheck_time = time.time()\n#------------------------------------------data process-------------------------------------------------------------\n#removing outliers, MANUALLY\ntrain = train.replace(-999999,2)\ntest = test.replace(-999999,2)\n\n#replace na\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\n\n# remove constant columns (std = 0)\nremove = []\nfor col in train.columns:\n    if train[col].std() == 0:\n        remove.append(col)\n\ntrain.drop(remove, axis=1, inplace=True)\ntest.drop(remove, axis=1, inplace=True)\nlen1 = len(remove)\nprint(train.shape, test.shape)\n\n# remove duplicated columns\nremove = []\ncols = train.columns\nfor i in range(len(cols)-1):\n    v = train[cols[i]].values\n    for j in range(i+1,len(cols)):\n        if np.array_equal(v,train[cols[j]].values):\n            remove.append(cols[j])\n\ntrain.drop(remove, axis=1, inplace=True)\ntest.drop(remove, axis=1, inplace=True)\nlen2 = len(remove)\n\n#label encoder\nlen3 = 0\ncols = train.columns\norigin_cols = train.columns[1:-1]        #original features\nprint(\"origin_cols: %s\" % origin_cols)\nfor col in origin_cols:\n     if train[col].dtype=='object':\n            print(col)\n            len3 += 1\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(train[col].values) + list(test[col].values))\n            train[col] = lbl.transform(list(train[col].values))\n            test[col] = lbl.transform(list(test[col].values))\n\nprint(\"data process ended, row droped:<<<<%s, row encoded:<<<<%s, time spend: <<<<%s\" %(len1+len2, len3, round(((time.time() - check_time)/60),2)))\ncheck_time = time.time()\n#useless if test.txt already exists\n#-----------------------------------------------------GBDT model------------------------------------------------------------------------\nfolds = 10\nskf = StratifiedKFold(target,\n                          n_folds=folds,\n                          shuffle=False,\n                          random_state=1580)  \na, sample_index = list(skf)[0]\ntrain_sample = train.loc[sample_index,:]\ntarget_sample = target[sample_index]\nxgbc = xgb.XGBClassifier(n_estimators=500, seed=1580)\nxgbc.fit(train_sample[origin_cols],target_sample)\nxgbc.booster().dump_model(GBDT_FILE)\nprint('gbdt trained on sample, time spend:<<<<%s' % round(((time.time() - check_time)/60),2))\ncheck_time = time.time()\n#-----------------------------------------------------READ FEATURE----------------------------------------------------------------------\nf = open(GBDT_FILE,'r')\nfeature_dict = {}\nfor line in f.readlines():\n    if '<' in line:           #feature line\n           line = line.split(':')[1].strip()\n           feature_re = re.match('\\[(.*)?\\]', line)\n           info = feature_re.group(0)              #should be only one group\n           info = re.sub('\\[|\\]','',info)\n           feature = info.split('<')[0].strip()\n           value = float(info.split('<')[1].strip())\n           value_set = feature_dict[feature] if feature in feature_dict else set()\n           value_set.add(value)\n           feature_dict[feature] = value_set\n\n#feature encoder\nfor feature,value_set in feature_dict.items():\n    #create two inf of the value_list\n    value_list = sorted(list(value_set))\n    min1 = value_list[0]\n    max1 = value_list[-1]\n    min0 = train[feature].min()\n    max0 = train[feature].max()\n    value_list.insert(0,min0)\n    value_list.insert(len(value_list),max0)\n    for i, value in enumerate(value_list):\n         #no need for the last\n         if len(value_list)==i+1:\n             break\n         #rule: right area of the value\n         low_bound = value\n         high_bound = value_list[i+1]\n         col = \"%s_gt_%s_lt_%s\" % (feature, low_bound, high_bound)    #name the col\n         train[col] = train[feature].apply(lambda x: 1 if x>=low_bound and x<high_bound else 0)\n         test[col] = test[feature].apply(lambda x: 1 if x>=low_bound and x<high_bound else 0)\n\n#remove original feature\ntrain = train.drop(origin_cols, axis=1)\ntest = test.drop(origin_cols, axis=1)\nprint('feature generated base on gbdt sub-model, <<<<%s feature generated, time spend:<<<<%s' % (test.shape[0]-1, round(((time.time() - check_time)/60),2)))\ncheck_time = time.time()\n#--------------------------------------------------------UNDERSAMPLE-------------------------------------------------------------------\ntrain1 = train[train['TARGET']==1]               #positive train samples\ntrain2 = train[train['TARGET']==0]               #negative train samples\n#train2 suppose to be the majority type, if not change it\nif train2.shape[0]<train1.shape[0]:\n    train1 = train[train['TARGET']==0]               #positive train samples\n    train2 = train[train['TARGET']==1]               #negative train samples\ntrain1 = train1.reset_index(drop=True)\ntrain2 = train2.reset_index(drop=True)\nfold = train2.shape[0] / train1.shape[0]\nfolds = int(fold)\nskf1 = StratifiedKFold(train1.TARGET.values,\n                          n_folds=folds,\n                          shuffle=False,\n                          random_state=1580)\nskf2 = StratifiedKFold(train2.TARGET.values,\n                          n_folds=folds,\n                          shuffle=False,\n                          random_state=1580)    \n#------------------------------------------------------LVL1 Logistic Regression--------------------------------------------------------\nclf = LogisticRegression(penalty='l1', random_state=1580, n_jobs=-1)\nfeatures = list(train.columns)\nfeatures.remove('ID')\nfeatures.remove('TARGET')\ndf_train_pred = []\ndf_pred = []\nfor i, (a, neg_index) in enumerate(skf2):\n        fold_tag = \"fold_%s\" % i\n        pos_index = list(skf1)[i][0]\n        train_pos = train1.loc[pos_index, :]\n        trainner = pd.concat((train_pos, train2.loc[neg_index,:]),axis=0, ignore_index=True)\n        y = trainner.TARGET.values\n        X = trainner[features]\n        clf.fit(X,y)\n        train_pred = clf.predict_proba(train[features])[:,1]\n        y_pred = clf.predict_proba(test[features])[:,1]\n        df_train_pred.append(train_pred)\n        df_pred.append(y_pred)\n\n#average results\ntrain_preds = np.average(np.array(df_train_pred), axis=0)\ntest_preds = np.average(np.array(df_pred), axis=0)\nprint('LVL1 Logistic Model trained, Average AUC: <<<<%s, time spend:<<<<%s' % (roc_auc_score(train.TARGET.values, train_preds), round(((time.time() - check_time)/60),2)))\ncheck_time = time.time()\n#-----------------------------------------------------OUTPUT----------------------------------------------------------------------------\npd.DataFrame({\"ID\": test_id, \"TARGET\": test_preds}).to_csv('../output/xgb_lr_imbalance.csv',index=False)\n#pd.DataFrame({\"ID\": test_id, \"TARGET\": test_preds}).to_csv('../output/gbdt_lr_sub_test.csv',index=False)         #test\nprint(\"end time: %s\" % time.ctime())"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e88bdf17-c99d-4523-95b0-371a0f3816cc"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}