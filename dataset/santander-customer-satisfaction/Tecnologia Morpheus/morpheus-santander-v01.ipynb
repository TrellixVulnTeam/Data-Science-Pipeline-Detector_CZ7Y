{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# 1) Import all Library that will be used\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nimport statsmodels.formula.api as smf\n\nfrom scipy import stats\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import linear_model, svm, gaussian_process\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nimport lightgbm as lgb\n\nfrom sklearn import preprocessing\nfrom sklearn import utils\n\nimport statsmodels.formula.api as smf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 1) GradientBoostingRegressor Model\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport statsmodels.formula.api as smf\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nparameters = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n    \"max_depth\":[3,5,8],\n    \"max_features\":[\"log2\",\"sqrt\"],\n    \"criterion\": [\"friedman_mse\",  \"mae\"],\n    \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    \"n_estimators\":[10]\n    }\nGBR = GradientBoostingRegressor()\n#GBR = GridSearchCV(GradientBoostingClassifier(), parameters, cv=10, n_jobs=-1)\n\n# 2) Logistic Regression Model\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\n\n# 3) Aplly Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(n_estimators=100)\n\n# 4) Aplly XGBOOST Model\nfrom xgboost import XGBClassifier\nXGB = XGBClassifier()\n\n# 5) Aplly KNeighbors Model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\n# 6) Aplly SVC Model\nSVC = SVC(probability=True)\n\n# 7) Aplly Decision Tree Model\nDTC = DecisionTreeClassifier()\n\n# 8) Aplly GaussianNB Model\nGNB = GaussianNB()\n\n# 9) Aplly Neural Model\nNN = MLPClassifier(hidden_layer_sizes=(100,100,50))\n\n# 10) Aplly lasso\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\n# 11) Apply Elastic Net\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n\n# 12) Apply Kernel Ridge\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n# 13) Apply LGBMRegressor\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\n# 14) Apply LGBMRegressor\nfrom sklearn.linear_model import LinearRegression\nLR2 = LinearRegression()\n\n#15) Linear Regression with Tensor Flow\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1) Data treatment and cleaning\n\ndf_train_original = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')\ndf_test_original = pd.read_csv('/kaggle/input/santander-customer-satisfaction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train_original\ndf_test = df_test_original\n\nprint ('df_train.shape: ', df_train.shape)\nprint ('df_test.shape: ', df_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('df_train.columns: ', df_train.columns)\nprint ('df_test.columns: ', df_test.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = pd.concat((df_train.loc[:,'ID':'var38'],\n                      df_test.loc[:,'ID':'var38']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get_Dummies para transformar categoricos em Numéricos \nall_data = pd.get_dummies(all_data)\n\n# Substitui os campos nulos pelas médias da coluna em questão\nall_data = all_data.fillna(all_data.mean())\n\n#Cria Matriz X_train utilizando a Matriz com todos os dados all_data: do inicio da matriz (:) até o fim  da matriz df_train.shape[0]\nX_train = all_data[:df_train.shape[0]]\n\n#Cria Matriz X_test utilizando a Matriz com todos os dados all_data: a partir do último registro matriz df_train.shape[0], ou seja, todos os registros que não estiverem em df_train\nX_test = all_data[df_train.shape[0]:]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Cria o y, ou seja, o que será previsto, apenas com o campo \"Survived\"\ny = df_train.TARGET\n\nprint ('X_train.shape: ', X_train.shape)\nprint ('X_test.shape: ', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\n# ! ! ! ! ! ! ! FUNÇÃO PRINCIPAL!!! Todos Modelos\n#\ndef RunModel (ModelName, Model, Df_Test_Original, X_train, X_test, y):\n\n    print ('# # # # Prediction for Model:  ', ModelName, '# # # #')\n    print ('Shape X_train: ', X_train.shape)\n    print ('Shape X_test: ', X_test.shape)\n    print ('Shape y: ', y.shape)\n    print ('Model: ', Model)\n    \n    print ('.FIT Model: ', Model)\n\n    Model.fit(X_train, y)\n\n    print ('PREDICT TRAIN: ', Model)\n\n    yhat_Train = Model.predict(X_train)\n    \n    print ('PREDICT TEST: ', Model)\n\n    yhat_test = Model.predict(X_test)\n\n    # Verify Accuracy and other metrics:\n    RunAcc(ModelName, yhat_Train, y)\n\n    \n    print ('# # # # Tamanho do Df_Test_Original:', Df_Test_Original.shape)\n    print ('# # # # Prediction:', yhat_test.shape, yhat_test)\n\n    Filename = 'Output_Santander_' + ModelName + '.csv'\n    \n    df_Output= pd.DataFrame()\n    df_Output['ID'] = Df_Test_Original['ID']\n    df_Output['TARGET'] = yhat_test\n    df_Output.to_csv(Filename, index = False)\n    \n    return yhat_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#\n# ! ! ! ! ! ! ! Função específica para Runlightgbm\n#\ndef Runlightgbm  (ModelName, Model, Df_Test_Original, X_train, X_test, y):\n\n    print ('# # # # Prediction for Model:  ', ModelName, '# # # #')\n    print ('Shape X_train: ', X_train.shape)\n    print ('Shape X_test: ', X_test.shape)\n    print ('Shape y: ', y.shape)\n    print ('Model: ', Model)\n\n    import lightgbm as lgb\n    d_train = lgb.Dataset(X_train, label=y)\n    \n    params ={\n                'task': 'train',\n                'boosting': 'goss',\n                'objective': 'regression',\n                'metric': 'rmse',\n                'learning_rate': 0.01,\n                'subsample': 0.9855232997390695,\n                'max_depth': 7,\n                'top_rate': 0.9064148448434349,\n                'num_leaves': 63,\n                'min_child_weight': 41.9612869171337,\n                'other_rate': 0.0721768246018207,\n                'reg_alpha': 9.677537745007898,\n                'colsample_bytree': 0.5665320670155495,\n                'min_split_gain': 9.820197773625843,\n                'reg_lambda': 8.2532317400459,\n                'min_data_in_leaf': 21,\n                'verbose': -1,\n                'seed':int(2),\n                'bagging_seed':int(2),\n                'drop_seed':int(2)\n                }\n    \n    print ('.FIT Model (Nesse caso, lgb.train: ', Model)\n\n    clf = lgb.train(params, d_train)\n\n    print ('PREDICT TEST: ', Model)\n\n    yhat_Train = clf.predict(X_train)\n    \n    yhat_test = clf.predict(X_test)\n    \n    # Verify Accuracy and other metrics:\n    RunAcc(ModelName, yhat_Train, y)\n    \n    print ('# # # # Tamanho do Df_Test_Original:', Df_Test_Original.shape)\n    print ('# # # # Prediction:', yhat_test.shape, yhat_test)\n\n    Filename = 'Output_' + ModelName + '.csv'\n    \n    df_Output= pd.DataFrame()\n    df_Output['ID'] = Df_Test_Original['ID']\n    df_Output['TARGET'] = yhat_test\n    df_Output.to_csv(Filename, index = False)\n    \n    return yhat_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\n\n# Calculate Accuracy:\n\ndef RunAcc (ModelName, yhat_Train, y):\n\n    # Micro and None = Accuracy\n    #micro - Calculate metrics globally by counting the total true positives, false negatives and false positives.\n    #macro - Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account'\n    #weighted - Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.'\n\n    \n    # Convert to int because does not accept continuous\n    print ('Accuracy - Antes da Transformação: ')\n    print ('yhat_Train: ', yhat_Train)\n    print ('yhat_test: ', y)\n    \n    yhat_Train_int = yhat_Train * 10\n    yhat_Train_int = yhat_Train_int.astype(int)\n    \n    yhat_test_int = y\n    yhat_test_int = yhat_test_int.astype(int)\n    \n    yhat_Train = yhat_Train_int\n    yhat_test = yhat_test_int\n    \n    print ('Accuracy - Depois da Transformação: ')\n    print ('yhat_Train: ', yhat_Train)\n    print ('yhat_test: ', yhat_test)\n    \n    Accuracy = accuracy_score(yhat_Train, yhat_test)\n\n    # - Recall ! ! !\n    Recall_Macro = recall_score(yhat_Train, yhat_test, average='macro')\n    Recall_weighted = recall_score(yhat_Train, yhat_test, average='weighted')\n    Recall_Micro = recall_score(yhat_Train, yhat_test, average='micro')\n    Recall_None = recall_score(yhat_Train, yhat_test, average=None)\n\n    # - Precision ! ! !\n    Preci_Macro = precision_score(yhat_Train, yhat_test, average='macro')\n    Preci_weighted = precision_score(yhat_Train, yhat_test, average='weighted')\n    Preci_Micro = precision_score(yhat_Train, yhat_test, average='micro')\n    Preci_None = precision_score(yhat_Train, yhat_test, average=None)\n\n    print ('# # # # # # # # # # # # # # # # # # # # #')\n    print ('# # #MODEL: ', ModelName)\n    print ('# # #Accuracy: ', Accuracy)\n    print ('# # # # # # # # # # # # # # # # # # # # #')\n    print ('Recall_Macro: ', Recall_Macro)\n    print ('Recall_Micro: ', Recall_Micro)\n    print ('Recall_None: ', Recall_None)\n    print ('Recall_weighted: ', Recall_weighted)\n    print ('# # # # # # # # # # # # # # # # # # # # #')\n    print ('Preci_Macro: ', Preci_Macro)\n    print ('Preci_Micro: ', Preci_Micro)\n    print ('Preci_None: ', Preci_None)\n    print ('Preci_weighted: ', Preci_weighted)\n    print ('# # # # # # # # # # # # # # # # # # # # #')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0) Run Runlightgbm: This model has exclusive parameters so a single function was created for it:\nSel_Model = lgb\nNameM = 'lgb'\nRunlightgbm(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1) Gradiente Boost Model:\nSel_Model = GBR\nNameM = 'GBR'\nMGBR = RunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 2) Linear Regression:\nSel_Model = LR\nNameM = 'LinearRegress'\nRunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3) Random Forest:\nSel_Model = RF \nNameM = 'RanFor'\nRunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 4) xgboost Model (Megazord):\nSel_Model = XGB\nNameM = 'XgBoost'\nRunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5) KNeighbors Model:\nSel_Model = knn\nNameM = 'KNeighbors'\n#RunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 6) SVC Model:\nSel_Model = SVC\nNameM = 'SVC'\n#RunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 7) Decision Tree Model:\nSel_Model = DTC\nNameM = 'DecisionTree'\nRunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 8) Gaussian:\nSel_Model = GNB\nNameM = 'Gaussian'\nRunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 9) Neural Model:\nSel_Model = NN\nNameM = 'NeuralModel'\nRunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 10) Lasso:\nSel_Model = lasso\nNameM = 'lasso'\nMLASSO = RunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 11) Elastic Net:\nSel_Model = ENet\nNameM = 'ElasticNet'\nMENET = RunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 12) Kernel Ridge:\nSel_Model = KRR\nNameM = 'NeuralModel'\n#RunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 13) LGBMRegressor:\nSel_Model = model_lgb\nNameM = 'LGBMRegressor'\n#LGBM  = RunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 14) Linear Regression:\nSel_Model = LR2\nNameM = 'LinearRegression'\n#LINEAR2 = RunModel(NameM, Sel_Model, df_test_original, X_train, X_test, y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#15) Ensemble with best values\n#ensemble = MENET * 0.40 + LGBM * 0.40 + LINEAR2 * 0.20\n\n#ModelName = 'Ensemble'\n#Filename = 'Output_' + ModelName + '.csv'\n    \n#df_Output= pd.DataFrame()\n#df_Output['ID_code'] = df_test_original['ID_code']\n#df_Output['target'] = ensemble\n#df_Output.to_csv(Filename, index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}