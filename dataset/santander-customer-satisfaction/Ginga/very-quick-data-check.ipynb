{"cells":[{"cell_type":"markdown","metadata":{},"source":"# Quick Data Check (+ Removal of Highly-Correlated Features)\n### Import files and take a quick look at them"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntrain = pd.read_csv(\"../input/train.csv\", index_col=None)\ntrain.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"test = pd.read_csv(\"../input/test.csv\", index_col=None)\ntest.head()"},{"cell_type":"markdown","metadata":{},"source":"### Check data shapes"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print (\"train.shape:\", train.shape)\nprint (\"test.shape:\", test.shape)"},{"cell_type":"markdown","metadata":{},"source":"### Check target value distribution"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"target = train.TARGET\nprint (target.describe())\nplt.hist(target)\nplt.ylabel(\"freq\")\nplt.xlabel(\"target value\")\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"### Check uncommon columns b/w train and test sets"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"uncomList = list(set(train.columns) ^ set(test.columns))\nprint (uncomList)"},{"cell_type":"markdown","metadata":{},"source":"### Drop \"TARGET\" from train.csv and combine train/test.csv"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train.drop(\"TARGET\", axis=1, inplace=True)\ncombi = pd.concat([train, test], axis=0)\nprint (\"train.shape[0] + test.shape[0]:\", train.shape[0]+test.shape[0])\nprint (\"combi.shape:\", combi.shape)"},{"cell_type":"markdown","metadata":{},"source":"### Check column dtypes"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"floatList = []\nintList = []\nobjectList = []\n\nfor t in combi.columns:\n    if combi[t].dtypes==np.float64 or combi[t].dtypes==np.float32:\n        floatList.append(t)\n    elif combi[t].dtypes==np.int64 or combi[t].dtypes==np.int32:\n        intList.append(t)\n    else:\n        objectList.append(t)\n        \nprint (\"The number of float columns:\", len(floatList))\nprint (\"The number of int columns:\", len(intList))\nprint (\"The number of non-numeric columns:\", len(objectList))"},{"cell_type":"markdown","metadata":{},"source":"### Check \"NaN\" count in each column"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"combiNan = np.sum(combi.isnull())\n\ncombiNanCounter = 0\ncombiNanCol = []\n\nfor n in range(len(combiNan)):\n    if combiNan[n] > 0:\n        print (combiNan.index[n])\n        combiNanCol.append(n)\n    combiNanCounter += 1\n    \nprint (\"Checked columns:\", combiNanCounter)\nprint (\"Columns with Nan:\", len(combiNanCol))"},{"cell_type":"markdown","metadata":{},"source":"### Check the number of unique values in each column"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"uniq10 = []\nuniq100 = []\nuniqMany = []\n\nfor u in combi.columns:\n    if combi[u].nunique() <= 10:\n        uniq10.append(u)\n    elif combi[u].nunique() > 10 & combi[u].nunique() <= 100:\n        uniq100.append(u)\n    else:\n        uniqMany.append(u)\n        \nprint (\"The number of columns with <= 10 unique values:\", len(uniq10))\nprint (\"The number of columns with 10<x<=100 unique values\", len(uniq100))\nprint (\"The number of columns with >100 unique values:\", len(uniqMany))"},{"cell_type":"markdown","metadata":{},"source":"### Create a correlation matrix to check (linearly) highly-correlated numeric (float) variables "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Check only for the colums with float values to avoid categoricals to be incorporated\ncombiFloat = combi[floatList]\n\n# <Removed from ver3>\n# May not need this feature scaling (0-1) since probably matplotlib could deal with it\n# But basically pearson correlation coefficient is sensitive to scale so this is just to make sure\n# from sklearn.preprocessing import MinMaxScaler\n# combiFloat = combiFloat.apply(lambda x: MinMaxScaler().fit_transform(x))\n\n# get correlation coefficient as a matrix\ncorrFloat = combiFloat.corr()\nprint (\"The shape of correlation coefficient matrix:\", corrFloat.shape)"},{"cell_type":"markdown","metadata":{},"source":"### Take a quick look at correlation matrix as a heatmap (float)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Below code URL: https://stanford.edu/~mwaskom/software/seaborn/examples/many_pairwise_correlations.html\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corrFloat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 17))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corrFloat, mask=mask, cmap=cmap, vmin=-1, vmax=1,\n            square=True, xticklabels=5, yticklabels=5,\n            linewidths=.5, cbar_kws={\"shrink\": .8}, ax=ax)"},{"cell_type":"markdown","metadata":{},"source":"### Remove 1 (the latter) of pairs of 2 highly-correlated variables (e.g. remove v2 for (v1, v2) pair)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# threshold is arbitrary, but in this example threshold = +/-0.8 (Pearson's correlation coefficient)\n\n# check the number of unique combinations of 2 variables\nimport itertools\npairs = list(itertools.combinations(corrFloat.columns, 2))\nprint (\"Variables pairs:\", len(pairs))\n\nhiCor = []\nhiCorCounter = 0\nfor i in range(corrFloat.shape[1]):\n    for j in range(0, i):\n        if corrFloat[corrFloat.columns[i]][j] > 0.8 or corrFloat[corrFloat.columns[i]][j] < -0.8:\n            hiCor.append(corrFloat.index[j])\n        hiCorCounter += 1\n\n# get unique values from the list\nhiCor = list(set(hiCor))\n\nprint (\"Checked pairs:\", hiCorCounter)\nprint (\"Columns to be removed due to high correlation:\", len(hiCor))\n\ncombi.drop(hiCor, axis=1, inplace=True)\n\nprint (\"New combi shape:\", combi.shape)"},{"cell_type":"markdown","metadata":{},"source":"### Split into train/test.csv with highly-correlated variables (float) removed"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train = combi[:train.shape[0]]\ntrain[\"TARGET\"] = target\ntest = combi[train.shape[0]:]\n\nprint (\"new train shape:\", train.shape)\nprint (\"new test shape:\", test.shape)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}