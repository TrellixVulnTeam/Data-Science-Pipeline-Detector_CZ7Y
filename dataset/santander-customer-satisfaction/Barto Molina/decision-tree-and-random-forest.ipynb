{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Random Forest\n---\nIn this kernel we're going to use a Decission Tree as a base model and then we'll build a Random Forest classifier. We're going to build different models based on the previous analysis, by using different training datasets:\n\n1. The initial dataset (cleaned and standarized)\n2. Resampled / PCA data\n3. Balanced data\n\nWe'll then compare the score of these models and determine what would be the best preprocessing strategy in this case."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport scikitplot as skplt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the train and test data files\ntrain_clean_standarized = pd.read_csv(\"../input/feature-exploration-and-dataset-preparation/train_clean_standarized.csv\", index_col=0)\ntrain_resampled_PCA = pd.read_csv(\"../input/pca-principal-component-analysis/train_PCA.csv\", index_col=0)\ntrain_resampled = pd.read_csv(\"../input/resampling/train_resampled.csv\", index_col=0)\ntest = pd.read_csv(\"../input/santander-customer-satisfaction/test.csv\", index_col=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Base model - Decision tree"},{"metadata":{},"cell_type":"markdown","source":"## 1.1 First attempt - Using the initial dataset"},{"metadata":{},"cell_type":"markdown","source":"We're going to start with a simple decision tree as our base model that we'll use to compare the rest of the models with. We'll first use the original standarized dataset and look at the model performance:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get our train test split data (25% test data)\ny = train_clean_standarized.TARGET\nX = train_clean_standarized.drop(\"TARGET\", axis=1)\ndata_train, data_test, target_train, target_test = train_test_split(X, y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate and fit the base model\n# we're just picking some random hyperparameters\ntree_clf = DecisionTreeClassifier(criterion='gini', max_depth=50) \ntree_clf.fit(data_train, target_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fea_imp = pd.DataFrame({'imp': tree_clf.feature_importances_, 'col': X.columns})\nfea_imp = fea_imp[fea_imp.imp > .005].sort_values(['imp', 'col'], ascending=[True, False])\nfea_imp.plot(kind='barh', x='col', y='imp', legend=None)\nplt.title('Decision Tree - Feature importance')\nplt.ylabel('Features')\nplt.xlabel('Importance');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As commmented in other kernels in Kaggle, we find that the most important features are:\n\n- var38: Mortgage\n- var15: Customer age\n- saldo_var30: This may correspond to the current account balance\n\nNow let's have a look at the model performance. For this, we're going to look at the confusion matrix and the classification report for the test set predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the test set predictions\npred = tree_clf.predict(data_test)\nskplt.metrics.plot_confusion_matrix(target_test, pred);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(target_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can see the problem of having a highly unbalanced dataset: The number of false positives is very high in relation with the true negatives, so even though the accuracy of the model is high (93%), this is the result of the model predicting the majority class (0 - customer satisfied) in most cases. In cases like this, where the classes are imbalance, the f1-score is a better metric."},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Second attempt using resampled dataset, stratify training/set data and weighting classes\nIn order to overcome the issues caused by the imbalanced dataset we'll:\n1. Use the dataset where we performed a downsample of the majority class (satisfied customers).\n2. Balance (stratify) the training / test datasets.\n3. Assign different weights to both classes in our model (even though we have resampled our data, there's still a majority of satisfied customers)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# get our train test split data (25% test data)\ny = train_resampled.TARGET\nX = train_resampled.drop(\"TARGET\", axis=1)\n\n# we use stratify to balance our data sets\ndata_train, data_test, target_train, target_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n\n# we assign a different weigth to the unsatisfied customer class (6.6 as there are 6.6 more satisfied customers in the dataset)\ntree_clf_resampled = DecisionTreeClassifier(criterion='gini', max_depth=50, class_weight={0:1,1:6.6}) \ntree_clf_resampled.fit(data_train, target_train)\n\n# calculate the test set predictions and display the confusion matrix and classification report\npred = tree_clf_resampled.predict(data_test)\nskplt.metrics.plot_confusion_matrix(target_test, pred)\nprint(classification_report(target_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1.3 Conclusions"},{"metadata":{},"cell_type":"markdown","source":"We can see that although the accuracy has decreased a little (from 94% to 82%) the precission of the model has increased notably and we have reduced the number of false positives."},{"metadata":{},"cell_type":"markdown","source":"# 2. Random forest"},{"metadata":{},"cell_type":"markdown","source":"Let's build a Random forest classifier and compare it with our base model. Initially, we'll pick some random hyperparameters. Later on, we'll run a Grid Search to find more optimal parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get our train test split data (25% test data)\ny = train_resampled.TARGET\nX = train_resampled.drop(\"TARGET\", axis=1)\n\n# we use stratify to balance our data sets\ndata_train, data_test, target_train, target_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n\n# we assign a different weigth to the unsatisfied customer class (6.6 as there are 6.6 more satisfied customers in the dataset)\nforest_clf = RandomForestClassifier(n_estimators=100, max_depth=50, class_weight={0:1,1:6.6}) \nforest_clf.fit(data_train, target_train)\n\n# calculate the test set predictions and display the confusion matrix and classification report\npred = forest_clf.predict(data_test)\nskplt.metrics.plot_confusion_matrix(target_test, pred)\nprint(classification_report(target_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Comparing the Random Forest with our base model, we can see precission is very similar, and the accuracy has increased a little (from 82% to 87%)"},{"metadata":{},"cell_type":"markdown","source":"## 2.1 Hyperparameter tunning\nBefore, we've just randomly tried different parameters to build our Random Forest. In this case, we're going to use a Grid Search to find the optimal values for these parameters. Note that this is computationally expensive, so we're not going to test many different parameters:"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_param_grid = {\n    'class_weight': [{0:1,1:3}, {0:1,1:6.6}, {0:1,1:10}],\n    'max_depth': [None, 50, 100],\n    'min_samples_split': [2, 40, 60],\n    'min_samples_leaf': [1, 4]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_grid_search = GridSearchCV(RandomForestClassifier(n_estimators=100), rf_param_grid, cv=3, return_train_score=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_grid_search.fit(data_train, target_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_grid_search.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate the test set predictions and display the confusion matrix and classification report\npred = rf_grid_search.best_estimator_.predict(data_test)\nskplt.metrics.plot_confusion_matrix(target_test, pred)\nprint(classification_report(target_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2 Random Forest with PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get our train test split data (25% test data)\ny = train_resampled_PCA.TARGET\nX = train_resampled_PCA.drop(\"TARGET\", axis=1)\n\n# we use stratify to balance our data sets\ndata_train, data_test, target_train, target_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=42)\n\nforest_clf_PCA = RandomForestClassifier(n_estimators=100, max_depth=50, class_weight={0:1,1:25}) \nforest_clf_PCA.fit(data_train, target_train)\n\n# calculate the test set predictions and display the confusion matrix and classification report\npred = forest_clf_PCA.predict(data_test)\nskplt.metrics.plot_confusion_matrix(target_test, pred)\nprint(classification_report(target_test, pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.3 Conclusions\nComparing the Random Forest with our base Decission Tree, we can see that we have slightly improved our model score while maintaining a similar ration on the False Positives, which is the main issue given that our dataset is highly imbalanced. Let's compare the different models:\n\nModel                      |    data      | accuracy | f1 avg | f1 weighted\n---------------------------|--------------|----------|--------|------------ \nBase Model (Decision Tree) | standarized  | .93      | .54    | .93\nBase Model (Decision Tree) |  resampled   | .81      | .60    | .81\nRandom Forest              |  resampled   | .85      | .62    | .84\nGridSearch Random Forest   |  resampled   | .87      | .60    | .84\nRandom Forest              | standard PCA | .95      | .53    | .94"},{"metadata":{},"cell_type":"markdown","source":"# 3. Submission\nWe're going to prepare the data to be submitted to the competition. However, we found that most models are not predicting any non satisfied customers. This is probably because we've dropped too many data points during the resampling phase:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare submission test data\ncolumn_diff = np.setdiff1d(test.columns.values, train_resampled.columns.values)\ntest_clean = test.drop(column_diff, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree predictions\npred = tree_clf.predict(test_clean)\nsubmission = pd.DataFrame({\"ID\":test_clean.index, \"TARGET\":pred})\n#submission.to_csv(\"submission_DecisionTree.csv\", index=False)\nsubmission.TARGET.value_counts(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Decision Tree predictions (resampled data)\npred = tree_clf_resampled.predict(test_clean)\nsubmission = pd.DataFrame({\"ID\":test_clean.index, \"TARGET\":pred})\n#submission.to_csv(\"submission_DecisionTree_Resampled.csv\", index=False)\nsubmission.TARGET.value_counts(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest predictions\npred = forest_clf.predict(test_clean)\nsubmission = pd.DataFrame({\"ID\":test_clean.index, \"TARGET\":pred})\n#submission.to_csv(\"submission_RandomFores.csv\", index=False)\nsubmission.TARGET.value_counts(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid Search Random Forest predictions\npred = rf_grid_search.best_estimator_.predict(test_clean)\nsubmission = pd.DataFrame({\"ID\":test_clean.index, \"TARGET\":pred})\n#submission.to_csv(\"submission_GridSearchRandomForest.csv\", index=False)\nsubmission.TARGET.value_counts(0)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}