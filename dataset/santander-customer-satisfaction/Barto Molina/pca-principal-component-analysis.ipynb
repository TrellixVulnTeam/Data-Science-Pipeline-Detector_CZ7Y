{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# PCA - Principal Component Analysis\n---\nIn this kernel we're going to reduce the number of features in our dataset through the PCA, this will help improving the performance of our models in computational time and it may help improving the overall model precission if we're dealing with overfitting. We'll then use both datasets (the original one and the after the PCA) to compare our models."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# load the train data file\ntrain = pd.read_csv(\"../input/feature-exploration-and-dataset-preparation/train_clean_standarized.csv\", index_col=0)\ntrain_resampled = pd.read_csv(\"../input/resampling/train_resampled.csv\", index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's separate our features from the target\nX = train.drop('TARGET', axis=1)\ny = train['TARGET'].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Feature correlation"},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the feature correlation: It's difficult to conclude anything by obvserving the heatmap below due to the high number of features. However at a first glance we can see that in general the correlation isn't so high and we'll probably need a large number of Principal components to explain the variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(X.corr());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nevertheless, looking at the correlation matrix we can see that some of the features (probably the ones that reference different time periods - i.e. imp_op_var39_comer_ult1 and imp_op_var39_comer_ult3 present a high correlation."},{"metadata":{},"cell_type":"markdown","source":"# 2. PCA\nWe're going to calculate the explained variance for the different number of components:"},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_test = PCA().fit(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot the Cumulative Summation of the Explained Variance for the different number of components\nplt.figure()\nplt.plot(np.cumsum(pca_test.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Dataset Explained Variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that with a number between 50 and 100 components, we'll still get a good % of variance explained. Let's try with 80 components:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate PCA\npca = PCA(n_components=80)\n\n# fit PCA\nprincipalComponents = pca.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's have a look at the dataset containing our 80 principal components:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pc = pd.DataFrame(data = principalComponents)\ntrain_target = pd.Series(y, name='TARGET')\n\ntrain_pc_df = pd.concat([train_pc, train_target], axis=1)\ntrain_pc_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train_pc.corr());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we calculate the variance explained by priciple component\nprint('Variance of each component:', pca.explained_variance_ratio_)\nprint('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see from the results above, this 90 principal components explain 91% of the variance in the data.\n\nWe're going to use these components to compare the result of our models against the ones built using the full dataset."},{"metadata":{},"cell_type":"markdown","source":"# 3. PCA on resampled data"},{"metadata":{},"cell_type":"markdown","source":"We're going to use the output from the resampling exercise to run a Principal Component Analysis on these data:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's separate our features from the target\nX_resampled = train_resampled.drop('TARGET', axis=1)\ny_resampled = train_resampled['TARGET'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(X_resampled.corr());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_resampled.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_resampled_test = PCA().fit(X_resampled)\n\n# plot the Cumulative Summation of the Explained Variance for the different number of components\nplt.figure()\nplt.plot(np.cumsum(pca_resampled_test.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Dataset Explained Variance')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate PCA\npca_resampled = PCA(n_components=80)\n\n# fit PCA\nprincipalComponents_resampled = pca_resampled.fit_transform(X_resampled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_resampled_pc = pd.DataFrame(data = principalComponents_resampled)\ntrain_resampled_target = pd.Series(y_resampled, name='TARGET')\n\ntrain_resampled_pc_df = pd.concat([train_resampled_pc, train_resampled_target], axis=1)\ntrain_resampled_pc_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train_resampled_pc_df.corr());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we calculate the variance explained by priciple component\nprint('Variance of each component:', pca_resampled.explained_variance_ratio_)\nprint('\\n Total Variance Explained:', round(sum(list(pca_resampled.explained_variance_ratio_))*100, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_pc_df.to_csv('train_PCA.csv')\ntrain_resampled_pc_df.to_csv('train_resampled_PCA.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}