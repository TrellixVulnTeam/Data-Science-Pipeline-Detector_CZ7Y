{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# XGBoost\n---\nIn this kernel we're going to build a XGBoost model. As with the Random Forest classifiers, we're going to determine the most important features."},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the train and test data files\ntrain_clean_standarized = pd.read_csv(\"../input/feature-exploration-and-dataset-preparation/train_clean_standarized.csv\", index_col=0)\ntest = pd.read_csv(\"../input/santander-customer-satisfaction/test.csv\", index_col=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. XGBClassifier\nWe'll build our XGBoost classifier and calculate the training and test accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"# get our train test split data (25% test data)\ny = train_clean_standarized.TARGET\nX = train_clean_standarized.drop(\"TARGET\", axis=1)\ndata_train, data_test, target_train, target_test = train_test_split(X, y, test_size=0.25, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate and fit XGBClassifier\nclf = XGBClassifier()\nclf.fit(data_train, target_train)\n\n# predict on training and test sets\ntraining_preds = clf.predict(data_train)\ntest_preds = clf.predict(data_test)\n\n# accuracy of training and test sets\ntraining_accuracy = accuracy_score(target_train, training_preds)\ntest_accuracy = accuracy_score(target_test, test_preds)\n\nprint('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\nprint('Validation Accuracy: {:.4}%'.format(test_accuracy * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our XGBoost model has an accuracy of 96%, however, that may be caused due to the highly impalanced classes. Let's look at the predictions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare submission test data\ncolumn_diff = np.setdiff1d(test.columns.values, train_clean_standarized.columns.values)\ntest_clean = test.drop(column_diff, axis=1)\n\n# XGBoost predictions\npred = clf.predict(test_clean)\nsubmission = pd.DataFrame({\"ID\":test_clean.index, \"TARGET\":pred})\n#submission.to_csv(\"submission_DecisionTree.csv\", index=False)\nsubmission.TARGET.value_counts(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Indeed, the minority class has been predicted only 31 times, so it seems that our classifier is just mostly predicting the majority class. As with the Random forest we will need to further work on the sampling or find other hyperparameters that mitigate this problem."},{"metadata":{},"cell_type":"markdown","source":"## 1.1 Feature importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"fea_imp = pd.DataFrame({'imp': clf.feature_importances_, 'col': X.columns})\nfea_imp = fea_imp[fea_imp.imp > .02].sort_values(['imp', 'col'], ascending=[True, False])\nfea_imp.plot(kind='barh', x='col', y='imp', legend=None)\nplt.title('XGBoost Tree - Feature importance')\nplt.ylabel('Features')\nplt.xlabel('Importance');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's a difference on the feature importance compared to the Random Forest. The Mortgage (var38) is not the most important feature anymore, but both the Customer age (var15) and the current balance (saldo_var30) remain as the two most important features."},{"metadata":{},"cell_type":"markdown","source":"## 1.2 Hyperparameter tunning"},{"metadata":{},"cell_type":"markdown","source":"Same as with the Random Forest, we're going to try finding the optimal parameters for our classifier. Given the number of features, we:"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    'max_depth': [2], #[2,3,4]\n    'subsample': [0.6], #[0.4,0.5,0.6,0.7],\n    'colsample_bytree': [0.5], #[0.5,0.6],\n    'n_estimators': [100], #[100,200]\n    'reg_alpha': [0.03] #[0.01, 0.02, 0.03, 0.04]\n}\n\nxgb_clf = GridSearchCV(XGBClassifier(), param_grid, cv=5, scoring=\"f1_weighted\")\nxgb_clf.fit(data_train, target_train)\nbest_est = xgb_clf.best_estimator_\nprint(best_est)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict on training and test sets\ntraining_preds = xgb_clf.predict(data_train)\ntest_preds = xgb_clf.predict(data_test)\n\n# accuracy of training and test sets\ntraining_accuracy = accuracy_score(target_train, training_preds)\ntest_accuracy = accuracy_score(target_test, test_preds)\n\nprint('Training Accuracy: {:.4}%'.format(training_accuracy * 100))\nprint('Validation Accuracy: {:.4}%'.format(test_accuracy * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare submission test data\ncolumn_diff = np.setdiff1d(test.columns.values, train_clean_standarized.columns.values)\ntest_clean = test.drop(column_diff, axis=1)\n\n# XGBoost predictions\npred = xgb_clf.predict(test_clean)\nsubmission = pd.DataFrame({\"ID\":test_clean.index, \"TARGET\":pred})\nsubmission.to_csv(\"submission_XGBoost.csv\", index=False)\nsubmission.TARGET.value_counts(0)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}