{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Resampling\n---\nAs we observed during the feature exploration, our target feature is highly imbalanced (less than 4% are unsatisfied customers). This can be problematic as our models can present a good accuracy just by predicting the majority class (i.e. a model that predicts that all customers are satisfied). Also, we need to take this into account in our modelling during the train_test_split phase. Ideally we would want to have approximately the same representation of both classes in the train and test datasets.\n\nIn this kernel, we're going to resample the data in order to balance the classes. Later on we'll use this balanced dataset to build our models and study and compare the differences of the models when using the original dataset, principal components and this resampled dataset."},{"metadata":{},"cell_type":"markdown","source":"# Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# load the train and test data files\ntrain = pd.read_csv(\"../input/feature-exploration-and-dataset-preparation/train_clean_standarized.csv\", index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see how imbalanced is our TARGET feature\nprint(train.TARGET.value_counts())\nax = train.TARGET.value_counts().plot(kind='bar', title='Customer satisfaction')\nax.set_xticklabels(['satisfied', 'unsatisfied']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Resampling\nOne of the most common techniques when dealing with unbalanced datasets is resampling:\n\n1. Undersampling: removing samples from the majority class\n2. Oversampling: adding samples to the minority class\n\nIn the first case, we could just randomly remove data points associated to \"satisfied\" customers from our dataset to match the number of unsatisfied customers. However, in this case we would be dropping most of the data which may contain important information to train our models. In this case, if we just do this, we would end up with around 6000 data points, which may not be enough, specially taking into account the number of features we have.\n\nOn the other hand, we could add samples to the minority class. The simplest technique would be just by duplicating records from the minority class, but this could cause overfitting.\n\nOther than random undersampling / oversampling, there are more advanced techniques that can be used:\n\n1. Undersampling:\n  - Tomek links (from the imbalanced-learn module (imblearn)): Tomek links are pairs of instances that are very close to each other, but belong to different classes. By removing the instance from the majority class, we'll facilitate the classification process.\n  - Cluster centroids: This technique generates centroids based on clustering methods and will remove data points while preserving the majority of the information.\n2. Oversampling:\n  - SMOTE: This technique uses the k-nearest neighbors in order to create new datapoints.\n\nHowever these techniques are based on clustering and ([don't work well with high-dimensional data](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-106)) as in our case. We would need to reduce the number of features, i.e. calculating PCA, and we've already seen during our PCA analysis that we need a fair amount of components.\n\nThat's why we're just going to reduce the number of data points by randomly remove instances from the majority class (for now we're going to keep 20000 thousand records):"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data between satisfied and unsatisfied customers\ntrain_satisfied = train[train.TARGET == 0]\ntrain_unsatisfied = train[train.TARGET == 1]\n\n# undersample the majority class to 20000 instances\ntrain_satisfied_under = train_satisfied.sample(20000)\n\n# combine the two classes\ntrain_resampled = pd.concat([train_satisfied_under, train_unsatisfied]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's see how imbalanced is our TARGET feature in the resampled dataset\nprint(train_resampled.TARGET.value_counts())\nax = train_resampled.TARGET.value_counts().plot(kind='bar', title='Customer satisfaction')\nax.set_xticklabels(['satisfied', 'unsatisfied']);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_resampled.to_csv('train_resampled.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}