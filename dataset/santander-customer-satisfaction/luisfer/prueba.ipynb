{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import linear_model, metrics\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.feature_selection import SelectFromModel\n\nimport xgboost as xgb\n\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n# clean and split data\nprint (\"dimention of the traing data\"+ str(train.shape))\nprint (\"dimention of the test data\"+ str(train.shape))\n\n\n# remove constant columns (std = 0)\nremove = []\nfor col in train.columns:\n    if train[col].std() == 0:\n        remove.append(col)\n\n\ntrain.drop(remove, axis=1, inplace=True)\ntest.drop(remove, axis=1, inplace=True)\nprint (\"removing \" + str(len(remove))+ \"vars\")\nprint (\"dimention of the traing removing 0 sd\"+ str(train.shape))\nprint (\"dimention of the test removing 0 sd\"+ str(train.shape))\n\n\n\n# remove duplicated columns\nremove_dups = []\ncols = train.columns\nfor i in range(len(cols)-1):\n    v = train[cols[i]].values\n    for j in range(i+1,len(cols)):\n        if np.array_equal(v,train[cols[j]].values):\n            remove_dups.append(cols[j])\n\nprint (\"removing \" + str(len(remove_dups))+ \"vars\")\ntrain.drop(remove_dups, axis=1, inplace=True)\ntest.drop(remove_dups, axis=1, inplace=True)\n\n\nprint (\"dimention of the traing data after duplicated \"+ str(train.shape))\nprint (\"dimention of the test data after duplicated \"+ str(train.shape))\n\n\n\n# split data into train and test\ntest_id = test.ID\ntest = test.drop([\"ID\"],axis=1)\n\n\n\n\nX = train.drop([\"TARGET\",\"ID\"],axis=1)\ny = train.TARGET.values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1729)\n\n\nprint(X_train.shape, X_test.shape, test.shape)\n\n## # Feature selection\nclf = RandomForestClassifier(random_state=1729)\nselector = clf.fit(X_train, y_train)\n# clf.feature_importances_ \nfs = SelectFromModel(selector, prefit=True)\n\nX_train = fs.transform(X_train)\nX_test = fs.transform(X_test)\ntest = fs.transform(test)\n\nprint(X_train.shape, X_test.shape, test.shape)\n\n\n## # Train Model\n# classifier from xgboost\nm2_xgb = xgb.XGBClassifier(n_estimators=110, nthread=-1, seed=1729)\nm2_xgb.fit(X_train, y_train, eval_metric=\"auc\",\n           eval_set=[(X_test, y_test)])\n\n# calculate the auc score\nprint(\"Roc AUC: \", roc_auc_score(y_test, m2_xgb.predict_proba(X_test)[:,1],\n              average='macro'))\n              \n## # Submission\nprobs = m2_xgb.predict_proba(test)\n\nsubmission = pd.DataFrame({\"ID\":test_id, \"TARGET\": probs[:,1]})\nsubmission.to_csv(\"submission.csv\", index=False)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}