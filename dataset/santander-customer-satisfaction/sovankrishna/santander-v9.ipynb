{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import pandas as pd\n\nfrom pandas import Series, DataFrame\nimport numpy as np\nimport matplotlib as mpl\n\nimport seaborn as sn\n# Form machine learning\nimport sklearn\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_curve, auc\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train = pd.read_csv(\"../input/train.csv\", index_col=None)\ntest = pd.read_csv(\"../input/test.csv\", index_col=None)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train_ID = train.ID\ntest_ID = test.ID\ntrain_TARGET = train.TARGET\n# Copying the contents of the columns to separate DFs, later will combine after the pre processing"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train.drop('TARGET', axis = 1, inplace = True)\ntrain.drop('ID', axis = 1, inplace = True)\ntest.drop('ID', axis = 1, inplace = True)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"total = pd.concat([train, test])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"total = total.replace(-999999,2)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"######################################### Checking the no of different datatype variables present in the dataset\n\nfloatlist = []\nintegerlist = []\nobjectlist = []\n\nfor i in total.columns:\n    if total[i].dtypes==np.float64 or total[i].dtypes==np.float32:\n        floatlist.append(i)\n    elif total[i].dtypes==np.int64 or total[i].dtypes==np.int32:\n        integerlist.append(i)\n    else:\n        objectlist.append(i)\n\nprint (\"The number of float variables:\", len(floatlist))\nprint (\"The number of integer variables:\", len(integerlist))\nprint (\"The number of non-numeric/class variables:\", len(objectlist))\n\n########################################### Categorizing each variables according to their unique values\nvar_0 = []\nvar_1 = []\nvar_2 = []\n\nfor i in total.columns:\n    if total[i].nunique() <= 10:\n        var_0.append(i)\n    elif total[i].nunique() > 10 & total[i].nunique() <= 100:\n        var_1.append(i)\n    else:\n        var_2.append(i)\n        \nprint (\"The number of columns with <= 10 unique values:\", len(var_0))\nprint (\"The number of columns with 10<x<=100 unique values\", len(var_1))\nprint (\"The number of columns with >100 unique values:\", len(var_2))\n\n########################################## Checking each variable for presence of missing values\n\ntotal_missing = total.isnull().sum()\n\ntotal_missing_counter = 0\ntotal_missing_varlist = []\n\nfor i in range(len(total_missing)):\n    if total_missing[i]>0:\n        total_missing_varlist.append(i)\n    total_missing_counter += 1\n    \nprint('No of variables checked for missing values:', total_missing_counter)\nprint('Variables having missing values:', total_missing_varlist)\n\n########################################### Removing constant columns (std == 0 )\n\ncolsToRemove = []\nfor col in total.columns:\n    if total[col].std() == 0:\n        colsToRemove.append(col)\n\ntotal.drop(colsToRemove, axis=1, inplace=True)\n\n########################################### Drop dulicate columns\ncolsToRemove = []\ncolumns = total.columns\nfor i in range(len(columns)-1):\n    v = total[columns[i]].values\n    for j in range(i+1,len(columns)):\n        if np.array_equal(v,total[columns[j]].values):\n            colsToRemove.append(columns[j])\n\ntotal.drop(colsToRemove, axis=1, inplace=True)\n\nprint('Duplicate variables:', colsToRemove)\n\n\ntotal.shape\n# the column size just decreased from 371 to 309 (27 variables)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"Train = total[:train.shape[0]]\nTrain[\"TARGET\"] = train_TARGET\ntest = total[train.shape[0]:]\n\nprint (\"new train shape:\", Train.shape)\nprint (\"new test shape:\", test.shape)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"Train.shape"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\n# Master split for training & test dataset\n\nTrain['is_Train'] = np.random.uniform(0, 1, len(Train)) <= .75\ntraining, validation = Train[Train['is_Train']==True], Train[Train['is_Train']==False]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"validation.shape"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Dependant variable\ny_train = training['TARGET']\n# Independant variable list\nfeatures = list(training.columns[:308])\nx_train = training[features]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Dependant variable\ny_validation = validation['TARGET']\n# Independant variable list\nfeatures = list(validation.columns[:308])\nx_validation = validation[features]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.ensemble import AdaBoostClassifier #For Classification\n# from sklearn.ensemble import AdaBoostRegressor #For Regression\n# from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier #For Classification\nfrom sklearn.ensemble import GradientBoostingRegressor #For Regression"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\nclf.fit(x_train, y_train)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"y_pred_class = clf.predict(x_validation)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# Confusion Matrix\nfrom sklearn import metrics\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"metrics.confusion_matrix(y_validation, y_pred_class)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# save confusion matrix and slice into four pieces\nconfusion = metrics.confusion_matrix(y_validation, y_pred_class)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"metrics.accuracy_score(y_validation, y_pred_class)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# store the predicted probabilities for class 1\ny_pred_prob = clf.predict_proba(x_validation)[:, 1]"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# allow plots to appear in the notebook\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.rcParams['font.size'] = 14\n\n# histogram of predicted probabilities\nplt.hist(y_pred_prob, bins=8)\nplt.xlim(0, 1)\nplt.title('Histogram of predicted probabilities')\nplt.xlabel('Predicted probability of ----')\nplt.ylabel('Frequency')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# IMPORTANT: first argument is true values, second argument is predicted probabilities\nfpr, tpr, thresholds = metrics.roc_curve(y_validation, y_pred_prob)\nplt.plot(fpr, tpr)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('------')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# AUC is the percentage of the ROC plot that is underneath the curve:\n# IMPORTANT: first argument is true values, second argument is predicted probabilities\nmetrics.roc_auc_score(y_validation, y_pred_prob)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"################################\ntest.shape"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"y_pred_class_test = clf.predict(test)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"y_pred_class_test.shape"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"y_predproba_test = clf.predict_proba(test)\n\nsubmission = pd.DataFrame({\"ID\":test_ID, \"TARGET\": y_predproba_test[:,1]})\nsubmission.to_csv(\"submission.csv\", index=False)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}