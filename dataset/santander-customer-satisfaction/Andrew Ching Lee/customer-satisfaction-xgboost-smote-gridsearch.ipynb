{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import feature_selection\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_selection import RFE\nfrom xgboost import XGBClassifier\nfrom sklearn.feature_selection import VarianceThreshold\nimport collections\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\nfrom sklearn.model_selection import ParameterGrid\nfrom imblearn.over_sampling import ADASYN","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train=pd.read_csv(\"../input/santander-customer-satisfaction/train.csv\",index_col=\"ID\")\ntest_X=pd.read_csv(\"../input/santander-customer-satisfaction/test.csv\",index_col=\"ID\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# DATASET ANALYSIS"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All features are numeric"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X.isnull().sum().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is no missing data in both test, train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\")\nsns.countplot(x=\"TARGET\",data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df=pd.DataFrame(train.TARGET.value_counts())\ndf['percentage']=100*df['TARGET']/train.shape[0]\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Huge Class Imbalance as shown above"},{"metadata":{},"cell_type":"markdown","source":"For detailed EDA please see https://www.kaggle.com/cast42/exploring-features#Clusters"},{"metadata":{},"cell_type":"markdown","source":"# DATA CLEANING"},{"metadata":{"trusted":true},"cell_type":"code","source":"# -999999 in var 3 means unkown so we are replacing it with the most common value in var3\ntrain.var3 = train.var3.replace(-999999,2)\ntest_X.var3 = test_X.var3.replace(-999999,2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.var3==-999999].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_X.loc[test_X.var3==-999999].shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Validation & Train set Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X, val_X, train_y, val_y = train_test_split(train.drop(labels=['TARGET'], axis=1),train['TARGET'],test_size=0.2,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Feature Variance Analysis*"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Use Variance Thereshold to remove both constant,quasi-constant features\nselector = VarianceThreshold(threshold=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"selector.fit(train_X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"constArr=selector.get_support()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"constCol=[col for col in train_X.columns if col not in train_X.columns[constArr]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# constant features\nconstCol","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check\ntrain_X.ind_var2_0.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dropping constant features from train,test,val set\ntrain_X.drop(columns=constCol,axis=1,inplace=True)\ntest_X.drop(columns=constCol,axis=1,inplace=True)\nval_X.drop(columns=constCol,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_X.shape)\nprint(test_X.shape)\nprint(val_X.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Remove Duplicate features*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def duplicateColumns(data):\n    dupliCols=[]\n    for i in range(0,len(data.columns)):\n        col1=data.columns[i]\n        for col2 in data.columns[i+1:]:\n            if data[col1].equals(data[col2]):\n                dupliCols.append(col1+','+col2)\n    return dupliCols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dupCol=duplicateColumns(train_X)\ndCols=[col.split(',')[1] for col in dupCol]\ndCols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dupCol","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"delete the any oen of them will be fine but we decide to delete the first column"},{"metadata":{"trusted":true},"cell_type":"code","source":"dCols=list(set(dCols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.drop(columns=dCols,axis=1,inplace=True)\nval_X.drop(columns=dCols,axis=1,inplace=True)\ntest_X.drop(columns=dCols,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Removing Features that are highly correlated to each other*"},{"metadata":{"trusted":true},"cell_type":"code","source":"def correlation(dataset,threshold):\n    col_corr=set() # set will contains unique values.\n    corr_matrix=dataset.corr() #finding the correlation between columns.\n    for i in range(len(corr_matrix.columns)): #number of columns\n        for j in range(i):\n            if abs(corr_matrix.iloc[i,j])>threshold: #checking the correlation between columns.\n                colName=corr_matrix.columns[i] #getting the column name\n                col_corr.add(colName) #adding the correlated column name heigher than threshold value.\n    return col_corr #returning set of column names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corrCol=list(correlation(train_X,0.8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corrCol)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_X.drop(columns=corrCol,axis=1,inplace=True)\nval_X.drop(columns=corrCol,axis=1,inplace=True)\ntest_X.drop(columns=corrCol,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have finished using filter method to select features"},{"metadata":{},"cell_type":"markdown","source":"*Scale the data*"},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler=StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_sca_X = scaler.fit_transform(train_X)\ntest_sca_X = scaler.transform(test_X)\nval_sca_X = scaler.transform(val_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Oversample Data"},{"metadata":{},"cell_type":"markdown","source":"*oversample data with smote*"},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(train_sca_X,columns=train_X.columns,index=train_X.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sm = SMOTE(random_state=42)\ntrain_res_X, train_res_y = sm.fit_resample(train_sca_X, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_res_y.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modelling with xgboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_xgb_clf = XGBClassifier(random_state=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_xgb_clf.fit(train_res_X,train_res_y,early_stopping_rounds=20,eval_metric=\"auc\",eval_set=[(val_sca_X, val_y)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y = baseline_xgb_clf.predict_proba(val_sca_X)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"roc_auc_score(val_y,pred_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Above Model has performed relatively well. Let's see if we could improve results by hyperparameter tuning"},{"metadata":{},"cell_type":"markdown","source":"# Hyperparameter Tuning"},{"metadata":{},"cell_type":"markdown","source":"Using Grid Search Cross Validation to find best hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# param_grid = {\"learning_rate\"    : [0.05, 0.10] ,\n#  \"max_depth\"        : [5, 6, 8, 10],\n#  \"min_child_weight\" : [ 1, 3, 5],\n#  \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4],\n#  \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_clf=XGBClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best_score=0\n# for g in ParameterGrid(param_grid):\n#      xgb_clf.set_params(**g)\n#      xgb_clf.fit(train_res_X,train_res_y,early_stopping_rounds=20,eval_metric=\"auc\",eval_set=[(val_sca_X, val_y)])\n#      pred_y = xgb_clf.predict_proba(val_sca_X)[:,1]\n#      score=roc_auc_score(val_y,pred_y)\n#      if score > best_score:\n#          best_score = roc_auc_score(val_y,pred_y)\n#          best_grid = g","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_grid={'colsample_bytree': 0.7,\n 'gamma': 0.1,\n 'learning_rate': 0.1,\n 'max_depth': 6,\n 'min_child_weight': 1}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_clf.set_params(**best_grid)\nxgb_clf.fit(train_res_X,train_res_y,early_stopping_rounds=20,eval_metric=\"auc\",eval_set=[(val_sca_X, val_y)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have finished hyperparameter tuning. Now we should make a prediction on the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_y = xgb_clf.predict_proba(test_sca_X)[:,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"subs = pd.DataFrame(({'ID': test_X.index, 'TARGET': pred_y}))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Another set of grid search to find best hyperparameter"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\"learning_rate\"    : [0.03] ,\n  \"max_depth\"        : [6],\n \"min_child_weight\" : [ 0,1],\n \"gamma\"            : [ 0.1],\n\"n_estimators\": [150,200,250],\n\"colsample_bytree\" : [ 0.8,0.85] }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(list(ParameterGrid(param_grid)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_score=0\nfor g in ParameterGrid(param_grid):\n    xgb_clf.set_params(**g)\n    xgb_clf.fit(train_res_X,train_res_y,early_stopping_rounds=20,eval_metric=\"auc\",eval_set=[(val_sca_X, val_y)])\n    pred_y = xgb_clf.predict_proba(val_sca_X)[:,1]\n    score=roc_auc_score(val_y,pred_y)\n    if score > best_score:\n        best_score = roc_auc_score(val_y,pred_y)\n        best_grid = g","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_grid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best_grid2={'colsample_bytree': 0.8,\n# 'gamma': 0.0,\n#  'learning_rate': 0.03,\n#  'max_depth': 6,\n#  'min_child_weight': 1,\n#  'n_estimators': 350}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#best_score2=0.8262192223595399","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#subs.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We arrive at 0.8265 accuracy it is not the best I might come back at this problem later. But it should be a fairly good notebook to show we tackle imbalanced class classification problem"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}