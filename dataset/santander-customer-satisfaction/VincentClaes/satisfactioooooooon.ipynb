{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# inspired by https://www.kaggle.com/srodriguex/santander-customer-satisfaction/model-and-feature-selection-with-python\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"train = pd.read_csv('../input/train.csv')\nprint(train.info())"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def create_sparse_matrix():\n    remove = []\n    for col in train.columns:\n        if train[col].std() == 0:\n            remove.append(col)\n    train.drop(remove, axis=1, inplace=True)\n    sparse_matrix = train.replace(0, np.nan).to_sparse()\n    print(sparse_matrix.info())\n    return sparse_matrix\n\n#train = create_sparse_matrix()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"### no None, NaN, inf values in the train dataset\n\ncount_nulls = (train.isnull().sum()==1).sum()\nprint(count_nulls)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"### we remove the columns with constant values\n\ndef remove_constant_columns(dataframe):\n    unique_values = dataframe.apply(lambda x : len(x.unique()))\n    colls_to_drop = unique_values[unique_values==1].index.values.tolist()\n    print('number of train columns before dropping constants {0}'.format(len(dataframe.columns.values)))\n    dataframe = dataframe.drop(colls_to_drop, axis=1)\n    print('number of train columns after dropping constants {0}'.format(len(dataframe.columns.values)))\n    return dataframe\n\n\ntrain = remove_constant_columns(train)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"from itertools import combinations\nfrom numpy import array,array_equal\n\ndef identify_equal_features(dataframe):\n    features_to_compare = list(combinations(dataframe.columns.tolist(),2))\n    equal_features = []\n    for compare in features_to_compare:\n        is_equal = array_equal(dataframe[compare[0]],dataframe[compare[1]])\n        if is_equal:\n            equal_features.append(list(compare))\n    return dataframe, equal_features\n\n\ndef drop_duplicates(dataframe, list_equal_features):\n    ### get the columns which have duplicates and remove them. \n    list_equal_unique_features = array(list_equal_features)[:,1]\n    print('number of columns to drop : {}'.format(len(list_equal_unique_features)))\n    try:\n        dataframe = dataframe.drop(list_equal_unique_features, axis=1)\n    except ValueError as v:\n        print('columns were already dropped')\n    return dataframe\n\n\ndef identify_and_drop_equal_features(dataframe):\n    dataframe, equal_features = identify_equal_features(dataframe)\n    print(equal_features)\n    return drop_duplicates(dataframe, equal_features)\n\n#train = identify_and_drop_equal_features(train)\n#dataframe, eaqual_features = identify_equal_features(train)\n#print(eaqual_features)\ntrain = identify_and_drop_equal_features(train)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"def create_features_and_target_df(df):\n    y_name = 'TARGET'\n    feature_names = train.columns.tolist()\n    feature_names.remove(y_name)\n    X = train[feature_names]\n    Y = train[y_name]\n    return X, Y\n\nX,Y = create_features_and_target_df(train)\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\nfrom sklearn import cross_validation as cv\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn import ensemble\nfrom sklearn import linear_model \nfrom sklearn import naive_bayes \n\nskf = cv.StratifiedKFold(Y, n_folds=3, shuffle=True)\nscore_metric = 'roc_auc'\nscores = {}\n\ndef score_model(model):\n    return cv.cross_val_score(model, X, Y, cv=skf, scoring=score_metric)\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# time: 10s\nscores['tree'] = score_model(tree.DecisionTreeClassifier()) \nprint(scores['tree'])"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# time: 7s\nscores['forest'] = score_model(ensemble.RandomForestClassifier())\nprint(scores['forest'])\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import xgboost as xgb\n# time: 4min\nscores['xgboost'] = score_model(xgb.XGBClassifier())"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"clf = xgb.XGBClassifier()\nclf.fit(X,Y)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"test = pd.read_csv('../input/test.csv')\ntest = remove_constant_columns(test)\ntest = identify_and_drop_equal_features(test)\nX_test,Y_test = create_features_and_target_df(train)\n\ntest_pred = clf.predict_proba(X_test)\nprint(test_pred)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print(len(test_pred))"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}