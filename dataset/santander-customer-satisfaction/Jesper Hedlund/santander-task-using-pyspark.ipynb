{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# installing pyspark\n!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading spark context"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n# Creating spark session containing spark context\n#sc = SparkContext(appName = \"Santander_Customer_Satisfaction\")\nspark = SparkSession.Builder().getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading data from files"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data\n\n\ntrain = pd.read_csv('../input/santander-customer-satisfaction/train.csv')\n\ntest = pd.read_csv('../input/santander-customer-satisfaction/test.csv')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data to spark session\ntrain_spark = spark.read.format(\"csv\").option(\"header\", \"true\").load('../input/santander-customer-satisfaction/train.csv')\ntest_spark = spark.read.format(\"csv\").option(\"header\", \"true\").load('../input/santander-customer-satisfaction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data exploration"},{"metadata":{},"cell_type":"markdown","source":"#### Spark mixed with some python"},{"metadata":{"trusted":true},"cell_type":"code","source":"# displaying first 5 rows\ntrain_spark.toPandas().head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looking at the distribution of the target column \ntotal_datapoints = len(train[\"TARGET\"])\nprint(train[\"TARGET\"].value_counts()/total_datapoints)\nplt.figure(figsize=(6,4))\nsns.barplot(y=train[\"TARGET\"].value_counts()/total_datapoints, x=[\"0\",\"1\"])\nsns.despine()\nplt.suptitle(\"Distribution of TARGET column in percentage\",fontsize=18)\nplt.title(\"blue=0 (satisfied), orange=1 (unsatisfied)\")\n\n# 0 = Satisfied, 1= unsatisfied\n# Alot more satisfied customers, about 96%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting stats on each column\ntrain_spark.describe().toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Min value: -999999 for var3 as value seems a bit wierd!"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.var3.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking distribution of rows with feature \"var3\" = -999999\nplt.figure(figsize=(6,4))\nsns.barplot(y=train.loc[train.var3 == -999999].TARGET.value_counts()/len(train.loc[train.var3 == -999999]),x=[\"0\",\"1\"])\nsns.despine()\nplt.suptitle(\"Distribution for var3=-999999\")\nplt.title(\"count of TARGET=0 (blue) and TARGET=1 (orange)\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The var3=-999999 seems to have a different distribution than the overall dataset\nwhen looking at the TARGET variable -> may contain information. So we keep those rows.\nOBS, subject of improvement, should be investigated further.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Checking for nan-values:\")\nprint(train.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assuming ID is not correlated with customer satisfaction\n#train = train.drop([\"ID\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assuming ID is not correlated with customer satisfaction so i drop it\ntrain_spark_drop_id = train_spark.drop('ID')\n#train_spark_drop_id.toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The dataset is heavily skew towards satisfied customers."},{"metadata":{},"cell_type":"markdown","source":"# Undersampling of data due to imbalanced target distribution"},{"metadata":{},"cell_type":"markdown","source":"#### Spark"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating one data frame for each class\ntrain_spark_target_0 = train_spark_drop_id.filter(\"TARGET=0\")\ntrain_spark_target_1 = train_spark_drop_id.filter(\"TARGET=1\")\n\n# Counting the number of samples for each of them\nnum_target_0 = train_spark_target_0.count()\nnum_target_1 = train_spark_target_1.count()\n\n\n# Downsampling the dataset of TARGET=0 to same about amount of rows as TARGET=1\n# OBS. This function does not sample exact amount, subject for improvement\ntrain_spark_target_0_under = train_spark_target_0.sample(True, num_target_1/num_target_0)\n\n# Concatenating the undersampled with TARGET=0 and the ordinary TARGET=1\ntrain_under_spark = train_spark_target_0_under.union(train_spark_target_1)\n\nprint(\"Precentage of each class after under sampling\")\nprint(train_under_spark.toPandas()[\"TARGET\"].value_counts()/train_under_spark.count())\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Python"},{"metadata":{"trusted":true},"cell_type":"code","source":"#count_class_0, count_class_1 = train.TARGET.value_counts()\n#train_target_0 = train[train['TARGET'] == 0]\n\n#train_target_1 = train[train['TARGET'] == 1]\n\n#train_target_0_under = train_target_0.sample(count_class_1)\n#train_under =  pd.concat([train_target_0_under, train_target_1], axis=0, ignore_index=True)\n\n#total_datapoints_under = len(train_under[\"TARGET\"])\n#print(\"Precentage of each class after under sampling\")\n#print(train_under[\"TARGET\"].value_counts()/total_datapoints_under)\n#train_under","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Data cleaning\n## Removing constant columns"},{"metadata":{},"cell_type":"markdown","source":"#### Spark"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating the amount of unique values for each column\n## This could maybe be solved in a better way than to cast as panda. Casting seems quite heavy\nconstant_in_train_spark= train_under_spark.toPandas().apply(lambda x: x.nunique(), axis=0)\n\n# Extracting the list of the columns with only one unique value\nc_train_ind_spark = list(constant_in_train_spark[constant_in_train_spark == 1].index.values)\n\n# Dropping all the columns \n## below '*'' is used to sen the list as arguments to drop since it could not take a list as input\ntrain_drop_1_spark = train_under_spark.drop(*c_train_ind_spark)\nprint('Number of cols dropped: ', len(c_train_ind_spark))\nprint(c_train_ind_spark)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Python"},{"metadata":{"trusted":true},"cell_type":"code","source":"# checking if some column is constant\n# axis=0 : applies to all columns\n#constant_in_train = train_under.apply(lambda x: x.nunique(), axis=0)\n#c_train_ind = constant_in_train[constant_in_train == 1].index.values\n#print('Number of cols dropped: ', len(c_train_ind)))\n#print(list(c_train_ind))\n\n#train_drop_1 = train_under\n\n# Dropping constant columns\n#train_drop_1.drop(list(c_train_ind), axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for highly correlated features"},{"metadata":{},"cell_type":"markdown","source":"#### Spark"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Spark\n#Calculating the correlation matrix . From https://stackoverflow.com/questions/51831874/how-to-get-correlation-matrix-values-pyspark/51834729\nfrom pyspark.mllib.stat import Statistics\n\n# copy df except TARGET\ndf_corr = train_drop_1_spark.drop(\"TARGET\")\n# copying columns\ncol_names = df_corr.columns\n# Creatting an rdd of all features\nfeatures = df_corr.rdd.map(lambda row: row[0:])\n#Calculating correlation\ncorr_mat=Statistics.corr(features, method=\"pearson\")\n# Creating a data frame from result\ncorr_matrix_spark = pd.DataFrame(corr_mat)\n# Setting column names of datafram\ncorr_matrix_spark.index, corr_matrix_spark.columns = col_names, col_names\n\ncorr_matrix_spark","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Python"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating correlation matrix for all features\n#corr_matrix = train_drop_1.corr()\n#corr_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dropping all features with correlation higher than 0.9"},{"metadata":{},"cell_type":"markdown","source":"#### Spark"},{"metadata":{"trusted":true},"cell_type":"code","source":"cols_to_remove_spark = []\n\n# Looping through \nfor col in range(len(corr_matrix_spark.columns)):\n    for row in range(col):\n        if (corr_matrix_spark.iloc[row,col] >0.5 \\\n            or corr_matrix_spark.iloc[row,col] < -0.5) \\\n            and (corr_matrix_spark.columns[row] not in cols_to_remove_spark):\n                \n            cols_to_remove_spark.append(corr_matrix_spark.columns[col])\n\ntrain_drop_2_spark = train_drop_1_spark.drop(*cols_to_remove_spark)\n\nprint(\"Columns removed:\")\nprint(len(cols_to_remove_spark))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Python"},{"metadata":{"trusted":true},"cell_type":"code","source":"#cols_to_remove = []\n#for col in range(len(corr_matrix.columns)):\n#    for row in range(col):\n#        if (corr_matrix.iloc[row,col] >0.9 or corr_matrix.iloc[row,col] < -0.9) and (corr_matrix.columns[row] not in cols_to_remove):\n#            cols_to_remove.append(corr_matrix.columns[col])\n\n#train_drop_2 = train_drop_1.drop(cols_to_remove, axis=1)\n\n#print(\"Columns removed:\")\n#print(len(cols_to_remove))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Removing same columns for test data"},{"metadata":{},"cell_type":"markdown","source":"#### Spark"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_no_target_cols_spark = train_drop_2_spark.columns[0:-1]\ntest_ids = test_spark.toPandas()[\"ID\"].values\ntest_remove_spark = test_spark.select(*train_no_target_cols_spark)\n#test_remove_spark.toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Python"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing same columns from test\n\n#train_no_target_cols = train_drop_2.columns[0:-1]\n\n#test_remove = test[train_no_target_cols]\n#test_remove","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Transforming from spark DF to RDD"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating RDD from panda\n#from pyspark.mllib.regression import LabeledPoint\n\n#training set\n#s_df_train = spark.createDataFrame(train_drop_2.sample(300))\n#RDD_train = s_df_train.rdd.map(lambda x: LabeledPoint(x[\"TARGET\"], x[:-1]))\n\n\n#test set\n#s_df_test = spark.createDataFrame(test_remove)\n#RDD_test = s_df_test.rdd.map(lambda x: x[:])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.mllib.regression import LabeledPoint\n\nRDD_train_spark = train_drop_2_spark.rdd.map(lambda x: LabeledPoint(x[\"TARGET\"], x[:-1]))\nRDD_test_spark = test_remove_spark.rdd.map(lambda x: x[:])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ML-part Random forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.mllib.tree import RandomForest, RandomForestModel\nfrom pyspark.mllib.util import MLUtils\n\n# Creating random forest model\nmodel = RandomForest.trainClassifier(RDD_train_spark, numClasses=2, categoricalFeaturesInfo={},\n                                     numTrees=200, featureSubsetStrategy=\"auto\",\n                                     impurity='gini', maxDepth=15, maxBins=32, seed=12345)\n#print(model.toDebugString())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predicting test values\npredictions = model.predict(RDD_test_spark).collect()\nprint(predictions[0:100])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a datafram to submit results on test set\nsubmission_df = pd.DataFrame(columns=[\"ID\", \"TARGET\"])\nsubmission_df[\"TARGET\"] = predictions\nsubmission_df[\"ID\"] = test_ids\nsubmission_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# WRiting results to csv-files\nsubmission_df.to_csv('santandersubmission_corma_test.cvs', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Score on test data : 0.75 "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}