{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"dcea82eb-8ff5-c63c-2a92-5334e0955c19"},"source":"#Analysis of the Santander Customer Satisfication Data"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4899d474-7715-f133-d376-55e1de5643d8"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as plt\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n#Load the training data\ndf = pd.read_csv('../input/train.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3ab73859-182d-1422-6f02-47bff469c9b5"},"outputs":[],"source":"train_labels = df['TARGET'].tolist();\nnum_unsatisfied = train_labels.count(1);\nnum_satisfied = train_labels.count(0);\nprint(str(round(100*num_unsatisfied/(num_satisfied + num_unsatisfied),2)) + \"% of customers unsatisfied.\")"},{"cell_type":"markdown","metadata":{"_cell_guid":"0c80b7bd-e99e-e8e9-0fcb-4b7b8da1667e"},"source":"Only 4% of the customers are unsatisfied. A good classifier should therefore have a misclassification error less than 4%.\nWe first examine the features and see which can be eliminated. Constant features and features which are highly correlated can be removed. (This step takes long time)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dfae26d9-e0d6-4e76-d6a9-fe4f1dc94893"},"outputs":[],"source":"#Remove the ID feature\ndel df['ID']\nprint(\"ID feature removed.\")\n\n#Remove features with variance less than 0.01\nk = 0;\nfor name in list(df.columns.values):\n    if name != 'TARGET' and np.var(df[name].tolist()) < 0.01:\n        del df[name];\n        k += 1;\nprint(str(k) + \" features removed because of low variance.\")\n\n#Remove highly correlated features\nfeatures = list(df.columns.values);\nremoved_features = set();\nk = 0;\nfor k1 in range(len(features)-1):\n    for k2 in range(k1+1,len(features)-1):\n        if features[k2] not in removed_features and abs(df[features[k1]].corr(df[features[k2]])) > 0.99:\n            removed_features.add(features[k2])\n            k += 1;\n            #print(features[k1] + \" and \" + features[k2] + \" are strongly correlated.\")\nfor feature in removed_features:\n    del df[feature]\nprint(str(k) + \" features removed because of strong correlation.\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f806e09-1c2f-92a6-9d0e-c2c950205ce8"},"outputs":[],"source":"#Find which features are most correlated with the TARGET feature\nfeatures = df.columns.values\ntarget_corr = []\nfor k1 in range(len(features)-1):\n    target_corr.append(abs(df['TARGET'].corr(df[features[k1]])))\n\nfeatures_target_corr = zip(features[:-2],target_corr)\nsorted_features_target_corr = sorted(features_target_corr,key = lambda pair: pair[1],reverse=True)\nsorted_features = [pair[0] for pair in sorted_features_target_corr]\nsorted_corr = [pair[1] for pair in sorted_features_target_corr]\nfor k1 in range(10):\n    print(sorted_features[k1] + \": \" + str(sorted_corr[k1]))"},{"cell_type":"markdown","metadata":{"_cell_guid":"6783e6ed-bec6-734e-c5e8-ef87388d65d9"},"source":"No specific class seems to be strongly correlated with unsatisfaction. We now try to use linear classifiers.\n\n__Approach 1__: Try to separate the classes using standard linear classifiers such as **logistic regression**."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5a92ae57-861b-715b-4f63-9569e37e4dbd"},"outputs":[],"source":"from sklearn import linear_model\nlogistic = linear_model.LogisticRegression()\n\n#Make training data into matrix\nrelevant_features = df.columns.values\nrelevant_features = relevant_features[:-2]\ntrain_data = df[relevant_features].as_matrix()\n\n##Normalize features\n#for k in range(len(train_data[0])):\n#    train_data[k] = (train_data[k] - train_data[k].mean())/train_data[k].std()\n#train_labels = np.asarray(train_labels)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c14db9df-b964-a3c8-b117-6eb78cc747a3"},"outputs":[],"source":"#Fit simple logistic model (no regularization)\nlogistic.fit(train_data, train_labels)\n\n#Compute training error\ndef compute_train_error(logistic0,train_data0,train_labels0):\n    train_predict0 = logistic0.predict(train_data0)\n    print(str(round(100*sum(train_predict0 != train_labels0)/len(train_labels0),2)) + \"% misclassification error.\")\n    \nprint(\"Training set error.\")\ncompute_train_error(logistic,train_data,train_labels)"},{"cell_type":"markdown","metadata":{"_cell_guid":"81af4c6c-e040-c489-c920-450710ccb1dc"},"source":"Logistic regression gives about 4% misclassification error on the training set. This is the same error as when assuming all customers are satisfied. Try to improve by balancing class weights (to compensate for the different number of samples in the classes)."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"721faaff-d9dc-c8ea-05e4-4d1068ed31a4"},"outputs":[],"source":"logistic2 = linear_model.LogisticRegression(class_weight='balanced')\n#logistic2 = linear_model.LogisticRegression(class_weight='balanced')\nlogistic2.fit(train_data,train_labels)\n\nprint(\"Training set error.\")\ncompute_train_error(logistic2,train_data,train_labels)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6bbbcd18-832b-e3b3-3511-e6460b7fc6a4"},"source":"The model with balanced classes gives **65% misclassification error** on the training set. This indicates that the classes are not linearly separable.\n\nWe try using only the most correlated features.\n\nWe now try to add polynomial features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"10c5188c-f342-e9a4-1b28-ec618de7167c"},"outputs":[],"source":"top_relevant_features = sorted_features[:10]\ntop_train_data = df[top_relevant_features].as_matrix()\n\nlogistic.fit(top_train_data, train_labels)\nprint(\"Training set error.\")\ncompute_train_error(logistic,top_train_data,train_labels)\nlogistic2.fit(top_train_data, train_labels)\nprint(\"Training set error. Balanced classes.\")\ncompute_train_error(logistic2,top_train_data,train_labels)"},{"cell_type":"markdown","metadata":{"_cell_guid":"e1ae2aa6-ede1-4f81-c288-edd7c534a302"},"source":"Does not seem to work well. Try adding polynomial features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0e606849-cbf6-f513-8b9f-eb9e97172a54"},"outputs":[],"source":"L = len(train_data[0])\npoly_train = np.empty([len(train_data),L+L*L])\nfor k1 in range(len(train_data)):\n    a = train_data[k]\n    b = np.kron(a,a)\n    poly_train[k] = np.concatenate((a,b),axis=0)\n\nlogistic.fit(poly_train, train_labels)\nprint(\"Training set error. Polynomial features.\")\ncompute_train_error(logistic,poly_train,train_labels)\nlogistic2.fit(poly_train, train_labels)\nprint(\"Training set error. Polynomial features and balanced classes.\")\ncompute_train_error(logistic2,poly_train,train_labels)"},{"cell_type":"markdown","metadata":{"_cell_guid":"7883f785-4a61-2bb5-1094-9abb50079381"},"source":"Does also work poorly.\n\n__Approach 3__: Try to use the support vector machine."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3aafb399-4e81-ee38-c087-2ffa49333710"},"outputs":[],"source":"from sklearn import svm\n\n#Fit simple SVM (no regularization)\nsvm1 = svm.SVC()\nsvm1.fit(train_data,train_labels)\n\n\n##Compute training error\n#def compute_train_error(logistic0,train_data0,train_labels0):\n#    train_predict0 = logistic0.predict(train_data0)\n#    print(str(round(100*sum(train_predict0 != train_labels0)/len(train_labels0),2)) + \"% misclassification error.\")\n    \nprint(\"Training set error.\")\ncompute_train_error(svm1,train_data,train_labels)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9490ac2c-8425-083c-b9e3-819f1af4b7f9"},"outputs":[],"source":"#Try 5 different training set divisions with 80% used as training data\nM = round(number_of_samples*0.2)\nfor i in range(5):\n    test_set = range(i*M,(i+1)*M)\n    train_set = list(set(range(number_of_samples)).difference(set(test_set)))\n    train_data1 = train_data[train_set]\n    train_labels1 = train_labels[train_set]\n    test_data1 = train_data[test_set]\n    test_labels1 = train_labels[test_set]\n\n    logistic2.fit(train_data1,train_labels1)\n\n    print(\"Cross-validation error \" + str(i+1) + \".\")\n    compute_train_error(logistic2,test_data1,test_labels1)"},{"cell_type":"markdown","metadata":{"_cell_guid":"0f27e789-a984-4a06-f7a1-88c8dd53ca33"},"source":"Cross validation gives us that the model (although quite simple) gives low empirical misclassification error."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4f8be6ba-6b7f-9802-45dc-09c7129120d3"},"outputs":[],"source":"W = range(10)\nfor k in W[:5]:\n    print(k)\nprint(\"---\")\nfor k in W[5:]:\n    print(k)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e3f0115f-9e19-640b-4bdc-17eb46d9bc1e"},"outputs":[],"source":"x = list()\nx.append(0)\nx.append(2)\nprint(x)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}