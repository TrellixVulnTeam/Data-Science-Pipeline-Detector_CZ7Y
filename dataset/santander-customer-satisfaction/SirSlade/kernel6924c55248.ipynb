{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport eli5\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom eli5.sklearn import PermutationImportance\nfrom eli5.xgboost import get_feature_importance_explanation\nfrom imblearn.over_sampling import SMOTE\nfrom category_encoders import OneHotEncoder\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import IsolationForest\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading and description","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# All unclean data\nX = pd.read_csv(\"/kaggle/input/santander-customer-satisfaction/train.csv\", index_col=\"ID\")\nY = X[\"TARGET\"]\nX.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#All negative labels\nX[X[\"TARGET\"] == 0].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#All positive labels\nX[X[\"TARGET\"] == 1].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pre-processing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# All columns that have just 1 large negative value in an otherwise positive set\ndesc = X.describe()\nneg_outs = [x for x in desc.columns if (desc.loc[\"min\", x] < 0) and (desc.loc['25%', x] >= 0)]\nprint(len(neg_outs))\nmark_20 = X.shape[0] * .1\nneg_outs = [x for x in neg_outs if (X[X[x] < 0][x].value_counts().shape[0] == 1)]\nindexes = []\nfor column in neg_outs:\n    indexes.extend((X[X[column] < 0]).index.tolist())\nindexes\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For dropping outliers\nX.drop(index=indexes, inplace=True)\nprint(str(len(indexes)) + \" rows dropped\")\n#outliers = IsolationForest().fit_predict(X)\n#X[outliers == -1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#All columns that have binary values\nbinaries = [x for x in X.columns if (X[x].value_counts().shape[0] == 2) & (x != \"TARGET\")]\nbinaries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#All columns that contain only 1 value\nsingle_values = [x for x in X.columns if X[x].value_counts().shape[0] == 1]\nprint(\"Length: \" + str(len(single_values)))\nX.drop(columns=single_values, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot encoding for features that have cardinality of less than 20\nlow_cardinality = [x for x in X.columns if X[x].nunique() < 20]\nlow_cardinality.remove(\"TARGET\")\nprint(\"Low cardinality features: \" + str(len(low_cardinality)))\nX = OneHotEncoder(cols=low_cardinality, return_df=True).fit_transform(X)\nX","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We drop the target column aftere the dataset has been cleaned\nY = X[\"TARGET\"]\nX.drop(columns=[\"TARGET\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model configuration","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\ndef run_model(model, tr_x, tr_y, te_x, te_y, debug=True):\n    model.fit(tr_x, tr_y)\n    preds = model.predict(tr_x)\n    training_error = (abs(tr_y-preds).sum())/tr_x.shape[0] *100\n    preds = model.predict(te_x)\n    error = (abs(te_y-preds).sum())/te_x.shape[0] *100 # MAE\n    \n    if debug:\n        print(\"Pred satisfied: \" + str((preds.sum()/te_x.shape[0])))\n        print(\"Act satisfied:\" + str((te_y.sum()/te_x.shape[0])))\n        print(\"Score: \" + str((te_y.sum()- preds.sum())/te_x.shape[0])) #The difference between the quantity predicted of how many survived\n        print(\"Error: \" + str(error)) # How many incorrect predictions divided by the number of predictions\n        print(\"Accuracy: \" + str(100 - error))\n        print(\"Training accuracy: \" + str(100 - training_error))\n    \ndef cross_validation(model, X, y, debug=True):\n    scores = cross_val_score(model, X=X, y=y, cv= 10)\n    if debug:\n        print(scores)\n    return scores.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\ntrainX, valX, trainY, valY = train_test_split(X, Y)\n\n# Create synthetic values only from training data\nsynthX, synthY= SMOTE(sampling_strategy={\"Satisfied\":6000}).fit_resample(trainX, trainY.map({1: \"Satisfied\", 0: \"NotSatisfied\"}))\nsynthY = synthY.map({\"Satisfied\" : 1, \"NotSatisfied\" : 0}) # We have to remap the values of Y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators = 10, random_state=101)\nrun_model(rfc, trainX, trainY, valX, valY)\nprint(\"\\nWith synthetic data:\")\nrun_model(rfc, synthX, synthY, valX, valY)\ncross_validation(rfc, X, Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scores for Random Forest Classifier\n### Without cleaning the data an variating n_estimators\n\n| n_estimators | Training accuracy | Crossval mean score |\n| :- | :- | :- |\n| 5 | 98.831886 | 95.046040 |\n| 10 | 98.863456 | 95.381478 |\n| 12 | 98.972200| 95.380163 | \n| 25 | 99.412435 | 95.245987 | \n| 50 | 99.528194 | 95.284135 |  \n| 100 | 99.579058 | 95.274927 |  \n\n### Different cleaning processes with n_estimators = 10\n| Dropped | Training accuracy | Crossval mean score |\n| :- | :- | :- |\n| No | 98.863456 | 95.381478 |\n| Negative single outliers | 98.852755 | 95.323679 |\n| Columns with single values | 98.838902 | 95.360431 |\n| One hot encoding | 98.826624 | 95.343330|\n| All of the above | 98.815861 | 95.390881|","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBClassifier(n_estimators = 10, random_state= 101)\nrun_model(xgb, trainX, trainY, valX, valY)\nprint(\"\\nWith synthetic data:\")\nrun_model(xgb, synthX, synthY, valX, valY)\ncross_validation(xgb, X, Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Scores XGBoost Classifier\n\n### Without cleaning the data an variating n_estimators\n| n_estimators | Training accuracy | Crossval mean score |\n| :- | :- | :- |\n| 5 | 96.067701 | 96.036569 | \n| 10 | 96.044900 | 96.185214 | \n| 12 | 96.086994| 96.041831 | (Same as below for some reason) \n| 25 | 96.164167 | 96.041831 | \n| 50 | 96.293957 | 96.024730 | \n| 100 | 96.555292 | 96.008945 | \n\n### Different cleaning processes with n_estimators = 10\n| Dropped | Training accuracy | Crossval mean score |\n| :- | :- | :- |\n| No | 96.044900 | 96.185214 | \n| Negative single outliers | 96.090936 | 96.036525 |\n| Columns with single values | 96.078225 | 95.040515|\n| One hot encoding | 96.148382 | 95.040515 |\n| All of the above | 96.054041 | 95.035207 |","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"perm_rfc = PermutationImportance(rfc, random_state=2).fit(valX, valY)\neli5.show_weights(perm_rfc, feature_names=valX.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"keep_columns = [\"var15\", \"num_var5_0\", \"saldo_medio_var8_ult3\", \"num_op_var41_efect_ult3\", \"ind_var9_ult1\", \"ID\", \"num_var42\", \"saldo_var37\", \n                \"num_var40_0\", \"saldo_var8\", \"ind_var25_cte\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocessor = ColumnTransformer(\n    transformers=[\n    (\"Important\", \"passthrough\", keep_columns)\n    ],\n    remainder=\"drop\")\n\npipeline = Pipeline(\n    steps=[\n        (\"prep\", preprocessor),\n        (\"RandomForest\", rfc)\n    ])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"binaries","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import itertools\n\nimport matplotlib.pyplot as plt\n\nnonbinaries = X.drop(columns=binaries).columns\nfor tupla in itertools.combinations(nonbinaries, 2):\n    sns.scatterplot(x=X[tupla[0]], y=X[tupla[1]], hue=X[binaries[0]])\n    plt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[\"ind_var2\"].value_counts().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(indexes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}