{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"def1bb57-0a28-7c30-e60e-8b05016b2889"},"source":"XGB- Feature selection and Parameter Tuning"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"609fc3e7-7eff-8f6e-a54e-f43c14e0309c"},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2cd7f16e-5f20-4f59-cf12-548084800ef5"},"outputs":[],"source":"# Load the packages for modeling\nfrom sklearn.grid_search import GridSearchCV\nimport xgboost as xgb\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e6117cca-7bbc-a00a-fed2-87bd83564416"},"outputs":[],"source":"# Load the datasets\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4048d661-ab99-9b24-b524-f482e401a451"},"outputs":[],"source":"# Seperate out predictors and target from the training data set\n# Remove the ID field from the test dataset and save it.\n# Drop the ID field from the training set\ntrain_y = train['TARGET']\ntrain.drop(['ID', 'TARGET'], axis=1, inplace=True)\ntrain_x = train\ntest_id = test['ID']\ndel test['ID']"},{"cell_type":"markdown","metadata":{"_cell_guid":"ef8b47d7-ddad-a7c1-b770-ec5f6f23aa41"},"source":"### Missing value imputation \n### Remove duplicate and constant column"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6c828ce1-78e7-6000-db9d-3f27a22db69f"},"outputs":[],"source":"# Fixing the outliers in column 'var3'\ntrain_x['var3'].replace(-999999,0, inplace=True)\ntest['var3'].replace(-999999,0, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"78e7b6ac-56ea-f44b-3b02-7d630dbd634c"},"outputs":[],"source":"# Remove all the columns which have constant values. \n# These columns have zero std deviation.\nrm_col=[] \nfor col in train_x.columns:\n    if train_x[col].std()==0:\n        rm_col.append(col)\n\ntrain_x.drop(rm_col, axis=1, inplace=True)\ntest.drop(rm_col, axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4c4631c2-28c6-d92d-4f22-89ab13e4150d"},"outputs":[],"source":"# Remove the duplicate columns. \n# Here we have columns with different name but exactly same values for each rows\n# We will compare all pairs of columns\ndups_col = []\nfor ii in range(len(train_x.columns)-1):\n    for jj in range(ii+1,len(train_x.columns)):\n        col1=train_x.columns[ii]\n        col2=train_x.columns[jj]\n        # take the columns as arrays adn then compare the values.\n        if np.array_equal(train_x[col1].values, train_x[col2].values) and not col2 in dups_col:\n            dups_col.append(col2)\n\ntrain_x.drop(dups_col, axis=1, inplace=True)\ntest.drop(dups_col, axis=1, inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"1edae830-1fae-3cad-b5ff-9189bdc5a5f2"},"source":"### Feature selection using XGBoost classifier\n#### We will leverage the feature importance attribute of the XGBoost  classifier to find top 50 features."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"63c2eda3-0fe7-17fa-acdd-5c88fc161564"},"outputs":[],"source":"# Define XGBoost classifier with some standard parameter settings\nxgb_clf = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5, min_child_weight=1,\n                           gamma=0, subsample=0.8, colsample_bytree=0.8, objective='binary:logistic',\n                           nthread=4,seed=10)\n\n# Learn the model with training data\nxgb_clf.fit(train_x,train_y)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"777623cd-66c5-5519-69ef-b1559a5e8845"},"outputs":[],"source":"# Plot the top 50 important features\nimp_feat_xgb=pd.Series(xgb_clf.feature_importances_, index=train_x.columns).sort_values(ascending=False)\nimp_feat_xgb[:50].plot(kind='bar',title='Top 50 Important features as per XGBoost', figsize=(12,8))\nplt.ylabel('Feature Importance Score')\nplt.subplots_adjust(bottom=0.25)\nplt.savefig('FeatureImportance.png')\nplt.show()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0f9fa5bb-dd98-3e78-49f2-58e572f618e3"},"outputs":[],"source":"# Save indexes of the important features in descending order of their importance\nindices = np.argsort(xgb_clf.feature_importances_)[::-1]\n\n# list the names of the names of top 50 selected features adn remove the unicode\nselect_feat =[str(s) for s in train_x.columns[indices][:50]]\n\n# Make the subsets with 50 features only\ntrain_x_sub = train_x[select_feat]\ntest_sub = test[select_feat]"},{"cell_type":"markdown","metadata":{"_cell_guid":"ce8091b3-1827-8d01-2895-a49e013c4aaf"},"source":"## Parameter Tuning\n#### We will use GridSearch package with cross validation to find best \n#### setting of the parameters from a range of values\n\n#### We will tune the parameters in multiple rounds. At each round, we we will take 1 or 2 parameters,\n#### find their best values and set them in next step for tuning different set of parameters."},{"cell_type":"markdown","metadata":{"_cell_guid":"a77e837b-e8cb-e51c-4924-68a241fabb7c"},"source":"### Round 1: Tune max_depth and min_child_weight"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"bab46f37-c418-8fdf-830c-c2d808e392b9"},"outputs":[],"source":"# Define a new XGBoost Classifier with default parameters\nselect_xgb_clf = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100, seed=10)\n\n# Set a list of parameters\nparam_grid = {\n    \n            'max_depth':[3,4,5],\n            'min_child_weight':[3,4,5]\n}\ngrid_clf = GridSearchCV(select_xgb_clf,param_grid,cv=5,scoring='roc_auc' )\n\n# Train the model\ngrid_clf.fit(train_x_sub,train_y)\ngrid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_\n\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"2af81c6c-e81c-0da6-ef9f-827c0a4b5821"},"source":"### Round 2: Tune subsample and colsample_bytree"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9aeea74c-84fc-4ff9-effe-ad49271f0f8c"},"outputs":[],"source":"# Define a new XGBoost Classifier setting the best value for the above parameter and \n# default for the rest\nselect_xgb_clf = xgb.XGBClassifier(learning_rate=0.1,n_estimators=100, max_depth= 5, \n                                   min_child_weight= 5,seed=10)\n\n# Set a list of parameters\nparam_grid = {\n    \n            'subsample':[0.6,0.7,0.8,0.9],\n            'colsample_bytree':[0.6,0.7,0.8,0.9]\n}\ngrid_clf = GridSearchCV(select_xgb_clf,param_grid,cv=5, scoring='roc_auc')\n\n# Train the model\ngrid_clf.fit(train_x_sub,train_y)\ngrid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"9e3ab7e2-6f82-90eb-9eda-6c82dec4bb6d"},"source":"### Round 3:  Tune reg_alpha"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a7d8f003-dd06-ddbe-0ed0-9245f4c24264"},"outputs":[],"source":"# Define a new XGBoost Classifier with parameters setting so far.\nselect_xgb_clf = xgb.XGBClassifier(learning_rate=0.1,n_estimators=100, max_depth= 5, min_child_weight= 5,\n                                   gamma=0,subsample=0.7, colsample_bytree=0.7, seed=10)\n\n# Set a list of parameters\nparam_grid = {\n    \n         'reg_alpha':[0.001, 0.005, 0.01, 0.05]   \n}\ngrid_clf = GridSearchCV(select_xgb_clf,param_grid,cv=5, scoring='roc_auc')\n\n# Train the model\ngrid_clf.fit(train_x_sub,train_y)\ngrid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"4d084198-1033-bd32-846c-0f7305c033df"},"source":"### Round 4: Tune learning_rate\n#### We will take values on both sides of the default learning rate (0.1)\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"67d07b3c-b7e2-b8c3-dcc8-df423d5c219b"},"source":"#### I have run the below code multiple times with best the learning rate and changing the n_estimators. \n#### It seems 75 gives me the best score so far."},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9c9b6613-85c5-d159-53f8-b91c031d314a"},"outputs":[],"source":"# Define a new XGBoost Classifier.\nselect_xgb_clf = xgb.XGBClassifier(n_estimators=75, max_depth= 5, min_child_weight= 5,gamma=0,\n                                   reg_alpha= 0.01,subsample=0.7, colsample_bytree=0.7, seed=10)\n\n# Set a list of parameters\nparam_grid = {\n    \n         'learning_rate':[0.05,0.08, 0.1, 0.15]   \n}\ngrid_clf = GridSearchCV(select_xgb_clf,param_grid,cv=5, scoring='roc_auc')\n\n# Train the model\ngrid_clf.fit(train_x_sub,train_y)\ngrid_clf.grid_scores_, grid_clf.best_params_, grid_clf.best_score_\n"},{"cell_type":"markdown","metadata":{"_cell_guid":"87b2cec1-efc5-1fac-4f8f-380955d0c863"},"source":"### Predict with the best model"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0265d0e0-f4fa-ffd6-9e64-df6a2501e234"},"outputs":[],"source":"# Take teh best model from the grid search\nbest_xgb_clf = grid_clf.best_estimator_\ngrid_clf.best_estimator_"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a22566af-b219-ba76-2cf2-36d5b0bb73a6"},"outputs":[],"source":"# Make prediction with test data\npredicted_proba = best_xgb_clf.predict_proba(test_sub)\n\n# Save the prediction in CSV file\n# predicted_proba has probabilities for each Target class for each observation.\n# We are concerned about probability of class 1 and hence taking predicted_proba[:,1]\nsubmission = pd.DataFrame({'ID':test_id,'TARGET':predicted_proba[:,1]})\nsubmission.to_csv('submission.csv', index=False)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"598c3784-6759-f7f0-b96d-a1a7f992d3b4"},"outputs":[],"source":""}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}