{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn import cross_validation\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\n\ntraining = pd.read_csv(\"../input/train.csv\", index_col=0)\ntest = pd.read_csv(\"../input/test.csv\", index_col=0)\n\nprint(training.shape)\nprint(test.shape)\n\n# Replace -999999 in var3 column with most common value 2 \n# See https://www.kaggle.com/cast42/santander-customer-satisfaction/debugging-var3-999999\n# for details\ntraining = training.replace(-999999,np.nan)\ntraining=training.replace(9999999999,np.nan)\n\nX = training.iloc[:,:-1]\ny = training.TARGET\n\n\n#X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, stratify=y, test_size=0.3)\n\n# xgboost parameter tuning with p = 75\n# recipe: https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/forums/t/19083/best-practices-for-parameter-tuning-on-models/108783#post108783\n\nratio = float(np.sum(y == 1)) / np.sum(y==0)\n# Initial parameters for the parameter exploration\n# clf = xgb.XGBClassifier(missing=9999999999,\n#                 max_depth = 10,\n#                 n_estimators=1000,\n#                 learning_rate=0.1, \n#                 nthread=4,\n#                 subsample=1.0,\n#                 colsample_bytree=0.5,\n#                 min_child_weight = 5,\n#                 scale_pos_weight = ratio,\n#                 seed=4242)\n\n# gives : validation_1-auc:0.845644\n# max_depth=8 -> validation_1-auc:0.846341\n# max_depth=6 -> validation_1-auc:0.845738\n# max_depth=7 -> validation_1-auc:0.846504\n# subsample=0.8 -> validation_1-auc:0.844440\n# subsample=0.9 -> validation_1-auc:0.844746\n# subsample=1.0,  min_child_weight=8 -> validation_1-auc:0.843393\n# min_child_weight=3 -> validation_1-auc:0.848534\n# min_child_weight=1 -> validation_1-auc:0.846311\n# min_child_weight=4 -> validation_1-auc:0.847994\n# min_child_weight=2 -> validation_1-auc:0.847934\n# min_child_weight=3, colsample_bytree=0.3 -> validation_1-auc:0.847498\n# colsample_bytree=0.7 -> validation_1-auc:0.846984\n# colsample_bytree=0.6 -> validation_1-auc:0.847856\n# colsample_bytree=0.5, learning_rate=0.05 -> validation_1-auc:0.847347\n# max_depth=8 -> validation_1-auc:0.847352\n# learning_rate = 0.07 -> validation_1-auc:0.847432\n# learning_rate = 0.2 -> validation_1-auc:0.846444\n# learning_rate = 0.15 -> validation_1-auc:0.846889\n# learning_rate = 0.09 -> validation_1-auc:0.846680\n# learning_rate = 0.1 -> validation_1-auc:0.847432\n# max_depth=7 -> validation_1-auc:0.848534\n# learning_rate = 0.05 -> validation_1-auc:0.847347\n# \n\n\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, stratify=y, test_size=0.3)\n\nxgbooster = xgb.XGBClassifier(missing=np.nan,\n                max_depth = 5,\n                n_estimators=1000,\n                learning_rate=0.1, \n                nthread=4,\n                subsample=1.0,\n                colsample_bytree=0.5,\n                min_child_weight = 3,\n                scale_pos_weight = ratio,\n                reg_alpha=0.03,\n                seed=np.random.randint(1000000))\n\nxgbooster.fit(X_train, y_train, early_stopping_rounds=1000, eval_metric=\"auc\",\neval_set=[(X_test, y_test)])\n  \n\n\"\"\"\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(15,10))\nplt.plot(iterations,scorestrain,'r')\nplt.plot(iterations,scorescv, 'b')\nplt.ylim(0.8,0.9)\nplt.xlim(1000,55000)\nplt.xlabel('# training examples')\nplt.ylabel('AUC')\nplt.legend(['Training Set','CV set'],loc='lower right')\n\n\n\"\"\"\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"fs = xgbooster.booster().get_fscore()\nfeatures=pd.Series(fs).sort_values(ascending=False).index\nprint(features)\nprint(len(features))"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"nfeatures=[]\nscorecv=[]\n  \n\nfor i in range(5,86,5):\n    print(\"Training using top %d features\" %i)\n    \n    xgbooster.fit(X_train[features].iloc[:,:i], y_train, early_stopping_rounds=100, eval_metric=\"auc\",\neval_set=[(X_test[features].iloc[:,:i], y_test)])\n    nfeatures.append(i)\n    scorecv.append(xgbooster._Booster.best_score)\n   "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"import matplotlib.pyplot as plt\n\nplt.style.use('seaborn-deep')\nplt.figure(figsize=(10,4))\nplt.plot(nfeatures,scorecv,lw=2)\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"AUC on CV set\")\nplt.xlim([0,85])\nplt.ylim([0.83,0.846])\nplt.title(\"AUC score as a function of the number of features\")\nplt.grid()\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"\n         \n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}