{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this file we will perform CNN on the Santander File.\n\nFeature Selection , inversing and transposing the file to achieve a better accuracy","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import the data\ntrain = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')\ntest = pd.read_csv('/kaggle/input/santander-customer-satisfaction/test.csv')\n\n# check the shape\ntrain.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()\n# test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our training data contains 'Target' Column extra ,which is our dependent variable and testing data contains only the independent variables.\n\nAlso the Column ID is of no relevance so will drop it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dividing the dataframe into dependent and independent variable\nX=train.drop(['ID','TARGET'], axis=1)\ny=train['TARGET']\ntest=test.drop(['ID'], axis=1)\n\n# check the shape again\nX.shape, y.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we will seperate the data into training and testing dataset\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=99)\n\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will perform Feature Selection Methods hers.\n\nRemoving Constant, Quasi Constant and Duplicate Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Constant and Quasi Constant\nfilter=VarianceThreshold(0.01)\n# this will be used to remove the data which has low variance of 1% or below","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=filter.fit_transform(X_train)\nX_test=filter.transform(X_test)\ntest=filter.transform(test)\n# now check the shape again\nX_train.shape, X_test.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that earlier we had 369 features, now our features are 268.\n\nNow we will remove the duplicate features as well. We will remove the duplicates by transposing the data , converting the columns into rows and rows into columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_T=X_train.T\nX_test_T=X_test.T\ntest_T=test.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting the transposed value into a dataframe\nX_train_T=pd.DataFrame(X_train_T)\nX_test_T=pd.DataFrame(X_test_T)\ntest_T=pd.DataFrame(test_T)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_T\nX_test_T\ntest_T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will check how many features are duplicate","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_T.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_T.duplicated().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Store the duplicates into a variable","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"duplicated=X_train_T.duplicated()\nduplicated","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here all the values which are True are the duplicate ones,\nWe will perform inversion on the values by changing True to False, and False to True.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# perform inversion\nfeatures_to_keep=[not index for index in duplicated]\nfeatures_to_keep","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will remove the duplicates and keep only the unique features and also we will transpose the dataframe again into its original shape","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train=X_train_T[features_to_keep].T\nprint(X_train.shape)\n\nX_test=X_test_T[features_to_keep].T\nprint(X_test.shape)\n\ntest=test_T[features_to_keep].T\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This preprocessing will help into getting a better accuracy for the model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is variance in the dataset so we will scale the data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# scaler\nscaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\ntest=scaler.transform(test)\n\nX_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check the shape again\nX_train.shape,X_test.shape,test.shape,y_train,y_test,","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we will convert the shape acceptable to our neural network and y_test and y_train into numpy format","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# reshaping\nX_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\nX_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)\ntest=test.reshape(test.shape[0],test.shape[1],1)\n# check the shape again\nX_train.shape, X_test.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# series to numpy\ny_train=y_train.to_numpy()\ny_test=y_test.to_numpy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now our data is completely preprocessed , we will Build a model for it","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# import libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Conv1D,MaxPool1D,BatchNormalization,Dropout\nfrom tensorflow.keras.optimizers import Adam","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model\nmodel=Sequential()\n# layers\nmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(252,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=2))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=2))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=2))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# compiling the model\nmodel.compile(optimizer=Adam(learning_rate=0.00005), loss='binary_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# fittting the model\nhistory=model.fit(X_train,y_train, epochs=10,batch_size=128 ,validation_data=(X_test,y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plotting the data\npd.DataFrame(history.history).plot(figsize=(10,8))\nplt.grid(True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction\ny_pred=model.predict_classes(test)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot confusion matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mat=confusion_matrix(y_test, y_pred[:15204,])\nplot_confusion_matrix(conf_mat=mat, figsize=(7,7))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred.ndim","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As the data in y_pred is 2 dimensional, we will convert the same into 1 dim","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred=np.ravel(y_pred)\ny_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.Series(y_pred,name=\"TARGET\")\nresults","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.concat([pd.Series(range(1,75819),name = \"ID\"),results],axis = 1)\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}