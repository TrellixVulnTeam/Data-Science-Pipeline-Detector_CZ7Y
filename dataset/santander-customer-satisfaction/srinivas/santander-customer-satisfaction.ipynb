{"cells":[{"metadata":{"id":"8eGBbe3i2kow"},"cell_type":"markdown","source":"## Problem Statement - Identify dissatisfied customers for Santander Bank","execution_count":null},{"metadata":{"id":"5OYmsDDl2_0y"},"cell_type":"markdown","source":"From frontline support teams to C-suites, customer satisfaction is a key measure of success. Unhappy customers don't stick around. What's more, unhappy customers rarely voice their dissatisfaction before leaving.\n\nSantander Bank is asking Kagglers to help them identify dissatisfied customers early in their relationship. Doing so would allow Santander to take proactive steps to improve a customer's happiness before it's too late.\n\nIn this competition, you'll work with hundreds of anonymized features to predict if a customer is satisfied or dissatisfied with their banking experience.","execution_count":null},{"metadata":{"id":"SIpKErRl3BTh","trusted":true},"cell_type":"code","source":"# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"id":"4EVCt7fG5q4e"},"cell_type":"markdown","source":"## Data Aanalysis","execution_count":null},{"metadata":{"id":"vg1rO1jS5uPi"},"cell_type":"markdown","source":"\n\n*   Dataset info , datatypes, etc.\n*   Identifying Categorical variables if any \n*   Null value/Missing value check\n*   Target value distribution\n*   Individual feature Analysis\n*   Finding constant features, duplicates features, and approx constant features\n*   Feature variance analysis\n*   Feature correlation analysis\n*   Feature outlier analysis\n\n\n\n","execution_count":null},{"metadata":{"id":"L3Ara6oo5idS","trusted":true},"cell_type":"code","source":"# creating train and test data from csv files\ntrain_data = pd.read_csv('../input/santander-customer-satisfaction/train.csv')\ntest_data = pd.read_csv('../input/santander-customer-satisfaction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"id":"0eKXbSGD6D5D","outputId":"e09682a1-0ae2-4616-9461-bdea3f407d06","trusted":true},"cell_type":"code","source":"# checking the shape of the train dataset\nprint(train_data.shape,test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"8_MoOLmx6UBU","outputId":"42958f57-648f-41ce-b0a8-a6a5bf6f8d13","trusted":true},"cell_type":"code","source":"# checking the 5 values from top in the dataset \ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"Mm_deJko6XHd","outputId":"c8ef3e37-4442-4fe0-f560-3695b828ea66","trusted":true},"cell_type":"code","source":"# It will describe the dataset by giving mean,standard deviation, min,max, top 25,50,75 percent values for each column\ntrain_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"bmjQLhZ0_2_X"},"cell_type":"markdown","source":"From the above data few observations as below:\n \n1.   we can see that var3 feature column having unknown value -999999, need to impute this value.\n2.   var 15 has values ranging from 5 to 105( this feature may represents age).\n3.   var38 has min value as 5163.75000 and max value as 22034740.000 based on this range we may assume that it might represent some replationship value between the bank and customer.\n ","execution_count":null},{"metadata":{"id":"AddtwBaN6a0-","outputId":"2247b9f5-5188-4dc6-f7fa-9f5a08bf00f2","trusted":true},"cell_type":"code","source":"# It will get column and its data type\ntrain_data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"id":"k_gojqKi7PH-","outputId":"510ed88a-260b-4883-dceb-f652de13d211","trusted":true},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"id":"oCuJM5Oo7QgM"},"cell_type":"markdown","source":"From the above output, it is clear that we don;t have any categorical features.\nWe have features with dtypes as below:\n*   float 64  :   111 features\n*   int 64    :   260 features\n\n","execution_count":null},{"metadata":{"id":"ln8k2DX16hbk","outputId":"075f96d9-52e5-4f06-c3ed-59d927f52fac","trusted":true},"cell_type":"code","source":"# To check the distribution of data in the dataset\nplt.figure(figsize = (10, 8))\nsns.countplot(x = 'TARGET', data = train_data) # from the ouput we can say that it is an unbalanced data \nplt.show()\nprint('Percentage of happy customers: ',len(train_data[train_data['TARGET']==0])/len(train_data['TARGET'])*100,\"%\")\nprint('Percentage of unhappy customers: ',len(train_data[train_data['TARGET']==1])/len(train_data['TARGET'])*100,\"%\")","execution_count":null,"outputs":[]},{"metadata":{"id":"7UN4z5A0643Q"},"cell_type":"markdown","source":"From the above data it is clear that dataset in unbalanced dataset","execution_count":null},{"metadata":{"id":"OviE_QRATqGp"},"cell_type":"markdown","source":"**Check Missing Value or Null Value**\n","execution_count":null},{"metadata":{"id":"Wm90TYgCTwaN","outputId":"d792999f-47fd-4cb4-8457-38ef593c8cc0","trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"hBoiPwIjUa0G"},"cell_type":"markdown","source":"Observations from above result:\n\n\n1.   We don't have any missing values in the dataset\n\n","execution_count":null},{"metadata":{"id":"YiTmLPkY_Y1y"},"cell_type":"markdown","source":"**Individual Fetaure Analysis**","execution_count":null},{"metadata":{"id":"eS1gR4dA63jI","outputId":"d38d0c31-4d0b-4ffb-dff8-71fb98c18f28","trusted":true},"cell_type":"code","source":"train_data['var3'].describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"QJcoNeFYCpD_","outputId":"4a315c39-5e87-4fe1-9ad6-51d850b801cf","trusted":true},"cell_type":"code","source":"train_data['var3'].hist(bins=14)","execution_count":null,"outputs":[]},{"metadata":{"id":"eM7HmBH3C38Z"},"cell_type":"markdown","source":"Most of the customers (75%) have same value which means it represents a feature which is common among the customers which might be Gender or Country of the customer. ","execution_count":null},{"metadata":{"id":"R6CoGINiDW9v","outputId":"206625aa-389b-4094-f052-00c8ff193779","trusted":true},"cell_type":"code","source":"train_data['var15'].describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"9bilOA7pDeLL","outputId":"000306cb-4f7f-404c-e68d-1d7dbfd8323d","trusted":true},"cell_type":"code","source":"train_data['var15'].hist(by=train_data['TARGET'],bins=50)\nplt.xlabel('age')\nplt.ylabel('no of customers')","execution_count":null,"outputs":[]},{"metadata":{"id":"EzUK0q0wB3OP","outputId":"e0d85045-5438-4f12-80df-6d9ca3c949c1","trusted":true},"cell_type":"code","source":"sns.FacetGrid(train_data, hue=\"TARGET\", size=6) \\\n   .map(plt.hist, \"var15\",edgecolor='w') \\\n   .add_legend()\nplt.xlabel(\"var15(Age)\")\nplt.ylabel(\"No of Customers\")\nplt.title('var 15 impact on Target value')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"CcXprf2qG-qf"},"cell_type":"markdown","source":"Observations from the above plot:\n\n\n1.   Most unhappy customers are in the range of 25-50\n2.   Most Happy customers are in the range of 21-24\n3.   Most customers are in the age range(25-35)(45,000 cutomers)\n\n\n\n","execution_count":null},{"metadata":{"id":"wvGE2rFmDV_p","outputId":"10c73453-c927-49f5-895e-fb51299003a2","trusted":true},"cell_type":"code","source":"train_data['var38'].describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"iDFYQPnoM8hL","outputId":"15001811-0829-493a-ea6e-76803b00af68","trusted":true},"cell_type":"code","source":"train_data['var38'].hist(by=train_data['TARGET'],bins=25)","execution_count":null,"outputs":[]},{"metadata":{"id":"wZ1JHKrDNiTP","trusted":true},"cell_type":"code","source":"import plotly.express as px\nfrom matplotlib import rcParams\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot","execution_count":null,"outputs":[]},{"metadata":{"id":"E5Cl2q6jNxQv","outputId":"779d975f-0466-4fb3-eba3-fc848609e51a","trusted":true},"cell_type":"code","source":"fig = px.bar(train_data, x=\"var15\", y=\"var38\", color=\"TARGET\", barmode=\"group\",labels={0,1},log_y=True,color_discrete_sequence=[\"green\",\"red\"])\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"UruNXYowN3yp","outputId":"a634a809-98bf-4c68-c717-c41b094dbcf1","trusted":true},"cell_type":"code","source":"unhappy = train_data.loc[train_data['TARGET']==1][\"var38\"]\nhappy = train_data.loc[train_data['TARGET']==0][\"var38\"]\nhist_data=[unhappy,happy]\nfig = ff.create_distplot(hist_data,group_labels=['unhappy','happy'],show_hist=False,show_rug=False)\nfig['layout'].update(title='Santander Customer Satisfaction Time Density Plot',xaxis=dict(title='amount',range=[5000,2000000]))\niplot(fig,filename='dist_only')","execution_count":null,"outputs":[]},{"metadata":{"id":"EVepRn18Sp1H"},"cell_type":"markdown","source":"Observations from the above plot:\n\n\n1.   Most of the unhappy customers are having relationship value less than 0.4M\n2.   Customers having relationship value greater than 0.5M have very less unhappy customers\n\n","execution_count":null},{"metadata":{"id":"FdFm9sHDVS0R"},"cell_type":"markdown","source":"**Constant and Duplicate features handling**","execution_count":null},{"metadata":{"id":"DJoBPp5zQrUh","outputId":"b643db48-ce14-4009-faf7-e995d6e8f600","trusted":true},"cell_type":"code","source":"# Removing the data with constant features (i.e zero variance where std=0)\nconstant_features = [\n    features for features in train_data.columns if train_data[features].std() == 0\n]\nlen(constant_features)\ntrain_data[constant_features]","execution_count":null,"outputs":[]},{"metadata":{"id":"CXd8lNYMVe_K","trusted":true},"cell_type":"code","source":"# Drop the constant features from traina and test dataset\ntrain_data.drop(labels = constant_features, axis = 1, inplace=True)\ntest_data.drop(labels = constant_features, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"n03M6hdLVi_R","outputId":"2b170524-0bb4-4573-b7b0-8580fcd18b99","trusted":true},"cell_type":"code","source":"# checking the shape of the dataset after dropping the constant features\nprint(train_data.shape,test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"cTyL-N8xWAg5","outputId":"2d3a4bd5-2868-4753-d317-ab31591d4637","trusted":true},"cell_type":"code","source":"# Find the columns where most of the values are equal( approx to 99.9% )\napprox_constants = []\nfor feature in train_data.columns:\n    approx_value = (train_data[feature].value_counts()/ np.float(\n        len(train_data))).sort_values(ascending=False).values[0]\n    if approx_value > 0.999:\n      approx_constants.append(feature)\nlen(approx_constants)\ntrain_data[approx_constants]","execution_count":null,"outputs":[]},{"metadata":{"id":"m_3bZMw7WHUZ","trusted":true},"cell_type":"code","source":"# Temporary dataframe for approx_constants\ntrain_data_ac=train_data.copy()\ntest_data_ac=test_data.copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"8aVpw5dbWP0v","trusted":true},"cell_type":"code","source":"# Drop the approximate constant features from traina and test dataset\ntrain_data_ac.drop(labels = approx_constants, axis = 1, inplace=True)\ntest_data_ac.drop(labels = approx_constants, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"HUlBoZRkWR0c","outputId":"58694394-ce3a-4f7c-cb1c-88c088d4637c","trusted":true},"cell_type":"code","source":"print(train_data.shape,train_data_ac.shape,test_data.shape,test_data_ac.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"8wFTs0NJWUIL","outputId":"9a2b86f8-ae0e-4e46-914e-59e1ab5bcad0","trusted":true},"cell_type":"code","source":"# Remove duplicate data, columns having same values\nduplicate_features=[]\n# Applying modified sort algorithm , instead of sorting we are creating a features list which mets condition, ignoring other columns\nfor i in range(0, len(train_data.columns)):\n    col_1 = train_data.columns[i]\n    for col_2 in train_data.columns[i + 1:]:\n        if train_data[col_1].equals(train_data[col_2]):\n            duplicate_features.append(col_2)\nlen(duplicate_features)\ntrain_data[duplicate_features]","execution_count":null,"outputs":[]},{"metadata":{"id":"tfwlqEsJWWqE","trusted":true},"cell_type":"code","source":"# Dropping the duplicate features from the dataset as their contribute towards prediction of target is neligible \ntrain_data.drop(labels = duplicate_features, axis = 1, inplace=True)\ntest_data.drop(labels = duplicate_features, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"KLAwz-8CWb6c","trusted":true},"cell_type":"code","source":"# Dropping the duplicate features from the dataset as their contribute towards prediction of target is neligible \nfor feature in duplicate_features:\n  if feature in train_data_ac and feature in test_data_ac:\n    train_data_ac.drop(labels = feature, axis = 1, inplace=True)\n    test_data_ac.drop(labels = feature, axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"id":"EqXcNYoxWfm5","outputId":"b72eda1c-221f-4a0e-c5b9-f9f57c0f5fda","trusted":true},"cell_type":"code","source":"\nprint(train_data.shape,test_data.shape,train_data_ac.shape,test_data_ac.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"2c6aJDe4WqLq"},"cell_type":"markdown","source":"**Feature Variance Analysis**","execution_count":null},{"metadata":{"id":"O9r070cFWiDr","trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import VarianceThreshold\ndef features_wo_low_variance(data):\n  threshold_n=0.98  \n  sel = VarianceThreshold(threshold=(threshold_n* (1 - threshold_n) ))\n  sel_var=sel.fit_transform(data)\n  return data.columns[sel.get_support(indices=True)] ","execution_count":null,"outputs":[]},{"metadata":{"id":"ZdS7BNi0Wuda","outputId":"6d06df35-3be0-48b3-aef7-8b377af7eb88","trusted":true},"cell_type":"code","source":"(train_data.var() < 0.02).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"NK3oDkoNXDo-"},"cell_type":"markdown","source":"We can remove the low variance(such as 0.02) features, as they might not have impact on target value","execution_count":null},{"metadata":{"id":"Nkja4L1DXVaK"},"cell_type":"markdown","source":"**Removing low variance features**","execution_count":null},{"metadata":{"id":"BWnstZOxW2lx","trusted":true},"cell_type":"code","source":"train_data_hv = train_data[features_wo_low_variance(train_data)].copy()\ntest_data_hv = test_data[features_wo_low_variance(train_data)[:-1]].copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"7UxVBKkRXggW","outputId":"fdc8b098-dd8c-4de0-9afb-257159823046","trusted":true},"cell_type":"code","source":"print(train_data_hv.shape,test_data_hv.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZnQ24KXXXi3S","trusted":true},"cell_type":"code","source":"# Dataset without accurate constants\ntrain_data_ac_hv = train_data_ac[features_wo_low_variance(train_data_ac)].copy()\ntest_data_ac_hv = test_data_ac[features_wo_low_variance(train_data_ac)[:-1]].copy()","execution_count":null,"outputs":[]},{"metadata":{"id":"3Ln3IQj8Xnoh","outputId":"c883e9f6-0761-4a07-8f37-c486f8242a97","trusted":true},"cell_type":"code","source":"print(train_data_ac_hv.shape,test_data_ac_hv.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"9AExkFEjX1Vf"},"cell_type":"markdown","source":"**Feature correlation analysis**\n*   As correlation matrix is symmetric, it is reduandant to consider the whole matrix.\n*   Instead we can consider either upper or lower triangle of the correaltion matrix.\n","execution_count":null},{"metadata":{"id":"uzvLU6YZXtGS","trusted":true},"cell_type":"code","source":"def find_correlated_features(data):\n  # Create correlation matrix\n  corr_matrix = data.corr().abs()\n\n  # Select upper triangle of correlation matrix\n  upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n  # Find features with correlation greater than 0.95\n  to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\n  print(len(to_drop))\n  return to_drop","execution_count":null,"outputs":[]},{"metadata":{"id":"htMNS4hBX86x","outputId":"962786cc-fca4-48de-9057-f6a1fd5f325f","trusted":true},"cell_type":"code","source":"plt.figure(figsize = (20,25))\nsns.heatmap(train_data_ac_hv.corr())","execution_count":null,"outputs":[]},{"metadata":{"id":"oY6b0_n4ZTJy","outputId":"1c5e6dc4-939c-4ab7-d54e-4bdb2528ecac","trusted":true},"cell_type":"code","source":"# Drop features having correlation greater than 0.95\ntrain_data_hv_co= train_data_hv.drop(find_correlated_features(train_data_hv), axis=1)\ntest_data_hv_co= test_data_hv.drop(find_correlated_features(train_data_hv), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"mMzvp2q3Zjng","outputId":"783f6cb6-5932-4715-f720-2356dc292d2e","trusted":true},"cell_type":"code","source":"# shape of the train_data after dropping high correlated features\nprint(train_data_hv_co.shape,test_data_hv_co.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"zQ5USQGaZnXN","outputId":"13b36938-d9e8-408d-c36d-264817b6800c","trusted":true},"cell_type":"code","source":"# Drop features having correlation greater than 0.95\ntrain_data_ac_hv_co= train_data_ac_hv.drop(find_correlated_features(train_data_ac_hv), axis=1)\ntest_data_ac_hv_co= test_data_ac_hv.drop(find_correlated_features(train_data_ac_hv), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"v9Pg4PqmZpw-","outputId":"24e3ad23-045b-475f-d8c5-4de11cb5e2ab","trusted":true},"cell_type":"code","source":"# shape of the train_data after dropping high correlated features\nprint(train_data_ac_hv_co.shape,test_data_ac_hv_co.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"B8W-XEYoZ7Cx"},"cell_type":"markdown","source":"**Feature Outlier Analysis**\n","execution_count":null},{"metadata":{"id":"XSkzK8NObn34","outputId":"035800e2-0a1c-4598-9898-f9fd50bfbdcd","trusted":true},"cell_type":"code","source":"for feature in train_data[train_data_ac_hv_co.var().sort_values(ascending=False).index[0:10]]:\n  plt.figure(figsize = (12, 8))\n  data = train_data.copy()\n  if 0 in data[feature].unique():\n    pass\n  else:\n    data[feature]=np.log(data[feature])\n    sns.boxplot(y = feature, x = 'TARGET', data = data)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.yticks()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"XUn641jrZsJw","outputId":"71c1453d-9900-4fc3-9281-1febf758a83d","trusted":true},"cell_type":"code","source":"# Calculating IQR\nQ1 = train_data_ac_hv_co.quantile(0.25)\nQ3 = train_data_ac_hv_co.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","execution_count":null,"outputs":[]},{"metadata":{"id":"_hTX4f_SZ_XC","outputId":"023c9408-1cf9-46ba-aaa1-b7e6eeb0c293","trusted":true},"cell_type":"code","source":"# Finding outliers in the dataset\nprint(train_data_ac_hv_co < (Q1 - 1.5 * IQR)) or (train_data_ac_hv_co > (Q3 + 1.5 * IQR))","execution_count":null,"outputs":[]},{"metadata":{"id":"3A-8ElrjaOIo","trusted":true},"cell_type":"code","source":"# Removing the outliers\ntrain_data_ol=train_data_ac_hv_co.copy()\ntrain_data_out = train_data_ol[((train_data_ol >= (Q1 - 1.5 * IQR)) & (train_data_ol <= (Q3 + 1.5 * IQR))).all(axis=1)]","execution_count":null,"outputs":[]},{"metadata":{"id":"pJjNleqOaZoi","outputId":"61c88446-e335-4d78-deec-1c27cba94f03","trusted":true},"cell_type":"code","source":"train_data_out.shape","execution_count":null,"outputs":[]},{"metadata":{"id":"rmL5FllHadbC","outputId":"59d6b57c-bfbc-4359-ad82-e8dd012928f9","trusted":true},"cell_type":"code","source":"train_data_out[\"TARGET\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"id":"4SfyrQs2akQ3"},"cell_type":"markdown","source":"Observations from outlier analysis:\n*    We are loosing valuable information if we are removing outliers\n*    We can say that outliers are important in predicting the target value\n*    Dataset reduced from (76020,135) to (27125,135) -- loosing valuable information.\n*   So, we are not considering to remove the outliers","execution_count":null},{"metadata":{"id":"i78CrbHNcGM7"},"cell_type":"markdown","source":"**Missing/garbage value treatment**","execution_count":null},{"metadata":{"id":"ivDoz6eeah-M","trusted":true},"cell_type":"code","source":"# Replacing value \"-999999\" in var3 column with most occuring value(75%) 2\ntrain_data.var3 = train_data.var3.replace(-999999,2)","execution_count":null,"outputs":[]},{"metadata":{"id":"_9psOASmcRPh","trusted":true},"cell_type":"code","source":"test_data.var3 = test_data.var3.replace(-999999,2)","execution_count":null,"outputs":[]},{"metadata":{"id":"9qMSqd0-cyVK","trusted":true},"cell_type":"code","source":"test_data_hv_co.var3 = test_data_hv_co.var3.replace(-999999,2)\ntrain_data_hv_co.var3 = train_data_hv_co.var3.replace(-999999,2)","execution_count":null,"outputs":[]},{"metadata":{"id":"ujNwYrSpcTUm","trusted":true},"cell_type":"code","source":"test_data_ac.var3 = test_data_ac.var3.replace(-999999,2)\ntrain_data_ac.var3 = train_data_ac.var3.replace(-999999,2)","execution_count":null,"outputs":[]},{"metadata":{"id":"bngunDrlcVkS","trusted":true},"cell_type":"code","source":"test_data_ac_hv.var3 = test_data_ac_hv.var3.replace(-999999,2)\ntrain_data_ac_hv.var3 = train_data_ac_hv.var3.replace(-999999,2)","execution_count":null,"outputs":[]},{"metadata":{"id":"stX94OELccW3","trusted":true},"cell_type":"code","source":"test_data_ac_hv_co.var3 = test_data_ac_hv_co.var3.replace(-999999,2)\ntrain_data_ac_hv_co.var3 = train_data_ac_hv_co.var3.replace(-999999,2)","execution_count":null,"outputs":[]},{"metadata":{"id":"t6aZPV-Gc9me"},"cell_type":"markdown","source":"## PCA Analysis","execution_count":null},{"metadata":{"id":"TIFa09eMdeo2","trusted":true},"cell_type":"code","source":"# Removing Target and ID columns to scale the data across all columns between -1 to 1\ntrain_data_scaled2= train_data_hv_co.drop([\"ID\",\"TARGET\"],axis=1)\ntrain_data_scaled1= train_data_ac_hv_co.drop([\"ID\",\"TARGET\"],axis=1)\ntrain_data_scaled= train_data.drop([\"ID\",\"TARGET\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"id":"iaNd9IfAeAGN","outputId":"d2438dc6-120a-445f-c8eb-9260d39d06a7","trusted":true},"cell_type":"code","source":"print(train_data_scaled.shape,train_data_scaled1.shape,train_data_scaled2.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"e7iSHUkNeEJ2","trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\ndef find_pca_components(data):\n  pca = PCA().fit(data)\n  plt.rcParams[\"figure.figsize\"] = (12,6)\n\n  fig, ax = plt.subplots()\n  xi = np.arange(1, data.shape[1]+1, step=1)\n  y = np.cumsum(pca.explained_variance_ratio_)\n\n  plt.ylim(0.0,1.1)\n  plt.plot(xi/2, y, marker='o', linestyle='--', color='b')\n\n  plt.xlabel('Number of Components')\n  plt.xticks(np.arange(0, data.shape[1]/2, step=2)) #change from 0-based array index to 1-based human-readable label\n  plt.ylabel('Cumulative variance (%)')\n  plt.title('The number of components needed to explain variance')\n  plt.axhline(y=0.98, color='r', linestyle='-')\n  plt.text(0.7, 0.85, '98% cut-off threshold', color = 'red', fontsize=16)\n  print(\"Pca component prediciton:\")\n  ax.grid(axis='x')\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"DSVSocOIe0pm","outputId":"5376cb98-766b-40f7-ef3c-241704e51676","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfor data in [train_data_scaled,train_data_scaled1,train_data_scaled2]:\n  scaler = StandardScaler()\n  find_pca_components(scaler.fit_transform(data))","execution_count":null,"outputs":[]},{"metadata":{"id":"b5D9NHUsgT9q"},"cell_type":"markdown","source":"From the above visualization, we can say that we need to have 48,44, and 60 pca components for each dataset respectively","execution_count":null},{"metadata":{"id":"Q5wR-ZjOfVMS","trusted":true},"cell_type":"code","source":"def pca_analysis(n_co,data):\n  pca = PCA(n_components=n_co)\n  data_transformed = pca.fit_transform(data)\n  print(\"PCA Analysis for %s pca components\"%(n_co))\n  print(\"Eigen vector for each principal component : \",pca.components_)\n  print(\"Amount of variance by each PCA : \", pca.explained_variance_)\n  print(\"Percentage of variance by each PCA : \", pca.explained_variance_ratio_)\n  print(\"number of features in training data : \", pca.n_features_)\n  print(\"number of samples in training data: \", pca.n_samples_)\n  print(\"noise variance of the data : \",pca.noise_variance_)\n  return pd.DataFrame(data_transformed)","execution_count":null,"outputs":[]},{"metadata":{"id":"nJbBQHBpgiU6","outputId":"70779d57-7719-417a-c00c-3b2453375831","trusted":true},"cell_type":"code","source":"data_transformed=[]\nfor no, data in [(48,train_data_scaled),(44,train_data_scaled1),(60,train_data_scaled2)]:\n  scaler = StandardScaler()\n  data_transformed.append(pca_analysis(no,scaler.fit_transform(data)))","execution_count":null,"outputs":[]},{"metadata":{"id":"-d10uJEci1Sm"},"cell_type":"markdown","source":"## Upsampling the data\n\nAs data is very unbalanced dataset, we will try to upsample the data with minority target","execution_count":null},{"metadata":{"id":"b26Cp0xCjGuo","trusted":true},"cell_type":"code","source":"from sklearn.utils import resample\ndef upsampling_dataset(data):\n  data_majority=data[data.TARGET==0] \n  data_minority=data[data.TARGET==1]  \n\n  data_minority_upsampled=resample(data_minority,replace=True,n_samples=73012)\n  data_upsampled=pd.concat([data_minority_upsampled,data_majority])\n\n  data_upsampled.info()\n  print(data_upsampled['TARGET'].value_counts())\n  return data_upsampled","execution_count":null,"outputs":[]},{"metadata":{"id":"5isrMQ_JjNbY","trusted":true},"cell_type":"code","source":"datasets = [train_data_ac,train_data_ac_hv_co,train_data_hv_co]","execution_count":null,"outputs":[]},{"metadata":{"id":"cnYYUePh5hU0","trusted":true},"cell_type":"code","source":"test_datasets = [test_data_ac,test_data_ac_hv_co,test_data_hv_co]","execution_count":null,"outputs":[]},{"metadata":{"id":"bVxhP48-jf9h","outputId":"90aa4fd8-73f0-473f-ab87-eb0e148669d3","trusted":true},"cell_type":"code","source":"upsampled_data = []\nfor data in datasets:\n  upsampled_data.append(upsampling_dataset(data))","execution_count":null,"outputs":[]},{"metadata":{"id":"CeEcoBnimlFB"},"cell_type":"markdown","source":"## Feature Selection\n","execution_count":null},{"metadata":{"id":"Q6WFxQawmj1E","trusted":true},"cell_type":"code","source":"if 'ID' not in data_transformed[2]:\n    data_transformed[2].insert(1,'ID',train_data['ID'])\nx_pca = data_transformed[2]\ny_pca = train_data[\"TARGET\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"k496UZY_nCOz","trusted":true},"cell_type":"code","source":"x_upsample = upsampled_data[2].drop(\"TARGET\",axis=1)\ny_upsample = upsampled_data[2][\"TARGET\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"39wWVG6jnG_K","trusted":true},"cell_type":"code","source":"x = datasets[2].drop(\"TARGET\",axis=1)\ny = datasets[2][\"TARGET\"]","execution_count":null,"outputs":[]},{"metadata":{"id":"6dyCPwRJnMNz","outputId":"87ce3001-fb92-429c-9c38-3bbe7d7a1fcc","trusted":true},"cell_type":"code","source":"print(x_pca.shape,y_pca.shape,x_upsample.shape,y_upsample.shape,x.shape,y.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZSwXhMw5nMtk","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=44)\nX_pca_train, X_pca_test, y_pca_train, y_pca_test = train_test_split(x_pca, y_pca, test_size=0.30, random_state=44)\nX_upsample_train, X_upsample_test, y_upsample_train, y_upsample_test = train_test_split(x_upsample, y_upsample, test_size=0.30, random_state=44)","execution_count":null,"outputs":[]},{"metadata":{"id":"AU2F8p-QnNRf","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel","execution_count":null,"outputs":[]},{"metadata":{"id":"SAmexMt4nL5r","outputId":"aff5440c-af36-4b5c-a751-94f84e254e70","trusted":true},"cell_type":"code","source":"sel_ = SelectFromModel(RandomForestClassifier(n_estimators=200))\nsel_.fit(X_train.fillna(0), y_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"5sGP_iR6nrOp","outputId":"bd24750b-52fe-4303-d17d-7b337721dea9","trusted":true},"cell_type":"code","source":"selected_features = X_train.columns[(sel_.get_support())]\nlen(selected_features)","execution_count":null,"outputs":[]},{"metadata":{"id":"Y3FJ1NdmnwfF","outputId":"97f9beb6-d2b4-4e26-cb9c-fb45112dafcf","trusted":true},"cell_type":"code","source":"train_data[selected_features]","execution_count":null,"outputs":[]},{"metadata":{"id":"7Q7Pxr4_hZCN"},"cell_type":"markdown","source":"## Model Building and Analysis\n\n*  We have created different datasets such as : original dataset, dataset without constant and duplicate features, dataset without constant,duplicate features,low variance,and correlated features and their PCA transformed data, upsampled datasets\n*  We will use above datasets to train the model using below algorithms\n1.   Logistic regression\n2.   Random Forest\n3.   Decision Tree\n4.   Bagging classifier\n5.   Support vector classifier\n6.   Gradient Boosting classifier\n7.   kNearest neighbors classifier\n\n\n","execution_count":null},{"metadata":{"id":"P3d3EHC2gpbn","trusted":true},"cell_type":"code","source":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,  BaggingClassifier,GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"id":"nczZFUxDkPpu","trusted":true},"cell_type":"code","source":"# Model preparation\nmodels = []\nmodels.append(('LR', LogisticRegression(class_weight='balanced')))\nmodels.append(('Bagging Classifier',BaggingClassifier()))\nmodels.append(('KNN', KNeighborsClassifier(weights='distance')))\nmodels.append(('RandomForest', RandomForestClassifier(class_weight='balanced')))\nmodels.append(('DecisionTree', DecisionTreeClassifier(class_weight='balanced')))\nmodels.append(('GradientBoosting', GradientBoostingClassifier()))\nmodels.append(('xgb', XGBClassifier(missing=np.nan, max_depth=6, \nn_estimators=350, learning_rate=0.025, nthread=4, subsample=0.95,\ncolsample_bytree=0.85, seed=4242)))","execution_count":null,"outputs":[]},{"metadata":{"id":"-JNZ8HLTkpbg","trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.metrics import f1_score,roc_curve,roc_auc_score,precision_score,recall_score,accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"id":"G9IsqcSMlZcf","trusted":true},"cell_type":"code","source":"def model_comparison_plot(model_metrics):\n  plt.figure(figsize = (12,4))\n  sns.heatmap(model_metrics, annot=True, cmap=sns.light_palette((210, 90, 60), input=\"husl\"),linewidth=2)\n  plt.title('Metrics comparison for diff models')\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"qEJNPcVamBff","trusted":true},"cell_type":"code","source":"def plot_roc_curve(y_test, prob_dict):\n  sns.set_style('whitegrid')\n  plt.figure()\n  i=0\n  fig, ax = plt.subplots(4,2,figsize=(16,30))\n  for key,prob in prob_dict.items():\n    fpr, tpr, thresholds = metrics.roc_curve( y_test, prob,\n                                                  drop_intermediate = False )\n    roc_auc = metrics.roc_auc_score( y_test, prob)\n    i+= 1\n    plt.subplot(4,2,i)\n    plt.plot( fpr, tpr, color='red',label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.axis('tight')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(key)\n  plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"ypiGm3HZlder","trusted":true},"cell_type":"code","source":"def model_analysis(title,x_train,y_train,x_test,y_test):\n  df_scores=pd.DataFrame()\n  pred_dict={}\n  for name,model in models:\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    pred_dict[name] = y_pred\n    confusion = confusion_matrix(y_test,y_pred)\n    TP = confusion[1, 1]\n    TN = confusion[0, 0]\n    FP = confusion[0, 1]\n    FN = confusion[1, 0]\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    error = 1-accuracy\n    sensitivity = TP / float(FN + TP)\n    specificity = TN / (TN + FP)\n    False_positive_rate = 1-specificity\n    precision = TP / float(TP + FP)\n    bal_acc = metrics.balanced_accuracy_score(y_test, y_pred)\n    Null_accuracy = max(y_test.mean(), (1 - y_test.mean()))\n    f1 = metrics.f1_score(y_test,y_pred)\n    auc_score = metrics.roc_auc_score(y_test,y_pred)\n    clf_score = pd.DataFrame(\n        {name: [accuracy, bal_acc, Null_accuracy,precision,sensitivity,f1,error,specificity,auc_score]},\n        index=['Accuracy', 'Balanced accuracy','Null_accuracy','precision','recall','f1 score','error','specificity','auc_score']\n    )\n   \n    df_scores = pd.concat([df_scores, clf_score], axis=1).round(decimals=3)\n  print(\"Roc_curve for all models\")\n  plot_roc_curve(y_test,pred_dict)\n  print(title,end='\\n\\n')\n  print(df_scores.to_markdown(),end='\\n\\n')\n  model_comparison_plot(df_scores)","execution_count":null,"outputs":[]},{"metadata":{"id":"G-6wVXnSmIbN","outputId":"43f8f477-c346-46f7-f9e0-e5664c9caa82","trusted":true},"cell_type":"code","source":"model_analysis(\"Model with normal data\",X_train,y_train,X_test,y_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"huBC58I2pf9W","outputId":"364d6a16-dcd8-4a09-955b-28fad8c528ea","trusted":true},"cell_type":"code","source":"model_analysis(\"Model with upsampled data\",X_upsample_train,y_upsample_train,X_upsample_test,y_upsample_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"N77IZ9QcrkDl","outputId":"c0e1ba59-7410-44d4-bc26-cdfb1ffca327","trusted":true},"cell_type":"code","source":"model_analysis(\"Model with pca data\",X_pca_train,y_pca_train,X_pca_test,y_pca_test)","execution_count":null,"outputs":[]},{"metadata":{"id":"B77e86Yvy9-t"},"cell_type":"markdown","source":"Observations from above model results:\n*   Among all the datasets, upsampled dataset is performing good acorss all models except logistic regression.\n*   Bagging classifier, Decision Tree, Random Forest, K nearest neighboirs, and XGB classifier performing well across all metrics.\n*   Bagging classifier and Decision Tree seems to overfit the model","execution_count":null},{"metadata":{"id":"3AjzAW55y8y0"},"cell_type":"markdown","source":"## Predicting the probability of every customer is unhappy","execution_count":null},{"metadata":{"id":"SkSFwiuqxB4V","trusted":true},"cell_type":"code","source":"final_model = RandomForestClassifier(class_weight='balanced',random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"id":"uiRvX6yI4tSr","outputId":"666c12b4-45d7-4f22-c4da-fe8b86de0112","trusted":true},"cell_type":"code","source":"final_model.fit(X_upsample_train,y_upsample_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"Mn56KncZ4_qm","trusted":true},"cell_type":"code","source":"probs = final_model.predict_proba(test_data_hv_co)","execution_count":null,"outputs":[]},{"metadata":{"id":"GVh6aSho54de","trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"ID\":test_data_hv_co.ID, \"TARGET\": probs[:,1]})","execution_count":null,"outputs":[]},{"metadata":{"id":"IYZzdYNh59Xz","trusted":true},"cell_type":"code","source":"#submission.to_csv(\"santander_solution.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"ZrXXlef86gcQ","outputId":"fdbcea0d-7ca5-463c-9d66-d400ff46c286","trusted":true},"cell_type":"code","source":"#submission","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Getting 0.783 score with randomsearch algorithm. So I will try XGBClassifier now to improve the model score","execution_count":null},{"metadata":{"id":"97XSPgZR_ikI","outputId":"33dfdd72-774f-43f6-a066-cafca4354f0f","trusted":true},"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nxgb_classifier = XGBClassifier(missing=np.nan, max_depth=6, \nn_estimators=350, learning_rate=0.025, nthread=4, subsample=0.95,\ncolsample_bytree=0.85, seed=4242)\nxgb_mdl = CalibratedClassifierCV(xgb_classifier, method='isotonic', cv=10)\nxgb_mdl.fit(X_upsample_train,y_upsample_train)","execution_count":null,"outputs":[]},{"metadata":{"id":"j-ifGnmWFVz7","trusted":true},"cell_type":"code","source":"probs_xgb = xgb_mdl.predict_proba(test_data_hv_co)","execution_count":null,"outputs":[]},{"metadata":{"id":"USCDq3zzG4p3","trusted":true},"cell_type":"code","source":"submission1 = pd.DataFrame({\"ID\":test_data_hv_co.ID, \"TARGET\": probs_xgb[:,1]})","execution_count":null,"outputs":[]},{"metadata":{"id":"wtXJWHzCNNHM","trusted":true},"cell_type":"code","source":"#submission1.to_csv(\"submission_xgb.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ezl9KWXtNVgM","trusted":true},"cell_type":"code","source":"submission1.to_csv(\"/kaggle/working/submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}