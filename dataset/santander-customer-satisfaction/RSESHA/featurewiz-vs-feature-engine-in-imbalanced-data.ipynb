{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Feature Selection Tool Comparison **\nWhich feature selection tool is better? Let's compare two of the top tools in Python\n1. Feature-Engine has 820 stars and 765K downloads (as of March 2022)\n1. Featurewiz has 234 stars on Github and has 226K downloads (as of March 2022)\n<a href=\"https://ibb.co/PmxS6SW\"><img src=\"https://i.ibb.co/ZLdZMZg/featurewiz-logos.png\" alt=\"featurewiz-logos\" border=\"0\"></a><br /><a target='_blank' href='https://imgbb.com/'>sun images free</a><br />","metadata":{}},{"cell_type":"code","source":"# let's install Feature-engine\n!pip install feature-engine","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:47:18.36132Z","iopub.execute_input":"2022-05-23T00:47:18.361654Z","iopub.status.idle":"2022-05-23T00:47:32.638533Z","shell.execute_reply.started":"2022-05-23T00:47:18.361624Z","shell.execute_reply":"2022-05-23T00:47:32.637674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Let's install featurewiz without any dependencies. Otherwise error!\n!pip install featurewiz --ignore-installed --no-deps","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:47:35.0096Z","iopub.execute_input":"2022-05-23T00:47:35.009964Z","iopub.status.idle":"2022-05-23T00:47:38.223643Z","shell.execute_reply.started":"2022-05-23T00:47:35.009926Z","shell.execute_reply":"2022-05-23T00:47:38.222609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## You must also install xlrd for featurewiz which is required\n!pip install xlrd\n### You need to install this since Kaggle has a wrong version ##\n!pip install Pillow==9.0.0","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:47:49.521037Z","iopub.execute_input":"2022-05-23T00:47:49.521382Z","iopub.status.idle":"2022-05-23T00:48:04.745329Z","shell.execute_reply.started":"2022-05-23T00:47:49.521344Z","shell.execute_reply":"2022-05-23T00:48:04.744001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\n# import selection classes from Feature-engine\n\nfrom feature_engine.selection import (\n    DropDuplicateFeatures,\n    DropConstantFeatures,\n    DropDuplicateFeatures,\n    DropCorrelatedFeatures,\n    SmartCorrelatedSelection,\n    SelectByShuffling,\n    SelectBySingleFeaturePerformance,\n    RecursiveFeatureElimination,\n)\n\n# from feature-engine\nfrom feature_engine.imputation import MeanMedianImputer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-23T00:48:06.147527Z","iopub.execute_input":"2022-05-23T00:48:06.147856Z","iopub.status.idle":"2022-05-23T00:48:07.072974Z","shell.execute_reply.started":"2022-05-23T00:48:06.147823Z","shell.execute_reply":"2022-05-23T00:48:07.072013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the Santander customer satisfaction dataset\n\ndata = pd.read_csv('/kaggle/input/santander-customer-satisfaction/train.csv')\nprint(data.shape)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:48:14.031789Z","iopub.execute_input":"2022-05-23T00:48:14.032171Z","iopub.status.idle":"2022-05-23T00:48:16.373279Z","shell.execute_reply.started":"2022-05-23T00:48:14.032138Z","shell.execute_reply":"2022-05-23T00:48:16.372317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This is a highly imbalanced class problem. ","metadata":{}},{"cell_type":"code","source":"target = 'TARGET'\nprint(data[target].value_counts(1))\ndata[target].hist()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:48:19.181124Z","iopub.execute_input":"2022-05-23T00:48:19.181479Z","iopub.status.idle":"2022-05-23T00:48:19.414632Z","shell.execute_reply.started":"2022-05-23T00:48:19.181445Z","shell.execute_reply":"2022-05-23T00:48:19.413865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# we must split the dataset into Train and Test before we do any feature engg or selection! This is a must.","metadata":{}},{"cell_type":"code","source":"modeltype = 'Classification'","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:48:23.66873Z","iopub.execute_input":"2022-05-23T00:48:23.669094Z","iopub.status.idle":"2022-05-23T00:48:23.675154Z","shell.execute_reply.started":"2022-05-23T00:48:23.669059Z","shell.execute_reply":"2022-05-23T00:48:23.673977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# separate dataset into train and test sets\nif modeltype == 'Regression':\n    X_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['ID',target], axis=1),\n    data[target],\n    test_size=0.2,\n    random_state=0)\nelse:\n    X_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['ID',target], axis=1),\n    data[target],\n    test_size=0.2,\n    random_state=0,\n    stratify=data[target])\n\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:48:26.202878Z","iopub.execute_input":"2022-05-23T00:48:26.20325Z","iopub.status.idle":"2022-05-23T00:48:26.508289Z","shell.execute_reply.started":"2022-05-23T00:48:26.203219Z","shell.execute_reply":"2022-05-23T00:48:26.507392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nX_train_copy = copy.deepcopy(X_train)\nX_test_copy = copy.deepcopy(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:48:29.153823Z","iopub.execute_input":"2022-05-23T00:48:29.154201Z","iopub.status.idle":"2022-05-23T00:48:29.225407Z","shell.execute_reply.started":"2022-05-23T00:48:29.154169Z","shell.execute_reply":"2022-05-23T00:48:29.224588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if there missing data (this datasets do not show NAs\n# as we will see in the empty list output)\n\ncols = data.columns[data.isnull().sum()>0].tolist()\ncols","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:48:30.082724Z","iopub.execute_input":"2022-05-23T00:48:30.083098Z","iopub.status.idle":"2022-05-23T00:48:30.145703Z","shell.execute_reply.started":"2022-05-23T00:48:30.083063Z","shell.execute_reply":"2022-05-23T00:48:30.144785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Selection using Feature-Engine first using a pipeline\n\nNow we will select features and train a machine learning model altogether in 1 pipeline.","metadata":{}},{"cell_type":"code","source":"#modeltype = 'Regression'\nmodeltype = 'Classification'","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:48:33.564728Z","iopub.execute_input":"2022-05-23T00:48:33.5651Z","iopub.status.idle":"2022-05-23T00:48:33.570041Z","shell.execute_reply.started":"2022-05-23T00:48:33.565069Z","shell.execute_reply":"2022-05-23T00:48:33.568856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nstart_time = time.time()\n\n# let's remove constant, quasi-constant and duplicates to speed things up\nif modeltype=='Regression':\n    the_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=1)\n    metric = 'neg_mean_squared_error'\nelse:\n    the_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1) \n    metric = \"roc_auc\"\n\nif len(cols) == 0:\n    pipe1 = Pipeline([\n    # ======== FEATURE SELECTION =======\n    ('constant', DropConstantFeatures(tol=0.998)), # drops constand and quasi-constant altogether\n    ('duplicated', DropDuplicateFeatures()), # drop duplicated\n    ('shuffle', SelectByShuffling( # select by feature shuffling\n        estimator = the_model,\n        scoring=metric, # the metric to determine model performance\n        cv=5, # the cross-validation fold\n    )),\n])\nelse:\n    mdi = MeanMedianImputer(\n        imputation_method='median',\n        variables=cols\n    )\n\n    pipe1 = Pipeline([\n    # ======== FEATURE SELECTION =======\n    ('imputer', mdi),\n    ('constant', DropConstantFeatures(tol=0.998)), # drops constand and quasi-constant altogether\n    ('duplicated', DropDuplicateFeatures()), # drop duplicated\n    ('shuffle', SelectByShuffling( # select by feature shuffling\n        estimator = the_model,\n        scoring=metric, # the metric to determine model performance\n        cv=5, # the cross-validation fold\n    )),\n])\n\n    \npipe2 = Pipeline([\n    # =====  the machine learning model ====\n    ('random_forest', the_model),\n])\n\n# remove variables\n\nprint('Number of original variables: ', X_train.shape[1])\n\nX_train = pipe1.fit_transform(X_train, y_train)\nX_test = pipe1.transform(X_test)\n\nprint('Number of variables after selection: ', X_train.shape[1])\nprint('Time taken: %0.2f seconds' %(time.time()-start_time))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:48:35.119746Z","iopub.execute_input":"2022-05-23T00:48:35.120261Z","iopub.status.idle":"2022-05-23T00:50:25.493388Z","shell.execute_reply.started":"2022-05-23T00:48:35.120222Z","shell.execute_reply":"2022-05-23T00:50:25.492589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"the_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:50:44.389294Z","iopub.execute_input":"2022-05-23T00:50:44.389635Z","iopub.status.idle":"2022-05-23T00:50:47.340416Z","shell.execute_reply.started":"2022-05-23T00:50:44.389602Z","shell.execute_reply":"2022-05-23T00:50:47.339515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# It took 120 seconds (2 mins) to run feature-engine to select 27 variables out of 369 (~90% reduction!)","metadata":{}},{"cell_type":"code","source":"# the pipeline takes in the raw data, removes all unwanted features and then\n# makes the prediction with the model trained on the final subset of variables\n\n# obtain predictions and determine model performance\ny_preds = the_model.predict(X_test)\ny_preds","metadata":{"execution":{"iopub.status.busy":"2022-05-23T01:04:36.542599Z","iopub.execute_input":"2022-05-23T01:04:36.542983Z","iopub.status.idle":"2022-05-23T01:04:36.657473Z","shell.execute_reply.started":"2022-05-23T01:04:36.542946Z","shell.execute_reply":"2022-05-23T01:04:36.656506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if modeltype == 'Regression':\n    from sklearn.metrics import r2_score, mean_squared_error\n    print('R-Squared = %0.0f%%' %(100*r2_score(y_test,y_preds)))\n    print('RMSE = %0.2f' %np.sqrt(mean_squared_error(y_test,y_preds)))\n    #plot_scatter(y_test,testm[target+'_XGBoost_predictions'])\nelse:\n    from sklearn.metrics import balanced_accuracy_score, classification_report\n    if isinstance(target, str): \n        print('Bal accu %0.0f%%' %(100*balanced_accuracy_score(y_test,y_preds)))\n        print(classification_report(y_test,y_preds))\n    elif len(target) == 1:\n            print('Bal accu %0.0f%%' %(100*balanced_accuracy_score(y_test,y_preds)))\n            print(classification_report(y_test,y_preds))\n    else:\n        for each_i, target_name in enumerate(target):\n            print('For %s:' %target_name)\n            print('    Bal accu %0.0f%%' %(100*balanced_accuracy_score(y_test.values[:,each_i],y_preds[:,each_i])))\n            print(classification_report(y_test.values[:,each_i],y_preds[:,each_i]))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T01:04:42.989101Z","iopub.execute_input":"2022-05-23T01:04:42.989471Z","iopub.status.idle":"2022-05-23T01:04:43.026242Z","shell.execute_reply.started":"2022-05-23T01:04:42.989438Z","shell.execute_reply":"2022-05-23T01:04:43.025211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The balanced accuracy score is unfortunately 50% which means that the selected features were somewhat worthless. ","metadata":{}},{"cell_type":"markdown","source":"# Now let's select features using Featurewiz and see how it performs","metadata":{}},{"cell_type":"code","source":"from featurewiz import FeatureWiz","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:51:21.628117Z","iopub.execute_input":"2022-05-23T00:51:21.628457Z","iopub.status.idle":"2022-05-23T00:51:25.800405Z","shell.execute_reply.started":"2022-05-23T00:51:21.628424Z","shell.execute_reply":"2022-05-23T00:51:25.799385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import featurewiz as FW","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:56:36.292869Z","iopub.execute_input":"2022-05-23T00:56:36.29325Z","iopub.status.idle":"2022-05-23T00:56:36.297989Z","shell.execute_reply.started":"2022-05-23T00:56:36.293218Z","shell.execute_reply":"2022-05-23T00:56:36.29712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nstart_time = time.time()\n\n# remove variables\n\nprint('Number of original variables: ', X_train_copy.shape[1])\n\nfeatures = FeatureWiz(corr_limit=0.70, feature_engg='', category_encoders='', dask_xgboost_flag=False, nrows=None, verbose=2)\nX_train_selected = features.fit_transform(X_train_copy, y_train)\nX_test_selected = features.transform(X_test_copy)\n\n### provides the list of selected features ###\nprint('Number of variables after selection: ', X_train_selected.shape[1])\nprint('Time taken: %0.2f seconds' %(time.time()-start_time))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:51:28.155844Z","iopub.execute_input":"2022-05-23T00:51:28.156266Z","iopub.status.idle":"2022-05-23T00:54:30.617015Z","shell.execute_reply.started":"2022-05-23T00:51:28.156231Z","shell.execute_reply":"2022-05-23T00:54:30.616023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train_selected.shape)\nX_train_selected.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:54:47.717209Z","iopub.execute_input":"2022-05-23T00:54:47.717572Z","iopub.status.idle":"2022-05-23T00:54:47.743987Z","shell.execute_reply.started":"2022-05-23T00:54:47.71754Z","shell.execute_reply":"2022-05-23T00:54:47.742973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Featurewiz took 184 seconds (3 mins) and selected 78 features. That's much more features than feature-engine and it took half the time. Let us use a similar RandomForestClassifier model to compare results","metadata":{}},{"cell_type":"code","source":"outputs = FW.complex_XGBoost_model(X_train_selected, y_train, \n                        X_test_selected, log_y=False, \n                GPU_flag=False, scaler='', enc_method='label', n_splits=5, verbose=-1)","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:58:24.802571Z","iopub.execute_input":"2022-05-23T00:58:24.802924Z","iopub.status.idle":"2022-05-23T00:58:27.455835Z","shell.execute_reply.started":"2022-05-23T00:58:24.802873Z","shell.execute_reply":"2022-05-23T00:58:27.454949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if modeltype != 'Regression':\n    #y_preds = lazy.yformer.inverse_transform(y_preds)\n    y_preds = outputs[0]\nelse:\n    y_preds = outputs[0]\ny_preds[:4]","metadata":{"execution":{"iopub.status.busy":"2022-05-23T00:58:50.140513Z","iopub.execute_input":"2022-05-23T00:58:50.140836Z","iopub.status.idle":"2022-05-23T00:58:50.146981Z","shell.execute_reply.started":"2022-05-23T00:58:50.140805Z","shell.execute_reply":"2022-05-23T00:58:50.14603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if modeltype == 'Regression':\n    from sklearn.metrics import r2_score, mean_squared_error\n    print('R-Squared = %0.0f%%' %(100*r2_score(y_test,y_preds)))\n    print('RMSE = %0.2f' %np.sqrt(mean_squared_error(y_test,y_preds)))\n    #plot_scatter(y_test,testm[target+'_XGBoost_predictions'])\nelse:\n    from sklearn.metrics import balanced_accuracy_score, classification_report\n    if isinstance(target, str): \n        print('Bal accu %0.0f%%' %(100*balanced_accuracy_score(y_test,y_preds)))\n        print(classification_report(y_test,y_preds))\n    elif len(target) == 1:\n            print('Bal accu %0.0f%%' %(100*balanced_accuracy_score(y_test,y_preds)))\n            print(classification_report(y_test,y_preds))\n    else:\n        for each_i, target_name in enumerate(target):\n            print('For %s:' %target_name)\n            print('    Bal accu %0.0f%%' %(100*balanced_accuracy_score(y_test.values[:,each_i],y_preds[:,each_i])))\n            print(classification_report(y_test.values[:,each_i],y_preds[:,each_i]))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T01:00:19.650434Z","iopub.execute_input":"2022-05-23T01:00:19.65079Z","iopub.status.idle":"2022-05-23T01:00:19.690156Z","shell.execute_reply.started":"2022-05-23T01:00:19.650758Z","shell.execute_reply":"2022-05-23T01:00:19.689205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Featurewiz shows very good promise in this dataset with a 74% balanced accuracy vs 50% for feature-engine. This means that in large datasets, using the SULOV algorithm and mRmR feature-selection techniques, featurewiz is able to provide superior performance.","metadata":{}},{"cell_type":"markdown","source":"# Let's see whether using all the features in the data set provides a better model","metadata":{}},{"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100,max_depth=5, random_state=1)\nmodel","metadata":{"execution":{"iopub.status.busy":"2022-05-23T01:02:29.002168Z","iopub.execute_input":"2022-05-23T01:02:29.002529Z","iopub.status.idle":"2022-05-23T01:02:29.009131Z","shell.execute_reply.started":"2022-05-23T01:02:29.002498Z","shell.execute_reply":"2022-05-23T01:02:29.008207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train)\ny_preds = model.predict(X_test)\ny_preds","metadata":{"execution":{"iopub.status.busy":"2022-05-23T01:02:30.148577Z","iopub.execute_input":"2022-05-23T01:02:30.148954Z","iopub.status.idle":"2022-05-23T01:02:33.368893Z","shell.execute_reply.started":"2022-05-23T01:02:30.148916Z","shell.execute_reply":"2022-05-23T01:02:33.367996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if modeltype == 'Regression':\n    from sklearn.metrics import r2_score, mean_squared_error\n    print('R-Squared = %0.0f%%' %(100*r2_score(y_test,y_preds)))\n    print('RMSE = %0.2f' %np.sqrt(mean_squared_error(y_test,y_preds)))\n    #plot_scatter(y_test,testm[target+'_XGBoost_predictions'])\nelse:\n    from sklearn.metrics import balanced_accuracy_score, classification_report\n    if isinstance(target, str): \n        print('Bal accu %0.0f%%' %(100*balanced_accuracy_score(y_test,y_preds)))\n        print(classification_report(y_test,y_preds))\n    elif len(target) == 1:\n            print('Bal accu %0.0f%%' %(100*balanced_accuracy_score(y_test,y_preds)))\n            print(classification_report(y_test,y_preds))\n    else:\n        for each_i, target_name in enumerate(target):\n            print('For %s:' %target_name)\n            print('    Bal accu %0.0f%%' %(100*balanced_accuracy_score(y_test.values[:,each_i],y_preds[:,each_i])))\n            print(classification_report(y_test.values[:,each_i],y_preds[:,each_i]))","metadata":{"execution":{"iopub.status.busy":"2022-05-23T01:02:50.580271Z","iopub.execute_input":"2022-05-23T01:02:50.580705Z","iopub.status.idle":"2022-05-23T01:02:50.6137Z","shell.execute_reply.started":"2022-05-23T01:02:50.58066Z","shell.execute_reply":"2022-05-23T01:02:50.612831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Actually using all features does not work that well for this dataset. Hence you are better off using feature selection methods.","metadata":{"execution":{"iopub.status.busy":"2022-05-23T01:03:38.223719Z","iopub.execute_input":"2022-05-23T01:03:38.224093Z","iopub.status.idle":"2022-05-23T01:03:38.228112Z","shell.execute_reply.started":"2022-05-23T01:03:38.22406Z","shell.execute_reply":"2022-05-23T01:03:38.227023Z"}}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}