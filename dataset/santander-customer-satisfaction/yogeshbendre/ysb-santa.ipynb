{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport tensorflow as tf\nfrom subprocess import check_output\nimport pickle as pkl\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\nprint(check_output([\"ls\", \"./\"]).decode(\"utf8\"))\n#test_data=pd.read_csv('../input/test.csv')\ntest_data=np.genfromtxt('../input/test.csv',delimiter=',')\n#train_data=pd.read_csv('../input/test.csv')\ntrain_data=np.genfromtxt('../input/test.csv',delimiter=',')\n#print(train_data)\npkl.dump(train_data[1:],open('train_data.pkl','wb'))\npkl.dump(test_data[1:],open('test_data.pkl','wb'))\n\ntrdt=pkl.load(open('train_data.pkl','rb'))\nprint('train_data:',len(trdt[1,:]))\ntrlb=trdt[:,-1]\ntrdt=trdt[:,:-1]\nntr=len(trdt)\nprint('train_data:',len(trdt[1,:]))\ntsdt=pkl.load(open('test_data.pkl','rb'))\nnts=len(tsdt)\n#print(trdt[:,-1])\n# Any results you write to the current directory are saved as output.\n\n#Tensorflow code\ngraph=tf.Graph()\nbatch_size=128\nwith graph.as_default():\n    tf_trdt=tf.placeholder(tf.float32,shape=(batch_size,ntr))\n    tf_trlb=tf.placeholder(tf.float32,shape=(batch_size,2))\n    tf_tsdt=tf.constant(tsdt)\n    \n    #Hidden Layer 1\n    nh1=256\n    w1=tf.Variable(tf.truncated_normal([ntr,nh1]))\n    b1=tf.Variable(tf.zeros([nh1]))\n    \n\n    #Hidden Layer 2\n    nh2=2 #Only two classes \n    w2=tf.Variable(tf.truncated_normal([nh1,nh2]))\n    b2=tf.Variable(tf.zeros([nh2]))\n    \n    #Network Mapping\n    h1=tf.nn.relu(tf.matmul(tf_trdt,w1)+b1)\n    h2=tf.nn.relu(tf.matmul(h1,w2)+b2)\n    \n    logits=tf.matmul(h1,w2)+b2\n    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,tf_trlb)) + 0.1*tf.nn.l2_loss(w1) + 0.01*tf.nn.l2_loss(w2)\n    \n    #Optimizer\n    optimizer=tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n    \n    #Predictions\n    trpr=tf.nn.softmax(logits)\n    tspr=tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tsdt,w1)+b1),w2)+b2)\n    \n\n#Tensorflow session to run    \nnum_steps=1001\n\nwith tf.Session(graph=graph) as session:\n    tf.initialize_all_variables().run()\n    print(\"Initialized\")\n    for step in range(num_steps):\n        offset=(step*batch_size)%(trlb.shape[0]-batch_size)\n        batch_data=trdt[offset:(offset+batch_size),:]\n        batch_labels=trlb[offset:(offset+batch_size),:]\n        feed_dict={tf_trdt:batch_data,tf_trlb:batch_labels}\n        _,l,pr=session.run([optimizer,loss,trpr],feed_dict=feed_dict)\n        if(step%100==0):\n            print('Minibatch loss at step %d : %f' % (step,l))\n            print('Minibatch accuracy: %.lf%%' % accuracy(trpr,batch_labels))\n            \n\n    print('Minibatch accuracy: %.lf%%' % accuracy(tspr.eval(),tslb))\n\n            \ndef accuracy(predictions, labels):\n  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n          / predictions.shape[0])\n\n        \n\n\n\n\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}