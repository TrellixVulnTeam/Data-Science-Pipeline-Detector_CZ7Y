{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# THIS WAS MY FIRST SCRIPT AND IT HAS SOME FLAWS. \n# IT'S STILL HERE BECAUSE I GOT FOND OF IT.\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%pylab inline\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom itertools import combinations\nfrom numpy import array,array_equal\n\nfrom sklearn import cross_validation as cv\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn import ensemble\nfrom sklearn import linear_model \nfrom sklearn import naive_bayes \n\nimport xgboost as xgb\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]\n).decode(\"utf8\"))\n\n\n# Any results you write to the current directory are saved as output.\ndef print_shapes():\n    print('Train: {}\\nTest: {}'.format(train_dataset.shape, test_dataset.shape))\ntrain_dataset = pd.read_csv('../input/train.csv', index_col='ID')\ntest_dataset = pd.read_csv('../input/test.csv', index_col='ID')\n\nprint_shapes()\n# How many nulls are there in the datasets?\nnulls_train = (train_dataset.isnull().sum()==1).sum()\nnulls_test = (test_dataset.isnull().sum()==1).sum()\nprint('There are {} nulls in TRAIN and {} nulls in TEST dataset.'.format(nulls_train, nulls_test))\n# Remove constant features\n\ndef identify_constant_features(dataframe):\n    count_uniques = dataframe.apply(lambda x: len(x.unique()))\n    constants = count_uniques[count_uniques == 1].index.tolist()\n    return constants\n\nconstant_features_train = set(identify_constant_features(train_dataset))\n\nprint('There were {} constant features in TRAIN dataset.'.format(\n        len(constant_features_train)))\n\n# Drop the constant features\ntrain_dataset.drop(constant_features_train, inplace=True, axis=1)\n\n\nprint_shapes()\n# Remove equals features\n\ndef identify_equal_features(dataframe):\n    features_to_compare = list(combinations(dataframe.columns.tolist(),2))\n    equal_features = []\n    for compare in features_to_compare:\n        is_equal = array_equal(dataframe[compare[0]],dataframe[compare[1]])\n        if is_equal:\n            equal_features.append(list(compare))\n    return equal_features\n\nequal_features_train = identify_equal_features(train_dataset)\n\nprint('There were {} pairs of equal features in TRAIN dataset.'.format(len(equal_features_train)))\n\n# Remove the second feature of each pair.\n\nfeatures_to_drop = array(equal_features_train)[:,1] \ntrain_dataset.drop(features_to_drop, axis=1, inplace=True)\n\nprint_shapes()\n# Define the variables model.\n\ny_name = 'TARGET'\nfeature_names = train_dataset.columns.tolist()\nfeature_names.remove(y_name)\n\nX = train_dataset[feature_names] # data\ny = train_dataset[y_name] # class\n\n# Save the features selected for later use.\npd.Series(feature_names).to_csv('features_selected_step1.csv', index=False)\nprint('Features selected\\n{}'.format(feature_names))\n   \n    \n# Proportion of classes\ny.value_counts()/len(y)\n\nskf = cv.StratifiedKFold(y, n_folds=3, shuffle=True)\n\n\n\n#score_metric = 'roc_auc'\n#scores = {}\n\n#def score_model(model):\n#    return cv.cross_val_score(model, X, y, cv=skf, scoring=score_metric)\n\n# time: 10s\n#scores['tree'] = score_model(tree.DecisionTreeClassifier()) \n\n# time: 9s\n#scores['extra_tree'] = score_model(ensemble.ExtraTreesClassifier())\n\n# time: 7s\n#scores['forest'] = score_model(ensemble.RandomForestClassifier())\n\n# time: 33s\n#scores['ada_boost'] = score_model(ensemble.AdaBoostClassifier())\n\n# time: 1min\n#scores['bagging'] = score_model(ensemble.BaggingClassifier())\n\n# time: 2min30s\n#scores['grad_boost'] = score_model(ensemble.GradientBoostingClassifier())\n\n# time: 49s\n#scores['ridge'] = score_model(linear_model.RidgeClassifier())\n\n# time: 4s\n#scores['passive'] = score_model(linear_model.PassiveAggressiveClassifier())\n\n# time: 4s\n#scores['sgd'] = score_model(linear_model.SGDClassifier())\n\n# time: 3s\n#scores['gaussian'] = score_model(naive_bayes.GaussianNB())\n\n# time: 4min\n#scores['xgboost'] = score_model(xgb.XGBClassifier())\n\n\n# Print the scores\n#model_scores = pd.DataFrame(scores).mean()\n#model_scores.sort_values(ascending=False)\n#model_scores.to_csv('model_scores.csv', index=False)\n#print('Model scores\\n{}'.format(model_scores))\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"# xboost classifier\nX_fit, X_eval, y_fit, y_eval= cv.train_test_split(X, y, test_size=0.3)\nclf = xgb.XGBClassifier(missing=np.nan, max_depth=5, n_estimators=570, learning_rate=0.02, nthread=4, subsample=0.68, colsample_bytree=0.7, seed=1235)\n# fitting\nclf.fit(X, y, early_stopping_rounds=500, eval_metric=\"auc\", eval_set=[(X_eval, y_eval)])\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false},"outputs":[],"source":"print('Overall AUC:', metrics.roc_auc_score(y, clf.predict_proba(X)[:,1]))\n\nX_test = test_dataset[feature_names]\n# predicting\ny_pred= clf.predict_proba(X_test)[:,1]\n\nsubmission = pd.DataFrame({\"ID\":X_test.index, \"TARGET\":y_pred})\nsubmission.to_csv(\"submission.csv\", index=False)\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":0}