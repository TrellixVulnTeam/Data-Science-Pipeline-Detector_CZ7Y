{"cells":[{"metadata":{"_cell_guid":"771332ad-7025-48c0-ac5c-e3326096f7f4","_uuid":"3271efebcbd994df291c5d8986f0e8666d1a9237"},"cell_type":"markdown","source":"Let's give this contest a go. Hopefully not too much data that it overloads the Kaggle kernel...\nPlan is to explore the data beforehand, and try to standardise where I can.\nThen try out a bunch of different models and see what results I get."},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport glob\nimport io\nimport math\nimport os\nimport matplotlib\n\n\nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import boxcox\nfrom sklearn.ensemble import AdaBoostRegressor\nimport xgboost as xgb\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\n\ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = '{:.1f}'.format\n\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV,RidgeClassifierCV,RidgeClassifier,LogisticRegression,LogisticRegressionCV\nfrom sklearn.model_selection import cross_val_score\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8d19629-5cbe-45cd-bbda-6ec4b0802ed8","_uuid":"191b0711b225e245913591a98519af95338297f4"},"cell_type":"markdown","source":"Read all the data in."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":false},"cell_type":"code","source":"# read in the data here\ntrain_df = pd.read_csv(\"../input/train.csv\",sep=\",\")\ntest_df = pd.read_csv(\"../input/test.csv\",sep=\",\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"55daf68d-ff0e-4e09-bd31-b8e8c8aa7f98","_uuid":"7621eaa9db9179c3277d50f42dcd545ea78b4f5e"},"cell_type":"markdown","source":"Combine the dataframes together for cleaning."},{"metadata":{"_cell_guid":"46af779f-853c-415d-a909-111c3604a4bb","_uuid":"0df9ea19dfc04c1054429c6f0d2975e4d69d8553","collapsed":true,"trusted":false},"cell_type":"code","source":"#Combine the dataframes\ncombined_df=pd.DataFrame()\ncombined_df=pd.concat([train_df,test_df])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fb6ab078-a79b-4723-835d-80fc9d7871aa","_uuid":"b2068b698db3343ca8313dffe66a73182c465c6a"},"cell_type":"markdown","source":"Looks like the data set is complete and all numerical. Since it's all anonymized, it's not clear what anything means.  To start with, I'll separate out the Target & ID, so I can start to deal with the numerical data."},{"metadata":{"_cell_guid":"1c486aaf-a78a-4d48-b577-b3c20e372a7e","_uuid":"0f238404faa434bf1cc7fad431ba293a1d65e221","collapsed":true,"trusted":false},"cell_type":"code","source":"num_df=pd.DataFrame.copy(combined_df)\n\n#Put the Target & ID aside for now\ntarget_df=pd.DataFrame()\ntarget_df[\"TARGET\"]=num_df[\"TARGET\"]\nnum_df=num_df.drop([\"TARGET\"],axis=1)\n\n#Same with ID\nID_df=pd.DataFrame()\nID_df[\"ID\"]=num_df[\"ID\"]\nnum_df=num_df.drop([\"ID\"],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"10dbf7ad-ca54-4841-8b69-e73a3a79bfa6","_uuid":"59813f8ec721aa3ce50ecd5f5bc61ab58560361c"},"cell_type":"markdown","source":"One problem is what to do with the huge values in some columns. It seems like -999999 is an error code in var3, which most people replace with 2. 9999999999 appears a lot in the data, and I need to decide how to deal with it. Some people seem to encode it as nan, although I might get away with changing it to another high value, but outside the range of the rest of the data (like 10/100). For now, I'm hoping converting to 100 is enough."},{"metadata":{"_cell_guid":"75dba8d1-818d-403d-8515-21c200df6de0","_uuid":"41967ceab2cedac21e32b6e7f98c651f2a336052","trusted":false,"collapsed":true},"cell_type":"code","source":"#Replaces huge -ve number with 2.\nclean_df=pd.DataFrame.copy(num_df)\nclean_df[\"var3\"].value_counts()\nclean_df.loc[clean_df['var3']==-999999,'var3']=2\nclean_df[\"var3\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9e757aa0-1cc6-47f2-967f-052d31242a2b","_uuid":"28d86d8dc9af3165978915fb7a0a521a8de1b38c","trusted":false,"collapsed":true},"cell_type":"code","source":"clean_df[num_df==9999999999] = 100\nclean_df.skew().sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d2da6cdb-506a-4bfb-b3c7-844a0a4bcf56","_uuid":"e69628d4e1211585968927870c0d8739aa7ee6cc"},"cell_type":"markdown","source":"Try to normalize the data a bit. I want to remove the columns which contain all zeros, then try log transforming some others if they are highly skewed."},{"metadata":{"_cell_guid":"ada1922e-9788-4bc6-9569-747b4425642a","_uuid":"6e825fd4d71a36aaeaaa6c39df2b8f7e62a9bd09","trusted":false,"collapsed":true},"cell_type":"code","source":"#Remove all fully zero columns\n\nclean_df=clean_df.loc[:, (clean_df != 0).any(axis=0)]\nclean_df.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"755c4808-1788-4d50-a65e-1487456f755e","_uuid":"a27cfe692e28a4cb79121996af72ea4546f15913","trusted":false,"collapsed":true},"cell_type":"code","source":"skewed_feats= clean_df.skew()\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nunskewed_feats= clean_df.skew()\nunskewed_feats = unskewed_feats[unskewed_feats < 0.75]\nunskewed_feats = unskewed_feats.index\n\nskewed_feats,unskewed_feats\n\n#transform_df=pd.DataFrame.copy(clean_df)\n#transform_df[unskewed_feats]=(No_sparse_df[unskewed_feats]\n#                               - No_sparse_df[unskewed_feats].mean()) / (No_sparse_df[unskewed_feats].max() - No_sparse_df[unskewed_feats].min())\n#transform_df[skewed_feats] = np.log1p(No_sparse_df[skewed_feats])\n\ntransform_df=pd.DataFrame.copy(clean_df)\ntransform_df=(transform_df-transform_df.mean()) / (transform_df.max() - transform_df.min())\ntransform_df[skewed_feats] = np.log1p(transform_df[skewed_feats])\n\ntransform_df[\"imp_reemb_var17_hace3\"].describe()\n\n#_ = transform_df.hist(bins=40, figsize=(18, 18), xlabelsize=10)\n#transform_df.skew().sort_values(ascending=False)\n#transform_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"68e9bcd9-0196-453a-88f5-65d3ca00ddf9","_uuid":"e87af493b3e667c3b67bda719d7b771b153c798a"},"cell_type":"markdown","source":"Combine the dataframes back.\n"},{"metadata":{"_cell_guid":"6854ece8-f9ad-4c0f-8a7a-94cfc846650c","_uuid":"022bbe122ba732d0517bf3060faf63e2129655b3","trusted":false,"collapsed":true},"cell_type":"code","source":"Eng_df=pd.DataFrame()\n\n#transform_df\nEng_df=pd.concat([transform_df,target_df,ID_df], axis=1)\n\n#Split back into the original data\n\nEng_train_df=pd.DataFrame()\nEng_train_df=Eng_df.head(76020)\nEng_test_df=pd.DataFrame()\nEng_test_df=Eng_df.tail(75818)\n\n\n# shuffle the data, just in case\nEng_train_df = Eng_train_df.reindex(np.random.permutation(Eng_train_df.index))\nEng_train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dae00f2e-69c0-422e-b5a5-51ef2eb166be","_uuid":"1e657548e8a7c2ba230f6ddacb9046639a2dfb1d","collapsed":true,"trusted":false},"cell_type":"code","source":"# Process features and target\ndef preprocess_features(training_df):\n \n  selected_features = training_df[training_df.columns.difference([\"TARGET\",\"ID\"])]\n  processed_features = selected_features.copy()\n  return processed_features\n\ndef preprocess_targets(training_df):\n \n  output_targets = pd.DataFrame()\n    \n  output_targets[\"TARGET\"] = (\n    training_df[\"TARGET\"])\n  return output_targets","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"61f0e19f-7c3e-4b07-a309-dc4b08aecb9c","_uuid":"b305fdead0c5dd0a94ca396aec25b6126dd2997c"},"cell_type":"markdown","source":"Getting the training / test inputs."},{"metadata":{"_cell_guid":"15bd831f-5a97-43fd-82e5-f5795c5a49ab","_uuid":"2fed50bb090ce57ce5024d3922a970524fd16d1e","collapsed":true,"trusted":false},"cell_type":"code","source":"training_examples= preprocess_features(Eng_train_df)\ntest_examples = preprocess_features(Eng_test_df)\ntraining_targets = preprocess_targets(Eng_train_df)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"00b2bfdc-43d3-43a0-a35f-afd16193038f","_uuid":"4ee0ebd05018074bb2b855f85b97b8ed3def5b0a"},"cell_type":"markdown","source":"To start off with, I want to try out a Ridge Classifier model to work, and see what the best auc I can get is.\nFor now, all I've done is remove the fully zero columns and try to normalise the data."},{"metadata":{"_cell_guid":"59ae4d6f-8ca4-405c-8b28-63d74a563aea","_uuid":"08b5bcc4ca7554f68b4d331a486473220f684d0b","collapsed":true,"trusted":false},"cell_type":"code","source":"#Let's just try ridge out of the box on this set, and alter alpha\n#Ridge=RidgeCV()\n#crossval_scores = cross_val_score(Ridge, training_examples, np.ravel(training_targets), scoring='roc_auc', cv=10)\n#crossval_scores.mean()\n#Default = 0.6993807210214967\n#After dropping some values, I get:#0.7896482020577332\n#Dropping just zero columns: 0.7256913010855297\n#Sparsity cut-off at 100: 0.789270318386544\n#500: 0.7888133742542437\n#1k:0.7896846933393219\n#5k: 0.7910464037042653 + normalise + some log _> 0.7945947795221165\n#10k: 0.7896401137512984\n#10%: 0.789513131590636\n#25%: 0.7845316876767147\n#5k was best of this lot, but I'm not sure numbers above 100 mean much.\n#1% & normalization : 0.7954232890589281\n#Cut off zeros, replace the huge numbers (99999 etc), try to normalize then try with cv=3:0.7957032628326774","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b9e0f53b-cef9-48ac-9301-5e1f6650f53d","_uuid":"97f5acca4e53ed22a3415f7d4d8e22ac2498bb59","collapsed":true,"trusted":false},"cell_type":"code","source":"#model_ridge=RidgeClassifierCV(scoring='roc_auc', cv=3)\n#model_ridge=RidgeCV(scoring='roc_auc', cv=3)\n#model_ridge.fit(training_examples, np.ravel(training_targets))\n#ridge_preds = model_ridge.predict(test_examples)\n\n#solution = pd.DataFrame({\"ID\":test_df[\"ID\"], \"Target\":np.abs(ridge_preds)})\n#solution.to_csv(\"Ridge.csv\", index = False)\n#This gets me ~.794. Still pretty bad, but much better! Seems like I shouldn't be using a classifier.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"da19f036-5029-43cc-903a-bd4726419137","_uuid":"5ab6058bbd98cc1a2115464ffdc4c8ac7db4bde6"},"cell_type":"markdown","source":"The ridge model got: 0.777054 / 0.794122 for private / public in the end."},{"metadata":{"_cell_guid":"0b57aa10-a7b7-4f38-911c-2d0cec90d612","_uuid":"3272a89ef8630858de61f97c6e2a66c8b5627273"},"cell_type":"markdown","source":"Next up is Lasso."},{"metadata":{"_cell_guid":"0a30a051-47f3-40ac-9b7b-c4c4e26ed9a6","_uuid":"af9da217711c0186e8b1aad87baf50a78e894d2a","collapsed":true,"trusted":false},"cell_type":"code","source":"#Lasso=LassoCV()\n#crossval_scores = cross_val_score(Lasso, training_examples, np.ravel(training_targets), scoring='roc_auc', cv=5)\n#crossval_scores.mean()\n#cv 3: 0.7940033210294137\n#cv 5: 0.794090160348801\n#cv 10: 0.7944051812300171","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"126cad9a-e731-47fd-a156-5ce3e09a5761","_uuid":"4d1ec8388140fe96a2d71cf708c43ecb1500e986","trusted":false,"collapsed":true},"cell_type":"code","source":"model_lasso = LassoCV(cv=10).fit(training_examples, np.ravel(training_targets))\ncoef = pd.Series(model_lasso.coef_, index = training_examples.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"021196df-8509-4373-a470-f23050c86077","_uuid":"682102d77abf24bb99f54e9594b675d1d9334c5c","collapsed":true,"trusted":false},"cell_type":"code","source":"lasso_preds = model_lasso.predict(test_examples)\n\nsolution = pd.DataFrame({\"ID\":test_df[\"ID\"], \"Target\":np.abs(lasso_preds)})\nsolution.to_csv(\"Lasso.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d5215e8d-45d4-4f61-8ff3-dc0eb0942416","_uuid":"e8a0bacb97c81d1150e79d6907a5f45428480c22"},"cell_type":"markdown","source":"Lasso manages: 0.774361 / 0.794044 on private / public, which is actually worse than ridge!"},{"metadata":{"_cell_guid":"2e8f77c3-86bf-4faf-b1ed-2a532369a084","_uuid":"56897eb59c668b4ad28cebf48d0d7f31079e6ac7"},"cell_type":"markdown","source":"Since Lasso sets coefficients to zero, I could use it to select feature for further work. No guarantee this will do anything, but it's worth a shot!\nThe 61 surviving features are stored below."},{"metadata":{"_cell_guid":"a6151898-88d1-4624-86a3-2387b63d9b47","_uuid":"aeb541f42f1fb4e3735135d72fd97d1046cd43cc","trusted":false,"collapsed":true},"cell_type":"code","source":"lasso_features = coef.iloc[coef.nonzero()[0]].sort_values()\nlasso_features=lasso_features.index\nlasso_features","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"67b74738-e9d4-42b9-8cc0-f97501d71695","_uuid":"98c31d29847895cb4706c20336fdbcbe124b8632"},"cell_type":"markdown","source":"For my final basic model,  I'm going to try xgboost on the data, and see how well it performs. Then try xgboost again, but with the features leftover from lasso."},{"metadata":{"_cell_guid":"7d28a805-0bb2-4ee1-899b-9a02a2df1965","_uuid":"fef9077904be61e49896305030758451fb409a7a","trusted":false,"collapsed":true},"cell_type":"code","source":"#xgb_model=xgb.XGBClassifier()\n#crossval_scores = cross_val_score(xgb_model, training_examples, np.ravel(training_targets), scoring='roc_auc', cv=5)\n#crossval_scores.mean()\n#crossval is 0.837819894645197 if I don't do anything to the settings.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2b079723-3ac5-41c3-88f2-fc1487368d38","_uuid":"848e303a721fbd13bda4d64a85e6aa0fe8868c7f","trusted":false,"collapsed":true},"cell_type":"code","source":"#n_estimators=360, max_depth=2, learning_rate=0.1\nmodel_xgb=xgb.XGBClassifier()\nmodel_xgb.fit(training_examples, np.ravel(training_targets))\nxgb_preds = model_xgb.predict_proba(test_examples)\n\n\nsolution = pd.DataFrame({\"ID\":test_df[\"ID\"], \"Target\":np.abs(xgb_preds[:,1])})\nsolution.to_csv(\"XGB_v1.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"01c6fe5a-d81d-4daa-a2c1-a255af07e09f","_uuid":"f554b988e00609acd899b62d3f69c1b14d56ea5c"},"cell_type":"markdown","source":"XGB using all the features gives a score of : 0.823318 / 0.836969 on private / public."},{"metadata":{"_cell_guid":"4f585e61-a5fa-4b5f-8760-d2ac69e6fe04","_uuid":"4fffebee5b35d1ed6e11436d7270ad92302a1042","trusted":false,"collapsed":true},"cell_type":"code","source":"#xgb_model2=xgb.XGBClassifier()\n#crossval_scores = cross_val_score(xgb_model2, training_examples[lasso_features], np.ravel(training_targets), scoring='roc_auc', cv=5)\n#crossval_scores.mean()\n#auc score is 0.8371358886285363 without tuning params. Apparently a bit worse!","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62678c70-2f85-4648-b1e8-8bb3dc9c56bb","_uuid":"63138a628ba8a57456599ea8b4b2444044d7cab0","collapsed":true,"trusted":false},"cell_type":"code","source":"#n_estimators=360, max_depth=2, learning_rate=0.1\nmodel_xgb2=xgb.XGBClassifier()\nmodel_xgb2.fit(training_examples[lasso_features], np.ravel(training_targets))\nxgb_preds2 = model_xgb2.predict_proba(test_examples[lasso_features])\nsolution = pd.DataFrame({\"ID\":test_df[\"ID\"], \"Target\":np.abs(xgb_preds2[:,1])})\nsolution.to_csv(\"XGB_v2.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f82cf7a6-e005-449c-bc1f-d005f21813ba","_uuid":"9ce3067afbb272b29d23254ee2425e733d742e4d","trusted":false,"collapsed":true},"cell_type":"code","source":"xgb_preds2[:,1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ed930026-5e39-438e-bac1-dde8f17e42dd","_uuid":"e08806206e288bff6c028bee7d80e63e800d4adf"},"cell_type":"markdown","source":"XGB using just the lasso derived features gives a score of : 0.820866 / 0.835203.\nThe private value is close to my CV value.\nLooking over the actual competition, what I've learnt is that assessing your score on the public leaderboard can be dangerous!\nThis would have utterly failed there, but actually beaten the best score in the private one. Overfitting happened!\nI have seen a few people push further, closer to .084, but not sure that's worth it."}],"metadata":{"language_info":{"name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","mimetype":"text/x-python","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.4"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}