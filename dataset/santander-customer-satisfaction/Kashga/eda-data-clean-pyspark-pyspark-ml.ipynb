{"cells":[{"metadata":{},"cell_type":"markdown","source":"# After study and understand the data with Panda, I did smililar data engineering here with Pyspark "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install pyspark","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.ml import Pipeline\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import DoubleType, StringType, StructType, StructField\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, QuantileDiscretizer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark import SparkContext\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build spark session\nspark = SparkSession.builder.appName(\"Spark on santander-customer-satisfaction\").getOrCreate()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading data to spark session\ntrain_spark = spark.read.csv(\"../input/santander-customer-satisfaction/train.csv\", header=\"true\", inferSchema=\"true\")\ntest_spark = spark.read.csv(\"../input/santander-customer-satisfaction/test.csv\", header=\"true\", inferSchema=\"true\")\n\n# Loading data panda to simplify data visuallization \n\n\ntrain = pd.read_csv('../input/santander-customer-satisfaction/train.csv')\n\ntest = pd.read_csv('../input/santander-customer-satisfaction/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" #  1.Overview and understand data "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_spark.printSchema()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#summary of the data"},{"metadata":{},"cell_type":"markdown","source":"# Transfer spark to panda dataframe due to bad visibility in the kaggle Kernel. Maybe there is solution to have better display. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_spark.describe().limit(5).toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_spark.describe().toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_spark.limit(5).toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Here I check number of rows for each ID?\n* Conclusion: there is one row for one ID\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import col\ndf_ID_count = train_spark.groupBy(\"ID\").count().orderBy('count', ascending=False)\n#df_ID_count = train_spark.groupBy(\"ID\").count().filter(\"`count` >= 10\").sort(col(\"count\").desc())\ndf_ID_count.show(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> function below will go through each column one by one to do the describe, it is really good pratic but it takes long time to process. Should not be used unless it requires to go through each parameters one by one. "},{"metadata":{"trusted":true},"cell_type":"code","source":"##########warning take long time do not start it unless need to review each column on by one #####################\ndef describe_Column(df, column, target='TARGET', numRows=20):\n    df.groupby(column).agg(F.count(column).alias('count'), \n                           F.mean(target).alias('mean'), \n                           F.stddev(target).alias('stddev'),\n                           F.min(target).alias('min'), \n                           F.max(target).alias('max')\n                          ).orderBy('count', ascending=False).show(numRows)\n    \n\n#for column, typ in train_spark_reduce.dtypes:\n    #print(column)\n    #describe_Column(train_spark_reduce, column)\n    # please comment out lines above to use ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check the type\ntype(df_ID_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Target column is 1 and 0 only,  It equals one for unsatisfied customers and 0 for satisfied customers."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_Target_count = train_spark.groupBy(\"TARGET\").count()\ndf_Target_count.show()\ntype(df_Target_count)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Understand the sample ratio. Balance between unsatisfied customers (1) and satisfied customers (0)."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyspark.sql.functions as f\nfrom pyspark.sql.window import Window\ndf_Target_count = df_Target_count.withColumn('ratio', f.col('count')/f.sum('count').over(Window.partitionBy()))\ndf_Target_count.orderBy('ratio', ascending=False).show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nprint('Histogram plot ')\nsns.countplot('TARGET', data=train_spark.toPandas())\nplt.title('Target size', fontsize=14)\nplt.show()\n\nprint(\"Dataset is imbalanced\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2.Processing and cleaning data"},{"metadata":{},"cell_type":"markdown","source":"# Data cleanning (remove irrelevant)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Assuming ID is not correlated with customer satisfaction so i drop it\ntrain_spark_NoID = train_spark.drop('ID')\ntrain_spark_NoID.limit(5).toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleanning (drop duplicate rows)\n* maybe it is not good to drop, since different customer may have exist same profile"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before dorp duplicate count: \",train_spark_NoID.count())\n#drop duplicate \ntrain_spark_NoID_NoDupRow = train_spark_NoID.dropDuplicates()\nprint(\"After dorp duplicate count: \",train_spark_NoID_NoDupRow.count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_spark_NoID_NoDupRow.distinct().count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleanning (drop duplicate columns)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove duplicated columns\nremove = []\ncols = train.columns\nfor i in range(len(cols)-1):\n    v = train[cols[i]].values\n    for j in range(i+1,len(cols)):\n        if np.array_equal(v,train[cols[j]].values):\n            remove.append(cols[j])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before dorp duplicate column count: \",len(train_spark_NoID_NoDupRow.columns))\ntrain_spark_NoID_NoDup = train_spark_NoID_NoDupRow.drop(*remove)\nprint(\"After dorp duplicate column count: \",len(train_spark_NoID_NoDup.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleanning (remove distinct count =1 )\n* code below used pure spark dataframe, if it is in panda dataframe, it will be calcuate each column STD.\n* Scripte is really slow, need improvement\n* script below only get one distinct column therefore, I remove it from running "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"print(\"Before dorp column count: \",len(train_spark_NoID_NoDup.columns))\n#from pyspark.sql.functions import * is need for the countDistinct\nfrom pyspark.sql.functions import *\n#apply countDistinct on each column\ncol_counts = train_spark_NoID_NoDup.agg(*(countDistinct(col(c)).alias(c) for c in train_spark_NoID_NoDup.columns)).collect()[0].asDict()\n\n#select the cols with Distinct count=1 in an array\ncols_to_drop = [col for col in train_spark_NoID_NoDup.columns if col_counts[col] == 1 ]\n\n#drop the selected column\ntrain_spark_drop1Distinct = train_spark_NoID_NoDup.drop(*cols_to_drop)\nprint('Number of cols dropped: ',len(cols_to_drop))\nprint(\"After dorp column count after removing distince count =1 : \",len(train_spark_drop1Distinct.columns))"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_spark_drop1Distinct = train_spark_NoID_NoDup","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data cleanning (replace strange value in columns)\n* the value -99999 looks werid/strange value, may need to replace"},{"metadata":{"trusted":true},"cell_type":"code","source":"# count name of werid number for each columns and sort by count\ncount_series = train[train<-100000].count()\ndf_count=count_series.to_frame().T\ndf_count.max().sort_values(ascending=False).head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# only var3 column has the strange value -999999\n# train_spark_drop1Distinct.filter(train_spark_drop1Distinct.var3 == -999999).toPandas()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_spark_drop = train_spark_drop1Distinct.withColumn('var3', F.when(train_spark_drop1Distinct['var3']< -100000,train['var3'].median()).otherwise(train_spark_drop1Distinct['var3']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_spark_drop.filter(train_spark_drop.var3 == -999999).toPandas()\ntrain_spark_drop.describe().toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Check Null\n* there is no null value columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.sql.functions import*\ntrain_spark_drop.select([count(when(isnan(c), c)).alias(c) for c in train_spark_drop.columns]).toPandas()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3.Normalize Imbalanced data\n* Most machine learning algorithms work best when the number of samples in each class are about equal. This is because most algorithms are designed to maximize accuracy and reduce error."},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nprint('Histogram plot after process data set')\nsns.countplot('TARGET', data=train_spark_drop.toPandas())\nplt.title('Target size', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pyspark.sql.functions as f\nfrom pyspark.sql.window import Window\ndf_Target_count_2 = train_spark_drop.groupBy(\"TARGET\").count()\ndf_Target_count_2.show()\ndf_Target_count_2 = df_Target_count_2.withColumn('ratio', f.col('count')/f.sum('count').over(Window.partitionBy()))\ndf_Target_count_2.orderBy('ratio', ascending=False).show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Resampling techniques — Undersample majority class\nStratified Sampling can be used as well "},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import *\nstart_time = time()\n\ntrain_spark_1 = train_spark_drop.filter(\"TARGET =1\")\ntrain_spark_0_OG = train_spark_drop.filter(\"TARGET =0\")\nratio = train_spark_1.count()/train_spark_0_OG.count()\nprint(\"Before Undersample 1 and 0: \",ratio)\ntrain_spark_0, train_spark_dump = train_spark_0_OG.randomSplit([ratio,1-ratio])\n\n#concate two dataframe together\ntrain_spark_Undersample= train_spark_0.union(train_spark_1)\nratio_Undersample = train_spark_Undersample.filter(\"TARGET =1\").count()/train_spark_Undersample.filter(\"TARGET =0\").count()\nprint(\"After Undersample 1 and 0: \",ratio_Undersample)\nend_time = time()\nelapsed_time = end_time - start_time\nprint(\"Time for this session: %.3f seconds\" % elapsed_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nprint('After Normalize Target distribution ')\nsns.countplot('TARGET', data=train_spark_Undersample.toPandas())\nplt.title('Target size', fontsize=14)\nplt.show()\ntrain_spark_Undersample.groupBy(\"TARGET\").count().show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Assembly"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_undersample = VectorAssembler(inputCols=train_spark_Undersample.columns[:-1],outputCol=\"features\")\nfeature_vector_undersample= feature_undersample.transform(train_spark_Undersample)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data split"},{"metadata":{"trusted":true},"cell_type":"code","source":"(trainingData_undersample, testData_undersample) = feature_vector_undersample.randomSplit([0.8, 0.2],seed = 11)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.Modelling "},{"metadata":{},"cell_type":"markdown","source":"# 4.1 Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic Regression\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"TARGET\", featuresCol=\"features\", maxIter=5)\nlrModel = lr.fit(trainingData_undersample)\nimport matplotlib.pyplot as plt\nimport numpy as np\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta, label =\"LogisticRegression\")\nplt.ylabel(\"Beta Coefficients\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainSet = lrModel.summary\nroc = trainSet.roc.toPandas()\nplt.plot(roc[\"FPR\"],roc[\"TPR\"], \"-r\", label=\"Logistic Regression ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.ylabel(\"False Positive Rate\")\nplt.xlabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.show()\nprint(\"TrainSet areaUnderROC: \" + str(trainSet.areaUnderROC))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.2 RandomForest\nRandom Forests are a group of decision trees, that uses Mojority of Votingfor each of the decision tree. This algorithm provides less risk of overfitting by combining decision trees."},{"metadata":{"trusted":true},"cell_type":"code","source":"## Random Forest Classifier\nfrom pyspark.ml.classification import RandomForestClassifier\n# Creating RandomForest model.\nrf = RandomForestClassifier(labelCol=\"TARGET\", featuresCol=\"features\", numTrees=2)\n## train the model\nrfModel = rf.fit(trainingData_undersample)\n## make predictions\npredictions = rfModel.transform(testData_undersample)\nrfPredictions = predictions.select(\"TARGET\", \"prediction\", \"probability\")\nrfPredictions.show(10)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Besides Logistic Regression (like Decision Trees or Random Forest which lack a model summary), therefore I used class CurveMetrics(BinaryClassificationMetrics):"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n# Python: https://spark.apache.org/docs/latest/api/python/_modules/pyspark/mllib/common.html\nclass CurveMetrics(BinaryClassificationMetrics):\n    def __init__(self, *args):\n        super(CurveMetrics, self).__init__(*args)\n\n    def _to_list(self, rdd):\n        points = []\n        # Note this collect could be inefficient for large datasets \n        # considering there may be one probability per datapoint (at most)\n        # The Scala version takes a numBins parameter, \n        # but it doesn't seem possible to pass this from Python to Java\n        for row in rdd.collect():\n            # Results are returned as type scala.Tuple2, \n            # which doesn't appear to have a py4j mapping\n            points += [(float(row._1()), float(row._2()))]\n        return points\n\n    def get_curve(self, method):\n        rdd = getattr(self._java_model, method)().toJavaRDD()\n        return self._to_list(rdd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the probability of getting the output either as 0 or 1\n# Returns as a list (false positive rate, true positive rate)\npreds = predictions.select(\"TARGET\",\"probability\").rdd.map(lambda row: (float(row[\"probability\"][1]), float(row[\"TARGET\"])))\npoints = CurveMetrics(preds).get_curve('roc')\n\nplt.figure()\nx_val = [x[0] for x in points]\ny_val = [x[1] for x in points]\nplt.title(\"ROC\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot(x_val, y_val, \"-r\", label=\"Random Forest Regression ROC Curve\")\nplt.legend(loc=\"lower right\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## evaluate the Rnadom Forest Classifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator(labelCol=\"TARGET\")\nevaluator.evaluate(predictions)\nprint(\"Random Forest Test areaUnderROC: {}\".format(evaluator.evaluate(predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4.3 Gradient-Boosted Tree Classifier\nGradient-Boosted Tree Classifiers are also a group of decision trees and they iteratively train decision trees in order to minimize a loss function."},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml import Pipeline\n## Gradient-Boosted Tree Classifier\nfrom pyspark.ml.classification import GBTClassifier\nstages = []\ngbt = GBTClassifier(labelCol=\"TARGET\", featuresCol=\"features\",maxIter=5)\npipeline = Pipeline(stages=stages+[gbt])\ngbtModel = pipeline.fit(trainingData_undersample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\npredictions =gbtModel.transform(testData_undersample)\n# Show predictions\npredictions.select(\"TARGET\", \"prediction\", \"probability\").show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the probability of getting the output either as 0 or 1\n# Returns as a list (false positive rate, true positive rate)\npreds_GBT = predictions.select(\"TARGET\",\"probability\").rdd.map(lambda row: (float(row[\"probability\"][1]), float(row[\"TARGET\"])))\npoints_GBT = CurveMetrics(preds_GBT).get_curve('roc')\n\nplt.figure()\nx_val = [x[0] for x in points_GBT]\ny_val = [x[1] for x in points_GBT]\nplt.title(\"ROC\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot(x_val, y_val,\"-r\", label=\"Gradient-Boosted Regression ROC Curve\")\nplt.legend(loc=\"lower right\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator(labelCol=\"TARGET\")\nprint(\"GBT Test Area Under ROC:\"  + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. ParamGridBuilder and CrossValidator\n find the best model or parameters for a given dataset to improve the performance\n# *I did run the code because it is time consuming and PC power.*****"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = ParamGridBuilder()\\\n .addGrid(lr.aggregationDepth,[2,5,10])\\\n .addGrid(lr.elasticNetParam,[0.0, 0.5, 1.0])\\\n .addGrid(lr.fitIntercept,[False, True])\\\n .addGrid(lr.maxIter,[10, 100, 1000])\\\n .addGrid(lr.regParam,[0.01, 0.5, 2.0]) \\\n .build()"},{"metadata":{},"cell_type":"markdown","source":"Model tuning, it find the best model or parameters for a given dataset to improve the performance."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=2)\n# Run cross validations\ncvModel = cv.fit(trainingData_undersample)\npredict_train=cvModel.transform(trainingData_undersample)\npredict_test=cvModel.transform(testData_undersample)\nprint(\"Cross-validation areaUnderROC for train set is {}\".format(evaluator.evaluate(predict_train)))\nprint(\"Cross-validation areaUnderROC for test set is {}\".format(evaluator.evaluate(predict_test)))"},{"metadata":{},"cell_type":"markdown","source":"# Resampling Techniques — Oversample minority class\n* Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don’t have a ton of data to work with.\n\n* panda dataframe I would use SMOTE to oversample (I used SMOTE for panda analysis which I did)"},{"metadata":{},"cell_type":"markdown","source":"# Important Note\n\nAlways split into test and train sets BEFORE trying oversampling techniques! Oversampling before splitting the data can allow the exact same observations to be present in both the test and train sets. This can allow our model to simply memorize specific data points and cause overfitting and poor generalization to the test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"(trainingData_spark, testData_spark) = train_spark_drop.randomSplit([0.8, 0.2],seed = 11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from time import *\nstart_time = time()\n\ntrain_spark_1_over_OG = trainingData_spark.filter(\"TARGET =1\")\ntrain_spark_0_over = trainingData_spark.filter(\"TARGET =0\")\nratio = train_spark_1_over_OG.count()/train_spark_0_over.count()\nprint(\"Before oversample ratio 1 and 0: \",ratio)\nsampleRatio = train_spark_0_over.count()/trainingData_spark.count()\nprint(\"sampleRatio:\", sampleRatio)\n\n# duplicate the minority rows\n# explode_range\nexplode_range = range(int(train_spark_0_over.count()/train_spark_1_over_OG.count()))\ntrain_spark_1_over = train_spark_1_over_OG.withColumn(\"dummy\", explode(array([lit(x) for x in explode_range]))).drop('dummy')\n#print(train_spark_1_over.count())\n\n#concate two dataframe together\ntrain_spark_oversample= train_spark_1_over.union(train_spark_0_over)\nratio_oversample = (train_spark_oversample.filter(\"TARGET =1\")).count()/(train_spark_oversample.filter(\"TARGET =0\")).count()\nprint(\"After oversample ratio 1 and 0: \",ratio_oversample)\nend_time = time()\nelapsed_time = end_time - start_time\nprint(\"Time for this session: %.3f seconds\" % elapsed_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nprint('After Normalize Target distribution ')\nsns.countplot('TARGET', data=train_spark_oversample.toPandas())\nplt.title('Target size', fontsize=14)\nplt.show()\ntrain_spark_oversample.groupBy(\"TARGET\").count().show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature assembly"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_spark_oversample, testData_spark\nfeature_oversample_train = VectorAssembler(inputCols=train_spark_oversample.columns[:-1],outputCol=\"features\")\ntrainingData_oversample= feature_oversample_train.transform(train_spark_oversample)\n\nfeature_oversample_test = VectorAssembler(inputCols=testData_spark.columns[:-1],outputCol=\"features\")\ntestData_oversample= feature_oversample_test.transform(testData_spark)\n\n\n#trainingData_oversample, testData_oversample\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Modelling\n# 6.1 Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Logistic Regression\nfrom pyspark.ml.classification import LogisticRegression\nlr = LogisticRegression(labelCol=\"TARGET\", featuresCol=\"features\", maxIter=5)\nlrModel = lr.fit(trainingData_oversample)\nimport matplotlib.pyplot as plt\nimport numpy as np\nbeta = np.sort(lrModel.coefficients)\nplt.plot(beta, label =\"LogisticRegression\")\nplt.ylabel(\"Beta Coefficients\")\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainSet = lrModel.summary\nroc = trainSet.roc.toPandas()\nplt.plot(roc[\"FPR\"],roc[\"TPR\"], \"-r\", label=\"Logistic Regression ROC Curve\")\nplt.legend(loc=\"lower right\")\nplt.ylabel(\"False Positive Rate\")\nplt.xlabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.show()\nprint(\"TrainSet areaUnderROC: \" + str(trainSet.areaUnderROC))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.2 RandomForest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#trainingData_oversample, testData_oversample\n## Random Forest Classifier\nfrom pyspark.ml.classification import RandomForestClassifier\n# Creating RandomForest model.\nrf = RandomForestClassifier(labelCol=\"TARGET\", featuresCol=\"features\", numTrees=2)\n## train the model\nrfModel = rf.fit(trainingData_oversample)\n## make predictions\npredictions = rfModel.transform(testData_oversample)\nrfPredictions = predictions.select(\"TARGET\", \"prediction\", \"probability\")\nrfPredictions.show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the probability of getting the output either as 0 or 1\n# Returns as a list (false positive rate, true positive rate)\npreds = predictions.select(\"TARGET\",\"probability\").rdd.map(lambda row: (float(row[\"probability\"][1]), float(row[\"TARGET\"])))\npoints = CurveMetrics(preds).get_curve('roc')\n\nplt.figure()\nx_val = [x[0] for x in points]\ny_val = [x[1] for x in points]\nplt.title(\"ROC\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot(x_val, y_val, \"-r\", label=\"Random Forest Regression ROC Curve\")\nplt.legend(loc=\"lower right\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## evaluate the Rnadom Forest Classifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluator = BinaryClassificationEvaluator(labelCol=\"TARGET\")\nevaluator.evaluate(predictions)\nprint(\"Random Forest Test areaUnderROC: {}\".format(evaluator.evaluate(predictions)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6.3 Gradient-Boosted Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"#trainingData_oversample, testData_oversample\nfrom pyspark.ml import Pipeline\n## Gradient-Boosted Tree Classifier\nfrom pyspark.ml.classification import GBTClassifier\nstages = []\ngbt = GBTClassifier(labelCol=\"TARGET\", featuresCol=\"features\",maxIter=5)\npipeline = Pipeline(stages=stages+[gbt])\ngbtModel = pipeline.fit(trainingData_oversample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pyspark.ml.evaluation import MulticlassClassificationEvaluator\npredictions =gbtModel.transform(testData_oversample)\n# Show predictions\npredictions.select(\"TARGET\", \"prediction\", \"probability\").show(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the probability of getting the output either as 0 or 1\n# Returns as a list (false positive rate, true positive rate)\npreds_GBT = predictions.select(\"TARGET\",\"probability\").rdd.map(lambda row: (float(row[\"probability\"][1]), float(row[\"TARGET\"])))\npoints_GBT = CurveMetrics(preds_GBT).get_curve('roc')\n\nplt.figure()\nx_val = [x[0] for x in points_GBT]\ny_val = [x[1] for x in points_GBT]\nplt.title(\"ROC\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.plot(x_val, y_val,\"-r\", label=\"Gradient-Boosted Regression ROC Curve\")\nplt.legend(loc=\"lower right\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator = BinaryClassificationEvaluator(labelCol=\"TARGET\")\nprint(\"GBT Test Area Under ROC:\"  + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"warning-----------------------------------------------------------------------------------------------warning\n*  code below is just extra things which I couldn't utilize. Could have good usage"},{"metadata":{},"cell_type":"markdown","source":"# Feature selection Matrix (extra need bit more understanding and usage)\n* find it interesting, but no time to understand optimiza the usage\n* try it out with undersample data, since the data volumn is not high."},{"metadata":{"trusted":true},"cell_type":"code","source":"# inspire from https://stackoverflow.com/questions/51831874/how-to-get-correlation-matrix-values-pyspark/51834729\nfrom pyspark.ml.stat import Correlation\nfrom pyspark.ml.feature import VectorAssembler\n\n#drop target column\ncorr_df = train_spark_oversample.drop(\"TARGET\")\n# copying columns names\ncolumn_names = corr_df.columns\n\n\n# get correlation matrix\n#matrix = Correlation.corr(feature_vector)\n\n\nvector_col = \"corr_features\"\nassembler = VectorAssembler(inputCols=train_spark_oversample.columns[:-1], \n                            outputCol=vector_col)\nfeature_vector = assembler.transform(train_spark_oversample).select(vector_col)\nmatrix = Correlation.corr(feature_vector, vector_col)\n\n# Setting column names of datafram\n\nconvert_matrix = (matrix.collect()[0][0]).toArray().tolist()\ndf_matrix  = pd.DataFrame( convert_matrix, column_names)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_matrix  = pd.DataFrame( convert_matrix, column_names,column_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.ticker as ticker\nimport matplotlib.cm as cm\nimport matplotlib as mpl\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf_matrix_04 = df_matrix[(df_matrix[:]>0.5)|(df_matrix[:]<-0.5)]\ndf_matrix_04.head(20)\nfig, ax = plt.subplots(figsize=(10,10)) \nsns.heatmap(df_matrix_04, cmap=\"YlGnBu\")\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# collection of coloumns which need to be reomoved \n\ncols_to_drop_1 = []\n \nfor col in range(len(df_matrix.columns)):\n    for row in range(col):\n        if (df_matrix.iloc[row,col] >0.5 \\\n            or df_matrix.iloc[row,col] < -0.5) \\\n            and (df_matrix.columns[row] not in cols_to_drop_1):\n            cols_to_drop_1.append(df_matrix.columns[col])\n\ntrain_spark_filter = train_spark_oversample.drop(*cols_to_drop_1)\n\nprint(\"Number of cols dropped: \",  len(cols_to_drop_1))\nprint(\"Number of cols train_spark_filter: \",  len(train_spark_filter.columns))\n# there is duplicate coloumn name therefore the size the big, need sometime to look into this. \n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}