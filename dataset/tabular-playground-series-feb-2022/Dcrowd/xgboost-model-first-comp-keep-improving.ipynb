{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Feb 2022\n\nThis dataset is aimed to practive ML skills by using the genomic analysis technique data with some data compression and data loss.\n\nATATGGCCTT will be represented as A2T4G2C2 instead of the sequence.\n\nLet's start with checking the data set and let's find out some direction for analysis.\n\nPS. This is my first notebook for a competition and I find it very fun. I hope you guys will enjoy watch it and please feel free to leave a comment for further discussion. What I have done was learnt by other experts on Kaggle or video. I think it will be great to have a practical notebook and try to ask for suggestion from others instead of keep watching the video. Thanks for watching & have fun!","metadata":{"id":"46fecb9a"}},{"cell_type":"markdown","source":"## Next Step & Discussion:\n\nUMAP may not be a good technique for this data set since there is no pattern & cluster can be found. It is great to preview the scatter plot first since it gave a lot of information that the UMAP is helpful or not.\nGridSearchCV is great as well since it can help to tune the hyper-parameter. GridSearchCV can be used again for further tuning but it may need to cost a lot of time and computational power. Next, the other model for classification can be used since the accuracy for XGBoost is hard to improve or it is hard to adjust the perfect one. Let's try again in the future.","metadata":{"id":"4f26e1a0"}},{"cell_type":"markdown","source":"#### Result of the submission\n\n17/02/2022 First trial (XGBoost): Score = 0.90366, Rank = 701\n\n18/02/2022 Second trial (XGBoost) : Score = 0.91611, Rank = 720 (Accuracy was increased)\n\n19/02/2022 Third trial (XGBoost with higher lambda & alpha): Score = 0.90005, Rank = NA\n\n19/02/2022 Fourth trial (XGBoost with 100 lambda & alpha): Score = 0.82967, Rank = NA\n\n19/02/2022 Fifth trial (XGBoost with no gamma + Second trial regularization): Score = 0.91781, Rank = 724\n\n21/02/2022 Sixth trial (XGBoost with 10-fold & depth adjusted): Score = 0.94448, Rank = 679\n\n22/02/2022 Seventh trial (XGBoost with 5-fold & PCA): Score = 0.86250, Rank = NA\n\n22/02/2022 Eighth trial (XGBoost with 5-fold & features amount PCA): Score = 0.87796, Rank = NA \n\n26/02/2022 Ninth trial (XGBoost with GridSearchCV): Score = 0.93042, Rank = NA\n\n26/02/2022 Tenth trial (XGBoost with UMAP): Score = 0.78098, Rank = NA","metadata":{"id":"fc3e6491"}},{"cell_type":"code","source":"# libraries set up\nimport pandas as pd\nimport numpy as np\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"id":"a02bf331","execution":{"iopub.status.busy":"2022-02-25T21:04:57.252172Z","iopub.execute_input":"2022-02-25T21:04:57.252664Z","iopub.status.idle":"2022-02-25T21:04:58.480481Z","shell.execute_reply.started":"2022-02-25T21:04:57.252508Z","shell.execute_reply":"2022-02-25T21:04:58.479531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read the data\ntrain_path = '../input/tabular-playground-series-feb-2022/train.csv'\ntest_path = '../input/tabular-playground-series-feb-2022/test.csv'\n\ntrain_set = pd.read_csv(train_path, index_col = 0)\ntest_set = pd.read_csv(test_path, index_col = 0)","metadata":{"id":"a24bcd40","execution":{"iopub.status.busy":"2022-02-25T21:04:58.482507Z","iopub.execute_input":"2022-02-25T21:04:58.482776Z","iopub.status.idle":"2022-02-25T21:05:39.071089Z","shell.execute_reply.started":"2022-02-25T21:04:58.482744Z","shell.execute_reply":"2022-02-25T21:05:39.069778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preview for the train data\ndf_train = train_set.copy()\ndf_train.head()","metadata":{"id":"03e87493","outputId":"e88af3ed-ca2d-4bef-bd92-188b1f8ad668","scrolled":true,"execution":{"iopub.status.busy":"2022-02-25T21:05:39.072514Z","iopub.execute_input":"2022-02-25T21:05:39.072826Z","iopub.status.idle":"2022-02-25T21:05:39.307466Z","shell.execute_reply.started":"2022-02-25T21:05:39.072779Z","shell.execute_reply":"2022-02-25T21:05:39.306648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preview for the test data\ndf_test = test_set.copy()\ndf_test.head()","metadata":{"id":"6a401eda","outputId":"44455507-f2ae-46b6-b9de-55b36d0faac0","execution":{"iopub.status.busy":"2022-02-25T21:05:39.309328Z","iopub.execute_input":"2022-02-25T21:05:39.309865Z","iopub.status.idle":"2022-02-25T21:05:39.539462Z","shell.execute_reply.started":"2022-02-25T21:05:39.309817Z","shell.execute_reply":"2022-02-25T21:05:39.538604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the shape for train set\ndf_train.shape","metadata":{"id":"3659bace","outputId":"c648dd50-5dd8-4b2e-d49a-8acc641bde58","execution":{"iopub.status.busy":"2022-02-25T21:05:39.541863Z","iopub.execute_input":"2022-02-25T21:05:39.542118Z","iopub.status.idle":"2022-02-25T21:05:39.549493Z","shell.execute_reply.started":"2022-02-25T21:05:39.542085Z","shell.execute_reply":"2022-02-25T21:05:39.548487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the shape for test set\ndf_test.shape","metadata":{"id":"4f8eb696","outputId":"0c656915-f6b3-46bc-960a-b9b2d1569908","execution":{"iopub.status.busy":"2022-02-25T21:05:39.551202Z","iopub.execute_input":"2022-02-25T21:05:39.551519Z","iopub.status.idle":"2022-02-25T21:05:39.563071Z","shell.execute_reply.started":"2022-02-25T21:05:39.551485Z","shell.execute_reply":"2022-02-25T21:05:39.56236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 200000 set data for model training. \n\n100000 testing sample was used for prediction and check with the performance by upload the result to Kaggle.\n\nNow, let's check the whole data set and see is there any duplicate/non-numeric data/ missing value and some column needed to be worked on for feature engineering.","metadata":{"id":"b571014e"}},{"cell_type":"markdown","source":"#### Missing value\n\nLet's check is there any missing value for each sample in both training and testing set","metadata":{"id":"df440eac"}},{"cell_type":"code","source":"# count the missing value in both data set\ntrain_missing_count = 0\nfor i in df_train.columns:\n    train_missing_count += df_train[i].isna().sum()\n\ntest_missing_count = 0\nfor i in df_test.columns:\n    test_missing_count += df_test[i].isna().sum()\n    \nprint('Missing value for train set: {0}'.format(train_missing_count))\nprint('Missing value for test set: {0}'.format(test_missing_count))","metadata":{"id":"5e32fc88","outputId":"ecc30f37-d353-446f-c056-e3b25c6fae95","execution":{"iopub.status.busy":"2022-02-25T21:05:39.564256Z","iopub.execute_input":"2022-02-25T21:05:39.565214Z","iopub.status.idle":"2022-02-25T21:05:39.931874Z","shell.execute_reply.started":"2022-02-25T21:05:39.565169Z","shell.execute_reply":"2022-02-25T21:05:39.9309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is no missing value for the data set and we don't have to fill with other value. All the information was remained & obtained.\n\nNext, we have to work on the duplicate.","metadata":{"id":"9ff195c7"}},{"cell_type":"markdown","source":"#### Duplicate","metadata":{"id":"960569c1"}},{"cell_type":"markdown","source":"Let's check the sample number for duplicate sample","metadata":{"id":"8ed5b34c"}},{"cell_type":"code","source":"# print the sample size for duplicate in both train and test set\nprint(f'Duplicate sample in train set: {df_train.duplicated().sum()}')\nprint(f'Duplicate sample in test set: {df_test.duplicated().sum()}')","metadata":{"id":"114f8662","outputId":"0d4007b9-ab2c-4717-88ae-ec07371f8531","execution":{"iopub.status.busy":"2022-02-25T21:05:39.933398Z","iopub.execute_input":"2022-02-25T21:05:39.934115Z","iopub.status.idle":"2022-02-25T21:05:42.510835Z","shell.execute_reply.started":"2022-02-25T21:05:39.934067Z","shell.execute_reply":"2022-02-25T21:05:42.509678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's remove the duplicates for training set since it may cause overfitting issues.","metadata":{"id":"508564f9"}},{"cell_type":"code","source":"# remove the duplicate and replace training set\ndf_train.drop_duplicates(subset = None, keep = 'first', inplace = True)","metadata":{"id":"aaa9fb0a","execution":{"iopub.status.busy":"2022-02-25T21:05:42.512531Z","iopub.execute_input":"2022-02-25T21:05:42.512804Z","iopub.status.idle":"2022-02-25T21:05:44.698587Z","shell.execute_reply.started":"2022-02-25T21:05:42.512772Z","shell.execute_reply":"2022-02-25T21:05:44.697504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Object type encoding/ mapping","metadata":{"id":"28786bac"}},{"cell_type":"markdown","source":"All the features should be numeric data except the target column but we can still check about the whole data set.","metadata":{"id":"9114b9e7"}},{"cell_type":"code","source":"# check for the object column in training set\nobject_list = []\nfor i in df_train.columns:\n    if df_train[i].dtypes == 'object':\n        object_list.append(i)\nprint(object_list)","metadata":{"id":"c2c860c2","outputId":"6e5108e9-9bfd-40ad-b2df-615ffe48e073","execution":{"iopub.status.busy":"2022-02-25T21:05:44.700354Z","iopub.execute_input":"2022-02-25T21:05:44.700603Z","iopub.status.idle":"2022-02-25T21:05:44.72057Z","shell.execute_reply.started":"2022-02-25T21:05:44.700568Z","shell.execute_reply":"2022-02-25T21:05:44.719224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that only the target columns was object type and it is the only column that we needed to do some feature engineering. We can check for the test set as well.","metadata":{"id":"497fa8f5"}},{"cell_type":"code","source":"# check for the object column in testing set\nobject_list = []\nfor i in df_test.columns:\n    if df_test[i].dtypes == 'object':\n        object_list.append(i)\nprint(object_list)","metadata":{"id":"d5a103ee","outputId":"fd728594-9d85-405b-a855-eba6615c2303","execution":{"iopub.status.busy":"2022-02-25T21:05:44.722376Z","iopub.execute_input":"2022-02-25T21:05:44.722722Z","iopub.status.idle":"2022-02-25T21:05:44.736241Z","shell.execute_reply.started":"2022-02-25T21:05:44.722662Z","shell.execute_reply":"2022-02-25T21:05:44.735575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, all the features parameter was numeric and it can be applied easily for our model without any transformation at this moment.\n\nLet's work on the target column in training set and change it to numeric value that's the model is readable.\n\nThere is two ways for handling this column: (1) Ordinal Encoding (2) One-Hot Encoding. From the description of the data set, there is ten type of bacteria as our target and Orginal Encoding will be a better way to deal with it since it is our target column and we dont want much column for each type of bacteria.","metadata":{"id":"cced40a3"}},{"cell_type":"code","source":"# let's check about what type of bacteria\nlist(df_train['target'].unique())\n\n# let's assign a number for each type\nordinal_target = dict(enumerate(df_train['target'].unique()))\nordinal_target = {y:x for x,y in ordinal_target.items()}\nordinal_target","metadata":{"id":"b7ef9695","outputId":"9b5e9322-a48a-4d99-dd89-f3798c7a99f3","execution":{"iopub.status.busy":"2022-02-25T21:05:44.737553Z","iopub.execute_input":"2022-02-25T21:05:44.737828Z","iopub.status.idle":"2022-02-25T21:05:44.774658Z","shell.execute_reply.started":"2022-02-25T21:05:44.737796Z","shell.execute_reply":"2022-02-25T21:05:44.773644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reverse_target = {y:x for x,y in ordinal_target.items()}\nreverse_target","metadata":{"id":"d3ffd827","outputId":"548bb0a5-fa56-44f2-f0cf-6833d537e545","execution":{"iopub.status.busy":"2022-02-25T21:05:44.775875Z","iopub.execute_input":"2022-02-25T21:05:44.776099Z","iopub.status.idle":"2022-02-25T21:05:44.783486Z","shell.execute_reply.started":"2022-02-25T21:05:44.776072Z","shell.execute_reply":"2022-02-25T21:05:44.782742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make another copy to prevent data change\nlabel_df_train = df_train.copy()\n\n# let's map with the dict that we create in the data set\nlabel_df_train['target'] = label_df_train['target'].map(ordinal_target)\n\n# let's check the data set after mapping/ ordinal encoding\nlabel_df_train.head()","metadata":{"id":"90f4882e","outputId":"b39896c6-6e40-4eeb-e665-f6d457bf38d1","execution":{"iopub.status.busy":"2022-02-25T21:05:44.786976Z","iopub.execute_input":"2022-02-25T21:05:44.787237Z","iopub.status.idle":"2022-02-25T21:05:44.98848Z","shell.execute_reply.started":"2022-02-25T21:05:44.787207Z","shell.execute_reply":"2022-02-25T21:05:44.987592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After the mapping, the data was ready for training the model and we can try it in a different.\n\nLet's work a simple XGBoost model and check with the result first.","metadata":{"id":"f0831659"}},{"cell_type":"markdown","source":"### XGBoost","metadata":{"id":"OfiL_gZ7ztjc"}},{"cell_type":"markdown","source":"In the XGBoost library, it got XGBoost Regressor & XGBoost Classifier. In the prediction, classification was needed and XGB Classifier will be used as the model.","metadata":{"id":"5a6c01d0"}},{"cell_type":"code","source":"# set up for machine learning libraries\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","metadata":{"id":"7a7e05c1","execution":{"iopub.status.busy":"2022-02-25T21:05:44.990028Z","iopub.execute_input":"2022-02-25T21:05:44.99028Z","iopub.status.idle":"2022-02-25T21:05:45.180054Z","shell.execute_reply.started":"2022-02-25T21:05:44.990248Z","shell.execute_reply":"2022-02-25T21:05:45.179195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Spliting of the data in data set by slicing","metadata":{"id":"a11b7e28"}},{"cell_type":"code","source":"# slice the dataset for features and target\nX = label_df_train.iloc[:,:-1].values\ny = label_df_train.iloc[:,-1].values.reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2)","metadata":{"id":"cc92da28","execution":{"iopub.status.busy":"2022-02-25T21:05:45.181133Z","iopub.execute_input":"2022-02-25T21:05:45.181961Z","iopub.status.idle":"2022-02-25T21:05:46.142567Z","shell.execute_reply.started":"2022-02-25T21:05:45.18192Z","shell.execute_reply":"2022-02-25T21:05:46.141167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Ninth trial with GrisSearchCV","metadata":{"id":"SPsN9VaAwSDq"}},{"cell_type":"markdown","source":"By using the PCA, the performance for the model is not that well for last trial. GridSearchCV is a great technique to help searching the best hyperparameter instead of try it one by one. Let's try with few combination and search for the best accuracy. It will repeat 540 fits to ensure that best hyperparameter was found. The parameters will be focused on n_estimators, max_depth & learning_rate.\n\nDue to the cost of time, K-fold will not be used since it may cost more time to find the hyperparameter.\nAfter the bset parameter was found, the model will be trained again and predict the final test set for accuracy checking.\n\nPS. the best model may not be used since the random_state was forgot to reproduce the result and the accuracy should be closed to the others","metadata":{"id":"0Rglh4elvs4f"}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import GridSearchCV\n\n# make a dictionary of hyperparameter to search\nsearch_space = {\n    \"n_estimators\" : [500, 1000],\n    \"max_depth\" : [8 , 10],\n    \"learning_rate\" : [0.01, 0.1, 1]\n}\n# build a xgb classifier model without the hyperparameter that we want to check\nmodel = XGBClassifier(objective = 'multi:softmax', booster = 'gbtree',\n                        eval_metric = 'auc', tree_method = 'hist', use_label_encoder = False)\n# build a GridSearchCV model to start for the searching of hyperparameter\nGS = GridSearchCV(estimator = model, # target model\n                  param_grid = search_space, # dictionary of the parameters\n                  cv = 2, # 2 cv to minimize the time for training\n                  verbose = 3 # message for the searching progress\n)\nGS.fit(X_train, y_train)","metadata":{"id":"TjkeNZFAZXDM","outputId":"fd0ba055-2911-4ec5-a40a-1405f964e356","execution":{"iopub.status.busy":"2022-02-25T21:05:46.144078Z","iopub.execute_input":"2022-02-25T21:05:46.144344Z","iopub.status.idle":"2022-02-25T21:05:54.562449Z","shell.execute_reply.started":"2022-02-25T21:05:46.144313Z","shell.execute_reply":"2022-02-25T21:05:54.560695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(GS.best_estimator_) # best model","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.563364Z","iopub.status.idle":"2022-02-25T21:05:54.563759Z","shell.execute_reply.started":"2022-02-25T21:05:54.56352Z","shell.execute_reply":"2022-02-25T21:05:54.563539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(GS.best_params_) # best hyperparameter","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.565718Z","iopub.status.idle":"2022-02-25T21:05:54.566327Z","shell.execute_reply.started":"2022-02-25T21:05:54.566111Z","shell.execute_reply":"2022-02-25T21:05:54.566137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(GS.best_score_) # best score for the model","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.56738Z","iopub.status.idle":"2022-02-25T21:05:54.567919Z","shell.execute_reply.started":"2022-02-25T21:05:54.567722Z","shell.execute_reply":"2022-02-25T21:05:54.567745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# slice the dataset for features and target\nX = label_df_train.iloc[:,:-1]\ny = label_df_train.iloc[:,-1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2)\n\nbest_model = XGBClassifier(objective = 'multi:softmax', booster = 'gbtree',\n                        eval_metric = 'auc', tree_method = 'hist', use_label_encoder = False,\n                          learning_rate = 0.1, max_depth = 8, n_estimators = 1000)\nbest_model.fit(X_train, y_train)\n\ny_pred = best_model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {acc}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.568946Z","iopub.status.idle":"2022-02-25T21:05:54.569471Z","shell.execute_reply.started":"2022-02-25T21:05:54.56928Z","shell.execute_reply":"2022-02-25T21:05:54.569304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = best_model.predict(df_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.570466Z","iopub.status.idle":"2022-02-25T21:05:54.571002Z","shell.execute_reply.started":"2022-02-25T21:05:54.570811Z","shell.execute_reply":"2022-02-25T21:05:54.570835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take the result appeared most in 5-fold result and generate the submission to test the accuracy\nfrom scipy.stats import mode\nxgb_result = pd.DataFrame()\nxgb_result['row_id'] = df_test.index\nxgb_result['target'] = predictions\nxgb_result['target'] = xgb_result['target'].map(reverse_target)\nxgb_result.to_csv(\"xgb_GSCV_submission.csv\", index = False)","metadata":{"id":"JGocHfH8xosQ","execution":{"iopub.status.busy":"2022-02-25T21:05:54.572011Z","iopub.status.idle":"2022-02-25T21:05:54.572526Z","shell.execute_reply.started":"2022-02-25T21:05:54.572338Z","shell.execute_reply":"2022-02-25T21:05:54.572361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After usign the best parameter from GridSearchCV, the accuracy of the submission was determined as 0.93042. It is not great than the sixth submission but we can enhance it by using the K-fold as well.","metadata":{}},{"cell_type":"markdown","source":"##### 10th submission: UMAP + previous model","metadata":{}},{"cell_type":"markdown","source":"Previously, PCA was used for dimension but it is not that great since the cluster was very close. Sample will have a chance to be predicted as a wrong target.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nX_scaled = ss.fit_transform(X)\npca = PCA(n_components = None)\nL = pca.fit_transform(X_scaled)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.573589Z","iopub.status.idle":"2022-02-25T21:05:54.573966Z","shell.execute_reply.started":"2022-02-25T21:05:54.573777Z","shell.execute_reply":"2022-02-25T21:05:54.573803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's recap for the scatter plotted by using the first two components","metadata":{}},{"cell_type":"code","source":"def pca_scatter(pca, standardised_values, classifs):\n    foo = pca.transform(standardised_values)\n    bar = pd.DataFrame(zip(foo[:,0], foo[:,1], classifs), columns = [\"PC1\",\"PC2\",\"Class\"])\n    sns.lmplot(data = bar, x = \"PC1\", y = \"PC2\", hue = \"Class\", fit_reg = False)\n    \npca_scatter(pca, X_scaled, y)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.575107Z","iopub.status.idle":"2022-02-25T21:05:54.575453Z","shell.execute_reply.started":"2022-02-25T21:05:54.575268Z","shell.execute_reply":"2022-02-25T21:05:54.575295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of PCA or t-SNE, UMAP was another great technique to reduce the dimension and form cluster. With this model, it may be helpful to classify our sample.","metadata":{}},{"cell_type":"code","source":"from umap import UMAP\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# create the UMAP model\nreducer = UMAP(n_components = 2, n_neighbors = 10)\n\n# fit and transform all the data with training data set\nembedding = reducer.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.576637Z","iopub.status.idle":"2022-02-25T21:05:54.576985Z","shell.execute_reply.started":"2022-02-25T21:05:54.576802Z","shell.execute_reply":"2022-02-25T21:05:54.576828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot and check with the cluster according to the target class","metadata":{}},{"cell_type":"code","source":"# plot a scatter plot and check with the class\nplt.scatter(embedding[:,0], embedding[:,1], s = 5, c = y_train, cmap = 'Spectral')\n# set the axis and limit equal\nplt.gca().set_aspect('equal', 'datalim')\n# set the title\nplt.title('Visualizing data with UMAP', fontsize= 24)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.578128Z","iopub.status.idle":"2022-02-25T21:05:54.578471Z","shell.execute_reply.started":"2022-02-25T21:05:54.578289Z","shell.execute_reply":"2022-02-25T21:05:54.578315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that there is no pattern with UMAP. It may not be a good way to get a high accuracy. Let's try and check with the accuracy with XGBoost.","metadata":{}},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\n# create the model for dimension reduction & xgboost\n_umap = UMAP(n_components = 2, n_neighbors = 10)\n_xgboost = XGBClassifier(objective = 'multi:softmax', booster = 'gbtree',\n                        eval_metric = 'auc', tree_method = 'hist', use_label_encoder = False,\n                          learning_rate = 0.1, max_depth = 8, n_estimators = 1000)\n# create the pipeline for the progress\nXGB_UMAP_model = Pipeline([\n    ('umap', _umap),\n    ('xgb', _xgboost)\n])\n# fit the training set into the pipeline for training\nXGB_UMAP_model.fit(X_train, y_train)\n# predict the validation set\ny_pred = XGB_UMAP_model.predict(X_test)\n# calculate the accuracy for the model\nacc = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {acc}')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.579798Z","iopub.status.idle":"2022-02-25T21:05:54.580454Z","shell.execute_reply.started":"2022-02-25T21:05:54.580174Z","shell.execute_reply":"2022-02-25T21:05:54.580207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the validation accuracy, it seems UMAP may not be helpful for dimension reduction. May be dimension reduction is not a good technique for this data set. Let's try to predict the final test set and check with the accuracy.","metadata":{}},{"cell_type":"code","source":"predictions = XGB_UMAP_model.predict(df_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.581527Z","iopub.status.idle":"2022-02-25T21:05:54.581885Z","shell.execute_reply.started":"2022-02-25T21:05:54.581712Z","shell.execute_reply":"2022-02-25T21:05:54.58173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take the result appeared most in 5-fold result and generate the submission to test the accuracy\nfrom scipy.stats import mode\nxgb_result = pd.DataFrame()\nxgb_result['row_id'] = df_test.index\nxgb_result['target'] = predictions\nxgb_result['target'] = xgb_result['target'].map(reverse_target)\nxgb_result.to_csv(\"xgb_UMAP_submission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:05:54.583349Z","iopub.status.idle":"2022-02-25T21:05:54.583767Z","shell.execute_reply.started":"2022-02-25T21:05:54.583516Z","shell.execute_reply":"2022-02-25T21:05:54.583533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy (0.78098)for using UMAP was terrible and it is great to know about the preview of UMAP scatter plot do really helpful for the accuracy.","metadata":{}}]}