{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"I recently came across the Pycaret and I'm amazed at how much you can do with only a few lines of code. I thought it would be a great opporunity to compare the results of different models, including a blended model and a stacked model.   ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:37:34.860275Z","iopub.execute_input":"2022-02-28T03:37:34.861204Z","iopub.status.idle":"2022-02-28T03:37:34.883773Z","shell.execute_reply.started":"2022-02-28T03:37:34.861067Z","shell.execute_reply":"2022-02-28T03:37:34.883184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install pycaret[full]\nfrom pycaret.classification import * ","metadata":{"execution":{"iopub.status.busy":"2022-02-28T03:37:34.899954Z","iopub.execute_input":"2022-02-28T03:37:34.900369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notebook Sections\n1. [Setup](#Setup)\n2. [Compare Models](#Compare-Models)\n3. [Extra Trees Classifier LB Results: 0.94729](#Extra-Trees-Classifier-LB-Results:-0.94729)\n4. [Random Forest LB Results: 0.94443](#Random-Forest-LB-Results:-0.94443)\n5. [Linear Disciminant Analysis LB Results: 0.71391](#Linear-Disciminant-Analysis-LB-Results:-0.71391)\n5. [Blended Model LB Results: 0.95296](#Blended-Model-LB-Results:-0.95296)\n6. [Stacked Model LB Results: 0.96215](#Stacked-Model-LB-Results:-0.96215)","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/tabular-playground-series-feb-2022/train.csv', index_col='row_id')\ntest_df = pd.read_csv('../input/tabular-playground-series-feb-2022/test.csv', index_col='row_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# reduce memory usage from https://www.kaggle.com/hasanbasriakcay/tps-feb22-pycaret-model-comparisons-0-97-lb\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df\n\ntrain = reduce_mem_usage(df)\ntest = reduce_mem_usage(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nmodel_setup = setup(data=df, \n                    target='target',\n                    train_size=0.6,\n                    session_id=123,\n                    use_gpu = True,\n                    data_split_shuffle = True,\n                    data_split_stratify=True,\n                    silent=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compare Models","metadata":{}},{"cell_type":"code","source":"%%time\n#compare different models, only 2 folds to speed up time\ncompare_models(fold=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# extratrees model and results\net_model = create_model('et', fold=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntuned_et_model = tune_model(et_model, fold=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npredict_model(et_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npredict_model(tuned_et_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So interestingly the tuned model did much worse than the untuned model. \n\nI tested the tuned results for a couple other algorithms and they all came out worse than the original model. \n\nThis seems to be a common theme for others using Pycaret. https://github.com/pycaret/pycaret/issues/234","metadata":{}},{"cell_type":"markdown","source":"Next, I want to compare Leader Board scores for different models","metadata":{}},{"cell_type":"code","source":"#extratrees LB results using the original extra trees model\net_test_preds = predict_model(et_model, data=test_df)\nsubmission = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\nsubmission = pd.DataFrame(list(zip(submission.row_id, et_test_preds.Label)),columns = ['row_id', 'target'])\nsubmission.to_csv('submission_et.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extra Trees Classifier LB Results: 0.94729","metadata":{}},{"cell_type":"code","source":"%%time\n# randomforest model and results\nrf_model = create_model('rf', fold=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#random forest LB results using the original rf model\nrf_test_preds = predict_model(rf_model, data=test_df)\nsubmission = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\nsubmission = pd.DataFrame(list(zip(submission.row_id, rf_test_preds.Label)),columns = ['row_id', 'target'])\nsubmission.to_csv('submission_rf.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest LB Results: 0.94443","metadata":{}},{"cell_type":"code","source":"%%time\n# linear discriminant model and results\nlda_model = create_model('lda', fold=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lda results using the original lda model\nlda_test_preds = predict_model(lda_model, data=test_df)\nsubmission = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\nsubmission = pd.DataFrame(list(zip(submission.row_id, lda_test_preds.Label)),columns = ['row_id', 'target'])\nsubmission.to_csv('submission_lda.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Linear Disciminant Analysis LB Results: 0.71391","metadata":{}},{"cell_type":"markdown","source":"The models I ran on the unseen test dataset all performed similarly to what was predicted on the validation set (40% of the train dataset)","metadata":{}},{"cell_type":"markdown","source":"# Testing a Blended Model and a Stacked Model","metadata":{}},{"cell_type":"code","source":"%%time\nlightgbm_model = create_model('lightgbm', fold=3)\nknn_model = create_model('knn', fold=3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blended_model = blend_models(estimator_list = [et_model, rf_model, lda_model, knn_model, lightgbm_model], method='hard')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_model(blended_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blend_test_preds = predict_model(blended_model, data=test_df)\nsubmission = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\nsubmission = pd.DataFrame(list(zip(submission.row_id, blend_test_preds.Label)),columns = ['row_id', 'target'])\nsubmission.to_csv('submission_blend.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Blended Model LB Results: 0.95296","metadata":{}},{"cell_type":"code","source":"# stacking models\nstacked_model = stack_models(estimator_list = [et_model, rf_model, lda_model, lightgbm_model, knn_model],\n                       meta_model = et_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_model(stacked_model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_test_preds = predict_model(stacked_model, data=test_df)\nsubmission = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\nsubmission = pd.DataFrame(list(zip(submission.row_id, stack_test_preds.Label)),columns = ['row_id', 'target'])\nsubmission.to_csv('submission_stack.csv', index = False)\nsubmission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Stacked Model LB Results: 0.96215","metadata":{}},{"cell_type":"markdown","source":"The stacked model performed the best so it would be worth spending more time on the stacked model to try to improve performance there. ","metadata":{}}]}