{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p align=\"right\">\n  <img src=\"https://drive.google.com/uc?export=view&id=1JydOvj55Xgv_T-7LM3jqsOVac2PhQrJ-\"/>\n</p>\n\n<a id=\"Title\"></a>","metadata":{"execution":{"iopub.status.busy":"2022-02-01T19:01:58.539886Z","iopub.execute_input":"2022-02-01T19:01:58.540322Z","iopub.status.idle":"2022-02-01T19:01:58.578295Z","shell.execute_reply.started":"2022-02-01T19:01:58.540153Z","shell.execute_reply":"2022-02-01T19:01:58.57697Z"}}},{"cell_type":"markdown","source":"# <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:180%; text-align:center; border-radius: 24px 0;\">Bacterial Species Prediction, EDA + Model</p>\n\n>This notebook is a walk through guide for dealing with TPS Feb 2022 competition.\n>* The **objective** of this notebook is to apply step-by-step approach to solve tabular data competition.\n>* The **subject** of this notebook is a multi-classification task, based on the idea from the following [paper](https://www.frontiersin.org/articles/10.3389/fmicb.2020.00257/full).  \"Bacterial antibiotic resistance is becoming a significant health threat, and rapid identification of antibiotic-resistant bacteria is essential to save lives and reduce the spread of antibiotic resistance.\" Our task is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0;\">Table of Contents</p>\n* [1. Import of Libraries](#1)\n* [2. Data Loading and Initial Visualization](#2)\n* [3. Exploratory Data Analysis](#34)\n* [4. Feature engineering](#4)\n* [6. Feature Importance](#6)\n* [5. Modeling PyTorch NN Model](#5)  \n","metadata":{}},{"cell_type":"markdown","source":"<a id='1'></a>\n# <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0;\">1. Import of Libraries</p>","metadata":{}},{"cell_type":"code","source":"import numpy as np # Linear algebra.\nimport pandas as pd # Data processing, CSV file I/O (e.g. pd.read_csv).\nimport datatable as dt # Data processing, CSV file I/O (e.g. dt.fread).\n\nimport seaborn as sns # Visualization.\nimport matplotlib.pyplot as plt # Visualization.\n\n# Machine Learning block.\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nimport random\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import defaultdict\nfrom copy import deepcopy\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f'\\n[INFO] Libraries set up has been completed.')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:46:48.046082Z","iopub.execute_input":"2022-02-02T03:46:48.046543Z","iopub.status.idle":"2022-02-02T03:46:48.057047Z","shell.execute_reply.started":"2022-02-02T03:46:48.046498Z","shell.execute_reply":"2022-02-02T03:46:48.055927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='2'></a>\n# <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0;\">2. Data Loading and Initial Visualization</p>\n>\n> **Let's read the data first** (I strongly recommend using 'datatable' to for faster data reading). It reads the data 3x time faster than pandas:","metadata":{}},{"cell_type":"code","source":"%%time\ndf_train = dt.fread('../input/tabular-playground-series-feb-2022/train.csv').to_pandas()\ndf_test = dt.fread('../input/tabular-playground-series-feb-2022/test.csv').to_pandas()\ndf_sub = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\n\n# Datatable reads target as bool by default.\nmask_bool = df_train.dtypes == bool\nbool_train = df_train.dtypes[mask_bool].index\nbool_test = df_test.dtypes[mask_bool].index\n\ndf_train[bool_train] = df_train[bool_train].astype('int8')\ndf_test[bool_train] = df_test[bool_train].astype('int8')\n\ndf_train.info(verbose=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:48:35.313055Z","iopub.execute_input":"2022-02-02T02:48:35.313356Z","iopub.status.idle":"2022-02-02T02:48:43.950856Z","shell.execute_reply.started":"2022-02-02T02:48:35.313326Z","shell.execute_reply":"2022-02-02T02:48:43.950215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":">**Let's have a sanity check if we have any missing values:**","metadata":{"execution":{"iopub.status.busy":"2022-02-01T20:37:32.662145Z","iopub.execute_input":"2022-02-01T20:37:32.662855Z","iopub.status.idle":"2022-02-01T20:37:32.668549Z","shell.execute_reply.started":"2022-02-01T20:37:32.662801Z","shell.execute_reply":"2022-02-01T20:37:32.66726Z"}}},{"cell_type":"code","source":"miss_val_train =df_train.isna().any().sum()\nmiss_val_test = df_test.isna().any().sum()\n\nprint(f'\\n[INFO] {miss_val_train} missing value(s) has/have been detected in the train dataset.')\nprint(f'[INFO] {miss_val_test} missing value(s) has/have been detected in the test dataset.')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:48:43.951793Z","iopub.execute_input":"2022-02-02T02:48:43.952286Z","iopub.status.idle":"2022-02-02T02:48:44.050554Z","shell.execute_reply.started":"2022-02-02T02:48:43.952255Z","shell.execute_reply":"2022-02-02T02:48:44.049969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> We do not really need **\"row_id\"** column. It will help us to reduce memory usage even more.\n> Let's fix it and cast our dtypes to the smaller ones (references: [**link**](https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/294356)):","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    start_mem = df.memory_usage().sum()/1024**2\n    numerics = ['int8', 'int16', 'int32', 'int64',\n                'float16', 'float32', 'float64']\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n        limit = abs(df[col]).max()\n\n        for tp in numerics:\n            cond1 = str(col_type)[0] == tp[0]\n            if tp[0] == 'i': cond2 = limit <= np.iinfo(tp).max\n            else: cond2 = limit <= np.finfo(tp).max\n\n            if cond1 and cond2:\n                df[col] = df[col].astype(tp)\n                break\n\n    end_mem = df.memory_usage().sum()/1024**2\n    reduction = (start_mem - end_mem)*100/start_mem\n    if verbose:\n        print(f'[INFO] Mem. usage decreased to {end_mem:.2f}'\n              f' MB {reduction:.2f}% reduction.')\n    return df\n\ntarget = df_train.target\ndf_train.drop(columns=['row_id', 'target'], inplace=True)\ndf_test.drop(columns='row_id', inplace=True)\n\ndf_train = reduce_mem_usage(df_train, verbose=True)\ndf_train['target'] = target\ndf_test = reduce_mem_usage(df_test, verbose=True)\n\nprint('\\n')\ndf_train.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:48:44.051463Z","iopub.execute_input":"2022-02-02T02:48:44.052194Z","iopub.status.idle":"2022-02-02T02:49:13.421656Z","shell.execute_reply.started":"2022-02-02T02:48:44.052159Z","shell.execute_reply":"2022-02-02T02:49:13.420759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rc = {\n    \"axes.facecolor\":\"#FFF2D9\",\n    \"figure.facecolor\":\"#FFF2D9\",\n    \"axes.edgecolor\":\"#383838\",\n    \"axes.spines.right\" : False,\n    \"axes.spines.top\" : False,\n}\n\nsns.set(rc=rc)\n\ndf_target_count = df_train.target.value_counts()\ns1 = df_target_count[:3]\ns2 = pd.Series(sum(df_target_count[3:]), index=[\"rest\"])\ns3 = s1.append(s2)\n\nf, axes = plt.subplots(ncols=2, figsize=(15, 4))\nplt.subplots_adjust(wspace=0)\n\nouter_sizes = s3\ninner_sizes = s3/4\nouter_colors = ['#1B03A3', '#1B03A3', '#1B03A3', '#0D0151']\ninner_colors = ['#6A02A3', '#6A02A3', '#6A02A3']\n\naxes[0].pie(\n    outer_sizes,colors=outer_colors, \n    labels=s3.index.tolist(), \n    startangle=90,frame=True, radius=1.3, \n    explode=(.05,.05,.05,.5),\n    wedgeprops={ 'linewidth' : 1, 'edgecolor' : 'white'}, \n    textprops={'fontsize': 12, 'weight': 'bold'}\n)\n\ntextprops = {\n    'size':13, \n    'weight': 'bold', \n    'color':'white'\n}\n\naxes[0].pie(\n    inner_sizes, colors=inner_colors,\n    radius=1, startangle=90,\n    autopct='%1.f%%',explode=(.1,.1,.1, -.5),\n    pctdistance=0.8, textprops=textprops\n)\n\ncenter_circle = plt.Circle((0,0), .68, color='black', \n                           fc='#FFF2D9', linewidth=0)\naxes[0].add_artist(center_circle)\n\nx = df_target_count\ny = df_target_count.index.astype(str)\nsns.barplot(\n    x=x, y=y, ax=axes[1],\n    color='#1B03A3', orient='horizontal'\n)\n\naxes[1].spines['top'].set_visible(False)\naxes[1].spines['right'].set_visible(False)\naxes[1].tick_params(\n    axis='x',         \n    which='both',      \n    bottom=False,      \n    labelbottom=False\n)\n\nfor i, v in enumerate(df_target_count):\n    axes[1].text(v, i+0.1, str(v), color='black', \n                 fontweight='bold', fontsize=12)\n \nplt.tight_layout()    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:49:13.423815Z","iopub.execute_input":"2022-02-02T02:49:13.424056Z","iopub.status.idle":"2022-02-02T02:49:13.897737Z","shell.execute_reply.started":"2022-02-02T02:49:13.424026Z","shell.execute_reply":"2022-02-02T02:49:13.897051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Good news: **`df_train`** is **class-balanced**.\n>\n> **Next**, let's get 30000 samples and plot it:","metadata":{}},{"cell_type":"code","source":"seed = 322\ndf_train_sample = df_train.sample(n=30000, random_state=seed)\ndf_test_sample = df_test.sample(n=30000, random_state=seed)\n\nnp.random.seed(seed) \nfeatures_choice = np.random.choice(\n    df_train_sample.keys()[1:-1], size=3, replace=False\n)\n\nmask = sorted(features_choice.tolist()) + ['target']\ndf_sample_three = df_train_sample[mask]\ndf_sample_three.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:49:13.898837Z","iopub.execute_input":"2022-02-02T02:49:13.899183Z","iopub.status.idle":"2022-02-02T02:49:14.218823Z","shell.execute_reply.started":"2022-02-02T02:49:13.899154Z","shell.execute_reply":"2022-02-02T02:49:14.217939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=3, figsize=(24, 24))\n\nfor i, feature in enumerate(sorted(features_choice)):\n     sns.scatterplot(\n         ax=ax[i], x=df_sample_three.index,\n         y=feature,data=df_sample_three,\n         hue='target',palette='magma',\n         legend=True,\n     )","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:49:14.219883Z","iopub.execute_input":"2022-02-02T02:49:14.220129Z","iopub.status.idle":"2022-02-02T02:49:22.192922Z","shell.execute_reply.started":"2022-02-02T02:49:14.220097Z","shell.execute_reply":"2022-02-02T02:49:22.191989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **Basic Statistics Train Set head(5) + tail(5)**:","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.float_format', lambda x: '%.2f' % x)\ndescr_tr = df_train.iloc[:, :-1].describe().T\\\n                     .sort_values(by='std', ascending=False)\n\n\npd.concat([descr_tr.iloc[:5,:], descr_tr.iloc[-5:,:]])\\\n                     .style.background_gradient(cmap='magma')\\\n                     .bar(subset=[\"mean\",], color='green')\\\n                     .bar(subset=[\"max\"], color='#BB0000')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:49:22.194168Z","iopub.execute_input":"2022-02-02T02:49:22.194405Z","iopub.status.idle":"2022-02-02T02:49:36.32159Z","shell.execute_reply.started":"2022-02-02T02:49:22.194377Z","shell.execute_reply":"2022-02-02T02:49:36.320724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"descr_ts = df_test.describe().T\\\n                     .sort_values(by='std', ascending=False)\n\n\npd.concat([descr_ts.iloc[:5,:], descr_ts.iloc[-5:,:]])\\\n                     .style.background_gradient(cmap='magma')\\\n                     .bar(subset=[\"mean\",], color='green')\\\n                     .bar(subset=[\"max\"], color='#BB0000')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:49:36.32314Z","iopub.execute_input":"2022-02-02T02:49:36.323987Z","iopub.status.idle":"2022-02-02T02:49:43.649039Z","shell.execute_reply.started":"2022-02-02T02:49:36.32394Z","shell.execute_reply":"2022-02-02T02:49:43.648158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Some observations from the tables above:\n> * Head and Tail of **`df_train`** and **`df_test`** share the statistics with negligible difference.\n> It is nice to have train and test dataset from the same distribution. It's been a while since we were introduced to Tabular Dataset of such quality.","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"<a id='3'></a>\n# <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0;\">3. Exploratory Data Analysis</p>\n>\n> Let's take a closer look at the distribution of the features:","metadata":{}},{"cell_type":"code","source":"%%time\nfigsize = (6*6, 6*6)\nfig = plt.figure(figsize=figsize)\ntitle = 'Probability Density Function Estimation'\nfor idx, col in enumerate(df_test.columns[:20]):\n    ax = plt.subplot(4, 5, idx + 1)\n    sns.kdeplot(\n        data=df_train_sample, hue='target', fill=True,\n        x=col, palette='cividis'\n    )\n            \n    ax.set_ylabel(''); ax.spines['top'].set_visible(False), \n    ax.set_xlabel(''); ax.spines['right'].set_visible(False)\n    ax.set_title(f'{col}', loc='right', \n                 weight='bold', fontsize=10)\n\nfig.supxlabel(f'\\n\\n{title} Train\\n\\n', ha='center', \n              fontweight='bold', fontsize=30)\nplt.tight_layout()\nplt.show()\n\nfig = plt.figure(figsize=figsize)\nfor idx, col in enumerate(df_test.columns[:20]):\n    ax = plt.subplot(4, 5, idx + 1)\n    sns.kdeplot(\n    data=df_train_sample, fill=True,\n    x=col, color='#1B03A3', label='Train'\n    )\n    sns.kdeplot(\n        data=df_test_sample, fill=False,\n        x=col, color='#E54232', label='Test'\n    )\n\n    ax.set_xticks([]); ax.set_xlabel(''); \n    ax.set_ylabel(''); ax.spines['right'].set_visible(False)\n    ax.set_yticks([]); ax.spines['top'].set_visible(False)\n    ax.set_title(f'{col}', loc='right', \n                 weight='bold', fontsize=10)\n    \nfig.supxlabel(f'\\n\\n{title} Train vs Test set', ha='center', \n              fontweight='bold', fontsize=30)\n       \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:49:43.650225Z","iopub.execute_input":"2022-02-02T02:49:43.650434Z","iopub.status.idle":"2022-02-02T02:50:02.699519Z","shell.execute_reply.started":"2022-02-02T02:49:43.65041Z","shell.execute_reply":"2022-02-02T02:50:02.69868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **We have plotted Probability Density Function estimation for each feature. What does it tell us?**\n>* The features are distributed differently;\n>* The data is not perfectly symmetrical. The most of the features right-skewed.\n>* There is no bell-shaped-like (e.g., Gaussian distribution) plots.\n>* The plot supports the assumption that the train and test data are from the same distribution.","metadata":{}},{"cell_type":"markdown","source":"> Let's us take a look at features correlation matrix:","metadata":{}},{"cell_type":"code","source":"corr_ = df_train_sample.corr().abs()\n\nfig, axes = plt.subplots(figsize=(20, 10))\nmask1 = np.zeros_like(corr_)\nmask1[np.triu_indices_from(mask1)] = True\nsns.heatmap(corr_, mask=mask1, linewidths=.5, cmap='magma_r')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:50:02.700837Z","iopub.execute_input":"2022-02-02T02:50:02.701754Z","iopub.status.idle":"2022-02-02T02:50:12.230844Z","shell.execute_reply.started":"2022-02-02T02:50:02.701711Z","shell.execute_reply":"2022-02-02T02:50:12.229807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> If we wish to label the strength of the features association, for absolute values of correlation, 0-0.19 is regarded as very weak (the most of our examples are: [0.00-0.20].","metadata":{}},{"cell_type":"markdown","source":"\n## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0;\">Highly Correlated Features</p>\n>\n> Let's zoom in and take a closer look at the highly correlated pairs of features, taking an arbitrary threshold of correlation as 0.7:","metadata":{}},{"cell_type":"code","source":"# Fill df diagonal with zeros\nnp.fill_diagonal(corr_.values, 0)\npivot = corr_.unstack()\ncorr_pairs = pivot.sort_values(kind=\"quicksort\", ascending=False)\nhigh_corr_pairs = corr_pairs[corr_pairs > .7]\npd.DataFrame(high_corr_pairs[::2], columns=['corr']).head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:50:12.232044Z","iopub.execute_input":"2022-02-02T02:50:12.232365Z","iopub.status.idle":"2022-02-02T02:50:12.259206Z","shell.execute_reply.started":"2022-02-02T02:50:12.232332Z","shell.execute_reply":"2022-02-02T02:50:12.258412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(figsize=(10, 6))\ndf_hcorr_pairs = pd.DataFrame(high_corr_pairs).unstack()\nsns.heatmap(df_hcorr_pairs, linewidths=.5, cmap='magma_r')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:50:12.260923Z","iopub.execute_input":"2022-02-02T02:50:12.261556Z","iopub.status.idle":"2022-02-02T02:50:12.678714Z","shell.execute_reply.started":"2022-02-02T02:50:12.261512Z","shell.execute_reply":"2022-02-02T02:50:12.677797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Assumption**: \n>\n    * We can use this knowledge for the future feature engineering.\n    * We need to check feature importance of the correlated pairs separately (preferably).","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}},{"cell_type":"markdown","source":"<a id='4'></a>\n# <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0;\">4. Feature engineering</p>\n\n> [**Feature engineering**](https://www.omnisci.com/technical-glossary/feature-engineering#:~:text=Feature%20engineering%20refers%20to%20the,machine%20learning%20or%20statistical%20modeling.) refers to the process of using domain knowledge to select and transform the most relevant variables from raw data.\n>\n> One of the naive approach to engineer features, is to aggregate them.  \n>\n> The problem with aggregation is that we might encounter **multicollinearity** (e.g., the high correlation of the explanatory variables). \"It should be noted that the presence of multicollinearity does not mean that the model is\nmisspecified. You only start to talk about it when you think that it is\naffecting the regression results seriously.\" [[1]](#9.1)\n","metadata":{}},{"cell_type":"markdown","source":"## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0;\">Naive approach</p>","metadata":{}},{"cell_type":"code","source":"agg_features = ['sum','mean','std','max','min']\nfeatures = df_test.columns\nfor ft in agg_features:\n    \n    class_method = getattr(pd.DataFrame, ft)\n    df_train_sample[ft] = class_method(df_train_sample[features], axis=1)\n    df_test_sample[ft] = class_method(df_test_sample[features], axis=1)\n\ndf_test_sample.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:50:12.681245Z","iopub.execute_input":"2022-02-02T02:50:12.681494Z","iopub.status.idle":"2022-02-02T02:50:17.312044Z","shell.execute_reply.started":"2022-02-02T02:50:12.681466Z","shell.execute_reply":"2022-02-02T02:50:17.311134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0;\">Greedy Elimination</p>","metadata":{}},{"cell_type":"markdown","source":"\nThe idea of this approach is to eliminate highly correlated features with respect to their pairs. \nThe features will be eliminated based on their feature importance iteratively during the training.\n>\nThe complete list of the features to be iteratively eliminated:","metadata":{}},{"cell_type":"code","source":"pairs = pd.DataFrame(high_corr_pairs[::2], columns=['corr'])\npairs","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:50:17.313355Z","iopub.execute_input":"2022-02-02T02:50:17.313852Z","iopub.status.idle":"2022-02-02T02:50:17.324467Z","shell.execute_reply.started":"2022-02-02T02:50:17.31382Z","shell.execute_reply":"2022-02-02T02:50:17.323566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0;\">Correlated Pairs Stack and Elimination</p>","metadata":{}},{"cell_type":"code","source":"def stack_elimination(train, test, pairs, elim=False):\n    \"\"\"\n\n    Creates combined mean feature based on the pair.\n    Takes highly correlated feature pairs and eliminate them.\n    \n    :param train: (pd.DataFrame)\n    :param test: (pd.DataFrame)\n    :param pairs: (multiIndex pd.Dataframe)\n    :param elim: bool\n    :return: train (pd.DataFrame), test (pd.DataFrame)\n    \"\"\"\n    \n    \n    for i, pair in enumerate(pairs.index): \n        \n       \n        # Creates combined mean feature.\n        train[f'pair{i}'] = train[list(pair)].mean(axis=1)\n        test[f'pair{i}'] = test[list(pair)].mean(axis=1)\n\n    if elim:\n        # Eliminates the paired features.       \n        flat_pairs_list = [j for i in pairs.index for j in i]\n        ft_train = train.columns\n        ft_test = test.columns\n        diff_train = set(ft_train).difference(flat_pairs_list)\n        diff_test = set(ft_test).difference(flat_pairs_list)\n        train = train[sorted(diff_train)]\n        test = test[sorted(diff_test)]\n        \n    return train, test\n\ntrain, test = stack_elimination(df_train_sample, df_test_sample, pairs)\ntrain.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T02:55:36.841272Z","iopub.execute_input":"2022-02-02T02:55:36.841538Z","iopub.status.idle":"2022-02-02T02:55:36.983485Z","shell.execute_reply.started":"2022-02-02T02:55:36.84151Z","shell.execute_reply":"2022-02-02T02:55:36.982839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5'></a>\n# <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0;\">5. PyTorch NN Model</p>\n\n> **Versions notes**:\n> * Version1: Model hidden layers + Batch_Norm [300, BN, 200, BN, 100, BN, 50 BN].\n\n> **Things to try**:\n> * Feature Engineering.\n> * Add evaluation plots and confusion matrices.\n","metadata":{}},{"cell_type":"markdown","source":"<a id='5.1'></a>\n## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 24px 0px;\">__5.1 Reload Data</p>","metadata":{}},{"cell_type":"code","source":"train_csv_path = '../input/tabular-playground-series-feb-2022/train.csv'\ntest_csv_path = '../input/tabular-playground-series-feb-2022/test.csv'\n\ntrain = dt.fread(train_csv_path).to_pandas()\ntest = dt.fread(test_csv_path).to_pandas()\n\n# Encode target labels with value between 0 and n_classes.\nle = LabelEncoder()\ntarget = le.fit_transform(train.target)\n\ncol_drop=['row_id']    \nif col_drop:\n    train.drop(columns=col_drop, inplace=True)\n    test.drop(columns=col_drop, inplace=True)\n    print(f'\\n[INFO] \"Id\" columns have been removed successfully.\\n')\n\n# Applies encoded target.\ntrain = train.iloc[:, :-1]\ntrain['target'] = target\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:44:28.51292Z","iopub.execute_input":"2022-02-02T03:44:28.513248Z","iopub.status.idle":"2022-02-02T03:44:32.127625Z","shell.execute_reply.started":"2022-02-02T03:44:28.513215Z","shell.execute_reply":"2022-02-02T03:44:32.126569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.2'></a>\n## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 24px 0px;\">__5.2 Dataset</p>","metadata":{}},{"cell_type":"code","source":"class TabularDataset(Dataset):\n    def __init__(self, x, y):\n        \"\"\"\n        Defines PyTorch dataset.\n        :param x: np.ndarray\n        :param y: np.ndarray\n        \"\"\"\n\n        self.len = x.shape[0]\n        self.x = torch.Tensor(x).float()\n        self.y = torch.LongTensor(y).long().flatten()\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n    def __len__(self):\n        return self.len","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:23:03.661981Z","iopub.execute_input":"2022-02-02T03:23:03.66229Z","iopub.status.idle":"2022-02-02T03:23:03.668131Z","shell.execute_reply.started":"2022-02-02T03:23:03.662258Z","shell.execute_reply":"2022-02-02T03:23:03.667446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.3'></a>\n## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 24px 0px;\">__5.3 Model and weights initialization</p>","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, in_features, num_cls):\n        super().__init__()\n\n        self.fc1 = nn.Linear(in_features, 300)\n        self.bn1 = nn.BatchNorm1d(300)\n        self.fc2 = nn.Linear(300, 200)\n        self.bn2 = nn.BatchNorm1d(200)\n        self.fc3 = nn.Linear(200, 100)\n        self.bn3 = nn.BatchNorm1d(100)\n        self.fc4 = nn.Linear(100, 50)\n        self.bn4 = nn.BatchNorm1d(50)\n        self.fc_out = nn.Linear(50, num_cls)\n\n        self.activation = nn.ReLU()\n        self.classifier = nn.Sigmoid()\n\n    def forward(self, x):\n\n        x = self.activation(self.fc1(x))\n        x = self.bn1(x)\n        x = self.activation(self.fc2(x))\n        x = self.bn2(x)\n        x = self.activation(self.fc3(x))\n        x = self.bn3(x)\n        x = self.activation(self.fc4(x))\n        x = self.bn4(x)\n        x = self.fc_out(x)\n\n        return x\n\ndef init_weights(layer):\n    if isinstance(layer, nn.Linear):\n        nn.init.xavier_normal_(layer.weight.data)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:23:25.074767Z","iopub.execute_input":"2022-02-02T03:23:25.075104Z","iopub.status.idle":"2022-02-02T03:23:25.084693Z","shell.execute_reply.started":"2022-02-02T03:23:25.075057Z","shell.execute_reply":"2022-02-02T03:23:25.08386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.4'></a>\n## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 24px 0px;\">__5.4 Device and model summary</p>","metadata":{}},{"cell_type":"code","source":"train.iloc[:, :-1].shape[1]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:28:30.614902Z","iopub.execute_input":"2022-02-02T03:28:30.615647Z","iopub.status.idle":"2022-02-02T03:28:30.759827Z","shell.execute_reply.started":"2022-02-02T03:28:30.615598Z","shell.execute_reply":"2022-02-02T03:28:30.758691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"[INFO] Available device: {device}.\\n\\n\")\n\nn_ft = train.iloc[:, :-1].shape[1]\nn_cls = len(set(target))\nmodel = Model(in_features=n_ft, num_cls=n_cls).to(torch.device(device))\n\ntry:\n    from torchsummary import summary\nexcept:\n    print(\"Installing Torchsummary..........\")\n    ! pip install torchsummary -q\n    from torchsummary import summary\n    \nsummary(model, (n_ft,))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:28:41.428653Z","iopub.execute_input":"2022-02-02T03:28:41.428955Z","iopub.status.idle":"2022-02-02T03:28:41.607822Z","shell.execute_reply.started":"2022-02-02T03:28:41.42892Z","shell.execute_reply":"2022-02-02T03:28:41.606841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.5'></a>\n## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 24px 0px;\">__5.5 Utils</p>","metadata":{}},{"cell_type":"code","source":"def accuracy(output, target, topk=(1,)):\n    \"\"\"\n    Computes the precision@k for the specified values of k\n    \"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\nclass MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )\n\n\ndef set_seed(seed):\n    \"\"\"\n    Fixes seed for the reproducible results.\n    \"\"\"\n\n    random.seed(seed)\n    np.random.seed(seed)\n\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:30:18.903898Z","iopub.execute_input":"2022-02-02T03:30:18.904603Z","iopub.status.idle":"2022-02-02T03:30:18.916165Z","shell.execute_reply.started":"2022-02-02T03:30:18.904557Z","shell.execute_reply":"2022-02-02T03:30:18.915003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.6'></a>\n## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 24px 0px;\">__5.6 Train and valid loops with tqdm bar</p>","metadata":{}},{"cell_type":"code","source":"def train_loop(train_loader, model, criterion, optimizer, epoch, device):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    for i, (x, y) in enumerate(stream, start=1):\n        features = x.to(device)\n        target = y.to(device)\n        output = model(features)\n        loss = criterion(output, target)\n        acc_train = accuracy(output, target)\n        metric_monitor.update(\"Loss\", loss.item())\n        metric_monitor.update(\"Accuracy\", acc_train[0].item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        desc = \"Epoch: {epoch}. Train.      {metric_monitor}\"\n        stream.set_description(\n          desc.format(epoch=epoch, metric_monitor=metric_monitor)\n        )\n    \n    loss_avg = metric_monitor.metrics[\"Loss\"]['avg']\n    acc_avg = metric_monitor.metrics[\"Accuracy\"]['avg']\n\n    return loss_avg, acc_avg\n\n\ndef val_loop(val_loader, model, criterion, epoch, device):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    with torch.no_grad():\n        for i, (x, y) in enumerate(stream, start=1):\n            features = x.to(device)\n            target = y.to(device)\n            output = model(features)\n            loss = criterion(output, target)\n            acc_val = accuracy(output, target)\n\n            metric_monitor.update(\"Loss\", loss.item())\n            metric_monitor.update(\"Accuracy\", acc_val[0].item())\n            desc = \"Epoch: {epoch}. Validation.      {metric_monitor}\"\n            stream.set_description(\n                desc.format(epoch=epoch, metric_monitor=metric_monitor)\n            )\n            \n    loss_avg = metric_monitor.metrics[\"Loss\"]['avg']\n    acc_avg = metric_monitor.metrics[\"Accuracy\"]['avg']\n\n    return loss_avg, acc_avg","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:30:59.323923Z","iopub.execute_input":"2022-02-02T03:30:59.324277Z","iopub.status.idle":"2022-02-02T03:30:59.335903Z","shell.execute_reply.started":"2022-02-02T03:30:59.324244Z","shell.execute_reply":"2022-02-02T03:30:59.335278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.7'></a>\n## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 24px 0px;\">__5.7 CFG</p>","metadata":{}},{"cell_type":"code","source":"param = {\n        'seed': 1,\n        'nfold': 10,\n        'lr': 9e-5,\n        'wd': 1e-5,\n        'plateau_factor': .5,\n        'plateau_patience': 4,\n        'batch': 1024,\n        'epochs': 40,\n        'early_stopping': 9\n    }","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:39:03.182971Z","iopub.execute_input":"2022-02-02T03:39:03.183302Z","iopub.status.idle":"2022-02-02T03:39:03.188432Z","shell.execute_reply.started":"2022-02-02T03:39:03.183268Z","shell.execute_reply":"2022-02-02T03:39:03.187297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='5.8'></a>\n## <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 24px 0px;\">__5.8 Training Main</p>","metadata":{}},{"cell_type":"code","source":"n_ft = train.iloc[:, :-1].shape[1]\nn_cls = len(set(target))\n\nX = train.iloc[:, :-1].values\ny = train.iloc[:, -1:].values\n\n# StratifiedKfold data split.\nskf = StratifiedKFold(\n    n_splits=param['nfold'],\n    shuffle=True,\n    random_state=param['seed']\n)\n    \n\nfor fold, (idx_train, idx_val) in enumerate(skf.split(X, y)):\n    \n    if fold == 1:\n        break\n    # Model, weights and seed init.\n    model = Model(in_features=n_ft, num_cls=n_cls)\n    model.apply(init_weights)\n    model = model.to(torch.device(device))\n    set_seed(param['seed'])\n\n    # Loss and optimizer.\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(\n        model.parameters(),\n        lr=param['lr'],\n        weight_decay=param['wd']\n    )\n\n    scheduler = ReduceLROnPlateau(\n                optimizer=optimizer,\n                factor=param['plateau_factor'],\n                patience=param['plateau_patience'],\n                mode='max', verbose=True\n            )\n    \n   \n    scaler = StandardScaler()\n    X_train, y_train = scaler.fit_transform(X[idx_train, :]), y[idx_train]\n    X_val, y_val = scaler.transform(X[idx_val, :]), y[idx_val]\n    print(\n        f'\\n[INFO] Fold: {fold+1}, '\n        f'X_train shape: {X_train.shape}, '\n        f'X_val shape: {X_val.shape}.\\n'\n    )\n\n    trainset = TabularDataset(X_train, y_train)\n    valset = TabularDataset(X_val, y_val)\n    train_loader = DataLoader(trainset, batch_size=param['batch'], shuffle=True)\n    val_loader = DataLoader(valset, batch_size=param['batch'], shuffle=True)\n    wait_counter = 0\n    valid_acc_best = 0\n    \n    for epoch in range(1, param['epochs'] + 1):\n        train_loss, train_acc = train_loop(train_loader, model, criterion, optimizer, epoch, device)\n        valid_loss, valid_acc = val_loop(val_loader, model, criterion, epoch, device)\n        \n        if valid_acc > valid_acc_best:\n            valid_acc_best = valid_acc\n            wait_counter = 0\n            best_model = deepcopy(model)\n            print(f'\\n[INFO] The best model has been saved.\\n')\n        else:\n            wait_counter += 1\n            if wait_counter > param['early_stopping']:\n                print(f\"\\n[INFO] There's been no improvement \"\n                      f\"in val_acc. Early stopping has been invoked.\")\n                break","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:39:17.612229Z","iopub.execute_input":"2022-02-02T03:39:17.612536Z","iopub.status.idle":"2022-02-02T03:43:04.682608Z","shell.execute_reply.started":"2022-02-02T03:39:17.612499Z","shell.execute_reply":"2022-02-02T03:43:04.681776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = scaler.transform(test)\ntestset = TabularDataset(X_test, np.ones((X_test.shape[0], 1)))\ntest_loader = DataLoader(testset, batch_size=1024)\ny_pred_list = []\n\nbest_model.eval()\nwith torch.no_grad():\n    for X_batch, _ in tqdm(test_loader):\n        X_batch = X_batch.to(device)\n        y_test_pred = model(X_batch.float())\n        _, y_pred_tags = torch.max(y_test_pred, dim=1)\n        y_pred_list.extend(y_pred_tags.cpu().numpy())\n\n\n# Rearranges classes back (e.g. [0,1,2,3,4,5] -> arr of strings;\n# Creates mapped test_y preds list.\ntest_y = le.inverse_transform(y_pred_list)\n            \ndf_sub['target'] = test_y\ndf_sub.to_csv('submission.csv', index=False)\ndf_sub  ","metadata":{"execution":{"iopub.status.busy":"2022-02-02T03:44:41.944483Z","iopub.execute_input":"2022-02-02T03:44:41.945602Z","iopub.status.idle":"2022-02-02T03:44:44.008665Z","shell.execute_reply.started":"2022-02-02T03:44:41.945538Z","shell.execute_reply":"2022-02-02T03:44:44.007684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{"execution":{"iopub.status.busy":"2022-02-02T01:38:47.890921Z","iopub.execute_input":"2022-02-02T01:38:47.89135Z","iopub.status.idle":"2022-02-02T01:38:47.90049Z","shell.execute_reply.started":"2022-02-02T01:38:47.891306Z","shell.execute_reply":"2022-02-02T01:38:47.899015Z"}}},{"cell_type":"markdown","source":"<a id=''></a>\n# <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0px;\">Work in progress...</p>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=''></a>\n# <p style=\"background-color:#1B03A3; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 24px 0;\">Any suggestions to improve this notebook will be greatly appreciated. P/s If I have forgotten to reference someone's work, please, do not hesitate to leave your comment. Any questions, suggestions or complaints are most welcome. Upvotes keep me motivated... Thank you.</p>\n","metadata":{}},{"cell_type":"markdown","source":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning 🔙</a>","metadata":{}}]}