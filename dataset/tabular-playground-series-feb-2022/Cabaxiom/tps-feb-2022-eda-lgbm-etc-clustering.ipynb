{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series February 2022","metadata":{}},{"cell_type":"markdown","source":"Credit: The EDA takes inspirtation from [AmbrosM](https://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense) his notebooks are amazing!\n\nI am still learning! If there's any mistakes, or you have any tips for me please let me know!","metadata":{}},{"cell_type":"markdown","source":"## Project Task","metadata":{}},{"cell_type":"markdown","source":"**Task:**\n\nFor the February 2022 Tabular Playground Series competition, your task is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss. In this technique, 10-mer snippets of DNA are sampled and analyzed to give the histogram of base count (Snippets of length 10 are analyzed using Raman spectroscopy that calculates the histogram of bases in the snippet). In other words, the DNA segment $ATATGGCCTT$ becomes $A_{2}T_{4}G_{2}C_{2}$ Can you use this lossy information to accurately predict bacteria species?\n\n**Evaluation Metric:**\n\nAccuracy = $\\frac{Correct predictions}{Total predictions}$\n\n**Info**\n\nEach row of data contains a spectrum of histograms generated by repeated measurements of a sample, each row containing the output of all 286 histogram possibilities (e.g. $A_{0}T_{0}G_{0}C_{10}$ to $A_{10}T_{0}G_{0}C{0}$), which then has a bias spectrum (of totally random ATGC) subtracted from the results.","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"markdown","source":"## Preliminaries","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-20T14:38:14.218879Z","iopub.execute_input":"2022-02-20T14:38:14.219227Z","iopub.status.idle":"2022-02-20T14:38:14.24502Z","shell.execute_reply.started":"2022-02-20T14:38:14.219146Z","shell.execute_reply":"2022-02-20T14:38:14.244304Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nsns.set_style('darkgrid')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:14.246499Z","iopub.execute_input":"2022-02-20T14:38:14.247257Z","iopub.status.idle":"2022-02-20T14:38:15.661155Z","shell.execute_reply.started":"2022-02-20T14:38:14.247217Z","shell.execute_reply":"2022-02-20T14:38:15.660393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/train.csv')\ntest_df = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:15.662135Z","iopub.execute_input":"2022-02-20T14:38:15.662677Z","iopub.status.idle":"2022-02-20T14:38:52.947862Z","shell.execute_reply.started":"2022-02-20T14:38:15.662646Z","shell.execute_reply":"2022-02-20T14:38:52.946724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) / start_mem\n            )\n        )\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:52.950111Z","iopub.execute_input":"2022-02-20T14:38:52.950444Z","iopub.status.idle":"2022-02-20T14:38:52.963334Z","shell.execute_reply.started":"2022-02-20T14:38:52.95039Z","shell.execute_reply":"2022-02-20T14:38:52.962631Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Description","metadata":{"jupyter":{"source_hidden":true}}},{"cell_type":"code","source":"print(\"Train Dimensions:\", train_df.shape)\nprint(\"Test Dimensions:\", test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:52.964834Z","iopub.execute_input":"2022-02-20T14:38:52.965127Z","iopub.status.idle":"2022-02-20T14:38:52.984804Z","shell.execute_reply.started":"2022-02-20T14:38:52.965086Z","shell.execute_reply":"2022-02-20T14:38:52.984127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:52.986387Z","iopub.execute_input":"2022-02-20T14:38:52.987123Z","iopub.status.idle":"2022-02-20T14:38:53.028875Z","shell.execute_reply.started":"2022-02-20T14:38:52.987082Z","shell.execute_reply":"2022-02-20T14:38:53.028175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:53.030367Z","iopub.execute_input":"2022-02-20T14:38:53.030653Z","iopub.status.idle":"2022-02-20T14:38:53.070253Z","shell.execute_reply.started":"2022-02-20T14:38:53.030618Z","shell.execute_reply":"2022-02-20T14:38:53.069388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:53.072012Z","iopub.execute_input":"2022-02-20T14:38:53.072258Z","iopub.status.idle":"2022-02-20T14:38:55.960631Z","shell.execute_reply.started":"2022-02-20T14:38:53.072228Z","shell.execute_reply":"2022-02-20T14:38:55.959992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing values","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    There are no missing values in the dataset\n\n    \n</div>","metadata":{}},{"cell_type":"code","source":"print(\"Number of missing values in train set: \", train_df.isna().sum().sum())\nprint(\"Number of missing values in test set: \", test_df.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:55.962611Z","iopub.execute_input":"2022-02-20T14:38:55.963288Z","iopub.status.idle":"2022-02-20T14:38:56.151434Z","shell.execute_reply.started":"2022-02-20T14:38:55.963252Z","shell.execute_reply":"2022-02-20T14:38:56.150528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Classes","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    The classes are balanced\n\n    \n</div>","metadata":{}},{"cell_type":"code","source":"targets = train_df['target']\nbacteria_counts = targets.value_counts()\nbacteria_counts = bacteria_counts.reset_index().rename(columns={\"index\":\"BacteriaSpecies\", \"target\":\"Count\"})\nbacteria_counts.set_index(\"BacteriaSpecies\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:56.152648Z","iopub.execute_input":"2022-02-20T14:38:56.152896Z","iopub.status.idle":"2022-02-20T14:38:56.179343Z","shell.execute_reply.started":"2022-02-20T14:38:56.152859Z","shell.execute_reply":"2022-02-20T14:38:56.178331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(15, 6))\nsns.barplot(x = \"BacteriaSpecies\", y=\"Count\", data = bacteria_counts)\nplt.xticks(rotation=30);","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:56.180472Z","iopub.execute_input":"2022-02-20T14:38:56.180727Z","iopub.status.idle":"2022-02-20T14:38:56.53673Z","shell.execute_reply.started":"2022-02-20T14:38:56.180695Z","shell.execute_reply":"2022-02-20T14:38:56.535893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"In the original paper they perform three important steps to the data we should consider:\n\n1. They introduce random errors to simulate gene mutations and experimental errors. For some rows they introduce more errors than others - where the fraction of the number of errors introduced is described by m.\n2. They add bias (the spectrum from a random sequence (bias) is subtracted from the simulated emperimental spectrum)\n3. When taking their measurements they use different numbers of reads (can think of as the number of measurements/scans made). The more reads made the more precise the resulting spectrum (row) and the better it matches the bacterium.","metadata":{}},{"cell_type":"markdown","source":"Quotes from the paper:\n\n> To test the robustness of identification of species or genes by deviation spectra in the presence of real experimental noise and random mutations in the DNA sequence, random errors are introduced into the FBC spectra (see section “Simulating Gene Mutations and Experimental Errors” for details) and then the resulting noisy data are divided by r and the bias spectrum is subtracted to obtain a simulated FBC deviation spectrum (with noise) for the given plasmid or genome. \n\n> For both the gDNA and pDNA, the FBC deviation spectra were created with the following parameters: k = 10; r = [10^2; 10^3; 10^4; 10^5; 10^6]; m = [0, 0.01, 0.05, 0.1, 0.25, 0.33, 0.5, 0.75, 0.9, 1]; and s = 1000; where k is the size of the k-mer, r is the number of pyramid tips for generating the sample FBC spectra, m is the fractional error rate, and s is the number of FBC deviation spectra created per DNA sequence (genome or plasmid).\n\n> We define an error rate m (where 0 ≤ m ≤ 1) to be the fraction of bases in the reference sequence that are expected to contain an error. \n\n> The robustness of each classification model was studied by measuring the predictive accuracy as a function of the parameters of the simulated BOC data. For each optical sequencing read number (r) and each error rate (m)\n","metadata":{}},{"cell_type":"markdown","source":"In the paper they perform:\n\n$Data = \\frac{originalData}{r} - bias$\n\nwhere Data is the data that we currently have. To see the orignal data \n\n$originalData = (Data + bias) * r $\n\nwhere r could be 10,100,1000,10000,100000\n\nObtaining the original data:","metadata":{}},{"cell_type":"code","source":"train_cols = list(train_df.columns.drop(['row_id','target']))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:56.537759Z","iopub.execute_input":"2022-02-20T14:38:56.537965Z","iopub.status.idle":"2022-02-20T14:38:56.542828Z","shell.execute_reply.started":"2022-02-20T14:38:56.537939Z","shell.execute_reply":"2022-02-20T14:38:56.541812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bias(w,x,y,z):\n    b = 1/4**10 * (np.math.factorial(10)/(np.math.factorial(w)*np.math.factorial(x)*np.math.factorial(y)*np.math.factorial(z)))\n    return b","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:56.544867Z","iopub.execute_input":"2022-02-20T14:38:56.545194Z","iopub.status.idle":"2022-02-20T14:38:56.55628Z","shell.execute_reply.started":"2022-02-20T14:38:56.545151Z","shell.execute_reply":"2022-02-20T14:38:56.555501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_bias(s):\n    w = int(s[1:s.index('T')])\n    x = int(s[s.index('T')+1:s.index('G')])\n    y = int(s[s.index('G')+1:s.index('C')])\n    z = int(s[s.index('C')+1:])\n    b = bias(w,x,y,z)\n    return b","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:56.558262Z","iopub.execute_input":"2022-02-20T14:38:56.558492Z","iopub.status.idle":"2022-02-20T14:38:56.568303Z","shell.execute_reply.started":"2022-02-20T14:38:56.558464Z","shell.execute_reply":"2022-02-20T14:38:56.567437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can tell which r value was used for each row by considering the greatest common divisor once we multiply the value by the largest possible r (10^6), where a GCD of 1 would mean r = 10^6, GCD of 10 = 10^5 etc.","metadata":{}},{"cell_type":"code","source":"train_df_bias = pd.DataFrame({col: (((train_df[col] + calc_bias(col))*10**6).round().astype(int)) for col in train_cols})\ntest_df_bias = pd.DataFrame({col: (((test_df[col] + calc_bias(col))*10**6).round().astype(int)) for col in train_cols})\ntrain_df_bias.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:56.569643Z","iopub.execute_input":"2022-02-20T14:38:56.571177Z","iopub.status.idle":"2022-02-20T14:38:57.749226Z","shell.execute_reply.started":"2022-02-20T14:38:56.571137Z","shell.execute_reply":"2022-02-20T14:38:57.748503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Calculating the r value used","metadata":{}},{"cell_type":"code","source":"def gcd_of_all(df_i):\n    gcd = df_i[train_cols[0]]\n    for col in train_cols[1:]:\n        gcd = np.gcd(gcd, df_i[col])\n    return gcd\n\ntrain_df_bias['gcd'] = gcd_of_all(train_df_bias)\ntrain_df['gcd'] = gcd_of_all(train_df_bias)\ntest_df['gcd'] = gcd_of_all(test_df_bias)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:38:57.750163Z","iopub.execute_input":"2022-02-20T14:38:57.750722Z","iopub.status.idle":"2022-02-20T14:39:00.091386Z","shell.execute_reply.started":"2022-02-20T14:38:57.75069Z","shell.execute_reply":"2022-02-20T14:39:00.090263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gcd_counts = train_df[\"gcd\"].value_counts().reset_index().rename(columns={\"gcd\":\"train_count\", \"index\":\"gcd\"}).sort_values(by=\"gcd\").set_index(\"gcd\")\ngcd_counts[\"test_count\"] = test_df[\"gcd\"].value_counts()\ngcd_counts[\"train_perc\"] = 100 * gcd_counts[\"train_count\"]/gcd_counts[\"train_count\"].sum()\ngcd_counts[\"test_perc\"] = 100 * gcd_counts[\"test_count\"]/gcd_counts[\"test_count\"].sum()\ngcd_counts","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:39:00.092609Z","iopub.execute_input":"2022-02-20T14:39:00.09287Z","iopub.status.idle":"2022-02-20T14:39:00.115941Z","shell.execute_reply.started":"2022-02-20T14:39:00.092839Z","shell.execute_reply":"2022-02-20T14:39:00.115075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\nThe GCD counts are relatively balanced - perhaps there is a slightly higher percentage of gcd 1 in the test set.\n","metadata":{}},{"cell_type":"markdown","source":"Greatest common divisors are of 1, 10, 1000 and 10000, corresponding to r (read) values of 1,000,000, 100,000, 1,000 and 100 respectively, with r values of 10,000 apparently absent from this dataset.","metadata":{}},{"cell_type":"code","source":"#Copying the targets over\ntrain_df_bias[\"target\"] = train_df['target']","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:39:00.118287Z","iopub.execute_input":"2022-02-20T14:39:00.118624Z","iopub.status.idle":"2022-02-20T14:39:00.129098Z","shell.execute_reply.started":"2022-02-20T14:39:00.118579Z","shell.execute_reply":"2022-02-20T14:39:00.128198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets visualise the data for different GCD values.\n\nTrain data:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\n\nfor i,gcd in enumerate([1,10,1000,10000]):\n    pca = PCA(n_components = 2, random_state = 10, whiten = True)\n    pca.fit(train_df_bias[train_df_bias[\"gcd\"] == gcd][train_cols])\n\n    X_PCA = pca.transform(train_df_bias[train_df_bias[\"gcd\"] == gcd][train_cols])\n\n    # Percentage of variance explained for each components\n    print( \"GCD: \", gcd, \" explained variance ratio (first two components): \", pca.explained_variance_ratio_)\n\n    PCA_df = pd.DataFrame({\"PCA_1\" : X_PCA[:,0], \"PCA_2\" : X_PCA[:,1], \"LABEL\":train_df_bias[train_df_bias[\"gcd\"] == gcd][\"target\"]})\n    \n    ax = plt.subplot(2, 2, i + 1)\n    plt.title(\"GCD: \" + str(gcd))\n    sns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = train_df_bias[\"target\"].unique())","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:39:00.130462Z","iopub.execute_input":"2022-02-20T14:39:00.130721Z","iopub.status.idle":"2022-02-20T14:39:23.642932Z","shell.execute_reply.started":"2022-02-20T14:39:00.13069Z","shell.execute_reply":"2022-02-20T14:39:23.642258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    <ol>\n        <li>Classification will be easier for lower GCD values.<\\li>\n         <li>We can see that there are 8 clusters for each GCD value (only noticeable for GCD 1 and 10). these correspond to the different simulated error (m) <\\li>\n    <\\ol>\n    <\\div>","metadata":{}},{"cell_type":"markdown","source":"Test data:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\n\nfor i,gcd in enumerate([1,10,1000,10000]):\n    pca = PCA(n_components = 2, random_state = 10, whiten = True)\n    pca.fit(test_df[test_df[\"gcd\"] == gcd][train_cols])\n\n    X_PCA = pca.transform(test_df[test_df[\"gcd\"] == gcd][train_cols])\n\n    # Percentage of variance explained for each components\n    print( \"GCD: \", gcd, \" explained variance ratio (first two components): \", pca.explained_variance_ratio_)\n\n    PCA_df = pd.DataFrame({\"PCA_1\" : X_PCA[:,0], \"PCA_2\" : X_PCA[:,1]})\n    \n    ax = plt.subplot(2, 2, i + 1)\n    plt.title(\"GCD: \" + str(gcd))\n    sns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:39:23.644073Z","iopub.execute_input":"2022-02-20T14:39:23.644381Z","iopub.status.idle":"2022-02-20T14:39:29.378774Z","shell.execute_reply.started":"2022-02-20T14:39:23.644354Z","shell.execute_reply":"2022-02-20T14:39:29.377885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simulated Error Classess","metadata":{}},{"cell_type":"markdown","source":"Lets get a better visual of the different simular error (m) values and try and classify these clusters","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 30))\n\nfor i,bacteria in enumerate(train_df_bias[\"target\"].unique()):\n    cluster_input_data = train_df_bias[(train_df_bias[\"gcd\"] == 1) & (train_df_bias[\"target\"] == bacteria)][train_cols]\n    cluster_input_data = StandardScaler().fit_transform(cluster_input_data)\n\n    clustering_gmm_m = GaussianMixture(n_components=8, covariance_type = 'diag', n_init=5, random_state=1).fit(cluster_input_data)\n    clustering_gmm = clustering_gmm_m.predict(cluster_input_data)\n\n    pca.fit(cluster_input_data)\n\n    X_PCA = pca.transform(cluster_input_data)\n\n    PCA_df = pd.DataFrame({\"PCA_1\" : X_PCA[:,0], \"PCA_2\" : X_PCA[:,1],\"Cluster\":clustering_gmm})    \n    \n    mvals = [\"m00\", \"m05\", \"m10\", \"m25\", \"m33\", \"m50\", \"m75\", \"m90\"]\n    PCA_1 = PCA_df.groupby(by=[\"Cluster\"])[\"PCA_1\"].mean().reset_index().sort_values(\"PCA_1\").reset_index(drop=True)\n    PCA_df = PCA_df.replace({'Cluster': {PCA_1.iloc[n][\"Cluster\"]: mvals[n] for n in range(0,8)}})\n    \n    if i == 0:\n        cluster_counts = PCA_df['Cluster'].value_counts().reset_index().rename(columns={\"index\":\"mval\", \"Cluster\":bacteria + \" count\"}).set_index(\"mval\").sort_index()\n    else:\n        cluster_counts[bacteria + \" count\"] = PCA_df['Cluster'].value_counts()\n    \n    ax = plt.subplot(5, 2, i + 1)\n    plt.title(\"GMM - Bacteria: \" + str(bacteria))\n    sns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue=\"Cluster\", hue_order=mvals, palette = sns.color_palette(\"husl\", 8))\ncluster_counts","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:39:29.380297Z","iopub.execute_input":"2022-02-20T14:39:29.381186Z","iopub.status.idle":"2022-02-20T14:40:17.449018Z","shell.execute_reply.started":"2022-02-20T14:39:29.381138Z","shell.execute_reply":"2022-02-20T14:40:17.44803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\"><ol>\n    <li>These different groups correspond to the simulated error added. Out of the possible m  = [0, 0.01, 0.05, 0.1, 0.25, 0.33, 0.5, 0.75, 0.9, 1], 8 ( have been selected for the version of the data we are using. These are likely [m = 0 OR 0.01, 0.05, 0.1, 0.25,0.33, 0.5, 0.75 and 0.9] based on the distances between clusters. \n    <li> We notice that the m=0 cluster has twice the number of points as we expect, this is likely because there is actually 9 clusters with m=0.01 and m=0 both currently being represented by the m0 cluster. The difference between them is just too small to pick up. <\\li>\n   <li> Some of the bacterium seems to have more distinct clusters than others.<\\li>\n    <li>   We can accuractely use clustering to predict which m value was used to create the data (for gcd = 1 and if the bacterium is known). <\\li>\n        <\\ol>\n<\\div>","metadata":{}},{"cell_type":"markdown","source":"### Unique Values","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    <ol>\n    <li>  Although the values for the DNA segment columns are floating points and there are 20,000 rows in the train set the number of possible values are not unique</li>\n    <li> Each DNA segmentcolumn varies significantly in the number of unique values. This makes sense as there are more possible permutations e.g. of $A_{3}T_{3}G_{3}C_{4}$ then $A_{10}T_{0}G_{0}C_{0}$ </li>\n    </ol>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Although the values for the DNA segment columns are floating points and there are 20,000 rows in the train set the number of possible values are not unique:","metadata":{}},{"cell_type":"code","source":"train_cols = list(train_df.columns.drop(['row_id','target']))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:40:17.45073Z","iopub.execute_input":"2022-02-20T14:40:17.451067Z","iopub.status.idle":"2022-02-20T14:40:17.456295Z","shell.execute_reply.started":"2022-02-20T14:40:17.451022Z","shell.execute_reply":"2022-02-20T14:40:17.455567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_nuniques(df, cols):\n    nuniques = []\n    for i in cols:\n        nuniques.append(df[i].nunique())\n    return nuniques\n\ntrain_uniques = calculate_nuniques(train_df, train_cols)\nnunique_df = pd.DataFrame({'Cols' : train_cols, 'nunique': train_uniques})\nnunique_df.T","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:40:17.457771Z","iopub.execute_input":"2022-02-20T14:40:17.458247Z","iopub.status.idle":"2022-02-20T14:40:18.184827Z","shell.execute_reply.started":"2022-02-20T14:40:17.458205Z","shell.execute_reply":"2022-02-20T14:40:18.184012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_uniques = calculate_nuniques(test_df, train_cols)\nnunique_test_df = pd.DataFrame({'Cols' : train_cols, 'nunique': test_uniques})\nnunique_test_df.T","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:40:18.189605Z","iopub.execute_input":"2022-02-20T14:40:18.189853Z","iopub.status.idle":"2022-02-20T14:40:18.59799Z","shell.execute_reply.started":"2022-02-20T14:40:18.189823Z","shell.execute_reply":"2022-02-20T14:40:18.597056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 7))\nax = sns.histplot(data = nunique_df, x = \"nunique\", bins=50)\nax.set_xlim(0);\nax.set_xlabel(\"Number of unique values\")\nax.set_title(\"Visualising the number of unique values for different DNA segments (features)\");","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:40:18.599362Z","iopub.execute_input":"2022-02-20T14:40:18.599741Z","iopub.status.idle":"2022-02-20T14:40:18.944799Z","shell.execute_reply.started":"2022-02-20T14:40:18.599696Z","shell.execute_reply":"2022-02-20T14:40:18.94413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Creation and experimentation","metadata":{}},{"cell_type":"markdown","source":"The ultimate goal would be to do determine the value of m just from patterns in the training columns, without also knowing the target bacterium. This is probably not possible. However useful features can also be created.\n\nIf we could determine the value of m, without knowledge of the bacteria then we would avoid making errors between ones bacterias m value and a different m value from a different bacteria, which otherwise would have similar values.","metadata":{}},{"cell_type":"markdown","source":"Number of non-zero rows:","metadata":{}},{"cell_type":"code","source":"non_zero = train_df_bias.drop(columns=[\"gcd\",\"target\"]).astype(bool).sum(axis=1).values\nnon_zero_test = test_df_bias.astype(bool).sum(axis=1).values\ntrain_df[\"non_zero_row\"] = non_zero\ntest_df[\"non_zero_row\"] = non_zero_test\n\nf, ax = plt.subplots(figsize=(20, 7))\nax = sns.histplot(data = train_df, x = \"non_zero_row\", bins=200, hue = \"gcd\",  palette = sns.color_palette(\"husl\", 4))\nax.set_xlabel(\"Number of non-zero values\")\nax.set_title(\"Number of non-zero entries in each row of the training data\");","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:40:18.945963Z","iopub.execute_input":"2022-02-20T14:40:18.946622Z","iopub.status.idle":"2022-02-20T14:40:21.519542Z","shell.execute_reply.started":"2022-02-20T14:40:18.946575Z","shell.execute_reply":"2022-02-20T14:40:21.518735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    <ol>\n    <li>  As we would expect the lower the GCD (the more reads) the more possible DNA observations are obtained, and the more non-zero columns</li>\n    <li> They are probably normally distributed as we would expect, but they do seem to be a little off. Perhaps this could be a result of the different bacterium, but it could also be a result of the different m values - Taking a look</li>\n    </ol>\n</div>","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 30))\nfor i, gcd in enumerate([1,10,1000,10000]):\n    plt.subplot(4, 1, i+1)\n    plt.title(\"GCD \" + str(gcd))\n    ax = sns.histplot(data = train_df[train_df[\"gcd\"] == gcd], x = \"non_zero_row\", bins=30, hue = \"target\", hue_order = train_df[\"target\"].unique(), palette = sns.color_palette(\"husl\", 10))\n    ax.set_xlabel(\"Number of non-zero values\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:40:21.520774Z","iopub.execute_input":"2022-02-20T14:40:21.521371Z","iopub.status.idle":"2022-02-20T14:40:26.453834Z","shell.execute_reply.started":"2022-02-20T14:40:21.521324Z","shell.execute_reply":"2022-02-20T14:40:26.453197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"std_dev = train_df_bias.drop(columns=[\"gcd\",\"target\"]).std(axis=1)\nstd_dev_test = test_df_bias.std(axis=1)\ntrain_df[\"std_row\"] = std_dev.values\ntest_df[\"std_row\"] = std_dev_test.values\n\nf, ax = plt.subplots(figsize=(20, 7))\nax = sns.histplot(data = train_df, x = \"std_row\", bins=200, hue = \"gcd\",  palette = sns.color_palette(\"husl\", 4))\nax.set_xlabel(\"Standard Deviation\");","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:40:26.454781Z","iopub.execute_input":"2022-02-20T14:40:26.455112Z","iopub.status.idle":"2022-02-20T14:40:29.022137Z","shell.execute_reply.started":"2022-02-20T14:40:26.455082Z","shell.execute_reply":"2022-02-20T14:40:29.021049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 30))\nfor i, gcd in enumerate([1,10,1000,10000]):\n    plt.subplot(4, 1, i+1)\n    plt.title(\"GCD \" + str(gcd))\n    ax = sns.histplot(data = train_df[train_df[\"gcd\"] == gcd], x = \"std_row\", bins=200, hue = \"target\", hue_order = train_df[\"target\"].unique(), palette = sns.color_palette(\"husl\", 10))\n    ax.set_xlabel(\"Standard Deviation\");","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:40:29.023744Z","iopub.execute_input":"2022-02-20T14:40:29.024097Z","iopub.status.idle":"2022-02-20T14:40:50.137257Z","shell.execute_reply.started":"2022-02-20T14:40:29.02405Z","shell.execute_reply":"2022-02-20T14:40:50.136379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    <ol>\n    <li>  standard deviation in rows seems like a very good predictor of bacteria</li>\n    <li> We can see the different peaks for the same bacterium (for low gcd/high reads). This is the effect of the different m values (see example below)</li>\n    </ol>\n</div>","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 7))\nax = sns.histplot(data = train_df[(train_df[\"gcd\"] == 1) & (train_df[\"target\"] == 'Campylobacter_jejuni')], x = \"std_row\", bins=200)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:40:50.138757Z","iopub.execute_input":"2022-02-20T14:40:50.139659Z","iopub.status.idle":"2022-02-20T14:40:50.769046Z","shell.execute_reply.started":"2022-02-20T14:40:50.13961Z","shell.execute_reply":"2022-02-20T14:40:50.768066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution","metadata":{}},{"cell_type":"code","source":"kurtosis = train_df_bias.drop(columns=[\"gcd\",\"target\"]).kurtosis(axis=1)\nkurtosis_test = test_df_bias.kurtosis(axis=1)\n\ntrain_df[\"kurtosis_row\"] = kurtosis.values\ntest_df[\"kurtosis_row\"] = kurtosis_test.values\n\nf, ax = plt.subplots(figsize=(20, 7))\nax = sns.histplot(data = train_df, x = \"kurtosis_row\", bins=200, hue = \"gcd\",  palette = sns.color_palette(\"husl\", 4))\nax.set_xlabel(\"Kurtosis\");","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:40:50.770509Z","iopub.execute_input":"2022-02-20T14:40:50.770848Z","iopub.status.idle":"2022-02-20T14:40:54.408298Z","shell.execute_reply.started":"2022-02-20T14:40:50.770802Z","shell.execute_reply":"2022-02-20T14:40:54.407235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 30))\nfor i, gcd in enumerate([1,10,1000,10000]):\n    plt.subplot(4, 1, i+1)\n    plt.title(\"GCD \" + str(gcd))\n    ax = sns.histplot(data = train_df[train_df[\"gcd\"] == gcd], x = \"kurtosis_row\", bins=200, hue = \"target\", hue_order = train_df[\"target\"].unique(), palette = sns.color_palette(\"husl\", 10))\n    ax.set_xlabel(\"Kurtosis\");","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:40:54.410409Z","iopub.execute_input":"2022-02-20T14:40:54.410731Z","iopub.status.idle":"2022-02-20T14:41:16.032399Z","shell.execute_reply.started":"2022-02-20T14:40:54.410697Z","shell.execute_reply":"2022-02-20T14:41:16.031595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.","metadata":{}},{"cell_type":"code","source":"skew =  train_df_bias.drop(columns=[\"gcd\",\"target\"]).skew(axis=1)\nskew_test = test_df_bias.skew(axis=1)\n\ntrain_df[\"skew_row\"] = skew.values\ntest_df[\"skew_row\"] = skew_test.values\n\nf, ax = plt.subplots(figsize=(20, 7))\nax = sns.histplot(data = train_df, x = \"skew_row\", bins=200, hue = \"gcd\",  palette = sns.color_palette(\"husl\", 4))\nax.set_xlabel(\"Skew\");","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:41:16.629637Z","iopub.execute_input":"2022-02-20T14:41:16.63Z","iopub.status.idle":"2022-02-20T14:41:20.595515Z","shell.execute_reply.started":"2022-02-20T14:41:16.629886Z","shell.execute_reply":"2022-02-20T14:41:20.594471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 30))\nfor i, gcd in enumerate([1,10,1000,10000]):\n    plt.subplot(4, 1, i+1)\n    plt.title(\"GCD \" + str(gcd))\n    ax = sns.histplot(data = train_df[train_df[\"gcd\"] == gcd], x = \"skew_row\", bins=200, hue = \"target\", hue_order = train_df[\"target\"].unique(), palette = sns.color_palette(\"husl\", 10))\n    ax.set_xlabel(\"Skew\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:41:20.597067Z","iopub.execute_input":"2022-02-20T14:41:20.597292Z","iopub.status.idle":"2022-02-20T14:41:41.07078Z","shell.execute_reply.started":"2022-02-20T14:41:20.597259Z","shell.execute_reply":"2022-02-20T14:41:41.069981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 7))\nax = sns.histplot(data = train_df[(train_df[\"gcd\"] == 1) & (train_df[\"target\"] == 'Campylobacter_jejuni')], x = \"skew_row\", bins=200)\nax.set_xlabel(\"Skew\");","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:41:41.072129Z","iopub.execute_input":"2022-02-20T14:41:41.072352Z","iopub.status.idle":"2022-02-20T14:41:41.657615Z","shell.execute_reply.started":"2022-02-20T14:41:41.072324Z","shell.execute_reply":"2022-02-20T14:41:41.656685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nunique_x = train_df_bias.drop(columns=[\"gcd\",\"target\"]).nunique(axis=1)\nnunique_x_test = test_df_bias.nunique(axis=1)\n\ntrain_df[\"nunique\"] = nunique_x.values\ntest_df[\"nunique\"] = nunique_x_test.values","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:41:41.658928Z","iopub.execute_input":"2022-02-20T14:41:41.659149Z","iopub.status.idle":"2022-02-20T14:41:58.342876Z","shell.execute_reply.started":"2022-02-20T14:41:41.659122Z","shell.execute_reply":"2022-02-20T14:41:58.341456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_quantiles(qlist):\n    for q in qlist:\n        quantile_x = train_df_bias.drop(columns=[\"gcd\",\"target\"]).quantile(q=q/100, axis=1)\n        quantile_x_test = test_df_bias.quantile(q=q/100, axis=1)\n\n        train_df[\"quantile_\"+str(q)] = quantile_x.values\n        test_df[\"quantile_\"+str(q)] = quantile_x_test.values\nset_quantiles([5,10,20,30,40,50,60,70,80,90, 92, 93, 94, 95])","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:41:58.344682Z","iopub.execute_input":"2022-02-20T14:41:58.345014Z","iopub.status.idle":"2022-02-20T14:42:36.484885Z","shell.execute_reply.started":"2022-02-20T14:41:58.344954Z","shell.execute_reply":"2022-02-20T14:42:36.484059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 30))\nfor i, gcd in enumerate([1,10,1000,10000]):\n    plt.subplot(4, 1, i+1)\n    plt.title(\"GCD \" + str(gcd))\n    ax = sns.histplot(data = train_df[train_df[\"gcd\"] == gcd], x = \"quantile_92\", bins=200, hue = \"target\", hue_order = train_df[\"target\"].unique(), palette = sns.color_palette(\"husl\", 10))\n    ax.set_xlabel(\"quantile_92\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:42:38.594506Z","iopub.execute_input":"2022-02-20T14:42:38.595365Z","iopub.status.idle":"2022-02-20T14:42:59.726561Z","shell.execute_reply.started":"2022-02-20T14:42:38.595316Z","shell.execute_reply":"2022-02-20T14:42:59.725634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Comparing Train and Test","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\n\nfor i,gcd in enumerate([1,10,1000,10000]):\n    pca = PCA(n_components = 2, random_state = 10, whiten = True)\n    pca.fit(train_df[train_df[\"gcd\"] == gcd][train_cols])\n    Xtr_PCA = pca.transform(train_df[train_df[\"gcd\"] == gcd][train_cols])\n    \n    #pca.fit(test_df[test_df[\"gcd\"] == gcd][train_cols])\n    Xte_PCA = pca.transform(test_df[test_df[\"gcd\"] == gcd][train_cols])\n\n    PCA_train_df = pd.DataFrame({\"PCA_1\" : Xtr_PCA[:,0], \"PCA_2\" : Xtr_PCA[:,1], \"Label\":[\"Train\"]*len(Xtr_PCA[:,0])})\n    PCA_test_df = pd.DataFrame({\"PCA_1\" : Xte_PCA[:,0], \"PCA_2\" : Xte_PCA[:,1], \"Label\":[\"Test\"]*len(Xte_PCA[:,0])})\n    PCA_df = pd.concat([PCA_train_df, PCA_test_df])\n    \n    ax = plt.subplot(2, 2, i + 1)\n    plt.title(\"GCD: \" + str(gcd))\n    sns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"Label\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:42:59.728428Z","iopub.execute_input":"2022-02-20T14:42:59.728761Z","iopub.status.idle":"2022-02-20T14:43:22.928122Z","shell.execute_reply.started":"2022-02-20T14:42:59.728715Z","shell.execute_reply":"2022-02-20T14:43:22.927132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\"  style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    The train data and the test data do not seem to match perfectly - this is most obvious for GCD 1 but it can be seen from GCD 10 as well. Could the train and test data be from different distributions?  However this difference is only noticed for some of the bacteria - others seem to match up relatively well.","metadata":{}},{"cell_type":"markdown","source":"## Duplicates","metadata":{}},{"cell_type":"code","source":"#FUNCTION adapted from https://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense\ndef plot_duplicates_per_gcd(df, title):\n    plt.figure(figsize=(16, 3))\n    plt.tight_layout()\n    for i, gcd in enumerate(np.unique(df.gcd)):\n        plt.subplot(1, 5, i+1)\n        duplicates = df[df.gcd == gcd][train_cols].duplicated().sum()\n        non_duplicates = len(df[df.gcd == gcd]) - duplicates\n        plt.pie([non_duplicates, duplicates],\n                labels=['not duplicate', 'duplicate'],\n                colors=['gray', 'r'],\n                startangle=90)\n        plt.title(\"GCD = \" + str(gcd))\n    \n    plt.subplot(1, 5, 5)\n    duplicates = df[train_cols].duplicated().sum()\n    print(\"In total there are\", duplicates, title, \"\\n\")\n    non_duplicates = len(df) - duplicates\n    plt.pie([non_duplicates, duplicates],\n                labels=['not duplicate', 'duplicate'],\n                colors=['gray', 'r'],\n                startangle=90)\n    plt.title(\"All Data\")\n    \n    plt.subplots_adjust(wspace=0.8)\n    plt.suptitle(title)\n    plt.show()\n        \nplot_duplicates_per_gcd(train_df, title=\"Duplicates in Training\")\nplot_duplicates_per_gcd(test_df, title=\"Duplicates in Test\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:43:22.929388Z","iopub.execute_input":"2022-02-20T14:43:22.930178Z","iopub.status.idle":"2022-02-20T14:43:29.741865Z","shell.execute_reply.started":"2022-02-20T14:43:22.930117Z","shell.execute_reply":"2022-02-20T14:43:29.740595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\"> \n  <ol>\n   <li> GCD Values of 1000 and 10000 have significantly more duplicates. <\\li>\n    <li>The test data has less duplicates<\\li>\n    \n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\"  style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">This is odd:\n\n1. We would expect more duplicates for lower reads (higher GCD) this makes sense as a more detailed read should give more possible outcomes. However why does GCD 1 and GCD 10 have the same number of duplicates; and why does GCD 1000 and 10000 have the same number of duplicates? This seems a little odd.\n2. Why would the test data have a lower percentage of duplicates than the training data? - Actually this could make sense as the training data has 200,000 rows and the test data only 100,000. Perhaps after 100,000 samples each new additional sample is more likely to have a duplicated entry.\n   </div>","metadata":{}},{"cell_type":"markdown","source":"As seen there are a lot of duplicate rows in the training data. Lets remove them, but save how many duplicates there are of each row. We can then use this list as sample weights when training the model, so that highly duplicated rows have a higher importance in training.\n\nWhy is this important?\n- Speed up training times\n- We don't want duplicates in the validation set when doing cross-fold validation as this will invalidate the CV results. ","metadata":{}},{"cell_type":"code","source":"train_df.drop(columns=[\"row_id\"], inplace=True) \na = train_df.groupby(by=train_df.columns.to_list()).size().values # Use groupby to identify duplicated rows\ntrain_df = train_df.drop_duplicates() \ntrain_df[\"duplicates\"] = a\nprint(\"Dropped:\", 200000 - len(train_df), \"rows\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:43:29.743661Z","iopub.execute_input":"2022-02-20T14:43:29.744326Z","iopub.status.idle":"2022-02-20T14:43:36.480658Z","shell.execute_reply.started":"2022-02-20T14:43:29.744273Z","shell.execute_reply":"2022-02-20T14:43:36.479816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s1 = pd.merge(train_df, test_df, how='inner', on=train_cols)\nprint(\"There are\", len(s1), \"rows in the test data that are also in the training data\")\nprint(\"Of these\", len(s1), \"test data rows,\", s1[\"row_id\"].nunique(), \"are unqiue in the test data\")\nprint(\"Of these\", len(s1), \"test data rows,\", len(s1[s1[\"gcd\"]==10000]), \"have a gcd of 10000\")\nplt.pie([len(test_df)-len(s1), len(s1)],\n                labels=['not duplicate', 'duplicate'],\n                colors=['gray', 'r'],\n                startangle=90)\nplt.title(\"Test data duplicated in the train data\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:43:36.482371Z","iopub.execute_input":"2022-02-20T14:43:36.482803Z","iopub.status.idle":"2022-02-20T14:43:47.234698Z","shell.execute_reply.started":"2022-02-20T14:43:36.482772Z","shell.execute_reply":"2022-02-20T14:43:47.232346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\"> <ol>\n   <li>We can ensure that we classify all 486 test rows that are also in the training data correctly.<\\li>\n     <li>All of these duplicated rows have a GCD of 10,000 which are the hardest for the model to predict.<\\li>\n    <\\ol>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\"  style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">However this is a little strange - we should expect WAY more duplicates, based on the number of duplicates in the train and test data. This could indicate that the test data comes from a different distribution OR most of the duplicates have been manually removed.\n    <\\div>","metadata":{}},{"cell_type":"markdown","source":"## Modelling","metadata":{}},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom lightgbm import plot_importance\nfrom lightgbm import early_stopping\nfrom lightgbm import log_evaluation\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nfrom sklearn.ensemble import ExtraTreesClassifier","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:43:47.236675Z","iopub.execute_input":"2022-02-20T14:43:47.23703Z","iopub.status.idle":"2022-02-20T14:43:48.198814Z","shell.execute_reply.started":"2022-02-20T14:43:47.236986Z","shell.execute_reply":"2022-02-20T14:43:48.197915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = reduce_memory_usage(train_df)\ntest_df = reduce_memory_usage(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:43:48.199907Z","iopub.execute_input":"2022-02-20T14:43:48.200192Z","iopub.status.idle":"2022-02-20T14:44:11.803839Z","shell.execute_reply.started":"2022-02-20T14:43:48.20016Z","shell.execute_reply":"2022-02-20T14:44:11.802967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:44:11.805058Z","iopub.execute_input":"2022-02-20T14:44:11.805284Z","iopub.status.idle":"2022-02-20T14:44:11.836741Z","shell.execute_reply.started":"2022-02-20T14:44:11.805254Z","shell.execute_reply":"2022-02-20T14:44:11.835859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:44:11.838044Z","iopub.execute_input":"2022-02-20T14:44:11.838279Z","iopub.status.idle":"2022-02-20T14:44:11.871171Z","shell.execute_reply.started":"2022-02-20T14:44:11.83825Z","shell.execute_reply":"2022-02-20T14:44:11.870244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_weights = train_df[\"duplicates\"]\nX_train = train_df.drop(columns=[ 'target', 'duplicates']) # row_id should already be dropped - could drop GCD too\nX_test = test_df.drop(columns=['row_id'])\n\nle = LabelEncoder() \nle.fit(train_df['target'])\ny_train = le.transform(train_df['target'])","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:44:11.872361Z","iopub.execute_input":"2022-02-20T14:44:11.872566Z","iopub.status.idle":"2022-02-20T14:44:12.155701Z","shell.execute_reply.started":"2022-02-20T14:44:11.872541Z","shell.execute_reply":"2022-02-20T14:44:12.154834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_SPLITS = 10\n\nkf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:44:12.157054Z","iopub.execute_input":"2022-02-20T14:44:12.15728Z","iopub.status.idle":"2022-02-20T14:44:12.161339Z","shell.execute_reply.started":"2022-02-20T14:44:12.157252Z","shell.execute_reply":"2022-02-20T14:44:12.160325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **ExtraTreesClassifier**","metadata":{}},{"cell_type":"code","source":"# Currently chosen randomly\n\net_params = {\n    'n_estimators': 1000,\n    'n_jobs': -1,\n    'random_state': 1\n}\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:44:12.16284Z","iopub.execute_input":"2022-02-20T14:44:12.163152Z","iopub.status.idle":"2022-02-20T14:44:12.17392Z","shell.execute_reply.started":"2022-02-20T14:44:12.163111Z","shell.execute_reply":"2022-02-20T14:44:12.173073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npred_validation_all_et = []\nvalidation_all = []\nval_ids_all = []\n\ny_pred_test_et = []\ny_pred_test_prob_et = []\n\nimportances_et = []\naccs_et = []\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X=X_train, y = y_train)):\n    val_ids_all.append(val_idx)\n    print(\"===== Fold\", fold,\" =====\")\n    X_tr = X_train.iloc[trn_idx]\n    y_tr = y_train[trn_idx]\n    X_val = X_train.iloc[val_idx]\n    y_val = y_train[val_idx]\n    sample_weight_tr = sample_weights.iloc[trn_idx].values\n    sample_weight_val = sample_weights.iloc[val_idx].values\n    \n    model_et = ExtraTreesClassifier(**et_params)\n    \n    model_et.fit(\n        X_tr,\n        y_tr,\n        sample_weight_tr)\n        \n    importances_et.append(model_et.feature_importances_)\n    \n    pred_val_et = model_et.predict(X_val)\n    pred_validation_all_et.append(pred_val_et)\n    validation_all.append(y_val)\n    \n    acc_et = accuracy_score(y_true = y_val, y_pred = pred_val_et, sample_weight=sample_weight_val)\n    accs_et.append(acc_et)\n    \n    print(\"FOLD\", fold, \"ETC Accuracy:\", acc_et)\n    \n    # Test data predictions\n    y_pred_test_et.append(model_et.predict(X_test))\n    y_pred_test_prob_et.append(model_et.predict_proba(X_test))\n    \nprint(\"======================================\")\nprint(\"Mean Accuracy (all folds) - ETC:\", np.mean(accs_et))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T14:44:12.175427Z","iopub.execute_input":"2022-02-20T14:44:12.175832Z","iopub.status.idle":"2022-02-20T15:21:34.828026Z","shell.execute_reply.started":"2022-02-20T14:44:12.175787Z","shell.execute_reply":"2022-02-20T15:21:34.826856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def confusion_matrix_mod(pred_val_all_mod, modelName):\n    preds_val_full = [item for sublist in pred_val_all_mod for item in sublist]\n    true_val_full = [item for sublist in validation_all for item in sublist]\n    ids_full = [item for sublist in val_ids_all for item in sublist]\n    gcd_full = train_df.iloc[ids_full][\"gcd\"].values\n    sample_weight_val = train_df[\"duplicates\"].iloc[ids_full].values\n    \n    cm_df = pd.DataFrame({\"Preds\":preds_val_full, \"True\":true_val_full, \"GCD\":gcd_full, \"Sample_weights\":sample_weight_val})\n    plt.figure(figsize=(17, 17))\n    \n    for i,gcd in enumerate([1,10,1000,10000]):\n        plot_data = cm_df[cm_df[\"GCD\"] == gcd]\n        cm = confusion_matrix(plot_data[\"True\"], plot_data[\"Preds\"], sample_weight = plot_data[\"Sample_weights\"])\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n        ax = plt.subplot(2, 2, i + 1)\n        plt.title(modelName + \"- GCD: \" + str(gcd))\n        disp.plot(ax=ax, xticks_rotation = 30);\n        plt.grid(False)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T15:21:34.830071Z","iopub.execute_input":"2022-02-20T15:21:34.830324Z","iopub.status.idle":"2022-02-20T15:21:34.840483Z","shell.execute_reply.started":"2022-02-20T15:21:34.830296Z","shell.execute_reply":"2022-02-20T15:21:34.839804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fold_feature_importances(model_importances, model_name):\n    importances_et_df = pd.DataFrame({\"feature_cols\": X_train.columns, \"importances_0\": model_importances[0]})\n    for i in range(1,N_SPLITS):\n        importances_et_df[\"importances_\"+str(i)] = importances_et[i]\n    importances_et_df[\"importances_median\"] = importances_et_df.drop(columns=[\"feature_cols\"]).median(axis=1)\n    importances_et_df = importances_et_df.sort_values(by=\"importances_median\", ascending=False)\n    f, ax = plt.subplots(figsize=(10, 15))\n    ax = sns.barplot(data = importances_et_df.iloc[0:80], x = \"importances_median\", y=\"feature_cols\")\n    plt.title(model_name)\n    ax.set_xlabel(\"Feature importance median across all folds\");","metadata":{"execution":{"iopub.status.busy":"2022-02-20T15:21:34.841533Z","iopub.execute_input":"2022-02-20T15:21:34.842108Z","iopub.status.idle":"2022-02-20T15:21:34.862437Z","shell.execute_reply.started":"2022-02-20T15:21:34.84207Z","shell.execute_reply":"2022-02-20T15:21:34.861467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plot the confusion matrix for each GCD value using the predictions from all of folds validation set combined.","metadata":{}},{"cell_type":"code","source":"confusion_matrix_mod(pred_validation_all_et, \"ETC\") \n# Something may be off with the sample weights not showing up as intended for GCD 1000,10000","metadata":{"execution":{"iopub.status.busy":"2022-02-20T15:21:34.863699Z","iopub.execute_input":"2022-02-20T15:21:34.864346Z","iopub.status.idle":"2022-02-20T15:21:38.685932Z","shell.execute_reply.started":"2022-02-20T15:21:34.864301Z","shell.execute_reply":"2022-02-20T15:21:38.685118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plot the median feature importances from the model across all folds.","metadata":{}},{"cell_type":"code","source":"fold_feature_importances(importances_et, \"ETC\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T15:21:38.687227Z","iopub.execute_input":"2022-02-20T15:21:38.687646Z","iopub.status.idle":"2022-02-20T15:21:40.022776Z","shell.execute_reply.started":"2022-02-20T15:21:38.687614Z","shell.execute_reply":"2022-02-20T15:21:40.021927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">Looks like our created features do well!","metadata":{}},{"cell_type":"markdown","source":"### **LGBM Classifier**","metadata":{}},{"cell_type":"code","source":"#Currently chosen randomly\n\nlgbm_params = {\n    'objective' : 'multiclass',\n    'n_estimators': 300,\n    'random_state': 43,\n    'learning_rate': 0.1,\n    'n_jobs' : -1\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-20T15:21:40.024161Z","iopub.execute_input":"2022-02-20T15:21:40.024366Z","iopub.status.idle":"2022-02-20T15:21:40.028373Z","shell.execute_reply.started":"2022-02-20T15:21:40.024339Z","shell.execute_reply":"2022-02-20T15:21:40.027484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npred_validation_all_lgbm = []\nvalidation_all = []\nval_ids_all = []\n\ny_pred_test_lgbm = []\ny_pred_test_prob_lgbm = []\n\nimportances_lgbm = []\naccs_lgbm = []\n\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X=X_train, y = y_train)):\n    val_ids_all.append(val_idx)\n    print(\"===== Fold\", fold,\" =====\")\n    X_tr = X_train.iloc[trn_idx]\n    y_tr = y_train[trn_idx]\n    X_val = X_train.iloc[val_idx]\n    y_val = y_train[val_idx]\n    sample_weight_tr = sample_weights.iloc[trn_idx].values\n    sample_weight_val = sample_weights.iloc[val_idx].values\n    \n    model_lgbm = LGBMClassifier(**lgbm_params)\n    \n    model_lgbm.fit(\n        X_tr, \n        y_tr,\n        sample_weight = sample_weight_tr,\n        eval_sample_weight = [sample_weight_val],\n        eval_set=[(X_val, y_val)],\n        eval_metric = ['multi_logloss', 'multi_error'],\n        callbacks = [early_stopping(30), log_evaluation(period=50)]\n    )\n    \n    importances_lgbm.append(model_lgbm.feature_importances_)\n    \n    pred_val_lgbm = model_lgbm.predict(X_val)\n    \n    pred_validation_all_lgbm.append(pred_val_lgbm)\n    validation_all.append(y_val)\n    \n    acc_lgbm = accuracy_score(y_true = y_val, y_pred = pred_val_lgbm, sample_weight=sample_weight_val)\n    accs_lgbm.append(acc_lgbm)\n    \n    print(\"FOLD\", fold, \"LGBM Accuracy:\", acc_lgbm)\n\n    # Test data predictions\n    y_pred_test_lgbm.append(model_lgbm.predict(X_test))\n    y_pred_test_prob_lgbm.append(model_lgbm.predict_proba(X_test))\n    \nprint(\"======================================\")\nprint(\"Mean Accuracy - LGBM:\", np.mean(accs_lgbm))","metadata":{"execution":{"iopub.status.busy":"2022-02-20T15:21:40.029852Z","iopub.execute_input":"2022-02-20T15:21:40.030126Z","iopub.status.idle":"2022-02-20T16:04:39.983452Z","shell.execute_reply.started":"2022-02-20T15:21:40.030095Z","shell.execute_reply":"2022-02-20T16:04:39.982356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix_mod(pred_validation_all_lgbm, \"LGBM\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:05:50.763595Z","iopub.execute_input":"2022-02-20T16:05:50.763938Z","iopub.status.idle":"2022-02-20T16:05:54.063205Z","shell.execute_reply.started":"2022-02-20T16:05:50.763904Z","shell.execute_reply":"2022-02-20T16:05:54.062265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold_feature_importances(importances_lgbm, \"LGBM\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:06:18.428414Z","iopub.execute_input":"2022-02-20T16:06:18.429024Z","iopub.status.idle":"2022-02-20T16:06:19.7675Z","shell.execute_reply.started":"2022-02-20T16:06:18.428986Z","shell.execute_reply":"2022-02-20T16:06:19.76658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensemble and Post-processing","metadata":{}},{"cell_type":"code","source":"print(\"Each model trained on: \", 100*(1-(1/N_SPLITS)), \"% of the training data.\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:06:24.298078Z","iopub.execute_input":"2022-02-20T16:06:24.298444Z","iopub.status.idle":"2022-02-20T16:06:24.304229Z","shell.execute_reply.started":"2022-02-20T16:06:24.298405Z","shell.execute_reply":"2022-02-20T16:06:24.303497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ExtraTreesClassifier","metadata":{}},{"cell_type":"markdown","source":"We sum the preidction probabilities of each class from each of the models in the ensemble. We then select the class for each row in the test data based on the highest probability","metadata":{}},{"cell_type":"code","source":"def gen_preds_from_ensemble_proba(pred_probs, class_weights):\n    \"\"\"We sum the preidction probabilities of each class from each of the models in the ensemble.\n    We then select the class for each row in the test data based on the highest probability\"\"\"\n    y_pred_probs_added = []\n    for row in range(len(X_test)):\n    \n        summed_prob = class_weights\n        for i in range(N_SPLITS):\n            summed_prob = np.add(summed_prob,pred_probs[i][row])\n    \n        y_pred_probs_added.append(summed_prob)\n    y_test_preds = [np.argmax(array) for array in y_pred_probs_added]\n    return y_test_preds","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:06:26.219237Z","iopub.execute_input":"2022-02-20T16:06:26.219578Z","iopub.status.idle":"2022-02-20T16:06:26.226296Z","shell.execute_reply.started":"2022-02-20T16:06:26.219539Z","shell.execute_reply":"2022-02-20T16:06:26.225583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weight = np.zeros(10)\ny_test_pred_et = gen_preds_from_ensemble_proba(pred_probs = y_pred_test_prob_et, class_weights=class_weight)\ny_test_pred_et = le.inverse_transform(y_test_pred_et)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:06:29.16005Z","iopub.execute_input":"2022-02-20T16:06:29.160404Z","iopub.status.idle":"2022-02-20T16:06:30.338293Z","shell.execute_reply.started":"2022-02-20T16:06:29.160363Z","shell.execute_reply":"2022-02-20T16:06:30.337357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bacteriaCount(model_predictions):\n    bacteria_counts = train_df['target'].value_counts()\n    bacteria_counts = bacteria_counts.reset_index().rename(columns={\"index\":\"BacteriaSpecies\", \"target\":\"train_count\"})\n    bacteria_counts = bacteria_counts.set_index(\"BacteriaSpecies\")\n    bacteria_counts[\"test_count\"] = pd.Series(model_predictions).value_counts()\n    bacteria_counts[\"train_perc\"] = 100 * bacteria_counts[\"train_count\"]/bacteria_counts[\"train_count\"].sum()\n    bacteria_counts[\"test_perc\"] = 100 * bacteria_counts[\"test_count\"]/bacteria_counts[\"test_count\"].sum()\n    bacteria_counts = bacteria_counts.sort_index()\n    return bacteria_counts","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:06:30.34011Z","iopub.execute_input":"2022-02-20T16:06:30.340415Z","iopub.status.idle":"2022-02-20T16:06:30.348302Z","shell.execute_reply.started":"2022-02-20T16:06:30.340372Z","shell.execute_reply":"2022-02-20T16:06:30.347081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bacteriaPlotFull(model_predictions):\n    test_df[\"preds\"] = model_predictions\n    bacteria_counts_full = test_df.groupby([\"gcd\",\"preds\"])[\"row_id\"].count().reset_index().rename(columns={\"row_id\":\"test_count\"})\n    bacteria_counts_full[\"train_count\"] = train_df.groupby([\"gcd\",\"target\"])[\"gcd\"].count().values\n    plt.figure(figsize=(15, 15))\n    for i,gcd in enumerate([1,10,1000,10000]):\n        \n        plot_b_count = bacteria_counts_full[bacteria_counts_full[\"gcd\"] == gcd]\n        ax = plt.subplot(2, 2, i + 1)\n        plt.title(\"GCD: \" + str(gcd))\n        ax = sns.barplot(data = plot_b_count, x = \"preds\", y = \"test_count\", hue_order = test_df[\"preds\"].unique())\n        plt.xticks(rotation=30);\n        ax.set_ylim(1500,3250)\n    return ","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:06:32.792141Z","iopub.execute_input":"2022-02-20T16:06:32.792995Z","iopub.status.idle":"2022-02-20T16:06:32.800992Z","shell.execute_reply.started":"2022-02-20T16:06:32.792916Z","shell.execute_reply":"2022-02-20T16:06:32.799996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bacteria_count_et = bacteriaCount(y_test_pred_et)\nbacteria_count_et","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:06:33.26959Z","iopub.execute_input":"2022-02-20T16:06:33.270084Z","iopub.status.idle":"2022-02-20T16:06:33.299286Z","shell.execute_reply.started":"2022-02-20T16:06:33.270046Z","shell.execute_reply":"2022-02-20T16:06:33.298564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bacteriaPlotFull(y_test_pred_et)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:06:43.404471Z","iopub.execute_input":"2022-02-20T16:06:43.404748Z","iopub.status.idle":"2022-02-20T16:06:44.42483Z","shell.execute_reply.started":"2022-02-20T16:06:43.404714Z","shell.execute_reply":"2022-02-20T16:06:44.424252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is likely some issue in our test predictions for GCD 1000 and GCD 10000 as a result of the test data distribution being different to the train data distribution.\n\nIn particular: \n- *E. coli* is often underpredicted in favour of *E. fergusonii*.\n- *E. hirae* is often underpredicted.\n\nWhy these mistakes:\n- *E. fergusonii* and *E. coli* are from the same genus and so only a small deviation from the train and test distributions will cause them to be misclassified.\n- *E. hirae* has one of the largest train/test distribution changes (as can be seen by the train/test PCA).","metadata":{}},{"cell_type":"markdown","source":"We also know that 486 test data rows for GCD 10,000 (r = 100) are also in the training data, lets have a look at how well our model predicts these:","metadata":{}},{"cell_type":"code","source":"test_df[\"preds\"] = y_test_pred_et\ns1 = pd.merge(train_df, test_df, how='inner', on=train_cols)\nprint(\"There are\", len(s1[s1[\"target\"] != s1[\"preds\"]]), \"probable mistakes that we can easily fix\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:23:30.953263Z","iopub.execute_input":"2022-02-20T16:23:30.954069Z","iopub.status.idle":"2022-02-20T16:23:41.260895Z","shell.execute_reply.started":"2022-02-20T16:23:30.954007Z","shell.execute_reply":"2022-02-20T16:23:41.259749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can try and fix this by matching the percentage of data in each class of the test data to the percentage of data in each class of the train data (each class should be about the same size). To do this we introduce a bias to the predicted probabilities for the class with fewer predictions than expected. This addedd bias makes it more likely that that class will be predicted. \n\nCredit to [AmbrosM](https://www.kaggle.com/ambrosm/tpsfeb22-02-postprocessing-against-the-mutants) for this idea.","metadata":{}},{"cell_type":"code","source":"class_weight_et = np.array([0, 0, 0.025, 0.035, 0, 0, 0, 0, 0, 0])*N_SPLITS\n\ny_test_pred_et = gen_preds_from_ensemble_proba(pred_probs = y_pred_test_prob_et, class_weights=class_weight_et)\ny_test_pred_et = le.inverse_transform(y_test_pred_et)\n\nbacteria_count_et[\"test_count\"] = pd.Series(y_test_pred_et).value_counts()\nbacteria_count_et[\"test_perc\"] = 100 * bacteria_count_et[\"test_count\"]/bacteria_count_et[\"test_count\"].sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:23:41.26282Z","iopub.execute_input":"2022-02-20T16:23:41.263059Z","iopub.status.idle":"2022-02-20T16:23:42.353791Z","shell.execute_reply.started":"2022-02-20T16:23:41.263028Z","shell.execute_reply":"2022-02-20T16:23:42.352814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see how that improved predictions:","metadata":{}},{"cell_type":"code","source":"bacteria_count_et.sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:23:47.079472Z","iopub.execute_input":"2022-02-20T16:23:47.079771Z","iopub.status.idle":"2022-02-20T16:23:47.093299Z","shell.execute_reply.started":"2022-02-20T16:23:47.079741Z","shell.execute_reply":"2022-02-20T16:23:47.092332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bacteriaPlotFull(y_test_pred_et)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:23:49.863911Z","iopub.execute_input":"2022-02-20T16:23:49.864856Z","iopub.status.idle":"2022-02-20T16:23:50.868892Z","shell.execute_reply.started":"2022-02-20T16:23:49.864807Z","shell.execute_reply":"2022-02-20T16:23:50.867957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets have a look at how good are predictions are:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\ntest_df[\"preds\"] = y_test_pred_et\n\nfor i,gcd in enumerate([1,10,1000,10000]):\n    pca = PCA(n_components = 2, random_state = 10, whiten = True)\n    pca.fit(test_df[test_df[\"gcd\"] == gcd][train_cols])\n\n    X_PCA = pca.transform(test_df[test_df[\"gcd\"] == gcd][train_cols])\n\n    PCA_df = pd.DataFrame({\"PCA_1\" : X_PCA[:,0], \"PCA_2\" : X_PCA[:,1],  \"LABEL\":test_df[test_df[\"gcd\"] == gcd][\"preds\"]})\n    \n    ax = plt.subplot(2, 2, i + 1)\n    plt.title(\"GCD: \" + str(gcd))\n    sns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique())","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:24:24.932767Z","iopub.execute_input":"2022-02-20T16:24:24.933518Z","iopub.status.idle":"2022-02-20T16:24:36.614866Z","shell.execute_reply.started":"2022-02-20T16:24:24.93346Z","shell.execute_reply":"2022-02-20T16:24:36.614051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets have another look at our 486 test data rows:","metadata":{}},{"cell_type":"code","source":"test_df[\"preds\"] = y_test_pred_et\ns1 = pd.merge(train_df, test_df, how='inner', on=train_cols)\nprint(\"There are\", len(s1[s1[\"target\"] != s1[\"preds\"]]), \"probable mistakes that we can easily fix\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:24:42.8892Z","iopub.execute_input":"2022-02-20T16:24:42.88954Z","iopub.status.idle":"2022-02-20T16:24:53.111286Z","shell.execute_reply.started":"2022-02-20T16:24:42.8895Z","shell.execute_reply":"2022-02-20T16:24:53.110365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looks like our model gets them all right anyway, no need to fix","metadata":{}},{"cell_type":"markdown","source":"### LGBM","metadata":{}},{"cell_type":"code","source":"class_weight = np.zeros(10)\ny_test_pred_lgbm = gen_preds_from_ensemble_proba(pred_probs = y_pred_test_prob_lgbm, class_weights=class_weight)\ny_test_pred_lgbm = le.inverse_transform(y_test_pred_lgbm)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:26:35.255247Z","iopub.execute_input":"2022-02-20T16:26:35.255556Z","iopub.status.idle":"2022-02-20T16:26:36.347911Z","shell.execute_reply.started":"2022-02-20T16:26:35.255522Z","shell.execute_reply":"2022-02-20T16:26:36.347022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bacteria_count_lgbm = bacteriaCount(y_test_pred_lgbm)\nbacteria_count_lgbm","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:26:36.349484Z","iopub.execute_input":"2022-02-20T16:26:36.34975Z","iopub.status.idle":"2022-02-20T16:26:36.381106Z","shell.execute_reply.started":"2022-02-20T16:26:36.349718Z","shell.execute_reply":"2022-02-20T16:26:36.380307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bacteriaPlotFull(y_test_pred_lgbm)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:26:39.527188Z","iopub.execute_input":"2022-02-20T16:26:39.528022Z","iopub.status.idle":"2022-02-20T16:26:41.513453Z","shell.execute_reply.started":"2022-02-20T16:26:39.527966Z","shell.execute_reply":"2022-02-20T16:26:41.512659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We also know that 486 test data rows for GCD 10,000 (r = 100) are also in the training data, lets have a look at how well our model predicts these:\ntest_df[\"preds\"] = y_test_pred_lgbm\ns1 = pd.merge(train_df, test_df, how='inner', on=train_cols)\nprint(\"There are\", len(s1[s1[\"target\"] != s1[\"preds\"]]), \" definite misclassifications in the test data for GCD 10,000\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:26:44.269246Z","iopub.execute_input":"2022-02-20T16:26:44.269639Z","iopub.status.idle":"2022-02-20T16:26:54.81703Z","shell.execute_reply.started":"2022-02-20T16:26:44.269592Z","shell.execute_reply":"2022-02-20T16:26:54.815924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_weight_lgbm = np.array([0, 0, 0.1, 0.2, 0, 0, 0, 0, 0, 0])*N_SPLITS\n\ny_test_pred_lgbm = gen_preds_from_ensemble_proba(pred_probs = y_pred_test_prob_lgbm, class_weights=class_weight_lgbm)\ny_test_pred_lgbm = le.inverse_transform(y_test_pred_lgbm)\n\n\nbacteria_count_lgbm[\"test_count\"] = pd.Series(y_test_pred_lgbm).value_counts()\nbacteria_count_lgbm[\"test_perc\"] = 100 * bacteria_count_lgbm[\"test_count\"]/bacteria_count_lgbm[\"test_count\"].sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:35:01.438595Z","iopub.execute_input":"2022-02-20T16:35:01.438953Z","iopub.status.idle":"2022-02-20T16:35:02.530051Z","shell.execute_reply.started":"2022-02-20T16:35:01.438914Z","shell.execute_reply":"2022-02-20T16:35:02.529159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\nLGBM seems very confident in its predictions and doesn't want to change without adding a massive bias. Making postpreccessing on LGBM very hard and just makes the performance worse. This is interesting and supports the hypothesis that the test data and train data have different distributions.\n\nI'm not sure why ETC doesn't also have this problem. Could LGBM be fitting more to the training/validation data, making its predictions almost too good?","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\"  style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\nWhat does this mean in terms of CV scores? If the train and test distributions are so different then how are we going to choose the optimal hyperparameters for the models? This is a rare case where we SHOULD actually trust the public leaderboard over the CV scores.","metadata":{}},{"cell_type":"code","source":"bacteria_count_lgbm.sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:35:02.531711Z","iopub.execute_input":"2022-02-20T16:35:02.531934Z","iopub.status.idle":"2022-02-20T16:35:02.544644Z","shell.execute_reply.started":"2022-02-20T16:35:02.531906Z","shell.execute_reply":"2022-02-20T16:35:02.543618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bacteriaPlotFull(y_test_pred_lgbm)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:35:07.993496Z","iopub.execute_input":"2022-02-20T16:35:07.993809Z","iopub.status.idle":"2022-02-20T16:35:09.0283Z","shell.execute_reply.started":"2022-02-20T16:35:07.993776Z","shell.execute_reply":"2022-02-20T16:35:09.027129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df[\"preds\"] = y_test_pred_lgbm\ns1 = pd.merge(train_df, test_df, how='inner', on=train_cols)\nprint(\"There are\", len(s1[s1[\"target\"] != s1[\"preds\"]]), \"definite misclassifications in the test data for GCD 10,000 that we can easily fix\")","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:35:41.602011Z","iopub.execute_input":"2022-02-20T16:35:41.602305Z","iopub.status.idle":"2022-02-20T16:35:52.114864Z","shell.execute_reply.started":"2022-02-20T16:35:41.602274Z","shell.execute_reply":"2022-02-20T16:35:52.113906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have actually just made more errors in our attempt to reduce them in post-processing with LGBM.","metadata":{}},{"cell_type":"markdown","source":"Visualising the predictions:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\ntest_df[\"preds\"] = y_test_pred_lgbm\n\nfor i,gcd in enumerate([1,10,1000,10000]):\n    pca = PCA(n_components = 2, random_state = 10, whiten = True)\n    pca.fit(test_df[test_df[\"gcd\"] == gcd][train_cols])\n\n    X_PCA = pca.transform(test_df[test_df[\"gcd\"] == gcd][train_cols])\n\n    PCA_df = pd.DataFrame({\"PCA_1\" : X_PCA[:,0], \"PCA_2\" : X_PCA[:,1],  \"LABEL\":test_df[test_df[\"gcd\"] == gcd][\"preds\"]})\n    \n    ax = plt.subplot(2, 2, i + 1)\n    plt.title(\"GCD: \" + str(gcd))\n    sns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique())","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:38:46.197699Z","iopub.execute_input":"2022-02-20T16:38:46.198093Z","iopub.status.idle":"2022-02-20T16:38:57.972645Z","shell.execute_reply.started":"2022-02-20T16:38:46.198051Z","shell.execute_reply":"2022-02-20T16:38:57.969191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### How many points are in each m cluster for the test data?","metadata":{}},{"cell_type":"markdown","source":"Assumming that we have classified all entries with GCD = 1 correctly and all m clusters follow the same pattern as in GCD = 1 ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 30))\n\nfor i,bacteria in enumerate(test_df[\"preds\"].unique()):\n    cluster_input_data = test_df[(test_df[\"gcd\"] == 1) & (test_df[\"preds\"] == bacteria)][train_cols]\n    cluster_input_data = StandardScaler().fit_transform(cluster_input_data)\n\n    clustering_gmm_m = GaussianMixture(n_components=8, covariance_type = 'diag', n_init=10, random_state=0).fit(cluster_input_data)\n    clustering_gmm = clustering_gmm_m.predict(cluster_input_data)\n\n    pca.fit(cluster_input_data)\n\n    X_PCA = pca.transform(cluster_input_data)\n\n    PCA_df = pd.DataFrame({\"PCA_1\" : X_PCA[:,0], \"PCA_2\" : X_PCA[:,1],\"Cluster\":clustering_gmm})    \n    \n    mvals = [\"m00\", \"m05\", \"m10\", \"m25\", \"m33\", \"m50\", \"m75\", \"m90\"]\n    PCA_1 = PCA_df.groupby(by=[\"Cluster\"])[\"PCA_1\"].mean().reset_index().sort_values(\"PCA_1\").reset_index(drop=True)\n    PCA_df = PCA_df.replace({'Cluster': {PCA_1.iloc[n][\"Cluster\"]: mvals[n] for n in range(0,8)}})\n    \n    if i == 0:\n        cluster_counts = PCA_df['Cluster'].value_counts().reset_index().rename(columns={\"index\":\"mval\", \"Cluster\":bacteria + \" count\"}).set_index(\"mval\").sort_index()\n    else:\n        cluster_counts[bacteria + \" count\"] = PCA_df['Cluster'].value_counts()\n    \n    ax = plt.subplot(5, 2, i + 1)\n    plt.title(\"GMM - Bacteria: \" + str(bacteria))\n    sns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue=\"Cluster\", hue_order=mvals, palette = sns.color_palette(\"husl\", 8))\ncluster_counts","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:38:57.974146Z","iopub.execute_input":"2022-02-20T16:38:57.974365Z","iopub.status.idle":"2022-02-20T16:40:05.845996Z","shell.execute_reply.started":"2022-02-20T16:38:57.974337Z","shell.execute_reply":"2022-02-20T16:40:05.845117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No suprises here. It follows the same pattern as in the train data.","metadata":{}},{"cell_type":"markdown","source":"## Improvements with clustering","metadata":{}},{"cell_type":"markdown","source":" Credit: This was inspired by [AsmosM](https://www.kaggle.com/ambrosm/tpsfeb22-03-clustering-improves-the-predictions)","metadata":{}},{"cell_type":"markdown","source":"Perhaps clustering can be used to improve a few of the predictions when GCD=10. \n\n* There are not any obvious mistakes for GCD = 1.\n* There are a few obvious mistakes for GCD = 10 which are easily fixable.\n* Clustering will be difficult for GCD = 1000 and GCD = 10000 but it may be possible.","metadata":{}},{"cell_type":"markdown","source":"### GCD 10","metadata":{}},{"cell_type":"markdown","source":"First lets get a better view of the models mistakes:","metadata":{}},{"cell_type":"code","source":"test_df[\"preds\"] = y_test_pred_et # Set to y_test_pred_lgbm to view lgbm predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:41:33.113691Z","iopub.execute_input":"2022-02-20T16:41:33.114031Z","iopub.status.idle":"2022-02-20T16:41:33.120054Z","shell.execute_reply.started":"2022-02-20T16:41:33.113996Z","shell.execute_reply":"2022-02-20T16:41:33.119339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca = PCA(n_components = 2, random_state = 10, whiten = True)\npca.fit(test_df[test_df[\"gcd\"] == 10][train_cols])\n\nX_PCA = pca.transform(test_df[test_df[\"gcd\"] == 10][train_cols])\n\nPCA_df = pd.DataFrame({\"PCA_1\" : X_PCA[:,0], \"PCA_2\" : X_PCA[:,1],  \"LABEL\":test_df[test_df[\"gcd\"] == 10][\"preds\"]})","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:41:33.456999Z","iopub.execute_input":"2022-02-20T16:41:33.457637Z","iopub.status.idle":"2022-02-20T16:41:34.301659Z","shell.execute_reply.started":"2022-02-20T16:41:33.457581Z","shell.execute_reply":"2022-02-20T16:41:34.300594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20,40))\n\nax = plt.subplot(5, 2, 1)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\n\nax = plt.subplot(5, 2, 2)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-1,0.5)\nplt.xlim(-1, 0.4)\n\nax = plt.subplot(5, 2, 3)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-1,0.2)\nplt.xlim(-1, -0.45)\n\nax = plt.subplot(5, 2, 4)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-1,0.2)\nplt.xlim(-0.8, -0.7)\n\nax = plt.subplot(5, 2, 5)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-0.17,0.1)\nplt.xlim(-0.82, -0.72)\n\nax = plt.subplot(5, 2, 6)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-0.4,-0.19)\nplt.xlim(-0.785, -0.72)\n\nax = plt.subplot(5, 2, 7)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-0.85,-0.55)\nplt.xlim(-0.77, -0.72);\n\nax = plt.subplot(5, 2, 8)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-0.85,-0.75)\nplt.xlim(-0.752, -0.725);\n\nax = plt.subplot(5, 2, 9)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-1,-0.8)\nplt.xlim(-0.6, -0.5);\n\nax = plt.subplot(5, 2, 10)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-1,-0.5)\nplt.xlim(-1, -0.5);","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:41:34.304027Z","iopub.execute_input":"2022-02-20T16:41:34.304626Z","iopub.status.idle":"2022-02-20T16:41:54.083192Z","shell.execute_reply.started":"2022-02-20T16:41:34.304578Z","shell.execute_reply":"2022-02-20T16:41:54.082212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The main issue for GCD 10 is classifying between Eschericha_coli and Eschericha_fergusonii (blue and light blue). Lets grab the points in some of the images and recluster them.","metadata":{}},{"cell_type":"code","source":"def recluster(xlim_low, xlim_high, ylim_low, ylim_high, ncomponents):\n    cluster_data = PCA_df[(PCA_df[\"PCA_1\"]>= xlim_low) & (PCA_df[\"PCA_1\"]<= xlim_high) & (PCA_df[\"PCA_2\"] >= ylim_low) & (PCA_df[\"PCA_2\"] <= ylim_high)]\n    cluster_data_X = X_test.iloc[cluster_data.index][train_cols].drop(columns=[\"gcd\"])\n    \n    #clustering_gmm_m = GaussianMixture(n_components=ncomponents, covariance_type = 'diag', n_init=20, random_state=2).fit(cluster_data_X)\n    #clustering_gmm = clustering_gmm_m.predict(cluster_data_X)\n    \n    kmeans = KMeans(n_clusters=ncomponents, n_init=40, random_state=0).fit(cluster_data_X)\n    \n    #cluster_data[\"LABEL_CLUSTER\"] = clustering_gmm\n    cluster_data[\"LABEL_CLUSTER\"] = kmeans.labels_\n    \n    # Labels from clustering are either 0 or 1 at random - changing 0 or 1's to match the correct labels (done by matching most points)\n    for i in range(ncomponents):\n        most = cluster_data[cluster_data[\"LABEL_CLUSTER\"]==i][\"LABEL\"].value_counts().index[0]\n        cluster_data = cluster_data.replace(i,most)\n    \n    f, ax = plt.subplots(figsize=(20,10))\n    ax = plt.subplot(1, 2, 1)\n    sns.scatterplot(data = cluster_data, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\");\n    plt.title(\"Original Predictions\")\n    plt.ylim(ylim_low,ylim_high)\n    plt.xlim(xlim_low,xlim_high)\n                    \n    ax = plt.subplot(1, 2, 2)\n    sns.scatterplot(data = cluster_data, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL_CLUSTER\");\n    plt.title(\"Cluster Predictions\")\n    plt.ylim(ylim_low, ylim_high)\n    plt.xlim(xlim_low,xlim_high)\n    \n    return cluster_data","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:44:08.56005Z","iopub.execute_input":"2022-02-20T16:44:08.560926Z","iopub.status.idle":"2022-02-20T16:44:08.572545Z","shell.execute_reply.started":"2022-02-20T16:44:08.560872Z","shell.execute_reply":"2022-02-20T16:44:08.57159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def update_predictions(cluster_data, y_test_pred_model):\n    change_preds = cluster_data[cluster_data[\"LABEL\"] != cluster_data[\"LABEL_CLUSTER\"]]\n    print(len(change_preds), \"predictions attempting to be changed\")\n    index_preds = change_preds.index\n    #Test if we are definitely changing the right predictions\n    if np.array_equal(y_test_pred_model[index_preds], change_preds[\"LABEL\"].values):\n        print(\"Check Passed: Changing correct entries\")\n        y_test_pred_model[index_preds] = change_preds[\"LABEL_CLUSTER\"]\n    return","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:41:54.102551Z","iopub.execute_input":"2022-02-20T16:41:54.103312Z","iopub.status.idle":"2022-02-20T16:41:54.120766Z","shell.execute_reply.started":"2022-02-20T16:41:54.103262Z","shell.execute_reply":"2022-02-20T16:41:54.119796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recluster(xlim_low = -0.82, xlim_high = -0.72, ylim_low = -0.17, ylim_high = 0.1, ncomponents=2)\n#Decided not to update these predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:44:12.603101Z","iopub.execute_input":"2022-02-20T16:44:12.604022Z","iopub.status.idle":"2022-02-20T16:44:17.905624Z","shell.execute_reply.started":"2022-02-20T16:44:12.603947Z","shell.execute_reply":"2022-02-20T16:44:17.904804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recluster(xlim_low = -0.785, xlim_high = -0.72, ylim_low = -0.375, ylim_high = -0.19, ncomponents=2)\n#Decided not to update these predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:44:25.792776Z","iopub.execute_input":"2022-02-20T16:44:25.79358Z","iopub.status.idle":"2022-02-20T16:44:30.289597Z","shell.execute_reply.started":"2022-02-20T16:44:25.793532Z","shell.execute_reply":"2022-02-20T16:44:30.288931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clust_data = recluster(xlim_low = -0.77, xlim_high = -0.72, ylim_low = -0.75, ylim_high = -0.55, ncomponents=3)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:44:50.093491Z","iopub.execute_input":"2022-02-20T16:44:50.093933Z","iopub.status.idle":"2022-02-20T16:44:56.016948Z","shell.execute_reply.started":"2022-02-20T16:44:50.093892Z","shell.execute_reply":"2022-02-20T16:44:56.016071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_predictions(cluster_data = clust_data, y_test_pred_model = y_test_pred_et)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:45:11.971633Z","iopub.execute_input":"2022-02-20T16:45:11.972001Z","iopub.status.idle":"2022-02-20T16:45:11.980458Z","shell.execute_reply.started":"2022-02-20T16:45:11.971949Z","shell.execute_reply":"2022-02-20T16:45:11.979228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clust_data = recluster(xlim_low = -0.752, xlim_high = -0.725, ylim_low = -0.85, ylim_high = -0.75, ncomponents=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:45:40.199573Z","iopub.execute_input":"2022-02-20T16:45:40.199877Z","iopub.status.idle":"2022-02-20T16:45:45.192249Z","shell.execute_reply.started":"2022-02-20T16:45:40.199846Z","shell.execute_reply":"2022-02-20T16:45:45.191503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"update_predictions(cluster_data = clust_data, y_test_pred_model = y_test_pred_et)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:45:48.991185Z","iopub.execute_input":"2022-02-20T16:45:48.992192Z","iopub.status.idle":"2022-02-20T16:45:49.000442Z","shell.execute_reply.started":"2022-02-20T16:45:48.992121Z","shell.execute_reply":"2022-02-20T16:45:48.999575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GCD 1000","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components = 2, random_state = 10, whiten = True)\npca.fit(test_df[test_df[\"gcd\"] == 1000][train_cols])\n\nX_PCA = pca.transform(test_df[test_df[\"gcd\"] == 1000][train_cols])\n\nPCA_df = pd.DataFrame({\"PCA_1\" : X_PCA[:,0], \"PCA_2\" : X_PCA[:,1],  \"LABEL\":test_df[test_df[\"gcd\"] == 1000][\"preds\"]})","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:45:55.826583Z","iopub.execute_input":"2022-02-20T16:45:55.827561Z","iopub.status.idle":"2022-02-20T16:45:56.655832Z","shell.execute_reply.started":"2022-02-20T16:45:55.827499Z","shell.execute_reply":"2022-02-20T16:45:56.654738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20,20))\n\nax = plt.subplot(3, 2, 1)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\n\nax = plt.subplot(3, 2, 2)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-0.5,2)\nplt.xlim(1.6, 2.6)\n\nax = plt.subplot(3, 2, 3)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(0,2)\nplt.xlim(-1.5, -0.3)\n\nax = plt.subplot(3, 2, 4)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-1.8,-1)\nplt.xlim(-1, 0)\n\nax = plt.subplot(3, 2, 5)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-0.5,1.2)\nplt.xlim(-0.5, 1.6)\n\nax = plt.subplot(3, 2, 6)\nsns.scatterplot(data = PCA_df, x = \"PCA_1\", y = \"PCA_2\", hue = \"LABEL\", hue_order = test_df[\"preds\"].unique());\nplt.ylim(-0.4,-0.19)\nplt.xlim(-0.785, -0.72);\n","metadata":{"execution":{"iopub.status.busy":"2022-02-20T17:03:27.540126Z","iopub.execute_input":"2022-02-20T17:03:27.54049Z","iopub.status.idle":"2022-02-20T17:03:42.430528Z","shell.execute_reply.started":"2022-02-20T17:03:27.540453Z","shell.execute_reply":"2022-02-20T17:03:42.429887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recluster(xlim_low = 1.6, xlim_high = 2.6, ylim_low = -0.5, ylim_high = 2, ncomponents=2)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:47:27.752332Z","iopub.execute_input":"2022-02-20T16:47:27.753251Z","iopub.status.idle":"2022-02-20T16:47:33.520628Z","shell.execute_reply.started":"2022-02-20T16:47:27.753194Z","shell.execute_reply":"2022-02-20T16:47:33.520028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Nothing to change here.","metadata":{}},{"cell_type":"markdown","source":"### Saving our predictions","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\"  style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\nWhat does this mean in terms of CV scores? If the train and test distributions are so different then how are we going to choose the optimal hyperparameters for the models? This is a rare case where we SHOULD actually trust the public leaderboard over the CV scores.\n\nFor example I have noticed that LGBM gets much worse predictions on the test set ~0.95 compared to ~0.99 for ETC despite having very similar CV scores.","metadata":{}},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/sample_submission.csv')\nsubmission['target'] = y_test_pred_et","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:48:45.271306Z","iopub.execute_input":"2022-02-20T16:48:45.27164Z","iopub.status.idle":"2022-02-20T16:48:45.377946Z","shell.execute_reply.started":"2022-02-20T16:48:45.271607Z","shell.execute_reply":"2022-02-20T16:48:45.376838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.head(10).T","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:48:45.772836Z","iopub.execute_input":"2022-02-20T16:48:45.773169Z","iopub.status.idle":"2022-02-20T16:48:45.7919Z","shell.execute_reply.started":"2022-02-20T16:48:45.773135Z","shell.execute_reply":"2022-02-20T16:48:45.790811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-20T16:48:48.937182Z","iopub.execute_input":"2022-02-20T16:48:48.937485Z","iopub.status.idle":"2022-02-20T16:48:49.090419Z","shell.execute_reply.started":"2022-02-20T16:48:48.937452Z","shell.execute_reply":"2022-02-20T16:48:49.089387Z"},"trusted":true},"execution_count":null,"outputs":[]}]}