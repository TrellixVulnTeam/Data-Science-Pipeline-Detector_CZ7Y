{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Welcome and have fun learning multiclass classification with plug and play Neural Network\n\n#### Metric: **Accuracy**. Softvoting and weighted average is the objective to score towards target class.\n\nObjective of this notebook used to be a ~simple~ and robust neural network multiclass classifier for future use.\n\n<blockquote style=\"margin-right:auto; margin-left:auto; padding: 1em; margin:24px;\">\n    <strong>Fork This Notebook!</strong><br>\nCreate your own editable copy of this notebook by clicking on the <strong>Copy and Edit</strong> button in the top right corner.\n</blockquote>","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\nfrom scipy import stats\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, RobustScaler, PowerTransformer, OneHotEncoder\nle = LabelEncoder()\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix, roc_curve, precision_recall_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import load_model\n\nfrom datetime import datetime\nfrom packaging import version\n\nprint(\"TensorFlow version: \", tf.__version__)\nassert version.parse(tf.__version__).release[0] >= 2, \\\n    \"This notebook requires TensorFlow 2.0 or above.\"\nimport tensorboard\ntensorboard.__version__\n# Clear any logs from previous runs\n!rm -rf ./logs/ \n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport gc\nimport os\nimport math\nimport random\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T14:41:01.927801Z","iopub.execute_input":"2022-02-21T14:41:01.928263Z","iopub.status.idle":"2022-02-21T14:41:09.04749Z","shell.execute_reply.started":"2022-02-21T14:41:01.928175Z","shell.execute_reply":"2022-02-21T14:41:09.046666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fine tuning\nFine tune the system using the hyperparameters and configs below:\n* FOLD - 5, 10, 15, 20.\n* SAMPLE - Set it to True for full sample run. Max sample per class.\n* BEST_OR_FOLD - True: use Best model, False: use KFOLD softvote\n* TPU - Only works on save version.\n* selu love lecun_normal","metadata":{}},{"cell_type":"code","source":"# -----------------------------------------------------------------\n# Some parameters to config \nPRODUCTION = True # True: For submission run. False: Fast trial run\n\n# Hyperparameters\nFOLDS = 20 if PRODUCTION else 5   # Only 5 or 10.\nEPOCHS = 88        # Does not matter with Early stopping. Deep network should not take too much epochs to learn\nBATCH_SIZE = 2048   # large enough to fit RAM. If unstable, tuned downward. 4096 2048\nACTIVATION = 'relu' # swish mish relu selu ;swish overfit more cause of narrow global minimun\nKERNEL_INIT = \"glorot_normal\" # Minimal impact, but give your init the right foot forward glorot_uniform lecun_normal\nLEARNING_RATE = 0.000965713 # Not used. Optimal lr is about half the maximum lr \nLR_FACTOR = 0.5   # LEARNING_RATE * LR_FACTOR = New Learning rate on ReduceLROnPlateau. lower down when the LR oscillate\nMIN_DELTA = 0.0000001 # Default 0.0001 0.0000001\nRLRP_PATIENCE = 5 # Learning Rate reduction on ReduceLROnPlateau\nES_PATIENCE = 21  # Early stopping\nDROPOUT = 0.05     # Act like L1 L2 regulator. lower your learning rate in order to overcome the \"boost\" that the dropout probability gives to the learning rate.\nHIDDEN_LAYERS = [512, 256, 128, 64]\nDIST_BOUND = True\nDIST_THRESHOLD = 0.3 # Distribution ratio\n\nOPTIMIZER = 'adam' # adam adamax nadam\nLOSS ='sparse_categorical_crossentropy' # sparse_categorical_crossentropy does not require onehot encoding on labels. categorical_crossentropy\nMETRICS ='sparse_categorical_accuracy'  # acc accuracy categorical_accuracy sparse_categorical_accuracy\nACC_VAL_METRICS = 'val_sparse_categorical_accuracy' # 'val_acc' val_accuracy val_sparse_categorical_accuracy\nACC_METRICS = 'sparse_categorical_accuracy' # acc accuracy 'sparse_categorical_accuracy'\n\n# The dataset is too huge for trial. Sampling it for speed run!\nSAMPLE = 20139 if PRODUCTION else 500   # True for FULL run. Max Sample size per category. For quick test: y counts [20139, 20063, 19947, 19958, 19937, 19847, 20030, 19929, 20074, 20076]  # 200000 total rows\nVALIDATION_SPLIT = 0.15 # Only used to min dataset for quick test\nMAX_TRIAL = 3           # speed trial any% Not used here\nMI_THRESHOLD = 0.001    # Mutual Information threshold value to drop.\n\nRANDOM_STATE = 42\nVERBOSE = 0\n\n# Admin\nID = \"row_id\"            # Id id x X index\nINPUT = \"../input/tabular-playground-series-feb-2022\"\nTPU = False           # True: use TPU.\nBEST_OR_FOLD = False # True: use Best model, False: use KFOLD softvote\nFEATURE_ENGINEERING = True\nPSEUDO_LABEL = False\nBLEND = False\n\nassert BATCH_SIZE % 2 == 0, \\\n    \"BATCH_SIZE must be even number.\"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T14:41:09.049862Z","iopub.execute_input":"2022-02-21T14:41:09.050149Z","iopub.status.idle":"2022-02-21T14:41:09.063856Z","shell.execute_reply.started":"2022-02-21T14:41:09.050112Z","shell.execute_reply":"2022-02-21T14:41:09.062428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n\nseed_everything(RANDOM_STATE)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:41:09.066656Z","iopub.execute_input":"2022-02-21T14:41:09.066898Z","iopub.status.idle":"2022-02-21T14:41:09.080259Z","shell.execute_reply.started":"2022-02-21T14:41:09.066864Z","shell.execute_reply":"2022-02-21T14:41:09.079426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reduce Memory usage","metadata":{}},{"cell_type":"code","source":"# for col in df.select_dtypes('int').columns:\n#     df[col] = pd.to_numeric(df[col], downcast = 'integer')\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n \n    return df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T14:41:09.084023Z","iopub.execute_input":"2022-02-21T14:41:09.084215Z","iopub.status.idle":"2022-02-21T14:41:09.097166Z","shell.execute_reply.started":"2022-02-21T14:41:09.084193Z","shell.execute_reply":"2022-02-21T14:41:09.095939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\n\n\ndef load_data():\n    # Read data\n    data_dir = Path(INPUT)\n    try:\n        # Read the parquet data.\n        df_train = pd.read_parquet('../input/tpsfeb22-soft-voting-baseline/train.parquet').pipe(reduce_mem_usage)\n        df_test = pd.read_parquet('../input/tpsfeb22-soft-voting-baseline/test.parquet').pipe(reduce_mem_usage)\n    except FileNotFoundError:\n        df_train = pd.read_csv(data_dir / \"train.csv\", index_col=ID).pipe(reduce_mem_usage)\n        df_test = pd.read_csv(data_dir / \"test.csv\", index_col=ID).pipe(reduce_mem_usage)\n\n    # Save the csv file to parquet.\n    # I learned parquet from this notebook: https://www.kaggle.com/wti200/one-vs-rest-approach\n    df_train.to_parquet('train.parquet')\n    df_test.to_parquet('test.parquet')\n    column_y = df_train.columns.difference(\n        df_test.columns)[0]  # column_y target_col label_col\n    return df_train, df_test, column_y","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T14:41:09.098509Z","iopub.execute_input":"2022-02-21T14:41:09.099375Z","iopub.status.idle":"2022-02-21T14:41:09.107699Z","shell.execute_reply.started":"2022-02-21T14:41:09.099339Z","shell.execute_reply":"2022-02-21T14:41:09.106953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing ##\n\nBefore we can do any feature engineering, we need to *preprocess* the data to get it in a form suitable for analysis. We'll need to:\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values\n\nWe'll wrap all these steps up in a function, which will make easy for you to get a fresh dataframe whenever you need. After reading the CSV file, we'll apply three preprocessing steps, `clean`, `encode`, and `impute`, and then create the data splits: one (`df_train`) for training the model, and one (`df_test`) for making the predictions that you'll submit to the competition for scoring on the leaderboard.","metadata":{}},{"cell_type":"code","source":"%%time\ntrain_data, test_data, TARGET_FEATURE_NAME = load_data()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T14:41:09.108945Z","iopub.execute_input":"2022-02-21T14:41:09.109392Z","iopub.status.idle":"2022-02-21T14:42:21.565208Z","shell.execute_reply.started":"2022-02-21T14:41:09.109356Z","shell.execute_reply":"2022-02-21T14:42:21.564412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(train_data[TARGET_FEATURE_NAME], return_counts=True)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T14:42:21.566731Z","iopub.execute_input":"2022-02-21T14:42:21.567192Z","iopub.status.idle":"2022-02-21T14:42:21.854245Z","shell.execute_reply.started":"2022-02-21T14:42:21.567149Z","shell.execute_reply":"2022-02-21T14:42:21.853474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Duplicate rows check\nhttps://www.kaggle.com/sfktrkl/tps-feb-2022/notebook","metadata":{}},{"cell_type":"code","source":"# Save original target distribution\ntarget_distribution = train_data[TARGET_FEATURE_NAME].value_counts().sort_index() / len(train_data) * 100\n\nduplicates_train = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates_train))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:21.855577Z","iopub.execute_input":"2022-02-21T14:42:21.855842Z","iopub.status.idle":"2022-02-21T14:42:23.371144Z","shell.execute_reply.started":"2022-02-21T14:42:21.855808Z","shell.execute_reply":"2022-02-21T14:42:23.370316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.groupby(list(train_data.columns.values)).size().reset_index(name='sample_weight').copy()\ntrain_data.drop_duplicates(keep='first', inplace=True)\nduplicates_train = train_data.duplicated().sum()\n\nprint('Train data shape:', train_data.shape)\nprint('Duplicates in train data: {0}'.format(duplicates_train))\ntrain_data","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:23.372443Z","iopub.execute_input":"2022-02-21T14:42:23.372744Z","iopub.status.idle":"2022-02-21T14:42:29.429327Z","shell.execute_reply.started":"2022-02-21T14:42:23.372701Z","shell.execute_reply":"2022-02-21T14:42:29.428507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_data.drop(columns=[TARGET_FEATURE_NAME, 'sample_weight'])\ny = train_data[[TARGET_FEATURE_NAME]]\n\nsample_weight = train_data['sample_weight']\nX_test = test_data.loc[:,X.columns]\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:29.432428Z","iopub.execute_input":"2022-02-21T14:42:29.433143Z","iopub.status.idle":"2022-02-21T14:42:29.816126Z","shell.execute_reply.started":"2022-02-21T14:42:29.433105Z","shell.execute_reply":"2022-02-21T14:42:29.815461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Undersampling\nFor experiment measurements","metadata":{}},{"cell_type":"code","source":"def sampling_size_params(labels, sampling_max_size = SAMPLE):\n    ''' Return sampling parameters {labels: sample_size}'''\n    sampling_key, sampling_count = np.unique(labels, return_counts=True)\n    sampling_count[sampling_count > sampling_max_size] = sampling_max_size\n    zip_iterator = zip(sampling_key, sampling_count)\n    return dict(zip_iterator)\n\n# not minority\nsampling_params = sampling_size_params(y, SAMPLE)\nundersample = RandomUnderSampler(\n    sampling_strategy=sampling_params, random_state=RANDOM_STATE)\n\nX, y = undersample.fit_resample(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:29.817272Z","iopub.execute_input":"2022-02-21T14:42:29.817508Z","iopub.status.idle":"2022-02-21T14:42:30.701361Z","shell.execute_reply.started":"2022-02-21T14:42:29.817473Z","shell.execute_reply":"2022-02-21T14:42:30.700547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare for multiclass classification tf.keras.utils.to_categorical(le.fit_transform(y[TARGET_FEATURE_NAME])) categorical_crossentropy\ny_cat = le.fit_transform(y[TARGET_FEATURE_NAME]) # y to categorical\n# train_data[TARGET_FEATURE_NAME] = y_cat","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:30.702478Z","iopub.execute_input":"2022-02-21T14:42:30.702739Z","iopub.status.idle":"2022-02-21T14:42:30.707672Z","shell.execute_reply.started":"2022-02-21T14:42:30.702707Z","shell.execute_reply":"2022-02-21T14:42:30.70696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_cols = X.columns # Numeric features","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:30.708804Z","iopub.execute_input":"2022-02-21T14:42:30.709408Z","iopub.status.idle":"2022-02-21T14:42:30.717597Z","shell.execute_reply.started":"2022-02-21T14:42:30.70935Z","shell.execute_reply":"2022-02-21T14:42:30.71682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check NA\nmissing_val = X.isnull().sum()\nprint(missing_val[missing_val > 0])","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:30.718854Z","iopub.execute_input":"2022-02-21T14:42:30.719216Z","iopub.status.idle":"2022-02-21T14:42:30.762704Z","shell.execute_reply.started":"2022-02-21T14:42:30.71918Z","shell.execute_reply":"2022-02-21T14:42:30.761991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_regression\ndef make_mi_scores(X, y, random_state=0):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=random_state)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.rcParams[\"figure.figsize\"] = (15,12)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\ndef drop_uninformative(df, mi_scores, threshold = 0.001):\n    return df.loc[:, mi_scores > threshold]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T14:42:30.763707Z","iopub.execute_input":"2022-02-21T14:42:30.764004Z","iopub.status.idle":"2022-02-21T14:42:30.782846Z","shell.execute_reply.started":"2022-02-21T14:42:30.763962Z","shell.execute_reply":"2022-02-21T14:42:30.782221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_features(features, df):\n    for feature in features:\n        plt.rcParams[\"figure.figsize\"] = (15,9)\n        for ctype in list(df[TARGET_FEATURE_NAME].unique()):\n            values = df.loc[df[TARGET_FEATURE_NAME] == ctype][feature].values\n            sns.scatterplot(x=values, y=np.arange(values.size), label=f\"{TARGET_FEATURE_NAME} {ctype}\", alpha=0.3, palette=\"deep\")\n        plt.title(feature)\n        plt.legend()\n        plt.show()\n\nfeatures_cols = ['A0T0G6C4',\n                 'A8T0G1C1',\n                ]\n\n# plot_features(features=features_cols,df=small_sampling)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:30.783859Z","iopub.execute_input":"2022-02-21T14:42:30.784172Z","iopub.status.idle":"2022-02-21T14:42:30.791314Z","shell.execute_reply.started":"2022-02-21T14:42:30.784137Z","shell.execute_reply":"2022-02-21T14:42:30.790525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(y, return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:30.792843Z","iopub.execute_input":"2022-02-21T14:42:30.79317Z","iopub.status.idle":"2022-02-21T14:42:30.80775Z","shell.execute_reply.started":"2022-02-21T14:42:30.793136Z","shell.execute_reply":"2022-02-21T14:42:30.806878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CSV_HEADER = list(train_data.columns[:])\n\nTARGET_FEATURE_LABELS = np.unique(y_cat)\n\nNUMERIC_FEATURE_NAMES = list(X.columns[:])\n\nCATEGORICAL_FEATURES_WITH_VOCABULARY = {}\n\nCATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n\nFEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n\nCOLUMN_DEFAULTS = [\n    [0] if feature_name in NUMERIC_FEATURE_NAMES + [TARGET_FEATURE_NAME] else [\"NA\"]\n    for feature_name in CSV_HEADER\n]\n\nNUM_CLASSES = len(TARGET_FEATURE_LABELS)\n\nINPUT_SHAPE = X.shape[-1]\nOUTPUT_SHAPE = le.classes_.shape[-1]","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:30.808914Z","iopub.execute_input":"2022-02-21T14:42:30.809209Z","iopub.status.idle":"2022-02-21T14:42:30.819295Z","shell.execute_reply.started":"2022-02-21T14:42:30.809175Z","shell.execute_reply":"2022-02-21T14:42:30.818576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(INPUT_SHAPE)\nprint(OUTPUT_SHAPE)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:30.822219Z","iopub.execute_input":"2022-02-21T14:42:30.822397Z","iopub.status.idle":"2022-02-21T14:42:30.827515Z","shell.execute_reply.started":"2022-02-21T14:42:30.822375Z","shell.execute_reply":"2022-02-21T14:42:30.826748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del mi_scores\n# del small_sampling\n# del train_data\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:30.82904Z","iopub.execute_input":"2022-02-21T14:42:30.829797Z","iopub.status.idle":"2022-02-21T14:42:30.98604Z","shell.execute_reply.started":"2022-02-21T14:42:30.82976Z","shell.execute_reply":"2022-02-21T14:42:30.985123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scaler transformer\nBy using RobustScaler(), we can remove the outliers\n![](https://github.com/furyhawk/kaggle_practice/blob/main/images/Scalers.png?raw=true)","metadata":{}},{"cell_type":"code","source":"transformer_num_cols = make_pipeline(\n    RobustScaler(),\n#     StandardScaler(),\n#     MinMaxScaler(feature_range=(0, 1))\n)\n\npreprocessor = make_column_transformer(\n    (transformer_num_cols, transform_cols)\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:30.987191Z","iopub.execute_input":"2022-02-21T14:42:30.987735Z","iopub.status.idle":"2022-02-21T14:42:30.996421Z","shell.execute_reply.started":"2022-02-21T14:42:30.987579Z","shell.execute_reply":"2022-02-21T14:42:30.995782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TODO: within folds","metadata":{}},{"cell_type":"code","source":"X_train_transformed = preprocessor.fit_transform(X)\nX_test_transformed = preprocessor.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:30.99842Z","iopub.execute_input":"2022-02-21T14:42:30.999305Z","iopub.status.idle":"2022-02-21T14:42:31.329356Z","shell.execute_reply.started":"2022-02-21T14:42:30.999266Z","shell.execute_reply":"2022-02-21T14:42:31.328653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model and Create Submissions #\n\nOnce you're satisfied with everything, it's time to create your final predictions! This cell will:\n- use the best trained model to make predictions from the test set\n- save the predictions to a CSV file\n\n$Softmax: \\sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K$\n\nK - number of classes\n\n$z_i$ - is a vector containing the scores of each class for the instance z.\n\n$\\sigma(z_i)$ - is the estimated probability that the instance z belongs to class K, given the scores of each class for that instance.\n\n$Relu(z) = max(0, z)$\n\nBinary Cross Entropy: $-{(y\\log(p) + (1 - y)\\log(1 - p))}$\n\nFor multiclass classification, we calculate a separate loss for each class label per observation and sum the result.\n\n$-\\sum_{c=1}^My_{o,c}\\log(p_{o,c})$\n\n\n    M - number of classes\n\n    log - the natural log\n\n    y - binary indicator (0 or 1) if class label c is the correct classification for observation o\n\n    p - predicted probability observation o is of class c\n\n","metadata":{}},{"cell_type":"markdown","source":"## Basic neural network blocks","metadata":{}},{"cell_type":"code","source":"modelCheckpoint = None\n\nif TPU:\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    modelCheckpoint = ModelCheckpoint(  'best_model', options = save_locally,\n                                        monitor = ACC_VAL_METRICS,\n                                        mode = 'max',\n                                        save_best_only = True,\n                                        verbose = VERBOSE,\n                                        )\nelse:\n    modelCheckpoint = ModelCheckpoint(\n                                        'best_model',\n                                        monitor = ACC_VAL_METRICS,\n                                        mode = 'max',\n                                        save_best_only = True,\n                                        verbose = VERBOSE,\n                                        )\n\nearly_stopping = EarlyStopping(\n        patience = ES_PATIENCE,\n        min_delta = MIN_DELTA,\n        monitor = ACC_VAL_METRICS,\n        mode = 'max',\n        restore_best_weights = True,       \n        baseline = None,\n        verbose = VERBOSE,\n    )\nplateau = ReduceLROnPlateau(\n        patience = RLRP_PATIENCE,\n        factor = LR_FACTOR,\n        min_lr = 1e-7,\n        monitor = 'val_loss', \n        mode = 'min',\n        verbose = VERBOSE,\n    )\n\ndef get_MLPmodel(**kwargs):\n# -----------------------------------------------------------------\n# Model , kernel_initializer=\"lecun_normal\"\n    model = keras.Sequential([\n#     layers.BatchNormalization(input_shape = [X.shape[-1]], name = 'input'),\n    layers.Dense(units = 300, input_shape = [INPUT_SHAPE], name = 'input', kernel_initializer = KERNEL_INIT, activation = ACTIVATION),\n#     layers.Dropout(rate = DROPOUT),\n    layers.BatchNormalization(),\n    layers.Dense(units = 200, kernel_initializer = KERNEL_INIT, activation = ACTIVATION),\n#     layers.Dropout(rate = DROPOUT),\n    layers.BatchNormalization(),\n    layers.Dense(units = 100, kernel_initializer = KERNEL_INIT, activation = ACTIVATION),\n#     layers.Dropout(rate = DROPOUT),\n    layers.BatchNormalization(),\n    layers.Dense(units = 50, kernel_initializer = KERNEL_INIT, activation = ACTIVATION),\n#     layers.Dropout(rate = DROPOUT),\n    layers.BatchNormalization(),\n    layers.Dense(units = OUTPUT_SHAPE, activation = 'softmax', name='output'), #y_cat.shape[-1]\n    ])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:31.330766Z","iopub.execute_input":"2022-02-21T14:42:31.331142Z","iopub.status.idle":"2022-02-21T14:42:31.342751Z","shell.execute_reply.started":"2022-02-21T14:42:31.331104Z","shell.execute_reply":"2022-02-21T14:42:31.342115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MCDropout(keras.layers.AlphaDropout):\n    '''Boost the performance of any trained dropout model without having to retrain it or even modify it at all.\n        Provide a much better measure of the models uncertainty'''\n    def call(self, inputs):\n        return super().call(inputs, training=True)\n\nclass Standardization(layers.Layer):\n    def adapt(self, data_sample):\n        self.means_ = np.mean(data_sample, axis = 0, keepdims = True)\n        self.stds_ = np.std(data_sample, axis = 0, keepdims = True)\n    def call(self, inputs):\n        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n    \n# create custom dense-block\nclass DenseBlock(layers.Layer):\n    def __init__(self, units, activation = ACTIVATION, dropout_rate = 0, l2 = 0, **kwargs):\n        super(DenseBlock, self).__init__(**kwargs)\n        self.dense = layers.Dense(\n            units = units, \n#             activation = activation,\n            kernel_initializer = KERNEL_INIT, \n#             kernel_regularizer=keras.regularizers.l2(l2)\n        )\n        self.batchn = layers.BatchNormalization()\n        self.activation = layers.Activation(activation)\n        if dropout_rate > 0:\n            self.dropout = layers.Dropout(rate = dropout_rate) #MCDropout layers.Dropout\n        else:\n            self.dropout = None\n    \n    def call(self, inputs):\n        x = self.dense(inputs)\n        x = self.activation(x)\n        x = self.batchn(x)\n        \n        if self.dropout is not None:\n            x = self.dropout(x)\n            \n        return x\n\n# create fully-connected NN\nclass MLP(keras.Model):\n    def __init__(self, hidden_layers = HIDDEN_LAYERS, activation = ACTIVATION, dropout_rate = DROPOUT, l2 = 0, **kwargs):\n        super(MLP, self).__init__(**kwargs)\n\n        self.hidden_layers = [DenseBlock(units = units,\n                                         activation = activation,\n                                         dropout_rate = dropout_rate,\n                                         l2 = l2\n                                        )\n                              for units in hidden_layers\n                             ]\n        self.softmax = layers.Dense(units = OUTPUT_SHAPE, activation = 'softmax', name='output')\n        \n    def call(self, inputs):\n        x = inputs\n        for dense_layer in self.hidden_layers:\n            x = dense_layer(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:31.344255Z","iopub.execute_input":"2022-02-21T14:42:31.344741Z","iopub.status.idle":"2022-02-21T14:42:31.360341Z","shell.execute_reply.started":"2022-02-21T14:42:31.344703Z","shell.execute_reply":"2022-02-21T14:42:31.35957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### DCNv2 Parrallel Deep & Cross Network\nCrossNet from https://www.kaggle.com/mlanhenke/tps-12-deep-cross-nn-keras","metadata":{}},{"cell_type":"code","source":"# create dense & cross model\nclass CrossNet(keras.Model):\n    def __init__(self, hidden_layers = HIDDEN_LAYERS, activation = ACTIVATION, dropout_rate = DROPOUT, l2 = 0, **kwargs):\n        super(CrossNet, self).__init__(**kwargs)\n\n        for i, units in enumerate(hidden_layers, start=1):\n            if i == 1: # Dropout before last layer only len(hidden_layers)\n                self.dense_layers = [DenseBlock(units = units, activation = activation, dropout_rate = dropout_rate, l2 = l2)]\n            else:\n                self.dense_layers.append(DenseBlock(units = units, activation = activation, dropout_rate = 0, l2 = l2))\n        \n        self.dense = layers.Dense(units = INPUT_SHAPE)\n        self.concat = layers.Concatenate()\n        self.batchn = layers.BatchNormalization()\n        self.softmax = layers.Dense(units = OUTPUT_SHAPE, activation = 'softmax', name='output')\n        \n    def call(self, inputs):\n        \n        dense, cross = inputs, inputs\n        \n        for dense_layer in self.dense_layers:\n            # Deep net TODO only dropout at last layer\n            dense = dense_layer(dense)\n            # Parrallel Cross net\n            cross_current = self.dense(cross)\n            cross = inputs * cross_current + cross\n            \n        cross = self.batchn(cross)\n        \n        merged = self.concat([dense, cross])\n        return self.softmax(merged)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:31.361731Z","iopub.execute_input":"2022-02-21T14:42:31.362156Z","iopub.status.idle":"2022-02-21T14:42:31.373319Z","shell.execute_reply.started":"2022-02-21T14:42:31.36212Z","shell.execute_reply":"2022-02-21T14:42:31.372602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def wide_deep_model(**kwargs):\n    il = layers.Input(shape=(INPUT_SHAPE), name=\"input\")\n    x1 = layers.Dense(units=100, activation=ACTIVATION)(il)\n    x = layers.Dropout(DROPOUT)(x1)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(units=200, activation=ACTIVATION)(x)\n    x = layers.Dropout(DROPOUT)(x)\n    x = layers.BatchNormalization()(x) #AlphaDropout\n    x = layers.Dense(units=100, activation=ACTIVATION)(layers.Concatenate()([x, x1]))\n    x = layers.Dropout(DROPOUT)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(units=50, activation=ACTIVATION)(x)\n    x = layers.BatchNormalization()(x)\n    output = layers.Dense(units=NUM_CLASSES, activation=\"softmax\", name=\"output\")(x)\n\n    model = tf.keras.Model([il], output)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:31.375973Z","iopub.execute_input":"2022-02-21T14:42:31.376444Z","iopub.status.idle":"2022-02-21T14:42:31.385619Z","shell.execute_reply.started":"2022-02-21T14:42:31.376408Z","shell.execute_reply":"2022-02-21T14:42:31.38466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_deep_and_cross_model(hidden_units = HIDDEN_LAYERS, dropout_rate = DROPOUT):\n    '''DCNv2 Model'''\n    inputs = layers.Input(len(X.columns)) #create_model_inputs()\n    x0 = encode_inputs(inputs, use_embedding=True)\n\n    cross = x0\n    for _ in hidden_units:\n        units = cross.shape[-1]\n        x = layers.Dense(units)(cross)\n        cross = x0 * x + cross\n    cross = layers.BatchNormalization()(cross)\n\n    deep = x0\n    for units in hidden_units:\n        deep = layers.Dense(units)(deep)\n        deep = layers.BatchNormalization()(deep)\n        deep = layers.ReLU()(deep)\n        deep = layers.Dropout(dropout_rate)(deep)\n\n    merged = layers.concatenate([cross, deep])\n    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(merged)\n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:31.386933Z","iopub.execute_input":"2022-02-21T14:42:31.387261Z","iopub.status.idle":"2022-02-21T14:42:31.397996Z","shell.execute_reply.started":"2022-02-21T14:42:31.387224Z","shell.execute_reply":"2022-02-21T14:42:31.397384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification with Gated Residual and Variable Selection Networks\n\nFrom [TPS-12] G-Res & Variable Selection NN (Keras) https://www.kaggle.com/mlanhenke/tps-12-g-res-variable-selection-nn-keras","metadata":{}},{"cell_type":"code","source":"class GatedLinearUnit(layers.Layer):\n    def __init__(self, units, **kwargs):\n        super(GatedLinearUnit, self).__init__(**kwargs)\n        self.linear = layers.Dense(units)\n        self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n\n    def call(self, inputs):\n        return self.linear(inputs) * self.sigmoid(inputs)\n\nclass GatedResidualNetwork(layers.Layer):\n    def __init__(self, units, dropout_rate, **kwargs):\n        super(GatedResidualNetwork, self).__init__(**kwargs)\n        self.units = units\n        self.elu_dense = layers.Dense(units, activation=\"elu\")\n        self.linear_dense = layers.Dense(units)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.gated_linear_unit = GatedLinearUnit(units)\n        self.layer_norm = layers.LayerNormalization()\n        self.project = layers.Dense(units)\n\n    def call(self, inputs):\n        x = self.elu_dense(inputs)\n        x = self.linear_dense(x)\n        x = self.dropout(x)\n        if inputs.shape[-1] != self.units:\n            inputs = self.project(inputs)\n        x = inputs + self.gated_linear_unit(x)\n        x = self.layer_norm(x)\n        return x\n\nclass VariableSelection(layers.Layer):\n    def __init__(self, num_features, units, dropout_rate, **kwargs):\n        super(VariableSelection, self).__init__(**kwargs)\n        self.grns = list()\n        for idx in range(num_features):\n            grn = GatedResidualNetwork(units, dropout_rate)\n            self.grns.append(grn)\n        self.grn_concat = GatedResidualNetwork(units, dropout_rate)\n        self.softmax = layers.Dense(units=num_features, activation=\"softmax\")\n\n    def call(self, inputs):\n        v = layers.concatenate(inputs)\n        v = self.grn_concat(v)\n        v = tf.expand_dims(self.softmax(v), axis=-1)\n\n        x = []\n        for idx, input in enumerate(inputs):\n            x.append(self.grns[idx](input))\n        x = tf.stack(x, axis=1)\n\n        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)\n        return outputs\ndef encode_vsn_inputs(inputs, encoding_size):\n    encoded_features = []\n    for col in range(inputs.shape[1]):\n        encoded_feature = tf.expand_dims(inputs[:, col], -1)\n        encoded_feature = layers.Dense(units=encoding_size)(encoded_feature)\n        encoded_features.append(encoded_feature)\n    return encoded_features\n\ndef create_grn_and_vsn_model(encoding_size, dropout_rate=DROPOUT):\n    inputs = layers.Input(len(X.columns))\n    feature_list = encode_vsn_inputs(inputs, encoding_size)\n    num_features = len(feature_list)\n\n    features = VariableSelection(num_features, encoding_size, dropout_rate)(\n        feature_list\n    )\n\n    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(features)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:31.403112Z","iopub.execute_input":"2022-02-21T14:42:31.403463Z","iopub.status.idle":"2022-02-21T14:42:31.421013Z","shell.execute_reply.started":"2022-02-21T14:42:31.403437Z","shell.execute_reply":"2022-02-21T14:42:31.420284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kf = StratifiedKFold(\n        n_splits=FOLDS, random_state=RANDOM_STATE, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:31.422387Z","iopub.execute_input":"2022-02-21T14:42:31.422793Z","iopub.status.idle":"2022-02-21T14:42:31.433149Z","shell.execute_reply.started":"2022-02-21T14:42:31.422758Z","shell.execute_reply":"2022-02-21T14:42:31.432492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\nSAMPLE_WEIGHT = True\n# Define the Keras TensorBoard callback.\n#logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n\nENCODING_SIZE = 4 if PRODUCTION else 4 # Encoding size for create_grn_and_vsn_model\nstrategy = None\nif TPU:\n    try:  # detect TPUs\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()  # TPU detection\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:  # detect GPUs\n        # default strategy that works on CPU and single GPU\n        strategy = tf.distribute.get_strategy()\n        print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n        # this is 8 on TPU v3-8, it is 1 on CPU and GPU\n        BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\n# if TPU:        # instantiating the model in the strategy scope creates the model on the TPU\n# resets\npreds_valid_f = {}\npreds_test = np.zeros((1, 1))  # [] init Soft voting\ntotal_acc = []\nf_scores = []\nmodels = []\n\n# best_model = load_model('../input/dcnv2-softmaxclassification/best_model')\n# preds_test = preds_test + best_model.predict(X_test_transformed, batch_size = BATCH_SIZE)\n\nfor fold, (train_index, valid_index) in enumerate(kf.split(X=X, y=y[TARGET_FEATURE_NAME])):\n\n    X_train, X_valid = X_train_transformed[train_index], X_train_transformed[valid_index]\n    y_train, y_valid = y_cat[train_index], y_cat[valid_index]\n\n    #   --------------------------------------------------------\n    # Preprocessing\n    index_valid = valid_index.tolist()\n    if SAMPLE_WEIGHT:\n        sample_weight_train = sample_weight[train_index]\n        sample_weight_valid = sample_weight[valid_index]\n    else:\n        sample_weight_train = None\n        sample_weight_valid = None\n\n    #    --------------------------------------------------------\n    #\n    #  ----------------------------------------------------------\n    # Model# instantiating the model in the strategy scope creates the model on the TPU\n    if TPU:\n        with strategy.scope():\n            model = CrossNet(\n                hidden_layers=HIDDEN_LAYERS,\n                activation=ACTIVATION,\n                dropout_rate=DROPOUT\n            )\n            model.compile(\n                optimizer=OPTIMIZER, loss=LOSS, metrics=[METRICS],\n#                         sample_weight=sample_weight_train,\n            )\n\n    else:  # CPU/GPU model = create_grn_and_vsn_model(encoding_size=ENCODING_SIZE)  wide_deep_model get_MLPmodel\n        model = CrossNet(\n                hidden_layers=HIDDEN_LAYERS,\n                activation=ACTIVATION,\n                dropout_rate=DROPOUT\n                )\n        model.compile(\n            optimizer=OPTIMIZER, loss=LOSS, metrics=[METRICS]\n            , sample_weight_mode=None\n\n            )\n\n    history = model.fit(\n#                         tf.keras.utils.pack_x_y_sample_weight(\n#                         X_train, y=y_train\n#                         , sample_weight=sample_weight_train\n#                         ),\n                        X_train, y_train,\n                        validation_data=(X_valid, y_valid),\n                        batch_size=BATCH_SIZE,\n                        epochs=EPOCHS,\n                        callbacks=[early_stopping, plateau, modelCheckpoint,  # tensorboard_callback\n                                   ],\n                        sample_weight=sample_weight_train,\n                        shuffle=True,\n                        verbose=VERBOSE,\n                        )\n\n    #  ----------------------------------------------------------\n    #  oof\n    preds_valid = model.predict(X_valid, batch_size=BATCH_SIZE)\n\n    #  ----------------------------------------------------------\n    #********  test dataset predictions for submission *********#\n    if not BEST_OR_FOLD:\n        preds_test = preds_test + \\\n            model.predict(X_test_transformed, batch_size=BATCH_SIZE)\n\n    #  ----------------------------------------------------------\n    #  Saving scores to plot the end\n    scores = pd.DataFrame(history.history)\n    scores['folds'] = fold\n    if fold == 0:\n        f_scores = scores\n        model.summary()\n#         keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")\n    else:\n        f_scores = pd.concat([f_scores, scores], axis=0)\n\n    #  ----------------------------------------------------------\n    #  concatenating valid preds\n    preds_valid_f.update(\n        dict(zip(index_valid, le.inverse_transform(np.argmax(preds_valid, axis=1)))))\n    # Getting score for a fold model\n    fold_acc = accuracy_score(y.iloc[valid_index].target, le.inverse_transform(\n        np.argmax(preds_valid, axis=1)), sample_weight=sample_weight_valid )\n    print(\n        f\"Fold {fold} accuracy_score: {fold_acc} Train: {X_train.shape} Valid: {X_valid.shape}\")\n    # Total acc\n    total_acc.append(fold_acc)\n\n    del model\n    gc.collect()\n    K.clear_session()\n\nprint(f\"mean accuracy_score: {np.mean(total_acc)}, std: {np.std(total_acc)}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:42:31.434478Z","iopub.execute_input":"2022-02-21T14:42:31.43512Z","iopub.status.idle":"2022-02-21T14:48:07.913026Z","shell.execute_reply.started":"2022-02-21T14:42:31.435085Z","shell.execute_reply":"2022-02-21T14:48:07.912302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load best_model\nif BEST_OR_FOLD:\n    if not TPU:\n        # load the saved model\n        best_model = load_model('best_model')\n    else: # TPU\n        with strategy.scope():\n            load_locally = tf.saved_model.LoadOptions(experimental_io_device = '/job:localhost')\n            best_model = tf.keras.models.load_model('best_model', options = load_locally) # loading in Tensorflow's \"SavedModel\" format\n    # Using best model to predict\n    # X_test = preprocessor.transform(X_test) Not using best loop fit TODO\n    preds_test = preds_test + best_model.predict(X_test_transformed, batch_size = BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:07.914309Z","iopub.execute_input":"2022-02-21T14:48:07.914552Z","iopub.status.idle":"2022-02-21T14:48:07.920739Z","shell.execute_reply.started":"2022-02-21T14:48:07.914519Z","shell.execute_reply":"2022-02-21T14:48:07.919954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_acc(f_scores):\n    for fold in range(f_scores['folds'].nunique()):\n        history_f = f_scores[f_scores['folds'] == fold]\n\n        best_epoch = np.argmin(np.array(history_f['val_loss']))\n        best_val_loss = history_f['val_loss'][best_epoch]\n\n        fig, ax1 = plt.subplots(1, 2, tight_layout=True, figsize=(15,4))\n\n        fig.suptitle('Fold : '+ str(fold+1) +\n                     \" Validation Loss: {:0.4f}\".format(history_f['val_loss'].min()) +\n                     \" Validation Accuracy: {:0.4f}\".format(history_f[ACC_VAL_METRICS].max()) +\n                     \" LR: {:0.8f}\".format(history_f['lr'].min())\n                     , fontsize=14)\n\n        plt.subplot(1,2,1)\n        plt.plot(history_f.loc[:, ['loss', 'val_loss']], label= ['loss', 'val_loss'])\n                \n        from_epoch = 0\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c = 'r', label = f'Best val_loss = {best_val_loss:.5f}')\n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history_f['val_loss'])[:best_epoch])\n            almost_val_loss = history_f['val_loss'][almost_epoch]\n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label = 'Second best val_loss')\n        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n        plt.xlabel('Epoch')\n        plt.ylabel('Loss')\n        plt.legend(loc='upper left')   \n        \n        ax2 = plt.gca().twinx()\n        ax2.plot(history_f.loc[:, ['lr']], 'y:', label='lr' ) # default color is same as first ax\n        ax2.set_ylabel('Learning rate')\n        ax2.legend(loc = 'upper right')\n        ax2.grid()\n\n        best_epoch = np.argmax(np.array(history_f[ACC_VAL_METRICS]))\n        best_val_acc = history_f[ACC_VAL_METRICS][best_epoch]\n        \n        plt.subplot(1,2,2)\n        plt.plot(history_f.loc[:, [ACC_METRICS, ACC_VAL_METRICS]],label= [ACC_METRICS, ACC_VAL_METRICS])\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_acc], c = 'r', label = f'Best val_acc = {best_val_acc:.5f}')\n        plt.xlabel('Epoch')\n        plt.ylabel('Accuracy')\n        plt.legend(loc = 'lower left')\n        plt.legend(fontsize = 15)\n        plt.grid(visible = True, linestyle = '-')\n\nplot_acc(f_scores)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:07.922193Z","iopub.execute_input":"2022-02-21T14:48:07.92244Z","iopub.status.idle":"2022-02-21T14:48:11.69313Z","shell.execute_reply.started":"2022-02-21T14:48:07.9224Z","shell.execute_reply":"2022-02-21T14:48:11.69248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But instead of just looking at the mean accuracy across the 10 cross-validation folds, let's plot all 10 scores for each model, along with a box plot highlighting the lower and upper quartiles, and \"whiskers\" showing the extent of the scores. Note that the `boxplot()` function detects outliers (called \"fliers\") and does not include them within the whiskers. Specifically, if the lower quartile is $Q_1$ and the upper quartile is $Q_3$, then the interquartile range $IQR = Q_3 - Q_1$ (this is the box's height), and any score lower than $Q_1 - 1.5 \\times IQR$ is a flier, and so is any score greater than $Q3 + 1.5 \\times IQR$.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (3, 5))\nplt.plot([1]*FOLDS, total_acc, \".\")\nplt.boxplot([total_acc], labels = (\"y\"))\nplt.ylabel(\"Accuracy\", fontsize = 14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:11.694404Z","iopub.execute_input":"2022-02-21T14:48:11.695113Z","iopub.status.idle":"2022-02-21T14:48:11.847786Z","shell.execute_reply.started":"2022-02-21T14:48:11.695074Z","shell.execute_reply":"2022-02-21T14:48:11.846932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_cm(cm):\n    metrics = {\n        'accuracy': cm / cm.sum(),\n        'recall' : cm / cm.sum(axis =1 ),\n        'precision': cm / cm.sum(axis = 0)\n    }\n    \n    fig, ax = plt.subplots(1,3, tight_layout = True, figsize = (25,8))\n    ax = ax.flatten()\n\n#     mask = (np.eye(cm.shape[0]) == 0) * 1\n\n    for idx, (name, matrix) in enumerate(metrics.items()):\n\n        ax[idx].set_title(name)\n\n        sns.heatmap(\n            data = matrix,\n            cmap = sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n            cbar = False,\n#             mask=mask,\n            lw = 0.25,\n            annot = True,\n            fmt = '.2f',\n            ax = ax[idx]\n        )\n    sns.despine()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:11.849107Z","iopub.execute_input":"2022-02-21T14:48:11.849568Z","iopub.status.idle":"2022-02-21T14:48:11.860743Z","shell.execute_reply.started":"2022-02-21T14:48:11.849528Z","shell.execute_reply":"2022-02-21T14:48:11.859656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_y_hat = []\nfor key, value in sorted(preds_valid_f.items()):\n    oof_y_hat.append(value)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:11.862327Z","iopub.execute_input":"2022-02-21T14:48:11.862901Z","iopub.status.idle":"2022-02-21T14:48:11.876347Z","shell.execute_reply.started":"2022-02-21T14:48:11.862858Z","shell.execute_reply":"2022-02-21T14:48:11.875203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion matrix\n\n$Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$\n\n$Precision = \\frac{TP}{TP+FP}$\n\n$Recall = \\frac{TP}{TP+FN}$\n\n$F1 = \\frac{2*Precision*Recall}{Precision+Recall} = \\frac{2*TP}{2*TP+FP+FN}$\n\nTODO: The gap is too huge. The network is overconfidence over some error.","metadata":{}},{"cell_type":"code","source":"# create confusion matrix, calculate accuracy,recall & precision\ncm = pd.DataFrame(data = confusion_matrix(y, oof_y_hat, labels = le.classes_), index = le.classes_, columns = le.classes_)\nplot_cm(cm)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:11.877929Z","iopub.execute_input":"2022-02-21T14:48:11.878437Z","iopub.status.idle":"2022-02-21T14:48:14.973552Z","shell.execute_reply.started":"2022-02-21T14:48:11.878382Z","shell.execute_reply":"2022-02-21T14:48:14.972859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Weighted Average using distribution\nhttps://www.kaggle.com/ambrosm/tpsfeb22-02-postprocessing-against-the-mutants\n\nThere are differences in error bias between train dataset and test dataset. This is due to data compression and data loss.\n\nHypothesis:\n- The target distribution of train dataset and test dataset remain the same.","metadata":{}},{"cell_type":"code","source":"def plot_cm_error(cm):\n    mask = (np.eye(cm.shape[0]) != 0) * 1\n    fig, ax = plt.subplots(tight_layout=True, figsize=(15,8))\n    sns.heatmap(\n                data = pd.DataFrame(data=cm, index=le.classes_, columns = le.classes_),\n#                 cmap=sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n                cbar = False,\n                lw = 0.25,\n                mask = mask,\n                annot = True,\n                fmt = '.0f',\n                ax = ax\n            )\n    sns.despine()\n    \nplot_cm_error(confusion_matrix(y, oof_y_hat, labels = le.classes_))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:14.974635Z","iopub.execute_input":"2022-02-21T14:48:14.975005Z","iopub.status.idle":"2022-02-21T14:48:15.749595Z","shell.execute_reply.started":"2022-02-21T14:48:14.974968Z","shell.execute_reply":"2022-02-21T14:48:15.748838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution Before:","metadata":{}},{"cell_type":"code","source":"target_distribution[0] = 9.974\ntarget_distribution[1] = 10.210\ntarget_distribution[2] = 9.909\ntarget_distribution[3] = 9.773\ntarget_distribution[4] = 9.909\ntarget_distribution[5] = 9.727\ntarget_distribution[6] = 10.345\ntarget_distribution[7] = 10.089\ntarget_distribution[8] = 10.150\ntarget_distribution[9] = 9.914","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T14:48:15.750765Z","iopub.execute_input":"2022-02-21T14:48:15.751521Z","iopub.status.idle":"2022-02-21T14:48:15.758818Z","shell.execute_reply.started":"2022-02-21T14:48:15.751483Z","shell.execute_reply":"2022-02-21T14:48:15.757803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series(le.inverse_transform(np.argmax(preds_test, axis=1)), index=X_test.index).value_counts().sort_index() / len(X_test) * 100","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T14:48:15.759991Z","iopub.execute_input":"2022-02-21T14:48:15.760743Z","iopub.status.idle":"2022-02-21T14:48:15.790599Z","shell.execute_reply.started":"2022-02-21T14:48:15.760703Z","shell.execute_reply":"2022-02-21T14:48:15.789891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Distribution After:","metadata":{}},{"cell_type":"code","source":"%%time\nfrom scipy.optimize import minimize\n\n# Credit from https://www.kaggle.com/sfktrkl/tps-feb-2022\ndef get_diff(tune):\n    y_pred_tuned = np.argmax(preds_test + tune, axis=1)\n    return target_distribution - pd.Series(le.inverse_transform(y_pred_tuned)).value_counts().sort_index() / len(X_test) * 100\n\ndef dist_diff_loss_func(weights):\n    \"\"\"Loss function to be minimized (square/sqrt.abs.get_diff.sum)\"\"\"\n    loss = np.square(abs(get_diff(weights)).sum())\n    return loss\n\nx0 = [0] * len(le.classes_) # Initial weights\nif DIST_BOUND:\n    # Bounded by classes accuracy.\n    cm = confusion_matrix(le.inverse_transform(y_cat), oof_y_hat, labels = le.classes_)\n    acc = cm / cm.sum(axis = 0 )\n    bounds = tuple((-(1-a[i])*DIST_THRESHOLD, (1-a[i])*DIST_THRESHOLD) for i, a in enumerate(acc))\nelse:\n    bounds = None\nres = minimize(dist_diff_loss_func, x0, method='Powell', bounds=bounds, options={'disp':True})\nprint(f'\\nBest weights: {res.x}')\nprint(f'\\nDiff sum(): {abs(get_diff(res.x)).sum()}')\n\ny_proba_tuned = preds_test.copy()\ny_proba_tuned[:] += res.x\ny_pred_tuned = np.argmax(y_proba_tuned, axis=1)\ny_pred_tuned = le.inverse_transform(y_pred_tuned)\n\nprint(f'\\nDistribution After:')\npd.Series(y_pred_tuned, index=X_test.index).value_counts().sort_index() / len(X_test) * 100","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T14:48:15.791756Z","iopub.execute_input":"2022-02-21T14:48:15.792236Z","iopub.status.idle":"2022-02-21T14:48:20.850113Z","shell.execute_reply.started":"2022-02-21T14:48:15.792197Z","shell.execute_reply":"2022-02-21T14:48:20.848685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:20.851401Z","iopub.execute_input":"2022-02-21T14:48:20.852087Z","iopub.status.idle":"2022-02-21T14:48:20.859049Z","shell.execute_reply.started":"2022-02-21T14:48:20.852046Z","shell.execute_reply":"2022-02-21T14:48:20.858233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(INPUT + '/sample_submission.csv')\nsub[TARGET_FEATURE_NAME] = y_pred_tuned\nsub.to_csv(\"submission.csv\", index=False)\n# sub.to_csv(\"submission_00.csv\", index=False)\ndisplay(sub.head(10))\ndisplay(sub.tail(10))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:20.860911Z","iopub.execute_input":"2022-02-21T14:48:20.861249Z","iopub.status.idle":"2022-02-21T14:48:20.956252Z","shell.execute_reply.started":"2022-02-21T14:48:20.861215Z","shell.execute_reply":"2022-02-21T14:48:20.955594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.unique(sub[TARGET_FEATURE_NAME], return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:20.957673Z","iopub.execute_input":"2022-02-21T14:48:20.957915Z","iopub.status.idle":"2022-02-21T14:48:21.05313Z","shell.execute_reply.started":"2022-02-21T14:48:20.957873Z","shell.execute_reply":"2022-02-21T14:48:21.052404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the distribution of the test predictions vs training set\nplt.figure(figsize=(20,5))\nplt.hist(y[TARGET_FEATURE_NAME], bins = np.linspace(0.5, 7.5, 8), density = True, label = 'Training labels')\nplt.hist(sub[TARGET_FEATURE_NAME], bins = np.linspace(0.5, 7.5, 8), density = True, rwidth = 0.7, label = 'Test predictions')\nplt.xlabel(TARGET_FEATURE_NAME)\nplt.ylabel('Frequency')\nplt.gca().yaxis.set_major_formatter(PercentFormatter())\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:21.055833Z","iopub.execute_input":"2022-02-21T14:48:21.05604Z","iopub.status.idle":"2022-02-21T14:48:21.390154Z","shell.execute_reply.started":"2022-02-21T14:48:21.056015Z","shell.execute_reply":"2022-02-21T14:48:21.389473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_x_labels(ax):\n    for rect in ax.patches:\n        height = rect.get_height()\n        ax.annotate(f'{int(height)}', xy=(rect.get_x()+rect.get_width()/2, height), \n                    xytext=(0, 5), textcoords='offset points', ha='center', va='bottom') \n\n# Plot the distribution of the test predictions\nfig, ax = plt.subplots(2,1,figsize = (20,8))\nsns.countplot(x = sub[TARGET_FEATURE_NAME], ax = ax[0], orient = \"h\").set_title(\"Prediction\")\nplot_x_labels(ax[0])\n# Plot the distribution of the training set\nsns.countplot(x = y[TARGET_FEATURE_NAME], ax = ax[1], orient = \"h\").set_title(\"Training labels\")\nplot_x_labels(ax[1])\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:48:21.391599Z","iopub.execute_input":"2022-02-21T14:48:21.391882Z","iopub.status.idle":"2022-02-21T14:48:22.089068Z","shell.execute_reply.started":"2022-02-21T14:48:21.391844Z","shell.execute_reply":"2022-02-21T14:48:22.088362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# From GOAT\n\nhttps://www.kaggle.com/ambrosm/tpsfeb22-03-clustering-improves-the-predictions/notebook","metadata":{}},{"cell_type":"code","source":"from math import factorial\n\nelements = [e for e in X.columns if e != ID and e != TARGET_FEATURE_NAME]\n\ndef bias(w, x, y, z):\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ndef bias_of(s):\n    w = int(s[1:s.index('T')])\n    x = int(s[s.index('T')+1:s.index('G')])\n    y = int(s[s.index('G')+1:s.index('C')])\n    z = int(s[s.index('C')+1:])\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ntrain_i = pd.DataFrame({col: ((train_data[col] + bias_of(col)) * 1000000).round().astype(int) for col in elements})\ntest_i = pd.DataFrame({col: ((X_test[col] + bias_of(col)) * 1000000).round().astype(int) for col in elements})\n\ndef gcd_of_all(df_i):\n    gcd = df_i[elements[0]]\n    for col in elements[1:]:\n        gcd = np.gcd(gcd, df_i[col])\n    return gcd\n\ntrain_data['gcd'] = gcd_of_all(train_i)\nX_test['gcd'] = gcd_of_all(test_i)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:50:12.567639Z","iopub.execute_input":"2022-02-21T14:50:12.568343Z","iopub.status.idle":"2022-02-21T14:50:14.463239Z","shell.execute_reply.started":"2022-02-21T14:50:12.568302Z","shell.execute_reply":"2022-02-21T14:50:14.462539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA, TruncatedSVD\n\ndef pca_gcd10_full(df, target, title):\n    \"\"\"Plot a 2d projection of all points of df with gcd = 10, colored by target\"\"\"\n    subset = df[df.gcd == 10]\n    pred_subset = le.transform(target)[df.gcd == 10]\n    # Compute the PCA\n    pca3 = TruncatedSVD(n_components=10, random_state=1)\n    pca3.fit(subset[elements])\n\n    # Transform the data so that the components can be analyzed\n    Xt_tr3 = pca3.transform(subset[elements])\n\n    # Plot a scattergram, projected to two PCA components\n    d0, d1 = 1, 2\n    plt.scatter(Xt_tr3[:,d0], Xt_tr3[:,d1], c=pred_subset, cmap='tab10', s=1)\n    plt.title(title)\n    \nplt.figure(figsize=(12,6))\nplt.subplot(1, 2, 1)\npca_gcd10_full(train_data, train_data[TARGET_FEATURE_NAME], 'Training, gcd=10')\nplt.subplot(1, 2, 2)\npca_gcd10_full(X_test, sub[TARGET_FEATURE_NAME], 'Current submission, gcd=10')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-21T14:50:28.230592Z","iopub.execute_input":"2022-02-21T14:50:28.230959Z","iopub.status.idle":"2022-02-21T14:50:31.313084Z","shell.execute_reply.started":"2022-02-21T14:50:28.230919Z","shell.execute_reply":"2022-02-21T14:50:31.31243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To submit these predictions to the competition, follow these steps:\n\n1. Begin by clicking on the blue **Save Version** button in the top right corner of the window.  This will generate a pop-up window.\n2. Ensure that the **Save and Run All** option is selected, and then click on the blue **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the blue **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\n# Next Steps #\n\nIf you want to keep working to improve your performance, select the blue **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.\n\nBe sure to check out [other users' notebooks](https://www.kaggle.com/c/tabular-playground-series-feb-2022/code) in this competition. You'll find lots of great ideas for new features and as well as other ways to discover more things about the dataset or make better predictions. There's also the [discussion forum](https://www.kaggle.com/c/tabular-playground-series-feb-2022/discussion), where you can share ideas with other Kagglers.\n\nHave fun Kaggling!","metadata":{}}]}