{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The task\n\nThe task is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss. In this technique, 10-mer snippets of DNA are sampled and analyzed to give the histogram of base count. In other words, the DNA segment ATATGGCCTT becomes A2T4G2C2. In the dataset we have 285 pieces of 10mner snippets and their amount for each bacteria sample. The aim is to use this lossy information to accurately predict bacteria species. \n\nThe bacteria species which need to be identified: \n* Streptococcus_pyogenes, \n* Salmonella_enterica, \n* Enterococcus_hirae, \n* Escherichia_coli, \n* Campylobacter_jejuni, Streptococcus_pneumoniae, \n* Staphylococcus_aureus, \n* Escherichia_fergusonii, \n* Bacteroides_fragilis, \n* Klebsiella_pneumoniae\n\nThe idea for this competition came from the following paper:\n\n@ARTICLE{10.3389/fmicb.2020.00257,\nAUTHOR={Wood, Ryan L. and Jensen, Tanner and Wadsworth, Cindi and Clement, Mark and Nagpal, Prashant and Pitt, William G.},   \nTITLE={Analysis of Identification Method for Bacterial Species and Antibiotic Resistance Genes Using Optical Data From DNA Oligomers},      \nJOURNAL={Frontiers in Microbiology},      \nVOLUME={11},      \nYEAR={2020},      \nURL={https://www.frontiersin.org/article/10.3389/fmicb.2020.00257},       \nDOI={10.3389/fmicb.2020.00257},      \nISSN={1664-302X}}\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Loading and inspecting the data","metadata":{}},{"cell_type":"code","source":"%%time\n# Imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-28T20:16:03.018126Z","iopub.execute_input":"2022-02-28T20:16:03.018963Z","iopub.status.idle":"2022-02-28T20:16:03.025857Z","shell.execute_reply.started":"2022-02-28T20:16:03.018918Z","shell.execute_reply":"2022-02-28T20:16:03.024897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Reading input data\ntrain = pd.read_csv('../input/tabular-playground-series-feb-2022/train.csv', dtype={'target':'category'})\ntrain.drop(columns=['row_id'], inplace= True)\n\n# Checking basic infos of the dataset\ndisplay(train.head())\ndisplay(train.info())\ndisplay(f'Duplicated rows: {train.duplicated().sum()}')\n\ndisplay(f'Missing data: {train.isna().sum().sum()}')","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:16:03.030945Z","iopub.execute_input":"2022-02-28T20:16:03.031641Z","iopub.status.idle":"2022-02-28T20:16:23.218776Z","shell.execute_reply.started":"2022-02-28T20:16:03.031592Z","shell.execute_reply":"2022-02-28T20:16:23.217753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The train dataset includes 76k duplicated rows. These could either be dropped to reduce memory usage of the dataset, or could be used to support that if a row appear more than one time, then most likely it is a correct (not lossy) sample of genomic sequence and this information can be used to give more confidence to this data. In this notebook i leave the duplicated rows in to see whether they improve the prediction. ","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:16:23.220456Z","iopub.execute_input":"2022-02-28T20:16:23.220701Z","iopub.status.idle":"2022-02-28T20:16:24.992777Z","shell.execute_reply.started":"2022-02-28T20:16:23.220657Z","shell.execute_reply":"2022-02-28T20:16:24.991765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Reducing the dataset's memory usage by optimizing datatypes in the dataset\n\ndisplay(f'Initial memory usage: {train.memory_usage().sum()/1024**2:.2f}')\n\n# The below memory reducing function is based on the function created by Daniil Kaprov in this notebook: (https://www.kaggle.com/vanguarde/tps-feb22-deep-eda-catboost-submission)\n\ndef reduce_memory_usage(df):\n    start_mem = df.memory_usage().sum()/1024**2\n    datatypes = ['float16', 'float32', 'float64']\n\n    for col in df.columns[:-1]:     \n        for dtp in datatypes:\n            if abs(df[col]).max() <= np.finfo(dtp).max:\n                df[col] = df[col].astype(dtp)\n                break\n\n    end_mem = df.memory_usage().sum()/1024**2\n    reduction = (start_mem - end_mem)*100/start_mem\n    print(f'Mem. usage decreased by {reduction:.2f}% to {end_mem:.2f}')\n    return df\n\n\ntrain = reduce_memory_usage(train)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:16:24.994233Z","iopub.execute_input":"2022-02-28T20:16:24.994439Z","iopub.status.idle":"2022-02-28T20:16:38.282322Z","shell.execute_reply.started":"2022-02-28T20:16:24.994414Z","shell.execute_reply":"2022-02-28T20:16:38.281404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visual EDA","metadata":{}},{"cell_type":"code","source":"# Checking target bacteria species\ndisplay(train.target.unique())","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:16:38.284095Z","iopub.execute_input":"2022-02-28T20:16:38.284341Z","iopub.status.idle":"2022-02-28T20:16:38.292204Z","shell.execute_reply.started":"2022-02-28T20:16:38.284313Z","shell.execute_reply":"2022-02-28T20:16:38.291294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking whether the dataset is balanced\nax = sns.countplot(data=train, y='target')\nax.set_title('Distribution of the target bacteria species')\nplt.xticks(rotation=90)\nsns.despine()\nplt.xlabel('Nr of observations')\nplt.ylabel('')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:16:38.293893Z","iopub.execute_input":"2022-02-28T20:16:38.294203Z","iopub.status.idle":"2022-02-28T20:16:38.484486Z","shell.execute_reply.started":"2022-02-28T20:16:38.294162Z","shell.execute_reply":"2022-02-28T20:16:38.483634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Luckily, the dataset is balanced, there are approximately equal amount of observations for each of the 10 bacteria species. ","metadata":{}},{"cell_type":"code","source":"# Checking cardinality: how distinct the values of the features are\nfeature_distinct_values = train.iloc[:,:-1].nunique(axis=0, dropna=True).sort_values()\nfeature_distinct_values = pd.DataFrame(feature_distinct_values, columns=['distincts'])\nfeature_distinct_values = feature_distinct_values.reset_index().rename(columns={'index':'sequence'})\n\n# Plotting cardinality of the features\nchart = sns.barplot(data=feature_distinct_values, y='sequence', x='distincts')\nplt.yticks([]) # hiding ticks\nplt.title('Cardinality: distinct values of features')\nplt.xlabel('No. of distinct values')\nplt.ylabel('Features')\nsns.despine()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:16:38.485524Z","iopub.execute_input":"2022-02-28T20:16:38.485798Z","iopub.status.idle":"2022-02-28T20:16:40.96656Z","shell.execute_reply.started":"2022-02-28T20:16:38.485767Z","shell.execute_reply":"2022-02-28T20:16:40.96573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that some sequences have a very low cardinality (below 50 distinct values), while there are sequences which where there are more than 6000 different values for a sequence.\nLet's check out the low cardinality sequences: ","metadata":{}},{"cell_type":"code","source":"print(f'Low cardinality features:')\ndisplay(feature_distinct_values[feature_distinct_values.distincts <50])\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:16:40.968108Z","iopub.execute_input":"2022-02-28T20:16:40.968605Z","iopub.status.idle":"2022-02-28T20:16:40.981084Z","shell.execute_reply.started":"2022-02-28T20:16:40.968556Z","shell.execute_reply":"2022-02-28T20:16:40.980129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that the above segments have a very low cardinality, so there are less than 30 distinct values among the 200k observations. \n\nIdea: it might be worth trying to prepare categorical data out of these at a later stage. ","metadata":{}},{"cell_type":"markdown","source":"### Checking correlation of features\n\nIf features are highly correlated, there is a chance that the problem of multicollinearity can impact the performance of a model leading to misleading results. Also, there are a lot of features (285 segments) and it would be useful for the model performance if we could easily eliminate features which do not add further information to the model anyway. \n","metadata":{}},{"cell_type":"code","source":"%%time\n# Preparing correlation matrix\nfeatures = train.iloc[:, :-1]\nfeatures_corr = features.corr()\n\n# Preparing heatmap \n # creating mask to hide the upper triangle (incl. diagonal) from the heatmap to prevent duplication\nmask = np.triu(np.ones_like(features_corr, dtype=bool), k=0) \nmask\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:16:40.982038Z","iopub.execute_input":"2022-02-28T20:16:40.982253Z","iopub.status.idle":"2022-02-28T20:17:08.807332Z","shell.execute_reply.started":"2022-02-28T20:16:40.982227Z","shell.execute_reply":"2022-02-28T20:17:08.806524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n# Plotting correlation matrix on a heatmap\n\nplt.figure(figsize =(12,8))\nax = sns.heatmap(\n    features_corr, \n    mask=mask,\n    cmap='coolwarm')\n\nax.set_title('Correlation of features')\nplt.xlabel('DNA segments')\nplt.ylabel('DNA segments')\nplt.xticks([]) # removing the ticks and labels as there are too many features to appear f\nplt.yticks([])","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:17:08.808488Z","iopub.execute_input":"2022-02-28T20:17:08.809428Z","iopub.status.idle":"2022-02-28T20:17:09.491952Z","shell.execute_reply.started":"2022-02-28T20:17:08.809384Z","shell.execute_reply":"2022-02-28T20:17:09.490883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The heatmap suggests that there is no close to 100% correlation between the features, but there may some features which are highly correlated. Let's search for the highest correlated features. ","metadata":{}},{"cell_type":"code","source":"# Sorting the correlation values from negative linear to positive linear (-1 to +1)\n \nsorted_correlation = features_corr.mask(mask).stack().sort_values() #  we selects the bottom triangle of the dataframe (wo diagonal) with the same mask as above to avoid duplication\nsorted_correlation","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:17:09.496224Z","iopub.execute_input":"2022-02-28T20:17:09.496994Z","iopub.status.idle":"2022-02-28T20:17:09.518667Z","shell.execute_reply.started":"2022-02-28T20:17:09.496929Z","shell.execute_reply":"2022-02-28T20:17:09.517875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that correlation ranges from -0.7 to +0.84, meaning that there is no perfectly linear (positive or negative) correlation between features, however, there are 4 pairs, where the correlation is strong, between 0.8 and 0.9. \n\nSuch pairs are \n* A3T0G5C2 - A3T0G3C4 (0.808503)\n* A1T2G7C0 - A1T2G6C1 (0.826917)\n* A4T1G4C1 - A4T0G1C5 (0.836726)\n* A3T1G3C3 - A3T0G3C4 (0.841112)\n\nIt seems we cannot significantly reduce dimensionality by eliminating highly correlated features.\n","metadata":{}},{"cell_type":"markdown","source":"## 2. Building model\n\n### Separating train, target and validation data","metadata":{}},{"cell_type":"code","source":"%%time\n# Defining X and y for model\nX = train.drop(columns=['target'])\ny = train.target\n\nX = np.array(X)\ny = np.array(y)\n\n# Splitting observations to train and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:17:36.39481Z","iopub.execute_input":"2022-02-28T20:17:36.395306Z","iopub.status.idle":"2022-02-28T20:17:37.311525Z","shell.execute_reply.started":"2022-02-28T20:17:36.395266Z","shell.execute_reply":"2022-02-28T20:17:37.310446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Standardizing train data and validation data. I scale only after splitting to train and validation set to avoid data leakage\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:17:37.312679Z","iopub.execute_input":"2022-02-28T20:17:37.31292Z","iopub.status.idle":"2022-02-28T20:17:37.960539Z","shell.execute_reply.started":"2022-02-28T20:17:37.312892Z","shell.execute_reply":"2022-02-28T20:17:37.959533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nle = LabelEncoder() # I make the label encoding only after splitting the dataset for training and validation\nle.fit(y_train)\ny_train = le.transform(y_train)\ny_valid = le.transform(y_valid)\ndisplay(y_train)\ndisplay(y_valid)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-28T20:17:37.96174Z","iopub.execute_input":"2022-02-28T20:17:37.961948Z","iopub.status.idle":"2022-02-28T20:17:37.99111Z","shell.execute_reply.started":"2022-02-28T20:17:37.961922Z","shell.execute_reply":"2022-02-28T20:17:37.990232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Selecting and optimizing model\n\nIn earlier notebooks, I experimented with RandomForestClassifier, SGDClassifier, KNeighborsClassifier, but reached good results with KNeighborsClassifier only (around 90% with hyperparameter tuning), the other models performed around 69-77% even after optimizing. However, while the scores with KNeighborsClassifier were promising, the model took ages to run. Interestingly, even if RandomForest did not perform well, ExtraTrees (short for Extremely Randomized Trees) came up with much better results.","metadata":{}},{"cell_type":"code","source":"%%time\nmodel = ExtraTreesClassifier(n_estimators = 1400, random_state=0)\ndisplay(model)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-28T20:17:37.992587Z","iopub.execute_input":"2022-02-28T20:17:37.992961Z","iopub.status.idle":"2022-02-28T20:17:38.010883Z","shell.execute_reply.started":"2022-02-28T20:17:37.992919Z","shell.execute_reply":"2022-02-28T20:17:38.009848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \n\n# This part below was used for grid search \n\n# hyperparameterspace\nparam = {   \n        'n_estimators': [900, 1000, 1200, 1400]\n}\n\nsearch = GridSearchCV(model, param, cv=5, scoring='accuracy', verbose=3, refit=True)\ndisplay(search)\n\n#search.fit(X_train_scaled, y_train)\n#display(search.best_estimator_)\n#display(search.best_params_) \n#display(search.best_score_)\n\n# using the best estimator found by grid search as our model\n\n#model = search.best_estimator_ \n#display(model)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:17:38.012638Z","iopub.execute_input":"2022-02-28T20:17:38.013225Z","iopub.status.idle":"2022-02-28T20:17:38.019881Z","shell.execute_reply.started":"2022-02-28T20:17:38.013177Z","shell.execute_reply":"2022-02-28T20:17:38.018919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nThe result of earlier grid searches: \n\n\nExtraTreesClassifier: param_grid={'n_estimators': [900, 1000, 1200, 1400]}\n(n_estimators=1400, random_state=0) score: 0.973\n\nExtraTreesClassifier: param_grid={'n_estimators': [200, 400, 600, 800, 1000]}\n(n_estimators=1000, random_state=0)\n\n\nKNeighborsClassifier: param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6]} : (n_neighbors=1)\n{'n_neighbors': 1}\n\nfor SGD Classifier: \nSGDClassifier(alpha=0.001, max_iter=10000)\n{'alpha': 0.001}","metadata":{}},{"cell_type":"code","source":"%%time\n\nmodel.fit(X_train_scaled, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:17:38.02114Z","iopub.execute_input":"2022-02-28T20:17:38.022023Z","iopub.status.idle":"2022-02-28T20:22:57.237283Z","shell.execute_reply.started":"2022-02-28T20:17:38.021977Z","shell.execute_reply":"2022-02-28T20:22:57.236408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ny_pred = model.predict(X_valid_scaled)\ndisplay(accuracy_score(y_valid, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:22:57.238508Z","iopub.execute_input":"2022-02-28T20:22:57.238765Z","iopub.status.idle":"2022-02-28T20:23:08.231087Z","shell.execute_reply.started":"2022-02-28T20:22:57.238735Z","shell.execute_reply":"2022-02-28T20:23:08.230452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#display(accuracy_score(y_train, model.predict(X_train_scaled))) #this was used to check overfitting","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:23:08.232067Z","iopub.execute_input":"2022-02-28T20:23:08.23277Z","iopub.status.idle":"2022-02-28T20:23:08.237565Z","shell.execute_reply.started":"2022-02-28T20:23:08.232732Z","shell.execute_reply":"2022-02-28T20:23:08.236829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fitting the model for all the observations","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Standardizing the values to all train data\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\ny = le.transform(y)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:23:08.238751Z","iopub.execute_input":"2022-02-28T20:23:08.238956Z","iopub.status.idle":"2022-02-28T20:23:09.072438Z","shell.execute_reply.started":"2022-02-28T20:23:08.238931Z","shell.execute_reply":"2022-02-28T20:23:09.071483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#Fitting the model for all the observations to so that the model has the most observation to work with\nmodel.fit(X_scaled, y) \n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:23:09.073863Z","iopub.execute_input":"2022-02-28T20:23:09.07426Z","iopub.status.idle":"2022-02-28T20:30:34.385743Z","shell.execute_reply.started":"2022-02-28T20:23:09.074212Z","shell.execute_reply":"2022-02-28T20:30:34.384913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Making predictions and submission csv","metadata":{}},{"cell_type":"code","source":"%%time\n# Reading test data\ntest = pd.read_csv('../input/tabular-playground-series-feb-2022/test.csv')\nX_test = test.drop(columns=['row_id'])\nX_test = reduce_memory_usage(X_test)\n\n\n\nX_test_scaled = scaler.transform(X_test)\ndisplay(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:30:34.387275Z","iopub.execute_input":"2022-02-28T20:30:34.388287Z","iopub.status.idle":"2022-02-28T20:31:12.056896Z","shell.execute_reply.started":"2022-02-28T20:30:34.388239Z","shell.execute_reply":"2022-02-28T20:31:12.055954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Predicting bacteria species\nprediction_encoded = model.predict(X_test_scaled)\nprediction = le.inverse_transform(prediction_encoded)\ndisplay(prediction)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:31:12.058677Z","iopub.execute_input":"2022-02-28T20:31:12.059027Z","iopub.status.idle":"2022-02-28T20:32:01.421372Z","shell.execute_reply.started":"2022-02-28T20:31:12.058974Z","shell.execute_reply":"2022-02-28T20:32:01.420365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Putting predicted results in a dataframe\nsubmission = pd.DataFrame({\n  'row_id':test.row_id,\n  'target': prediction\n})\n\nsubmission.head()\n\n# Generating submission file\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T20:32:01.423089Z","iopub.execute_input":"2022-02-28T20:32:01.423395Z","iopub.status.idle":"2022-02-28T20:32:01.573129Z","shell.execute_reply.started":"2022-02-28T20:32:01.423353Z","shell.execute_reply":"2022-02-28T20:32:01.572242Z"},"trusted":true},"execution_count":null,"outputs":[]}]}