{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary \n\n* v1 base model with CV \n* v2 Pseudo lables\n* v3 Z-score of each samples fpr each bacteria \n* V4 - add pseudo labels\n* v5 feature elimination","metadata":{}},{"cell_type":"markdown","source":"# Additonal Steps to improve \n\n* Extra Trees seems to come out on top - as it is overfitting (score on PB LB = 0.94) - we should regularize this  \n* Noise was added in the paper and this could help regularize the ET \n* Our data is not Imbalanced however our accuracy is in the 90% so any small improvement is good  -- potentially apply imbalanced scoring metrics? \n* Clustering could also create additional features","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.cluster import DBSCAN, KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold, mutual_info_classif, SelectKBest\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom collections import Counter\n\nfrom scipy.stats import mode\nfrom math import factorial\n\nimport gc","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T08:04:17.251985Z","iopub.execute_input":"2022-02-11T08:04:17.252602Z","iopub.status.idle":"2022-02-11T08:04:18.689326Z","shell.execute_reply.started":"2022-02-11T08:04:17.25245Z","shell.execute_reply":"2022-02-11T08:04:18.688467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# experimental params \n\nITERATIONS = 1030\nSEED = 42\nFOLDS = 10\n\nPSEUDO = True\nCLUSTER = True\n\nK_BEST_COLS = 250\n\nDROP_DUPS = True","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:04:18.693126Z","iopub.execute_input":"2022-02-11T08:04:18.693389Z","iopub.status.idle":"2022-02-11T08:04:18.698824Z","shell.execute_reply.started":"2022-02-11T08:04:18.693358Z","shell.execute_reply":"2022-02-11T08:04:18.697711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-feb-2022/train.csv\", index_col = 0) \ntest = pd.read_csv(\"../input/tabular-playground-series-feb-2022/test.csv\", index_col = 0) \nsub = pd.read_csv(\"../input/tabular-playground-series-feb-2022/sample_submission.csv\", index_col = 0)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T08:04:18.699881Z","iopub.execute_input":"2022-02-11T08:04:18.700153Z","iopub.status.idle":"2022-02-11T08:04:58.891826Z","shell.execute_reply.started":"2022-02-11T08:04:18.700124Z","shell.execute_reply":"2022-02-11T08:04:58.891176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T08:04:58.894395Z","iopub.execute_input":"2022-02-11T08:04:58.894969Z","iopub.status.idle":"2022-02-11T08:04:58.929308Z","shell.execute_reply.started":"2022-02-11T08:04:58.89492Z","shell.execute_reply":"2022-02-11T08:04:58.928441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print( \"Total segments\" , len( train.columns ) -1 ) ","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T08:04:58.930323Z","iopub.execute_input":"2022-02-11T08:04:58.930521Z","iopub.status.idle":"2022-02-11T08:04:58.937031Z","shell.execute_reply.started":"2022-02-11T08:04:58.930496Z","shell.execute_reply":"2022-02-11T08:04:58.936182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering \n#### From the master Luca Massaron \nhttps://www.kaggle.com/lucamassaron/basic-eda-and-model-to-start/notebook#Feature-engineering","metadata":{}},{"cell_type":"code","source":"features = train.columns[:-1]","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:04:58.938484Z","iopub.execute_input":"2022-02-11T08:04:58.938753Z","iopub.status.idle":"2022-02-11T08:04:58.949149Z","shell.execute_reply.started":"2022-02-11T08:04:58.93872Z","shell.execute_reply":"2022-02-11T08:04:58.948087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineering(df):\n    \n    df['mean'] = df[features].mean(axis=1)\n    df['median'] = df[features].median(axis=1)\n    df['q01'] = df[features].quantile(q=0.01, axis=1)\n    df['q05'] = df[features].quantile(q=0.05, axis=1)\n    df['q10'] = df[features].quantile(q=0.10, axis=1)\n    df['q25'] = df[features].quantile(q=0.25, axis=1)\n    \n    #added\n    df['q40'] = df[features].quantile(q=0.40, axis=1)\n    df['q60'] = df[features].quantile(q=0.50, axis=1)\n    \n    df['q75'] = df[features].quantile(q=0.75, axis=1)\n    df['q90'] = df[features].quantile(q=0.90, axis=1)\n    df['q95'] = df[features].quantile(q=0.95, axis=1)\n    df['q99'] = df[features].quantile(q=0.99, axis=1)\n    df['max'] = df[features].max(axis=1)\n    df['min'] = df[features].min(axis=1)\n    \n    df['std'] = df[features].std(axis=1)\n    df['range'] = df['max'] - df['min']\n    df['iqr'] = df['q75'] - df['q25']\n    df['tails'] = df['range'] / df['iqr']\n    df['dispersion'] = df['std'] / df['mean']\n    df['dispersion_2'] = df['iqr'] / df['median']\n    df['skew'] = df[features].skew(axis=1)\n    df['kurt'] = df[features].kurt(axis=1)\n    \n    df['median-max'] = df['median'] - df['max']\n    df['median-min'] = df['median'] - df['min']\n    df['q99-q95'] = df['q99'] - df['q95']\n    df['q99-q90'] = df['q99'] - df['q90']\n    df['q01-q05'] = df['q01'] - df['q05']\n    df['q01-q10'] =  df['q01'] - df['q10']\n    \n    return df\n\nfeature_engineering(test)\nfeature_engineering(train)\n\noriginal_features = features\nfeatures","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:04:58.950645Z","iopub.execute_input":"2022-02-11T08:04:58.951477Z","iopub.status.idle":"2022-02-11T08:05:48.688676Z","shell.execute_reply.started":"2022-02-11T08:04:58.951432Z","shell.execute_reply":"2022-02-11T08:05:48.687679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the amazing Ambrosm \nhttps://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense","metadata":{}},{"cell_type":"code","source":"def bias(w, x, y, z):\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ndef bias_of(s):\n    w = int(s[1:s.index('T')])\n    x = int(s[s.index('T')+1:s.index('G')])\n    y = int(s[s.index('G')+1:s.index('C')])\n    z = int(s[s.index('C')+1:])\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ntrain_i = pd.DataFrame({col: ((train[col] + bias_of(col)) * 1000000).round().astype(int) for col in original_features})\ntest_i = pd.DataFrame({col: ((test[col] + bias_of(col)) * 1000000).round().astype(int) for col in original_features})\n\ndef gcd_of_all(df_i, elements=original_features):\n    gcd = df_i.index\n    for col in elements:\n        gcd = np.gcd(gcd, df_i[col])\n    return gcd\n\ntrain['gcd'] = gcd_of_all(train_i)\ntest['gcd'] = gcd_of_all(test_i)\n\ndel train_i\ndel test_i\n","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:05:48.690581Z","iopub.execute_input":"2022-02-11T08:05:48.690874Z","iopub.status.idle":"2022-02-11T08:05:51.441084Z","shell.execute_reply.started":"2022-02-11T08:05:48.690842Z","shell.execute_reply":"2022-02-11T08:05:51.440104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pseudolabels","metadata":{}},{"cell_type":"code","source":"et_1 = pd.read_csv(\"../input/extra-trees-cv-voting/submission.csv\",index_col=0)\net_2 = pd.read_csv(\"../input/early-ensemble/submission.csv\",index_col=0)\net_3 = pd.read_csv(\"../input/tps-feb-2022/submission.csv\",index_col=0)\nensemble = pd.DataFrame({\"target\": et_2[\"target\"],\"target2\": et_1[\"target\"],\"target3\": et_3[\"target\"], })","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:05:51.44483Z","iopub.execute_input":"2022-02-11T08:05:51.445387Z","iopub.status.idle":"2022-02-11T08:05:51.697105Z","shell.execute_reply.started":"2022-02-11T08:05:51.445337Z","shell.execute_reply":"2022-02-11T08:05:51.696124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if PSEUDO:\n    print(\"Adding Pseudolabels\")\n    index_e = ensemble[ (ensemble[\"target\"]==ensemble[\"target2\"]) & (ensemble[\"target2\"]==ensemble[\"target3\"])].index\n    ensemble =ensemble.loc[index_e].drop([\"target2\",\"target3\"],axis =1)\n\n    pseudo = pd.concat([test.loc[index_e],ensemble],axis=1 )\n    \n    train = pd.concat([train,pseudo],axis=0).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:05:51.701906Z","iopub.execute_input":"2022-02-11T08:05:51.702337Z","iopub.status.idle":"2022-02-11T08:05:53.527554Z","shell.execute_reply.started":"2022-02-11T08:05:51.702288Z","shell.execute_reply":"2022-02-11T08:05:53.526891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Duplicates","metadata":{}},{"cell_type":"code","source":"if DROP_DUPS:\n    print(\"Dropping Dups\")\n    train.drop_duplicates(keep=\"first\", inplace=True,ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:05:53.530028Z","iopub.execute_input":"2022-02-11T08:05:53.530365Z","iopub.status.idle":"2022-02-11T08:05:58.149441Z","shell.execute_reply.started":"2022-02-11T08:05:53.530321Z","shell.execute_reply":"2022-02-11T08:05:58.148461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustering - DBSCAN\nThe assumption comes from the paper and shows that there is clustering for the classes \\\nThe paper also noted that with the addition of noise, the classes where still distinctive when clustered \n\n* we will try cluster with multiple distance/min_samples and see if this extra feature assists our final model in distinguishing classes","metadata":{}},{"cell_type":"code","source":"if CLUSTER:\n    all_df = pd.concat([train.assign(ds=\"a\"),test.assign(ds=\"b\")],axis =0).drop(\"target\",axis=1)\n    for idex_e, e in enumerate ([1]): # increase will increase clusters\n        for idex_s, min_s in enumerate([6 ,7,8,9, 10]): #increase will decrease clusters \n            print(f\"eps: {e} | min_samples: {min_s}\")\n            clustering = DBSCAN(eps=e, min_samples=min_s).fit(all_df.drop(\"ds\",axis =1))\n            labels = clustering.labels_\n            print (len(set(labels)) - (1 if -1 in labels else 0))\n\n            col = f\"cluster_{idex_e}{idex_s}\"\n\n            all_df[col] = labels\n            train[col] =  all_df[all_df[\"ds\"]==\"a\"][col]\n            test[col] =  all_df[all_df[\"ds\"]==\"b\"][col]\n            \n    for idex_e, e in enumerate ([0.05]): \n        for idex_s, min_s in enumerate([5, 6,7]): \n            print(f\"eps: {e} | min_samples: {min_s}\")\n            clustering = DBSCAN(eps=e, min_samples=min_s).fit(all_df.drop(\"ds\",axis =1))\n            labels = clustering.labels_\n            print (len(set(labels)) - (1 if -1 in labels else 0))\n\n            col = f\"cluster_{idex_e}{idex_s}\"\n\n            all_df[col] = labels\n            train[col] =  all_df[all_df[\"ds\"]==\"a\"][col]\n            test[col] =  all_df[all_df[\"ds\"]==\"b\"][col]","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:49:02.724174Z","iopub.execute_input":"2022-02-11T08:49:02.724376Z","iopub.status.idle":"2022-02-11T08:49:02.738036Z","shell.execute_reply.started":"2022-02-11T08:49:02.72435Z","shell.execute_reply":"2022-02-11T08:49:02.737175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Z-scoring \n### Difference to zscore for each bacteria & for each column  \n1. We will look at each column (that is a DNA segment) and calculate the mean and standard deviation for each bacteria\n1. We will then take each sample and find the zscore against each bacteria\n1. The minimum zscore is identified - and saved in a new column ","metadata":{}},{"cell_type":"code","source":"encoder = LabelEncoder()\ntrain[\"target\"] = encoder.fit_transform(train[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:49:02.739651Z","iopub.execute_input":"2022-02-11T08:49:02.740129Z","iopub.status.idle":"2022-02-11T08:49:02.833061Z","shell.execute_reply.started":"2022-02-11T08:49:02.740098Z","shell.execute_reply":"2022-02-11T08:49:02.831992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_min_zscore(x, bac_mean, bac_std):\n    \n    z_score = (x-bac_mean)/bac_std\n    \n    #variance = (x - np.array(bac_mean))**2/ (len(train)-1)\n    return np.argmin(z_score)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:49:02.834666Z","iopub.execute_input":"2022-02-11T08:49:02.835471Z","iopub.status.idle":"2022-02-11T08:49:02.841434Z","shell.execute_reply.started":"2022-02-11T08:49:02.835433Z","shell.execute_reply":"2022-02-11T08:49:02.840502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"segments = [col for col in train.columns if \"A\" in col]\nvariance_df = pd.DataFrame(index=train[\"target\"].unique(), columns=[segments])\n\n#for each column \nfor col in segments :\n    \n    #Create arrays to hold mean/std for each bacteria\n    bacteria_mean = np.array(train[\"target\"].unique()).astype(\"float64\")\n    bacteria_std = np.array(train[\"target\"].unique()).astype(\"float64\")\n    \n    #for each bacteria type\n    for idx, bacteria in enumerate(train[\"target\"].unique()):\n        \n        #get a slice of train for bacteria and column - then get mean and std dev\n        splice = train[train[\"target\"]==bacteria][col]\n        \n        #create a list of mean values for each bacteria slice \n        bacteria_mean[idx] = np.mean(splice)\n        bacteria_std[idx] = np.std(splice)\n        \n        #std_dev =np.std(splice)\n        \n    variance_df[col] = bacteria_mean\n    print(col)\n    #create new column - for each value in column - check if closest to which mean\n    train[col+\"_\"+\"z\"] = train[col].apply(lambda x: check_min_zscore(x, bacteria_mean,bacteria_std) )\n    test[col+\"_\"+\"z\"] = test[col].apply(lambda x: check_min_zscore(x, bacteria_mean,bacteria_std) )","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-11T08:49:02.842642Z","iopub.execute_input":"2022-02-11T08:49:02.842859Z","iopub.status.idle":"2022-02-11T09:01:22.837093Z","shell.execute_reply.started":"2022-02-11T08:49:02.842832Z","shell.execute_reply":"2022-02-11T09:01:22.836184Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### We have created many new features lets see if it helps \nSecond option is to create 4 columns for each bacteria for each feature \n= Column1_zscore_bacteria1","metadata":{}},{"cell_type":"markdown","source":"# Feature Elimination \n\nAmazing notebook by [ ARJUN PRASAD SARKHEL](https://www.kaggle.com/arjunprasadsarkhel/tps-feb-k-best-features/notebook)\n\n","metadata":{}},{"cell_type":"code","source":"def finding_correlation(data, threshold):\n    correlated_columns = set()\n    correlation_matrix = data[features].corr()\n    for i in range(correlation_matrix.shape[0]):\n        for j in range(i):\n            if abs(correlation_matrix.iloc[i,j]) > threshold:\n                column_name = correlation_matrix.columns[i]\n                correlated_columns.add(column_name)\n    return correlated_columns","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:05:58.150728Z","iopub.execute_input":"2022-02-11T08:05:58.151437Z","iopub.status.idle":"2022-02-11T08:05:58.157476Z","shell.execute_reply.started":"2022-02-11T08:05:58.151398Z","shell.execute_reply":"2022-02-11T08:05:58.156382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_correlated_cols = finding_correlation(train, .8)\nprint(\"Train correlated cols=\", len(tr_correlated_cols))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:05:58.159293Z","iopub.execute_input":"2022-02-11T08:05:58.159674Z","iopub.status.idle":"2022-02-11T08:06:56.692148Z","shell.execute_reply.started":"2022-02-11T08:05:58.15963Z","shell.execute_reply":"2022-02-11T08:06:56.691293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"te_correlated_cols = finding_correlation(test, .8)\nprint(\"Test correlated cols=\", len(te_correlated_cols))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:06:56.693338Z","iopub.execute_input":"2022-02-11T08:06:56.693559Z","iopub.status.idle":"2022-02-11T08:07:20.092303Z","shell.execute_reply.started":"2022-02-11T08:06:56.693533Z","shell.execute_reply":"2022-02-11T08:07:20.091348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cor_cols = list(set(tr_correlated_cols).intersection(te_correlated_cols))\ncor_cols","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:07:20.093536Z","iopub.execute_input":"2022-02-11T08:07:20.09378Z","iopub.status.idle":"2022-02-11T08:07:20.09988Z","shell.execute_reply.started":"2022-02-11T08:07:20.093751Z","shell.execute_reply":"2022-02-11T08:07:20.099147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(cor_cols, inplace=True, axis=1)\ntest.drop(cor_cols, inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:07:20.101036Z","iopub.execute_input":"2022-02-11T08:07:20.101859Z","iopub.status.idle":"2022-02-11T08:07:20.419459Z","shell.execute_reply.started":"2022-02-11T08:07:20.101823Z","shell.execute_reply":"2022-02-11T08:07:20.418791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Remove mutual information columns  ","metadata":{}},{"cell_type":"code","source":"features = [column for column in train.columns if column not in ('target')]\nX_feat = train[features]\nY_feat = train.target\nprint(X_feat.shape, Y_feat.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:07:20.420561Z","iopub.execute_input":"2022-02-11T08:07:20.421348Z","iopub.status.idle":"2022-02-11T08:07:20.627235Z","shell.execute_reply.started":"2022-02-11T08:07:20.421306Z","shell.execute_reply":"2022-02-11T08:07:20.626318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%time\nmutual_info = mutual_info_classif(X_feat, Y_feat)\n\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X_feat.columns\nmutual_info.sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:07:20.628466Z","iopub.execute_input":"2022-02-11T08:07:20.62902Z","iopub.status.idle":"2022-02-11T08:28:10.984916Z","shell.execute_reply.started":"2022-02-11T08:07:20.628977Z","shell.execute_reply":"2022-02-11T08:28:10.983929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select K best columns \nuse information obtained from Mutual info and select k best columns ","metadata":{}},{"cell_type":"code","source":"select_k_features  = SelectKBest(mutual_info_classif, k=K_BEST_COLS)\nselect_k_features.fit(X_feat, Y_feat)\nselect_k_features","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:28:10.986258Z","iopub.execute_input":"2022-02-11T08:28:10.986504Z","iopub.status.idle":"2022-02-11T08:49:01.968036Z","shell.execute_reply.started":"2022-02-11T08:28:10.986474Z","shell.execute_reply":"2022-02-11T08:49:01.967145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = X_feat.columns[select_k_features.get_support()]\nprint(list(cols))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:49:01.969835Z","iopub.execute_input":"2022-02-11T08:49:01.970464Z","iopub.status.idle":"2022-02-11T08:49:01.979115Z","shell.execute_reply.started":"2022-02-11T08:49:01.970418Z","shell.execute_reply":"2022-02-11T08:49:01.978265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ = train[cols]\ntrain_['target'] = train.target\ntrain = train_.copy()\ndel train_\ngc.collect()\ntest = test[cols]","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:49:01.980505Z","iopub.execute_input":"2022-02-11T08:49:01.980807Z","iopub.status.idle":"2022-02-11T08:49:02.722815Z","shell.execute_reply.started":"2022-02-11T08:49:01.980768Z","shell.execute_reply":"2022-02-11T08:49:02.721974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Downcasting ","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    df = df.copy()\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test) ","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:01:22.838291Z","iopub.execute_input":"2022-02-11T09:01:22.8385Z","iopub.status.idle":"2022-02-11T09:02:17.819291Z","shell.execute_reply.started":"2022-02-11T09:01:22.838475Z","shell.execute_reply":"2022-02-11T09:02:17.818488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"features_added= [col for col in train.columns if col not in features]\nfeatures = [col for col in train.columns if col not in ['target']]\nfeatures_added.remove(\"target\")\nprint(list(features_added))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:02:17.820861Z","iopub.execute_input":"2022-02-11T09:02:17.821107Z","iopub.status.idle":"2022-02-11T09:02:17.829278Z","shell.execute_reply.started":"2022-02-11T09:02:17.821074Z","shell.execute_reply":"2022-02-11T09:02:17.828274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train.drop(\"target\",axis =1)\ny = train[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.30, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:02:17.838822Z","iopub.execute_input":"2022-02-11T09:02:17.839509Z","iopub.status.idle":"2022-02-11T09:02:19.310491Z","shell.execute_reply.started":"2022-02-11T09:02:17.83945Z","shell.execute_reply":"2022-02-11T09:02:19.309649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Base model ","metadata":{}},{"cell_type":"code","source":"params ={} \nmodel = ExtraTreesClassifier( n_estimators = ITERATIONS ,random_state=SEED, n_jobs=-1 )\n\ndef build_run_model(estimator, params, X_train, y_train,X_test,y_test): \n    \n    estimator.fit(X_train,y_train)\n    y_pred = estimator.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred) \n    print(\"Accuracy\",accuracy )\n\n    return accuracy, estimator, y_pred","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:02:19.391479Z","iopub.execute_input":"2022-02-11T09:02:19.39172Z","iopub.status.idle":"2022-02-11T09:02:19.3986Z","shell.execute_reply.started":"2022-02-11T09:02:19.391678Z","shell.execute_reply":"2022-02-11T09:02:19.397741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy, model, preds = build_run_model(model, params, X_train, y_train,X_test,y_test)\nprint(\"Accuracy\",accuracy)\nprint(classification_report(y_test,preds))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:02:19.400371Z","iopub.execute_input":"2022-02-11T09:02:19.40066Z","iopub.status.idle":"2022-02-11T09:09:00.372308Z","shell.execute_reply.started":"2022-02-11T09:02:19.400623Z","shell.execute_reply":"2022-02-11T09:09:00.367777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importance = pd.DataFrame(data= model.feature_importances_, index = X.columns, columns=[\"importance\"],dtype= \"float64\").sort_values(by = \"importance\", ascending = False)\nfeature_importance","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:09:00.374052Z","iopub.execute_input":"2022-02-11T09:09:00.374283Z","iopub.status.idle":"2022-02-11T09:09:00.79404Z","shell.execute_reply.started":"2022-02-11T09:09:00.374257Z","shell.execute_reply":"2022-02-11T09:09:00.793454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#added features\nfeature_importance.loc[features_added].sort_values(by = \"importance\", ascending= False).transpose()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:09:00.795337Z","iopub.execute_input":"2022-02-11T09:09:00.795697Z","iopub.status.idle":"2022-02-11T09:09:00.818169Z","shell.execute_reply.started":"2022-02-11T09:09:00.795667Z","shell.execute_reply":"2022-02-11T09:09:00.817646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(25,10))\n# sns.barplot(data= feature_importance.loc[features_added[:150]], x = feature_importance.loc[features_added[:150]].index,y=\"importance\" )\n# plt.xticks(rotation = 90)\n# plt.title(\"Added features part 1\")\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:09:00.819485Z","iopub.execute_input":"2022-02-11T09:09:00.819736Z","iopub.status.idle":"2022-02-11T09:09:03.181776Z","shell.execute_reply.started":"2022-02-11T09:09:00.819699Z","shell.execute_reply":"2022-02-11T09:09:03.180858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(25,10))\n# sns.barplot(data= feature_importance.loc[features_added[150:]], x = feature_importance.loc[features_added[150:]].index,y=\"importance\" )\n# plt.xticks(rotation = 90)\n# plt.title(\"Added features part 2\")\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:09:03.183189Z","iopub.execute_input":"2022-02-11T09:09:03.183513Z","iopub.status.idle":"2022-02-11T09:09:04.543901Z","shell.execute_reply.started":"2022-02-11T09:09:03.183461Z","shell.execute_reply":"2022-02-11T09:09:04.543289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Features with 0 importance\nprint(\"Zero important features\\n\", list(feature_importance[feature_importance.importance==0].index))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:09:04.545092Z","iopub.execute_input":"2022-02-11T09:09:04.545342Z","iopub.status.idle":"2022-02-11T09:09:04.551755Z","shell.execute_reply.started":"2022-02-11T09:09:04.545302Z","shell.execute_reply":"2022-02-11T09:09:04.55055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"0. 'Bacteroides_fragilis', \n1. 'Campylobacter_jejuni',\n1. 'Enterococcus_hirae',\n1. 'Escherichia_coli',\n1. 'Escherichia_fergusonii',\n1. 'Klebsiella_pneumoniae', \n1. 'Salmonella_enterica',\n1. 'Staphylococcus_aureus', \n1. 'Streptococcus_pneumoniae',\n1. 'Streptococcus_pyogenes'","metadata":{}},{"cell_type":"markdown","source":"# Cross Validation - Train","metadata":{}},{"cell_type":"code","source":"del X_train\ndel X_test\ndel y_train\ndel y_test\ndel feature_importance\ndel preds\ndel accuracy\ndel DBSCAN","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:09:04.553149Z","iopub.execute_input":"2022-02-11T09:09:04.553431Z","iopub.status.idle":"2022-02-11T09:09:04.577948Z","shell.execute_reply.started":"2022-02-11T09:09:04.55339Z","shell.execute_reply":"2022-02-11T09:09:04.5768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:09:04.579315Z","iopub.execute_input":"2022-02-11T09:09:04.580723Z","iopub.status.idle":"2022-02-11T09:09:04.589375Z","shell.execute_reply.started":"2022-02-11T09:09:04.580672Z","shell.execute_reply":"2022-02-11T09:09:04.588321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_val(X,y):\n    \n    scores = []\n    val_preds = []\n    test_preds = []\n    test_proba = []\n\n    #for viz\n    actuals =[]\n    preds= []\n\n    for idx, (train_idx, val_idx) in enumerate( cv.split(X,y)):\n        print (f\"Running fold {idx}\")\n\n        X_train , X_test = X.iloc[train_idx] , X.iloc[val_idx]\n        y_train, y_test = y[train_idx], y[val_idx]\n\n        model.fit(X_train,y_train)\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred) \n        print(\"Accuracy\",accuracy )\n\n        #save preditions on Training set\n        #train.loc[y_test.index, \"pred_train\"]= y_pred \n\n        scores.append(accuracy)\n        test_preds.append(model.predict(test) ) \n        test_proba.append( model.predict_proba(test))\n\n        #save actuals and preds for visualisation of missclassified\n        actuals.extend( list(y_test))\n        preds.extend(list(y_pred))\n\n    print( \"\\nFinal Accuracy\" , np.mean(scores)  ) \n    \n    return test_preds, test_proba, scores, actuals, preds","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:09:04.591029Z","iopub.execute_input":"2022-02-11T09:09:04.591757Z","iopub.status.idle":"2022-02-11T09:09:04.60479Z","shell.execute_reply.started":"2022-02-11T09:09:04.591708Z","shell.execute_reply":"2022-02-11T09:09:04.603821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds, test_proba, scores, actuals_viz, preds_viz = cross_val(X,y)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T09:09:04.606367Z","iopub.execute_input":"2022-02-11T09:09:04.606947Z","iopub.status.idle":"2022-02-11T10:49:23.041118Z","shell.execute_reply.started":"2022-02-11T09:09:04.606903Z","shell.execute_reply":"2022-02-11T10:49:23.039242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"miss_class = pd.DataFrame(data = {\"actuals\":actuals_viz, \"preds\":preds_viz})\nmiss_class[\"mismatch\"] = miss_class[\"actuals\"].astype(str)+\"_\"+miss_class[\"preds\"].astype(str)\n\nplt.figure(figsize=(25,8))\nsns.countplot(miss_class[miss_class[\"actuals\"]!= miss_class[\"preds\"]][\"mismatch\"], order = miss_class[miss_class[\"actuals\"]!= miss_class[\"preds\"]][\"mismatch\"].value_counts().index)\nplt.title(\"Miss-Classification: Actual_Predicted \")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:49:23.048043Z","iopub.execute_input":"2022-02-11T10:49:23.048524Z","iopub.status.idle":"2022-02-11T10:49:24.958662Z","shell.execute_reply.started":"2022-02-11T10:49:23.04847Z","shell.execute_reply":"2022-02-11T10:49:24.957403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post Processing \nProbabilities check and shift ","metadata":{}},{"cell_type":"markdown","source":"# Submission ","metadata":{}},{"cell_type":"code","source":"# Sum and argmax Predictions \ntest_preds = np.argmax(sum(test_proba) / len(test_proba), axis=1)\ntest_preds","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:49:24.960242Z","iopub.execute_input":"2022-02-11T10:49:24.960578Z","iopub.status.idle":"2022-02-11T10:49:25.024803Z","shell.execute_reply.started":"2022-02-11T10:49:24.960522Z","shell.execute_reply":"2022-02-11T10:49:25.023843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = encoder.inverse_transform(test_preds)\nsub[\"target\"] = final_preds\nsub.to_csv(\"submission.csv\")\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:49:25.026509Z","iopub.execute_input":"2022-02-11T10:49:25.026886Z","iopub.status.idle":"2022-02-11T10:49:25.357878Z","shell.execute_reply.started":"2022-02-11T10:49:25.026842Z","shell.execute_reply":"2022-02-11T10:49:25.356846Z"},"trusted":true},"execution_count":null,"outputs":[]}]}