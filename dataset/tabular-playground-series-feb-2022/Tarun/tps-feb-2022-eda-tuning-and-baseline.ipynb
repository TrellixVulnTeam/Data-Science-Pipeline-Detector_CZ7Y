{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TPS FEB 2022 EDA and Baseline","metadata":{}},{"cell_type":"markdown","source":"![@3dparadise Unsplash](https://images.unsplash.com/photo-1628595351029-c2bf17511435?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1032&q=50)\n\n### Objective\n\nHere the task is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss.\n\n### Evaluation\n\n`Accuaracy` will be used as metric to evaluate submissions.\n\n$$ Accuracy = \\frac{Total\\ Correct\\ Predictions}{Total\\ Number\\ of\\ Observations} $$\n\n### About Data\n\nEach row of data contains a spectrum of histograms generated by repeated measurements of a sample, each row containing the output of all 286 histogram possibilities (e.g.,  `A0G0T0B10 to A10G0T0B0` ), which then has a bias spectrum (of totally random ATGC) subtracted from the results.\n\nThe data (both train and test) also contains simulated measurement errors (of varying rates) for many of the samples, which makes the problem more challenging.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.preprocessing as prep\nimport sklearn.model_selection as ms\nimport sklearn.metrics as metrics\nimport sklearn.ensemble as esm\nimport catboost as ctb\nimport sklearn.impute as imputer\nimport scipy.stats as stats\nfrom sklearn.experimental import enable_iterative_imputer\nfrom skopt import BayesSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.style.use(\"ggplot\")\n%matplotlib inline","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T12:57:37.945399Z","iopub.execute_input":"2022-02-04T12:57:37.945885Z","iopub.status.idle":"2022-02-04T12:57:39.656593Z","shell.execute_reply.started":"2022-02-04T12:57:37.945798Z","shell.execute_reply":"2022-02-04T12:57:39.655621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_csv = pd.read_csv(\"../input/tabular-playground-series-feb-2022/train.csv\")\ntest_csv = pd.read_csv(\"../input/tabular-playground-series-feb-2022/test.csv\")\nsample = pd.read_csv(\"../input/tabular-playground-series-feb-2022/sample_submission.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T12:57:42.394744Z","iopub.execute_input":"2022-02-04T12:57:42.39506Z","iopub.status.idle":"2022-02-04T12:58:22.840529Z","shell.execute_reply.started":"2022-02-04T12:57:42.395024Z","shell.execute_reply":"2022-02-04T12:58:22.83966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA (exploratory data analysis)","metadata":{}},{"cell_type":"markdown","source":"### Basic EDA","metadata":{}},{"cell_type":"code","source":"print(\"Number of samples in train_csv\", len(train_csv))\nprint(\"Number of samples in test_csv\", len(test_csv))\nprint(\"train_csv shape\", train_csv.shape)\nprint(\"test_csv shape\", test_csv.shape)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:42:41.059464Z","iopub.execute_input":"2022-02-04T07:42:41.059714Z","iopub.status.idle":"2022-02-04T07:42:41.066852Z","shell.execute_reply.started":"2022-02-04T07:42:41.059679Z","shell.execute_reply":"2022-02-04T07:42:41.065691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets take a high level look at train data","metadata":{}},{"cell_type":"code","source":"train_csv.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T07:42:41.069791Z","iopub.execute_input":"2022-02-04T07:42:41.070117Z","iopub.status.idle":"2022-02-04T07:42:41.115984Z","shell.execute_reply.started":"2022-02-04T07:42:41.070074Z","shell.execute_reply":"2022-02-04T07:42:41.114892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets visualize some samples from train csv file to know its structure","metadata":{}},{"cell_type":"code","source":"train_csv.head(20)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:42:41.117732Z","iopub.execute_input":"2022-02-04T07:42:41.117982Z","iopub.status.idle":"2022-02-04T07:42:41.179854Z","shell.execute_reply.started":"2022-02-04T07:42:41.117941Z","shell.execute_reply":"2022-02-04T07:42:41.178637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`row-id` denotes index and can be neglected from features list, also target which we have to predict are strings which we will convert to integer using some encoding.","metadata":{}},{"cell_type":"code","source":"FEATURES = [col for col in train_csv.columns if col not in [\"row_id\", \"target\"]]\nTARGET = \"target\"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T12:58:51.145495Z","iopub.execute_input":"2022-02-04T12:58:51.145805Z","iopub.status.idle":"2022-02-04T12:58:51.150755Z","shell.execute_reply.started":"2022-02-04T12:58:51.145772Z","shell.execute_reply":"2022-02-04T12:58:51.149992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets briefly take a glimpse of values range, mean and standard deviation of every column in train data and test data","metadata":{}},{"cell_type":"code","source":"train_csv.describe()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:42:41.190004Z","iopub.execute_input":"2022-02-04T07:42:41.19036Z","iopub.status.idle":"2022-02-04T07:42:43.349146Z","shell.execute_reply.started":"2022-02-04T07:42:41.190323Z","shell.execute_reply":"2022-02-04T07:42:43.348125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_csv.describe()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:42:43.350545Z","iopub.execute_input":"2022-02-04T07:42:43.350777Z","iopub.status.idle":"2022-02-04T07:42:44.899254Z","shell.execute_reply.started":"2022-02-04T07:42:43.350746Z","shell.execute_reply":"2022-02-04T07:42:44.898435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"all `286` features are float, lets also take a look if data contains any null values in both train and test csv","metadata":{}},{"cell_type":"code","source":"print(\"Number of NaN values in train_csv:\", train_csv.isnull().sum().sum())\nprint(\"Number of NaN values in test_csv:\", test_csv.isnull().sum().sum())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:42:44.900406Z","iopub.execute_input":"2022-02-04T07:42:44.901033Z","iopub.status.idle":"2022-02-04T07:42:45.021192Z","shell.execute_reply.started":"2022-02-04T07:42:44.900989Z","shell.execute_reply":"2022-02-04T07:42:45.020182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"No null values thats good!","metadata":{}},{"cell_type":"markdown","source":"### Individual Feature Distribution","metadata":{}},{"cell_type":"markdown","source":"We are plotting all `286` features to get the idea of distribution of these features.","metadata":{}},{"cell_type":"code","source":"counts = 0","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:42:45.02571Z","iopub.execute_input":"2022-02-04T07:42:45.026232Z","iopub.status.idle":"2022-02-04T07:42:45.030925Z","shell.execute_reply.started":"2022-02-04T07:42:45.026193Z","shell.execute_reply":"2022-02-04T07:42:45.02983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = 10\ncols = 5\nfig, axes = plt.subplots(rows, cols, figsize=(20, 20))\nfig.suptitle(f\"Numerical continious features distribution: {counts+1} - {counts+(rows*cols)}\", fontsize=24, y=1.01)\naxes = axes.ravel()\nfor num, feat in enumerate(FEATURES[counts:counts+(rows*cols)]):\n    ax = sns.kdeplot(x=feat, data=train_csv, shade=True, ax=axes[num])\n    ax.set_title(feat.upper())\n    ax.tick_params(axis=\"x\", labelrotation=90)\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()\ncounts += rows*cols","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:42:45.032295Z","iopub.execute_input":"2022-02-04T07:42:45.032538Z","iopub.status.idle":"2022-02-04T07:44:17.483642Z","shell.execute_reply.started":"2022-02-04T07:42:45.032502Z","shell.execute_reply":"2022-02-04T07:44:17.482779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = 10\ncols = 5\nfig, axes = plt.subplots(rows, cols, figsize=(20, 20))\nfig.suptitle(f\"Numerical continious features distribution: {counts+1} - {counts+(rows*cols)}\", fontsize=24, y=1.01)\naxes = axes.ravel()\nfor num, feat in enumerate(FEATURES[counts:counts+(rows*cols)]):\n    ax = sns.kdeplot(x=feat, data=train_csv, shade=True, ax=axes[num])\n    ax.set_title(feat.upper())\n    ax.tick_params(axis=\"x\", labelrotation=90)\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()\ncounts += rows*cols","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:44:17.485154Z","iopub.execute_input":"2022-02-04T07:44:17.486331Z","iopub.status.idle":"2022-02-04T07:45:10.095391Z","shell.execute_reply.started":"2022-02-04T07:44:17.486265Z","shell.execute_reply":"2022-02-04T07:45:10.094638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = 10\ncols = 5\nfig, axes = plt.subplots(rows, cols, figsize=(20, 20))\nfig.suptitle(f\"Numerical continious features distribution: {counts+1} - {counts+(rows*cols)}\", fontsize=24, y=1.01)\naxes = axes.ravel()\nfor num, feat in enumerate(FEATURES[counts:counts+(rows*cols)]):\n    ax = sns.kdeplot(x=feat, data=train_csv, shade=True, ax=axes[num])\n    ax.set_title(feat.upper())\n    ax.tick_params(axis=\"x\", labelrotation=90)\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()\ncounts += rows*cols","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:45:10.09642Z","iopub.execute_input":"2022-02-04T07:45:10.096606Z","iopub.status.idle":"2022-02-04T07:46:43.712102Z","shell.execute_reply.started":"2022-02-04T07:45:10.096582Z","shell.execute_reply":"2022-02-04T07:46:43.711478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = 10\ncols = 5\nfig, axes = plt.subplots(rows, cols, figsize=(20, 20))\nfig.suptitle(f\"Numerical continious features distribution: {counts+1} - {counts+(rows*cols)}\", fontsize=24, y=1.01)\naxes = axes.ravel()\nfor num, feat in enumerate(FEATURES[counts:counts+(rows*cols)]):\n    ax = sns.kdeplot(x=feat, data=train_csv, shade=True, ax=axes[num])\n    ax.set_title(feat.upper())\n    ax.tick_params(axis=\"x\", labelrotation=90)\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()\ncounts += rows*cols","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:46:43.713422Z","iopub.execute_input":"2022-02-04T07:46:43.713889Z","iopub.status.idle":"2022-02-04T07:48:19.690406Z","shell.execute_reply.started":"2022-02-04T07:46:43.713861Z","shell.execute_reply":"2022-02-04T07:48:19.687044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = 10\ncols = 5\nfig, axes = plt.subplots(rows, cols, figsize=(20, 20))\nfig.suptitle(f\"Numerical continious features distribution: {counts+1} - {counts+(rows*cols)}\", fontsize=24, y=1.01)\naxes = axes.ravel()\nfor num, feat in enumerate(FEATURES[counts:counts+(rows*cols)]):\n    ax = sns.kdeplot(x=feat, data=train_csv, shade=True, ax=axes[num])\n    ax.set_title(feat.upper())\n    ax.tick_params(axis=\"x\", labelrotation=90)\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()\ncounts += rows*cols","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:48:19.692561Z","iopub.execute_input":"2022-02-04T07:48:19.693038Z","iopub.status.idle":"2022-02-04T07:49:56.678091Z","shell.execute_reply.started":"2022-02-04T07:48:19.692984Z","shell.execute_reply":"2022-02-04T07:49:56.677238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = 8\ncols = 5\nfig, axes = plt.subplots(rows, cols, figsize=(20, 20))\nfig.suptitle(f\"Numerical continious features distribution: {counts+1} - {counts+(rows*cols)}\", fontsize=24, y=1.01)\naxes = axes.ravel()\nfor num, feat in enumerate(FEATURES[counts:counts+(rows*cols)]):\n    ax = sns.kdeplot(x=feat, data=train_csv, shade=True, ax=axes[num])\n    ax.set_title(feat.upper())\n    ax.tick_params(axis=\"x\", labelrotation=90)\n    ax.set_xlabel(None)\n    ax.set_ylabel(None)\nfig.tight_layout()\ncounts += rows*cols\nfig.delaxes(axes[num+1])\nfig.delaxes(axes[num+2])\nfig.delaxes(axes[num+3])\nfig.delaxes(axes[num+4])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:49:56.679695Z","iopub.execute_input":"2022-02-04T07:49:56.679941Z","iopub.status.idle":"2022-02-04T07:51:09.915121Z","shell.execute_reply.started":"2022-02-04T07:49:56.679909Z","shell.execute_reply":"2022-02-04T07:51:09.914075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All these features distribution seems to be right skewed, so we have to transform these features and try to fix these, \n\nsome transforms for fixing right skewed distribution\n- log transform\n- boxcox transform\n- cuberoot transform\n- inverse transform\n\nLets also check skewness using pandas `skew` method for all features","metadata":{}},{"cell_type":"markdown","source":"### Features skewness and transformation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nxcol = \"Features\"\nycol = \"Skewness\"\ndf = pd.DataFrame(columns=[xcol, ycol])\ndf[xcol] = FEATURES\nskewness = []\nfor feat in FEATURES:\n    skewness.append(train_csv[feat].skew())\ndf[ycol] = skewness\nax = sns.barplot(x=xcol, y=ycol, data=df)\nax.tick_params(axis=\"x\", labelbottom=False)\n_ = ax.set_title(\"Feature Skewness Plot\", fontsize=24, y=1.01)\nplt.show()\n\nprint(\"============ Feature Skewness ==============\")\nprint(\"\\nFEATURES\\tSKEWNESS\\n\")\nfor feat, skew in zip(FEATURES, skewness):\n    print(f\"{feat}\\t{skew}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:51:09.916333Z","iopub.execute_input":"2022-02-04T07:51:09.917426Z","iopub.status.idle":"2022-02-04T07:51:12.646012Z","shell.execute_reply.started":"2022-02-04T07:51:09.917349Z","shell.execute_reply":"2022-02-04T07:51:12.644533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the features are right skewed (positive skewed), except two `A2T3G2C3` and `A2T3G3C2` also most of these features are highly right skewed, we will focus in transforming highly skewed features whose skewness is greater than 1.","metadata":{}},{"cell_type":"code","source":"skewed_features = [feat for feat, skew in zip(FEATURES, skewness) if skew>1]\nprint(\"Number of right skewed features: \", len(skewed_features))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T07:51:12.64907Z","iopub.execute_input":"2022-02-04T07:51:12.649386Z","iopub.status.idle":"2022-02-04T07:51:12.658174Z","shell.execute_reply.started":"2022-02-04T07:51:12.649352Z","shell.execute_reply":"2022-02-04T07:51:12.657528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"log and boxcox transforms needs values to be positive, also log accepts value greater than 0 keeping that in mind before apply these transforms. These two transformations are mostly use to fix right skewed distributions","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nxcol = \"Features\"\nycol = \"Skewness\"\ndf = pd.DataFrame(columns=[xcol, ycol])\ndf[xcol] = skewed_features\nskewness = []\nfor feat in skewed_features:\n    skewness.append(pd.Series(stats.boxcox(train_csv[feat] + 1)[0]).skew())\ndf[ycol] = skewness\nax = sns.barplot(x=xcol, y=ycol, data=df)\nax.tick_params(axis=\"x\", labelbottom=False)\n_ = ax.set_title(\"Feature Skewness Plot after boxcox transform\", fontsize=24, y=1.01)\nplt.show()\n\nprint(\"\\nFEATURES\\tSKEWNESS\\n\")\nfor feat, skew in zip(FEATURES, skewness):\n    print(f\"{feat}\\t{skew}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:51:12.659658Z","iopub.execute_input":"2022-02-04T07:51:12.659908Z","iopub.status.idle":"2022-02-04T07:51:47.227771Z","shell.execute_reply.started":"2022-02-04T07:51:12.659878Z","shell.execute_reply":"2022-02-04T07:51:47.220358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 10))\nxcol = \"Features\"\nycol = \"Skewness\"\ndf = pd.DataFrame(columns=[xcol, ycol])\ndf[xcol] = skewed_features\nskewness = []\nfor feat in skewed_features:\n    skewness.append(pd.Series(np.log10(train_csv[feat] + 1)).skew())\ndf[ycol] = skewness\nax = sns.barplot(x=xcol, y=ycol, data=df)\nax.tick_params(axis=\"x\", labelbottom=False)\n_ = ax.set_title(\"Feature Skewness Plot after Log transform\", fontsize=24, y=1.01)\nplt.show()\n\nprint(\"\\nFEATURES\\tSKEWNESS\\n\")\nfor feat, skew in zip(FEATURES, skewness):\n    print(f\"{feat}\\t{skew}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:51:47.240434Z","iopub.execute_input":"2022-02-04T07:51:47.240686Z","iopub.status.idle":"2022-02-04T07:51:51.692394Z","shell.execute_reply.started":"2022-02-04T07:51:47.240636Z","shell.execute_reply":"2022-02-04T07:51:51.689888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"boxcox transform fix our issue and reduce right skewness with good margin, log transform do not show any effect here. Why? I will need investigate this currently I do not know why it do not work at all here, if someone knows comment below so we all can know about it.","metadata":{}},{"cell_type":"markdown","source":"### Target Distribution","metadata":{}},{"cell_type":"code","source":"print(\"Number of Target Classes: \", train_csv[TARGET].nunique())\nprint(\"\\nTarget Classes:\")\nfor i, unique in enumerate(train_csv[TARGET].unique()):\n    print(f\"{i+1}- {unique}\")","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:51:51.694893Z","iopub.execute_input":"2022-02-04T07:51:51.69524Z","iopub.status.idle":"2022-02-04T07:51:51.756935Z","shell.execute_reply.started":"2022-02-04T07:51:51.695201Z","shell.execute_reply":"2022-02-04T07:51:51.755446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets plot a count plot of targets and check data imbalance","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nax = sns.countplot(x=TARGET, data=train_csv)\n_ = ax.set_title(\"Target Distribution Counts\", fontsize=24, y=1.01)\nax.set_xlabel(None)\nax.set_ylabel(None)\nax.tick_params(axis=\"x\", labelrotation=45)\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T07:51:51.758686Z","iopub.execute_input":"2022-02-04T07:51:51.759282Z","iopub.status.idle":"2022-02-04T07:51:52.282788Z","shell.execute_reply.started":"2022-02-04T07:51:51.75924Z","shell.execute_reply":"2022-02-04T07:51:52.281499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cool! data is almost balanced, thats why `accuracy` was choosen for competition metrics, if targets were unbalanced then metrics like `auc` would be the choice.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\ncounts = train_csv[TARGET].value_counts()\nper = counts.values / len(train_csv) * 100\nCLASSES = counts.index\nxcol = \"Classes\"\nycol = \"Percentage %\"\ndf = pd.DataFrame(columns=[xcol, ycol])\ndf[xcol] = CLASSES\ndf[ycol] = per\nax = sns.barplot(x=xcol, y=ycol, data=df)\n_ = ax.set_title(\"Target Distribution Percentage\", fontsize=24, y=1.01)\nax.set_xlabel(None)\nax.set_ylabel(None)\nax.tick_params(axis=\"x\", labelrotation=45)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T07:51:52.284888Z","iopub.execute_input":"2022-02-04T07:51:52.285157Z","iopub.status.idle":"2022-02-04T07:51:52.59016Z","shell.execute_reply.started":"2022-02-04T07:51:52.285124Z","shell.execute_reply":"2022-02-04T07:51:52.589066Z"},"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Encoding Target Variables\n\nThere are various ways to encode target variables, as they are string we cannot directly use it to train model, some common encoding includes:\n- LabelEncoder\n- OneHotEncoder\n- BinaryEncoder\n- Mean Encoding\netc.\n\nHere we will use simple label encoding which will assign integer to target categories.","metadata":{}},{"cell_type":"code","source":"encoder = prep.LabelEncoder()\ntrain_csv[TARGET] = encoder.fit_transform(train_csv[TARGET])\nCLASSES = encoder.classes_","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:59:48.265557Z","iopub.execute_input":"2022-02-04T12:59:48.266451Z","iopub.status.idle":"2022-02-04T12:59:48.337228Z","shell.execute_reply.started":"2022-02-04T12:59:48.26641Z","shell.execute_reply":"2022-02-04T12:59:48.336356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"transforming training and test data and saving it, we will run crossvalidation using data with and without transformation later.","metadata":{}},{"cell_type":"code","source":"train_transformed = train_csv.copy(deep=True)\ntest_transformed = test_csv.copy(deep=True)\nfor feat in FEATURES:\n    train_transformed[feat] = stats.boxcox(train_csv[feat] + 1)[0]\n    test_transformed[feat] = stats.boxcox(test_csv[feat] + 1)[0]\ntrain_transformed.to_csv(\"train_transformed.csv\", index=False)\ntest_transformed.to_csv(\"test_transformed.csv\", index=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T12:59:51.174768Z","iopub.execute_input":"2022-02-04T12:59:51.175061Z","iopub.status.idle":"2022-02-04T13:04:16.374197Z","shell.execute_reply.started":"2022-02-04T12:59:51.175027Z","shell.execute_reply":"2022-02-04T13:04:16.373245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data correlation","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\ncorr = train_csv.corr()\nax = sns.heatmap(corr, mask=np.tril(corr))\n_ = ax.set_title(\"Train Correlation matrix\", fontsize=24, y=1.05)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T13:33:19.890588Z","iopub.execute_input":"2022-02-04T13:33:19.890885Z","iopub.status.idle":"2022-02-04T13:34:06.581491Z","shell.execute_reply.started":"2022-02-04T13:33:19.890854Z","shell.execute_reply":"2022-02-04T13:34:06.58063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CrossValidation","metadata":{}},{"cell_type":"markdown","source":"We will use ExtraTree classifier and run crossvalidation using non-transformed data and transformed data. We want to find out that the transformations we performed helped in improving crossvalidation score.","metadata":{}},{"cell_type":"code","source":"seed = 42\nfolds = 5","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T13:04:16.375876Z","iopub.execute_input":"2022-02-04T13:04:16.376112Z","iopub.status.idle":"2022-02-04T13:04:16.382472Z","shell.execute_reply.started":"2022-02-04T13:04:16.376084Z","shell.execute_reply":"2022-02-04T13:04:16.381421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CrossValidation:\n    def __init__(self, df, shuffle,random_state=None):\n        self.df = df\n        self.random_state = random_state\n        self.shuffle = shuffle\n        if shuffle is True:\n            self.df = df.sample(frac=1,\n                random_state=self.random_state).reset_index(drop=True)\n\n    def hold_out_split(self,percent,stratify=None):\n        if stratify is not None:\n            y = self.df[stratify]\n            train,val = ms.train_test_split(self.df, test_size=percent/100,\n                stratify=y, random_state=self.random_state)\n            return train,val\n        size = len(self.df) - int(len(self.df)*(percent/100))\n        train = self.df.iloc[:size,:]\n        val = self.df.iloc[size:,:]\n        return train,val\n\n    def kfold_split(self, splits, stratify=None):\n        if stratify is not None:\n            kf = ms.StratifiedKFold(n_splits=splits,\n                shuffle=self.shuffle,\n                random_state=self.random_state)\n            y = self.df[stratify]\n            for train, val in kf.split(X=self.df,y=y):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v\n        else:\n            kf = ms.KFold(n_splits=splits, shuffle=self.shuffle,\n                random_state=self.random_state)\n            for train, val in kf.split(X=self.df):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T13:04:16.383628Z","iopub.execute_input":"2022-02-04T13:04:16.383886Z","iopub.status.idle":"2022-02-04T13:04:16.399487Z","shell.execute_reply.started":"2022-02-04T13:04:16.383857Z","shell.execute_reply":"2022-02-04T13:04:16.398678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using non-transformed Data","metadata":{}},{"cell_type":"code","source":"cv = CrossValidation(train_csv, shuffle=True, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:08:18.923994Z","iopub.execute_input":"2022-02-04T11:08:18.924296Z","iopub.status.idle":"2022-02-04T11:08:20.994141Z","shell.execute_reply.started":"2022-02-04T11:08:18.924264Z","shell.execute_reply":"2022-02-04T11:08:20.993265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfold_accuracy = []\nfor fold, (train_, val_) in enumerate(cv.kfold_split(splits=folds, stratify=TARGET)):\n    print(\"CV fold\", fold+1)\n    model = esm.ExtraTreesClassifier(n_jobs=-1, random_state=seed)\n    model.fit(train_[FEATURES], train_[TARGET])\n    preds = model.predict(val_[FEATURES])\n    acc_score = metrics.accuracy_score(val_[TARGET], preds)\n    print(\"FOLD ACCURACY: \", acc_score)\n    print(metrics.classification_report(val_[TARGET], preds))\n    fold_accuracy.append(acc_score)\nprint(\"CV SCORE: \", np.mean(fold_accuracy))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-04T08:53:57.501456Z","iopub.execute_input":"2022-02-04T08:53:57.501971Z","iopub.status.idle":"2022-02-04T08:56:09.763955Z","shell.execute_reply.started":"2022-02-04T08:53:57.501927Z","shell.execute_reply":"2022-02-04T08:56:09.763001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using transformed data","metadata":{}},{"cell_type":"code","source":"cv = CrossValidation(train_transformed, shuffle=True, random_state=seed)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T13:04:16.40105Z","iopub.execute_input":"2022-02-04T13:04:16.402344Z","iopub.status.idle":"2022-02-04T13:04:17.071121Z","shell.execute_reply.started":"2022-02-04T13:04:16.402305Z","shell.execute_reply":"2022-02-04T13:04:17.070415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfold_accuracy = []\nfor fold, (train_, val_) in enumerate(cv.kfold_split(splits=folds, stratify=TARGET)):\n    print(\"CV fold\", fold+1)\n    model = esm.ExtraTreesClassifier(n_jobs=-1, random_state=seed)\n    model.fit(train_[FEATURES], train_[TARGET])\n    preds = model.predict(val_[FEATURES])\n    acc_score = metrics.accuracy_score(val_[TARGET], preds)\n    print(\"FOLD ACCURACY: \", acc_score)\n    print(metrics.classification_report(val_[TARGET], preds))\n    fold_accuracy.append(acc_score)\nprint(\"CV SCORE: \", np.mean(fold_accuracy))","metadata":{"execution":{"iopub.status.busy":"2022-02-04T08:56:23.535816Z","iopub.execute_input":"2022-02-04T08:56:23.536194Z","iopub.status.idle":"2022-02-04T08:58:40.25102Z","shell.execute_reply.started":"2022-02-04T08:56:23.536152Z","shell.execute_reply":"2022-02-04T08:58:40.250333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see little improvement with feature transformation,\n\n- CV before feature transformation: `0.9940849999999999`\n- CV after feature transformation: `0.9942400000000001`","metadata":{}},{"cell_type":"markdown","source":"# Parameter Optimization\n\nWe will be using Bayesian Search to search for best hyperparameters for model, `skopt` package provide `BayesSearchCV` that implements bayesian search for hyperparameter optimization, we will use just a subset of train data for faster parameters searches.","metadata":{}},{"cell_type":"code","source":"%%time\next_tree = esm.ExtraTreesClassifier(n_jobs=-1)\n\nrandom_params = {\"n_estimators\": [50, 300],\n                 \"min_samples_leaf\": [1, 10],\n                 \"min_samples_split\": [2, 10]\n                }\n\nopt = BayesSearchCV(\n    ext_tree,\n    random_params,\n    n_iter=50,\n    cv=3,\n    n_jobs=-1\n)\n\ndata = train_transformed.sample(1000)\nopt.fit(data[FEATURES], data[TARGET])\nprint(\"Best Params : \", dict(opt.best_params_))\nprint(\"Best Score : \", opt.best_score_)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T11:23:22.358172Z","iopub.execute_input":"2022-02-04T11:23:22.358531Z","iopub.status.idle":"2022-02-04T11:25:11.201061Z","shell.execute_reply.started":"2022-02-04T11:23:22.358489Z","shell.execute_reply":"2022-02-04T11:25:11.199495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_PARAMS = dict(opt.best_params_)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T12:01:55.593243Z","iopub.execute_input":"2022-02-04T12:01:55.593747Z","iopub.status.idle":"2022-02-04T12:01:55.59932Z","shell.execute_reply.started":"2022-02-04T12:01:55.593707Z","shell.execute_reply":"2022-02-04T12:01:55.597971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Predicting Test Data","metadata":{}},{"cell_type":"code","source":"def model_train_and_predict(cv, model_params, test_csv, seed):\n    valid_preds = {}\n    test_preds = []\n    val_fold_acc = []\n    for fold, (train_, val_) in enumerate(cv.kfold_split(splits=folds, stratify=TARGET)):\n        print(\"Training fold: \", fold+1)\n        model = esm.ExtraTreesClassifier(**model_params, n_jobs=-1, verbose=0, random_state=seed)\n        trainX = train_[FEATURES]\n        trainY = train_[TARGET]\n        valX = val_[FEATURES]\n        valY = val_[TARGET]\n\n        val_ids = val_.row_id.values.tolist()\n\n        model.fit(trainX, trainY)\n\n        predY = model.predict(valX)\n        val_acc = metrics.accuracy_score(valY, predY)\n        print(f\"Fold {fold+1} accuracy\", val_acc)\n        val_fold_acc.append(val_acc)\n\n        valid_preds.update(dict(zip(val_ids, predY)))\n\n        predY = model.predict_proba(test_csv[FEATURES])\n        test_preds.append(predY)\n    return val_fold_acc, valid_preds, test_preds","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T13:18:11.645104Z","iopub.execute_input":"2022-02-04T13:18:11.6458Z","iopub.status.idle":"2022-02-04T13:18:11.656236Z","shell.execute_reply.started":"2022-02-04T13:18:11.645754Z","shell.execute_reply":"2022-02-04T13:18:11.655092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def max_voting(predictions):\n    idxs = np.argmax(predictions, axis=1)\n    return np.take_along_axis(predictions, idxs[:, None], axis=1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T13:18:13.385245Z","iopub.execute_input":"2022-02-04T13:18:13.388203Z","iopub.status.idle":"2022-02-04T13:18:13.394741Z","shell.execute_reply.started":"2022-02-04T13:18:13.38812Z","shell.execute_reply":"2022-02-04T13:18:13.393888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_count = 1\ntotal_val_fold_accuracy = []\ntest_predictions = []\n#Rebalancing the classes with respect to training set, credit: https://www.kaggle.com/ambrosm/tpsfeb22-02-postprocessing-against-the-mutants\nweights = np.array([0, 0, 0.03, 0.036, 0, 0, 0, 0, 0, 0])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T13:27:22.940457Z","iopub.execute_input":"2022-02-04T13:27:22.940742Z","iopub.status.idle":"2022-02-04T13:27:22.945981Z","shell.execute_reply.started":"2022-02-04T13:27:22.940713Z","shell.execute_reply":"2022-02-04T13:27:22.945179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nval_fold_acc, valid_preds, test_preds = model_train_and_predict(cv, MODEL_PARAMS, test_transformed, seed=42)\n\nfold_acc = np.mean(val_fold_acc)\nprint(\"Fold Accuracy: \", fold_acc)\n\ntotal_val_fold_accuracy.append(fold_acc)\n\ntest_df = pd.DataFrame(columns=[\"row_id\", f\"pred_{model_count}\"]) \ntest_df[\"row_id\"] = test_csv[\"row_id\"]\n\ntest_preds = sum(test_preds)/len(test_preds)\ntest_preds += weights\ntest_preds = np.argmax(test_preds, axis=1)\ntest_df[f\"pred_{model_count}\"] = test_preds\ntest_df.to_csv(f\"test_pred_{model_count}.csv\", index=False)\ntest_predictions.append(test_preds)\n\nmodel_count += 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T13:27:24.059692Z","iopub.execute_input":"2022-02-04T13:27:24.059999Z","iopub.status.idle":"2022-02-04T13:27:31.819193Z","shell.execute_reply.started":"2022-02-04T13:27:24.059956Z","shell.execute_reply":"2022-02-04T13:27:31.818557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nval_fold_acc, valid_preds, test_preds = model_train_and_predict(cv, MODEL_PARAMS, test_transformed, seed=111)\n\nfold_acc = np.mean(val_fold_acc)\nprint(\"Fold Accuracy: \", fold_acc)\n\ntotal_val_fold_accuracy.append(fold_acc)\n\ntest_df = pd.DataFrame(columns=[\"row_id\", f\"pred_{model_count}\"]) \ntest_df[\"row_id\"] = test_csv[\"row_id\"]\n\ntest_preds = sum(test_preds)/len(test_preds)\ntest_preds += weights\ntest_preds = np.argmax(test_preds, axis=1)\ntest_df[f\"pred_{model_count}\"] = test_preds\ntest_df.to_csv(f\"test_pred_{model_count}.csv\", index=False)\ntest_predictions.append(test_preds)\n\nmodel_count += 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T13:27:45.908691Z","iopub.execute_input":"2022-02-04T13:27:45.909584Z","iopub.status.idle":"2022-02-04T13:27:53.494704Z","shell.execute_reply.started":"2022-02-04T13:27:45.909539Z","shell.execute_reply":"2022-02-04T13:27:53.493746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nval_fold_acc, valid_preds, test_preds = model_train_and_predict(cv, MODEL_PARAMS, test_transformed, seed=555)\n\nfold_acc = np.mean(val_fold_acc)\nprint(\"Fold Accuracy: \", fold_acc)\n\ntotal_val_fold_accuracy.append(fold_acc)\n\ntest_df = pd.DataFrame(columns=[\"row_id\", f\"pred_{model_count}\"]) \ntest_df[\"row_id\"] = test_csv[\"row_id\"]\n\ntest_preds = sum(test_preds)/len(test_preds)\ntest_preds += weights\ntest_preds = np.argmax(test_preds, axis=1)\ntest_df[f\"pred_{model_count}\"] = test_preds\ntest_df.to_csv(f\"test_pred_{model_count}.csv\", index=False)\ntest_predictions.append(test_preds)\n\nmodel_count += 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T13:27:58.624811Z","iopub.execute_input":"2022-02-04T13:27:58.625087Z","iopub.status.idle":"2022-02-04T13:28:06.193323Z","shell.execute_reply.started":"2022-02-04T13:27:58.625058Z","shell.execute_reply":"2022-02-04T13:28:06.192418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nval_fold_acc, valid_preds, test_preds = model_train_and_predict(cv, MODEL_PARAMS, test_transformed, seed=777)\n\nfold_acc = np.mean(val_fold_acc)\nprint(\"Fold Accuracy: \", fold_acc)\n\ntotal_val_fold_accuracy.append(fold_acc)\n\ntest_df = pd.DataFrame(columns=[\"row_id\", f\"pred_{model_count}\"]) \ntest_df[\"row_id\"] = test_csv[\"row_id\"]\n\ntest_preds = sum(test_preds)/len(test_preds)\ntest_preds += weights\ntest_preds = np.argmax(test_preds, axis=1)\ntest_df[f\"pred_{model_count}\"] = test_preds\ntest_df.to_csv(f\"test_pred_{model_count}.csv\", index=False)\ntest_predictions.append(test_preds)\n\nmodel_count += 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T13:28:11.866116Z","iopub.execute_input":"2022-02-04T13:28:11.866442Z","iopub.status.idle":"2022-02-04T13:28:19.426502Z","shell.execute_reply.started":"2022-02-04T13:28:11.866401Z","shell.execute_reply":"2022-02-04T13:28:19.425562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nval_fold_acc, valid_preds, test_preds = model_train_and_predict(cv, MODEL_PARAMS, test_transformed, seed=999)\n\nfold_acc = np.mean(val_fold_acc)\nprint(\"Fold Accuracy: \", fold_acc)\n\ntotal_val_fold_accuracy.append(fold_acc)\n\ntest_df = pd.DataFrame(columns=[\"row_id\", f\"pred_{model_count}\"]) \ntest_df[\"row_id\"] = test_csv[\"row_id\"]\n\ntest_preds = sum(test_preds)/len(test_preds)\ntest_preds += weights\ntest_preds = np.argmax(test_preds, axis=1)\ntest_df[f\"pred_{model_count}\"] = test_preds\ntest_df.to_csv(f\"test_pred_{model_count}.csv\", index=False)\ntest_predictions.append(test_preds)\n\nmodel_count += 1","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T13:28:28.675695Z","iopub.execute_input":"2022-02-04T13:28:28.676001Z","iopub.status.idle":"2022-02-04T13:28:36.224233Z","shell.execute_reply.started":"2022-02-04T13:28:28.675967Z","shell.execute_reply":"2022-02-04T13:28:36.223539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating Submission","metadata":{}},{"cell_type":"code","source":"def create_submission(sub_name,\n                      predictions, \n                      encoder,\n                      template_path=\"../input/tabular-playground-series-feb-2022/sample_submission.csv\"):\n    template = pd.read_csv(template_path)\n    template[TARGET] = encoder.inverse_transform(predictions)\n    template.to_csv(sub_name+\".csv\", index=False)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-04T13:28:40.855626Z","iopub.execute_input":"2022-02-04T13:28:40.856675Z","iopub.status.idle":"2022-02-04T13:28:40.863567Z","shell.execute_reply.started":"2022-02-04T13:28:40.856614Z","shell.execute_reply":"2022-02-04T13:28:40.862551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = max_voting(np.column_stack(test_predictions))\npredictions.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-04T13:28:43.155092Z","iopub.execute_input":"2022-02-04T13:28:43.155596Z","iopub.status.idle":"2022-02-04T13:28:43.165426Z","shell.execute_reply.started":"2022-02-04T13:28:43.155544Z","shell.execute_reply":"2022-02-04T13:28:43.164758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"create_submission(\"submission\", predictions, encoder=encoder)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T13:28:46.325095Z","iopub.execute_input":"2022-02-04T13:28:46.325603Z","iopub.status.idle":"2022-02-04T13:28:46.667148Z","shell.execute_reply.started":"2022-02-04T13:28:46.325551Z","shell.execute_reply":"2022-02-04T13:28:46.666247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.read_csv(\"submission.csv\").head()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T13:28:48.194808Z","iopub.execute_input":"2022-02-04T13:28:48.19566Z","iopub.status.idle":"2022-02-04T13:28:48.249934Z","shell.execute_reply.started":"2022-02-04T13:28:48.195609Z","shell.execute_reply":"2022-02-04T13:28:48.248977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I might miss some important things, if that the case then let us know, it will help all the readers of this notebook. Thanks!\n\n","metadata":{}}]}