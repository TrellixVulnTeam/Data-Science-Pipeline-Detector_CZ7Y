{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nThis month, the problem proposed by the TPS is fresh and new.\n\nI will try to start an EDA and propose a baseline :-)\n\n*Version history and highlights*\n\nvers 1-4: first attempt at understanding data\n\nvers 5: understanding how the histograms have been configured\n\nvers 6: plotting variable importance of ERT\n\nvers 7: adding variability to distribution of histogram plots\n\nvers 8-9: adding case-wise exploration and feature engineering\n\nvers 10-13: adversarial validation\n\nvers 14: adding decamwers from Ambrosm EDA\n\nvers 15: adding cv validation, voting ensembling and model saving\n\nvers 16-20: experiments with features, number of folds and estimators","metadata":{}},{"cell_type":"code","source":"# IntelÂ® Extension for Scikit-learn installation:\n!pip install scikit-learn-intelex","metadata":{"execution":{"iopub.status.busy":"2022-02-03T07:28:46.886495Z","iopub.execute_input":"2022-02-03T07:28:46.886875Z","iopub.status.idle":"2022-02-03T07:29:24.304573Z","shell.execute_reply.started":"2022-02-03T07:28:46.886782Z","shell.execute_reply":"2022-02-03T07:29:24.303522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom math import factorial\nimport gc\nimport joblib\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_validate","metadata":{"execution":{"iopub.status.busy":"2022-02-03T07:29:24.306876Z","iopub.execute_input":"2022-02-03T07:29:24.307145Z","iopub.status.idle":"2022-02-03T07:29:25.911088Z","shell.execute_reply.started":"2022-02-03T07:29:24.307113Z","shell.execute_reply":"2022-02-03T07:29:25.910372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's first upload the data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/tabular-playground-series-feb-2022/train.csv\")\ntest = pd.read_csv(\"../input/tabular-playground-series-feb-2022/test.csv\")\nsub = pd.read_csv(\"../input/tabular-playground-series-feb-2022/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T07:29:25.912542Z","iopub.execute_input":"2022-02-03T07:29:25.913843Z","iopub.status.idle":"2022-02-03T07:30:05.374306Z","shell.execute_reply.started":"2022-02-03T07:29:25.913757Z","shell.execute_reply":"2022-02-03T07:30:05.373142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train shape\", train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T07:30:05.376163Z","iopub.execute_input":"2022-02-03T07:30:05.376429Z","iopub.status.idle":"2022-02-03T07:30:05.382364Z","shell.execute_reply.started":"2022-02-03T07:30:05.376399Z","shell.execute_reply":"2022-02-03T07:30:05.381364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Target exploration","metadata":{}},{"cell_type":"markdown","source":"Target analysis is not particularly revealing. Since it is an experiment we have a fairly balanced distribution of targets. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\n\ntrain.target.value_counts().plot.bar()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T07:30:05.386565Z","iopub.execute_input":"2022-02-03T07:30:05.386828Z","iopub.status.idle":"2022-02-03T07:30:05.724921Z","shell.execute_reply.started":"2022-02-03T07:30:05.386795Z","shell.execute_reply":"2022-02-03T07:30:05.724036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature exploration","metadata":{}},{"cell_type":"markdown","source":"Let's turn our attention to features.","metadata":{}},{"cell_type":"markdown","source":"Excluding target and the row index, there are 286 histograms. Let's explore better how they are distributed. Each 10-mer has exactly 10 bases (A, T, C, G). Position does not matter for the histogram count (thank you @grayjay for noticing this from the paper).","metadata":{}},{"cell_type":"markdown","source":"We first prepare a way to convert the columns into a ATGC distribution.","metadata":{}},{"cell_type":"code","source":"features = train.columns[1:-1].tolist()\n\nimport re\n\ndef split_feature(st):\n    counts = list(map(int, re.split('A|T|G|C', st)[1:]))\n    return counts\n\nfeat2counts = {c: split_feature(c) for c in features}","metadata":{"execution":{"iopub.status.busy":"2022-02-03T07:30:05.726174Z","iopub.execute_input":"2022-02-03T07:30:05.726508Z","iopub.status.idle":"2022-02-03T07:30:05.733013Z","shell.execute_reply.started":"2022-02-03T07:30:05.726472Z","shell.execute_reply":"2022-02-03T07:30:05.732147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sums = list()\nfor feat in features:\n    xa, xt, xg, xc = feat2counts[feat]\n    s = xa+xt+xg+xc\n    sums.append(s)\n    \nprint(f\"All the histograms sums the bases to: {np.unique(sums)}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:06:39.331174Z","iopub.execute_input":"2022-02-02T15:06:39.331413Z","iopub.status.idle":"2022-02-02T15:06:39.345474Z","shell.execute_reply.started":"2022-02-02T15:06:39.331384Z","shell.execute_reply":"2022-02-02T15:06:39.344413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = [0 for i in range(11)]\nt = [0 for i in range(11)]\ng = [0 for i in range(11)]\nc = [0 for i in range(11)]\n\nfor feat in features:\n    xa, xt, xg, xc = feat2counts[feat]\n    a[xa] += 1\n    t[xt] += 1\n    g[xt] += 1\n    c[xc] += 1\n    \nplt.figure(figsize=(20, 3))\n\nplt.subplot(2, 2, 1,)\nplt.title(\"A\")\nplt.bar(range(11), height=a)\n\nplt.subplot(2, 2, 2,)\nplt.title(\"T\")\nplt.bar(range(11), height=t)\n\nplt.subplot(2, 2, 3,)\nplt.title(\"G\")\nplt.bar(range(11), height=g)\n\nplt.subplot(2, 2, 4,)\nplt.title(\"C\")\nplt.bar(range(11), height=c)\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.7, \n                    top=1.3, \n                    wspace=0.3, \n                    hspace=0.5)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:06:39.347486Z","iopub.execute_input":"2022-02-02T15:06:39.348448Z","iopub.status.idle":"2022-02-02T15:06:39.872022Z","shell.execute_reply.started":"2022-02-02T15:06:39.348384Z","shell.execute_reply":"2022-02-02T15:06:39.870802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\n\nplt.plot(np.sqrt(a))\nplt.title(\"Count decrease\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:06:39.873357Z","iopub.execute_input":"2022-02-02T15:06:39.873586Z","iopub.status.idle":"2022-02-02T15:06:40.060297Z","shell.execute_reply.started":"2022-02-02T15:06:39.873558Z","shell.execute_reply":"2022-02-02T15:06:40.059098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Given the constrain to only have 10 bases, the distribution is equal for all the letters and unbalanced toward lower counts: lower counts are more frequent than higher ones and the decrease fits a squared root transformation.","metadata":{}},{"cell_type":"markdown","source":"This opens up to two further questions (that I will try to answer):\n\n* is there a way to represent the space of histograms better than the flat form proposed by the competition that could be exploited by deep learning? (as a sequence or as a graph maybe)\n\n* do we need all the measured histograms? Can't we do with much less?","metadata":{}},{"cell_type":"markdown","source":"We can now try to visualize each target what distribution has on the original features. The plot may be not very informative because of the many bars plotted, but similar shapes can hit at possible difficulties since we expect each target to have a distinct shape, partially masked by measurement noise.","metadata":{}},{"cell_type":"code","source":"freqs = train.set_index('row_id').groupby('target').mean()\nvaria = train.set_index('row_id').groupby('target').std()\n\nfor target in train.target.unique():\n    \n    fig = plt.figure(figsize=(14, 6))\n    plt.margins(0.015, tight=True)\n    \n    ax = plt.subplot(1, 2, 1)\n    freqs[freqs.index==target].plot.bar(ax=ax, width=1.0)\n    _lg = ax.get_legend()\n    _lg.remove()\n\n    plt.xticks(rotation = 0)\n\n    plt.title(\"Distribution of measurements\")\n    \n    ax = plt.subplot(1, 2, 2)\n    varia[varia.index==target].plot.bar(ax=ax, width=1.0)\n    _lg = ax.get_legend()\n    _lg.remove()\n\n    plt.xticks(rotation = 0)\n\n    plt.title(\"Variability of measurements\")\n\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:06:40.063196Z","iopub.execute_input":"2022-02-02T15:06:40.063546Z","iopub.status.idle":"2022-02-02T15:07:01.517095Z","shell.execute_reply.started":"2022-02-02T15:06:40.063498Z","shell.execute_reply":"2022-02-02T15:07:01.516142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Each bacteria, seem to have a specific mean profile, but the variability is also quite high.","metadata":{}},{"cell_type":"code","source":"def plot_a_sample(case):\n    fig = plt.figure(figsize=(14, 6))\n    bacterium = train.set_index('row_id').iloc[case, -1]\n    x = train.set_index('row_id').iloc[case, :-1]\n    x.plot.bar()\n    plt.xticks(np.arange(0, len(features), 24), \"\")\n    plt.title(f\"row {case} | bacterium {bacterium}\")\n    plt.show()\n    \nplot_a_sample(case=0)\nplot_a_sample(case=6)\nplot_a_sample(case=9)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:07:01.518371Z","iopub.execute_input":"2022-02-02T15:07:01.518609Z","iopub.status.idle":"2022-02-02T15:07:05.916813Z","shell.execute_reply.started":"2022-02-02T15:07:01.518577Z","shell.execute_reply":"2022-02-02T15:07:05.915695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The variability previosly highlighted depends on the fact that samples from the same bacterium are actullay quite different. Here we plotted three samples of Streptococcus Pyogenes and each one has spikes in different locations. Also the range of values changes and the way the values distributes around the spikes.","metadata":{}},{"cell_type":"markdown","source":"A good feature engineering probably has to take into accoutn also about these aspects by providing info about:\n\n* peaks\n* statistics\n* distributions","metadata":{}},{"cell_type":"markdown","source":"## Feature engineering","metadata":{}},{"cell_type":"markdown","source":"The idea here is to represent measures such as:\n\n* point-wise distribution descriptions (such as mean, specific quantiles)\n* distribution measures\n* measures of uncertainty relative to peaks","metadata":{}},{"cell_type":"code","source":"def feature_engineering(df):\n    \n    df['mean'] = df[features].mean(axis=1)\n    df['median'] = df[features].median(axis=1)\n    df['q01'] = df[features].quantile(q=0.01, axis=1)\n    df['q05'] = df[features].quantile(q=0.05, axis=1)\n    df['q10'] = df[features].quantile(q=0.10, axis=1)\n    df['q25'] = df[features].quantile(q=0.25, axis=1)\n    df['q75'] = df[features].quantile(q=0.75, axis=1)\n    df['q90'] = df[features].quantile(q=0.90, axis=1)\n    df['q95'] = df[features].quantile(q=0.95, axis=1)\n    df['q99'] = df[features].quantile(q=0.99, axis=1)\n    df['max'] = df[features].max(axis=1)\n    df['min'] = df[features].min(axis=1)\n    \n    df['std'] = df[features].std(axis=1)\n    df['range'] = df['max'] - df['min']\n    df['iqr'] = df['q75'] - df['q25']\n    df['tails'] = df['range'] / df['iqr']\n    df['dispersion'] = df['std'] / df['mean']\n    df['dispersion_2'] = df['iqr'] / df['median']\n    df['skew'] = df[features].skew(axis=1)\n    df['kurt'] = df[features].kurt(axis=1)\n    \n    df['median-max'] = df['median'] - df['max']\n    df['median-min'] = df['median'] - df['min']\n    df['q99-q95'] = df['q99'] - df['q95']\n    df['q99-q90'] = df['q99'] - df['q90']\n    df['q01-q05'] = df['q01'] - df['q05']\n    df['q01-q10'] =  df['q01'] - df['q10']\n    \n    return df\n\nfeature_engineering(test)\nfeature_engineering(train)\noriginal_features = features[:]\n#features = [col for col in train.columns if col not in ['row_id', 'target']]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:07:05.918109Z","iopub.execute_input":"2022-02-02T15:07:05.918321Z","iopub.status.idle":"2022-02-02T15:07:44.865473Z","shell.execute_reply.started":"2022-02-02T15:07:05.918294Z","shell.execute_reply":"2022-02-02T15:07:44.864622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding decamers","metadata":{}},{"cell_type":"markdown","source":"As explained in @AMBROSM's EDA there are different number of decamers in the sample due to the process described in the paper \"Analysis of Identification Method for Bacterial Species and Antibiotic Resistance Genes Using Optical Data From DNA Oligomers\" (https://www.frontiersin.org/articles/10.3389/fmicb.2020.00257/full). We want to add also this feature to test if it is informative.\n\nPlease refer to this notebook https://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense for the original code and upvote it if you find it useful,","metadata":{}},{"cell_type":"code","source":"def bias(w, x, y, z):\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ndef bias_of(s):\n    w = int(s[1:s.index('T')])\n    x = int(s[s.index('T')+1:s.index('G')])\n    y = int(s[s.index('G')+1:s.index('C')])\n    z = int(s[s.index('C')+1:])\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ntrain_i = pd.DataFrame({col: ((train[col] + bias_of(col)) * 1000000).round().astype(int) for col in original_features})\ntest_i = pd.DataFrame({col: ((test[col] + bias_of(col)) * 1000000).round().astype(int) for col in original_features})\n\ndef gcd_of_all(df_i, elements=original_features):\n    gcd = df_i[elements[0]]\n    for col in elements[1:]:\n        gcd = np.gcd(gcd, df_i[col])\n    return gcd\n\ntrain['gcd'] = gcd_of_all(train_i)\ntest['gcd'] = gcd_of_all(test_i)\n\nfeatures.append('gcd')\n\ndel([train_i, test_i])\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:07:44.866995Z","iopub.execute_input":"2022-02-02T15:07:44.867239Z","iopub.status.idle":"2022-02-02T15:07:47.831495Z","shell.execute_reply.started":"2022-02-02T15:07:44.867207Z","shell.execute_reply":"2022-02-02T15:07:47.830487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adversarial Validation","metadata":{}},{"cell_type":"markdown","source":"Before proceeding with the new features, let's check using adversarial validationif any of these causes overfitting. We achieve that by fitting a model trying to distinguish train from test. We evaluate on the fit using a cross-validation approach, averaging the results.","metadata":{}},{"cell_type":"code","source":"lr = ExtraTreesClassifier(n_estimators=100, n_jobs=-1)\nXt = train[features].append(test[features])\nyt = [0 for _ in range(len(train))] + [1 for _ in range(len(test))]\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\ncv_results = cross_validate(lr, Xt, yt, cv=cv, scoring='roc_auc', return_estimator=True, verbose=1)\n\ndel([Xt, yt])\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:07:47.8328Z","iopub.execute_input":"2022-02-02T15:07:47.83327Z","iopub.status.idle":"2022-02-02T15:08:20.057935Z","shell.execute_reply.started":"2022-02-02T15:07:47.833234Z","shell.execute_reply":"2022-02-02T15:08:20.056938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roc = np.mean(cv_results['test_score'])\nprint(f\"ROC_AUC score for adversarial validation {roc:0.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:08:20.060297Z","iopub.execute_input":"2022-02-02T15:08:20.060725Z","iopub.status.idle":"2022-02-02T15:08:20.067588Z","shell.execute_reply.started":"2022-02-02T15:08:20.060671Z","shell.execute_reply":"2022-02-02T15:08:20.06649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_importances = np.mean([mod.feature_importances_ for mod in cv_results['estimator']], axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:08:20.069345Z","iopub.execute_input":"2022-02-02T15:08:20.069797Z","iopub.status.idle":"2022-02-02T15:08:20.595575Z","shell.execute_reply.started":"2022-02-02T15:08:20.069751Z","shell.execute_reply":"2022-02-02T15:08:20.594563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_imp = pd.DataFrame({'feature': features, 'importance': feature_importances})\n\nplt.figure(figsize=(14, 7))\nsns.barplot(x=\"importance\", y=\"feature\", \n            data=df_imp.sort_values(by=\"importance\", ascending=False).iloc[:25])\nplt.title(\"ERT Feature Importance\")\nplt.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:08:20.596974Z","iopub.execute_input":"2022-02-02T15:08:20.597221Z","iopub.status.idle":"2022-02-02T15:08:21.064425Z","shell.execute_reply.started":"2022-02-02T15:08:20.597188Z","shell.execute_reply":"2022-02-02T15:08:21.063567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del(cv_results)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:08:21.065676Z","iopub.execute_input":"2022-02-02T15:08:21.065921Z","iopub.status.idle":"2022-02-02T15:08:21.253673Z","shell.execute_reply.started":"2022-02-02T15:08:21.06589Z","shell.execute_reply":"2022-02-02T15:08:21.252412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There seems to be some features that have characteristic values in the train set. We can use different options here, from recursively removing them in order to make the training more similar to the test to trying to use pseudo-labelling.","metadata":{}},{"cell_type":"markdown","source":"## Baseline modelling","metadata":{}},{"cell_type":"markdown","source":"Here is a first model taking into account the most frequent measurements of all the spectrum of histograms and some feature engineering.","metadata":{}},{"cell_type":"code","source":"FOLDS = 10\nESTIMATORS = 1_000","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:18:01.587668Z","iopub.execute_input":"2022-02-02T15:18:01.58798Z","iopub.status.idle":"2022-02-02T15:18:01.593359Z","shell.execute_reply.started":"2022-02-02T15:18:01.587948Z","shell.execute_reply":"2022-02-02T15:18:01.592288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we cross-validate, we store away, using joblib, the models for later ensembling a cv predict on the test set.","metadata":{}},{"cell_type":"code","source":"lb = LabelEncoder()\n\nX = train[features]\ny = lb.fit_transform(train['target'])\n\ncv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=0)\ntest_score = list()\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n    lr = ExtraTreesClassifier(n_estimators=ESTIMATORS, \n                              criterion='gini', \n                              max_features=128, \n                              min_samples_split=4, \n                              n_jobs=-1)\n    lr.fit(X.iloc[train_idx], y[train_idx])\n    cv_preds = lr.predict(X.iloc[val_idx])\n    val_acc = accuracy_score(y_true=y[val_idx], y_pred=cv_preds)\n    test_score.append(val_acc)\n    print(f\"fold {fold} - validation accuracy = {val_acc:0.5f}\")\n    joblib.dump(value=lr, filename=f'etr_models_fold_{fold}.gz', compress=9)\n    gc.collect()\n    \ncv_accuracy = np.mean(test_score)\nprint(f\"CV accuracy = {cv_accuracy:0.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:15:12.80699Z","iopub.execute_input":"2022-02-02T15:15:12.807305Z","iopub.status.idle":"2022-02-02T15:15:33.755908Z","shell.execute_reply.started":"2022-02-02T15:15:12.807274Z","shell.execute_reply":"2022-02-02T15:15:33.754935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After training a random forest or an extremely randomized trees (ERT) classifier you can figure what features are important by looking at the splits in the forest. By summing up the impurity gain (how much that split helped in the classification) for each variable's splits you can get an idea of how much contribution a variable brings. Here we take the most importat features from the model trained on the last cv folds.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nsns.histplot(lr.feature_importances_, bins=50, kde=True)\nplt.title(\"Impurity-based feature importances\")\n\nplt.axvline(x=0.0, color='red', linestyle='--', label=\"zero\")\nplt.axvline(x=np.percentile(lr.feature_importances_, q=25), color='black', linestyle='--', label=\"q=25\")\nplt.axvline(x=np.median(lr.feature_importances_), color='blue', linestyle='--', label=\"median\")\nplt.axvline(x=np.percentile(lr.feature_importances_, q=75), color='black', linestyle='--', label=\"q=75\")\n\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:16:20.426031Z","iopub.execute_input":"2022-02-02T15:16:20.42687Z","iopub.status.idle":"2022-02-02T15:16:21.232106Z","shell.execute_reply.started":"2022-02-02T15:16:20.426825Z","shell.execute_reply":"2022-02-02T15:16:21.231131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The problem with ERT is that splits are randomly decided, therefore each feature has a possibility to be assigned some importance if it has some correlation with the target. That's the reason why you don't have any feature at zero importance. Anyway, from the 75th percentile you can see that there fewer and fewer features, pointing out that some features are indeed more important than others. ","metadata":{}},{"cell_type":"markdown","source":"Here are the top features pointed out:","metadata":{}},{"cell_type":"code","source":"df_imp = pd.DataFrame({'feature': features, 'importance': lr.feature_importances_})\n\nplt.figure(figsize=(14, 7))\nsns.barplot(x=\"importance\", y=\"feature\", \n            data=df_imp.sort_values(by=\"importance\", ascending=False).iloc[:25])\nplt.title(\"ERT Feature Importance\")\nplt.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:16:26.783135Z","iopub.execute_input":"2022-02-02T15:16:26.783444Z","iopub.status.idle":"2022-02-02T15:16:27.323494Z","shell.execute_reply.started":"2022-02-02T15:16:26.783414Z","shell.execute_reply":"2022-02-02T15:16:27.322384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By taking the first 25 feature for importance, we notice that the ranges of the bases is in specifical ranges:\n\n* A : 0-5\n* T : 1-5\n* G : 1-5\n* C : 0-5\n\nProbably higher bases count do contribute less. ","metadata":{}},{"cell_type":"markdown","source":"Let's now proceed to predict by retrieving all the ETR models we previously stored during cross-validation.","metadata":{}},{"cell_type":"code","source":"preds = [joblib.load(f'etr_models_fold_{fold}.gz').predict(test[features]) for fold in range(FOLDS)]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:18:06.358959Z","iopub.execute_input":"2022-02-02T15:18:06.359225Z","iopub.status.idle":"2022-02-02T15:18:08.748405Z","shell.execute_reply.started":"2022-02-02T15:18:06.359197Z","shell.execute_reply":"2022-02-02T15:18:08.747301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We find the most frequent class for each sample (using the mode function from Scipy), which equates to a voting ensemble.","metadata":{}},{"cell_type":"code","source":"from scipy import stats\nmost_frequent_pred = stats.mode(np.array(preds))[0][0]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:18:48.189713Z","iopub.execute_input":"2022-02-02T15:18:48.19068Z","iopub.status.idle":"2022-02-02T15:18:51.545475Z","shell.execute_reply.started":"2022-02-02T15:18:48.190603Z","shell.execute_reply":"2022-02-02T15:18:51.544736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We reconvert labels to the bacteria names and we store the test submission.","metadata":{}},{"cell_type":"code","source":"sub.target = lb.inverse_transform(most_frequent_pred)\nsub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T15:19:20.978732Z","iopub.execute_input":"2022-02-02T15:19:20.979099Z","iopub.status.idle":"2022-02-02T15:19:21.248762Z","shell.execute_reply.started":"2022-02-02T15:19:20.979066Z","shell.execute_reply":"2022-02-02T15:19:21.24782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## If you found interesting this notebook, please consider to upvote :-)","metadata":{}}]}