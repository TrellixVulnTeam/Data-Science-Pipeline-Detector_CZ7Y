{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### What are you trying to do in this notebook?\nMy task is to classify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss. In this technique, 10-mer snippets of DNA are sampled and analyzed to give the histogram of base count. \n\n#### Why are you trying it?\nThe DNA segment ATATGGCCTT becomes A2T4G2C2. Can you use this lossy information to accurately predict bacteria species?\n\nFrom this data we need to recover the genome fingerprint to find the bacteria. To classify 10 different bacteria species given genome sequencing data. This data has been compressed so that for instance ATATGGCCTT becomes A2T4G2C2.\n\nFor this challenge, we will be predicting bacteria species based on repeated lossy measurements of DNA snippets. Snippets of length 10 are analyzed using Raman spectroscopy that calculates the histogram of bases in the snippet.\n\nEach row of data contains a spectrum of histograms generated by repeated measurements of a sample, each row containing the output of all 286 histogram possibilities (e.g., A0T0G0C10 to A10T0G0C0), which then has a bias spectrum (of totally random ATGC) subtracted from the results.\n\nThe data (both train and test) also contains simulated measurement errors (of varying rates) for many of the samples, which makes the problem more challenging.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-24T09:10:24.479474Z","iopub.execute_input":"2022-02-24T09:10:24.479888Z","iopub.status.idle":"2022-02-24T09:10:24.545724Z","shell.execute_reply.started":"2022-02-24T09:10:24.479795Z","shell.execute_reply":"2022-02-24T09:10:24.545043Z"}}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-02-25T05:58:08.405415Z","iopub.execute_input":"2022-02-25T05:58:08.405739Z","iopub.status.idle":"2022-02-25T05:58:08.451318Z","shell.execute_reply.started":"2022-02-25T05:58:08.405706Z","shell.execute_reply":"2022-02-25T05:58:08.450591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/train.csv')\ntest = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/test.csv')\nsub = pd.read_csv('../input/early-ensemble/submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-25T05:58:15.887301Z","iopub.execute_input":"2022-02-25T05:58:15.887607Z","iopub.status.idle":"2022-02-25T05:58:54.348989Z","shell.execute_reply.started":"2022-02-25T05:58:15.887565Z","shell.execute_reply":"2022-02-25T05:58:54.347988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [e for e in test.columns if e not in ('row_id')]\ntrain.drop_duplicates(subset=cols, keep='first',inplace = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T05:58:54.350303Z","iopub.execute_input":"2022-02-25T05:58:54.3505Z","iopub.status.idle":"2022-02-25T05:58:56.485935Z","shell.execute_reply.started":"2022-02-25T05:58:54.350472Z","shell.execute_reply":"2022-02-25T05:58:56.485057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T06:04:07.425337Z","iopub.execute_input":"2022-02-25T06:04:07.42564Z","iopub.status.idle":"2022-02-25T06:04:07.430827Z","shell.execute_reply.started":"2022-02-25T06:04:07.425607Z","shell.execute_reply":"2022-02-25T06:04:07.43006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s1 = pd.merge(train, test, how='inner', on=cols)\n\ns1","metadata":{"execution":{"iopub.status.busy":"2022-02-25T06:04:10.471992Z","iopub.execute_input":"2022-02-25T06:04:10.472641Z","iopub.status.idle":"2022-02-25T06:04:25.649805Z","shell.execute_reply.started":"2022-02-25T06:04:10.472607Z","shell.execute_reply":"2022-02-25T06:04:25.648933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s1.row_id_y.nunique()","metadata":{"execution":{"iopub.status.busy":"2022-02-25T06:05:21.481163Z","iopub.execute_input":"2022-02-25T06:05:21.481998Z","iopub.status.idle":"2022-02-25T06:05:21.492484Z","shell.execute_reply.started":"2022-02-25T06:05:21.481942Z","shell.execute_reply":"2022-02-25T06:05:21.491739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dic = {}\nfor i in range(len(s1)):\n    dic[s1.loc[i]['row_id_y']] = s1.loc[i]['row_id_x']","metadata":{"execution":{"iopub.status.busy":"2022-02-25T06:05:22.929362Z","iopub.execute_input":"2022-02-25T06:05:22.930095Z","iopub.status.idle":"2022-02-25T06:05:23.323327Z","shell.execute_reply.started":"2022-02-25T06:05:22.930048Z","shell.execute_reply":"2022-02-25T06:05:23.322377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dic)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T06:05:24.527616Z","iopub.execute_input":"2022-02-25T06:05:24.527939Z","iopub.status.idle":"2022-02-25T06:05:24.535174Z","shell.execute_reply.started":"2022-02-25T06:05:24.527904Z","shell.execute_reply":"2022-02-25T06:05:24.534157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for e in dic.items():\n    print(e)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T06:05:27.478424Z","iopub.execute_input":"2022-02-25T06:05:27.478737Z","iopub.status.idle":"2022-02-25T06:05:27.693158Z","shell.execute_reply.started":"2022-02-25T06:05:27.478704Z","shell.execute_reply":"2022-02-25T06:05:27.691029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for e in dic:\n    sub.loc[sub[sub['row_id']==e].index.to_list(),'target'] = train.loc[train[train['row_id']==dic[e]].index.tolist()[0],'target']","metadata":{"execution":{"iopub.status.busy":"2022-02-25T06:05:33.988212Z","iopub.execute_input":"2022-02-25T06:05:33.98851Z","iopub.status.idle":"2022-02-25T06:05:35.221793Z","shell.execute_reply.started":"2022-02-25T06:05:33.988477Z","shell.execute_reply":"2022-02-25T06:05:35.220814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T06:05:35.734326Z","iopub.execute_input":"2022-02-25T06:05:35.734641Z","iopub.status.idle":"2022-02-25T06:05:35.977078Z","shell.execute_reply.started":"2022-02-25T06:05:35.734607Z","shell.execute_reply":"2022-02-25T06:05:35.976129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"execution":{"iopub.status.busy":"2022-02-25T06:05:37.34343Z","iopub.execute_input":"2022-02-25T06:05:37.343914Z","iopub.status.idle":"2022-02-25T06:05:37.356454Z","shell.execute_reply.started":"2022-02-25T06:05:37.343862Z","shell.execute_reply":"2022-02-25T06:05:37.355494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Did it work?\nTarget column is the target variable which consists of 10 kinds of bacteria Streptococcus_pyogenes, Salmonella_enterica, Enterococcus_hirae, Escherichia_coli, Campylobacter_jejuni, Streptococcus_pneumoniae, Staphylococcus_aureus, Escherichia_fergusonii, Bacteroides_fragilis, Klebsiella_pneumoniae.\n\nTrain dataset has 200,000 rows and 288 columns which contains286 features, 1 target variable target and 1 column of row_id.\n\nTest dataset has 100,000 rows and 287 columns which contains286 features with1 column of row_id No missing values in train and test dataset.\n\n#### What did you not understand about this process?\nWell, everything provides in the competition data page. I've no problem while working on it. If you guys don't understand the thing that I'll do in this notebook then please comment on this notebook.\n\n#### What else do you think you can try as part of this approach?\nClassify 10 different bacteria species using data from a genomic analysis technique that has some data compression and data loss. In this technique, 10-mer snippets of DNA are sampled and analyzed to give the histogram of base count.","metadata":{}}]}