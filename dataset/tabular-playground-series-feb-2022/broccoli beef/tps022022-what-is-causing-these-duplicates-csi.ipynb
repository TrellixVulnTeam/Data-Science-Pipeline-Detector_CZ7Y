{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-08T02:57:23.09843Z","iopub.execute_input":"2022-02-08T02:57:23.099405Z","iopub.status.idle":"2022-02-08T02:57:23.128652Z","shell.execute_reply.started":"2022-02-08T02:57:23.0993Z","shell.execute_reply":"2022-02-08T02:57:23.127707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nLike most TPS competitions in the past, TPS February 2022 took an unexpected turn as kagglers (e.g., [discussion thread](https://www.kaggle.com/c/tabular-playground-series-feb-2022/discussion/305364)) discovered that there are a lot of duplicate rows in the training and test data. Subsequently there have been debates whether these duplicates should be dropped or kept or accounted for such as assigning a sample weight. \n\nInitially my reaction was that this phenomenon was mainly caused by combinatorics. I was led to this thinking because most duplicates occur in the low-resolution rows. Briefly, the original [paper](https://www.frontiersin.org/articles/10.3389/fmicb.2020.00257/full) speaks of the number \\\\(r\\\\) of \"pyramid tips\" (`num_reads` in their implementation), which determines the number of BOC reads for each sample. In that context, \\\\(r=100\\\\) gives a low-resolution histogram whereas \\\\(r=1000000\\\\) gives a high-resolution histogram. Combinatorially, a low-resolution histogram has less possible combinations, more precisely there are \\\\(\\frac{(r+285)!}{r!285!}\\\\) possible histograms. This is still a huge number, but perhaps if the spectrum to be sampled has a lot of zeroes, then that might limit the number such as repetitions might be more likely -- so I thought. When I took the time to perform some simulations later, it turns out that getting duplicates this way is next to impossible, even for \\\\(r=100\\\\), either with a genome distribution or the bias distribution.\n\nI became curious. There must be a way to replicate this phenomenon. It is unlikely I can replicate the exact data in this competition, but I should be able to replicate the phenomenon. That's when I decided to look at the [accompanying implementation](https://github.com/rlwphd/DNAFingerprints) by the authors of the paper. The following are my findings.\n\n**Disclaimer**: This is an investigation of the data generation process in the original paper. I do not have any knowledge of how the data was generated in this competition. For what I know, kaggle might take some ideas from the paper, but use a completely different methodology in data generation.\n\n**Disclaimer\\\\({}^2\\\\):** The \"CSI\" part of the title of this notebook is just a joke. It is not intended to imply that any crime has occurred. ","metadata":{}},{"cell_type":"markdown","source":"# Extracting just enough code to replicate the phenomenon","metadata":{}},{"cell_type":"markdown","source":"First let us download the whole package. ","metadata":{}},{"cell_type":"code","source":"!wget https://github.com/rlwphd/DNAFingerprints/archive/refs/heads/master.zip\nimport zipfile\nwith zipfile.ZipFile('master.zip', 'r') as zip_ref:\n    zip_ref.extractall('.')   ","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:57:23.130917Z","iopub.execute_input":"2022-02-08T02:57:23.13123Z","iopub.status.idle":"2022-02-08T02:57:46.6385Z","shell.execute_reply.started":"2022-02-08T02:57:23.13119Z","shell.execute_reply":"2022-02-08T02:57:46.637638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport math\nimport time\nimport datetime\nimport dask.dataframe as ddf\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:57:46.639887Z","iopub.execute_input":"2022-02-08T02:57:46.640118Z","iopub.status.idle":"2022-02-08T02:57:47.581458Z","shell.execute_reply.started":"2022-02-08T02:57:46.640086Z","shell.execute_reply":"2022-02-08T02:57:47.580665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We need to specify where to read the raw data and where to output the BOC reads.","metadata":{}},{"cell_type":"code","source":"bacteria = './DNAFingerprints-master/Bacteria/'\nlocal_BOC = '.'","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:57:47.582911Z","iopub.execute_input":"2022-02-08T02:57:47.583246Z","iopub.status.idle":"2022-02-08T02:57:47.587949Z","shell.execute_reply.started":"2022-02-08T02:57:47.583204Z","shell.execute_reply":"2022-02-08T02:57:47.587136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code in the following cell is directly copied from the original source.","metadata":{}},{"cell_type":"code","source":"def str_count(str_part):\n    A = str_part.count('A')\n    T = str_part.count('T')\n    G = str_part.count('G')\n    C = str_part.count('C')\n\n    return (A, T, G, C), (T, A, C, G)\n\ndef kmer_fingerprints(whole_str,dna_length,kmer_range):\n    str_part = (whole_str[ii:ii+dna_length] for ii in range(len(whole_str)-dna_length+1))\n    kmer_list = [item for string in str_part for item in str_count(string)]\n    kmer_dict = Counter(kmer_list)\n    results = [kmer_dict[val] for val in kmer_range]\n\n    return results\n\ndef title_extraction(header, resistance):\n    heading = header.split(' ')\n    seq_record_id = heading[0]\n    genus = heading[1]\n    if len(heading) >= 3:\n        species = heading[2]\n        name = genus + ' ' + species\n    else:\n        name = genus\n        \n    for word in heading:\n        if 'plasmid' in word:\n            dna_type = 'Plasmid'\n            break\n        elif 'genome' in word:\n            dna_type = 'Genome'\n            break\n        elif 'sequence'in word:\n            dna_type = 'Sequence'\n            break\n        else:\n            dna_type = \"\"\n    \n    bacteria_type = \"\"\n    strain = \"\"\n    data_add = [seq_record_id[1:], resistance, name, genus, dna_type, strain, bacteria_type,  header]\n    \n    return data_add\n\ndef multiple_str_check(file):\n    header = []\n    start = []\n    with open(file, 'r') as f:\n        for ii, line in enumerate(f):\n            if '>' == line[0]:\n                header.append(line)\n                start.extend([ii+1])\n            end = ii+1\n        start.extend([end])\n    return header, start\n\ndef str_extraction(file,start,end,dna_length):\n    string = []\n    with open(file,'r') as f:\n        for ii, line in enumerate(f):\n            if ii >= start and ii < end:\n                string.append(line)\n            elif ii == end:\n                break\n                \n    \n    section = ''.join(line.strip() for line in string)\n    whole_string = ''.join([section,section[0:dna_length-1]])\n    return whole_string\n\ndef kmer_main(file,dna_length,kmer_range,data_index,df_Genome,df_Plasmid, resistance):\n    header,start = multiple_str_check(file)\n    if len(header) > 1:\n        for ii in range(len(header)):\n            whole_str = str_extraction(file,start[ii],start[ii+1]-1,dna_length)\n            species_data = title_extraction(header[ii], resistance)\n            kmer_results = kmer_fingerprints(whole_str,dna_length,kmer_range)\n            species_data.extend(kmer_results)\n            df_kmer = pd.DataFrame(species_data, index=data_index)\n            if species_data[4] != 'Plasmid':\n                df_Genome = df_Genome.append(df_kmer.T, ignore_index=True)\n            elif species_data[4] == 'Plasmid':\n                df_Plasmid = df_Plasmid.append(df_kmer.T, ignore_index=True)\n    else:\n        whole_str = str_extraction(file,start[0],start[1]-1,dna_length)\n        species_data = title_extraction(header[0], resistance)\n        kmer_results = kmer_fingerprints(whole_str,dna_length,kmer_range)\n        species_data.extend(kmer_results)\n        df_kmer = pd.DataFrame(species_data, index=data_index)\n        if species_data[4] != 'Plasmid':\n            df_Genome = df_Genome.append(df_kmer.T, ignore_index=True)\n        elif species_data[4] == 'Plasmid':\n            df_Plasmid = df_Plasmid.append(df_kmer.T, ignore_index=True)\n    \n    type_change = {}\n    for ii,name in enumerate(data_index):\n        if ii < 8:\n            type_change[name] = 'object'\n        else:\n            type_change[name]='int32'\n    df_Genome = df_Genome.astype(type_change)\n    df_Plasmid = df_Plasmid.astype(type_change)\n    \n    return df_Genome,df_Plasmid\n\ndef kmer_length():\n    k = None\n    while k is None:\n        input_value = input(\"Please enter DNA segment length (5-100): \")\n        try:\n        # try and convert the string input to a number\n            k = int(input_value)\n            if k < 5:\n                print(\"{input} is not a valid integer, please enter a valid integer between 5-100\".format(input=input_value))\n                k = None\n            elif k > 100:\n                print(\"{input} is not a valid integer, please enter a valid integer between 5-100\".format(input=input_value))\n                k = None\n        except ValueError:\n        # tell the user off\n            print(\"{input} is not a valid integer, please enter a valid integer between 5-100\".format(input=input_value))\n    return k","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:57:47.590949Z","iopub.execute_input":"2022-02-08T02:57:47.591438Z","iopub.status.idle":"2022-02-08T02:57:47.621751Z","shell.execute_reply.started":"2022-02-08T02:57:47.591392Z","shell.execute_reply":"2022-02-08T02:57:47.620868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function `SERS_values` is an important function that generates the BOC reads. I put it in a separate cell for emphasis but otherwise it is a verbatim copy of the original source.","metadata":{}},{"cell_type":"code","source":"def SERS_values(sers,categories,num_reads,arr,mutate,mutation):\n\n    jj = 0\n    while 1:\n    # Randomizing the pyramid array\n        mr = np.random.permutation(arr)\n    # Adding in the mutations\n        mr = np.where(mutate > 0, mutation, mr)\n    # Getting the respective kmer counts and dividing the values \n    # by the total value to get the frequencies\n        for nn in range(len(num_reads)):\n        # Setting the limits for how much of the sequence it's using\n            if num_reads[nn] > 10000:\n                ff = 10000\n                max_depth = int(num_reads[nn]/10000)\n            else:\n                ff = num_reads[nn]\n                max_depth = 1 \n            for mm in range(mr.shape[1]):\n                for kk in range(max_depth):\n                    sers[nn,mm,jj,:] += np.bincount(mr[kk,mm,:ff],minlength=categories)\n                    \n        jj+=1\n        if jj >= sers.shape[2]:\n            break\n    sers /= np.array(num_reads).reshape((-1,1,1,1))\n    return sers","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:57:47.623141Z","iopub.execute_input":"2022-02-08T02:57:47.623797Z","iopub.status.idle":"2022-02-08T02:57:47.638676Z","shell.execute_reply.started":"2022-02-08T02:57:47.623765Z","shell.execute_reply":"2022-02-08T02:57:47.637925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The function `SERS_reads` is also an important function because it performs the random sampling of FBC spectra. It runs a loop through each genome, pre-samples the FBC spectrum and bias spectrum \\\\(200\\times10000\\\\) times, passes the pre-sampled arrays to `SERS_values` to get the BOC reads, then write the reads to a file for each combination of `error_rate` and `num_reads`. \n\nI modified the original code in 2 ways:\n* breaks out of the main loop after processing one genome. I just want to confirm the statistics of duplicates, and processing one genome is enough.\n* saves the output to CSV instead of HDF5","metadata":{}},{"cell_type":"code","source":"def SERS_reads(dna_length,df,group,DNAtype,data_categories,bias,num_training_samples,num_reads,error_rate):\n\n#Dividing the number of occurences of each bin by the total number of occurences\n    df_prob = df.loc[:,data_categories[0]:data_categories[-1]].div(df.loc[:,data_categories[0]:data_categories[-1]].sum(axis=1),axis=0)\n\n    for ii in range(len(df_prob.index)):\n    # Getting just the probility values from the kmer counts\n        print(F'Processing {df.iloc[ii][\"Resistance\"]} starts')\n        prob = df_prob.iloc[ii,:].values\n    # Getting the probility for the largest pyramid of interest and setting the data type\n        read = np.random.RandomState(seed=231).choice(len(data_categories),(int(2*max(num_reads)/10000),10000),p=prob)\n        read = np.stack([read for _ in range(len(error_rate))],axis=1)\n        \n    # Creating the mutation array\n        mutate = np.zeros((int(2*max(num_reads)/10000),len(error_rate),10000),dtype=np.int16)\n        for int_mut,mut in enumerate(error_rate):\n            mutations = np.concatenate([np.random.RandomState(seed=123).choice([0,1],min(num_reads),p=[1-mut,mut]) for _ in range(int(2*max(num_reads)/min(num_reads)))]).reshape((-1,10000))\n            mutate[:,int_mut,:] = mutations\n        mutation = np.random.RandomState(seed=321).choice(len(data_categories),(int(2*max(num_reads)/10000),10000),p=bias)\n        mutation = np.stack([mutation for _ in range(len(error_rate))],axis=1)\n        \n    # Getting the training samples for the species\n        sers = np.empty((len(num_reads),len(error_rate),num_training_samples,len(data_categories)))\n        sers_results = SERS_values(sers,len(data_categories),num_reads,read,mutate,mutation)\n    # Subtracting off the random bias \n        sers_results -= bias\n        \n    # Cycling through each mutation array\n        for mut_int in range(len(error_rate)):\n        # Cycling through each pyramid size\n            for read_int in range(len(num_reads)):\n            # Storing the files as an hdf5 file\n                save_path = os.path.join(local_BOC,'SERS_%s_%s_%s_%s_%smer_data.csv' % (str(int(error_rate[mut_int]*100)), str(num_reads[read_int]), group, DNAtype, str(dna_length)))\n                # Putting it into a pandas dataframe and storing it\n                df_SERS = pd.DataFrame(sers_results[read_int,mut_int,:,:], columns=data_categories)\n                df_SERS['Name'] = [df.iloc[ii,1]]*len(df_SERS.index)\n                df_SERS.to_csv(save_path,index=False)\n        \n        print(F'Processing {df.iloc[ii][\"Resistance\"]} ends')\n        break # processing just one genome\n        \n        if ii % 10 == 0:\n            print(ii)\n            print('Saved %s' % (datetime.datetime.now().isoformat()))\n            \n\n    return","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:57:47.639789Z","iopub.execute_input":"2022-02-08T02:57:47.640196Z","iopub.status.idle":"2022-02-08T02:57:47.659695Z","shell.execute_reply.started":"2022-02-08T02:57:47.640167Z","shell.execute_reply":"2022-02-08T02:57:47.658946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following cell is also direct copy of the original code. It runs through each genome sequence to produce the FBC spectrum and save to .h5 file.","metadata":{}},{"cell_type":"code","source":"#dna_length=kmer_length()\ndna_length = 10\n# Recording the time it takes to run everything\nprint(datetime.datetime.now().isoformat())\nstart = time.perf_counter()\n# number of samples per species \nnum_training_samples = 1000\n# error rate\nerror_rate = [0,0.01,0.05,0.1,0.25,0.33,0.5,0.75,0.9,1]\n# number of optical sequencing reads\nnum_reads = [100,1000,10000,100000,1000000]\n\n# creating the correct tuples for how many A, T, G and C's are in each bin\nkmer_range = [(aa, tt, gg, cc) for aa in range(dna_length + 1) for tt in range(dna_length + 1) for gg in range(dna_length + 1) for cc in range(dna_length + 1) if aa + tt + cc + gg == dna_length]\n# setting dna length based bias\nbias = np.array([(1/4**dna_length) * math.factorial(dna_length)/(math.factorial(kmer[0]) * math.factorial(kmer[1]) * math.factorial(kmer[2]) * math.factorial(kmer[3])) for kmer in kmer_range])\n# creating the categorical labels for storing data\ndata_categories = [\"A%sT%sG%sC%s\" % (str(aa), str(tt), str(gg), str(cc)) for aa in range(dna_length + 1) for tt in range(dna_length + 1) for gg in range(dna_length + 1) for cc in range(dna_length + 1) if aa + tt + cc + gg == dna_length]\n# creating the labels for the non-numerical information\ndata_index = ['Seq Record ID', 'Resistance', 'Name', 'Genus', 'DNA Type', 'Strain', 'Bacteria Type', 'Notes']\n# combining the the non-numerical and categorical labels for the pandas dataframe\ndata_index.extend(data_categories)\n\n# getting the list of all of the folders and files that have the DNA sequences in them\nfile_list = [(os.path.join(root,name),root[3:].split('/')[-1],name) for root, dirs, files in os.walk(bacteria) for name in files if name.endswith(\".txt\") or name.endswith(\".fna\")]\n\n# Running through all of the DNA sequence files\nfor int_file,file in enumerate(file_list):\n# creating the empty dataframes to store the data in\n    df_Genome = pd.DataFrame(columns=data_index)\n    df_Plasmid = pd.DataFrame(columns=data_index)\n\n# creating the 10mer data files for the DNA sequences\n    resistance = file[2][:-4]\n    df_Genome, df_Plasmid = kmer_main(file[0],dna_length,kmer_range, data_index,df_Genome,df_Plasmid, resistance)\n    print('Completed %s' % file[2][:-4])\n\n# saving the dataframe\n    if len(df_Genome.index) > 0:\n        df_Genome.to_hdf('PandasDataFrame_%s_Genome_%smer_data.h5' % (file[1],str(dna_length)),'df%s' % (int_file),mode='a',format='table')\n    if len(df_Plasmid.index) > 0:\n        df_Plasmid.to_hdf('PandasDataFrame_%s_Plasmid_%smer_data.h5' % (file[1],str(dna_length)),'df%s' % (int_file),mode='a',format='table')\n    print('file saved')\n\n    del(df_Genome)\n    del(df_Plasmid)\n\nend = time.perf_counter()\nprint('# of hours to run code: %s' % ((end-start)/3600))\nprint(datetime.datetime.now().isoformat())","metadata":{"execution":{"iopub.status.busy":"2022-02-08T02:57:47.660858Z","iopub.execute_input":"2022-02-08T02:57:47.661584Z","iopub.status.idle":"2022-02-08T03:02:57.019036Z","shell.execute_reply.started":"2022-02-08T02:57:47.661533Z","shell.execute_reply":"2022-02-08T03:02:57.018033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK so I am only interested in how the training data is generated, that would be from the file `PandasDataFrame_Training_Genome_10mer_data.h5`. For the first genome (Bacteroides_fragilis; remember I break out the loop after one genome) `num_training_samples = 1000` samples are generated for each combination of `error_rate` and `num_reads`.","metadata":{}},{"cell_type":"code","source":"file = './PandasDataFrame_Training_Genome_10mer_data.h5'\nfile_split = file[:-3].split('_')\ndf = ddf.read_hdf(file, 'df*').compute()\nSERS_reads(dna_length,df,file_split[1],file_split[2],data_categories,bias,num_training_samples,num_reads,error_rate)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:02:57.020865Z","iopub.execute_input":"2022-02-08T03:02:57.021122Z","iopub.status.idle":"2022-02-08T03:07:22.148132Z","shell.execute_reply.started":"2022-02-08T03:02:57.021091Z","shell.execute_reply":"2022-02-08T03:07:22.146939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see what we got. First, `error_rate = 0.0`, `num_reads = 100`","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('./SERS_0_100_Training_Genome_10mer_data.csv')\nnp.unique(train_data.drop('Name',axis=1).to_numpy().astype(np.float64),axis=0).shape","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:07:22.14973Z","iopub.execute_input":"2022-02-08T03:07:22.150068Z","iopub.status.idle":"2022-02-08T03:07:22.280263Z","shell.execute_reply.started":"2022-02-08T03:07:22.150026Z","shell.execute_reply":"2022-02-08T03:07:22.279253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How about a higher `num_reads` but still less than 10000? `error_rate = 0.0`, `num_reads = 1000`","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('./SERS_0_1000_Training_Genome_10mer_data.csv')\nnp.unique(train_data.drop('Name',axis=1).to_numpy().astype(np.float64),axis=0).shape","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:07:22.281788Z","iopub.execute_input":"2022-02-08T03:07:22.282118Z","iopub.status.idle":"2022-02-08T03:07:22.403241Z","shell.execute_reply.started":"2022-02-08T03:07:22.282076Z","shell.execute_reply":"2022-02-08T03:07:22.402644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What if `num_reads` is larger than 10000? `error_rate = 0.0`, `num_reads = 100000`","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('./SERS_0_100000_Training_Genome_10mer_data.csv')\nnp.unique(train_data.drop('Name',axis=1).to_numpy().astype(np.float64),axis=0).shape","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:07:22.404249Z","iopub.execute_input":"2022-02-08T03:07:22.404961Z","iopub.status.idle":"2022-02-08T03:07:22.507643Z","shell.execute_reply.started":"2022-02-08T03:07:22.404917Z","shell.execute_reply":"2022-02-08T03:07:22.50674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try a couple of intermediate cases, with 0.5 `error_rate` and a low and high `num_reads`.","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('./SERS_50_100_Training_Genome_10mer_data.csv')\nnp.unique(train_data.drop('Name',axis=1).to_numpy().astype(np.float64),axis=0).shape","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:07:22.509044Z","iopub.execute_input":"2022-02-08T03:07:22.509305Z","iopub.status.idle":"2022-02-08T03:07:22.634317Z","shell.execute_reply.started":"2022-02-08T03:07:22.509275Z","shell.execute_reply":"2022-02-08T03:07:22.633217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('./SERS_50_100000_Training_Genome_10mer_data.csv')\nnp.unique(train_data.drop('Name',axis=1).to_numpy().astype(np.float64),axis=0).shape","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:07:22.637068Z","iopub.execute_input":"2022-02-08T03:07:22.637416Z","iopub.status.idle":"2022-02-08T03:07:22.743531Z","shell.execute_reply.started":"2022-02-08T03:07:22.637384Z","shell.execute_reply":"2022-02-08T03:07:22.742554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now what happens when the `error_rate` is 100%? Even with highest resolution `num_reads = 1000000`?","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('./SERS_100_1000000_Training_Genome_10mer_data.csv')\nnp.unique(train_data.drop('Name',axis=1).to_numpy().astype(np.float64),axis=0).shape","metadata":{"execution":{"iopub.status.busy":"2022-02-08T03:07:22.745103Z","iopub.execute_input":"2022-02-08T03:07:22.745318Z","iopub.status.idle":"2022-02-08T03:07:22.94329Z","shell.execute_reply.started":"2022-02-08T03:07:22.745293Z","shell.execute_reply":"2022-02-08T03:07:22.942709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Interpretations\n\n* `num_reads` \\\\(\\le10000\\\\), `error_rate` \\\\(< 1.0\\\\): It is not hard to see why the number of unique rows is at most 200. The sampling scheme pre-samples a \\\\(200\\times10000\\\\) array and for each sample, pick a random row (first row of the permuted array). Obviously there can be at most 200 unique rows.\n\n* `num_reads` \\\\(>10000\\\\), `error_rate` \\\\(< 1.0\\\\): In this case, `max_depth` rows from the presampled array are randomly chosen, where `max_depth = int(num_read/10000)`. There are a lot more variability and you would have to be extremely lucky to get duplicate rows.\n\n* `error_rate` \\\\(=1.0\\\\): The output is always a constant row?? Well, it is clear why this is happening if you look the code more carefully.","metadata":{}},{"cell_type":"markdown","source":"At the beginning of the function `SERS_values` you can find these lines. `jj` indexes the samples to be generated. Inside the \"infinite loop\", the BOC reads `arr` are first randomized row-wise and saved as `mr`. If the `error_rate` is 1.0, `mutate` is identically one's. So, after \"adding in the mutations\", `mr` becomes the same as `mutation`, a \\\\(200\\times10000\\\\) presampled array of the bias distribution. But wait, this array has not be randomized. Subsequently, only the first `max_depth` rows of this array are used as output, so the output is always a constant. Should the `mutation` array have been permuted for each sample generation? Common sense of mine says yes, but I am not a biologist nor a DNA device engineer to tell if a real device would behave this way. \n\n```\ndef SERS_values(sers,categories,num_reads,arr,mutate,mutation):\n\n    jj = 0\n    while 1:\n    # Randomizing the pyramid array\n        mr = np.random.permutation(arr)\n    # Adding in the mutations\n        mr = np.where(mutate > 0, mutation, mr)\n    .\n    .\n    .\n```\n\nThat concludes the reveal of the mystery of constant mutation.","metadata":{}},{"cell_type":"markdown","source":"# Conclusions\n\nIt is important to emphasis that this is a scientific investigation of how the data might have been generated in the original paper, according to their published accompanying code. I have no knowledge of how the data was generated in this competition. Whether one should remove duplicates or keep them or do something special about them, is one of the modeling decisions a modeler needs to make. I do believe that understanding the data generation model is an important step in the modeling, even if that data generation model might not reflect reality or might be buggy.","metadata":{}}]}