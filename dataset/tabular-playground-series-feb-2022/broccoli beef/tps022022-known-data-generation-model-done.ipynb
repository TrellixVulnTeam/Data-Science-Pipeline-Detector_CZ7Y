{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-21T01:17:37.145064Z","iopub.execute_input":"2022-02-21T01:17:37.145413Z","iopub.status.idle":"2022-02-21T01:17:37.183481Z","shell.execute_reply.started":"2022-02-21T01:17:37.145311Z","shell.execute_reply":"2022-02-21T01:17:37.182821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.special import entr, rel_entr\nfrom tqdm import tqdm\nfrom sklearn.decomposition import PCA\nfrom matplotlib import pyplot as plt\npd.set_option('max_columns',None)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-21T01:17:37.184548Z","iopub.execute_input":"2022-02-21T01:17:37.184793Z","iopub.status.idle":"2022-02-21T01:17:38.622327Z","shell.execute_reply.started":"2022-02-21T01:17:37.184736Z","shell.execute_reply":"2022-02-21T01:17:38.620985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nThis notebook investigates the following question: If the generation model for the TPS Feb 2022 data were known completely and perfectly, would the classification task be trivial? For that purpose, let's assume the ideal situation where samples are independently drawn from the reference FBC spectra of the 10 bacteria in question, and no error is introduced. In that situation, can we achieve perfect classification by simply finding the reference FBC spectrum that is closest to the sample?\n\nTo make the notebook slightly more interesting, we also investigate the choice of several obvious distance functions.","metadata":{}},{"cell_type":"markdown","source":"# The Setup\n\nTo begin, let's load the reference spectra from the dataset I prepared earlier.","metadata":{}},{"cell_type":"code","source":"fbc_spectra = pd.read_csv('../input/tps022022-reference-fbc-spectra/train_ref_fbc_spec.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-21T01:17:38.62449Z","iopub.execute_input":"2022-02-21T01:17:38.625104Z","iopub.status.idle":"2022-02-21T01:17:38.660882Z","shell.execute_reply.started":"2022-02-21T01:17:38.625073Z","shell.execute_reply":"2022-02-21T01:17:38.659184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fbc_spectra","metadata":{"execution":{"iopub.status.busy":"2022-02-21T01:17:38.663243Z","iopub.execute_input":"2022-02-21T01:17:38.663509Z","iopub.status.idle":"2022-02-21T01:17:39.002547Z","shell.execute_reply.started":"2022-02-21T01:17:38.663481Z","shell.execute_reply":"2022-02-21T01:17:39.000987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boc_cols = []\nfor aa in range(11):\n    for tt in range(11):\n        for gg in range(11):\n            for cc in range(11):\n                if aa+tt+gg+cc == 10:\n                    boc_cols.append(F'A{aa}T{tt}G{gg}C{cc}')\n                    \nbacteria = np.sort(np.unique(fbc_spectra.Resistance))","metadata":{"execution":{"iopub.status.busy":"2022-02-21T01:17:39.004425Z","iopub.execute_input":"2022-02-21T01:17:39.004685Z","iopub.status.idle":"2022-02-21T01:17:39.018634Z","shell.execute_reply.started":"2022-02-21T01:17:39.004649Z","shell.execute_reply":"2022-02-21T01:17:39.017788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating the datasets\n\nThe sampling is done straightforwardly. Unlike the implementation provided along with the paper, which presamples the reference spectrum a certain number of times and then draws with replacement, we choose to do it correctly, i.e., each sample is drawn independently.","metadata":{}},{"cell_type":"code","source":"def sample_spectrum(num_samples, num_reads, spectrum, rng):\n    boc_reads = rng.choice(len(boc_cols),(num_samples,num_reads), p=spectrum)\n    samples = []\n    for i in range(num_samples):\n        samples.append(np.bincount(boc_reads[i,:],minlength=len(boc_cols)))\n    return np.array(samples)/num_reads","metadata":{"execution":{"iopub.status.busy":"2022-02-21T01:17:39.019905Z","iopub.execute_input":"2022-02-21T01:17:39.020384Z","iopub.status.idle":"2022-02-21T01:17:39.034668Z","shell.execute_reply.started":"2022-02-21T01:17:39.020342Z","shell.execute_reply":"2022-02-21T01:17:39.033352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_dataset(num_reads, num_samples_per_species):\n    rng = np.random.default_rng(42)\n    samples = []\n    target = []\n    for species in bacteria:\n        spectrum = fbc_spectra[fbc_spectra.Resistance==species][boc_cols].to_numpy().flatten()\n        samples.append(sample_spectrum(num_samples_per_species,num_reads,spectrum,rng))\n        target.append(np.full((num_samples_per_species,),species))\n    return np.concatenate(samples), np.concatenate(target)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T01:17:39.036321Z","iopub.execute_input":"2022-02-21T01:17:39.036688Z","iopub.status.idle":"2022-02-21T01:17:39.04822Z","shell.execute_reply.started":"2022-02-21T01:17:39.036654Z","shell.execute_reply":"2022-02-21T01:17:39.047414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize these datasets by projecting onto the first two principal components, using only 100 samples per species for time sake.","metadata":{}},{"cell_type":"code","source":"%%time\nfig = plt.figure(figsize=(12,4))\nfor j,num_reads in enumerate([100000,1000,100]):\n    X,y = generate_dataset(num_reads,100)\n    x = PCA().fit_transform(X)\n    fig.add_subplot(1,3,j+1)\n    for i,species in enumerate(bacteria):\n        plt.plot(x[y==species,0],x[y==species,1],'.',alpha=0.1,color=F'C{i}')\n    plt.axis('equal')\n    plt.title(F'num_reads={num_reads}')\nplt.tight_layout()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-02-21T01:17:39.049229Z","iopub.execute_input":"2022-02-21T01:17:39.050038Z","iopub.status.idle":"2022-02-21T01:17:48.030991Z","shell.execute_reply.started":"2022-02-21T01:17:39.050007Z","shell.execute_reply":"2022-02-21T01:17:48.029786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OK it doesn't look too promising for the low `num_reads`. We'll carry on anyway.","metadata":{}},{"cell_type":"markdown","source":"# Choice of the distance function\n\nThere are several obvious choices for distance in this feature space. Since each row is an empirical probability distribution, the natural choice is the Kullback-Leibler divergence. For comparison, we also consider other convenient (but theoretically incorrect) choices such as \\\\(\\ell^2\\\\), \\\\(\\ell^1\\\\) and \\\\(\\ell^\\infty\\\\) norms.","metadata":{}},{"cell_type":"code","source":"def calculate_distances(X, spectra, p = 0):\n    distances = []\n    for i in range(spectra.shape[0]):\n        spectrum = spectra[i,:].reshape((1,-1))\n        if p == 0: # KL divergence\n            distances.append(np.sum(rel_entr(X,spectrum),axis=1,keepdims=True))\n        else:\n            distances.append(np.linalg.norm(X-spectrum,ord=p,axis=1,keepdims=True))\n    return np.concatenate(distances,axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T01:41:11.34532Z","iopub.execute_input":"2022-02-21T01:41:11.348252Z","iopub.status.idle":"2022-02-21T01:41:11.360876Z","shell.execute_reply.started":"2022-02-21T01:41:11.348208Z","shell.execute_reply":"2022-02-21T01:41:11.359103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Results\n\nFor this experiment, we will be generating datasets for `num_reads` 100000, 1000, and 100, with 5000 samples for each species. We are skipping the case `num-reads=1000000` because the answer to the question posed in the title of this notebook is most likely \"yes\", and it would take a long time to generate the dataset. \n\nThe classification algorithm is simply to take the species whose reference spectrum is closest to the sample in question.","metadata":{}},{"cell_type":"code","source":"num_samples_per_species = 5000\nresults  = []\nfor num_reads in tqdm([100000,1000,100]):\n    row = [num_reads]\n    X,y = generate_dataset(num_reads, num_samples_per_species)\n    for p in [0,2,1,np.inf]:\n        distances = calculate_distances(X,fbc_spectra[boc_cols].to_numpy(),p)\n        preds = bacteria[np.argmin(distances,axis=1)]\n        row.append(np.sum(preds==y)/len(y))\n    results.append(row)","metadata":{"execution":{"iopub.status.busy":"2022-02-21T01:17:48.046824Z","iopub.execute_input":"2022-02-21T01:17:48.047129Z","iopub.status.idle":"2022-02-21T01:24:48.760324Z","shell.execute_reply.started":"2022-02-21T01:17:48.047094Z","shell.execute_reply":"2022-02-21T01:24:48.759148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results (accuracy) tabulated:","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(results,columns=['num_reads','KL Divergence','l_2 norm', 'l_1 norm', 'max norm'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-21T01:24:48.761583Z","iopub.execute_input":"2022-02-21T01:24:48.761866Z","iopub.status.idle":"2022-02-21T01:24:48.774052Z","shell.execute_reply.started":"2022-02-21T01:24:48.761829Z","shell.execute_reply":"2022-02-21T01:24:48.77306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nBased on this simulation in the ideal situation (no error reads), a simple distance-based classification does not give acceptable results for `num_reads` 1000 or 100. While efforts to model the error rates or to replicate the data generation model would be interesting from an intellectual perspective, it is unclear that classification would benefit from such knowledge alone, without substantial effort in feature engineering and/or discriminative modeling.\n","metadata":{}}]}