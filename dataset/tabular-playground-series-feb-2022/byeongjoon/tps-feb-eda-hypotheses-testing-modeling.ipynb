{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tabular Playground Series - Feb 2022\nCreated: 2022-02-08\n\n* Base EDA\n* Baseline Model\n* Define the hypotheses\n* Test the hypotheses\n\n#### LB\n* 2022-02-09: Baseline Model(Not tunning lightgbm): 0.93223\n* 2022-02-17: KNN Model: 0.97751\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-16T15:43:33.1648Z","iopub.execute_input":"2022-02-16T15:43:33.165117Z","iopub.status.idle":"2022-02-16T15:43:33.208319Z","shell.execute_reply.started":"2022-02-16T15:43:33.165036Z","shell.execute_reply":"2022-02-16T15:43:33.207778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN, KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nwarnings.filterwarnings(action='ignore')\ntf.debugging.set_log_device_placement(True)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:46:30.249131Z","iopub.execute_input":"2022-02-16T15:46:30.249405Z","iopub.status.idle":"2022-02-16T15:46:30.256911Z","shell.execute_reply.started":"2022-02-16T15:46:30.249379Z","shell.execute_reply":"2022-02-16T15:46:30.255846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA for baseline submit.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/tabular-playground-series-feb-2022/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/tabular-playground-series-feb-2022/test.csv\")\nsample_submission = pd.read_csv(\"/kaggle/input/tabular-playground-series-feb-2022/sample_submission.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:43:52.657975Z","iopub.execute_input":"2022-02-16T15:43:52.658361Z","iopub.status.idle":"2022-02-16T15:44:51.869796Z","shell.execute_reply.started":"2022-02-16T15:43:52.658326Z","shell.execute_reply":"2022-02-16T15:44:51.868535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for df in [train, test, sample_submission]:\n    print(df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:44:51.872236Z","iopub.execute_input":"2022-02-16T15:44:51.872485Z","iopub.status.idle":"2022-02-16T15:44:51.878926Z","shell.execute_reply.started":"2022-02-16T15:44:51.872419Z","shell.execute_reply":"2022-02-16T15:44:51.878293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:44:51.879725Z","iopub.execute_input":"2022-02-16T15:44:51.880736Z","iopub.status.idle":"2022-02-16T15:44:51.936653Z","shell.execute_reply.started":"2022-02-16T15:44:51.880709Z","shell.execute_reply":"2022-02-16T15:44:51.935795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:44:51.939044Z","iopub.execute_input":"2022-02-16T15:44:51.939249Z","iopub.status.idle":"2022-02-16T15:44:51.949493Z","shell.execute_reply.started":"2022-02-16T15:44:51.939227Z","shell.execute_reply":"2022-02-16T15:44:51.948382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### How many features are there?(변수의 개수는?)\n-> 286 (Except for row_id, target)","metadata":{}},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:44:51.950703Z","iopub.execute_input":"2022-02-16T15:44:51.950885Z","iopub.status.idle":"2022-02-16T15:44:51.962298Z","shell.execute_reply.started":"2022-02-16T15:44:51.950863Z","shell.execute_reply":"2022-02-16T15:44:51.961099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### What is the distribution of targets?(타겟 클래스의 분포는?)\n-> 10% each for 10 classes.(10개 클래스가 각각 10%씩 균등하게 분포)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T10:41:48.222762Z","iopub.execute_input":"2022-02-08T10:41:48.224093Z","iopub.status.idle":"2022-02-08T10:41:48.229021Z","shell.execute_reply.started":"2022-02-08T10:41:48.224009Z","shell.execute_reply":"2022-02-08T10:41:48.227957Z"}}},{"cell_type":"code","source":"train[\"target\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:44:51.965856Z","iopub.execute_input":"2022-02-16T15:44:51.966136Z","iopub.status.idle":"2022-02-16T15:44:51.9986Z","shell.execute_reply.started":"2022-02-16T15:44:51.966108Z","shell.execute_reply":"2022-02-16T15:44:51.997631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"target\"].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:44:52.000127Z","iopub.execute_input":"2022-02-16T15:44:52.000421Z","iopub.status.idle":"2022-02-16T15:44:52.021925Z","shell.execute_reply.started":"2022-02-16T15:44:52.000387Z","shell.execute_reply":"2022-02-16T15:44:52.020752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There are features with missing values?(결측치 있는 칼럼이 있나?) \n-> Nothing.","metadata":{}},{"cell_type":"code","source":"train.isnull().sum().values","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:44:52.023493Z","iopub.execute_input":"2022-02-16T15:44:52.023834Z","iopub.status.idle":"2022-02-16T15:44:52.120239Z","shell.execute_reply.started":"2022-02-16T15:44:52.023811Z","shell.execute_reply":"2022-02-16T15:44:52.119559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### What's the data type of features?(칼럼의 자료형은?)\n-> It's all float(286개 칼럼 모두 float)","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:44:52.121346Z","iopub.execute_input":"2022-02-16T15:44:52.121535Z","iopub.status.idle":"2022-02-16T15:44:52.15356Z","shell.execute_reply.started":"2022-02-16T15:44:52.121513Z","shell.execute_reply":"2022-02-16T15:44:52.152629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"row_id\"].dtype","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:44:52.155653Z","iopub.execute_input":"2022-02-16T15:44:52.155875Z","iopub.status.idle":"2022-02-16T15:44:52.163799Z","shell.execute_reply.started":"2022-02-16T15:44:52.155847Z","shell.execute_reply":"2022-02-16T15:44:52.163122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[\"target\"].dtype","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:44:52.164979Z","iopub.execute_input":"2022-02-16T15:44:52.165151Z","iopub.status.idle":"2022-02-16T15:44:52.176847Z","shell.execute_reply.started":"2022-02-16T15:44:52.165126Z","shell.execute_reply":"2022-02-16T15:44:52.176326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Is the distribution of train and test data the same?\n-> There are features that t-test p-value is under 0.05(139 of 286 features)","metadata":{}},{"cell_type":"code","source":"unequal_variance = []\nfor idx, col in enumerate(train.columns[1:-1]):\n    p_value = stats.ttest_ind(\n        train[train.columns[1:-1]][col],\n        test[test.columns[1:]][col]\n    ).pvalue\n    if p_value <0.05:\n        print(f\"[{idx+1}]{col}\\'s p-value: {p_value:.3f}\")\n        unequal_variance.append(col)\n\nprint(len(unequal_variance))","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:44:52.177976Z","iopub.execute_input":"2022-02-16T15:44:52.178323Z","iopub.status.idle":"2022-02-16T15:45:24.86303Z","shell.execute_reply.started":"2022-02-16T15:44:52.178296Z","shell.execute_reply":"2022-02-16T15:45:24.862199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline Model: Not tunning lightgbm","metadata":{}},{"cell_type":"code","source":"%%time\n\n# K-Fold Cross validation\nkfold = KFold(n_splits=5, random_state=0, shuffle=True)\n\n\n# Baseline Model\ndef get_model(model, x, y, idx_df):   \n    train_idx, test_idx = idx_df\n    train_x, test_x = x.iloc[train_idx], x.iloc[test_idx]\n    train_y, test_y = y.iloc[train_idx], y.iloc[test_idx]\n\n    print(\"train start\")\n    model.fit(\n        train_x,\n        train_y,\n        eval_set=[(test_x,test_y)],\n        eval_metric=\"multi_logloss\",\n        early_stopping_rounds=30,\n        verbose=True\n    )\n    \n    pred = model.predict(test_x)\n    print(f\"accuracy: {accuracy_score(test_y, pred):.2f}\")\n    return model\n\n\n# Plot validation logloss\ndef plot_eval(model, model_name=\"\"):\n    pd.DataFrame(model._evals_result[\"valid_0\"].values()).T.plot(\n        title=f\"{model_name} logloss line plot\",\n        xlabel=\"Rounds\",\n        ylabel=\"Logloss\",\n        grid=True,\n        legend=False\n    )\n\nclf = lgb.LGBMClassifier(\n    objective=\"multiclass\",\n    learning_rate=0.05,\n    n_estimators=1000,\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:45:24.86424Z","iopub.execute_input":"2022-02-16T15:45:24.864426Z","iopub.status.idle":"2022-02-16T15:45:24.874589Z","shell.execute_reply.started":"2022-02-16T15:45:24.864404Z","shell.execute_reply":"2022-02-16T15:45:24.872523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1 = get_model(clf, train[train.columns[1:-1]], train[\"target\"], list(kfold.split(train[train.columns[1:-1]]))[0])\n# model_2 = get_model(clf, train[train.columns[1:-1]], train[\"target\"], list(kfold.split(train[train.columns[1:-1]]))[1])\n# model_3 = get_model(clf, train[train.columns[1:-1]], train[\"target\"], list(kfold.split(train[train.columns[1:-1]]))[2])\n# model_4 = get_model(clf, train[train.columns[1:-1]], train[\"target\"], list(kfold.split(train[train.columns[1:-1]]))[3])\n# model_5 = get_model(clf, train[train.columns[1:-1]], train[\"target\"], list(kfold.split(train[train.columns[1:-1]]))[4])","metadata":{"execution":{"iopub.status.busy":"2022-02-11T07:02:42.898253Z","iopub.execute_input":"2022-02-11T07:02:42.898579Z","iopub.status.idle":"2022-02-11T07:08:32.275591Z","shell.execute_reply.started":"2022-02-11T07:02:42.898544Z","shell.execute_reply":"2022-02-11T07:08:32.274059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_eval(model_1)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T07:08:32.277923Z","iopub.execute_input":"2022-02-11T07:08:32.278192Z","iopub.status.idle":"2022-02-11T07:08:32.50077Z","shell.execute_reply.started":"2022-02-11T07:08:32.278159Z","shell.execute_reply":"2022-02-11T07:08:32.499626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Features importance\nConfirmed that each feature has a different effect on the model.(변수마다 모델에 주는 영향이 다른 것을 확인)","metadata":{}},{"cell_type":"code","source":"lgb.plot_importance(model_1);","metadata":{"execution":{"iopub.status.busy":"2022-02-11T07:09:41.871289Z","iopub.execute_input":"2022-02-11T07:09:41.871584Z","iopub.status.idle":"2022-02-11T07:09:46.553552Z","shell.execute_reply.started":"2022-02-11T07:09:41.871552Z","shell.execute_reply":"2022-02-11T07:09:46.552695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hypotheses & Test\n* ~~If scaling, classification is better.(스케일링을하면, 분류를 더 잘한다.~~ -> Reject\n\n* ~~If I handle outliers, classification is better.(이상치를 처리해주면, 분류를 더 잘한다.)~~ -> Reject\n* ~~If I apply pca, classification is better.(pca를 하면, 분류를 더 잘한다.)~~ -> Reject\n* ~~DNN is better at classifying. (DNN이 분류를 더 잘한다.)~~ -> Reject\n* ~~If I apply ensemble, classification is better.(앙상블을 하면, 분류가 더 잘된다.)~~ -> Reject\n* If I use KNN, classification is better. [(Reference)](https://www.kaggle.com/leehomhuang/simple-k-neighbors)(KNN을 쓰면, 분류를 더 잘한다.) -> Accept\n* If I use Random Forest, classification is better.(랜덤포레스트를 쓰면, 분류를 더 잘한다.)","metadata":{}},{"cell_type":"markdown","source":"### H1: If scaling, classification is better.(스케일링을 하면, 분류를 더 잘한다.) -> Reject\nbaseline accuracy: 0.93  \nstandard scaling accuracy: 0.93  \nmin max scaling accuracy: 0.92","metadata":{}},{"cell_type":"code","source":"standard_scaler = preprocessing.StandardScaler()\nstandard_x = standard_scaler.fit_transform(train[train.columns[1:-1]])\n\nget_model(\n    clf,\n    pd.DataFrame(standard_x),\n    train[\"target\"],\n    list(kfold.split(standard_x))[0]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T06:31:26.742784Z","iopub.execute_input":"2022-02-11T06:31:26.743109Z","iopub.status.idle":"2022-02-11T06:32:59.802115Z","shell.execute_reply.started":"2022-02-11T06:31:26.743079Z","shell.execute_reply":"2022-02-11T06:32:59.801189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_max_scaler = preprocessing.MinMaxScaler()\nmin_max_x = min_max_scaler.fit_transform(train[train.columns[1:-1]])\n\nget_model(\n    clf,\n    pd.DataFrame(min_max_x),\n    train[\"target\"],\n    list(kfold.split(min_max_x)[0]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T05:21:15.978392Z","iopub.execute_input":"2022-02-11T05:21:15.979378Z","iopub.status.idle":"2022-02-11T05:22:26.040516Z","shell.execute_reply.started":"2022-02-11T05:21:15.979318Z","shell.execute_reply":"2022-02-11T05:22:26.039236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### H1: If I handle outliers, classification is better.(이상치를 처리하면, 분류가 더 잘된다.) -> Reject\nbaseline accuracy: 0.93  \nAdjust outlies accuracy: 0.92\n","metadata":{}},{"cell_type":"markdown","source":"Confirmed that there are features with outliers.(이상치가 있는 속성이 존재하는 것을 확인)","metadata":{}},{"cell_type":"code","source":"train[train.columns[1:-1]].plot(kind=\"box\", figsize=(30,10))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T05:40:29.741609Z","iopub.execute_input":"2022-02-11T05:40:29.741937Z","iopub.status.idle":"2022-02-11T05:40:42.596141Z","shell.execute_reply.started":"2022-02-11T05:40:29.741904Z","shell.execute_reply":"2022-02-11T05:40:42.592282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Adjust outlies to IQR range.(이상치를 사분위수 범위로 조정해준다.)","metadata":{}},{"cell_type":"code","source":"def replace_outlier_iqr_range(df):\n    for col in df.columns:\n        q1 = df[col].quantile(0.25)\n        q3 = df[col].quantile(0.75)\n        iqr = q3 - q1\n\n        df[col].loc[df[col] > q3+1.5*iqr] = q3+1.5*iqr\n        df[col].loc[df[col] < q1-1.5*iqr] = q1-1.5*iqr\n    return df\n\ndf_iqr_range = replace_outlier_iqr_range(train[train.columns[1:-1]])\n\ndf_iqr_range.plot(kind=\"box\", figsize=(30,10))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T06:09:15.839277Z","iopub.execute_input":"2022-02-11T06:09:15.839556Z","iopub.status.idle":"2022-02-11T06:09:25.276788Z","shell.execute_reply.started":"2022-02-11T06:09:15.839525Z","shell.execute_reply":"2022-02-11T06:09:25.275841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_model(\n    clf,\n    df_iqr_range,\n    train[\"target\"],\n    list(kfold.split(df_iqr_range))[0]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T06:11:12.373195Z","iopub.execute_input":"2022-02-11T06:11:12.373574Z","iopub.status.idle":"2022-02-11T06:12:44.467257Z","shell.execute_reply.started":"2022-02-11T06:11:12.373544Z","shell.execute_reply":"2022-02-11T06:12:44.466713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### H1: If I apply pca, classification is better.(pca를 하면, 분류를 더 잘한다.) -> Reject\nbaseline accuracy: 0.93  \nPCA accuracy: 0.88","metadata":{}},{"cell_type":"code","source":"standard_scaler = preprocessing.StandardScaler()\nstandard_x = standard_scaler.fit_transform(train[train.columns[1:-1]])\n\nn = 10\npca = PCA(n_components=n)\nprintcipal_components = pca.fit_transform(standard_x)\npca_df = pd.DataFrame(data=printcipal_components, columns = list(range(1,n+1)))","metadata":{"execution":{"iopub.status.busy":"2022-02-11T06:43:46.737309Z","iopub.execute_input":"2022-02-11T06:43:46.738191Z","iopub.status.idle":"2022-02-11T06:43:51.197531Z","shell.execute_reply.started":"2022-02-11T06:43:46.738149Z","shell.execute_reply":"2022-02-11T06:43:51.196788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"{sum(pca.explained_variance_ratio_)*100:.1f}% explain\")\npca_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T06:43:51.199217Z","iopub.execute_input":"2022-02-11T06:43:51.199511Z","iopub.status.idle":"2022-02-11T06:43:51.217721Z","shell.execute_reply.started":"2022-02-11T06:43:51.199473Z","shell.execute_reply":"2022-02-11T06:43:51.216909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_model(\n    clf,\n    pca_df,\n    train[\"target\"],\n    list(kfold.split(pca_df))[0]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T07:01:32.760567Z","iopub.execute_input":"2022-02-11T07:01:32.760842Z","iopub.status.idle":"2022-02-11T07:01:39.790723Z","shell.execute_reply.started":"2022-02-11T07:01:32.760813Z","shell.execute_reply":"2022-02-11T07:01:39.78997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### H1: DNN is better at classifying. (DNN이 분류를 더 잘한다.) --> Reject\n- Baseline LB Score: 0.93\n- DNN LB Score: 0.87","metadata":{}},{"cell_type":"code","source":"# KFold\ntrain_idx, test_idx = list(kfold.split(train[train.columns[1:-1]]))[0]\ntrain_x, test_x = train[train.columns[1:-1]].iloc[train_idx], train[train.columns[1:-1]].iloc[test_idx]\ntrain_y, test_y = pd.get_dummies(train[\"target\"]).iloc[train_idx], pd.get_dummies(train[\"target\"]).iloc[test_idx]\n\n# DNN\ndnn = Sequential()\ndnn.add(Dense(128, input_dim=train_x.shape[1], activation=\"relu\"))\ndnn.add(Dense(64, activation=\"relu\"))\ndnn.add(Dense(10, activation=\"softmax\"))\n\ndnn.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=\"adam\",\n#     optimizer=\"rmsprop\",\n    metrics=[\"accuracy\"]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:38:41.949859Z","iopub.execute_input":"2022-02-11T08:38:41.950589Z","iopub.status.idle":"2022-02-11T08:38:45.003134Z","shell.execute_reply.started":"2022-02-11T08:38:41.950541Z","shell.execute_reply":"2022-02-11T08:38:45.002258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with tf.device('/GPU:0'):\nhistory = dnn.fit(\n    train_x,\n    train_y,\n    epochs=30,\n    batch_size=32,\n    validation_data=(test_x, test_y)\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:44:44.521635Z","iopub.execute_input":"2022-02-11T08:44:44.522303Z","iopub.status.idle":"2022-02-11T08:50:37.019358Z","shell.execute_reply.started":"2022-02-11T08:44:44.522264Z","shell.execute_reply":"2022-02-11T08:50:37.018721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:50:46.759494Z","iopub.execute_input":"2022-02-11T08:50:46.759767Z","iopub.status.idle":"2022-02-11T08:50:46.979664Z","shell.execute_reply.started":"2022-02-11T08:50:46.759736Z","shell.execute_reply":"2022-02-11T08:50:46.979021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For DNN submit\n# pred_proba = dnn.predict(test[test.columns[1:]])\n# test_pred = train_y.columns[pred_proba.argmax(1)]\n# sub = sample_submission.copy()\n# sub[\"target\"] = test_pred\n# sub.to_csv(\"submission.csv\",index=None)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:21:33.424624Z","iopub.execute_input":"2022-02-11T08:21:33.424927Z","iopub.status.idle":"2022-02-11T08:21:37.720639Z","shell.execute_reply.started":"2022-02-11T08:21:33.424888Z","shell.execute_reply":"2022-02-11T08:21:37.719766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### H1: If I apply ensemble, classification is better. -> Reject\nbaseline LB Score: 0.93  \nEnsemble LB Score: 0.91","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:25:38.688224Z","iopub.execute_input":"2022-02-11T08:25:38.688875Z","iopub.status.idle":"2022-02-11T08:25:38.827395Z","shell.execute_reply.started":"2022-02-11T08:25:38.68882Z","shell.execute_reply":"2022-02-11T08:25:38.82616Z"}}},{"cell_type":"code","source":"baseline = get_model(clf, train[train.columns[1:-1]], train[\"target\"], list(kfold.split(train[train.columns[1:-1]]))[1])\n\n# Ensemble\npred_proba = (\n    baseline.predict_proba(test[test.columns[1:]])\n    + dnn.predict(test[test.columns[1:]])\n)/2\ntest_pred = baseline.classes_[pred_proba.argmax(1)]\n\n# sub = sample_submission.copy()\n# sub[\"target\"] = test_pred\n# sub.to_csv(\"submission.csv\",index=None)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T08:51:07.247502Z","iopub.execute_input":"2022-02-11T08:51:07.247766Z","iopub.status.idle":"2022-02-11T09:23:42.540643Z","shell.execute_reply.started":"2022-02-11T08:51:07.247734Z","shell.execute_reply":"2022-02-11T09:23:42.539871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### H1: If I use KNN, classification is better. [(Reference)](https://www.kaggle.com/leehomhuang/simple-k-neighbors)(KNN을 쓰면, 분류를 더 잘한다.) -> Accept\n\nBaseline LB Score: 0.93  \nKNN LB Score: 0.977","metadata":{}},{"cell_type":"code","source":"train_idx, test_idx = list(kfold.split(train[train.columns[1:-1]]))[0]\ntrain_x, test_x = train[train.columns[1:-1]].iloc[train_idx], train[train.columns[1:-1]].iloc[test_idx]\ntrain_y, test_y = train[\"target\"].iloc[train_idx], train[\"target\"].iloc[test_idx]\n\nknn = KNeighborsClassifier(n_neighbors=1, p=2)\nknn.fit(train_x, train_y)\n\npred = knn.predict(test_x)\nprint(f\"accuracy: {accuracy_score(test_y, pred):.2f}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:53:46.012281Z","iopub.execute_input":"2022-02-16T15:53:46.012866Z","iopub.status.idle":"2022-02-16T15:56:52.673777Z","shell.execute_reply.started":"2022-02-16T15:53:46.012829Z","shell.execute_reply":"2022-02-16T15:56:52.672732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For KNN submit\ntest_pred = knn.predict(test[test.columns[1:]])\nsub = sample_submission.copy()\nsub[\"target\"] = test_pred\nsub.to_csv(\"submission.csv\",index=None)","metadata":{"execution":{"iopub.status.busy":"2022-02-16T15:58:06.661549Z","iopub.execute_input":"2022-02-16T15:58:06.661843Z","iopub.status.idle":"2022-02-16T16:27:09.174802Z","shell.execute_reply.started":"2022-02-16T15:58:06.661814Z","shell.execute_reply":"2022-02-16T16:27:09.173668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### H1: If I use Random Forest, classification is better.(랜덤포레스트를 쓰면, 분류를 더 잘한다.)","metadata":{}},{"cell_type":"markdown","source":"## Predict the test data to submission \n","metadata":{}},{"cell_type":"code","source":"# Ensemble\n# pred_proba = (\n#     model_1.predict_proba(test[test.columns[1:]])\n#     + model_2.predict_proba(test[test.columns[1:]])\n#     + model_3.predict_proba(test[test.columns[1:]])\n#     + model_4.predict_proba(test[test.columns[1:]])\n#     + model_5.predict_proba(test[test.columns[1:]])\n# )/5\n# test_pred = model_1.classes_[pred_proba.argmax(1)]\n\n# one model\ntest_pred = model_1.predict(test[test.columns[1:]])\n\nsub = sample_submission.copy()\nsub[\"target\"] = test_pred\nsub.to_csv(\"submission.csv\",index=None)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T07:11:35.742046Z","iopub.execute_input":"2022-02-11T07:11:35.742305Z","iopub.status.idle":"2022-02-11T07:11:40.671643Z","shell.execute_reply.started":"2022-02-11T07:11:35.742278Z","shell.execute_reply":"2022-02-11T07:11:40.67058Z"},"trusted":true},"execution_count":null,"outputs":[]}]}