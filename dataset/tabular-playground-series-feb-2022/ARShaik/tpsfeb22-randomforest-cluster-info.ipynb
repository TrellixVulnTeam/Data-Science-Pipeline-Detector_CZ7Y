{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## This notebook presents application of Randomforest technique to the current problem\n###  preprocessing is taken from notebook of @amrbosm\n### https://www.kaggle.com/ambrosm/tpsfeb22-02-postprocessing-against-the-mutants","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom math import factorial\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.cluster import KMeans","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-feb-2022/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-feb-2022/test.csv')\n\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"elements = [e for e in train_df.columns if e != 'row_id' and e != 'target']\n\n# Convert the 10 bacteria names to the integers 0 .. 9\nle = LabelEncoder()\ntrain_df['target_num'] = le.fit_transform(train_df.target)\n\ntrain_df.shape, test_df.shape","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bias(w, x, y, z):\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ndef bias_of(s):\n    w = int(s[1:s.index('T')])\n    x = int(s[s.index('T')+1:s.index('G')])\n    y = int(s[s.index('G')+1:s.index('C')])\n    z = int(s[s.index('C')+1:])\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ntrain_i = pd.DataFrame({col: ((train_df[col] + bias_of(col)) * 1000000).round().astype(int) for col in elements})\ntest_i = pd.DataFrame({col: ((test_df[col] + bias_of(col)) * 1000000).round().astype(int) for col in elements})\ntrain_i","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gcd_of_all(df_i):\n    gcd = df_i[elements[0]]\n    for col in elements[1:]:\n        gcd = np.gcd(gcd, df_i[col])\n    return gcd\n\ntrain_df['gcd'] = gcd_of_all(train_i)\ntest_df['gcd'] = gcd_of_all(test_i)\nnp.unique(train_df['gcd'], return_counts=True), np.unique(test_df['gcd'], return_counts=True)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for scale in np.sort(train_df['gcd'].unique()):\n    # Compute the PCA\n    pca = PCA(whiten=True, random_state=1)\n    pca.fit(train_i[elements][train_df['gcd'] == scale])\n\n    # Transform the data so that the components can be analyzed\n    Xt_tr = pca.transform(train_i[elements][train_df['gcd'] == scale])\n    Xt_te = pca.transform(test_i[elements][test_df['gcd'] == scale])\n\n    # Plot a scattergram, projected to two PCA components, colored by classification target\n    plt.figure(figsize=(6,6))\n    plt.scatter(Xt_tr[:,0], Xt_tr[:,1], c=train_df.target_num[train_df['gcd'] == scale], s=1)\n    plt.title(f\"{1000000 // scale} decamers ({(train_df['gcd'] == scale).sum()} samples with gcd = {scale})\")\n    plt.show()","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_duplicates_per_gcd(df, title):\n    plt.figure(figsize=(14, 3))\n    plt.tight_layout()\n    for i, gcd in enumerate(np.unique(df.gcd)):\n        plt.subplot(1, 4, i+1)\n        duplicates = df[df.gcd == gcd][elements].duplicated().sum()\n        non_duplicates = len(df[df.gcd == gcd]) - duplicates\n        plt.pie([non_duplicates, duplicates],\n                labels=['not duplicate', 'duplicate'],\n                colors=['gray', 'r'],\n                startangle=90)\n        plt.title(f\"GCD = {gcd}\")\n    plt.subplots_adjust(wspace=0.8)\n    plt.suptitle(title)\n    plt.show()\n        \nplot_duplicates_per_gcd(train_df, title=\"Duplicates in Training\")\nplot_duplicates_per_gcd(test_df, title=\"Duplicates in Test\")","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show how the spectra are scaled\n# The original spectrum gives a peak at 1.0;\n# the seven error rates give seven other peaks in the histograms\n# The eight peaks in the histograms correspond to the eight clusters \n# in the scattergram above\nv = train_df[elements].abs().sum(axis=1)\nchosen_gcd = 1\n\nplt.figure(figsize=(18, 14))\nplt.tight_layout()\nfor t in range(10): # loop over the ten bacteria species\n    plt.subplot(5, 2, t+1)\n    plt.title(le.inverse_transform([t])[0])\n    \n    # Select a single GCD and a single species\n    vt = v[(train_df['gcd'] == chosen_gcd) & (train_df['target_num'] == t)]\n    \n    # Do a one-dimensional clustering to get the cluster centers and the cluster sizes\n    km = KMeans(n_clusters=8)\n    km.fit(vt.values.reshape(-1, 1))\n    cluster_max = km.cluster_centers_.max() # label this cluster with 1.0 (no simulated errors)\n    print(sorted((km.cluster_centers_ / cluster_max).ravel().round(2)),\n          np.unique(km.predict(vt.values.reshape(-1, 1)), return_counts=True)[1][np.argsort(km.cluster_centers_.ravel())])\n\n    # Plot a histogram of the eight clusters\n    plt.hist(vt / cluster_max, bins=np.linspace(0, (vt / cluster_max).max(), 200), color='m', density=True)\n    plt.xticks(ticks=(km.cluster_centers_ / cluster_max).round(2))\n    plt.xlabel('scale')\n    plt.ylabel('density')\n    #plt.ylim(0, 1100)\nplt.subplots_adjust(hspace=0.5)\nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scale = 1\n\n# Compute the PCA\npca = PCA(whiten=True, random_state=1)\npca.fit(train_i[elements][train_df['gcd'] == scale])\n\n# Transform the data so that the components can be analyzed\nXt_tr = pca.transform(train_i[elements][train_df['gcd'] == scale])\nXt_te = pca.transform(test_i[elements][test_df['gcd'] == scale])\n\n# Plot a scattergram, projected to two PCA components, of training and test data\nplt.figure(figsize=(6,6))\nplt.scatter(Xt_tr[:,0], Xt_tr[:,1], c='b', s=1, label='Train')\nplt.scatter(Xt_te[:,0], Xt_te[:,1], c='r', s=1, label='Test')\nplt.title(\"The test data deviate from the training data\")\nplt.legend()\nplt.show()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=train_df.copy()\n\nX_train.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_train[\"Target_gcd\"] = X_train[\"target\"]+X_train[\"gcd\"].astype(str)\ny_train= X_train[\"target\"]+X_train[\"gcd\"].astype(str)\nX_train=X_train.drop(['target'], axis = 1)\nX_train=X_train.drop(['row_id','target_num'], axis = 1)\n\nX_train.head()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Try-1: use Randomforest classfifier with pre or post processing \n### hyper parameters are selected based on the gridsearch and applied in here\n### Public leaderboard score is 0.84....","metadata":{}},{"cell_type":"code","source":"#Import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\n#clf=RandomForestClassifier(n_estimators=100, min_samples_split=12, max_depth=12,min_samples_leaf=4,max_features=0.34966)\n#clf.fit(X_train,y_train)\n#test_df2=test_df.drop(['row_id'], axis = 1)\n#y_pred=clf.predict(test_df2)\n#output = pd.DataFrame({'row_id': test_df2.index, 'target': y_pred})\n#output.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Try2: applying strong crossvalidation technique for Randomforest and deducting duplicate data with sample weight\n### As suggested by @ambrosm training dataframe is converted to a new dataframe without the duplicated rows. To compensate for dropping the duplicates, we add a column sample_weight to the dataframe. This step is extremely important as most of the kagglers are removing the duplicates without applying sample weight.","metadata":{}},{"cell_type":"code","source":"\ntrain_df2 = pd.read_csv('../input/tabular-playground-series-feb-2022/train.csv', index_col='row_id')\ntest_df2 = pd.read_csv('../input/tabular-playground-series-feb-2022/test.csv', index_col='row_id')\n\n# Count the duplicates in the training data\ntrain_df2.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new dataframe without duplicates, but with an additional sample_weight column\nvc = train_df2.value_counts()\ndedup_train = pd.DataFrame([list(tup) for tup in vc.index.values], columns=train_df2.columns)\ndedup_train['sample_weight'] = vc.values\ndedup_train.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### After deduplicating the training data, we apply two small changes to the training loop:\n\n### When calling fit(), we add the sample weights of the training data.\n### When calling accuracy_score(), we add the sample weights of the validation data.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nTARGET = train_df2.columns.difference(test_df2.columns)[0]\nfeatures = train_df2.columns[train_df2.columns != TARGET]\n# Encoding categorical features\nle = LabelEncoder()\n\nX_dedup = dedup_train[features]\ny_dedup = pd.DataFrame(le.fit_transform(dedup_train[TARGET]), columns=[TARGET])\nsample_weight = dedup_train['sample_weight']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm\nN_SPLITS = 10\nfolds = StratifiedKFold(n_splits=N_SPLITS, shuffle=True)\ny_pred_list, y_proba_list, scores = [], [], []\n\nfor fold, (train_id, valid_id) in enumerate(tqdm(folds.split(X_dedup, y_dedup), total=N_SPLITS)):\n    print('####### Fold: ', fold)\n    \n    # Splitting\n    X_train, y_train, sample_weight_train = X_dedup.iloc[train_id], y_dedup.iloc[train_id], sample_weight.iloc[train_id]\n    X_valid, y_valid, sample_weight_valid = X_dedup.iloc[valid_id], y_dedup.iloc[valid_id], sample_weight.iloc[valid_id]\n    \n    model=RandomForestClassifier(n_estimators=100, min_samples_split=12, max_depth=26, min_samples_leaf=1,max_features=0.34966)\n    model.fit(X_train,y_train, sample_weight_train)\n    \n        \n    # Validation\n    valid_pred = model.predict(X_valid)\n    valid_score = accuracy_score(y_valid, valid_pred, sample_weight=sample_weight_valid)\n    print(f'Accuracy score: {valid_score:5f}\\n')\n    scores.append(valid_score)\n    \n    # Prediction for submission\n    y_pred_list.append(model.predict(test_df2))\n    y_proba_list.append(model.predict_proba(test_df2))\n    \nscore = np.array(scores).mean()\nprint(f'Mean accuracy score: {score:6f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Majority vote\nfrom scipy.stats import mode\ny_pred = mode(y_pred_list).mode[0]\ny_pred = le.inverse_transform(y_pred)\ntarget_distrib = pd.DataFrame({\n    'count': train_df2.target.value_counts(),\n    'share': train_df2[TARGET].value_counts() / train_df2.shape[0] * 100\n})\n\ntarget_distrib['pred_count'] = pd.Series(y_pred, index=test_df2.index).value_counts()\ntarget_distrib['pred_share'] = target_distrib['pred_count'] / len(test_df2) * 100\ntarget_distrib.sort_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_proba = sum(y_proba_list) / len(y_proba_list)\ny_proba += np.array([0.01, 0.02, 0.01, 0.015, 0.0015, 0.01, 0.01, 0.01, 0.00001, 0.0015])\ny_pred_tuned = le.inverse_transform(np.argmax(y_proba, axis=1))\npd.Series(y_pred_tuned, index=test_df2.index).value_counts().sort_index() / len(test_df2) * 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\nsubmission[TARGET] = y_pred_tuned\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## the public leaderboard score is 0.93052. I have noticed that most of the other kagglers also has similar score.\n## Next step use clustering information collected in my another notebook\nhttps://www.kaggle.com/abdulravoofshaik/clustering-extra-tree-lb-98-34","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom math import factorial\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.cluster import KMeans","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:57:11.164696Z","iopub.execute_input":"2022-02-24T06:57:11.165107Z","iopub.status.idle":"2022-02-24T06:57:11.784438Z","shell.execute_reply.started":"2022-02-24T06:57:11.164998Z","shell.execute_reply":"2022-02-24T06:57:11.783405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading train and testing datasets\noutput = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/sample_submission.csv')\ndedup_train = pd.read_csv('../input/tpsfeb2022-v2/train_cluster_target.csv')\ntest_df2 = pd.read_csv('../input/tpsfeb2022-v2/test_cluster_target.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:57:33.924755Z","iopub.execute_input":"2022-02-24T06:57:33.925503Z","iopub.status.idle":"2022-02-24T06:57:51.294109Z","shell.execute_reply.started":"2022-02-24T06:57:33.925451Z","shell.execute_reply":"2022-02-24T06:57:51.292441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df2.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:58:00.982001Z","iopub.execute_input":"2022-02-24T06:58:00.983234Z","iopub.status.idle":"2022-02-24T06:58:01.018443Z","shell.execute_reply.started":"2022-02-24T06:58:00.983137Z","shell.execute_reply":"2022-02-24T06:58:01.017826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_weight = dedup_train['sample_weight']\ndedup_train=dedup_train.drop(['sample_weight'],axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:58:02.446396Z","iopub.execute_input":"2022-02-24T06:58:02.446749Z","iopub.status.idle":"2022-02-24T06:58:02.555363Z","shell.execute_reply.started":"2022-02-24T06:58:02.446714Z","shell.execute_reply":"2022-02-24T06:58:02.554213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le2 = LabelEncoder()\ndedup_train['target']=le2.fit_transform(dedup_train['target'])\ndedup_train['target']","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:58:03.379874Z","iopub.execute_input":"2022-02-24T06:58:03.380951Z","iopub.status.idle":"2022-02-24T06:58:03.426985Z","shell.execute_reply.started":"2022-02-24T06:58:03.380854Z","shell.execute_reply":"2022-02-24T06:58:03.426288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nTARGET = 'target' #dedup_train.columns.difference(test_df.columns)[0]\nfeatures = dedup_train.columns[dedup_train.columns != TARGET]\n# Encoding categorical features\n#le = LabelEncoder()\n\nX_dedup = dedup_train[features]\ny_dedup = dedup_train[TARGET]     #pd.DataFrame(le.fit_transform(dedup_train[TARGET]), columns=[TARGET])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:58:04.288724Z","iopub.execute_input":"2022-02-24T06:58:04.289855Z","iopub.status.idle":"2022-02-24T06:58:04.385186Z","shell.execute_reply.started":"2022-02-24T06:58:04.289794Z","shell.execute_reply":"2022-02-24T06:58:04.383899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_dedup.head()\n#TARGET","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:58:05.697161Z","iopub.execute_input":"2022-02-24T06:58:05.697733Z","iopub.status.idle":"2022-02-24T06:58:05.706979Z","shell.execute_reply.started":"2022-02-24T06:58:05.697676Z","shell.execute_reply":"2022-02-24T06:58:05.705861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%%time\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tqdm import tqdm\nN_SPLITS = 5\nfolds = StratifiedKFold(n_splits=N_SPLITS, shuffle=True)\ny_pred_list, y_proba_list, scores = [], [], []\n\nfor fold, (train_id, valid_id) in enumerate(tqdm(folds.split(X_dedup, y_dedup), total=N_SPLITS)):\n    print('####### Fold: ', fold)\n    \n    # Splitting\n    X_train, y_train, sample_weight_train = X_dedup.iloc[train_id], y_dedup.iloc[train_id], sample_weight.iloc[train_id]\n    X_valid, y_valid, sample_weight_valid = X_dedup.iloc[valid_id], y_dedup.iloc[valid_id], sample_weight.iloc[valid_id]\n    \n    model=RandomForestClassifier(n_estimators=100, min_samples_split=12, max_depth=26, min_samples_leaf=1,max_features=0.34966)\n    model.fit(X_train,y_train, sample_weight_train)\n    \n        \n    # Validation\n    valid_pred = model.predict(X_valid)\n    valid_score = accuracy_score(y_valid, valid_pred, sample_weight=sample_weight_valid)\n    print(f'Accuracy score: {valid_score:5f}\\n')\n    scores.append(valid_score)\n    \n    # Prediction for submission\n    y_pred_list.append(model.predict(test_df2))\n    y_proba_list.append(model.predict_proba(test_df2))\n    \nscore = np.array(scores).mean()\nprint(f'Mean accuracy score: {score:6f}')","metadata":{"execution":{"iopub.status.busy":"2022-02-24T06:58:06.573376Z","iopub.execute_input":"2022-02-24T06:58:06.57424Z","iopub.status.idle":"2022-02-24T07:43:43.481417Z","shell.execute_reply.started":"2022-02-24T06:58:06.574196Z","shell.execute_reply":"2022-02-24T07:43:43.480512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Majority vote\nfrom scipy.stats import mode\ny_pred = mode(y_pred_list).mode[0]\ny_pred = le2.inverse_transform(y_pred)\ntarget_distrib2 = pd.DataFrame({\n    'count': dedup_train.target.value_counts(),\n    'share': dedup_train[TARGET].value_counts() / dedup_train.shape[0] * 100\n})\n\ntarget_distrib2['pred_count'] = pd.Series(y_pred, index=test_df2.index).value_counts().values\ntarget_distrib2['pred_share'] = target_distrib2['pred_count'] / len(test_df2) * 100\ntarget_distrib2.sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T07:57:06.360274Z","iopub.execute_input":"2022-02-24T07:57:06.360596Z","iopub.status.idle":"2022-02-24T07:57:09.830008Z","shell.execute_reply.started":"2022-02-24T07:57:06.360565Z","shell.execute_reply":"2022-02-24T07:57:09.829236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# As you can see above that even with clustering information, model failed to predict Escherichia_coli (see index=4 row) properly.\n# The conclusion is Randomforest is not suitable for this dataset","metadata":{}}]}