{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n\n!pip install scikit-learn-intelex\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport gc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:23:35.669003Z","iopub.execute_input":"2022-02-08T14:23:35.670641Z","iopub.status.idle":"2022-02-08T14:24:13.556813Z","shell.execute_reply.started":"2022-02-08T14:23:35.670441Z","shell.execute_reply":"2022-02-08T14:24:13.555559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/tabular-playground-series-feb-2022/train.csv\", index_col=0)\ntest_df = pd.read_csv(\"../input/tabular-playground-series-feb-2022/test.csv\", index_col=0)\n\nprint(f\"Nb samples in train: {train_df.shape[0]}\\nNb features in train: {train_df.shape[1]}\\nNb samples in test: {test_df.shape[0]}\\nNb features in test: {test_df.shape[1]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:24:13.559053Z","iopub.execute_input":"2022-02-08T14:24:13.559316Z","iopub.status.idle":"2022-02-08T14:24:56.108081Z","shell.execute_reply.started":"2022-02-08T14:24:13.559282Z","shell.execute_reply":"2022-02-08T14:24:56.106436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a quick look at the data:","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:24:56.111422Z","iopub.execute_input":"2022-02-08T14:24:56.111792Z","iopub.status.idle":"2022-02-08T14:24:56.17109Z","shell.execute_reply.started":"2022-02-08T14:24:56.111749Z","shell.execute_reply":"2022-02-08T14:24:56.170155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Total number of duplicated rows: {train_df.duplicated().sum()} out of {train_df.shape[0]}\")\ntrain_df = train_df.drop_duplicates()\nprint(f\"Total number of rows after removal: {train_df.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:24:56.173347Z","iopub.execute_input":"2022-02-08T14:24:56.173636Z","iopub.status.idle":"2022-02-08T14:25:02.270452Z","shell.execute_reply.started":"2022-02-08T14:24:56.173584Z","shell.execute_reply":"2022-02-08T14:25:02.269321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ax = train_df.target.value_counts(normalize=True).plot(kind='barh')\nax.set_xlabel('% of the total')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:25:02.272192Z","iopub.execute_input":"2022-02-08T14:25:02.272428Z","iopub.status.idle":"2022-02-08T14:25:02.637971Z","shell.execute_reply.started":"2022-02-08T14:25:02.272395Z","shell.execute_reply":"2022-02-08T14:25:02.636866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are a similar amount of samples for each type of bacteria, which makes an almost balanced dataset (one thing less to worry about). What about NaN's?","metadata":{}},{"cell_type":"code","source":"# To check if any NaNÂ´s in columns\n(train_df.isna().sum()).any()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:25:02.640043Z","iopub.execute_input":"2022-02-08T14:25:02.640642Z","iopub.status.idle":"2022-02-08T14:25:02.743371Z","shell.execute_reply.started":"2022-02-08T14:25:02.640576Z","shell.execute_reply":"2022-02-08T14:25:02.742343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Very basic EDA","metadata":{}},{"cell_type":"markdown","source":"Before starting with any kind of EDA, it is a good practice to split the data into train and test datasets, so we don't peek too much into our test set and avoiding any possible overfitting at the beginning (I call it 'test', although in the competition the data that will be submitted is called test as well). Since the data is balanced, we don't need to use a stratified splitting.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nX = train_df.drop(columns='target')\ny = pd.DataFrame(le.fit_transform(train_df.target), columns=['target'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2022)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:25:02.744982Z","iopub.execute_input":"2022-02-08T14:25:02.745606Z","iopub.status.idle":"2022-02-08T14:25:03.227169Z","shell.execute_reply.started":"2022-02-08T14:25:02.745528Z","shell.execute_reply":"2022-02-08T14:25:03.226221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run to print all the features\n#for col in X:\n#    print(col)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:25:03.228635Z","iopub.execute_input":"2022-02-08T14:25:03.229017Z","iopub.status.idle":"2022-02-08T14:25:03.232376Z","shell.execute_reply.started":"2022-02-08T14:25:03.228981Z","shell.execute_reply":"2022-02-08T14:25:03.231684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_mat = X_train.corr()\ncorr_val = corr_mat.values[np.tril(corr_mat, -1).astype(bool)]","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:25:03.233516Z","iopub.execute_input":"2022-02-08T14:25:03.233801Z","iopub.status.idle":"2022-02-08T14:25:21.880873Z","shell.execute_reply.started":"2022-02-08T14:25:03.23377Z","shell.execute_reply":"2022-02-08T14:25:21.879485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, axs = plt.subplots(1, 2, figsize=(18,7))\nsns.heatmap(corr_mat, ax=axs[0])\nsns.histplot(data=corr_val, kde=True, stat='probability', ax=axs[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:25:21.884193Z","iopub.execute_input":"2022-02-08T14:25:21.88521Z","iopub.status.idle":"2022-02-08T14:25:24.589802Z","shell.execute_reply.started":"2022-02-08T14:25:21.885155Z","shell.execute_reply":"2022-02-08T14:25:24.588889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The heatmap for the correlation map shows kind of a checkerboard pattern that must be related to relationships between neighbours (i.e., A1T1G4C4 is correlated to its neighbours, A1T1G3C5 and A1T1G5C3). The distribution of the correlation coefficients (fig. right) seems to have a longer tail on the positive values, up to 0.8, while the negative correlation coefficients don't reach 0.6. Interestingly, these inverse correlations appear on the heatmap as well, especially between distant areas. This could indicate that the more differents the two strings there is a chance of low correlation or of inverse correlation. It can be interesting to look at the specific relationships that hold these patterns. ","metadata":{}},{"cell_type":"markdown","source":"\nI'm wondering... what if we plot the Hamming distance between strings?","metadata":{}},{"cell_type":"code","source":"def hamming(a, b):\n    return len([i for i in filter(lambda x: x[0] != x[1], zip(a, b))])\n\nfeatures = [col for col in X]\n\nhamm_d = np.zeros((len(features), len(features)))\nfor i, ft1 in enumerate(features):\n    for j, ft2 in enumerate(features):\n        hamm_d[i, j] = hamming(ft1,ft2)\n\nhamm_d = pd.DataFrame(hamm_d, index=features, columns=features)\n\nf = plt.figure(figsize=(9, 7))\nsns.heatmap(hamm_d, square=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:25:24.591219Z","iopub.execute_input":"2022-02-08T14:25:24.59179Z","iopub.status.idle":"2022-02-08T14:25:26.01431Z","shell.execute_reply.started":"2022-02-08T14:25:24.591745Z","shell.execute_reply":"2022-02-08T14:25:26.012985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown in the figure, the patches of high correlation in the previous heatmap kind of match the strings with a lower Hamming distance. What about the inverse correlation? It might be interesting to explore that path...","metadata":{}},{"cell_type":"markdown","source":"## Model: ExtraTreesClassifier","metadata":{}},{"cell_type":"markdown","source":"### Cross-Validation","metadata":{}},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:25:26.015846Z","iopub.execute_input":"2022-02-08T14:25:26.016133Z","iopub.status.idle":"2022-02-08T14:25:26.031704Z","shell.execute_reply.started":"2022-02-08T14:25:26.016089Z","shell.execute_reply":"2022-02-08T14:25:26.029394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first time I tried to predict on this dataset I used a 5-CV and XGBoost getting pretty good results in CV and test (my test dataset). I saw other kernels using Extra Trees (i.e. this one https://www.kaggle.com/maxencefzr/tps-feb22-eda-extratrees) and achieving better results, so I decided to go in that direction too. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"etc_params = {\n    'n_estimators': 300,\n    'n_jobs': -1,\n    'bootstrap': False,\n    'verbose': 0,\n    'random_state': 2022\n}\ny_probs = []\ny_preds = []\nperf = []\ncv = KFold(n_splits=10, shuffle=True, random_state=2022)\nfor fold, (train_idx, valid_idx) in enumerate(cv.split(X_train, y_train)):    \n    X_train_cv = X_train.iloc[train_idx] \n    y_train_cv = y_train.iloc[train_idx]  \n    X_valid = X_train.iloc[valid_idx]\n    y_valid = y_train.iloc[valid_idx]\n    \n    # train\n    clf = ExtraTreesClassifier(**etc_params)    \n    clf.fit(X_train_cv, y_train_cv)\n    \n    # predict\n    y_pred_cv = clf.predict(X_valid)\n    acc = accuracy_score(y_valid,  y_pred_cv)\n    perf.append(acc)\n    y_preds.append(y_pred_cv)\n    y_probs.append(clf.predict_proba(test_df))\n\n    \n    print(f\"CV - {fold+1}: {acc:.4f}\")\n    \nprint(f\"Average across folds: {np.mean(perf)}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:25:26.034314Z","iopub.execute_input":"2022-02-08T14:25:26.03532Z","iopub.status.idle":"2022-02-08T14:34:33.142191Z","shell.execute_reply.started":"2022-02-08T14:25:26.035254Z","shell.execute_reply":"2022-02-08T14:34:33.141087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test set","metadata":{}},{"cell_type":"code","source":"# Credit: https://www.kaggle.com/grfiv4/plot-a-confusion-matrix\n\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http://matplotlib.org/examples/color/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(10, 8))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=90)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.2f}; misclass={:0.2f}'.format(accuracy, misclass))\n    plt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-02-08T14:34:33.146338Z","iopub.execute_input":"2022-02-08T14:34:33.146651Z","iopub.status.idle":"2022-02-08T14:34:33.16679Z","shell.execute_reply.started":"2022-02-08T14:34:33.146615Z","shell.execute_reply":"2022-02-08T14:34:33.165774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\ny_pred = clf.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\n\nplot_confusion_matrix(cm           = cm, \n                      normalize    = False,\n                      target_names = le.classes_,\n                      title        = \"Confusion Matrix\")","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:34:33.168504Z","iopub.execute_input":"2022-02-08T14:34:33.169008Z","iopub.status.idle":"2022-02-08T14:34:35.580079Z","shell.execute_reply.started":"2022-02-08T14:34:33.168968Z","shell.execute_reply":"2022-02-08T14:34:35.57903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As remekkinas says, I also think this is overfitting the data\ny_prob = sum(y_probs) / len(y_probs)\n# The explanations for these numbers are in AMBROSM's code\ny_prob += np.array([0, 0, 0.01, 0.03, 0, 0, 0, 0, 0, 0])\ny_pred_tuned = le.inverse_transform(np.argmax(y_prob, axis=1))\npd.Series(y_pred_tuned, index=test_df.index).value_counts().sort_index() / len(test_df) * 100","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:34:35.581999Z","iopub.execute_input":"2022-02-08T14:34:35.582571Z","iopub.status.idle":"2022-02-08T14:34:35.642674Z","shell.execute_reply.started":"2022-02-08T14:34:35.582497Z","shell.execute_reply":"2022-02-08T14:34:35.641626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(\"../input/tabular-playground-series-feb-2022/sample_submission.csv\", index_col=0)\nsub['target'] = y_pred_tuned\nprint(sub.head(10))\nsub.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T14:34:35.644583Z","iopub.execute_input":"2022-02-08T14:34:35.64524Z","iopub.status.idle":"2022-02-08T14:34:36.021628Z","shell.execute_reply.started":"2022-02-08T14:34:35.645188Z","shell.execute_reply":"2022-02-08T14:34:36.020813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To make further progress I think I should get more insight into the kind of data we are dealing with. In my opinion it shows some particular relationships between features that should be addressed somehow. I'll try to follow that direction.","metadata":{}},{"cell_type":"markdown","source":"Thanks for reading and happy kaggling! :)","metadata":{}},{"cell_type":"markdown","source":"## Appendix: Why Extra Trees perform better?\n\nThe Extra Trees classifier, or Extremely Randomized is a meta estimator that fits a random number of trees to a random subsample of the dataset averaging the result to apply the logic \"the wisdom of the crowd\". XGBoost, on the other hand, is also an ensemble of weak learners, in this case also decision trees. However, in the case of the XGBoost, each tree (or each weak learner) is trained to \"correct\" the errors made by previous trees in the ensemble. In the ExtraTrees model, that part of the optimization is made randomly (the algorithm is not trying to find the optimal splitting point). The obvious result of this is that each tree has more freedom to deviate from the dominant structure underlying the data, and as you might imagine this is good in our case of highly correlated features. In a nutshell, in the bias-variance tradeoff, with Extra Trees we are choosing to lose flexibility in order to make a more robust model, i.e., less prone to overfitting.  \n\n\nSources:\n\nhttps://www.kaggle.com/questions-and-answers/196968\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n\nhttps://towardsdatascience.com/an-intuitive-explanation-of-random-forest-and-extra-trees-classifiers-8507ac21d54b","metadata":{}},{"cell_type":"markdown","source":"PS: I've run into this site recently and they have an AMAZING visual explanation of the bias-variance trade-off in case you are interested:\nhttps://mlu-explain.github.io/bias-variance/","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}