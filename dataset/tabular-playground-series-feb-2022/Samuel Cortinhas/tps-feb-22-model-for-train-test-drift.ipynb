{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"The objective is to classify 10 different bacteria species given genomic sequencing data. This data has been compressed so that for example ATATGGCCTT becomes A2T4G2C2. From this lossy data we need recover the genome fingerprint to find the corresponding bacteria. \n\n**Acknowledgements:**\n* [TPS - Feb 2022](https://www.kaggle.com/sfktrkl/tps-feb-2022) by [Safak Tukeli](https://www.kaggle.com/sfktrkl).\n* [TPSFEB22-01 EDA which makes sense](https://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense) by [AmbrosM](https://www.kaggle.com/ambrosm). \n* [Analysis of Identification Method for Bacterial Species and Antibiotic Resistance Genes Using Optical Data From DNA Oligomers](https://www.frontiersin.org/articles/10.3389/fmicb.2020.00257/full) by R. Wood et al. \n* [Semi-supervised Learning & ExtraTrees](https://www.kaggle.com/vpallares/semi-supervised-learning-extratrees/notebook?scriptVersionId=87570753) by [Vicente Pallares](https://www.kaggle.com/vpallares).\n* [TPS - Feb 2022 - Only 4 Features - A, T, C, G](https://www.kaggle.com/roberterffmeyer/tps-feb-2022-only-4-features-a-t-c-g/notebook) by [Robert Erffmeyer](https://www.kaggle.com/roberterffmeyer).","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"# Core\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import combinations\nimport math\nfrom math import factorial\nimport statistics\nimport scipy.stats\nfrom scipy.stats import pearsonr\nimport time\nfrom datetime import datetime\nimport matplotlib.dates as mdates\nimport dateutil.easter as easter\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, plot_roc_curve, roc_curve\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.decomposition import PCA\n\n# Models\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom lightgbm import LGBMRegressor, LGBMClassifier\n\n# Tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:13:40.325188Z","iopub.execute_input":"2022-02-27T12:13:40.325629Z","iopub.status.idle":"2022-02-27T12:13:49.839701Z","shell.execute_reply.started":"2022-02-27T12:13:40.325517Z","shell.execute_reply":"2022-02-27T12:13:49.83841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"**Load data**","metadata":{}},{"cell_type":"code","source":"# Save to df\ntrain_data=pd.read_csv('../input/tabular-playground-series-feb-2022/train.csv', index_col='row_id')\ntest_data=pd.read_csv('../input/tabular-playground-series-feb-2022/test.csv', index_col='row_id')\n\n# Shape and preview\nprint('Training data df shape:',train_data.shape)\nprint('Test data df shape:',test_data.shape)\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:13:49.842355Z","iopub.execute_input":"2022-02-27T12:13:49.842664Z","iopub.status.idle":"2022-02-27T12:14:30.482533Z","shell.execute_reply.started":"2022-02-27T12:13:49.842631Z","shell.execute_reply":"2022-02-27T12:14:30.481815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Missing values**","metadata":{}},{"cell_type":"code","source":"print('Number of missing values in training set:',train_data.isna().sum().sum())\nprint('')\nprint('Number of missing values in test set:',test_data.isna().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:14:30.483872Z","iopub.execute_input":"2022-02-27T12:14:30.484264Z","iopub.status.idle":"2022-02-27T12:14:30.718719Z","shell.execute_reply.started":"2022-02-27T12:14:30.484224Z","shell.execute_reply":"2022-02-27T12:14:30.717355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Duplicates**","metadata":{}},{"cell_type":"code","source":"print(f'Duplicates in training set: {train_data.duplicated().sum()}')\nprint('')\nprint(f'Duplicates in test set: {test_data.duplicated().sum()}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:14:30.722047Z","iopub.execute_input":"2022-02-27T12:14:30.72234Z","iopub.status.idle":"2022-02-27T12:14:34.031821Z","shell.execute_reply.started":"2022-02-27T12:14:30.722306Z","shell.execute_reply":"2022-02-27T12:14:34.029986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Describe**","metadata":{}},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:14:34.03509Z","iopub.execute_input":"2022-02-27T12:14:34.035464Z","iopub.status.idle":"2022-02-27T12:14:39.050532Z","shell.execute_reply.started":"2022-02-27T12:14:34.035418Z","shell.execute_reply":"2022-02-27T12:14:39.049685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Initial thoughts:*\n* This is a big dataset. It would be helpful to reduce the size for storage and compute reasons. \n* Each column represents one of 286 combinations for compressed genome sequence. They represent a one-hot encoding. It would be ideal to transform these into 4 columns where each one counts how many of A-G-C-T units appear. \n* There is a small range between min and max values and some of the data is negative which doesn't make much biological sense. It would be good clean this data and transform it. \n* Perhaps biological knowledge would be helpful here. I imagine some combinations are more likely than others (some maybe even impossible).\n* There are many duplicates, so we could save on computational cost by removing these.","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"**Target distribution**","metadata":{}},{"cell_type":"code","source":"# Figure\nplt.figure(figsize=(15,5))\n\n# Countplot\nsns.countplot(data=train_data, x='target')\n\n# Aesthetics\nplt.xticks(rotation=40, ha='right')\nplt.title('Target distribution')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:14:39.052228Z","iopub.execute_input":"2022-02-27T12:14:39.052511Z","iopub.status.idle":"2022-02-27T12:14:39.737909Z","shell.execute_reply.started":"2022-02-27T12:14:39.052477Z","shell.execute_reply":"2022-02-27T12:14:39.737043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target is (highly) balanced, which is great.","metadata":{}},{"cell_type":"markdown","source":"**PCA plot**","metadata":{}},{"cell_type":"code","source":"# PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(train_data.drop('target', axis=1))\n\n# Convert to data frame\nprincipal_df = pd.DataFrame(data = X_pca, columns = ['PC1', 'PC2'])\nprincipal_df = pd.concat([principal_df,pd.Series(LabelEncoder().fit_transform(train_data['target']))], axis=1)\n\n# Figure size\nplt.figure(figsize=(8,6))\n\n# Scatterplot\nplt.scatter(principal_df.iloc[:,0], principal_df.iloc[:,1], c=principal_df.iloc[:,2], s=5, cmap='tab10')\n\n# Aesthetics\nplt.title('PCA plot in 2D', fontsize=15)\nplt.xlabel('PC1', fontsize=15)\nplt.ylabel('PC2', fontsize=15)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:14:39.739442Z","iopub.execute_input":"2022-02-27T12:14:39.739921Z","iopub.status.idle":"2022-02-27T12:14:51.830641Z","shell.execute_reply.started":"2022-02-27T12:14:39.739862Z","shell.execute_reply":"2022-02-27T12:14:51.82972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quite a lot of overlap. However, AmbrosM managed to better separate the data by reverse engineering the pipeline from the original [paper](https://www.frontiersin.org/articles/10.3389/fmicb.2020.00257/full). We'll see this in a bit.","metadata":{"execution":{"iopub.status.busy":"2022-02-20T15:41:08.20549Z","iopub.execute_input":"2022-02-20T15:41:08.205792Z","iopub.status.idle":"2022-02-20T15:41:08.219093Z","shell.execute_reply.started":"2022-02-20T15:41:08.20576Z","shell.execute_reply":"2022-02-20T15:41:08.217637Z"}}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"**Reverse engineering**","metadata":{}},{"cell_type":"markdown","source":"The feature values were originally integers. The integers were divided by 1000000 and a constant was subtracted. We can reverse this process to get the integers back.","metadata":{}},{"cell_type":"code","source":"def bias_of(s):\n    w = int(s[1:s.index('T')])\n    x = int(s[s.index('T')+1:s.index('G')])\n    y = int(s[s.index('G')+1:s.index('C')])\n    z = int(s[s.index('C')+1:])\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\nelements = [e for e in train_data.columns if e != 'row_id' and e != 'target']\n\ntrain_data[elements] = pd.DataFrame({col: ((train_data[col] + bias_of(col)) * 1000000).round().astype(int)\n                        for col in elements})\ntest_data[elements] = pd.DataFrame({col: ((test_data[col] + bias_of(col)) * 1000000).round().astype(int)\n                       for col in elements})\n\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:14:51.83214Z","iopub.execute_input":"2022-02-27T12:14:51.832403Z","iopub.status.idle":"2022-02-27T12:15:32.203658Z","shell.execute_reply.started":"2022-02-27T12:14:51.832369Z","shell.execute_reply":"2022-02-27T12:15:32.202679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**GCD**","metadata":{}},{"cell_type":"markdown","source":"From AmbrosM\n> For every sample, the researchers did one of four things:\n> \n> 1. They put 1000000 decamers into the machine and saved the machine's output.\n> 2. They put 100000 decamers into the machine and multiplied the machine's output by 10.\n> 3. They put 1000 decamers into the machine and multiplied the machine's output by 1000.\n> 4. They put 100 decamers into the machine and multiplied the machine's output by 10000.\n> \n> With this procedure, the row sums are always 1000000 and we get gcd values of 1, 10, 1000 and 10000.","metadata":{}},{"cell_type":"code","source":"# Create gcd feature\ntrain_data['gcd'] = np.gcd.reduce(train_data[elements], axis=1)\ntest_data['gcd'] = np.gcd.reduce(test_data[elements], axis=1)\n\n# Print unique gcd values and their distributions\nnp.unique(train_data['gcd'], return_counts=True), np.unique(test_data['gcd'], return_counts=True)","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-02-27T12:15:32.205045Z","iopub.execute_input":"2022-02-27T12:15:32.205286Z","iopub.status.idle":"2022-02-27T12:15:34.038925Z","shell.execute_reply.started":"2022-02-27T12:15:32.205246Z","shell.execute_reply":"2022-02-27T12:15:34.037774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PCA**","metadata":{}},{"cell_type":"markdown","source":"The higher the gcd, the more the noise is amplified by the scalling factor. So we can expect the variance of the data to increase. We can visualise this by doing pca plots for different gcd values. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,12))\n\n# Train PCA\npca=PCA(n_components=2)\npca.fit(train_data.loc[train_data['gcd'] == 1, elements])\n\nfor i, scale in enumerate(np.sort(train_data['gcd'].unique())):\n    # Transform data onto PCA space\n    X_pca=pca.transform(train_data.loc[train_data['gcd'] == scale, elements])\n    \n    # Convert to data frame\n    principal_df = pd.DataFrame(data = X_pca, columns = ['PC1', 'PC2'])\n    principal_df = pd.concat([principal_df,pd.Series(LabelEncoder().fit_transform(train_data[train_data['gcd'] == scale]['target']))], axis=1)\n    \n    # Plot pca\n    ax=plt.subplot(2, 2, i+1)\n    plt.scatter(principal_df.iloc[:,0], principal_df.iloc[:,1], axes=ax, c=principal_df.iloc[:,2], s=2, cmap='tab10')\n    plt.title(f\"gcd={scale}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:15:34.04253Z","iopub.execute_input":"2022-02-27T12:15:34.042788Z","iopub.status.idle":"2022-02-27T12:15:43.99477Z","shell.execute_reply.started":"2022-02-27T12:15:34.042752Z","shell.execute_reply":"2022-02-27T12:15:43.993665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Duplicates on a gcd basis**","metadata":{}},{"cell_type":"code","source":"def plot_duplicates_per_gcd(df, title):\n    plt.figure(figsize=(14, 3))\n    plt.tight_layout()\n    for i, gcd in enumerate(np.unique(df.gcd)):\n        plt.subplot(1, 4, i+1)\n        duplicates = df[df.gcd == gcd][elements].duplicated().sum()\n        non_duplicates = len(df[df.gcd == gcd]) - duplicates\n        plt.pie([non_duplicates, duplicates],\n                labels=['not duplicate', 'duplicate'],\n                colors=['lightgray', 'b'],\n                startangle=90)\n        plt.title(f'GCD = {gcd}')\n    plt.subplots_adjust(wspace=0.8)\n    plt.suptitle(title)\n    plt.show()\n        \nplot_duplicates_per_gcd(train_data, title='Duplicates in Training')\nplot_duplicates_per_gcd(test_data, title='Duplicates in Test')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:15:43.996348Z","iopub.execute_input":"2022-02-27T12:15:43.997478Z","iopub.status.idle":"2022-02-27T12:15:48.340534Z","shell.execute_reply.started":"2022-02-27T12:15:43.997433Z","shell.execute_reply":"2022-02-27T12:15:48.336345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The number of duplicates for gcd 1 or 10 is very small, whereas for gcd 1000 or 10000 the proportion of duplicates is over 50%! Perhaps we could use pseudolabels to improve the accuracy on predictions for gcd 1000 and 10000.","metadata":{}},{"cell_type":"markdown","source":"**Drop duplicates**","metadata":{}},{"cell_type":"code","source":"# Drop duplicates to save on computing time\ntrain_data=train_data.drop_duplicates(keep='first')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:15:48.343075Z","iopub.execute_input":"2022-02-27T12:15:48.344014Z","iopub.status.idle":"2022-02-27T12:15:50.725937Z","shell.execute_reply.started":"2022-02-27T12:15:48.34395Z","shell.execute_reply":"2022-02-27T12:15:50.724845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Distribution on a gcd basis**","metadata":{}},{"cell_type":"markdown","source":"Let's check the target distribution is also balanced within each gcd subset after duplicates have been removed. This would make it feasible to train 4 separate classifiers for each gcd value.","metadata":{}},{"cell_type":"code","source":"# Figure\nplt.figure(figsize=(12,8))\n\nfor index, i in enumerate([1, 10, 1000, 10000]):\n    ax=plt.subplot(2,2,index+1)\n    sns.countplot(data=train_data[train_data['gcd']==1], x='target', axes=ax)\n    ax.set(xticklabels=[])\n    ax.set(xlabel=None)\n    ax.set(ylabel=None)\n    plt.title(f'gcd={i}')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:15:50.72721Z","iopub.execute_input":"2022-02-27T12:15:50.72744Z","iopub.status.idle":"2022-02-27T12:15:51.81708Z","shell.execute_reply.started":"2022-02-27T12:15:50.727411Z","shell.execute_reply":"2022-02-27T12:15:51.816104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target is still highly balanced within each gcd group so it is possible to create a model for each subset.","metadata":{}},{"cell_type":"markdown","source":"# Measurement errors","metadata":{}},{"cell_type":"markdown","source":"**Train test drift**","metadata":{}},{"cell_type":"markdown","source":"The test set has a slightly different distribution to the train set. We need to understand this to improve the accuracy of our models.","metadata":{}},{"cell_type":"code","source":"# From https://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense#Comparing-train-and-test\nscale = 1\n\n# Compute the PCA\npca = PCA(n_components=2)\npca.fit(train_data[elements][train_data['gcd'] == scale])\n\n# Transform the data so that the components can be analyzed\nXt_tr = pca.transform(train_data[elements][train_data['gcd'] == scale])\nXt_te = pca.transform(test_data[elements][test_data['gcd'] == scale])\n\n# Plot a scattergram, projected to two PCA components, of training and test data\nplt.figure(figsize=(6,6))\nplt.scatter(Xt_tr[:,0], Xt_tr[:,1], c='b', s=1, label='Train')\nplt.scatter(Xt_te[:,0], Xt_te[:,1], c='r', s=1, label='Test')\nplt.title(\"The test data deviate from the training data\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:15:51.818329Z","iopub.execute_input":"2022-02-27T12:15:51.81855Z","iopub.status.idle":"2022-02-27T12:15:57.504861Z","shell.execute_reply.started":"2022-02-27T12:15:51.818523Z","shell.execute_reply":"2022-02-27T12:15:57.50357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The researchers added noise to the test set to model mutations and measurement errors when reading the decamers. I read the paper and I believe they modelled this noise as follows:\n1. Mutations and measurement errors are combined into a single global error rate p (e.g. p=0.01).\n2. p gives the probablity that any decamer has been incorrectly read by the machine. \n3. So for every decamer, we can flip a coin with heads probablity p. If it comes up tails we say the decamer has been correctly read by the machine and leave it alone. If it comes up heads, we say the decamer is erroneous and sample a new one from the bias distribution. \n4. It gets a bit trickier when gcd>1 as the data has been scaled. For example, assume gcd=10 and the decamer count for that feature is 50; we flip 50/10=5 coins and see how many come up heads. Say 3 coins come up tails, then we say 3*10=30 of the 50 decamers were correctly read by the machine and the other 20 were not. We then sample from the bias distribution twice and scale the answer by the gcd (i.e. 10). This ensures that the rows still have the same gcd and still sum to 1000000.","metadata":{}},{"cell_type":"markdown","source":"**Bias distribution**","metadata":{}},{"cell_type":"code","source":"# Plot bias/noise distribution\nplt.figure(figsize=(12,5))\nplt.plot([bias_of(e) for e in elements])\nplt.title('Bias distribution (i.e. noise distribution)')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:16:03.932795Z","iopub.execute_input":"2022-02-27T12:16:03.933107Z","iopub.status.idle":"2022-02-27T12:16:04.234951Z","shell.execute_reply.started":"2022-02-27T12:16:03.933079Z","shell.execute_reply":"2022-02-27T12:16:04.233988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model for measurement errors**","metadata":{}},{"cell_type":"code","source":"def add_noise(train_data, elements, p):\n    '''Add noise to training data,\n    train_data : training set,\n    elements : features A0T0G0C10 to A10T0G0C0,\n    m : error rate.\n    '''\n    # Start time\n    start=time.time()\n    \n    # Bias distribution\n    grid_bias=[bias_of(e) for e in elements]\n    grid_index=np.arange(len(elements))\n\n    train_with_noise=train_data.copy()\n    \n    for i in train_with_noise.index:\n        incorrect_count=0\n        for j in elements:\n            # Simulate decamers incorrectly labelled because of noise\n            incorrect=np.random.binomial((train_with_noise.loc[i,j]/train_with_noise.loc[i,'gcd']).astype(int),p)\n\n            if incorrect>0:\n                # Subtract correctly labelled samples (in batches of gcd)\n                train_with_noise.loc[i,j]-=incorrect*train_with_noise.loc[i,'gcd']\n\n                # Total incorrectly labelled samples in row\n                incorrect_count+=incorrect*train_with_noise.loc[i,'gcd']\n\n        # Choose new samples (noise) according to bias distribution\n        noise_index=np.random.choice(grid_index, size=(incorrect_count/train_with_noise.loc[i,'gcd']).astype(int), p=grid_bias)\n\n        # Add the noise to the training set\n        unique_index, counts = np.unique(noise_index, return_counts=True)\n        train_with_noise.loc[i,[elements[u] for u in unique_index]]+=counts*train_with_noise.loc[i,'gcd']\n        \n        if (i%1000)==0:\n            print(f'Iteration:{i}', f'time: {np.round((time.time()-start)/60,2)} mins')\n\n    return train_with_noise","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:16:07.773437Z","iopub.execute_input":"2022-02-27T12:16:07.773758Z","iopub.status.idle":"2022-02-27T12:16:07.78831Z","shell.execute_reply.started":"2022-02-27T12:16:07.773723Z","shell.execute_reply":"2022-02-27T12:16:07.786995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create noisy train data\n#train_with_noise=add_noise(train_data, elements, 1e-2)\n\n# I ran the above function and saved it to this dataset\ntrain_with_noise=pd.read_csv('../input/tps-feb-22-train-set-with-measurement-errors/train_with_noise.csv')\ntrain_with_noise.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:18:01.191434Z","iopub.execute_input":"2022-02-27T12:18:01.191898Z","iopub.status.idle":"2022-02-27T12:18:05.733909Z","shell.execute_reply.started":"2022-02-27T12:18:01.191847Z","shell.execute_reply":"2022-02-27T12:18:05.732629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"code","source":"target_encoder = LabelEncoder()\ntrain_with_noise['target']=target_encoder.fit_transform(train_with_noise['target'])\ndef run_model(train_data, test_data, scale):\n    # Labels and features\n    X=train_data[train_data['gcd']==scale].copy()\n    y=X['target']\n    X=X.drop('target', axis=1)\n    \n    # Test subset\n    gcd_test=test_data[test_data['gcd']==scale]\n    y_preds_index=gcd_test.index\n    \n    # Initialise outputs\n    scores = []\n    y_probs = []\n    \n    # Cross-validation\n    folds = StratifiedKFold(n_splits=N_SPLITS, random_state=0, shuffle=True)\n    for fold, (train_id, test_id) in enumerate(folds.split(X, y)):\n        X_train = X.iloc[train_id]\n        y_train = y.iloc[train_id]\n        X_valid = X.iloc[test_id]\n        y_valid = y.iloc[test_id]\n        \n        # Model\n        model = ExtraTreesClassifier(\n            n_estimators=ESTIMATORS,\n            random_state=0,\n            n_jobs=-1\n        )\n        \n        # Train and predict\n        model.fit(X_train, y_train)\n        valid_pred = model.predict(X_valid)\n        valid_score = accuracy_score(y_valid, valid_pred)\n        \n        print('gcd:', scale,', fold:', fold + 1,', accuracy:', valid_score)\n        scores.append(valid_score)\n        \n        # Predict only on corresponding gcd subset in test_data\n        y_probs.append(model.predict_proba(gcd_test))\n        \n    # Mean of probabilities\n    y_proba=np.array(y_probs).sum(axis=0)/N_SPLITS\n    \n    # Mean accuracy\n    print(f'Mean accuracy score for gcd={scale}:', np.array(scores).mean())\n    \n    return y_proba, y_preds_index","metadata":{"_kg_hide-output":false,"execution":{"iopub.status.busy":"2022-02-27T12:18:05.735938Z","iopub.execute_input":"2022-02-27T12:18:05.736227Z","iopub.status.idle":"2022-02-27T12:18:05.790875Z","shell.execute_reply.started":"2022-02-27T12:18:05.736195Z","shell.execute_reply":"2022-02-27T12:18:05.7896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**gcd = 1, 10**","metadata":{}},{"cell_type":"code","source":"N_SPLITS = 5\nESTIMATORS = 1000","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:18:06.208352Z","iopub.execute_input":"2022-02-27T12:18:06.208905Z","iopub.status.idle":"2022-02-27T12:18:06.21262Z","shell.execute_reply.started":"2022-02-27T12:18:06.208868Z","shell.execute_reply":"2022-02-27T12:18:06.212067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_proba_concat = np.array([]).reshape(0,10)\ny_preds_index_concat = np.array([])\nfor scale in [1, 10]:\n    # Run model\n    y_proba, y_preds_index = run_model(train_with_noise, test_data, scale)\n    \n    # Store predictions and corresponding indices\n    y_proba_concat=np.concatenate((y_proba_concat, y_proba),axis=0)\n    y_preds_index_concat=np.concatenate((y_preds_index_concat, y_preds_index),axis=0)\n    \n# Recover class names\ny_preds=np.argmax(y_proba_concat, axis=1)\ny_preds=y_preds.astype(int)\ny_preds=target_encoder.inverse_transform(y_preds)\n\n# Save predictions to df\npreds_df=pd.DataFrame({'row_id': y_preds_index_concat.astype(int), 'target': y_preds})","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:18:09.210372Z","iopub.execute_input":"2022-02-27T12:18:09.211203Z","iopub.status.idle":"2022-02-27T12:18:17.338727Z","shell.execute_reply.started":"2022-02-27T12:18:09.211158Z","shell.execute_reply":"2022-02-27T12:18:17.337883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**gcd = 1000, 10000**","metadata":{}},{"cell_type":"code","source":"N_SPLITS = 10\nESTIMATORS = 3000","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:18:17.340219Z","iopub.execute_input":"2022-02-27T12:18:17.341065Z","iopub.status.idle":"2022-02-27T12:18:17.344752Z","shell.execute_reply.started":"2022-02-27T12:18:17.341024Z","shell.execute_reply":"2022-02-27T12:18:17.343849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_proba_concat2 = np.array([]).reshape(0,10)\ny_preds_index_concat2 = np.array([])\nfor scale in [1000, 10000]:\n    # Run model\n    y_proba, y_preds_index = run_model(train_with_noise, test_data, scale)\n    \n    # Store predictions and corresponding indices\n    y_proba_concat2=np.concatenate((y_proba_concat2, y_proba),axis=0)\n    y_preds_index_concat2=np.concatenate((y_preds_index_concat2, y_preds_index),axis=0)\n    \n# Recover class names\ny_preds2=np.argmax(y_proba_concat2, axis=1)\ny_preds2=y_preds2.astype(int)\ny_preds2=target_encoder.inverse_transform(y_preds2)\n\n# Save predictions to df\npreds_df=preds_df.append(pd.DataFrame({'row_id': y_preds_index_concat2.astype(int), 'target': y_preds2}))\npreds_df=preds_df.sort_values('row_id')","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:18:17.345985Z","iopub.execute_input":"2022-02-27T12:18:17.346226Z","iopub.status.idle":"2022-02-27T12:18:40.934215Z","shell.execute_reply.started":"2022-02-27T12:18:17.346197Z","shell.execute_reply":"2022-02-27T12:18:40.932902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check distribution of predictions**","metadata":{}},{"cell_type":"code","source":"# Compare distribution of predictions to training set\ntrain_share=pd.DataFrame({'share':100*train_data['target'].value_counts()/len(train_data)})\npreds_share=pd.DataFrame({'pred_share':100*preds_df['target'].value_counts()/len(preds_df)})\ndf_share=pd.concat([train_share,preds_share], axis=1)\ndf_share","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:18:40.936403Z","iopub.execute_input":"2022-02-27T12:18:40.936773Z","iopub.status.idle":"2022-02-27T12:18:40.988858Z","shell.execute_reply.started":"2022-02-27T12:18:40.936738Z","shell.execute_reply":"2022-02-27T12:18:40.988018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# Save to csv\npreds_df.to_csv('submission.csv', index=False)\n\n# Check format\npreds_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:18:51.922446Z","iopub.execute_input":"2022-02-27T12:18:51.923672Z","iopub.status.idle":"2022-02-27T12:18:52.196484Z","shell.execute_reply.started":"2022-02-27T12:18:51.923623Z","shell.execute_reply":"2022-02-27T12:18:52.195544Z"},"trusted":true},"execution_count":null,"outputs":[]}]}