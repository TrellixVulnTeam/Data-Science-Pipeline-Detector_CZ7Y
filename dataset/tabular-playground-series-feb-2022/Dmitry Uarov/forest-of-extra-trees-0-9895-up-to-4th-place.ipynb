{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport plotly as py\nfrom statistics import mean\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\nimport seaborn as sns\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom umap import UMAP\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport optuna\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input, Concatenate\n\nnp.random.seed(228)\ntf.random.set_seed(228)\n\npd.set_option('display.max_columns', None)\n\n#########################################################\n\ntrain = pd.read_csv('../input/tabular-playground-series-feb-2022/train.csv', index_col='row_id')\ntest = pd.read_csv('../input/tabular-playground-series-feb-2022/test.csv', index_col='row_id')\nss = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\n\n#########################################################\n\ndef reduce_mem_usage(df):\n\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-12T13:53:09.485751Z","iopub.execute_input":"2022-02-12T13:53:09.486124Z","iopub.status.idle":"2022-02-12T13:54:14.907224Z","shell.execute_reply.started":"2022-02-12T13:53:09.486091Z","shell.execute_reply":"2022-02-12T13:54:14.906116Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"**Basic information**","metadata":{}},{"cell_type":"code","source":"def info(data):\n    print(f'Length of data: {len(data)}')\n    print('')\n    \n    x = pd.Series([data[i].dtypes for i in data.columns.tolist()])\n    print(x.value_counts().to_frame().reset_index().rename(columns={0: 'count', 'index': 'type'}))\n    print('')\n    \n    if data.isna().sum().sum() > 0:\n        print(f'Missing values: {data.isna().sum().sum()} ({round(data.isna().sum().sum()/len(data)*100, 2)}%)')\n    else:\n        print(f'Missing values: False')\n    print('')\n    \n    if data.duplicated().sum() > 0:\n        print(f'Duplicated values: {data.duplicated().sum()} ({round(data.duplicated().sum()/len(data)*100, 2)}%)')\n    else:\n        print(f'Duplicated values: False')\n    print('')\n    \n    try:\n        print(f'Unique target values: {data[\"target\"].nunique()}')\n    except:\n        pass\n    \nprint('TRAINING DATASET INFORMATION')\nprint('')\ninfo(train)\nprint('--------------------------------------')\nprint('TEST DATASET INFORMATION')\nprint('')\ninfo(test)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-12T13:54:14.909773Z","iopub.execute_input":"2022-02-12T13:54:14.910494Z","iopub.status.idle":"2022-02-12T13:54:24.531969Z","shell.execute_reply.started":"2022-02-12T13:54:14.91044Z","shell.execute_reply":"2022-02-12T13:54:24.530852Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are a large number of duplicates in the data, they need to be dropped, but try take into account the important idea of [AmbrosM](https://www.kaggle.com/ambrosm) and add weights to duplicate observations.","metadata":{}},{"cell_type":"code","source":"# vc = train.value_counts()\n# train = pd.DataFrame([list(tup) for tup in vc.index.values], columns=train.columns)\n# train['sample_weight'] = vc.values","metadata":{"execution":{"iopub.status.busy":"2022-02-07T13:16:08.398073Z","iopub.execute_input":"2022-02-07T13:16:08.398339Z","iopub.status.idle":"2022-02-07T13:16:23.545736Z","shell.execute_reply.started":"2022-02-07T13:16:08.398304Z","shell.execute_reply":"2022-02-07T13:16:23.544975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"But I will also try without dropping duplicates and with dropping duplicates, but without sample weights.","metadata":{}},{"cell_type":"code","source":"#train.drop_duplicates(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:56:04.818829Z","iopub.execute_input":"2022-02-12T13:56:04.819695Z","iopub.status.idle":"2022-02-12T13:56:07.119304Z","shell.execute_reply.started":"2022-02-12T13:56:04.819648Z","shell.execute_reply":"2022-02-12T13:56:07.118398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Features**","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\ntrain[train.columns.tolist()[0:-1]].describe().transpose().sort_values('mean')[['mean', 'std', 'min', 'max']].style.background_gradient(cmap='Blues')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-12T13:55:20.904786Z","iopub.execute_input":"2022-02-12T13:55:20.905168Z","iopub.status.idle":"2022-02-12T13:55:35.514074Z","shell.execute_reply.started":"2022-02-12T13:55:20.905128Z","shell.execute_reply":"2022-02-12T13:55:35.51314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Target**","metadata":{}},{"cell_type":"code","source":"pd.options.display.max_rows = 60\n\nplt.figure(figsize = (12, 4))\nsns.set_style(\"white\")\nplt.title('Distribution of target without dropping duplicates', fontname = 'monospace', fontsize = 20, color = '#313233', x = 0.5, y = 1.1)\na = sns.barplot(data = train['target'].value_counts().reset_index(), x = 'target', y = 'index',\\\n                palette = (['#63a2eb', '#5ca0ed', '#5299eb', '#4a96ed', '#4091ed', '#378ced', '#2f88ed', '#2482ed', '#1a7ded', '#0e75eb'][::-1]),\\\n                linestyle = \"-\", linewidth = 1, edgecolor = \"black\")\nplt.xticks([])\nplt.yticks(fontname = 'monospace', size = 12, color = '#313233')\nplt.xlabel('')\nplt.ylabel('')\n\nfor j in ['right', 'top', 'left', 'bottom']:\n    a.spines[j].set_visible(False)\n    \nfor p in a.patches:\n    width = p.get_width()\n    plt.text(800 + width, p.get_y() + 0.55*p.get_height(), f'{round((width / len(train)) * 100, 2)}%',\n             ha = 'center', va = 'center', fontname = 'monospace', fontsize = 12, color = '#313233')\n        \nplt.show()","metadata":{"_kg_hide-input":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-02-12T13:55:41.531903Z","iopub.execute_input":"2022-02-12T13:55:41.532418Z","iopub.status.idle":"2022-02-12T13:55:41.872182Z","shell.execute_reply.started":"2022-02-12T13:55:41.532364Z","shell.execute_reply":"2022-02-12T13:55:41.871132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize = (12, 4))\nsns.set_style(\"white\")\nplt.title('Distribution of target with dropping duplicates', fontname = 'monospace', fontsize = 20, color = '#313233', x = 0.5, y = 1.1)\na = sns.barplot(data = train['target'].value_counts().reset_index(), x = 'target', y = 'index',\\\n                palette = (['#63a2eb', '#5ca0ed', '#5299eb', '#4a96ed', '#4091ed', '#378ced', '#2f88ed', '#2482ed', '#1a7ded', '#0e75eb'][::-1]),\\\n                linestyle = \"-\", linewidth = 1, edgecolor = \"black\")\nplt.xticks([])\nplt.yticks(fontname = 'monospace', size = 12, color = '#313233')\nplt.xlabel('')\nplt.ylabel('')\n\nfor j in ['right', 'top', 'left', 'bottom']:\n    a.spines[j].set_visible(False)\n    \nfor p in a.patches:\n    width = p.get_width()\n    plt.text(550 + width, p.get_y() + 0.55*p.get_height(), f'{round((width / len(train)) * 100, 2)}%',\n             ha = 'center', va = 'center', fontname = 'monospace', fontsize = 12, color = '#313233')\n        \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-12T13:56:15.51337Z","iopub.execute_input":"2022-02-12T13:56:15.514297Z","iopub.status.idle":"2022-02-12T13:56:15.75794Z","shell.execute_reply.started":"2022-02-12T13:56:15.51423Z","shell.execute_reply":"2022-02-12T13:56:15.75674Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retarget = {train['target'].value_counts().reset_index()['index'][i]: i for i in range(len(train['target'].value_counts()))}\nretarget2 = {i: k for k, i in retarget.items()}\ntrain['target'] = train['target'].map(retarget)\n\numap = UMAP(n_components = 2, n_neighbors = 10, min_dist = 0.99).fit_transform(train.drop('target', axis = 1).sample(15000, random_state = 228), train['target'].sample(15000, random_state = 228))\n\nplt.figure(figsize=(15, 12))\nplt.title('Target UMAP', size = 25, y = 1.03, fontname = 'monospace')\nscu = sns.scatterplot(x = umap[:, 0], y = umap[:, 1], hue = train['target'].sample(15000, random_state = 228), s = 5, edgecolor = 'none', alpha = 0.8,\\\n                     palette = ['#d15858', '#6b1e1e', '#7f2d7d', '#805ead', '#406eb3', '#31a2c4', '#3da69a', '#3da65b', '#4c822b', '#a89d38'])\nplt.xticks([])\nplt.yticks([])\nfor i in ['right', 'left', 'top']:\n    scu.spines[i].set_visible(False)\nplt.legend(ncol = 2, borderpad = 1, frameon = True, fontsize = 11)\nscu.text(12, -23, '''n_components = 2\nn_neighbors = 10\nmin_dist = 0.99''', fontname = 'monospace', fontsize = 12)\nplt.legend(labels = list(retarget.keys()), title = \"Bacteria species\", ncol = 2, borderpad = 1, frameon = False, fontsize = 12, bbox_to_anchor = (0.5, 0))\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-12T13:56:55.100143Z","iopub.execute_input":"2022-02-12T13:56:55.100511Z","iopub.status.idle":"2022-02-12T13:57:34.485147Z","shell.execute_reply.started":"2022-02-12T13:56:55.100473Z","shell.execute_reply":"2022-02-12T13:57:34.484042Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"features = train.columns.tolist()[0:-1]\n# sample_weight = train['sample_weight']\n\ntrain['std'] = train[features].std(axis = 1)\ntest['std'] = test[features].std(axis = 1)\n\ntrain['min'] = train[features].min(axis = 1)\ntest['min'] = test[features].min(axis = 1)\n\ntrain['max'] = train[features].max(axis = 1)\ntest['max'] = test[features].max(axis = 1)\n\nfeatures += ['std', 'min', 'max']\n\nle = LabelEncoder()\ntrain['target'] = le.fit_transform(train['target'])\n\nsc = StandardScaler()\ntrain[features] = sc.fit_transform(train[features])\ntest[features] = sc.transform(test[features])\n\nX = train[features]\ny = train['target']","metadata":{"execution":{"iopub.status.busy":"2022-02-12T11:45:09.624606Z","iopub.execute_input":"2022-02-12T11:45:09.62551Z","iopub.status.idle":"2022-02-12T11:45:42.899202Z","shell.execute_reply.started":"2022-02-12T11:45:09.625472Z","shell.execute_reply":"2022-02-12T11:45:42.898326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGB","metadata":{}},{"cell_type":"markdown","source":"I don't think that the results of LGBM or CB would be much different, so I decided to try XGB. The parameters were searched for of course with Optuna. For all trials I used the same parameters.\n\n1. First trial without dropping duplicates. LB result - **0.95025**.\n\n2. Second trial only with dropping duplicates. LB result - **0.94678**\n\n3. Third trial with dropping duplicates and sample weights. LB result - **0.94101**.\n\nIn the following models I will not try dropping duplicates and using sample weights. ","metadata":{}},{"cell_type":"code","source":"paramsXGB = {'max_depth': 8,\n             'learning_rate': 0.2478225904887278, \n             'min_child_weight': 8, \n             'gamma': 0.018329940112279165, \n             'alpha': 0.00019394894279195157, \n             'lambda': 0.06161761858777205, \n             'colsample_bytree': 0.6721122683333417, \n             'subsample': 0.6155733760919804,\n             'n_estimators': 3000,\n             'tree_method': 'gpu_hist',\n             'booster': 'gbtree',\n             'random_state': 228,\n             'use_label_encoder': False,\n             'objective': 'multi:softmax',\n             'eval_metric': 'mlogloss',\n             'predictor': 'gpu_predictor'}","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:52:14.597621Z","iopub.execute_input":"2022-02-07T14:52:14.597868Z","iopub.status.idle":"2022-02-07T14:52:14.603475Z","shell.execute_reply.started":"2022-02-07T14:52:14.597834Z","shell.execute_reply":"2022-02-07T14:52:14.602729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions, scores = [], []\n \nk = StratifiedKFold(n_splits = 10, random_state = 228, shuffle = True)\nfor i, (trn_idx, val_idx) in enumerate(k.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = XGBClassifier(**paramsXGB)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 30)\n    \n    val_pred = model.predict(X_val)\n    val_score = accuracy_score(y_val, val_pred)\n    print(f'Fold {i+1} accuracy score: {round(val_score, 4)}')\n    \n    scores.append(val_score)\n    predictions.append(model.predict(test))\nprint('')    \nprint(f'Mean accuracy - {round(mean(scores), 4)}')","metadata":{"execution":{"iopub.status.busy":"2022-02-07T14:52:14.605007Z","iopub.execute_input":"2022-02-07T14:52:14.605566Z","iopub.status.idle":"2022-02-07T14:59:54.005762Z","shell.execute_reply.started":"2022-02-07T14:52:14.605522Z","shell.execute_reply":"2022-02-07T14:59:54.004046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss['target'] = stats.mode(np.column_stack(predictions), axis=1)[0]\nss['target'] = ss['target'].map(retarget2)\nss.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T15:00:21.264636Z","iopub.execute_input":"2022-02-07T15:00:21.265144Z","iopub.status.idle":"2022-02-07T15:00:24.316353Z","shell.execute_reply.started":"2022-02-07T15:00:21.265083Z","shell.execute_reply":"2022-02-07T15:00:24.315596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# NN","metadata":{}},{"cell_type":"markdown","source":"1. First trial without dropping duplicates. In less than 50 epochs each fold of data has reached ~0.993 accuracy. Mean accuracy on 10 folds - **0.9961**. To avoid overfitting I tried low patience, but the LB result is very bad - **0.9124**\n2. Second trial with dropping duplicates. In less than 30 epochs each fold of data has reached ~0.97 accuracy. Mean accuracy on 10 folds - **0.971**. LB result - **0.8944**","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 512\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:58:21.30024Z","iopub.execute_input":"2022-02-12T13:58:21.300592Z","iopub.status.idle":"2022-02-12T13:58:21.325926Z","shell.execute_reply.started":"2022-02-12T13:58:21.300558Z","shell.execute_reply":"2022-02-12T13:58:21.325028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_model():\n    x_input = Input(shape=(X.shape[-1]), name=\"input\")\n    x1 = Dense(256, activation='selu')(x_input)\n    b1 = BatchNormalization()(x1)\n    x2 = Dense(128, activation='selu')(b1)\n    b2 = BatchNormalization()(x2)\n    x3 = Dense(128, activation='selu')(b1)\n    b3 = BatchNormalization()(x3)\n    \n    d1 = Dropout(0.15)(Concatenate()([b2, b3]))\n    x4 = Dense(128, activation='relu')(d1) \n    b4 = BatchNormalization()(x4)\n    x5 = Dense(64, activation='selu')(b4)\n    b5 = BatchNormalization()(x5)\n    x6 = Dense(32, activation='selu')(b5)\n    b6 = BatchNormalization()(x6)\n    output = Dense(10, activation=\"softmax\", name=\"output\")(b6)\n    \n    model = tf.keras.Model(x_input, output, name='DNN_Model')\n    return model\n\nmodel = my_model()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-12T13:58:33.085301Z","iopub.execute_input":"2022-02-12T13:58:33.086319Z","iopub.status.idle":"2022-02-12T13:58:33.431433Z","shell.execute_reply.started":"2022-02-12T13:58:33.08626Z","shell.execute_reply":"2022-02-12T13:58:33.430385Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(\n    model, \n    to_file='Super_Model.png', \n    show_shapes=True,\n    show_layer_names=True\n)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-12T13:58:39.520623Z","iopub.execute_input":"2022-02-12T13:58:39.520939Z","iopub.status.idle":"2022-02-12T13:58:40.933741Z","shell.execute_reply.started":"2022-02-12T13:58:39.520905Z","shell.execute_reply":"2022-02-12T13:58:40.932611Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = X.values\ny = y.values\ntest = test.values","metadata":{"execution":{"iopub.status.busy":"2022-02-07T15:30:54.578934Z","iopub.execute_input":"2022-02-07T15:30:54.579224Z","iopub.status.idle":"2022-02-07T15:30:54.58808Z","shell.execute_reply.started":"2022-02-07T15:30:54.579198Z","shell.execute_reply":"2022-02-07T15:30:54.587241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VERBOSE = True\npredictions, scores = [], []\nk = StratifiedKFold(n_splits = 10, random_state = 228, shuffle = True)\n\n    \nfor fold, (train_idx, test_idx) in enumerate(k.split(X, y)):\n    X_train, X_val = X[train_idx], X[test_idx]\n    y_train, y_val = y[train_idx], y[test_idx]\n        \n    model = my_model()\n    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n\n    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.6, \n                               patience=3, verbose=VERBOSE)\n\n    es = EarlyStopping(monitor=\"val_loss\", patience=7, \n                           verbose=VERBOSE, mode=\"min\", \n                           restore_best_weights=True)\n        \n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    chk_point = ModelCheckpoint(f'./TPS1_model_2022_{fold+1}C.h5', options=save_locally, \n                                    monitor='val_loss', verbose=VERBOSE, \n                                    save_best_only=True, mode='min')\n        \n    model.fit(X_train, y_train, \n                  validation_data=(X_val, y_val), \n                  epochs=300,\n                  verbose=VERBOSE,\n                  batch_size=BATCH_SIZE, \n                  callbacks=[lr, chk_point, es])\n        \n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n    model = load_model(f'./TPS1_model_2022_{fold+1}C.h5', options=load_locally)\n        \n    y_pred = model.predict(X_val, batch_size=BATCH_SIZE)\n    score = accuracy_score(y_val, np.argmax(y_pred, axis=1))\n    scores.append(score)\n    predictions.append(np.argmax(model.predict(test, batch_size=BATCH_SIZE), axis=1))\n    print(f\"Fold-{fold+1} | OOF Score: {score}\")\n    \nprint(f'Mean accuracy on {k.n_splits} folds - {mean(scores)}')","metadata":{"execution":{"iopub.status.busy":"2022-02-07T15:30:59.816729Z","iopub.execute_input":"2022-02-07T15:30:59.817269Z","iopub.status.idle":"2022-02-07T15:46:44.522892Z","shell.execute_reply.started":"2022-02-07T15:30:59.817236Z","shell.execute_reply":"2022-02-07T15:46:44.521078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss['target'] = stats.mode(np.column_stack(predictions), axis=1)[0]\nss['target'] = ss['target'].map(retarget2)\nss.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-07T15:46:55.469598Z","iopub.execute_input":"2022-02-07T15:46:55.469891Z","iopub.status.idle":"2022-02-07T15:46:58.942821Z","shell.execute_reply.started":"2022-02-07T15:46:55.469862Z","shell.execute_reply":"2022-02-07T15:46:58.94194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extra trees","metadata":{}},{"cell_type":"markdown","source":"I knew that in this competition Extra Trees has a big advantage over the others, but I wanted to try one of gradient trees and simple NN. Soo.. \n\nTrials without dropping duplicates:\n1. First trial with 300 estimators and no limit in depth. Mean accuracy on 10 folds - **0.9962**. LB result - **0.97816**\n2. Second trial with 1000 estimators and no limit in depth. Mean accuracy on 10 folds - **0.9966**. LB result - **0.97796**\n\nTrials with dropping duplicates:\n\n1. First trial with 300 estimators and no limit in depth. Mean accuracy on 10 folds - **0.9755**. LB result - **0.97700**\n2. Second trial with 1000 estimators and no limit in depth. Mean accuracy on 10 folds - **0.9778**. LB result - **0.97771**\n\nLet's try postprocessing on predictions without dropping duplicates:\n\nLB result - **0.98890**","metadata":{}},{"cell_type":"code","source":"predictions, scores = [], []\n \nk = StratifiedKFold(n_splits = 10, random_state = 228, shuffle = True)\nfor i, (trn_idx, val_idx) in enumerate(k.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = ExtraTreesClassifier(n_estimators=1111, n_jobs=-1)\n    model.fit(X_train, y_train)\n    \n    val_pred = model.predict(X_val)\n    val_score = accuracy_score(y_val, val_pred)\n    print(f'Fold {i+1} accuracy score: {round(val_score, 4)}')\n    \n    scores.append(val_score)\n    predictions.append(model.predict_proba(test))\nprint('')    \nprint(f'Mean accuracy - {round(mean(scores), 4)}')","metadata":{"execution":{"iopub.status.busy":"2022-02-12T11:45:42.901472Z","iopub.execute_input":"2022-02-12T11:45:42.902671Z","iopub.status.idle":"2022-02-12T12:54:17.492235Z","shell.execute_reply.started":"2022-02-12T11:45:42.902618Z","shell.execute_reply":"2022-02-12T12:54:17.490619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_proba = sum(predictions) / len(predictions)\ny_proba += np.array([0, 0, 0.025, 0.045, 0, 0, 0, 0, 0, 0])\ny_pred_tuned = le.inverse_transform(np.argmax(y_proba, axis=1))\npd.Series(y_pred_tuned, index=test.index).value_counts().sort_index() / len(test) * 100","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:54:17.496825Z","iopub.execute_input":"2022-02-12T12:54:17.497143Z","iopub.status.idle":"2022-02-12T12:54:17.610168Z","shell.execute_reply.started":"2022-02-12T12:54:17.497107Z","shell.execute_reply":"2022-02-12T12:54:17.609216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss['target'] = y_pred_tuned\nss.to_csv('submission__.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T12:54:17.612097Z","iopub.execute_input":"2022-02-12T12:54:17.612478Z","iopub.status.idle":"2022-02-12T12:54:17.904497Z","shell.execute_reply.started":"2022-02-12T12:54:17.612434Z","shell.execute_reply":"2022-02-12T12:54:17.903594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Forest of Extra Trees","metadata":{}},{"cell_type":"markdown","source":"Thanks for the results to: [Marco](https://www.kaggle.com/marcobr95/predicting-bacteria-species/notebook?scriptVersionId=87489264), [Safak](https://www.kaggle.com/sfktrkl/tps-feb-2022/notebook?scriptVersionId=87059305) and [Alex](https://www.kaggle.com/alexryzhkov/tps-feb-22-lightautoml-pseudolabel)\n\nLB result - **0.9895**","metadata":{}},{"cell_type":"code","source":"sub1 = pd.read_csv('../input/tps-feb-results/alex_submission 0.98865.csv')\nsub2 = pd.read_csv('../input/tps-feb-results/safak_submission 0.98830.csv')\nsub3 = pd.read_csv('../input/tps-feb-results/marco_submission 0.98790.csv')\n\nss['target'] = le.fit_transform(ss['target'])\nsub1['target'] = le.transform(sub1['target'])\nsub2['target'] = le.transform(sub2['target'])\nsub3['target'] = le.transform(sub3['target'])\n\nblend_preds = []\nfor preds in [ss, sub1, sub2, sub3]:\n    blend_preds.append(preds['target'])\n    \nblend_ss = ss.copy()\nblend_ss['target'] = le.inverse_transform(stats.mode(np.column_stack(blend_preds), axis=1)[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:45:25.585379Z","iopub.execute_input":"2022-02-12T13:45:25.58612Z","iopub.status.idle":"2022-02-12T13:45:29.333119Z","shell.execute_reply.started":"2022-02-12T13:45:25.586073Z","shell.execute_reply":"2022-02-12T13:45:29.332352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blend_ss.to_csv('submission__blend.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T13:45:38.784385Z","iopub.execute_input":"2022-02-12T13:45:38.786325Z","iopub.status.idle":"2022-02-12T13:45:39.085636Z","shell.execute_reply.started":"2022-02-12T13:45:38.78625Z","shell.execute_reply":"2022-02-12T13:45:39.08458Z"},"trusted":true},"execution_count":null,"outputs":[]}]}