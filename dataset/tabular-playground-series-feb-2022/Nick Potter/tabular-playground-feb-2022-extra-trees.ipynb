{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Here's my workbook for the February 2022 Tabular Playground. Nothing too special here, just an ExtraTreesClassifier.\n\nI learnt a lot looking through others' notebooks. In particular, the following had lots of good ideas (some of which I borrowed for my entries):\n\nhttps://www.kaggle.com/ayoubchaoui/extratreesclassifier-vs-randomforestclassifier\n\nhttps://www.kaggle.com/munumbutt/extratrees-stratifiedkfold-memory-optimization\n\nhttps://www.kaggle.com/maxencefzr/tps-feb22-eda-extratrees\n\nhttps://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense\n\nhttps://www.kaggle.com/kotrying/extra-blender-addition\n\nhttps://www.kaggle.com/ambrosm/tpsfeb22-03-clustering-improves-the-predictions","metadata":{}},{"cell_type":"markdown","source":"Script parameters:","metadata":{}},{"cell_type":"code","source":"num_estimators = 80\nnum_splits = 10","metadata":{"id":"5b0mDq5u8-2p","execution":{"iopub.status.busy":"2022-02-28T18:21:09.856965Z","iopub.execute_input":"2022-02-28T18:21:09.857276Z","iopub.status.idle":"2022-02-28T18:21:09.861544Z","shell.execute_reply.started":"2022-02-28T18:21:09.857243Z","shell.execute_reply":"2022-02-28T18:21:09.860479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Library imports and data importing:","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"id":"d-DGnEBzT-qI","outputId":"a305877c-1ae0-4781-b26d-d3738ce34cb5","execution":{"iopub.status.busy":"2022-02-28T18:21:09.86297Z","iopub.execute_input":"2022-02-28T18:21:09.863484Z","iopub.status.idle":"2022-02-28T18:21:09.875796Z","shell.execute_reply.started":"2022-02-28T18:21:09.863452Z","shell.execute_reply":"2022-02-28T18:21:09.875107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/train.csv',\n                   index_col='row_id')\n#y = train['target']\nfrom sklearn.preprocessing import LabelEncoder\ntarget_encoder = LabelEncoder()\ny = pd.Series(target_encoder.fit_transform(train[\"target\"]))\nX = train.drop(labels=['target'], axis=1)","metadata":{"id":"P1F03xbwTrlL","outputId":"4364b674-68eb-437b-eca1-8a48851d7e54","execution":{"iopub.status.busy":"2022-02-28T18:21:09.876882Z","iopub.execute_input":"2022-02-28T18:21:09.877632Z","iopub.status.idle":"2022-02-28T18:21:25.743075Z","shell.execute_reply.started":"2022-02-28T18:21:09.877581Z","shell.execute_reply":"2022-02-28T18:21:25.742208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/test.csv',\n                    index_col='row_id')\nprint(test.head())","metadata":{"id":"sQ00Q8xA_MZw","outputId":"4085fa16-0cfc-4590-eaac-d5b4e791399f","execution":{"iopub.status.busy":"2022-02-28T18:21:25.746825Z","iopub.execute_input":"2022-02-28T18:21:25.747112Z","iopub.status.idle":"2022-02-28T18:21:37.668522Z","shell.execute_reply.started":"2022-02-28T18:21:25.747082Z","shell.execute_reply":"2022-02-28T18:21:37.667323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train an ExtraTreesClassifier. I tried RandomForest and lots of different hyperparameters evaluated using sklearn.model_selection.GridSearchCV etc., but nothing really provided much improvement over an ExtraTreesClassifier.\n\nFor each fold of the cross validation, test-set prediction probabilities are saved for later use.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport time\n\nfold_probs = [] # Store the probabilities from each fold for later use\n                # The final predicted value is determined by the\n                # average across all cross-validation folds\n\n# evaluate the model using Stratified K-Fold cross validation:\nfor fold, (train_id, test_id) in enumerate(StratifiedKFold(n_splits=num_splits, \n                                                           shuffle=True, \n                                                           random_state=456).split(X,y)): \n                                                                                   \n    Xt = X.iloc[train_id]\n    yt = y.iloc[train_id]\n    Xv = X.iloc[test_id]\n    yv = y.iloc[test_id]\n    model = ExtraTreesClassifier(n_estimators = num_estimators)\n    start = time.time()\n\n    model.fit(Xt, yt)\n    \n    end = time.time()\n    \n    valid_pred = model.predict(Xv)\n    valid_score = accuracy_score(yv, valid_pred)\n    \n    print(\"Fold:\", fold + 1, \"Accuracy:\", valid_score, 'Time (min.):', (end - start)/60)\n    \n    \n    fold_probs.append(model.predict_proba(test))\n    ","metadata":{"id":"D0LOGVIyiNW6","execution":{"iopub.status.busy":"2022-02-28T18:21:37.670288Z","iopub.execute_input":"2022-02-28T18:21:37.670511Z","iopub.status.idle":"2022-02-28T18:31:58.699068Z","shell.execute_reply.started":"2022-02-28T18:21:37.670483Z","shell.execute_reply":"2022-02-28T18:31:58.698031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Although the cross-validated accuracy is quite high, the accuracy on the test set is a lot lower due to target drift. Essentially, bacteria mutated between the training and test set, and decision boundaries calculated on the training set are not as accurate. This is explained in more detail (with figures as well) in AmbrosM's notebooks (see above) and elsewhere. I spent a bit of time exploring this, but ran out of time in coming up with a novel way to improve the test-set predictions.\n\nNext, we average the category probabilities across the cross-validation to come up with the best prediction:","metadata":{}},{"cell_type":"code","source":"mean_prob = sum(fold_probs) / len(fold_probs) # Mean probability for each row\nprint(mean_prob)\n\nmean_pred = target_encoder.inverse_transform(np.argmax(mean_prob, axis=1))\nprint(mean_pred)","metadata":{"id":"FlGg4COi_-mR","execution":{"iopub.status.busy":"2022-02-28T18:31:58.701389Z","iopub.execute_input":"2022-02-28T18:31:58.701734Z","iopub.status.idle":"2022-02-28T18:31:58.73166Z","shell.execute_reply.started":"2022-02-28T18:31:58.701693Z","shell.execute_reply":"2022-02-28T18:31:58.730687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame(data = {'row_id': test.index, 'target': mean_pred})\nprint(output)\noutput.to_csv('submission.csv', index=False)","metadata":{"id":"KYNaWdTSTrlS","execution":{"iopub.status.busy":"2022-02-28T18:31:58.733316Z","iopub.execute_input":"2022-02-28T18:31:58.733519Z","iopub.status.idle":"2022-02-28T18:31:59.001713Z","shell.execute_reply.started":"2022-02-28T18:31:58.733494Z","shell.execute_reply":"2022-02-28T18:31:59.0008Z"},"trusted":true},"execution_count":null,"outputs":[]}]}