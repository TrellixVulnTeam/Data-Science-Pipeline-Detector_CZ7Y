{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Dive into Duplicated Data\n\nThis notebook built upon [this](https://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense) excellent notebook by AmbrosM, the [observation](https://www.kaggle.com/c/tabular-playground-series-feb-2022/discussion/305364) that many rows are duplicated from Teck Meng Wong and the [observation](https://www.kaggle.com/thexyzt/intersection-between-training-and-test-sets) that we have some rows that are the same between train and test by thexyzt.\n\nSo as the contributors above discovered, we have duplicated data in this dataset. Why? Most discussions until now assumed that it was a data quality issue, that should be simply addressed by removing the duplicates. However, I will show you in this notebook that data duplication shows key insights about the way that the data is built and should probably be kept in.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom math import factorial\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.cluster import KMeans","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T14:31:51.10788Z","iopub.execute_input":"2022-02-06T14:31:51.108287Z","iopub.status.idle":"2022-02-06T14:31:52.491109Z","shell.execute_reply.started":"2022-02-06T14:31:51.108255Z","shell.execute_reply":"2022-02-06T14:31:52.490196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-feb-2022/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-feb-2022/test.csv')\n\nelements = [e for e in train_df.columns if e != 'row_id' and e != 'target']\n\n# Convert the 10 bacteria names to the integers 0 .. 9\nle = LabelEncoder()\ntrain_df['target_num'] = le.fit_transform(train_df.target)\n\ntrain_df.shape, test_df.shape","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-06T14:36:50.576713Z","iopub.execute_input":"2022-02-06T14:36:50.577335Z","iopub.status.idle":"2022-02-06T14:37:14.011321Z","shell.execute_reply.started":"2022-02-06T14:36:50.577299Z","shell.execute_reply":"2022-02-06T14:37:14.01048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of duplicates in train data:\", train_df[elements].duplicated().sum())\nprint(\"Number of duplicates in test data:\" ,test_df[elements].duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:37:14.012808Z","iopub.execute_input":"2022-02-06T14:37:14.01302Z","iopub.status.idle":"2022-02-06T14:37:16.648366Z","shell.execute_reply.started":"2022-02-06T14:37:14.012983Z","shell.execute_reply":"2022-02-06T14:37:16.647709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we see we have many duplicates. But why? First, we must understand the resolution and error rate of the data","metadata":{}},{"cell_type":"markdown","source":"So, the data that we get is a transformed version, where the raw version is the time each histogram was sampled. Each row has a different resolution - it contains either 100000, 10000, 100 or 10 samples, which we can now from the divisor of this inversly transformed data. Rows with more samples have less data we can use to successfully classify them","metadata":{}},{"cell_type":"code","source":"def bias(w, x, y, z):\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ndef bias_of(s):\n    w = int(s[1:s.index('T')])\n    x = int(s[s.index('T')+1:s.index('G')])\n    y = int(s[s.index('G')+1:s.index('C')])\n    z = int(s[s.index('C')+1:])\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ntrain_i = pd.DataFrame({col: ((train_df[col] + bias_of(col)) * 1000000).round().astype(int) for col in elements})\ntest_i = pd.DataFrame({col: ((test_df[col] + bias_of(col)) * 1000000).round().astype(int) for col in elements})\ntrain_i","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-06T14:32:34.463946Z","iopub.execute_input":"2022-02-06T14:32:34.464178Z","iopub.status.idle":"2022-02-06T14:32:35.649341Z","shell.execute_reply.started":"2022-02-06T14:32:34.464151Z","shell.execute_reply":"2022-02-06T14:32:35.64841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gcd_of_all(df_i):\n    gcd = df_i[elements[0]]\n    for col in elements[1:]:\n        gcd = np.gcd(gcd, df_i[col])\n    return gcd\n\ntrain_df['gcd'] = gcd_of_all(train_i)\ntest_df['gcd'] = gcd_of_all(test_i)\nnp.unique(train_df['gcd'], return_counts=True), np.unique(test_df['gcd'], return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:39:59.469216Z","iopub.execute_input":"2022-02-06T14:39:59.469694Z","iopub.status.idle":"2022-02-06T14:40:00.851286Z","shell.execute_reply.started":"2022-02-06T14:39:59.469627Z","shell.execute_reply":"2022-02-06T14:40:00.850692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that each resolution of data is represented in roughly equal numbers. Now let's run a PCA of the data","metadata":{}},{"cell_type":"code","source":"for scale in np.sort(train_df['gcd'].unique()):\n    # Compute the PCA\n    pca = PCA(whiten=True, random_state=1)\n    pca.fit(train_i[elements][train_df['gcd'] == scale])\n\n    # Transform the data so that the components can be analyzed\n    Xt_tr = pca.transform(train_i[elements][train_df['gcd'] == scale])\n    Xt_te = pca.transform(test_i[elements][test_df['gcd'] == scale])\n\n    # Plot a scattergram, projected to two PCA components, colored by classification target\n    plt.figure(figsize=(6,6))\n    plt.scatter(Xt_tr[:,0], Xt_tr[:,1], c=train_df.target_num[train_df['gcd'] == scale], s=1)\n    plt.title(f\"{1000000 // scale} decamers ({(train_df['gcd'] == scale).sum()} samples with gcd = {scale})\")\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-06T14:41:31.545888Z","iopub.execute_input":"2022-02-06T14:41:31.546496Z","iopub.status.idle":"2022-02-06T14:41:44.463258Z","shell.execute_reply.started":"2022-02-06T14:41:31.546461Z","shell.execute_reply":"2022-02-06T14:41:44.462381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the plots for higher resolution data (low gcd), we see that there are 8 clusters for each class and gcd combination. These clusters represent different error rates, with the ones farther away from the center in which points converge having smaller errors, since at higher error rates classes get less distinct. In the plots for lower resolution data, everything blends together","metadata":{}},{"cell_type":"markdown","source":"# The reveal\nWe will now recreate the same plots, but with only points that have duplicates","metadata":{}},{"cell_type":"code","source":"for scale in np.sort(train_df['gcd'].unique()):\n    # Compute the PCA\n    pca = PCA(whiten=True, random_state=1)\n    pca.fit(train_i[elements][train_df['gcd'] == scale])\n\n    # Transform the data so that the components can be analyzed\n    Xt_tr = pca.transform(train_i[elements][train_df['gcd'] == scale][train_df[elements].duplicated(keep=False)])\n    Xt_te = pca.transform(test_i[elements][test_df['gcd'] == scale][test_df[elements].duplicated(keep=False)])\n\n    # Plot a scattergram, projected to two PCA components, colored by classification target\n    plt.figure(figsize=(6,6))\n    plt.scatter(Xt_tr[:,0], Xt_tr[:,1], c=train_df.target_num[train_df['gcd'] == scale][train_df[elements].duplicated(keep=False)], s=1)\n    plt.title(f\"{1000000 // scale} decamers ({train_i[elements][train_df['gcd'] == scale][train_df[elements].duplicated(keep=False)].shape[0]} samples with gcd = {scale})\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:01:50.276207Z","iopub.execute_input":"2022-02-06T15:01:50.276517Z","iopub.status.idle":"2022-02-06T15:02:27.229845Z","shell.execute_reply.started":"2022-02-06T15:01:50.276488Z","shell.execute_reply":"2022-02-06T15:02:27.229221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Suddenly, the reason for the duplicated data reveals itself! In the first two plots showing high resolution data, we only see one cluster per class instead of 8. This is the same cluster for the lowest error rate data! If we were to sample the same bacterium with low error rates, we are bound to get some duplicates. Moreover, in the earlier plots we see every cluster has a roughly equal size, which means that the highest resolution clusters should have roughly $50000/8 = 6250$ data points in total, almost the number of duplicated data we see. Probably, low error rate data also gets duplicated in the lower resolution data, but we can't clearly see it in the PCA plots.\n\nHowever, the lower resolution data have much more duplicates than can be explained by low error rates, and they seem to be all over the place. Which leads us to the second reason the data is duplicated. When the resolution is very low, we might get rows that are the same by chance alone, especially when many samples come from the same distribution, which might be low entropy.\n\n## Conclusions\n\nWe uncovered two reasons for data duplication, low error rates and low resolution. None of these are data quality issues, so we should think twice before we remove them. However, there is still an open question. If both of these reasons were the only reason our data was duplicated, we would expect to see significantly more duplicated at the 10000 gcd level than the 1000 level, because of the much higher resolution. However, numbers are similar. Please write a comment if you have an idea why this happens.\n\n## To remove or not to remove?\n\nBased on these insights, when we remove duplicated we will be removing some of our highest quality data points for training. We are also removing some legitimately sampled data points that just happen to be the same as others. This will cause our machine learning algorithm to learn a skewed distribution of the data. Therefore, I recommend you do not remove duplicates. An alternate approach might be to attempt to only remove the duplicated stemming from low resolution data, to skew the learning towards high quality samples.\n\nFrom my own experience, removing duplicates significantly lowered my performance on the public leaderboard and created a stronger skew towards classifying as certain classes over others.\n\nMany people also said that our CV can no longer be trusted after duplicates were discovered. However, I'm not sure that conclusion is justified or that removing duplicates solves it. First, if we get legitimate duplicates then \"data leakage\" is no longer a bug in the CV strategy but rather a feature of our ML problem that will also happen between train and test. Second, we were seeing some paradoxical results after removal where test performance was better than CV performance. Third, the lower CV performance conceals the most important, hardest problem in this competition, the mutations that happened between train and test, which are the real reason the public LB scores are lower than CV scores","metadata":{}},{"cell_type":"markdown","source":"# Intersecting points\nWe turn ourselves to the second, related mystery. We seem to have 1502 data points shared between train and test, why?","metadata":{}},{"cell_type":"code","source":"intersection = pd.merge(train_df, test_df, on=list(test_df.columns[1:]), how=\"inner\")\nintersection.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:34:27.971816Z","iopub.execute_input":"2022-02-06T15:34:27.972311Z","iopub.status.idle":"2022-02-06T15:34:49.488817Z","shell.execute_reply.started":"2022-02-06T15:34:27.97228Z","shell.execute_reply":"2022-02-06T15:34:49.487713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's filter to only the lowest resolution points","metadata":{}},{"cell_type":"code","source":"intersection[intersection['gcd'] == 10000].shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-02-06T15:34:57.105883Z","iopub.execute_input":"2022-02-06T15:34:57.10673Z","iopub.status.idle":"2022-02-06T15:34:57.114051Z","shell.execute_reply.started":"2022-02-06T15:34:57.106685Z","shell.execute_reply":"2022-02-06T15:34:57.113404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions\n\nAll of the intersecting points are of the lowest resolution, which means they are yet more examples of low-resolution duplications. Since we know there were mutation shifts between train and test, we no longer get low-error duplications between train and test. The implication is that we can't even be sure if the labels of these points are the same as the train labels for the same points. Maybe a bacterium, after mutating, got the same low-resolution results as a different bacterium in the train data. My recommendation for the intersection therefore is to ignore it. It's a red herring and I don't see how analyzing it can lead to improvements.","metadata":{}}]}