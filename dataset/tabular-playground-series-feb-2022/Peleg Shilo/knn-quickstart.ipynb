{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing","metadata":{}},{"cell_type":"code","source":"#%%capture\n\n# IntelÂ® Extension for Scikit-learn installation:\n#\n!pip install scikit-learn-intelex\n!pip install metric-learn\nimport os\nimport warnings\n\nimport numpy as np  # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.stats import mode\nfrom tqdm import tqdm\nfrom pathlib import Path\n\n\n\nfrom sklearnex import patch_sklearn\n#patch_sklearn()\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom math import factorial\n\n# Mute warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-05T10:29:39.782233Z","iopub.execute_input":"2022-02-05T10:29:39.782587Z","iopub.status.idle":"2022-02-05T10:30:37.027858Z","shell.execute_reply.started":"2022-02-05T10:29:39.782552Z","shell.execute_reply":"2022-02-05T10:30:37.026877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thanks @sy-tuan nguyen for the memory reducing routine","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:30:37.030182Z","iopub.execute_input":"2022-02-05T10:30:37.030534Z","iopub.status.idle":"2022-02-05T10:30:37.045695Z","shell.execute_reply.started":"2022-02-05T10:30:37.030492Z","shell.execute_reply":"2022-02-05T10:30:37.044042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_dir = Path('../input/tabular-playground-series-feb-2022')\n\ndf_train = pd.read_csv(data_dir / 'train.csv', index_col='row_id').pipe(reduce_mem_usage)\ndf_test  = pd.read_csv(data_dir / 'test.csv', index_col='row_id').pipe(reduce_mem_usage)\nTARGET = df_train.columns.difference(df_test.columns)[0]\nfeatures = df_train.columns[df_train.columns != TARGET]","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:30:37.047637Z","iopub.execute_input":"2022-02-05T10:30:37.047948Z","iopub.status.idle":"2022-02-05T10:31:46.654597Z","shell.execute_reply.started":"2022-02-05T10:30:37.047906Z","shell.execute_reply":"2022-02-05T10:31:46.653174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing\nThere are two preprocessing steps here. First I drop the duplicates, then I match the means of train and test to partially counteract drift in the populations. Then I standardize the data, which you should pretty much always do for KNN. I tried some other scalers and they gave worse results.\nBecause KNN takes so long to run when predicting, instead of performing cross-validation I split the training data into train and validation. The validation set is small because we have a lot of data, so we don't need a large validation dataset to get accurate results. The validation set still has 10000 examples","metadata":{}},{"cell_type":"code","source":"df_train.drop_duplicates(keep='first', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:31:46.657947Z","iopub.execute_input":"2022-02-05T10:31:46.658203Z","iopub.status.idle":"2022-02-05T10:31:49.081523Z","shell.execute_reply.started":"2022-02-05T10:31:46.658175Z","shell.execute_reply":"2022-02-05T10:31:49.08028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pre_process(train, test):\n    return train + (test.mean(axis=0) - train.mean(axis=0))\ndf_train[features] = pre_process(df_train[features], df_test[features])","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:31:49.08271Z","iopub.execute_input":"2022-02-05T10:31:49.083334Z","iopub.status.idle":"2022-02-05T10:31:51.078218Z","shell.execute_reply.started":"2022-02-05T10:31:49.083284Z","shell.execute_reply":"2022-02-05T10:31:51.077497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n# Encoding categorical features\nle = LabelEncoder()\nscaler = StandardScaler()\nX = df_train[features]\ny = pd.DataFrame(le.fit_transform(df_train[TARGET]), columns=[TARGET])\nX = scaler.fit_transform(X)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.05, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:31:51.079987Z","iopub.execute_input":"2022-02-05T10:31:51.080221Z","iopub.status.idle":"2022-02-05T10:31:59.124452Z","shell.execute_reply.started":"2022-02-05T10:31:51.080194Z","shell.execute_reply":"2022-02-05T10:31:59.123619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = df_test[features]\nX_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:31:59.125932Z","iopub.execute_input":"2022-02-05T10:31:59.12652Z","iopub.status.idle":"2022-02-05T10:32:00.32818Z","shell.execute_reply.started":"2022-02-05T10:31:59.126476Z","shell.execute_reply":"2022-02-05T10:32:00.327283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training\nI tuned the number of neighbors and the distance metric.","metadata":{}},{"cell_type":"code","source":"%%time\nmodel = KNeighborsClassifier(n_neighbors=4, p=1)\nmodel.fit(X_train, y_train)\nmodel.score(X_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:32:00.329842Z","iopub.execute_input":"2022-02-05T10:32:00.330475Z","iopub.status.idle":"2022-02-05T10:34:55.565824Z","shell.execute_reply.started":"2022-02-05T10:32:00.330426Z","shell.execute_reply":"2022-02-05T10:34:55.565045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\nHere I retrain the model on the full data","metadata":{}},{"cell_type":"code","source":"model = KNeighborsClassifier(n_neighbors=4, p=1)\nmodel.fit(X, y)\ny_pred = model.predict(X_test)\ny_pred = le.inverse_transform(y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-02-05T10:34:55.567286Z","iopub.execute_input":"2022-02-05T10:34:55.567538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsubmission = pd.read_csv(data_dir / 'sample_submission.csv')\nsubmission[TARGET] = y_pred\nsubmission.to_csv('submission.csv', index=False)\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}