{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EDA for the February TPS\n\nThe present EDA **explains the structures in the competition's dataset and derives ideas for modeling and for creating a classifier.**\n\nIn particular, it explains:\n- Why the feature values are discrete and what we can do with this information\n- How error rates are determined\n- How the test data deviate from the training data and what this means\n\nAt the end, a possible approach for solving the classification problem is proposed.\n\nSome information in this EDA comes from the paper [\"Analysis of Identification Method for Bacterial Species and Antibiotic Resistance Genes Using Optical Data From DNA Oligomers\"](https://www.frontiersin.org/articles/10.3389/fmicb.2020.00257/full).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom math import factorial\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.cluster import KMeans\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-19T20:31:09.522462Z","iopub.execute_input":"2022-02-19T20:31:09.523148Z","iopub.status.idle":"2022-02-19T20:31:10.905928Z","shell.execute_reply.started":"2022-02-19T20:31:09.523029Z","shell.execute_reply":"2022-02-19T20:31:10.905012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We start by reading the data and converting the bacteria names to numbers:","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-feb-2022/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-feb-2022/test.csv')\n\nelements = [e for e in train_df.columns if e != 'row_id' and e != 'target']\n\n# Convert the 10 bacteria names to the integers 0 .. 9\nle = LabelEncoder()\ntrain_df['target_num'] = le.fit_transform(train_df.target)\n\ntrain_df.shape, test_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:31:10.907363Z","iopub.execute_input":"2022-02-19T20:31:10.90761Z","iopub.status.idle":"2022-02-19T20:31:49.996514Z","shell.execute_reply.started":"2022-02-19T20:31:10.90758Z","shell.execute_reply":"2022-02-19T20:31:49.995807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'll skip the paragraphs about missing values (there are none) and about class balance (the ten classes are balanced) and immediately jump into the interesting analysis.","metadata":{}},{"cell_type":"markdown","source":"# Discreteness of the values\n\nLet's look at the unique values of an arbitrary feature. We notice:\n1. Although the feature is a floating point number, there are not 200000 unique values, but only about one hundred.\n2. The last few digits are always the same (they always end with 0846558 from 1.00846558e-05 through 9.70846558e-05).\n","metadata":{}},{"cell_type":"code","source":"np.unique(train_df.A0T0G2C8)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:31:49.99742Z","iopub.execute_input":"2022-02-19T20:31:49.998106Z","iopub.status.idle":"2022-02-19T20:31:50.017996Z","shell.execute_reply.started":"2022-02-19T20:31:49.998065Z","shell.execute_reply":"2022-02-19T20:31:50.01721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This observation strongly suggests that these values originally were integers. These integers were divided by 1000000 and a constant was subtracted.\n\nThe [paper](https://www.frontiersin.org/articles/10.3389/fmicb.2020.00257/full) describes this process and gives the formula for the additive constant, which they call *bias*. With the help of this formula, we can convert the floating point numbers back to the original integers:","metadata":{}},{"cell_type":"code","source":"def bias(w, x, y, z):\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ndef bias_of(s):\n    w = int(s[1:s.index('T')])\n    x = int(s[s.index('T')+1:s.index('G')])\n    y = int(s[s.index('G')+1:s.index('C')])\n    z = int(s[s.index('C')+1:])\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ntrain_i = pd.DataFrame({col: ((train_df[col] + bias_of(col)) * 1000000).round().astype(int)\n                        for col in elements})\ntest_i = pd.DataFrame({col: ((test_df[col] + bias_of(col)) * 1000000).round().astype(int)\n                       for col in elements})\ntrain_i","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:31:50.02004Z","iopub.execute_input":"2022-02-19T20:31:50.020352Z","iopub.status.idle":"2022-02-19T20:31:51.143789Z","shell.execute_reply.started":"2022-02-19T20:31:50.020321Z","shell.execute_reply":"2022-02-19T20:31:51.142879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The integers sum up to one million in every row:","metadata":{}},{"cell_type":"code","source":"train_i.sum(axis=1).min(), train_i.sum(axis=1).max()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:31:51.144882Z","iopub.execute_input":"2022-02-19T20:31:51.145107Z","iopub.status.idle":"2022-02-19T20:31:51.270008Z","shell.execute_reply.started":"2022-02-19T20:31:51.145079Z","shell.execute_reply":"2022-02-19T20:31:51.26919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Did you notice that in some rows all entries are multiples of 10 or of 1000? We'll verify this systematically by computing the greatest common divisor (gcd) for every row:","metadata":{}},{"cell_type":"code","source":"# def gcd_of_all(df_i):\n#     gcd = df_i[elements[0]]\n#     for col in elements[1:]:\n#         gcd = np.gcd(gcd, df_i[col])\n#     return gcd\n\ntrain_df['gcd'] = np.gcd.reduce(train_i[elements], axis=1)\ntest_df['gcd'] = np.gcd.reduce(test_i[elements], axis=1)\n# train_df['gcd'] = train_i[elements].apply(np.gcd.reduce, axis=1) # slow\n# test_df['gcd'] = test_i[elements].apply(np.gcd.reduce, axis=1)\n# train_df['gcd'] = gcd_of_all(train_i)\n# test_df['gcd'] = gcd_of_all(test_i)\nnp.unique(train_df['gcd'], return_counts=True), np.unique(test_df['gcd'], return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:31:51.271569Z","iopub.execute_input":"2022-02-19T20:31:51.272414Z","iopub.status.idle":"2022-02-19T20:31:52.505406Z","shell.execute_reply.started":"2022-02-19T20:31:51.272366Z","shell.execute_reply":"2022-02-19T20:31:52.504543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that there are four gcd values (1, 10, 1000 and 10000) with equal frequencies. Connecting this result with what they write in the paper, we understand this part of the experiment:\n\nFor every row, they take the DNA of a bacterium and cut it into [decamers](https://en.wikipedia.org/wiki/K-mer) (DNA substrings of length 10). Then they put 1000000, 100000, 1000 or 100 decamers into their machine, and the machine counts how many times every of the 286 types from A0T0G0C10 to A10T0G0C0 occurs. This is what they call the spectrum (one could as well call it a histogram with 286 bins). They normalize the spectrum by dividing all counts by the row sum and subtracting the bias.\n\nEvery bacterium has its own characteristic spectrum, and the competition task is predicting the bacterium's name from the spectrum of a sample. If the sample spectrum is made from a million decamers, we'll have accurate estimates of the true frequencies and predicting the name will be easy; if the spectrum is made from only 100 decamers, we have little information and the prediction will be hard (the classes overlap). We can see the influence of the number of decamers in the following four PCA plots:","metadata":{}},{"cell_type":"code","source":"for scale in np.sort(train_df['gcd'].unique()):\n    # Compute the PCA\n    pca = PCA(whiten=True, random_state=1)\n    pca.fit(train_i[elements][train_df['gcd'] == scale])\n\n    # Transform the data so that the components can be analyzed\n    Xt_tr = pca.transform(train_i[elements][train_df['gcd'] == scale])\n    Xt_te = pca.transform(test_i[elements][test_df['gcd'] == scale])\n\n    # Plot a scattergram, projected to two PCA components, colored by classification target\n    plt.figure(figsize=(6,6))\n    plt.scatter(Xt_tr[:,0], Xt_tr[:,1], c=train_df.target_num[train_df['gcd'] == scale], cmap='tab10', s=1)\n    plt.title(f\"{1000000 // scale} decamers ({(train_df['gcd'] == scale).sum()} samples with gcd = {scale})\")\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:36:07.733633Z","iopub.execute_input":"2022-02-19T20:36:07.733988Z","iopub.status.idle":"2022-02-19T20:36:20.596386Z","shell.execute_reply.started":"2022-02-19T20:36:07.733952Z","shell.execute_reply":"2022-02-19T20:36:20.595205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight:**\n- We may want to create four separate classifiers for the four GCD values. For GCD = 1, we expect high accuracy; for GCD = 10000, accuracy will be lower.\n- If we create only a single classifier, the gcd can be used as an additional feature.","metadata":{}},{"cell_type":"markdown","source":"# Duplicates\n\nA third of the data are duplicates (thanks to @[teckmengwong](https://www.kaggle.com/teckmengwong) for [pointing this out](https://www.kaggle.com/c/tabular-playground-series-feb-2022/discussion/305364)):","metadata":{}},{"cell_type":"code","source":"train_df[elements].duplicated().sum(), test_df[elements].duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:32:09.807393Z","iopub.execute_input":"2022-02-19T20:32:09.807655Z","iopub.status.idle":"2022-02-19T20:32:12.73042Z","shell.execute_reply.started":"2022-02-19T20:32:09.807626Z","shell.execute_reply":"2022-02-19T20:32:12.729534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Counting the duplicates for the four GCD values separately, we see that most duplicates occur for the high GCD values. This can be explained: If every observation consists of 100 decamers in 286 bins, we should expect more duplicates than if 1000000 decamers are put in 286 bins.","metadata":{}},{"cell_type":"code","source":"def plot_duplicates_per_gcd(df, title):\n    plt.figure(figsize=(14, 3))\n    plt.tight_layout()\n    for i, gcd in enumerate(np.unique(df.gcd)):\n        plt.subplot(1, 4, i+1)\n        duplicates = df[df.gcd == gcd][elements].duplicated().sum()\n        non_duplicates = len(df[df.gcd == gcd]) - duplicates\n        plt.pie([non_duplicates, duplicates],\n                labels=['not duplicate', 'duplicate'],\n                colors=['gray', 'r'],\n                startangle=90)\n        plt.title(f\"GCD = {gcd}\")\n    plt.subplots_adjust(wspace=0.8)\n    plt.suptitle(title)\n    plt.show()\n        \nplot_duplicates_per_gcd(train_df, title=\"Duplicates in Training\")\nplot_duplicates_per_gcd(test_df, title=\"Duplicates in Test\")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:32:12.731963Z","iopub.execute_input":"2022-02-19T20:32:12.732282Z","iopub.status.idle":"2022-02-19T20:32:16.75625Z","shell.execute_reply.started":"2022-02-19T20:32:12.73224Z","shell.execute_reply":"2022-02-19T20:32:16.755355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight:**\n- We can reduce training and inference time by dropping the duplicates (and adjusting the sample weights).\n- If we don't drop the duplicates, we must ensure that they don't inflate the cv scores. If the validation fold contains duplicates of a training fold's values, cv scores will be too high.\n- There seems to be a fundamental difference between the low gcds (which have few duplicates) and the high gcds (which have many duplicates). ","metadata":{}},{"cell_type":"markdown","source":"# Understanding the simulated errors\n\nNow we plot two arbitrary features at the highest precision (1000000 decamers). We see that the points of every class are grouped into eight clusters, the eight clusters lie on a straight line, and all these straight lines intersect in the origin:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6,6))\nplt.scatter(train_df.iloc[:,240][train_df['gcd'] == 1],\n            train_df.iloc[:,181][train_df['gcd'] == 1],\n            c=train_df.target_num[train_df['gcd'] == 1],\n            cmap='tab10', s=1)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:33:44.178336Z","iopub.execute_input":"2022-02-19T20:33:44.179163Z","iopub.status.idle":"2022-02-19T20:33:44.899071Z","shell.execute_reply.started":"2022-02-19T20:33:44.179115Z","shell.execute_reply":"2022-02-19T20:33:44.898221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again, this is explained in the paper: Although every bacterium has its characteristic spectrum, the experiment produces a lot of noise. They model this noise by replacing a part of the bacterium's decamers by random decamers, and this amounts to a scaling of the spectrum towards the origin. \n\nThe original clusters (far away from the origin) do not overlap, and classification is easy. With higher error rates, the clusters get nearer to the origin and start to overlap. Of course: higher error rates make classification more difficult.\n\nThe following histograms show that there are eight scalings (corresponding to eight error rates), and that the scalings are the same for all ten bacteria. We can read the eight values out of the histograms or determine them by a one-dimensional k-means clustering. The seven scaled-down clusters have size 550 and no duplicates within them; the unscaled cluster has size 1100 (of which 800 are unique and 300 are duplicates).","metadata":{}},{"cell_type":"code","source":"# Show how the spectra are scaled\n# The original spectrum gives a peak at 1.0;\n# the seven error rates give seven other peaks in the histograms\n# The eight peaks in the histograms correspond to the eight clusters \n# in the scattergram above\nv = train_df[elements].abs().sum(axis=1)\nchosen_gcd = 1\n\nplt.figure(figsize=(18, 14))\nplt.tight_layout()\nfor t in range(10): # loop over the ten bacteria species\n    plt.subplot(5, 2, t+1)\n    plt.title(le.inverse_transform([t])[0])\n    \n    # Select a single GCD and a single species\n    vt = v[(train_df['gcd'] == chosen_gcd) & (train_df['target_num'] == t)]\n    \n    # Do a one-dimensional clustering to get the cluster centers and the cluster sizes\n    km = KMeans(n_clusters=8)\n    km.fit(vt.values.reshape(-1, 1))\n    cluster_max = km.cluster_centers_.max() # label this cluster with 1.0 (no simulated errors)\n    print(le.inverse_transform(np.array([t]))[0])\n    print('Cluster centers:', sorted((km.cluster_centers_ / cluster_max).ravel().round(2)))\n    print('Cluster sizes:', np.unique(km.predict(vt.values.reshape(-1, 1)), return_counts=True)[1][np.argsort(km.cluster_centers_.ravel())])\n    print('Cluster unique elements:', \n          train_df[elements][(train_df['gcd'] == chosen_gcd) & (train_df['target_num'] == t)].\n          groupby(km.predict(vt.values.reshape(-1, 1))).apply(lambda df: np.unique(df.values, axis=0).shape[0]).\n          values[np.argsort(km.cluster_centers_.ravel())])\n    print()\n    \n    # Plot a histogram of the eight clusters\n    plt.hist(vt / cluster_max, bins=np.linspace(0, (vt / cluster_max).max(), 200),\n             color='m', density=True)\n    plt.xticks(ticks=(km.cluster_centers_ / cluster_max).round(2))\n    plt.xlabel('scale')\n    plt.ylabel('density')\n    #plt.ylim(0, 1100)\nplt.subplots_adjust(hspace=0.5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:32:17.493826Z","iopub.execute_input":"2022-02-19T20:32:17.494356Z","iopub.status.idle":"2022-02-19T20:32:26.45821Z","shell.execute_reply.started":"2022-02-19T20:32:17.494324Z","shell.execute_reply":"2022-02-19T20:32:26.457556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Insight:**\n- It should be possible to create a model which has 286 parameters per bacterium. These parameters should suffice to classify all samples. To fit these parameters, it may be useful to give the samples farther away from the origin a higher sample weight.\n- Something is strange with the duplicates. These duplicates need further investigation.","metadata":{}},{"cell_type":"markdown","source":"# Comparing train and test\n\nWe can plot training and test data in the same diagram. There is a small deviation. This was announced in the paper: We train the classifier with some bacteria DNA, but the bacteria in the test set will have undergone mutation and have slightly different DNA.","metadata":{}},{"cell_type":"code","source":"scale = 1\n\n# Compute the PCA\npca = PCA(whiten=True, random_state=1)\npca.fit(train_i[elements][train_df['gcd'] == scale])\n\n# Transform the data so that the components can be analyzed\nXt_tr = pca.transform(train_i[elements][train_df['gcd'] == scale])\nXt_te = pca.transform(test_i[elements][test_df['gcd'] == scale])\n\n# Plot a scattergram, projected to two PCA components, of training and test data\nplt.figure(figsize=(6,6))\nplt.scatter(Xt_tr[:,0], Xt_tr[:,1], c='b', s=1, label='Train')\nplt.scatter(Xt_te[:,0], Xt_te[:,1], c='r', s=1, label='Test')\nplt.title(\"The test data deviate from the training data\")\nplt.legend()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-19T20:32:26.459189Z","iopub.execute_input":"2022-02-19T20:32:26.45981Z","iopub.status.idle":"2022-02-19T20:32:29.701538Z","shell.execute_reply.started":"2022-02-19T20:32:26.459769Z","shell.execute_reply":"2022-02-19T20:32:29.700611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Every dot in the diagram represents a cluster of 500 observations.\n\n**Insight:**\n- Training a classifier on the training data isn't enough. For really accurate results, we will somehow need to model the deviation between training and test data.\n- We have to think about our cross-validation strategy: What sense does it make to cross-validate if the validation data are distributed differently from the test data?\n- Maybe we can artificially generate data with the test distribution by fitting the spectra of the test bacteria and then using a [numpy.random.Generator.multinomial](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multinomial.html) to augment the data.\n- Or perhaps pseudo-labeling or another semi-supervised method will serve as magic trick.","metadata":{}},{"cell_type":"markdown","source":"# Modeling approach\n\nThe data we have seen suggest that gradient boosting is not necessarily the optimal method for the task at hand. Maybe it is better to fit the characteristic spectrum for every bacterium and then try a maximum likelihood classification.\n\nThe cross-validation strategy will have to take the drift of the test distribution into account.\n\nThe gcd can be used as a feature, or we can create four separate classifiers based on gcd.\n\nAnother approach could be to use a [numpy.random.Generator.multinomial](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multinomial.html) to augment the data.","metadata":{}}]}