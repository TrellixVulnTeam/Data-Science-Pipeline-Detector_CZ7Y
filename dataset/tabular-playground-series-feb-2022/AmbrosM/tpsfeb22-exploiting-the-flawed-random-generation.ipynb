{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exploiting the Flawed Random Generation\n\nThis notebook shows how to exploit the flawed random generation process of the February TPS competition.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom math import factorial\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import RadiusNeighborsClassifier\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-26T20:56:02.906899Z","iopub.execute_input":"2022-02-26T20:56:02.907605Z","iopub.status.idle":"2022-02-26T20:56:04.068171Z","shell.execute_reply.started":"2022-02-26T20:56:02.907499Z","shell.execute_reply":"2022-02-26T20:56:04.067179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following four lines of code demonstrate the flawed random generation. Let us generate two arrays of 30 digits in the range from 0 through 9. The `choice()` function requires an array of ten probabilities as input. We call `choice()` twice with different probability arrays, but with the same seed:\n","metadata":{}},{"cell_type":"code","source":"prob_train = [0.10, 0.10, 0.10, 0.10, 0.1, 0.10, 0.08, 0.12, 0.10, 0.10]\nprob_test  = [0.12, 0.09, 0.09, 0.15, 0.1, 0.05, 0.10, 0.10, 0.09, 0.11]\n\nprint('Train:', np.random.RandomState(seed=231).choice(10, size=30, p=prob_train))\nprint('Test: ', np.random.RandomState(seed=231).choice(10, size=30, p=prob_test))\n","metadata":{"execution":{"iopub.status.busy":"2022-02-26T20:56:04.07012Z","iopub.execute_input":"2022-02-26T20:56:04.070461Z","iopub.status.idle":"2022-02-26T20:56:04.083224Z","shell.execute_reply.started":"2022-02-26T20:56:04.070415Z","shell.execute_reply":"2022-02-26T20:56:04.082289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can see that the two random arrays have almost the same content except for three positions where the training array has a 4 and the test array has a 3. Why is this the case? The answer can be found in the [source code of the np.random.RandomState.choice function](https://github.com/numpy/numpy/blob/4a8007d5d916126965e811cd1b41ff4de44663b3/numpy/random/mtrand.pyx#L954-L957). The function first generates 30 random floats between 0 and 1 and then maps them to p.cumsum(). If we apply a small change to p, we'll only get a small change in the output:\n\n```\ncdf = p.cumsum()\nuniform_samples = self.random_sample(shape)\nidx = cdf.searchsorted(uniform_samples, side='right')\n```\n\nThe demonstration above corresponds to how the data for this competition was generated: The authors used random decamers of one bacterium with specific decamer probabilities for training, and they used random decamers of a related bacterium with slightly different probabilities for testing. Their mistake was that they used the same seed for both datasets. We are going to exploit this mistake to find the matching train row for some test rows.\n\nIn some sense, we are implementing a nearest neighbors classification algorithm, but first we need to transform the data so that the matching rows become nearest neighbors.\n\n# Preparation\n\nWe prepare the classification as always, reading the data into `train_df` and `test_df`, converting the features into integers (`train_i` and `test_i`), converting the labels into numbers (0..9) and adding a `gcd` column.","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/tabular-playground-series-feb-2022/train.csv')\ntest_df = pd.read_csv('../input/tabular-playground-series-feb-2022/test.csv')\n\nelements = [e for e in train_df.columns if e != 'row_id' and e != 'target']\n\n# Convert the 10 bacteria names to the integers 0 .. 9\nle = LabelEncoder()\ntrain_df['target_num'] = le.fit_transform(train_df.target)\n\ntrain_df.shape, test_df.shape","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-26T20:56:04.084435Z","iopub.execute_input":"2022-02-26T20:56:04.085114Z","iopub.status.idle":"2022-02-26T20:56:41.809395Z","shell.execute_reply.started":"2022-02-26T20:56:04.085078Z","shell.execute_reply":"2022-02-26T20:56:41.80862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute gcd and integer representations\ndef bias_of(s):\n    \"\"\"\n    Bias is between 9.5e-7 and 2.4e-2. The sum of all biases is 1.\"\"\"\n    w = int(s[1:s.index('T')])\n    x = int(s[s.index('T')+1:s.index('G')])\n    y = int(s[s.index('G')+1:s.index('C')])\n    z = int(s[s.index('C')+1:])\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\nbias_vector = np.array([bias_of(col) for col in elements])\n\ntrain_i = pd.DataFrame(((train_df[elements].values + bias_vector) * 1000000).round().astype(int), columns=elements, index=train_df.index)\ntest_i = pd.DataFrame(((test_df[elements].values + bias_vector) * 1000000).round().astype(int), columns=elements, index=test_df.index)\n\ntrain_df['gcd'] = np.gcd.reduce(train_i[elements], axis=1)\ntest_df['gcd'] = np.gcd.reduce(test_i[elements], axis=1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-26T20:56:41.81147Z","iopub.execute_input":"2022-02-26T20:56:41.811951Z","iopub.status.idle":"2022-02-26T20:56:44.02904Z","shell.execute_reply.started":"2022-02-26T20:56:41.811906Z","shell.execute_reply":"2022-02-26T20:56:44.028206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we select all rows with gcd = 10000 and create dataframes `Z_tr` and `Z_te` so that every row has 286 features whose sum is 100 (because every row corresponds to 100 decamers of a bacterium's DNA). We then drop the duplicates from the training data to reduce the running time.","metadata":{}},{"cell_type":"code","source":"# Select training samples with gcd=10000 (i.e. num_reads=100), drop duplicates and convert to integer\nZ_tr = (train_i[(train_df.gcd == 10000)].drop_duplicates(elements) // 10000)\ny_tr = train_df[(train_df.gcd == 10000)].drop_duplicates(elements).target_num\n\n# Select test samples with gcd=10000 (i.e. num_reads=100) and convert to integer\nZ_te = (test_i[(test_df.gcd == 10000)] // 10000)\nZ_tr.shape, y_tr.shape, Z_te.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-26T20:56:44.03036Z","iopub.execute_input":"2022-02-26T20:56:44.030647Z","iopub.status.idle":"2022-02-26T20:56:44.893472Z","shell.execute_reply.started":"2022-02-26T20:56:44.030609Z","shell.execute_reply":"2022-02-26T20:56:44.892593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we plot a 2d projection of the data, we cannot see the dependency between train and test yet:","metadata":{}},{"cell_type":"code","source":"def plot_pca(tr, te, title):\n    pca = PCA(n_components=2)\n    tr_p = pca.fit_transform(tr)\n    te_p = pca.transform(te)\n\n    plt.figure(figsize=(18, 8))\n    plt.gca().set_facecolor((0.7, 0.7, 0.7))\n    #plt.scatter(tr_p[:,0], tr_p[:,1], s=3, c=y_tr, cmap='tab10', label='Training')\n    plt.scatter(tr_p[:,0], tr_p[:,1], s=3, c='#0057b8', label='Train') # train: blue\n    plt.scatter(te_p[:,0], te_p[:,1], s=3, c='#ffd700', label='Test') # test: yellow\n    plt.legend()\n    plt.title(title)\n    plt.show()\n    \nplot_pca(Z_tr, Z_te, 'The untransformed data')","metadata":{"execution":{"iopub.status.busy":"2022-02-26T20:56:44.894908Z","iopub.execute_input":"2022-02-26T20:56:44.895335Z","iopub.status.idle":"2022-02-26T20:56:46.139175Z","shell.execute_reply.started":"2022-02-26T20:56:44.895292Z","shell.execute_reply":"2022-02-26T20:56:46.138538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature transformation\n\nNow we are going to transform the data from its 286-dimensional feature space into a new 100-dimensional space with a suitable metric which makes the dependency visible.\n\nWhere do the 286 features come from? For every row of the dataframe, 100 random numbers in the range 0..285 were generated, the 100 numbers were put into 286 bins and counted. We'll now do the reverse transformation: From the 286 counts, we recreate the 100 random numbers. Every A0T0G010 gives a 0, every A0T0G1C9 gives a 1, ..., and finally every A10T0G0C0 gives a 285.\n\nThe transformation takes several minutes. It could be made faster by removing the duplicates within the test data and perhaps by using functions such as `pd.apply()`, but here I prefer readability to speed. ","metadata":{}},{"cell_type":"code","source":"%%time\n# Convert Z_tr and Z_te to arrays X_tr and X_te of shape (n_samples, 100)\ndef transform(Z):\n    ll = [] # list of lists which will be converted to a 2d array\n    for i in range(len(Z)):\n        l = [] # list which will be converted to a row of the new 2d array\n        for j in range(Z.shape[1]):\n            for k in range(Z.iloc[i, j]): l.append(j)\n        ll.append(l)\n    return np.array(ll)\n\nX_tr = transform(Z_tr)\nX_te = transform(Z_te)\nX_tr.shape, X_te.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-26T20:56:46.14028Z","iopub.execute_input":"2022-02-26T20:56:46.140584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plotting a 2d projection of the transformed data, we see that many samples occur in pairs: The diagram contains many pairs of a blue training sample and a yellow test sample. The members of such a pair have been generated with the same seed of the random generator.","metadata":{}},{"cell_type":"code","source":"plot_pca(X_tr, X_te, 'The transformed data makes the pairs visible')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction\n\nNow we could apply our whole portfolio of classifiers to the transformed data, predict labels for the test set and then decide which classifier is best. A previous version of the notebook used `ExtraTreesClassifier` and was a failure (public lb 0.98790), but `RadiusNeighborsClassifier` is good.\n\nThe `RadiusNeighborsClassifier` finds all pairs where the Manhattan distance between a test sample and a training sample is [below a given radius](https://scikit-learn.org/stable/modules/neighbors.html). If it doesn't find a training sample within the given radius, it predicts -1.\n\nThere are certainly more precise algorithms than `RadiusNeighborsClassifier` to find pairs, but `RadiusNeighborsClassifier` is good enough for the purpose.","metadata":{}},{"cell_type":"code","source":"# Predict with RadiusNeighborsClassifier\nrnc = RadiusNeighborsClassifier(radius=18, weights='distance',\n                                p=1, outlier_label=-1, n_jobs=-1)\nrnc.fit(X_tr, y_tr)\ny_pred = rnc.predict(X_te)\nprint('Unique predictions:', np.unique(y_pred))\nprint('Frequencies:', np.unique(y_pred, return_counts=True)[1])\nprint('Samples:', len(y_pred))\nprint('Predicted samples:', \n      len(y_pred) - np.unique(y_pred, return_counts=True)[1][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, we merge the predictions of the `RadiusNeighborsClassifier` into the submission of a public notebook.","metadata":{}},{"cell_type":"code","source":"# Read the top public submission and merge it with our predictions\ntop_submission = pd.read_csv('../input/extrablenderadditionv12/submission.csv')\ntest_df['target_num'] = le.transform(top_submission.target)\ntest_df.loc[(test_df.gcd == 10000), 'target_num'] = y_pred # can contain -1\ntest_df['target_num'] = np.where(test_df.target_num == -1,\n                                 le.transform(top_submission.target),\n                                 test_df.target_num)\ntest_df['target'] = le.inverse_transform(test_df.target_num.astype(int))\nsubmission = test_df[['row_id', 'target']]\nprint('Modified:', (submission.target != top_submission.target).sum())\nsubmission.to_csv('submission_radiusneighbors.csv', index=False)\nsubmission\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: A standard cross-validation for tuning the hyperparameters of the classifier is impossible in this setting. I tuned the radius value against the public leaderboard.","metadata":{}}]}