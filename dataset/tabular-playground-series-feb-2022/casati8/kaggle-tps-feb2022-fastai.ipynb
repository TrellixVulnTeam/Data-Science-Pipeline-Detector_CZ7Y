{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# The February 2022 competition with Fastai v2","metadata":{}},{"cell_type":"markdown","source":"This notebook is a quick demonstration, who to use the Fastai v2 library for a Kaggle tabular competition. Fastai v2 is based on pytorch and allows you, to build a decent machine learning application. For more information please visit the Fastai documentation: https://docs.fast.ai/. I will link to \"Chapter 9, Tabular Modelling Deep Dive\" and the notebook \"09_tabular.ipynb\"","metadata":{}},{"cell_type":"markdown","source":"This monthly competition is a classical classification task: find the correct bacteria species for the offered test data set. In this notebook i will use a neural network approach and i will train this network with the offered traing data set.\n\nLet's start and import the needed stuff ..","metadata":{}},{"cell_type":"code","source":"from fastai.tabular.all import * \nfrom fastai.test_utils import show_install\nfrom math import factorial\n\nshow_install()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-28T10:31:34.408188Z","iopub.execute_input":"2022-02-28T10:31:34.408616Z","iopub.status.idle":"2022-02-28T10:31:34.629993Z","shell.execute_reply.started":"2022-02-28T10:31:34.408558Z","shell.execute_reply":"2022-02-28T10:31:34.629077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:31:34.634382Z","iopub.execute_input":"2022-02-28T10:31:34.634751Z","iopub.status.idle":"2022-02-28T10:31:34.648515Z","shell.execute_reply.started":"2022-02-28T10:31:34.634705Z","shell.execute_reply":"2022-02-28T10:31:34.646436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed_value(seed=718):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nset_seed_value()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:31:34.652983Z","iopub.execute_input":"2022-02-28T10:31:34.653313Z","iopub.status.idle":"2022-02-28T10:31:34.664379Z","shell.execute_reply.started":"2022-02-28T10:31:34.653272Z","shell.execute_reply":"2022-02-28T10:31:34.663438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = Path('../input/tabular-playground-series-feb-2022')\nPath.BASE_PATH = path\npath.ls()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:31:34.670579Z","iopub.execute_input":"2022-02-28T10:31:34.671597Z","iopub.status.idle":"2022-02-28T10:31:34.68356Z","shell.execute_reply.started":"2022-02-28T10:31:34.671552Z","shell.execute_reply":"2022-02-28T10:31:34.682518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(os.path.join(path, 'train.csv')).set_index(\"row_id\")\ntest_df = pd.read_csv(os.path.join(path, 'test.csv')).set_index(\"row_id\")\nsample_submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:31:34.686701Z","iopub.execute_input":"2022-02-28T10:31:34.687461Z","iopub.status.idle":"2022-02-28T10:31:59.876021Z","shell.execute_reply.started":"2022-02-28T10:31:34.687414Z","shell.execute_reply":"2022-02-28T10:31:59.874931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I use Pandas to import them and to verify, where null values are there or some values are missing. The result shows, that the data set is complete, so that no additional data completion is needed. That's a goog result!","metadata":{}},{"cell_type":"code","source":"train_df.isna().sum().sum(), test_df.isna().sum().sum(), train_df.isnull().sum().sum(), test_df.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:31:59.87799Z","iopub.execute_input":"2022-02-28T10:31:59.878317Z","iopub.status.idle":"2022-02-28T10:32:00.306982Z","shell.execute_reply.started":"2022-02-28T10:31:59.878272Z","shell.execute_reply":"2022-02-28T10:32:00.305757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have look on the training data set.","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:00.308503Z","iopub.execute_input":"2022-02-28T10:32:00.30935Z","iopub.status.idle":"2022-02-28T10:32:00.341017Z","shell.execute_reply.started":"2022-02-28T10:32:00.309297Z","shell.execute_reply":"2022-02-28T10:32:00.339792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We must define our depended variable 'target' and get the number of different bacteria species in the training data set. 10 different species are detected.","metadata":{}},{"cell_type":"code","source":"dep_var= 'target'\nindep_vars = [e for e in train_df.columns if e != dep_var]\n\ntrain_df[dep_var] = train_df[dep_var].astype('category')\nnunOfBacteriaTypes = len(train_df[dep_var].unique())\n\nprint('Number of dectected species ', nunOfBacteriaTypes)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:00.343297Z","iopub.execute_input":"2022-02-28T10:32:00.343642Z","iopub.status.idle":"2022-02-28T10:32:00.377491Z","shell.execute_reply.started":"2022-02-28T10:32:00.343582Z","shell.execute_reply":"2022-02-28T10:32:00.376501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That's a good result, all species or target values are equal distributed, there is no sknewness!","metadata":{}},{"cell_type":"markdown","source":"Oher of this competition mentioned a public paper, where a analysis method for bacterial species is described. The following link refers to the publication: https://www.frontiersin.org/articles/10.3389/fmicb.2020.00257/full\nThe paper describes a method, where some bais values are add to the orignal measued valuea and how the are transformed into values like our ones. At the following cells i will add a function to transform the folating point values into integer values.  These transformed values are used as learning data for the neural network below.","metadata":{}},{"cell_type":"code","source":"do_conversion = True","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:00.379263Z","iopub.execute_input":"2022-02-28T10:32:00.379501Z","iopub.status.idle":"2022-02-28T10:32:00.386494Z","shell.execute_reply.started":"2022-02-28T10:32:00.37947Z","shell.execute_reply":"2022-02-28T10:32:00.385281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_bias(w, x, y, z):\n    return factorial(10) / (factorial(int(w)) * factorial(int(x)) * factorial(int(y)) * factorial(int(z)) * 4**10)\n\ndef convert_to_FBC_spectrum(df):\n    for col in indep_vars:\n        coeffs = re.findall(r'\\d+', col)\n        bias = get_bias(coeffs[0], coeffs[1], coeffs[2], coeffs[3])\n        df[col] = ((df[col] + bias) *1e6).round()\n        df[col] = df[col].astype(int)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:00.392033Z","iopub.execute_input":"2022-02-28T10:32:00.392285Z","iopub.status.idle":"2022-02-28T10:32:00.401285Z","shell.execute_reply.started":"2022-02-28T10:32:00.392255Z","shell.execute_reply":"2022-02-28T10:32:00.400013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if do_conversion:\n    train_df = convert_to_FBC_spectrum(train_df)\n    test_df = convert_to_FBC_spectrum(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:00.403466Z","iopub.execute_input":"2022-02-28T10:32:00.40404Z","iopub.status.idle":"2022-02-28T10:32:34.124699Z","shell.execute_reply.started":"2022-02-28T10:32:00.403775Z","shell.execute_reply":"2022-02-28T10:32:34.123651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:34.126576Z","iopub.execute_input":"2022-02-28T10:32:34.126902Z","iopub.status.idle":"2022-02-28T10:32:34.158777Z","shell.execute_reply.started":"2022-02-28T10:32:34.126856Z","shell.execute_reply":"2022-02-28T10:32:34.157794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's list the value counts for each column. We can use this info later, when we split the columns into catgegorised and continous variables. ","metadata":{}},{"cell_type":"code","source":"val_counts = pd.DataFrame(columns=['col', 'value_counts'])\n\nfor col in indep_vars:\n    val_counts['col'] = col\n    val_counts['value_counts'] = train_df[col].value_counts()\n    \nval_counts =val_counts.sort_values(by=['value_counts'], ascending=True)\nval_counts.head(10)    ","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:34.160321Z","iopub.execute_input":"2022-02-28T10:32:34.163902Z","iopub.status.idle":"2022-02-28T10:32:34.954647Z","shell.execute_reply.started":"2022-02-28T10:32:34.163868Z","shell.execute_reply":"2022-02-28T10:32:34.953702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we will check whether duplicated rows in the training dataset exists. We can remove these duplicates by setting the flag 'drop_duplicates' ","metadata":{}},{"cell_type":"code","source":"drop_duplicates = False","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:34.95664Z","iopub.execute_input":"2022-02-28T10:32:34.957697Z","iopub.status.idle":"2022-02-28T10:32:34.96304Z","shell.execute_reply.started":"2022-02-28T10:32:34.957647Z","shell.execute_reply":"2022-02-28T10:32:34.96201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:34.964571Z","iopub.execute_input":"2022-02-28T10:32:34.965366Z","iopub.status.idle":"2022-02-28T10:32:36.527881Z","shell.execute_reply.started":"2022-02-28T10:32:34.965316Z","shell.execute_reply":"2022-02-28T10:32:36.526746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if drop_duplicates:\n    train_df = train_df.drop_duplicates()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:36.529437Z","iopub.execute_input":"2022-02-28T10:32:36.529778Z","iopub.status.idle":"2022-02-28T10:32:36.536547Z","shell.execute_reply.started":"2022-02-28T10:32:36.529734Z","shell.execute_reply":"2022-02-28T10:32:36.535488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We know that the test dataset contains also duplicated rows. We will get their indices and we will store them in a list of list to verify the predicted species later on. The assumption is that all rows with duplicated rows must have the same  specie name.","metadata":{}},{"cell_type":"code","source":"def get_index_of_duplicated(df):\n    tmp_df = df.copy()\n    tmp_df = tmp_df[tmp_df.duplicated(keep=False)]\n    duplicated_ids = tmp_df.groupby(list(tmp_df)).apply(lambda x: tuple(x.index)).tolist()\n    return duplicated_ids","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:36.538132Z","iopub.execute_input":"2022-02-28T10:32:36.539261Z","iopub.status.idle":"2022-02-28T10:32:36.548008Z","shell.execute_reply.started":"2022-02-28T10:32:36.539182Z","shell.execute_reply":"2022-02-28T10:32:36.546962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicated_test_ids = get_index_of_duplicated(test_df)\n\nlen(duplicated_test_ids)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:36.549394Z","iopub.execute_input":"2022-02-28T10:32:36.550402Z","iopub.status.idle":"2022-02-28T10:32:47.300684Z","shell.execute_reply.started":"2022-02-28T10:32:36.550357Z","shell.execute_reply":"2022-02-28T10:32:47.299701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see the first 5 entries of the list. The rows (247934, 260325, 267489, 280701, 299358) must have the same predicted specie later on. And (294925, 296888) and so on.","metadata":{}},{"cell_type":"code","source":"duplicated_test_ids[:5]","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:47.302592Z","iopub.execute_input":"2022-02-28T10:32:47.302971Z","iopub.status.idle":"2022-02-28T10:32:47.310961Z","shell.execute_reply.started":"2022-02-28T10:32:47.302924Z","shell.execute_reply":"2022-02-28T10:32:47.309868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"How are the target values or species distributed in the training data set?","metadata":{}},{"cell_type":"markdown","source":"I need a list of the column names, which are candidates for category variables and which are no candidates, also called continous variables. The Fastai library offers the function 'cont_cat_split' to do this for us. Our training data set contains only floating values for the independed variables, therefore we expect that no category variables are available.","metadata":{}},{"cell_type":"code","source":"cont_vars, cat_vars = cont_cat_split(train_df, dep_var=dep_var)\nlen(cat_vars), len(cont_vars)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T11:02:16.680196Z","iopub.execute_input":"2022-02-28T11:02:16.680504Z","iopub.status.idle":"2022-02-28T11:02:17.143189Z","shell.execute_reply.started":"2022-02-28T11:02:16.680471Z","shell.execute_reply":"2022-02-28T11:02:17.142068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next step is to create a data loader. The Fastai library offers a powerful helper called 'TabularPandas'. It needs the data frame, list of the category and continous variables, the depened variable and a splitter. The splitter divides the data set into two parts: one for the training and one for the validation and for internal optimization step in each epoch. The batch size is set to 1024, because we have a large data set. We can use a random split because the rows in the data set are independed.\n\nYou can use the option 'genSmallDataset=True' if you need for a quick turn around or for a prototyping a faster learning process.","metadata":{}},{"cell_type":"code","source":"def getData(df, batchSize=1024, randomSplit=True, genSmallDataset=False):\n    \n  if genSmallDataset: \n    example_idx = np.random.choice(range(len(df)), 25000)\n    df = df.iloc[example_idx]\n  \n  splits = null\n  if randomSplit:  \n    splits = RandomSplitter(valid_pct=0.2, seed=718)(range_of(df))\n  else:\n    l = len(df)\n    splits = (L(np.arange(0, 0.8*l), use_list=True),\n              L(np.arange(0.8*l+1, l-1), use_list=True))\n  to_train = TabularPandas(df, \n                           [Categorify, Normalize],\n                           cat_vars,\n                           cont_vars, \n                           splits=splits,  \n                           device = device,\n                           y_block=CategoryBlock(),\n                           y_names=dep_var) \n\n  return to_train.dataloaders(bs=batchSize)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T11:02:19.841761Z","iopub.execute_input":"2022-02-28T11:02:19.842494Z","iopub.status.idle":"2022-02-28T11:02:19.853909Z","shell.execute_reply.started":"2022-02-28T11:02:19.842458Z","shell.execute_reply":"2022-02-28T11:02:19.851123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dls = getData(train_df, batchSize=2048, randomSplit=True, genSmallDataset=False)\nlen(dls.train), len(dls.valid), type(dls.train), dls.train.device","metadata":{"execution":{"iopub.status.busy":"2022-02-28T11:02:25.395608Z","iopub.execute_input":"2022-02-28T11:02:25.395936Z","iopub.status.idle":"2022-02-28T11:02:33.177974Z","shell.execute_reply.started":"2022-02-28T11:02:25.395903Z","shell.execute_reply":"2022-02-28T11:02:33.177017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show me the transformed data, which will be used in the network later. ","metadata":{}},{"cell_type":"code","source":"dls.show_batch()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T11:03:26.54373Z","iopub.execute_input":"2022-02-28T11:03:26.544094Z","iopub.status.idle":"2022-02-28T11:03:27.003252Z","shell.execute_reply.started":"2022-02-28T11:03:26.544046Z","shell.execute_reply":"2022-02-28T11:03:27.002244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BatchSwapNoise(Module):\n    \"Swap Noise Module\"\n    def __init__(self, p): store_attr()\n\n    def forward(self, x):\n        if self.training:\n            mask = torch.rand(x.size()) > (1 - self.p)\n            l1 = torch.floor(torch.rand(x.size()) * x.size(0)).type(torch.LongTensor)\n            l2 = (mask.type(torch.LongTensor) * x.size(1))\n            res = (l1 * l2).view(-1)\n            idx = torch.arange(x.nelement()) + res\n            idx[idx>=x.nelement()] = idx[idx>=x.nelement()]-x.nelement()\n            return x.flatten()[idx].view(x.size())\n        else:\n            return x","metadata":{"execution":{"iopub.status.busy":"2022-02-28T11:03:28.711414Z","iopub.execute_input":"2022-02-28T11:03:28.711741Z","iopub.status.idle":"2022-02-28T11:03:28.721778Z","shell.execute_reply.started":"2022-02-28T11:03:28.711707Z","shell.execute_reply":"2022-02-28T11:03:28.720422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cell\nclass TabularModelBatchSwapNoise(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, emb_szs, n_cont, out_sz, layers, ps=None, embed_p=0.,\n                 y_range=None, bs_noise=None, use_bn=True, bn_final=False, bn_cont=True, act_cls=nn.ReLU(inplace=True),\n                 lin_first=True):\n        \n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont) if bn_cont else None\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont = n_emb,n_cont\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [act_cls for _ in range(len(sizes)-2)] + [None]\n        _layers = [LinBnDrop(sizes[i], sizes[i+1], bn=use_bn and (i!=len(actns)-1 or bn_final), p=p, act=a, lin_first=lin_first)\n                       for i,(p,a) in enumerate(zip(ps+[0.],actns))]\n        if y_range is not None: _layers.append(SigmoidRange(*y_range))\n        self.layers = nn.Sequential(*_layers)\n        \n        self.bsn_value = bs_noise\n        if(self.bsn_value != None): self.batch_swap_noise = BatchSwapNoise(self.bsn_value)\n\n    def forward(self, x_cat, x_cont=None):\n        if self.bsn_value != None:\n            x_cat = self.batch_swap_noise(x_cat)\n            x_cont = self.batch_swap_noise(x_cont)\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            if self.bn_cont is not None: x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        return self.layers(x)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:57.894867Z","iopub.execute_input":"2022-02-28T10:32:57.895769Z","iopub.status.idle":"2022-02-28T10:32:57.917061Z","shell.execute_reply.started":"2022-02-28T10:32:57.895724Z","shell.execute_reply":"2022-02-28T10:32:57.915648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@delegates(Learner.__init__)\ndef tabular_learner_with_swap_noise(dls, layers=None, emb_szs=None, config=None, n_out=None, y_range=None,  **kwargs):\n    \"Get a `Learner` using `dls`, with `metrics`, including a `TabularModel` created using the remaining params.\"\n    if config is None: config = tabular_config()\n    if layers is None: layers = [286,100]\n    to = dls.train_ds\n    emb_szs = get_emb_sz(dls.train_ds, {} if emb_szs is None else emb_szs)\n    if n_out is None: n_out = get_c(dls)\n    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n    if y_range is None and 'y_range' in config: y_range = config.pop('y_range')\n    # use a 'batch swap noise' to swap some columns \n    bs_noise=None\n    if 'bs_noise' in config: bs_noise = config.pop('bs_noise')\n    model = TabularModelBatchSwapNoise(emb_szs, len(dls.cont_names), n_out, layers, y_range=y_range, bs_noise=bs_noise, **config)\n    return TabularLearner(dls, model, **kwargs)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T10:32:57.918523Z","iopub.execute_input":"2022-02-28T10:32:57.919655Z","iopub.status.idle":"2022-02-28T10:32:57.932753Z","shell.execute_reply.started":"2022-02-28T10:32:57.919608Z","shell.execute_reply":"2022-02-28T10:32:57.931539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At least i create a learner pasing the dataloader into it. The default settings are two hidden layers with 200 and 100 elements. Remember that we have 286 independed variables, therefor i choose a hidden layer structure of [286,128] as my default and baseline strcuture..\n\nIncreasing the number of parameters in the neural network will improve the accuarcy and score, hopefully.","metadata":{}},{"cell_type":"code","source":"my_config = tabular_config(ps=0.15, \n                           embed_p=0.15, \n                           use_bn=True, \n                           bn_cont=True, \n                           bs_noise=0.15, \n                           y_range=(0, nunOfBacteriaTypes-1))\n\nlearn = tabular_learner_with_swap_noise(dls,\n                        n_out = nunOfBacteriaTypes,\n                        #layer=[286,100], as default or baseline\n                        layers=[1024,1024,1204,512,128,32],\n                        config=my_config,\n                        metrics=[accuracy])\nlearn.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T12:27:35.403308Z","iopub.execute_input":"2022-02-28T12:27:35.403622Z","iopub.status.idle":"2022-02-28T12:27:35.700666Z","shell.execute_reply.started":"2022-02-28T12:27:35.403589Z","shell.execute_reply":"2022-02-28T12:27:35.699399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"learn.lr_find()","metadata":{"execution":{"iopub.status.busy":"2022-02-28T12:27:42.369962Z","iopub.execute_input":"2022-02-28T12:27:42.370265Z","iopub.status.idle":"2022-02-28T12:27:49.648841Z","shell.execute_reply.started":"2022-02-28T12:27:42.370231Z","shell.execute_reply":"2022-02-28T12:27:49.647843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will use a maximum learning rate of 5e-3. Starting the learning process is quite easy, i will run for 600 epochs. I will save the model with the best, with the lowest validation lost value. The Fastai library offers the SaveModelCallback callback. You must specify the file name only. The option with_opt=True stores the values of the optimizer also. You will find the new file under models/kaggle_tps_feb_2022.pth","metadata":{}},{"cell_type":"code","source":"learn.fit_one_cycle(600, 5e-3, wd=0.01, cbs=SaveModelCallback(fname='kaggle_tps_feb_2022', with_opt=True)) ","metadata":{"execution":{"iopub.status.busy":"2022-02-28T12:27:55.259538Z","iopub.execute_input":"2022-02-28T12:27:55.26037Z","iopub.status.idle":"2022-02-28T13:19:45.379151Z","shell.execute_reply.started":"2022-02-28T12:27:55.260322Z","shell.execute_reply":"2022-02-28T13:19:45.378121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The confusion matrix below shows us the quality of test data predictions.","metadata":{}},{"cell_type":"code","source":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(normalize=True, norm_dec=3, figsize=(10, 10))","metadata":{"execution":{"iopub.status.busy":"2022-02-28T13:21:13.604823Z","iopub.execute_input":"2022-02-28T13:21:13.605643Z","iopub.status.idle":"2022-02-28T13:21:15.251733Z","shell.execute_reply.started":"2022-02-28T13:21:13.605605Z","shell.execute_reply":"2022-02-28T13:21:15.250739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To calculate the predictions for this competition, i will load the best model from the training process. Best model means the model with the lowest validation loss value.","metadata":{}},{"cell_type":"code","source":" learn.load('kaggle_tps_feb_2022')","metadata":{"execution":{"iopub.status.busy":"2022-02-28T13:21:19.015106Z","iopub.execute_input":"2022-02-28T13:21:19.01807Z","iopub.status.idle":"2022-02-28T13:21:19.080926Z","shell.execute_reply.started":"2022-02-28T13:21:19.017974Z","shell.execute_reply":"2022-02-28T13:21:19.079963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now it's time to calculate the predictions for the test data set.","metadata":{}},{"cell_type":"code","source":"dlt = learn.dls.test_dl(test_df, bs=4096) \npreds, _ = learn.get_preds(dl=dlt) \nprint(preds[:2])","metadata":{"execution":{"iopub.status.busy":"2022-02-28T13:21:21.10958Z","iopub.execute_input":"2022-02-28T13:21:21.109902Z","iopub.status.idle":"2022-02-28T13:21:35.131113Z","shell.execute_reply.started":"2022-02-28T13:21:21.109868Z","shell.execute_reply":"2022-02-28T13:21:35.130002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I got the 'one hot encoded' prediction values, which are probabilities for the different specie types. np.argmax returns the index with the maximum probability value, like 1 or 7. This index must be translated into the specie name.  The function 'vocab.map_ids' returns the name for the index values.","metadata":{}},{"cell_type":"code","source":"decoded_preds_str = dls.train.categorize.vocab.map_ids(np.argmax(preds, axis=1))\nsample_submission[dep_var] =  decoded_preds_str\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T13:21:57.503956Z","iopub.execute_input":"2022-02-28T13:21:57.504277Z","iopub.status.idle":"2022-02-28T13:21:58.45531Z","shell.execute_reply.started":"2022-02-28T13:21:57.504245Z","shell.execute_reply":"2022-02-28T13:21:58.454332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the next step i verify that the rows with duplicated features in the test dataset predict the same specie name: they must have 1 unique name. If not the prediction generates a wrong value and is less accurate.","metadata":{}},{"cell_type":"code","source":"tmp_submission = sample_submission.copy()\ntmp_submission.set_index('row_id', inplace=True)\n\nerror_count = 0\nfor idxs in duplicated_test_ids:\n    dups = tmp_submission[tmp_submission.index.isin(idxs)]\n    num_unique_values = dups.nunique().item()\n    if num_unique_values != 1:\n        print( dups)\n        error_count  += 1\n        \nprint('Number of wrong duplicated predictions ', error_count)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T13:22:04.154772Z","iopub.execute_input":"2022-02-28T13:22:04.155674Z","iopub.status.idle":"2022-02-28T13:22:34.491723Z","shell.execute_reply.started":"2022-02-28T13:22:04.15562Z","shell.execute_reply":"2022-02-28T13:22:34.490458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The following table shows the distribution of the different specie name in the training data set and in the prediction from the test data set. The total number and the quota in percent are shown. I assume that the distribution in the test data sholld be same as in the training data set. Do you agree?","metadata":{}},{"cell_type":"code","source":"dep_var_dist = pd.DataFrame({\n    'target_count': train_df[dep_var].value_counts(),\n    'target_quota (%)': train_df[dep_var].value_counts() / train_df.shape[0] * 100,\n})\n\ndep_var_dist['pred_count'] = pd.Series(decoded_preds_str, index=test_df.index).value_counts()\ndep_var_dist['pred_quota (%)'] = dep_var_dist['pred_count'] / len(test_df) * 100\ndep_var_dist.sort_index().head(11)","metadata":{"execution":{"iopub.status.busy":"2022-02-28T13:22:38.314349Z","iopub.execute_input":"2022-02-28T13:22:38.314659Z","iopub.status.idle":"2022-02-28T13:22:38.358947Z","shell.execute_reply.started":"2022-02-28T13:22:38.314623Z","shell.execute_reply":"2022-02-28T13:22:38.357985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -la ","metadata":{"execution":{"iopub.status.busy":"2022-02-28T13:22:41.644434Z","iopub.execute_input":"2022-02-28T13:22:41.645009Z","iopub.status.idle":"2022-02-28T13:22:42.567266Z","shell.execute_reply.started":"2022-02-28T13:22:41.644959Z","shell.execute_reply":"2022-02-28T13:22:42.566129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The end!","metadata":{}}]}