{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Extra-Trees, DNN, XGBoost ðŸŒ±\n\n**Overview**\n\nThis Notebook explores the benefits of combining multiple model predictions to enhance the inference capabilities,</br> \nI used the following Notebook as Guidance https://www.kaggle.com/dmitryuarov/forest-of-extra-trees-0-9895-up-to-4th-place so thanks very much to the author for sharing\nI apply multiple modifications to the original code based on my experience in this competition and other models that I developed.\n\n**Goal:** Explore model blending for TPS February 2022, Improve LB Score\n\n---\n**Notebook Overview**\n- Notebook Goals: A few lines on the main objective of this Notebook\n- Table of Content\n- Notebook Updates\n- Future Ideas to Implement\n\n**Installing Machine Learning Libraries**\n\n**Loading the Required Python Libraries**\n\n**Loading the Dataset Information**\n\n**Understanding the Information Loaded, EDA, and Others**\n\n**Data Pre-Processing**\n\n- Memory Optimization\n- Outlier Elimination\n- Other Modifications Required (Merge, Join, Others)\n\n**Feature Engineering.**\n\n**Data Processing for Training**\n\n- Label Encoding\n- Feature Selection and Creation of Train Dataset and Labels\n- Train, Test Split\n\n**Baseline Model.**\n\n**Model Inference and Evaluation.**\n\n**Cross-Validation Loop.**\n\n**Advance Model Development and Training.**\n\n**Model Inference and Evaluation.**\n\n**Development of Other Model Architectures.**\n\n**Model Ensembling, Blending, or Stacking.**\n\n**Model Inference and Evaluation.**\n\n**Model Submission.**\n...\n\n---\n\n**Notebook Updates**\n\n...\n\n**Future Ideas**\n...\n\n\nhttps://www.kaggle.com/dmitryuarov/forest-of-extra-trees-0-9895-up-to-4th-place","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-19T19:34:50.556843Z","iopub.execute_input":"2022-02-19T19:34:50.557233Z","iopub.status.idle":"2022-02-19T19:34:50.5829Z","shell.execute_reply.started":"2022-02-19T19:34:50.557147Z","shell.execute_reply":"2022-02-19T19:34:50.582144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom scipy import stats","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:34:50.584662Z","iopub.execute_input":"2022-02-19T19:34:50.58494Z","iopub.status.idle":"2022-02-19T19:34:51.806764Z","shell.execute_reply.started":"2022-02-19T19:34:50.584904Z","shell.execute_reply":"2022-02-19T19:34:51.805955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:34:51.8081Z","iopub.execute_input":"2022-02-19T19:34:51.809174Z","iopub.status.idle":"2022-02-19T19:34:51.815649Z","shell.execute_reply.started":"2022-02-19T19:34:51.80913Z","shell.execute_reply":"2022-02-19T19:34:51.814838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Notebook Configuration...\n\n# Amount of data we want to load into the Model...\nDATA_ROWS = None\n# Dataframe, the amount of rows and cols to visualize...\nNROWS = 50\nNCOLS = 15\n# Main data location path...\nBASE_PATH = '/kaggle/input/ubiquant-market-prediction/'","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:34:51.817193Z","iopub.execute_input":"2022-02-19T19:34:51.817501Z","iopub.status.idle":"2022-02-19T19:34:51.826065Z","shell.execute_reply.started":"2022-02-19T19:34:51.817465Z","shell.execute_reply":"2022-02-19T19:34:51.825245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.5f}'.format\npd.set_option('display.max_columns', NCOLS) \npd.set_option('display.max_rows', NROWS)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:34:51.828859Z","iopub.execute_input":"2022-02-19T19:34:51.829181Z","iopub.status.idle":"2022-02-19T19:34:51.835415Z","shell.execute_reply.started":"2022-02-19T19:34:51.829146Z","shell.execute_reply":"2022-02-19T19:34:51.834478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_data = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/train.csv')\ntst_data = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:34:51.836921Z","iopub.execute_input":"2022-02-19T19:34:51.837359Z","iopub.status.idle":"2022-02-19T19:35:25.813176Z","shell.execute_reply.started":"2022-02-19T19:34:51.837324Z","shell.execute_reply":"2022-02-19T19:35:25.812363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsub = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:25.814548Z","iopub.execute_input":"2022-02-19T19:35:25.814845Z","iopub.status.idle":"2022-02-19T19:35:25.874251Z","shell.execute_reply.started":"2022-02-19T19:35:25.814808Z","shell.execute_reply":"2022-02-19T19:35:25.873493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:25.875482Z","iopub.execute_input":"2022-02-19T19:35:25.876184Z","iopub.status.idle":"2022-02-19T19:35:25.902378Z","shell.execute_reply.started":"2022-02-19T19:35:25.876141Z","shell.execute_reply":"2022-02-19T19:35:25.901657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntst_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:25.903593Z","iopub.execute_input":"2022-02-19T19:35:25.904067Z","iopub.status.idle":"2022-02-19T19:35:25.921342Z","shell.execute_reply.started":"2022-02-19T19:35:25.904024Z","shell.execute_reply":"2022-02-19T19:35:25.920522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:25.922785Z","iopub.execute_input":"2022-02-19T19:35:25.923041Z","iopub.status.idle":"2022-02-19T19:35:28.228366Z","shell.execute_reply.started":"2022-02-19T19:35:25.923007Z","shell.execute_reply":"2022-02-19T19:35:28.227554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:28.229807Z","iopub.execute_input":"2022-02-19T19:35:28.230076Z","iopub.status.idle":"2022-02-19T19:35:28.240828Z","shell.execute_reply.started":"2022-02-19T19:35:28.230041Z","shell.execute_reply":"2022-02-19T19:35:28.24002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrn_data = reduce_mem_usage(trn_data)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:28.242099Z","iopub.execute_input":"2022-02-19T19:35:28.242739Z","iopub.status.idle":"2022-02-19T19:35:48.481471Z","shell.execute_reply.started":"2022-02-19T19:35:28.242696Z","shell.execute_reply":"2022-02-19T19:35:48.480737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntst_data = reduce_mem_usage(tst_data)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:48.482947Z","iopub.execute_input":"2022-02-19T19:35:48.483206Z","iopub.status.idle":"2022-02-19T19:35:58.658172Z","shell.execute_reply.started":"2022-02-19T19:35:48.48317Z","shell.execute_reply":"2022-02-19T19:35:58.656598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"%%time\nignore = ['target', 'row_id']\nfeatures = [feat for feat in trn_data.columns if feat not in ignore]","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:58.662233Z","iopub.execute_input":"2022-02-19T19:35:58.662451Z","iopub.status.idle":"2022-02-19T19:35:58.671293Z","shell.execute_reply.started":"2022-02-19T19:35:58.662424Z","shell.execute_reply":"2022-02-19T19:35:58.668321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ndef create_features(df):\n    \"\"\"\n    Created multiple features...\n    \"\"\"    \n    df['A_sum'] = df[features].sum(axis = 1)\n    df['A_min'] = df[features].min(axis = 1)\n    df['A_max'] = df[features].max(axis = 1)    \n    df['A_std'] = df[features].std(axis = 1)\n    df['A_mad'] = df[features].mad(axis = 1)\n    df['A_var'] = df[features].var(axis = 1)\n    df['A_mean'] = df[features].mean(axis = 1)\n    df['A_median'] = df[features].median(axis = 1)\n\n    df['A_positive'] = df.select_dtypes(include='float64').gt(0).sum(axis=1)\n    \n    df['q01'] = df[features].quantile(q=0.01, axis=1)\n    df['q05'] = df[features].quantile(q=0.05, axis=1)\n    df['q10'] = df[features].quantile(q=0.10, axis=1)\n    df['q25'] = df[features].quantile(q=0.25, axis=1)\n    df['q75'] = df[features].quantile(q=0.75, axis=1)\n    df['q90'] = df[features].quantile(q=0.90, axis=1)\n    df['q95'] = df[features].quantile(q=0.95, axis=1)\n    df['q99'] = df[features].quantile(q=0.99, axis=1)\n    df['max'] = df[features].max(axis=1)\n    df['min'] = df[features].min(axis=1)\n    \n    df['std'] = df[features].std(axis=1)\n    df['range'] = df['max'] - df['min']\n    df['iqr'] = df['q75'] - df['q25']\n    df['tails'] = df['range'] / df['iqr']\n    df['dispersion'] = df['std'] / df['A_mean']\n    df['dispersion_2'] = df['iqr'] / df['A_median']\n    df['skew'] = df[features].skew(axis=1)\n    df['kurt'] = df[features].kurt(axis=1)\n    \n    df['median-max'] = df['A_median'] - df['max']\n    df['median-min'] = df['A_median'] - df['min']\n    df['q99-q95'] = df['q99'] - df['q95']\n    df['q99-q90'] = df['q99'] - df['q90']\n    df['q01-q05'] = df['q01'] - df['q05']\n    df['q01-q10'] = df['q01'] - df['q10']\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:58.672857Z","iopub.execute_input":"2022-02-19T19:35:58.673145Z","iopub.status.idle":"2022-02-19T19:35:58.689193Z","shell.execute_reply.started":"2022-02-19T19:35:58.673107Z","shell.execute_reply":"2022-02-19T19:35:58.688415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#trn_data = create_features(trn_data)\n#tst_data = create_features(tst_data)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:58.690467Z","iopub.execute_input":"2022-02-19T19:35:58.691005Z","iopub.status.idle":"2022-02-19T19:35:58.700478Z","shell.execute_reply.started":"2022-02-19T19:35:58.690965Z","shell.execute_reply":"2022-02-19T19:35:58.69973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Post Processing","metadata":{}},{"cell_type":"code","source":"encoder = LabelEncoder()\ntrn_data['target'] = encoder.fit_transform(trn_data['target'])","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:58.701488Z","iopub.execute_input":"2022-02-19T19:35:58.702818Z","iopub.status.idle":"2022-02-19T19:35:58.754067Z","shell.execute_reply.started":"2022-02-19T19:35:58.70278Z","shell.execute_reply":"2022-02-19T19:35:58.753371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = trn_data[features]\ny = trn_data['target']\nX_test = tst_data[features]","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:58.755352Z","iopub.execute_input":"2022-02-19T19:35:58.755842Z","iopub.status.idle":"2022-02-19T19:35:59.151036Z","shell.execute_reply.started":"2022-02-19T19:35:58.755805Z","shell.execute_reply":"2022-02-19T19:35:59.150187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning Model Development","metadata":{}},{"cell_type":"code","source":"SPLITS = 5\nSEED = 51\nSHUFFLE = True","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:59.152596Z","iopub.execute_input":"2022-02-19T19:35:59.152919Z","iopub.status.idle":"2022-02-19T19:35:59.157371Z","shell.execute_reply.started":"2022-02-19T19:35:59.152879Z","shell.execute_reply":"2022-02-19T19:35:59.156422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Model","metadata":{}},{"cell_type":"code","source":"XGB_params = {'max_depth': 8,\n              'learning_rate': 0.2478225904887278, \n              'min_child_weight': 8, \n              'gamma': 0.018329940112279165, \n              'alpha': 0.00019394894279195157, \n              'lambda': 0.06161761858777205, \n              'colsample_bytree': 0.6721122683333417, \n              'subsample': 0.6155733760919804,\n              'n_estimators': 3000,\n              'tree_method': 'gpu_hist',\n              'booster': 'gbtree',\n              'random_state': 228,\n              'use_label_encoder': False,\n              'objective': 'multi:softmax',\n              'eval_metric': 'mlogloss',\n              'predictor': 'gpu_predictor'\n             }","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:59.159047Z","iopub.execute_input":"2022-02-19T19:35:59.15933Z","iopub.status.idle":"2022-02-19T19:35:59.167133Z","shell.execute_reply.started":"2022-02-19T19:35:59.159293Z","shell.execute_reply":"2022-02-19T19:35:59.166363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores, predictions = [], []\n\nk = StratifiedKFold(n_splits = SPLITS, random_state = SEED, shuffle = SHUFFLE)\n\nfor iteration, (trn_idx, val_idx) in enumerate(k.split(X, y)):\n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n    \n    model = XGBClassifier(**XGB_params)\n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], verbose = False, early_stopping_rounds = 30)\n    val_pred = model.predict(X_val)\n    val_score = accuracy_score(y_val, val_pred)\n    print(f'Fold {iteration} accuracy score: {round(val_score, 4)}')\n    \n    scores.append(val_score)\n    predictions.append(model.predict(X_test))\n    \nprint('')    \nprint(f'Mean accuracy - {round(np.mean(scores), 4)}')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:35:59.168573Z","iopub.execute_input":"2022-02-19T19:35:59.169113Z","iopub.status.idle":"2022-02-19T19:46:19.625274Z","shell.execute_reply.started":"2022-02-19T19:35:59.169072Z","shell.execute_reply":"2022-02-19T19:46:19.624364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'] = stats.mode(np.column_stack(predictions), axis = 1)[0]\nsub.to_csv('xg_boost_submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:46:19.626765Z","iopub.execute_input":"2022-02-19T19:46:19.627022Z","iopub.status.idle":"2022-02-19T19:46:22.642206Z","shell.execute_reply.started":"2022-02-19T19:46:19.626988Z","shell.execute_reply":"2022-02-19T19:46:22.64133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target']","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:46:22.644554Z","iopub.execute_input":"2022-02-19T19:46:22.645007Z","iopub.status.idle":"2022-02-19T19:46:22.654152Z","shell.execute_reply.started":"2022-02-19T19:46:22.644968Z","shell.execute_reply":"2022-02-19T19:46:22.653391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extra Trees Model","metadata":{}},{"cell_type":"code","source":"EXT_params = {'n_estimators': 2373,\n              'max_depth':3691,\n              'min_samples_split':3,\n              'min_samples_leaf':1,\n              'criterion':'gini',\n             }","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:46:22.655836Z","iopub.execute_input":"2022-02-19T19:46:22.656115Z","iopub.status.idle":"2022-02-19T19:46:22.661691Z","shell.execute_reply.started":"2022-02-19T19:46:22.65608Z","shell.execute_reply":"2022-02-19T19:46:22.66062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores, predictions = [], []\n\nk = StratifiedKFold(n_splits = SPLITS, random_state = SEED, shuffle = SHUFFLE)\n\nfor iteration, (trn_idx, val_idx) in enumerate(k.split(X, y)):\n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n    \n    model = ExtraTreesClassifier(**EXT_params)\n    model.fit(X_train, y_train)\n    val_pred = model.predict(X_val)\n    val_score = accuracy_score(y_val, val_pred)\n    print(f'Fold {iteration+1} accuracy score: {round(val_score, 4)}')\n    \n    scores.append(val_score)\n    predictions.append(model.predict(X_test))\n    \nprint('')    \nprint(f'Mean accuracy - {round(np.mean(scores), 4)}')","metadata":{"execution":{"iopub.status.busy":"2022-02-19T19:46:22.663617Z","iopub.execute_input":"2022-02-19T19:46:22.663989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'] = stats.mode(np.column_stack(predictions), axis = 1)[0]\nsub.to_csv('extra_trees_submission.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Neuronal Network","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input, Concatenate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def my_dnn_model():\n    x_input = Input(shape = (X.shape[-1]), name = \"input\")\n    x1 = Dense(256, activation='selu')(x_input)\n    b1 = BatchNormalization()(x1)\n    x2 = Dense(128, activation='selu')(b1)\n    b2 = BatchNormalization()(x2)\n    x3 = Dense(128, activation='selu')(b1)\n    b3 = BatchNormalization()(x3)\n    \n    d1 = Dropout(0.15)(Concatenate()([b2, b3]))\n    x4 = Dense(128, activation='relu')(d1) \n    b4 = BatchNormalization()(x4)\n    x5 = Dense(64, activation='selu')(b4)\n    b5 = BatchNormalization()(x5)\n    x6 = Dense(32, activation='selu')(b5)\n    b6 = BatchNormalization()(x6)\n    output = Dense(10, activation=\"softmax\", name=\"output\")(b6)\n    \n    model = tf.keras.Model(x_input, output, name='DNN_Model')\n    return model\n\nmodel = my_dnn_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, to_file='Super_Model.png', show_shapes=True,show_layer_names=True, dpi = 65)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VERBOSE = False\nBATCH_SIZE = 64\nEPOCHS = 250","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores, predictions = [], []\n\nk = StratifiedKFold(n_splits = SPLITS, random_state = SEED, shuffle = SHUFFLE)\n\nfor iteration, (trn_idx, val_idx) in enumerate(k.split(X, y)):\n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n    \n    model = my_dnn_model()\n    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n    \n    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.6, patience=3, verbose=VERBOSE)\n    es = EarlyStopping(monitor=\"val_loss\", patience=7, verbose=VERBOSE, mode=\"min\", restore_best_weights=True)\n\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n    chk_point = ModelCheckpoint(f'./TPS1_model_2022_{iteration+1}C.h5', options=save_locally, monitor='val_loss', verbose=VERBOSE, save_best_only=True, mode='min')\n    \n    \n    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, verbose=VERBOSE, batch_size=BATCH_SIZE, callbacks=[lr, chk_point, es])\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n    model = load_model(f'./TPS1_model_2022_{iteration+1}C.h5', options=load_locally)\n    \n    y_pred = model.predict(X_val, batch_size=BATCH_SIZE)\n    score = accuracy_score(y_val, np.argmax(y_pred, axis=1))\n    scores.append(score)\n    \n    predictions.append(np.argmax(model.predict(X_test, batch_size=BATCH_SIZE), axis=1))\n    print(f\"Fold-{iteration+1} | OOF Score: {score}\")\n    \nprint(f'Mean accuracy on {k.n_splits} folds - {np.mean(scores)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target'] = stats.mode(np.column_stack(predictions), axis = 1)[0]\nsub.to_csv('dnn_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub['target']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Blending Models","metadata":{}},{"cell_type":"code","source":"submission_01 = pd.read_csv('./extra_trees_submission.csv')\nsubmission_02 = pd.read_csv('./xg_boost_submission.csv')\nsubmission_03 = pd.read_csv('./dnn_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_01.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_02.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_03.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blend_predictions = []\nfor prediction in [submission_01, submission_02, submission_03]:\n    blend_predictions.append(prediction['target'])\n    \nsub['target'] = encoder.inverse_transform(stats.mode(np.column_stack(blend_predictions), axis = 1)[0])\nsub.to_csv('blended_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}