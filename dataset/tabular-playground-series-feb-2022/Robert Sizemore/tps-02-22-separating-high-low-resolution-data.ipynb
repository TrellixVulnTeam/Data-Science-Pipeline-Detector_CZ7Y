{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Separating High and Low Resolution Data\n\nThe original paper found that certain algorithms performed better on data generated from higher/low resolution samples or that had fewer/more errors. In this notebook, we subset the data based off of the number of reads from which it was generated and train models on various combinations of these subsets to see if we can get improved results. \n\n## Overview\n\nFrom the description in the original paper, each training example consists of length 10 samples of the full bacterial DNA gotten by the following process:\n\n$$\\cdots \\text{ATCG}\\textbf{CTGGATAGCA}\\text{TACG} \\cdots \\xrightarrow{sample} \\text{CTGGATAGCA} \\xrightarrow{bin} A_3T_2G_3C_2$$\n\nEach training example consists of either 1 million, 100k, 1k or 100 length 10 reads, which are then turned into a probability distribution and altered by subtracting off the bias distribution (expected distribution of randomly generated DNA sequences). We refer to the subset of data generated from 1 million or 100k reads as the **high resolution data** and the subset generated by 1000 or 100 reads as the **low resolution data**. With some work, we can retrieve (a constant multiple of) the original samples from the provided training data and we distinguish each subset by the GCD of the samples.\n\nWe can predict almost perfectly on the high resolution data and with varying results on the low resolution data. So we will attempt to train separate models on combinations of these subsets to see if we can get improved CV or public leaderboard scores. \n\n**Credits:** Some of the helper functions I used to process the data were borrowed from this [EDA notebook](https://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense) by AmbrosM, so please check it out and give it an upvote.","metadata":{}},{"cell_type":"code","source":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 99\nNUM_FOLDS = 10","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:50:26.589825Z","iopub.execute_input":"2022-02-26T18:50:26.590147Z","iopub.status.idle":"2022-02-26T18:50:26.615761Z","shell.execute_reply.started":"2022-02-26T18:50:26.590053Z","shell.execute_reply":"2022-02-26T18:50:26.614881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generic Imports\nimport numpy as np\nimport pandas as pd\nimport pyarrow\nimport time\nimport gc\n\n# Label Encoding\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\n# Hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-26T18:50:26.617526Z","iopub.execute_input":"2022-02-26T18:50:26.61792Z","iopub.status.idle":"2022-02-26T18:50:27.624039Z","shell.execute_reply.started":"2022-02-26T18:50:26.617873Z","shell.execute_reply":"2022-02-26T18:50:27.62324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparation\n\nWe remove duplicate samples from the training set by consolidating them into one row and adding a `sample_weight` column. For cross-validation we use a stratified approach using the labels and resolution/GCD.","metadata":{}},{"cell_type":"code","source":"# Helper functions from https://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense/\nfrom math import factorial\nfrom random import choices, setstate\nfrom collections import Counter\nfrom itertools import product\n\n# Function for downcasting float/int datatypes\ndef reduce_memory_usage(path):\n    df = pd.read_csv(path);\n    for col, dtype in df.dtypes.iteritems():\n        if dtype.name.startswith('int'):\n            df[col] = pd.to_numeric(df[col], downcast ='integer')\n        elif dtype.name.startswith('float'):\n            df[col] = pd.to_numeric(df[col], downcast ='float')\n    return df\n\ndef bias(w, x, y, z):\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\ndef bias_of(column):\n    w = int(column[1:column.index('T')])\n    x = int(column[column.index('T')+1:column.index('G')])\n    y = int(column[column.index('G')+1:column.index('C')])\n    z = int(column[column.index('C')+1:])\n    return bias(w, x, y, z)\n\ndef get_histograms(input_df):\n    return pd.DataFrame({\n        col: ((input_df[col] + bias_of(col)) * 1000000).round().astype(int) for col in features\n    })\n\ndef gcd_of_all(df_i):\n    gcd = df_i[features[0]]\n    for col in features[1:]:\n        gcd = np.gcd(gcd, df_i[col])\n    return gcd\n\ndef get_target_bins():\n    temp = train[['target','target']].copy()\n    temp.columns = ['row_id','target']\n    temp['row_id'] = gcd_of_all(get_histograms(train[features]))\n    return temp['row_id'].astype(str) + temp['target'].astype(str)\n\ndef remove_duplicates(input_df):\n    # Create a new dataframe without duplicates, but with an additional sample_weight column\n    vc = input_df[features + ['target']].value_counts()\n    dedup_train = pd.DataFrame([list(tup) for tup in vc.index.values], columns = input_df[features + ['target']].columns)\n    dedup_train['sample_weight'] = vc.values\n    return dedup_train","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-26T18:50:27.627206Z","iopub.execute_input":"2022-02-26T18:50:27.627502Z","iopub.status.idle":"2022-02-26T18:50:27.644272Z","shell.execute_reply.started":"2022-02-26T18:50:27.627471Z","shell.execute_reply":"2022-02-26T18:50:27.643447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Load and Reduce memory\ntry:\n    train = pd.read_feather('train.feather')\n    test = pd.read_feather('test.feather')\nexcept:\n    train = reduce_memory_usage('../input/tabular-playground-series-feb-2022/train.csv')\n    train.to_feather('train.feather')\n    test = reduce_memory_usage('../input/tabular-playground-series-feb-2022/test.csv')\n    test.to_feather('test.feather')\n    \n# Label encoding\nencoder = LabelEncoder()\ntrain['target'] = encoder.fit_transform(train['target'])\n\n# Features and Target Bins\nfeatures = [x for x in train.columns if x not in ['row_id','target']]\ntrain = remove_duplicates(train)\ntrain['gcd'] = gcd_of_all(get_histograms(train[features]))\ntarget_bins = get_target_bins()\n\n# Stratified K-fold\nSKF = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n\nsubmission = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\n\nprint(f'Training Samples: {len(train)}')","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:50:27.646501Z","iopub.execute_input":"2022-02-26T18:50:27.646709Z","iopub.status.idle":"2022-02-26T18:52:02.448392Z","shell.execute_reply.started":"2022-02-26T18:50:27.646683Z","shell.execute_reply":"2022-02-26T18:52:02.447711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.base import clone\nfrom scipy.stats import mode\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, accuracy_score\nimport matplotlib.pyplot as plt\n\n# Scoring/Training Baseline Function\ndef score_model(sklearn_model):\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((len(train),))\n    test_proba = np.zeros((len(test),len(train['target'].unique())))\n    test_preds = list()\n    scores = np.zeros(NUM_FOLDS)\n    print('')\n    \n    # Stratified k-fold cross-validation\n    for fold, (train_idx, valid_idx) in enumerate(SKF.split(train, target_bins)):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx], train['target'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['target'].iloc[valid_idx]\n        X_test = test[features]\n        \n        # Create model\n        model = clone(sklearn_model); start = time.time()\n        model.fit(X_train, y_train, sample_weight = train['sample_weight'].iloc[train_idx])\n        \n        # Predictions\n        valid_preds = np.argmax(model.predict_proba(X_valid), axis = 1)\n        test_prob = model.predict_proba(X_test)\n        test_proba += test_prob / NUM_FOLDS\n        test_preds.append(np.argmax(test_prob, axis = 1))\n        scores[fold] = accuracy_score(y_valid, valid_preds, sample_weight = train['sample_weight'].iloc[valid_idx])\n        oof_preds[valid_idx] = valid_preds\n        \n        print(f'Fold {fold}: {round(scores[fold], 5)} accuracy in {round(time.time()-start,2)}s.')\n    \n    print(\"\\nAverage Accuracy:\", round(scores.mean(), 5))\n    return oof_preds, np.argmax(test_proba, axis = 1), mode(test_preds).mode[0]\n\n# Confusion Matrix Plotting\ndef plot_confusion_matrix(true_values, pred_values, gcds, sample_weight = None, plot_title = \"Confusion Matrix\"):\n    \n    gcd = [[1,10],[1000,10000]]\n    \n    # Confusion matrix\n    fig, ax = plt.subplots(2, 2, figsize = (12,10))\n    for row in range(2):\n        for col in range(2):\n            idx = 2*row + col\n            if sample_weight is not None:\n                cm = confusion_matrix(true_values[gcds == gcd[row][col]], pred_values[gcds == gcd[row][col]], sample_weight = sample_weight[gcds == gcd[row][col]])\n                acc = accuracy_score(true_values[gcds == gcd[row][col]], pred_values[gcds == gcd[row][col]], sample_weight = sample_weight[gcds == gcd[row][col]])\n            else:\n                cm = confusion_matrix(true_values[gcds == gcd[row][col]], pred_values[gcds == gcd[row][col]])\n                acc = accuracy_score(true_values[gcds == gcd[row][col]], pred_values[gcds == gcd[row][col]])\n            np.fill_diagonal(cm, 0)\n            disp = ConfusionMatrixDisplay(confusion_matrix = cm)\n            disp.plot(ax = ax[row,col])\n            ax[row,col].set_title(f'GCD = {gcd[row][col]}, Accuracy: {round(acc, 5)}')\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-26T18:52:02.449735Z","iopub.execute_input":"2022-02-26T18:52:02.450174Z","iopub.status.idle":"2022-02-26T18:52:02.469445Z","shell.execute_reply.started":"2022-02-26T18:52:02.450142Z","shell.execute_reply":"2022-02-26T18:52:02.468749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baselines\n\nFor our baseline, we use an ensemble of `ExtraTreesClassifier` models with bagging trained using all the data.","metadata":{}},{"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import ExtraTreesClassifier, BaggingClassifier\n\n# Default Parameters\nEXT_PARAMS = dict(n_estimators = 300, random_state = RANDOM_SEED, n_jobs = -1)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-26T18:52:02.47075Z","iopub.execute_input":"2022-02-26T18:52:02.470986Z","iopub.status.idle":"2022-02-26T18:52:02.67877Z","shell.execute_reply.started":"2022-02-26T18:52:02.470958Z","shell.execute_reply":"2022-02-26T18:52:02.67817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ExtraTrees Baseline\noof_preds, test_soft, test_hard = score_model(\n    BaggingClassifier(\n        ExtraTreesClassifier(**EXT_PARAMS), \n        random_state = RANDOM_SEED\n    ) \n)\n\n# Submission (Hard Voting)\nsubmission['target'] = encoder.inverse_transform(test_hard)\nsubmission.to_csv('extratrees_hardvoting_submission.csv', index=False)\n\n# Submission (Soft Voting)\nsubmission['target'] = encoder.inverse_transform(test_soft)\nsubmission.to_csv('extratrees_softvoting_submission.csv', index=False)\n\n# Confusion Matrix\nplot_confusion_matrix(train['target'], oof_preds, train['gcd'], train['sample_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T18:52:02.679723Z","iopub.execute_input":"2022-02-26T18:52:02.68025Z","iopub.status.idle":"2022-02-26T19:06:26.872587Z","shell.execute_reply.started":"2022-02-26T18:52:02.680202Z","shell.execute_reply":"2022-02-26T19:06:26.870291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment 1: Four Separate Models\n\nIn this instance, we train a separate `ExtraTreesClassifier` on each subset of the data. That is, on the samples corresponding to 1M, 100k, 1k, and 100 reads, respectively. Unhide the following code block to see the details:","metadata":{}},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, clone\n\nclass FourResolutions(BaseEstimator):\n    \n    def __init__(self, base_estimator = BaggingClassifier(ExtraTreesClassifier(**EXT_PARAMS), random_state = RANDOM_SEED)):\n        self.base_estimator = base_estimator\n        \n    def clone_models(self):\n        self.model1 = clone(self.base_estimator) # Model for 1,000,000 BOC Reads\n        self.model2 = clone(self.base_estimator) # Model for 100,000 BOC Reads\n        self.model3 = clone(self.base_estimator) # Model for 1,000 BOC Reads\n        self.model4 = clone(self.base_estimator) # Model for 100 BOC Reads\n            \n    def gcd_of_all(self, df_i):\n        features = [x for x in df_i.columns]\n        gcd = df_i[features[0]]\n        for col in features[1:]:\n            gcd = np.gcd(gcd, df_i[col])\n        self.gcd1 = (gcd == 1)\n        self.gcd2 = (gcd == 10)\n        self.gcd3 = (gcd == 1000)\n        self.gcd4 = (gcd == 10000)\n        \n    def get_histograms(self, input_df):\n        return pd.DataFrame({\n            col: ((input_df[col] + bias_of(col)) * 1000000).round().astype(int) for col in features\n        })\n        \n    def fit(self, X, y, sample_weight = None):\n        self.clone_models()\n        temp = self.get_histograms(X)\n        self.gcd_of_all(temp)\n        self.num_labels = len(np.unique(y))\n        if sample_weight is not None:\n            self.model1.fit(X[self.gcd1], y[self.gcd1], sample_weight[self.gcd1])\n            self.model2.fit(X[self.gcd2], y[self.gcd2], sample_weight[self.gcd2])\n            self.model3.fit(X[self.gcd3], y[self.gcd3], sample_weight[self.gcd3])\n            self.model4.fit(X[self.gcd4], y[self.gcd4], sample_weight[self.gcd4])\n        else:\n            self.model1.fit(X[self.gcd1], y[self.gcd1])\n            self.model2.fit(X[self.gcd2], y[self.gcd2])\n            self.model3.fit(X[self.gcd3], y[self.gcd3])\n            self.model4.fit(X[self.gcd4], y[self.gcd4])\n            \n    def predict_proba(self, X):\n        temp = self.get_histograms(X)\n        self.gcd_of_all(temp)\n        temp = np.zeros((len(X),self.num_labels))\n        temp[self.gcd1] = self.model1.predict_proba(X[self.gcd1])\n        temp[self.gcd2] = self.model2.predict_proba(X[self.gcd2])\n        temp[self.gcd3] = self.model3.predict_proba(X[self.gcd3])\n        temp[self.gcd4] = self.model4.predict_proba(X[self.gcd4])\n        return temp\n        \n    def predict(self, X):\n        temp = self.get_histograms(X)\n        self.gcd_of_all(temp)\n        temp = np.zeros((len(X),))\n        temp[self.gcd1] = self.model1.predict(X[self.gcd1])\n        temp[self.gcd2] = self.model2.predict(X[self.gcd2])\n        temp[self.gcd3] = self.model3.predict(X[self.gcd3])\n        temp[self.gcd4] = self.model4.predict(X[self.gcd4])\n        return temp.astype(int)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-26T19:06:26.875195Z","iopub.execute_input":"2022-02-26T19:06:26.875753Z","iopub.status.idle":"2022-02-26T19:06:26.903332Z","shell.execute_reply.started":"2022-02-26T19:06:26.875697Z","shell.execute_reply":"2022-02-26T19:06:26.90232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_preds, test_soft, test_hard = score_model(FourResolutions())\n\n# Submission (Hard Voting)\nsubmission['target'] = encoder.inverse_transform(test_hard)\nsubmission.to_csv('fourmodels_hardvoting_submission.csv', index=False)\n\n# Submission (Soft Voting)\nsubmission['target'] = encoder.inverse_transform(test_soft)\nsubmission.to_csv('fourmodels_softvoting_submission.csv', index=False)\n\n# Confusion Matrix\nplot_confusion_matrix(train['target'], oof_preds, train['gcd'], train['sample_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:06:26.906455Z","iopub.execute_input":"2022-02-26T19:06:26.906694Z","iopub.status.idle":"2022-02-26T19:17:34.741832Z","shell.execute_reply.started":"2022-02-26T19:06:26.906667Z","shell.execute_reply":"2022-02-26T19:17:34.739427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment 2: Split High/Low Resolution\n\nIn this instance we train two separate ExtraTrees models, one using the high resolution data (1 million and 100k reads) and one using the low resolution data (1000 and 100 reads).","metadata":{}},{"cell_type":"code","source":"class HighLowSplit(BaseEstimator):\n    \n    def __init__(self, base_estimator = BaggingClassifier(ExtraTreesClassifier(**EXT_PARAMS), random_state = RANDOM_SEED)):\n        self.base_estimator = base_estimator\n        \n    def clone_models(self):\n        self.model1 = clone(self.base_estimator) # Model for 10^6 and 10^5 BOC Reads\n        self.model2 = clone(self.base_estimator) # Model for 10^2 and 10^3 BOC Reads\n            \n    def gcd_of_all(self, df_i):\n        features = [x for x in df_i.columns]\n        gcd = df_i[features[0]]\n        for col in features[1:]:\n            gcd = np.gcd(gcd, df_i[col])\n        self.gcd1 = (gcd == 1) | (gcd == 10)\n        self.gcd2 = (gcd == 1000) | (gcd == 10000)\n        \n    def get_histograms(self, input_df):\n        return pd.DataFrame({\n            col: ((input_df[col] + bias_of(col)) * 1000000).round().astype(int) for col in features\n        })\n        \n    def fit(self, X, y, sample_weight = None):\n        self.clone_models()\n        temp = self.get_histograms(X)\n        self.gcd_of_all(temp)\n        self.num_labels = len(np.unique(y))\n        if sample_weight is not None:\n            self.model1.fit(X[self.gcd1], y[self.gcd1], sample_weight[self.gcd1])\n            self.model2.fit(X[self.gcd2], y[self.gcd2], sample_weight[self.gcd2])\n        else:\n            self.model1.fit(X[self.gcd1], y[self.gcd1])\n            self.model2.fit(X[self.gcd2], y[self.gcd2])\n            \n    def predict_proba(self, X):\n        temp = self.get_histograms(X)\n        self.gcd_of_all(temp)\n        temp = np.zeros((len(X),self.num_labels))\n        temp[self.gcd1] = self.model1.predict_proba(X[self.gcd1])\n        temp[self.gcd2] = self.model2.predict_proba(X[self.gcd2])\n        return temp\n        \n    def predict(self, X):\n        temp = self.get_histograms(X)\n        self.gcd_of_all(temp)\n        temp = np.zeros((len(X),))\n        temp[self.gcd1] = self.model1.predict(X[self.gcd1])\n        temp[self.gcd2] = self.model2.predict(X[self.gcd2])\n        return temp.astype(int)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-26T19:17:35.007264Z","iopub.execute_input":"2022-02-26T19:17:35.007552Z","iopub.status.idle":"2022-02-26T19:17:35.026779Z","shell.execute_reply.started":"2022-02-26T19:17:35.007517Z","shell.execute_reply":"2022-02-26T19:17:35.025763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_preds, test_soft, test_hard = score_model(HighLowSplit())\n\n# Submission (Hard Voting)\nsubmission['target'] = encoder.inverse_transform(test_hard)\nsubmission.to_csv('split_highlow_hardvoting_submission.csv', index=False)\n\n# Submission (Soft Voting)\nsubmission['target'] = encoder.inverse_transform(test_soft)\nsubmission.to_csv('split_highlow_softvoting_submission.csv', index=False)\n\n# Confusion Matrix\nplot_confusion_matrix(train['target'], oof_preds, train['gcd'], train['sample_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:17:35.027968Z","iopub.execute_input":"2022-02-26T19:17:35.029519Z","iopub.status.idle":"2022-02-26T19:30:19.634638Z","shell.execute_reply.started":"2022-02-26T19:17:35.029464Z","shell.execute_reply":"2022-02-26T19:30:19.63379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment 3: Isolate Lowest Resolution\n\nIn this instance, we again train only two moodels, one using all but the lowest resolution data and the other only on the lowest resolution data (100 reads).","metadata":{}},{"cell_type":"code","source":"class IsolateLowRes(BaseEstimator):\n    \n    def __init__(self, base_estimator = BaggingClassifier(ExtraTreesClassifier(**EXT_PARAMS), random_state = RANDOM_SEED)):\n        self.base_estimator = base_estimator\n        \n    def clone_models(self):\n        self.model1 = clone(self.base_estimator) # Model for 10^6, 10^5 10^3 BOC Reads\n        self.model2 = clone(self.base_estimator) # Model for 10^2 BOC Reads\n            \n    def gcd_of_all(self, df_i):\n        features = [x for x in df_i.columns]\n        gcd = df_i[features[0]]\n        for col in features[1:]:\n            gcd = np.gcd(gcd, df_i[col])\n        self.gcd1 = (gcd == 1) | (gcd == 10) | (gcd == 1000) \n        self.gcd2 = (gcd == 10000)\n        \n    def get_histograms(self, input_df):\n        return pd.DataFrame({\n            col: ((input_df[col] + bias_of(col)) * 1000000).round().astype(int) for col in features\n        })\n        \n    def fit(self, X, y, sample_weight = None):\n        self.clone_models()\n        temp = self.get_histograms(X)\n        self.gcd_of_all(temp)\n        self.num_labels = len(np.unique(y))\n        if sample_weight is not None:\n            self.model1.fit(X[self.gcd1], y[self.gcd1], sample_weight[self.gcd1])\n            self.model2.fit(X[self.gcd2], y[self.gcd2], sample_weight[self.gcd2])\n        else:\n            self.model1.fit(X[self.gcd1], y[self.gcd1])\n            self.model2.fit(X[self.gcd2], y[self.gcd2])\n            \n    def predict_proba(self, X):\n        temp = self.get_histograms(X)\n        self.gcd_of_all(temp)\n        temp = np.zeros((len(X),self.num_labels))\n        temp[self.gcd1] = self.model1.predict_proba(X[self.gcd1])\n        temp[self.gcd2] = self.model2.predict_proba(X[self.gcd2])\n        return temp\n        \n    def predict(self, X):\n        temp = self.get_histograms(X)\n        self.gcd_of_all(temp)\n        temp = np.zeros((len(X),))\n        temp[self.gcd1] = self.model1.predict(X[self.gcd1])\n        temp[self.gcd2] = self.model2.predict(X[self.gcd2])\n        return temp.astype(int)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-26T19:30:19.636142Z","iopub.execute_input":"2022-02-26T19:30:19.636414Z","iopub.status.idle":"2022-02-26T19:30:19.656153Z","shell.execute_reply.started":"2022-02-26T19:30:19.636383Z","shell.execute_reply":"2022-02-26T19:30:19.655245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_preds, test_soft, test_hard = score_model(IsolateLowRes())\n\n# Submission (Hard Voting)\nsubmission['target'] = encoder.inverse_transform(test_hard)\nsubmission.to_csv('isolate_lowest_hardvoting_submission.csv', index=False)\n\n# Submission (Soft Voting)\nsubmission['target'] = encoder.inverse_transform(test_soft)\nsubmission.to_csv('isolate_lowest_softvoting_submission.csv', index=False)\n\n# Confusion Matrix\nplot_confusion_matrix(train['target'], oof_preds, train['gcd'], train['sample_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T19:30:19.658706Z","iopub.execute_input":"2022-02-26T19:30:19.659176Z","iopub.status.idle":"2022-02-26T19:43:52.354148Z","shell.execute_reply.started":"2022-02-26T19:30:19.659129Z","shell.execute_reply":"2022-02-26T19:43:52.353153Z"},"trusted":true},"execution_count":null,"outputs":[]}]}