{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Augmentation\n\nIn this notebook, we use knowledge about how our data was generated to augment or create new training samples. This data is then combined with the original data to see if we can improve our CV or test results.\n\n## Original Data Generation\n\nFrom the description in the original paper, each training example consists of length 10 samples of the full bacterial DNA gotten by the following process:\n\n$$\\cdots \\text{ATCG}\\textbf{CTGGATAGCA}\\text{TACG} \\cdots \\xrightarrow{sample} \\text{CTGGATAGCA} \\xrightarrow{bin} A_3T_2G_3C_2$$\n\nEach training example consists of either 1 million, 100k, 1k or 100 length 10 reads, which are then turned into a probability distribution and altered by subtracting off the bias distribution (expected distribution of randomly generated DNA sequences). We refer to the subset of data generated from 1 million or 100k reads as the **high resolution data** and the subset generated by 1000 or 100 reads as the **low resolution data**. With some work, we can retrieve (a constant multiple of) the original samples from the provided training data and we distinguish each subset by the GCD of the samples.\n\n## Augmentation/Resampling\n\nWe can predict almost perfectly on the high resolution data and with varying results on the low resolution data. So we will focus on the low resolution data, we will use two main two approaches.\n\nThe first approach we will augment training examples. Recall that each low resolution training example consists 100 or 1000 DNA samples. We augment a data point by replace a fraction of these samples with new samples from the same (or approximately the same) distribution, or by replacing them with randomly generated DNA samples (this can be thought of as sampling error). The second approach we will create brand new low resolution training examples using the high resolution data to approximate the original DNA. In particular, we will try the following:\n\n1. Simulate errors by replacing samples with completely random DNA reads\n2. Augment low resolution data with samples taken from high resolution *training* data\n3. Augment low resolution data with samples taken from high resolution *test* data\n4. Create new low resolution data by sampling from high resolution *training* data\n5. Create new low resolution data by sampling from high resolution *test* data\n\nI will update successive versions of this notebook with the resulting public leaderboard scores for each experiment.\n\n**Credits:** The idea for this notebook and a few of these helper functions I used are adapted from [this great notebook](https://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense/).","metadata":{}},{"cell_type":"code","source":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 99\nNUM_FOLDS = 10\nALTER_RATE = 0.05\nEXT_PARAMS = dict(n_estimators = 300, random_state = RANDOM_SEED, n_jobs = -1)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T20:49:11.678996Z","iopub.execute_input":"2022-02-25T20:49:11.679355Z","iopub.status.idle":"2022-02-25T20:49:11.703472Z","shell.execute_reply.started":"2022-02-25T20:49:11.679264Z","shell.execute_reply":"2022-02-25T20:49:11.702733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generic\nimport numpy as np\nimport pandas as pd\nimport pyarrow\nimport time\nimport gc\n\n# Sklearn\nfrom sklearn.base import clone, BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_predict, StratifiedKFold\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, accuracy_score\n\n# Other stuff\nfrom math import factorial\nfrom random import choices, setstate\nfrom collections import Counter\nfrom itertools import product\nfrom scipy.stats import mode\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nimport seaborn as sns\nfrom tqdm import tqdm\n\n# Hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-25T20:49:11.704903Z","iopub.execute_input":"2022-02-25T20:49:11.705113Z","iopub.status.idle":"2022-02-25T20:49:13.061598Z","shell.execute_reply.started":"2022-02-25T20:49:11.705088Z","shell.execute_reply":"2022-02-25T20:49:13.060884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preliminaries","metadata":{}},{"cell_type":"code","source":"# Helper functions from https://www.kaggle.com/ambrosm/tpsfeb22-01-eda-which-makes-sense/\n\ndef bias_of(column):\n    w = int(column[1:column.index('T')])\n    x = int(column[column.index('T')+1:column.index('G')])\n    y = int(column[column.index('G')+1:column.index('C')])\n    z = int(column[column.index('C')+1:])\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\n\n# Convert to integer histograms\ndef get_histograms(input_df):\n    return pd.DataFrame({col: ((input_df[col] + BIAS[col]) * 1000000).round().astype(int) for col in features})\n\n# Finds GCD of each row, assumes integer histograms\ndef gcd_of_all(df_i):\n    gcd = df_i[features[0]]\n    for col in features[1:]:\n        gcd = np.gcd(gcd, df_i[col])\n    return gcd\n\n# Create a new dataframe without duplicates, but with an additional sample_weight column\ndef remove_duplicates(input_df):\n    vc = input_df.value_counts()\n    dedup_train = pd.DataFrame([list(tup) for tup in vc.index.values], columns = input_df.columns)\n    dedup_train['sample_weight'] = vc.values\n    return dedup_train\n\n# Function for downcasting float/int datatypes\ndef reduce_memory_usage(path):\n    df = pd.read_csv(path);\n    for col, dtype in df.dtypes.iteritems():\n        if dtype.name.startswith('int'):\n            df[col] = pd.to_numeric(df[col], downcast ='integer')\n        elif dtype.name.startswith('float'):\n            df[col] = pd.to_numeric(df[col], downcast ='float')\n    return df\n\n# Convert deviation spectrum to raw histograms\ndef convert_to_histograms(input_df):\n    temp = pd.DataFrame({\n        col: ((input_df[col] + BIAS[col]) * 1000000).round().astype(int) for col in features\n    })\n    temp['gcd'] = input_df['gcd']\n    for col in features:\n        temp[col] //= temp['gcd']\n    return temp\n\n# Create Counters from integer histograms\ndef make_counters(input_df):\n    samples, gcd, labels = list(), list(), list()\n    for (index, row), dupes, val, label in zip(input_df[features].iterrows(), input_df['sample_weight'], input_df['gcd'], input_df['target']):\n        sample = row.to_dict()\n        for _ in range(dupes):\n            gcd.append(val)\n            labels.append(label)\n            samples.append(Counter(sample))\n    return samples, gcd, labels\n\n# Convert raw histograms to deviation spectrum\ndef normalize_histograms(input_df):\n    temp = input_df.copy()\n    for col in features:\n        temp[col] *= temp['gcd']\n        temp[col] /= 1000000\n        temp[col] -= BIAS[col]\n    return input_df","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-25T20:49:13.062925Z","iopub.execute_input":"2022-02-25T20:49:13.063134Z","iopub.status.idle":"2022-02-25T20:49:13.083051Z","shell.execute_reply.started":"2022-02-25T20:49:13.063109Z","shell.execute_reply":"2022-02-25T20:49:13.082029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data\n\nWe save the preprocessed data locally in feather format to speedup successive runs of this notebook.","metadata":{}},{"cell_type":"code","source":"%%time\n\ntry:\n    train = pd.read_feather('train.feather')\n    test = pd.read_feather('test.feather')\n    features = [x for x in train.columns if x not in ['row_id','target','gcd','sample_weight']]\n    BIAS = {x: bias_of(x) for x in features}\n    submission = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\nexcept:\n    train = reduce_memory_usage('../input/tabular-playground-series-feb-2022/train.csv')\n    train.drop(columns='row_id', inplace = True)\n    features = [x for x in train.columns if x not in ['row_id','target','gcd']]\n    BIAS = {x: bias_of(x) for x in features}\n    train = remove_duplicates(train)\n    train['gcd'] = gcd_of_all(get_histograms(train[features]))\n    train.to_feather('train.feather')\n    test = reduce_memory_usage('../input/tabular-playground-series-feb-2022/test.csv')\n    test.drop(columns='row_id', inplace = True)\n    test['gcd'] = gcd_of_all(get_histograms(test[features]))\n    test.to_feather('test.feather')\n    submission = pd.read_csv('../input/tabular-playground-series-feb-2022/sample_submission.csv')\n    \n# Labels encoding\nencoder = LabelEncoder()\ntrain['target'] = encoder.fit_transform(train['target'])\n\n# Stratified K-fold\ntarget_bins = train['target'].astype(str) + train['gcd'].astype(str)\nSKF = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n\nprint(f'Training Samples: {len(train)}')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-25T20:49:13.085183Z","iopub.execute_input":"2022-02-25T20:49:13.085556Z","iopub.status.idle":"2022-02-25T20:50:55.887763Z","shell.execute_reply.started":"2022-02-25T20:49:13.085525Z","shell.execute_reply":"2022-02-25T20:50:55.886787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scoring/Evaluation\n\nWe ensemble our predictions with both hard and soft voting as well as return the out-of-fold predictions on the training data so we can do error analysis.","metadata":{}},{"cell_type":"code","source":"# Scoring/Training Baseline Function\ndef score_model(sklearn_model):\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((len(train),))\n    test_proba = np.zeros((len(test),len(train['target'].unique())))\n    test_preds = list()\n    scores = np.zeros(NUM_FOLDS)\n    print('')\n    \n    # Stratified k-fold cross-validation\n    for fold, (train_idx, valid_idx) in enumerate(SKF.split(train, target_bins)):\n        \n        # Train/Test/Validation Sets\n        X_train, y_train = train[features + ['gcd']].iloc[train_idx], train['target'].iloc[train_idx]\n        X_valid, y_valid = train[features + ['gcd']].iloc[valid_idx], train['target'].iloc[valid_idx]\n        train_weights, valid_weights = train['sample_weight'].iloc[train_idx], train['sample_weight'].iloc[valid_idx]\n        X_test = test[features + ['gcd']]; start = time.time()\n        \n        # Train Model\n        model = clone(sklearn_model)\n        model.fit(X_train, y_train, sample_weight = train_weights)\n        gc.collect()\n        \n        # Get Predictions\n        valid_preds = np.argmax(model.predict_proba(X_valid), axis = 1)\n        test_prob = model.predict_proba(X_test)\n        \n        # Save Predictions\n        test_proba += test_prob / NUM_FOLDS\n        test_preds.append(np.argmax(test_prob, axis = 1))\n        scores[fold] = accuracy_score(y_valid, valid_preds, sample_weight = valid_weights)\n        oof_preds[valid_idx] = valid_preds\n        print(f'Fold {fold}: {round(scores[fold], 5)} accuracy in {round(time.time()-start,2)}s.')\n    \n    print(\"\\nAverage Accuracy:\", round(scores.mean(), 5))\n    return oof_preds, np.argmax(test_proba, axis = 1), mode(test_preds).mode[0]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-25T20:50:55.889384Z","iopub.execute_input":"2022-02-25T20:50:55.89016Z","iopub.status.idle":"2022-02-25T20:50:55.904301Z","shell.execute_reply.started":"2022-02-25T20:50:55.89011Z","shell.execute_reply":"2022-02-25T20:50:55.903393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix Plotting\ndef plot_confusion_matrix(true_values, pred_values, gcds, sample_weight = None, plot_title = \"Confusion Matrix\"):\n    \n    gcd = [[1,10],[1000,10000]]\n    \n    # Create plot for each data resolution\n    fig, ax = plt.subplots(2, 2, figsize = (12,10))\n    for row in range(2):\n        for col in range(2):\n            idx = 2*row + col\n            if sample_weight is not None:\n                cm = confusion_matrix(true_values[gcds == gcd[row][col]], pred_values[gcds == gcd[row][col]], sample_weight = sample_weight[gcds == gcd[row][col]])\n                acc = accuracy_score(true_values[gcds == gcd[row][col]], pred_values[gcds == gcd[row][col]], sample_weight = sample_weight[gcds == gcd[row][col]])\n            else:\n                cm = confusion_matrix(true_values[gcds == gcd[row][col]], pred_values[gcds == gcd[row][col]])\n                acc = accuracy_score(true_values[gcds == gcd[row][col]], pred_values[gcds == gcd[row][col]])\n            np.fill_diagonal(cm, 0)\n            disp = ConfusionMatrixDisplay(confusion_matrix = cm)\n            disp.plot(ax = ax[row,col])\n            ax[row,col].set_title(f'GCD = {gcd[row][col]}, Accuracy: {round(acc, 5)}')\n    plt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-25T20:50:55.905949Z","iopub.execute_input":"2022-02-25T20:50:55.906177Z","iopub.status.idle":"2022-02-25T20:50:55.922802Z","shell.execute_reply.started":"2022-02-25T20:50:55.906148Z","shell.execute_reply":"2022-02-25T20:50:55.922103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model\n\nThe model we use is based on [my previous notebook](https://www.kaggle.com/rsizem2/tps-02-22-separating-high-low-resolution-data), and trains a separate `ExtraTreesClassifier` on each subset of the data (based on resolution).","metadata":{}},{"cell_type":"code","source":"class FourResolutions(BaseEstimator):\n    \n    def __init__(self):\n        self.model1 = ExtraTreesClassifier(**EXT_PARAMS) # Model for 1,000,000 BOC Reads\n        self.model2 = ExtraTreesClassifier(**EXT_PARAMS) # Model for 100,000 BOC Reads\n        self.model3 = ExtraTreesClassifier(**EXT_PARAMS) # Model for 1,000 BOC Reads\n        self.model4 = ExtraTreesClassifier(**EXT_PARAMS) # Model for 100 BOC Reads\n        \n    def get_gcds(self, X):\n        self.gcd1 = (X.gcd == 1)\n        self.gcd2 = (X.gcd == 10)\n        self.gcd3 = (X.gcd == 1000)\n        self.gcd4 = (X.gcd == 10000)\n    \n    def fit(self, X, y, sample_weight = None):\n        self.get_gcds(X)\n        self.num_labels = len(np.unique(y))\n        if sample_weight is not None:\n            self.model1.fit(X[self.gcd1], y[self.gcd1], sample_weight[self.gcd1])\n            self.model2.fit(X[self.gcd2], y[self.gcd2], sample_weight[self.gcd2])\n            self.model3.fit(X[self.gcd3], y[self.gcd3], sample_weight[self.gcd3])\n            self.model4.fit(X[self.gcd4], y[self.gcd4], sample_weight[self.gcd4])\n        else:\n            self.model1.fit(X[self.gcd1], y[self.gcd1])\n            self.model2.fit(X[self.gcd2], y[self.gcd2])\n            self.model3.fit(X[self.gcd3], y[self.gcd3])\n            self.model4.fit(X[self.gcd4], y[self.gcd4])\n            \n    def predict_proba(self, X):\n        self.get_gcds(X)\n        temp = np.zeros((len(X),self.num_labels))\n        temp[self.gcd1] = self.model1.predict_proba(X[self.gcd1])\n        temp[self.gcd2] = self.model2.predict_proba(X[self.gcd2])\n        temp[self.gcd3] = self.model3.predict_proba(X[self.gcd3])\n        temp[self.gcd4] = self.model4.predict_proba(X[self.gcd4])\n        return temp\n        \n    def predict(self, X):\n        self.get_gcds(X)\n        temp = np.zeros((len(X),))\n        temp[self.gcd1] = self.model1.predict(X[self.gcd1])\n        temp[self.gcd2] = self.model2.predict(X[self.gcd2])\n        temp[self.gcd3] = self.model3.predict(X[self.gcd3])\n        temp[self.gcd4] = self.model4.predict(X[self.gcd4])\n        return temp.astype(int)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-25T20:50:55.923957Z","iopub.execute_input":"2022-02-25T20:50:55.924622Z","iopub.status.idle":"2022-02-25T20:50:55.943749Z","shell.execute_reply.started":"2022-02-25T20:50:55.924587Z","shell.execute_reply":"2022-02-25T20:50:55.94279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline\n\nTraining on the original dataset with no modifications","metadata":{}},{"cell_type":"code","source":"oof_preds, test_soft, test_hard = score_model(\n    FourResolutions()\n)\n\n# Save Predictions \ntest['target'] = test_hard\n\n# Confusion Matrix\nplot_confusion_matrix(train['target'], oof_preds, train['gcd'], train['sample_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T20:50:55.945488Z","iopub.execute_input":"2022-02-25T20:50:55.945838Z","iopub.status.idle":"2022-02-25T20:52:17.412689Z","shell.execute_reply.started":"2022-02-25T20:50:55.945796Z","shell.execute_reply":"2022-02-25T20:52:17.411747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sample Distributions\n\nThe following functions create the probability distributions which we'll use to create new DNA reads with which we'll augment our data or create all new training samples.","metadata":{}},{"cell_type":"code","source":"def get_distributions(input_df):\n    \n    # Only use highest resolution samples\n    temp = convert_to_histograms(input_df)\n    temp['target'] = input_df['target'].values\n    temp = temp[temp.gcd == 1].reset_index(drop=True)\n    \n    # Get sample distributions for each bacteria\n    dist = dict()\n    for label in np.unique(temp['target']):\n        temp_vec = np.sum(temp[temp.target == label][features], axis = 0) \n        dist[label] = temp_vec / np.sum(temp_vec)\n    return dist","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-25T20:52:17.413884Z","iopub.execute_input":"2022-02-25T20:52:17.414156Z","iopub.status.idle":"2022-02-25T20:52:17.422757Z","shell.execute_reply.started":"2022-02-25T20:52:17.414116Z","shell.execute_reply":"2022-02-25T20:52:17.421704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RANDOM_DIST = [bias_of(x) for x in features]\nTRAIN_DIST = get_distributions(train)\nTEST_DIST = get_distributions(test)","metadata":{"execution":{"iopub.status.busy":"2022-02-25T20:52:17.426311Z","iopub.execute_input":"2022-02-25T20:52:17.426936Z","iopub.status.idle":"2022-02-25T20:52:20.234039Z","shell.execute_reply.started":"2022-02-25T20:52:17.426893Z","shell.execute_reply":"2022-02-25T20:52:20.233153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment 1: Error Simulation\n\nIn our first experiment, for each training example we replace a fraction of the original DNA reads with completely random DNA reads. ","metadata":{}},{"cell_type":"code","source":"# Error Simulation\nclass ErrorSimulator(BaseEstimator):\n    \n    def __init__(self, error_rate = 0.1):\n        \n        self.model = FourResolutions()\n        self.rng = np.random.default_rng()\n        self.error_rate = error_rate\n        \n    def fit(self, X, y, sample_weight = None):\n        \n        # Get original histograms and convert to Counters\n        new_samples = convert_to_histograms(X)\n        new_samples['target'] = y.values\n        new_samples['sample_weight'] = sample_weight.values\n        new_samples = new_samples[new_samples.gcd > 100].reset_index(drop = True)\n        new_samples, gcd, labels = make_counters(new_samples)\n        gc.collect()\n        \n        # Simulate errors \n        for val, sample in zip(gcd, new_samples):\n            num_errors = self.rng.binomial(n = 1000000 // val, p = self.error_rate)\n            old_reads = self.rng.choice(list(sample.elements()), size = num_errors, replace = False)\n            new_reads = self.rng.choice(features, size = len(old_reads), p = RANDOM_DIST, replace = True)\n            sample.subtract(old_reads)\n            sample.update(new_reads)\n        new_samples = pd.DataFrame.from_records(new_samples, columns = features).fillna(0).astype(int)\n        new_samples['target'] = labels\n        new_samples['gcd'] = gcd\n        new_samples = new_samples.sample(frac=1).reset_index(drop=True); gc.collect()\n        new_samples = remove_duplicates(normalize_histograms(new_samples))\n        \n        # Duplicate original data and combine with altered samples \n        original = X.reset_index(drop = True)\n        original['target'] = y.values\n        original['sample_weight'] = sample_weight.values\n        new_samples = pd.concat([original, new_samples], ignore_index = True).sample(frac=1).reset_index(drop=True)\n        \n        # Fit model\n        self.model.fit(new_samples[features + ['gcd']], new_samples['target'], sample_weight = new_samples['sample_weight'])\n        return self\n    \n    def predict_proba(self, X):\n        return self.model.predict_proba(X)\n    \n    def predict(self, X):\n        return self.model.predict(X)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-25T20:52:20.235163Z","iopub.execute_input":"2022-02-25T20:52:20.23539Z","iopub.status.idle":"2022-02-25T20:52:20.250558Z","shell.execute_reply.started":"2022-02-25T20:52:20.235362Z","shell.execute_reply":"2022-02-25T20:52:20.249834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oof_preds, test_soft, test_hard = score_model(\n    ErrorSimulator(\n        error_rate = ALTER_RATE\n    )\n)\n\n# Submission (Hard Voting)\nsubmission['target'] = encoder.inverse_transform(test_hard)\nsubmission.to_csv('random_error_hardvoting_submission.csv', index=False)\n\n# Submission (Soft Voting)\nsubmission['target'] = encoder.inverse_transform(test_soft)\nsubmission.to_csv('random_error_softvoting_submission.csv', index=False)\n\n# Confusion Matrix\nplot_confusion_matrix(train['target'], oof_preds, train['gcd'], train['sample_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T20:52:20.251621Z","iopub.execute_input":"2022-02-25T20:52:20.252276Z","iopub.status.idle":"2022-02-25T21:00:27.724418Z","shell.execute_reply.started":"2022-02-25T20:52:20.252242Z","shell.execute_reply":"2022-02-25T21:00:27.723142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment 2: Data Augmentation\n\nIn our second experiment, for each training example we replace a fraction of the original DNA reads with new DNA reads generated using the high resolution data as an approximation of the original target bacteria DNA. ","metadata":{}},{"cell_type":"code","source":"# Augment data with reads from \nclass DataAugmentation(BaseEstimator):\n    \n    def __init__(self, augment_rate = 0.1, test_samples = False):\n        \n        self.model = FourResolutions()\n        self.rng = np.random.default_rng()\n        self.augment_rate = augment_rate\n        self.test_samples = test_samples\n        \n    def fit(self, X, y, sample_weight = None):\n        \n        # Get original histograms\n        new_samples = convert_to_histograms(X)\n        new_samples['target'] = y.values\n        new_samples['sample_weight'] = sample_weight.values\n        new_samples = new_samples[new_samples.gcd > 100].reset_index(drop = True)\n        new_samples, gcd, labels = make_counters(new_samples)\n        gc.collect()\n        \n        # Replace with alternate samples\n        for val, label, sample in zip(gcd, labels, new_samples):\n            num_errors = self.rng.binomial(n = 1000000 // val, p = self.augment_rate)\n            old_reads = self.rng.choice(list(sample.elements()), size = num_errors, replace = False)\n            if self.test_samples:\n                new_reads = self.rng.choice(features, size = len(old_reads), p = TEST_DIST[label], replace = True)\n            else:\n                new_reads = self.rng.choice(features, size = len(old_reads), p = TRAIN_DIST[label], replace = True)\n            sample.subtract(old_reads)\n            sample.update(new_reads)\n        new_samples = pd.DataFrame.from_records(new_samples, columns = features).fillna(0).astype(int)\n        new_samples['target'] = labels\n        new_samples['gcd'] = gcd\n        new_samples = new_samples.sample(frac=1).reset_index(drop=True); gc.collect()\n        new_samples = remove_duplicates(normalize_histograms(new_samples))\n        \n        # Duplicate original data and combine with altered samples \n        original = X.reset_index(drop = True)\n        original['target'] = y.values\n        original['sample_weight'] = sample_weight.values\n        new_samples = pd.concat([original, new_samples], ignore_index = True).sample(frac=1).reset_index(drop=True)\n        \n        # Fit model\n        self.model.fit(new_samples[features + ['gcd']], new_samples['target'], sample_weight = new_samples['sample_weight'])\n        return self\n    \n    def predict_proba(self, X):\n        return self.model.predict_proba(X)\n    \n    def predict(self, X):\n        return self.model.predict(X)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-25T21:00:27.726191Z","iopub.execute_input":"2022-02-25T21:00:27.726579Z","iopub.status.idle":"2022-02-25T21:00:27.743842Z","shell.execute_reply.started":"2022-02-25T21:00:27.726545Z","shell.execute_reply":"2022-02-25T21:00:27.742886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample Using Train Data","metadata":{}},{"cell_type":"code","source":"oof_preds, test_soft, test_hard = score_model(\n    DataAugmentation(\n        augment_rate = ALTER_RATE,\n        test_samples = False\n    )\n)\n\n# Submission (Hard Voting)\nsubmission['target'] = encoder.inverse_transform(test_hard)\nsubmission.to_csv('augment_train_hardvoting_submission.csv', index=False)\n\n# Submission (Soft Voting)\nsubmission['target'] = encoder.inverse_transform(test_soft)\nsubmission.to_csv('augment_train_softvoting_submission.csv', index=False)\n\n# Confusion Matrix\nplot_confusion_matrix(train['target'], oof_preds, train['gcd'], train['sample_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:00:27.745719Z","iopub.execute_input":"2022-02-25T21:00:27.74641Z","iopub.status.idle":"2022-02-25T21:08:28.607508Z","shell.execute_reply.started":"2022-02-25T21:00:27.746367Z","shell.execute_reply":"2022-02-25T21:08:28.605716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample Using Test Data","metadata":{}},{"cell_type":"code","source":"oof_preds, test_soft, test_hard = score_model(\n    DataAugmentation(\n        augment_rate = ALTER_RATE,\n        test_samples = True\n    )\n)\n\n# Submission (Hard Voting)\nsubmission['target'] = encoder.inverse_transform(test_hard)\nsubmission.to_csv('augment_test_hardvoting_submission.csv', index=False)\n\n# Submission (Soft Voting)\nsubmission['target'] = encoder.inverse_transform(test_soft)\nsubmission.to_csv('augment_test_softvoting_submission.csv', index=False)\n\n# Confusion Matrix\nplot_confusion_matrix(train['target'], oof_preds, train['gcd'], train['sample_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:08:28.608959Z","iopub.execute_input":"2022-02-25T21:08:28.609875Z","iopub.status.idle":"2022-02-25T21:16:26.798437Z","shell.execute_reply.started":"2022-02-25T21:08:28.609827Z","shell.execute_reply":"2022-02-25T21:16:26.797571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiment 3: Resampling\n\nOur final experiment, we will make entirely new training data (instead of altering existing data) using the sample distributions generated from the high resolution data.","metadata":{}},{"cell_type":"code","source":"# Augment data with reads from \nclass Resampler(BaseEstimator):\n    \n    def __init__(self, test_samples = False):\n        \n        self.model = FourResolutions()\n        self.rng = np.random.default_rng()\n        self.test_samples = test_samples\n        \n    def fit(self, X, y, sample_weight = None):\n        \n        \n        # Total new samples\n        num_samples = len(X)\n        num_samples //= 2\n        num_samples //= len(np.unique(y))\n        new_samples, gcd, labels = list(), list(), list()\n        gc.collect()\n        \n        # Replace with alternate samples\n        for val in [1000, 10000]:\n            for label in np.unique(y):\n                for _ in range(num_samples):\n                    num_reads = 1000000 // val\n                    if self.test_samples:\n                        temp = Counter(\n                            self.rng.choice(\n                                features, \n                                size = 1000000 // val, \n                                p = TEST_DIST[label], \n                                replace = True\n                            )\n                        )\n                    else:\n                        temp = Counter(\n                            self.rng.choice(\n                                features, \n                                size = 1000000 // val, \n                                p = TRAIN_DIST[label], \n                                replace = True\n                            )\n                        )\n                    new_samples.append(temp)\n                    gcd.append(val)\n                    labels.append(label)\n        new_samples = pd.DataFrame.from_records(new_samples, columns = features).fillna(0).astype(int)\n        new_samples['target'] = labels\n        new_samples['gcd'] = gcd\n        new_samples = new_samples.sample(frac=1).reset_index(drop=True); gc.collect()\n        new_samples = remove_duplicates(normalize_histograms(new_samples))\n        \n        # Duplicate original data and combine with altered samples \n        original = X.reset_index(drop = True)\n        original['target'] = y.values\n        original['sample_weight'] = sample_weight.values\n        new_samples = pd.concat([original, new_samples], ignore_index = True).sample(frac=1).reset_index(drop=True)\n        \n        # Fit model\n        self.model.fit(new_samples[features + ['gcd']], new_samples['target'], sample_weight = new_samples['sample_weight'])\n        return self\n    \n    def predict_proba(self, X):\n        return self.model.predict_proba(X)\n    \n    def predict(self, X):\n        return self.model.predict(X)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-25T21:16:26.801175Z","iopub.execute_input":"2022-02-25T21:16:26.802845Z","iopub.status.idle":"2022-02-25T21:16:26.828201Z","shell.execute_reply.started":"2022-02-25T21:16:26.802792Z","shell.execute_reply":"2022-02-25T21:16:26.827207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample Using Training Data","metadata":{}},{"cell_type":"code","source":"oof_preds, test_soft, test_hard = score_model(\n    Resampler(\n        test_samples = False\n    )\n)\n\n# Submission (Hard Voting)\nsubmission['target'] = encoder.inverse_transform(test_hard)\nsubmission.to_csv('resample_train_hardvoting_submission.csv', index=False)\n\n# Submission (Soft Voting)\nsubmission['target'] = encoder.inverse_transform(test_soft)\nsubmission.to_csv('resample_train_softvoting_submission.csv', index=False)\n\n# Confusion Matrix\nplot_confusion_matrix(train['target'], oof_preds, train['gcd'], train['sample_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:16:26.829755Z","iopub.execute_input":"2022-02-25T21:16:26.830722Z","iopub.status.idle":"2022-02-25T21:25:18.845749Z","shell.execute_reply.started":"2022-02-25T21:16:26.830627Z","shell.execute_reply":"2022-02-25T21:25:18.841879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample Using Test Data","metadata":{}},{"cell_type":"code","source":"oof_preds, test_soft, test_hard = score_model(\n    Resampler(\n        test_samples = True\n    )\n)\n\n# Submission (Hard Voting)\nsubmission['target'] = encoder.inverse_transform(test_hard)\nsubmission.to_csv('resample_test_hardvoting_submission.csv', index=False)\n\n# Submission (Soft Voting)\nsubmission['target'] = encoder.inverse_transform(test_soft)\nsubmission.to_csv('resample_test_softvoting_submission.csv', index=False)\n\n# Confusion Matrix\nplot_confusion_matrix(train['target'], oof_preds, train['gcd'], train['sample_weight'])","metadata":{"execution":{"iopub.status.busy":"2022-02-25T21:25:18.847339Z","iopub.execute_input":"2022-02-25T21:25:18.848608Z","iopub.status.idle":"2022-02-25T21:34:14.589846Z","shell.execute_reply.started":"2022-02-25T21:25:18.848558Z","shell.execute_reply":"2022-02-25T21:34:14.588509Z"},"trusted":true},"execution_count":null,"outputs":[]}]}