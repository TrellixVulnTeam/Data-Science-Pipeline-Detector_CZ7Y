{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tab2img","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-11T13:05:46.161065Z","iopub.execute_input":"2022-02-11T13:05:46.161712Z","iopub.status.idle":"2022-02-11T13:05:56.246122Z","shell.execute_reply.started":"2022-02-11T13:05:46.161621Z","shell.execute_reply":"2022-02-11T13:05:56.245283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nSo I am going to change the tabular data here into images and then apply a CNN to the result which is quite a novel approach to the problem. I have seen this used successfully in a commercial project and I do believe that in general this approach can be effective. Particular as an additional means of analysing tabular data when also employing other methods. I saw @remekkinas great notebook [here](https://www.kaggle.com/remekkinas/bacteria-image-conv2d-cv-grad-cam) and was inspired but I wanted to try it with my usual approach.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom PIL import Image\nfrom dateutil.parser import parse\nfrom typing import List\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom torch import optim\nimport torch.nn as nn","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-11T13:05:56.249897Z","iopub.execute_input":"2022-02-11T13:05:56.25013Z","iopub.status.idle":"2022-02-11T13:05:58.373626Z","shell.execute_reply.started":"2022-02-11T13:05:56.250103Z","shell.execute_reply":"2022-02-11T13:05:58.372914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Data\n\nWe will load the tabular data. Process it and transform the data to images.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/tabular-playground-series-feb-2022/train.csv\")\ntest_df = pd.read_csv(\"../input/tabular-playground-series-feb-2022/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:05:58.374946Z","iopub.execute_input":"2022-02-11T13:05:58.375181Z","iopub.status.idle":"2022-02-11T13:06:34.092702Z","shell.execute_reply.started":"2022-02-11T13:05:58.375148Z","shell.execute_reply":"2022-02-11T13:06:34.091981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Change the target column to categorical numbers. This is neccessary for training a neural network","metadata":{}},{"cell_type":"code","source":"df[\"target_code\"] = df.target.astype('category').cat.codes\nsaved_link_df = df.loc[:,['target','target_code']].drop_duplicates()\nsaved_link_df","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:06:34.09388Z","iopub.execute_input":"2022-02-11T13:06:34.095842Z","iopub.status.idle":"2022-02-11T13:06:34.162873Z","shell.execute_reply.started":"2022-02-11T13:06:34.0958Z","shell.execute_reply":"2022-02-11T13:06:34.162213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the class balance","metadata":{}},{"cell_type":"code","source":"plt.xticks(rotation = -80) \nsns.countplot(x=df[\"target\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:06:34.165007Z","iopub.execute_input":"2022-02-11T13:06:34.165318Z","iopub.status.idle":"2022-02-11T13:06:34.740953Z","shell.execute_reply.started":"2022-02-11T13:06:34.165284Z","shell.execute_reply":"2022-02-11T13:06:34.740322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split X and y","metadata":{}},{"cell_type":"code","source":"y = df[\"target_code\"]\nX = df.drop(columns=[\"target\",\"target_code\",\"row_id\"], axis=1)\nX_test = test_df.drop(columns=[\"row_id\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:06:34.741987Z","iopub.execute_input":"2022-02-11T13:06:34.743469Z","iopub.status.idle":"2022-02-11T13:06:34.946845Z","shell.execute_reply.started":"2022-02-11T13:06:34.743429Z","shell.execute_reply":"2022-02-11T13:06:34.945481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check column order, if the columns aren't in order between test and train we will have a terrible result as the images will be different","metadata":{}},{"cell_type":"code","source":"idx = 0\nbad_cols = []\nfor col, col2 in zip(df.columns, test_df.columns):\n    if col != col2:\n        bad_cols.append(col)\nif len(bad_cols) == 0:\n    print(\"Columns are in order :)\")","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:06:34.948534Z","iopub.execute_input":"2022-02-11T13:06:34.94891Z","iopub.status.idle":"2022-02-11T13:06:34.95832Z","shell.execute_reply.started":"2022-02-11T13:06:34.948874Z","shell.execute_reply":"2022-02-11T13:06:34.957526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seperate training and validation sets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\ntrain_ratio = 0.90\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y, test_size=1 - train_ratio, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:06:34.959911Z","iopub.execute_input":"2022-02-11T13:06:34.960166Z","iopub.status.idle":"2022-02-11T13:06:35.669497Z","shell.execute_reply.started":"2022-02-11T13:06:34.960134Z","shell.execute_reply":"2022-02-11T13:06:35.668687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create polynomial features so that the images will be much larger","metadata":{}},{"cell_type":"code","source":"temp_train_df = pd.concat([X_train, y_train], axis=1)\ntrain_corr = temp_train_df.corr().sort_values('target_code', ascending=False).index","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:06:35.673886Z","iopub.execute_input":"2022-02-11T13:06:35.675844Z","iopub.status.idle":"2022-02-11T13:07:10.979342Z","shell.execute_reply.started":"2022-02-11T13:06:35.675805Z","shell.execute_reply":"2022-02-11T13:07:10.978386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_series = train_corr.to_series()[1:11]\ntrain_series = pd.Series(train_series.values)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:10.980639Z","iopub.execute_input":"2022-02-11T13:07:10.980956Z","iopub.status.idle":"2022-02-11T13:07:10.987102Z","shell.execute_reply.started":"2022-02-11T13:07:10.980916Z","shell.execute_reply":"2022-02-11T13:07:10.986391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\ndef add_poly_features(X_temp, X_to_concat, poly):\n    polyfeatures = poly.transform(X_temp[train_series].copy())\n    polyfeatures = pd.DataFrame(data=polyfeatures)\n    X_reset = X_to_concat.reset_index()\n    X_concat = pd.concat([X_reset, polyfeatures], axis=1, ignore_index=True)\n    return X_concat.iloc[:,1:]\n\npoly = PolynomialFeatures(interaction_only=True)\ntemp_X_train = X_train[train_series].copy()\npoly = poly.fit(temp_X_train)\nX_train_concat = add_poly_features(X_train.copy(), X_train.copy(), poly)\nX_val_concat = add_poly_features(X_val.copy(), X_val.copy(), poly)\nX_test_concat = add_poly_features(X_test.copy(), X_test.copy(), poly)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:10.988737Z","iopub.execute_input":"2022-02-11T13:07:10.9893Z","iopub.status.idle":"2022-02-11T13:07:12.484874Z","shell.execute_reply.started":"2022-02-11T13:07:10.989259Z","shell.execute_reply":"2022-02-11T13:07:12.484142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do the next lot of polynomial features. This is broken up so that we don't use all the ram of the kaggle instance","metadata":{}},{"cell_type":"code","source":"for i in range(11,91,10):\n    train_series = train_corr.to_series()[i:(i + 10)]\n    train_series = pd.Series(train_series.values)\n    poly = PolynomialFeatures(interaction_only=True)\n    temp_X_train = X_train[train_series].copy()\n    poly = poly.fit(temp_X_train)\n    X_train_concat = add_poly_features(X_train.copy(), X_train_concat, poly)\n    X_val_concat = add_poly_features(X_val.copy(), X_val_concat, poly)\n    X_test_concat = add_poly_features(X_test.copy(), X_test_concat, poly)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:12.485982Z","iopub.execute_input":"2022-02-11T13:07:12.486221Z","iopub.status.idle":"2022-02-11T13:07:34.268032Z","shell.execute_reply.started":"2022-02-11T13:07:12.486191Z","shell.execute_reply":"2022-02-11T13:07:34.267312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scale X and y","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import PolynomialFeatures\n\nscaler = preprocessing.MinMaxScaler().fit(X_train_concat)\nX_scaled_train = scaler.transform(X_train_concat)\nX_scaled_val = scaler.transform(X_val_concat)\nX_scaled_test = scaler.transform(X_test_concat)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:34.269187Z","iopub.execute_input":"2022-02-11T13:07:34.26945Z","iopub.status.idle":"2022-02-11T13:07:37.112193Z","shell.execute_reply.started":"2022-02-11T13:07:34.269417Z","shell.execute_reply":"2022-02-11T13:07:37.111432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert tabular data to images","metadata":{}},{"cell_type":"code","source":"from tab2img.converter import Tab2Img\nmodel = Tab2Img()\ntrain_images = model.fit_transform(X_scaled_train, y_train.values)\nval_images = model.transform(X_scaled_val)\ntest_images = model.transform(X_scaled_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:37.115298Z","iopub.execute_input":"2022-02-11T13:07:37.115636Z","iopub.status.idle":"2022-02-11T13:07:44.641046Z","shell.execute_reply.started":"2022-02-11T13:07:37.1156Z","shell.execute_reply":"2022-02-11T13:07:44.640294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's visualize the images","metadata":{}},{"cell_type":"code","source":"fig,ax = plt.subplots(2,5)\nfor i in range(10):\n    nparray = test_images[i].reshape(29,29)\n    image = Image.fromarray(nparray * 255)\n    ax[i%2][i//2].imshow(image)\nfig.show()\n\nfig,ax = plt.subplots(2,5)\nfor i in range(10):\n    nparray = train_images[i].reshape(29,29)\n    image = Image.fromarray(nparray * 255)\n    ax[i%2][i//2].imshow(image)\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:44.642265Z","iopub.execute_input":"2022-02-11T13:07:44.642623Z","iopub.status.idle":"2022-02-11T13:07:46.136857Z","shell.execute_reply.started":"2022-02-11T13:07:44.642586Z","shell.execute_reply":"2022-02-11T13:07:46.13608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create the Custom Dataset Class\n\nWe need this to be able to load the image and label into the model we will create. So we will create a custom dataset to handle this","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass CustomDataset(Dataset):\n  def __init__(self, X, y, BatchSize, transform):\n    super().__init__()\n    self.BatchSize = BatchSize\n    self.y = y\n    self.X = X\n    self.transform = transform\n    \n  def num_of_batches(self):\n    \"\"\"\n    Detect the total number of batches\n    \"\"\"\n    return math.floor(len(self.list_IDs) / self.BatchSize)\n\n  def __getitem__(self,idx):\n    class_id = self.y[idx]\n    img = self.transform(np.nan_to_num(self.X[idx]))\n    return img, torch.tensor(class_id)\n\n  def __len__(self):\n    return len(self.X)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:46.138296Z","iopub.execute_input":"2022-02-11T13:07:46.138563Z","iopub.status.idle":"2022-02-11T13:07:46.146353Z","shell.execute_reply.started":"2022-02-11T13:07:46.138523Z","shell.execute_reply":"2022-02-11T13:07:46.145406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instantiate the Datasets\n\nWe will form them into torch dataloaders to make the data easier to work with. We are also going to put in a minor amount of image augmentation in the train dataset.from sklearn.preprocessing import PolynomialFeatures","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n                transforms.ToTensor()\n            ])\n\ndataset_stages = ['train', 'val', 'test']\n\nbatch_size = 32\nimage_datasets = {'train' : CustomDataset(train_images, y_train.values, batch_size, transform), 'val' : CustomDataset(val_images, y_val.values, batch_size, transform), 'test' : CustomDataset(test_images, range(0,len(test_df)), batch_size, transform)}\ndataloaders = {'train' : DataLoader(image_datasets['train'], batch_size=image_datasets['train'].BatchSize, shuffle=True, num_workers=0), \n               'val' : DataLoader(image_datasets['val'], batch_size=image_datasets['val'].BatchSize, shuffle=True, num_workers=0), \n               'test' : DataLoader(image_datasets['test'], batch_size=image_datasets['test'].BatchSize, shuffle=False, num_workers=0)}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:46.147786Z","iopub.execute_input":"2022-02-11T13:07:46.148353Z","iopub.status.idle":"2022-02-11T13:07:46.330956Z","shell.execute_reply.started":"2022-02-11T13:07:46.148292Z","shell.execute_reply":"2022-02-11T13:07:46.330283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check an image from the dataset","metadata":{}},{"cell_type":"code","source":"image = transforms.ToPILImage()(image_datasets['train'][412][0].cpu()).convert(\"RGB\")\ndisplay(image)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:46.332266Z","iopub.execute_input":"2022-02-11T13:07:46.332542Z","iopub.status.idle":"2022-02-11T13:07:46.401678Z","shell.execute_reply.started":"2022-02-11T13:07:46.332507Z","shell.execute_reply":"2022-02-11T13:07:46.40094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a Training Function","metadata":{}},{"cell_type":"code","source":"import time\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=10, early_stop_value=0, categorical=True):\n    since = time.time()\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n            num_batches = 0\n            outputs = None\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                # Loading Bar\n                if (phase == 'train'):\n                    num_batches += 1\n                    percentage_complete = ((num_batches * batch_size) / (dataset_sizes[phase])) * 100\n                    percentage_complete = np.clip(percentage_complete, 0, 100)\n                    print(\"{:0.2f}\".format(percentage_complete), \"% complete\", end='\\r')\n\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    labels = labels.type(torch.LongTensor)\n                    labels = labels.to(device)\n                    outputs = outputs.float().to(device)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        # TODO: try removal\n                        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                if categorical:\n                    predicted = torch.max(outputs.data, 1)[1] \n                    running_correct = (predicted == labels).sum()\n                    running_corrects += running_correct\n                else:\n                    running_loss += loss.item() * inputs.size(0)\n                    running_correct = 0\n                    for i in  range(0,len(outputs)):\n                        label = labels.unsqueeze(1).float()[i]\n                        running_correct += abs(abs(outputs[i]) -  abs(label))\n                    running_corrects += running_correct\n                    \n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            \n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n            #epoch_acc = sum(epoch_acc) / len(epoch_acc)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc.item()))\n            # Early Stop\n            if early_stop_value > 0:\n                if phase == 'val':\n                    val_accuracy = epoch_acc.item()\n        if early_stop_value > 0 and val_accuracy > early_stop_value:\n            print(\"*** EARLY STOP ***\")\n            break\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:46.40286Z","iopub.execute_input":"2022-02-11T13:07:46.403249Z","iopub.status.idle":"2022-02-11T13:07:46.46536Z","shell.execute_reply.started":"2022-02-11T13:07:46.403214Z","shell.execute_reply":"2022-02-11T13:07:46.464652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"from torchvision import models\nfrom torch.optim import lr_scheduler\n\nshufflenet = models.shufflenet_v2_x1_0()\nshufflenet.conv1[0] = nn.Conv2d(1, 24, kernel_size=(2, 2), stride=(1, 1))\nshufflenet.fc = nn.Linear(in_features=1024, out_features=10, bias=True)\nmodel_ft = shufflenet","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:46.467028Z","iopub.execute_input":"2022-02-11T13:07:46.467269Z","iopub.status.idle":"2022-02-11T13:07:46.517973Z","shell.execute_reply.started":"2022-02-11T13:07:46.467236Z","shell.execute_reply":"2022-02-11T13:07:46.517366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.Adam(model_ft.parameters(), lr=0.01)\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.01)\n\nmodel_ft = train_model(model_ft.to(device), criterion, optimizer_ft, exp_lr_scheduler, 16)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:07:46.518921Z","iopub.execute_input":"2022-02-11T13:07:46.519164Z","iopub.status.idle":"2022-02-11T14:13:30.449795Z","shell.execute_reply.started":"2022-02-11T13:07:46.519131Z","shell.execute_reply":"2022-02-11T14:13:30.449019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run on Test Set","metadata":{}},{"cell_type":"code","source":"predictions = []\n\noutputs = None\n\nfor inputs, labels in dataloaders['test']:\n    model_ft.eval()\n    model_ft.eval()\n    \n    inputs = inputs.to(device)\n    labels = labels.to(device)\n    outputs = model_ft(inputs)\n    \n    for o in torch.max(outputs.data, 1)[1]:\n        predictions.append(o.cpu().item())    ","metadata":{"execution":{"iopub.status.busy":"2022-02-11T14:13:30.450948Z","iopub.execute_input":"2022-02-11T14:13:30.451201Z","iopub.status.idle":"2022-02-11T14:14:15.422393Z","shell.execute_reply.started":"2022-02-11T14:13:30.451166Z","shell.execute_reply":"2022-02-11T14:14:15.421645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Convert back to names as required for submission","metadata":{}},{"cell_type":"code","source":"label_dict = {}\nfor _, row in saved_link_df.iterrows():\n    label_dict[row[\"target_code\"]] = row[\"target\"]\n\nlabels = []\nfor prediction in predictions:\n    labels.append(label_dict[prediction])","metadata":{"execution":{"iopub.status.busy":"2022-02-11T14:14:15.42372Z","iopub.execute_input":"2022-02-11T14:14:15.423947Z","iopub.status.idle":"2022-02-11T14:14:15.444758Z","shell.execute_reply.started":"2022-02-11T14:14:15.423916Z","shell.execute_reply":"2022-02-11T14:14:15.444097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissiondf = pd.read_csv(\"../input/tabular-playground-series-feb-2022/sample_submission.csv\")\nsubmissiondf[\"target\"] = labels\nsubmissiondf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-11T14:14:15.445925Z","iopub.execute_input":"2022-02-11T14:14:15.446301Z","iopub.status.idle":"2022-02-11T14:14:15.528354Z","shell.execute_reply.started":"2022-02-11T14:14:15.446268Z","shell.execute_reply":"2022-02-11T14:14:15.527697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submissiondf.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T14:14:15.529377Z","iopub.execute_input":"2022-02-11T14:14:15.529831Z","iopub.status.idle":"2022-02-11T14:14:15.743363Z","shell.execute_reply.started":"2022-02-11T14:14:15.529795Z","shell.execute_reply":"2022-02-11T14:14:15.742741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nThis is actually my first experiment with creating polynomial features to tabular data and then applying a neural network to the result. It would have been nice to see more accuracy out of this but I am happy whenever an experiment has been seen through to its end point. Perhaps with some experimentation much greater accuracy can be achieved with this process. I doubt this is the last time I attempt this approach.   ","metadata":{}}]}