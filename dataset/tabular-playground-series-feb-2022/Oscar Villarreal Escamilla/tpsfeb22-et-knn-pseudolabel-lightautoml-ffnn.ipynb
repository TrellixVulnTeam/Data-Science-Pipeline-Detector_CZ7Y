{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro","metadata":{}},{"cell_type":"markdown","source":"Two algorithms from the scikit-learn module performed great out-of-the-box in this competition, compared to the gradient boosting machines and deep learning models:\n* ExtraTreesClassifier, see for example https://www.kaggle.com/hiro5299834/tps-feb-2022-extratreeclassifier by BIZEN (upvote!)\n* KNeighborsClassifier, see for example https://www.kaggle.com/kartik2khandelwal/startified-kfold-with-knn-improved-score/ by Kartik Khandelwal (upvote!)\n\nAmbrosM has explained in his insightful notebook https://www.kaggle.com/ambrosm/tpsfeb22-03-clustering-improves-the-predictions (upvote!) that \"the bacteria in the test set will have undergone mutation and have slightly different DNA\", which explains the unreliability of out-of-fold cross-validation. The competition paper also explains that there are two kinds of obstacle: experimental noise and mutations. https://www.frontiersin.org/articles/10.3389/fmicb.2020.00257/full Experimental noise seems to affect both training and testing sets equally, but as pointed out by AmbrosM, mutations affect each set differently.\n\nIf I understood these insights correctly, this competition is therefore about how to deal with non-representative data due to both sampling bias (i.e. mutations) and sampling noise (i.e. experimental noise):\n* The sampling noise does not seem to be an issue thanks to the large amount of data.\n* The sampling bias is the main issue because it cannot be avoided without \"peeking\" into the testing set, or can it? Interestingly, the fact that \"extra trees\" and \"nearest neighbors\" performed so well might provide some hints about this. Let's start by loading the data.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Input","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nxtrain = pd.read_csv('../input/tabular-playground-series-feb-2022/train.csv',index_col=0).drop_duplicates()\nxtest = pd.read_csv('../input/tabular-playground-series-feb-2022/test.csv',index_col=0)\nytrain = xtrain.pop('target').to_frame()\n\n# Target encoding as one-hot or integer:\nytrain_ohe = pd.get_dummies(ytrain['target']) #one-hot\nytoname = {x:y for x,y in enumerate(ytrain_ohe.columns)} #integer to name\nytoint = {y:x for x,y in enumerate(ytrain_ohe.columns)} #name to integer\nytrain['target_num'] = ytrain['target'].map(ytoint)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"markdown","source":"Two observations regarding the nearest neighbor algorithm:\n* The performance is better without cross-validation, that is, using the full training set only once.\n* The performance is better with the manhattan metric, inverse-distance weights and only 2 neighbors.\n\nThis indicates that for a specific observation in the testing set, there might be one or two observations in the training set with similar mutations. The absolute value used by the Manhattan distance (L1) appears to be more robust against outlier mutations than the euclidean distance (L2).","metadata":{}},{"cell_type":"code","source":"# Fit on full training set once and predict class probabilities:\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_jobs=-1,metric='manhattan',weights='distance',n_neighbors=2)\nmodel.fit(xtrain,ytrain['target_num'])\nytest_prob = pd.DataFrame(model.predict_proba(xtest),index=xtest.index,columns=ytrain_ohe.columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As AmbrosM pointed out, the probabilities need to be adjusted in order to balance the amount of observations per class. Most notebooks did this by adding different constants to each class. ","metadata":{}},{"cell_type":"code","source":"# Check class distributions:\nytest_knn = ytest_prob.apply(lambda x: x.argmax(),axis=1)\nytest_knn = ytest_knn.map(ytoname).rename('target').reset_index()\ndisplay(ytest_knn.target.value_counts(normalize=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I found that the 90th percentile could be used more effectively in this case, notice how it is centered around 50% probability?:","metadata":{}},{"cell_type":"code","source":"display(ytest_prob.quantile(0.9))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can therefore simply rescale the probabilities of each class through the 90th percentile and then make it go back to 50% probability, which will result in 10% of the observations having more than 50% probability for each class, as pointed out by xuedaolao in the comments section:","metadata":{}},{"cell_type":"code","source":"ytest_prob /= ytest_prob.quantile(0.9)\nytest_prob /= 2\ndisplay(ytest_prob.quantile(0.9))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The resulting submission scored **0.98835** in the public LB:","metadata":{}},{"cell_type":"code","source":"ytest_knn = ytest_prob.apply(lambda x: x.argmax(),axis=1)\nytest_knn = ytest_knn.map(ytoname).rename('target').to_frame()\nytest_knn[['target']].to_csv('submission_knn.csv')\ndisplay(ytest_knn.target.value_counts(normalize=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ET + pseudolabel LightAutoML ","metadata":{}},{"cell_type":"markdown","source":"The \"Extra Trees\" approach has been excellently applied by several people. An additional improvement in performance has been observed by using the extra-trees predictions on the testing-set as \"pseudolabels\" to re-fit a second model. An excellent example of this is the following notebook by Alexander Ryzhkov, in which the \"LightAutoML\" model was fitted on the pseudolabels: https://www.kaggle.com/alexryzhkov/tps-feb-22-lightautoml-pseudolabel (upvote!).\n\nMost notebooks combined different algorithms by taking the modes of the predicted classes; however, I found that simply averaging the probabilities predicted by KNN and LightAutoML worked better (the submission file below scored **0.99066** in the public LB). Since the notebook above did not output the probabilities, the following lines had to be added in a separate fork of the notebook, in addition to updating the test predictions to the following shared ensemble: https://www.kaggle.com/kotrying/extra-blender-addition by kotrying (upvote!) as shown below:","metadata":{}},{"cell_type":"markdown","source":"> pseudolabels = pd.read_csv('../input/extra-blender-addition/submission.csv')","metadata":{}},{"cell_type":"markdown","source":"> test_pred_out = pd.DataFrame(test_pred.data) <br>\n> test_pred_out.columns = mapper.keys() <br>\n> test_pred_out.to_csv('test_pred.csv') <br>","metadata":{}},{"cell_type":"code","source":"# Combine probabilities with current best tree-based model shared by Alexander Ryzhkov:\n# https://www.kaggle.com/alexryzhkov/tps-feb-22-lightautoml-pseudolabel\nytest_prob_automl = pd.read_csv(\n    '../input/a-fork-of-tps-feb-22-lightautoml-pseudolabel/test_pred.csv',index_col=0,header=0\n)\nytest_prob = 0.5*ytest_prob_automl[ytest_prob.columns].values + 0.5*ytest_prob\nytest = ytest_prob.apply(lambda x: x.argmax(),axis=1).rename('target_num').to_frame()\nytest['target'] = ytest.target_num.map(ytoname)\nytest[['target']].to_csv('submission_knn_automl.csv')\ndisplay(ytest.target.value_counts(normalize=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scikit-learn user guide explains that the \"extra trees\" approach \"allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias\" in comparison to other tree-based models. https://scikit-learn.org/stable/modules/ensemble.html#forest This would mean that a very small change in the data caused by the mutations could cause in turn the model to \"jump\" to another bacteria class prediction and therefore this model is better able to avoid overfitting on the mutations, similarly to the Manhattan (L1) distance in the case of nearest neighbors.","metadata":{}},{"cell_type":"markdown","source":"# GCD postprocessing with pseudolabel KNN","metadata":{}},{"cell_type":"markdown","source":"In AmbrosM's notebook (referenced in the intro), he applied an unsupervised method on a subset of the GCDs in order to re-label overlapping clusters of observations, which required searching for and isolating each pair of overlapping clusters in a lower-dimensional space. \n\nOne alternative and perhaps simpler way is to fit a supervised algorithm such as KNN on the pseudolabels of the GCDs of interest. Two things to point out:\n* Due to the training set's sampling bias, the algorithm was fitted entirely on the testing set.\n* Since the distance of each observation to itself is zero, the number of neighbors had to be increased and the weights had to become \"uniform\" instead of \"inverse-distance\".","metadata":{}},{"cell_type":"code","source":"# Get GCD as explained by AmbrosM:\n# https://www.kaggle.com/ambrosm/tpsfeb22-03-clustering-improves-the-predictions\nimport numpy as np\nfrom math import factorial\ndef bias_of(s):\n    w = int(s[1:s.index('T')])\n    x = int(s[s.index('T')+1:s.index('G')])\n    y = int(s[s.index('G')+1:s.index('C')])\n    z = int(s[s.index('C')+1:])\n    return factorial(10) / (factorial(w) * factorial(x) * factorial(y) * factorial(z) * 4**10)\ndef gcd_of_all(df_i):\n    gcd = df_i[xtrain.columns[0]]\n    for col in xtrain.columns[1:]:\n        gcd = np.gcd(gcd, df_i[col])\n    return gcd\nitrain = pd.DataFrame({col: ((xtrain[col] + bias_of(col)) * 1000000).round().astype(int) for col in list(xtrain.columns)})\nitest = pd.DataFrame({col: ((xtest[col] + bias_of(col)) * 1000000).round().astype(int) for col in list(xtest.columns)})\nytrain['gcd'], ytest['gcd'] = gcd_of_all(itrain), gcd_of_all(itest)\n\n# Subset GCD to either 1 or 10:\nxtest_sel, ytest_sel = xtest.loc[ytest['gcd'].isin([1,10])], ytest.loc[ytest['gcd'].isin([1,10])]\nytest_ohe = pd.get_dummies(ytest_sel['target'])\n\n# Re-label classes through Manhattan metric and uniform weights:\nytest_proba_knn = 0*ytest_ohe.copy()\nytest_int = 0*ytest_sel['target_num'].copy()\nmodel = KNeighborsClassifier(n_jobs=-1,n_neighbors=20,weights='uniform',metric='manhattan')\nytest_proba_knn += model.fit(xtest_sel,ytest_sel.target_num).predict_proba(xtest_sel)\nytest_int += np.argmax(ytest_proba_knn.values, axis=1)\ndisplay(pd.crosstab(ytest_sel.target_num,ytest_int))\n\n\n# Convert to target name, check class distributions and save:\nysub = ytest.copy()\nysub.loc[ytest_int.index,'target_num'] = ytest_int\nysub['target'] = ysub['target_num'].map(ytoname)\ndisplay(ysub.target.value_counts(normalize=True))\nysub[['target']].to_csv('submission_pseudo_knn.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GCD postprocessing with pseudolabel FFNN","metadata":{}},{"cell_type":"markdown","source":"I am curious about whether the sampling bias of the training set is an inherent property of this data or whether a new variable could be added to measure the \"mutation level\" of each observation relative to some reference point. Perhaps then it would be possible for a neural network to learn how the data evolves as a function of the mutations... But as it is, I found it better to fit the model (a simple feed-forward network) entirely on the testing set only.","metadata":{}},{"cell_type":"code","source":"# Load Tensorflow libraries:\nimport os,random\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\nfrom tensorflow_addons.layers import WeightNormalization\nfrom sklearn.model_selection import StratifiedKFold\ndef do_seed_tf(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n# A simple feed-forward network with both batch and weight normalization:\ndef model_builder():\n    in_num = L.Input(shape=(xtrain.shape[1],))\n    out_num = L.BatchNormalization()(in_num)\n    out_num = L.Dropout(0.1)(out_num)\n    out_num = WeightNormalization(L.Dense(688,activation='relu',kernel_initializer=\"HeUniform\"))(out_num)\n    out_num = L.BatchNormalization()(out_num)\n    out_num = L.Dropout(0.1)(out_num)\n    out_num = WeightNormalization(L.Dense(688,activation='swish'))(out_num)\n    out_num = L.BatchNormalization()(out_num)\n    out_num = L.Dropout(0.1)(out_num)\n    out = WeightNormalization(L.Dense(10,activation='softmax'))(out_num)\n    model = tf.keras.Model(inputs=in_num, outputs=out)\n    model.compile(\n        metrics=['accuracy'], loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.01),\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001)\n    )\n    return(model)\n\n# K-fold CV of coordinates:\nseeds, folds = 5, 10\nytest_proba_ffnn = 0*ytest_ohe.copy()\nytest_int = 0*ytest_sel['target_num'].copy()\nfor seed in range(seeds):\n    skf = StratifiedKFold(n_splits=folds,random_state=seed,shuffle=True)\n    for fold, (idt,idv) in enumerate(skf.split(ytest_sel,ytest_sel['target_num'])):\n        print('\\r',seed,fold,end='\\t')\n        K.clear_session()\n        model = model_builder()\n        do_seed_tf(seed)\n        history = model.fit(\n            xtest_sel.iloc[idt], ytest_ohe.iloc[idt],\n            validation_data=(xtest_sel.iloc[idv], ytest_ohe.iloc[idv]),\n            batch_size=256, epochs=100, verbose=0,\n            callbacks=[\n                ReduceLROnPlateau(monitor='val_loss',mode='min',\n                    verbose=0,factor=0.5,patience=4),\n                EarlyStopping(monitor='val_accuracy',mode='max',restore_best_weights=True,\n                    verbose=0,min_delta=1e-5,patience=12),\n                ModelCheckpoint(f'tmp.hdf5',monitor='val_accuracy',mode='max',\n                    verbose=0,save_best_only=True,save_weights_only=True),\n            ]\n        )\n        model.load_weights(f'tmp.hdf5')\n        ytest_proba_ffnn.iloc[idv] += model.predict(xtest_sel.iloc[idv]) / seeds\n\n# Predictions:\nytest_int += np.argmax(ytest_proba_ffnn.values, axis=1)\ndisplay(pd.crosstab(ytest_sel.target_num,ytest_int))\n\n# Save result:\nysub = ytest.copy()\nysub.loc[ytest_int.index,'target_num'] = ytest_int\nysub['target'] = ysub['target_num'].map(ytoname)\ndisplay(ysub.target.value_counts(normalize=True))\nysub[['target']].to_csv('submission_pseudo_ffnn.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The last step was to average the GCD postprocessing probabilities:","metadata":{}},{"cell_type":"code","source":"# Ensemble pseudo KNN and pseudo FFNN:\nytest_proba_ensemble = 0*ytest_ohe.copy()\nytest_int = 0*ytest_sel['target_num'].copy()\nytest_proba_ensemble += 0.5*ytest_proba_knn.values + 0.5*ytest_proba_ffnn.values\nytest_int += np.argmax(ytest_proba_ensemble.values, axis=1)\ndisplay(pd.crosstab(ytest_sel.target_num,ytest_int))\n\n# Save result:\nysub = ytest.copy()\nysub.loc[ytest_int.index,'target_num'] = ytest_int\nysub['target'] = ysub['target_num'].map(ytoname)\ndisplay(ysub.target.value_counts(normalize=True))\nysub[['target']].to_csv('submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thank you for reading!","metadata":{}}]}