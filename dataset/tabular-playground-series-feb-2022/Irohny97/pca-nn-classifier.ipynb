{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-11T10:58:23.892256Z","iopub.execute_input":"2022-02-11T10:58:23.892679Z","iopub.status.idle":"2022-02-11T10:58:23.925888Z","shell.execute_reply.started":"2022-02-11T10:58:23.892556Z","shell.execute_reply":"2022-02-11T10:58:23.925163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Simple Principle component with neural net approach for classification. \nDon't make things too complicated :)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom sklearn.decomposition import PCA, SparsePCA\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\nimport random","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T10:58:23.927488Z","iopub.execute_input":"2022-02-11T10:58:23.928141Z","iopub.status.idle":"2022-02-11T10:58:29.595479Z","shell.execute_reply.started":"2022-02-11T10:58:23.928103Z","shell.execute_reply":"2022-02-11T10:58:29.594645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/train.csv')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T10:58:29.599992Z","iopub.execute_input":"2022-02-11T10:58:29.600547Z","iopub.status.idle":"2022-02-11T10:58:54.101165Z","shell.execute_reply.started":"2022-02-11T10:58:29.600503Z","shell.execute_reply":"2022-02-11T10:58:54.100427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's first take a quick look at our data set and how many times each bacterium was measured. Furthermore, let's look at the emerging 10-mer DNA sequences. ","metadata":{}},{"cell_type":"code","source":"df = train_data['target'].value_counts()\nprint(df)\nnames = list(df.keys())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T10:58:54.102375Z","iopub.execute_input":"2022-02-11T10:58:54.102624Z","iopub.status.idle":"2022-02-11T10:58:54.145027Z","shell.execute_reply.started":"2022-02-11T10:58:54.10259Z","shell.execute_reply":"2022-02-11T10:58:54.144171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = np.array((train_data.columns), dtype='str')[1:-1]\ncols","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T10:58:54.147668Z","iopub.execute_input":"2022-02-11T10:58:54.148108Z","iopub.status.idle":"2022-02-11T10:58:54.158887Z","shell.execute_reply.started":"2022-02-11T10:58:54.148069Z","shell.execute_reply":"2022-02-11T10:58:54.158098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we consider the mean measured spectra. For this we use two different approaches. In the first section, we consider the mean with standard deviation in an error bar plot. ","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 5, figsize=(30, 15))\nmeans = np.zeros((10, 286))\nstdv = np.zeros((10, 286))\n\nfor i in range(10):\n    subset = np.array(train_data[train_data['target']==names[i]])\n    subset = np.array(subset[:, 1:-1], dtype=np.float64)\n    N = np.sum(subset, 1)\n    mean = np.mean(subset, 0)\n    means[i, :] = mean\n    std = np.std(subset, 0)\n    stdv[i, :] = std\n    \n    x = np.linspace(0, len(mean)-1, len(mean))\n    \n    axs[i%2, i%5].plot(x, mean, 'r', lw=2, label='Mean')\n    axs[i%2, i%5].vlines(x=x, ymin=mean-std, ymax=mean+std, color='b', alpha=0.4, label='stdv.')\n    axs[i%2, i%5].set_title(names[i])\n    axs[i%2, i%5].grid()\n    axs[i%2, i%5].set_xlim([-2, 286])\n    axs[i%2, i%5].set_xticks([0, 50, 100, 150, 200, 250])\n    axs[i%2, i%5].set_xticklabels(cols[0::50], rotation=-45)\n    #axs[i%2, i%5].set_xlabel('DNA Sequence')\n    axs[i%2, i%5].legend(loc='upper left')\n\ndel subset, mean, std, fig, axs","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T10:58:54.160342Z","iopub.execute_input":"2022-02-11T10:58:54.160592Z","iopub.status.idle":"2022-02-11T10:59:05.225012Z","shell.execute_reply.started":"2022-02-11T10:58:54.160559Z","shell.execute_reply":"2022-02-11T10:59:05.224308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we break down the distribution of the emerging 10-mer DNA sequences in a heatmap. here we can see nicely that only certain 10-mer sequences appear and only some of them are really suitable for classification. For this focus on the important 10-mers, we will use the principle component analysis in the following steps. In addition, the standard deviations of the 10-mer measurements are shown. Here it can be seen that, as expected, the deviations per 10-mer sequence hardly vary across the individual classes, so that a fixed standard deviation per 10-mer can be used for the subsequent data augmentation.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1,1,figsize=(30, 10))\nim = axs.matshow(means, aspect='auto')\naxs.set_yticks(range(10))\naxs.grid()\n#axs.set_xlabel('DNA Sequence')\naxs.set_yticklabels(names)\naxs.set_xticks([0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275])\naxs.set_xticklabels(cols[0::25], rotation=-45)\naxs.set_title('Mean Normalized Raman Spectrum')\ncax = fig.add_axes([axs.get_position().x1+0.01,axs.get_position().y0,0.02,axs.get_position().height])\naxs.tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\nfig.colorbar(im, cax=cax)  \n\nfig, axs = plt.subplots(1,1,figsize=(30, 10))\nim = axs.matshow(stdv, aspect='auto')\naxs.set_yticks(range(10))\naxs.grid()\n#axs.set_xlabel('DNA Sequence')\naxs.set_yticklabels(names)\naxs.set_xticks([0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275])\naxs.set_xticklabels(cols[0::25], rotation=-45)\naxs.set_title('Standard diviation of the messurements')\naxs.tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\ncax = fig.add_axes([axs.get_position().x1+0.01,axs.get_position().y0,0.02,axs.get_position().height])\nfig.colorbar(im, cax=cax)\n\nfig, axs = plt.subplots(1,1, figsize=(30, 5))\naxs.errorbar(range(len(cols)), np.mean(stdv, 0), yerr=np.std(stdv, 0), fmt='o')\naxs.set_xticks([0, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250, 275])\naxs.set_xticklabels(cols[0::25], rotation=-45)\naxs.set_title('mean standard diviation of all meassurnents')\naxs.grid()\naxs.set_xlim([0, 286])\ndel  means, fig, axs","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T10:59:05.225983Z","iopub.execute_input":"2022-02-11T10:59:05.226198Z","iopub.status.idle":"2022-02-11T10:59:06.799937Z","shell.execute_reply.started":"2022-02-11T10:59:05.226166Z","shell.execute_reply":"2022-02-11T10:59:06.799126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now create a training and validation dataset that will be used for training the neural network and PCA. For the PCA, the individual measurements are normalized to their mean and standard deviation (z-score).","metadata":{}},{"cell_type":"code","source":"sort_dataset = []\nlabel = []\nfor i in range(10):\n    subset = np.array(train_data[train_data['target']==names[i]])\n    if i == 0:\n        sort_dataset = np.array(subset[:, 1:-1], dtype=np.float32)\n        label = i*np.ones(np.shape(subset)[0])\n    else:\n        sort_dataset = np.append(sort_dataset, np.array(subset[:, 1:-1], dtype=np.float32), axis=0)\n        label = np.append(label, i*np.ones(np.shape(subset)[0]), axis=0)\n        \n\ntrain_feat, test_feat, train_label, test_label = train_test_split(sort_dataset, label, test_size=0.2, shuffle=True) \n\n#del train_data, subset, sort_dataset, label\n\ntrain_feat = ((train_feat.T-np.mean(train_feat.T, 0))/np.std(train_feat.T, 0)).T\ntest_feat = ((test_feat.T-np.mean(test_feat.T, 0))/np.std(test_feat.T, 0)).T\n\nprint('Trainset: %.f, %.f' %(np.shape(train_feat)[0], np.shape(train_feat)[1]))\nprint('Train Label: %.f ' %np.shape(train_label))\nprint('Testset: %.f, %.f' %(np.shape(test_feat)[0], np.shape(test_feat)[1]))\nprint('Test Label: %.f' %np.shape(test_label))\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T10:59:06.801286Z","iopub.execute_input":"2022-02-11T10:59:06.801593Z","iopub.status.idle":"2022-02-11T10:59:16.904702Z","shell.execute_reply.started":"2022-02-11T10:59:06.801557Z","shell.execute_reply":"2022-02-11T10:59:16.903943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we consider the variance of the data set, which we can explain using the individual pca components. To do this, we look at the variance of the individual components, as well as the commulated explained variance. We can see that with the first 100 pca components we can already explain more than 90% of the occurring variance of the data. This should be a good starting value for the input dimension of the neural network, which can be varied afterwards. ","metadata":{}},{"cell_type":"code","source":"pca = PCA(n_components=286, whiten=True, random_state=42)\n#pca = SparsePCA(n_components=286, random_state=42)\n\n\npca.fit(train_feat) # input shape (NSample, NFeature)\npca_vectors = pca.components_ # shape [n_components, n_features]\n\nexp_var = pca.explained_variance_ratio_\nfig, axs = plt.subplots(1,3, figsize=(24,8))\naxs[0].plot(range(len(exp_var)), exp_var, 'o')\naxs[0].set_title('Explained variance by each principle component')\n\naxs[1].plot(range(len(exp_var)), np.cumsum(exp_var))\naxs[1].set_title('Cumulative Sum of pc')\n\naxs[2].matshow(pca_vectors[0:10, :].T, aspect='auto')\naxs[2].set_title('')\naxs[2].set_xlabel(' Principle Component ')\naxs[2].set_ylabel('DNA Sequence')\n\nfor i in range(2):\n    axs[i].grid()\n    axs[i].set_xlabel('principle component')\n    axs[i].set_ylabel('explained variance')\n    \ndel fig, axs","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:59:16.906016Z","iopub.execute_input":"2022-02-11T10:59:16.906755Z","iopub.status.idle":"2022-02-11T10:59:21.399838Z","shell.execute_reply.started":"2022-02-11T10:59:16.906716Z","shell.execute_reply":"2022-02-11T10:59:21.399129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_trans = pca.transform(train_feat)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T10:59:21.400909Z","iopub.execute_input":"2022-02-11T10:59:21.40128Z","iopub.status.idle":"2022-02-11T10:59:21.909929Z","shell.execute_reply.started":"2022-02-11T10:59:21.40123Z","shell.execute_reply":"2022-02-11T10:59:21.909057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us now take a look at the data in the first two pca components.","metadata":{}},{"cell_type":"code","source":"fig1, axs1 = plt.subplots(1,1, figsize=(15, 10))\nfor i in range(10):\n    idx = np.array(np.where(train_label==i)).squeeze()\n    axs1.plot(train_trans[idx, 0],train_trans[idx, 1], 'o', alpha=0.6)\naxs1.legend(names, bbox_to_anchor=(1.4, 1), loc='upper right', fontsize=15)\naxs1.grid()\naxs1.set_xlabel('PC1 (%.2f)'%(exp_var[0]*100), fontsize=20)\naxs1.set_ylabel('PC2 (%.2f)'%(exp_var[1]*100), fontsize=20)\n\ndel fig1, axs1, train_trans","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T10:59:21.91153Z","iopub.execute_input":"2022-02-11T10:59:21.912029Z","iopub.status.idle":"2022-02-11T10:59:22.999813Z","shell.execute_reply.started":"2022-02-11T10:59:21.91199Z","shell.execute_reply":"2022-02-11T10:59:22.999125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us now set up the input pipeline and the neural network. The network is a simple combination of dense layers with dropout and batch normalization. Additionally we have the possibility to become a L2 regularization. (Warning spoiler: while searching for the hyperparameters it turned out that L2 regularization is not needed for good performance on the test set). The input pipeline has the task to give a noise on the training data to account for the errors within the individual measurements and to prevent overfitting. The noise consists of random numbers from a normal distribution with a mean of 0 and a standard deviation adjusted to the 10-measurement sequence (mean standard deviation of the individual 10-measurements from above).","metadata":{}},{"cell_type":"code","source":"def NN_Classifier(input_dim, Nclasses, NLayer=5, lr=1e-3, dp_rate=0.4, l2_rate=1e-4):\n    inp = Input(batch_shape=(None, input_dim))\n    x = Dense(1024,  kernel_regularizer=regularizers.l2(l2_rate), activation='relu')(inp)\n    x = BatchNormalization()(x)\n    x = Dropout(dp_rate)(x)\n\n    for i in range(NLayer):\n        x = Dense(1024,  kernel_regularizer=regularizers.l2(l2_rate), activation='relu')(x)\n        x = BatchNormalization()(x)\n        x = Dropout(dp_rate)(x)\n        \n    out = Dense(Nclasses, activation='softmax')(x)\n    model = Model(inputs=inp, outputs=out)\n    \n    tf.keras.optimizers.Adam(learning_rate=lr)\n    model.compile(optimizer='Adam', loss= tf.keras.losses.categorical_crossentropy, metrics=[tf.keras.metrics.CategoricalAccuracy()])\n    model.summary()\n    return model\n\nclass augmenation_pipeline():\n    def __init__(self):\n        self.data = [] # Trainingsdata\n        self.stdv = [] # stdv\n        self.label =  []\n        self.pca_vectors = []\n        self.one_hot_label = []\n        self.batch_size = 512\n        self.NClasses = 10\n        self.noise_width = 1\n        \n    def train_generator(self):\n        NData, NFeat = np.shape(self.data)\n                \n        self.generate_label()\n        \n        NRuns_per_epoch = NData//self.batch_size\n        for i in range(NRuns_per_epoch):\n            start = i*self.batch_size\n            \n            noise = self.generate_noise(NData, NFeat, self.label[start:start+self.batch_size])\n            data = self.data[start:start+self.batch_size, :] + noise\n            feat = self.pca_dimension_reduction(self.normalization(data).T)\n            \n            label =  self.one_hot_label[start:start+self.batch_size, :]\n            \n            yield feat, label\n    \n    def test_generator(self):\n        NData, NFeat = np.shape(self.data)\n                \n        self.generate_label()\n        \n        NRuns_per_epoch = NData//self.batch_size\n        for i in range(NRuns_per_epoch):\n            start = i*self.batch_size\n            \n            data = (self.data[start:start+self.batch_size, :])\n            \n            feat = self.pca_dimension_reduction(self.normalization(data).T)\n            \n            label =  self.one_hot_label[start:start+self.batch_size, :]\n            \n            yield feat, label\n    \n    def prediction_generator(self, data):\n        return self.pca_dimension_reduction(self.normalization(data).T)\n    \n    def generate_noise(self, NData, NFeat, label):\n        noise = np.zeros((self.batch_size, NFeat))\n        for i in range(NFeat):\n            noise[:,i] = np.random.normal(0, self.noise_width*np.mean(self.stdv[:, i]), size=self.batch_size)\n        return noise\n\n    def generate_label(self):\n        self.one_hot_label = np.zeros((len(self.label), self.NClasses))\n    \n        for i in range(len(self.label)):\n            self.one_hot_label[i, int(self.label[i])] = 1 # dim [sample, Nclasses]\n            \n    def pca_dimension_reduction(self, data):\n        # Input:\n        # dim. pca_vec = [component, feature]\n        # dim. data = [feature, sample]\n        # Output:\n        # dim. [sample, component]\n        return np.matmul(self.pca_vectors, data).T\n    \n    def normalization(self, data):\n        return ((data.T-np.mean(data.T, 0))/np.std(data.T, 0)).T\n        \n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:59:23.001415Z","iopub.execute_input":"2022-02-11T10:59:23.00201Z","iopub.status.idle":"2022-02-11T10:59:23.025802Z","shell.execute_reply.started":"2022-02-11T10:59:23.001974Z","shell.execute_reply":"2022-02-11T10:59:23.025062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"now let's build the training setup.","metadata":{}},{"cell_type":"code","source":"Npc = 150\nlr = 1e-3\nl2 = 0#1e-10\ndr = 0.4\nbs = 256\nNlayer = 10\nnwd = 1\n\nmodel = NN_Classifier(Npc, 10, Nlayer, lr, dr, l2)\ntrain_generator = augmenation_pipeline()\ntest_generator = augmenation_pipeline()\n\ntrain_generator.data = train_feat\ntrain_generator.stdv = stdv\ntrain_generator.label = train_label\ntrain_generator.batch_size = bs\ntrain_generator.pca_vectors = pca_vectors[0:Npc, :]\ntrain_generator.noise_width = nwd\n\ntest_generator.data = test_feat\ntest_generator.stdv = stdv\ntest_generator.label = test_label\ntest_generator.batch_size = bs\ntest_generator.pca_vectors = pca_vectors[0:Npc, :]\n\nfeat_size = (bs, Npc,)\nlabel_size = (bs, 10, )\n\ntrain_dataset = tf.data.Dataset.from_generator(generator=train_generator.train_generator, output_types=(tf.float32, tf.float32), output_shapes=(feat_size, label_size)) \ntest_dataset = tf.data.Dataset.from_generator(generator=test_generator.test_generator, output_types=(tf.float32, tf.float32), output_shapes=(feat_size, label_size)) ","metadata":{"execution":{"iopub.status.busy":"2022-02-11T10:59:23.027184Z","iopub.execute_input":"2022-02-11T10:59:23.027602Z","iopub.status.idle":"2022-02-11T10:59:25.348871Z","shell.execute_reply.started":"2022-02-11T10:59:23.027563Z","shell.execute_reply":"2022-02-11T10:59:25.348075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's the training begin ","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\ncustom_early_stopping = EarlyStopping(\n    monitor='val_categorical_accuracy', \n    patience=10, \n    min_delta=0.0001, \n    mode='max',\n    restore_best_weights=True\n)\nhist = model.fit(train_dataset, validation_data=(test_dataset), batch_size=bs, epochs=200, callbacks=[custom_early_stopping])","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-02-11T10:59:25.351882Z","iopub.execute_input":"2022-02-11T10:59:25.352105Z","iopub.status.idle":"2022-02-11T11:20:28.468854Z","shell.execute_reply.started":"2022-02-11T10:59:25.352074Z","shell.execute_reply":"2022-02-11T11:20:28.467987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"as we can see, the different batteries can be classified well.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1,2, figsize=(10, 5))\naxs[0].plot(range(len(hist.history['loss'])), hist.history['loss'], 'r', label='train')\naxs[0].plot(range(len(hist.history['loss'])), hist.history['val_loss'], 'b', label='validation')\naxs[1].plot(range(len(hist.history['loss'])), hist.history['categorical_accuracy'], 'r', label='train')\naxs[1].plot(range(len(hist.history['loss'])), hist.history['val_categorical_accuracy'], 'b', label='validation')\nfor i in range(2):\n    axs[i].grid()\n    axs[i].set_xlabel('Epochs')\n    axs[i].legend()\naxs[0].set_ylabel('categorical crossentropy')\naxs[1].set_ylabel('categorical accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-02-11T11:20:28.475908Z","iopub.execute_input":"2022-02-11T11:20:28.480205Z","iopub.status.idle":"2022-02-11T11:20:28.862888Z","shell.execute_reply.started":"2022-02-11T11:20:28.480153Z","shell.execute_reply":"2022-02-11T11:20:28.862114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now add a little analysis of the klassiefizier. The output of the model can be seen as a probability distribution, which indicates which bacterium is involved for a given input. First, let's look at the mean probability distribution of each class. In addition, we look at the incorrectly assigned bacteria and see what they were swapped with.","metadata":{}},{"cell_type":"code","source":"def label_id_to_label(pred_label_id, label_name):\n    id_label = np.argmax(pred_label_id, 1)\n\n    label = []\n    for i in range(len(id_label)):\n        label.append(label_name[id_label[i]])\n\n    return label\n\npred_label_id = model.predict(test_generator.prediction_generator(test_feat))\npred_label = np.argmax(pred_label_id, 1)\n\nwsk_label = (pred_label_id.T/np.sum(pred_label_id, 1)).T\n\nconf_mat = np.zeros((10, 10))\nfalse_mat = np.zeros((10, 10))\n\nfor i in range(10):\n    idx = np.array(np.where(test_label==i)).squeeze()\n    tmp = np.sum(wsk_label[idx], 0)/len(idx)\n    conf_mat[i, :] = tmp\n    \n    id_false = np.array(np.where(pred_label[idx] != i)).squeeze()\n    if (len(id_false) > 1):\n        for j in range(len(id_false)):\n            idf = int(pred_label[idx[id_false[j]]])\n            false_mat[i, idf] = false_mat[i, idf]+1\n        false_mat[i, :] = false_mat[i, :]/len(id_false)\n        \n    elif  (len(id_false) == 1):\n        false_mat[i, id_false]=1\n    \ndel tmp","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T11:20:28.864393Z","iopub.execute_input":"2022-02-11T11:20:28.864635Z","iopub.status.idle":"2022-02-11T11:20:31.378064Z","shell.execute_reply.started":"2022-02-11T11:20:28.864599Z","shell.execute_reply":"2022-02-11T11:20:31.377293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1,2, figsize=(28,8))\nlabel_ticks = np.linspace(0, 9, 10)\ngrid_ticks =  np.linspace(-0.5,9.5 , 11)\n\npcol = axs[0].matshow(conf_mat, aspect='auto', cmap='Reds', alpha=0.6)\nclb = fig.colorbar(pcol, ax=axs[0]) \nclb.set_label('probability',size=15)\n\nfor i in range(10):\n    for j in range(10):\n        axs[0].text(j-0.4, i, '%.3f'%(conf_mat[i,j]))\n\npcol = axs[1].matshow(false_mat, aspect='auto', cmap='Reds', alpha=0.6)\nclb = fig.colorbar(pcol, ax=axs[1]) \nclb.set_label('probability',size=15)\n\nfor i in range(10):\n    for j in range(10):\n        axs[1].text(j-0.4, i, '%.3f'%(false_mat[i,j]))\n\nfor i in range(2):\n    axs[i].set_xticks(label_ticks, fontsize=12)\n    axs[i].set_yticks(label_ticks, fontsize=12)\n    axs[i].set_xticklabels(names, rotation=-75)\n    axs[i].set_yticklabels(names)\n\n    axs[i].set_xticks(grid_ticks, minor=True)\n    axs[i].set_yticks(grid_ticks, minor=True)\n    axs[i].grid(which='minor')\n\n    axs[i].tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\n    axs[i].set_xlabel('predited label', fontsize=15)\n    axs[i].set_ylabel('true label', fontsize=15)\n\naxs[0].set_title('mean output distribution per class', fontsize=20)\naxs[1].set_title('distribution of false predicted inputs', fontsize=20)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T11:20:31.379261Z","iopub.execute_input":"2022-02-11T11:20:31.379503Z","iopub.status.idle":"2022-02-11T11:20:33.541875Z","shell.execute_reply.started":"2022-02-11T11:20:31.37947Z","shell.execute_reply":"2022-02-11T11:20:33.541149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It follows the classification of the submission data.","metadata":{}},{"cell_type":"code","source":"sub_data = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/test.csv')\nsubm = pd.read_csv('/kaggle/input/tabular-playground-series-feb-2022/sample_submission.csv')\n\n#sub_data = convert_data_back_to_histogram(sub_data)\nsub_data = np.array(sub_data, dtype=np.float32) \nsub_data = sub_data[:, 1:]","metadata":{"execution":{"iopub.status.busy":"2022-02-11T11:20:33.542995Z","iopub.execute_input":"2022-02-11T11:20:33.543381Z","iopub.status.idle":"2022-02-11T11:20:45.440376Z","shell.execute_reply.started":"2022-02-11T11:20:33.543344Z","shell.execute_reply":"2022-02-11T11:20:45.43959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission_feat = pca_dimension_reduction(pca_vectors[0:Npc, :], test_data.T)\n#test_generator.data = sub_data\nprediction = model.predict(test_generator.prediction_generator(sub_data))\n    \npred_label = label_id_to_label(prediction, names)\n\nsubm['target']=pred_label\nsubm.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T11:20:45.441662Z","iopub.execute_input":"2022-02-11T11:20:45.441902Z","iopub.status.idle":"2022-02-11T11:20:56.390099Z","shell.execute_reply.started":"2022-02-11T11:20:45.44187Z","shell.execute_reply":"2022-02-11T11:20:56.389294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"last but not least some code for the hyperparameter fitting. Based on random parameter combinations to get an impression of the behavior of the individual parameters.","metadata":{}},{"cell_type":"code","source":"hyperfitting = False\n\nif hyperfitting == True : \n    del model, train_generator, test_generator\n    tf.keras.backend.clear_session()\n    NRuns = 50\n    \n    custom_early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=10, min_delta=0.001, mode='max')\n    \n    BatchSize = [128, 256, 512, 1024]\n    learning_rates = [1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5]\n    dropout_rates = [0.3, 0.4, 0.5, 0.6]\n    l2_rates = [5e-3, 1e-3, 5e-4, 1e-4]\n    Npca = [2, 50, 100, 150, 200, 268]\n    Nlayer = [5, 10, 20, 50, 100]\n    book = np.zeros((NRuns, 7))\n    \n    for i in range(NRuns):\n        print('Run %.f/%.f' % (i+1, NRuns))\n        Npc = int(np.array(random.sample(Npca, 1), dtype=np.int32))\n        book[i, 0] = Npc\n        lr = float(np.array(random.sample(learning_rates, 1), dtype=np.float32))\n        book[i, 1] = lr\n        dpr = float(np.array(random.sample(dropout_rates, 1), dtype=np.float32))\n        book[i, 2] = dpr\n        l2 = float(np.array(random.sample(l2_rates, 1), dtype=np.float32))\n        book[i, 3] = l2\n        bs = int(np.array(random.sample(BatchSize, 1), dtype=np.int32))\n        book[i, 4] = bs \n        nl = int(np.array(random.sample(Nlayer, 1), dtype=np.int32))\n        book[i, 5] = nl\n        \n        model = NN_Classifier(int(Npc), 10, int(nl), float(lr), float(dpr), float(l2))\n\n        model = NN_Classifier(Npc, 10, nl, lr, dr, l2)\n        train_generator = augmenation_pipeline()\n        test_generator = augmenation_pipeline()\n\n        train_generator.data = train_feat\n        train_generator.stdv = stdv\n        train_generator.label = train_label\n        train_generator.batch_size = bs\n        train_generator.pca_vectors = pca_vectors[0:Npc, :]\n\n        test_generator.data = test_feat\n        test_generator.stdv = stdv\n        test_generator.label = test_label\n        test_generator.batch_size = bs\n        test_generator.pca_vectors = pca_vectors[0:Npc, :]\n\n        feat_size = (bs, Npc,)\n        label_size = (bs, 10, )\n\n        train_dataset = tf.data.Dataset.from_generator(generator=train_generator.train_generator, output_types=(tf.float32, tf.float32), output_shapes=(feat_size, label_size)) \n        test_dataset = tf.data.Dataset.from_generator(generator=test_generator.test_generator, output_types=(tf.float32, tf.float32), output_shapes=(feat_size, label_size))\n        \n        hist = model.fit(train_dataset, validation_data=(test_dataset), batch_size=bs, epochs=200, callbacks=[custom_early_stopping])\n        book[i, 6] = np.max(hist.history['val_categorical_accuracy'])\n        \n        del model, train_generator, test_generator\n        tf.keras.backend.clear_session()\n    \n    frame = pd.DataFrame(book, columns=['N_PCA', 'LR', 'DR', 'L2', 'BS', 'NL', 'ACC'])","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T11:20:56.391375Z","iopub.execute_input":"2022-02-11T11:20:56.391622Z","iopub.status.idle":"2022-02-11T11:20:56.411049Z","shell.execute_reply.started":"2022-02-11T11:20:56.391588Z","shell.execute_reply":"2022-02-11T11:20:56.410319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if hyperfitting == True:\n    frame = frame.sort_values(by=['ACC'], ascending=False)\n    print(frame)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-02-11T11:20:56.412555Z","iopub.execute_input":"2022-02-11T11:20:56.413034Z","iopub.status.idle":"2022-02-11T11:20:56.424808Z","shell.execute_reply.started":"2022-02-11T11:20:56.412994Z","shell.execute_reply":"2022-02-11T11:20:56.424056Z"},"trusted":true},"execution_count":null,"outputs":[]}]}