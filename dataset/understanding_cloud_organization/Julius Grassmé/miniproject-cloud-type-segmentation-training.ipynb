{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This notebook have trained the models for the miniproject, currently it is setup to run the baseline model \n\n# It uses U-Net++ with the pretrained EfficientNet B5 weights on to save kaggle training time, \n# I use the code from another notebook intended for the Serverstal competition but which i have adapted for this competition, i have forked the original notebook to show support.\n# The notebook code is used as upposed to writing it from scratch as it have features which is otherwise quite time consuming to reimplement\n# The main features of this notebook are:\n\n# 1: It uses pretrained weights for the backbone in Unet which is preferable since GPU time is limited\n# See: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet or the appropriate dataset here on kaggle\n\n# 2: That it comes with an implementation of the accumulative optemizer which allows for the usage of larger batch sizes than what the GPU can handle in 1 go\n# see for more info: https://translate.googleusercontent.com/translate_c?depth=1&rurl=translate.google.com&sl=zh-CN&sp=nmt4&tl=en&u=https://kexue.fm/archives/6794&xid=25657,15700021,15700186,15700190,15700256,15700259,15700262,15700265,15700271,15700283&usg=ALkJrhj73QF8gX6v1hUInBfN1JueIUFY9g \n# also: https://github.com/bojone/accum_optimizer_for_keras/blob/master/accum_optimizer.py\n\n# 3: The dynamic decoding and of the dataset is set up which is needed for this dataset as having all the data in memory is infeseable here on kaggle\n# this is just the standard generator https://keras.io/preprocessing/image/ (i first did it from scratch without in another notebook but discovered the issue there)\n\n\n# The steps taken in this notebook are\n# 1: Preprocessing which removes images with no lables so the net overfits less to empty images \n# 2: The model is built with the pretrained weights from the Efficientnet, here weights that was trained on imagenet 1000 are used\n# 3: The generator for labels and images are defiend i downscale 1400, 2100 to 256, 384 as this preserves aspect ratio and speed up computation(but can be a problem for classes where the features are small in the images)\n# 4: the model is trained using Adam for 21 epochs with the accumelation optimizer to allow grater batch sizes, here the dice coefficient is used as this is the evaluation metric for the competition\n# 5: the model is saved and the training history is printed out\n\n\n\"\"\"\n# About this kernel(from starter code)\n\nPlease note that this training kernel can't be submitted directly, since it is trained for more than one hour. Please see [the inference kernel](https://www.kaggle.com/xhlulu/severstal-efficient-u-net-inference), with LB > 0.87.\n\n* **Preprocessing**: Discard all of the training data that have all 4 masks missing. We will only train on images that have at least 1 mask, so that our model does not overfit on empty masks.\n* **Utility Functions**: Utilities for encoding and decoding masks. Also create a `DataGenerator` class slightly modified from the other class, such that it can resize the encoded and decoded masks to the desired size.\n* **Model: U-Net++ with EfficientNet Encoder**: Idea comes from [Siddartha's Efficient U-Net++](https://www.kaggle.com/meaninglesslives/unet-plus-plus-with-efficientnet-encoder). I modified the architecture by using simple convolution layers instead of residual blocks for decoding. Furthermore, my implementation is slightly different from the original paper since it performs a resolution reduction in the `H` and `U` functions, which was not exactly specified in the original paper. I also refactored most of the code in order to match the notation in the paper, and included the last efficientnet block (which was omitted by Siddartha).\n* **Training the model**: We train the model for 30 epochs using an Adam with learning rate of 3e-4. The model with the best `val_dice_coef` is saved.\n* **Evaluation**: Display the change in loss and Dice score throughout the epochs.\n\n## Changelog\n* **V28 [LB ?]**: Disabled GroupNormalization, added gradient accumulation.\n* **V27 [LB 0.87328]**: Replaced all BatchNormalization with GroupNormalization, following [this discussion](https://www.kaggle.com/c/aptos2019-blindness-detection/discussion/104686).\n* **V18 [LB 0.87501]**: Added albumentations and many different types of transforms. Not yet evaluated.\n* **V16 [LB 0.8766]**: Added random horizontal and vertical flip.\n* **V13 [LB 0.8737]**: Changed loss from `binary_cross_entropy` to `bce_dice_loss`.\n\n## References\n* GroupNormalization: https://www.kaggle.com/c/aptos2019-blindness-detection/discussion/104686\n* UNet ++ Original Architecture: https://www.kaggle.com/meaninglesslives/unet-plus-plus-with-efficientnet-encoder\n* Data generator: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n* RLE encoding and decoding: https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\n* Mask encoding: https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation/data\n* Original Kernel U-Net: https://www.kaggle.com/xhlulu/severstal-simple-keras-u-net-boilerplate\n* Original 2-step pipeline: https://www.kaggle.com/xhlulu/severstal-simple-2-step-pipeline\n* Missing mask predictor: https://www.kaggle.com/xhlulu/severstal-predict-missing-masks\n* Source for `bce_dice_loss`: https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/\n\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport json\nimport gc\n\nimport albumentations as albu\nimport cv2\nimport keras\nfrom keras import backend as K\nfrom keras.engine import Layer, InputSpec\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras import initializers, constraints, regularizers, layers\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dropout, Conv2D, BatchNormalization, add\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras import backend as K\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ELU\nfrom keras.losses import binary_crossentropy\nfrom keras.layers.merge import concatenate, Concatenate, Add\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback, ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data overview"},{"metadata":{},"cell_type":"markdown","source":"## Sample submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '../input/understanding_cloud_organization'\nos.listdir(path)\ntrain = pd.read_csv(f'{path}/train.csv')\nsub = pd.read_csv(f'{path}/sample_submission.csv')\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## train test data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the image dataset consists of:' )\ntrain_num = len(os.listdir(f'{path}/train_images'))\ntest_num = len(os.listdir(f'{path}/test_images'))\nprint(f'{train_num} images in the training dataset')\nprint(f'{test_num} images in the testing dataset')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('where the lables for the training is distributed like so')\ntrain.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[1]).value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('the distribution of images with multiple classes looks like this')\ntrain.loc[train['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[0]).value_counts().value_counts()\nprint('there are a lot of images with multiple classes')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## image data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from PIL import Image\n\ndef rle_decode(mask_rle, shape=(350, 525)):\n    \n    try:\n        s = mask_rle[0].split()\n\n        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n        starts -= 1\n        ends = starts + lengths\n        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n        for lo, hi in zip(starts, ends):\n            img[lo:hi] = 1\n            \n        return img.reshape(shape, order='F')  # Needed to align to RLE direction\n    except:\n        #print(\"fail\")\n        return np.zeros((350, 525)) \n\n\ntrain['label'] = train['Image_Label'].apply(lambda x: x.split('_')[1])\ntrain['im_id'] = train['Image_Label'].apply(lambda x: x.split('_')[0])\n\nsub['label'] = sub['Image_Label'].apply(lambda x: x.split('_')[1])\nsub['im_id'] = sub['Image_Label'].apply(lambda x: x.split('_')[0])\n\nfig = plt.figure(figsize=(25, 16))\nfor j, im_id in enumerate(np.random.choice(train['im_id'].unique(), 4)):\n    for i, (idx, row) in enumerate(train.loc[train['im_id'] == im_id].iterrows()):\n        ax = fig.add_subplot(5, 4, j * 4 + i + 1, xticks=[], yticks=[])\n        im = Image.open(f\"{path}/train_images/{row['Image_Label'].split('_')[0]}\")\n        plt.imshow(im)\n        mask_rle = row['EncodedPixels']\n        mask = rle_decode(mask_rle)\n        plt.imshow(mask, alpha=0.5, cmap='gray')\n        ax.set_title(f\"im: {row['Image_Label'].split('_')[0]}. la: {row['label']}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## data distribution considerations\n\nthe data as seen above are unbalanced in the frequency of occourance as there are far more annotations of sugar and gravel than flower and fish \nthe sugar and gravel class are however less concreate in deffinition and so this might balance the training results a bit\n\napprox 75% of the images have more than 1 label which means that it is possible to have label overlap especially with sugar and gravel this might pose a problem\n\nlastly the data provided have empty masks for the lables that do not excist, thiese have to be removed before training otherwise they bias the training towards empty mask\n\nthe lables are square and there is data missing in many images, the data missing might pose a problem(although i am not sure on this, maybe the batch norm will handle it) \n"},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/efficientnet-keras-source-code/repository/qubvel-efficientnet-c993591/ --quiet\nimport efficientnet.keras as efn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df = pd.read_csv('../input/cloud-experiment-dataset/trainC1-4.csv') # the 4 pure classes of the 16 derived classes\n#train_df = pd.read_csv('../input/cloud-experiment-dataset/trainC1-16.csv')# the 16 derived classes\ntrain_df = pd.read_csv('../input/understanding_cloud_organization/train.csv') # the original data\n\ntrain_df['ImageId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[0])\ntrain_df['ClassId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[1])\ntrain_df['hasMask'] = ~ train_df['EncodedPixels'].isna()\n\nprint(train_df.shape)\ntrain_df.head()\n\n\nmask_count_df = train_df.groupby('ImageId').agg(np.sum).reset_index()\nmask_count_df.sort_values('hasMask', ascending=False, inplace=True)\nprint(mask_count_df.shape)\nmask_count_df.head()\n\nsub_df = pd.read_csv('../input/understanding_cloud_organization/sample_submission.csv')\nsub_df['ImageId'] = sub_df['Image_Label'].apply(lambda x: x.split('_')[0])\ntest_imgs = pd.DataFrame(sub_df['ImageId'].unique(), columns=['ImageId'])\ntest_imgs.head()\n\nnon_missing_train_idx = mask_count_df[mask_count_df['hasMask'] > 0]\nnon_missing_train_idx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def np_resize(img, input_shape):\n    \"\"\"Reshape a numpy array, which is input_shape=(height, width), as opposed to input_shape=(width, height) for cv2\"\"\"\n    height, width = input_shape\n    return cv2.resize(img, (width, height))\n    \ndef mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle2mask(rle, input_shape):\n    width, height = input_shape[:2]\n    \n    mask= np.zeros( width*height ).astype(np.uint8)\n    \n    array = np.asarray([int(x) for x in rle.split()])\n    starts = array[0::2]\n    lengths = array[1::2]\n\n    current_position = 0\n    for index, start in enumerate(starts):\n        mask[int(start):int(start+lengths[index])] = 1\n        current_position += lengths[index]\n        \n    return mask.reshape(height, width).T\n\ndef build_masks(rles, input_shape, reshape=None):\n    depth = len(rles)\n    if reshape is None:\n        masks = np.zeros((*input_shape, depth))\n    else:\n        masks = np.zeros((*reshape, depth))\n    \n    for i, rle in enumerate(rles):\n        if type(rle) is str:\n            if reshape is None:\n                masks[:, :, i] = rle2mask(rle, input_shape)\n            else:\n                mask = rle2mask(rle, input_shape)\n                reshaped_mask = np_resize(mask, reshape)\n                masks[:, :, i] = reshaped_mask\n    \n    return masks\n\ndef build_rles(masks):\n    width, height, depth = masks.shape\n    \n    rles = [mask2rle(masks[:, :, i])\n            for i in range(depth)]\n    \n    return rles","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, df, target_df=None, mode='fit',\n                 #base_path='../input/severstal-steel-defect-detection/train_images',\n                 #batch_size=32, dim=(256, 1600), n_channels=3, reshape=None,\n                 base_path='../input/understanding_cloud_organization/train_images',\n                 batch_size=32, dim=(1400, 2100), n_channels=3, reshape=None,\n                 augment=False, n_classes=4, random_state=2019, shuffle=True):\n        self.dim = dim\n        self.batch_size = batch_size\n        self.df = df\n        self.mode = mode\n        self.base_path = base_path\n        self.target_df = target_df\n        self.list_IDs = list_IDs\n        self.reshape = reshape\n        self.n_channels = n_channels\n        self.augment = augment\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.random_state = random_state\n        \n        self.on_epoch_end()\n        np.random.seed(self.random_state)\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_batch = [self.list_IDs[k] for k in indexes]\n        \n        X = self.__generate_X(list_IDs_batch)\n        \n        if self.mode == 'fit':\n            y = self.__generate_y(list_IDs_batch)\n            \n            if self.augment:\n                X, y = self.__augment_batch(X, y)\n            \n            return X, y\n        \n        elif self.mode == 'predict':\n            return X\n\n        else:\n            raise AttributeError('The mode parameter should be set to \"fit\" or \"predict\".')\n        \n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.seed(self.random_state)\n            np.random.shuffle(self.indexes)\n    \n    def __generate_X(self, list_IDs_batch):\n        'Generates data containing batch_size samples'\n        # Initialization\n        if self.reshape is None:\n            X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        else:\n            X = np.empty((self.batch_size, *self.reshape, self.n_channels))\n        \n        # Generate data\n        for i, ID in enumerate(list_IDs_batch):\n            im_name = self.df['ImageId'].iloc[ID]\n            img_path = f\"{self.base_path}/{im_name}\"\n            img = self.__load_rgb(img_path)\n            \n            if self.reshape is not None:\n                img = np_resize(img, self.reshape)\n            \n            # Store samples\n            X[i,] = img\n\n        return X\n    \n    def __generate_y(self, list_IDs_batch):\n        if self.reshape is None:\n            y = np.empty((self.batch_size, *self.dim, self.n_classes), dtype=int)\n        else:\n            y = np.empty((self.batch_size, *self.reshape, self.n_classes), dtype=int)\n        \n        for i, ID in enumerate(list_IDs_batch):\n            im_name = self.df['ImageId'].iloc[ID]\n            image_df = self.target_df[self.target_df['ImageId'] == im_name]\n            \n            rles = image_df['EncodedPixels'].values\n            \n            if self.reshape is not None:\n                masks = build_masks(rles, input_shape=self.dim, reshape=self.reshape)\n            else:\n                masks = build_masks(rles, input_shape=self.dim)\n            \n            y[i, ] = masks\n\n        return y\n    \n    def __load_grayscale(self, img_path):\n        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n        img = img.astype(np.float32) / 255.\n        img = np.expand_dims(img, axis=-1)\n\n        return img\n    \n    def __load_rgb(self, img_path):\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = img.astype(np.float32) / 255.\n\n        return img\n    \n    def __random_transform(self, img, masks):\n        composition = albu.Compose([\n            albu.HorizontalFlip(),\n            albu.VerticalFlip(),\n            albu.ShiftScaleRotate(rotate_limit=30),\n#             albu.OpticalDistortion(),\n#             albu.GridDistortion(),\n#             albu.ElasticTransform()\n        ])\n        \n        composed = composition(image=img, mask=masks)\n        aug_img = composed['image']\n        aug_masks = composed['mask']\n        \n        return aug_img, aug_masks\n    \n    def __augment_batch(self, img_batch, masks_batch):\n        for i in range(img_batch.shape[0]):\n            img_batch[i, ], masks_batch[i, ] = self.__random_transform(img_batch[i, ], masks_batch[i, ])\n        \n        return img_batch, masks_batch","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GroupNormalization(keras.layers.Layer):\n    \"\"\"Group normalization layer\n    Group Normalization divides the channels into groups and computes within each group\n    the mean and variance for normalization. GN's computation is independent of batch sizes,\n    and its accuracy is stable in a wide range of batch sizes\n    # Arguments\n        groups: Integer, the number of groups for Group Normalization.\n        axis: Integer, the axis that should be normalized\n            (typically the features axis).\n            For instance, after a `Conv2D` layer with\n            `data_format=\"channels_first\"`,\n            set `axis=1` in `BatchNormalization`.\n        epsilon: Small float added to variance to avoid dividing by zero.\n        center: If True, add offset of `beta` to normalized tensor.\n            If False, `beta` is ignored.\n        scale: If True, multiply by `gamma`.\n            If False, `gamma` is not used.\n            When the next layer is linear (also e.g. `nn.relu`),\n            this can be disabled since the scaling\n            will be done by the next layer.\n        beta_initializer: Initializer for the beta weight.\n        gamma_initializer: Initializer for the gamma weight.\n        beta_regularizer: Optional regularizer for the beta weight.\n        gamma_regularizer: Optional regularizer for the gamma weight.\n        beta_constraint: Optional constraint for the beta weight.\n        gamma_constraint: Optional constraint for the gamma weight.\n    # Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n    # Output shape\n        Same shape as input.\n    # References\n        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n    \"\"\"\n\n    def __init__(self,\n                 groups=32,\n                 axis=-1,\n                 epsilon=1e-5,\n                 center=True,\n                 scale=True,\n                 beta_initializer='zeros',\n                 gamma_initializer='ones',\n                 beta_regularizer=None,\n                 gamma_regularizer=None,\n                 beta_constraint=None,\n                 gamma_constraint=None,\n                 **kwargs):\n        super(GroupNormalization, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.groups = groups\n        self.axis = axis\n        self.epsilon = epsilon\n        self.center = center\n        self.scale = scale\n        self.beta_initializer = initializers.get(beta_initializer)\n        self.gamma_initializer = initializers.get(gamma_initializer)\n        self.beta_regularizer = regularizers.get(beta_regularizer)\n        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n        self.beta_constraint = constraints.get(beta_constraint)\n        self.gamma_constraint = constraints.get(gamma_constraint)\n\n    def build(self, input_shape):\n        dim = input_shape[self.axis]\n\n        if dim is None:\n            raise ValueError('Axis ' + str(self.axis) + ' of '\n                             'input tensor should have a defined dimension '\n                             'but the layer received an input with shape ' +\n                             str(input_shape) + '.')\n\n        if dim < self.groups:\n            raise ValueError('Number of groups (' + str(self.groups) + ') cannot be '\n                             'more than the number of channels (' +\n                             str(dim) + ').')\n\n        if dim % self.groups != 0:\n            raise ValueError('Number of groups (' + str(self.groups) + ') must be a '\n                             'multiple of the number of channels (' +\n                             str(dim) + ').')\n\n        self.input_spec = InputSpec(ndim=len(input_shape),\n                                    axes={self.axis: dim})\n        shape = (dim,)\n\n        if self.scale:\n            self.gamma = self.add_weight(shape=shape,\n                                         name='gamma',\n                                         initializer=self.gamma_initializer,\n                                         regularizer=self.gamma_regularizer,\n                                         constraint=self.gamma_constraint)\n        else:\n            self.gamma = None\n        if self.center:\n            self.beta = self.add_weight(shape=shape,\n                                        name='beta',\n                                        initializer=self.beta_initializer,\n                                        regularizer=self.beta_regularizer,\n                                        constraint=self.beta_constraint)\n        else:\n            self.beta = None\n        self.built = True\n\n    def call(self, inputs, **kwargs):\n        input_shape = K.int_shape(inputs)\n        tensor_input_shape = K.shape(inputs)\n\n        # Prepare broadcasting shape.\n        reduction_axes = list(range(len(input_shape)))\n        del reduction_axes[self.axis]\n        broadcast_shape = [1] * len(input_shape)\n        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n        broadcast_shape.insert(1, self.groups)\n\n        reshape_group_shape = K.shape(inputs)\n        group_axes = [reshape_group_shape[i] for i in range(len(input_shape))]\n        group_axes[self.axis] = input_shape[self.axis] // self.groups\n        group_axes.insert(1, self.groups)\n\n        # reshape inputs to new group shape\n        group_shape = [group_axes[0], self.groups] + group_axes[2:]\n        group_shape = K.stack(group_shape)\n        inputs = K.reshape(inputs, group_shape)\n\n        group_reduction_axes = list(range(len(group_axes)))\n        group_reduction_axes = group_reduction_axes[2:]\n\n        mean = K.mean(inputs, axis=group_reduction_axes, keepdims=True)\n        variance = K.var(inputs, axis=group_reduction_axes, keepdims=True)\n\n        inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n\n        # prepare broadcast shape\n        inputs = K.reshape(inputs, group_shape)\n        outputs = inputs\n\n        # In this case we must explicitly broadcast all parameters.\n        if self.scale:\n            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n            outputs = outputs * broadcast_gamma\n\n        if self.center:\n            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n            outputs = outputs + broadcast_beta\n\n        outputs = K.reshape(outputs, tensor_input_shape)\n\n        return outputs\n\n    def get_config(self):\n        config = {\n            'groups': self.groups,\n            'axis': self.axis,\n            'epsilon': self.epsilon,\n            'center': self.center,\n            'scale': self.scale,\n            'beta_initializer': initializers.serialize(self.beta_initializer),\n            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n            'beta_constraint': constraints.serialize(self.beta_constraint),\n            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n        }\n        base_config = super(GroupNormalization, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        return input_shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class AccumOptimizer(keras.optimizers.Optimizer):\n    \"\"\"继承Optimizer类，包装原有优化器，实现梯度累积。\n    # 参数\n        optimizer：优化器实例，支持目前所有的keras优化器；\n        steps_per_update：累积的步数。\n    # 返回\n        一个新的keras优化器\n    Inheriting Optimizer class, wrapping the original optimizer\n    to achieve a new corresponding optimizer of gradient accumulation.\n    # Arguments\n        optimizer: an instance of keras optimizer (supporting\n                    all keras optimizers currently available);\n        steps_per_update: the steps of gradient accumulation\n    # Returns\n        a new keras optimizer.\n    \"\"\"\n    def __init__(self, optimizer, steps_per_update=1, **kwargs):\n        super(AccumOptimizer, self).__init__(**kwargs)\n        self.optimizer = optimizer\n        with K.name_scope(self.__class__.__name__):\n            self.steps_per_update = steps_per_update\n            self.iterations = K.variable(0, dtype='int64', name='iterations')\n            self.cond = K.equal(self.iterations % self.steps_per_update, 0)\n            self.lr = self.optimizer.lr\n            self.optimizer.lr = K.switch(self.cond, self.optimizer.lr, 0.)\n            for attr in ['momentum', 'rho', 'beta_1', 'beta_2']:\n                if hasattr(self.optimizer, attr):\n                    value = getattr(self.optimizer, attr)\n                    setattr(self, attr, value)\n                    setattr(self.optimizer, attr, K.switch(self.cond, value, 1 - 1e-7))\n            for attr in self.optimizer.get_config():\n                if not hasattr(self, attr):\n                    value = getattr(self.optimizer, attr)\n                    setattr(self, attr, value)\n            # 覆盖原有的获取梯度方法，指向累积梯度\n            # Cover the original get_gradients method with accumulative gradients.\n            def get_gradients(loss, params):\n                return [ag / self.steps_per_update for ag in self.accum_grads]\n            self.optimizer.get_gradients = get_gradients\n    def get_updates(self, loss, params):\n        self.updates = [\n            K.update_add(self.iterations, 1),\n            K.update_add(self.optimizer.iterations, K.cast(self.cond, 'int64')),\n        ]\n        # 累积梯度 (gradient accumulation)\n        self.accum_grads = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n        grads = self.get_gradients(loss, params)\n        for g, ag in zip(grads, self.accum_grads):\n            self.updates.append(K.update(ag, K.switch(self.cond, ag * 0, ag + g)))\n        # 继承optimizer的更新 (inheriting updates of original optimizer)\n        self.updates.extend(self.optimizer.get_updates(loss, params)[1:])\n        self.weights.extend(self.optimizer.weights)\n        return self.updates\n    def get_config(self):\n        iterations = K.eval(self.iterations)\n        K.set_value(self.iterations, 0)\n        config = self.optimizer.get_config()\n        K.set_value(self.iterations, iterations)\n        return config","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model: U-Net++ with EfficientNet Encoder\n\nPlease check the original implementation here: https://www.kaggle.com/meaninglesslives/unet-plus-plus-with-efficientnet-encoder#Defining-UEfficientNet-Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Losses and Metrics\n\ndef dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def H(lst, name, use_gn=False):\n    if use_gn:\n        norm = GroupNormalization(groups=1, name=name+'_gn')\n    else:\n        norm = BatchNormalization(name=name+'_bn')\n    \n    x = concatenate(lst)\n    num_filters = int(x.shape.as_list()[-1]/2)\n    \n    x = Conv2D(num_filters, (2, 2), padding='same', name=name)(x)\n    x = norm(x)\n    x = LeakyReLU(alpha=1.0, name=name+'_activation')(x)\n    \n    return x\n\ndef U(x, use_gn=False):\n    if use_gn:\n        norm = GroupNormalization(groups=1)\n    else:\n        norm = BatchNormalization()\n    \n    num_filters = int(x.shape.as_list()[-1]/2)\n    \n    x = Conv2DTranspose(num_filters, (3, 3), strides=(2, 2), padding='same')(x)\n    x = norm(x)\n    x = LeakyReLU(alpha=1.0)(x)\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def EfficientUNet(input_shape):\n    backbone = efn.EfficientNetB4(\n        weights=None,\n        include_top=False,\n        input_shape=input_shape\n    )\n    \n    backbone.load_weights(('../input/efficientnet-keras-weights-b0b5/'\n                           'efficientnet-b4_imagenet_1000_notop.h5'))\n    \n    # Skipping block 4f and block 6h since they have the same output dim as 5f and 7b\n    x00 = backbone.input  # (256, 384, 3) \n    x10 = backbone.get_layer('stem_activation').output  # (128, 192‬, 4) \n    x20 = backbone.get_layer('block2d_add').output  # (64, 96, 32)\n    x30 = backbone.get_layer('block3d_add').output  # (32, 48, 56)\n    x40 = backbone.get_layer('block5f_add').output  # (16, 24, 160)\n    x50 = backbone.get_layer('block7b_add').output  # (8, 12, 448)\n    \n    x01 = H([x00, U(x10)], 'X01')\n    x11 = H([x10, U(x20)], 'X11')\n    x21 = H([x20, U(x30)], 'X21')\n    x31 = H([x30, U(x40)], 'X31')\n    x41 = H([x40, U(x50)], 'X41')\n    \n    x02 = H([x00, x01, U(x11)], 'X02')\n    x12 = H([x11, U(x21)], 'X12')\n    x22 = H([x21, U(x31)], 'X22')\n    x32 = H([x31, U(x41)], 'X32')\n    \n    x03 = H([x00, x01, x02, U(x12)], 'X03')\n    x13 = H([x12, U(x22)], 'X13')\n    x23 = H([x22, U(x32)], 'X23')\n    \n    x04 = H([x00, x01, x02, x03, U(x13)], 'X04')\n    x14 = H([x13, U(x23)], 'X14')\n    \n    x05 = H([x00, x01, x02, x03, x04, U(x14)], 'X05')\n    \n    x_out = Concatenate(name='bridge')([x01, x02, x03, x04, x05])\n    x_out = Conv2D(4, (3,3), padding=\"same\", name='final_output', activation=\"sigmoid\")(x_out)\n    \n    return Model(inputs=x00, outputs=x_out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = EfficientUNet((256, 512, 3))\nmodel = EfficientUNet((256, 384 ,3))\nmodel.compile(optimizer=AccumOptimizer(Adam(2e-3), 4), loss=bce_dice_loss, metrics=[dice_coef])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 8\n\ntrain_idx, val_idx = train_test_split(\n    non_missing_train_idx.index,  \n    random_state=2019, \n    test_size=0.2\n)\n\ntrain_generator = DataGenerator(\n    train_idx, \n    reshape=(256, 384),\n    df=mask_count_df,\n    target_df=train_df,\n    augment=True,\n    batch_size=BATCH_SIZE, \n    n_classes=4\n)\n\nval_generator = DataGenerator(\n    val_idx, \n    reshape=(256, 384), \n    df=mask_count_df,\n    target_df=train_df,\n    augment=False,\n    batch_size=BATCH_SIZE, \n    n_classes=4\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = ModelCheckpoint(\n    'model.h5', \n    monitor='val_loss', \n    verbose=0, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)\n\nhistory = model.fit_generator(\n    train_generator,\n    validation_data=val_generator, \n    callbacks=[checkpoint],\n    verbose=1,\n    epochs=1\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # History"},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('history.json', 'w') as f:\n    json.dump(history.history, f)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot()\nhistory_df[['dice_coef', 'val_dice_coef']].plot()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}