{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ensembling/Stacking in Python (Korean ver.)\n\n- author : **Yewon Kang**\n- notebook : https://www.kaggle.com/dolylupec/ensembling-stacking-in-python-titanic-dataset\n- dataset : https://www.kaggle.com/c/titanic\n- original author : https://www.kaggle.com/arthurtok \n\n**Ensemble** 이란? 여러개의 머신러닝 모델의 결과를 합쳐서 성능을 높이는 방법.\n\n### 이 노트북에서 사용하는 앙상블 기법\n- Ensemble 모델 (Random Forest, Extra Trees, Adaboost, Gradient Boosting)\n- Out-of-Fold\n- Stacking","metadata":{}},{"cell_type":"markdown","source":"**Table of Contents**\n- Feature Enginnering\n- Visualization\n- out-of-fold\n- 초기 모델 설정 (parameter)\n    - Random Forest Classifier\n    - Extra Trees Classifier\n    - Adaboost Classifier\n    - Gradient Boosting Classifier\n    - Support Vector Machine (Classifier)\n- Interactive feature importances via Plotly scatterplots,barplot\n- Stacking (2번째 학습)\n  - XGBoost","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# stacking에서 5개의 base model 사용\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\n# sklearn.cross_validation -> sklearn.model_selection\nfrom sklearn.model_selection import KFold","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:30.011976Z","iopub.execute_input":"2021-11-21T15:11:30.013545Z","iopub.status.idle":"2021-11-21T15:11:31.872722Z","shell.execute_reply.started":"2021-11-21T15:11:30.013114Z","shell.execute_reply":"2021-11-21T15:11:31.871891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Exploration, Engineering and Cleansing","metadata":{}},{"cell_type":"code","source":"# Load in train and test datasets\n# path 변경\ntrain = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ntrain.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:31.874509Z","iopub.execute_input":"2021-11-21T15:11:31.874762Z","iopub.status.idle":"2021-11-21T15:11:31.933523Z","shell.execute_reply.started":"2021-11-21T15:11:31.874732Z","shell.execute_reply":"2021-11-21T15:11:31.932616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:31.934843Z","iopub.execute_input":"2021-11-21T15:11:31.935143Z","iopub.status.idle":"2021-11-21T15:11:31.942349Z","shell.execute_reply.started":"2021-11-21T15:11:31.935112Z","shell.execute_reply":"2021-11-21T15:11:31.941234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"full_data = [train, test]\n\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test['Cabin'].apply(lambda x: 0 if type(x) == float else 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:31.943524Z","iopub.execute_input":"2021-11-21T15:11:31.94376Z","iopub.status.idle":"2021-11-21T15:11:31.960142Z","shell.execute_reply.started":"2021-11-21T15:11:31.943731Z","shell.execute_reply":"2021-11-21T15:11:31.959214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create new feature FamilySize as a combination of SibSp and Parch\n# SibSp : 형제자매, Parch : 부모자식\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\n# Create a new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n# Remove all NULLS in the Embarked column\n# embarked : 출발지\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    \n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n# Fare : 요금\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n\n# Create a New feature CategoricalAge\n# 이후에 지울 feature임\nfor dataset in full_data:\n    for dataset in full_data:\n        age_avg = dataset['Age'].mean()\n        age_std = dataset['Age'].std()\n        age_null_count = dataset['Age'].isnull().sum()\n        age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n        dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n        dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:31.962921Z","iopub.execute_input":"2021-11-21T15:11:31.963957Z","iopub.status.idle":"2021-11-21T15:11:32.00957Z","shell.execute_reply.started":"2021-11-21T15:11:31.963876Z","shell.execute_reply":"2021-11-21T15:11:32.008467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search('  ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:32.010965Z","iopub.execute_input":"2021-11-21T15:11:32.011571Z","iopub.status.idle":"2021-11-21T15:11:32.016973Z","shell.execute_reply.started":"2021-11-21T15:11:32.011533Z","shell.execute_reply":"2021-11-21T15:11:32.016065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n    \n# Group all non-common titles into one single grouping 'Rare'\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:32.018786Z","iopub.execute_input":"2021-11-21T15:11:32.019081Z","iopub.status.idle":"2021-11-21T15:11:32.037613Z","shell.execute_reply.started":"2021-11-21T15:11:32.01905Z","shell.execute_reply":"2021-11-21T15:11:32.03696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map({'female' : 0, 'male':1}).astype(int)\n    \n    # Mapping titles\n    title_mapping = {'Mr' : 1, 'Miss' : 2, 'Mrs' : 3, 'Master' : 4, 'Rare' : 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map({'S' : 0, 'C' : 1, 'Q' : 2}).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n    dataset.loc[dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[dataset['Age'] > 64, 'Age'] = 4","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:32.038839Z","iopub.execute_input":"2021-11-21T15:11:32.040189Z","iopub.status.idle":"2021-11-21T15:11:32.072485Z","shell.execute_reply.started":"2021-11-21T15:11:32.040151Z","shell.execute_reply":"2021-11-21T15:11:32.071533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest = test.drop(drop_elements, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:32.073854Z","iopub.execute_input":"2021-11-21T15:11:32.074442Z","iopub.status.idle":"2021-11-21T15:11:32.084423Z","shell.execute_reply.started":"2021-11-21T15:11:32.074401Z","shell.execute_reply":"2021-11-21T15:11:32.083595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:32.085502Z","iopub.execute_input":"2021-11-21T15:11:32.085858Z","iopub.status.idle":"2021-11-21T15:11:32.106421Z","shell.execute_reply.started":"2021-11-21T15:11:32.085831Z","shell.execute_reply":"2021-11-21T15:11:32.105468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pearson Correlation Heatmap\ncorrelation(상관도) 보기 위해\n- +1과 -1 사이의 값을 가짐\n    - +1은 완벽한 양의 선형 상관 관계, 0은 선형 상관 관계 없음, -1은 완벽한 음의 선형 상관 관계","metadata":{}},{"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize = (14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size = 15)\nsns.heatmap(train.astype(float).corr(), linewidths = 0.1, vmax = 1.0,\n           square = True, cmap = colormap, linecolor = 'white', annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:32.107499Z","iopub.execute_input":"2021-11-21T15:11:32.107866Z","iopub.status.idle":"2021-11-21T15:11:32.816698Z","shell.execute_reply.started":"2021-11-21T15:11:32.107831Z","shell.execute_reply":"2021-11-21T15:11:32.815777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Pairplots","metadata":{}},{"cell_type":"markdown","source":"두 feature간의 분포를 보기 위함","metadata":{}},{"cell_type":"code","source":"g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked', u'FamilySize', u'Title']], \n                 hue='Survived', palette = 'seismic', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10))\ng.set(xticklabels=[])","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:32.818244Z","iopub.execute_input":"2021-11-21T15:11:32.818488Z","iopub.status.idle":"2021-11-21T15:11:39.613789Z","shell.execute_reply.started":"2021-11-21T15:11:32.818461Z","shell.execute_reply":"2021-11-21T15:11:39.611323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ensembling & Stacking models\n\n- Stacking \n    - base classifier들의 output(prediction)을 second-level-model(meta 모델)의 input으로 사용\n- 전체 데이터로 base classifier를 train하고 해당 output을 메타모델에 사용하면 과적합(overfitting)될 수 있음\n    - data를 train, test로 분리한 뒤, train data만 cross validation으로 훈련하고 test data로 평가","metadata":{}},{"cell_type":"markdown","source":"### Helpers via Python Classes","metadata":{}},{"cell_type":"markdown","source":"- 5개의 다른 classifier에 중복 코드 작성 → 좀 더 효율적으로 class로\n    - 여기에서는 크게 차이 없음, 모델이 100개 이상정도일 때 유용할 듯\n- Sklearn 라이브러리에서 호출 가능한 모델 5개 (rf, et, ada, gb, svc)\n- sklearn classifier 내에 이미 존재하는 해당 메소드를 호출하는 클래스의 메소드","metadata":{}},{"cell_type":"code","source":"# Some useful parameters which will come in handy later on\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits=NFOLDS, random_state=SEED) # 변경 : n_folds -> n_splits, dataset_length(=ntrain) 입력 X https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.615262Z","iopub.status.idle":"2021-11-21T15:11:39.615603Z","shell.execute_reply.started":"2021-11-21T15:11:39.615426Z","shell.execute_reply":"2021-11-21T15:11:39.615444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed = 0, params = None):\n        params['random_state'] = seed\n        self.clf = clf(**params) # unpacking을 두번 일어남. hyperparameter를 dictionary\n        \n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n        \n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self, x, y):\n        return self.clf.fit(x, y)\n    \n    def feature_importances(self, x, y):\n        print(self.clf.fit(x, y).feature_importances_)\n        \n# Class to extend XGboost classifier","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.616514Z","iopub.status.idle":"2021-11-21T15:11:39.61681Z","shell.execute_reply.started":"2021-11-21T15:11:39.616653Z","shell.execute_reply":"2021-11-21T15:11:39.616669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Out-of-Fold Predictions\n\n- K-Fold Cross Validation \n    - train data를 K개(여기에서는 5개)로 쪼개서 사용\n\n- Stacking 위해 모델마다 결과물 \n    - 결과 내는 방식 : Cross Validation을 활용한 OOF 방식 사용\n        - CV : K-fold, fold = 5\n\n- get_oof\n    - train : fold별로 결과 내서 concat\n    - train : 모델의 결과를 평균","metadata":{}},{"cell_type":"code","source":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    \n    # kf -> kf.split(dataset)으로 문법 변경\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n        \n        clf.train(x_tr, y_tr)\n        \n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n        \n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.617483Z","iopub.status.idle":"2021-11-21T15:11:39.617774Z","shell.execute_reply.started":"2021-11-21T15:11:39.617618Z","shell.execute_reply":"2021-11-21T15:11:39.617635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generating our Base First-Level Models\nsklearn에서 해당 classifier class를 제공함\n\n1. Random Forest classifier\n2. Extra Trees classifier\n3. AdaBoost classifier\n4. Gradient Boosting classifier\n5. Support Vector Machine\n\n1~4 : tree-based model","metadata":{}},{"cell_type":"markdown","source":"### Base models\n\n**Tree-based model**\n- bagging과 boosting 대부분 tree-based model\n- train하기 쉽고, 빠름\n- **Random Forest**\n    - bagging의 대표적인 알고리즘\n        - bagging : 같은 classifier 사용, 데이터 샘플링을 서로 다르게 가져가면서 train해 voting을 수행\n    - Decision Tree 여러개의 결과를 평균\n    - 여러개의 DT를 전체 데이터에서 bagging 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤 최종적으로 모든 classifier가 voting을 통해 결과\n- **Extra Tree**\n    - Random Forest와 비슷\n    - split을 할 때 무작위로 feature 선정\n    - 빠른 속도, bias/variance 낮음\n\n\n**Boosting**\n- 여러개의 tree-based classifier가 순차적으로 train - predict, 잘못 예측한 데이터에 weight을 부여하면서 오류를 개선하는 방식\n- **AdaBoost** \n    - classifier가 순차적으로 오류 값에 대해 가중치를 부여한 예측 결정 기준을 모두 결합해 예측 수행\n- **Gradient Boosting**\n    - AdaBoost와 유사, 가중치 업데이트를 경사 하강법(gradient descent)를 이용\n\n**Support Vector Machine** \n- decision boundary를 데이터와 가장 멀리 떨어지게 결정하는 방법","metadata":{}},{"cell_type":"markdown","source":"### tree-based model parameters\n\n- `n_jobs` : training에서 사용된 CPU 코어 수 (-1 : 전체 사용)\n- `n_estimators` : classification tree의 개수 (default : 10)\n    - 여기에 사용된 모델들은 tree-based 모델 (boosting model 포함)\n- `max_depth` : tree의 최대 깊이 (overfitting 고려해서 적절하게)\n- `verbose` : training 과정 출력할지 여부 \n    - 0 : 텍스트 숨김, 3 : 모든 반복에서 프로세스 출력","metadata":{}},{"cell_type":"code","source":"# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n    'warm_start': True,\n    # 'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features': 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n    # 'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate': 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n    # 'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters\nsvc_params = {\n    'kernel' : 'linear',\n    'C': 0.025\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.618971Z","iopub.status.idle":"2021-11-21T15:11:39.619306Z","shell.execute_reply.started":"2021-11-21T15:11:39.619114Z","shell.execute_reply":"2021-11-21T15:11:39.619159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"각 classifier를 공통된 형식으로 사용할 수 있게 class로 만들어 놓음 (class 굳이 필요 없음)","metadata":{}},{"cell_type":"code","source":"# Create 5 objects that represent our 4 models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.620199Z","iopub.status.idle":"2021-11-21T15:11:39.620521Z","shell.execute_reply.started":"2021-11-21T15:11:39.620341Z","shell.execute_reply":"2021-11-21T15:11:39.620364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating NumPy arrays out of our train and test sets","metadata":{}},{"cell_type":"markdown","source":"Dataframe 형태의 data를 numpy 형태로 추출","metadata":{}},{"cell_type":"code","source":"# Create Numpy arrays of train, test and target(Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an array of the train data (column명을 제외한 값들 뽑아냄)\nx_test = test.values # Creates an array of the test data","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.622037Z","iopub.status.idle":"2021-11-21T15:11:39.622391Z","shell.execute_reply.started":"2021-11-21T15:11:39.622225Z","shell.execute_reply":"2021-11-21T15:11:39.622241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"out-of-fold로 5개의 base classifier로 예측 (output 생성)","metadata":{}},{"cell_type":"code","source":"# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees \nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.6234Z","iopub.status.idle":"2021-11-21T15:11:39.623697Z","shell.execute_reply.started":"2021-11-21T15:11:39.623537Z","shell.execute_reply":"2021-11-21T15:11:39.623553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature importances generated from the different classifiers","metadata":{}},{"cell_type":"markdown","source":"tree-based model은 feature importance기준으로 tree를 나누므로 feature_importance 추출 가능","metadata":{}},{"cell_type":"code","source":"# tree based model에서는 feature importance를 뽑을 수 있음\nrf_feature = rf.feature_importances(x_train, y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.624495Z","iopub.status.idle":"2021-11-21T15:11:39.624788Z","shell.execute_reply.started":"2021-11-21T15:11:39.624632Z","shell.execute_reply":"2021-11-21T15:11:39.624648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_features = [0.10474135,  0.21837029,  0.04432652,  0.02249159,  0.05432591, 0.02854371,\n               0.07570305,  0.01088129 , 0.24247496,  0.13685733 , 0.06128402]\net_features = [0.12165657,  0.37098307  ,0.03129623 , 0.01591611 , 0.05525811, 0.028157,\n               0.04589793 , 0.02030357 , 0.17289562 , 0.04853517,  0.08910063]\nada_features = [0.028, 0.008, 0.012, 0.05866667, 0.032, 0.008, 0.04666667, 0. , 0.05733333,\n                0.73866667, 0.01066667]\ngb_features = [0.06796144 , 0.03889349 , 0.07237845 , 0.02628645 , 0.11194395, 0.04778854,\n               0.05965792, 0.02774745, 0.07462718, 0.4593142, 0.01340093]","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.625645Z","iopub.status.idle":"2021-11-21T15:11:39.625999Z","shell.execute_reply.started":"2021-11-21T15:11:39.625806Z","shell.execute_reply":"2021-11-21T15:11:39.625823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 시각화 편하게 하기 위해 dataframe 생성\ncols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame({'features': cols, \n                                  'Random Forest feature importances': rf_features,\n                                  'Extra Trees feature importances': et_features,\n                                  'AdaBoost feature importances': ada_features,\n                                  'Gradient Boost feature importances': gb_features})","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.627113Z","iopub.status.idle":"2021-11-21T15:11:39.627424Z","shell.execute_reply.started":"2021-11-21T15:11:39.627265Z","shell.execute_reply":"2021-11-21T15:11:39.627281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Interactive feature importances via Plotly scatterplots","metadata":{}},{"cell_type":"code","source":"# Scatter plot\n# data\ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode = 'markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True),\n    text = feature_dataframe['features'].values)\ndata = [trace]\n\n# 제목, 축 등의 layout\nlayout = go.Layout(\n    autosize = True,\n    title = 'Random Forest Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='scatter2010')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.62846Z","iopub.status.idle":"2021-11-21T15:11:39.628758Z","shell.execute_reply.started":"2021-11-21T15:11:39.628598Z","shell.execute_reply":"2021-11-21T15:11:39.628614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter plot\ntrace = go.Scatter(\n    y = feature_dataframe['Extra Trees feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Extra Trees feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Extra Trees Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename = 'scatter2010')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.629777Z","iopub.status.idle":"2021-11-21T15:11:39.630107Z","shell.execute_reply.started":"2021-11-21T15:11:39.629936Z","shell.execute_reply":"2021-11-21T15:11:39.629957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter plot\ntrace = go.Scatter(\n    y = feature_dataframe['AdaBoost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['AdaBoost feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'AdaBoost Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\n\nfig = go.Figure(data=data, layout = layout)\npy.iplot(fig, filename = 'scatter2010')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.631013Z","iopub.status.idle":"2021-11-21T15:11:39.631321Z","shell.execute_reply.started":"2021-11-21T15:11:39.631164Z","shell.execute_reply":"2021-11-21T15:11:39.63118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter plot\ntrace = go.Scatter(\n    y = feature_dataframe['Gradient Boost feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode = 'markers',\n    marker = dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Gradient Boost feature importances'].values,\n        colorscale = 'Portland',\n        showscale = True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Gradient Boosting Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend =  False\n)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'scatter2010')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.632309Z","iopub.status.idle":"2021-11-21T15:11:39.632607Z","shell.execute_reply.started":"2021-11-21T15:11:39.632446Z","shell.execute_reply":"2021-11-21T15:11:39.632461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"모델에 따라 서로 다른 feature importance\n- Random Forest, Extra Tree : Sex, FamilySize\n- AdaBoost, Gradient Descent : IsAlone","metadata":{}},{"cell_type":"markdown","source":"Ensemble \n- 서로 다른 feature를 잘 보는 (상관관계가 낮은) 모델끼리 성능이 좋아질 것으로 예상","metadata":{}},{"cell_type":"code","source":"# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis = 1) # axis = 1 : computes the mean row-wise\nfeature_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.790768Z","iopub.status.idle":"2021-11-21T15:11:39.791238Z","shell.execute_reply.started":"2021-11-21T15:11:39.79101Z","shell.execute_reply":"2021-11-21T15:11:39.791035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotly Barplot of Average Feature Importances","metadata":{}},{"cell_type":"markdown","source":"원래는 비교를 위해서는 barplot으로 시각화해야함","metadata":{}},{"cell_type":"code","source":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n    x = x,\n    y = y,\n    width = 0.5,\n    marker = dict(\n        color = feature_dataframe['mean'].values,\n        colorscale = 'Portland',\n        showscale = True,\n        reversescale = False\n    ),\n    opacity = 0.6\n)]\n\nlayout = go.Layout(\n    autosize = True,\n    title = 'Barplot of Mean Feature Importance',\n    hovermode = 'closest',\n    yaxis = dict(\n        title = 'Feature Importance',\n        ticklen = 5,\n        gridwidth = 2\n    ),\n    showlegend = False\n)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'bar-direct-labels')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.792604Z","iopub.status.idle":"2021-11-21T15:11:39.793132Z","shell.execute_reply.started":"2021-11-21T15:11:39.792771Z","shell.execute_reply":"2021-11-21T15:11:39.792796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Second-Level Predictions from the First-level Output","metadata":{}},{"cell_type":"markdown","source":"### First-level output as new features\n- ravel() : 다차원 배열(array)을 1차원 배열로 평평하게 펴주는 NumPy 함수\n- https://rfriend.tistory.com/349","metadata":{}},{"cell_type":"code","source":"base_predictions_train = pd.DataFrame({\n    'RandomForest': rf_oof_train.ravel(),\n    'ExtraTrees': et_oof_train.ravel(),\n    'AdaBoost': ada_oof_train.ravel(),\n    'GradientBoost': gb_oof_train.ravel()\n})\nbase_predictions_train.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.794272Z","iopub.status.idle":"2021-11-21T15:11:39.794641Z","shell.execute_reply.started":"2021-11-21T15:11:39.794438Z","shell.execute_reply":"2021-11-21T15:11:39.794462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Heatmap of the Second Level Training set\n- 각 base model output의 상관관계를 시각화를 통해 확인\n    - Second Level 모델에서의 input","metadata":{}},{"cell_type":"code","source":"data = [\n    go.Heatmap(\n        z = base_predictions_train.astype(float).corr().values,\n        x = base_predictions_train.columns.values,\n        y = base_predictions_train.columns.values,\n            colorscale = 'Viridis',\n            showscale = True,\n            reversescale = True\n    )\n]\n\npy.iplot(data, filename = 'labelled-heatmap')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.796004Z","iopub.status.idle":"2021-11-21T15:11:39.796349Z","shell.execute_reply.started":"2021-11-21T15:11:39.796167Z","shell.execute_reply":"2021-11-21T15:11:39.796191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = np.concatenate((et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis = 1)\nx_test = np.concatenate((et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.797761Z","iopub.status.idle":"2021-11-21T15:11:39.798131Z","shell.execute_reply.started":"2021-11-21T15:11:39.797941Z","shell.execute_reply":"2021-11-21T15:11:39.797962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Second level learning model via XGBoost\n- XGBoost : https://xgboost.readthedocs.io/en/latest/","metadata":{}},{"cell_type":"markdown","source":"#### XGBoost \n- tree-based model 중 가장 높은 성능\n- GBM 기반, 단점인 수행 시간 및 regularization 문제 해결, 병렬 CPU 환경에서 병렬 학습 빠름\n- https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn","metadata":{}},{"cell_type":"code","source":"gbm = xgb.XGBClassifier(\n    n_estimators = 2000,\n    max_depth = 4,\n    min_child_weight = 2,\n    gamma = 0.9,\n    subsample = 0.8,\n    colsample_bytree = 0.8,\n    objective = 'binary:logistic', \n    nthread = -1,\n    scale_pos_weight=1\n).fit(x_train, y_train)\npredictions = gbm.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.799578Z","iopub.status.idle":"2021-11-21T15:11:39.799964Z","shell.execute_reply.started":"2021-11-21T15:11:39.799748Z","shell.execute_reply":"2021-11-21T15:11:39.799772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate Submission File\nStackingSubmission = pd.DataFrame({'PassengerId': PassengerId,\n                                  'Survived': predictions})\nStackingSubmission.to_csv('StackingSubmission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-21T15:11:39.801509Z","iopub.status.idle":"2021-11-21T15:11:39.801848Z","shell.execute_reply.started":"2021-11-21T15:11:39.801668Z","shell.execute_reply":"2021-11-21T15:11:39.801693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n  ","metadata":{}}]}