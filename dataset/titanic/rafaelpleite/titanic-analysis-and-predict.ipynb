{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Titanic - Analysis and Predict \nThis is my first Notebook in Kaggle, then if you find any error tell me, please."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Manipulation of vectors library\nimport numpy as np\n# Manipulation of data library\nimport pandas as pd\n#Ploting graphs library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Manipulation of the system library\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see the files on kaggle/input/titanic folder."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"! cd /kaggle/input/titanic && ls","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, define the train and test variables from system folder."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/titanic/train.csv')\ndf_test = pd.read_csv('/kaggle/input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Some data of train DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Describing Data\n\nLet's begin with some statistics."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe().transpose()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Realize that there's some missing values on the data, we have to handle with them. Also, 38.38% of passengers survived, so let's compare the survival rate with the others columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby('Pclass').mean()['Survived']*100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Notice that, the survival rate grow with de class of the passager. "},{"metadata":{},"cell_type":"markdown","source":"Moreover, the data have some missing data values e we'll handle with them."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation is"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But, this is hard to vizualize, then, let's see this using an heatmap plot from seaborn."},{"metadata":{},"cell_type":"markdown","source":"## Ploting Data"},{"metadata":{},"cell_type":"markdown","source":"Heatmap of correlation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,9))\nsns.heatmap(df_train.corr(), annot=True, cmap='viridis')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this correlation map maybe:\n\n\n* Fare is inverse of Pclass, then passagers of low class paid less for the trip;\n* SibSp is proportional to Parch, this is expected;\n* Age is inversely proportional to class, maybe the upper class is more elderly than the lower, we'll se it below;\n* Pclass is inverse of Survived, this is alredy expected once the class 1 have 62.96% rate of survived;\n* Fare is proportional to Survived, maybe who paid more for the trip belong to class 1, thus have more chances to survive;"},{"metadata":{},"cell_type":"markdown","source":"  Let's response these questions."},{"metadata":{},"cell_type":"markdown","source":"* Fare is inverse of Pclass, then passagers of low class paid less for the trip;\n\n\nYES, as you can see, classes 2 and 3 paid less for the trip. Moreover, for the class 1 there's some peoples that paid over than 200."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.groupby('Pclass').mean()['Fare'])\nplt.figure(figsize=(10,8))\nsns.boxplot(x='Pclass', y='Fare', data=df_train)\nplt.title('Pclass x Fare')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* SibSp is proportional to Parch, this is expected;\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,9))\nsns.barplot(x='SibSp', y='Parch', data=df_train)\nplt.title('SubSp x Parch')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Age is inversely proportional to class, maybe the upper class is more elderly than the lower, we'll se it below;\n\n\nAffirmative. As you can see, the age is inversely proportinal to class."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.groupby('Pclass').mean()['Age'])\nplt.figure(figsize=(16,9))\nsns.barplot(x='Pclass', y='Age', data=df_train)\nplt.title('Pclass x Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pclass is inverse of Survived, this is alredy expected once the class 1 have 62.96% rate of survived;\n\nAs alredy expected."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.groupby('Pclass').mean()['Survived'])\nplt.figure(figsize=(16,9))\nsns.barplot(x='Pclass', y='Survived', data=df_train)\nplt.title('Pclass x Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Fare is proportional to Survived, maybe who paid more for the trip belong to class 1, thus have more chances to suvive;\n\nAgain, yes. But, realize that the columns, Fare and Pclass have strong relationship with Survived. Thus, Who belongs to the upper class or paid more for the trip probably survived."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.groupby(['Pclass', 'Survived']).mean()['Fare'])\nsns.FacetGrid(df_train, col='Pclass', height=5, aspect=4/3).map(sns.barplot, 'Survived', 'Fare', order=[0, 1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"They are very correlated, almost linear, that means that the feature importance is directly related to the outcome. Certainly Pclass and Fare have lot importance for predict who survived. Therefore, our model need to learn this."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16*(2/3),9*(2/3)))\nsns.heatmap(df_train[['Fare', 'Survived', 'Pclass']].corr(), cmap='viridis', annot=True)\nplt.title('Correlation: Fare x Survived x Pclass')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, most of survived is woman (74.20%)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.groupby('Sex').mean()[['Survived']]*100)\nplt.figure(figsize=(16,9))\nsns.countplot(x='Sex', hue='Survived', data=df_train, palette='viridis')\nplt.title('Count of Survive by Sex')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Treating Missing Values\n\nWe have to handle with these missing values, let's see this again."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.isna().sum(),'\\n\\n',df_test.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Begin treating the column Embarked."},{"metadata":{},"cell_type":"markdown","source":"Let's try to fit the NaN into a Embarked."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[df_train['Embarked'].isna() == True]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using a barplot to vizualize some columns. We can't label the missing data on 'S', 'C' or 'Q'. Let's try other way to do this."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,9))\nsns.FacetGrid(df_train, col='Embarked', row='Sex', height=4, aspect=.8).map(sns.barplot, 'Pclass', 'Fare', order=[1, 2, 3])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see below, most of people who embark in the \"Q\" belong to class 3 and have paid 13.28 $\\pm$ 14.18, may the missing data not belong to \"Q\". But is not possible to go so far, then the missing data values will fit in Embarked \"S\", because this is more commom than Embarked \"C\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.groupby('Embarked').agg(['mean', 'std']).drop('PassengerId', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For train data\ndf_train['Embarked'] = df_train['Embarked'].fillna(value='S')\n\n#For test data\ndf_test['Embarked'] = df_test['Embarked'].fillna(value='S')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, apply get_dummies to separete the Embarked colum."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.get_dummies(df_train, columns=['Embarked'], drop_first=True)\ndf_test = pd.get_dummies(df_test, columns=['Embarked'], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, let's handle with Cabin column. Couting the NaN data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Cabin.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There's some repeated values."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Cabin.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As you can see 77.10% of da Cabin column is missing data and apparently there's no relationship between this column and the Survived, so let's just drop it."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.Cabin.isna().sum() / len(df_train.Cabin) * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we drop this for train and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"#For train and test data\ndf_train = df_train.drop('Cabin', axis=1)\ndf_test = df_test.drop('Cabin', axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, we treating the colum Sex using get_dummies."},{"metadata":{"trusted":true},"cell_type":"code","source":"#for train data\ndf_train = pd.get_dummies(df_train, columns=['Sex'], drop_first=True)\n\n#for test data\ndf_test = pd.get_dummies(df_test, columns=['Sex'], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At leats, let's handle with Age missing data.\n\nHighest class is more elders."},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,9))\nsns.boxplot(x='Pclass', y='Age', data=df_train)\nplt.title('Pclass x Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To solve missing data problem, let's build up a regression model using Keras from Tensorflow."},{"metadata":{},"cell_type":"markdown","source":"But before drop columns of string type."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = df_train.drop(['PassengerId', 'Name', 'Ticket'], axis=1)\ndf_test = df_test.drop(['PassengerId', 'Name', 'Ticket'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use Normalizer to normalize data, this bring all variable to same range [0,1]."},{"metadata":{"trusted":true},"cell_type":"code","source":"norm = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We use the train DataFrame with not missing data to do regression, then we apply it on de missing values of train and test DataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_notna = df_train[df_train['Age'].notna() == True].drop(['Survived'], axis=1)\n\ndf_X = df_train_notna.drop(['Age'], axis=1)\ndf_y = df_train_notna['Age']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the data into train data and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=72)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Apply normalization into X_train and X_test."},{"metadata":{"trusted":true},"cell_type":"code","source":"norm.fit(X_train)\nX_train = norm.transform(X_train)\nX_test = norm.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import TensorFlow, for training a deep neural networks to predict the age."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, activation='relu', input_shape=[7]),\n    #This sequential begin with 10 neurons, the activation is ReLu and the input shape is the number of columns of our training dataset.\n    \n    tf.keras.layers.BatchNormalization(),\n    #Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n    \n    tf.keras.layers.Dense(15, activation='relu'),\n    #Add 15 neurons using ReLu as activation function\n    \n    tf.keras.layers.Dense(1)\n    #The output must be 1 label for a regression model\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A summary of our model."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* SGD, stochastic gradient descent, optimizer is a method used to minimize the cost fuction;\n* Mean squared logarithmic error, MSLE is a function that measure the ration between the true and predict values;\n* The metrics accuracy and MSE(Mean Squared Error) measure the behavior of test and validation data along the fitting;\n* EarlyStopping is a callback used to avoid overfitting of the data;"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='SGD',loss='mean_squared_logarithmic_error',metrics=['accuracy', 'MSE'])\nhistory = model.fit(X_train, y_train, epochs=1000, validation_data=(X_train, y_train), callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['loss'], 'r', label='loss')\nplt.plot(history.history['val_loss'], 'b', label='val_loss')\nplt.title('Loss')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['accuracy'], 'r', label='accuracy')\nplt.plot(history.history['val_accuracy'], 'b', label='val_accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['MSE'], 'r', label='MSE')\nplt.plot(history.history['val_MSE'], 'b', label='val_MSE')\nplt.title('MSE')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is a little bad, but is the best that we have. There's three ways to continue, use this regreesion for age on the missing data, use mean age to fill them or delete them. At the end of the notebook we'll try these three ways to handle with this, but now with notebook using regression."},{"metadata":{"trusted":true},"cell_type":"code","source":"classifications = model.predict(X_test)\nprint(f'Correlation Matrix:\\n{np.corrcoef(classifications[:,0], y_test)}')\nplt.figure(figsize=(16,9))\nsns.scatterplot(x = classifications[:,0], y = y_test)\nsns.lineplot(x = y_test, y = y_test, color='b')\nplt.xlim(.94*min(classifications[:,0]), 1.02*max(classifications[:,0]))\nplt.xlabel('Predicted Age')\nplt.ylabel('Real Age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then, we apply our model into our train and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Age'] = df_train['Age'].replace({np.nan: np.array(model.predict(norm.fit_transform(df_train.drop(['Age', 'Survived'], axis=1))))})\ndf_test['Age'] = df_test['Age'].replace({np.nan: np.array(model.predict(norm.fit_transform(df_test.drop(['Age'], axis=1))))})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_train.isna().sum(),'\\n\\n',df_test.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's just use mean values on the Fare missing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test['Fare'] = df_test['Fare'].fillna((df_test.Fare.mean()))\ndf_train['Fare'] = df_train['Fare'].fillna((df_train.Fare.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling Deep Neural Network for Predict Survival"},{"metadata":{},"cell_type":"markdown","source":"We begin sorting out train and test data."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_train.drop('Survived',axis=1)\ny_train = df_train['Survived']\nX_test = df_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Normalizing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"norm = MinMaxScaler()\nnorm.fit(X_train)\nX_train = norm.transform(X_train)\nX_test = norm.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's build model using keras."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(10, activation='relu', input_shape=[8]),\n    #This sequential begin with 10 neurons, the activation is ReLu and the input shape is the number of columns of our training dataset.\n    \n    tf.keras.layers.BatchNormalization(),\n    #Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n    \n    tf.keras.layers.Dense(14, activation='relu'),\n    #Add 14 neurons using ReLu as activation function\n    \n    tf.keras.layers.Dense(1, activation='sigmoid')\n    #The output must be 1 label and de activation is a sigmoid function for binary classification\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Binary crossentropy is a loss function/\n* SGD, stochastic gradient descent, optimizer is a method used to minimize the cost fuction;\n* The metrics accuracy and MSE(Mean Squared Error) measure the behavior of test and validation data along the fitting;\n* EarlyStopping is a callback used to avoid overfitting of the data monitoring the value of loss;"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),metrics=['accuracy', 'MSE'])\nhistory = model.fit(X_train, y_train, epochs=500, validation_split=.2, callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['loss'], 'r', label='loss')\nplt.plot(history.history['val_loss'], 'b', label='val_loss')\nplt.title('Loss')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['accuracy'], 'r', label='accuracy')\nplt.plot(history.history['val_accuracy'], 'b', label='val_accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(16*.7,9*.7))\nplt.plot(history.history['MSE'], 'r', label='MSE')\nplt.plot(history.history['val_MSE'], 'b', label='val_MSE')\nplt.title('MSE')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"DNN is not the best way to handle with this type of problem. Also, there is not much data to build up a good model. However, let's build up the same model above using all training data, i.e, validation_split=0."},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9),metrics=['accuracy', 'MSE'])\nhistory = model.fit(X_train, y_train, epochs=500, validation_split=0, callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss'))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction & Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifications = model.predict(X_train)\nfor i, ival in enumerate(classifications):\n    if ival >= 0.5: classifications[i] = 1\n    else: classifications[i] = 0\nprint(f'Correlation Matrix:\\n{np.corrcoef(classifications[:,0], y_train)}\\n\\n')\nprint(f'Confusion Matrix:\\n{confusion_matrix(y_train, classifications)}\\n\\n')\nprint(f'Accuracy Matrix:\\n{accuracy_score(y_train, classifications)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test_X = norm.fit_transform(df_test)\ndf_test_y = model.predict(df_test_X)\nfor i, ival in enumerate(df_test_y):\n    if ival >= 0.5: df_test_y[i] = 1\n    else: df_test_y[i] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\", index_col='PassengerId')\nsubmission['Survived'] = df_test_y.astype(int)\nsubmission.to_csv('TitanicKNN.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baselines"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Machine Learning Algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"MLA = [\n    #SVM\n    ('SVC', SVC()),\n    ('LinearSVC', LinearSVC()),\n    \n    #Tree\n    ('DecisionTreeClassifier', DecisionTreeClassifier()),\n    ('ExtraTreeClassifier', ExtraTreeClassifier()),\n    \n    #Linear Model\n    ('LogisticRegression', LogisticRegressionCV()),\n    \n    #Neighbors\n    ('KNeighborsClassifier', KNeighborsClassifier()),\n    \n    #Gaussian Process\n    ('GaussianProcessClassifier', GaussianProcessClassifier()),\n    \n    #Ensemble\n    ('AdaBoostClassifier', AdaBoostClassifier()),\n    \n    #Naive Bayes\n    ('GaussianNB', GaussianNB())\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(df_train.drop('Survived', axis=1))\ny = np.array(df_train['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shape = (10,10)\nresults = np.zeros((len(MLA), shape[0], shape[1]))\nfor count, algorithm in enumerate(MLA):\n    cv = RepeatedKFold(n_splits=10, n_repeats=10)\n    model = Pipeline([('scaler', MinMaxScaler()), algorithm])\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=1)\n    results[count] = scores.reshape(shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot below represent a boxplot of each ML Algorithm with 10 repeats using KFold. The horizontal lines represents the mean of scores and the (mean +/- std) of scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"for index, result in enumerate(results):\n    plt.title(MLA[index][0])\n    plt.boxplot(result, showmeans=True)\n    plt.axhline(np.mean(result), c='r')\n    plt.axhline(np.mean(result) + np.std(result), c='r', ls='--')\n    plt.axhline(np.mean(result) - np.std(result), c='r', ls='--')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking at best result of Machine Learning Algorithms"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_mean = [np.mean(x) for x in results]\nindex_best_mean = best_mean.index(max(best_mean))\nprint(f'mean of best result ({MLA[index_best_mean][0]})')\nfor i in results[index_best_mean]:\n    print(f'{np.round(np.mean(i), decimals=3)} +/- {np.round(np.std(i), decimals=3)}')\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}