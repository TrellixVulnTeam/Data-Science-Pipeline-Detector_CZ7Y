{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b29fdb6d-5f1c-0b57-640a-311fe21a72b8"},"source":"#**Titanic: Machine Learning from Disaster**\n\nThe notebook is about predicting the Titanic Survivors with basic Machine Learning process in Python. It's also my first Kaggle Challenge. I am still working on improving the accuracy of my model. Please feel free to give me some feedback.\n\n# **content**\n**1. Introduction**\n   \n1.1 Understanding the Problem  \n1.2 Goal and Metric  \n1.3 Exploring the Data \n\n**2. Data Cleaning**\n\n**3. Model and Prediction**"},{"cell_type":"markdown","metadata":{"_cell_guid":"75dfde7a-a980-70ce-3ee2-246db662c61b"},"source":"# 1. Introduction\n\n## 1.1 Problem Understanding  \nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nthe challenge is to complete the analysis of what sorts of people were likely to survive and to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n## 1.2 Goal and Metric\n\n - Goal: To predict if a passenger survived the sinking of the Titanic or not.   \n - Metric: Accuracy, The percentage of passengers the model correctly predict. \n\n## 1.3 Exploring the Data "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"58e3e546-88b8-5e74-5051-c2a6d2ef6996"},"outputs":[],"source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\ntrain.info()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"508aaa37-1a92-7bde-6834-4b5fd0b03ffc"},"outputs":[],"source":"train.describe()"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8311a09e-c00d-6c3f-5149-61a74a6b8a74"},"outputs":[],"source":"# show the overall survival rate (38.38), as the standard when choosing the fts\nprint('Overall Survival Rate:',train['Survived'].mean())"},{"cell_type":"markdown","metadata":{"_cell_guid":"95804645-44c1-6334-866a-0c0da4f4b7f5"},"source":"# 2. Data Cleaning and Features Choosing"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f46c8089-3caf-250a-4eee-a0908f57514d"},"outputs":[],"source":"# get_dummies function\ndef dummies(col,train,test):\n    train_dum = pd.get_dummies(train[col])\n    test_dum = pd.get_dummies(test[col])\n    train = pd.concat([train, train_dum], axis=1)\n    test = pd.concat([test,test_dum],axis=1)\n    train.drop(col,axis=1,inplace=True)\n    test.drop(col,axis=1,inplace=True)\n    return train, test\n\n# get rid of the useless cols\ndropping = ['PassengerId', 'Name', 'Ticket']\ntrain.drop(dropping,axis=1, inplace=True)\ntest.drop(dropping,axis=1, inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b5b2257e-0995-d95b-d514-8905c4da7f4b"},"outputs":[],"source":"#pclass\n# ensure no na contained\nprint(train.Pclass.value_counts(dropna=False))\nsns.factorplot('Pclass', 'Survived',data=train, order=[1,2,3])\n# according to the graph, we found there are huge differences between\n# each pclass group. keep the ft\ntrain, test = dummies('Pclass', train, test)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5634cdbb-ddb8-bb7f-f4bb-b6e4867f35bf"},"outputs":[],"source":"# sex\nprint(train.Sex.value_counts(dropna=False))\nsns.factorplot('Sex','Survived', data=train)\n# female survival rate is way better than the male\ntrain, test = dummies('Sex', train, test)\n# cos the male survival rate is so low, delete the male col\ntrain.drop('male',axis=1,inplace=True)\ntest.drop('male',axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"49527b86-4dd8-36c2-e4ed-47f2e432bae8"},"outputs":[],"source":"#age \n#dealing the missing data\nnan_num = train['Age'].isnull().sum()\n# there are 177 missing value, fill with random int\nage_mean = train['Age'].mean()\nage_std = train['Age'].std()\nfilling = np.random.randint(age_mean-age_std, age_mean+age_std, size=nan_num)\ntrain['Age'][train['Age'].isnull()==True] = filling\nnan_num = train['Age'].isnull().sum()\n\n# dealing the missing val in test\nnan_num = test['Age'].isnull().sum()\n# 86 null\nage_mean = test['Age'].mean()\nage_std = test['Age'].std()\nfilling = np.random.randint(age_mean-age_std,age_mean+age_std,size=nan_num)\ntest['Age'][test['Age'].isnull()==True]=filling\nnan_num = test['Age'].isnull().sum()\n\n#look into the age col\ns = sns.FacetGrid(train,hue='Survived',aspect=3)\ns.map(sns.kdeplot,'Age',shade=True)\ns.set(xlim=(0,train['Age'].max()))\ns.add_legend()\n\n# from the graph, we see that the survival rate of children\n# is higher than other and the 15-30 survival rate is lower\ndef under15(row):\n    result = 0.0\n    if row<15:\n        result = 1.0\n    return result\ndef young(row):\n    result = 0.0\n    if row>=15 and row<30:\n        result = 1.0\n    return result\n\ntrain['under15'] = train['Age'].apply(under15)\ntest['under15'] = test['Age'].apply(under15)\ntrain['young'] = train['Age'].apply(young)\ntest['young'] = test['Age'].apply(young)\n\ntrain.drop('Age',axis=1,inplace=True)\ntest.drop('Age',axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"541ebae8-13b1-5c2f-365f-2ec3ec827ad7"},"outputs":[],"source":"#family\n# chek\nprint(train['SibSp'].value_counts(dropna=False))\nprint(train['Parch'].value_counts(dropna=False))\n\nsns.factorplot('SibSp','Survived',data=train,size=5)\nsns.factorplot('Parch','Survived',data=train,size=5)\n\n'''through the plot, we suggest that with more family member, \nthe survival rate will drop, we can create the new col\nadd up the parch and sibsp to check our theory''' \ntrain['family'] = train['SibSp'] + train['Parch']\ntest['family'] = test['SibSp'] + test['Parch']\nsns.factorplot('family','Survived',data=train,size=5)\n\ntrain.drop(['SibSp','Parch'],axis=1,inplace=True)\ntest.drop(['SibSp','Parch'],axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ade4acd4-5e83-1894-c424-903afe44cdd4"},"outputs":[],"source":"# fare\n# checking null, found one in test group. leave it alone til we find out\n# wether we should use this ft\ntrain.Fare.isnull().sum()\ntest.Fare.isnull().sum()\n\nsns.factorplot('Survived','Fare',data=train,size=5)\n#according to the plot, smaller fare has higher survival rate, keep it\n#dealing the null val in test\ntest['Fare'].fillna(test['Fare'].median(),inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9643756d-9325-28bf-3264-0dd107eac145"},"outputs":[],"source":"#Cabin\n# checking missing val\n# 687 out of 891 are missing, drop this col\ntrain.Cabin.isnull().sum()\ntrain.drop('Cabin',axis=1,inplace=True)\ntest.drop('Cabin',axis=1,inplace=True)"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"567560ea-434c-9919-d66c-c1eb2c7f20ee"},"outputs":[],"source":"#Embark\ntrain.Embarked.isnull().sum()\n# 2 missing value\ntrain.Embarked.value_counts()\n# fill the majority val,'s', into missing val col\ntrain['Embarked'].fillna('S',inplace=True)\n\nsns.factorplot('Embarked','Survived',data=train,size=6)\n# c has higher survival rate, drop the other two\ntrain,test = dummies('Embarked',train,test)\ntrain.drop(['S','Q'],axis=1,inplace=True)\ntest.drop(['S','Q'],axis=1,inplace=True)"},{"cell_type":"markdown","metadata":{"_cell_guid":"6020aa37-3e1d-21d1-18c1-d3e8243e59da"},"source":"# 3. Model and Prediction "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9359358e-43e3-30c1-3eb6-5e2b70f59987"},"outputs":[],"source":"# import machine learning libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score, KFold\n\ndef modeling(clf,ft,target):\n    acc = cross_val_score(clf,ft,target,cv=kf)\n    acc_lst.append(acc.mean())\n    return \n\naccuracy = []\ndef ml(ft,target,time):\n    accuracy.append(acc_lst)\n\n    #logisticregression\n    logreg = LogisticRegression()\n    modeling(logreg,ft,target)\n    #RandomForest\n    rf = RandomForestClassifier(n_estimators=50,min_samples_split=4,min_samples_leaf=2)\n    modeling(rf,ft,target)\n    #svc\n    svc = SVC()\n    modeling(svc,ft,target)\n    #knn\n    knn = KNeighborsClassifier(n_neighbors = 3)\n    modeling(knn,ft,target)\n    \n    \n    # see the coefficient\n    logreg.fit(ft,target)\n    feature = pd.DataFrame(ft.columns)\n    feature.columns = ['Features']\n    feature[\"Coefficient Estimate\"] = pd.Series(logreg.coef_[0])\n    print(feature)\n    return "},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8ab38d92-85bc-72c4-6091-c365ecd01832"},"outputs":[],"source":"# testing no.1, using all the feature\ntrain_ft=train.drop('Survived',axis=1)\ntrain_y=train['Survived']\n#set kf\nkf = KFold(n_splits=3,random_state=1)\nacc_lst = []\nml(train_ft,train_y,'test_1')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"330ec3f8-463c-0d82-5aea-789ca4ba7782"},"outputs":[],"source":"# testing 2, lose young\ntrain_ft_2=train.drop(['Survived','young'],axis=1)\ntest_2 = test.drop('young',axis=1)\ntrain_ft.head()\n\n# ml\nkf = KFold(n_splits=3,random_state=1)\nacc_lst=[]\nml(train_ft_2,train_y,'test_2')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"24c370cb-e624-1c44-6dad-52749f4ae00f"},"outputs":[],"source":"#test3, lose young, c\ntrain_ft_3=train.drop(['Survived','young','C'],axis=1)\ntest_3 = test.drop(['young','C'],axis=1)\ntrain_ft.head()\n\n# ml\nkf = KFold(n_splits=3,random_state=1)\nacc_lst = []\nml(train_ft_3,train_y,'test_3')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2ea7e76f-9cf9-e4cc-e156-6058d534888b"},"outputs":[],"source":"# test4, no FARE\ntrain_ft_4=train.drop(['Survived','Fare'],axis=1)\ntest_4 = test.drop(['Fare'],axis=1)\ntrain_ft.head()\n# ml\nkf = KFold(n_splits=3,random_state=1)\nacc_lst = []\nml(train_ft_4,train_y,'test_4')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"45142dc9-6146-065c-00a0-9c05d88588ed"},"outputs":[],"source":"# test5, get rid of c \ntrain_ft_5=train.drop(['Survived','C'],axis=1)\ntest_5 = test.drop('C',axis=1)\n\n# ml\nkf = KFold(n_splits=3,random_state=1)\nacc_lst = []\nml(train_ft_5,train_y,'test_5')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"cc86c101-e612-5dcd-cef7-ae30024a1403"},"outputs":[],"source":"# test6, lose Fare and young\ntrain_ft_6=train.drop(['Survived','Fare','young'],axis=1)\ntest_6 = test.drop(['Fare','young'],axis=1)\ntrain_ft.head()\n# ml\nkf = KFold(n_splits=3,random_state=1)\nacc_lst = []\nml(train_ft_6,train_y,'test_6')"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88809257-8fe7-3e42-63e4-79adb3ec407f"},"outputs":[],"source":"accuracy_df=pd.DataFrame(data=accuracy,\n                         index=['test1','test2','test3','test4','test5','test6'],\n                         columns=['logistic','rf','svc','knn'])\naccuracy_df"},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5ecb8b71-d34a-d832-5ec8-0ed364e5e35a"},"outputs":[],"source":"'''\nAccording to the accuracy chart, 'features test4 with svc'\ngot best performance\n'''  \n#test4 svc as submission\nsvc = SVC()\nsvc.fit(train_ft_4,train_y)\nsvc_pred = svc.predict(test_4)\nprint(svc.score(train_ft_4,train_y))\n\n\ntest = pd.read_csv('../input/test.csv')\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": svc_pred\n    })\n#submission.to_csv(\"kaggle.csv\", index=False)"}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}