{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Ensembel from starter to Expert!\n\n## Overview\n![](https://miro.medium.com/max/1000/1*QDnOdWR7dL69rYVP3z0WaQ.png)\nImage from: miro.medium.com\n\n\n\n> In this notebook, I'll trying to build as many as Ensemble model for Kaggler\n<br>\n\n<code>Ensemble</code> is the method that ensembling same or different model to build powerful model<br>\nEspecially if you are kaggler then you must use Ensemble for hight performance and get highest score!<br>"},{"metadata":{},"cell_type":"markdown","source":"We're going to learn about Vanilla Ensemble, Bagging, Boosting, XGBoost, LightGBM and Stacking\n* Vanilla Ensemble\n* Bagging\n* Boosting\n* XGBoost\n* LightGBM\n* Stacking\n\nOkay Let's get it started~!\n"},{"metadata":{},"cell_type":"markdown","source":"[[](http://)](http://)<h3 style=\"color:green\">If you think this notebook is helpful, upvotes would be greatly appreciated :-) </h3>"},{"metadata":{},"cell_type":"markdown","source":"## What is Ensemble?\n* Emsemble is the method that makes many model & vote their results to get one score\n* e.g. for regression problems, we try to get mean of many models\n* <code>Mete Classifier</code> : meta_classifier is simply the classifier that makes a final prediction among all the predictions by using those predictions as features.\n* <code>Stacking(Meta-ensemble)</code>: It involves combining the predictions from multiple machine learning models on the same dataset, like bagging and boosting.\n* Ensemble **takes long time**, but it is really **powerful method**\n* Especially for *structured dataset!*"},{"metadata":{},"cell_type":"markdown","source":"<table><tr>\n<td> <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ6sEJxaMhsqcQkNKCUE7Zm45qhwVmPfYJ4Cw&usqp=CAU\" alt=\"Drawing\" style=\"width: 520px;\"/> </td>\n<td> <img src=\"https://machinelearningmastery.com/wp-content/uploads/2020/07/Example-of-Combining-Decision-Boundaries-Using-an-Ensemble.png\" alt=\"Drawing\" style=\"width: 550px;\"/> </td>\n</tr></table>"},{"metadata":{},"cell_type":"markdown","source":"# Import Library"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import Library\n\nimport numpy as np  # for linear algebra\nimport pandas as pd  # for dataframe\n\nimport matplotlib.pyplot as plt  # for visualization\nimport seaborn as sns\n%matplotlib inline\n\n# Pipelines for Training\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier  # for voting classifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import StackingClassifier\n\nfrom sklearn.model_selection import GridSearchCV  # for hyperparameter search\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report  # for report\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preprocessing (Using Pipelines)\n* Load Data\n* Remove Null Values\n* Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Data & Remove rows\nX = pd.read_csv(\"../input/titanic/train.csv\", index_col='PassengerId')\nX_test = pd.read_csv(\"../input/titanic/test.csv\", index_col='PassengerId')\n\n# Remove rows\nX.dropna(axis=0, subset=['Survived'], inplace=True)\ny = X.Survived\nX.drop(labels=['Survived'], axis=1, inplace=True)\n\n# \"Cardinality\" means the number of unique values in columns\n# Select categorical columns with relatively low cardinality\ncategorical_cols = [cname for cname in  X.columns if X[cname].nunique() < 10 and X[cname].dtype=='object']\n# Select numerical columns\nnumerical_cols = [cname for cname in X.columns if X[cname].dtype in [\"int64\", \"float64\"]]\n\n# keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX = X[my_cols].copy()\nX_test = X_test[my_cols].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X.shape, y.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_ = pd.read_csv('../input/titanic/train.csv')\n\nf, ax = plt.subplots(1, 2, figsize=(12, 4))\nX_['Survived'].value_counts().plot.pie(autopct='%1.1f%%', ax=ax[0])\nsns.countplot('Survived', data=X_, ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pipelines for training\n* numerical_transformer/categorical_transformer\n* preprocessor(ColumnTransformer)\n* define model\n* Bundle preprocessing and modeling\n* Preprocessing of training data, fit model\n* Preprocessing of validation data, get predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='most_frequent')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])  # pipeline steps: list of tuples that are chained, in the order in which they are chained\n\n# Preprocessing for numerical and categorical data\n# column transformer: Applies transformers to columns of an array or pandas DataFrame.\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Baseline"},{"metadata":{},"cell_type":"markdown","source":"* we need to define baseline for comparison"},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_clf = RandomForestClassifier()\n\n# Bundle preprocessing and modeling code in a pipeline\n# pipeline steps: list of tuples that are chained, in the order in which they are chained\nclf = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('scaler', StandardScaler()),\n    ('model', rf_clf)])\n\nprint(cross_val_score(clf, X, y, cv=10).mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_val)\n\nprint(classification_report(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Voting Classifier\n*a.k.a Majority Voting or Vanilla Ensemble*<br>\n\nA <code>Voting Classifier</code> is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output\n\n* Voting Classifier is basic Ensemble Classifier\n* using Majority Voting"},{"metadata":{},"cell_type":"markdown","source":"> Let's do it!"},{"metadata":{},"cell_type":"markdown","source":"### Practice Voting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier\n# Add Scaler\nfrom sklearn.preprocessing import StandardScaler\n\nclf1 = LogisticRegression(random_state=42)\nclf2 = DecisionTreeClassifier(random_state=42)\nclf3 = GaussianNB()\n\n# Voting Classifier\nclf = VotingClassifier(estimators=[\n    ('lr', clf1),\n    ('rf', clf2),\n    ('gnb', clf3)],\n    voting = 'hard')\n\n\neclf = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('scaler', StandardScaler()),\n    ('model', clf)\n])\n\nprint(cross_val_score(eclf, X, y, cv=10).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Bagging\n* <code>Bagging Classifiers</code> that are simply made with the same dataset are meaningless (e.g. if you consider multiple trees made with the same dataset, they will be made very similar and do not affect performance)\n* The data we are seeing is only a fraction of the very large data\n* Likewise, if we think of the sample itself as a population and create a model by sampling this population, we can make a very robust model.\n* It would be good to apply it to malware classification research that lacks data.\n\n## TL;DR\n> Let's make various classifiers with various sampling datasets!"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf2 = DecisionTreeClassifier(random_state=42)\neclf2 = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('sacler', StandardScaler()),\n    ('model', clf2)\n])\n\nprint(cross_val_score(eclf2, X, y, cv=10).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Adaboost\n*What is Boosting?*<br>\n\n* Proceeding the training round, creating a model -> updating the weight of each row by model\n* Model creation centered on rows with high instance weight (misclassified)\n* Create an ensemble model from these models\n* The purpose is to better categorize misclassified data.\n<br>\n\n> TL;DR - makes week learner stronger!"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf3 = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\neclf3 = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('scaler', StandardScaler()),\n    ('model', clf3)\n])\n\nprint(cross_val_score(eclf3, X, y, cv=10).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Gradient Boosting\n* Some kind of boosting technique like Adaboost\n* can be used for both Regression, Classification\n* Sequential + Additive Model\n* Reinforcing the model with the residual of the previous model\n* Weak learner developing by predicting residual"},{"metadata":{"trusted":true},"cell_type":"code","source":"clf4 = GradientBoostingClassifier()\neclf4 = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('scaler', StandardScaler()),\n    ('model', clf4)\n])\n\nprint(cross_val_score(eclf4, X, y, cv=10).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. XGboost & LightGBM\n* There is a package for parallel processing with GBM associative quantity.\n* Representative packages are XGBoost and LightGBM\n* <code>XGBoost</code>: eXtremeGradient Boosting\n* <code>LightGBM</codE>: Light Gradient Boosting Machine\n* Implementation algorithm differs in some parts, but the goal is similar\n* Both are *Tree-based Gradient Boosting!!*\n<br>\n\n![XGBoost-LGB](https://image.slidesharecdn.com/xgboostandlightgbm-180201121028/95/xgboost-lightgbm-21-638.jpg?cb=1517487076)\nfrom slidehsarecdn"},{"metadata":{},"cell_type":"markdown","source":"## 5.1. XGBoost"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"clf5_1 = XGBClassifier()\neclf5_1 = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('scaler', StandardScaler()),\n    ('model', clf5_1)\n])\n\nprint(cross_val_score(eclf5_1, X, y, cv=10).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.8227215980024969"},{"metadata":{},"cell_type":"markdown","source":"## 5.2. LightGBM"},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"clf5_2 = LGBMClassifier()\neclf5_2 = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('scaler', StandardScaler()),\n    ('model', clf5_2)\n])\n\nprint(cross_val_score(eclf5_2, X, y, cv=10).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Stacking\n* Stack of estimators with final classifier\n* Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.\n\n"},{"metadata":{"trusted":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"estimators = [\n    ('voting_clf', clf1),\n    ('bagging_clf', clf2),\n    ('adaboost_clf', clf3),\n    ('gb_clf', clf4),\n    ('XGB_clf', clf5_1),\n    ('LGBM_clf', clf5_2)]\n\nstack_clf = StackingClassifier(estimators=estimators)\n\nstack_clf_pip = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('scaler', StandardScaler()),\n    ('model', stack_clf)\n])\n\nprint(cross_val_score(stack_clf_pip, X, y, cv=10).mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"0.835043695380774"},{"metadata":{},"cell_type":"markdown","source":"## Classification Report"},{"metadata":{"trusted":true,"_kg_hide-output":false,"_kg_hide-input":false},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\nstack_clf_pip.fit(X_train, y_train)\npreds = stack_clf_pip.predict(X_val)\n\nprint(classification_report(y_val, preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### AUC-ROC Curve"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfpr, tpr, thresholds = roc_curve(y_val,preds)\nplt.plot(fpr, tpr, label='ROC curve')\nplt.plot([0, 1], [0, 1], 'k--', label='Random guess')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.xlim([-0.02, 1])\nplt.ylim([0, 1.02])\nplt.legend(loc=\"lower right\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Interpretation\n* Accuracy: 83%\n* f1-score(macro avg): 81%\n* f1-score(weighted avg): 82%\n* for class 1, f1-score is relatively low..."},{"metadata":{},"cell_type":"markdown","source":"# Conclusion"},{"metadata":{},"cell_type":"markdown","source":"* In this tutorial we studied various Ensemble Model from sklearn\n* Vanilla Ensemble, Voting Classifier, Bagging Classifier, Adaboost, XGBoost, lightGBM and Stacking\n* <code>Stacking</code> is Stack of estimators with final classifier and gave us better results\n* But But besides the ensemble, I think we have to learn a lot of things.\n    * e.g. Hypterparameter tuning, Feature Engineering, etc...\n* Therefore, in the next notebook, I'll look at the data a little more and do modeling and hyperparameter tuning :)"},{"metadata":{},"cell_type":"markdown","source":"[[](http://)](http://)<h3 style=\"color:green\">If you think this notebook is helpful, upvotes would be greatly appreciated :-) </h3>"},{"metadata":{},"cell_type":"markdown","source":"## More about...\n[[Model Evaluation]1. Classification Metrics](https://www.kaggle.com/leeyj0511/model-evaluation-1-classification-metrics)\n\n[[for Starter] top 30%, Machine Learning Pipelines](https://www.kaggle.com/leeyj0511/for-starter-top-30-machine-learning-pipelines)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}