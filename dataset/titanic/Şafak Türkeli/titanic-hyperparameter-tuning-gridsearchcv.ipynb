{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Importing libraries and loading datasets","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# Modelling\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\n\n# KNeighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Perceptron\nfrom sklearn.linear_model import Perceptron\n\n# Support Vector Machines\nfrom sklearn.svm import SVC\n\n# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\n# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# AdaBoost\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# XGBoost\nfrom xgboost import XGBClassifier\n\n# LightGBM\nfrom lightgbm import LGBMClassifier","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:30.048105Z","iopub.execute_input":"2021-12-13T20:43:30.048385Z","iopub.status.idle":"2021-12-13T20:43:30.05734Z","shell.execute_reply.started":"2021-12-13T20:43:30.048357Z","shell.execute_reply":"2021-12-13T20:43:30.056175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/titanic/train.csv')\ntest_data = pd.read_csv('../input/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:43:30.059264Z","iopub.execute_input":"2021-12-13T20:43:30.060109Z","iopub.status.idle":"2021-12-13T20:43:30.090765Z","shell.execute_reply.started":"2021-12-13T20:43:30.060067Z","shell.execute_reply":"2021-12-13T20:43:30.089061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Explore data","metadata":{}},{"cell_type":"code","source":"train_data","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:30.092566Z","iopub.execute_input":"2021-12-13T20:43:30.093078Z","iopub.status.idle":"2021-12-13T20:43:30.122172Z","shell.execute_reply.started":"2021-12-13T20:43:30.093023Z","shell.execute_reply":"2021-12-13T20:43:30.121277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:30.124523Z","iopub.execute_input":"2021-12-13T20:43:30.124829Z","iopub.status.idle":"2021-12-13T20:43:30.157072Z","shell.execute_reply.started":"2021-12-13T20:43:30.124791Z","shell.execute_reply":"2021-12-13T20:43:30.156265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Columns: \\n{0} \".format(train_data.columns.tolist()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:30.158494Z","iopub.execute_input":"2021-12-13T20:43:30.158793Z","iopub.status.idle":"2021-12-13T20:43:30.164154Z","shell.execute_reply.started":"2021-12-13T20:43:30.158757Z","shell.execute_reply":"2021-12-13T20:43:30.163253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Basic data check","metadata":{}},{"cell_type":"markdown","source":"## Missing values","metadata":{}},{"cell_type":"code","source":"missing_values = train_data.isna().any()\nprint('Columns which have missing values: \\n{0}'.format(missing_values[missing_values == True].index.tolist()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:30.165863Z","iopub.execute_input":"2021-12-13T20:43:30.16627Z","iopub.status.idle":"2021-12-13T20:43:30.178152Z","shell.execute_reply.started":"2021-12-13T20:43:30.16623Z","shell.execute_reply":"2021-12-13T20:43:30.177154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Percentage of missing values in `Age` column: {0:.2f}\".format(100.*(train_data.Age.isna().sum()/len(train_data))))\nprint(\"Percentage of missing values in `Cabin` column: {0:.2f}\".format(100.*(train_data.Cabin.isna().sum()/len(train_data))))\nprint(\"Percentage of missing values in `Embarked` column: {0:.2f}\".format(100.*(train_data.Embarked.isna().sum()/len(train_data))))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:30.180296Z","iopub.execute_input":"2021-12-13T20:43:30.18058Z","iopub.status.idle":"2021-12-13T20:43:30.189257Z","shell.execute_reply.started":"2021-12-13T20:43:30.180544Z","shell.execute_reply":"2021-12-13T20:43:30.188274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for duplicates","metadata":{}},{"cell_type":"code","source":"duplicates = train_data.duplicated().sum()\nprint('Duplicates in train data: {0}'.format(duplicates))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:30.190557Z","iopub.execute_input":"2021-12-13T20:43:30.191152Z","iopub.status.idle":"2021-12-13T20:43:30.204282Z","shell.execute_reply.started":"2021-12-13T20:43:30.19111Z","shell.execute_reply":"2021-12-13T20:43:30.203344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical variables","metadata":{}},{"cell_type":"code","source":"categorical = train_data.nunique().sort_values(ascending=True)\nprint('Categorical variables in train data: \\n{0}'.format(categorical))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:30.208119Z","iopub.execute_input":"2021-12-13T20:43:30.209744Z","iopub.status.idle":"2021-12-13T20:43:30.219305Z","shell.execute_reply.started":"2021-12-13T20:43:30.209705Z","shell.execute_reply":"2021-12-13T20:43:30.218515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data cleaning","metadata":{}},{"cell_type":"code","source":"def clean_data(data):\n    # Too many missing values\n    data.drop(['Cabin'], axis=1, inplace=True)\n    \n    # Probably will not provide some useful information\n    data.drop(['Name', 'Ticket', 'Fare', 'Embarked'], axis=1, inplace=True)\n    \n    return data\n    \ntrain_data = clean_data(train_data)\ntest_data = clean_data(test_data)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:43:30.220816Z","iopub.execute_input":"2021-12-13T20:43:30.221341Z","iopub.status.idle":"2021-12-13T20:43:30.232288Z","shell.execute_reply.started":"2021-12-13T20:43:30.221305Z","shell.execute_reply":"2021-12-13T20:43:30.231527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.tail()","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:43:30.233704Z","iopub.execute_input":"2021-12-13T20:43:30.234773Z","iopub.status.idle":"2021-12-13T20:43:30.247573Z","shell.execute_reply.started":"2021-12-13T20:43:30.234735Z","shell.execute_reply":"2021-12-13T20:43:30.246802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Feature engineering\n\nAlthough I have eliminated most of the columns for simplicity, in the future I am planning to recover those columns. They may contain some useful information.  \nFor now encoding the `Sex` column and filling `Age` column is enough to run a model.","metadata":{}},{"cell_type":"code","source":"train_data['Sex'].replace({'male':0, 'female':1}, inplace=True)\ntest_data['Sex'].replace({'male':0, 'female':1}, inplace=True)\n\n# Merge two data to get the average Age and fill the column\nall_data = pd.concat([train_data, test_data])\naverage = all_data.Age.median()\nprint(\"Average Age: {0}\".format(average))\ntrain_data.fillna(value={'Age': average}, inplace=True)\ntest_data.fillna(value={'Age': average}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:43:30.24881Z","iopub.execute_input":"2021-12-13T20:43:30.249209Z","iopub.status.idle":"2021-12-13T20:43:30.264964Z","shell.execute_reply.started":"2021-12-13T20:43:30.249171Z","shell.execute_reply":"2021-12-13T20:43:30.264278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.tail()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:30.266299Z","iopub.execute_input":"2021-12-13T20:43:30.2667Z","iopub.status.idle":"2021-12-13T20:43:30.280376Z","shell.execute_reply.started":"2021-12-13T20:43:30.266666Z","shell.execute_reply":"2021-12-13T20:43:30.279512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Modelling\n\nTry different models with different parameters to understand which models give better results.","metadata":{}},{"cell_type":"code","source":"# Set X and y\nX = train_data.drop(['Survived', 'PassengerId'], axis=1)\ny = train_data['Survived']\ntest_X = test_data.drop(['PassengerId'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:43:30.282059Z","iopub.execute_input":"2021-12-13T20:43:30.282868Z","iopub.status.idle":"2021-12-13T20:43:30.290086Z","shell.execute_reply.started":"2021-12-13T20:43:30.282827Z","shell.execute_reply":"2021-12-13T20:43:30.289331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To store models created\nbest_models = {}\n\n# Split data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n\ndef print_best_parameters(hyperparameters, best_parameters):\n    value = \"Best parameters: \"\n    for key in hyperparameters:\n        value += str(key) + \": \" + str(best_parameters[key]) + \", \"\n    if hyperparameters:\n        print(value[:-2])\n\ndef get_best_model(estimator, hyperparameters, fit_params={}):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    grid_search = GridSearchCV(estimator=estimator, param_grid=hyperparameters, n_jobs=-1, cv=cv, scoring=\"accuracy\")\n    best_model = grid_search.fit(train_X, train_y, **fit_params)\n    best_parameters = best_model.best_estimator_.get_params()\n    print_best_parameters(hyperparameters, best_parameters)\n    return best_model\n\ndef evaluate_model(model, name):\n    print(\"Accuracy score:\", accuracy_score(train_y, model.predict(train_X)))\n    best_models[name] = model","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:30.292224Z","iopub.execute_input":"2021-12-13T20:43:30.293527Z","iopub.status.idle":"2021-12-13T20:43:30.305325Z","shell.execute_reply.started":"2021-12-13T20:43:30.293482Z","shell.execute_reply":"2021-12-13T20:43:30.304509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Features: \\n{0} \".format(X.columns.tolist()))","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:30.306693Z","iopub.execute_input":"2021-12-13T20:43:30.307496Z","iopub.status.idle":"2021-12-13T20:43:30.316779Z","shell.execute_reply.started":"2021-12-13T20:43:30.307458Z","shell.execute_reply":"2021-12-13T20:43:30.315939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n\nTune the logistic regression model by changing some of its parameters.\n\nLogistic regression parameters:  \n\n* **solver: {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’**  \n    * Algorithm to use in the optimization problem. Default is ‘lbfgs’. To choose a solver, you might want to consider the following aspects:\n        * For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones;\n        * For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss;\n        * ‘liblinear’ is limited to one-versus-rest schemes.\n\n> **Warning**  \n> The choice of the algorithm depends on the penalty chosen: Supported penalties by solver:  \n> * ‘newton-cg’ - [‘l2’, ‘none’]  \n> * ‘lbfgs’ - [‘l2’, ‘none’]  \n> * ‘liblinear’ - [‘l1’, ‘l2’]  \n> * ‘sag’ - [‘l2’, ‘none’]  \n> * ‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, ‘none’]  \n\n* **penalty: {‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’**  \n    * Specify the norm of the penalty:\n        * 'none': no penalty is added;\n        * 'l2': add a L2 penalty term and it is the default choice;\n        * 'l1': add a L1 penalty term;\n        * 'elasticnet': both L1 and L2 penalty terms are added.\n\n> **Warning**  \n> Some penalties may not work with some solvers. See the parameter solver below, to know the compatibility between the penalty and solver. \n\n* **C: float, default=1.0**  \n    Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n","metadata":{}},{"cell_type":"code","source":"# https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\nhyperparameters = {\n    'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],\n    'penalty' : ['l2'],\n    'C'       : [100, 10, 1.0, 0.1, 0.01]\n}\nestimator = LogisticRegression(random_state=1)\nbest_model_logistic = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:43:30.318278Z","iopub.execute_input":"2021-12-13T20:43:30.318724Z","iopub.status.idle":"2021-12-13T20:43:38.487245Z","shell.execute_reply.started":"2021-12-13T20:43:30.318682Z","shell.execute_reply":"2021-12-13T20:43:38.486495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_logistic.best_estimator_, 'logistic')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:38.491323Z","iopub.execute_input":"2021-12-13T20:43:38.491639Z","iopub.status.idle":"2021-12-13T20:43:38.508539Z","shell.execute_reply.started":"2021-12-13T20:43:38.49156Z","shell.execute_reply":"2021-12-13T20:43:38.507068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)","metadata":{}},{"cell_type":"markdown","source":"### [Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n\n* **var_smoothing: float, default=1e-9**  \n    Portion of the largest variance of all features that is added to variances for calculation stability.","metadata":{}},{"cell_type":"code","source":"# https://www.analyticsvidhya.com/blog/2021/01/gaussian-naive-bayes-with-hyperpameter-tuning/\nhyperparameters = {\n    'var_smoothing': np.logspace(0, -9, num=100)\n}\nestimator = GaussianNB()\nbest_model_gaussian_nb = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:43:38.511115Z","iopub.execute_input":"2021-12-13T20:43:38.51157Z","iopub.status.idle":"2021-12-13T20:43:49.400758Z","shell.execute_reply.started":"2021-12-13T20:43:38.511528Z","shell.execute_reply":"2021-12-13T20:43:49.399983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_gaussian_nb.best_estimator_, 'gaussian_nb')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:49.404547Z","iopub.execute_input":"2021-12-13T20:43:49.406849Z","iopub.status.idle":"2021-12-13T20:43:49.418753Z","shell.execute_reply.started":"2021-12-13T20:43:49.406807Z","shell.execute_reply":"2021-12-13T20:43:49.417847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n\n* **alpha: float, default=1.0**  \n    Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n    \n* **fit_prior: bool, default=True**  \n    Whether to learn class prior probabilities or not. If false, a uniform prior will be used.","metadata":{}},{"cell_type":"code","source":"# https://medium.com/@kocur4d/hyper-parameter-tuning-with-pipelines-5310aff069d6\nhyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n}\nestimator = MultinomialNB()\nbest_model_multinominal_nb = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:43:49.423573Z","iopub.execute_input":"2021-12-13T20:43:49.425642Z","iopub.status.idle":"2021-12-13T20:43:50.601868Z","shell.execute_reply.started":"2021-12-13T20:43:49.425558Z","shell.execute_reply":"2021-12-13T20:43:50.601046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_multinominal_nb.best_estimator_, 'multinominal_nb')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:50.603293Z","iopub.execute_input":"2021-12-13T20:43:50.603817Z","iopub.status.idle":"2021-12-13T20:43:50.611872Z","shell.execute_reply.started":"2021-12-13T20:43:50.603767Z","shell.execute_reply":"2021-12-13T20:43:50.610879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [Complement Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html)\n\n* **alpha: float, default=1.0**  \n    Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n\n* **fit_prior: bool, default=True**  \n    Only used in edge case with a single class in the training set.\n\n* **norm: bool, default=False**  \n    Whether or not a second normalization of the weights is performed. The default behavior mirrors the implementations found in Mahout and Weka, which do not follow the full algorithm described in Table 9 of the paper.","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n    'norm'      : [True, False]\n}\nestimator = ComplementNB()\nbest_model_complement_nb = get_best_model(estimator, hyperparameters)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:50.613028Z","iopub.execute_input":"2021-12-13T20:43:50.613284Z","iopub.status.idle":"2021-12-13T20:43:52.882829Z","shell.execute_reply.started":"2021-12-13T20:43:50.613232Z","shell.execute_reply":"2021-12-13T20:43:52.881953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_complement_nb.best_estimator_, 'complement_nb')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:52.887718Z","iopub.execute_input":"2021-12-13T20:43:52.888364Z","iopub.status.idle":"2021-12-13T20:43:52.896199Z","shell.execute_reply.started":"2021-12-13T20:43:52.888321Z","shell.execute_reply":"2021-12-13T20:43:52.895363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### [Bernoulli Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n\n* **alpha: float, default=1.0**  \n    Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).\n    \n* **fit_prior: bool, default=True**  \n    Whether to learn class prior probabilities or not. If false, a uniform prior will be used.","metadata":{}},{"cell_type":"code","source":"hyperparameters = {\n    'alpha'     : [0.5, 1.0, 1.5, 2.0, 5],\n    'fit_prior' : [True, False],\n}\nestimator = BernoulliNB()\nbest_model_bernoulli_nb = get_best_model(estimator, hyperparameters)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:43:52.898157Z","iopub.execute_input":"2021-12-13T20:43:52.898599Z","iopub.status.idle":"2021-12-13T20:43:54.166389Z","shell.execute_reply.started":"2021-12-13T20:43:52.898557Z","shell.execute_reply":"2021-12-13T20:43:54.165632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_bernoulli_nb.best_estimator_, 'bernoulli_nb')","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:43:54.167662Z","iopub.execute_input":"2021-12-13T20:43:54.170544Z","iopub.status.idle":"2021-12-13T20:43:54.177686Z","shell.execute_reply.started":"2021-12-13T20:43:54.170512Z","shell.execute_reply":"2021-12-13T20:43:54.176825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [K-nearest neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n\nTune k-nearest neighbors model by changing some of its parameters.\n\n* **n_neighbors: int, default=5**  \n    Number of neighbors to use by default for kneighbors queries.\n\n\n* **weights: {‘uniform’, ‘distance’} or callable, default=’uniform’**  \n    * Weight function used in prediction. Possible values:\n        * ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.\n        * ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.\n        * [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.\n\n\n* **algorithm: {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’**  \n    * Algorithm used to compute the nearest neighbors:  \n        * ‘ball_tree’ will use BallTree\n        * ‘kd_tree’ will use KDTree\n        * ‘brute’ will use a brute-force search.\n        * ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.\n        \n> Note: fitting on sparse input will override the setting of this parameter, using brute force.\n\n\n* **leaf_size: int, default=30**  \n    Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.\n    \n* **p: int, default=2**  \n    Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n* **n_neighbors: int, default=5**  \n    Number of neighbors to use by default for kneighbors queries.","metadata":{}},{"cell_type":"code","source":"# https://medium.datadriveninvestor.com/k-nearest-neighbors-in-python-hyperparameters-tuning-716734bc557f\nhyperparameters = {\n    'n_neighbors' : list(range(1,5)),\n    'weights'     : ['uniform', 'distance'],\n    'algorithm'   : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'leaf_size'   : list(range(1,10)),\n    'p'           : [1,2]\n}\nestimator = KNeighborsClassifier()\nbest_model_kneighbors = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:43:54.179681Z","iopub.execute_input":"2021-12-13T20:43:54.180239Z","iopub.status.idle":"2021-12-13T20:44:56.249539Z","shell.execute_reply.started":"2021-12-13T20:43:54.180197Z","shell.execute_reply":"2021-12-13T20:44:56.248438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_kneighbors.best_estimator_, 'kneighbors')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:44:56.250712Z","iopub.status.idle":"2021-12-13T20:44:56.251383Z","shell.execute_reply.started":"2021-12-13T20:44:56.251146Z","shell.execute_reply":"2021-12-13T20:44:56.25117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)\n\n* **penalty: {‘l2’,’l1’,’elasticnet’}, default=None**  \n    The penalty (aka regularization term) to be used.\n\n* **max_iter: int, default=1000**  \n    The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit method.\n    \n* **eta0: double, default=1**  \n    Constant by which the updates are multiplied.","metadata":{}},{"cell_type":"code","source":"# https://machinelearningmastery.com/perceptron-algorithm-for-classification-in-python/\n# https://machinelearningmastery.com/manually-optimize-hyperparameters/\nhyperparameters = {\n    'penalty'  : ['l1', 'l2', 'elasticnet'],\n    'eta0'     : [0.0001, 0.001, 0.01, 0.1, 1.0],\n    'max_iter' : list(range(50, 200, 50))\n}\nestimator = Perceptron(random_state=1)\nbest_model_perceptron = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:44:56.252628Z","iopub.status.idle":"2021-12-13T20:44:56.25352Z","shell.execute_reply.started":"2021-12-13T20:44:56.25321Z","shell.execute_reply":"2021-12-13T20:44:56.253235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_perceptron.best_estimator_, 'perceptron')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:44:56.254781Z","iopub.status.idle":"2021-12-13T20:44:56.255632Z","shell.execute_reply.started":"2021-12-13T20:44:56.255366Z","shell.execute_reply":"2021-12-13T20:44:56.255392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Support Vector Machines](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n\n* **C: float, default=1.0**  \n    Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n\n* **kernel: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’**  \n    Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples).\n\n\n* **gamma{‘scale’, ‘auto’} or float, default=’scale’**  \n    Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.\n    * if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,\n    * if ‘auto’, uses 1 / n_features.","metadata":{}},{"cell_type":"code","source":"# https://www.geeksforgeeks.org/svm-hyperparameter-tuning-using-gridsearchcv-ml/\n# https://towardsdatascience.com/hyperparameter-tuning-for-support-vector-machines-c-and-gamma-parameters-6a5097416167\nhyperparameters = {\n    'C'      : [0.1, 1, 10, 100],\n    'gamma'  : [0.0001, 0.001, 0.01, 0.1, 1],\n    'kernel' : ['rbf']\n}\nestimator = SVC(random_state=1)\nbest_model_svc = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:44:56.256721Z","iopub.status.idle":"2021-12-13T20:44:56.257531Z","shell.execute_reply.started":"2021-12-13T20:44:56.257285Z","shell.execute_reply":"2021-12-13T20:44:56.257311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_svc.best_estimator_, 'svc')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:44:56.258645Z","iopub.status.idle":"2021-12-13T20:44:56.259396Z","shell.execute_reply.started":"2021-12-13T20:44:56.259163Z","shell.execute_reply":"2021-12-13T20:44:56.259187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Stochastic Gradient Descent](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n\n* **loss: str, default=’hinge’**  \n    The loss function to be used. Defaults to ‘hinge’, which gives a linear SVM.  \n    The possible options are ‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’, or a regression loss: ‘squared_error’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.  \n    The ‘log’ loss gives logistic regression, a probabilistic classifier. ‘modified_huber’ is another smooth loss that brings tolerance to outliers as well as probability estimates. ‘squared_hinge’ is like hinge but is quadratically penalized. ‘perceptron’ is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description.\n\n* **penalty: {‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’**  \n    The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear SVM models. ‘l1’ and ‘elasticnet’ might bring sparsity to the model (feature selection) not achievable with ‘l2’.\n    \n* **alpha: float, default=0.0001**  \n    Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when set to learning_rate is set to ‘optimal’.","metadata":{}},{"cell_type":"code","source":"# https://towardsdatascience.com/how-to-make-sgd-classifier-perform-as-well-as-logistic-regression-using-parfit-cc10bca2d3c4\n# https://www.knowledgehut.com/tutorials/machine-learning/hyperparameter-tuning-machine-learning\nhyperparameters = {\n    'loss'    : ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n    'penalty' : ['l1', 'l2', 'elasticnet'],\n    'alpha'   : [0.01, 0.1, 1, 10]\n}\nestimator = SGDClassifier(random_state=1, early_stopping=True)\nbest_model_sgd = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:44:56.262371Z","iopub.status.idle":"2021-12-13T20:44:56.263254Z","shell.execute_reply.started":"2021-12-13T20:44:56.263006Z","shell.execute_reply":"2021-12-13T20:44:56.263031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_sgd.best_estimator_, 'sgd')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:44:56.264321Z","iopub.status.idle":"2021-12-13T20:44:56.26694Z","shell.execute_reply.started":"2021-12-13T20:44:56.266695Z","shell.execute_reply":"2021-12-13T20:44:56.26672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)\n\n* **loss: {‘deviance’, ‘exponential’}, default=’deviance’**  \n    The loss function to be optimized. ‘deviance’ refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss ‘exponential’ gradient boosting recovers the AdaBoost algorithm.\n\n* **learning_rate: float, default=0.1**  \n    Learning rate shrinks the contribution of each tree by learning_rate. There is a trade-off between learning_rate and n_estimators.\n\n* **n_estimators: int, default=100**  \n    The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n\n* **subsample: float, default=1.0**  \n    The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators. Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.\n\n* **max_depth: int, default=3**  \n    The maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables.","metadata":{}},{"cell_type":"code","source":"# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\nhyperparameters = {\n    'loss'          : ['deviance', 'exponential'],\n    'learning_rate' : [0.01, 0.1, 0.2, 0.3],\n    'n_estimators'  : [50, 100, 200],\n    'subsample'     : [0.1, 0.2, 0.5, 1.0],\n    'max_depth'     : [2, 3, 4, 5]\n}\nestimator = GradientBoostingClassifier(random_state=1)\nbest_model_gbc = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T21:04:34.849281Z","iopub.execute_input":"2021-12-13T21:04:34.849563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_gbc.best_estimator_, 'gbc')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [AdaBoost Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n\n* **n_estimators: int, default=50**  \n    The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n    \n* **learning_rate: float, default=1.0**  \n    Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the learning_rate and n_estimators parameters.","metadata":{}},{"cell_type":"code","source":"# https://medium.com/@chaudhurysrijani/tuning-of-adaboost-with-computational-complexity-8727d01a9d20\nhyperparameters = {\n    'n_estimators'  : [10, 50, 100, 500],\n    'learning_rate' : [0.001, 0.01, 0.1, 1.0]\n}\nestimator = AdaBoostClassifier(random_state=1)\nbest_model_adaboost = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:44:56.271975Z","iopub.status.idle":"2021-12-13T20:44:56.272584Z","shell.execute_reply.started":"2021-12-13T20:44:56.272349Z","shell.execute_reply":"2021-12-13T20:44:56.272372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_adaboost.best_estimator_, 'adaboost')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:44:56.273931Z","iopub.status.idle":"2021-12-13T20:44:56.274667Z","shell.execute_reply.started":"2021-12-13T20:44:56.27437Z","shell.execute_reply":"2021-12-13T20:44:56.274398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n\nTune decision tree classifier model by changing some of its parameters.\n\n* **criterion: {“gini”, “entropy”}, default=”gini”**  \n    The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n\n* **splitter: {“best”, “random”}, default=”best”**  \n    The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n\n* **max_depth: int, default=None**  \n    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n    \n\n* **min_samples_split: int or float, default=2**  \n    * The minimum number of samples required to split an internal node:\n        * If int, then consider min_samples_split as the minimum number.\n        * If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n\n\n* **min_samples_leaf: int or float, default=1**  \n   The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.  \n    * If int, then consider min_samples_leaf as the minimum number.\n    * If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.","metadata":{}},{"cell_type":"code","source":"# https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680\n# https://www.kaggle.com/gauravduttakiit/hyperparameter-tuning-in-decision-trees\nhyperparameters = {\n    'criterion'         : ['gini', 'entropy'],\n    'splitter'          : ['best', 'random'],\n    'max_depth'         : [None, 1, 2, 3, 4, 5],\n    'min_samples_split' : list(range(2,5)),\n    'min_samples_leaf'  : list(range(1,5))\n}\nestimator = DecisionTreeClassifier(random_state=1)\nbest_model_decision_tree = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:44:56.275858Z","iopub.status.idle":"2021-12-13T20:44:56.276472Z","shell.execute_reply.started":"2021-12-13T20:44:56.276236Z","shell.execute_reply":"2021-12-13T20:44:56.276261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_decision_tree.best_estimator_, 'decision_tree')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:44:56.277688Z","iopub.status.idle":"2021-12-13T20:44:56.278291Z","shell.execute_reply.started":"2021-12-13T20:44:56.278055Z","shell.execute_reply":"2021-12-13T20:44:56.278079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n\n* **n_estimators: int, default=100**  \n    The number of trees in the forest.\n\n\n* **max_features: {“auto”, “sqrt”, “log2”}, int or float, default=”auto”**  \n    * The number of features to consider when looking for the best split:\n        * If int, then consider max_features features at each split.\n        * If float, then max_features is a fraction and round(max_features * n_features) features are considered at each split.\n        * If “auto”, then max_features=sqrt(n_features).\n        * If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n        * If “log2”, then max_features=log2(n_features).\n        * If None, then max_features=n_features.\n\n> Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n\n* **criterion: {“gini”, “entropy”}, default=”gini”**  \n    The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.\n\n* **max_depth: int, default=None**  \n    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n    \n    \n* **min_samples_split: int or float, default=2**  \n    * The minimum number of samples required to split an internal node:\n        * If int, then consider min_samples_split as the minimum number.\n        * If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n\n\n* **min_samples_leaf: int or float, default=1**  \n    The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.  \n     * If int, then consider min_samples_leaf as the minimum number.\n     * If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.","metadata":{}},{"cell_type":"code","source":"# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n# https://www.analyticsvidhya.com/blog/2020/03/beginners-guide-random-forest-hyperparameter-tuning/\nhyperparameters = {\n    'n_estimators'      : list(range(10, 50, 10)),\n    'max_features'      : ['auto', 'sqrt', 'log2'],\n    'criterion'         : ['gini', 'entropy'],\n    'max_depth'         : [None, 1, 2, 3, 4, 5],\n    'min_samples_split' : list(range(2,5)),\n    'min_samples_leaf'  : list(range(1,5))\n}\nestimator = RandomForestClassifier(random_state=1)\nbest_model_random_forest = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:44:56.279478Z","iopub.status.idle":"2021-12-13T20:44:56.280101Z","shell.execute_reply.started":"2021-12-13T20:44:56.279856Z","shell.execute_reply":"2021-12-13T20:44:56.27988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_random_forest.best_estimator_, 'random_forest')","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:44:56.281284Z","iopub.status.idle":"2021-12-13T20:44:56.281896Z","shell.execute_reply.started":"2021-12-13T20:44:56.281661Z","shell.execute_reply":"2021-12-13T20:44:56.281684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [XGBClassifier](https://xgboost.readthedocs.io/en/stable/parameter.html)\n\n* **eta [default=0.3, alias: learning_rate]**  \n    * Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n    * range: [0,1]\n\n\n* **gamma [default=0, alias: min_split_loss]**  \n    * Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n    * range: [0,∞]\n\n\n* **max_depth [default=6]**  \n    * Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit. 0 is only accepted in lossguide growing policy when tree_method is set as hist or gpu_hist and it indicates no limit on depth. Beware that XGBoost aggressively consumes memory when training a deep tree.\n    * range: [0,∞] (0 is only accepted in lossguide growing policy when tree_method is set as hist or gpu_hist)\n\n\n* **lambda [default=1, alias: reg_lambda]**  \n    L2 regularization term on weights. Increasing this value will make model more conservative.\n\n* **alpha [default=0, alias: reg_alpha]**  \n    L1 regularization term on weights. Increasing this value will make model more conservative.","metadata":{}},{"cell_type":"code","source":"# https://towardsdatascience.com/binary-classification-xgboost-hyperparameter-tuning-scenarios-by-non-exhaustive-grid-search-and-c261f4ce098d\nhyperparameters = {\n    'learning_rate' : [0.3, 0.4, 0.5],\n    'gamma'         : [0, 0.4, 0.8],\n    'max_depth'     : [2, 3, 4],\n    'reg_lambda'    : [0, 0.1, 1],\n    'reg_alpha'     : [0.1, 1]\n}\nfit_params = {\n    'verbose'               : False,\n    'early_stopping_rounds' : 40,\n    'eval_metric'           : 'logloss',\n    'eval_set'              : [(val_X, val_y)]\n}\nestimator = XGBClassifier(seed=1, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False)\nbest_model_xgb = get_best_model(estimator, hyperparameters, fit_params)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:44:56.283132Z","iopub.status.idle":"2021-12-13T20:44:56.283748Z","shell.execute_reply.started":"2021-12-13T20:44:56.283499Z","shell.execute_reply":"2021-12-13T20:44:56.283522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_xgb.best_estimator_, 'xgb')","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-12-13T20:44:56.284924Z","iopub.status.idle":"2021-12-13T20:44:56.285525Z","shell.execute_reply.started":"2021-12-13T20:44:56.285291Z","shell.execute_reply":"2021-12-13T20:44:56.285314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## [LGBMClassifier](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)\n\n* **boosting_type: (str, optional (default='gbdt'))**  \n    * ‘gbdt’, traditional Gradient Boosting Decision Tree.\n    * ‘dart’, Dropouts meet Multiple Additive Regression Trees.\n    * ‘goss’, Gradient-based One-Side Sampling.\n    * ‘rf’, Random Forest.\n\n\n* **num_leaves: (int, optional (default=31))**  \n    Maximum tree leaves for base learners.\n\n* **learning_rate: (float, optional (default=0.1))**  \n    Boosting learning rate. You can use callbacks parameter of fit method to shrink/adapt learning rate in training using reset_parameter callback. Note, that this will ignore the learning_rate argument in training.\n\n* **n_estimators: (int, optional (default=100))**  \n    Number of boosted trees to fit.\n\n* **reg_alpha: (float, optional (default=0.))**  \n    L1 regularization term on weights.\n\n* **reg_lambda: (float, optional (default=0.))**  \n    L2 regularization term on weights.","metadata":{}},{"cell_type":"code","source":"# https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5\nhyperparameters = {\n    'boosting_type' : ['gbdt', 'dart', 'goss'],\n    'num_leaves'    : [4, 8, 16, 32],\n    'learning_rate' : [0.01, 0.1, 1],\n    'n_estimators'  : [25, 50, 100],\n    'reg_alpha'     : [0, 0.1, 1],\n    'reg_lambda'    : [0, 0.1, 1],\n}\nestimator = LGBMClassifier(random_state=1, device='gpu')\nbest_model_lgbm = get_best_model(estimator, hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:44:56.286735Z","iopub.status.idle":"2021-12-13T20:44:56.287354Z","shell.execute_reply.started":"2021-12-13T20:44:56.287122Z","shell.execute_reply":"2021-12-13T20:44:56.287147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_model(best_model_lgbm.best_estimator_, 'lgbm')","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:44:56.288545Z","iopub.status.idle":"2021-12-13T20:44:56.289173Z","shell.execute_reply.started":"2021-12-13T20:44:56.288926Z","shell.execute_reply":"2021-12-13T20:44:56.28895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WORK IN PROGRESS","metadata":{}},{"cell_type":"markdown","source":"# 7. Submission","metadata":{}},{"cell_type":"code","source":"# Get predictions for each model and create submission files\nfor model in best_models:\n    predictions = best_models[model].predict(test_X)\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('submission_' + model + '.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-13T20:44:56.290357Z","iopub.status.idle":"2021-12-13T20:44:56.290963Z","shell.execute_reply.started":"2021-12-13T20:44:56.290729Z","shell.execute_reply":"2021-12-13T20:44:56.290753Z"},"trusted":true},"execution_count":null,"outputs":[]}]}