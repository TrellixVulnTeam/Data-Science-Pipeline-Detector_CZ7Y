{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# ***Interactive*** Titanic Notebook","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://media.giphy.com/media/g2e22SIHPcQw0/giphy.gif\">","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Hi there! This is my first Kaggle notebook, and instead of doing the clichÃ© basic scripts or highly advanced ML scripts (since I am not an expert), I decided to add some fun interactive plots and visualizations to showcase the Titanic dataset and evaluate some generic algorithms. \n\n`Aesthetics are a moral imperative`","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"> ## Quick rundown:\n* [Starting housekeeping](#first-section)\n* [Data Visualization](#second-section)\n* [Evaluation of ML Models](#third-section)\n* [Submission](#fourth-section)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Starting housekeeping<a class=\"anchor\" id=\"first-section\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Importing tools, data, and cleaning up the data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's start off importing some python tools\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom tabulate import tabulate\n\n# Visualization libraries \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport plotly as py\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n#!pip install heatmapz # Really great heatmap visualization from https://pypi.org/project/heatmapz/\n#from heatmap import heatmap, corrplot\n%matplotlib inline\n#%matplotlib notebook\n\n# ML tools\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import svm\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB,CategoricalNB,MultinomialNB\nfrom sklearn.linear_model import Perceptron, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\n\n# Importing Datasets to pandas DataFrames\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ncomplete_df = [train_df, test_df] #Helpful when cleaning both dataframes at once\n\n# Completing or deleting missing values in the dataset\n# I am filling in median age, mode embark, and mediam fare for missing values\nfor datadf in complete_df:    \n    #complete missing age with median\n    datadf['Age'].fillna(datadf['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    datadf['Embarked'].fillna(datadf['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    datadf['Fare'].fillna(datadf['Fare'].median(), inplace = True)\n    \n# Getting rid of irrelevant columns: Passanger ID, Cabin number, and Ticker number\ndrop_column = ['PassengerId','Cabin', 'Ticket']\nfor datadf in complete_df:    \n    datadf.drop(drop_column, axis=1, inplace = True)\n    \n# Ok, time to visualize the data!\nimport warnings\nimport numpy as np\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nprint('x' in np.arange(5))   #returns False, without Warning","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"> ## Data Visualization <a class=\"anchor\" id=\"second-section\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### For distribution of data, statistical analysis is much more powerful visually than in a giant chart. In the first plot, I am using simple \"*seaborn*\" functions to combine a swarmplot and boxplot to highlight the age and sex distribution. Individual people are colorcoded to show if they survived, as this parameter is our target. Box plot is shown around the datapoints. \n\n### Sometimes simple is best.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# General age and sex distribution with added swarmplot\nplt.figure(figsize=(12,8))\n\n# sns plot would add a default legend of 0 or 1 for Survived column, so change it to \"Died\" and \"Survived\"\ncustom = [Line2D([], [], marker='o', color='#023EFF', linestyle='None'),\n          Line2D([], [], marker='o', color='#FF7C00', linestyle='None')]\nsns.swarmplot(x=\"Age\", y=\"Sex\", hue=\"Survived\", data=train_df, palette=\"bright\")\nax = sns.boxplot(x=\"Age\", y=\"Sex\", data=train_df, color='white')\n\n# Making the boxplot edge colors black for better contrast\nfor i,box in enumerate(ax.artists):\n    box.set_edgecolor('black')\n    box.set_facecolor('white')\n    # iterate over whiskers and median lines\n    for j in range(6*i,6*(i+1)):\n         ax.lines[j].set_color('black')\n            \n# Setting the text for legend, overall font size, and title of figure            \nplt.legend(custom, ['Died', 'Survived'], loc='upper right')\nplt.rc('font', size=20)\nplt.rc('axes', titlesize=20)\n\n# This plot gives a nice view of the phrase \"Women and Children first!\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's see how port of embarkation and fare correlate\nplt.figure(figsize=(12,8))\n\n# sns plot would add a default legend of 0 or 1 for Survived column, so change it to \"Died\" and \"Survived\"\ncustom = [Line2D([], [], marker='o', color='#023EFF', linestyle='None'),\n          Line2D([], [], marker='o', color='#FF7C00', linestyle='None')]\nsns.swarmplot(x=\"Fare\", y=\"Embarked\", hue=\"Survived\", data=train_df, palette=\"bright\")\nax = sns.boxplot(x=\"Fare\", y=\"Embarked\", data=train_df, color='white')\n\n# Making the boxplot edge colors black for better contrast\nfor i,box in enumerate(ax.artists):\n    box.set_edgecolor('black')\n    box.set_facecolor('white')\n    # iterate over whiskers and median lines\n    for j in range(6*i,6*(i+1)):\n         ax.lines[j].set_color('black')\n            \n# Setting the text for legend, overall font size, and title of figure            \nplt.legend(custom, ['Died', 'Survived'], loc='upper right')\nplt.rc('font', size=20)\nplt.rc('axes', titlesize=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### For more interactive plots, \"*plotpy*\" express library provides quick and simple visualization. A 2D scatter plot may also contain built in histograms or box plots. By clicking on the legend marker groups, you can select to display only certain results. Hovering over individual points yields the full parameters for each point.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Survived\"] = train_df[\"Survived\"].astype(str)\nfig = px.scatter(train_df, x=\"Age\", y=\"Fare\", color=\"Survived\",hover_data=['SibSp','Parch','Pclass','Embarked'],\n                 marginal_x=\"box\", marginal_y=\"box\" ,title=\"Box plots of Age and Fare\",hover_name='Name')\nfig.update_layout(legend_title_text='Survived? 0=No, 1=Yes')\nfig","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How does the passenger distribution vary for the three Ticket Classes (1st, 2nd, 3rd)? Pull slider to find out! The colors help distinguish port of embarkation. (Survived 0=No, 1=Yes)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.sort_values(by=['Pclass'])\nfig = px.scatter(train_df, x=\"Age\", y=\"Fare\",animation_frame=\"Pclass\",\n                 color=\"Embarked\", hover_name=\"Name\", facet_col=\"Survived\",\n           title=\"Scatter Plots of Age and Fare\",log_x=False, size_max=45)\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Can we visualize the data in 3d? This would make more sense with other types of data, but we can try with Titanic passengers. Try moving the 3d plot around with your mouse. \n\n### Here, the size of the data point is total family members (siblings + spouse + children + parents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Survived\"] = train_df[\"Survived\"].astype(str)\ntrain_df = train_df.sort_values(by=['Survived'])\nfig = px.scatter_3d(train_df, x='Age', y='Fare', z='Pclass', size = train_df['SibSp']+train_df['Parch'],\n              color='Survived', size_max=40,\n              symbol='Sex', opacity=0.8,hover_name=\"Name\",hover_data=['SibSp','Parch','Embarked'])\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### How can we see number of family members of the passengers? Here, z axis is total family members (siblings + spouse + children + parents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 3D plot based on Age, Fare, and Family size (siblings+spouse+children+parents in z)\ntrain_df[\"Survived\"] = train_df[\"Survived\"].astype(str)\ntrain_df = train_df.sort_values(by=['Survived'])\n\nfig = px.scatter_3d(train_df, x='Age', y='Fare', z=train_df['SibSp']+train_df['Parch'],\n              color='Survived', size_max=10,\n              symbol='Sex', opacity=0.7,hover_name=\"Name\",hover_data=['SibSp','Parch','Embarked'])\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation of ML Models <a class=\"anchor\" id=\"third-section\"></a>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Before we try various models, the data needs some additional pre-processing. Specifically, we should covert categorical features to numerical values (Sex, Embarked).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Sex'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\ntrain_df['Embarked'].replace(to_replace=['C','Q', 'S'], value=[0,1,2],inplace=True)\n\ntest_df['Sex'].replace(to_replace=['male','female'], value=[0,1],inplace=True)\ntest_df['Embarked'].replace(to_replace=['C','Q', 'S'], value=[0,1,2],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature (X) and label (y) selection. Then normalization of data to give data zero mean and unit variance. X_submit is the data for final submission. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train_df\ny = train_df['Survived'].values\nX.drop(['Name'],axis = 1, inplace = True)\nX.drop(['Survived'],axis = 1, inplace = True)\n\nX_submit = test_df\nX_submit.drop(['Name'],axis = 1, inplace = True)\n\nX = preprocessing.StandardScaler().fit(X).transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# From sklearn.tree import DecisionTreeClassifier\nDT_model = DecisionTreeClassifier(criterion=\"entropy\")\nDT_model.fit(X_train,y_train)\n\nDT_yhat = DT_model.predict(X_test)\n\nprint(\"DT accuracy: %.2f\" % accuracy_score(y_test, DT_yhat))\nprint(\"DT Jaccard index: %.2f\" % jaccard_score(y_test, DT_yhat,pos_label='1'))\nprint(\"DT F1-score: %.2f\" % f1_score(y_test, DT_yhat, average='weighted') )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"RanFor_model = RandomForestClassifier(n_estimators=10,random_state=1).fit(X_train,y_train)\nRanFor_yhat = RanFor_model.predict(X_test)\n\nprint(\"Random Forest accuracy: %.2f\" % accuracy_score(y_test, RanFor_yhat))\nprint(\"Random Forest Jaccard index: %.2f\" % jaccard_score(y_test, RanFor_yhat,pos_label='1'))\nprint(\"Random Forest F1-score: %.2f\" % f1_score(y_test, RanFor_yhat, average='weighted') )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# From sklearn import svm.SVC()\nSVM_model = SVC()\nSVM_model.fit(X_train,y_train)\n\nSVM_yhat = SVM_model.predict(X_test)\n\nprint(\"SVM accuracy: %.2f\" % accuracy_score(y_test, SVM_yhat))\nprint(\"SVM Jaccard index: %.2f\" % jaccard_score(y_test, SVM_yhat,pos_label='1'))\nprint(\"SVM F1-score: %.2f\" % f1_score(y_test, SVM_yhat, average='weighted') )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"LR_model = LogisticRegression(C=0.01).fit(X_train,y_train)\nLR_yhat = LR_model.predict(X_test)\n\nprint(\"LR accuracy: %.2f\" % accuracy_score(y_test, LR_yhat))\nprint(\"LR Jaccard index: %.2f\" % jaccard_score(y_test, LR_yhat,pos_label='1'))\nprint(\"LR F1-score: %.2f\" % f1_score(y_test, LR_yhat, average='weighted') )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bernoulli Naive Bayes","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"NB_model = BernoulliNB(2).fit(X_train,y_train)\nNB_yhat = NB_model.predict(X_test)\n\nprint(\"NB accuracy: %.2f\" % accuracy_score(y_test, NB_yhat))\nprint(\"NB Jaccard index: %.2f\" % jaccard_score(y_test, NB_yhat,pos_label='1'))\nprint(\"NB F1-score: %.2f\" % f1_score(y_test, NB_yhat, average='weighted') )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB_model=XGBClassifier(max_depth=3).fit(X_train,y_train)\nXGB_pred=XGB_model.predict(X_test)\n    \nprint(\"XGB accuracy: %.2f\" % accuracy_score(y_test, XGB_pred))\nprint(\"XGB Jaccard index: %.2f\" % jaccard_score(y_test, XGB_pred,pos_label='1'))\nprint(\"XGB F1-score: %.2f\" % f1_score(y_test, XGB_pred, average='weighted') )    \n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model comparison:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://imgs.xkcd.com/comics/machine_learning.png\">","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from tabulate import tabulate\ndata = [['Decision Tree', accuracy_score(y_test, DT_yhat), jaccard_score(y_test, DT_yhat,pos_label='1'), f1_score(y_test, DT_yhat, average='weighted')],\n['Random Forest Classifier', accuracy_score(y_test, RanFor_yhat), jaccard_score(y_test, RanFor_yhat,pos_label='1'), f1_score(y_test, RanFor_yhat, average='weighted')],\n['Support Vector Machine', accuracy_score(y_test, SVM_yhat), jaccard_score(y_test, SVM_yhat,pos_label='1'), f1_score(y_test, SVM_yhat, average='weighted')],\n['Logistic Regression', accuracy_score(y_test, LR_yhat), jaccard_score(y_test, LR_yhat,pos_label='1'), f1_score(y_test, LR_yhat, average='weighted')],\n['Bernoulli Naive_Bayes', accuracy_score(y_test, NB_yhat), jaccard_score(y_test, NB_yhat,pos_label='1'), f1_score(y_test, NB_yhat, average='weighted')],\n['XGB Classifier', accuracy_score(y_test, XGB_pred), jaccard_score(y_test, XGB_pred,pos_label='1'), f1_score(y_test, XGB_pred, average='weighted')]]\nprint (tabulate(data, headers=[\"Model\", \"Accuracy\", \"Jaccard score\", \"F1-Score\"]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Highest accuracy and F1-score were obtained from Random Forest and XGB Classifier.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Submission <a class=\"anchor\" id=\"fourth-section\"></a>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction_submit = RanFor_model.predict(X_submit)\n\ntestdata = pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\noutput = pd.DataFrame({'PassengerId': testdata.PassengerId, 'Survived': prediction_submit})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### *Note that additional data and correlations may have been extracted from the Name column by parsing the title, first and last name. For example, the \"Mrs.\" and \"Miss\" title would imply a woman being married or unmarried. However, I did not go into such detail.\n    ","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}