{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pycaret\n\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn.preprocessing as preprocessing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-16T09:09:19.198183Z","iopub.execute_input":"2022-03-16T09:09:19.199281Z","iopub.status.idle":"2022-03-16T09:09:31.476839Z","shell.execute_reply.started":"2022-03-16T09:09:19.19918Z","shell.execute_reply":"2022-03-16T09:09:31.476002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nWhile working on another experimental [notebook](https://www.kaggle.com/taranmarley/automl-from-scratch-1), I realised I have never taken a real attempt at the original Titanic dataset. This surprises me because it is a fun and approachable dataset. Let's see what we get. ","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering\n\nThe aim here is to examine the features and then see if we can change everything to numerical so that it can be easily understood by machine learning algorithms.","metadata":{}},{"cell_type":"markdown","source":"**Load Data**","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\ndf = pd.read_csv(\"../input/titanic/train.csv\")\ntest_df = pd.read_csv(\"../input/titanic/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:31.4789Z","iopub.execute_input":"2022-03-16T09:09:31.479221Z","iopub.status.idle":"2022-03-16T09:09:31.498747Z","shell.execute_reply.started":"2022-03-16T09:09:31.479176Z","shell.execute_reply":"2022-03-16T09:09:31.497909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Look at Datatypes**\n\nI've highlighted the objects in red to indicate they are the ones we need to work on.","metadata":{}},{"cell_type":"code","source":"from termcolor import colored\nfor idx, item in df.dtypes.iteritems():\n    if item == \"object\":\n        print(\"{:<15}\".format(idx), colored(item, \"red\"))\n    else:\n        print(\"{:<15}\".format(idx), colored(item, \"green\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:31.500316Z","iopub.execute_input":"2022-03-16T09:09:31.500729Z","iopub.status.idle":"2022-03-16T09:09:31.511512Z","shell.execute_reply.started":"2022-03-16T09:09:31.500681Z","shell.execute_reply":"2022-03-16T09:09:31.510568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Detect NaNs in Dataset**\n\nNaN or null values can't be comprehended by most machine learning methods, so it is important to detect them.","metadata":{}},{"cell_type":"code","source":"def detect_NaNs(df_temp): \n    print('NaNs in data: ', df_temp.isnull().sum().sum())\n    print('******')\n    count_nulls = df_temp.isnull().sum().sum()\n    if count_nulls > 0:\n        for col in df_temp.columns:\n            print('NaNs in', col + \": \", df_temp[col].isnull().sum().sum())\n    print('******')\n    print('')\ndetect_NaNs(df)\ndetect_NaNs(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:31.514413Z","iopub.execute_input":"2022-03-16T09:09:31.514721Z","iopub.status.idle":"2022-03-16T09:09:31.546963Z","shell.execute_reply.started":"2022-03-16T09:09:31.514679Z","shell.execute_reply":"2022-03-16T09:09:31.5461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot NaNs**","metadata":{}},{"cell_type":"code","source":"ax = sns.barplot(x=df[[\"Age\",\"Cabin\",\"Embarked\"]].isnull().sum().index, y=df[[\"Age\",\"Cabin\",\"Embarked\"]].isnull().sum().values)\nax.set_title(\"Training Data\")\nplt.show()\nax2 = sns.barplot(x=df[[\"Age\",\"Fare\",\"Cabin\"]].isnull().sum().index, y=df[[\"Age\",\"Fare\",\"Cabin\"]].isnull().sum().values)\nax2.set_title(\"Test Data\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:31.548034Z","iopub.execute_input":"2022-03-16T09:09:31.548275Z","iopub.status.idle":"2022-03-16T09:09:31.969519Z","shell.execute_reply.started":"2022-03-16T09:09:31.548246Z","shell.execute_reply":"2022-03-16T09:09:31.968648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Fill in NaNs**\n\nI will fill the NaN values and creates a new column that records they were NaNsfor col in columns:\n        df_temp[col + \"_was_null\"] = df_temp[col].isnull().astype(int)\n        df_temp[col] = df_temp[col].fillna(value)","metadata":{}},{"cell_type":"code","source":"for col in df.columns:\n    if df[col].isnull().values.any(): \n        df[col + \"_was_null\"] = df[col].isnull().astype(int)\n        df[col] = df[col].fillna(0)\n        \nfor col in test_df.columns:\n    if test_df[col].isnull().values.any(): \n        test_df[col + \"_was_null\"] = test_df[col].isnull().astype(int)\n        test_df[col] = df[col].fillna(0)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:31.971086Z","iopub.execute_input":"2022-03-16T09:09:31.971417Z","iopub.status.idle":"2022-03-16T09:09:32.008281Z","shell.execute_reply.started":"2022-03-16T09:09:31.971384Z","shell.execute_reply":"2022-03-16T09:09:32.007388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's look at Cabin**","metadata":{}},{"cell_type":"code","source":"df[\"Cabin\"].unique()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.009518Z","iopub.execute_input":"2022-03-16T09:09:32.010151Z","iopub.status.idle":"2022-03-16T09:09:32.015759Z","shell.execute_reply.started":"2022-03-16T09:09:32.010116Z","shell.execute_reply":"2022-03-16T09:09:32.015238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first thing that jumps to mind is that the first letter can be extracted out of this. ","metadata":{}},{"cell_type":"code","source":"df[\"Cabin_First_Letter\"] = df[\"Cabin\"].str[:1]\ndf[\"Cabin_First_Letter\"] = df[\"Cabin_First_Letter\"].fillna(0)\ntest_df[\"Cabin_First_Letter\"] = test_df[\"Cabin\"].str[:1]\ntest_df[\"Cabin_First_Letter\"] = test_df[\"Cabin_First_Letter\"].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.01691Z","iopub.execute_input":"2022-03-16T09:09:32.017135Z","iopub.status.idle":"2022-03-16T09:09:32.034661Z","shell.execute_reply.started":"2022-03-16T09:09:32.017104Z","shell.execute_reply":"2022-03-16T09:09:32.033916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.035771Z","iopub.execute_input":"2022-03-16T09:09:32.036017Z","iopub.status.idle":"2022-03-16T09:09:32.053177Z","shell.execute_reply.started":"2022-03-16T09:09:32.035987Z","shell.execute_reply":"2022-03-16T09:09:32.052491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's look at the ticket**","metadata":{}},{"cell_type":"code","source":"df[\"Ticket\"].unique()[:20]","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.056305Z","iopub.execute_input":"2022-03-16T09:09:32.056748Z","iopub.status.idle":"2022-03-16T09:09:32.06447Z","shell.execute_reply.started":"2022-03-16T09:09:32.056715Z","shell.execute_reply":"2022-03-16T09:09:32.063745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pretty clear we would want to break out the first word","metadata":{}},{"cell_type":"code","source":"df[\"Ticket_First\"] = df.Ticket.str.split().str.get(0)\ntest_df[\"Ticket_First\"] = test_df.Ticket.str.split().str.get(0)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.065738Z","iopub.execute_input":"2022-03-16T09:09:32.066426Z","iopub.status.idle":"2022-03-16T09:09:32.077493Z","shell.execute_reply.started":"2022-03-16T09:09:32.066391Z","shell.execute_reply":"2022-03-16T09:09:32.076575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This could also be helpful in consideration of the name where the last name might imply family relations","metadata":{}},{"cell_type":"code","source":"df[\"Name_First\"] = df.Name.str.split().str.get(0)\ntest_df[\"Name_First\"] = test_df.Name.str.split().str.get(0)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.079239Z","iopub.execute_input":"2022-03-16T09:09:32.079603Z","iopub.status.idle":"2022-03-16T09:09:32.090728Z","shell.execute_reply.started":"2022-03-16T09:09:32.079573Z","shell.execute_reply":"2022-03-16T09:09:32.089697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Interactions**\n\nWe can create some interactions between various columns to generate new data that may help a machine learning method","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.091949Z","iopub.execute_input":"2022-03-16T09:09:32.092538Z","iopub.status.idle":"2022-03-16T09:09:32.113021Z","shell.execute_reply.started":"2022-03-16T09:09:32.092502Z","shell.execute_reply":"2022-03-16T09:09:32.111793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"PclassXSibSp\"] = df[\"Pclass\"] * df[\"SibSp\"]\ntest_df[\"PclassXSibSp\"] = test_df[\"Pclass\"] * test_df[\"SibSp\"]\ndf[\"AgeXSibSp\"] = df[\"Age\"] * df[\"SibSp\"]\ntest_df[\"AgeXSibSp\"] = test_df[\"Age\"] * test_df[\"SibSp\"]\ndf[\"AgeXFare\"] = df[\"Age\"] * df[\"Fare\"]\ntest_df[\"AgeXFare\"] = test_df[\"Age\"] * test_df[\"Fare\"]\ndf[\"PclassXFare\"] = df[\"Pclass\"] * df[\"Fare\"]\ntest_df[\"PclassXFare\"] = test_df[\"Pclass\"] * test_df[\"Fare\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.114419Z","iopub.execute_input":"2022-03-16T09:09:32.114672Z","iopub.status.idle":"2022-03-16T09:09:32.129788Z","shell.execute_reply.started":"2022-03-16T09:09:32.11464Z","shell.execute_reply":"2022-03-16T09:09:32.128699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Check for id columns**\n\nIf a column has a unique value for every row we should delete it because that is of little predictive value for use and would get used by a machine learning algorithm as a cheat.","metadata":{}},{"cell_type":"code","source":"for col in df.columns:\n    if len(df[col]) == len(df[col].unique()):\n        df.drop(columns=col, inplace=True)\n        test_df.drop(columns=col, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.131395Z","iopub.execute_input":"2022-03-16T09:09:32.131652Z","iopub.status.idle":"2022-03-16T09:09:32.149479Z","shell.execute_reply.started":"2022-03-16T09:09:32.131621Z","shell.execute_reply":"2022-03-16T09:09:32.148663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Encode Columns**","metadata":{}},{"cell_type":"code","source":"def encode_columns(df, columns, test_df = None):\n    for col in columns:\n        le = preprocessing.LabelEncoder()\n        classes_to_encode = df[col].astype(str).unique().tolist()\n        classes_to_encode.sort()\n        classes_to_encode.append('None')\n        le.fit(classes_to_encode)\n        if len(le.classes_) < 12:\n            df = pd.get_dummies(df, columns = [col])\n            if test_df is not None:\n                test_df = pd.get_dummies(test_df, columns = [col])\n        else:\n            check_col = df.copy()[col]\n            df[col] = le.transform(df[col].astype(str))\n            if test_df is not None:\n                #Clean out unseen labels\n                inputs = []\n                for idx, row in test_df.iterrows():\n                    if row[col] in pd.unique(check_col):\n                        inputs.append(row[col])\n                    else:\n                        inputs.append('None')\n                test_df[col] = inputs\n                test_df[col] = le.transform(test_df[col].astype(str))\n    return df, test_df\n#encode_columns(df, [\"HomePlanet\", \"CryoSleep\", \"Destination\", \"VIP\", \"Name\", \"letters\", \"final_letters\"], test_df)\ndf, test_df = encode_columns(df, [\"Sex\", \"Ticket\",\"Embarked\",\"Cabin\", \"Cabin_First_Letter\", \"Ticket_First\", \"Name_First\"], test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.152413Z","iopub.execute_input":"2022-03-16T09:09:32.154422Z","iopub.status.idle":"2022-03-16T09:09:32.655134Z","shell.execute_reply.started":"2022-03-16T09:09:32.154368Z","shell.execute_reply":"2022-03-16T09:09:32.654365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check this has gotten rid of any objects:","metadata":{}},{"cell_type":"code","source":"from termcolor import colored\nfor idx, item in df.dtypes.iteritems():\n    if item == \"object\":\n        print(\"{:<15}\".format(idx), colored(item, \"red\"))\n    else:\n        print(\"{:<15}\".format(idx), colored(item, \"green\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.657459Z","iopub.execute_input":"2022-03-16T09:09:32.657793Z","iopub.status.idle":"2022-03-16T09:09:32.672947Z","shell.execute_reply.started":"2022-03-16T09:09:32.657738Z","shell.execute_reply":"2022-03-16T09:09:32.66901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All green is great! Now we need to check that the columns are matched between test and training data by removing those that aren't.","metadata":{}},{"cell_type":"code","source":"for col in df.columns:\n    if col not in test_df.columns:\n        if col != \"Survived\":\n            df.drop(columns=col, axis=1, inplace=True)\nfor col in test_df.columns:\n    if col not in df.columns:\n        if col  != \"Survived\":\n            test_df.drop(columns=col, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.674191Z","iopub.execute_input":"2022-03-16T09:09:32.674434Z","iopub.status.idle":"2022-03-16T09:09:32.687016Z","shell.execute_reply.started":"2022-03-16T09:09:32.674404Z","shell.execute_reply":"2022-03-16T09:09:32.686262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Search for Anomolies**\n\nWe can look for anomalies and also add this information to the dataset. ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\n\nX = df.copy().drop(columns=\"Survived\")\ntest_X = test_df.copy()\niforest = IsolationForest(random_state=0).fit(X)\ndf[\"anomaly\"] = iforest.predict(X)\ndf[\"anomaly_score\"] = iforest.score_samples(X)\ntest_df[\"anomaly\"] = iforest.predict(test_X)\ntest_df[\"anomaly_score\"] = iforest.score_samples(test_X)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:32.688223Z","iopub.execute_input":"2022-03-16T09:09:32.688641Z","iopub.status.idle":"2022-03-16T09:09:33.190116Z","shell.execute_reply.started":"2022-03-16T09:09:32.688587Z","shell.execute_reply":"2022-03-16T09:09:33.189481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"anomaly\"] = df[\"anomaly\"].replace({-1:0})\ntest_df[\"anomaly\"] = df[\"anomaly\"].replace({-1:0})","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:33.191433Z","iopub.execute_input":"2022-03-16T09:09:33.191858Z","iopub.status.idle":"2022-03-16T09:09:33.197891Z","shell.execute_reply.started":"2022-03-16T09:09:33.191824Z","shell.execute_reply":"2022-03-16T09:09:33.197186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from yellowbrick.features import PCA as yellowPCA\n\ny = df[\"anomaly\"]\nX = df.drop(columns=[\"anomaly\"]).replace({-1:0})\n\nvisualizer = yellowPCA(scale=True, projection=2, alpha=0.4)\nvisualizer.fit_transform(X, y)\nvisualizer.show()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:33.199341Z","iopub.execute_input":"2022-03-16T09:09:33.199589Z","iopub.status.idle":"2022-03-16T09:09:33.569152Z","shell.execute_reply.started":"2022-03-16T09:09:33.199548Z","shell.execute_reply":"2022-03-16T09:09:33.568551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Add PCA Features**\n\nThe above graph shows that PCA with anomaly generates some clusters and that could be used to classify. Below I will combine test and training dataframes to do a pca on both. ","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf_temp = pd.concat([df.copy(), test_df.copy()], ignore_index=True)\ny = df_temp[\"Survived\"]\nX = df_temp.drop(columns=\"Survived\", axis=1)\nX_scaled = MinMaxScaler().fit_transform(X)\npca = PCA(n_components=3)\nX_p = pca.fit(X_scaled).transform(X_scaled)\ndf[\"PCA_0\"] = X_p[:891,0]\ndf[\"PCA_1\"] = X_p[:891,1]\ndf[\"PCA_2\"] = X_p[:891,2]\ntest_df[\"PCA_0\"] = X_p[891:,0]\ntest_df[\"PCA_1\"] = X_p[891:,1]\ntest_df[\"PCA_2\"] = X_p[891:,2]","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:33.570267Z","iopub.execute_input":"2022-03-16T09:09:33.570969Z","iopub.status.idle":"2022-03-16T09:09:33.631289Z","shell.execute_reply.started":"2022-03-16T09:09:33.570935Z","shell.execute_reply":"2022-03-16T09:09:33.630322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\n\n**Let's look at the Target First**","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=df[\"Survived\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:33.633661Z","iopub.execute_input":"2022-03-16T09:09:33.635496Z","iopub.status.idle":"2022-03-16T09:09:33.890085Z","shell.execute_reply.started":"2022-03-16T09:09:33.635441Z","shell.execute_reply":"2022-03-16T09:09:33.889451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plot Data**","metadata":{}},{"cell_type":"code","source":"pltdf = df.copy()\npltdf = pltdf.sample(frac=1, random_state=42).reset_index(drop=True)\npltdf.iloc[:50, :16].plot(subplots=True, layout=(7,4), figsize=(15,10))\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:33.891366Z","iopub.execute_input":"2022-03-16T09:09:33.891803Z","iopub.status.idle":"2022-03-16T09:09:37.082518Z","shell.execute_reply.started":"2022-03-16T09:09:33.891759Z","shell.execute_reply":"2022-03-16T09:09:37.081797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pivot Table**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\ndef quantile_column_wise(df_temp, target_col = \"\"):\n    df_temp = df_temp.copy()\n    for col in df_temp.columns:\n        if col != target_col:\n            df_temp[col] = QuantileTransformer(n_quantiles=500).fit_transform(df_temp[col].values.reshape(-1, 1))\n    return df_temp\n\ndef pivot_table(df_temp, target_col):\n    df_temp = df_temp.copy()\n    y = df_temp[target_col]\n    X = df_temp.drop(columns=target_col, axis=1)\n    df_temp = pd.DataFrame(X)  \n    df_temp.columns = X.columns\n    df_temp[target_col] = y\n    table = pd.pivot_table(data=df_temp,index=[target_col]).T\n    sns.heatmap(table, annot=True, cmap=\"Blues\")\n    return table","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-03-16T09:09:37.083895Z","iopub.execute_input":"2022-03-16T09:09:37.08435Z","iopub.status.idle":"2022-03-16T09:09:37.092674Z","shell.execute_reply.started":"2022-03-16T09:09:37.084316Z","shell.execute_reply":"2022-03-16T09:09:37.091854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,15))\ntable = pivot_table(quantile_column_wise(df, \"Survived\"), \"Survived\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:37.09409Z","iopub.execute_input":"2022-03-16T09:09:37.094339Z","iopub.status.idle":"2022-03-16T09:09:37.979217Z","shell.execute_reply.started":"2022-03-16T09:09:37.094312Z","shell.execute_reply":"2022-03-16T09:09:37.97844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Gender is very visible in this as a difference.\n\n**See Important Correlations**\n\nI will generate a heatmap below with the more important data correlations","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\n\ndef calculate_correlations(df_temp, target_col, ratio, verbose=1):\n    df_temp = df_temp.copy()\n    cols = []\n    cols_done = []\n    if df_temp[target_col].dtype == object:\n        le = preprocessing.LabelEncoder()\n        df_temp[target_col] = le.fit_transform(df_temp[target_col])\n    df_temp[target_col] = MinMaxScaler().fit_transform(df_temp[target_col].values.reshape(-1, 1))\n    if verbose == 1:\n        print(\"Correlations with\",target_col + \":\")\n    for col_one in df_temp.iloc[:,:].columns:\n        correlation_value =  abs(df_temp[col_one].corr(df_temp[target_col]))\n        if verbose == 1:\n            print(col_one, \":\", df_temp[col_one].corr(df_temp[target_col]))\n        if correlation_value > ratio:\n            cols.append(col_one)\n        cols_done.append(col_one)\n    corrdf = df_temp.copy()\n    corrdf = corrdf[cols].corr()\n    sns.heatmap(abs(corrdf), cmap=\"Blues\")\n    return cols","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:37.980596Z","iopub.execute_input":"2022-03-16T09:09:37.980807Z","iopub.status.idle":"2022-03-16T09:09:37.989991Z","shell.execute_reply.started":"2022-03-16T09:09:37.98078Z","shell.execute_reply":"2022-03-16T09:09:37.989199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correlation_cols = calculate_correlations(df.drop(columns=\"Sex_female\"), \"Survived\", 0.2, 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:37.99413Z","iopub.execute_input":"2022-03-16T09:09:37.994395Z","iopub.status.idle":"2022-03-16T09:09:38.477101Z","shell.execute_reply.started":"2022-03-16T09:09:37.994364Z","shell.execute_reply":"2022-03-16T09:09:38.4762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot Bar Plots","metadata":{}},{"cell_type":"code","source":"def bar_plots(df, columns_to_plot, target_col):\n    df_temp = df.copy()\n    fig, axs = plt.subplots(len(columns_to_plot), 1, figsize=(10, 15))\n    i = 0 \n    for col in columns_to_plot:\n        sns.barplot(ax=axs[i], x=target_col, y=col, data=df)# .set_title(col + \" X \" + target_col)\n        axs[i].set_title = \"test\"\n        i = i + 1\n    # fig.subplots_adjust(hspace=0.4)\nbar_plots(df, [\"Sex_male\", \"Pclass\", \"Cabin\", \"AgeXFare\"], \"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:38.478186Z","iopub.execute_input":"2022-03-16T09:09:38.478864Z","iopub.status.idle":"2022-03-16T09:09:39.399876Z","shell.execute_reply.started":"2022-03-16T09:09:38.478817Z","shell.execute_reply":"2022-03-16T09:09:39.398892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pair Grid","metadata":{}},{"cell_type":"code","source":"def pair_grid_plot(df, cols):\n    g = sns.PairGrid(df[cols].iloc[:500,:], diag_sharey=False)\n    g.map_upper(sns.scatterplot, s=15)\n    g.map_lower(sns.kdeplot)\n    g.map_diag(sns.kdeplot, lw=2)\n    \npair_grid_plot(df, [\"Sex_male\", \"Pclass\", \"Cabin\", \"AgeXFare\", \"Survived\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:39.401227Z","iopub.execute_input":"2022-03-16T09:09:39.401594Z","iopub.status.idle":"2022-03-16T09:09:47.228707Z","shell.execute_reply.started":"2022-03-16T09:09:39.401555Z","shell.execute_reply":"2022-03-16T09:09:47.227864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dimensionality Reduction","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ndef pca_dimension_reduction_info(df_temp, target_col):\n    df_temp = df_temp.copy()\n    y = df_temp[target_col]\n    X = df_temp.drop(columns=target_col, axis=1)\n    X_scaled = MinMaxScaler().fit_transform(X)\n    print(str(len(X_scaled[0])) + \" initial feature components\")\n    pca = PCA(n_components=0.95)\n    X_p = pca.fit(X_scaled).transform(X_scaled)\n    print(\"95% variance explained by \" + str(len(X_p[0])) + \" components by principle component analysis\")\n    pca = PCA(n_components=3)\n    X_p = pca.fit(X_scaled).transform(X_scaled)\n    print(str(round(pca.explained_variance_ratio_.sum() * 100)) + \"% variance explained by 3 components by principle component analysis\")\n    pca = PCA(n_components=2)\n    X_p = pca.fit(X_scaled).transform(X_scaled)\n    print(str(round(pca.explained_variance_ratio_.sum() * 100)) + \"% variance explained by 2 components by principle component analysis\")\n\npca_dimension_reduction_info(df.drop(columns=[\"PCA_0\",\"PCA_1\",\"PCA_2\"]), \"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:47.230167Z","iopub.execute_input":"2022-03-16T09:09:47.231419Z","iopub.status.idle":"2022-03-16T09:09:47.310264Z","shell.execute_reply.started":"2022-03-16T09:09:47.231363Z","shell.execute_reply":"2022-03-16T09:09:47.30936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from yellowbrick.features import PCA as yellowPCA\n\ny = df[\"Survived\"]\nX = df.drop(columns=[\"Survived\",\"PCA_0\",\"PCA_1\",\"PCA_2\"])\n\nvisualizer = yellowPCA(scale=True, projection=2, alpha=0.4)\nvisualizer.fit_transform(X, y)\nvisualizer.show()\nvisualizer = yellowPCA(scale=True, projection=3, alpha=0.4, size=(700,700))\nvisualizer.fit_transform(X, y)\nvisualizer.show()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:47.311908Z","iopub.execute_input":"2022-03-16T09:09:47.313124Z","iopub.status.idle":"2022-03-16T09:09:48.168792Z","shell.execute_reply.started":"2022-03-16T09:09:47.313068Z","shell.execute_reply":"2022-03-16T09:09:48.167808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decision Tree\n\nSimple decision trees can be easily interpreted for knowledge about the data","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nimport graphviz\n\ndef decision_tree(df_temp, depth, target_col):\n    tree_set = df_temp.copy()\n    target = tree_set[target_col]\n    tree_set.drop([target_col], axis=1, inplace=True)\n    tree_clf = DecisionTreeClassifier(max_depth=depth, random_state=1)\n    tree_clf.fit(tree_set, target)\n    text_representation = tree.export_text(tree_clf, feature_names=tree_set.columns.tolist())\n    print(\"accuracy: \" + str(tree_clf.score(tree_set, target)))    \n    plt.figure(figsize=(18,18))\n    # tree.plot_tree(tree_clf, feature_names=tree_set.columns, filled=True)\n    class_column_values = df_temp[target_col].values.ravel()\n    class_unique_values = pd.unique(class_column_values)\n    class_unique_values = np.sort(class_unique_values)\n    class_unique_values = class_unique_values.astype('str')\n    le = preprocessing.LabelEncoder()\n    target = le.fit_transform(target)\n    dot_data = tree.export_graphviz(tree_clf, out_file=None, \n                                    feature_names=tree_set.columns,  \n                                    class_names=class_unique_values,\n                                    filled=True)\n    display(graphviz.Source(dot_data, format=\"png\")) ","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:48.170321Z","iopub.execute_input":"2022-03-16T09:09:48.170873Z","iopub.status.idle":"2022-03-16T09:09:48.191982Z","shell.execute_reply.started":"2022-03-16T09:09:48.170828Z","shell.execute_reply":"2022-03-16T09:09:48.191325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"decision_tree(df, 3, \"Survived\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:09:48.193191Z","iopub.execute_input":"2022-03-16T09:09:48.193909Z","iopub.status.idle":"2022-03-16T09:09:49.104483Z","shell.execute_reply.started":"2022-03-16T09:09:48.193855Z","shell.execute_reply":"2022-03-16T09:09:49.10332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow 82% accuracy from 3 splits.","metadata":{}},{"cell_type":"markdown","source":"# Pycaret\n\nI will go through here and determine the best model to classify on this dataset. I may exclude some models to keep the time to run this notebook on kaggle under control.","metadata":{}},{"cell_type":"markdown","source":"I will remove some columns here that in testing just weren't helpful:","metadata":{}},{"cell_type":"code","source":"df = df.drop(columns=\"Name_First\")\ntest_df = test_df.drop(columns=\"Name_First\")","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:29:00.997178Z","iopub.execute_input":"2022-03-16T09:29:00.997513Z","iopub.status.idle":"2022-03-16T09:29:01.006893Z","shell.execute_reply.started":"2022-03-16T09:29:00.997479Z","shell.execute_reply":"2022-03-16T09:29:01.005905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pycaret.classification import *\nfrom sklearn import preprocessing\n\nsetup(data = df.copy(), \n             target = \"Survived\",\n             silent = True, normalize = True, session_id=1, data_split_stratify=True)\ndisplay()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:16:37.638381Z","iopub.execute_input":"2022-03-16T09:16:37.639282Z","iopub.status.idle":"2022-03-16T09:16:39.330173Z","shell.execute_reply.started":"2022-03-16T09:16:37.639196Z","shell.execute_reply":"2022-03-16T09:16:39.328973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top3 = compare_models(n_select=3, exclude=[\"xgboost\",\"catboost\",\"gbc\"])","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:19:28.413641Z","iopub.execute_input":"2022-03-16T09:19:28.413979Z","iopub.status.idle":"2022-03-16T09:19:48.906064Z","shell.execute_reply.started":"2022-03-16T09:19:28.413941Z","shell.execute_reply":"2022-03-16T09:19:48.904982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will blend the top 3 for a combined model","metadata":{}},{"cell_type":"code","source":"blend = blend_models(top3)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:21:11.580261Z","iopub.execute_input":"2022-03-16T09:21:11.580549Z","iopub.status.idle":"2022-03-16T09:21:15.009347Z","shell.execute_reply.started":"2022-03-16T09:21:11.580518Z","shell.execute_reply":"2022-03-16T09:21:15.008622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_blend = finalize_model(blend)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:22:18.696887Z","iopub.execute_input":"2022-03-16T09:22:18.697476Z","iopub.status.idle":"2022-03-16T09:22:22.457977Z","shell.execute_reply.started":"2022-03-16T09:22:18.697436Z","shell.execute_reply":"2022-03-16T09:22:22.456449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(final_blend, \"confusion_matrix\")","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:22:28.487942Z","iopub.execute_input":"2022-03-16T09:22:28.488284Z","iopub.status.idle":"2022-03-16T09:22:29.277977Z","shell.execute_reply.started":"2022-03-16T09:22:28.488248Z","shell.execute_reply":"2022-03-16T09:22:29.277078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(final_blend, \"error\")","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:22:41.528365Z","iopub.execute_input":"2022-03-16T09:22:41.528693Z","iopub.status.idle":"2022-03-16T09:22:42.425442Z","shell.execute_reply.started":"2022-03-16T09:22:41.528655Z","shell.execute_reply":"2022-03-16T09:22:42.423896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(final_blend, \"boundary\")","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:22:55.671348Z","iopub.execute_input":"2022-03-16T09:22:55.671668Z","iopub.status.idle":"2022-03-16T09:22:59.248172Z","shell.execute_reply.started":"2022-03-16T09:22:55.671636Z","shell.execute_reply":"2022-03-16T09:22:59.247231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission","metadata":{}},{"cell_type":"code","source":"predictions = predict_model(final_blend, data=test_df)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:23:39.447136Z","iopub.execute_input":"2022-03-16T09:23:39.447528Z","iopub.status.idle":"2022-03-16T09:23:40.124068Z","shell.execute_reply.started":"2022-03-16T09:23:39.447488Z","shell.execute_reply":"2022-03-16T09:23:40.122776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:23:56.719305Z","iopub.execute_input":"2022-03-16T09:23:56.719602Z","iopub.status.idle":"2022-03-16T09:23:56.749001Z","shell.execute_reply.started":"2022-03-16T09:23:56.71957Z","shell.execute_reply":"2022-03-16T09:23:56.748074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(\"../input/titanic/gender_submission.csv\")\nsubmission[\"Survived\"] = predictions[\"Label\"]\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-16T09:24:34.213192Z","iopub.execute_input":"2022-03-16T09:24:34.213514Z","iopub.status.idle":"2022-03-16T09:24:34.230891Z","shell.execute_reply.started":"2022-03-16T09:24:34.21348Z","shell.execute_reply":"2022-03-16T09:24:34.230157Z"},"trusted":true},"execution_count":null,"outputs":[]}]}