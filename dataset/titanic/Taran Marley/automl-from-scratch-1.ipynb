{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport typing","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-15T06:39:50.463174Z","iopub.execute_input":"2022-03-15T06:39:50.463789Z","iopub.status.idle":"2022-03-15T06:39:51.466877Z","shell.execute_reply.started":"2022-03-15T06:39:50.463697Z","shell.execute_reply":"2022-03-15T06:39:51.466062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction\n\nThe premise for this notebook is to work through how an AutoML system might be created. I intend to take this concept further in future notebooks and eventually create my own autoML release to tackle common tasks. This will be a learning experience for me and hopefully will be useful for you the reader.\n\nFor this notebook I will attempt to create a series of functions that are entirely generic and then apply them to the problem to make a submission. This is entirely different to the usual approach of creating specific solution to the problem at hand. It is my hope that these generic functions can eventually be placed into classes to form an automl learning pipeline.  \n\nThe from scratch in the title references that I won't be forking an existing AutoML model or copying and pasting blocks of code from one. \n\nFurther notebooks on the development of the RabbitML library are below: \n\n[AutoML from Scratch #1](https://www.kaggle.com/code/taranmarley/automl-from-scratch-1/notebook)\n\n[AutoML from Scratch #2](https://www.kaggle.com/code/taranmarley/automl-from-scratch-2/notebook)\n\n[AutoML from Scratch #3](https://www.kaggle.com/code/taranmarley/automl-from-scratch-3/notebook)\n\n[AutoML from Scratch #4](https://www.kaggle.com/code/taranmarley/automl-from-scratch-4/notebook)\n\n[AutoML from Scratch #5](https://www.kaggle.com/code/taranmarley/automl-from-scratch-5/notebook)\n\n[AutoML from Scratch #6](https://www.kaggle.com/code/taranmarley/automl-from-scratch-6/notebook)\n\n[AutoML from Scratch #7](https://www.kaggle.com/code/taranmarley/automl-from-scratch-7/notebook)\n\n[AutoML from Scratch #8](https://www.kaggle.com/code/taranmarley/automl-from-scratch-8/notebook)\n\n[AutoML from Scratch #9](https://www.kaggle.com/code/taranmarley/automl-from-scratch-9/notebook)","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/titanic/train.csv\")\ntest_df = pd.read_csv(\"../input/titanic/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.468345Z","iopub.execute_input":"2022-03-15T06:39:51.468545Z","iopub.status.idle":"2022-03-15T06:39:51.496158Z","shell.execute_reply.started":"2022-03-15T06:39:51.46852Z","shell.execute_reply":"2022-03-15T06:39:51.495478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.497356Z","iopub.execute_input":"2022-03-15T06:39:51.497702Z","iopub.status.idle":"2022-03-15T06:39:51.521546Z","shell.execute_reply.started":"2022-03-15T06:39:51.497669Z","shell.execute_reply":"2022-03-15T06:39:51.5207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Detect NaNs\n\nFirst let's create a generic function that gives us feedback on NaNs in the data. I will attempt to make a pythonic docstring since we will be using this later on.","metadata":{}},{"cell_type":"code","source":"def detect_NaNs(df_temp : pd.DataFrame, name = '', silent : bool = False, plot : bool = True):\n    \"\"\"\n    Detect NaNs in a provided dataframe and return the columns that NaNs were detected in     \n    \n    Parameters\n    ----------\n    df_temp : pd.DataFrame\n        Dataframe to detect NaN values in\n    name : str\n        Name of the dataframe which helps give a more descriptive read out\n    silent : bool\n        Whether the print statements should fire\n    plot : bool\n        Whether to return a plot of the counts of NaNs in the data\n    \n    Returns\n    -------\n    typing.List\n        List of columns in the provided dataframe that contain NaN values\n    \"\"\"\n    count_nulls = df_temp.isnull().sum().sum()\n    columns_with_NaNs = []\n    # Count NaNs by column\n    if count_nulls > 0:\n        for col in df_temp.columns:\n            if df_temp[col].isnull().sum().sum() > 0:\n                columns_with_NaNs.append(col)\n    # Print out the NaN values\n    if not silent:            \n        if name != '': \n            print('Detecting NaNs in', name)\n        print('NaNs in data:', count_nulls)\n        if count_nulls > 0:\n            print('******')\n            for col in columns_with_NaNs:\n                print('NaNs in', col + \": \", df_temp[col].isnull().sum().sum())\n            print('******')\n    print('')\n    # Plot the NaN values in columns in bar plot\n    if plot and count_nulls > 0:\n        sns.barplot(x=df_temp[columns_with_NaNs].isnull().sum().index, y=df_temp[columns_with_NaNs].isnull().sum().values)\n        plt.show()\n    return columns_with_NaNs","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.523167Z","iopub.execute_input":"2022-03-15T06:39:51.523885Z","iopub.status.idle":"2022-03-15T06:39:51.534408Z","shell.execute_reply.started":"2022-03-15T06:39:51.523846Z","shell.execute_reply":"2022-03-15T06:39:51.533711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's run this to check that it works as expected","metadata":{}},{"cell_type":"code","source":"nan_columns = detect_NaNs(df, \"Training Data\")","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.536141Z","iopub.execute_input":"2022-03-15T06:39:51.536875Z","iopub.status.idle":"2022-03-15T06:39:51.760491Z","shell.execute_reply.started":"2022-03-15T06:39:51.53684Z","shell.execute_reply":"2022-03-15T06:39:51.759593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fill NaNs in Dataframe\n\nNow I will create a function to fill in NaN values and return the dataframe with the filled values and new columns signifying if there was a NaN value there in case this is significant.","metadata":{}},{"cell_type":"code","source":"def fill_nans_create_columns(df_temp : pd.DataFrame, columns : typing.List, value : float = 0):\n    \"\"\"\n    Fill NaN of provided columns and create columns to signify they weren't there.\n    \n    Parameters\n    ----------\n    df_temp : pd.DataFrame\n        Dataframe to modify\n    columns : typing.List\n        Columns of the provided dataframe to modify\n    value : float\n        Value to replace the NaN values with\n    \n    Returns\n    -------\n    pd.DataFrame\n        Modified Dataframe with NaNs filled and new columns signifying the rows that contained NaNs\n    \"\"\"\n    for col in columns:\n        df_temp[col + \"_was_null\"] = df_temp[col].isnull().astype(int)\n        df_temp[col] = df_temp[col].fillna(value)\n    return(df_temp)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.762233Z","iopub.execute_input":"2022-03-15T06:39:51.762541Z","iopub.status.idle":"2022-03-15T06:39:51.769756Z","shell.execute_reply.started":"2022-03-15T06:39:51.762498Z","shell.execute_reply":"2022-03-15T06:39:51.76856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = fill_nans_create_columns(df, nan_columns, 0)\ntest_df = fill_nans_create_columns(test_df, nan_columns, 0)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.77135Z","iopub.execute_input":"2022-03-15T06:39:51.771979Z","iopub.status.idle":"2022-03-15T06:39:51.791992Z","shell.execute_reply.started":"2022-03-15T06:39:51.771932Z","shell.execute_reply":"2022-03-15T06:39:51.791296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Detect Duplicates\n\nNow I will look at detecting duplicates in the data with a generic function.","metadata":{}},{"cell_type":"code","source":"def detect_duplicates(df_temp : pd.DataFrame, silent : bool = False): \n    \"\"\"\n    Detect duplicates in data and return the columns in which duplicates where detected.\n    \n    Parameters\n    ----------\n    df_temp : pd.DataFrame\n        Dataframe to detect duplicates in\n    silent : bool\n        Whether to run print statements \n    \"\"\"\n    # Filter out identity columns\n    cols_to_use = []\n    id_cols = []\n    for col in df_temp.columns:\n        if len(df_temp[col].unique()) != len(df_temp[col]):\n            cols_to_use.append(col)\n        else:\n            id_cols.append(col)\n    df_temp = df_temp.copy()[cols_to_use]    \n    count_dupes = df_temp.duplicated().sum()\n    if not silent:\n        print('Duplicates in data: ', str(count_dupes))\n        print('When filtering out id columns: ', str(id_cols))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.793603Z","iopub.execute_input":"2022-03-15T06:39:51.793934Z","iopub.status.idle":"2022-03-15T06:39:51.80253Z","shell.execute_reply.started":"2022-03-15T06:39:51.793892Z","shell.execute_reply":"2022-03-15T06:39:51.801569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"detect_duplicates(df)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.803708Z","iopub.execute_input":"2022-03-15T06:39:51.804167Z","iopub.status.idle":"2022-03-15T06:39:51.82565Z","shell.execute_reply.started":"2022-03-15T06:39:51.804119Z","shell.execute_reply":"2022-03-15T06:39:51.824799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Break Up Object Columns by Spaces\n\nIt would be helpful it the object columns were broken up by the spaces in them. So that multiple columns are created.","metadata":{}},{"cell_type":"code","source":"def break_up_by_string(df_temp : pd.DataFrame, splitting_string : str):\n    \"\"\"\n    Break up columns by string to create new columns from each split.\n\n    Parameters\n    ----------\n    df_temp : pd.DataFrame\n        Dataframe to start splitting up object columns\n    splitting_string : str\n        String to split up columns by\n        \n        \n    Returns\n    -------\n    pd.DataFrame\n        modified dataframe with extra columns containing split up values\n    \"\"\"\n    obj_cols = df_temp.select_dtypes(include=[object])\n    # count spaces\n    for col in obj_cols:\n        if df_temp[col].str.contains(splitting_string).sum() > 0:\n            df2 = df_temp[col].str.split(splitting_string, expand=True)\n            # Rename columns\n            rename_dict = {}\n            for rename_col in df2.columns:\n                if (splitting_string != \" \"):\n                    rename_dict[rename_col] = col + splitting_string + str(rename_col)\n                else:\n                    rename_dict[rename_col] = col + str(rename_col)\n            df2 = df2.rename(columns=rename_dict)\n            df2 = df2.fillna(0)\n            df_temp = pd.concat([df_temp,df2], axis=1) \n    return df_temp\n\ndf = break_up_by_string(df, ' ')\ntest_df = break_up_by_string(test_df, ' ')","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.827114Z","iopub.execute_input":"2022-03-15T06:39:51.827588Z","iopub.status.idle":"2022-03-15T06:39:51.884199Z","shell.execute_reply.started":"2022-03-15T06:39:51.827547Z","shell.execute_reply":"2022-03-15T06:39:51.883549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove the ID Columns\n\nID columns commonly have no predictive power. In this dataset names actually might be useful but I'm trying for an automated system here, certain kinds of feature engineering may be hard to replicate. I don't believe even the best thought out feature engineering is likely to defeat human feature engineering at this time. ","metadata":{}},{"cell_type":"code","source":"def detect_id_columns(df_temp : pd.DataFrame):\n    \"\"\"\n    Detect which columns are ID columns, those for which one unique value exists for each row.\n    \n    Parameters\n    ----------\n    df_temp : pd.DataFrame\n        Dataframe to detect ID columns\n        \n    Returns\n    -------\n    typing.List\n        List of Identity columns that were detected\n    \"\"\"\n    id_cols = []\n    for col in df.columns:\n        if len(df[col].unique()) == len(df[col]):\n            id_cols.append(col)\n    return id_cols\n\nid_cols = detect_id_columns(df)\ndf = df.drop(columns=id_cols)\ntest_df = test_df.drop(columns=id_cols)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.885785Z","iopub.execute_input":"2022-03-15T06:39:51.886359Z","iopub.status.idle":"2022-03-15T06:39:51.904642Z","shell.execute_reply.started":"2022-03-15T06:39:51.886322Z","shell.execute_reply":"2022-03-15T06:39:51.903917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Remove Unshared Columns\n\nIt is important that there aren't columns in the test dataframe that aren't in the train dataframe","metadata":{}},{"cell_type":"code","source":"def drop_unshared_columns(df_temp : pd.DataFrame, df_temp_2 : pd.DataFrame, exclude_columns : typing.List):\n    \"\"\"\n    Detect which columns are not shared between the two dataframes excepting for a target_col if provided.\n    Delete in place.\n    \n    Parameters\n    ----------\n    df_temp : pd.DataFrame\n        Dataframe to check for shared columns        \n    df_temp_2 : pd.DataFrame\n        Second dataframe to check for shared columns\n    exclude_columns : typing.List\n        Columns not to remove in this process\n    \"\"\"    \n    for col in df_temp_2.columns:\n        if col not in df_temp.columns:\n            if col not in exclude_columns:\n                df_temp_2.drop(columns=col, axis=1, inplace=True)\n    for col in df_temp.columns:\n        if col not in df_temp_2.columns:\n            if col not in exclude_columns:\n                df_temp.drop(columns=col, axis=1, inplace=True)\ndrop_unshared_columns(df, test_df, ['Survived'])\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.905833Z","iopub.execute_input":"2022-03-15T06:39:51.90619Z","iopub.status.idle":"2022-03-15T06:39:51.919904Z","shell.execute_reply.started":"2022-03-15T06:39:51.906158Z","shell.execute_reply":"2022-03-15T06:39:51.919243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Encoding\n\nWe will now go through and encode the features in an automated fashion","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\n\ndef encode_columns(df : pd.DataFrame, columns : pd.Series, test_df : pd.DataFrame = None, cutoff : int = 12):\n    \"\"\"\n    Encode columns based on the number of unique values in each column\n    \n    Parameters\n    ----------\n    df_temp : pd.DataFrame\n        Dataframe to encode columns in \n    columns : pd.Series\n        Columns to encode\n    test_df : pd.DataFrame\n        Test dataframe to encode based on classes in the Dataframe\n    cut_off : int\n        The cut off number of classes to choose between label encoding and get dummies. This keeps the dimensionality under control\n        \n    Returns\n    -------\n    (pd.DataFrame, pd.DataFrame)\n        Original dataframe and the test dataframe\n    \"\"\"    \n    for col in columns:\n        le = preprocessing.LabelEncoder()\n        classes_to_encode = df[col].astype(str).unique().tolist()\n        classes_to_encode.sort()\n        classes_to_encode.append('None')\n        le.fit(classes_to_encode)\n        if len(le.classes_) < cutoff:\n            df = pd.get_dummies(df, columns = [col])\n            if test_df is not None:\n                test_df = pd.get_dummies(test_df, columns = [col])\n        else:\n            check_col = df.copy()[col]\n            df[col] = le.transform(df[col].astype(str))\n            if test_df is not None:\n                #Clean out unseen labels\n                inputs = []\n                for idx, row in test_df.iterrows():\n                    if row[col] in pd.unique(check_col):\n                        inputs.append(row[col])\n                    else:\n                        inputs.append('None')\n                test_df[col] = inputs\n                test_df[col] = le.transform(test_df[col].astype(str))\n    return df, test_df\n\ndf, test_df = encode_columns(df, df.select_dtypes(include=['object']).columns, test_df, 12)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:51.920834Z","iopub.execute_input":"2022-03-15T06:39:51.921466Z","iopub.status.idle":"2022-03-15T06:39:53.516633Z","shell.execute_reply.started":"2022-03-15T06:39:51.921427Z","shell.execute_reply":"2022-03-15T06:39:53.515585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We then drop unshared columns again to get rid of the get dummies columns that don't fit.","metadata":{}},{"cell_type":"code","source":"drop_unshared_columns(df, test_df, ['Survived'])","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:53.519897Z","iopub.execute_input":"2022-03-15T06:39:53.520142Z","iopub.status.idle":"2022-03-15T06:39:53.529476Z","shell.execute_reply.started":"2022-03-15T06:39:53.520114Z","shell.execute_reply":"2022-03-15T06:39:53.528574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quantile Transformation\n\nQuantile transformation can be helpful in my experience for analysis and learning. So this will be my first scaling function","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import QuantileTransformer\n\ndef quantile_transform_column_wise(df_temp : pd.DataFrame, target_col : str = \"\"):\n    \"\"\"\n    Transform values in dataframe to quantile uniform distribution\n    \n    Parameters\n    ----------\n    df_temp : pd.DataFrame\n        Dataframe to quantile transform \n    target_col : str\n        This is the target col and is not transformed\n        \n    Returns\n    -------\n    pd.DataFrame\n        Modified dataframe\n    \"\"\"    \n    df_temp = df_temp.copy()\n    # find n_samples\n    n_samples : int = 1000\n    if len(df_temp) < 1000:\n        n_samples = len(df_temp)\n    for col in df_temp.columns:\n        if col != target_col:\n            transformed = QuantileTransformer(random_state=1, n_quantiles=n_samples).fit_transform(df_temp[col].values.reshape(-1, 1))\n            df_temp[col] = pd.Series(transformed[:,0], index=df_temp[col].index, name=df_temp[col].name)\n    return df_temp\n\ndf = quantile_transform_column_wise(df, \"Survived\")\ntest_df = quantile_transform_column_wise(df, \"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:53.53087Z","iopub.execute_input":"2022-03-15T06:39:53.531183Z","iopub.status.idle":"2022-03-15T06:39:53.700432Z","shell.execute_reply.started":"2022-03-15T06:39:53.531126Z","shell.execute_reply":"2022-03-15T06:39:53.699547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Examine Pipeline\n\nWe have something reasonable now. Though I already have many ideas for improvement. Let's see if we can start from the top and run this as a pipeline which takes the dataframe and applies all modifications in order.","metadata":{}},{"cell_type":"code","source":"def data_pipeline(df : pd.DataFrame, test_df : pd.DataFrame, target_col : str = \"\", plot : bool = False, silent = True):\n    \"\"\"\n    Perform a series of modifications to make a tabular dataset more suitable for machine learning analysis\n    \n    Parameters\n    ----------\n    df : pd.DataFrame\n        Dataframe to transform \n    test_df : pd.DataFrame\n        Test dataframe to transform  \n    target_col : str\n        This is the target col and is not transformed or removed\n    plot : bool\n        This tells it whether to plot graphs in called functions\n    silent : bool\n        This tells it whether to avoid print statements in called functions\n        \n    Returns\n    -------\n    (pd.DataFrame, pd.DataFrame)\n        Modified dataframe versions of the provided dataframes\n    \"\"\"    \n    # Detect NaNs\n    nan_columns : typing.List = detect_NaNs(df, \"Training Data\", silent, plot)\n    test_nan_columns : typing.List = detect_NaNs(test_df, \"Test Data\", silent, plot)\n    # Fill NaNs\n    df = fill_nans_create_columns(df, nan_columns, 0)\n    test_df = fill_nans_create_columns(test_df, test_nan_columns, 0)\n    # Detect duplicates\n    detect_duplicates(df, silent)\n    # Break up the columns by spaces\n    df = break_up_by_string(df, ' ')\n    test_df = break_up_by_string(test_df, ' ')\n    # Break up the columns by slash\n    df = break_up_by_string(df, '/')\n    test_df = break_up_by_string(test_df, '/')\n    # Remove id columns  \n    id_cols = detect_id_columns(df.copy())\n    df = df.drop(columns=id_cols)\n    test_df = test_df.drop(columns=id_cols)    \n    # Remove unshared columns   \n    drop_unshared_columns(df, test_df, [target_col])\n    # Encode columns\n    df, test_df = encode_columns(df, df.select_dtypes(include=['object']).columns, test_df, 12)\n    # Remove new unshared columns   \n    drop_unshared_columns(df, test_df, [target_col])\n    # Quantile transform features\n    df = quantile_transform_column_wise(df, target_col)\n    test_df = quantile_transform_column_wise(test_df, target_col)\n    return df, test_df\n\ndf, test_df = data_pipeline(pd.read_csv(\"../input/titanic/train.csv\"), pd.read_csv(\"../input/titanic/test.csv\"), \"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:53.701656Z","iopub.execute_input":"2022-03-15T06:39:53.701894Z","iopub.status.idle":"2022-03-15T06:39:56.06026Z","shell.execute_reply.started":"2022-03-15T06:39:53.701864Z","shell.execute_reply":"2022-03-15T06:39:56.059479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Quick Prediction Test\n\nI will now do a quick machine learning test using logistic regression on the data from the pipeline. We are definitely not trying to set a record score here. Just see our work in practice.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\n\ny = df[\"Survived\"].values\nX = df.drop(columns=\"Survived\").values\n\nskf = StratifiedKFold(n_splits=5)\nlr = LogisticRegression(random_state=1, max_iter=200)\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    lr.fit(X[train_index], y[train_index])\n    scores.append(lr.score(X[test_index], y[test_index]))\n\nprint(\"Score:\", sum(scores) / len(scores))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:56.061669Z","iopub.execute_input":"2022-03-15T06:39:56.062154Z","iopub.status.idle":"2022-03-15T06:39:56.401163Z","shell.execute_reply.started":"2022-03-15T06:39:56.062109Z","shell.execute_reply":"2022-03-15T06:39:56.400284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's try it to compare if we simply used the csv file as is with the absolute minimum modifications for logistic regression to be able to run. This will show if our modifications achieve anything.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/titanic/train.csv\")\ny = df[\"Survived\"].values\nX = df.drop(columns=\"Survived\").select_dtypes(exclude=['object']).fillna(0).values\n\nskf = StratifiedKFold(n_splits=5)\nlr = LogisticRegression(random_state=1, max_iter=200)\nscores = []\nfor train_index, test_index in skf.split(X, y):\n    lr.fit(X[train_index], y[train_index])\n    scores.append(lr.score(X[test_index], y[test_index]))\n\nprint(\"Score:\", sum(scores) / len(scores))","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:56.402945Z","iopub.execute_input":"2022-03-15T06:39:56.403577Z","iopub.status.idle":"2022-03-15T06:39:56.611443Z","shell.execute_reply.started":"2022-03-15T06:39:56.403528Z","shell.execute_reply":"2022-03-15T06:39:56.610765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the score has changed dramaticaly and therefore our pipeline is effective against simply opening the csv. Due to its generic nature this pipeline therefore should offer improved accuracy against similar datasets without requiring the user to do anything through automated feature engineering.","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\nFeature engineering is a core part of autoML and while what I have here doesn't neccessarily replace human feature engineering it is nice to have this baseline for future projects to compare results too. This presents already an effort free feature engineering that I can attempt in future project. \n\nThere is definitely room for expansion and I will come back to this in a future notebook. Eventually I hope to have a large enough body of code neatly tucked into classes to create a cohesive pip package that people can use and contribute to. Thanks for reading and I'd love to here your suggestions and feedback.\n\nPart #2 is done here: https://www.kaggle.com/taranmarley/automl-from-scratch-2","metadata":{}},{"cell_type":"markdown","source":"# Create Submission\n\nJust for fun I will now create a submission from what I have so far.","metadata":{}},{"cell_type":"code","source":"df, test_df = data_pipeline(pd.read_csv(\"../input/titanic/train.csv\"), pd.read_csv(\"../input/titanic/test.csv\"), \"Survived\")\nlr = LogisticRegression(random_state=1, max_iter=200)\ny = df[\"Survived\"].values\nX = df.drop(columns=\"Survived\").values\nlr.fit(X, y)\nsubmission_df = pd.read_csv(\"../input/titanic/gender_submission.csv\")\ntest_X = test_df.values\nsubmission_df[\"Survived\"] = lr.predict(test_X)\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-15T06:39:56.612513Z","iopub.execute_input":"2022-03-15T06:39:56.612844Z","iopub.status.idle":"2022-03-15T06:39:58.867846Z","shell.execute_reply.started":"2022-03-15T06:39:56.612816Z","shell.execute_reply":"2022-03-15T06:39:58.866885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At the time of writing this submission is in the top 55% which in my opinion really isn't too bad for untuned logistic regression with entirely generic feature engineering. Obviously future work will be much better than that. ","metadata":{}}]}