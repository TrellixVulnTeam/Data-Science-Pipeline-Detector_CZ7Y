{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Introduction**"},{"metadata":{},"cell_type":"markdown","source":"***Howdy, Welcome to the Titanic***"},{"metadata":{},"cell_type":"markdown","source":"**To whom does this notebook appeal to?**\n* If you are just starting the Titanic Competition and want to learn how to implement Neural Networks, I suggest you start here!"},{"metadata":{},"cell_type":"markdown","source":"![](https://faithmag.com/sites/default/files/styles/article_full/public/2018-09/titanic2.jpg?h=6521bd5e&itok=H8td6QVv)"},{"metadata":{},"cell_type":"markdown","source":"**Basic Imports**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Reading In the Data"},{"metadata":{},"cell_type":"markdown","source":"We will be first importing the data, and creating copies. I recommend this because it give you access to a clean untouched base file.\nNext, we will be dropping some things that we don't need such as passenger Id and Ticket price"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')\n#make a copy so your original data is not touched\ntrain = train_data.copy()\ntest = test_data.copy()\ntrain.shape\ny_train = train['Survived']\n\n#We won't need passenger ID or ticket price for the model! They do not provide much insight on the training.\nId = pd.DataFrame(test['PassengerId'])\ntrain.drop(['PassengerId'], axis = 1, inplace=True)\ntest.drop(['PassengerId'], axis = 1, inplace=True)\ntrain.drop(['Survived'], axis = 1, inplace=True)\ntrain.drop(['Ticket'], axis = 1, inplace=True)\ntest.drop(['Ticket'], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's take a look at our data!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets see how many null values there are! We need to fill out these values later.**"},{"metadata":{},"cell_type":"markdown","source":"**We see that we have some midding values from age, and a ton missing from cabin**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train.isnull(),yticklabels=False,cbar='BuPu')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum().sort_values(ascending=False)[0:20]\n# we can see that cabin is midding a lot of values, and age is tooi!","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Data Cleaning**"},{"metadata":{},"cell_type":"markdown","source":"**This is an awesome function I created that preprocesses the data. It does thes following**\n\n* Fills in null values based on mean or mode\n* Drops columns that are missing 50 percent of the data\n\n*You guys are free to copy this for loop for your own projects!*[](http://)"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n#clean the train data\nfor i in list(train.columns):\n    dtype = train[i].dtype\n    values = 0\n    if(dtype == float or dtype == int):\n        method = 'mean'\n    else:\n        method = 'mode'\n    if(train[i].notnull().sum() / 891 <= .5):\n        train.drop(i, axis = 1, inplace=True)\n    elif method == 'mean':\n        train[i]=train[i].fillna(train[i].mean())\n\n    else:\n        train[i]=train[i].fillna(train[i].mode()[0])\n\n# WE CAN DO THIS FOR THE TEST SET TOO!\n\n#clean the test data\nfor i in list(test.columns):\n    dtype = test[i].dtype\n    values = 0\n    if(dtype == float or dtype == int):\n        method = 'mean'\n    else:\n        method = 'mode'\n    if(test[i].notnull().sum() / 418 <= .5):\n        test.drop(i, axis = 1, inplace=True)\n    elif method == 'mean':\n        test[i]=test[i].fillna(test[i].mean())\n\n    else:\n        test[i]=test[i].fillna(test[i].mode()[0])\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As we can see, all of the missing values are gone!**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"sns.heatmap(train.isnull(),yticklabels=False,cbar='BuPu')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature Engineering**"},{"metadata":{},"cell_type":"markdown","source":"**Title**"},{"metadata":{},"cell_type":"markdown","source":"Lets take out the Mr, Miss, etc from the name section, and create a new column names title!"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TITLE\n\ntrain_test_data = [train, test] # combining train and test dataset\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n\ntitle_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 1, \n                 \"Master\": 0, \"Dr\": 1, \"Rev\": 0, \"Col\": 0, \"Major\": 0, \"Mlle\": 1,\"Countess\": 1,\n                 \"Ms\": 1, \"Lady\": 1, \"Jonkheer\": 1, \"Don\": 0, \"Dona\" : 1, \"Mme\": 0,\"Capt\": 0,\"Sir\": 0 }\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sex**\n"},{"metadata":{},"cell_type":"markdown","source":"We can make male and female into catagorical variables such as 0 and 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"    \nsex_mapping = {\"male\": 0, \"female\":1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Embarked**"},{"metadata":{},"cell_type":"markdown","source":"Lets test to see if there is any correlation with Pclass and Embarked "},{"metadata":{"trusted":true},"cell_type":"code","source":"Pclass1 = train_data[train_data['Pclass'] == 1]['Embarked'].value_counts()\nPclass2 = train_data[train_data['Pclass'] == 2]['Embarked'].value_counts()\nPclass3 = train_data[train_data['Pclass'] == 3]['Embarked'].value_counts()\n\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can assign each embarked value to a numberical value for training later!"},{"metadata":{"trusted":true},"cell_type":"code","source":"for data in train_test_data:\n    data['Embarked'] = data['Embarked'].fillna(\"S\")\n    \nembarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Family Size**"},{"metadata":{},"cell_type":"markdown","source":"Parch ( Parent & child ) and Sibsp( Sibling & Spouse ) are both contributing factors to family size, so lets make a new column called family size, and drop the other ones."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"FamilySize\"] = train['SibSp'] + train['Parch'] + 1\ntest[\"FamilySize\"] = test['SibSp'] + test['Parch'] + 1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Correlations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train.corr(),cbar='plasma')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['Name'], axis = 1, inplace=True)\ntest.drop(['Name'], axis = 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **PreProcessing**"},{"metadata":{},"cell_type":"markdown","source":"**Now that our data looks good, lets get ready to build our models!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#imports\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten, Conv2D,MaxPool2D\n\nimport keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We should scale the values in the data, so that the neural network can train better!"},{"metadata":{"trusted":true},"cell_type":"code","source":"continuous = ['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'FamilySize']\n\nscaler = StandardScaler()\n\nfor var in continuous:\n    train[var] = train[var].astype('float64')\n    train[var] = scaler.fit_transform(train[var].values.reshape(-1, 1))\nfor var in continuous:\n    test[var] = test[var].astype('float64')\n    test[var] = scaler.fit_transform(test[var].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include='all').T\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Neural Network**"},{"metadata":{},"cell_type":"markdown","source":"Lets create our model! make sure to add the dropout value, so that our model does not over fit."},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntf.keras.optimizers.Adam(\n    learning_rate=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n    name='Adam', \n)\n#creating a model with sequential columns\nmodel = Sequential()\n\n#flattens the data into a 1d array\nmodel.add(Flatten())\n#creates the first latyer with the input dimanetion. \n\nmodel.add(Dense(32, input_dim=train.shape[1],kernel_initializer = 'uniform', activation='relu'))\n#next layer with 32 dense nodes\nmodel.add(Dense(32, kernel_initializer = 'uniform', activation = 'relu'))\n#drops 0.4 of the values from the next layer, so it does not over fit!\n\nmodel.add(Dropout(0.4))\n#last layer is initiated here\nmodel.add(Dense(32,kernel_initializer = 'uniform', activation = 'relu'))\n\n# create output layer\n    # Feel free to experiment with the activation functions and the optimizers\nmodel.add(Dense(1, activation='sigmoid'))  # output layer\n    \nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let's fit our model!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train, y_train, epochs=20, batch_size=50, validation_split = 0.2)\n\n#val_acc = np.mean(training.history['val_acc'])\n#print(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(model.summary())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluate the model!**"},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate(train, y_train, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Create Predictions**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(test)\n\ny_final = (y_pred > 0.5).astype(int).reshape(test.shape[0])\n\noutput = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': y_final})\noutput.to_csv('prediction-ann.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **NN Analyasis**"},{"metadata":{},"cell_type":"markdown","source":"We can plot the loss, and val_loss & acc and val_acc\nThe main purpose of this is to make sure the model follows a good trend, and to make sure that you are not overfitteing your model."},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_train = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1,21)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_train = history.history['accuracy']\nloss_val = history.history['val_accuracy']\nepochs = range(1,21)\nplt.plot(epochs, loss_train, 'g', label='Training accuracy')\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Thank you for viewing this notebook!**\n\nHave a great day!"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}