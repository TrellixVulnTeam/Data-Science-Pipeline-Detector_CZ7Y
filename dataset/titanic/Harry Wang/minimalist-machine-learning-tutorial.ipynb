{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n\nThe goal of this tutorial is to demonstrate the key components of an end-to-end data science/machine learning project. Note that the focus is to show the overall workflow not to build the best performing model.\n\nThe following shows the key steps:\n\n- Load and split train/test data\n- Exploratory Data Analysis (EDA)\n- Data pre-processing and pipeline\n- Model building, evaluation, tuning, and selection\n- Feature importance analysis and feature selection\n- Model persistence\n\nSee other related code and examples (such as Machine Learning Web App via Streamlit, AutoML, MLOps with ClearML, etc.) at https://harrywang.me/mini-ml/","metadata":{}},{"cell_type":"code","source":"# import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn')  # change the default style","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:05.921664Z","iopub.execute_input":"2021-07-20T20:34:05.922235Z","iopub.status.idle":"2021-07-20T20:34:06.960161Z","shell.execute_reply.started":"2021-07-20T20:34:05.922148Z","shell.execute_reply":"2021-07-20T20:34:06.959032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# read csv data into pandas dataframe\ndf = pd.read_csv('/kaggle/input/titanic/train.csv')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:06.963877Z","iopub.execute_input":"2021-07-20T20:34:06.964196Z","iopub.status.idle":"2021-07-20T20:34:06.982519Z","shell.execute_reply.started":"2021-07-20T20:34:06.964166Z","shell.execute_reply":"2021-07-20T20:34:06.980597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# basic shape, data type, null values\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:06.984623Z","iopub.execute_input":"2021-07-20T20:34:06.985045Z","iopub.status.idle":"2021-07-20T20:34:07.017783Z","shell.execute_reply.started":"2021-07-20T20:34:06.985005Z","shell.execute_reply":"2021-07-20T20:34:07.016017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first 5 lines of data\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:07.019651Z","iopub.execute_input":"2021-07-20T20:34:07.020152Z","iopub.status.idle":"2021-07-20T20:34:07.057604Z","shell.execute_reply.started":"2021-07-20T20:34:07.020105Z","shell.execute_reply":"2021-07-20T20:34:07.056492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the data by separating X and y\n# dropping unimportant features, such as passenger id, name, ticket number and cabin number\n# note that interesting features might be engieered from the dropped features above\n\n# axis = 1 below means dropping by columns, 0 means by rows\nX = df.drop(['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ny = df['Survived']\nX.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:07.058916Z","iopub.execute_input":"2021-07-20T20:34:07.059234Z","iopub.status.idle":"2021-07-20T20:34:07.076349Z","shell.execute_reply.started":"2021-07-20T20:34:07.059204Z","shell.execute_reply":"2021-07-20T20:34:07.075243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data into a training set and a test set. \n# Any number for the random_state is fine, see 42: https://en.wikipedia.org/wiki/42_(number) \n# We choose to use 20% (test_size=0.2) of the data set as the test set.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(X_train.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:07.07759Z","iopub.execute_input":"2021-07-20T20:34:07.077904Z","iopub.status.idle":"2021-07-20T20:34:07.282188Z","shell.execute_reply.started":"2021-07-20T20:34:07.077873Z","shell.execute_reply":"2021-07-20T20:34:07.280996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic EDA\nYou can show basic descriptive statistics using pandas easily. ","metadata":{}},{"cell_type":"code","source":"# basic stats\nX_train.describe(include='all')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:07.283479Z","iopub.execute_input":"2021-07-20T20:34:07.283822Z","iopub.status.idle":"2021-07-20T20:34:07.34422Z","shell.execute_reply.started":"2021-07-20T20:34:07.28379Z","shell.execute_reply":"2021-07-20T20:34:07.34281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Histogram\nUse the histogram to check the followings:\n\n- The distributions of the data \n- center and spread of the data\n- skewness of the data\n- presence of outliers","metadata":{}},{"cell_type":"code","source":"# histograms for all numerical features\nX_train.hist(figsize=(15,15))","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:07.347461Z","iopub.execute_input":"2021-07-20T20:34:07.34782Z","iopub.status.idle":"2021-07-20T20:34:08.479679Z","shell.execute_reply.started":"2021-07-20T20:34:07.347787Z","shell.execute_reply":"2021-07-20T20:34:08.478689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# key findings with potential processing\n# long tail (skewed left): log transformation\n# some outliers: outlier removal\nX_train['Fare'].hist(bins=100)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:08.482057Z","iopub.execute_input":"2021-07-20T20:34:08.482431Z","iopub.status.idle":"2021-07-20T20:34:08.870343Z","shell.execute_reply.started":"2021-07-20T20:34:08.482396Z","shell.execute_reply":"2021-07-20T20:34:08.86941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Box Plot\n\nA boxplot displays the dataset based on a five-number summary:\n\n- Median (Q2 / 50th Percentile) : the middle value of the dataset.\n\n- First quartile (Q1 / 25th Percentile) : the middle value between the smallest number and the median of the dataset.\n\n- Third quartile (Q3 / 75th Percentile) : the middle value between the largest number and the median of the dataset.\n\nInterquartile Range (IQR) is the distance between the upper and lower quartile: IQR=Q3-Q1, \nIQR is used to determine outliers, which are points that are more than 1.5IQR from the median\n\n- Minimum (NOT the smallest): the lowest data point excluding any outliers.\n\n- Maximum (NOT the largest): the largest data point excluding any outliers.\n\n\n\n\n### A box plot identifies the middle 50% of the data (the box), the median (the line in the box), and the outliers (the dots outside the max and min)","metadata":{}},{"cell_type":"code","source":"X_train['Age'].plot.box()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:08.871575Z","iopub.execute_input":"2021-07-20T20:34:08.871952Z","iopub.status.idle":"2021-07-20T20:34:09.028476Z","shell.execute_reply.started":"2021-07-20T20:34:08.871905Z","shell.execute_reply":"2021-07-20T20:34:09.02744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scatter Plot\n\nScatter plot is often used for **correlation analysis** between different features. Correlation coefficient is between -1 and 1, representing negative and positive correlations. 0 means there is no liner correlation. Correlation is said to be linear if the ratio of change is constant, otherwise is non-linear. ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots()\nax.scatter(x=X_train['Age'], y=X_train['Fare'], alpha=0.2) # alpha=0.2 specifies the opacity\nax.set_xlabel('Age')\nax.set_ylabel('Fare')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:09.029975Z","iopub.execute_input":"2021-07-20T20:34:09.030286Z","iopub.status.idle":"2021-07-20T20:34:09.239331Z","shell.execute_reply.started":"2021-07-20T20:34:09.030256Z","shell.execute_reply":"2021-07-20T20:34:09.238276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pairplot example using seaborn\nsns.pairplot(data=X_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:09.240631Z","iopub.execute_input":"2021-07-20T20:34:09.241016Z","iopub.status.idle":"2021-07-20T20:34:14.791169Z","shell.execute_reply.started":"2021-07-20T20:34:09.240983Z","shell.execute_reply":"2021-07-20T20:34:14.790442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data pre-processing\nWe will build a pipeline to do some of the following tasks:\n\n- Missing data\n- Feature scaling (important for certain model such as Gradient Descent based models)\n- Categorical feature encoding\n- Outlier removal\n- Transformation\n- Custom processing","metadata":{}},{"cell_type":"code","source":"# any missing values?\nX_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:14.792242Z","iopub.execute_input":"2021-07-20T20:34:14.792663Z","iopub.status.idle":"2021-07-20T20:34:14.801334Z","shell.execute_reply.started":"2021-07-20T20:34:14.792623Z","shell.execute_reply":"2021-07-20T20:34:14.800465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We will train our decision tree classifier with the following features:\n# Numerical Features: ['Age', 'SibSp', 'Fare', 'Parch']\n# Categorical Features:['Sex', 'Embarked', 'Pclass'\n\nnum_features = ['Age', 'SibSp', 'Parch', 'Fare']\ncat_features = ['Sex', 'Embarked', 'Pclass']","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:14.802615Z","iopub.execute_input":"2021-07-20T20:34:14.803057Z","iopub.status.idle":"2021-07-20T20:34:14.810853Z","shell.execute_reply.started":"2021-07-20T20:34:14.803024Z","shell.execute_reply":"2021-07-20T20:34:14.809648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# Create the preprocessing pipeline for numerical features\n# There are two steps in this pipeline\n# Pipeline(steps=[(name1, transform1), (name2, transform2), ...]) \n# NOTE the step names can be arbitrary\n\n# Step 1 is what we discussed before - filling the missing values if any using mean\n# Step 2 is feature scaling via standardization - making features look like normal-distributed \n# see sandardization: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\nnum_pipeline = Pipeline(\n    steps=[\n        ('num_imputer', SimpleImputer()),  # we will tune differet strategies later\n        ('scaler', StandardScaler()),\n        ]\n)\n\n# Create the preprocessing pipelines for the categorical features\n# There are two steps in this pipeline:\n# Step 1: filling the missing values if any using the most frequent value\n# Step 2: one hot encoding\n\ncat_pipeline = Pipeline(\n    steps=[\n        ('cat_imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder()),\n    ]\n)\n\n# Assign features to the pipelines and Combine two pipelines to form the preprocessor\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num_pipeline', num_pipeline, num_features),\n        ('cat_pipeline', cat_pipeline, cat_features),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:14.812132Z","iopub.execute_input":"2021-07-20T20:34:14.812436Z","iopub.status.idle":"2021-07-20T20:34:14.998388Z","shell.execute_reply.started":"2021-07-20T20:34:14.812408Z","shell.execute_reply":"2021-07-20T20:34:14.997516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline prediction\n\nIt's always helpful to have some baseline predictions based on heuristics/rules so that you can benchmark your model performance. The following shows that female has much higher survival rate than male so that we can have a rule-based baseline to predict all female survivied and male died. For regression problem, an easy baseline could be using the training sample mean for all predictions.","metadata":{}},{"cell_type":"code","source":"# calculate the survival rates by gender\n# female survival rate: 74.2%\n# male survival rate: 18.9%\ngroup_norm = df.groupby('Sex')['Survived'].value_counts(normalize=True)\ngroup_norm","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:14.999977Z","iopub.execute_input":"2021-07-20T20:34:15.000719Z","iopub.status.idle":"2021-07-20T20:34:15.017071Z","shell.execute_reply.started":"2021-07-20T20:34:15.00065Z","shell.execute_reply":"2021-07-20T20:34:15.015936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.head()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:15.018356Z","iopub.execute_input":"2021-07-20T20:34:15.018631Z","iopub.status.idle":"2021-07-20T20:34:15.034756Z","shell.execute_reply.started":"2021-07-20T20:34:15.018604Z","shell.execute_reply":"2021-07-20T20:34:15.033715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rule-based prediction\nbaseline_pred = X_test['Sex'].apply(lambda x: 0 if x == 'male' else 1)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:15.03614Z","iopub.execute_input":"2021-07-20T20:34:15.036454Z","iopub.status.idle":"2021-07-20T20:34:15.051427Z","shell.execute_reply.started":"2021-07-20T20:34:15.036425Z","shell.execute_reply":"2021-07-20T20:34:15.050687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nprint(f'Baseline Accuracy Score : {accuracy_score(y_test, baseline_pred)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:15.052453Z","iopub.execute_input":"2021-07-20T20:34:15.05286Z","iopub.status.idle":"2021-07-20T20:34:15.066527Z","shell.execute_reply.started":"2021-07-20T20:34:15.052829Z","shell.execute_reply":"2021-07-20T20:34:15.065347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model traning, tuning, evaluation and selection\n\nNext, I attach three different models (Decision Tree, SVC, Random Forest) to the same pre-processing pipeline and tune the some parameters using GridSearch with cross validation. Then, we compare their performance and choose the best model to proceed. ","metadata":{}},{"cell_type":"code","source":"# Specify the model to use, which is DecisionTreeClassifier\n# Make a full pipeline by combining preprocessor and the model\nfrom sklearn.tree import DecisionTreeClassifier\n\npipeline_dt = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor),\n        ('clf_dt', DecisionTreeClassifier()),\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:15.068189Z","iopub.execute_input":"2021-07-20T20:34:15.068499Z","iopub.status.idle":"2021-07-20T20:34:15.110686Z","shell.execute_reply.started":"2021-07-20T20:34:15.068469Z","shell.execute_reply":"2021-07-20T20:34:15.109608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we show how to use GridSearch with K-fold cross validation (K=10) to fine tune the model\n# we use the accuracy as the scoring metric with training score return_train_score=True\nfrom sklearn.model_selection import GridSearchCV\n\n# set up the values of hyperparameters you want to evaluate\n# here you must use the step names as the prefix followed by two under_scores to sepecify the parameter names and the \"full path\" of the steps\n\n# we are trying 2 different impputer strategies \n# 2x5 different decision tree models with different parameters\n# in total we are trying 2x2x5 = 20 different combinations\n\nparam_grid_dt = [\n    {\n        'preprocessor__num_pipeline__num_imputer__strategy': ['mean', 'median'],\n        'clf_dt__criterion': ['gini', 'entropy'], \n        'clf_dt__max_depth': [3, 4, 5, 6, 7],\n    }\n]\n\n# set up the grid search \ngrid_search_dt = GridSearchCV(pipeline_dt, param_grid_dt, cv=10, scoring='accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:15.112062Z","iopub.execute_input":"2021-07-20T20:34:15.112395Z","iopub.status.idle":"2021-07-20T20:34:15.118224Z","shell.execute_reply.started":"2021-07-20T20:34:15.112362Z","shell.execute_reply":"2021-07-20T20:34:15.117488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model using the full pipeline\ngrid_search_dt.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:15.119216Z","iopub.execute_input":"2021-07-20T20:34:15.119633Z","iopub.status.idle":"2021-07-20T20:34:21.363406Z","shell.execute_reply.started":"2021-07-20T20:34:15.1196Z","shell.execute_reply":"2021-07-20T20:34:21.362326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the best performing parameter combination\ngrid_search_dt.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:21.368902Z","iopub.execute_input":"2021-07-20T20:34:21.369242Z","iopub.status.idle":"2021-07-20T20:34:21.376014Z","shell.execute_reply.started":"2021-07-20T20:34:21.369213Z","shell.execute_reply":"2021-07-20T20:34:21.374813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build-in CV results keys\nsorted(grid_search_dt.cv_results_.keys())","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:21.378982Z","iopub.execute_input":"2021-07-20T20:34:21.379482Z","iopub.status.idle":"2021-07-20T20:34:21.391267Z","shell.execute_reply.started":"2021-07-20T20:34:21.379434Z","shell.execute_reply":"2021-07-20T20:34:21.389927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test score for the 20 decision tree models\ngrid_search_dt.cv_results_['mean_test_score']","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:21.393792Z","iopub.execute_input":"2021-07-20T20:34:21.39435Z","iopub.status.idle":"2021-07-20T20:34:21.408067Z","shell.execute_reply.started":"2021-07-20T20:34:21.394299Z","shell.execute_reply":"2021-07-20T20:34:21.406577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best decistion tree model test score\ngrid_search_dt.best_score_","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:21.40924Z","iopub.execute_input":"2021-07-20T20:34:21.409539Z","iopub.status.idle":"2021-07-20T20:34:21.42096Z","shell.execute_reply.started":"2021-07-20T20:34:21.40951Z","shell.execute_reply":"2021-07-20T20:34:21.419914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try SVM classifer\nfrom sklearn.svm import SVC\n\n# SVC pipeline\npipeline_svc = Pipeline([\n    ('preprocessor', preprocessor),\n    ('clf_svc', SVC(probability=True)),  # we need the probability scores later\n])\n\n# here we are trying three different kernel and three degree values for polynomail kernel\n# in total 5 different combinations\nparam_grid_svc = [\n    {\n        'clf_svc__kernel': ['linear', 'poly', 'rbf'], \n        'clf_svc__degree': [3, 4, 5],  # only for poly kernel\n    }\n]\n\n# set up the grid search \ngrid_search_svc = GridSearchCV(pipeline_svc, param_grid_svc, cv=10, scoring='accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:21.422419Z","iopub.execute_input":"2021-07-20T20:34:21.422751Z","iopub.status.idle":"2021-07-20T20:34:21.434341Z","shell.execute_reply.started":"2021-07-20T20:34:21.422714Z","shell.execute_reply":"2021-07-20T20:34:21.433295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model using the full pipeline\ngrid_search_svc.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:21.435443Z","iopub.execute_input":"2021-07-20T20:34:21.435915Z","iopub.status.idle":"2021-07-20T20:34:29.709149Z","shell.execute_reply.started":"2021-07-20T20:34:21.435882Z","shell.execute_reply":"2021-07-20T20:34:29.708076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best test score\ngrid_search_svc.best_score_","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:29.710567Z","iopub.execute_input":"2021-07-20T20:34:29.710906Z","iopub.status.idle":"2021-07-20T20:34:29.717056Z","shell.execute_reply.started":"2021-07-20T20:34:29.710875Z","shell.execute_reply":"2021-07-20T20:34:29.715912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try random forest classifer\nfrom sklearn.ensemble import RandomForestClassifier\n\n# rf pipeline\npipeline_rf = Pipeline([\n    ('preprocessor', preprocessor),\n    ('clf_rf', RandomForestClassifier()),\n])\n\n# here we are trying 2x3 different rf models\nparam_grid_rf = [\n    {\n        'clf_rf__criterion': ['gini', 'entropy'], \n        'clf_rf__n_estimators': [50, 100, 150],  \n    }\n]\n\n# set up the grid search \ngrid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=10, scoring='accuracy')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:29.718527Z","iopub.execute_input":"2021-07-20T20:34:29.718955Z","iopub.status.idle":"2021-07-20T20:34:29.755355Z","shell.execute_reply.started":"2021-07-20T20:34:29.718914Z","shell.execute_reply":"2021-07-20T20:34:29.754599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# train the model using the full pipeline\ngrid_search_rf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:29.75663Z","iopub.execute_input":"2021-07-20T20:34:29.75694Z","iopub.status.idle":"2021-07-20T20:34:46.159274Z","shell.execute_reply.started":"2021-07-20T20:34:29.756911Z","shell.execute_reply":"2021-07-20T20:34:46.158183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# best test score\nprint('best dt score is: ', grid_search_dt.best_score_)\nprint('best svc score is: ', grid_search_svc.best_score_)\nprint('best rf score is: ', grid_search_rf.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.160547Z","iopub.execute_input":"2021-07-20T20:34:46.160871Z","iopub.status.idle":"2021-07-20T20:34:46.167145Z","shell.execute_reply.started":"2021-07-20T20:34:46.160841Z","shell.execute_reply":"2021-07-20T20:34:46.166079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select the best model\n# the best parameters are shown, note SimpleImputer() implies that mean strategry is used\nclf_best = grid_search_dt.best_estimator_\nclf_best","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.168274Z","iopub.execute_input":"2021-07-20T20:34:46.168547Z","iopub.status.idle":"2021-07-20T20:34:46.273611Z","shell.execute_reply.started":"2021-07-20T20:34:46.16852Z","shell.execute_reply":"2021-07-20T20:34:46.272609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# final test on the testing set\n# To predict on new data: simply calling the predict method \n# the full pipeline steps will be applied to the testing set followed by the prediction\ny_pred = clf_best.predict(X_test)\n\n# calculate accuracy, Note: y_test is the ground truth for the tesing set\n# we have similiar score for the testing set as the cross validation score - good\n\nprint(f'Accuracy Score : {accuracy_score(y_test, y_pred)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.274876Z","iopub.execute_input":"2021-07-20T20:34:46.27516Z","iopub.status.idle":"2021-07-20T20:34:46.289729Z","shell.execute_reply.started":"2021-07-20T20:34:46.275132Z","shell.execute_reply":"2021-07-20T20:34:46.288441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot the confusion matrix \nplt.style.use('default') # use default styple for confusion matrix plots\n\nfrom sklearn.metrics import plot_confusion_matrix\n\n# the default confusion matrix with default label order (ascending order, 0, 1, etc.)\n# by defualt the default True Positive is in the bottom right quadrant\n#matrix = plot_confusion_matrix(clf_best, X_test, y_test) \n\n# plot the confusion matrix with preferred label order and label name\n# true positive in the left upper quadrant\nclass_names = ['1: Survived', '0: Died']\nmatrix = plot_confusion_matrix(clf_best, X_test, y_test, labels=[1, 0], display_labels=class_names)\n\n# disp.confusion_matrix returns confusion matrix as an array\nprint(f'Confusion Matrix: \\n {matrix.confusion_matrix}' )","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.291138Z","iopub.execute_input":"2021-07-20T20:34:46.29146Z","iopub.status.idle":"2021-07-20T20:34:46.545001Z","shell.execute_reply.started":"2021-07-20T20:34:46.291418Z","shell.execute_reply":"2021-07-20T20:34:46.54414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# our model is better than the base line - good\nprint(f'Baseline Accuracy Score : {accuracy_score(y_test, baseline_pred)}')\nprint(f'Our Best Accuracy Score : {accuracy_score(y_test, y_pred)}')","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.54622Z","iopub.execute_input":"2021-07-20T20:34:46.546708Z","iopub.status.idle":"2021-07-20T20:34:46.553495Z","shell.execute_reply.started":"2021-07-20T20:34:46.546647Z","shell.execute_reply":"2021-07-20T20:34:46.55237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Precision-Recall Trade-off and ROC/AUC\nI chose the best model based on accuracy score above. In classification, the final prediction for a data point actually has probability scores. For example, for one person in this Titanic dataset, a prediction looks like `[0.35, 0.65]`, which means the predicted probability for this person to be 0 (died) is 0.35 and the predicted probability to be 1 (survived) is 0.65. The default decision threshold is 0.5 for decision tree classifier, therefore this person is predicted to be 1 (survived). However, if we change the threshold to be 0.7, then the same person would be predicted to be 0 (died). \n\nChanging the decision threshold often leads to changes in precision and recall. Increasing precision often decreases recall and vice versa, which is called precision-recall trade-off. Given a specific context, you may favor precision over recall or the other way around. \n\nReceiver Operating Characteristic (ROC) is another metric to evaluate classifier output quality using Recall (True Positive Rate) and FPR (False Positive Rate). For classification problems with very imbalanced data (such as the current COVID-19 testing data, way more people are negative), the default threshold can result in poor model performance. **ROC/AUC is often a better metric than accuracy for imbalancd data.**\n\nNext, I show the AUC scores for the three different models and plot the ROC curves.","metadata":{}},{"cell_type":"code","source":"# get the probability score the decision tree model\n# for each prediction, we have two probabilities for two labels 0 means died, 1 means survived\ny_pred_proba_dt = grid_search_dt.best_estimator_.predict_proba(X_test)\nprint(y_pred_proba_dt[0])  # 0.881 died, 0.118 survivied\ny_scores_dt = y_pred_proba_dt[:, 1]  # this is the score of positive class","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.554847Z","iopub.execute_input":"2021-07-20T20:34:46.555157Z","iopub.status.idle":"2021-07-20T20:34:46.575916Z","shell.execute_reply.started":"2021-07-20T20:34:46.555118Z","shell.execute_reply":"2021-07-20T20:34:46.574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the probability scores for svc and random forest\ny_pred_proba_svc = grid_search_svc.best_estimator_.predict_proba(X_test)\ny_scores_svc = y_pred_proba_svc[:, 1]  # this is the score of positive class\n\ny_pred_proba_rf = grid_search_rf.best_estimator_.predict_proba(X_test)\ny_scores_rf = y_pred_proba_rf[:, 1]  # this is the score of positive class","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.577513Z","iopub.execute_input":"2021-07-20T20:34:46.577854Z","iopub.status.idle":"2021-07-20T20:34:46.614641Z","shell.execute_reply.started":"2021-07-20T20:34:46.577822Z","shell.execute_reply":"2021-07-20T20:34:46.613708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fpt: false positive rate, tpr: true positive rate (recall)\n# random forest is the best model according to AUC score and its ROC curve is closer to the top-left corner\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_scores_dt)\nfpr_svc, tpr_svc, thresholds_svc = roc_curve(y_test, y_scores_svc)\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_scores_rf)\n\nprint(f'AUC score for dt is {roc_auc_score(y_test, y_scores_dt)}')\nprint(f'AUC score for svc is {roc_auc_score(y_test, y_scores_svc)}')\nprint(f'AUC score for rf is {roc_auc_score(y_test, y_scores_rf)}')\n\n# plot the ROC Curve\nplt.style.use('seaborn')\nfig, ax = plt.subplots()\n\nax.plot(fpr_dt, tpr_dt, label=\"Decision Tree\")\nax.plot(fpr_svc, tpr_svc, label=\"SVC\")\nax.plot(fpr_rf, tpr_rf, label=\"Ramdom Forest\")\nax.set_title('ROC Curve')\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate (Recall)')\nax.legend()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.615813Z","iopub.execute_input":"2021-07-20T20:34:46.616181Z","iopub.status.idle":"2021-07-20T20:34:46.867026Z","shell.execute_reply.started":"2021-07-20T20:34:46.616147Z","shell.execute_reply":"2021-07-20T20:34:46.86611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Importance\n\nGiven that we are using pipeline and one-hot encoding, the feature importance scores are not very straightforward to get. The following code shows how to get the feature importance scores from the decision tree model and create a plot.","metadata":{}},{"cell_type":"code","source":"clf_best.named_steps","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.868321Z","iopub.execute_input":"2021-07-20T20:34:46.868616Z","iopub.status.idle":"2021-07-20T20:34:46.893063Z","shell.execute_reply.started":"2021-07-20T20:34:46.868579Z","shell.execute_reply":"2021-07-20T20:34:46.892094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_best.named_steps['preprocessor']","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.894152Z","iopub.execute_input":"2021-07-20T20:34:46.894434Z","iopub.status.idle":"2021-07-20T20:34:46.92249Z","shell.execute_reply.started":"2021-07-20T20:34:46.894407Z","shell.execute_reply":"2021-07-20T20:34:46.921614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = clf_best['clf_dt'].feature_importances_\ni","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.923611Z","iopub.execute_input":"2021-07-20T20:34:46.924073Z","iopub.status.idle":"2021-07-20T20:34:46.931274Z","shell.execute_reply.started":"2021-07-20T20:34:46.924032Z","shell.execute_reply":"2021-07-20T20:34:46.930372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_best['preprocessor'].transformers_","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.932535Z","iopub.execute_input":"2021-07-20T20:34:46.932838Z","iopub.status.idle":"2021-07-20T20:34:46.946784Z","shell.execute_reply.started":"2021-07-20T20:34:46.932811Z","shell.execute_reply":"2021-07-20T20:34:46.946064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get columnTransformer\nclf_best[0] ","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.947691Z","iopub.execute_input":"2021-07-20T20:34:46.948101Z","iopub.status.idle":"2021-07-20T20:34:46.978063Z","shell.execute_reply.started":"2021-07-20T20:34:46.948071Z","shell.execute_reply":"2021-07-20T20:34:46.977139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf_best[0].transformers_","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.979324Z","iopub.execute_input":"2021-07-20T20:34:46.979618Z","iopub.status.idle":"2021-07-20T20:34:46.988536Z","shell.execute_reply.started":"2021-07-20T20:34:46.97959Z","shell.execute_reply":"2021-07-20T20:34:46.987491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_original_feature_names = clf_best[0].transformers_[0][2]\nnum_original_feature_names","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:46.989579Z","iopub.execute_input":"2021-07-20T20:34:46.989939Z","iopub.status.idle":"2021-07-20T20:34:47.002812Z","shell.execute_reply.started":"2021-07-20T20:34:46.989909Z","shell.execute_reply":"2021-07-20T20:34:47.001715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_original_feature_names = clf_best[0].transformers_[1][2]\ncat_original_feature_names","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:47.004136Z","iopub.execute_input":"2021-07-20T20:34:47.004463Z","iopub.status.idle":"2021-07-20T20:34:47.015128Z","shell.execute_reply.started":"2021-07-20T20:34:47.004433Z","shell.execute_reply":"2021-07-20T20:34:47.014217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_new_feature_names = list(clf_best[0].transformers_[1][1]['onehot'].get_feature_names(cat_original_feature_names))\ncat_new_feature_names","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:47.016507Z","iopub.execute_input":"2021-07-20T20:34:47.016827Z","iopub.status.idle":"2021-07-20T20:34:47.028704Z","shell.execute_reply.started":"2021-07-20T20:34:47.016799Z","shell.execute_reply":"2021-07-20T20:34:47.027539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names = num_original_feature_names + cat_new_feature_names\nfeature_names","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:47.030091Z","iopub.execute_input":"2021-07-20T20:34:47.030386Z","iopub.status.idle":"2021-07-20T20:34:47.040755Z","shell.execute_reply.started":"2021-07-20T20:34:47.030359Z","shell.execute_reply":"2021-07-20T20:34:47.039587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r = pd.DataFrame(i, index=feature_names, columns=['importance'])\nr","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:47.042169Z","iopub.execute_input":"2021-07-20T20:34:47.042454Z","iopub.status.idle":"2021-07-20T20:34:47.058078Z","shell.execute_reply.started":"2021-07-20T20:34:47.042427Z","shell.execute_reply":"2021-07-20T20:34:47.056901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.sort_values('importance', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:47.059153Z","iopub.execute_input":"2021-07-20T20:34:47.059453Z","iopub.status.idle":"2021-07-20T20:34:47.08122Z","shell.execute_reply.started":"2021-07-20T20:34:47.059417Z","shell.execute_reply":"2021-07-20T20:34:47.080223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"r.sort_values('importance', ascending=False).plot.bar()","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:47.082764Z","iopub.execute_input":"2021-07-20T20:34:47.083321Z","iopub.status.idle":"2021-07-20T20:34:47.347087Z","shell.execute_reply.started":"2021-07-20T20:34:47.083282Z","shell.execute_reply":"2021-07-20T20:34:47.345941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we remove the most important feature Sex and see how the model is affected\n# result: accuracy drops from ~0.826 to ~0.716\nnum_features = ['Age', 'SibSp', 'Parch', 'Fare']\ncat_features = ['Embarked', 'Pclass']\n\n# you must update preprocess and pipeline after changing the feature list\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num_pipeline', num_pipeline, num_features),\n        ('cat_pipeline', cat_pipeline, cat_features),\n    ]\n)\n\npipeline_dt = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor),\n        ('clf_dt', DecisionTreeClassifier()),\n    ]\n)\n\n# update the grid search \ngrid_search_dt_updated = GridSearchCV(pipeline_dt, param_grid_dt, cv=10, scoring='accuracy')\n\n# train the model using the updated full pipeline\ngrid_search_dt_updated.fit(X_train, y_train)  # # note here X_train is still having 7 features only 6 is used\n\nprint('best dt score is: ', grid_search_dt.best_score_)\nprint('best dt score after feature selection is: ', grid_search_dt_updated.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:47.348311Z","iopub.execute_input":"2021-07-20T20:34:47.348593Z","iopub.status.idle":"2021-07-20T20:34:52.687891Z","shell.execute_reply.started":"2021-07-20T20:34:47.348565Z","shell.execute_reply":"2021-07-20T20:34:52.686919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we remove unimportant features: Parch and Embarked and see the model is affected\n# result: no difference with less features!!\nnum_features = ['Age', 'SibSp', 'Fare']\ncat_features = ['Sex', 'Pclass']\n\n# you must update preprocess and pipeline after changing the feature list\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num_pipeline', num_pipeline, num_features),\n        ('cat_pipeline', cat_pipeline, cat_features),\n    ]\n)\n\npipeline_dt = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor),\n        ('clf_dt', DecisionTreeClassifier()),\n    ]\n)\n\n# update the grid search \ngrid_search_dt_updated = GridSearchCV(pipeline_dt, param_grid_dt, cv=10, scoring='accuracy')\n\n# train the model using the updated full pipeline\ngrid_search_dt_updated.fit(X_train, y_train) # note here X_train is still having 7 features only 5 is used\n\nprint('best dt score is: ', grid_search_dt.best_score_)\nprint('best dt score after feature selection is: ', grid_search_dt_updated.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:52.689093Z","iopub.execute_input":"2021-07-20T20:34:52.68937Z","iopub.status.idle":"2021-07-20T20:34:58.159264Z","shell.execute_reply.started":"2021-07-20T20:34:52.689344Z","shell.execute_reply":"2021-07-20T20:34:58.158174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need to split the data to make X_train expect 5 features instead of 7\n\n# drop 'Parch', 'Embarked'\nX = df.drop(['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin', 'Parch', 'Embarked'], axis=1)\ny = df['Survived']\n\n# re-split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# updated feature lists\nnum_features = ['Age', 'SibSp', 'Fare']\ncat_features = ['Sex', 'Pclass']\n\n# updated preprocess and pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num_pipeline', num_pipeline, num_features),\n        ('cat_pipeline', cat_pipeline, cat_features),\n    ]\n)\n\npipeline_dt = Pipeline(\n    steps=[\n        ('preprocessor', preprocessor),\n        ('clf_dt', DecisionTreeClassifier()),\n    ]\n)\n\n# updated the grid search \ngrid_search_dt_updated = GridSearchCV(pipeline_dt, param_grid_dt, cv=10, scoring='accuracy')\n\n# train the model using the updated full pipeline\ngrid_search_dt_updated.fit(X_train, y_train) # note here X_train is still having 7 features only 5 is used\n\nprint('best dt score is: ', grid_search_dt.best_score_)\nprint('best dt score after feature selection is: ', grid_search_dt_updated.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:34:58.160878Z","iopub.execute_input":"2021-07-20T20:34:58.161284Z","iopub.status.idle":"2021-07-20T20:35:03.40882Z","shell.execute_reply.started":"2021-07-20T20:34:58.161241Z","shell.execute_reply":"2021-07-20T20:35:03.407803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reassign the best model to have only 5 features\nclf_best = grid_search_dt_updated.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:35:03.410178Z","iopub.execute_input":"2021-07-20T20:35:03.410521Z","iopub.status.idle":"2021-07-20T20:35:03.414667Z","shell.execute_reply.started":"2021-07-20T20:35:03.41049Z","shell.execute_reply":"2021-07-20T20:35:03.413706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Persist the Model\nThe following code shows how to save the trained model as a pickle file, which can be loaded in to make predictions.","metadata":{}},{"cell_type":"code","source":"# Save the model as a pickle file\nimport joblib\njoblib.dump(clf_best, \"clf-best.pickle\")","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:35:03.416084Z","iopub.execute_input":"2021-07-20T20:35:03.416477Z","iopub.status.idle":"2021-07-20T20:35:03.434153Z","shell.execute_reply.started":"2021-07-20T20:35:03.416434Z","shell.execute_reply":"2021-07-20T20:35:03.433188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the model from a pickle file\nsaved_tree_clf = joblib.load(\"clf-best.pickle\")\nsaved_tree_clf","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:35:03.435319Z","iopub.execute_input":"2021-07-20T20:35:03.435932Z","iopub.status.idle":"2021-07-20T20:35:03.539396Z","shell.execute_reply.started":"2021-07-20T20:35:03.435889Z","shell.execute_reply":"2021-07-20T20:35:03.538753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"passenger1 = pd.DataFrame(\n    {\n        'Pclass': [3],\n        'Sex': ['male'], \n        'Age': [23],\n        'SibSp': [0],\n        'Fare': [5.5],\n    }\n)\npassenger1","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:35:03.540311Z","iopub.execute_input":"2021-07-20T20:35:03.54069Z","iopub.status.idle":"2021-07-20T20:35:03.552405Z","shell.execute_reply.started":"2021-07-20T20:35:03.540654Z","shell.execute_reply":"2021-07-20T20:35:03.551744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"passenger2 = pd.DataFrame(\n    {\n        'Pclass': [1],\n        'Sex': ['female'], \n        'Age': [21],\n        'SibSp': [0],\n        'Fare': [80],\n    }\n)\npassenger2","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:35:03.553642Z","iopub.execute_input":"2021-07-20T20:35:03.553986Z","iopub.status.idle":"2021-07-20T20:35:03.572222Z","shell.execute_reply.started":"2021-07-20T20:35:03.553956Z","shell.execute_reply":"2021-07-20T20:35:03.571316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# died\npred1 = saved_tree_clf.predict(passenger1)\npred1","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:35:03.573507Z","iopub.execute_input":"2021-07-20T20:35:03.573833Z","iopub.status.idle":"2021-07-20T20:35:03.592426Z","shell.execute_reply.started":"2021-07-20T20:35:03.573793Z","shell.execute_reply":"2021-07-20T20:35:03.59163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# survived\npred2 = saved_tree_clf.predict(passenger2)\npred2","metadata":{"execution":{"iopub.status.busy":"2021-07-20T20:35:03.593476Z","iopub.execute_input":"2021-07-20T20:35:03.593913Z","iopub.status.idle":"2021-07-20T20:35:03.61913Z","shell.execute_reply.started":"2021-07-20T20:35:03.593872Z","shell.execute_reply":"2021-07-20T20:35:03.617818Z"},"trusted":true},"execution_count":null,"outputs":[]}]}