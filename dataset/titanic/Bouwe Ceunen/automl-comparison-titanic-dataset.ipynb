{"cells":[{"metadata":{},"cell_type":"markdown","source":"# AutoML Comparison On Titanic Dataset"},{"metadata":{},"cell_type":"markdown","source":"We are going to try the following AutoML libraries and train an XGBoost as a baseline.\n\n- [TPOT](https://github.com/EpistasisLab/tpot)\n- [AutoGluon](https://github.com/awslabs/autogluon)\n- [AutoSklearn](https://github.com/automl/auto-sklearn)\n- [H2OAutoML](https://github.com/h2oai/h2o-3)\n- [AutoKeras](https://github.com/keras-team/autokeras)\n- [MLJarSupervised](https://github.com/mljar/mljar-supervised)\n- [HyperOptSklearn](https://github.com/hyperopt/hyperopt-sklearn)"},{"metadata":{},"cell_type":"markdown","source":"## <center style=\"background-color: #6dc8b5; width:30%;\">Contents</center>\n* [Import Libraries](#Import)\n* [Load Data](#Load)\n* [Visualize Data](#Visualize)\n* [Preprocess Data](#Preprocess)\n* [Train Models](#Train)\n    1. [XGBoost](#XGBoost)\n    2. [AutoSklearn](#AutoSklearn)\n    3. [HyperOptSklearn](#HyperOptSklearn)\n    4. [TPOT](#TPOT)\n    5. [AutoGluon](#AutoGluon)\n    6. [H2OAutoML](#H2OAutoML)\n    7. [AutoKeras](#AutoKeras)\n    8. [MLJarSupervised](#MLJarSupervised)\n* [Submission File](#Submission)\n* [Cleanup](#Cleanup)"},{"metadata":{"papermill":{"duration":0.022078,"end_time":"2020-12-11T07:29:57.004545","exception":false,"start_time":"2020-12-11T07:29:56.982467","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"Import\"></a>\n# Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n# https://github.com/parrt/dtreeviz/issues/108\n# updated versions are needed for MLJarSupervised\n! pip3 install graphviz==0.15.0\nimport graphviz\nprint(graphviz.__version__)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-11T07:30:22.227522Z","iopub.status.busy":"2020-12-11T07:30:22.22675Z","iopub.status.idle":"2020-12-11T07:30:36.263975Z","shell.execute_reply":"2020-12-11T07:30:36.263256Z"},"papermill":{"duration":14.10465,"end_time":"2020-12-11T07:30:36.264092","exception":false,"start_time":"2020-12-11T07:30:22.159442","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"import os\nimport time\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport warnings\nimport logging\nfrom warnings import simplefilter\n\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":0.061383,"end_time":"2020-12-11T07:30:36.8861","exception":false,"start_time":"2020-12-11T07:30:36.824717","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"Load\"></a>\n# Load Data"},{"metadata":{"execution":{"iopub.execute_input":"2020-12-11T07:30:37.020656Z","iopub.status.busy":"2020-12-11T07:30:37.019881Z","iopub.status.idle":"2020-12-11T07:30:37.071248Z","shell.execute_reply":"2020-12-11T07:30:37.070555Z"},"papermill":{"duration":0.125328,"end_time":"2020-12-11T07:30:37.07136","exception":false,"start_time":"2020-12-11T07:30:36.946032","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/titanic/train.csv')\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-11T07:30:37.203155Z","iopub.status.busy":"2020-12-11T07:30:37.202465Z","iopub.status.idle":"2020-12-11T07:30:37.230014Z","shell.execute_reply":"2020-12-11T07:30:37.229393Z"},"papermill":{"duration":0.09528,"end_time":"2020-12-11T07:30:37.230134","exception":false,"start_time":"2020-12-11T07:30:37.134854","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/titanic/test.csv')\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-11T07:30:37.366277Z","iopub.status.busy":"2020-12-11T07:30:37.36547Z","iopub.status.idle":"2020-12-11T07:30:37.383754Z","shell.execute_reply":"2020-12-11T07:30:37.38307Z"},"papermill":{"duration":0.088804,"end_time":"2020-12-11T07:30:37.383872","exception":false,"start_time":"2020-12-11T07:30:37.295068","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/titanic/gender_submission.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"Visualize\"></a>\n# Visualize Data"},{"metadata":{},"cell_type":"markdown","source":"Check the NaN values which we will later solve."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train_data.isnull(), cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(test_data.isnull(), cbar=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check the outliers (if there are any) we will remove."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(data=train_data, x='Sex', y='Age', hue=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(data=train_data, x='Sex', y='Fare', hue=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(data=train_data, x='Sex', y='Pclass', hue=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(data=train_data, x='Sex', y='Parch', hue=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(data=train_data, x='Sex', y='SibSp', hue=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"Preprocess\"></a>\n# Preprocess Data"},{"metadata":{},"cell_type":"markdown","source":"### Impute/Remove NaN Values"},{"metadata":{},"cell_type":"markdown","source":"First we are going to impute/remove the NaN values."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\ndef impute_nan_values(data, column):\n    imr = SimpleImputer(missing_values=np.nan, strategy='median')\n    print(f\"Number of {column} NaN values before impute: {data[column].isnull().sum().sum()}\")\n    imr = imr.fit(data[[column]])\n    data[column] = imr.transform(data[[column]]).ravel()\n    print(f\"Number of {column} NaN values after impute: {data[column].isnull().sum().sum()}\")\n\ndef remove_nan_values(data, column):\n    print(f\"Number of {column} NaN values before impute: {data[column].isnull().sum().sum()}\")\n    _data = data[data[column].notnull()]\n    print(f\"Number of {column} NaN values after impute: {_data[column].isnull().sum().sum()}\")\n    return _data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in train_data.columns:\n    print(f\"{column}: {str(sum(train_data[column].isnull()))} missing values\")\n\nimpute_nan_values(train_data, 'Age')\ntrain_data = remove_nan_values(train_data, 'Embarked')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for column in test_data.columns:\n    print(f\"{column}: {str(sum(test_data[column].isnull()))} missing values\")\n\nimpute_nan_values(test_data, 'Age')\nimpute_nan_values(test_data, 'Fare')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove Outliers"},{"metadata":{},"cell_type":"markdown","source":"Secondly we are going to remove outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nUsage of 'Z-score' (z = x – μ / σ) to find outliers\n\"\"\"\ndef outliers_z_score(data):\n    outliers=[]\n    threshold = 6\n\n    mean_y = np.mean(data)\n    stdev_y = np.std(data)\n\n    for i in data:\n        z_score = (i-mean_y) / stdev_y\n        if np.abs(z_score) > threshold:\n            outliers.append(i)\n    return outliers","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Age feature\nage_outliers = outliers_z_score(train_data['Age'])\nprint(f\"Age outliers: {age_outliers}\")\nfor ao in age_outliers:     \n    train_data = train_data[train_data.Age != ao]\n\n\n# Fare feature\nfare_outliers = outliers_z_score(train_data['Fare'])\nprint(f\"Fare outliers: {fare_outliers}\")\nfor fo in fare_outliers:     \n    train_data = train_data[train_data.Fare != fo]\n    \n    \n# Parch feature\nfare_outliers = outliers_z_score(train_data['Parch'])\nprint(f\"Parch outliers: {fare_outliers}\")\nfor po in fare_outliers:     \n    train_data = train_data[train_data.Parch != po]\n\n# SibSp feature\nsibsp_outliers = outliers_z_score(train_data['SibSp'])\nprint(f\"SibSp outliers: {sibsp_outliers}\")\nfor so in sibsp_outliers:     \n    train_data = train_data[train_data.SibSp != so]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(data=train_data, x='Sex', y='Fare', hue=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(data=train_data, x='Sex', y='Age', hue=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(data=train_data, x='Sex', y='Parch', hue=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(data=train_data, x='Sex', y='SibSp', hue=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Without the outliers it already looks much better!"},{"metadata":{},"cell_type":"markdown","source":"### Drop redundant columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"# not going to use these columns to train/test on\ntrain_data.drop(['Name', 'PassengerId', 'Cabin', 'Ticket'], inplace=True, axis=1)\ntest_data.drop(['Name', 'PassengerId', 'Cabin', 'Ticket'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Categorical To Numerical Columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\ntrain_data[\"Embarked\"] = label_encoder.fit_transform(train_data[\"Embarked\"])\ntrain_data[\"Sex\"] = label_encoder.fit_transform(train_data[\"Sex\"])\n\ntest_data[\"Embarked\"] = label_encoder.fit_transform(test_data[\"Embarked\"])\ntest_data[\"Sex\"] = label_encoder.fit_transform(test_data[\"Sex\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_data.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(train_data.corr(), annot=True, linewidths=1, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Small recap about the fiels:\n* sibsp - Number of Siblings/Spouses Aboard\n* parch - Number of Parents/Children Aboard\n* class - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n\nWe can several things on this corr plot:\n- the higher the fare the lower the Pclass (that indicates higher class) => negative correlation of -0.61\n- the higher the survival rate the lower the sex (that indicates female 0 / male 1) => negative correlation of -0.55\n- the higher the Parch the higher the SibSp (that indicates large families) => positive correlation of 0.40"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"Train\"></a>\n# Train Models"},{"metadata":{},"cell_type":"markdown","source":"I had a hard time getting everything installed in one environment, there were a lot of package versions clashing because each AutoML library needed specific versions. I install each AutoML library when I need to, although this might break other installations."},{"metadata":{"execution":{"iopub.execute_input":"2020-12-11T07:30:38.006984Z","iopub.status.busy":"2020-12-11T07:30:38.006246Z","iopub.status.idle":"2020-12-11T07:30:38.018604Z","shell.execute_reply":"2020-12-11T07:30:38.017817Z"},"papermill":{"duration":0.084194,"end_time":"2020-12-11T07:30:38.018729","exception":false,"start_time":"2020-12-11T07:30:37.934535","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"target = train_data['Survived']\ntrain_data.drop(['Survived'], inplace=True, axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(train_data, target, test_size=0.25, random_state=42, shuffle=False)\nprint(f'Sizes: X_train={X_train.shape}, y_train={y_train.shape}, X_test={X_test.shape}, y_test={y_test.shape}')\n\n# will need this later on for AutoGluon\nX_train_with_target = X_train.copy()\nX_train_with_target['Survived'] = target\n\nprint(f'Sizes: X_train_with_targer={X_train_with_target.shape}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check one last time for null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(X_train.isnull(), cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_model = None\nbest_model_name = None\nbest_model_acc = 0.0\n\nmodels = []\n\ndef validate_model(model_name, model, accuracy):\n    global best_model, best_model_name, best_model_acc, models\n    \n    models.append([model_name, accuracy])\n\n    print()\n    print(f\"Current accuracy of model {model_name}: {accuracy}\")\n    print(f\"Previous best accuracy of model {best_model_name}: {best_model_acc}\")\n\n    if accuracy > best_model_acc:\n        print(f\"Improved previous accuracy!\")\n        best_model_acc = accuracy\n        best_model = model\n        best_model_name = model_name\n    else:\n        print(f\"Did not improve previous accuracy.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"XGBoost\"></a>\n### XGBoost"},{"metadata":{},"cell_type":"markdown","source":"We are going to train an xgboost as a baseline."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom xgboost import XGBClassifier\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\nxgboost_model = XGBClassifier(tree_method='gpu_hist')\nxgboost_model.fit(X_train, y_train)\ny_preds_xgboost = xgboost_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_xgboost)\nvalidate_model('xgboost', xgboost_model, accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"AutoSklearn\"></a>\n### AutoSklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n%%bash\n# https://github.com/automl/auto-sklearn/issues/101\napt-get remove swig\napt-get install swig3.0\nln -s /usr/bin/swig3.0 /usr/bin/swig\npip3 install pyrfr\n# https://stackoverflow.com/questions/55833509/attributeerror-type-object-callable-has-no-attribute-abc-registry\npip3 uninstall -y typing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n%%bash\n# actual installation\n# curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip3 install\npip3 install auto-sklearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport autosklearn.classification\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# set time_left_for_this_task to prevent trail getting stuck (default 3600 seconds)\nauto_sklearn_model = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=3600, n_jobs=-1)\n\nauto_sklearn_model.fit(X_train, y_train)\ny_preds_autosklearn = auto_sklearn_model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_autosklearn)\nvalidate_model('autosklearn', auto_sklearn_model, accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"HyperOptSklearn\"></a>\n### HyperOptSklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n%%bash\nrm -rf hyperopt-sklearn\ngit clone https://github.com/hyperopt/hyperopt-sklearn.git\n(cd hyperopt-sklearn && pip3 install -e .)\nmv hyperopt-sklearn/hpsklearn /opt/conda/lib/python3.7/site-packages/hpsklearn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! export OMP_NUM_THREADS=1\nos.environ['OMP_NUM_THREADS'] = \"1\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom hpsklearn import HyperoptEstimator, any_classifier, any_preprocessing\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# setting seed to avoid trail getting stuck\nnp.random.seed(42)\n\n# set max_evals to prevent too long search (default 100)\n# set trail timeout to prevent trail getting stuck (default None)\nestim = HyperoptEstimator(\n    classifier=any_classifier('my_clf'),\n    preprocessing=any_preprocessing('my_pre'),\n    n_jobs=-1,\n    max_evals=40,\n    trial_timeout=400\n)\n\nestim.fit(X_train, y_train)\ny_preds_hyperopt = estim.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_hyperopt)\nvalidate_model('hyperopt', estim, accuracy)\n\nprint(estim.best_model())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"TPOT\"></a>\n### TPOT"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n%%bash\npip3 install tpot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom tpot import TPOTClassifier\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# set generations and population_size to prevent too long search (default 100 both)\ntpot_classifier = TPOTClassifier(generations=50, population_size=50, verbosity=2, n_jobs=-1)\ntpot_classifier.fit(X_train, y_train)\ny_preds_tpot = tpot_classifier.predict(X_test)\n\ntpot_classifier.export('tpot_pipeline.py')\n\naccuracy = accuracy_score(y_test, y_preds_tpot)\nvalidate_model('tpot', tpot_classifier, accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"AutoGluon\"></a>\n### AutoGluon"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n%%bash\npython3 -m pip install --upgrade \"mxnet<2.0.0\"\npip3 install autogluon autogluon.tabular\n# https://github.com/awslabs/autogluon/issues/810\npip3 install --upgrade pillow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom autogluon.tabular import TabularPrediction as task\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# autogluon needs target in the training_data\npredictor = task.fit(train_data=X_train_with_target, label='Survived')\ny_preds_autogluon = predictor.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_autogluon)\nvalidate_model('autogluon', predictor, accuracy)\n\nprint(predictor.leaderboard())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"H2OAutoML\"></a>\n### H2OAutoML"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n%%bash\npip3 install h2o","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport h2o\nfrom h2o.sklearn import H2OAutoMLClassifier\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\nh2o.init()\n\n# set max_runtime_secs to prevent too long search (default 3600)\naml = H2OAutoMLClassifier(max_runtime_secs=3600)\n\naml.fit(X_train, y_train.values)\ny_preds_h2o = aml.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_h2o)\nvalidate_model('H2OautoML', aml, accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"AutoKeras\"></a>\n### AutoKeras"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n%%bash\n# https://github.com/tensorflow/tensorflow/issues/42441\npip3 install autokeras emcee pyDOE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimport autokeras as ak\nimport tensorflow as tf\ntf.get_logger().setLevel(logging.ERROR)\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# set max_trials to prevent too long search (default 100)\nclf = ak.StructuredDataClassifier(overwrite=True, max_trials=100)\nclf.fit(x=X_train, y=y_train)\ny_preds_autokeras = clf.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_autokeras)\nvalidate_model('autokeras', clf, accuracy)\n\nprint(clf.export_model().summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"MLJarSupervised\"></a>\n### MLJarSupervised"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture\n%%bash\npip3 install mljar-supervised\npip3 install matplotlib==3.1.3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom supervised.automl import AutoML\nwarnings.filterwarnings('ignore')\nlogging.captureWarnings(True)\nsimplefilter(action='ignore', category=FutureWarning)\n\n# https://github.com/mljar/mljar-supervised#available-modes-books\n# set total_time_limit to prevent too long search (default 3600 seconds)\n# features_selection causes issues with xgboost on gpu\nautoml = AutoML(\n    mode=\"Compete\",\n    stack_models=True,\n    train_ensemble=True,\n    total_time_limit=3600,\n    features_selection=False\n)\n\nautoml.fit(X_train, y_train)\ny_preds_mljar = automl.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_preds_mljar)\nvalidate_model('mljar-supervised', automl, accuracy)\n\nautoml.get_leaderboard()","execution_count":null,"outputs":[]},{"metadata":{"papermill":{"duration":2.480117,"end_time":"2020-12-11T07:51:41.69986","exception":false,"start_time":"2020-12-11T07:51:39.219743","status":"completed"},"tags":[]},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"Submission\"></a>\n# Submission File"},{"metadata":{"trusted":true},"cell_type":"code","source":"models_df = pd.DataFrame(models, columns=['model_name', 'accuracy'])\nmodels_df.sort_values(by=['accuracy'], ascending=False, inplace=True)\nmodels_df = models_df.reset_index(drop=True)\nmodels_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Below we print out the best performing AutoML model. Let's use this model to generate predictions for our final submission."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(best_model_name)\nprint(best_model)\nprint(best_model_acc)","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2020-12-11T07:51:46.709507Z","iopub.status.busy":"2020-12-11T07:51:46.708769Z","iopub.status.idle":"2020-12-11T07:51:47.256606Z","shell.execute_reply":"2020-12-11T07:51:47.255618Z"},"papermill":{"duration":3.111956,"end_time":"2020-12-11T07:51:47.256758","exception":false,"start_time":"2020-12-11T07:51:44.144802","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"y_preds = best_model.predict(test_data)\nsubmission['Survived'] = y_preds.ravel().astype(int)\nsubmission.to_csv('submission.csv', index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sidenote: it was really really really difficult to get all of these AutoML algorithms to work in one notebook, I've encountered a lot of dependency issues. If you ever use AutoML, pick one to run in your notebook."},{"metadata":{},"cell_type":"markdown","source":"TODO: incorporate [AutoPyTorch](https://github.com/automl/Auto-PyTorch)"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"Cleanup\"></a>\n# Cleanup"},{"metadata":{"trusted":true},"cell_type":"code","source":"! rm -rf */","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}