{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic Challenge","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* This one can be in top 3% if you try a few times\n* After figuring out that I have classified many who did not have a good chance to survive but actully survived as \"Not Survived\", I have added a random feature to make some of them become 0 to 1, and then I could enter the range inside top 3% \n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 1. Import libraries & data  데이터준비 및 모듈 임포트","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# They are for data manipulation/ 기본 데이터 정리 및 처리\nimport pandas as pd\nimport numpy as np\n\n# For Visualization / 시각화\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\nimport missingno\n\n# For preprocessing and ML algorithms / 전처리 및 머신 러닝 알고리즘\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Tunning and Evaluation / 모델 튜닝 및 평가\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import model_selection\n\n# Ignore warnings / 경고 제거 (Pandas often makes warnings)\nimport sys\nimport warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### CSV to DF\n\nimport modules -> import data -> turning data into data frame (pandas) \n\n* 이어서 이를 데이터프레임에 임포트하여 데이터셋이 판다스 데이터프레임에 임포트 되도록 합니다.\n* Import the data into the dataframe so the dataset is converted as a pandas dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This may be harder than expected for a firsttimer, but if the data was not delivered correctly from the copy, press \"+ Add Data\", load \"Titanic Data\" from \"Competition Data\", and click each file to check the path address.\n# 이 것이 처음하는 사람에게 예상보다 어려울 수 있는데 복사한 것에서 데이터가 전달이 잘 안 되었다면 \"+Add Data\" 누르시고 'Competition Data'에서 \"Titanic Data\" 불러온 후 파일을 찍어서 경로 주소 확인해야 함 \ntest = pd.read_csv('../input/titanic/test.csv')\ntrain = pd.read_csv('../input/titanic/train.csv')\n\n# Now csv files, test and train, have become data frames.    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Basic operations with dataframes","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"`head()` shows first five rows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`tail()` shows last five rows","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`describe()` shows basic statistics for columns. By default it shows information only about continuous features. You can see information about all features by setting `include='all'`","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`dtypes` shows types of all columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`info()` is an advanced version of `dtypes` which shows not only types, but also the number of non-null values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`columns` shows the list of all columns in dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can slice the list of columns like a usual python list object","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns[3], train.columns[3:5]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You can select several rows of dataframe by indexing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[5:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`shape` shows number of rows and number of columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. 파일 분리 및 병합 (File Separations and Merges) \n\n* Secure the shape of ntrain and ntest. (Preparation for later splitting from the merged one)\n* y_train is a known result.\n* Separate the passenger ID of the test as it will be added to the final result later.\n* Merge train and test to create a file called data. When you convert a string to a number, or group numbers by interval, it is easier for us to put them together to do it all at once.\n* ntrain과 ntest의 shape을 확보해놓습니다. (병합 한 것을 나중에 다시 갈라 놓기 위한 준비)\n* y_train은 알려진 결과 값이니 따로 모셔 놓고\n* 테스트의 승객 아이디는 나중에 최종 결과에 넣을 것이기 때문에 따로 떼어 놓습니다.\n* train과 test를 병합하여 data 란 파일을 만듭니다. 문자로 된 것을 숫자로 바꾼다든가.숫자를 인터발 별로 그룹화 한다든가 할 때 한꺼번에 하기 위해 합해 놓습니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 병합 준비\nntrain = train.shape[0]\nntest = test.shape[0]\n\n# 아래는 따로 잘 모셔 둡니다.\ny_train = train['Survived'].values\npassId = test['PassengerId']\n\n# 병함 파일 만들기\ndata = pd.concat((train, test))\n\n# 데이터 행과 열의 크기는\nprint(\"data size is: {}\".format(data.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Survived'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that 342 people survived and 549 didn't survive","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 3. Check the files / 파일을 보겠습니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"missingno.matrix(data, figsize = (15,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This plot shows missing values in each column and in each row. Note that in the lower half of `Survived` column all the values are missing. This is because these rows are from test data.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().sum() #비어 있는 값들을 체크해 본다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Age.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Names of columns / 열 이름을 보겠습니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Features 항목\n\n#### 항목의 종류 There are many types of features.\n* 범주형 항목 (Categorical Features)\n\n범주형 변수는 둘 이상의 결과 요소가 있는 변수이며 해당 기능의 각 값을 범주별로 분류 할 수 있습니다. 예를 들어 성별은 두 가지 범주 (남성과 여성)의 범주 형 변수입니다. 이산형 변수(discrete variable) = 범주형 변수 (categorical variable) 의 하나로 명목 변수 norminal variable 라고도합니다.\n\nCategorical variables are variables that have more than one result element, and each value of that function can be classified by category. For example, gender is a categorical variable in two categories (male and female). Discrete variable = It is one kind of categorical variable, and is also known as nominal variable .\n\n* 데이터 셋에서 명목 항목 : Sex, Embark 이며 우리는 Name, Ticket 등을 이로 변환해야 할 것 같습니다. The nominal items in the data set are: Sex, Embark and we have to convert Name, Ticket, etc to numbers.\n\n* Ordinal Variable :\n\n순위 변수는 범주 형의 하나지만 그 차이점은 값 사이의 상대 순서(=서열) 또는 정렬이 가능하다는 것입니다.\n\nOrdinal variables are one of the categorical types, but the difference is the relative order (= sequence) or sorting between the values.\n\n데이터 셋에서 순위 항목 : PClass 이며 우리는 Cabin을 이 범주로 변환해서 사용해야 할 것 같습니다.\n\nOrdinal variables in the data set: PClass and we think we should convert Cabin to this category.\n\n* 연속형 항목 (Continuous Features):\n\n서로 연속된 값을 가진 변수를 가진 항목이며 여기에서 우리는 연령을 대표적인 것으로 볼 수 있습니다.\n\nThis is an item with variables with continuous values, age is one good sample\n\nAge, SipSp, Parch, Fare는 interval variable로 만들어 이에 적용해야 할 것 같습니다.\n\nAge, SipSp, Parch, and Fare should be catgorized to the interval variable.\n\n* Feature Information\n\n          Variable        Definition                Key\n\n          survival          Survival                    0 = No, 1 = Yes\n\n          pclass          Ticket class                1 = 1st, 2 = 2nd, 3 = 3rd\n\n          sex              Sex    \n\n          Age              Age in years    \n\n          sibsp              # of siblings / spouses aboard the Titanic    \n\n          parch              # of parents / children aboard the Titanic    \n\n          ticket          Ticket number    \n\n          fare              Passenger fare    \n\n          cabin              Cabin number    \n\n          embarked          Port of Embarkation         C = Cherbourg, Q = Queenstown, S = Southampton","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## 5. 데이터 탐구  Exploratory Data Analysis\n\n* train파일 순서대로 데이터 파일의 열들을 봅니다. \n* Let's view the columns of the data file in the order in the train file.\n\n![](https://1.bp.blogspot.com/-rBTabaGeOTo/XicYRmv9s7I/AAAAAAAAKts/WQDUpGJbv20xbAO8vfnOkqtbCHyme3zNQCLcBGAsYHQ/s640/grey%2Barea.png)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 파일 각 열의 상관 관계를 보겠습니다.\n\nLet's take a look at the correlation of each column in the file.\n\nCo-relation 매트릭스는 seaborn에서 변수 간 상관 계수를 보여주는 표입니다. 표의 각 셀은 두 변수 간의 상관 관계를 보여줍니다. 상관 매트릭스는 고급 분석에 대한 입력 및 고급 분석에 대한 진단으로 데이터를 요약하는 데 사용됩니다. 참고: https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n\nCo-relation matrix is a table showing the correlation coefficient between variables in seaborn. Each cell in the table shows a correlation between two variables. Correlation matrices are used to summarize data as input to advanced analysis and as diagnostics for advanced analysis. Note: https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n\n아래 마스크 셋업은 0로 행렬을 상관 행렬과 같은 모양으로 만든 후 여기에 불리안 값을 넣고 이를 다시 True만 만듭니다.\n\nThe mask setup below makes a matrix that looks like a correlation matrix with zeros, then puts a Boolean value into it and makes it true.\n\ntriu 는 우측 상단 삼각행렬을 의미\n\ntriu means upper right triangle\n\nannot= True는 각 셀에 숫자를 표시하라는 것이고, False는 하지 말라는 것이구요\n\nannot = True means to display a number in each cell, False means oppposite. https://seaborn.pydata.org/generated/seaborn.heatmap.html\n\n이어서 이를 heatmap으로 런칭합니다.\n\nThen launch it as a heatmap.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Co-relation 매트릭스\ncorr = data.corr()\n# 마스크 셋업\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# 그래프 셋업\nplt.figure(figsize=(14, 8))\n# 그래프 타이틀\nplt.title('Overall Correlation of Titanic Features', fontsize=18)\n#  Co-relation 매트릭스 런칭\nsns.heatmap(corr, mask=mask, annot=False,cmap='RdYlGn', linewidths=0.2, annot_kws={'size':20})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* \"Surived\" 분석\n\n한 열씩 검토해 보겠습니다.\n\nWe will look at the columns one by one\n\nSurvived - Key: (0 - Not Survived, 1- Survived)\n\nSurvived는 수자로 값을 주지만 Categorical Variable인 셈입니다.\n\n죽던지 살던지 둘 중 하나의 값을 줍니다.\n\ncountplot을 그려 봅니다.\n\n사이즈는 가로 10인치 세로 2인치\n\n생존 여부 0과 1의 숫자를 세어 본 후 그림을 그리도록 명령을 하는 것입니다.\n\npyplot(plt)의 figure라는 메소드를 써서 그림판의 크기를 정하고, seaborn의 카운트플롯을 그리라는 것입니다.\n\n\"Survived\" gives a number, but it's a categorical variable.\n\nEither Survived or Not Survived\n\nDraw a countplot.\n\nSize is 10 inches wide by 2 inches long\n\nSurvival counts\n\nUse pyplot (plt) figure method to size the paint and draw a seaborn count plot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10,2))\nsns.countplot(y='Survived', data=train)\nprint(train.Survived.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 불행히도 사망자가 훨씬 많아 보입니다.\n* 전체 사망자 비율을 좀 보겠습니다.\n \n* Unfortunately, there are more deaths than \"survived\".\n \n* Let's take a look at the overall rate.\n* 파이그래프랑 카운트 플롯을 서브플롯으로 그립니다.\n* 행은 하나 열은 2개의 서브 플롯입니다. 사이즈는 가로 15인치 세로 6인치\n* 'Survived'의 값을 카운트해서 파이플롯을 만듭니다.\n* explode는 폭발하는 것이니까 1이면 튀어 나가는 것인데 0을 주면 분리만 되고 돌출은 되지 않습니다. 이어서 0, 1인 것은 첫 번째 것은 아니고 두번 째 것은 분리된다는 의미로 생각하시면 됩니다.\n* autopercent는 1.1이 표현하는 부분은 소수점 한 자리까지 보여 주라는 의미입니다. 뒤에 점 이하가 4면 둘 다 소수점 4자리수 까지 보여 줍니다.\n* ax[0]은 첫번째 칸입니다.\n* set_title 메소드는 서브 플롯의 제목을 보여 줍니다.\n \n* Draw a pie chart and count plot in the subplot parts.\n \n* Two rows and one column. Size : 15 inches wide by 6 inches high\n* Create a pieplot by counting the value of 'Survived'.\n* Explode means \"protrusion\", so if it's 1, it's popping out. 0 and 1 means that the second one is poping out.\n* Autopercent means 1.1 shows up to one decimal place. If there is 4 after the dot, both show up to four decimal places.\n* ax [0] is the first cell.\n* The set_title method displays the title of the subplot.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1, 2, figsize=(15, 6))\ntrain['Survived'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=train, ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 위의 것을 아래와 같이 함수로 만들겠습니다. (물론 자주 쓰이지는 않겠지만 연습이니)\n* Let's make the above as a function. (it's not going to be used often, but it's a practice for making a function)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def piecount(col):\n    f, ax = plt.subplots(1, 2, figsize=(15, 6))\n    train[col].value_counts().plot.pie(explode=[0.1 for i in range(train[col].nunique())], autopct='%1.1f%%', ax=ax[0], shadow=True)\n    ax[0].set_title(col)\n    ax[0].set_ylabel('')\n    sns.countplot(col, data=train, ax=ax[1])\n    ax[1].set_title(col)\n    plt.show()\n\npiecount('Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This function works for features with any number of categories","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"piecount(\"Pclass\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* \"Pclass\" 분석\n\n* Pclass는 값이 숫자이나 서열이 정해진 Ordinal Feature이다.\n* Pclass is an \"Ordinal Feature\" whose values are numerical but sequenced.\n* Key:1 = 1st, 2= 2nd, 3 = 3rd\n* \n* 각 클래스 당 생존자를 보겠습니다.\n* * Let's look at the survivors for each class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby(['Pclass','Survived'])['Survived'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(train.Pclass, train.Survived, margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 1등급 객실의 사람들은 생존자가 더 많고, 2등급은 생존자에 비해 사망자가 조금 더 많으나, 3등급은 사망자가 3배 이상 많다는 것을 알 수 있습니다.\n* The 1st class cabin has more survivors, the 2nd class has a few more deaths than the survivors, but the 3rd class has three times more deaths.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(12, 6))\ntrain[['Pclass','Survived']].groupby(['Pclass']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived per Pcalss')\nsns.countplot('Pclass', hue='Survived', data=train, ax=ax[1])\nax[1].set_title('Pcalss Survived vs Not Survived')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 위에 만든 함수를 한 번 써 먹어 볼까요?\n* Let's use a function we made above.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"piecount(\"Pclass\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* %는 3등칸이 반이 넘으나 위의 그래프에서 생존자는 1등석이 가장 많다는 것을 알 수 있습니다.\n* 각 클래스 당 생존률을 볼까요?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### \"Name\" 분석\n* 이름은 거의 모두 다를 가능성이 큽니다. Family Name, First Name, Middle Name and even Dr. Capt, master and so on 모두 감안하면...\n* 분류를 한 번 해 봅니다.\n* 리스트를 한 번 주 욱 보겠습니다.\n* The names are most likely different...all of them . Family Name, First Name, Middle Name and even Dr. Given Capt, Master and so on ...\n* Try to classify them.\n* Let's look at the list once.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"data.Name.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 이름은 언뜻 보아서 감이 안 옵니다. 중간에 있는 Mr. 같은 호칭을 볼까요.\n* ['Initial']이란 열을 새로 만들어서 여기에 Name에서 추출한 Regular Expression을 넣습니다.\n* 아래에서 str.extract('([A-Za-z]+).')부분은 str에서 대문자 A~Z, 소문자 a~z 중에 . 명령을 통해 .으로 끝나는 부분을 추출해 내는 것입니다.\n* ('^([A-Za-z]+)')으로 하면 처음에 나오는 문자 덩어리가 될 것이고 +를 빼면 첫 스펠링 한캐릭터만 추출합니다.\n \n* The name doesn't seem to have any meaning at first glance.\n \n* Create a new column called ['Initial'] and put the regular expressions extracted from Name.\n* The str.extract ('([A-Za-z] +) .') Part of the lower part of the str is the . Is to extract the part that ends with.\n* ('^ ([A-Za-z] +)') will be the first chunk of characters, and minus + will extract only the first spelled character.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* 안전을 위해 카피를 하나 만들어서 새로운 항목을 만들어 봅니다.\n* Just in case, let's make a copy of the df and make new features on it.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = data.copy()\ntemp['Initial'] = 0\ntemp['Initial'] = data.Name.str.extract('([A-Za-z0-9]+)\\.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that several titles are common, but most of them are rare.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Initial'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이를 성별로 봅니다.\nView it by gender.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(temp.Initial, temp.Sex).T.style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 생존률로 봅니다.\n* by survival percentage","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def survpct(col):\n    return temp.groupby(col)['Survived'].mean()\n\nsurvpct('Initial')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 생존 숫자로 봅니다.\n* by number","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* test 에 있는 Dona의 나이를 보고 어디에 넣을지 보겠습니다.\n* Ms. 는 현대처럼 Miss + Mrs를 합친 말이 아니라 당시에는 귀족미망인을 의미하는 것이 었습니다. Mlle나 Mme등도 마드모아젤과 마담의 줄인말일 경우일 것입니다. 귀족 여성들로 보아야겠죠.\n \n* Let's look at Dona's age and decide to where to classify her.\n  \n* Ms. did not mean Miss + Mrs combined like these days, at the time it meant widow. Also Mlle, & Mme may be short for Mademoiselle and Madame. I should see them as women in a higher class.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['LastName'] = data.Name.str.extract('([A-Za-z]+)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(temp.LastName, temp.Survived).T.style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 이제 우리는 Initial에서 Mr.등의 호칭을 뽑아내었고, 성을 뽑아내었습니다.\n* Now we have extracted salutations in 'Initial' and last names in 'LastName'\n\n* 머신이 알파벳보다는 숫자를 좋아 하므로 숫자로 바꿉니다.\n* Since our machine prefers numbers over alphabets, we change them to numbers.\n\n* 아, 그러기 전에 Dona를 처리해야지요.\n* But we have to take care of Dona first.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.loc[temp['Initial'] == 'Dona']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 나이로 추측해서 Mrs.로 넣습니다.\n* Let me guess she is Mrs. based on her age.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.loc[temp['Initial'] == 'Dona', 'Initial'] = 'Mrs'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(temp.Initial, temp.Survived).T.style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Last name 은 전부 숫자로 바꿉니다.\n* Let's change the Last Names to numbers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['NumName'] = temp['LastName'].factorize()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(temp.NumName, temp.Survived).T.style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.loc[temp['LastName'] == 'Ali']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 보시다시피 같은 Last name에 같은 번호가 쓰여졌다.\n* As you see NumnName is the same as their Last Names are the same.\n* The part at the end: [0] means you are only taking the labels, throwing away the uniques that map back to your input.\n* 끝에 [0]은 라벨만 보고 번호를 붙이는 것으로 정말 unique한 것이란 것은 안 본다는 것입니다.\n\n\n* We have turned 'Initial' & 'NumName' representing \"Name\" and \"Last Name\" into numbers. Let's move on.\n* 자 이제 이름을 의미하는 중요한 요소 두 개를 숫자로 바꾸었으니 다음으로 갑니다.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### \"Sex\" 분석\n\n* 함수를 만들어서 train파일을 보지요\n* Let's see the graph Survived / Sex","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['Sex','Survived']].groupby(['Sex']).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def bag(col, target, title, title1):\n    f,ax=plt.subplots(1,2,figsize=(12,5))\n    train.groupby([col])[target].mean().plot(kind='bar', ax=ax[0])\n    ax[0].set_title(title)\n    sns.countplot(col, hue=target, data=train, ax=ax[1])\n    ax[1].set_title(title1)\n    plt.show()\n\nbag('Sex','Survived','Survived per Sex','Sex Survived vs Not Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 배에 있던 남자의 수는 여자의 수보다 훨씬 많습니다. 여전히 생존 여성 수는 남성 수의 거의 두 배입니다. 선박 여성의 생존율은 약 75 % 인 반면 남성의 생존율은 약 18-19 %입니다.\n* The number of men on the ship is much more than the number of women. Still, the number of surviving women is almost twice that of men. The survival rate of ship women is about 75%, while the survival rate of men is about 18-19%.\n* 이 것은 남성/여성을 1,2로 나누면 될 것 같은 뻔해 보이는 것이지만 좀 더 새분화하면 좋아 보입니다.\n* 예를 들어 아기들은 아기이지, 남자인지 여자인지 구명보트 태울 때 안 물어 볼 것이기 때문입니다.\n* 오히려 (불행하게도) 귀족 아기인지 서민의 아기인지는 행과불행을 가를 수 있습니다 ㅠㅠ\n* 생존 Pclass별로 성별을 봅니다.\n* This seems obvious to divide males / females by 1,2, but it looks good if you break it down a bit.\n* For example, babies are just babies and would not be matter whether it is boy or girl.\n* (Unfortunately) whether you are a baby in a higher social class or one from a humble family, it might have mattered.\n* View gender by survival Pclass.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab([train.Sex, train.Survived],train.Pclass,margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 사회는 불공평 했으나 최소한 남자들의 신사도는 있었다고 할 수 있을 것 같습니다.\n* The society was unfair but gentlemenship worked there.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Age 분석\n \n* Age는 Continuous한 값입니다.\n* 빈칸이 많아서 빈칸처리가 결정적인 역할을 할 것 같습니다.\n \n* Age의 최대, 최소, 중간을 보겠습니다.\n \n* Age is a continuous value.\n \n* Because there are a lot of blanks, blank processing seems to play a decisive role.\n \n* Let's take a look at the maximum, minimum, and middle of Age.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Oldest Passenger was', data['Age'].max(), 'Years')\nprint('Youngest Passenger was', data['Age'].min(), 'Years')\nprint('Average Age on the ship was', int(data['Age'].mean()), 'Years')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.swarmplot(x=train['Survived'], y=train['Age'])\nplt.xlabel(\"Survived\")\nplt.ylabel(\"Age\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, ax = plt.subplots(1,2,figsize=(18,8))\nsns.violinplot(\"Pclass\", \"Age\", hue=\"Survived\", data=train, split=True, ax=ax[0])\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0, 110, 10))\nsns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=train, split=True, ax=ax[1])\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0, 110, 10))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 관찰 :\n\n1) Pclass에 따라 어린이 수가 증가하고 10 세 미만의 어린이 (즉, 어린이)의 생존율은 Pclass에 상관없이 양호해 보입니다.\n\n2) Pclass1에서 20-50세의 Passeneger의 생존 가능성은 높고 여성에게는 더 좋습니다.\n\n3) 남성의 경우 생존 확률은 나이가 증가함에 따라 감소합니다.\n\n#### observation :\n\n1) The number of children increases with Pclass, and the survival rate for children under 10 years old (i.e. children) looks good regardless of Pclass.\n\n2) Passeneger, 20-50 years old, in Pclass1 has a higher chance of survival and is better for women.\n\n3) In men, the probability of survival decreases with age.\n\n우선 age의 빈칸 부터 해결 합니다.\n\nFirst, resolve the issue of null values of age.\n\n앞에서 살펴본 것처럼 Age 항목에는 177 null 값이 있습니다. 이러한 NaN 값을 대체하기 위해 데이터 집합의 평균 수명을 지정할 수 있습니다.\n\n그러나 문제는 평균 연령이 29 세를 4세 아이에게 할당 할 수 없습니다. 승객이 어떤 연령대에 있는지 알 수있는 방법이 있을까요? 이름에서 힌트를 찾아 봅니다.\n\nAs we saw earlier, the Age item has a 177 null values. To replace these NaN values, you can specify the average age of the dataset.\n\nBut the problem is that the average age is 29 years old can not be assigned this to a kid. Is there any other way ? Look for some hints in their names.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* 그리고 Initial 별 평균 연령을 보고 Age에 적용 시키는 것이 좋을 것 같습니다.\n* And it is better to see the average age by Initial and apply it to Age.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.groupby('Initial').agg({'Age': ['mean', 'count']}) #이니셜 별 평균 연령 체크","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Assining NaN Age items with mean value of Initials\ntemp = temp.reset_index(drop=True)\n\ntemp['Age'] = temp.groupby('Initial')['Age'].apply(lambda x: x.fillna(x.mean()))\n\ntemp[31:50]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 이제 Initial을 좀 정리합니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Initial'].replace(['Capt', 'Col', 'Countess', 'Don', 'Dona' , 'Dr', 'Jonkheer', 'Lady', 'Major', 'Master',  'Miss'  ,'Mlle', 'Mme', 'Mr', 'Mrs', 'Ms', 'Rev', 'Sir'], ['Sacrificed', 'Respected', 'Nobles', 'Mr', 'Mrs', 'Respected', 'Mr', 'Nobles', 'Respected', 'Kids', 'Miss', 'Nobles', 'Nobles', 'Mr', 'Mrs', 'Nobles', 'Sacrificed', 'Nobles'],inplace=True)\ntemp['Initial'].replace(['Kids', 'Miss', 'Mr', 'Mrs', 'Nobles', 'Respected', 'Sacrificed'], [4, 4, 2, 5, 6, 3, 1], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Age_Range'] = pd.qcut(temp['Age'], 10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"survpct('Age_Range')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Agroup'] = 0\n\ntemp.loc[temp['Age'] < 1.0, 'Agroup'] = 1\ntemp.loc[(temp['Age'] >=1.0) & (temp['Age'] <= 3.0), 'Agroup'] = 2\ntemp.loc[(temp['Age'] > 3.0) & (temp['Age'] < 11.0), 'Agroup'] = 7\ntemp.loc[(temp['Age'] >= 11.0) & (temp['Age'] < 15.0), 'Agroup'] = 13\ntemp.loc[(temp['Age'] >= 15.0) & (temp['Age'] < 18.0), 'Agroup'] = 16\ntemp.loc[(temp['Age'] >= 18.0) & (temp['Age'] <=  20.0), 'Agroup'] = 18\ntemp.loc[(temp['Age'] > 20.0) & (temp['Age'] <= 22.0), 'Agroup'] = 21\ntemp.loc[(temp['Age'] > 22.0) & (temp['Age'] <= 26.0), 'Agroup'] = 24\ntemp.loc[(temp['Age'] > 26.0) & (temp['Age'] <= 30.0), 'Agroup'] = 28\ntemp.loc[(temp['Age'] > 30.0) & (temp['Age'] <= 32.0), 'Agroup'] = 31\ntemp.loc[(temp['Age'] > 32.0) & (temp['Age'] <= 34.0), 'Agroup'] = 33\ntemp.loc[(temp['Age'] > 34.0) & (temp['Age'] <= 38.0), 'Agroup'] = 36\ntemp.loc[(temp['Age'] > 38.0) & (temp['Age'] <= 52.0), 'Agroup'] = 45\ntemp.loc[(temp['Age'] > 52.0) & (temp['Age'] <= 75.0), 'Agroup'] = 60\ntemp.loc[temp['Age'] > 75.0, 'Agroup'] = 78\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Age는 그룹화 시키면 좋으나 학습을 위해서 그냥 놓아두고, 그룹화 연습은 Fare로 하겠습니다.\n* Age should be grouped, but we are focusing on practices not on competition itself, we will just let it be as is and group \"fare\" later on. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Let's combine Pclass & Sex and make one coulmn called Gclass = Gender class","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Gclass']=0\ntemp.loc[((temp['Sex'] == 'male') & (temp['Pclass'] == 1)), 'Gclass'] = 1\ntemp.loc[((temp['Sex'] == 'male') & (temp['Pclass'] == 2)), 'Gclass'] = 2\ntemp.loc[((temp['Sex'] == 'male') & (temp['Pclass'] == 3)), 'Gclass'] = 2\ntemp.loc[((temp['Sex'] == 'female') & (temp['Pclass'] == 1)), 'Gclass'] = 3\ntemp.loc[((temp['Sex'] == 'female') & (temp['Pclass'] == 2)), 'Gclass'] = 4\ntemp.loc[((temp['Sex'] == 'female') & (temp['Pclass'] == 3)), 'Gclass'] = 5\ntemp.loc[(temp['Age'] < 1), 'Gclass'] = 6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"survpct('Gclass')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Family or Alone?\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill in missing Fare value by overall Fare mean\ntemp['Fare'].fillna(temp['Fare'].mean(), inplace=True)\n\n# Setting coin flip (e.g. random chance of surviving)\ndefault_survival_chance = 0.5\ntemp['Family_Survival'] = default_survival_chance\n\n# Grouping data by last name and fare - looking for families\nfor grp, grp_df in temp[['Survived','Name', 'LastName', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['LastName', 'Fare']):\n    \n    # If not equal to 1, a family is found \n    # Then work out survival chance depending on whether or not that family member survived\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                temp.loc[temp['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin == 0.0):\n                temp.loc[temp['PassengerId'] == passID, 'Family_Survival'] = 0\n\n# Print the headline\nprint(\"Number of passengers with family survival information:\", \n      temp.loc[temp['Family_Survival']!=0.5].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If not equal to 1, a group member is found\n# Then work out survival chance depending on whether or not that group member survived\nfor _, grp_df in temp.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    temp.loc[temp['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    temp.loc[temp['PassengerId'] == passID, 'Family_Survival'] = 0\n\n# Print the headline\nprint(\"Number of passenger with family/group survival information: \" \n      +str(temp[temp['Family_Survival']!=0.5].shape[0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bag('Parch', 'Survived', 'Survived per Parch', 'Parch Survived vs Not Survived')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab([temp.Family_Survival, temp.Survived], temp.Pclass, margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating two features of relatives and not alone\ntemp['Family Size'] = temp['SibSp'] + temp['Parch']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"survpct('Family Size')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### \"Ticket\"분석\n* Ticket의 형태를 보겠습니다.\n* Let's look at the numbers of the ticket.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.Ticket.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 도무지 감이 안 잡히는 배열입니다.\n* 빈칸이 없는지 보겠습니다.\n \n* It is an array that may not make any sense.\n* Let's see if there are any blanks.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.Ticket.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 티켓에서 영문있는 것과 숫자만 있는 것을 따봅니다.\n* See the tickets with letters or numbers only ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Initick'] = temp.Ticket.str.extract('^([A-Za-z]+)')\n\ntemp = temp.reset_index(drop=True)  # to avoid `ValueError: cannot reindex from a duplicate axis`\n\ntemp.loc[temp.Initick.isnull(), 'Initick'] = temp['Ticket']\n\ntemp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['NumTicket'] = temp['Initick'].factorize()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.head(n=15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.groupby('NumTicket')['Survived'].mean().to_frame().plot(kind='hist')\nplt.title('Distribution of survival rate for different tickets');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### \"Fare\" 분석","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Highest Fare was:', temp['Fare'].max())\nprint('Lowest Fare was:', temp['Fare'].min())\nprint('Average Fare was:', temp['Fare'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(1, 3, figsize=(20, 6))\nsns.distplot(train[train['Pclass'] == 1].Fare,ax=ax[0])\nax[0].set_title('Fares in Pclass 1')\nsns.distplot(train[train['Pclass'] == 2].Fare,ax=ax[1])\nax[1].set_title('Fares in Pclass 2')\nsns.distplot(train[train['Pclass'] == 3].Fare,ax=ax[2])\nax[2].set_title('Fares in Pclass 3')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Pclass1의 승객 요금에는 큰 분포가있는 것으로 보이며 불연속 값으로 변환 할 수 있습니다.\n* The passenger fare for Pclass1 seems to have a large distribution and can be converted to discrete values.\n\n* Fare를 그룹으로 나누어 놓겠습니다.\n* qcut을 활용하면 원하는 조각으로 데이터를 나누어 줍니다.\n \n* I will divide the Fare into groups.\n* Use qcut to divide the data into the desired pieces.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def groupmean(a,b):\n    return temp.groupby([a])[b].mean().to_frame().style.background_gradient(cmap='summer_r')\n\ntemp['Fare_Range'] = pd.qcut(train['Fare'], 10)\ngroupmean('Fare_Range', 'Fare')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Fare를 그룹화 시킵니다. Fgroup이라고 이름 짓겠습니다.\n* Group Fare with the name of Fgroup","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"0 and below -> 0\n\n7.125 and below-> 5.0\n\n7.9 and below-> 7.5\n\n8.03 or less-> 8.0\n\nLess than 10.5-> 9.5\n\nLess than 23-> 16.0\n\n27.8 and below-> 25.5\n\n51 and below-> 38\n\n73.5 and below-> 62\n\nOver 73.5-> 100","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Fgroup'] = 0\n\ntemp.loc[temp['Fare'] <= 0,'Fgroup'] = 0\ntemp.loc[(temp['Fare'] > 0) & (temp['Fare'] <= 7.125), 'Fgroup'] = 1\ntemp.loc[(temp['Fare'] > 7.125) & (temp['Fare'] <= 7.9), 'Fgroup'] = 2\ntemp.loc[(temp['Fare'] > 7.9) & (temp['Fare'] <= 8.03), 'Fgroup'] = 3\ntemp.loc[(temp['Fare'] > 8.03) & (temp['Fare'] < 10.5), 'Fgroup'] = 4\ntemp.loc[(temp['Fare'] >= 10.5) & (temp['Fare'] < 23.0), 'Fgroup'] = 5\ntemp.loc[(temp['Fare'] >= 23.0) & (temp['Fare'] <= 27.8), 'Fgroup'] = 6\ntemp.loc[(temp['Fare'] > 27.8) & (temp['Fare'] <= 51.0), 'Fgroup'] = 7\ntemp.loc[(temp['Fare'] > 51.0) & (temp['Fare'] <= 73.5), 'Fgroup'] = 8\ntemp.loc[temp['Fare'] > 73.5, 'Fgroup'] = 9\n\ntemp.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### \"Cabin\" 분석\n* cabin 의 위치에 따라 달라지는 것이 있는지 보겠습니다.\n* Let's see if there is anything that depends on the location of the cabin.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.Cabin.value_counts().head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.Cabin.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 빈칸이 무척 많습니다.\n* Lots of null values\n* Cabin에 비어 있는 것이 많아 이를 다른 분류로 일단 잡고 기존 것은 이니셜로 분류합니다.\n* 빈 것은 X로 구분하려는데 이 또한 1,2,3 Pclass와 연동될 것 같으니 비어있고 1등급은 X, 2등급은 Y, 3등급은 Z로 하겠습니다.\n \n* There are so many empty bins in the cabin,\n* Existing ones are classified as initials.\n* I want to classify the nulls as X, but this is also reclassified by Pclasses, so there will be X, Y and Z","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Inicab'] = 0\ntemp['Inicab'] = temp['Cabin'].str.extract('^([A-Za-z]+)')\ntemp.loc[((temp['Cabin'].isnull()) & (temp['Pclass'].values == 1)), 'Inicab'] = 'X'\ntemp.loc[((temp['Cabin'].isnull()) & (temp['Pclass'].values == 2)), 'Inicab'] = 'Y'\ntemp.loc[((temp['Cabin'].isnull()) & (temp['Pclass'].values == 3)), 'Inicab'] = 'Z'\n    \ntemp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Inicab'] = temp['Inicab'].factorize()[0]\n    \ntemp[11:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### \"Embarked\" 분석","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab([temp.Embarked, temp.Pclass], [temp.Sex, temp.Survived], margins=True).style.background_gradient(cmap='summer_r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Survival rates per Embarked ports\n* 승선 장소 별로 생존 확률","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('Embarked', 'Survived', data=temp)\nfig = plt.gcf()\nfig.set_size_inches(5, 3)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f,ax=plt.subplots(2,2,figsize=(20,15))\nsns.countplot('Embarked', data=temp, ax=ax[0,0])\nax[0,0].set_title('No. Of Passengers Boarded')\nsns.countplot('Embarked', hue='Sex', data=temp, ax=ax[0,1])\nax[0,1].set_title('Male-Female Split for Embarked')\nsns.countplot('Embarked', hue='Survived', data=temp, ax=ax[1,0])\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked', hue='Pclass', data=temp, ax=ax[1,1])\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1) 포트 C의 생존 가능성은 0.55 정도이며 S는 가장 낮습니다.S에서 탑승 최대. 대다수는 Pclass3\n\n2) C의 승객들은 많은 비율이 살아남았습니다. 그 이유는 Pclass1 및 Pclass2 승객이 많아서 일 것입니다\n\n3) Embark S는 대부분의 부자들이 탑승한 항구지만 생존 가능성은 낮습니다. Pclass3의 승객도 많았습니다.\n\n4) 포트 Q는 승객의 거의 95 %가 Pclass3\n\nobservation :\n\n1) Maximum boarding in S. The majority is Pclass3\n\n2) Passengers of C survived a large proportion. The reason would be due to the large number of Pclass1 and Pclass2 passengers\n\n3) Embark S is the port where most rich people board, but it is unlikely to survive. Lots of passengers in Pclass3.\n\n4) Port Q has almost 95% of passengers Pclass3\n\n* 빈칸이 두개 있는데 보겠습니다.\n* Let us see 2 null values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.loc[(temp.Embarked.isnull())]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 두 사람의 티켓 번호가 같습니다.\n* 혹시 같은 티켓 번호가 있는 다른 사람이 있는지 봅니다.\n \n* Two people have the same ticket number.\n* See if anyone else has the same ticket number.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.loc[(temp.Ticket == '113572')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 가장 비슷한 번호를 찾아 보겠습니다.\n* Let's find the similar numbers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.sort_values(['Ticket'], ascending = True)[55:70]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* 앞 뒤로 모두 S이고 Pclass도 모두 1인 것으로 봐서 S일 가능성이 큽니다.\n* It is most likely that it is S because both front and back are S and Pclass is all 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.loc[(temp.Embarked.isnull()), 'Embarked'] = 'S'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp.loc[(temp.Embarked.isnull())]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Embarked'] = temp['Embarked'].factorize()[0]\n    \ntemp[11:20]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 6.1 Turning string to numbers\n\n* Already done above","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 6.2 Adding Features\n\n* Some done above\n\n* 그래도 몇 개 만들어 볼까요? But why don't we try making some more\n\n* Priority - (1) Nobles (2) Women in Pclass 1 (3) Babies under 1 (4) Kids under 17 in Pclass 1 & 2  (5) Women in Pclass 3\n* FH - Female Higher Survival Group\n* MH - Male Higher Survival Group\n* FL - Female Lower Surival Group\n* ML - Male Lower Survival Group\n\n* And I will keep making new ones until all the data are either over 80% or le","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Priority - (1) Nobles (2) Women in Pclass 1 (3) Babies under 1 (4) Kids under 17 in Pclass 1 & 2  (5) Women in Pclass 2 (6) Higher Fare","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Priority'] = 0\ntemp.loc[(temp['Initial'] == 6), 'Priority'] = 1\ntemp.loc[(temp['Gclass'] == 3), 'Priority'] = 2\ntemp.loc[(temp['Gclass'] == 6), 'Priority'] = 3\ntemp.loc[(temp['Pclass'] == 1) & (temp['Age'] <= 17), 'Priority'] = 4\ntemp.loc[(temp['Pclass'] == 2) & (temp['Age'] <= 17), 'Priority'] = 5\ntemp.loc[(temp['Pclass'] == 2) & (temp['Sex'] == 2), 'Priority'] = 6\ntemp.loc[(temp['Fgroup'] == 9), 'Priority'] = 7\n\nsurvpct('Priority')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['FH'] = 0\ntemp.loc[(temp['Gclass'] == 1), 'FH'] = 0\ntemp.loc[(temp['Gclass'] == 2), 'FH'] = 0\ntemp.loc[(temp['Gclass'] == 3), 'FH'] = 1\ntemp.loc[(temp['Gclass'] == 4) & (temp['Family Size'] == 2), 'FH'] = 2\ntemp.loc[(temp['Gclass'] == 4) & (temp['Family Size'] == 3), 'FH'] = 3\ntemp.loc[(temp['Gclass'] == 4) & (temp['Family Size'] == 4), 'FH'] = 4\ntemp.loc[(temp['Gclass'] == 4) & (temp['Family Size'] == 1) & (temp['Pclass'] == 1), 'FH'] = 5\ntemp.loc[(temp['Gclass'] == 4) & (temp['Family Size'] == 1) & (temp['Pclass'] == 2), 'FH'] = 6\ntemp.loc[(temp['Gclass'] == 4) & (temp['Fgroup'] == 3), 'FH'] = 7\ntemp.loc[(temp['Gclass'] == 4) & (temp['Fgroup'] >= 5), 'FH'] = 8\n\nsurvpct('FH')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['MH'] = 0\ntemp.loc[(temp['Sex'] == 2), 'MH'] = 0\ntemp.loc[(temp['Gclass'] == 1), 'MH'] = 1\ntemp.loc[(temp['Gclass'] == 1) & (temp['Family Size'] == 2), 'MH'] = 2\ntemp.loc[(temp['Gclass'] == 1) & (temp['Family Size'] == 3), 'MH'] = 3\ntemp.loc[(temp['Gclass'] == 1) & (temp['Family Size'] == 4), 'MH'] = 4\ntemp.loc[(temp['Gclass'] == 1) & (temp['Family Size'] == 1) & (temp['Pclass'] == 1), 'MH'] = 5\ntemp.loc[(temp['Gclass'] == 1) & (temp['Family Size'] == 1) & (temp['Pclass'] == 2), 'MH'] = 6\ntemp.loc[(temp['Gclass'] == 1) & (temp['Fgroup'] == 3), 'MH'] = 7\ntemp.loc[(temp['Gclass'] == 1) & (temp['Fgroup'] >= 5), 'MH'] = 8\n\nsurvpct('MH')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['FL'] = 0\ntemp.loc[(temp['Gclass'] != 5), 'FL'] = 0\ntemp.loc[(temp['Gclass'] == 5) & (temp['Fgroup'] < 5), 'FL'] = 1\ntemp.loc[(temp['Gclass'] == 5) & (temp['Fgroup'] != 3), 'FL'] = 2\ntemp.loc[(temp['Gclass'] == 5) & (temp['FH'] == 1), 'FL'] = 3\ntemp.loc[(temp['Gclass'] == 5) & (temp['Family Size'] < 2), 'FL'] = 4\ntemp.loc[(temp['Gclass'] == 5) & (temp['Family Size'] > 4), 'FL'] = 5\ntemp.loc[(temp['Gclass'] == 5) & (temp['Family Size'] == 1) & (temp['Pclass'] == 3), 'FL'] = 6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"survpct('FL')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['ML'] = 0\ntemp.loc[(temp['Gclass'] == 2) & (temp['Fgroup'] < 5), 'ML'] = 1\ntemp.loc[(temp['Gclass'] == 2) & (temp['Fgroup'] != 3), 'ML'] = 2\ntemp.loc[(temp['Gclass'] == 2) & (temp['MH'] <7), 'ML'] = 3\ntemp.loc[(temp['Gclass'] == 2) & (temp['Family Size'] < 2), 'ML'] = 4\ntemp.loc[(temp['Gclass'] == 2) & (temp['Family Size'] > 4), 'ML'] = 5\ntemp.loc[(temp['Gclass'] == 2) & (temp['Family Size'] == 1) & (temp['Pclass'] == 3), 'ML'] = 6\ntemp.loc[(temp['Gclass'] == 3) & (temp['Fgroup'] < 5), 'ML'] = 1\ntemp.loc[(temp['Gclass'] == 3) & (temp['Fgroup'] != 3), 'ML'] = 2\ntemp.loc[(temp['Gclass'] == 3) & (temp['MH'] <7), 'ML'] = 3\ntemp.loc[(temp['Gclass'] == 3) & (temp['Family Size'] < 2), 'ML'] = 4\ntemp.loc[(temp['Gclass'] == 3) & (temp['Family Size'] > 4), 'ML'] = 5\ntemp.loc[(temp['Gclass'] == 3) & (temp['Family Size'] == 1) & (temp['Pclass'] == 3), 'ML'] = 6\n\nsurvpct('ML')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import randint\ntemp['RAND'] = [ randint(1,149)  for k in temp.index]\n\ntemp.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Score1']= 0\ntemp.loc[(temp['Priority'] > 0), 'Score1'] = 1\ntemp.loc[(temp['RAND'] > 116), 'Score1'] = 1\n\ntemp.Score1.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from random import randint\ntemp['rand'] = [ randint(1,9)  for k in temp.index]\n\ntemp.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp['Score2']= 0\ntemp.loc[(temp['Priority'] > 0), 'Score2'] = 1\ntemp.loc[((temp['Family Size'] > 1) & (temp['Family Size'] < 5)), 'Score2'] = 1\ntemp.loc[(temp['rand'] > 7), 'Score2'] = 1\n\ntemp.Score2.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missingno.matrix(temp, figsize = (15,8))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Deciding final features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Now we are ready to move on.\n* 이제 다음 단계로 갑니다.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* Let's make two new dataframes, one for label encoding, the other for one-hot-encoding\n* 두개의 새로운 데이터 프레임을 만듭니다. 하나는 레이블 인코딩 다른 하나는 원핫 인코딩 (둘이 꼭 필요한 것이 아니라 연습이나 두 가지 방 법 모두 사용해봄)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* 대표적인 인코딩에 Label Encoding이 있는데 이는 각 항목의 값을 서열화 시켜 주~욱 줄세운 것이라 생각하시면 됩니다.\n* 그 외에 자주쓰는 One Hot Encoding 같은 경우 열 내에서의 항목을 나누어서 (열이 주~욱 늘어나며) 이를 0이냐 1이냐로 구분해 놓은 것입니다.\n* A typical encoding is Label Encoding, which can be considered to be a very good order of the values of each item.\n* In addition, the one-of-a-kind One Hot Encoding is divided into 0 or 1 by dividing the items in the column (they increase in number).","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* 다시 말씀 드려서 레이블 인코딩은 줄을 세워서 번호를 부여하는 것이고, 원핫인코딩은 긴가 아닌가 두 가지입니다.\n* label encoding assign ordinal numbers while one hot encoding gives 1 or 0\n\n* For example 16 year old is number 5 and 17 year old is number 6 ..that's the way it is for the label encoding\n* 예를들어 나이별로 줄을 세워 너는 5번, 너는 6번이런식이 레이블 인코딩이고\n\n* If you are 16 yes or no , 17 yes or no...that's one hot encoding\n* 16살이야? 1, 16살 아냐 0 ..그 다음 17살이야? 1 17살 아냐 0 ..이런 식으라 열의 수가 무지하게 늘어납니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"새로운 Data Frame을 만듭니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dfl = pd.DataFrame() # for label encoding","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"good_columns = ['Priority', 'Gclass', 'Agroup', 'Family_Survival', 'Initial','NumName', 'Initick', 'FL', 'ML', 'FH', 'MH', 'Fgroup', 'Family Size','Embarked', 'Score1', 'Score2']\ndfl[good_columns] = temp[good_columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfl.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfh = dfl.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dfl_enc = dfl.apply(LabelEncoder().fit_transform)\n                          \ndfl_enc.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_cols = dfh.columns.tolist()\ndfh_enc = pd.get_dummies(dfh, columns=one_hot_cols)\n\ndfh_enc.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Making ML models / 머신러닝 모델 만들기","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"* 자, 이제 머신 러닝 모델을 만들어 보지요.\n* 우선 인코딩한 파일을 train과 test로 아까 구분해 놓은 행으로 쪼갭니다\n\n* Now let's create a machine learning model.\n* First, split the encoded file into the lines that were previously separated by train and test.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = dfh_enc[:ntrain]\ntest = dfh_enc[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = test\nX_train = train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier()\ngbc = GradientBoostingClassifier()\nsvc = SVC(probability=True)\next = ExtraTreesClassifier()\nada = AdaBoostClassifier()\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier()\n\n# Prepare lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nmodel_names = ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier']\nscores = {}\n\n# Sequentially fit and cross validate all models\nfor ind, mod in enumerate(models):\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores[model_names[ind]] = acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 결과 테이블을 만듭니다.\nresults = pd.DataFrame(scores).T\nresults['mean'] = results.mean(1)\n\nresult_df = results.sort_values(by='mean', ascending=False)#.reset_index()\nresult_df.head(11)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nresult_df = result_df.drop(['mean'], axis=1)\nsns.boxplot(data=result_df.T, orient='h')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 중요도를 보는 함수를 만듭니다.\ndef importance_plotting(data, xlabel, ylabel, title, n=20):\n    sns.set(style=\"whitegrid\")\n    ax = data.tail(n).plot(kind='barh')\n    \n    ax.set(title=title, xlabel=xlabel, ylabel=ylabel)\n    ax.xaxis.grid(False)\n    ax.yaxis.grid(True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 데이터 프레임에 항목 중요도를 넣습니다.\nfi = {'Features':train.columns.tolist(), 'Importance':xgb.feature_importances_}\nimportance = pd.DataFrame(fi, index=fi['Features']).sort_values('Importance', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 그래프 제목\ntitle = 'Top 20 most important features in predicting survival on the Titanic: XGB'\n\n# 그래프 그리기\nimportance_plotting(importance, 'Importance', 'Features', title, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 중요도를 데이터프레임에 넣습니다. Logistic regression에서는 중요도보다 coefficients를 사용합니다. \n# 아래는 Features라는 열에 트레인의 열들의 이름을 리스트로 만들어서 넣고 Importance에는 Logistic regression에는 coefficient를 바꾸어 넣어라는 넘파이 명령입니다.(즉 가로를 세로로)\nfi = {'Features':train.columns.tolist(), 'Importance':np.transpose(log.coef_[0])}\nimportance = pd.DataFrame(fi, index=fi['Features']).sort_values('Importance', ascending=True)\n# 그래프 타이틀\ntitle = 'Top 20 important features in predicting survival on the Titanic: Logistic Regression'\n\n# 그래프 그리기\nimportance_plotting(importance, 'Importance', 'Features', title, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 5가지 모델에 대한 항목 중요도 얻기\ngbc_imp = pd.DataFrame({'Feature':train.columns, 'gbc importance':gbc.feature_importances_})\nxgb_imp = pd.DataFrame({'Feature':train.columns, 'xgb importance':xgb.feature_importances_})\nran_imp = pd.DataFrame({'Feature':train.columns, 'ran importance':ran.feature_importances_})\next_imp = pd.DataFrame({'Feature':train.columns, 'ext importance':ext.feature_importances_})\nada_imp = pd.DataFrame({'Feature':train.columns, 'ada importance':ada.feature_importances_})\n\n# 이를 하나의 데이터프레임으로\nimportances = gbc_imp.merge(xgb_imp, on='Feature').merge(ran_imp, on='Feature').merge(ext_imp, on='Feature').merge(ada_imp, on='Feature')\n\n# 항목당 평균 중요도\nimportances['Average'] = importances.mean(axis=1)\n\n# 랭킹 정하기\nimportances = importances.sort_values(by='Average', ascending=False).reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 중요도를 다시 데이터 프레임에 넣기\nfi = {'Features':importances['Feature'], 'Importance':importances['Average']}\nimportance = pd.DataFrame(fi).set_index('Features').sort_values('Importance', ascending=True)\n\n# 그래프 타이틀\ntitle = 'Top 20 important features in predicting survival on the Titanic: 5 model average'\n\n# 그래프 보기\nimportance_plotting(importance, 'Importance', 'Features', title, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importance1 = importance[-120:]\n\nimportance1[111:120]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 영양가 있는 120개만 넣기\nmylist = list(importance1.index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train1 = pd.DataFrame()\ntest1 = pd.DataFrame()\n\nfor i in mylist:\n    train1[i] = train[i]\n    test1[i]= test[i]\n    \ntrain1.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train1\ntest = test1\n\n# 모델의 변수를 다시 정의하고\nX_train = train\nX_test = test\n\n# 바꿉니다.\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier(random_state=1)\ngbc = GradientBoostingClassifier(random_state=1)\nsvc = SVC(probability=True)\next = ExtraTreesClassifier(random_state=1)\nada = AdaBoostClassifier(random_state=1)\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier(random_state=1)\n\n# Prepare lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nmodel_names = ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier']\nscores2 = {}\n\n# Sequentially fit and cross validate all models\nfor ind, mod in enumerate(models):\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores2[model_names[ind]] = acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 결과 테이블을 만듭니다.\nresults = pd.DataFrame(scores2).T\nresults['mean'] = results.mean(1)\n\nresult_df = results.sort_values(by='mean', ascending=False)#.reset_index()\nresult_df.head(11)\nresult_df = result_df.drop(['mean'], axis=1)\nsns.boxplot(data=result_df.T, orient='h')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 하이퍼파라미터 튜닝\n\n### SVC\n* Scikit-Learn에서는 3가지 모형 최적화 도구를 지원하는데 validation_curve/ GridSearchCV/ ParameterGrid이다\n* fit 메소드를 호출하면 grid search가 자동으로 여러개의 내부 모형을 생성하고 이를 모두 실행시켜서 최적 파라미터를 찾는다.\n\n* bestscore는 최고 점수이고 best estimator는 최고 점수를 낸 파라미터를 가진 모형\n* c값과 gamma값은 10의 배수로 일반적으로 한다.\n* 감마 매개 변수는 단일 학습 예제의 영향이 도달하는 정도를 정의하며 낮은 값은 'far'를, 높은 값은 'close'를 나타냅니다. 감마 매개 변수는 서포트 벡터로 모델에 의해 선택된 샘플의 영향 반경의 역으로 볼 수 있습니다.\n* C 매개 변수는 의사 결정 표면의 단순성에 대한 훈련 예제의 오 분류를 제거합니다. C가 낮을수록 결정 표면이 매끄럽고 높은 C는 모델이 더 많은 샘플을 서포트 벡터로 자유롭게 선택할 수 있도록하여 모든 학습 예제를 올바르게 분류하는 것을 목표로합니다.\n* Verbose는 불리안 값으로 True로 넣으면 꼬치 꼬치 다 알려주는데, 대신 시간이 좀 더 오래 걸립니다.\n* cv =5는 5 fold로 교차 검증한다는 뜻입니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 파라미터 서치\nCs = [0.01, 0.1, 1, 5, 10, 15, 20, 50]\ngammas = [0.001, 0.01, 0.1]\n\n# 파라미터 그리드 셋팅\nhyperparams = {'C': Cs, 'gamma' : gammas}\n\n# 교차검증\ngd=GridSearchCV(estimator = SVC(probability=True), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\n# 모델 fiting 및 결과\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient Boosting Classifier\n* learning_rate는 각 트리의 기여를 줄이는 역할을 합니다.\n* n_estimator는 각 경우의 트리 숫자입니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = [0.01, 0.05, 0.1, 0.2, 0.5]\nn_estimators = [100, 1000, 2000]\nmax_depth = [3, 5, 10, 15]\n\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\ngd=GridSearchCV(estimator = GradientBoostingClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression\n* Penalty - L1 을 사용하는 회귀 모델을 Lasso Regression이라고하고 L2를 사용하는 모델을 Ridge Regression이라고합니다. 이 둘의 주요 차이점은 페널티입니다. 릿지 회귀는 손실 함수에 페널티 항으로 계수의 \"제곱 크기\"를 추가합니다. L2-norm이 오차를 제곱하기 때문에 (오류> 1 인 경우 로트가 증가 함) 모델은 L1-norm보다 훨씬 큰 오차 (e vs e ^ 2)를 보게되므로 훨씬 더 민감합니다. 따라서 오류를 최소화하기 위해 모델을 조정해줍니다.\n* C는 estimator 입니다. logspace 1차원 10개 배열로 0에서 4까지를 estimator로 놓은 것입니다.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"penalty = ['l1', 'l2']\nC = np.logspace(0, 4, 10)\n\nhyperparams = {'penalty': penalty, 'C': C}\n\ngd=GridSearchCV(estimator = LogisticRegression(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost Step 1.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"learning_rate = [0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\nn_estimators = [10, 50, 100, 250, 500, 1000]\n\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\ngd=GridSearchCV(estimator = XGBClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGB Step 2.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"max_depth = [3, 4, 5, 6, 7, 8, 9, 10]\nmin_child_weight = [1, 2, 3, 4, 5, 6]\n\nhyperparams = {'max_depth': max_depth, 'min_child_weight': min_child_weight}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.2, n_estimators=10), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGB Step 3.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"gamma = [i*0.1 for i in range(0,5)]\n\nhyperparams = {'gamma': gamma}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.2, n_estimators=10, max_depth=6, \n                                          min_child_weight=1), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGB Step 4","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"subsample = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\ncolsample_bytree = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n    \nhyperparams = {'subsample': subsample, 'colsample_bytree': colsample_bytree}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.2, n_estimators=10, max_depth=6, \n                                          min_child_weight=1, gamma=0), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGB Step 5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nreg_alpha = [1e-5, 1e-2, 0.1, 1, 100]\n    \nhyperparams = {'reg_alpha': reg_alpha}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.2, n_estimators=10, max_depth=6, \n                                          min_child_weight=1, gamma=0, subsample=1, colsample_bytree=1),\n                                         param_grid = hyperparams, verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gaussian Process","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_restarts_optimizer = [0, 1, 2, 3]\nmax_iter_predict = [1, 2, 5, 10, 20, 35, 50, 100]\nwarm_start = [True, False]\n\nhyperparams = {'n_restarts_optimizer': n_restarts_optimizer, 'max_iter_predict': max_iter_predict, 'warm_start': warm_start}\n\ngd=GridSearchCV(estimator = GaussianProcessClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adaboost.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [10, 100, 200, 500]\nlearning_rate = [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2]\n\nhyperparams = {'n_estimators': n_estimators, 'learning_rate': learning_rate}\n\ngd=GridSearchCV(estimator = AdaBoostClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_neighbors = [1, 2, 3, 4, 5]\nalgorithm = ['auto']\nweights = ['uniform', 'distance']\nleaf_size = [1, 2, 3, 4, 5, 10]\n\nhyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n               'n_neighbors': n_neighbors}\n\ngd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [10, 50, 100, 200]\nmax_depth = [3, None]\nmax_features = [0.1, 0.2, 0.5, 0.8]\nmin_samples_split = [2, 6]\nmin_samples_leaf = [2, 6]\n\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\ngd=GridSearchCV(estimator = RandomForestClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Extra Trees","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [10, 25, 50, 75, 100]\nmax_depth = [3, None]\nmax_features = [0.1, 0.2, 0.5, 0.8]\nmin_samples_split = [2, 10]\nmin_samples_leaf = [2, 10]\n\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\ngd=GridSearchCV(estimator = ExtraTreesClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Bagging Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators = [10, 50, 75, 100, 200]\nmax_samples = [0.1, 0.2, 0.5, 0.8, 1.0]\nmax_features = [0.1, 0.2, 0.5, 0.8, 1.0]\n\nhyperparams = {'n_estimators': n_estimators, 'max_samples': max_samples, 'max_features': max_features}\n\ngd=GridSearchCV(estimator = BaggingClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\", n_jobs=-1)\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_params_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 모델 재 트레이닝","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# 튜닝 모델 시작\n# sample을 split하는 것은 전체데이터 80%를 트레인셋에 20%는 테스트셋에 줌  \nran = RandomForestClassifier(max_depth=3, max_features=0.8, min_samples_leaf=6, min_samples_split=2, n_estimators=200, random_state=1)\n\nknn = KNeighborsClassifier(leaf_size=1, n_neighbors=2, weights='uniform')\n\nlog = LogisticRegression(C=1.0, penalty='l2')\n\nxgb = XGBClassifier(learning_rate=0.05, n_estimators=250, max_depth=3, \n                                          min_child_weight=6, gamma=0.4, subsample=1, colsample_bytree=0.65, reg_alpha=1e-05)\n\ngbc = GradientBoostingClassifier(learning_rate=0.05, max_depth=3, n_estimators=100)\n\nsvc = SVC(probability=True, gamma=0.001, C=10)\n\next = ExtraTreesClassifier(max_depth=3, max_features=0.08, min_samples_leaf=6, min_samples_split=2, n_estimators=200, random_state=1)\n\nada = AdaBoostClassifier(learning_rate=1, n_estimators=200, random_state=1)\n\ngpc = GaussianProcessClassifier(max_iter_predict=1, n_restarts_optimizer=0, warm_start=True)\n\nbag = BaggingClassifier(max_features=0.2, max_samples=0.2, n_estimators=100, random_state=1)\n\n# 리스트\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nmodel_names = ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier']\nscores3 = {}\n\n# Sequentially fit and cross validate all models\nfor ind, mod in enumerate(models):\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores3[model_names[ind]] = acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = pd.DataFrame(scores).T\nresults['mean'] = results.mean(1)\nresult_df = results.sort_values(by='mean', ascending=False)\nresult_df.head(11)\n\n\nresult_df = result_df.drop(['mean'], axis=1)\nsns.boxplot(data=result_df.T, orient='h')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Voting (Hard/Soft)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#튜닝한 파라미터로 하드보팅\ngrid_hard = VotingClassifier(estimators = [('Random Forest', ran), \n                                           ('Logistic Regression', log),\n                                           ('XGBoost', xgb),\n                                           ('Gradient Boosting', gbc),\n                                           ('Extra Trees', ext),\n                                           ('AdaBoost', ada),\n                                           ('Gaussian Process', gpc),\n                                           ('SVC', svc),\n                                           ('K Nearest Neighbour', knn),\n                                           ('Bagging Classifier', bag)], voting = 'hard')\n\ngrid_hard_cv = model_selection.cross_validate(grid_hard, X_train, y_train, cv=10)\ngrid_hard.fit(X_train, y_train)\n\nprint(\"Hard voting on test set score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_soft = VotingClassifier(estimators = [('Random Forest', ran), \n                                           ('Logistic Regression', log),\n                                           ('XGBoost', xgb),\n                                           ('Gradient Boosting', gbc),\n                                           ('Extra Trees', ext),\n                                           ('AdaBoost', ada),\n                                           ('Gaussian Process', gpc),\n                                           ('SVC', svc),\n                                           ('K Nearest Neighbour', knn),\n                                           ('Bagging Classifier', bag)], voting = 'soft')\n\ngrid_soft_cv = model_selection.cross_validate(grid_soft, X_train, y_train, cv=10)\ngrid_soft.fit(X_train, y_train)\n\nprint(\"Soft voting on test set score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean() * 100))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 마지막 모델 예측","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final predictions\n\npredictions = grid_hard.predict(X_test)\n\nsubmission = pd.concat([pd.DataFrame(passId), pd.DataFrame(predictions)], axis = 'columns')\n\nsubmission.columns = [\"PassengerId\", \"Survived\"]\nsubmission.to_csv('titanic_submission305.csv', header = True, index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final predictions2\n\npredictions = grid_soft.predict(X_test)\n\nsubmission = pd.concat([pd.DataFrame(passId), pd.DataFrame(predictions)], axis = 'columns')\n\nsubmission.columns = [\"PassengerId\", \"Survived\"] \nsubmission.to_csv('titanic_submission306.csv', header = True, index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 제출","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# And we finally make a submission 그리고 제출 합니다.\n# Please make sure you \"commit\" (It take a few minutes) / commit버턴을 누르시는 것을 잊지 마세요 (몇 분 걸립니다)\n# And then you will see the submission file on the top right hand side at Data>Output>Kaggle/working / 그럼 우측 상단 데이터 아웃풋에서 제출용 결과물이 나올 것입니다.","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}