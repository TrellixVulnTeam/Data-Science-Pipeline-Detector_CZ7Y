{"cells":[{"metadata":{"id":"2_qJgEDjZCbC"},"cell_type":"markdown","source":"# Titanic Survival Prediction using Cost Sensitive Learning","execution_count":null},{"metadata":{"id":"wp3FsQIaZCbE"},"cell_type":"markdown","source":"**Highlights:**\n> 1. Correlation analysis using Pearson coefficient for continuous features and Cramer's V for categorical features\n>\n> 2. Data visualization using Seaborn FacetGrid plots\n>\n> 3. Classification of imbalanced dataset using `class weighted` or `cost sensitive` learning\n>\n> 4. Results for each ML algorithm are presented after performing 5-fold cross validation based on F1-score\n>\n> 5. Random forests has the highest F1-score among the non-overfitting classifiers","execution_count":null},{"metadata":{"id":"RN-6AhP6ZCbE"},"cell_type":"markdown","source":"* [1.  Import and Clean Data](#import-and-clean-data)\n    * [1.1.  Missing values](#missing-values)\n    * [1.2.  Deleting features](#deleting-features)\n    * [1.3.  Find categorical columns and change their *Dtype* from `object` to `Categorical`](#find-categorical-columns-and-change-their-%2Adtype%2A-from-%60object%60-to-%60categorical%60)\n* [2.  Correlations in the Data](#correlations-in-the-data)\n    * [2.1.  Correlation between Quantitative variables](#correlation-between-quantitative-variables)\n    * [2.2.  Correlation between Qualitative/ Categorical variables](#correlation-between-qualitative/-categorical-variables)\n* [3.  Data Visualization](#data-visualization)\n    * [3.1.  Frequency distribution: Categorical Variables](#frequency-distribution%3A-categorical-variables)\n    * [3.2.  Frequency distribution: Continuous Variables](#frequency-distribution%3A-continuous-variables)\n    * [3.3.  Box plots : Outlier detection](#box-plots-%3A-outlier-detection)\n    * [3.4.  Survival among various categories](#survival-among-various-categories)\n    * [3.5.  Survival among Passenger Class, Sex, and Sibling/Spouse](#survival-among-passenger-class%2C-sex%2C-and-sibling/spouse)\n    * [3.6.  Survival among Passenger Class, Sex, and Embarked](#survival-among-passenger-class%2C-sex%2C-and-embarked)\n* [4.  Data Preprocessing](#data-preprocessing)\n    * [4.1.  Train-Test split](#train-test-split)\n    * [4.2.  One-hot Encoding and Standardization](#one-hot-encoding-and-standardization)\n* [5.  Data Modeling](#data-modeling)\n    * [5.1.  Utility Functions](#utility-functions)\n    * [5.2.  Naive Bayes](#naive-bayes)\n    * [5.3.  Logistic Regression](#logistic-regression)\n    * [5.4.  K-Nearest Neighbors](#k-nearest-neighbors)\n    * [5.5.  Decision Tree](#decision-tree)\n    * [5.6.  Decision Trees with Bagging](#decision-trees-with-bagging)\n    * [5.7.  Random Forests](#random-forests)\n    * [5.8.  Decision Trees with AdaBoost](#decision-trees-with-adaboost)\n    * [5.9.  Linear SVC](#linear-svc)\n    * [5.10.  SVM with RBF kernel](#svm-with-rbf-kernel)\n    * [5.11.  XGBoost](#xgboost)\n    * [5.12.  LightGBM](#lightgbm)\n    * [5.13.  CatBoost](#catboost)\n* [6.  Model Comparison](#model-comparison)\n    * [6.1.  Evaluation Metrics](#evaluation-metrics)\n    * [6.2.  ROC and PR Curves](#roc-and-pr-curves)","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"n_wZR8HYZCbF"},"cell_type":"code","source":"import os, sys\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true,"id":"nTJjK4NJZCbK"},"cell_type":"markdown","source":"<a id=\"import-and-clean-data\"></a>\n# 1.  Import and Clean Data","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"3psBwnRqZCbK"},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/titanic/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"SC3JhXaMZCbO","outputId":"7444a0c7-a0b2-4583-ab9a-2bf79d70dcab"},"cell_type":"code","source":"df.head(2)","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"id":"38TrVlowZCbR"},"cell_type":"markdown","source":"<a id=\"missing-values\"></a>\n## 1.1.  Missing values","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"jFPDOT_TZCbS","outputId":"485b03bd-0ba1-4e58-828b-2381b36f9c53"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"JKrELWtmZCbV"},"cell_type":"code","source":"df.dropna(subset=['Embarked'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"ewCG19AGZCbY","outputId":"da926a7a-aa1c-483a-fe7a-16f252f0e099"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"id":"tJcKw_wnZCbc"},"cell_type":"markdown","source":"<a id=\"deleting-features\"></a>\n## 1.2.  Deleting features","execution_count":null},{"metadata":{"id":"aE92OTsqZCbd"},"cell_type":"markdown","source":"Since `PassengerId`, `Name`, and `Ticket` columns do not provide any relevant information in predicting the survival of a passenger, we can delete the columns.<br/>\nAlso, since most of the values in the `Cabin` column are missing, it is difficult to impute them and hence the column has to be deleted.","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"ZdMwV4IZZCbd"},"cell_type":"code","source":"df.drop(columns=['PassengerId', 'Name', 'Ticket', 'Cabin'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"DZZjeTSDZCbg","outputId":"487b9ccd-c92b-4579-a8ce-382e5bd52a2e"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"id":"RqNLQDvtZCbj"},"cell_type":"markdown","source":"<a id=\"find-categorical-columns-and-change-their-%2Adtype%2A-from-%60object%60-to-%60categorical%60\"></a>\n## 1.3.  Find categorical columns and change their *Dtype* from `object` to `Categorical`","execution_count":null},{"metadata":{"id":"U3IKJNDJZCbk"},"cell_type":"markdown","source":"Finding categorical features and converting their pandas *dtype* to `categorical` will ease visualization","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"Fc0o9c_6ZCbl"},"cell_type":"code","source":"def summarize_categoricals(df, show_levels=False):\n    \"\"\"\n        Display uniqueness in each column\n    \"\"\"\n    data = [[df[c].unique(), len(df[c].unique()), df[c].isnull().sum()] for c in df.columns]\n    df_temp = pd.DataFrame(data, index=df.columns,\n                           columns=['Levels', 'No. of Levels', 'No. of Missing Values'])\n    return df_temp.iloc[:, 0 if show_levels else 1:]\n\n\ndef to_categorical(columns, df):\n    \"\"\"\n        Converts the columns passed in `columns` to categorical datatype\n    \"\"\"\n    for col in columns:\n        df[col] = df[col].astype('category')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"v2zL2NTRZCbo","outputId":"25456e2d-f6e9-4f94-9db5-b140f822bbd9"},"cell_type":"code","source":"summarize_categoricals(df, show_levels=True)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"0Vq1TIGzZCbr"},"cell_type":"code","source":"# df = to_categorical(['Sex', 'Embarked', 'Survived'], df)\n\ndf = to_categorical(['Survived', 'Pclass', 'Sex',\n                     'SibSp', 'Parch', 'Embarked'], df)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"cFFOh4K0ZCbu","outputId":"9a8ec351-abc1-46e9-9a45-65d7d933d01c"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true,"id":"2iyUBobvZCbx"},"cell_type":"markdown","source":"<a id=\"correlations-in-the-data\"></a>\n# 2.  Correlations in the Data","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"mSgJGSI4ZCby","outputId":"a9d542e9-3410-4baa-d3f5-30591c980b26"},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{"id":"hy0RLx_2ZCb1"},"cell_type":"markdown","source":"<a id=\"correlation-between-quantitative-variables\"></a>\n## 2.1.  Correlation between Quantitative variables","execution_count":null},{"metadata":{"Collapsed":"false","trusted":true,"id":"BxXDVPy_ZCb2","outputId":"50000db5-c5bb-443d-e35f-c8a782428289"},"cell_type":"code","source":"sns.heatmap(data=df.astype({'Survived': 'int64'}).corr(),\n            annot=True, cmap='coolwarm');","execution_count":null,"outputs":[]},{"metadata":{"id":"3E4lDGBRZCb5"},"cell_type":"markdown","source":"<a id=\"correlation-between-qualitative/-categorical-variables\"></a>\n## 2.2.  Correlation between Qualitative/ Categorical variables","execution_count":null},{"metadata":{"Collapsed":"false","id":"OEmMQkrXZCb5"},"cell_type":"markdown","source":"`Cramer's V` is more appropriate than Pearson correlation to find correlation between two nominal variables. Here, the `Cramer's V` metric is implemented.","execution_count":null},{"metadata":{"Collapsed":"false","trusted":true,"id":"3kxasz-9ZCb6"},"cell_type":"code","source":"def cramers_corrected_stat(contingency_table):\n    \"\"\"\n        Computes corrected Cramer's V statistic for categorial-categorial association\n    \"\"\"\n    chi2 = chi2_contingency(contingency_table)[0]\n    n = contingency_table.sum().sum()\n    phi2 = chi2/n\n    \n    r, k = contingency_table.shape\n    r_corrected = r - (((r-1)**2)/(n-1))\n    k_corrected = k - (((k-1)**2)/(n-1))\n    phi2_corrected = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n    \n    return (phi2_corrected / min( (k_corrected-1), (r_corrected-1)))**0.5","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":true,"id":"Ky4QmTFsZCb-"},"cell_type":"code","source":"def categorical_corr_matrix(df):\n    \"\"\"\n        Computes corrected Cramer's V statistic between all the categorical variables in the dataframe\n    \"\"\"\n    df = df.select_dtypes(include='category')\n    cols = df.columns\n    n = len(cols)\n    corr_matrix = pd.DataFrame(np.zeros(shape=(n, n)), index=cols, columns=cols)\n    \n    for col1 in cols:\n        for col2 in cols:\n            if col1 == col2:\n                corr_matrix.loc[col1, col2] = 1\n                break\n            df_crosstab = pd.crosstab(df[col1], df[col2], dropna=False)\n            corr_matrix.loc[col1, col2] = cramers_corrected_stat(df_crosstab)\n    \n    # Flip and add to get full correlation matrix\n    corr_matrix += np.tril(corr_matrix, k=-1).T\n    return corr_matrix","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":true,"id":"eF6nIzf-ZCcC","outputId":"d1a083dd-0e2b-43f2-f366-a6c3c3fd0048"},"cell_type":"code","source":"sns.heatmap(categorical_corr_matrix(df), annot=True, cmap='coolwarm', square=True);","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","id":"eN7CSOpmZCcF"},"cell_type":"markdown","source":"***Inference:*** `Sex` and `Survived` features are highly correlated. Therefore, `Sex` feature alone has the ability to predict ~54% of the samples correctly. \n\nSimilarly, `Pclass` and `Survived` are also correlated.","execution_count":null},{"metadata":{"id":"AM3d6JH4ZCcF"},"cell_type":"markdown","source":"<a id=\"data-visualization\"></a>\n# 3.  Data Visualization","execution_count":null},{"metadata":{"id":"k6VAEWHfZCcG"},"cell_type":"markdown","source":"<a id=\"frequency-distribution%3A-categorical-variables\"></a>\n## 3.1.  Frequency distribution: Categorical Variables","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"43YkNfuOZCcG","outputId":"9c619d5b-59c7-491e-8968-982c5445100e"},"cell_type":"code","source":"fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(10, 7))\ntitles = ['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n\nax_title_pairs = zip(axs.flat, titles)\n\nfor ax, title in ax_title_pairs:\n    sns.countplot(x=title, data=df, palette='Pastel2', ax=ax)\n    ax.set_title(title)\n    ax.set_ylim(0, 900)\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"06MZJvhQZCcJ"},"cell_type":"markdown","source":"***Inference:*** From the `Survived` plot we can see that the dataset is imbalanced with respect to the target variable.","execution_count":null},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true,"id":"l5_XJAUWZCcJ"},"cell_type":"markdown","source":"<a id=\"frequency-distribution%3A-continuous-variables\"></a>\n## 3.2.  Frequency distribution: Continuous Variables","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"mg5YCYInZCcK","outputId":"83e46a4c-483d-41d2-f860-aeae83c1bba0"},"cell_type":"code","source":"df_grouped = df.groupby(by='Survived')\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\ncols = ['Age', 'Fare']\nfor i in range(len(cols)):\n    sns.distplot(df_grouped.get_group(0)[cols[i]], bins=20, ax=ax[i], label='No')\n    sns.distplot(df_grouped.get_group(1)[cols[i]], bins=20, ax=ax[i], label='Yes')\n    ax[i].legend(title='Survived')\n\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"id":"braYiVfcZCcM"},"cell_type":"markdown","source":"<a id=\"box-plots-%3A-outlier-detection\"></a>\n## 3.3.  Box plots : Outlier detection","execution_count":null},{"metadata":{"trusted":true,"id":"PRTyRVIxZCcN","outputId":"b6b44106-1509-4799-9130-f1bf8c0dab83"},"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 5))\ncols = ['Fare', 'Age']\nfor i in range(len(cols)):\n    sns.boxplot(x='Survived', y=cols[i], data=df, ax=axes[i])\nfig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true,"id":"EaUhq6OlZCcP"},"cell_type":"markdown","source":"<a id=\"survival-among-various-categories\"></a>\n## 3.4.  Survival among various categories","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"zoz9yk-iZCcQ","outputId":"0fc5afeb-6e29-408a-a5b7-ca29d7410652"},"cell_type":"code","source":"## Adding new column to the dataframe temporarily\n# When the dataframe is grouped later, the estimator in `barplot` will give groupwise survival rate\ndf['survival_rate'] = (pd.Series(list(df['Survived'])) * 100) / df['Survived'].size\n\nfig, axs = plt.subplots(nrows=2, ncols=3, figsize=(10, 7))\ntitles = ['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\n\n## axs.flat is an attribute and contains a flattened axs vector/ list\nax_title_pairs = zip(axs.flat, titles)\n\nfor ax, title in ax_title_pairs:\n    sns.barplot(x=title, y='survival_rate', hue='Survived',\n                data=df, estimator=np.sum, palette='Pastel2', ax=ax)\n    ax.set_ylim(0, 30)\n    ax.set_xlabel(title)\n    ax.set_ylabel('Survived (in %)')\n\nfig.tight_layout()\ndf.drop(columns=['survival_rate'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true,"id":"t5M-x1YqZCcT"},"cell_type":"markdown","source":"<a id=\"survival-among-passenger-class%2C-sex%2C-and-sibling/spouse\"></a>\n## 3.5.  Survival among Passenger Class, Sex, and Sibling/Spouse","execution_count":null},{"metadata":{"id":"faW_IJoQZCcU"},"cell_type":"markdown","source":"Modifying seaborn countplot make it work with FacetGrid when all 3 arguments (`hue`, `row`, and `col`) are used.","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"LntPTxnKZCcV"},"cell_type":"code","source":"def modified_countplot(**kargs):\n    \"\"\"\n        Assumes that columns to be plotted are in of pandas dtype='CategoricalDtype'\n    \"\"\"\n    facet_gen = kargs['facet_generator']    ## Facet generator over facet data\n    curr_facet, facet_data = None, None\n    \n    while True:\n        ## Keep yielding until non-empty dataframe is found\n        curr_facet = next(facet_gen)            ## Yielding facet genenrator\n        df_rows = curr_facet[1].shape[0]\n        \n        ## Skip the current facet if its corresponding dataframe empty\n        if df_rows:\n            facet_data = curr_facet[1]\n            break\n    \n    x_hue = (kargs.get('x'), kargs.get('hue'))\n    cols = [col for col in x_hue if col]\n    col_categories = [facet_data[col].dtype.categories if col else None for col in x_hue]\n    \n    palette = kargs['palette'] if 'palette' in kargs.keys() else 'Pastel2'\n    sns.countplot(x=cols[0], hue=x_hue[1], \n                  order=col_categories[0], hue_order=col_categories[1],\n                  data=facet_data.loc[:, cols], palette=palette)","execution_count":null,"outputs":[]},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"M5AKBybYZCcZ","outputId":"b91c4a94-39d1-4ebb-ebf7-559c848d6f0f"},"cell_type":"code","source":"display(pd.crosstab(df['Survived'], [df['Pclass'], df['Sex']], dropna=False))\n\nfacet = sns.FacetGrid(df, col='Pclass',\n                      sharex=False, sharey=False,\n                      margin_titles=True)\nfacet.map(modified_countplot, x='Sex', hue='Survived',\n          palette='Pastel2', facet_generator=facet.facet_data())\nfacet.set_xlabels('Sex')\nfacet.set_ylabels('Count')\nfacet.add_legend(title='Survived');","execution_count":null,"outputs":[]},{"metadata":{"toc-hr-collapsed":true,"toc-nb-collapsed":true,"id":"VJcYDZ6kZCcc"},"cell_type":"markdown","source":"<a id=\"survival-among-passenger-class%2C-sex%2C-and-embarked\"></a>\n## 3.6.  Survival among Passenger Class, Sex, and Embarked","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"0GOBhp4hZCcc","outputId":"28b8432e-5683-451d-d2db-59872f7b55f1"},"cell_type":"code","source":"display(pd.crosstab(df['Survived'], [df['Pclass'], df['Sex'], df['Embarked']], dropna=False))\n\nfacet = sns.FacetGrid(df, row='Embarked', col='Pclass',\n                      sharex=False, sharey=False,\n                      margin_titles=True)\nfacet.map(modified_countplot, x='Sex', hue='Survived',\n          palette='Pastel2', facet_generator=facet.facet_data())\nfacet.set_xlabels('Sex')\nfacet.set_ylabels('Count')\nfacet.add_legend(title='Survived');","execution_count":null,"outputs":[]},{"metadata":{"id":"INDOIj8KZCcf"},"cell_type":"markdown","source":"<a id=\"data-preprocessing\"></a>\n# 4.  Data Preprocessing","execution_count":null},{"metadata":{"id":"4VPmAO7gZCcf"},"cell_type":"markdown","source":"Data needs to be one-hot-encoded before applying machine learning models.","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"wQ3UO0d4ZCcg"},"cell_type":"code","source":"x = df.iloc[:, 1:]\ny = df['Survived']\n\ncategorical_columns = list(x.select_dtypes(include='category').columns)\nnumeric_columns = list(x.select_dtypes(exclude='category').columns)","execution_count":null,"outputs":[]},{"metadata":{"id":"zlt5ERBzZCci"},"cell_type":"markdown","source":"<a id=\"train-test-split\"></a>\n## 4.1.  Train-Test split\nCatBoost classifier does not require any knd of preprocessing while Naive bayes requires a different kind of preprocesing. Therefore, we will use raw/ unmodified data (`x_train_cat, x_test_cat, y_train_cat, y_test_cat`) for CatBoost and preprocessed data (`x_train, x_test, y_train, y_test`) for all other classifiers. For Naive Bayes, we will use the raw data (`x_train_cat, x_test_cat, y_train_cat, y_test_cat`) and preprocess it as required in the Naive Bayes section.","execution_count":null},{"metadata":{"trusted":true,"id":"6zyBdlHZZCcj","outputId":"b5f38542-af09-4191-86b9-b87e18163b2b"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata_splits = train_test_split(x, y, test_size=0.25, random_state=0,\n                               shuffle=True, stratify=y)\nx_train, x_test, y_train, y_test = data_splits\n\n\n# For CatBoost and Naive Bayes\ndata_splits = train_test_split(x, y, test_size=0.25, random_state=0,\n                               shuffle=True, stratify=y)\nx_train_cat, x_test_cat, y_train_cat, y_test_cat = data_splits\n\n\nlist(map(lambda x: x.shape, [x, y, x_train, x_test, y_train, y_test]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"N0ahoHU3ZCcm","outputId":"4a442d9d-ffe4-4da5-c7ac-973c74fd4880"},"cell_type":"code","source":"pd.Series(y_test).value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"Xvs473uuZCco","outputId":"f756e0ec-4a6f-4b8d-e678-a0c124493889"},"cell_type":"code","source":"sns.countplot(x=y_test);","execution_count":null,"outputs":[]},{"metadata":{"id":"PiBuC-a-ZCcr"},"cell_type":"markdown","source":"<a id=\"one-hot-encoding-and-standardization\"></a>\n## 4.2.  One-hot Encoding and Standardization\nWe need to standardize the continuous or quantitative variables/ features before applying Machine Learning models. This is important because if we don't standardize the features, features with high variance that are orders of magnitude larger that others might dominate the model fitting process and causing the model unable to learn from other features (with lower variance) correctly as expected. <br/>\nThere is no need to standardize categorical variables.\n\n***Also we need to standardize the data only after performing train-test split because if we standardize before splitting then there is a chance for some information leak from the test set into the train set. We always want the test set to be completely new to the ML models. [Read more](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)***","execution_count":null},{"metadata":{"trusted":true,"id":"MR28s2_bZCcr","outputId":"3604fd46-7e38-40f4-a158-217c89fd95c1"},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"id":"fp1i5nqxZCcu"},"cell_type":"markdown","source":"We need to impute values in `Age`.","execution_count":null},{"metadata":{"jupyter":{"outputs_hidden":true},"trusted":true,"id":"aczOr410ZCcv"},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline \n\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore', dtype=np.int))])\n\n## Column Transformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_columns),\n        ('cat', categorical_transformer, categorical_columns)],\n    remainder='passthrough')\n\n\n## Applying Column Transformer\nx_train = preprocessor.fit_transform(x_train)\nx_test = preprocessor.transform(x_test)\n\n\n## Label encoding\ny_trans = LabelEncoder()\ny_train = y_trans.fit_transform(y_train)\ny_test = y_trans.transform(y_test)\n\n\n## Save feature names after one-hot encoding for feature importances plots\nfeature_names = list(preprocessor.named_transformers_['cat'].named_steps['onehot'] \\\n                            .get_feature_names(input_features=categorical_columns))\nfeature_names = feature_names + numeric_columns","execution_count":null,"outputs":[]},{"metadata":{"id":"stQ0EQL2ZCcx"},"cell_type":"markdown","source":"<a id=\"data-modeling\"></a>\n# 5.  Data Modeling\nSince the dataset is imbalanced we will be using class-weighted/ cost-sensitive learning. In cost-sensitive learning, a weighted cost function is used. Therefore, misclassifying a sample from the minority class will cost the classifiers more than misclassifying a sample from the majority class. In most of the Sklearn classifiers, cost-sensitive learning can be enabled by setting `class_weight='balanced'`.","execution_count":null},{"metadata":{"id":"4cFqWf0zZCcy"},"cell_type":"markdown","source":"<a id=\"utility-functions\"></a>\n## 5.1.  Utility Functions","execution_count":null},{"metadata":{"Collapsed":"false","trusted":true,"id":"WIgCBH94ZCcy"},"cell_type":"code","source":"import timeit\nimport pickle\nimport sys\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, \\\n                            precision_recall_curve, roc_curve, accuracy_score\nfrom sklearn.exceptions import NotFittedError","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":true,"id":"EyD-IKETZCc1"},"cell_type":"code","source":"def confusion_plot(matrix, labels=None):\n    \"\"\" Display binary confusion matrix as a Seaborn heatmap \"\"\"\n    \n    labels = labels if labels else ['Negative (0)', 'Positive (1)']\n    \n    fig, ax = plt.subplots(nrows=1, ncols=1)\n    sns.heatmap(data=matrix, cmap='Blues', annot=True, fmt='d',\n                xticklabels=labels, yticklabels=labels, ax=ax)\n    ax.set_xlabel('PREDICTED')\n    ax.set_ylabel('ACTUAL')\n    ax.set_title('Confusion Matrix')\n    plt.close()\n    \n    return fig","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":true,"id":"Gd3Aq4uVZCc4"},"cell_type":"code","source":"def roc_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Receiver Operating Characteristic (ROC) curve \n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    fpr, tpr, thresh = roc_curve(y_true, y_probs,\n                                 drop_intermediate=False)\n    auc = round(roc_auc_score(y_true, y_probs), 2)\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    label = ' '.join([label, f'({auc})']) if compare else None\n    sns.lineplot(x=fpr, y=tpr, ax=axis, label=label)\n    \n    if compare:\n        axis.legend(title='Classifier (AUC)', loc='lower right')\n    else:\n        axis.text(0.72, 0.05, f'AUC = { auc }', fontsize=12,\n                  bbox=dict(facecolor='green', alpha=0.4, pad=5))\n            \n        # Plot No-Info classifier\n        axis.fill_between(fpr, fpr, tpr, alpha=0.3, edgecolor='g',\n                          linestyle='--', linewidth=2)\n        \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('ROC Curve')\n    axis.set_xlabel('False Positive Rate [FPR]\\n(1 - Specificity)')\n    axis.set_ylabel('True Positive Rate [TPR]\\n(Sensitivity or Recall)')\n    \n    plt.close()\n    \n    return axis if ax else fig","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":true,"id":"bOcPEwMvZCc7"},"cell_type":"code","source":"def precision_recall_plot(y_true, y_probs, label, compare=False, ax=None):\n    \"\"\" Plot Precision-Recall curve.\n        Set `compare=True` to use this function to compare classifiers. \"\"\"\n    \n    p, r, thresh = precision_recall_curve(y_true, y_probs)\n    p, r, thresh = list(p), list(r), list(thresh)\n    p.pop()\n    r.pop()\n    \n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1)\n    \n    if compare:\n        sns.lineplot(r, p, ax=axis, label=label)\n        axis.set_xlabel('Recall')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n    else:\n        sns.lineplot(thresh, p, label='Precision', ax=axis)\n        axis.set_xlabel('Threshold')\n        axis.set_ylabel('Precision')\n        axis.legend(loc='lower left')\n\n        axis_twin = axis.twinx()\n        sns.lineplot(thresh, r, color='limegreen', label='Recall', ax=axis_twin)\n        axis_twin.set_ylabel('Recall')\n        axis_twin.set_ylim(0, 1)\n        axis_twin.legend(bbox_to_anchor=(0.24, 0.18))\n    \n    axis.set_xlim(0, 1)\n    axis.set_ylim(0, 1)\n    axis.set_title('Precision Vs Recall')\n    \n    plt.close()\n    \n    return axis if ax else fig","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":true,"id":"9HxflgdSZCc9"},"cell_type":"code","source":"def feature_importance_plot(importances, feature_labels, ax=None):\n    fig, axis = (None, ax) if ax else plt.subplots(nrows=1, ncols=1, figsize=(5, 10))\n    sns.barplot(x=importances, y=feature_labels, ax=axis)\n    axis.set_title('Feature Importance Measures')\n    \n    plt.close()\n    \n    return axis if ax else fig","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":true,"id":"3DDRd9EVZCdC"},"cell_type":"code","source":"def train_clf(clf, x_train, y_train, sample_weight=None, refit=False):\n    train_time = 0\n    \n    try:\n        if refit:\n            raise NotFittedError\n        y_pred_train = clf.predict(x_train)\n    except NotFittedError:\n        start = timeit.default_timer()\n        \n        if sample_weight is not None:\n            clf.fit(x_train, y_train, sample_weight=sample_weight)\n        else:\n            clf.fit(x_train, y_train)\n        \n        end = timeit.default_timer()\n        train_time = end - start\n        \n        y_pred_train = clf.predict(x_train)\n    \n    train_acc = accuracy_score(y_train, y_pred_train)\n    return clf, y_pred_train, train_acc, train_time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"ZXpjJsL9ZCdI"},"cell_type":"code","source":"def model_memory_size(clf):\n    return sys.getsizeof(pickle.dumps(clf))","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":true,"id":"kDXGDduOZCdL"},"cell_type":"code","source":"def report(clf, x_train, y_train, x_test, y_test, display_scores=[],\n           sample_weight=None, refit=False, importance_plot=False,\n           confusion_labels=None, feature_labels=None, verbose=True):\n    \"\"\" Trains the passed classifier if not already trained and reports\n        various metrics of the trained classifier \"\"\"\n    \n    dump = dict()\n    \n    ## Train if not already trained\n    clf, train_predictions, \\\n    train_acc, train_time = train_clf(clf, x_train, y_train,\n                                      sample_weight=sample_weight,\n                                      refit=refit)\n    ## Testing\n    start = timeit.default_timer()\n    test_predictions = clf.predict(x_test)\n    end = timeit.default_timer()\n    test_time = end - start\n    \n    test_acc = accuracy_score(y_test, test_predictions)\n    y_probs = clf.predict_proba(x_test)[:, 1]\n    \n    roc_auc = roc_auc_score(y_test, y_probs)\n        \n    ## Additional scores\n    scores_dict = dict()\n    for func in display_scores:\n        scores_dict[func.__name__] = [func(y_train, train_predictions),\n                                      func(y_test, test_predictions)]\n        \n    ## Model Memory\n    model_mem = round(model_memory_size(clf) / 1024, 2)\n    \n    print(clf)\n    print(\"\\n=============================> TRAIN-TEST DETAILS <======================================\")\n    \n    ## Metrics\n    print(f\"Train Size: {x_train.shape[0]} samples\")\n    print(f\" Test Size: {x_test.shape[0]} samples\")\n    print(\"---------------------------------------------\")\n    print(f\"Training Time: {round(train_time, 3)} seconds\")\n    print(f\" Testing Time: {round(test_time, 3)} seconds\")\n    print(\"---------------------------------------------\")\n    print(\"Train Accuracy: \", train_acc)\n    print(\" Test Accuracy: \", test_acc)\n    print(\"---------------------------------------------\")\n    \n    if display_scores:\n        for k, v in scores_dict.items():\n            score_name = ' '.join(map(lambda x: x.title(), k.split('_')))\n            print(f'Train {score_name}: ', v[0])\n            print(f' Test {score_name}: ', v[1])\n            print()\n        print(\"---------------------------------------------\")\n    \n    print(\" Area Under ROC (test): \", roc_auc)\n    print(\"---------------------------------------------\")\n    print(f\"Model Memory Size: {model_mem} kB\")\n    print(\"\\n=============================> CLASSIFICATION REPORT <===================================\")\n    \n    ## Classification Report\n    clf_rep = classification_report(y_test, test_predictions, output_dict=True)\n    \n    print(classification_report(y_test, test_predictions,\n                                target_names=confusion_labels))\n    \n    \n    if verbose:\n        print(\"\\n================================> CONFUSION MATRIX <=====================================\")\n    \n        ## Confusion Matrix HeatMap\n        display(confusion_plot(confusion_matrix(y_test, test_predictions),\n                               labels=confusion_labels))\n        print(\"\\n=======================================> PLOTS <=========================================\")\n\n\n        ## Variable importance plot\n        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\n        roc_axes = axes[0, 0]\n        pr_axes = axes[0, 1]\n        importances = None\n\n        if importance_plot:\n            if not feature_labels:\n                raise RuntimeError(\"'feature_labels' argument not passed \"\n                                   \"when 'importance_plot' is True\")\n\n            try:\n                importances = pd.Series(clf.feature_importances_,\n                                        index=feature_labels) \\\n                                .sort_values(ascending=False)\n            except AttributeError:\n                try:\n                    importances = pd.Series(clf.coef_.ravel(),\n                                            index=feature_labels) \\\n                                    .sort_values(ascending=False)\n                except AttributeError:\n                    pass\n\n            if importances is not None:\n                # Modifying grid\n                grid_spec = axes[0, 0].get_gridspec()\n                for ax in axes[:, 0]:\n                    ax.remove()   # remove first column axes\n                large_axs = fig.add_subplot(grid_spec[0:, 0])\n\n                # Plot importance curve\n                feature_importance_plot(importances=importances.values,\n                                        feature_labels=importances.index,\n                                        ax=large_axs)\n                large_axs.axvline(x=0)\n\n                # Axis for ROC and PR curve\n                roc_axes = axes[0, 1]\n                pr_axes = axes[1, 1]\n            else:\n                # remove second row axes\n                for ax in axes[1, :]:\n                    ax.remove()\n        else:\n            # remove second row axes\n            for ax in axes[1, :]:\n                ax.remove()\n\n\n        ## ROC and Precision-Recall curves\n        clf_name = clf.__class__.__name__\n        roc_plot(y_test, y_probs, clf_name, ax=roc_axes)\n        precision_recall_plot(y_test, y_probs, clf_name, ax=pr_axes)\n\n        fig.subplots_adjust(wspace=5)\n        fig.tight_layout()\n        display(fig)\n    \n    ## Dump to report_dict\n    dump = dict(clf=clf, accuracy=[train_acc, test_acc], **scores_dict,\n                train_time=train_time, train_predictions=train_predictions,\n                test_time=test_time, test_predictions=test_predictions,\n                test_probs=y_probs, report=clf_rep, roc_auc=roc_auc,\n                model_memory=model_mem)\n    \n    return clf, dump","execution_count":null,"outputs":[]},{"metadata":{"Collapsed":"false","trusted":true,"id":"LIBHPi6oZCdQ"},"cell_type":"code","source":"def compare_models(y_test=None, clf_reports=[], labels=[], score='accuracy'):\n    \"\"\" Compare evaluation metrics for the True Positive class [1] of \n        binary classifiers passed in the argument and plot ROC and PR curves.\n        \n        Arguments:\n        ---------\n        y_test: to plot ROC and Precision-Recall curves\n         score: is the name corresponding to the sklearn metrics\n        \n        Returns:\n        -------\n        compare_table: pandas DataFrame containing evaluated metrics\n                  fig: `matplotlib` figure object with ROC and PR curves \"\"\"\n\n    \n    ## Classifier Labels\n    default_names = [rep['clf'].__class__.__name__ for rep in clf_reports]\n    clf_names =  labels if len(labels) == len(clf_reports) else default_names\n    \n    \n    ## Compare Table\n    table = dict()\n    index = ['Train ' + score, 'Test ' + score, 'Overfitting', 'ROC Area',\n             'Precision', 'Recall', 'F1-score', 'Support']\n    for i in range(len(clf_reports)):\n        scores = [round(i, 3) for i in clf_reports[i][score]]\n        \n        roc_auc = clf_reports[i]['roc_auc']\n        \n        # Get metrics of True Positive class from sklearn classification_report\n        true_positive_metrics = list(clf_reports[i]['report'][\"1\"].values())\n        \n        table[clf_names[i]] = scores + [scores[1] < scores[0], roc_auc] + \\\n                              true_positive_metrics\n    \n    table = pd.DataFrame(data=table, index=index)\n    \n    \n    ## Compare Plots\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n    \n    # ROC and Precision-Recall\n    for i in range(len(clf_reports)):\n        clf_probs = clf_reports[i]['test_probs']\n        roc_plot(y_test, clf_probs, label=clf_names[i],\n                 compare=True, ax=axes[0])\n        precision_recall_plot(y_test, clf_probs, label=clf_names[i],\n                              compare=True, ax=axes[1])\n    # Plot No-Info classifier\n    axes[0].plot([0,1], [0,1], linestyle='--', color='green')\n        \n    fig.tight_layout()\n    plt.close()\n    \n    return table.T, fig","execution_count":null,"outputs":[]},{"metadata":{"id":"j0rJkkPpZCdW"},"cell_type":"markdown","source":"<a id=\"naive-bayes\"></a>\n## 5.2.  Naive Bayes\nThe fundamental assumption made by Naive Bayes regarding the data is ***class conditional independence of features***. Sklearn provides different variants of Naive Bayes depending on whether the features follow a categorical distribution (CategoricalNB), normal distribution (GaussianNB), bernoulli distribution (BernoulliNB), multinomial distribution (MultinomialNB).\n\nSince majority of the features are categorical and follow a categorical distribution, we will use CategoricalNB. Continuous features will be discretized.","execution_count":null},{"metadata":{"trusted":true,"id":"y4ZD595pZCdX"},"cell_type":"code","source":"from sklearn.metrics import f1_score\nprimary_eval_metric = f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"id":"EunkqMqJZCda","outputId":"86cd8d63-4164-45fa-d359-b75f70b38a6b"},"cell_type":"code","source":"from sklearn.naive_bayes import CategoricalNB, GaussianNB \nfrom sklearn.preprocessing import KBinsDiscretizer, OrdinalEncoder\n\nconfusion_lbs = ['Not Survived', 'Survived']\n\nnumeric_trans_nb = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler()),\n    ('kbn', KBinsDiscretizer(n_bins=10, encode='ordinal'))])\n\ncategorical_trans_nb = Pipeline(steps=[\n    ('ordinal', OrdinalEncoder(dtype=np.int64))])\n\n## Column Transformer\npreprocessor_nb = ColumnTransformer(\n    transformers=[\n        ('num', numeric_trans_nb, numeric_columns),\n        ('cat', categorical_trans_nb, categorical_columns)],\n    remainder='passthrough')\n\n\n## Applying Column Transformer\nx_train_nb = preprocessor_nb.fit_transform(x_train_cat)\nx_test_nb = preprocessor_nb.transform(x_test_cat)\n\nnb_clf = CategoricalNB()\n\nnb_clf, nb_report = report(nb_clf, x_train_nb, y_train,\n                           x_test_nb, y_test,\n                           display_scores=[primary_eval_metric],\n                           refit=True,\n                           confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"SaxhbCldZCde"},"cell_type":"markdown","source":"<a id=\"logistic-regression\"></a>\n## 5.3.  Logistic Regression","execution_count":null},{"metadata":{"trusted":true,"id":"JsjGXaTeZCde","outputId":"ddf34da7-c8ee-449c-844b-bdc6a8a66206"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegressionCV\n\nlogit_cv = LogisticRegressionCV(class_weight='balanced', cv=5,\n                                max_iter=500, penalty='l2',\n                                scoring='f1', solver='liblinear',\n                                n_jobs=-1, random_state=0,\n                                refit=True, verbose=0)\n\nlogit_cv, logit_report = report(logit_cv, x_train, y_train,\n                                x_test, y_test,\n                                display_scores=[primary_eval_metric],\n                                importance_plot=True,\n                                feature_labels=feature_names,\n                                confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"sVM6U8lzZCdh"},"cell_type":"markdown","source":"<a id=\"k-nearest-neighbors\"></a>\n## 5.4.  K-Nearest Neighbors\nKNN estimator in Scikit-learn does not provide a way to pass class-weights to enable cost-sensitive/ class-weighted learning.","execution_count":null},{"metadata":{"trusted":true,"id":"xcZ1YpgUZCdh","outputId":"8c47bf37-c85e-41b5-8f9b-ef85fd7510f5"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=153, p=2,\n                           weights='uniform', n_jobs=-1)\n\nknn, knn_report = report(knn, x_train, y_train,\n                         x_test, y_test,\n                         display_scores=[primary_eval_metric],\n                         importance_plot=True,\n                         feature_labels=feature_names,\n                         confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"UMr4ihMeZCdo"},"cell_type":"markdown","source":"<a id=\"decision-tree\"></a>\n## 5.5.  Decision Tree","execution_count":null},{"metadata":{"trusted":true,"id":"MV9-wsfrZCdp","outputId":"fb7e8965-e656-4da6-f8ef-fb65e6fa1cdb"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(class_weight='balanced',\n                                       criterion='entropy',\n                                       max_depth=2, random_state=0)\n\ndecision_tree, decision_tree_report = report(decision_tree, x_train, y_train,\n                                             x_test, y_test,\n                                             display_scores=[primary_eval_metric],\n                                             importance_plot=True,\n                                             feature_labels=feature_names,\n                                             confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"q-6SzM1pZCdt"},"cell_type":"markdown","source":"<a id=\"decision-trees-with-bagging\"></a>\n## 5.6.  Decision Trees with Bagging","execution_count":null},{"metadata":{"trusted":true,"id":"ogSMPgn0ZCdt","outputId":"151fe674-8a33-4d0b-a637-da3c190b2bfd"},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n\nbagging_dtree = DecisionTreeClassifier(max_depth=2, class_weight='balanced',\n                                       criterion='entropy', random_state=0)\n\nbagging_clf = BaggingClassifier(base_estimator=bagging_dtree,\n                                max_samples=0.63, n_estimators=10,\n                                max_features=0.89, n_jobs=-1,\n                                random_state=0)\n\nbagging_clf, bagging_clf_report = report(bagging_clf, x_train, y_train,\n                                         x_test, y_test,\n                                         display_scores=[primary_eval_metric],\n                                         feature_labels=feature_names,\n                                         confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"tr9cQ0q7ZCdv"},"cell_type":"markdown","source":"<a id=\"random-forests\"></a>\n## 5.7.  Random Forests","execution_count":null},{"metadata":{"trusted":true,"id":"9ce0WV_9ZCdv","outputId":"8f0f11ee-d2f2-4ebe-8f95-8fce77241a7b"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(class_weight='balanced', criterion='entropy',\n                                       max_depth=2, max_samples=0.63, n_estimators=110,\n                                       n_jobs=-1, random_state=0)\n\nrandom_forest, random_forest_report = report(random_forest, x_train, y_train,\n                                             x_test, y_test,\n                                             display_scores=[primary_eval_metric],\n                                             importance_plot=True,\n                                             feature_labels=feature_names,\n                                             confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"34JTPklSZCdz"},"cell_type":"markdown","source":"<a id=\"decision-trees-with-adaboost\"></a>\n## 5.8.  Decision Trees with AdaBoost\nThe default base estimator for `AdaBoostClassifier` is `DecisionTreeClassifier(max_depth=1)`","execution_count":null},{"metadata":{"trusted":true,"id":"IdfB2kZzZCd1","outputId":"8723b30d-bebe-4637-9ea1-804bdda621d1"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nboosting_dtree = DecisionTreeClassifier(class_weight='balanced',\n                                        criterion='entropy',\n                                        max_depth=1, random_state=0)\nadaboot = AdaBoostClassifier(base_estimator=boosting_dtree,\n                             n_estimators=70, learning_rate=0.045,\n                             random_state=0)\n\nadaboot, adaboot_report = report(adaboot, x_train, y_train,\n                                 x_test, y_test,\n                                 display_scores=[primary_eval_metric],\n                                 importance_plot=True,\n                                 feature_labels=feature_names,\n                                 confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"CBB70B6WZCd7"},"cell_type":"markdown","source":"<a id=\"linear-svc\"></a>\n## 5.9.  Linear SVC","execution_count":null},{"metadata":{"trusted":true,"id":"ypaaCAjlZCd8","outputId":"b9947a26-33a8-4955-891b-aca7c8e1d99e"},"cell_type":"code","source":"from sklearn.svm import SVC\n\nlinear_svc = SVC(C=0.004, kernel='linear', probability=True,\n                 class_weight='balanced', random_state=0)\n\nlinear_svc, linear_svc_report = report(linear_svc, x_train, y_train,\n                                       x_test, y_test,\n                                       display_scores=[primary_eval_metric],\n                                       importance_plot=True,\n                                       feature_labels=feature_names,\n                                       confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"B3UrokQeZCd_"},"cell_type":"markdown","source":"<a id=\"svm-with-rbf-kernel\"></a>\n## 5.10.  SVM with RBF kernel","execution_count":null},{"metadata":{"trusted":true,"id":"R8nGrgqqZCd_","outputId":"f8d78b08-75ab-4714-9c82-6cced5699549"},"cell_type":"code","source":"rbf_svc = SVC(C=0.15, degree=3, kernel='rbf', probability=True,\n              class_weight='balanced', random_state=0)\n\nrbf_svc, rbf_svc_report = report(rbf_svc, x_train, y_train,\n                                 x_test, y_test,\n                                 display_scores=[primary_eval_metric],\n                                 importance_plot=True,\n                                 feature_labels=feature_names,\n                                 confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"ae1pNEG8ZCeC"},"cell_type":"markdown","source":"<a id=\"xgboost\"></a>\n## 5.11.  XGBoost","execution_count":null},{"metadata":{"trusted":true,"id":"_yhb4ARsZCeC","outputId":"ff422efc-8b40-406c-9cb2-e8f9087d4540"},"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.utils import class_weight\n\n## Compute `class_weights` using sklearn\ncls_weight = (y_train.shape[0] - np.sum(y_train)) / np.sum(y_train)\n\nxgb_clf = XGBClassifier(learning_rate=0.01, n_estimators=132, \n                        max_depth=2,\n                        scale_pos_weight=cls_weight,\n                        random_state=0, n_jobs=-1)\nxgb_clf.fit(x_train, y_train);\n\nxgb_clf, xgb_report = report(xgb_clf, x_train, y_train,\n                             x_test, y_test,\n                             display_scores=[primary_eval_metric],\n                             importance_plot=True,\n                             feature_labels=feature_names,\n                             confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"hQUlWijaZCeG"},"cell_type":"markdown","source":"<a id=\"lightgbm\"></a>\n## 5.12.  LightGBM\nLightGBM is similar to XGBoost but grows the trees in the ensemble based on Leaf-wise growth algorithm unlike Level-wise algorithm in XGBoost.","execution_count":null},{"metadata":{"trusted":true,"id":"XYDhyjf1ZCeG","outputId":"ef323e06-80fd-4f99-c3a5-a8744db07763"},"cell_type":"code","source":"from lightgbm import LGBMClassifier\n\nparams = {'objective': 'binary', 'verbose': -1, 'lambda_l1': 0.0043026218632336,\n          'lambda_l2': 3.7892016634143017e-07, 'num_leaves': 31,\n          'feature_fraction': 0.4, 'bagging_fraction': 0.462444786473255,\n          'bagging_freq': 5, 'min_child_samples': 20}\n\nlgbm_clf = LGBMClassifier(**params, random_state=0, n_jobs=-1)\nlgbm_clf.fit(x_train, y_train);\n\nlgbm_clf, lgbm_report = report(lgbm_clf, x_train, y_train,\n                               x_test, y_test,\n                               display_scores=[primary_eval_metric],\n                               importance_plot=True,\n                               feature_labels=feature_names,\n                               confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"lJUgBpWmZCeI"},"cell_type":"markdown","source":"<a id=\"catboost\"></a>\n## 5.13.  CatBoost\nCat boost performs better without One-hot encoding because it performs an internal categorical encoding that is similar to Leave One Out Encoding (LOOE). So, we can give the dataframe as input to the catboost classifier.","execution_count":null},{"metadata":{"trusted":true,"id":"_LJOe2eEZCeK","outputId":"28f0a561-08a4-4e05-e351-c8008dfa8031"},"cell_type":"code","source":"from catboost import CatBoostClassifier\n\n# Basic working\n\ncatboost_clf = CatBoostClassifier(cat_features=categorical_columns,\n                                  l2_leaf_reg=120, depth=6,\n                                  auto_class_weights='Balanced',\n                                  iterations=200, learning_rate=0.16,\n                                  use_best_model=True,\n                                  early_stopping_rounds=150,\n                                  eval_metric='F1', random_state=0)\n\ncatboost_clf.fit(x_train_cat, y_train, \n                 eval_set=(x_train_cat, y_train),\n                 verbose=False)\n\n\nf_labels = categorical_columns+numeric_columns\ncatboost_clf, catboost_report = report(catboost_clf, x_train_cat, y_train,\n                                       x_test_cat, y_test,\n                                       display_scores=[primary_eval_metric],\n                                       importance_plot=True,\n                                       feature_labels=f_labels,\n                                       confusion_labels=confusion_lbs)","execution_count":null,"outputs":[]},{"metadata":{"id":"N8VZt08QZCeM"},"cell_type":"markdown","source":"<a id=\"model-comparison\"></a>\n# 6.  Model Comparison\nSince input data format for Naive Bayes and CatBoost are different, we will add them to the comparison manually.","execution_count":null},{"metadata":{"trusted":true,"id":"e3vBejgLZCeN"},"cell_type":"code","source":"report_list = [nb_report, logit_report, knn_report, decision_tree_report,               \n               bagging_clf_report, random_forest_report, adaboot_report,\n               xgb_report, lgbm_report, catboost_report, linear_svc_report,\n               rbf_svc_report, ]\nclf_labels = [rep['clf'].__class__.__name__ for rep in report_list]\nclf_labels[-2], clf_labels[-1] = 'Linear SVC', 'RBF SVC'","execution_count":null,"outputs":[]},{"metadata":{"id":"_zQ2sqE4ZCeP"},"cell_type":"markdown","source":"<a id=\"evaluation-metrics\"></a>\n## 6.1.  Evaluation Metrics","execution_count":null},{"metadata":{"trusted":true,"id":"_i0PYNYWZCeQ","outputId":"e54c29df-48d9-4fd4-ba9c-a2b2304d542d"},"cell_type":"code","source":"compare_table, compare_plot = compare_models(y_test, clf_reports=report_list,\n                                             labels=clf_labels,\n                                             score=primary_eval_metric.__name__)\n\ncompare_table.sort_values(by=['Overfitting', 'F1-score'], ascending=[True, False])","execution_count":null,"outputs":[]},{"metadata":{"id":"LRywwsZzZCeU"},"cell_type":"markdown","source":"<a id=\"roc-and-pr-curves\"></a>\n## 6.2.  ROC and PR Curves","execution_count":null},{"metadata":{"trusted":true,"id":"e9tSRoNvZCeU","outputId":"74097ffb-f664-4cef-82d1-9bcd7e469d1d"},"cell_type":"code","source":"compare_plot","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}