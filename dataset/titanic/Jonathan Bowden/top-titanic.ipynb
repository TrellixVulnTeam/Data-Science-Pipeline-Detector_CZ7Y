{"cells":[{"metadata":{},"cell_type":"markdown","source":"* # MY APPROACH TO A LEADING SCORE\n\nHi, thanks for reading. Please **UPVOTE** if you enjoy this. Outlined below I set out the following procedure for predicting the survivability of passengers from the famous Titanic sinking. My approach for building a prediction model is as follows:\n\n1. Load the data\n2. Explore the data\n3. Modify the data\n    3.1 Impute\n    3.2 Remove outliers\n    3.3 Scale\n    3.4 Drop where too many missing values\n4. Create new features\n5. Build an Sklearn Pipeline\n6. Train and Test several models\n7. Generate submission file\n    "},{"metadata":{},"cell_type":"markdown","source":"## Import basic packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport math\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(\"../input/titanic/train.csv\").set_index('PassengerId')\ntest = pd.read_csv(\"../input/titanic/test.csv\").set_index('PassengerId')\n\ny = train['Survived']\ntrain = train.drop('Survived',axis=1)\n\ndisplay(train.head())\ndisplay(test.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## View the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.concat([train,test])\nX.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Convert Pclass to categorical, as it is made up of class either 1,2 or 3."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pclass is actually categorical\nX['Pclass'] = X['Pclass'].astype(object)\n\nnum_col = X.select_dtypes(include=['float64','int64']).columns\ncat_col = X.select_dtypes(include=['object']).columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## View the Seaborn Pairplot for Numerical Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(X[num_col],corner=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this, I would make the following points:\n* Most of the data is positively skewed\n* There are a couple of outliers in Fare\n* There are no glaringly obvious strong correlations here"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\ncorrelation = X[num_col].corr()\nsns.heatmap(correlation, mask = correlation <0.4, cmap='Blues')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As discussed above, there are no correlations between our features here, so there is no **multicolinearity**. What does this mean? In essense a model with features that share some form of relationship won't improve our model. It's probable that it won't negatively effect are model but it's far more likely to result in an overfitted and biased model. Leaving any variables in that have multicolinearity will make your fit appear to be good but when you come to submit the model, you will find poor results. "},{"metadata":{},"cell_type":"markdown","source":"## Imputation for Numerical Columns\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X[num_col].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Age"},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"In order to deal with the above missing values, we explore a few approaches. The most troublesome at the moment is the missing values in age. Simply imputing the median has a huge effect on the distribution, see charts below:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"sns.distplot(X.Age).set_title(\"Age Before Imputing\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's see the median imputed results..."},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"imputer = SimpleImputer(strategy='median')\nimputed = imputer.fit_transform(X[['Age']])\n\nsns.distplot(imputed).set_title(\"Age After Median Imputing\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This isn't great, as our model may end up thinking that being 28 is super important in determining chance of survival, which it probably wasn't. As such, I have created a random imputer in the range 20,55 to retain the distribution. This is a somewhat arbitrary choice and a more scientific method would be preferred but for our purposes here, this should be fine:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def replace_with_random(a):\n    \"\"\"\n    a: Value or NaN to be replaced\n    \n    Cannot set a random state as it would generate the same value each time this function\n    is called. This is unlikely to be the derired behaviour\n    \"\"\"    \n    \n    from random import randint\n        \n    if pd.isnull(a):\n        return randint(20,55)\n    else:\n        return a","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"randimpute = X['Age'].apply(lambda a: replace_with_random(a))\n\nsns.distplot(randimpute).set_title(\"Age After Random Imputing\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"And this is the direibution of ages we will use."},{"metadata":{"trusted":true},"cell_type":"code","source":"# For now I will use my random approach for Age\nX['Age'] = randimpute","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Fare"},{"metadata":{},"cell_type":"markdown","source":"There is only one value to impute in Fare, using a median seems sensible"},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = SimpleImputer(strategy='median')\nX['Fare'] = imputer.fit_transform(X[['Fare']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That should be it for missing data in numerical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"X[num_col].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### ... beautiful!"},{"metadata":{},"cell_type":"markdown","source":"## Imputation for categorical columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col = X.select_dtypes(include=['object']).columns\nX[cat_col]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[cat_col].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's a lot of missing data for cabin, lets explore deck from it:"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Do something about cabin feature, at least extract deck where possible\nplt.figure(figsize=(8,6))\nX['Deck'] = X['Cabin'].str[0]\nsns.countplot(x='Deck',data=X,palette=\"husl\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What does survival look like by Deck?"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"temp_data = pd.merge(X['Deck'],y,on='PassengerId')\ntemp_data = temp_data.groupby('Deck').sum()\nsns.barplot(x=temp_data.index,y=temp_data['Survived'],palette='husl')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So on the limited data we have, clearly being in the upper decks improves survival. It would be nice to include this, but 1,014 values is just too many to impute. For now I will drop Deck/Cabin\n\nAn idea would be to try to infer Deck from ticket, as there seems to be some information in there that might help."},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X.drop(['Cabin','Deck'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_col = X.select_dtypes(include=['object']).columns\nX[cat_col].isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are still two missing values for \"Embarked\", but we will let pipline handle the missing embarked value, see below for embedded iumputation using Sklearn's pipeline functions. There are only 2, so \"most common\" method should be OK."},{"metadata":{},"cell_type":"markdown","source":"## Remove outliers\nBack to the numerical data. Let's clean up those outliers from before. Pipline imputation will take care of any missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"X['Fare'] = X['Fare'][X['Fare']<400]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"In order to simplify our features, let's create a family variable that combines Parch and SibSp. It would be preferalble to have 1 dimension with only 60% zeros over 2 dimensions with at least 70% zeros. This is OK to do because, the data Parent/Child and Sibling/Spouse is of the same kind: they are counts of people."},{"metadata":{"trusted":true},"cell_type":"code","source":"X['FamilySize'] = X['SibSp'] + X['Parch']\nX = X.drop(['SibSp','Parch'],axis=1)\n\nnum_col = X.select_dtypes(include=['float64','int64']).columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Pipline"},{"metadata":{},"cell_type":"markdown","source":"Here we prepare the pipeline. See sklearn for further imformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[\n    ('imputer',SimpleImputer(strategy='constant')),\n    ('scaler',StandardScaler())\n    ])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_col),\n        ('cat', categorical_transformer, cat_col)\n    ])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the data back into train and test datasets and split for modelling purposes."},{"metadata":{"trusted":true},"cell_type":"code","source":"# IMPORTANT: Now data is pre-processed, put it back into train and test sets and then split X and y.\ntest = X.loc[test.index]\nX = X.loc[train.index]\ny = y.loc[train.index]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.75,random_state=81)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train RF model model, I did a Grid Search CV on this, and it yielded the following setup of parameters:\nRandomForest = RandomForestClassifier(n_estimators=500,\n                                      min_samples_split=5,\n                                      min_samples_leaf=1,\n                                      random_state=81)\n\nRF_pipeline = Pipeline(steps=[('preprocessor', preprocessor),('model', RandomForest)])\n\nRF_pipeline.fit(X_train, y_train)\n\ny_pred = RF_pipeline.predict(X_test)\n\nRF_accuracy = accuracy_score(y_test,y_pred)\n\nprint(\"Accuracy:\",RF_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n# from sklearn.metrics import make_scorer\n\n# parameters = {'model__n_estimators':[100,500,750,1000],\n#               'model__min_samples_split':[2,5,10],\n#               'model__min_samples_leaf':[1,2,5,10],\n#               'model__max_depth':[1,3,5,10,20]}\n\n# scorer = make_scorer(accuracy_score,greater_is_better=True)\n\n# grid = GridSearchCV(RF_pipeline,parameters,scoring=scorer)\n\n# grid.fit(X_train,y_train)\n\n# y_pred = grid.predict(X_test)\n\n# accuracy = accuracy_score(y_test,y_pred)\n\n# final_params = grid.best_params_\n\n# print(\"Accuracy:\",accuracy)\n# print(final_params)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"XGB = XGBClassifier(eta=0.0001,max_depth = 12, gamma = 3,random_state=81)\n\nXGB_pipe = Pipeline(steps=[('preprocessor', preprocessor),('model', XGB)])\n\nXGB_pipe.fit(X_train, y_train)\n\ny_pred = XGB_pipe.predict(X_test)\n\nXGB_accuracy = accuracy_score(y_test,y_pred)\n\nprint(\"Accuracy:\",XGB_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\n\nparameters = {'model__eta':[0.0001,0.0005,0.001,0.01],\n              'model__max_depth':[8,10,12,15],\n              'model__gamma':[1,2,3,4,5]}\n\nscorer = make_scorer(accuracy_score,greater_is_better=True)\n\ngrid = GridSearchCV(XGB_pipe,parameters,scoring=scorer)\n\ngrid.fit(X_train,y_train)\n\ny_pred = grid.predict(X_test)\n\naccuracy = accuracy_score(y_test,y_pred)\n\nfinal_params = grid.best_params_\n\nprint(\"Accuracy:\",accuracy)\nprint(final_params)\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Perceptron"},{"metadata":{"trusted":true},"cell_type":"code","source":"Perceptron = Perceptron()\n\nPerc_pipeline = Pipeline(steps=[('preprocessor',preprocessor),('model',Perceptron)])\n\nPerc_pipeline.fit(X_train,y_train)\n\ny_pred = Perc_pipeline.predict(X_test)\n\nPerceptron_accuracy = accuracy_score(y_test,y_pred)\n\nprint(\"Accuracy:\",Perceptron_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"LogRegCV = LogisticRegressionCV(cv=5)\n\nLR_pipeline = Pipeline(steps=[('preprocessor',preprocessor),('model',LogRegCV)])\n\nLR_pipeline.fit(X_train,y_train)\n\ny_pred = LR_pipeline.predict(X_test)\n\nLogReg_accuracy = accuracy_score(y_test,y_pred)\n\nprint(\"Accuracy:\",LogReg_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Ada Boost"},{"metadata":{"trusted":true},"cell_type":"code","source":"ADA = AdaBoostClassifier()\n\nADA_pipeline = Pipeline(steps=[('preprocessor',preprocessor),('model',ADA)])\n\nADA_pipeline.fit(X_train,y_train)\n\ny_pred = ADA_pipeline.predict(X_test)\n\nADA_accuracy = accuracy_score(y_test,y_pred)\n\nprint(\"Accuracy:\",ADA_accuracy)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Stacking for all models\nThis combines all the models, to see if the combined models can predict better. Logistic Regression is used to choose the overall result from amongst the underlying models."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.ensemble import StackingClassifier\n\nestimators = [('RF',RF_pipeline),\n              ('Perceptron',Perc_pipeline),\n              ('ADA',ADA_pipeline),\n              ('LogReg',LR_pipeline),\n              ('XGB',XGB_pipe)]\n\nstack = StackingClassifier(estimators=estimators)\nstack.fit(X_train,y_train)\ny_pred = stack.predict(X_test)\nstack_accuracy = accuracy_score(y_test,y_pred)\nprint(\"Accruacy:\",stack_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nfrom sklearn.ensemble import VotingClassifier\n\nestimators = [('RF',RF_pipeline),\n              ('Perceptron',Perc_pipeline),\n              ('ADA',ADA_pipeline),\n              ('LogReg',LR_pipeline),\n              ('XGB',XGB_pipe)]\n\nvote = VotingClassifier(estimators=estimators)\nvote.fit(X_train,y_train)\ny_pred = vote.predict(X_test)\nvote_accuracy = accuracy_score(y_test,y_pred)\nprint(\"Accruacy:\",vote_accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"results = pd.DataFrame({'Model':['Random Forest','Perceptron','Logistic Regression','ADA Boost','XGBoost','Stacked Model','Vote Model'],\n                        'Accuracy':[RF_accuracy, Perceptron_accuracy,LogReg_accuracy,ADA_accuracy,XGB_accuracy,stack_accuracy,vote_accuracy]}).set_index('Model')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"results.sort_values('Accuracy',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So the RF model performs best. It is possible that running a GridSearchCV on ADA Boost and Perceptron may lead to better results and ultimately the Stack may improve by extension."},{"metadata":{},"cell_type":"markdown","source":"## Generate Submission"},{"metadata":{},"cell_type":"markdown","source":"Thank you for reading. Please **UPVOTE** if you have enjoyed and leave a comment to indicate any suggestions for improvement, either to my approach or code. \n\nThanks again\n\n**Jon**"},{"metadata":{},"cell_type":"markdown","source":"![UPVOTE](https://i.imgur.com/RVyQY7r.png)"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = RF_pipeline.predict(test)\n\nsubmission = pd.DataFrame(test_pred,index=test.index,columns=['Survived'])\n\nsubmission.to_csv(\"./submission.csv\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}