{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib as ml\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport matplotlib.pyplot as plt\n%matplotlib inline\nml.style.use('ggplot')\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, roc_curve, roc_auc_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"td = pd.read_csv('/kaggle/input/titanic/train.csv')\nprint(td.shape)\ntd.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration\n*  Describing the data\n*  Finding missing values\n*  Quality, completeness and tidiness issues\n    -  Search if any data type is falsely assigned.\n*  Cocluding relations between features. (heatmap)\n    -  Distplot for numerical data\n    -  Countplot for categorical data\n*  Outlier detection","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us see what columns we have and assert their data types \n*  If any datatype doesn't match ur criteria. If that is the case, try modifying it to the datatype it should be in.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There are 4 categorical columns :\n*  Survived (NOM)\n*  PClass (ORD)\n*  Sex (NOM)\n*  Embarked (NOM)\n\n#### There are 8 numerical columns :\n*  PassengerId\n*  Name\n*  Age\n*  SibSp'\n*  Parch\n*  Ticket\n*  Fare\n*  Cabin","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's check for null values and data types.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### QUALITY AND COMPLETENESS ISSUES\n1. Handle missing values\n2. De-label Pclass to make the data easier to interpret.\n3. Sib(SIBLING) and Sp(SPOUSE) in one column.\n4. Parch in one column (Par = PARENTS ; ch = CHILDREN)\n5. Decide whether or not to keep Cabin.\n6. Merging Sibsp and Parch into one column called 'Fam' meaning family.\n7. Separate out title from name\n8. Encode the 'Sex' and 'Embarked' columns\n9. Drop the 'Ticket' column\n10. Drop Name after feature engineering\n11. If not significant, drop PassengerId\n\n##### TIDINESS ISSUES\n1. Sib(SIBLING) and Sp(SPOUSE) in one column.\n2. Parch in one column (Par = PARENTS ; ch = CHILDREN)\n3. Pclass in un-interpretable format\n4. Merging Sibsp and Parch into one column called 'Fam' meaning family.\n5. Separate out title from name and then drop it\n6. Drop the 'Ticket' column\n7. Drop Name after feature engineering\n8. If not significant, drop PassengerId","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### VISUALIZING THE DATA BEFORE PROCESSING THE DATA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td.hist(figsize=(20,10), color='maroon', bins=25)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.distplot(td[td.Survived==1]['Age'])\nsns.distplot(td[td.Survived==0]['Age'])\nplt.legend(['SURVIVED','DID NOT SURVIVE'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[\n    go.Bar(name='SURVIVED', x=list(td.Sex.value_counts().index), y=td[td.Survived==1]['Sex'].value_counts().values),\n    go.Bar(name='DID NOT SURVIVE', x=list(td.Sex.value_counts().index), y=td[td.Survived==0]['Sex'].value_counts().values)\n])\nfig.update_layout(barmode='group',title=\"SEX\")\nfig.show()\n\nfig = go.Figure(data=[go.Pie(labels=['MALES','FEMALES'],\n                             values=[td[(td.Sex=='male') & (td.Survived==1)].shape[0],td[(td.Sex=='female') & (td.Survived==1)].shape[0]])])\nfig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n                  marker=dict(colors=['yellow','purple'],line=dict(color='#000000', width=2)))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[\n    go.Bar(name='SURVIVED', x=list(td.Pclass.value_counts().index), y=td[td.Survived==1]['Pclass'].value_counts().values),\n    go.Bar(name='DID NOT SURVIVE', x=list(td.Pclass.value_counts().index), y=td[td.Survived==0]['Pclass'].value_counts().values)\n])\nfig.update_layout(barmode='group',title=\"PCLASS\")\nfig.show()\n\nfig = go.Figure(data=[go.Pie(labels=['1st CLASS','2nd CLASS','3rd CLASS'],\n                             values=[td[(td.Pclass==1) & (td.Survived==1)].shape[0],td[(td.Pclass==2) & (td.Survived==1)].shape[0],td[(td.Pclass==3) & (td.Survived==1)].shape[0]])])\nfig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n                  marker=dict(colors=['yellow','lightgreen','darkorange'],line=dict(color='#000000', width=2)))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = go.Figure(data=[\n    go.Bar(name='SURVIVED', x=list(td.Embarked.value_counts().index), y=td[td.Survived==1]['Embarked'].value_counts().values),\n    go.Bar(name='DID NOT SURVIVE', x=list(td.Embarked.value_counts().index), y=td[td.Survived==0]['Embarked'].value_counts().values)\n])\nfig.update_layout(barmode='group',title=\"EMBARKED\")\nfig.show()\n\nfig = go.Figure(data=[go.Pie(labels=['S','C','Q'],\n                             values=[td[(td.Embarked=='S') & (td.Survived==1)].shape[0],\n                                     td[(td.Embarked=='C') & (td.Survived==1)].shape[0],\n                                     td[(td.Embarked=='Q') & (td.Survived==1)].shape[0]])])\nfig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n                  marker=dict(colors=['maroon','pink','darkturquoise'],line=dict(color='#000000', width=2)))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### PRIMARY CONCLUSIONS DERIVED\n-  Most of the people were of the age range 20-40\n-  Maximum number of people bought tickets of fare < 100.\n-  Most of the people were travelling alone, without parents, children, siblings or spouse.\n-  The highest numbe of passengers were from Pclass 3.\n    -  Most number of people from Pclass 3 did not survive. This is evident as they were passengers of the inferior class.\n    -  Since facilities were more easily available for 1st class passengers, hence survival:deceased ratio is higher for them(39.8%)\n-  Survival:Deceased ratio is higher in females than males. Maybe due to the 'women and children' first policy.\n    -  More females survived(233) than males(109)\n-  Highest survival rate is for 'Embarked' class 'S'(63.8%)\n\n##### FEATURE CORRELATION BEFORE PROCESSING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(td.corr(),annot=True,linewidth=1,linecolor='white')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### CORRELATION ANALYSIS\n1. Pclass and Fare show high anti-correlation. This is expected. A lower Pclass(1) values actually indicates a higher social/economic strata, like the Royals,the Aristocrats etc. So as Fare increases, numeric value of Pclass drops, and it concentrate more towarda '1', meaning, a higher class person.\n2. Pclass and Survived show high anti-correlation. This is expected as first class passengers' safety was given more importance than other classes.\n3. Survived and Fare have a faintly moderate correlation. First class passengers' safety was given more importance than other classes, and obviously high class passengers paid more Fare than lower class passengers.\n4. Parch and Sibsp have moderate correlation, maybe bacuse they fall under 'family' only.\n\n##### DATA WRANGLING\nWe see that Fare and Pclass are highly correlated(anti-correlation) to each other than any other features(magnitude>0.50). A low Pclass(means a higher class) will automatically mean a high Fare. We remove Fare.\nPclass can be further processed.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td.drop(columns=['Fare'],inplace=True)\ntd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### COMPLETENESS ISSUES\n1. Handle missing data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(td.isnull().sum())\nprint(\"\\nFraction of values that are missing in the 'Cabin' feature : \", (td.Cabin.isnull().sum()/td.shape[0]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### More than 77% of the values in Cabin are missing. Since it is impossible to replace so many missing values without introducing errors, we remove the feature named 'Cabin'.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td.drop(columns=['Cabin'],inplace=True)\ntd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Replace missing Age values with mean.\n1. We can directly replace the missing values by the mean of the ages value.\n2. Another method could be to fill the missing values based on 'Pclass' based grouping. For example, fill in the missing value of a particular 'Age' entry, which has a Pclass say '1', with a value that is the mean of the all ages corresponding to that particular Pclass only. Idea source : https://www.kaggle.com/thomaswoolley/rf-and-k-nn-titanic-0-79-score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td1 = td.copy()\ntd2 = td.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 1st METHOD : THE ONE I GENERALLY DO","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td1.Age.fillna(np.mean(td.Age),inplace=True)\ntd1.Age.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 2nd METHOD : BASED ON PCLASS\n1. Group the dataset on the basis of Pclass and for every Pclass, find the mean of the ages. Store them orderwise in a list.\n2. Now, loop through the Pclass values ->\n    -  For every Pclass, say i:\n        -  Pick all rows under the 'Age' column for that Pclass and replace the NaN values with the corresponding mean from the previously created list.\n        (For example, for Pclass 1, we'll replace NaN values with the 1st value of the list.)\n       end loop.\n   Done.\n\n###### FINALLY, PLOT A DISTPLOT TO CHECK NEW DISTRIBUTION OF AGES. DOES OUR PRIMARY OBSERVATION DEVIATE AFTER PROCESSING ?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(td2.groupby('Pclass')['Age'].mean())\nmean_list = list(td2.groupby('Pclass')['Age'].mean().values)\nprint(\"\\nList of means of Ages grouped according to Pclass\",mean_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing by looping through Pclass values. Total number of unique values of Pclass = 3. So the loop runs 3 times.\nfor i in range(3):\n    td2.loc[td2['Pclass']==i+1,'Age'] = td2.loc[td2['Pclass']==i+1,'Age'].fillna(mean_list[i])\nprint(td2.Age.isnull().sum())\ntd = td2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.distplot(td['Age'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Replacing the two missing values in 'Embarked' with the most common value under this feature(handling missing categorical data)\nLet's find out the most popular 'Embarked' type","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Most popular type : \", td.Embarked.value_counts().sort_values(ascending=False).index[0])\nto_replace = td.Embarked.value_counts().sort_values(ascending=False).index[0]\nsns.countplot(x='Embarked',data=td)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td.Embarked.fillna(to_replace,inplace=True)\ntd.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### ALL MISSING VALUES HAVE BEEN HANDLED.\n##### QUALITY ISSUES\n##### 1. Merge Sibsp and Parch into one column and drop the other two","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td['Fam'] = td['SibSp'] + td['Parch']\ntd.drop(columns=['SibSp','Parch'],inplace=True)\ntd.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Plot to check distribution","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.distplot(td['Fam'])\nplt.title(\"DISTRIBUTION OF FAMILY\")\nplt.show()\n\nplt.figure(figsize=(20,10))\nsns.countplot(x='Fam',data=td,hue='Pclass')\nplt.title(\"NO. OF FAMILY MEMBERS VS PCLASS\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### CONCLUSIONS :\n1. Most people were travelling alone.\n2. The highest number of family members one was travelling with was 10\n3. Maximum number of people travelling alone were from 3rd class.\n4. No person from 1st or 2nd class travelled with > 5 family members,\n\n##### 3. Remove PassengerId and Ticket","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td.drop(columns=['PassengerId','Ticket'],inplace=True)\ntd.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 2. FEATURE ENGINEERING\n-  Separate Title from Name and then drop Name\n-  Decode Pclass and OHE it.\n-  OHE Sex\n-  OHE Embarked\n\n\n1. Separate Title from Name\n\n\n-  Create a new column named 'Title'\n-  Apply regex to extract title from name\n    -  Check for anomalies in the extracted titles and clean the untidy data.\n-  Drop Name","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td['Title'] = td['Name']\n\n# Apply regex per name\n# Use function : Series.str.extract()\nfor name in td['Name']:\n    td['Title'] = td['Name'].str.extract('([A-Za-z]+)\\.',expand=True)    # Regex to get title : ([A-Za-z]+)\\.\n\n# Drop Name\ntd.drop(columns=['Name'],inplace=True)\ntd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check extracted data for quality\ntd.Title.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### NOTE : Found unmatching titles like -> 'Don','Rev','Mme','Ms','Major','Lady','Sir','Mlle','Col','Capt','Countess','Jonkheer'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"title_mapping = {'Don':'Rare','Rev':'Rare','Mme':'Miss','Ms':'Miss','Major':'Rare','Lady':'Royal','Sir':'Royal','Mlle':'Miss','Col':'Rare','Capt':'Rare','Countess':'Royal','Jonkheer':'Royal'}\n\ntd.replace({'Title':title_mapping},inplace=True)\ntd.Title.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. Decode Pclass","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td['Pclass_new']=np.nan\nrep_list = ['first','second','third']\n\n# Decode manually for all 3 columns\nfor i in range(3):\n    td.loc[td['Pclass']==i+1,'Pclass_new'] = rep_list[i]\n    \n# Drop Pclass\ntd.drop(columns=['Pclass'],inplace=True)\ntd.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"td.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"3. Perform OHE on Pclass_new, Sex, Embarked and Title","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use pd.get_dummies(data,drop_first)\nencd_col = ['Pclass_new','Sex','Embarked','Title']\n\nohe_features = pd.get_dummies(data=td.loc[:,encd_col],drop_first=True)   # In OHE we usually create k-1 encoded features for k classes.\n# Drop original columns\ntd.drop(columns=encd_col,inplace=True)\ntd = td.join(ohe_features)\ntd.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### FINAL CHECK OF CURRENT CORRELATION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nsns.heatmap(td.corr(),annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### LOOKS PRETTY OKAY.\n##### OUR TRAINING DATASET IS READY. LET'S CHECK OUR TESTING DATASET","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tstd = pd.read_csv('/kaggle/input/titanic/test.csv')\ntstd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### So as we can see, we have to repeat all steps we did on the training data to make it an appropriate testing dataset.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### We know which columns to drop. We drop them without further analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tstd.drop(columns=['Ticket','Fare','Cabin'],inplace=True)              # We don't drop PassengerID because we need it for creating o/p file\ntstd.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Filling in the missing values in Age","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"age_lst = list(tstd.groupby('Pclass')['Age'].mean().values)\nfor i in range(3):\n    tstd.loc[tstd['Pclass']==i+1,'Age'] = tstd.loc[tstd['Pclass']==i+1,'Age'].fillna(age_lst[i])\ntstd.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Extracting and handling Title, simultaneously dropping Name","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tstd['Title'] = tstd['Name']\nfor i in tstd['Name']:\n    tstd['Title'] = tstd['Name'].str.extract('([A-Za-z]+)\\.',expand=True)\n# Dropping Name\ntstd.drop(columns=['Name'],inplace=True)\n# Replacing by mapping\ntitle_mapping = {'Don':'Rare','Rev':'Rare','Mme':'Miss','Ms':'Miss','Major':'Rare','Dona':'Royal','Mlle':'Miss','Col':'Rare','Capt':'Rare'}\n\ntstd.replace({'Title':title_mapping},inplace=True)\nprint(tstd.Title.unique())\ntstd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Handling Pclass","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tstd['Pclass_new'] = np.nan\nnew_pc = ['first','second','third']\nfor i in range(3):\n    tstd.loc[tstd.Pclass==i+1,'Pclass_new'] = new_pc[i]\ntstd.drop(columns=['Pclass'],inplace=True)\ntstd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Combining SibSp and Parch to Fam","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tstd['Fam'] = tstd['SibSp'] + tstd['Parch']\ntstd.drop(columns=['SibSp','Parch'],inplace=True)\ntstd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### OHE features Sex, Embarked, Title, Pclass_new","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use pd.get_dummies(data,drop_first)\nencd_col1 = ['Pclass_new','Sex','Embarked','Title']\n\nohe_features2 = pd.get_dummies(data=tstd.loc[:,encd_col1],drop_first=True)   # In OHE we usually create k-1 encoded features for k classes.\n# Drop original columns\ntstd.drop(columns=encd_col1,inplace=True)\ntstd = tstd.join(ohe_features2)\ntstd.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### COMPARING TRAIN AND TEST DATA","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"td.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tstd.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### LOOKS GOOD.\n##### NOW WE START BUILDING THE MODELS\nWe'll focus on these models :\n1. Logistic Regression (as it is classification based)\n2. KNN\n3. Random Forest\n4. Adaboost\n\n##### LOGISTIC REGRESSION","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the training data by the conventional 80-20 split\nX = td.drop(columns=['Survived'])\nY = td['Survived'].values\ntrainx, testx, trainy, testy = train_test_split(X,Y,test_size=0.2)\nx,y = np.array(td.iloc[:,1:].values),np.array(td.iloc[:,0].values)\ntest = np.array(tstd.iloc[:,:].values)\nprint(\"Train : \",trainx.shape,trainy.shape)\nprint(\"Test : \",testx.shape,testy.shape)\n\n# Creating the model\nlogr = LogisticRegression(penalty='l2',C=1.0,solver='lbfgs')\nlogr.fit(trainx,trainy)\n\n# Preds and accuracy\ny_pred1 = logr.predict_proba(testx)\n# We are interested in the True and False Positives only.\nfptp = y_pred1[:,1]  # As 2nd value tells the probability of getting a 1\n\n# Getting the ROC-AUC score and plotting the ROC curve\nlogr_score = roc_auc_score(testy,fptp)\nprint(\"ROC AUC score = \",logr_score)\nlr_fp,lr_tp,_ = roc_curve(testy,fptp)   # Returns FPR, TPR and thresholds.\nplt.figure(figsize=(20,10))\nplt.plot(lr_fp,lr_tp,marker='.',label=\"Logistic Regression ROC Curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix and accuracy score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred2 = logr.predict(testx)\ntn,fp,fn,tp = confusion_matrix(testy,y_pred2).ravel()\nacc1 = (tp+tn)/(tp+tn+fp+fn)\nprint(acc1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating output file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"op1 = logr.predict(tstd.drop(columns=['PassengerId'],axis=1))\nopf_df1 = pd.DataFrame({'PassengerId': tstd.PassengerId, 'Survived': op1})\nopf_df1.to_csv('Balaka_LGR.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nscaler = StandardScaler()\ntrainx_scaled = scaler.fit_transform(trainx)\n\n# Hyperparameter tuning\nparam_grid = {\n    'n_neighbors' : [3,5,7,9],\n    'weights' : ['uniform','distance'],\n    'metric' : ['euclidean','manhattan','minkowski'],\n    'algorithm' : ['auto','ball_tree','kd_tree','brute']\n}\nknn_gs = GridSearchCV(estimator=knn,param_grid=param_grid,cv=10)\nknn_gs.fit(trainx_scaled,trainy)\nprint(knn_gs.best_score_)\nprint(knn_gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the knn classifier\nknn_best = KNeighborsClassifier(n_neighbors=knn_gs.best_params_.get('n_neighbors'),weights=knn_gs.best_params_.get('weights'),algorithm=knn_gs.best_params_.get('algorithm'),metric=knn_gs.best_params_.get('metric'))\nknn_best.fit(trainx,trainy)\n\n# ROC-AUC score\ny_pred3 = knn_best.predict_proba(testx)\n# We are interested in the True and False Positives only.\nfptp2 = y_pred3[:,1]  # As 2nd value tells the probability of getting a 1\n\n# Getting the ROC-AUC score and plotting the ROC curve\nknn_score = roc_auc_score(testy,fptp2)\nprint(\"ROC AUC score = \",knn_score)\nlr_fp2,lr_tp2,_ = roc_curve(testy,fptp2)   # Returns FPR, TPR and thresholds.\nplt.figure(figsize=(20,10))\nplt.plot(lr_fp2,lr_tp2,marker='.',label=\"KNN ROC Curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix and accuracy score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred4 = knn_best.predict(testx)\ntn,fp,fn,tp = confusion_matrix(testy,y_pred4).ravel()\nacc2 = (tp+tn)/(tp+tn+fp+fn)\nprint(acc2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating O/P file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"op2 = knn_best.predict(tstd.drop(columns=['PassengerId'],axis=1))\nopf_df2 = pd.DataFrame({'PassengerId': tstd.PassengerId, 'Survived': op2})\nopf_df2.to_csv('Balaka_KNN.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier()\n\n# Hyperparameter tuning\nparam_grid = {\n    'n_estimators' : [80,90,100],\n    'criterion' : ['gini','entropy'],\n    'max_depth' : [5,6,7,9],\n    'max_features' : ['auto','sqrt','log2']\n}\nrf_gs = GridSearchCV(estimator=rf,param_grid=param_grid,cv=10)\nrf_gs.fit(trainx,trainy)\nprint(rf_gs.best_score_)\nprint(rf_gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the rf classifier\nrf_best = RandomForestClassifier(n_estimators=rf_gs.best_params_.get('n_estimators'),criterion=rf_gs.best_params_.get('criterion'),max_depth=rf_gs.best_params_.get('max_depth'),max_features=rf_gs.best_params_.get('max_features'))\nrf_best.fit(trainx,trainy)\n\n# ROC-AUC score\ny_pred5 = rf_best.predict_proba(testx)\n# We are interested in the True and False Positives only.\nfptp3 = y_pred5[:,1]  # As 2nd value tells the probability of getting a 1\n\n# Getting the ROC-AUC score and plotting the ROC curve\nrf_score = roc_auc_score(testy,fptp3)\nprint(\"ROC AUC score = \",rf_score)\nlr_fp3,lr_tp3,_ = roc_curve(testy,fptp3)   # Returns FPR, TPR and thresholds.\nplt.figure(figsize=(20,10))\nplt.plot(lr_fp3,lr_tp3,marker='.',label=\"RF ROC Curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix and accuracy score.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Creating O/P file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"op3 = rf_best.predict(tstd.drop(columns=['PassengerId'],axis=1))\nopf_df3 = pd.DataFrame({'PassengerId': tstd.PassengerId, 'Survived': op3})\nopf_df3.to_csv('Balaka_RF2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## AdaBoostClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"adb = AdaBoostClassifier()\n\n# Hyperparameter tuning\nparam_grid = {\n    'n_estimators' : [20,30,40,50],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\nadb_gs = GridSearchCV(estimator=adb,param_grid=param_grid,cv=10)\nadb_gs.fit(trainx,trainy)\nprint(adb_gs.best_score_)\nprint(adb_gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining the adb classifier\nadb_best = AdaBoostClassifier(n_estimators=adb_gs.best_params_.get('n_estimators'),algorithm=adb_gs.best_params_.get('algorithm'))\nadb_best.fit(trainx,trainy)\n\n# ROC-AUC score\ny_pred7 = adb_best.predict_proba(testx)\n# We are interested in the True and False Positives only.\nfptp4 = y_pred7[:,1]  # As 2nd value tells the probability of getting a 1\n\n# Getting the ROC-AUC score and plotting the ROC curve\nadb_score = roc_auc_score(testy,fptp4)\nprint(\"ROC AUC score = \",adb_score)\nlr_fp4,lr_tp4,_ = roc_curve(testy,fptp4)   # Returns FPR, TPR and thresholds.\nplt.figure(figsize=(20,10))\nplt.plot(lr_fp4,lr_tp4,marker='.',label=\"AdaBoost ROC Curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Confusion matrix and accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred8 = adb_best.predict(testx)\ntn,fp,fn,tp = confusion_matrix(testy,y_pred8).ravel()\nacc4 = (tp+tn)/(tp+tn+fp+fn)\nprint(acc4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating the O/P file","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"op4 = adb_best.predict(tstd.drop(columns=['PassengerId'],axis=1))\nopf_df4 = pd.DataFrame({'PassengerId': tstd.PassengerId, 'Survived': op4})\nopf_df4.to_csv('Balaka_ADBoost.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparison","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"names = ['Logistic Regression','KNN','Random Forest','Adaboost']\nvals = [acc1,knn_gs.best_score_,rf_gs.best_score_,adb_gs.best_score_]\nres_df = pd.DataFrame({'Algorithm': names,'Accuracy': vals})\nres_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## XGBoost","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nparam_grid = { \n    'learning_rate' : [0.1, 0.2],\n    'max_depth': [3, 5, 7],   \n}\n\nxgb_gs = GridSearchCV(estimator = xgb,param_grid=param_grid,cv=3)\nxgb_gs.fit(trainx,trainy)\nprint(xgb_gs.best_score_)\nprint(xgb_gs.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_best = XGBClassifier(learning_rate=xgb_gs.best_params_.get('learning_rate'),max_depth=xgb_gs.best_params_.get('max_depth'))\nxgb_best.fit(trainx,trainy)\n\n# ROC-AUC score\ny_pred11 = xgb_best.predict_proba(testx)\n# We are interested in the True and False Positives only.\nfptp6 = y_pred11[:,1]  # As 2nd value tells the probability of getting a 1\n\n# Getting the ROC-AUC score and plotting the ROC curve\nxgb_score = roc_auc_score(testy,fptp6)\nprint(\"ROC AUC score = \",xgb_score)\nlr_fp6,lr_tp6,_ = roc_curve(testy,fptp6)   # Returns FPR, TPR and thresholds.\nplt.figure(figsize=(20,10))\nplt.plot(lr_fp6,lr_tp6,marker='.',label=\"XGBoost ROC Curve\")\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred9 = xgb_best.predict(testx)\ntn,fp,fn,tp = confusion_matrix(testy,y_pred9).ravel()\nacc5 = (tp+tn)/(tp+tn+fp+fn)\nprint(acc5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"op5 = xgb_best.predict(tstd.drop(columns=['PassengerId'],axis=1))\nopf_df5 = pd.DataFrame({'PassengerId': tstd.PassengerId, 'Survived': op5})\nopf_df5.to_csv('Balaka_XGBoost2.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}