{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Titanic\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a2/Titanic_lifeboat.jpg\" width=\"500\">"},{"metadata":{},"cell_type":"markdown","source":"# 1. Load Data & Check Information"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/titanic/train.csv')\ndf_test = pd.read_csv('../input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=\"3\">\nAs you can see, data is divided into two groups, which are train and test sets. <br /> Train set contains 891 personal information, and test set contains 417 personal information based on PassengerId. Each person has different information."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.tail()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test.tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=3>\nBased on the describe, only 38% of the people in the titanic were survived. <br />\nPclass(=Ticket class) is divided into three classes which are 1st, 2nd, 3rd class.<br />\nOldest person in the titanic was 80 years old and yougest person was less than one year. <br />\nMore than 50% of people did not come with any siblings or spouses<br />\nAlso, more than 75% of people came along to the titanic <br />\nLastly, highest price for ticket was \\$512.3 and lowest price was $0!"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Data Overview"},{"metadata":{},"cell_type":"markdown","source":"<font size=3>\n1. Passenger Class = More than a half people in 1st class survived! Unfortunately, lots of 2nd and 3rd class people did not make it... <br />\n2. Female or Male = Interestingly, the ratio of female has 3 times higher than the ration of male <br />\n3. Port of Embarkation = People who were embarked from C(=Cherbourg) was highest, Q(=Queenstown) was second, and S(=Southampton) was third."},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nplt.style.use('seaborn')\n\nfig1, ax1 = plt.subplots(nrows=1, ncols=3,figsize=(10,5))\ndf_train.groupby('Pclass')['Survived'].mean().plot.bar(ax=ax1[0],rot=0, title='Passenger Class',edgecolor=\"k\", xlabel='')\ndf_train.groupby('Sex')['Survived'].mean().plot.bar(ax=ax1[1],rot=0, title = 'Female or Male',edgecolor=\"k\", xlabel='')\ndf_train.groupby('Embarked')['Survived'].mean().plot.bar(ax=ax1[2],rot=0, title = 'Port of Embarkation',edgecolor=\"k\",xlabel='')\n\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=3>\n1. Age = Under 20 years old has highest ratio and Under 80 years old has lowest ratio. <br />\n2. # of siblings / spouses = Less siblings or spouses has more chance to survive <br />\n3. # of parents / children = This result is kind of tricky for me. 3 parents of children has highest percentage of survive. <br />\n4. Passenger Fare = This result has very close relation with Passenger Class"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Age\nage_bins = np.arange(0, 100, 20, dtype='int')\nage_labels = [f'Under {i}' for i in age_bins[1:]]\nage_group = pd.cut(df_train['Age'], bins=age_bins, labels=age_labels, right=False).rename(None)\n\n#Fare\nfare_bins = np.arange(0, 601, 200, dtype='int')\nfare_labels = [f'Under {i}' for i in fare_bins[1:]]\nfare_group = pd.cut(df_train['Fare'], bins=fare_bins, labels=fare_labels, right=False).rename(None)\n\nfig2, ax2 = plt.subplots(nrows=2, ncols=2,figsize=(15,10))\ndf_train.groupby(age_group)['Survived'].mean().plot.bar(ax=ax2[0][0], rot=0, title='Age',edgecolor=\"k\")\ndf_train.groupby('SibSp')['Survived'].mean().plot.bar(ax=ax2[0][1],rot=0, title = '# of siblings / spouses', edgecolor=\"k\",xlabel='')\ndf_train.groupby('Parch')['Survived'].mean().plot.bar(ax=ax2[1][0],rot=0, title = '# of parents / children',edgecolor=\"k\",xlabel='')\ndf_train.groupby(fare_group)['Survived'].mean().plot.bar(ax=ax2[1][1], rot=0, title='Passenger Fare',edgecolor=\"k\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Data Engineering"},{"metadata":{},"cell_type":"markdown","source":"<font size=3>\nBased on the Data Dictionary in 'Titanic - Machine Learning from Disaster', Categorical features are pclass, Sex, and embarked. In addition, Numerical features are Age, sibsp, parch, ticket, fare, and cabin. <br />\nCategorical and Numerical features will be divided separately. <br />\nAfter splitting each other, all the data engineering will be happened to apply at the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=3>\nSince scikit-learn cannot use dataframe directly, I made new class, DataFrameSelector, which can take data as a dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n  def __init__(self, attribute_names):\n    self.attribute_names = attribute_names\n  def fit(self, X, y=None):\n    return self\n  def transform(self, X):\n    return X[self.attribute_names].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorical Features** <br />\nChanging Age's null data to median or mean can lead wrong solution to model, so I deleted null data. <br />\nUsing OneHotEncoder can apply categorical features to machine learning model.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_drop = df_train.dropna(subset=['Age', 'Embarked']).drop('Cabin', axis=1)\ndf_test_drop = (df_test.dropna(subset=['Age', 'Embarked'])).drop('Cabin', axis=1)\nX_train = df_train_drop.drop('Survived', axis=1)\ny_train = df_train_drop['Survived']\nX_test = df_test_drop.copy()\n\n\ncat_attribute = ['Pclass', 'Sex', 'Embarked']\ncat_pipeline = Pipeline([\n    ('selector', DataFrameSelector(cat_attribute)),\n    ('cat_encoder', OneHotEncoder(categories='auto', sparse=False))\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Numerical Features** <br />\nUsing StandardScaler can increase accuracy of machine learning model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_attribute = ['Age', 'SibSp', 'Parch', 'Fare']\nnum_pipeline = Pipeline([\n    ('selector', DataFrameSelector(num_attribute)),\n    ('Imputer', SimpleImputer()),\n    ('num_scale', StandardScaler())\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ColumnTransformer** <br />\nAfter handling Categorical and Numerical features, both are going to combined together to apply into machine learning model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\n\nfull_pipeline = ColumnTransformer([\n    ('num_pipeline', num_pipeline, num_attribute),\n    ('cat_pipeline', cat_pipeline, cat_attribute)\n])\n\ntrain_prepared = full_pipeline.fit_transform(X_train)\ntrain_prepared.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Model Selection & GridSearch"},{"metadata":{},"cell_type":"markdown","source":"<font size=3>\nI am going to use Logistic Regression and SVM. By using gridsearch, each model will be tuned. After finishing tuning, we will check which model has higher accuracy."},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nimport math\n\nlog_reg = LogisticRegression(random_state=42, n_jobs=-1)\nlog_params = {'tol':[1e-4, 1e-2, 1e-1], 'C':[1,3,5]}\nlog_grid = GridSearchCV(log_reg, log_params, cv=3)\nlog_grid.fit(train_prepared, y_train)\nlog_predict = log_grid.predict(train_prepared)\nlog_graph = log_grid.predict_proba(train_prepared)[:,1]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SVM**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC(random_state=42)\nsvc_params = {'kernel':('poly', 'rbf'), 'C':[1,3,5], 'tol':[1e-4, 1e-2, 1e-1]}\nsvc_grid = GridSearchCV(svc, svc_params, cv=3)\nsvc_grid.fit(train_prepared, y_train)\nsvc_predict = svc_grid.predict(train_prepared)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font size=3>\nAs you can see, SVC is almost 4% more accurate than Logistic Regression. <br />\nTherefore, I am going to use SVC to test data sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nlog_score = str(round(accuracy_score(df_train_drop['Survived'], log_predict) * 100, 2))\nsvc_score = str(round(accuracy_score(df_train_drop['Survived'], svc_predict) * 100, 2))\n\nprint('The accuracy score for Logistic Regression is ' + log_score + '%')\nprint('The accuracy score for SVC is ' + svc_score + '%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Submission"},{"metadata":{},"cell_type":"markdown","source":"<font size=3>\nNow, it's time to submit our machine model!"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_prepared = full_pipeline.fit_transform(df_test)\nY_pred = svc_grid.predict(test_prepared)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}