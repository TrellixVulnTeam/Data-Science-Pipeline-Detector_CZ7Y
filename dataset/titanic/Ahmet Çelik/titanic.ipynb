{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Titanic - Machine Learning from Disaster\nHello everyone,\n\nI am new to meachine learning and would like to try Titanic problem. So I'm welcome to any comment and feedback.\n\nMy plan is to first understand the data with some visualization, then process the data for modeling and finally creating a ML model for prediction.\n\n**Best Score: 0.79186 - Top %7**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:38:45.009452Z","iopub.execute_input":"2022-02-17T08:38:45.00978Z","iopub.status.idle":"2022-02-17T08:38:46.034235Z","shell.execute_reply.started":"2022-02-17T08:38:45.009696Z","shell.execute_reply":"2022-02-17T08:38:46.033475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:38:46.035704Z","iopub.execute_input":"2022-02-17T08:38:46.03594Z","iopub.status.idle":"2022-02-17T08:38:46.083776Z","shell.execute_reply.started":"2022-02-17T08:38:46.035912Z","shell.execute_reply":"2022-02-17T08:38:46.083206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 0. Reference Score","metadata":{}},{"cell_type":"markdown","source":"[Alexi Cook's Titanic Tutorial notebook](https://www.kaggle.com/alexisbcook/titanic-tutorial) is a great tutorial for how to use Kaggle, approach Titanic problem and create a basic ML model and make a prediction. Thanks for the tutorial!\n\nFirst, I would like to use the same code from tutorial and make a prediction. So that, i can see how Random Forest model performs and use that score as a benchmark.","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-02-11T12:46:15.405713Z","iopub.execute_input":"2022-02-11T12:46:15.405918Z","iopub.status.idle":"2022-02-11T12:46:15.970657Z","shell.execute_reply.started":"2022-02-11T12:46:15.405895Z","shell.execute_reply":"2022-02-11T12:46:15.969532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After submitting this prediction, **score** is **0.77511** (v1 of this notebook)","metadata":{}},{"cell_type":"markdown","source":"# 1. Understanding the Data","metadata":{}},{"cell_type":"code","source":"df_train.info()\nprint(\"\\n\" + \"-\"*50 + \"\\n\")\n\nprint(\"# of Missing values in train data:\")\nn_miss_val = df_train.isnull().sum().sort_values(ascending=False)\nprint(n_miss_val[n_miss_val>0])\nprint(\"# of Missing values in test data:\")\nn_miss_val_test = df_test.isnull().sum().sort_values(ascending=False)\nprint(n_miss_val_test[n_miss_val_test>0])\nprint(\"\\n\" + \"-\"*50 + \"\\n\")\n\nprint(\"Distribution of Survived column:\")\ndf_train[\"Survived\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:38:57.74456Z","iopub.execute_input":"2022-02-17T08:38:57.74549Z","iopub.status.idle":"2022-02-17T08:38:57.784547Z","shell.execute_reply.started":"2022-02-17T08:38:57.745405Z","shell.execute_reply":"2022-02-17T08:38:57.783487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So, we have 891 values in Survived column. Unfortunately, we see from the distribution that most of the people did not survive.\n\nFrom intuition, i fell like Class and Sex columns have strong relation with surviving. Lets start with them:","metadata":{}},{"cell_type":"markdown","source":"**a. Pclass and Sex**","metadata":{}},{"cell_type":"code","source":"df_train[[\"Pclass\", \"Survived\"]].groupby([\"Pclass\"], as_index=False).mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:38:59.006932Z","iopub.execute_input":"2022-02-17T08:38:59.00775Z","iopub.status.idle":"2022-02-17T08:38:59.024274Z","shell.execute_reply.started":"2022-02-17T08:38:59.007704Z","shell.execute_reply":"2022-02-17T08:38:59.023254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index=False).mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:38:59.411849Z","iopub.execute_input":"2022-02-17T08:38:59.412127Z","iopub.status.idle":"2022-02-17T08:38:59.426357Z","shell.execute_reply.started":"2022-02-17T08:38:59.412101Z","shell.execute_reply":"2022-02-17T08:38:59.425588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pclass and Sex columns have a strong relationship with surviving. I will use these columns as it is.","metadata":{}},{"cell_type":"markdown","source":"**b. SibSp and Parch**\n","metadata":{}},{"cell_type":"code","source":"df_train[[\"SibSp\", \"Survived\"]].groupby(\"SibSp\").agg([\"count\",\"mean\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:00.525318Z","iopub.execute_input":"2022-02-17T08:39:00.525761Z","iopub.status.idle":"2022-02-17T08:39:00.54828Z","shell.execute_reply.started":"2022-02-17T08:39:00.525717Z","shell.execute_reply":"2022-02-17T08:39:00.547483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[[\"Parch\", \"Survived\"]].groupby(\"Parch\").agg([\"count\",\"mean\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:00.880905Z","iopub.execute_input":"2022-02-17T08:39:00.881199Z","iopub.status.idle":"2022-02-17T08:39:00.897346Z","shell.execute_reply.started":"2022-02-17T08:39:00.881171Z","shell.execute_reply":"2022-02-17T08:39:00.896546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* SipSp is the number of siblings + spouses\n* Parch is the number of parents + children\n\n\n* People with 0 SipSp / Parch have about 0.34 survival chance\n* People with a few family members seems to have more survival chance\n* But when people have more family members, survival chance suddenly drops\n\n\n* To test my theory, i will define a new column for family size 'FamSize'\n* If my theory is correct, i will define a new categorical column for family type\n* Categories will be: Alone, Small Family, Large Family","metadata":{}},{"cell_type":"code","source":"df_train['FamSize'] = df_train['SibSp'] + df_train['Parch']\ndf_train[['FamSize', 'Survived']].groupby('FamSize').agg(['count','mean'])","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:01.645971Z","iopub.execute_input":"2022-02-17T08:39:01.646585Z","iopub.status.idle":"2022-02-17T08:39:01.665732Z","shell.execute_reply.started":"2022-02-17T08:39:01.646539Z","shell.execute_reply":"2022-02-17T08:39:01.66483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['FamType'] = np.where(df_train['FamSize'] == 0, \"Alone\", \"Small_Family\")\ndf_train.loc[df_train['FamSize'] > 3, 'FamType'] = \"Large_Family\"\n\ndf_train[['FamType', 'Survived']].groupby('FamType').agg(['count','mean'])","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:02.04722Z","iopub.execute_input":"2022-02-17T08:39:02.047942Z","iopub.status.idle":"2022-02-17T08:39:02.069165Z","shell.execute_reply.started":"2022-02-17T08:39:02.047904Z","shell.execute_reply":"2022-02-17T08:39:02.068518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same operations in test data\ndf_test['FamSize'] = df_test['SibSp'] + df_test['Parch']\ndf_test['FamType'] = np.where(df_test['FamSize'] == 0, \"Alone\", \"Small_Family\")\ndf_test.loc[df_test['FamSize'] > 3, 'FamType'] = \"Large_Family\"","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:02.314495Z","iopub.execute_input":"2022-02-17T08:39:02.315545Z","iopub.status.idle":"2022-02-17T08:39:02.324179Z","shell.execute_reply.started":"2022-02-17T08:39:02.3155Z","shell.execute_reply":"2022-02-17T08:39:02.32344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a result, I am happy with my new column Family Type and use it in my model.","metadata":{}},{"cell_type":"markdown","source":"**c. Name**","metadata":{}},{"cell_type":"markdown","source":"\"Name\" variable seem to be formed of \"LastName, Title. FirstName\". We can extract these three information from this column. I don't think FirstName is a valuable information for us. I am also not sure about about LastName, we already defined a column for families. So for now i will extract Title and see if it is useful.","metadata":{}},{"cell_type":"code","source":"# extracting the string between \",\" and \".\"\ndf_train[\"Title\"] = df_train[\"Name\"].apply(lambda x :x.split(\",\")[-1]).str.split(\".\",expand=True).loc[:,0]\ndf_train[\"Title\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:03.716304Z","iopub.execute_input":"2022-02-17T08:39:03.716747Z","iopub.status.idle":"2022-02-17T08:39:03.733132Z","shell.execute_reply.started":"2022-02-17T08:39:03.716714Z","shell.execute_reply":"2022-02-17T08:39:03.732141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mr, Miss, Mrs and Master titles are looking good\n# But other titles have really low occurence\n# I will replace them with \"Other\"\ntitle_counts = df_train[\"Title\"].value_counts()\ntitles_to_keep = title_counts[title_counts>10]\ntitles_to_replace = list( set(title_counts.index) - set(titles_to_keep.index) )\ndf_train[\"TitleProcessed\"] = df_train[\"Title\"].replace(titles_to_replace, \"Other\")\n\ndf_train[[\"TitleProcessed\", \"Survived\"]].groupby(\"TitleProcessed\").agg([\"count\",\"mean\"])","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:04.120224Z","iopub.execute_input":"2022-02-17T08:39:04.120513Z","iopub.status.idle":"2022-02-17T08:39:04.143492Z","shell.execute_reply.started":"2022-02-17T08:39:04.120485Z","shell.execute_reply":"2022-02-17T08:39:04.142542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same operations in test data\ndf_test[\"Title\"] = df_test[\"Name\"].apply(lambda x :x.split(\",\")[-1]).str.split(\".\",expand=True).loc[:,0]\n\ntitle_counts_test = df_test[\"Title\"].value_counts() # we will keep same columns with train\ntitles_to_replace_test = list( set(title_counts_test.index) - set(titles_to_keep.index) )\ndf_test[\"TitleProcessed\"] = df_test[\"Title\"].replace(titles_to_replace_test, \"Other\")\ndf_test[\"TitleProcessed\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:04.659919Z","iopub.execute_input":"2022-02-17T08:39:04.660205Z","iopub.status.idle":"2022-02-17T08:39:04.680782Z","shell.execute_reply.started":"2022-02-17T08:39:04.660174Z","shell.execute_reply":"2022-02-17T08:39:04.680053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**d. Age**","metadata":{}},{"cell_type":"markdown","source":"Age column in the train dataset has 177 missing values. There are severals ways to fill these values. For example filling with mean/median, using a placeholder value or even generating random number from a gaussian distribution using column's mean and std.\n\nIn Pedro Marcelino's (pmarcelino) [Data analysis and feature extraction with Python\n](https://www.kaggle.com/pmarcelino/data-analysis-and-feature-extraction-with-python) notebook, I've seen a really interesting approach. It is mentioned that these values can be estimated based on known relationships. Person's title is relevant to the age, so mean value of each title's age is used to fill missing values.\n\nInspired from this approach, I will calculate average ages for my TitleProcessed's categories and use them for filling.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(2, 1, figsize=(12, 7))\nfig.suptitle('Distribution of Age - Before Filling Missing Values')\nsns.kdeplot(ax=axes[0], data=df_train, x=\"Age\", hue=\"Survived\", shade=True)\nsns.barplot(ax=axes[1], data=df_train, x=\"TitleProcessed\", y=\"Age\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:06.041195Z","iopub.execute_input":"2022-02-17T08:39:06.041494Z","iopub.status.idle":"2022-02-17T08:39:06.614623Z","shell.execute_reply.started":"2022-02-17T08:39:06.041463Z","shell.execute_reply":"2022-02-17T08:39:06.613767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a dictionary for the average age value of each Title\ntitles_avg_age = df_train.groupby('TitleProcessed')['Age'].mean().round(2).to_dict()\ntitles_avg_age","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:06.616317Z","iopub.execute_input":"2022-02-17T08:39:06.617174Z","iopub.status.idle":"2022-02-17T08:39:06.626595Z","shell.execute_reply.started":"2022-02-17T08:39:06.61713Z","shell.execute_reply":"2022-02-17T08:39:06.625807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling missing values\ndf_train['Age'].fillna(df_train['TitleProcessed'].map(titles_avg_age), inplace=True)\ndf_test['Age'].fillna(df_test['TitleProcessed'].map(titles_avg_age), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:07.451074Z","iopub.execute_input":"2022-02-17T08:39:07.451343Z","iopub.status.idle":"2022-02-17T08:39:07.45918Z","shell.execute_reply.started":"2022-02-17T08:39:07.451316Z","shell.execute_reply":"2022-02-17T08:39:07.458516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 1, figsize=(12, 4))\nfig.suptitle('Distribution of Age - After Filling Missing Values')\nsns.kdeplot(data=df_train, x=\"Age\", hue=\"Survived\", shade=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:09.266574Z","iopub.execute_input":"2022-02-17T08:39:09.266994Z","iopub.status.idle":"2022-02-17T08:39:09.487004Z","shell.execute_reply.started":"2022-02-17T08:39:09.266963Z","shell.execute_reply":"2022-02-17T08:39:09.486367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**e. Embarked**","metadata":{}},{"cell_type":"code","source":"df_train[[\"Embarked\", \"Survived\"]].groupby([\"Embarked\"]).agg(['count','mean'])","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:11.900549Z","iopub.execute_input":"2022-02-17T08:39:11.901242Z","iopub.status.idle":"2022-02-17T08:39:11.91607Z","shell.execute_reply.started":"2022-02-17T08:39:11.901209Z","shell.execute_reply":"2022-02-17T08:39:11.915268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"Embarked\", y=\"Survived\", data=df_train, kind = \"bar\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:12.308803Z","iopub.execute_input":"2022-02-17T08:39:12.309322Z","iopub.status.idle":"2022-02-17T08:39:12.67953Z","shell.execute_reply.started":"2022-02-17T08:39:12.30929Z","shell.execute_reply":"2022-02-17T08:39:12.678625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In Embarked column, we only have 2 missing values. \n# So I will fill them with the most occurent value S\ndf_train[\"Embarked\"].fillna(\"S\", inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:12.881469Z","iopub.execute_input":"2022-02-17T08:39:12.881787Z","iopub.status.idle":"2022-02-17T08:39:12.886957Z","shell.execute_reply.started":"2022-02-17T08:39:12.881745Z","shell.execute_reply":"2022-02-17T08:39:12.886086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**f. Fare**","metadata":{}},{"cell_type":"code","source":"df_train[\"Fare\"].describe()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:17.316831Z","iopub.execute_input":"2022-02-17T08:39:17.317374Z","iopub.status.idle":"2022-02-17T08:39:17.326336Z","shell.execute_reply.started":"2022-02-17T08:39:17.317338Z","shell.execute_reply":"2022-02-17T08:39:17.325522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(15, 4))\nsns.kdeplot(ax=axes[0], data=df_train, x=\"Fare\", hue=\"Survived\")\nsns.violinplot(ax=axes[1], data=df_train, x=\"Survived\", y=\"Fare\")\nsns.violinplot(ax=axes[2], data=df_train, x=\"Pclass\", y=\"Fare\", hue=\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:18.293826Z","iopub.execute_input":"2022-02-17T08:39:18.294325Z","iopub.status.idle":"2022-02-17T08:39:19.013155Z","shell.execute_reply.started":"2022-02-17T08:39:18.294273Z","shell.execute_reply":"2022-02-17T08:39:19.012184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As the fare increases, chance of survival also seems to increase.We can also see from the third plot that there is a relation between fare and class.\n\nIn PClass 1, there are really expensive tickets which we can consider as outlier. Fare is a positively skewed variable, so i also want to see plots after log transform. Since the minimum value of the fare is 0, we have to make it bigger than 1 to apply logarithm. From intuition, first I will add the median of the fare and later take log transform.","metadata":{}},{"cell_type":"code","source":"df_train[\"FareLog\"] = np.log( df_train[\"Fare\"] + df_train[\"Fare\"].median() )\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nsns.kdeplot(ax=axes[0], data=df_train, x=\"FareLog\", hue=\"Survived\")\nsns.violinplot(ax=axes[1], data=df_train, x=\"Survived\", y=\"FareLog\")\nsns.violinplot(ax=axes[2], data=df_train, x=\"Pclass\", y=\"FareLog\", hue=\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:51.804098Z","iopub.execute_input":"2022-02-17T08:39:51.804388Z","iopub.status.idle":"2022-02-17T08:39:52.44108Z","shell.execute_reply.started":"2022-02-17T08:39:51.804359Z","shell.execute_reply":"2022-02-17T08:39:52.440188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We obtained an interesting variable. I am going to try out this variable for the model.","metadata":{"execution":{"iopub.status.busy":"2022-02-09T15:45:42.001679Z","iopub.execute_input":"2022-02-09T15:45:42.001963Z","iopub.status.idle":"2022-02-09T15:45:42.012053Z","shell.execute_reply.started":"2022-02-09T15:45:42.001934Z","shell.execute_reply":"2022-02-09T15:45:42.011435Z"}}},{"cell_type":"code","source":"# in the test data, we have 1 missing Fare. I will basicly fill it with median\n# after that, apply the same log operation\ndf_test[\"Fare\"].fillna(df_test[\"Fare\"].median(), inplace=True)\ndf_test[\"FareLog\"] = np.log( df_test[\"Fare\"] + df_test[\"Fare\"].median() )","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:39:58.083795Z","iopub.execute_input":"2022-02-17T08:39:58.0841Z","iopub.status.idle":"2022-02-17T08:39:58.091316Z","shell.execute_reply.started":"2022-02-17T08:39:58.084068Z","shell.execute_reply":"2022-02-17T08:39:58.090491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**g. Ticket and Cabin**","metadata":{}},{"cell_type":"code","source":"print(\"Random 10 Tickets:\")\nprint( df_train[\"Ticket\"].sample(10, random_state=42) )\nprint()\n\nprint(\"Checking max repetition:\")\nticket_counts = df_train[\"Ticket\"].value_counts()\nprint(ticket_counts.head())\nprint()\n\nprint(\"# of unique tickets: {}\".format(df_train[\"Ticket\"].nunique()))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:01.045854Z","iopub.execute_input":"2022-02-17T08:40:01.046299Z","iopub.status.idle":"2022-02-17T08:40:01.057686Z","shell.execute_reply.started":"2022-02-17T08:40:01.046268Z","shell.execute_reply":"2022-02-17T08:40:01.057009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Random 10 Cabins:\")\nprint( df_train[\"Cabin\"].sample(10, random_state=42) )\nprint()\n\nprint(\"Checking max repetition:\")\ncabin_counts = df_train[\"Cabin\"].value_counts()\nprint(cabin_counts.head())\nprint()\n\nprint(\"# of unique cabins: {}\".format(df_train[\"Cabin\"].nunique()))\nprint(\"# of missing values: {}\".format(df_train[\"Cabin\"].isnull().sum()))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:02.533611Z","iopub.execute_input":"2022-02-17T08:40:02.534127Z","iopub.status.idle":"2022-02-17T08:40:02.545723Z","shell.execute_reply.started":"2022-02-17T08:40:02.534085Z","shell.execute_reply":"2022-02-17T08:40:02.544677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ticket:\n* I'm not sure if ticket has a relationship with surviving\n* Some values are numerical, some of them are not\n* Most of the tickets are unique\n* There are some repeated tickets, maybe they are family members\n\nCabin:\n* If there were more values, maybe we could process this column and find some information\n* But most of the values are missing\n\nFor now, I will not use these Ticket and Cabin columns in my model.","metadata":{}},{"cell_type":"markdown","source":"# 2. Preprocess Data","metadata":{}},{"cell_type":"code","source":"# In this section:\n# Selecting the columns that i will use in model\n# Applying get_dummies for categorical columns\n# Dropping one column for each categorical column\n#    because the other columns contain that information\n# Finally, feature scaling\n\nX_train = df_train[[\"Pclass\",\"TitleProcessed\",\"Sex\",\"Age\",\"FamType\",\"FareLog\",\"Embarked\"]]\nX_train = pd.get_dummies(X_train)\nX_train = X_train.drop([\"TitleProcessed_Other\",\"Sex_female\",\"FamType_Large_Family\",\"Embarked_Q\"], axis=1)\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:05.126844Z","iopub.execute_input":"2022-02-17T08:40:05.127344Z","iopub.status.idle":"2022-02-17T08:40:05.152864Z","shell.execute_reply.started":"2022-02-17T08:40:05.127296Z","shell.execute_reply":"2022-02-17T08:40:05.151985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same operation to test data\nX_test = df_test[[\"Pclass\",\"TitleProcessed\",\"Sex\",\"Age\",\"FamType\",\"FareLog\",\"Embarked\"]]\nX_test = pd.get_dummies(X_test)\nX_test = X_test.drop([\"TitleProcessed_Other\",\"Sex_female\",\"FamType_Large_Family\",\"Embarked_Q\"], axis=1)\nX_test.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:07.002986Z","iopub.execute_input":"2022-02-17T08:40:07.003309Z","iopub.status.idle":"2022-02-17T08:40:07.029935Z","shell.execute_reply.started":"2022-02-17T08:40:07.003275Z","shell.execute_reply":"2022-02-17T08:40:07.029009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = df_train[\"Survived\"]\ny_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:07.566378Z","iopub.execute_input":"2022-02-17T08:40:07.567142Z","iopub.status.idle":"2022-02-17T08:40:07.57387Z","shell.execute_reply.started":"2022-02-17T08:40:07.56709Z","shell.execute_reply":"2022-02-17T08:40:07.57299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_scaled = sc.fit_transform(X_train)\nX_test_scaled = sc.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:08.074328Z","iopub.execute_input":"2022-02-17T08:40:08.074654Z","iopub.status.idle":"2022-02-17T08:40:08.21819Z","shell.execute_reply.started":"2022-02-17T08:40:08.074604Z","shell.execute_reply":"2022-02-17T08:40:08.217483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model Selection","metadata":{}},{"cell_type":"markdown","source":"In this section, I will try different classifiers and compare cross validation score. I will use both scaled and unscaled data to compare.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:17.569958Z","iopub.execute_input":"2022-02-17T08:40:17.570731Z","iopub.status.idle":"2022-02-17T08:40:17.965234Z","shell.execute_reply.started":"2022-02-17T08:40:17.570685Z","shell.execute_reply":"2022-02-17T08:40:17.964395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LogisticRegression\nclassifier = LogisticRegression(solver='liblinear')\n\nscore_unscaled = cross_val_score(classifier, X_train, y_train, cv=10).mean().round(3)\nscore_scaled = cross_val_score(classifier, X_train_scaled, y_train, cv=10).mean().round(3)\nprint(\"Logistic Regression\")\nprint(\"Cross Val Score: {}\".format(score_unscaled))\nprint(\"Cross Val Score for Scaled Data: {}\".format(score_scaled))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:25.045692Z","iopub.execute_input":"2022-02-17T08:40:25.045967Z","iopub.status.idle":"2022-02-17T08:40:25.175249Z","shell.execute_reply.started":"2022-02-17T08:40:25.045939Z","shell.execute_reply":"2022-02-17T08:40:25.174271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# K-Nearest Neighbors\nclassifier = KNeighborsClassifier()\n\nscore_unscaled = cross_val_score(classifier, X_train, y_train, cv=10).mean().round(3)\nscore_scaled = cross_val_score(classifier, X_train_scaled, y_train, cv=10).mean().round(3)\nprint(\"K-Nearest Neighbors\")\nprint(\"Cross Val Score: {}\".format(score_unscaled))\nprint(\"Cross Val Score for Scaled Data: {}\".format(score_scaled))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:33.500638Z","iopub.execute_input":"2022-02-17T08:40:33.501343Z","iopub.status.idle":"2022-02-17T08:40:33.690634Z","shell.execute_reply.started":"2022-02-17T08:40:33.501295Z","shell.execute_reply":"2022-02-17T08:40:33.689678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Support Vector Classifier (SVC)\nclassifier = SVC()\n\nscore_unscaled = cross_val_score(classifier, X_train, y_train, cv=10).mean().round(3)\nscore_scaled = cross_val_score(classifier, X_train_scaled, y_train, cv=10).mean().round(3)\nprint(\"Support Vector Classifier\")\nprint(\"Cross Val Score: {}\".format(score_unscaled))\nprint(\"Cross Val Score for Scaled Data: {}\".format(score_scaled))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:36.57639Z","iopub.execute_input":"2022-02-17T08:40:36.576682Z","iopub.status.idle":"2022-02-17T08:40:37.014381Z","shell.execute_reply.started":"2022-02-17T08:40:36.576654Z","shell.execute_reply":"2022-02-17T08:40:37.013474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Naive Bayes\nclassifier = GaussianNB()\n\nscore_unscaled = cross_val_score(classifier, X_train, y_train, cv=10).mean().round(3)\nscore_scaled = cross_val_score(classifier, X_train_scaled, y_train, cv=10).mean().round(3)\nprint(\"Naive Bayes\")\nprint(\"Cross Val Score: {}\".format(score_unscaled))\nprint(\"Cross Val Score for Scaled Data: {}\".format(score_scaled))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:44.043909Z","iopub.execute_input":"2022-02-17T08:40:44.044219Z","iopub.status.idle":"2022-02-17T08:40:44.129194Z","shell.execute_reply.started":"2022-02-17T08:40:44.044189Z","shell.execute_reply":"2022-02-17T08:40:44.128189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Decision Tree Classification\nclassifier = DecisionTreeClassifier()\n\nscore_unscaled = cross_val_score(classifier, X_train, y_train, cv=10).mean().round(3)\nscore_scaled = cross_val_score(classifier, X_train_scaled, y_train, cv=10).mean().round(3)\nprint(\"Decision Tree Classifier\")\nprint(\"Cross Val Score: {}\".format(score_unscaled))\nprint(\"Cross Val Score for Scaled Data: {}\".format(score_scaled))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:46.349753Z","iopub.execute_input":"2022-02-17T08:40:46.350103Z","iopub.status.idle":"2022-02-17T08:40:46.463564Z","shell.execute_reply.started":"2022-02-17T08:40:46.350065Z","shell.execute_reply":"2022-02-17T08:40:46.462631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest Classification\nclassifier = RandomForestClassifier()\n\nscore_unscaled = cross_val_score(classifier, X_train, y_train, cv=10).mean().round(3)\nscore_scaled = cross_val_score(classifier, X_train_scaled, y_train, cv=10).mean().round(3)\nprint(\"Random Forest Classifier\")\nprint(\"Cross Val Score: {}\".format(score_unscaled))\nprint(\"Cross Val Score for Scaled Data: {}\".format(score_scaled))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:40:47.948937Z","iopub.execute_input":"2022-02-17T08:40:47.949217Z","iopub.status.idle":"2022-02-17T08:40:52.560582Z","shell.execute_reply.started":"2022-02-17T08:40:47.949183Z","shell.execute_reply":"2022-02-17T08:40:52.559653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Modeling and Submission","metadata":{}},{"cell_type":"markdown","source":"In section 3, Logistic Regression and SVC have the best cross validation scores, and later KNN and Random Forest come. In this section I will submit the predictions from these classifiers and compare the submission score. Later, I will try to improve the best classifier with hyperparameter tuning.","metadata":{}},{"cell_type":"code","source":"# SVC\nclassifier = SVC()\nclassifier.fit(X_train_scaled, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = classifier.predict(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2022-02-11T13:00:27.912093Z","iopub.execute_input":"2022-02-11T13:00:27.912393Z","iopub.status.idle":"2022-02-11T13:00:27.927914Z","shell.execute_reply.started":"2022-02-11T13:00:27.912362Z","shell.execute_reply":"2022-02-11T13:00:27.926536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': y_pred})\nresult.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.to_csv('submission.csv', index=False)\nprint(\"Resuls are saved to submission.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So without hyperparameter tuning, i submitted all the predictions and achieved the following scores. SVC has the highest score. Now, lets tune hyperparameters using grid search and submit again.\n* LogReg  : 0.76076 (V4)\n* SVC     : **0.78947** (V5)\n* RandFor : 0.75119 (V6)\n* KNN     : 0.76794 (V7)","metadata":{}},{"cell_type":"code","source":"# Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'C': [0.1, 1, 10, 100, 1000],\n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf', 'sigmoid']\n             }\n\ngrid = GridSearchCV(SVC(), parameters, refit = True)\ngrid.fit(X_train_scaled,y_train)\n\n# printing results\nprint(\"Best Score: {}\".format(grid.best_score_))\nprint(\"Best Parameters: {}\".format(grid.best_params_))\nprint(\"Model: {}\".format(grid.best_estimator_))","metadata":{"execution":{"iopub.status.busy":"2022-02-17T08:44:26.741476Z","iopub.execute_input":"2022-02-17T08:44:26.741787Z","iopub.status.idle":"2022-02-17T08:44:36.232042Z","shell.execute_reply.started":"2022-02-17T08:44:26.741759Z","shell.execute_reply":"2022-02-17T08:44:36.231104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After parameter tuning, i obtained best validation score for the parameters {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}. Even so, i didn't get a better competition score using these parameters.\n\nFor now, I get the best score using SVC with default parameters.\n\nV9 update:\nIn Fare section, I used to add mean value before taking log transform. Now, I tried adding median instead of mean. The distribution of FareLog is now better. With keeping other parameters same, I increased my score from 0.78947 to 0.79186\n\nThank you very much for your interest in my notebook! :)","metadata":{}}]}