{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" <h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Table of Contents</h1>\n\n<ul style=\"list-style-type:none;\">\n        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\"\n                aria-controls=\"settings\">1. Introduction<span class=\"badge badge-primary badge-pill\">1</span></a>\n        </li>\n        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\"\n                aria-controls=\"settings\">2. Handling Missing Values<span\n                    class=\"badge badge-primary badge-pill\">2</span></a>\n            <ul style=\"list-style-type:none;\">\n                <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2.1\" role=\"tab\"\n                        aria-controls=\"settings\">2.1 Visualizing Missing Data <span\n                            class=\"badge badge-primary badge-pill\">3</span></a>\n                    <ul style=\"list-style-type:none;\">\n                        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2.1.1\"\n                                role=\"tab\" aria-controls=\"settings\">2.1.1 Matrix <span\n                                    class=\"badge badge-primary badge-pill\">4</span></a></li>\n                        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2.1.2\"\n                                role=\"tab\" aria-controls=\"settings\">2.1.2 Correlation Heatmap <span\n                                    class=\"badge badge-primary badge-pill\">5</span></a></li>\n                        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2.1.3\"\n                                role=\"tab\" aria-controls=\"settings\">2.1.3 Dendrogram <span\n                                    class=\"badge badge-primary badge-pill\">6</span></a></li>\n                        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2.1.4\"\n                                role=\"tab\" aria-controls=\"settings\">2.1.4 Simple Numeric Summaries<span\n                                    class=\"badge badge-primary badge-pill\">7</span></a></li>\n                    </ul>\n                </li>\n                <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2.2\" role=\"tab\"\n                        aria-controls=\"settings\">2.2 Method to Handle Missing Data<span\n                            class=\"badge badge-primary badge-pill\">8</span></a>\n                    <ul style=\"list-style-type:none;\">\n                        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2.2.1\"\n                                role=\"tab\" aria-controls=\"settings\">2.2.1 Deletion of Data <span\n                                    class=\"badge badge-primary badge-pill\">9</span></a></li>\n                        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2.2.2\"\n                                role=\"tab\" aria-controls=\"settings\">2.2.2 Encoding Missingness<span\n                                    class=\"badge badge-primary badge-pill\">10</span></a></li>\n                        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2.2.3\"\n                                role=\"tab\" aria-controls=\"settings\">2.2.3 Imputation Methods<span\n                                    class=\"badge badge-primary badge-pill\">11</span></a></li>\n                    </ul>\n                </li>\n            </ul>\n        </li>\n        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\"\n                aria-controls=\"settings\">3. Encoding Categorical Attributes<span\n                    class=\"badge badge-primary badge-pill\">12</span></a>\n            <ul style=\"list-style-type:none;\">\n                <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3.1\" role=\"tab\"\n                        aria-controls=\"settings\">3.1 Supervised Encoding Methods <span\n                            class=\"badge badge-primary badge-pill\">13</span></a>\n                    <ul style=\"list-style-type:none;\">\n                        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3.1.1\"\n                                role=\"tab\" aria-controls=\"settings\">3.1.1 Likelihood Encoding <span\n                                    class=\"badge badge-primary badge-pill\">14</span></a></li>\n                        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3.1.2\"\n                                role=\"tab\" aria-controls=\"settings\">3.1.2 Target Encoding <span\n                                    class=\"badge badge-primary badge-pill\">15</span></a></li>\n                        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3.1.3\"\n                                role=\"tab\" aria-controls=\"settings\">3.1.3 Deep Learning Methods <span\n                                    class=\"badge badge-primary badge-pill\">16</span></a></li>\n                    </ul>\n                </li>\n                <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3.2\" role=\"tab\"\n                        aria-controls=\"settings\">3.2 Approaches for Novel Categories<span\n                            class=\"badge badge-primary badge-pill\">17</span></a>\n            </ul>\n        </li>\n        <li><a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\"\n            aria-controls=\"settings\">4. References <span\n                class=\"badge badge-primary badge-pill\">18</span></a> </li>\n\n\n </ul>\n","metadata":{}},{"cell_type":"markdown","source":"\n<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">1. Introduction</h1><a id = \"1\" ></a>\n\n\n\nWe all know the Importance of good features for machine learning models. In Machine learning task we have features which we need to process to make them good and this is done by data preprocessing  tasks.\n\n**Data Preprocessing** : Data preprocessing is a process of preparing the raw data and making it suitable for a machine learning model. It is the first and crucial step while creating a machine learning model. When creating a machine learning project, it is not always a case that we come across the clean and formatted data. And while doing any operation with data, it is mandatory to clean it and put in a formatted way. So for this, we use data preprocessing task.\n\nA real-world data generally contains noises, missing values, and maybe in an unusable format which cannot be directly used for machine learning models. Data preprocessing is required tasks for cleaning the data and making it suitable for a machine learning model which also increases the accuracy and efficiency of a machine learning model.\n\nData Preprocessing involves below steps:\n\n- Getting the dataset\n- Importing libraries\n- Importing datasets\n- Finding Missing Data\n- Encoding Categorical Data\n\nMany of us know traditional approaches for above listed steps but in this notebook I will discuss some different approaches which could be game changer for your next project. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:45.06426Z","iopub.execute_input":"2022-04-14T14:01:45.064581Z","iopub.status.idle":"2022-04-14T14:01:45.069835Z","shell.execute_reply.started":"2022-04-14T14:01:45.064548Z","shell.execute_reply":"2022-04-14T14:01:45.06869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/loan-default-dataset/Loan_Default.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:45.122387Z","iopub.execute_input":"2022-04-14T14:01:45.123303Z","iopub.status.idle":"2022-04-14T14:01:46.272317Z","shell.execute_reply.started":"2022-04-14T14:01:45.123264Z","shell.execute_reply":"2022-04-14T14:01:46.271138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:46.274288Z","iopub.execute_input":"2022-04-14T14:01:46.274925Z","iopub.status.idle":"2022-04-14T14:01:46.583546Z","shell.execute_reply.started":"2022-04-14T14:01:46.274884Z","shell.execute_reply":"2022-04-14T14:01:46.582366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">2. Handling Missing Values</h1><a id = \"2\" ></a>\n\nMissing data are not rare in real data sets. In fact, the chance that at least one data point is missing increases as the data set size increases. Missing data can occur any number of ways, some of which include the following.\n\n- Merging of source data sets\n- Random events\n- Failures of measurement\n\n","metadata":{}},{"cell_type":"markdown","source":"<h2  style=\"text-align: center\" class=\"list-group-item list-group-item-success\"> 2.1 Visualizing Missing Data</h2><a id = \"2.1\" ></a>\n\n\nVisualizations as well as numeric summaries are the first step in understanding the challenge of missing information in a data set. For small to moderate data (100s of samples and 100s of attributes), several techniques are available that allow the visualization of all of the samples and Attributes simultaneously.\n\nIn this notebook I'll Cover Following visualizations for missing values:-\n- Matrix\n- Correlation Heatmap\n- Dendrogram\n- Simple numerical summaries\n\nQuestion may arise that why we need Visualizations?\nBecause it is wise to explore relationships within the attributes that might be related to missingness. ","metadata":{}},{"cell_type":"markdown","source":"<h3  style=\"text-align: center\" class=\"list-group-item list-group-item-warning\"> 2.1.1 Matrix</h3><a id = \"2.1.1\" ></a>\n\nIt is the nullity matrix that allows us to see the distribution of data across all columns in the whole dataset. It also shows a sparkline (or, in some cases, a striped line) that emphasizes rows in a dataset with the highest and lowest nullity.","metadata":{}},{"cell_type":"code","source":"import missingno as msno","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:46.584914Z","iopub.execute_input":"2022-04-14T14:01:46.585193Z","iopub.status.idle":"2022-04-14T14:01:46.804796Z","shell.execute_reply.started":"2022-04-14T14:01:46.585161Z","shell.execute_reply":"2022-04-14T14:01:46.803837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.matrix(df)\nplt.figure(figsize = (15,9))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:46.807282Z","iopub.execute_input":"2022-04-14T14:01:46.807565Z","iopub.status.idle":"2022-04-14T14:01:49.462273Z","shell.execute_reply.started":"2022-04-14T14:01:46.807533Z","shell.execute_reply":"2022-04-14T14:01:49.460791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above plot we can interpret our dataset has lots of missing values in it ","metadata":{}},{"cell_type":"markdown","source":"<h3  style=\"text-align: center\" class=\"list-group-item list-group-item-warning\"> 2.1.2 Correlation Heatmap </h3><a id = \"2.1.2\" ></a>\n\nCorrelation heatmap measures nullity correlation between columns of the dataset. It shows how strongly the presence or absence of one feature affects the other.\n\nNullity correlation ranges from(-1 to 1):\n- -1 means if one column(attribute) is present, the other is almost certainly absent.\n- 0 means there is no dependence between the columns(attributes).\n- 1 means if one column(attributes) is present, the other is also certainly present.\n\nUnlike in a familiar correlation heatmap, if you see here, many columns are missing. Those columns which are always full or always empty have no meaningful correlation and are removed from the visualization.\n\nThe heatmap is helpful for identifying data completeness correlations between attribute pairs, but it has the limited explanatory ability for broader relationships and no special support for really big datasets.","metadata":{}},{"cell_type":"code","source":"msno.heatmap(df, labels = True)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:49.463784Z","iopub.execute_input":"2022-04-14T14:01:49.464572Z","iopub.status.idle":"2022-04-14T14:01:50.874158Z","shell.execute_reply.started":"2022-04-14T14:01:49.464524Z","shell.execute_reply":"2022-04-14T14:01:50.873013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From above visualization we can easily interpret missingness of attribute rate_of_interest and upfront_charges is dependent on each other(correlation value = 1) means if one will be present another will be present. ","metadata":{}},{"cell_type":"markdown","source":"<h3  style=\"text-align: center\" class=\"list-group-item list-group-item-warning\"> 2.1.3 Dendrogram </h3><a id = \"2.1.3\" ></a>\n\n\nThe dendrogram shows the hierarchical nullity relationship between columns. The dendrogram uses a hierarchical clustering algorithm against one another by their nullity correlation.\n\n[More about Hierarchical Clustering Algorithm](http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)","metadata":{}},{"cell_type":"code","source":"# Columns having missing values\nmissing_columns = [col for col in df.columns if df[col].isnull().sum() > 0]\nmissing_columns","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:50.875834Z","iopub.execute_input":"2022-04-14T14:01:50.876252Z","iopub.status.idle":"2022-04-14T14:01:51.181918Z","shell.execute_reply.started":"2022-04-14T14:01:50.876201Z","shell.execute_reply":"2022-04-14T14:01:51.180719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.dendrogram(df[missing_columns])","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:51.183328Z","iopub.execute_input":"2022-04-14T14:01:51.183577Z","iopub.status.idle":"2022-04-14T14:01:51.79926Z","shell.execute_reply.started":"2022-04-14T14:01:51.183546Z","shell.execute_reply":"2022-04-14T14:01:51.798118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We interpret the dendrogram based on a top-down approach, i.e., to focus on the height at which any two columns are joined together with matters of nullity. More will be the height less will be the relation and vice versa is also True. \n\nFor example if we see pair of attributes LTV and property value has height 0 implies they are highly correlated in case of nullity. Similarly attribute LTV and rate_of_interest have maximum height implies they are less correlated with each other.  ","metadata":{}},{"cell_type":"markdown","source":"<h3  style=\"text-align: center\" class=\"list-group-item list-group-item-warning\"> 2.1.4 Simple Numerical Summaries </h3><a id = \"2.1.4\" ></a>\n\n\nMoving Forward lets try to analyse numerical summary of missing attributes. Simple numerical summaries are effective at identifying problematic predictors and samples when the data become too large to visually inspect.","metadata":{}},{"cell_type":"code","source":"def get_numerical_summary(df):\n    total = df.shape[0]\n    missing_columns = [col for col in df.columns if df[col].isnull().sum() > 0]\n    missing_percent = {}\n    for col in missing_columns:\n        null_count = df[col].isnull().sum()\n        per = (null_count/total) * 100\n        missing_percent[col] = per\n        print(\"{} : {} ({}%)\".format(col, null_count, round(per, 3)))\n    return missing_percent","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:51.80098Z","iopub.execute_input":"2022-04-14T14:01:51.80148Z","iopub.status.idle":"2022-04-14T14:01:51.80843Z","shell.execute_reply.started":"2022-04-14T14:01:51.801428Z","shell.execute_reply":"2022-04-14T14:01:51.807518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_percent = get_numerical_summary(df)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:51.809832Z","iopub.execute_input":"2022-04-14T14:01:51.810137Z","iopub.status.idle":"2022-04-14T14:01:52.210607Z","shell.execute_reply.started":"2022-04-14T14:01:51.810103Z","shell.execute_reply":"2022-04-14T14:01:52.209381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I guess visualization part is done lets move forward to methods which we can use to handle missing values.","metadata":{}},{"cell_type":"markdown","source":"\n<h2  style=\"text-align: center\" class=\"list-group-item list-group-item-success\"> 2.2 Methods to Handle Missing Data</h2><a id = \"2.2\" ></a>\n\nAs we Know if our data has missing values than our model will not train except few models which can tolerate them like some tree based models but the point is we want to handle this and how can we handle them. So, in this notebook to handle missing data I will discuss following techniques :-\n\n- Deletion of Data \n- Encoding Missingness\n- Imputation Methods\n","metadata":{}},{"cell_type":"markdown","source":"<h3  style=\"text-align: center\" class=\"list-group-item list-group-item-warning\"> 2.2.1 Deletion of Data </h3><a id = \"2.2.1\" ></a>\n\n\n\nThe simplest approach for dealing with missing values is to remove entire attribute(s) and/or sample(s) that contain missing values. However, one must carefully consider a number of aspects of the data prior to taking this approach. For example, missing values could be eliminated by removing all predictors that contain at least one missing value. Similarly, missing values could be eliminated by removing all samples with any missing values.\n\n**Note: When it is difficult to obtain samples or when the data contain a small number of samples (i.e., rows), then it is not desirable to remove samples from the data.**\n\nConsider this small intuition shown below\n\nLet M = Number of Samples(rows).\\\nand Let N = Number of Attributes(columns).\n\n\nCase 1: Deletion of Attributes\n\nIf N has range of [1-10]\\\nThen don't delete the attribute that contain missing values but if that attribute has missing values around 80-90% then deletion of that attribute will be good option instead of just predicting values of those 80-90% data based on that 10-20% data. \n\nCase 2: Deletion of Samples\n\nIf M is a large number according to your task\\\nThen deletion of sample can be a Good step but if that sample has few missing values with respect to attribute, then you should consider methods to fill those missing values.\n\nLets move on to the implementation part, I will just show how to delete data for both cases but you can interpret more according to your tasks.","metadata":{}},{"cell_type":"markdown","source":"**Deletion of an Attribute**\n\nAccording to Simple numerical Summaries the attribute Upfront_charges has largest missing values percentage of (26.664%) which is not ideal percentage to remove a feature but just for sake of implementation I will remove that feature.","metadata":{}},{"cell_type":"code","source":"df_temp = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:52.216157Z","iopub.execute_input":"2022-04-14T14:01:52.216468Z","iopub.status.idle":"2022-04-14T14:01:52.235774Z","shell.execute_reply.started":"2022-04-14T14:01:52.216434Z","shell.execute_reply":"2022-04-14T14:01:52.234938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Threshold to remove attribute having missing values greater than threshold\nATTRIBUTE_THRESHOLD = 25 #25% in this case \n\nfor col, per in missing_percent.items():\n    if per > ATTRIBUTE_THRESHOLD:\n        df_temp.drop(col, axis = 1, inplace = True)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:52.237056Z","iopub.execute_input":"2022-04-14T14:01:52.23741Z","iopub.status.idle":"2022-04-14T14:01:52.281546Z","shell.execute_reply.started":"2022-04-14T14:01:52.237379Z","shell.execute_reply":"2022-04-14T14:01:52.280566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By generating numerical summary of df_temp we can see now attribute Upfont_chargers being removed from the dataset as it has missing values percentage greater than threshold we defined","metadata":{}},{"cell_type":"code","source":"_ = get_numerical_summary(df_temp)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:52.283018Z","iopub.execute_input":"2022-04-14T14:01:52.283425Z","iopub.status.idle":"2022-04-14T14:01:52.67324Z","shell.execute_reply.started":"2022-04-14T14:01:52.283392Z","shell.execute_reply":"2022-04-14T14:01:52.672038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_temp","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:52.674661Z","iopub.execute_input":"2022-04-14T14:01:52.675006Z","iopub.status.idle":"2022-04-14T14:01:52.688152Z","shell.execute_reply.started":"2022-04-14T14:01:52.674971Z","shell.execute_reply":"2022-04-14T14:01:52.68725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Deletion of the Samples**\n\nWe will try to delete those samples having missing values in more than 5 attributes  ","metadata":{}},{"cell_type":"code","source":"df_temp = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:52.690504Z","iopub.execute_input":"2022-04-14T14:01:52.691029Z","iopub.status.idle":"2022-04-14T14:01:52.718771Z","shell.execute_reply.started":"2022-04-14T14:01:52.690973Z","shell.execute_reply":"2022-04-14T14:01:52.717523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting Missing count of each sample            \n\nfor idx in range(df_temp.shape[0]):\n    df_temp.loc[idx, 'missing_count'] = df_temp.iloc[idx, :].isnull().sum()  ","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:01:52.720997Z","iopub.execute_input":"2022-04-14T14:01:52.721378Z","iopub.status.idle":"2022-04-14T14:04:35.441371Z","shell.execute_reply.started":"2022-04-14T14:01:52.721328Z","shell.execute_reply":"2022-04-14T14:04:35.440177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Threshold to remove samples having missing values greater than threshold\nSAMPLE_THRESHOLD = 5\n\nprint(\"Samples Before Removal : {}\".format(df_temp.shape[0]))\n\ndf_temp.drop(df_temp[df_temp['missing_count'] > SAMPLE_THRESHOLD].index, axis = 0, inplace = True)\n\nprint(\"Samples After Removal : {}\".format(df_temp.shape[0]))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:04:35.443484Z","iopub.execute_input":"2022-04-14T14:04:35.443791Z","iopub.status.idle":"2022-04-14T14:04:35.534499Z","shell.execute_reply.started":"2022-04-14T14:04:35.443745Z","shell.execute_reply":"2022-04-14T14:04:35.533343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_temp","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:04:35.53635Z","iopub.execute_input":"2022-04-14T14:04:35.536671Z","iopub.status.idle":"2022-04-14T14:04:35.551523Z","shell.execute_reply.started":"2022-04-14T14:04:35.536638Z","shell.execute_reply":"2022-04-14T14:04:35.550355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3  style=\"text-align: center\" class=\"list-group-item list-group-item-warning\"> 2.2.2 Encoding Missingness </h3><a id = \"2.2.2\" ></a>\n\n\n\nWhen an attribute is discrete in nature, missingness can be directly encoded into the attribute as if it were a naturally occurring category. For example in this dataset the attribute loan_limit has 3344 missing values so we can assign some new category to these missing values. ","metadata":{}},{"cell_type":"code","source":"cat_missing_cols = [col for col in missing_columns if df[col].dtype == 'object']\ncat_missing_cols","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:04:35.553253Z","iopub.execute_input":"2022-04-14T14:04:35.553545Z","iopub.status.idle":"2022-04-14T14:04:35.574558Z","shell.execute_reply.started":"2022-04-14T14:04:35.55351Z","shell.execute_reply":"2022-04-14T14:04:35.573336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.loan_limit.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:04:35.576279Z","iopub.execute_input":"2022-04-14T14:04:35.576787Z","iopub.status.idle":"2022-04-14T14:04:35.615322Z","shell.execute_reply.started":"2022-04-14T14:04:35.576751Z","shell.execute_reply":"2022-04-14T14:04:35.614027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[cat_missing_cols] = df[cat_missing_cols].fillna('Missing')\ndf.loan_limit.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:04:35.616992Z","iopub.execute_input":"2022-04-14T14:04:35.617298Z","iopub.status.idle":"2022-04-14T14:04:35.756706Z","shell.execute_reply.started":"2022-04-14T14:04:35.617262Z","shell.execute_reply":"2022-04-14T14:04:35.755493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[cat_missing_cols].info()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:04:35.758644Z","iopub.execute_input":"2022-04-14T14:04:35.759831Z","iopub.status.idle":"2022-04-14T14:04:35.861389Z","shell.execute_reply.started":"2022-04-14T14:04:35.759784Z","shell.execute_reply":"2022-04-14T14:04:35.860283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3  style=\"text-align: center\" class=\"list-group-item list-group-item-warning\"> 2.2.3 Imputation Methods</h3><a id = \"2.2.3\" ></a>\n\n\n\nAnother approach to handling missing values is to impute or estimate them. Imputation uses information and relationships among the non-missing\nattributes to provide an estimate to fill in the missing value.\n\nIn this section we will work on imputation models which will help us impute missing values by extracting interesting patterns from attributes which don't have missing values at that point on time.\n\n\n- Within a sample data point, other variables may also be missing. For this reason, an imputation method should be tolerant of other missing data.\n\n- Imputation creates a model embedded within another model. There is a prediction equation associated with every attribute in the training set that might have missing data. It is desirable for the imputation method to be fast and have a compact prediction equation.\n\n- Many data sets often contain both numeric and discrete attributes. Rather than generating dummy variables for discrete attributes, a useful imputation method would be able to use attributes of various types as inputs.\n\n- The model for predicting missing values should be numerically stable and not be overly influenced by outlying data points.\n\nVirtually any machine learning model could be used to impute the data. Here, the focus will be on several that are good candidates to consider.\n\nQuestion arise if an attribute has missing values around 50-60% then can we use imputation methods? And the answer is it depends upon datasets which we are using because an attribute with 60% missing values may has very good correlation with some other attribute which can be helpful to fill those missing values on the other side if let say some column like ID column which is independent of all columns has missing values around 10% using imputation methods we may not get results we wanted. ","metadata":{}},{"cell_type":"markdown","source":"In this notebook we are gonna work on following imputation methods:-\n\n- KNN for Imputation\n- Tree Based Imputation\n- Linear Models for Imputation   ","metadata":{}},{"cell_type":"markdown","source":"<h3>(a) K-Nearest Neighbors(KNN) for Imputation</h3>\n\nWhen the training set is small or moderate in size, K-nearest neighbors can be a quick and effective method for imputing missing values. This procedure identifies a sample with one or more missing values. Then it identifies the K most similar samples in the training data that are complete (i.e., have no missing values in some columns). Similarity of samples for this method is defined by a distance metric. When all of the predictors are\nnumeric, standard Euclidean distance is commonly used as the similarity metric. \n\nAfter computing the distances, the K closest samples to the sample with the missing value are identified and the average value of the predictor of interest is calculated. This value is then used to replace the missing value of the sample.","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import KNNImputer\n\ndf_temp = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:04:35.862904Z","iopub.execute_input":"2022-04-14T14:04:35.863192Z","iopub.status.idle":"2022-04-14T14:04:35.923051Z","shell.execute_reply.started":"2022-04-14T14:04:35.86315Z","shell.execute_reply":"2022-04-14T14:04:35.921843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we haven't done categorical encoding yet (We'll cover it in next section) so, for time being lets impute on numerical data only later we will impute on full data after encoding.","metadata":{}},{"cell_type":"code","source":"num_cols = [col for col in df_temp.columns if df_temp[col].dtype != 'object']\nprint(num_cols)\ndf_temp = df_temp[num_cols]","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:04:35.924577Z","iopub.execute_input":"2022-04-14T14:04:35.924836Z","iopub.status.idle":"2022-04-14T14:04:35.947967Z","shell.execute_reply.started":"2022-04-14T14:04:35.924804Z","shell.execute_reply":"2022-04-14T14:04:35.946974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing KNNImputer\nknn = KNNImputer(n_neighbors = 3)\n\nknn.fit(df_temp)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:04:35.949731Z","iopub.execute_input":"2022-04-14T14:04:35.950043Z","iopub.status.idle":"2022-04-14T14:04:35.990081Z","shell.execute_reply.started":"2022-04-14T14:04:35.949995Z","shell.execute_reply":"2022-04-14T14:04:35.989201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = knn.transform(df_temp)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:04:35.991601Z","iopub.execute_input":"2022-04-14T14:04:35.992492Z","iopub.status.idle":"2022-04-14T14:21:14.346778Z","shell.execute_reply.started":"2022-04-14T14:04:35.992452Z","shell.execute_reply":"2022-04-14T14:21:14.34572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp = pd.DataFrame(X, columns = num_cols)\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:21:14.348767Z","iopub.execute_input":"2022-04-14T14:21:14.349175Z","iopub.status.idle":"2022-04-14T14:21:14.639138Z","shell.execute_reply.started":"2022-04-14T14:21:14.349109Z","shell.execute_reply":"2022-04-14T14:21:14.638023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_temp","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:21:14.640886Z","iopub.execute_input":"2022-04-14T14:21:14.641244Z","iopub.status.idle":"2022-04-14T14:21:14.646433Z","shell.execute_reply.started":"2022-04-14T14:21:14.641209Z","shell.execute_reply":"2022-04-14T14:21:14.645255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>(b) Trees</h3>\n\nTree-based models are a reasonable choice for an imputation technique since a tree can be constructed in the presence of other missing data. While a single tree could be used as an imputation technique, it is known to produce results that have low bias but high variance. And we all know who kills the bias its Ensembles of trees. \n\nRandom forests is one such technique and has been studied for this purpose. However, there are a couple of notable drawbacks when using this technique in a predictive modeling setting. First and foremost, the random selection of predictors at each split necessitates a large number of trees (500 to 2000) to achieve a stable, reliable model. This can present a challenge as the number of attributes with missing data increases since a separate model must be built and retained for each predictor. Also Random forest will have heavy computations.\n\nA good alternative that has a smaller computational footprint is a bagged tree. A bagged tree is constructed in a similar fashion to a random forest.\nThe primary difference is that in a bagged model, all attributes are evaluated at each split in each tree. The performance of a bagged tree using 25–50 trees is generally in the ballpark of the performance of a random forest model. And the smaller number of trees is a clear advantage when the goal is to find reasonable imputed values for missing data.\n\n\n","metadata":{}},{"cell_type":"code","source":"missing_columns","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:21:14.656179Z","iopub.execute_input":"2022-04-14T14:21:14.656476Z","iopub.status.idle":"2022-04-14T14:21:14.663282Z","shell.execute_reply.started":"2022-04-14T14:21:14.656443Z","shell.execute_reply":"2022-04-14T14:21:14.662437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ndr = DecisionTreeRegressor()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:21:14.664738Z","iopub.execute_input":"2022-04-14T14:21:14.664975Z","iopub.status.idle":"2022-04-14T14:21:14.674291Z","shell.execute_reply.started":"2022-04-14T14:21:14.664917Z","shell.execute_reply":"2022-04-14T14:21:14.67338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"income = df['income']","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:21:14.675778Z","iopub.execute_input":"2022-04-14T14:21:14.676147Z","iopub.status.idle":"2022-04-14T14:21:14.692251Z","shell.execute_reply.started":"2022-04-14T14:21:14.676093Z","shell.execute_reply":"2022-04-14T14:21:14.691236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we have to encode categorical variables into numerical data to use sklearn's tree based models so for the time being I am encoding categorical variables using Label Encoding Method","metadata":{}},{"cell_type":"code","source":"df_temp = df.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:21:14.693773Z","iopub.execute_input":"2022-04-14T14:21:14.694031Z","iopub.status.idle":"2022-04-14T14:21:14.72165Z","shell.execute_reply.started":"2022-04-14T14:21:14.694Z","shell.execute_reply":"2022-04-14T14:21:14.720638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlb = LabelEncoder()\n\ncat_cols = [col for col in df.columns if df[col].dtype == 'object']\n\nfor col in cat_cols:\n    df_temp[col] = lb.fit_transform(df_temp[col])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:21:14.723335Z","iopub.execute_input":"2022-04-14T14:21:14.723568Z","iopub.status.idle":"2022-04-14T14:21:15.831705Z","shell.execute_reply.started":"2022-04-14T14:21:14.723538Z","shell.execute_reply":"2022-04-14T14:21:15.830704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n\ndef tree_imputation(df):\n    missing_cols = [col for col in df.columns if df[col].isnull().sum() > 0]\n    non_missing_cols = [col for col in df.columns if df[col].isnull().sum() == 0]\n    # num_cols = [col for col in missing_cols if df[col].dtype != 'object']\n\n    # df = df[num_cols]\n    for col in missing_cols:\n\n        # Defining a new bagging model for each attribute  \n        model = BaggingRegressor(DecisionTreeRegressor(), n_estimators = 40, max_samples = 1.0, max_features = 1.0, bootstrap = False, n_jobs = -1)\n\n        col_missing = df[df[col].isnull()]\n        temp = df.drop(df[df[col].isnull()].index, axis = 0)\n\n        # print(temp.columns)\n        # X = temp.drop(col, axis = 1)\n        X = temp.loc[:, non_missing_cols]\n        y = temp[col]\n\n        model.fit(X, y)\n\n        y_pred = model.predict(col_missing[non_missing_cols])\n        # col_missing[col] = y_pred\n\n        df.loc[col_missing.index, col] = y_pred\n        \n    return df\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:21:15.83323Z","iopub.execute_input":"2022-04-14T14:21:15.833452Z","iopub.status.idle":"2022-04-14T14:21:15.84398Z","shell.execute_reply.started":"2022-04-14T14:21:15.833422Z","shell.execute_reply":"2022-04-14T14:21:15.843039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new = tree_imputation(df_temp)\ndf_new.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:21:15.84581Z","iopub.execute_input":"2022-04-14T14:21:15.846058Z","iopub.status.idle":"2022-04-14T14:24:22.21533Z","shell.execute_reply.started":"2022-04-14T14:21:15.846028Z","shell.execute_reply":"2022-04-14T14:24:22.214237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msno.bar(df_new)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:24:22.217049Z","iopub.execute_input":"2022-04-14T14:24:22.217318Z","iopub.status.idle":"2022-04-14T14:24:25.543829Z","shell.execute_reply.started":"2022-04-14T14:24:22.217281Z","shell.execute_reply":"2022-04-14T14:24:25.542833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see all missing values from the dataset are gone. Now as we temporarily encode categorical variables because we will encode them in later section so lets decode them.","metadata":{}},{"cell_type":"code","source":"df_new = pd.concat([df[cat_cols], df_new.drop(cat_cols, axis = 1)], axis = 1)\ndf_new.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:24:25.545639Z","iopub.execute_input":"2022-04-14T14:24:25.546295Z","iopub.status.idle":"2022-04-14T14:24:25.654841Z","shell.execute_reply.started":"2022-04-14T14:24:25.54625Z","shell.execute_reply":"2022-04-14T14:24:25.653894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_new.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:24:25.6562Z","iopub.execute_input":"2022-04-14T14:24:25.656419Z","iopub.status.idle":"2022-04-14T14:24:25.947033Z","shell.execute_reply.started":"2022-04-14T14:24:25.65639Z","shell.execute_reply":"2022-04-14T14:24:25.945922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>(c) Linear Methods</h3>\n\nWhen a complete Attribute shows a strong linear relationship with a attribute that requires imputation, a straightforward linear model may be the best approach. Linear models can be computed very quickly. Linear regression can be used for a numeric attribute that requires imputation.\nSimilarly, logistic regression is appropriate for a categorical attribute that requires imputation.\n\nLet say feature rate_of_interest and Interest_rate_spread are dependent features means one feature can be defined using other. If feature rate_of_interest has missing values than it can be imputed using simple linear model trained on Interest_rate_spread.","metadata":{}},{"cell_type":"markdown","source":"<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">3. Encoding Categorical Attributes</h1><a id = \"3\" ></a>\n\n\nCategorical Features are those that contain qualitative data.This Section focuses primarily on methods that encode categorical data to numeric values.\n\nCategorical variables/features are any feature type can be classified into three major types:\n\n- Nominal\n- Ordinal\n- Binary\n\n**Nominal variables** are variables that have two or more categories which do not have any kind of order associated with them. For Example if our dataset has any 4 types of colors, i.e. Red, Blue, Orange, Green it can be considered as a nominal variable.\n\n**Ordinal variables** on the other hand, have “levels” or categories with a particular order associated with them. For example, an ordinal categorical variable can be a feature with three different levels: low, medium and high. Order is important.\n\n**Binary Variables** are same as nominal variables but with only categories For example, if gender is classified into two groups, i.e. male and female.\n\nFor Nominal Variables We generally uses Label Encoding Scheme in which we encode each category by just converting it to some integer values this kind of encoding can work in case of Ordinal variables but **for label encoding it has the disadvantage that the numeric values can be misinterpreted by algorithms as having some sort of hierarchy/order in them**. This ordering issue is addressed in another common alternative approach called 'One-Hot Encoding'.\n\nOne-Hot-Encoding has the advantage that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. **The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting with the curse of dimensionality.**\n\nAnother big issue with encoding schemes is new category or while splitting data in train/validation/test set all samples of the rare classes may split into validation/test set then during it will raise error while predicting.  \n","metadata":{}},{"cell_type":"markdown","source":"Because of few potential issues with traditional approaches we now need to search for some unique approaches some of them are listed below:-\n\n- Supervised Encoding Methods\n- Approaching for Novel Categories \n\nStarting from Supervised Encoding Methods lets define it first \n\n\n<h2  style=\"text-align: center\" class=\"list-group-item list-group-item-success\"> 3.1 Supervised Encoding Methods</h2><a id = \"3.1\" ></a>\n\n\nThere are several methods of encoding categorical variable to numeric columns using the output data as a guide (so that they are supervised methods). In Supervised Techniques we will discuss following methods to encode categorical variable:-\n\n- Effect or Likelihood Encoding  \n- Target Encoding \n- Deep Learning Methods\n\n\n\n<h3  style=\"text-align: center\" class=\"list-group-item list-group-item-warning\"> 3.1.1 Likelihood Encoding</h3><a id = \"3.1.1\" ></a>\n\n\n\nThe effect of the factor level on the output data is measured and this effect is used as the numeric encoding. Here effect of that particular category on output data can be calculated using simple linear models or mean, mode and median methods. \n\nFor classification problems, a simple logistic regression model can be used to measure the effect between the categorical outcome and the categorical predictor. After Computing Effects we will compute log-odds of those effects, If the effect is p, the odds of that event are defined as p/(1 − p) and log odds by simply taking log of odds. After when we get the log odds of each category we than encode each category with them using map function.     \n","metadata":{}},{"cell_type":"code","source":"df_temp = df_new.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:24:25.948501Z","iopub.execute_input":"2022-04-14T14:24:25.94878Z","iopub.status.idle":"2022-04-14T14:24:25.980752Z","shell.execute_reply.started":"2022-04-14T14:24:25.948747Z","shell.execute_reply":"2022-04-14T14:24:25.979705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n\n## Again we have to temporarily encode variables\nlb = LabelEncoder()\n\ncat_cols = [col for col in df_temp.columns if df_temp[col].dtype == 'object']\n\nfor col in cat_cols:\n    df_temp[col] = lb.fit_transform(df_temp[col])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:24:25.982212Z","iopub.execute_input":"2022-04-14T14:24:25.982448Z","iopub.status.idle":"2022-04-14T14:24:27.108611Z","shell.execute_reply.started":"2022-04-14T14:24:25.982416Z","shell.execute_reply":"2022-04-14T14:24:27.107526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def likelihood_encoding(df, cat_cols, target_variable = \"Status\"):\n    # cat_cols.remove(target_variable)\n    df_temp = df.copy()\n    for col in cat_cols:\n        effect = {}\n        print(col)\n        for category in df[col].unique():\n            print(category)\n\n            try:\n                temp = df[df[col] == category]\n                lr = LogisticRegression()\n                X = temp.drop(target_variable, axis = 1, inplace = False)\n                y = temp[target_variable]\n                # print(temp.drop(target_variable, axis = 1).isnull().sum())\n                lr.fit(X, y)\n\n                effect[category] = accuracy_score(y, lr.predict(X))\n            except Exception as E:\n                print(E)\n        \n        for key, value in effect.items():\n            effect[key] = np.log(effect[key] / (1 - effect[key] + 1e-6))\n            \n        df_temp.loc[:, col] = df_temp.loc[:, col].map(effect)\n    return df_temp","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:24:27.110734Z","iopub.execute_input":"2022-04-14T14:24:27.111102Z","iopub.status.idle":"2022-04-14T14:24:27.121985Z","shell.execute_reply.started":"2022-04-14T14:24:27.111052Z","shell.execute_reply":"2022-04-14T14:24:27.120876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp = likelihood_encoding(df_temp, cat_cols)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:24:27.12381Z","iopub.execute_input":"2022-04-14T14:24:27.124358Z","iopub.status.idle":"2022-04-14T14:25:11.379069Z","shell.execute_reply.started":"2022-04-14T14:24:27.124309Z","shell.execute_reply":"2022-04-14T14:25:11.378001Z"},"_kg_hide-output":true,"scrolled":true,"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:25:11.380839Z","iopub.execute_input":"2022-04-14T14:25:11.381506Z","iopub.status.idle":"2022-04-14T14:25:11.44121Z","shell.execute_reply.started":"2022-04-14T14:25:11.381451Z","shell.execute_reply":"2022-04-14T14:25:11.440095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:25:11.442998Z","iopub.execute_input":"2022-04-14T14:25:11.443602Z","iopub.status.idle":"2022-04-14T14:25:11.496293Z","shell.execute_reply.started":"2022-04-14T14:25:11.443536Z","shell.execute_reply":"2022-04-14T14:25:11.495175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df_temp","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:25:11.498144Z","iopub.execute_input":"2022-04-14T14:25:11.498579Z","iopub.status.idle":"2022-04-14T14:25:11.503721Z","shell.execute_reply.started":"2022-04-14T14:25:11.498527Z","shell.execute_reply":"2022-04-14T14:25:11.502664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Implementation part is done While very fast, it has drawbacks. For example, what happens when a factor level has a single value? Theoretically, the log-odds should be infinite in the appropriate direction i.e. p/(1 - p) tends to infinity if p = 1. And numerically, it is usually capped at a large (and inaccurate) value.\n\nFor example in above implementation for column construction_type their are 2 categories 1 has around 148637 values and other has 33 values so for category mh which has 33 values p becomes 1 and it raises the error. Then This lead us to move to next technique known as target encoding which is simpler that likelihood encoding. \n\n\n\n<h3  style=\"text-align: center\" class=\"list-group-item list-group-item-warning\"> 3.1.2 Target Encoding</h3><a id = \"3.1.2\" ></a>\n\n\nIt is same as likelihood encoding but the difference is we use average of output variable for that particular category to encode values inplace of some linear model. Lets move to the implementation.","metadata":{}},{"cell_type":"code","source":"def target_encoding(df, cat_cols, target_variable = \"Status\"):\n\n    for col in cat_cols:\n        weight = 7\n        feat = df.groupby(col)[target_variable].agg([\"mean\", \"count\"])\n        mean = feat['mean']\n        count = feat['count']\n        \n        smooth = (count * mean + weight * mean) / (weight + count)\n\n        df.loc[:, col] = df.loc[:, col].map(smooth)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:25:11.505716Z","iopub.execute_input":"2022-04-14T14:25:11.507199Z","iopub.status.idle":"2022-04-14T14:25:11.52564Z","shell.execute_reply.started":"2022-04-14T14:25:11.50714Z","shell.execute_reply":"2022-04-14T14:25:11.524539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp = df_new.copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:25:11.527448Z","iopub.execute_input":"2022-04-14T14:25:11.527934Z","iopub.status.idle":"2022-04-14T14:25:11.559109Z","shell.execute_reply.started":"2022-04-14T14:25:11.527844Z","shell.execute_reply":"2022-04-14T14:25:11.558211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n\n## Again we have to temporarily encode variables\nlb = LabelEncoder()\n\ncat_cols = [col for col in df_temp.columns if df_temp[col].dtype == 'object']\n\nfor col in cat_cols:\n    df_temp[col] = lb.fit_transform(df_temp[col])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:25:11.560914Z","iopub.execute_input":"2022-04-14T14:25:11.561289Z","iopub.status.idle":"2022-04-14T14:25:12.689958Z","shell.execute_reply.started":"2022-04-14T14:25:11.561239Z","shell.execute_reply":"2022-04-14T14:25:12.689046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp = target_encoding(df_temp, cat_cols)","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:25:12.691349Z","iopub.execute_input":"2022-04-14T14:25:12.691602Z","iopub.status.idle":"2022-04-14T14:25:12.866053Z","shell.execute_reply.started":"2022-04-14T14:25:12.69157Z","shell.execute_reply":"2022-04-14T14:25:12.865326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_temp.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:25:12.867294Z","iopub.execute_input":"2022-04-14T14:25:12.867626Z","iopub.status.idle":"2022-04-14T14:25:12.90011Z","shell.execute_reply.started":"2022-04-14T14:25:12.867595Z","shell.execute_reply":"2022-04-14T14:25:12.899175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Target Encoding could be good choice for binary classification but for regression it is not, because it ignores intra-category variation of the target variable. This is addressed in Bayesian Target Encoding.\n\n- Target encoding has a tendency to overfit due to the target leakage.\n\n- Another problem is that some of the categories have few training examples, and the mean target value for these categories may assume extreme values, so encoding these values with mean may reduce the model performance.\n\nThese issues are addressed in Bayesian Target Encoding. Which you can read from this informational blog [Target Encoding and Bayesian Target Encoding](https://towardsdatascience.com/target-encoding-and-bayesian-target-encoding-5c6a6c58ae8c)\n","metadata":{}},{"cell_type":"code","source":"df['age'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-04-14T14:25:12.901645Z","iopub.execute_input":"2022-04-14T14:25:12.902116Z","iopub.status.idle":"2022-04-14T14:25:12.932061Z","shell.execute_reply.started":"2022-04-14T14:25:12.902065Z","shell.execute_reply":"2022-04-14T14:25:12.93124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3  style=\"text-align: center\" class=\"list-group-item list-group-item-warning\"> 3.1.3 Deep Learning Methods</h3><a id = \"3.1.3\" ></a>\n\n\nAnother supervised approach comes from the deep learning literature on the analysis of textual data. In this case, large amounts of text can be cut up into individual words. Rather than making each of these words into its own indicator variable, word embedding or entity embedding approaches have been developed. Similar to the dimension reduction methods, the idea is to estimate a smaller set of numeric features that can be used to adequately represent the categorical predictors.\n\nIn addition to the dimension reduction, there is the possibility that these methods can estimate semantic relationships between words so that words with similar themes (e.g., “dog”, “pet”, etc.) have similar values in the new encodings. This technique is not limited to text data and can be used to encode any type of qualitative variable.\n\nThe idea is well very simple do not extract features manually use neural network to do the hard part and just wait for the results.","metadata":{}},{"cell_type":"markdown","source":"<h2  style=\"text-align: center\" class=\"list-group-item list-group-item-success\"> 3.2 Approaches for Novel Categories</h2><a id = \"3.2\" ></a>\n\n\n\nWhat if some new category introduce to some attribute in future how will we encode that variable then? If there is a possibility of encountering a new category in the future, one strategy would be to use the \"other\" category to capture new values. \n\nWhile this approach may not be the most effective at extracting predictive information relative to the response for this specific category, it does enable the original model to be applied to new data without completely refitting and we do need to ensure that the \"other\" category is present in the training/testing data.\n\nAfter assigning \"other\" category to novel category than we can do all kinds of encodings we studied above","metadata":{}},{"cell_type":"markdown","source":"This is it for this notebook in upcoming notebooks will try to cover more data preprocessing techniques like Binning for reducing noise, application of PCA in feature engineering, advance feature selection techniques and many more. ","metadata":{}},{"cell_type":"markdown","source":"<h1 style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">References</h1><a id = \"\" ></a>\n\n- [Data Preprocessing in Machine Learning](https://www.javatpoint.com/data-preprocessing-machine-learning)\n- [Easy Way of Finding and Visualizing Missing Data in Python](https://medium.datadriveninvestor.com/easy-way-of-finding-and-visualizing-missing-data-in-python-bf5e3f622dc5)\n- [Visualizing Missing Values in Python is Shockingly Easy](https://towardsdatascience.com/visualizing-missing-values-in-python-is-shockingly-easy-56ed5bc2e7ea)\n- [Encoding categorical variables using likelihood estimation](https://datascience.stackexchange.com/questions/11024/encoding-categorical-variables-using-likelihood-estimation)\n- [Target Encoding and Bayesian Target Encoding](https://towardsdatascience.com/target-encoding-and-bayesian-target-encoding-5c6a6c58ae8c)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}