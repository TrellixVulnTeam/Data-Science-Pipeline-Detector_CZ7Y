{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelBinarizer, OneHotEncoder, StandardScaler\nfrom sklearn import set_config\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-06-30T07:05:53.565189Z","iopub.execute_input":"2022-06-30T07:05:53.565894Z","iopub.status.idle":"2022-06-30T07:05:55.269396Z","shell.execute_reply.started":"2022-06-30T07:05:53.565792Z","shell.execute_reply":"2022-06-30T07:05:55.267934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this notebook, we'll use the Titanic dataset to train and tune a sklearn pipeline. We'll define boilerplate code that you can easily reuse in your other projects. \n\nThis notebook is structured in 4 parts: \n1. Exploratory Data Analysis (EDA) \n2. sklearn pipeline \n3. Hyperparameter Tuning \n4. Final Pipeline and submission","metadata":{}},{"cell_type":"code","source":"# Let's first load the data using pandas\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:05:55.272184Z","iopub.execute_input":"2022-06-30T07:05:55.273018Z","iopub.status.idle":"2022-06-30T07:05:55.306667Z","shell.execute_reply.started":"2022-06-30T07:05:55.272969Z","shell.execute_reply":"2022-06-30T07:05:55.305349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Exploratory Data Analysis - EDA\n\nThen, let's explore the data using pandas and seaborn.","metadata":{}},{"cell_type":"code","source":"#constants\nfeatures = ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\ntarget = 'Survived'\n\nprint(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:05:55.30812Z","iopub.execute_input":"2022-06-30T07:05:55.309252Z","iopub.status.idle":"2022-06-30T07:05:55.346186Z","shell.execute_reply.started":"2022-06-30T07:05:55.309201Z","shell.execute_reply":"2022-06-30T07:05:55.344948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(train, hue=\"Survived\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:05:55.349262Z","iopub.execute_input":"2022-06-30T07:05:55.350005Z","iopub.status.idle":"2022-06-30T07:06:07.977252Z","shell.execute_reply.started":"2022-06-30T07:05:55.349964Z","shell.execute_reply":"2022-06-30T07:06:07.976256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(np.round(train.isna().sum()/len(train)*100,2), columns=['percentage_missing'])","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:06:07.978725Z","iopub.execute_input":"2022-06-30T07:06:07.979534Z","iopub.status.idle":"2022-06-30T07:06:07.995253Z","shell.execute_reply.started":"2022-06-30T07:06:07.979471Z","shell.execute_reply":"2022-06-30T07:06:07.993859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cabin has too many missing values, so we might as well want to drop it.","metadata":{}},{"cell_type":"markdown","source":"## 2. Machine Learning Pipeline","metadata":{}},{"cell_type":"markdown","source":"Let's define a sklearn pipeline with an imputer and a scaler for both numerical and categorical features. ","metadata":{}},{"cell_type":"code","source":"def pipeline(numerical_imputer, numerical_scaler, numerical_features, categorical_imputer, categorical_encoder, categorical_features, estimator):\n    numerical_transformer = Pipeline(\n        steps=[\n            (\"numerical_imputer\", numerical_imputer),\n            (\"numerical_scaler\", numerical_scaler),\n        ]\n    )\n    categorical_transformer = Pipeline(\n        steps=[\n            (\"categorical_imputer\", categorical_imputer),\n            (\"categorical_encoder\", categorical_encoder),\n        ]\n    )\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"num\", numerical_transformer, numerical_features),\n            (\"cat\", categorical_transformer, categorical_features),\n        ]\n    )\n    clf = Pipeline(  # or just Pipeline if we don't care about PMML format\n        steps=[(\"preprocessor\", preprocessor), (\"classifier\", estimator)]\n    )\n    return clf","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:06:08.026802Z","iopub.execute_input":"2022-06-30T07:06:08.027885Z","iopub.status.idle":"2022-06-30T07:06:08.035922Z","shell.execute_reply.started":"2022-06-30T07:06:08.027853Z","shell.execute_reply":"2022-06-30T07:06:08.03482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#parameters of the pipeline\nrandom_state = 0\nnumerical_imputer = SimpleImputer(strategy='mean')\nnumerical_scaler = StandardScaler()\nnumerical_features = ['Age', 'Fare']\ncategorical_imputer = SimpleImputer(strategy='most_frequent')\ncategorical_encoder = OneHotEncoder(handle_unknown='ignore')\ncategorical_features = ['Pclass'\n            , 'Sex'\t\n            , 'SibSp'\n            , 'Parch'\n            , 'Ticket'\n            , 'Embarked'\n           ]\nestimator = GradientBoostingClassifier(random_state = random_state)\n\nX_train = train[numerical_features + categorical_features]\nX_test = test[numerical_features + categorical_features]\ny_train = train[target]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = pipeline(numerical_imputer = numerical_imputer\n                 , numerical_scaler = numerical_scaler\n                 , numerical_features = numerical_features  \n                 , categorical_imputer = categorical_imputer\n                 , categorical_encoder = categorical_encoder\n                 , categorical_features = categorical_features\n                 , estimator = estimator)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:06:08.037178Z","iopub.execute_input":"2022-06-30T07:06:08.037845Z","iopub.status.idle":"2022-06-30T07:06:08.052408Z","shell.execute_reply.started":"2022-06-30T07:06:08.03781Z","shell.execute_reply":"2022-06-30T07:06:08.051404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This boilerplate code is very useful, feel free to copy it for your other projects. Also you can use sklearn's set_config to visualize your pipeline. \n\nOur pipeline splits between numerical and categorical variables, does some preprocessing and finally has an estimator (here a GradientBoostingClassifier)","metadata":{}},{"cell_type":"code","source":"set_config(display=\"diagram\")\nmodel","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:06:08.053901Z","iopub.execute_input":"2022-06-30T07:06:08.054501Z","iopub.status.idle":"2022-06-30T07:06:08.146967Z","shell.execute_reply.started":"2022-06-30T07:06:08.054468Z","shell.execute_reply":"2022-06-30T07:06:08.1455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores = cross_val_score(model, X_train, y_train, cv=5, scoring = 'roc_auc')\nprint(\"Average CV score:\", scores.mean())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:06:08.152048Z","iopub.execute_input":"2022-06-30T07:06:08.152445Z","iopub.status.idle":"2022-06-30T07:06:09.068604Z","shell.execute_reply.started":"2022-06-30T07:06:08.15239Z","shell.execute_reply":"2022-06-30T07:06:09.067203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thats good, our default pipeline gives us an cross validated AUC of .86. Let's see if we can improve this through parameter tuning! ","metadata":{}},{"cell_type":"markdown","source":"## 3. Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"Let's use sklearn's GridSearchCV to perform hyperparameter tuning on n_estimators. There are many other hyperparameters to optimize so feel free to fork and try to improve the performance! ","metadata":{}},{"cell_type":"code","source":"parameter_grid = {\n    \"classifier__n_estimators\": [250, 500, 750, 1000],\n    \n}\nsearch = GridSearchCV(model, parameter_grid, n_jobs=2)\nsearch = search.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:06:09.070028Z","iopub.execute_input":"2022-06-30T07:06:09.070361Z","iopub.status.idle":"2022-06-30T07:06:21.696431Z","shell.execute_reply.started":"2022-06-30T07:06:09.07033Z","shell.execute_reply":"2022-06-30T07:06:21.694725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)\npd.DataFrame(search.cv_results_)[['param_classifier__n_estimators', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:06:21.699313Z","iopub.execute_input":"2022-06-30T07:06:21.700411Z","iopub.status.idle":"2022-06-30T07:06:21.726165Z","shell.execute_reply.started":"2022-06-30T07:06:21.70036Z","shell.execute_reply":"2022-06-30T07:06:21.724993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Final Model and Submission","metadata":{}},{"cell_type":"markdown","source":"Now that we have a good cross validation score, let's retrain on the full training set (as the performance of machine learning models increases with the amount of data).\n\nAlso, increasing the number of estimators and decreasing the learning rate proportionally usually gives better results. So you can try 2x the optimal n_estimator and /2 the optimal learning rate","metadata":{}},{"cell_type":"code","source":"final_estimator = GradientBoostingClassifier(random_state = random_state, n_estimators=1000)\nfinal_model = pipeline(numerical_imputer = numerical_imputer\n                 , numerical_scaler = numerical_scaler\n                 , numerical_features = numerical_features  \n                 , categorical_imputer = categorical_imputer\n                 , categorical_encoder = categorical_encoder\n                 , categorical_features = categorical_features\n                 , estimator = final_estimator)\nfinal_scores = cross_val_score(final_model, X_train, y_train, cv=5, scoring = 'roc_auc')\nprint(\"Average CV score:\", final_scores.mean())","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:06:21.727398Z","iopub.execute_input":"2022-06-30T07:06:21.728252Z","iopub.status.idle":"2022-06-30T07:06:28.963872Z","shell.execute_reply.started":"2022-06-30T07:06:21.728204Z","shell.execute_reply":"2022-06-30T07:06:28.962467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We increased the AUC from .86 to .88\nBut there are plenty of other parameters to play with! You can adjust a lot of other parameters, notably: \n- learning_rate\n- loss\n- subsample ","metadata":{}},{"cell_type":"code","source":"# get the final predictions\nfinal_model.fit(X=X_train, y=train[target])\npredictions = final_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:06:28.96557Z","iopub.execute_input":"2022-06-30T07:06:28.966628Z","iopub.status.idle":"2022-06-30T07:06:30.588901Z","shell.execute_reply.started":"2022-06-30T07:06:28.966576Z","shell.execute_reply":"2022-06-30T07:06:30.587643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#prepare the submission and load to submission.csv which is then used by kaggle (note that here we have to get the passenger id from test data in order for it to work)\nsubmission = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T07:06:30.590236Z","iopub.execute_input":"2022-06-30T07:06:30.590616Z","iopub.status.idle":"2022-06-30T07:06:30.604114Z","shell.execute_reply.started":"2022-06-30T07:06:30.590582Z","shell.execute_reply":"2022-06-30T07:06:30.602699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__Hope you liked this notebook! If so, please don't forget to upvote this notebook! Happy learning!__","metadata":{}}]}