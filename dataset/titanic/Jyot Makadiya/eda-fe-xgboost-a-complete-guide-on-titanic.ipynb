{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1 style=\"background-color:lightgreen;font-family:newtimeroman;font-size:300%;text-align:center\">Index</h1>\n<span style='font-family: \"Times New Roman\", Times; font-size: 28px;'>\n\n* [1.Importing Libraries](#1)    \n* [2.Quick look using automated EDA](#2)\n* [3.EDA and Data Visualizations](#3)\n* [4.Data Cleaning and Feature Engineering](#4)\n* [5.Training and Ensemble](#5)\n* [6.Conclusion](#6)                    \n</span>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>Hello Everyone, <br>\n    This notebook showscases xgboost pipeline for titanic competition with EDA and FE, the conclusions drawn from this EDA are defined in section 3, our main focus will be to build a model capable of drawing inference from new features which we will be generating using feature engineering. Finally this notebook demonstrates how you can also do some stacking/ensembling. This notebook demonstrates common steps to be followed in Titanic competition. Further info can be found at this <a href = 'https://www.kaggle.com/c/titanic/discussion/218052'> post</a>.\n</span></p>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n\n<h1 style=\"background-color:lightgreen;font-family:newtimeroman;font-size:250%;text-align:center\">Importing Libraires And Functions</h1>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\")\n\n#basics\nimport numpy as np \nimport pandas as pd \n\n\n#methods n algo\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport lightgbm as lgbm\nimport xgboost as xgb\n\n#plots and other utilities\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n#pd.set_option('max_columns',100)\nimport plotly.express as ex\n\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nfrom pandas.plotting import parallel_coordinates\nimport pandas_profiling\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nplt.rc('figure',figsize=(17,13))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>Importing Train and Test Dataset\n</span></p>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>We will be installing sweetviz library for automated EDA, it can give us general overview about data, its distribution and null values.\n</span></p>"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#comment this after installation of sweetviz (running second time)\n!pip install sweetviz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n\n<h1 style=\"background-color:lightgreen;font-family:newtimeroman;font-size:250%;text-align:center\">Quick look using automated EDA</h1>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#analyzing data using sweetviz (automated EDA)\nimport sweetviz as sv\ndata_report = sv.analyze(train_data)\ndata_report.show_html('Analysis.html')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import IFrame\nIFrame(src = 'Analysis.html',width=1000,height=600)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>We can notice some null values in Age, Cabin and Embarked! Now let's visualize the data distribution of individual features.\n    Feature selection seems trivial but it is of great importance when it comes to performance, we are going for feature engineering after this section, it will hopefully lead us to some conclusions with we can engineer new features</span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"features = [\"Pclass\",\"Age\",\"Fare\", \"Sex\", \"SibSp\", \"Parch\",\"Embarked\"]\ntarget = \"Survived\"\n\ntrain_df = train_data[features]\ntest_df = test_data[features]\ntarget_data = train_data[target]\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n\n<h1 style=\"background-color:lightgreen;font-family:newtimeroman;font-size:250%;text-align:center\">EDA and Data Visualizations</h1>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>Let's try our hands on some Exploratory Data Analysis, we will be using training data only for visualization as of now, as seen from the dataset, test and train are having almost same distribution, so it won't matter that much.\n</span></p>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n\n<h1 style=\"background-color:lightgreen;font-family:newtimeroman;font-size:200%;text-align:center\">Numerical Features</h1>\n"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>Let's start with Age distribution among passengers\n</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(14,8))\nfig = sns.distplot(train_df['Age'], color='Blue')\nfig.set_xlabel(\"Age of passengers\",size=15)\nfig.set_ylabel(\"Passengers Frequency\",size=15)\nplt.title('Age Distribution Among Passengers',size = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n<a id=\"2\"></a>\n\n<h2 style=\"background-color:lightgreen;font-family:newtimeroman;font-size:200%;text-align:center\">Categorical vs Numerical </h2>\n"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>We have two numerical features, Age and Fare, which we will use to plot against categorical features, Pclass, Gender \n</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#colors = ['lightblue', 'mediumturquoise', 'red', 'lightgreen']*9\nfig =   ex.histogram(train_df,x='Fare',title=\"Fare Distribution Among Passenger Classes\",color='Pclass')\nfig.update_traces(marker=dict(line=dict(color='#000000', width=0.8)))\n# Here we modify the layout\nfig.update_layout(\n                 title='Fare Distribution Among Passenger Classes',\n                legend=dict(\n                    x=1.0,\n                    y=1.0,\n                    bgcolor='rgba(50, 200, 100, 0.3)',\n                    bordercolor='rgba(255, 255, 255, 0.6)'\n                ),\n                 )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>The Fare distribution looks skewed, to look closer we will only using data points with less than 150 fare value\n</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#let's scale this distribution to get a clear picture\nfig =   ex.histogram(train_df[train_df['Fare']<=150],x='Fare',title=\"Scaled Fare Distribution Among Passengers Classes\",color='Pclass')\nfig.update_traces(marker=dict(line=dict(color='#000000', width=0.8)))\n# Here we modify the layout\nfig.update_layout(\n                 title='Scaled Fare Distribution Among Passengers Classes',\n                legend=dict(\n                    x=1.0,\n                    y=1.0,\n                    bgcolor='rgba(50, 200, 100, 0.3)',\n                    bordercolor='rgba(255, 255, 255, 0.6)'\n                ),\n                 )\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>It looks pretty good, now we will be visualizing age distribution against class\n</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig =   ex.histogram(train_df,x='Age',title=\"Age Distribution Among Classes\",color = 'Pclass')\nfig.update_traces(marker=dict(line=dict(color='#000000', width=0.8)))\n# Here we modify the layout\nfig.update_layout(\n                 title='Age Distribution Among Classes',\n                legend=dict(\n                    x=1.0,\n                    y=1.0,\n                    bgcolor='rgba(50, 200, 100, 0.3)',\n                    bordercolor='rgba(255, 255, 255, 0.6)'\n                ),\n                 )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>Let's have a look at Age against Gender\n</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig =   ex.histogram(train_df,x='Age',title=\"Age Distribution Among Gender\",color = 'Sex')\nfig.update_traces(marker=dict(line=dict(color='#000000', width=0.8)))\n# Here we modify the layout\nfig.update_layout(\n                 title='Age Distribution Among Gender',\n                legend=dict(\n                    x=1.0,\n                    y=1.0,\n                    bgcolor='rgba(50, 200, 100, 0.3)',\n                    bordercolor='rgba(255, 255, 255, 0.6)'\n                ),\n                 )\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n\n<h1 style=\"background-color:lightgreen;font-family:newtimeroman;font-size:200%;text-align:center\">Categorical Features</h1>\n"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'> Different Categorical features can give us insight on how each value represents which fraction of overall dataset, this includes Gender, Pclass, and emabarked feature\n</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\ncolors = ['lightbrown', 'mediumturquoise', 'red', 'lightgreen']\n\nlabels = train_data.Embarked.value_counts().index\nvalues = train_data.Embarked.value_counts().values\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.1)])\n\nfig.update_traces(marker=dict(colors = colors, line=dict(color='#000000', width=3)))\n\n# Here we modify the layout\nfig.update_layout(\n                 title='Passengers count by Embarked Feature',\n                legend=dict(\n                    x=1.0,\n                    y=1.0,\n                    bgcolor='rgba(50, 200, 100, 0.3)',\n                    bordercolor='rgba(255, 255, 255, 0.6)'\n                ),\n                 )\nfig.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"colors = ['mediumturquoise', 'lightred', 'lightgreen']\n\nlabels = train_data.Sex.value_counts().index\nvalues = train_data.Sex.value_counts().values\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.1)])\n\nfig.update_traces(marker=dict(colors = colors, line=dict(color='#000000', width=3)))\n# Here we modify the layout\nfig.update_layout(\n                 title='Passengers count by Gender',\n                legend=dict(\n                    x=1.0,\n                    y=1.0,\n                    bgcolor='rgba(50, 200, 100, 0.3)',\n                    bordercolor='rgba(255, 255, 255, 0.6)'\n                ),\n                 )\n\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"colors = [' ', 'mediumturquoise', 'red', 'lightgreen']\n\nlabels = train_data.Pclass.value_counts().index\nvalues = train_data.Pclass.value_counts().values\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.1)])\n\nfig.update_traces(marker=dict(colors = colors, line=dict(color='#000000', width=3)))\n\n# Here we modify the layout\nfig.update_layout(\n                 title='Passengers count by Class',\n                legend=dict(\n                    x=1.0,\n                    y=1.0,\n                    bgcolor='rgba(50, 200, 100, 0.3)',\n                    bordercolor='rgba(255, 255, 255, 0.6)'\n                ),\n                 )\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'> Next section describes how survived feature correlates with other categorical features, for example the next graph gives info about how many passengers from which class survived the most and which class suffered the most. Hopefully this will help better \n</span></p>"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=train_data['Survived'].unique(),\n    y=train_data.query(\"Pclass == 1\").groupby('Survived').count()['Pclass'],\n    name='class 1',\n    marker_color='mediumturquoise'\n))\nfig.add_trace(go.Bar(\n    x=train_data['Survived'].unique(),\n    y=train_data.query(\"Pclass == 2\").groupby('Survived').count()['Pclass'],\n    name='class 2',\n    marker_color='lightsalmon'\n))\nfig.add_trace(go.Bar(\n    x=train_data['Survived'].unique(),\n    y=train_data.query(\"Pclass == 3\").groupby('Survived').count()['Pclass'],\n    name='class 3',\n    marker_color='lightgreen'\n))\n\n# Here we modify the layout\nfig.update_layout(barmode='group',\n                 title='Survived Passengers Among Class',\n                #xaxis_tickfont_size=14,\n                yaxis=dict(\n                    title='Passanger Count',\n                    titlefont_size=16,\n                    tickfont_size=14,\n                ),\n                  xaxis=dict(\n                    title='Passanger Survived No/Yes',\n                    titlefont_size=16,\n                    tickfont_size=14,\n                ),\n                legend=dict(\n                    x=1.0,\n                    y=1.0,\n                    bgcolor='rgba(50, 200, 100, 0.3)',\n                    bordercolor='rgba(255, 255, 255, 0.6)'\n                ),\n                bargap=0.15, # gap between bars of adjacent location coordinates.\n                bargroupgap=0. # gap between bars of the same location coordinate.\n                 )\nfig.update_traces(marker=dict(line=dict(color='#000000', width=0.8)))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=train_data['Survived'].unique(),\n    y=train_data.query(\"Sex=='male'\").groupby('Survived').count()['Sex'],\n    name='male',\n    marker_color='mediumturquoise'\n))\nfig.add_trace(go.Bar(\n    x=train_data['Survived'].unique(),\n    y=train_data.query(\"Sex=='female'\").groupby('Survived').count()['Sex'],\n    name='female',\n    marker_color='lightsalmon'\n))\n\n# Here we modify the layout\nfig.update_layout(barmode='group',\n                 title='Survived Passengers Among Gender',\n                #xaxis_tickfont_size=14,\n                yaxis=dict(\n                    title='Passanger Count',\n                    titlefont_size=16,\n                    tickfont_size=14,\n                ),\n                  xaxis=dict(\n                    title='Passanger Survived No/Yes',\n                    titlefont_size=16,\n                    tickfont_size=14,\n                ),\n                legend=dict(\n                    x=1.0,\n                    y=1.0,\n                    bgcolor='rgba(50, 200, 100, 0.3)',\n                    bordercolor='rgba(255, 255, 255, 0.6)'\n                ),\n                bargap=0.15, # gap between bars of adjacent location coordinates.\n                bargroupgap=0. # gap between bars of the same location coordinate.\n                 )\nfig.update_traces(marker=dict(line=dict(color='#000000', width=0.8)))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=train_data['Survived'].unique(),\n    y=train_data.query(\"SibSp>=1\").groupby('Survived').count()['SibSp'],\n    name='Having Sib/Sp',\n    marker_color='mediumturquoise'\n))\nfig.add_trace(go.Bar(\n    x=train_data['Survived'].unique(),\n    y=train_data.query(\"SibSp==0\").groupby('Survived').count()['SibSp'],\n    name='Not having Sib/Sp',\n    marker_color='lightsalmon'\n))\n\n# Here we modify the layout\nfig.update_layout(barmode='group',\n                 title='Survived Passengers Having sibling/Spouse',\n                #xaxis_tickfont_size=14,\n                yaxis=dict(\n                    title='Passanger Count',\n                    titlefont_size=16,\n                    tickfont_size=14,\n                ),\n                  xaxis=dict(\n                    title='Passanger Survived No/Yes',\n                    titlefont_size=16,\n                    tickfont_size=14,\n                ),\n                legend=dict(\n                    x=1.0,\n                    y=1.0,\n                    bgcolor='rgba(50, 200, 100, 0.3)',\n                    bordercolor='rgba(255, 255, 255, 0.6)'\n                ),\n                bargap=0.15, # gap between bars of adjacent location coordinates.\n                bargroupgap=0. # gap between bars of the same location coordinate.\n                 )\nfig.update_traces(marker=dict(line=dict(color='#000000', width=0.8)))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=train_data['Survived'].unique(),\n    y=train_data.query(\"Parch>=1\").groupby('Survived').count()['Parch'],\n    name='With Par/Ch',\n    marker_color='mediumturquoise'\n))\nfig.add_trace(go.Bar(\n    x=train_data['Survived'].unique(),\n    y=train_data.query(\"Parch==0\").groupby('Survived').count()['Parch'],\n    name='Without Par/Ch',\n    marker_color='lightsalmon'\n))\n\n# Here we modify the layout\nfig.update_layout(barmode='group',\n                 title='Survived Passengers With Parents/Children',\n                #xaxis_tickfont_size=14,\n                yaxis=dict(\n                    title='Passanger Count',\n                    titlefont_size=16,\n                    tickfont_size=14,\n                ),\n                  xaxis=dict(\n                    title='Passanger Survived No/Yes',\n                    titlefont_size=16,\n                    tickfont_size=14,\n                ),\n                legend=dict(\n                    x=1.0,\n                    y=1.0,\n                    bgcolor='rgba(50, 200, 100, 0.3)',\n                    bordercolor='rgba(255, 255, 255, 0.6)'\n                ),\n                bargap=0.15, # gap between bars of adjacent location coordinates.\n                bargroupgap=0. # gap between bars of the same location coordinate.\n                 )\nfig.update_traces(marker=dict(line=dict(color='#000000', width=0.8)))\nfig.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: left;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>We have looked at data distribution and now we can draw some conclusions from our data analaysis. <br>\n   1. The passengers with parents/children and siblings/spouse had higher chances of survival (You better travel with your family next time)<br>\n 2. There was higher chance of death among male passengers comapred to female passengers<br>\n    3. The survival rate of class 3 is lower compared to other 2 classes<br>\n    4. Fare distribtuion is uneven showing the ticket prices may be split according to classes<br>\n    5. There are a lot of passengers in class 3 compared to class 1<br>\n  </span></p>"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"4\"></a>\n\n<h1 style=\"background-color:lightgreen;font-family:newtimeroman;font-size:250%;text-align:center\">Feature Engineering</h1>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>We have looked at data distribution and now it's time for feature engineering, precisely we will modify data based on conclusions drawn from above visualizations. Hopefully, these can help to see some patterns and boost our performance using our custom features</span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#describe train data\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.isnull().sum(), test_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>From the above data we can see that there are indeed some missing values in Age which we are going to fill using median values.</span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#remove null value from Age\nprint('median Age of the passengers: ',train_df['Age'].median())\nprint(train_df['Age'].isnull().sum(),test_df['Age'].isnull().sum(),\"null values present in train, test\")\n#now we replace all missing values with that value\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace=True)\n\n\ntest_df['Age'].fillna(test_df['Age'].median(), inplace=True)\nprint(train_df['Age'].isnull().sum(),train_df['Age'].isnull().sum(),\"null values remaining in train, test\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>Now it's time to generate some new features from our dataset, we have drawn some conclusions which will be helpful, for eg. merging Parch and SibSp features </span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def func(x):\n    if x[0]>0 or x[1]>0:\n        return True\n    else:\n        return False\n    \n#Apply new function to features, and generate with family feature\ntrain_df['WithFamily']=train_df[['SibSp','Parch']].apply(func,axis=1)\ntest_df['WithFamily']=test_df[['SibSp','Parch']].apply(func,axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>The features Sibsp and Parch are redunadnt as its information is already stored inside withFamily feature, so we drop them</span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(columns=['SibSp','Parch'],inplace=True)\ntest_df.drop(columns=['SibSp','Parch'],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>We will replace the two rows in data where Embarked feature is missing with S since 72% of the Passengers  </span></p>"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"train_df['Embarked'].fillna('S',inplace = True)\ntest_df['Embarked'].fillna('S',inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>We have already done some of the feature processing in previous section and Now we will be focusing on converting numerical continuous features into binned features</span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#this part is adapted from this kernal\n#checkout this great kernal on FE,https://www.kaggle.com/udit1907/titanic-disaster-beginner-python-documentation/notebook\n\n\n#we will use binning for Age and Fare features, hopefully will produce better results\n#func to process values\ndef encodeAgeFare(train):\n    train.loc[train['Age'] <= 16, 'Age'] = 0\n    train.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\n    train.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\n    train.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\n    train.loc[ (train['Age'] > 48) & (train['Age'] <= 80), 'Age'] = 4\n    \n    train.loc[train['Fare'] <= 7.91, 'Fare'] = 0\n    train.loc[(train['Fare'] > 7.91) & (train['Fare'] <= 14.454), 'Fare'] = 1\n    train.loc[(train['Fare'] > 14.454) & (train['Fare'] <= 31.0), 'Fare'] = 2\n    train.loc[(train['Fare'] > 31.0) & (train['Fare'] <= 512.329), 'Fare'] = 3\n\nencodeAgeFare(train_df)\nencodeAgeFare(test_df)\n","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#no need to apply \nprint(train_df.isnull().sum())\n#train_df = train_df.dropna(axis = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>We need to change encoding of our train-test data for categorical data, we will be using One-hot encoding for that. You can find further info <a href='https://www.kaggle.com/dansbecker/using-categorical-data-with-one-hot-encoding'>(here)</span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"#To change categorical to numerical, we will be using above One-hot encoding \ntrain_df=pd.get_dummies(train_df,columns=['Sex','Embarked','WithFamily'],drop_first=True)\ntest_df=pd.get_dummies(test_df,columns=['Sex','Embarked','WithFamily'],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>We will be using parameters found by tuning using Grid Search, hopefully will produce better results than manual tuning</span></p>"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# param_grid_xgb = {'n_estimators':[400, 600],\n#                   'learning_rate':[0.01, 0.03, 0.05],\n#                   'max_depth':[3, 4],\n#                   'subsample':[0.5, 0.7],\n#                   'colsample_bylevel':[0.5, 0.7],\n#                   'reg_lambda':[15, None],\n#                  }\n\n# clf = xgb.XGBClassifier()\n# grid = GridSearchCV(clf, cv=5, param_grid = param_grid_xgb,scoring='accuracy', verbose=2, n_jobs=-1)\n# grid.fit(train_df, target_data)\n# grid.best_params_\n# model = grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"5\"></a>\n\n<h1 style=\"background-color:lightgreen;font-family:newtimeroman;font-size:250%;text-align:center\">Training and Ensemble</h1>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'> These parameters are tuned using grid search CV, it takes a lot of time to run through the search space, for optimal results, we will be directly using best parameters results, in case you want to try out yourself the HPO, expand the previous cell and have a look at it.\n</span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = xgb.XGBClassifier(**{'colsample_bylevel': 0.7, 'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 1000,\n                                 'reg_lambda': 15,'eval_metric': 'error','subsample': 0.5}).fit(train_df, target_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#as a final step, we will be predicting with our best model\n\ntest_y = model.predict(test_df).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>As the final step, we will be using ensembling from a <a href ='https://www.kaggle.com/mauricef/titanic'>previous notebook </a>, it contains information from Name feature which we have not taken into account\n    </span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_z = pd.read_csv('/kaggle/input/k/mauricef/titanic/survived.csv')['Survived'].to_list()\n\nfinal_preds = [z * 0.75 + y * 0.25 for z,y in zip(test_z,test_y)]\n\nfinal_preds = (np.array(final_preds) > 0.5) * 1\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': final_preds})\noutput.to_csv('my_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('XGBoost score on train data:', round(model.score(train_df, target_data) * 100, 2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"6\"></a>\n\n<h1 style=\"background-color:lightgreen;font-family:newtimeroman;font-size:250%;text-align:center\">Conclusion </h1>"},{"metadata":{},"cell_type":"markdown","source":"<p style=\"text-align: left;\"><span style='font-family: \"Times New Roman\", Times; font-size: 22px;'>We can draw some conclusions from above notebook and our process, notably : <br>\n    1.We found that EDA can be a powerful tool in some cases where you can get good insight into data<br>\n    2.Feature engineering can boost score significantly when features are used properly<br>\n    3. Hyper parameter optimization is important but don't waste too much time doing it<br>\n    4. Basic pipeline is a must to understanding problem statement and gettting started. <br>\n    5. Stacking with right model is important, but for that first we need to know the diversity of predictions and how different it is from our model<br>\n    At last we trained XGboost model and worked on our submission with ensembling with another predictions\n    </span></p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}