{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <p style=\"font-family:Papyrus;color:Orange;font-size:1.em\"> Titanic Disaster EDA , Visualization and Survival Prediction using SVM </p>"},{"metadata":{},"cell_type":"markdown","source":"## Introduction :\n<p style=\"color: indigo\">\nThe Kernel is trying to follow through a typical data science workflow to bulid a prediction model for survival of passenger onboard on Titanic. If you like the work here , please upvote. If you have suggestions to make this better , will love to read them in the comments. Thanks.\n    \n</p>\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Objective Definition\n\n\n**1. Data Acquisition of the Titanic Data set  :** We will read, understand available data , validate high level data quality in the source.\n\n**2. Data Wrangling :** Clean data where applicable. Handle any missing values. Build Integrated views based on associations. Build Summaries if needed.\n\n**3. Exploratory Analysis :** Here we will perform detailed analysis to explore hiddent patterns , co-relations etc. Use Visualize where needed.\n\n**4. Feature Engineering :** This is where we will identify Features to develop the model, check and prepare if any Derived Features are needed.\n\n**5. Model Preparation :** Building the model using defined features. Based on nature of problem definition, we are having a Classification Problem at hand. At the same time, Regression pattern can also be applicable on the data. We will restrict our Model Algorithms to use (SVM) Classification and (Logistict Regression) Regression.\n\n**6. Model Evaluation :** We will build evaluation around both the model we will prepare and evaluate the accuracy score that can be obtained from both. Where applicable , we will use Visualization to compare the accuracy scores. Through evaluation , we will pick the better scoring prediction model.\n\n**7. Predict :** Final stage, we will run our final model to execute predictions."},{"metadata":{},"cell_type":"markdown","source":"# Data Acuisition"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Input data files are available in the read-only \"../input/\" directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import necessary packages\n# Create any reusable methods to use later\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def screen_data(df):\n    print('-'*40)\n    print('list of Columns : ',df.columns.to_list())\n    print('-'*40)\n    print('Missing Values in the columns : \\n')\n    print(df.isnull().sum())\n    print('-'*40)\n    print('Unique Value Counts : \\n')\n    print(df.nunique())\n    print('-'*40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/titanic/train.csv')\nscreen_data(df_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random sampling in the data\ndf_train.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = pd.read_csv(  '../input/titanic/test.csv'  )\nscreen_data(df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we see that both `train.csv` and `test.csv` has common behavior of **missing values in columns Age and Cabin.**"},{"metadata":{},"cell_type":"markdown","source":"### To understand the missing values and percentages better, we will run a heat map on the dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1,2, figsize=(18,6))\ncolors = ['lightyellow' , 'red']\n\nf1 = sns.heatmap(df_train[['Age','Cabin','Embarked','Fare']].sort_values('Age').isnull(), cmap=sns.color_palette(colors), ax=ax1)\nf1.set_title('Missing Values - Train Data')\n\nf2 = sns.heatmap(df_test[['Age','Cabin','Embarked','Fare']].sort_values('Age').isnull(), cmap=sns.color_palette(colors), ax=ax2)\nf2.set_title('Missing Values - Test Data')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The red area in the above heat maps shows distribution of missing values in respective datasets.\n    \n   - It is seens tha we have large amount of missing data for `Cabin` column,\n   \n   - Missing data for `Age` is also considerable , but we can work around the same.\n   \n   - Column `Embarked` and `Fare` does not have any significant missing data.\n   "},{"metadata":{},"cell_type":"markdown","source":"# Data Exploration and  Data Wrangling"},{"metadata":{},"cell_type":"markdown","source":"Since exploring the data and accordingly wrangle it further the next exploration is an iterative process, we will combine the two stages together. \n\nAs part of Data Exploration stage, we would want to explore possible relationships of the avaialble features by looking at the `Survived` fact for the passengers.\n\nWe will also want to identify if there are new features we want to derive , that can be a better indicator for the survival chances.\n\nDuring the Wrangling , our goal is to identify and create a reusable function for the necessary pre-processing operations that needs to go on both train and test data sets so that they remain in unison standards when applied to model in later stages."},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train[['PassengerId','Age']].groupby('Age').count().reset_index().rename(columns={'PassengerId' : 'Cnt'}).sort_values('Cnt', ascending=False).head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Age_range'] = pd.cut(df_train['Age'], 10, precision=0)\n\nfig , ax = plt.subplots(1,1, figsize=(14,5))\n\nz = sns.barplot(data = df_train[['Survived','Age_range']] , x='Age_range' , y='Survived',  ax = ax, palette=sns.color_palette('pastel'))\nz.set_title('Age comparison for Survival')\nplt.show()\n\ndf_train.drop('Age_range', axis = 1, inplace  = True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Age data is working better when put into range bins.\n\n#### Ticket data is basically ticket identifiers, so they can simply be removed from analysis as Ticket Id may not have any significance to survival rate."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ns1 = sns.barplot(data = df_train, y='Survived' ,  hue='Sex' , x='Sex')\ns1.set_title('Male-Female Survival Comparison')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Female passengers had higher Survival rate compared to Male passengers."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Finding Titles in the names\n\nimport re\nfrom collections import Counter\n\n\ndef check_title(x) : \n    return re.search(' ([A-Za-z]+)\\.', x).group(1)\n\nCounter(df_train['Name'].map(check_title).to_list())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### We will try to explore if the Title have any co-relation with survival"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['Title'] = df_train['Name'].map(check_title)\n\nfig , ax = plt.subplots(1,1, figsize=(16,6))\nbar = sns.barplot(data = df_train[['Survived' , 'Title']] , y='Title' , x='Survived',  orient='h', ax=ax, palette=sns.color_palette('Blues'))\n# bar = sns.swarmplot(data = df_train[['Survived' , 'Title']] , x='Title' , y='Survived', ax=ax)\nbar.set_title ('Survival Comparison for Passengers with Titles')\nplt.show()\n\ndf_train.drop('Title', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### There is a definite inclination to survival for some Titles like Lady, Sir, Countess etc. To capture this in our prediction, we will want to add a new Derived Feature for 'Title' in our Data set. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train['Fare_range'] = pd.cut(df_train['Fare'], 10, precision=0)\n\nfig , ax = plt.subplots(1,1, figsize=(16,4))\nbar = sns.violinplot(data = df_train[['Survived' , 'Fare']] , y='Fare' , x='Survived', ax=ax)\nbar.set_title ('Survival based on Fare')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring mutliple features together"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df_train, hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can observe in above visuals that survival rate is :\n    \n   1. better on certain Pclass values.\n   2. better on certain Age ranges\n   3. not so focussed on Parch Values.\n   4. better to for very high Fare rate.\n   \nFor other Features such as PassengerId, the survival rate is not having any centralized inclination on the feature at individual values level. "},{"metadata":{},"cell_type":"markdown","source":"### Checking Embarked for Survival"},{"metadata":{"trusted":true},"cell_type":"code","source":"grid = sns.FacetGrid(df_train, row='Survived', col='Embarked', height=2, aspect=2, palette=sns.color_palette('ocean'))\ngrid.map(plt.hist, 'Embarked', alpha=.5, bins=50)\ngrid.add_legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### Building a Reusable Function to apply on both Train and Test Data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_process(df):\n    df['Embarked'].fillna(df['Embarked'].mode(), inplace=True)  # Fix missing values in Embarked if any\n    df['Fare'].fillna(df['Fare'].mean() , inplace=True)  # Fix missing values in Fare\n    df['Age'].fillna(df['Age'].median() , inplace=True)  # Put median value for Age\n    \n    if 'Cabin' in df.columns:\n        df.drop('Cabin', axis = 1, inplace = True)  # drop Cabin Column\n    \n    if 'Ticket' in df.columns:\n        df.drop('Ticket', axis = 1, inplace = True)  # drop Ticket Column , being a ticket number it has no relevance\n    \n    df['Age_cd'] = pd.cut(df_train['Age'], 10, precision=0).astype('category').cat.codes  # new Column to bucket the age ranges and putting code for it\n    \n\n#     df['Fare_range'] = pd.cut(df_train['Fare'], 10, precision=0)  # new Column\n    \n    df['Embarked_cd'] = df['Embarked'].astype('category').cat.codes # new Column\n    \n    df['Title_cd'] = df['Name'].map(check_title).astype('category').cat.codes # new Column\n    \n    df['Sex_cd'] = df['Sex'].astype('category').cat.codes  # Change sex to codes\n           \n    print(\"Preprocessing on the data complete ..\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Running the preprocessing on Train and Test Data set"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_process(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_process(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking Correlation of various Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"h = sns.heatmap(pd.get_dummies(df_train[['Survived', 'Pclass', 'Sex', 'Age_cd']], \n               columns=['Survived', 'Pclass', 'Sex', 'Age_cd']).corr(),\n           annot=True,cmap='RdYlGn_r',linewidths=0.2)\n\nfig=plt.gcf()\n\nh.set_title('Correlation on Various key Features in consideration')\n\nfig.set_size_inches([18,10])\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the Model\n\n\nIn this stage we will be building our models. As already identified in the objective section above, we will be looking to build 2 type of models.\n\n1. SVM model\n\n2. Logistic Regression model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# identified features to be used\nfeatures = ['Pclass', 'SibSp', 'Parch', 'Fare',  'Age_cd', 'Sex_cd', 'Embarked_cd', 'Title_cd']\n\n\nX_train, X_test, y_train, y_test = train_test_split(df_train[features],  df_train['Survived'], test_size=0.3 , random_state=25)\n\n# Check basic setup of train and test\nfor x , y in enumerate([X_train, X_test, y_train, y_test]):\n    print(f'{x+1} :  {y.shape}')  \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic regression\nlr = LogisticRegression(max_iter=2000)\nlr.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SVM \nsvm = SVC(kernel='rbf', C=100 , random_state=1)\nsvm.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Evaluation\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(model_name) : \n    print(\"-\"*40,'\\n')\n    print(\"Evaluation for Model : \", model_name)\n    print('\\n',\"-\"*40,'\\n')\n    y_predict = model_name.predict(X_test)\n    acc = accuracy_score(y_test , y_predict)\n    print(f'Accuracy score of the model : {acc*100} %')\n    cmat = confusion_matrix(y_test , y_predict)\n    scores = cmat.diagonal() / cmat.sum(axis=1)\n    for x in zip(['Not Survived' , 'Survived' ], scores) :\n        print(f'Accuracy Scores for - {x[0]} : {x[1]*100} %')\n    print('\\n',\"-\"*40,'\\n')\n    sns.heatmap(cmat, cmap='Set3' , annot=True , fmt = '4.0f')\n    title = f'Confusion_matrix : {model_name}'\n    plt.title(f'{title}', y=1.1, size=20)\n    plt.show()\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(lr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_model(svm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Overall accuracy scores received for both the models are close by. However, SVM score of individual category of prediction shows that it is slightly better when  predicting 'Survived'. We will use SVM for final prediction."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Prediction of the Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = svm.predict(df_test[features])\nfinal = pd.DataFrame ({'PassengerId' : df_test['PassengerId'], 'Survived': prediction})\nfinal.to_csv('./submission_svm.csv', index=False)\n\nfinal.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_result = final.groupby('Survived').count().reset_index().rename(columns = {'PassengerId': 'Passenger Count'})\nres = sns.barplot(data=df_result, x = 'Survived' , y='Passenger Count', hue='Survived', palette='cool_r')\nres.set_title('Final Results from Prediction')\nplt.show()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}