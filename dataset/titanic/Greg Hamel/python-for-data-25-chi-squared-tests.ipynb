{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Python for Data 25: Chi-Squared Tests\n[back to index](https://www.kaggle.com/hamelg/python-for-data-analysis-index)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Last lesson we introduced the framework of statistical hypothesis testing and the t-test for investigating differences between numeric variables. In this lesson, we turn our attention to a common statistical test for categorical variables: the chi-squared test."},{"metadata":{},"cell_type":"markdown","source":"# Chi-Squared Goodness-Of-Fit Test"},{"metadata":{},"cell_type":"markdown","source":"In our study of t-tests, we introduced the one-way t-test to check whether a sample mean differs from the an expected (population) mean. The chi-squared goodness-of-fit test is an analog of the one-way t-test for categorical variables: it tests whether the distribution of sample categorical data matches an expected distribution. For example, you could use a chi-squared goodness-of-fit test to check whether the race demographics of members at your church or school match that of the entire U.S. population or whether the computer browser preferences of your friends match those of Internet uses as a whole.\n\nWhen working with categorical data, the values themselves aren't of much use for statistical testing because categories like \"male\", \"female,\" and \"other\" have no mathematical meaning. Tests dealing with categorical variables are based on variable counts instead of the actual value of the variables themselves.\n\nLet's generate some fake demographic data for U.S. and Minnesota and walk through the chi-square goodness of fit test to check whether they are different:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"national = pd.DataFrame([\"white\"]*100000 + [\"hispanic\"]*60000 +\\\n                        [\"black\"]*50000 + [\"asian\"]*15000 + [\"other\"]*35000)\n           \n\nminnesota = pd.DataFrame([\"white\"]*600 + [\"hispanic\"]*300 + \\\n                         [\"black\"]*250 +[\"asian\"]*75 + [\"other\"]*150)\n\nnational_table = pd.crosstab(index=national[0], columns=\"count\")\nminnesota_table = pd.crosstab(index=minnesota[0], columns=\"count\")\n\nprint( \"National\")\nprint(national_table)\nprint(\" \")\nprint( \"Minnesota\")\nprint(minnesota_table)","execution_count":2,"outputs":[{"output_type":"stream","text":"National\ncol_0      count\n0               \nasian      15000\nblack      50000\nhispanic   60000\nother      35000\nwhite     100000\n \nMinnesota\ncol_0     count\n0              \nasian        75\nblack       250\nhispanic    300\nother       150\nwhite       600\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Chi-squared tests are based on the so-called chi-squared statistic. You calculate the chi-squared statistic with the following formula:\n\n$$ sum(\\frac{(observed-expected)^2}{expected}) $$\n\nIn the formula, observed is the actual observed count for each category and expected is the expected count based on the distribution of the population for the corresponding category. Let's calculate the chi-squared statistic for our data to illustrate:"},{"metadata":{"trusted":true},"cell_type":"code","source":"observed = minnesota_table\n\nnational_ratios = national_table/len(national)  # Get population ratios\n\nexpected = national_ratios * len(minnesota)   # Get expected counts\n\nchi_squared_stat = (((observed-expected)**2)/expected).sum()\n\nprint(chi_squared_stat)","execution_count":3,"outputs":[{"output_type":"stream","text":"col_0\ncount    18.194805\ndtype: float64\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"*Note: The chi-squared test assumes none of the expected counts are less than 5.*\n\nSimilar to the t-test where we compared the t-test statistic to a critical value based on the t-distribution to determine whether the result is significant, in the chi-square test we compare the chi-square test statistic to a critical value based on the [chi-square distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution). The scipy library shorthand for the chi-square distribution is chi2. Let's use this knowledge to find the critical value for 95% confidence level and check the p-value of our result:"},{"metadata":{"trusted":true},"cell_type":"code","source":"crit = stats.chi2.ppf(q = 0.95, # Find the critical value for 95% confidence*\n                      df = 4)   # Df = number of variable categories - 1\n\nprint(\"Critical value\")\nprint(crit)\n\np_value = 1 - stats.chi2.cdf(x=chi_squared_stat,  # Find the p-value\n                             df=4)\nprint(\"P value\")\nprint(p_value)","execution_count":4,"outputs":[{"output_type":"stream","text":"Critical value\n9.487729036781154\nP value\n[0.00113047]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Since our chi-squared statistic exceeds the critical value, we'd reject the null hypothesis that the two distributions are the same.\n\nYou can carry out a chi-squared goodness-of-fit test automatically using the scipy function scipy.stats.chisquare():"},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.chisquare(f_obs= observed,   # Array of observed counts\n                f_exp= expected)   # Array of expected counts","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"Power_divergenceResult(statistic=array([18.19480519]), pvalue=array([0.00113047]))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The test results agree with the values we calculated above."},{"metadata":{},"cell_type":"markdown","source":"# Chi-Squared Test of Independence"},{"metadata":{},"cell_type":"markdown","source":"[Independence](https://en.wikipedia.org/wiki/Independence_(probability_theory)) is a key concept in probability that describes a situation where knowing the value of one variable tells you nothing about the value of another. For instance, the month you were born probably doesn't tell you anything about which web browser you use, so we'd expect birth month and browser preference to be independent. On the other hand, your month of birth might be related to whether you excelled at sports in school, so month of birth and sports performance might not be independent.\n\nThe chi-squared test of independence tests whether two categorical variables are independent. The test of independence is commonly used to determine whether variables like education, political views and other preferences vary based on demographic factors like gender, race and religion. Let's generate some fake voter polling data and perform a test of independence:"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(10)\n\n# Sample data randomly at fixed probabilities\nvoter_race = np.random.choice(a= [\"asian\",\"black\",\"hispanic\",\"other\",\"white\"],\n                              p = [0.05, 0.15 ,0.25, 0.05, 0.5],\n                              size=1000)\n\n# Sample data randomly at fixed probabilities\nvoter_party = np.random.choice(a= [\"democrat\",\"independent\",\"republican\"],\n                              p = [0.4, 0.2, 0.4],\n                              size=1000)\n\nvoters = pd.DataFrame({\"race\":voter_race, \n                       \"party\":voter_party})\n\nvoter_tab = pd.crosstab(voters.race, voters.party, margins = True)\n\nvoter_tab.columns = [\"democrat\",\"independent\",\"republican\",\"row_totals\"]\n\nvoter_tab.index = [\"asian\",\"black\",\"hispanic\",\"other\",\"white\",\"col_totals\"]\n\nobserved = voter_tab.iloc[0:5,0:3]   # Get table without totals for later use\nvoter_tab","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"            democrat  independent  republican  row_totals\nasian             21            7          32          60\nblack             65           25          64         154\nhispanic         107           50          94         251\nother             15            8          15          38\nwhite            189           96         212         497\ncol_totals       397          186         417        1000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>democrat</th>\n      <th>independent</th>\n      <th>republican</th>\n      <th>row_totals</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>asian</th>\n      <td>21</td>\n      <td>7</td>\n      <td>32</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>black</th>\n      <td>65</td>\n      <td>25</td>\n      <td>64</td>\n      <td>154</td>\n    </tr>\n    <tr>\n      <th>hispanic</th>\n      <td>107</td>\n      <td>50</td>\n      <td>94</td>\n      <td>251</td>\n    </tr>\n    <tr>\n      <th>other</th>\n      <td>15</td>\n      <td>8</td>\n      <td>15</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>white</th>\n      <td>189</td>\n      <td>96</td>\n      <td>212</td>\n      <td>497</td>\n    </tr>\n    <tr>\n      <th>col_totals</th>\n      <td>397</td>\n      <td>186</td>\n      <td>417</td>\n      <td>1000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Note that we did not use the race data to inform our generation of the party data so the variables are independent.\n\nFor a test of independence, we use the same chi-squared formula that we used for the goodness-of-fit test. The main difference is we have to calculate the expected counts of each cell in a 2-dimensional table instead of a 1-dimensional table. To get the expected count for a cell, multiply the row total for that cell by the column total for that cell and then divide by the total number of observations. We can quickly get the expected counts for all cells in the table by taking the row totals and column totals of the table, performing an outer product on them with the np.outer() function and dividing by the number of observations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"expected =  np.outer(voter_tab[\"row_totals\"][0:5],\n                     voter_tab.loc[\"col_totals\"][0:3]) / 1000\n\nexpected = pd.DataFrame(expected)\n\nexpected.columns = [\"democrat\",\"independent\",\"republican\"]\nexpected.index = [\"asian\",\"black\",\"hispanic\",\"other\",\"white\"]\n\nexpected","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"          democrat  independent  republican\nasian       23.820       11.160      25.020\nblack       61.138       28.644      64.218\nhispanic    99.647       46.686     104.667\nother       15.086        7.068      15.846\nwhite      197.309       92.442     207.249","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>democrat</th>\n      <th>independent</th>\n      <th>republican</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>asian</th>\n      <td>23.820</td>\n      <td>11.160</td>\n      <td>25.020</td>\n    </tr>\n    <tr>\n      <th>black</th>\n      <td>61.138</td>\n      <td>28.644</td>\n      <td>64.218</td>\n    </tr>\n    <tr>\n      <th>hispanic</th>\n      <td>99.647</td>\n      <td>46.686</td>\n      <td>104.667</td>\n    </tr>\n    <tr>\n      <th>other</th>\n      <td>15.086</td>\n      <td>7.068</td>\n      <td>15.846</td>\n    </tr>\n    <tr>\n      <th>white</th>\n      <td>197.309</td>\n      <td>92.442</td>\n      <td>207.249</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now we can follow the same steps we took before to calculate the chi-square statistic, the critical value and the p-value:"},{"metadata":{"trusted":true},"cell_type":"code","source":"chi_squared_stat = (((observed-expected)**2)/expected).sum().sum()\n\nprint(chi_squared_stat)","execution_count":8,"outputs":[{"output_type":"stream","text":"7.169321280162059\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"*Note: We call .sum() twice: once to get the column sums and a second time to add the column sums together, returning the sum of the entire 2D table.*"},{"metadata":{"trusted":true},"cell_type":"code","source":"crit = stats.chi2.ppf(q = 0.95, # Find the critical value for 95% confidence*\n                      df = 8)   # *\n\nprint(\"Critical value\")\nprint(crit)\n\np_value = 1 - stats.chi2.cdf(x=chi_squared_stat,  # Find the p-value\n                             df=8)\nprint(\"P value\")\nprint(p_value)","execution_count":9,"outputs":[{"output_type":"stream","text":"Critical value\n15.50731305586545\nP value\n0.518479392948842\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"*Note: The degrees of freedom for a test of independence equals the product of the number of categories in each variable minus 1. In this case we have a 5x3 table so df = 4x2 = 8.*\n\nAs with the goodness-of-fit test, we can use scipy to conduct a test of independence quickly. Use stats.chi2_contingency() function to conduct a test of independence automatically given a frequency table of observed counts:"},{"metadata":{"trusted":true},"cell_type":"code","source":"stats.chi2_contingency(observed= observed)","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(7.169321280162059, 0.518479392948842, 8, array([[ 23.82 ,  11.16 ,  25.02 ],\n        [ 61.138,  28.644,  64.218],\n        [ 99.647,  46.686, 104.667],\n        [ 15.086,   7.068,  15.846],\n        [197.309,  92.442, 207.249]]))"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The output shows the chi-square statistic, the p-value and the degrees of freedom followed by the expected counts.\n\nAs expected, given the high p-value, the test result does not detect a significant relationship between the variables."},{"metadata":{},"cell_type":"markdown","source":"# Wrap Up"},{"metadata":{},"cell_type":"markdown","source":"Chi-squared tests provide a way to investigate differences in the distributions of categorical variables with the same categories and the dependence between categorical variables. In the next lesson, we'll learn about a third statistical inference test, the analysis of variance, that lets us compare several sample means at the same time."},{"metadata":{},"cell_type":"markdown","source":"# Next Lesson: [Python for Data 26: ANOVA](https://www.kaggle.com/hamelg/python-for-data-26-ANOVA)\n[back to index](https://www.kaggle.com/hamelg/python-for-data-analysis-index)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}