{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is the submission v10 of the classic titanic dataset ML model\n\n**This notebook include the following techniques:**\n* Data Imputation using MICE\n* label Encoding\n* Feature Scaling\n* Hyperparameter tuning\n* Feature Selection\n* Ensemble RandomForest Model\n\nWhat it does not include:\n* Extensive EDA - as most other notebooks on the titanic dataset has extensive graphs and analysis"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport numpy as np \nimport pandas as pd \nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntrain_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, both the train and test sets have missing values. These will be dealt with an imputer model later"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Name, Ticket and Cabin features are dropped as they are contain unique values, which do not provide any valuable information on the survived/not survived classification"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train_data['Survived']\nx_train = train_data.drop(['Survived','Name' ,'Ticket', 'Cabin'], axis=1)\nids = test_data['PassengerId']\nx_test = test_data.drop(['Name' ,'Ticket', 'Cabin'], axis=1)\nx_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**LABEL ENCODING**\n\nLabel Encoding the categorical features using sklearn LabelEncoder class. Learn more about the class here https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"labelEncoder1 = LabelEncoder()\nx_train['Sex'] = labelEncoder1.fit_transform(x_train['Sex'])\nx_test['Sex'] = labelEncoder1.transform(x_test['Sex'])\n\nlabelEncoder2 = LabelEncoder()\nx_train['Embarked'] = labelEncoder2.fit_transform(x_train['Embarked'])\nx_test['Embarked'] = labelEncoder2.transform(x_test['Embarked'])\nx_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATA SCALING**\n\n**MINMAX SCALER**\n\nA normalization technique is required to scale the continuous features to a range. For this the minmax scaler has been used. Learn more about min max scaler here https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"minmax = MinMaxScaler()\nx_train[[\"Age\", \"Fare\"]] = minmax.fit_transform(x_train[[\"Age\", \"Fare\"]])\nx_test[[\"Age\", \"Fare\"]] = minmax.fit_transform(x_test[[\"Age\", \"Fare\"]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**DATA IMPUTATION**\n\n**MICE IMPUTER**\n\nAn imputer is used to fill the missing values in a dataset. I favoured the MICE technique as it takes into account the other features in the dataset in order to fill the missing values of a particular feature. It helps in reducing the anomlaies that may occur due to the univariate imputation techiques. Learn more about MICE here https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html"},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer = IterativeImputer(random_state=42, verbose=1)\ntrain_imputed = pd.DataFrame(imputer.fit_transform(x_train), columns = ['PassengerId','Pclass', 'Sex' ,'Age', \n                                                                        'SibSp', 'Parch', 'Fare','Embarked'])\n\ntest_imputed = pd.DataFrame(imputer.transform(x_test), columns = ['PassengerId','Pclass', 'Sex' ,'Age', 'SibSp', \n                                                                  'Parch', 'Fare', 'Embarked'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Hyperparameter Tuning**\n\nIt is used to find the best subset of parameters to find the best model by using all provided combinations of parameters. GridSearch can be used for this purpose"},{"metadata":{"trusted":true},"cell_type":"code","source":"run_gs = False\n\nif run_gs:\n    parameter_grid = {\n                 'max_depth' : [2, 4, 6],\n                 'n_estimators': [100, 50],\n                 'criterion' : ['entropy', 'gini'],\n                 'min_samples_split': [2, 4, 6],\n                 'min_samples_leaf': [1, 3, 6]\n                 }\n    forest = RandomForestClassifier()\n    cross_validation = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    grid_search = GridSearchCV(forest,\n                               scoring='accuracy',\n                               param_grid=parameter_grid,\n                               cv=cross_validation,\n                               verbose=1\n                              )\n\n    grid_search.fit(train_imputed, y_train)\n    model = grid_search\n    parameters = grid_search.best_params_\n\n    print('Best score: {}'.format(grid_search.best_score_))\n    print('Best parameters: {}'.format(grid_search.best_params_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Engineering**\n\nFeature Engineering is used to select features based on statistic or wrapper methods to find the subset of features that can improve the model performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=12)\n\nestimator = RandomForestClassifier(max_depth=6, criterion='gini' , min_samples_leaf=1, min_samples_split=4, \n                             n_estimators=50, bootstrap=True, random_state=42)\nselector = RFECV(estimator, step=1, cv=cv, min_features_to_select=1)\nselector = selector.fit(train_imputed, y_train)\nselector.support_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_imputed = train_imputed.drop(['Parch', 'Embarked'], axis=1)\ntest_imputed = test_imputed.drop(['Parch', 'Embarked'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RandomForestClassifier(max_depth=6, criterion='gini' , min_samples_leaf=1, min_samples_split=4, \n                             n_estimators=50, bootstrap=True, random_state=42)\nclf.fit(train_imputed, y_train)\npredictions = clf.predict(test_imputed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}