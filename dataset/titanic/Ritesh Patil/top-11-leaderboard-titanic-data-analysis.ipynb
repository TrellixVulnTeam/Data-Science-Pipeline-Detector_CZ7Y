{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Preamble\nI'd appreciate any feedback you have to give. I'm still not sure on which models really need data to be setup numerically with a gaussian distribution or whether it's okay to have ordinal data encoded as integers, but here's my first attempt at a notebook. Enjoy! Upvote if possible!"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.options.mode.chained_assignment = None \n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/titanic/train.csv\", index_col=\"PassengerId\")\ntest_data = pd.read_csv(\"../input/titanic/test.csv\", index_col=\"PassengerId\")","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"## First step is to load up the data and describe the columns and check for differences."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"print(train_data.info())\nprint(train_data.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"print(test_data.info())\nprint(test_data.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"In the training set we are missing 177 ages, 2 embarked and 687 cabins.\nIn the test set we're missing 86 ages, 1 fare and 327 cabins.\n\nWith a lot of missing cabins it might make sense to drop the column but we will keep it for now.\n\n## Visualising the Data\nI want to see how each of the variables is associated with survival. Lets go in order:\n\n### 1) Pclass:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"print(train_data[\"Pclass\"].unique())\ntrain_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"There are three classes (1, 2 and 3) representing first, second or third class tickets on the boat.\n\nIt seems like the passenger's class has a strong association with survival, with the higher class passengers having a higher survival rate.\nIt makes sense to include this in the model, likely without much change.\n\n### 2) Name:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"print(train_data[\"Name\"])","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"I can think of a few things we could do with the names.\n\nFirstly, we could match up surnames to group families together.\nI could imagine that whole families either survived or died together.\n\nSecondly, we can get the titles of the names. As well as there being common ones such as\nMr and Miss, it seems like there are rare/unique ones such as Rev (reverend). If\nsomeone is important enough to have their own title they might have been more likely to survive."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data.Name[1].split()","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"I'm still pretty new to python so I'm not sure what the cannonical way of doing this is, but using\na string split seems like the way to go.\n\nAfter fiddling around with google, I think I want to use the .assign method for a pandas dataframe.\n\nIf I split by comma, the first and second entry will give the family name and title respectively."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data = train_data.assign(fname = train_data.Name.str.split(\",\").str[0])\ntrain_data[\"title\"] = pd.Series([i.split(\",\")[1].split(\".\")[0].strip() for i in train_data.Name], index=train_data.index)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"I think we can drop the name columns now as we won't need it.\nWe'll also need to repeat the above for the test set.\n\n\n(Edit: I originally didn't have the index=train_data.index and all of my pd.Series list comprehensions were coming up\none value short. The joys of 0 indexing vs 1 indexing!)"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"test_data = test_data.assign(fname = test_data.Name.str.split(\",\").str[0])\ntest_data[\"title\"] = pd.Series([i.split(\",\")[1].split(\".\")[0].strip() for i in test_data.Name], index=test_data.index)\ntrain_data.drop(\"Name\", axis=1, inplace=True)\ntest_data.drop(\"Name\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"Now to look at what we've made:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"print(test_data.fname.nunique())\nprint(test_data.title.nunique())","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"ts = sns.countplot(x=\"title\",data=train_data)\nts = plt.setp(ts.get_xticklabels(), rotation=90)\nprint(train_data[\"title\"].unique())\nprint(test_data[\"title\"].unique())\nother_titles = [title\n                for title in train_data[\"title\"]\n                if title not in [\"Mr\", \"Miss\", \"Mme\", \"Mlle\", \"Mrs\", \"Ms\"]]\nother_titles.append(\"Dona\")","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"There are a lot of uniques so  I think it makes sense to group them.\n\n#### Titles:\nFor now we will stick to headings that representing male, female, child and other.\nI'll then encode them as numerical.\nI will use the pandas dataframe replace and map functions for this:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data[\"title\"] = train_data['title'].replace(other_titles, 'Other')\ntrain_data[\"title\"] = train_data[\"title\"].map({\"Mr\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Master\":2, \"Other\":3})\ntest_data[\"title\"] = test_data['title'].replace(other_titles, 'Other')\ntest_data[\"title\"] = test_data[\"title\"].map({\"Mr\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Master\":2, \"Other\":3})","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"print(train_data.title)\nprint(test_data.title.isna().sum()) # No NaNs left","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\noh = OneHotEncoder(handle_unknown=\"ignore\", sparse = False)\n\ntrain_data = train_data.join(pd.DataFrame(oh.fit_transform(train_data[[\"fname\", \"title\"]]), index = train_data.index))\ntest_data = test_data.join(pd.DataFrame(oh.transform(test_data[[\"fname\", \"title\"]]), index = test_data.index))\ntrain_data.drop(\"fname\", axis = 1, inplace = True)\ntest_data.drop(\"fname\", axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"### 3) Sex:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"print(train_data[\"Sex\"].unique())\ntrain_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"We have two labels for Sex, with females having a much higher survival rate.\nIt makes sense to include Sex in the model. It is possible that we could use sex to create a new feature by combining\nit with other features. For example, what about Sex and Pclass that we looked at earlier?\n\nAcross all males and females, females have a much higher survival rate. But what if wealthy males have a higher survival\nthan poor females? It might make sense to segment this out explicity."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"interactions = train_data.assign(sex_class = train_data['Sex'] + \"_\" + train_data['Pclass'].astype(\"str\"))\ninteractions[['sex_class', 'Survived']].groupby(['sex_class'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"It certainly seems like this interaction feature adds something...\n\nAs I'm still new to this I don't yet know if the models will pick up this interaction without me\nexplicitly adding it as a feature. If I  DO include this column, it will be pretty highly associated with\nboth sex and class so again I'm not sure if that is something that can interfere with modeling.\n\nFor now, my ignorance will let me add it to the dataset and deal with any issues that arise later on."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data = train_data.assign(sex_class = train_data['Sex'] + \"_\" + train_data['Pclass'].astype(\"str\"))\ntest_data = test_data.assign(sex_class = test_data['Sex'] + \"_\" + test_data['Pclass'].astype(\"str\"))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"Something else that just stood out to me is that I'm not quite sure about how important encoding variables is.\nI've read some places that many models need everything to be encoded as numbers.\n\nThis seems straight forward but the more I think about it, the more confused I get.\nTake Pclass for example. This is encoded numerically and I'm pretty sure it most models would\nhappily take it and not throw out any errors. But if it's left as is, it would be treated the same as\nsomething like Age. While Pclass is ordinal, and having it encoded as 1, 2 and 3 doesn't seem too outrageous,\nI have an uneasy feeling about encoding something with discrete levels the same as a continguous variable (like Age).\n\nI don't know enough about machine learning to actually justify this feeling but just in case I will encode\nanything discrete using dummy variables/one-hot encoding.\n"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data = train_data.join(pd.get_dummies(train_data['Pclass'], prefix=\"Pclass\"))\ntest_data = test_data.join(pd.get_dummies(test_data['Pclass'], prefix=\"Pclass\"))","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"While I'm at it, I'll encode it and Sex as numeric using the map method."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data[\"Sex\"] = train_data[\"Sex\"].map({\"female\":0, \"male\":1})\ntest_data[\"Sex\"] = test_data[\"Sex\"].map({\"female\":0, \"male\":1})","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data[\"sex_class\"] = train_data[\"sex_class\"].map({\"female_1\":0, \"female_2\":1, \"female_3\":2, \"male_1\":4, \"male_2\":5, \"male_3\":6})\ntest_data[\"sex_class\"] = test_data[\"sex_class\"].map({\"female_1\":0, \"female_2\":1, \"female_3\":2, \"male_1\":4, \"male_2\":5, \"male_3\":6})","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"### 4) Age\n\nFirst thing's first, let's look at the distribution of age and see if there is any association with survival."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train_data, col='Survived')\ng = g.map(sns.distplot, \"Age\")","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"First, there are some missing values that need to be dealt with.\nThere are (at least) three ways we can deal with this, each one being slightly more effort.\n\n1) We can just drop the rows with missing data. While this might be tempting, dropping a row with around 14 other entries just because of one missing value doesn't sound like the brightest idea.\n\n2) We can replace the missing data with the average age (whether it's median/mode/mean) of the data set. This would be a good first pass method and it would let us get the models up and running.\n\n3) We can replace the missing data with the average from similar passengers. For example, if we're missing the age of a 1st class passenger, who is female, who embarked from C etc. we could substitute in the age of other passengers who fit that description."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"def find_similar_passengers(id, dataset):\n    subset = dataset[(dataset.title == dataset.title[id]) &\n                    (dataset.Pclass == dataset.Pclass[id])]\n\n    if subset[\"Age\"].mean() == \"NaN\":\n        subset = dataset[(dataset[\"sex_class\"] == dataset.iloc[id][\"sex_class\"])]\n\n    if subset[\"Age\"].mean() == \"NaN\":\n        subset = dataset[(dataset[\"sex\"] == dataset.iloc[id][\"sex\"])]\n\n    age = subset[\"Age\"].mean()\n    return age","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"no_ages = train_data[train_data[\"Age\"].isna()].index\nfor pid in no_ages:\n    train_data.Age[pid] = find_similar_passengers(pid, train_data)\n\nno_ages_test = test_data[test_data[\"Age\"].isna()].index\nfor pid2 in no_ages_test:\n    test_data.Age[pid2] = find_similar_passengers(pid2, test_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now that the missing data is filled in, we can start to reorganise the Age column to make it easier for a model to \"see\" what we want it to, namely that children have a much higher survival rate and the elderly have a much lower. I think segmenting them into groups of <5, 5-65 and >65 might be a good first pass.\n\nAfter yet MORE goolging, pandas has a .cut function to replace a range of values with new labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"age_group\"] =  pd.cut(train_data[\"Age\"], bins=[0,5,65,100], labels=[0,1,2]).astype(\"int64\")\ntest_data[\"age_group\"] = pd.cut(test_data[\"Age\"], bins=[0,5,65,100], labels=[0,1,2]).astype(\"int64\")","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"### 5 & 6) SibSp and Parch:\n\nAs these both relate to family size it's probably best to tackle them together.\n\nSibSp: The number of siblings or spouses aboard the titanic.\n\nParch: The number of parents/children aboard the titanic."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"Both stories tell a similar story, that smaller families tended to survive more than larger families."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data[\"fsize\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1\ntest_data[\"fsize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data[['fsize', 'Survived']].groupby(['fsize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"This looks okay. Small families (4 or less) survived better than people who were alone or in bigger families, we can throw this in the model.\n\n### 7) Ticket:\n\nLet's take a look at what values tickets take on:"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"print(train_data.Ticket.nunique())\nprint(train_data.Ticket.tail())","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"They seem to be numbers, with some having letter prefixes. There are only 681 unique ones in the training dataset\nand with no missing values, it means that some tickets have multiple people on them.\nI'll do the same trick as with the family name and titles, use string split to separate prefixes."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data[\"ticket_prefix\"] = pd.Series([len(i.split()) > 1 for i in train_data.Ticket], index=train_data.index)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data[['ticket_prefix', 'Survived']].groupby(['ticket_prefix'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data.drop(\"ticket_prefix\", axis=1, inplace=True)\ntrain_data.drop(\"Ticket\", axis=1, inplace=True)\ntest_data.drop(\"Ticket\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"### 8) Fare"},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train_data, col='Survived')\ng = g.map(sns.distplot, \"Fare\")","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%% md\n"}},"cell_type":"markdown","source":"While the picture isn't super clear, you can see that survivors had more expensive fares and a wider spread of fare prices.\nThere is at least one outlier with a fare of >500 so dropping it.\n\nApart from that the data is pretty skewed. Take a log transformation to reduce the skew and to decrease the massive range in fares."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"import numpy as np\ntrain_data[\"Fare\"] = train_data[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\ntest_data[\"Fare\"] = test_data[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\ng = sns.FacetGrid(train_data, col='Survived')\ng = g.map(sns.distplot, \"Fare\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9) Cabin:\n\nFrom earlier we saw that many cabin entries were missing. We could probably do something to impute the data but for now drop it."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data.drop(\"Cabin\", axis=1, inplace=True)\ntest_data.drop(\"Cabin\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 10) Embarked:\n\nNot much to do here, theres a few missing values which we can fill in."},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(\"S\")\ntrain_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(\"S\")\nprint(train_data.Embarked.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data = train_data.join(pd.get_dummies(train_data['Embarked'], prefix=\"Embarked_\"))\ntest_data = test_data.join(pd.get_dummies(test_data['Embarked'], prefix=\"Embarked_\"))\n#train_data[\"Embarked\"] = train_data[\"Embarked\"].map({\"S\":0, \"Q\":1, \"C\":2})\n#test_data[\"Embarked\"] = test_data[\"Embarked\"].map({\"S\":0, \"Q\":1, \"C\":2})","execution_count":null,"outputs":[]},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"train_data.drop(\"Embarked\", axis=1, inplace=True)\ntest_data.drop(\"Embarked\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling\n\nI am going to try my hand at a few different models. Starting with the very simple (linear regression/classifier) and gradually moving up in complexity."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\ntrain_y = train_data[\"Survived\"]\ntrain_data.drop(\"Survived\", axis=1, inplace=True)\n\nscoring_method = \"f1\"\n\ntrain_scaled = ss.fit_transform(train_data)\ntest_scaled = ss.transform(test_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1) Logistic Regression:\n\nFirst I will split the dataframes up into the independant variables (usually denoted as the matrix X) and the dependant variable (the vector y). I'll do one last check to make sure I have no NAs. "},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"print(train_data.isna().sum())\nprint(test_data.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm going to use a function from the model_selection module in sklearn. This lets me supply a grid of possible values for the parameters and it will test all possible combinations, storing the best result. As I don't want this \"best result\" to be overfitted, I'm going to set the cross-validate (cv) parameter to 8, so it will do 8-fold validation. "},{"metadata":{"pycharm":{"name":"#%%\n"},"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nmodel = LogisticRegression(random_state=10, max_iter = 1000)\nlogit_params = {\n    \"C\": [1, 3, 10, 20, 30, 40],\n    \"solver\": [\"lbfgs\", \"liblinear\"]\n    \n}\nlogit_gs = GridSearchCV(model, logit_params, scoring=\"f1\", cv = 5, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit_gs.fit(train_data, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(logit_gs.best_params_)\nprint(logit_gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2) Random Forest:\n\nNext I'm going to try to implement a random forest model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier()\n\nrf_params ={\n    'bootstrap': [True, False],\n    'max_depth': [10, None],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [1, 2, 4],\n    'min_samples_split': [2, 5, 10],\n    'n_estimators': [5, 10, 15, 20, 25, 30]}\n\nrf_gs = GridSearchCV(rf_model, rf_params, scoring=scoring_method, cv=8, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_gs.fit(train_data, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(rf_gs.best_params_)\nprint(rf_gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3) SVM"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nsvc_model = SVC()\n\ntest_parameters = {\n    \"C\": [1, 3, 10, 30, 100],\n    \"kernel\": [\"linear\", \"poly\", \"rbf\" , \"sigmoid\"],\n}\nsvc_gs = GridSearchCV(svc_model, test_parameters, scoring=\"f1\", cv=5, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_gs.fit(train_scaled, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(svc_gs.best_params_)\nprint(svc_gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4) Light Gradient Boosting:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nlgb_model = LGBMClassifier()\ntest_parameters = {\n    \"n_estimators\": [int(x) for x in np.linspace(5, 30, 6)],\n    \"reg_alpha\": [0, 0.75, 1, 1.25],\n    \"learning_rate\": [0.5, 0.4, 0.35, 0.3, 0.25, 0.2],\n    \"subsample\": [0.5, 0.75, 1]\n}\nlgb_gs = GridSearchCV(lgb_model, test_parameters, scoring=scoring_method, cv=8, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_gs.fit(train_data, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(lgb_gs.best_params_)\nprint(lgb_gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Comparing models\n\nI do eventually want to include a short bit on comparing the models. From what I've read, these different models are probably placing different levels of importance on different features/variables. I think the key to a good ensembler/voter is to have models that have different predictions. Of course if all the models predict the same thing, there isn't much point in having them \"vote\". In the next iteration I will add models that pick out features differently (maybe some linear models, gradient based models and a neural network). I'll see how their predictions compare and pick a few good but slightly different models to ensemble."},{"metadata":{},"cell_type":"markdown","source":"### Ensembling/Voting\nNow that I have a few models, I'm going to use a voting classifier to use the three above models to make an overall prediction. I'm leaving out the logistic regression for the time being as it the model was fitted with the scaled dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nensemble_model = VotingClassifier(estimators=[\n    (\"logit\", logit_gs.best_estimator_),\n    (\"rf\", rf_gs.best_estimator_),\n    (\"svc\", svc_gs.best_estimator_),\n    (\"lgb\", lgb_gs.best_estimator_),\n], voting = \"hard\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_model.fit(train_data, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble_model.score(train_data, train_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = ensemble_model.predict(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'PassengerId': test_data.index,\n                       'Survived': preds})\n\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}