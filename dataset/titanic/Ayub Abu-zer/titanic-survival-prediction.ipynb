{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Titanic Survival Prediction"},{"metadata":{},"cell_type":"markdown","source":"### 1. Background"},{"metadata":{},"cell_type":"markdown","source":"The sinking of the [Titanic](https://www.kaggle.com/c/titanic) is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others."},{"metadata":{},"cell_type":"markdown","source":"### 2. Problem"},{"metadata":{},"cell_type":"markdown","source":"Our target is to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc)."},{"metadata":{},"cell_type":"markdown","source":"### 3. Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nimport re\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, learning_curve, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import VotingClassifier\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. Gathering data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5. EDA - Exploratory data analysis"},{"metadata":{},"cell_type":"markdown","source":"take a brief look to the data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset Columns :\n\n* PassengerId - Unique ID of the passenger\n* Survived - Survived (1) or died (0)\n* Pclass - Passenger's class (1st, 2nd, or 3rd)\n* Name - Passenger's name\n* Sex - Passenger's sex\n* Age - Passenger's age\n* SibSp - Number of siblings/spouses aboard the Titanic\n* Parch - Number of parents/children aboard the Titanic\n* Ticket - Ticket number\n* Fare - Fare paid for ticket\n* Cabin - Cabin number\n* Embarked - Where the passenger got on the ship (C - Cherbourg, S - Southampton, Q = Queenstown)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"some statistic info"},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['Survived'] == 1]['Survived'].count()/train['Survived'].count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"38% survived"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Survived', data=train, hue='Sex')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that more females had been survived, and more males had been not survived"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[[\"Sex\", \"Survived\"]].groupby('Sex', as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"74% of the women had been survived, and only 18% of the men had been survived"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['Fare'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that average of fare paid for ticket is about 30"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Fare'].mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"average fare is 32.2"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[['Pclass', 'Survived']].groupby('Pclass', as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"more than 62% of the passenger that were in class 1 had been survived."},{"metadata":{"trusted":true},"cell_type":"code","source":"# box plot of age for pclass\nplt.figure(figsize=(12, 8))\nsns.boxplot(x='Pclass', y='Age', hue='Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['Age'].dropna())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that the average age is about 30"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Age'].dropna().mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train, col='Survived', row='Pclass')\ng.map(plt.hist, 'Age', bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Embarked', hue='Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.countplot(x='Parch', hue='Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.countplot(x='SibSp', hue='Survived', data=train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that people with more siblings or family have less probability to survived."},{"metadata":{},"cell_type":"markdown","source":" ### 6. Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"#### 6.1 Outliers"},{"metadata":{},"cell_type":"markdown","source":"First of all i will remove outliers from numeric features, that might badly influnce the machine learning process."},{"metadata":{"trusted":true},"cell_type":"code","source":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show the outliers rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[Outliers_to_drop] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_len = len(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let us combine the train and test data sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = pd.concat([train, test], ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"let us remove the PassengerId column wich will not add anything to the machine learning process"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop('PassengerId', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 6.2 Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(combined.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(train.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(test.isnull())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"process Embarked with the most fequent embarked"},{"metadata":{"trusted":true},"cell_type":"code","source":"embarked_mode = train['Embarked'].mode()[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embarked_mode","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Embarked'] = combined['Embarked'].fillna(embarked_mode)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"prcoess cabin - update cabin to be the first letter of cabin"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Cabin'].dropna().apply(str).apply(lambda x: x[0]).unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"fill missing Cabin with 'U' as 'Unknown'"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Cabin'].fillna('U', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Cabin'] = combined['Cabin'].apply(str).apply(lambda x : x[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(combined[\"Cabin\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.where(np.isnan(combined['Fare']))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the row is 1033, lets get the mean of Fare according to the 152 row's features"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.iloc[1033]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train[[\"Fare\",\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot=True, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that Fare is negativly correlated with Pclass"},{"metadata":{"trusted":true},"cell_type":"code","source":"train[(train['Pclass'] == 3)]['Fare'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Fare'][1033] = train[(train['Pclass'] == 3)]['Fare'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# combined[\"Fare\"] = combined[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7. Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"process name"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Name'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"try to grab the title of each passenger."},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Title'] = combined['Name'].apply(lambda x : (x.split(', ')[1].split('.')[0]))\ncombined.drop('Name', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Title'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that  Dr, Rev, Major, Col, Mlle, Capt, Don, Jonkheer, the Countess, Ms, Sir, Lady, Mme    \nis really rare titles, so i will map them as Rare"},{"metadata":{"trusted":true},"cell_type":"code","source":"titles_map = {\n 'Capt' : 'Rare',\n 'Col' : 'Rare',\n 'Don': 'Rare',\n 'Dona': 'Rare',\n 'Dr' : 'Rare',\n 'Jonkheer' :'Rare' ,\n 'Lady': 'Rare',\n 'Major': 'Rare',\n 'Master': 'Master',\n 'Miss' : 'Miss',\n 'Mlle' : 'Rare',\n 'Mme': 'Rare',\n 'Mr': 'Mr',\n 'Mrs': 'Mrs',\n 'Ms': 'Rare',\n 'Rev': 'Rare',\n 'Sir': 'Rare',\n 'the Countess': 'Rare'\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Title'] = combined['Title'].map(titles_map)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"process age"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot=True, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that the Age is negativly correlated to Parch, Pclass, Sibsp.\nso let us let us fill missing age rows, with the median of the ages based on gender and class"},{"metadata":{"trusted":true},"cell_type":"code","source":"def impute_age(row):\n    pclass = row['Pclass']\n    parch = row['Parch']\n    sibsp = row['SibSp']\n    age = row['Age']\n    if pd.isnull(age):\n        age_median = train['Age'].median()\n        similar_age =  train[(train['Pclass'] == pclass) & (train['Parch'] == parch)\n                       & (train['SibSp'] == sibsp)]['Age'].median()\n        if( similar_age > 0): return similar_age\n        else :  return age_median\n    else :return age","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Age'] = combined.apply(impute_age, axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"process parch and sbisp - create new columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Family_size'] = combined.apply(lambda row : 1 + (row['Parch'] + row['SibSp']), axis=1)\ncombined['Alone'] = combined.apply(lambda row : 1 if (row['Parch'] + row['SibSp']) == 0 else 0, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(x='Family_size' , data=combined, hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that passenger with small family (2-4) had more change to survived"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Small_family'] = combined.apply(lambda row : 1 if 2 <= (row['Family_size']) <= 4 else 0, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.drop(['Parch', 'SibSp'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"process ticket - get only prefix of the ticket unsing regex"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Ticket'] = combined['Ticket'].apply(lambda x : 'X' if x.isdigit() else x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combined['Ticket'] = combined['Ticket'].apply(lambda x : re.sub(\"[\\d\\.]\", \"\", x).split('/')[0].strip() if not x.isdigit() else x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7.1 One Hot Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined = pd.get_dummies(combined, columns = [\"Embarked\"], prefix=\"Em\")\n\ncombined = pd.get_dummies(combined, columns = [\"Cabin\"], prefix=\"Cb\")\n\ncombined = pd.get_dummies(combined, columns = [\"Title\"], prefix=\"Title\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 7.2 Label Encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"sex_encoder = LabelEncoder().fit(combined['Sex'])\ncombined['Sex'] = sex_encoder.transform(combined['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ticket_encoder = LabelEncoder().fit(combined['Ticket'])\ncombined['Ticket'] = ticket_encoder.transform(combined['Ticket'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum((combined.drop('Survived', axis=1).isnull()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"no empty data - we are good to go."},{"metadata":{},"cell_type":"markdown","source":"## 8. Model - Machine learning"},{"metadata":{"trusted":true},"cell_type":"code","source":"combined.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = combined[:train_len]\ntest = combined[train_len:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.drop('Survived', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum((test.isnull()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"test data is cleaned."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Survived'] = train['Survived'].astype(int) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = train.drop('Survived', axis=1)\ny = train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9. Model Evaluation"},{"metadata":{},"cell_type":"markdown","source":"#### 9.1 Cross Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nrandom_state = 42\n \nmodel_names = ['LogisticRegression', 'DecisionTreeClassifier', 'SVC', \n              'RandomForestClassifier', 'XGBClassifier', 'ExtraTreesClassifier'\n              , 'GradientBoostingClassifier','AdaBoostClassifier','GaussianNB']\nmodels = [ ('LogisticRegression',LogisticRegression(random_state=random_state)),\n          ('DecisionTreeClassifier', DecisionTreeClassifier(random_state=random_state)),\n          ('SVC', SVC(random_state=random_state)),\n          ('RandomForestClassifier',RandomForestClassifier(random_state=42)),\n          ('XGBClassifier',XGBClassifier(random_state=random_state)),\n          ('ExtraTreesClassifier',ExtraTreesClassifier(random_state=random_state)),\n          ('GradientBoostingClassifier',GradientBoostingClassifier(random_state=random_state)),\n          ('AdaBoostClassifier',AdaBoostClassifier(random_state=random_state)),\n          ('GaussianNB',GaussianNB())\n         ]\n\nmodel_accuracy = []\nfor k,model in models:\n    model.fit(X, y)\n    accuracy = cross_val_score(model, X_train, y_train, cv=10).mean()\n    model_accuracy.append(accuracy)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.concat([pd.Series(model_names), pd.Series(model_accuracy)], axis=1).sort_values(by=1, ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"i will choose XGBClassifier, GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier  to continue with the hyperparameter tuning"},{"metadata":{},"cell_type":"markdown","source":"#### 9.2 Hyperparameter tuning"},{"metadata":{},"cell_type":"markdown","source":"let's use the grid search to find the best estimator."},{"metadata":{"trusted":true},"cell_type":"code","source":"best_models=[]\n\nxgboot_param_grid = {\n     'n_estimators': [100,200,300],\n     'max_depth': [4, 6, 8],\n     'learning_rate': [.4, .45, .5, .55, .6],\n     'colsample_bytree': [.6, .7, .8, .9, 1]\n}\n\nada_param_grid = {\n 'n_estimators':[100,200,300],\n 'learning_rate' : [0.01,0.05,0.1,0.3,1],\n 'algorithm' : ['SAMME', 'SAMME.R']\n }\n\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300, 400],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10, 15],\n              \"min_samples_split\": [2, 3, 10, 15],\n              \"min_samples_leaf\": [1, 3, 10, 15],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,200,300, 400],\n              \"criterion\": [\"gini\"]}\n\n\nrf_param_grid  = { \n    'n_estimators': [100,200,300, 400],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\nlog_param_grid = {\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}\n\nsvv_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\nmodels = [ \n    ('AdaBoostClassifier',AdaBoostClassifier(), ada_param_grid),\n          ('XGBClassifier',XGBClassifier(), xgboot_param_grid),\n          ('GradientBoostingClassifier',GradientBoostingClassifier(), gb_param_grid),\n        ('RandomForestClassifier',RandomForestClassifier(), rf_param_grid),\n          ('ExtraTreesClassifier',ExtraTreesClassifier(), ex_param_grid),\n    ('SVC',SVC(probability=True), svv_param_grid),\n    ('LogisticRegression',LogisticRegression(), log_param_grid)\n         ]\n\n\nfor name, model, param in  models:\n    grid_search = GridSearchCV(model,\n                               scoring='accuracy',\n                               param_grid=param,\n                               cv=10,\n                               verbose=2,\n                               n_jobs=-1)\n    grid_search.fit(X, y)\n    print (name, ':', grid_search.best_score_, '\\n')\n    best_models.append(grid_search.best_estimator_)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\nfor model in best_models:\n    plot_learning_curve(model,model.__class__.__name__ + \" RF mearning curves\",X,y,cv=5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that XGBClassifier has underfiting\nRF, LogisticRegression, AdaBoost has well fiting."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n    '''\n    plot feature importances of a tree-based sklearn estimator\n    \n    Note: X_train and y_train are pandas DataFrames\n    \n    Note: Scikit-plot is a lovely package but I sometimes have issues\n              1. flexibility/extendibility\n              2. complicated models/datasets\n          But for many situations Scikit-plot is the way to go\n          see https://scikit-plot.readthedocs.io/en/latest/Quickstart.html\n    \n    Parameters\n    ----------\n        clf         (sklearn estimator) if not fitted, this routine will fit it\n        \n        X_train     (pandas DataFrame)\n        \n        y_train     (pandas DataFrame)  optional\n                                        required only if clf has not already been fitted \n        \n        top_n       (int)               Plot the top_n most-important features\n                                        Default: 10\n                                        \n        figsize     ((int,int))         The physical size of the plot\n                                        Default: (8,8)\n        \n        print_table (boolean)           If True, print out the table of feature importances\n                                        Default: False\n        \n    Returns\n    -------\n        the pandas dataframe with the features and their importance\n        \n    Author\n    ------\n        George Fisher\n    '''\n    \n    __name__ = \"plot_feature_importances\"\n    \n    \n    from xgboost.core     import XGBoostError\n    from lightgbm.sklearn import LightGBMError\n    \n    try: \n        if not hasattr(clf, 'feature_importances_'):\n            clf.fit(X_train.values, y_train.values.ravel())\n\n            if not hasattr(clf, 'feature_importances_'):\n                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n                                    format(clf.__class__.__name__))\n                \n    except (XGBoostError, LightGBMError, ValueError):\n        clf.fit(X_train.values, y_train.values.ravel())\n            \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table:\n        from IPython.display import display\n        print(\"Top {} features in descending order of importance\".format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for model in best_models:\n    try:\n        _ = plot_feature_importances(model, X_train, y_train, top_n=X.shape[1], title=model.__class__.__name__)\n    except AttributeError as e:\n        print(e)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 10. Model Ensembling"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = []\nfor model in best_models:\n    pred.append(pd.Series(model.predict(test), name=model.__class__.__name__))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = pd.DataFrame(pred).transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred.sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g= sns.heatmap(pred.corr(),annot=True, cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"we can see that AdaBosstClassifier, RandaomForest, ExtraTreeClassifier, LogisticRegression is  highly correlated together."},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = pd.read_csv('/kaggle/input/titanic/test.csv')['PassengerId']\nvotingC = VotingClassifier(estimators=[\n                                    ('ada', best_models[0]),\n                                       ('rf', best_models[3]),\n                                       ('ext', best_models[4]),\n                                       ('log', best_models[6]),\n                                      ], voting='soft', n_jobs=-1)\nvotingC.fit(X, y)\ntest_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"ensemble_prediction.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ids = pd.read_csv('/kaggle/input/titanic/test.csv')['PassengerId']\nvotingC = VotingClassifier(estimators=[\n                                    ('ada', best_models[0]),\n                                       ('rf', best_models[3]),\n                                       ('log', best_models[6]),\n                                      ], voting='soft', n_jobs=-1)\nvotingC.fit(X, y)\ntest_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"ensemble_prediction2.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ada_best = best_models[0]\nada_best.fit(X, y)\ntest_Survived = pd.Series(ada_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"Ada.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_best = best_models[1]\nxgb_best.fit(X, y)\ntest_Survived = pd.Series(xgb_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"XGB.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grb_best = best_models[2]\ngrb_best.fit(X, y)\ntest_Survived = pd.Series(grb_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"GRB.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rnf_best = best_models[3]\nrnf_best.fit(X, y)\ntest_Survived = pd.Series(rnf_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"RNF.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ext_best = best_models[4]\next_best.fit(X, y)\ntest_Survived = pd.Series(ext_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"EXT.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svc_best = best_models[5]\nsvc_best.fit(X, y)\ntest_Survived = pd.Series(svc_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"SVC.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"log_best = best_models[6]\nlog_best.fit(X, y)\ntest_Survived = pd.Series(log_best.predict(test), name=\"Survived\")\n\nresults = pd.concat([ids,test_Survived],axis=1)\n\nresults.to_csv(\"LOG.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}