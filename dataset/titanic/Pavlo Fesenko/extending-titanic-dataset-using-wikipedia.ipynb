{"cells":[{"metadata":{},"cell_type":"markdown","source":"Thanks for **UPVOTING** this kernel! Trying to become a Kernels Master. ðŸ¤˜\n\nCheck out my other cool projects:\n- [ðŸ’² Minimizing investment risk for high interest loans](https://www.kaggle.com/pavlofesenko/minimizing-investment-risk-for-high-interest-loans)\n- [ðŸ“Š Interactive Titanic dashboard using Bokeh](https://www.kaggle.com/pavlofesenko/interactive-titanic-dashboard-using-bokeh)\n- [ðŸ‘ª Titanic extended dataset (Kaggle + Wikipedia)](https://www.kaggle.com/pavlofesenko/titanic-extended)"},{"metadata":{"_uuid":"b80b37a46cde382106874822e51d8f2d5f4de1ae"},"cell_type":"markdown","source":"# 1. Introduction\n\nThis project was inspired by [this post of Salamati](https://www.kaggle.com/c/titanic/discussion/73133#441915) from the Kaggle forum:\n\n> Hello, I did a project on the Titanic Data. I need to merge the titanic data with some other data sets. I was looking for other features like Demographic of the lifeboats, passengers' employment category, and passenger nationality, etc to merge it to the Titanic based on the passenger ID. I was not able to find any data to merge with the Titanic so far. Do you think if these data sets or other data sets are available that I can add it to the Titanic based on the passenger ID. If not, is there any other data sets on the Kaggle that I can merge with other data sets based on the specific column. I have to merge 2 or three data sets based on the specific column and then do my analyses. I would very appreciate for any help/suggestion on this matter. Thank you,\n\nThe current [Kaggle Titanic dataset](https://www.kaggle.com/c/titanic/data) is based on the dataset that was assembled by Thomas E. Cason, an undergraduate research assistant of the University of Virginia (more info [here](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.html)). It was created using the data from the [*Encyclopedia Titanica*](https://www.encyclopedia-titanica.org/titanic-passenger-list/) available as of 2 August 1999. Note that the original Titanic dataset contained 3 extra features: `boat` (Lifeboat), `body` (Body Identification Number), `home.dest` (Home/Destination). The first 2 features were obviously not included in the Kaggle dataset since they immediately indicate if the person has survived or not. The last feature was not included probably due to a lot of missing values. It does, however, contain some interesting information such as home country that can be used to engineer new features, for example, nationality.\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/Wikipedia-logo-en-big.png\" width=\"200\" align=\"right\"/>\n\nBesides Kaggle, there is also the [Titanic passenger list on Wikipedia](https://en.wikipedia.org/wiki/Passengers_of_the_RMS_Titanic). It is divided in 3 tables according to the class and each of the tables contains the following columns: `Name`, `Age`, `Hometown`, `Boarded`, `Destination`, `Lifeboat`, `Body`. The survived passengers are highlighted with blue and the victims with white. This list was created using the data from the [*Encyclopedia Titanica*](https://www.encyclopedia-titanica.org/titanic-passenger-list/) that was retrieved in 2011 and from couple of other sources. The big advantage of the Wikipedia list is that it has almost no missing values. Therefore, it would be a great source to complement the existing Kaggle Titanic dataset.\n\nI also checked the [*Encyclopedia Titanica*](https://www.encyclopedia-titanica.org/titanic-passenger-list/) that has the most extensive database of the Titanic passengers untill now. Its drawback, however, is that the Encyclopedia Titanica doesn't allow to copy its materials for public use.\n\nTherefore, I decided to merge the Kaggle and Wikipedia databases and use the Encyclopedia Titanica only to distinguish the passengers with difficult names from the previous two databases. In this way I won't be violating the copyright of the Encyclopedia Titanica.\n\nThroughout this notebook I will be using only two Python libraries: `pandas` and `unidecode`. The latter is very convenient to convert special characters into ASCII equivalents (for example, it converts `Ã©` to `e`)."},{"metadata":{"trusted":true,"_uuid":"a216175b2099e4f1292ab7b26dc68f3e623ef3d1"},"cell_type":"code","source":"import pandas as pd\nfrom unidecode import unidecode","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e54fcf8664455e984790c107618b683800e32d1"},"cell_type":"markdown","source":"# 2. Importing Kaggle dataset\n\nI start by importing the training and testing datasets from the [Kaggle Titanic competition](https://www.kaggle.com/c/titanic/data)."},{"metadata":{"trusted":true,"_uuid":"9c898c833a82def6c574f74e47aae41d6ca1c06a"},"cell_type":"code","source":"train = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')\n\nkagg = pd.concat([train, test], sort=False)\nkagg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"086aac023b9150b7dad3efe3c9a2a768469e8973"},"cell_type":"markdown","source":"# 3. Importing Wikipedia dataset\n\nTo get the [Titanic passenger list on Wikipedia](https://en.wikipedia.org/wiki/Passengers_of_the_RMS_Titanic), I scrape it using the function `pd.read_html()`. It returns all tables from the web page as a list of DataFrames. There are 3 DataFrames that correspond to 3 passenger classes and are assigned to the variables `wiki1`, `wiki2`, `wiki3`. For reproducibility I used the permanent link to the [version of 18 February 2019](https://en.wikipedia.org/w/index.php?title=Passengers_of_the_RMS_Titanic&oldid=883859055) because Wikipedia constantly updates its content. The parameter `encoding='unicode'` is used here because some of the passenger names contain non-ASCII characters."},{"metadata":{"trusted":true,"_uuid":"302a648e2628fca5e0f16827f6a850f5fc037980"},"cell_type":"code","source":"wiki1 = pd.read_html('https://en.wikipedia.org/w/index.php?title=Passengers_of_the_RMS_Titanic&oldid=883859055', header=0, encoding='unicode')[1]\nwiki2 = pd.read_html('https://en.wikipedia.org/w/index.php?title=Passengers_of_the_RMS_Titanic&oldid=883859055', header=0, encoding='unicode')[2]\nwiki3 = pd.read_html('https://en.wikipedia.org/w/index.php?title=Passengers_of_the_RMS_Titanic&oldid=883859055', header=0, encoding='unicode')[3]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5117126ba132e75fce11769c2ad761aacd56c14f"},"cell_type":"markdown","source":"Let's have a look at the table for the 1st class (see below).\n\nNote that after some of the names there are rows starting with \"and &lt;profession&gt;, &lt;Title&gt; &lt;Name&gt; &lt;Surname&gt;\". These rows correspond to the servants who were travelling with some of the families from the 1st class.\n\nSometimes rows have a reference. For example, [59] says:\n\n> Though their employers travelled in first class, this servant was given second class accommodations, as their services were not needed while their employers were on board.\n\nNote also that `Body` contains some letters after the body number. These are references for the name of the ship that found the body. For example, \"MB\" means \"Mackay-Bennett\".\n\nFor the extended Titanic dataset I will leave everything as it is to preserve as much information as possible."},{"metadata":{"trusted":true,"_uuid":"a92966e82cb5984757ab3c800d10ac35982f2b2a"},"cell_type":"code","source":"wiki1.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ef7e81f1068dfc25e47e8be9143314664e51ca5"},"cell_type":"markdown","source":"I also add the column `Class` to the DataFrame `wiki1` and assign it to 1. This is not always true (see the reference 59 above) but it can always be taken into account later."},{"metadata":{"trusted":true,"_uuid":"c9b703f1c13994baea289796ef3ea481f353e6b9"},"cell_type":"code","source":"wiki1['Class'] = 1\nwiki1.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40bc8206c3f105479e41ebfa9362da3c6edc2ae0"},"cell_type":"markdown","source":"The table for the 2nd class (see below) has the same structure as the one for the 1st class. Note, however, that `Hometown` sometimes doesn't have a town and only the country is mentioned. Also, the countries are not always consistent. For example, in the table for the 1st class \"England\" is followed by \"UK\" (see row 3 above) but in the table for the 2nd class (see rows 3-4 below) only \"England\" is mentioned. The country is also sometimes missed in the column `Destination` (see, for example, row 4 below)."},{"metadata":{"trusted":true,"_uuid":"3093273cec5e05f4c2bb82b7241e6134753728b5"},"cell_type":"code","source":"wiki2.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d44db57074d4bb30ea1e48ae9086aba2e10f072c"},"cell_type":"markdown","source":"Just like with the previous DataFrame I add the column `Class` but assign it to 2."},{"metadata":{"trusted":true,"_uuid":"36d0ae9d68ab9a83ca0ac6539d43d808028ba036"},"cell_type":"code","source":"wiki2['Class'] = 2\nwiki2.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95d9c138376559e3ee2ec022c7f009ab3d64d28c"},"cell_type":"markdown","source":"The table for the 3rd class (see below) has an additional column `Home country` apart from the column `Hometown`."},{"metadata":{"trusted":true,"_uuid":"f5057ac8568c1d52e997edb82e53d17ded24e699"},"cell_type":"code","source":"wiki3.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9dd5a4ab360a12d60f406eb3f0edf1adbc8f909"},"cell_type":"markdown","source":"I join the columns `Hometown` and `Home country` so that all 3 tables have the same structure and can be later concatenated. Just like with the previous DataFrames I add the column `Class` but assign it to 3."},{"metadata":{"trusted":true,"_uuid":"1bb2d9f0376e11d7d81f7ccb952e006c3631c23d"},"cell_type":"code","source":"wiki3['Hometown'] = wiki3['Hometown'] + ', ' + wiki3['Home country']\nwiki3 = wiki3.drop('Home country', axis=1, errors='ignore')\n\nwiki3['Class'] = 3\n\nwiki3.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7e62f53129fd51f8340d7e11fae752b8a39bb4b"},"cell_type":"markdown","source":"Finally, I concatenate all 3 tables and for convenience reset the index using the parameter `ignore_index=True`."},{"metadata":{"trusted":true,"_uuid":"23d25a731c87e9ae71db8997b1ba8f26e5e98359"},"cell_type":"code","source":"wiki = pd.concat([wiki1, wiki2, wiki3], ignore_index=True)\nwiki.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"64d91d511733ef4d038d0a34dbb035f803720e0a"},"cell_type":"markdown","source":"# 4. Preparing Kaggle dataset for merge\n\nIn order to merge the Kaggle and Wikipedia datasets, I need a matching column(s), for example, `Name`. But if you compare the column `Name` in both datasets, you will see that the names are often spelled differently, for example, \"Eli**z**abeth\" on Wikipedia (see row 1 above) and \"Eli**s**abeth\" on Kaggle (see row 1 below). So `Name` is not a good matching column, however, I could use it to generate other matching columns. These could be, for example, the first 3 letters of a surname or the first 3 letters of a name. This should be precise enough for most of the cases and will hopefully avoid misspelling issues. In case of duplicates I could use the column `Age` to differentiate between them."},{"metadata":{"trusted":true,"_uuid":"a6d84b9110a855405177cda3fcae7329e28a0f04"},"cell_type":"code","source":"kagg.sort_values(['Pclass', 'Name']).reset_index(drop=True).head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8590c6b3ce7aa6a9836d04c127a168f2a168bd63"},"cell_type":"markdown","source":"Before I start extracting surname and name codes (the first 3 letters), note that in the Kaggle dataset the title of \"Mrs.\" is followed first by the name of the husband and only then in parentheses by the wife's actual name (see row 4 above). Therefore, I need to extract surname and name codes for passengers with the title \"Mrs.\" separately. For this purpose I use regular expressions (see more in the [Python regex documentation](https://docs.python.org/3/howto/regex.html)). The new DataFrame will be called `kagg_corr` and for convenience I sort it by `Pclass` and `Name`.\n\nThe rows that contain \"Mrs.\" in the column `Name` are matched using the first regular expression. Matched groups `(?P<Surname>.*)`, `(?P<Husband_name>.*)` and `(?P<Wife_name>.*)` are extracted as columns in the DataFrame `temp`. It is important not to leave any space between `(?P<Husband_name>.*)` and `\\((?P<Wife_name>.*)\\)` because sometimes the husband's name is missing and the matching won't work with the extra space between them.\n\nAfterwards all rows are matched using the second regular expression. This time the named groups for matching `(?P<Surname>.*)`, `(?P<Title>.*)` and `?P<Name>.*)` are extracted as columns in the DataFrame `temp2`. In this case the rows that contain \"Mrs.\" will be matched wrongly but it's ok because they have already been correctly matched using the previous regular expression.\n\nNote that the named groups `(?P<Husband_name>.*)` and `(?P<Title>.*)` can be simply replaced by `.*` since they are not used later on. Nevertheless, I included them for clarity.\n\nDue to incosistencies in compound surnames, for example, \"van Billiard\" in Kaggle and \"Van Billiard\" in Wikipedia datasets, I transform all surnames/names to begin with a capital letter using the built-in method `title()`."},{"metadata":{"trusted":true,"_uuid":"9890105b07bfe99dc6b4f0ff835301586c1f19ee"},"cell_type":"code","source":"kagg_corr = kagg.copy()\n\n# Sorting by class and name\nkagg_corr = kagg_corr.sort_values(['Pclass', 'Name']).reset_index(drop=True)\n\n# Extracting surnames and names using regular expressions\ntemp = kagg_corr.Name.str.extract(r'(?P<Surname>.*), Mrs\\. (?P<Husband_name>.*)\\((?P<Wife_name>.*)\\)')\ntemp2 = kagg_corr.Name.str.extract(r'(?P<Surname>.*), (?P<Title>.*)\\. (?P<Name>.*)')\n\n# Adding Kaggle surname codes\nsurname = temp.Surname\nsurname2 = temp2.Surname\nsurname = surname.fillna(surname2)\nsurname = surname.str.title()\nsurname_code = surname.str[0:3]\nkagg_corr['Surname_code'] = surname_code\n\n# Adding Kaggle name codes\nname = temp.Wife_name\nname2 = temp2.Name\nname = name.fillna(name2)\nname = name.str.title()\nname_code = name.str[0:3]\nkagg_corr['Name_code'] = name_code\n\nkagg_corr.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"348287462a1d2732ef5794a1c64c135054e92676"},"cell_type":"markdown","source":"# 5. Preparing Wikipedia dataset for merge\n\nJust like with the Kaggle dataset it is useful to have a column with a unique ID for each passenger. Let's name this column `WikiId` and insert it in the beginning of the dataset using the method `insert()`.\n\nThen I correct a typo in row 339 that has \".\" after the surname instead of \",\".\n\nI also replace the missing values in the Wikipedia dataset written as \"--\" with `NaN`.\n\nAfterwards I use the same strategy as in the Kaggle dataset by matching the name strings with two regular expressions. This time the first regular expression is used to match the servants that have a different name structure starting with \"and ...\". Note that the group `(?P<Title>.*?)` now has `?` in the end and indicates non-greedy matching. This is important for the cases when the title doesn't end with \".\", for example, \"Miss\" (unlike the Kaggle dataset where all titles end with \".\" even \"Miss.\"). For the same reason `(?P<Title>.*?)` is followed by `\\.*` where `*` indicates that \".\" might not be present at all.\n\nHere I also convert special characters into their ASCII equivalents using the function `unidecode()`. Unlike the Kaggle dataset, the Wikipedia dataset has a lot of these characters especially in Scandinavian names (for example, \"BjÃ¶rnstrÃ¶m-Steffanson, Mr. Mauritz HÃ¥kan\")\n\nSince I will use the column `Age` to help resolving duplicates, I need to change its type to `float` just like in the Kaggle dataset. This column, however, indicates the age of babies in months (for example, \"11 mo.\") so it should be first extracted using the matched group `(?P<Months>\\d*)` and divided by 12. The missing values are then filled with the rest of the values from `Age` and its type is changed to `float`. The rounding to 2 decimal digits is applied so that the format is consistent with the Kaggle dataset.\n\nSince both Kaggle and Wikipedia datasets has columns `Name`, `Age`, `Surname_code` and `Name_code`, let's add the suffix `_wiki` to the features from the Wikipedia dataset in order to distinguish them."},{"metadata":{"trusted":true,"_uuid":"90ddc633161c658697e1afc98b6337e7733ed366"},"cell_type":"code","source":"wiki_corr = wiki.copy()\n\n# Adding WikiId\nwiki_corr.insert(0, 'WikiId', wiki_corr.index + 1)\n\n# Correcting a typo\nwiki_corr.loc[339, 'Name'] = 'Beane, Mrs. Ethel (nÃ©e Clarke)'\n\n# Replacing -- with NaN\nwiki_corr = wiki_corr.replace('â€“', float('nan'))\n\n# Extracting surnames and names using regular expressions\ntemp = wiki_corr.Name.str.extract(r'and (?P<Profession>.*), (?P<Title>.*?)\\.* (?P<Name>.*) (?P<Surname>.*)')\ntemp2 = wiki_corr.Name.str.extract(r'(?P<Surname>.*), (?P<Title>.*?)\\.* (?P<Name>.*)')\n\n# Adding Wikipedia surname codes\nsurname = temp.Surname\nsurname2 = temp2.Surname\nsurname = surname.fillna(surname2)\nsurname = surname.str.title()\nsurname = surname.apply(unidecode)\nsurname_code = surname.str[0:3]\nwiki_corr['Surname_code'] = surname_code\n\n# Adding Wikipedia name codes\nname = temp.Name\nname2 = temp2.Name\nname = name.fillna(name2)\nname = name.str.title()\nname = name.apply(unidecode)\nname_code = name.str[0:3]\nwiki_corr['Name_code'] = name_code\n\n# Converting age type to float\nmonths = wiki_corr.Age.str.extract(r'(?P<Months>\\d*) mo.', expand=False).astype('float64')\nage = months / 12.0\nage = age.fillna(wiki_corr.Age)\nwiki_corr['Age'] = age.astype('float64').round(2)\n\n# Adding Wikipedia suffixes\nwiki_corr = wiki_corr.rename(columns={'Name': 'Name_wiki', 'Age': 'Age_wiki', 'Surname_code': 'Surname_code_wiki', 'Name_code': 'Name_code_wiki'})\n\nwiki_corr.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"897f555fdf52734e473903594d7d5ba51fd23168"},"cell_type":"markdown","source":"# 6. Merging Kaggle and Wikipedia datasets\n\nNow the Kaggle and Wikipedia datasets are ready to be merged. I will do it in several stages in order to maximize the number of passengers that are matched automatically."},{"metadata":{"_uuid":"3ae99529ee1b6e4bc837b9464e40b6a124a9beaf"},"cell_type":"markdown","source":"## 6.1. Merging datasets using surname codes, name codes and age\n\nDuring the first stage, the datasets are merged using surname codes, name codes and age.\n\nThen the function `merge_report()` counts all duplicates either in `PassengerId` or `WikiId` and prints the report that contains the number of matched passengers, the number of duplicates, the number of unmatched passengers in comparison with the original Kaggle dataset and the total number of passengers in the original Kaggle dataset.\n\nAfter the first stage more than half of the passengers are matched automatically."},{"metadata":{"trusted":true,"_uuid":"5885a6f4d98f50bb832485ee7eb6586e57f13f28"},"cell_type":"code","source":"merg = pd.merge(kagg_corr, wiki_corr, left_on=['Surname_code', 'Name_code', 'Age'],\n                right_on=['Surname_code_wiki', 'Name_code_wiki', 'Age_wiki'])\n\ndef merge_report(df, kagg):\n    dupl = df.PassengerId.duplicated(keep=False)\n    dupl2 = df.WikiId.duplicated(keep=False)\n    dupl_num = (dupl | dupl2).sum()\n    print(f'''Matched: {df.shape[0]} ({dupl_num} duplicates)\nUnmatched: {kagg.shape[0] - df.shape[0]}\nTotal: {kagg.shape[0]}''')\n    \nmerge_report(merg, kagg_corr)\nmerg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c070cd4bd05d2ee6d6726fa2be5ba4e515e563f2"},"cell_type":"markdown","source":"To deal with the duplicates I define the function `dupl_drop()` that removes duplicates both in the columns `PassengerId` and `WikiId`. The new `merge_report()` shows that indeed it effectively removed the duplicates."},{"metadata":{"trusted":true,"_uuid":"e45c606392354df31d4a4d4c9190460f8b34e208"},"cell_type":"code","source":"def dupl_drop(df):\n    df_corr = df.drop_duplicates(subset=['PassengerId'], keep=False)\n    df_corr = df_corr.drop_duplicates(subset=['WikiId'], keep=False)\n    return df_corr\n\nmerg_corr = dupl_drop(merg)\n\nmerge_report(merg_corr, kagg_corr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8f2e432d6c3f433bc30a70a042efa0bf3c4291b"},"cell_type":"markdown","source":"## 6.2. Merging the unmatched passengers using surname codes and name codes\n\nDuring the next stage, I try to merge the unmatched passengers using only surname codes and name codes which is a weaker criteria. Since more than a half of the passengers are already matched, it is very likely that the remaining passengers will have significantly fewer duplicates and will be matched as well.\n\nI first obtain the rest Kaggle and Wikipedia datasets using the newly defined function `df_rest()` and then merge these two rest datasets. The duplicates are removed using the previously defined function `dupl_drop()` and the function `merge_report()` summarizes the result.\n\nThis approach turned out to be very effective leaving only 178 unmatched passengers."},{"metadata":{"trusted":true,"_uuid":"57d2b3e3d508d1c27c92fdf424d00be5b3058a98"},"cell_type":"code","source":"def df_rest(df, kagg, wiki):\n    kagg_rest = kagg[~kagg.PassengerId.isin(df.PassengerId)].copy()\n    wiki_rest = wiki[~wiki.WikiId.isin(df.WikiId)].copy()\n    return kagg_rest, wiki_rest\n\nkagg_rest, wiki_rest = df_rest(merg_corr, kagg_corr, wiki_corr)\n\nmerg_rest = pd.merge(kagg_rest, wiki_rest, left_on=['Surname_code', 'Name_code'],\n                     right_on=['Surname_code_wiki', 'Name_code_wiki'])\nmerg_rest = dupl_drop(merg_rest)\n\nmerge_report(merg_rest, kagg_rest)\nmerg_rest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd7e93f662350de003511a1cb09f3cadaffc56c5"},"cell_type":"markdown","source":"## 6.3. Merging the unmatched passengers using surname codes and age\n\nDuring the next stage, I try to match the remaining passengers using the surname codes and age which is an even weaker criteria than the previous one.\n\nThis approach allows to decrease the number of unmatched passengers even further down to 145."},{"metadata":{"trusted":true,"_uuid":"f89e91b5a870e147d24c7c9a0c08c9bad2076f94"},"cell_type":"code","source":"kagg_rest2, wiki_rest2 = df_rest(merg_rest, kagg_rest, wiki_rest)\n\nmerg_rest2 = pd.merge(kagg_rest2, wiki_rest2, left_on=['Surname_code', 'Age'],\n                      right_on=['Surname_code_wiki', 'Age_wiki'])\nmerg_rest2 = dupl_drop(merg_rest2)\n\nmerge_report(merg_rest2, kagg_rest2)\nmerg_rest2.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4aeca7d850101021932d08ecebd6b193f551f0f"},"cell_type":"markdown","source":"## 6.4. Merging the unmatched passengers using surname codes\n\nDuring the next stage I try to merge the unmatched passengers using only the surname codes which is the weakest criteria of all.\n\nUltimately I decreased the number of unmatched passengers down to 113."},{"metadata":{"trusted":true,"_uuid":"24532752164cc2e444b4b011587a71a4b7d5da35"},"cell_type":"code","source":"kagg_rest3, wiki_rest3 = df_rest(merg_rest2, kagg_rest2, wiki_rest2)\n\nmerg_rest3 = pd.merge(kagg_rest3, wiki_rest3, left_on=['Surname_code'],\n                      right_on=['Surname_code_wiki'])\nmerg_rest3 = dupl_drop(merg_rest3)\n\nmerge_report(merg_rest3, kagg_rest3)\nmerg_rest3.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcf16c7da9b2baceb2214a647e9b7d1c53aa5ef6"},"cell_type":"markdown","source":"## 6.5. Merging the unmatched passengers manually\n\nDuring the last stage I merge the remaining 113 unmatched passengers manually. To do so, I sort the datasets with unmatched passengers by names, save these datasets as CSV files and match the passengers, for example, by inspecting their names in Excel. Most of the passenger names are easy to match, however, in some difficult cases I had to type the ticket number from the Kaggle dataset into the Encyclopedia Titanica to understand the real name of a passenger."},{"metadata":{"trusted":true,"_uuid":"34499ea23849d75ac1bd0563ee1ab20b09e691d8"},"cell_type":"code","source":"kagg_rest4, wiki_rest4 = df_rest(merg_rest3, kagg_rest3, wiki_rest3)\n\nkagg_rest4 = kagg_rest4.sort_values('Name')\nwiki_rest4 = wiki_rest4.sort_values('Name_wiki')\n\nkagg_rest4.to_csv('kagg_rest4.csv', index=False)\nwiki_rest4.to_csv('wiki_rest4.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31be383fcd251d429493b362b8457a6eae666520"},"cell_type":"markdown","source":"The matched values of `WikiId` are presented in the list `wiki_id_match` and are added as an extra column to the DataFrame `kagg_rest4_corr` which is a copy of `kagg_rest4`."},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_id_match = [622,float('nan'),822,661,662,671,672,670,669,667,852,1188,960,311,\n                 697,698,853,696,741,717,720,float('nan'),1002,402,789,791,float('nan'),\n                 float('nan'),798,1205,603,722,804,319,802,859,981,434,879,961,1202,\n                 1308,902,908,612,629,989,948,float('nan'),1021,999,1000,1053,1006,\n                 1008,1027,float('nan'),1045,1046,1044,1311,1057,1055,1056,519,1072,\n                 1071,1075,1082,1084,782,1085,229,668,1139,702,703,float('nan'),1137,\n                 1138,552,555,553,312,float('nan'),183,1063,1181,1182,1190,1189,1191,\n                 1199,1222,1223,270,275,1250,1248,1249,1059,1265,893,602,604,181,1310,\n                 1291,1309,607,884,885,354]\n\nkagg_rest4_corr = kagg_rest4.reset_index(drop=True).copy()\nkagg_rest4_corr['WikiId'] = wiki_id_match\nkagg_rest4_corr.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c1f1b565a70aff61d20d0e5293a71082187c33b"},"cell_type":"markdown","source":"Then I merge two datasets based on `WikiId`. Unfortunately 8 passengers couldn't be matched but I will have a closer look at them in the next section."},{"metadata":{"trusted":true,"_uuid":"8dc230828a034cff4c9bbc122d5c14a8842b1652"},"cell_type":"code","source":"merg_rest4 = pd.merge(kagg_rest4_corr, wiki_rest4, on=['WikiId'])\n\nmerge_report(merg_rest4, kagg_rest4)\nmerg_rest4.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cd89aa758cb4fcca7ee253f9bfe578c775be379"},"cell_type":"markdown","source":"For convenience I concatenate all matched passengers in one DataFrame `merg_all`."},{"metadata":{"trusted":true,"_uuid":"d89ba6f7ff0fe4c46a61873e2cb033a089159e1e"},"cell_type":"code","source":"merg_all = pd.concat([merg_corr, merg_rest, merg_rest2, merg_rest3, merg_rest4],\n                     ignore_index=True)\n\nmerge_report(merg_all, kagg_corr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fe943109f916c4e4100870bb9b859d85efc04ad"},"cell_type":"markdown","source":"## 6.6. Inspecting the unmatched passengers\n\nLet's inspect the unmatched passengers from the Kaggle dataset (8 passengers) and from the Wikipedia dataset (13 passengers)."},{"metadata":{"trusted":true,"_uuid":"49b252f7f02b3e96022f1ac9cded55c488baf34b"},"cell_type":"code","source":"kagg_rest5, wiki_rest5 = df_rest(merg_all, kagg_corr, wiki_corr)\n\nkagg_rest5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b1f681efc82b161cb7451effc266c0d2e50c248"},"cell_type":"code","source":"wiki_rest5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f55d9cef1268d1989b8cfab2459bc93471421c0"},"cell_type":"markdown","source":"By manually inspecting each unmatched name from the Kaggle dataset and looking for it in the Wikipedia dataset, I discovered several matching mistakes. For example, the name \"Peters, Miss. Katie\" (see the last row in the above Kaggle dataset) is actually present in the Wikipedia dataset (see below) but wasn't matched for some reason."},{"metadata":{"trusted":true,"_uuid":"cfca6d94f80a0cbadd61a45945e3aaea0616ee69"},"cell_type":"code","source":"wiki_corr[wiki_corr.Name_wiki.str.contains('Peters')]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"277a1a46a2461375de1325201286e29571dd9664"},"cell_type":"markdown","source":"To understand the issue, I looked for the `WikiId=1128` in the final DataFrame with all matched passengers `merg_all`. It turns out that these passengers were indeed incorrectly matched because their surname and name codes are the same \"Pet\"/\"Cat\". Moreover, the incorrect match from the Kaggle dataset (\"Peter, Mrs. Catherine (Catherine Rizk)\") actually corresponds to one of the unmatched passengers from the Wikipedia dataset (\"Butrus-Youssef, Mrs. Katarin (nÃ©e Rizk)\") but is just spelled differently."},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"b7f6c5aa56d3324479dc63f385576dcc485f216c"},"cell_type":"code","source":"merg_all[merg_all.WikiId == 1128]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9db8227eaae1c6d1b8877a4fc94f3f8293f7ecf"},"cell_type":"markdown","source":"After inspecting all the unmatched passengers from the Kaggle dataset, the following manipulations should be performed to correct the mistakes:\n\n- Drop the rows with the following `PassengerId`: 1146, 569, 534, 919, 508\n- Merge the following pairs `PassengerId-WikiId`: 147-1293, 1146-980, 6-785, 681-1128, 534-701, 569-750, 919-1203, 508-41\n\nThe result of these manipulations looks correct."},{"metadata":{"trusted":true,"_uuid":"02267ba48c1ac8df0eb2abbd318d6856e9b827f5"},"cell_type":"code","source":"merg_all_corr = merg_all[(merg_all.PassengerId != 1146) &\n                         (merg_all.PassengerId != 569) &\n                         (merg_all.PassengerId != 534) &\n                         (merg_all.PassengerId != 919) &\n                         (merg_all.PassengerId != 508)]\n\nkagg_rest5_corr, wiki_rest5_corr = df_rest(merg_all_corr, kagg_corr, wiki_corr)\n\nkagg_rest5_corr.loc[:, 'WikiId'] = float('nan')\nkagg_rest5_corr.loc[kagg_rest5_corr.PassengerId == 147, 'WikiId'] = 1293\nkagg_rest5_corr.loc[kagg_rest5_corr.PassengerId == 1146, 'WikiId'] = 980\nkagg_rest5_corr.loc[kagg_rest5_corr.PassengerId == 6, 'WikiId'] = 785\nkagg_rest5_corr.loc[kagg_rest5_corr.PassengerId == 681, 'WikiId'] = 1128\nkagg_rest5_corr.loc[kagg_rest5_corr.PassengerId == 534, 'WikiId'] = 701\nkagg_rest5_corr.loc[kagg_rest5_corr.PassengerId == 569, 'WikiId'] = 750\nkagg_rest5_corr.loc[kagg_rest5_corr.PassengerId == 919, 'WikiId'] = 1203\nkagg_rest5_corr.loc[kagg_rest5_corr.PassengerId == 508, 'WikiId'] = 41\n\nmerg_rest5 = pd.merge(kagg_rest5_corr, wiki_rest5_corr, on=['WikiId'])\nmerg_rest5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Then the corrected dataset for all merged passengers `merg_all2` is obtained."},{"metadata":{"trusted":true,"_uuid":"b6c8c050c231c56f4c1ab1dbcfea20eb3592a4c2"},"cell_type":"code","source":"merg_all2 = pd.concat([merg_all_corr, merg_rest5], ignore_index=True)\n\nmerge_report(merg_all2, kagg_corr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a69e29f8066e8f0aafe901ebc78d672f680c9c92"},"cell_type":"markdown","source":"Below are the final lists of unmatched passengers from the Kaggle dataset `kagg_rest6` and from the Wikipedia datasets `wiki_rest6`. The 5 unmatched passengers from the Kaggle dataset can be found on the Encyclopedia Titanica but for some reason are absent in the Wikipedia dataset. This is a bit strange because the Kaggle dataset is based on the version of the Encyclopedia Titanica retrieved in 1999 while the Wikipedia one is based on the version from 2011.\n\nNotice also that most of the unmatched passengers from the Wikipedia dataset have the reference [60] that says:\n\n> \"See the list of crew members on board RMS Titanic article for further information.\"\n\nThese passengers were part of the crew members and probably that's why they aren't mentioned in the Kaggle dataset."},{"metadata":{"trusted":true,"_uuid":"82be109c345a664a2f4fe442d8a3e94b62f3fd9e"},"cell_type":"code","source":"kagg_rest6, wiki_rest6 = df_rest(merg_all2, kagg_corr, wiki_corr)\nkagg_rest6","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5cec3c5f4b7c7cdf1c85d8ba44eff2d78b0bb5c"},"cell_type":"code","source":"wiki_rest6","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"709f7dbf33b3be05d56d753db122dab0794240e2"},"cell_type":"markdown","source":"Finally, the 5 unmatched passengers from `kagg_rest6` are added to the rest of the matched passengers in `merg_all2`. After concatenating both DataFrames, 5 duplicates appear. These are 5 unmatched passengers that have the same value of `NaN` in the column `WikiId` ."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"dc16bbdc68d890f5d4a9cf3b9c22dd6629bc0b14"},"cell_type":"code","source":"merg_all3 = pd.concat([merg_all2, kagg_rest6], ignore_index=True, sort=False)\n\nmerge_report(merg_all3, kagg_corr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c73743646d496a6ee30eb7ebb355fb2aab1950c7"},"cell_type":"markdown","source":"For the final dataset I drop the surname and name codes because they aren't needed anymore. I also sort the DataFrame according to the `PassengerId` and reset the index so that it looks like the original Kaggle dataset."},{"metadata":{"trusted":true,"_uuid":"df8a619703d124444f37be9f1f76e938a9231972"},"cell_type":"code","source":"merg_all3_corr = merg_all3.drop(['Surname_code', 'Name_code', 'Surname_code_wiki', 'Name_code_wiki'], axis=1)\nmerg_all3_corr = merg_all3_corr.sort_values('PassengerId').reset_index(drop=True)\nmerg_all3_corr.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cb4a4e34a7c1fe488ab2aed7d8dcb1b29ed386e"},"cell_type":"markdown","source":"# 7. Splitting in training and testing datasets\n\nFor convenience I also split the full dataset into the training and testing parts."},{"metadata":{"trusted":true,"_uuid":"e9e4e5fc1c158f50e231ac701cc74cd22bc82045"},"cell_type":"code","source":"full = merg_all3_corr.copy()\n\ntrain = full[:891]\n\ntest = full[891:]\ntest = test.drop('Survived', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bb4955bf151f3f8546840c0903a314a8aaa3604"},"cell_type":"markdown","source":"All datasets were saved as CSV files and can be easily added to your kernel from the page [Titanic extended dataset (Kaggle + Wikipedia)](https://www.kaggle.com/pavlofesenko/titanic-extended)."},{"metadata":{"trusted":true,"_uuid":"e7fca052a0c765106ed20adb2431b40b31c80880"},"cell_type":"code","source":"full.to_csv('full.csv', index=False)\ntrain.to_csv('train.csv', index=False)\ntest.to_csv('test.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1860ce1625d358caa90edf4d4e8a6b62514b6fa8"},"cell_type":"markdown","source":"# 8. Conclusion\n\nIn this kernel I have extended the original Kaggle dataset with the features available on Wikipedia as of 18 February 2019. Although most of the features from the Wikipedia dataset are similar to the ones from the Kaggle dataset, they are more up-to-date and have much fewer missing values. In the resulting dataset all features were left unprocessed to keep as much as information as possible. Among 1309 passengers from the Kaggle dataset 1304 were successfully matched with the passengers from the Wikipedia dataset. The new extended Titanic dataset can be potentially used to create better models (due to fewer missing values and additional features) or to perform an additial EDA for Titanic passengers (for example, using interactive visualizations)."},{"metadata":{},"cell_type":"markdown","source":"Thanks for **UPVOTING** this kernel! Trying to become a Kernels Master. ðŸ¤˜\n\nCheck out my other cool projects:\n- [ðŸ’² Minimizing investment risk for high interest loans](https://www.kaggle.com/pavlofesenko/minimizing-investment-risk-for-high-interest-loans)\n- [ðŸ“Š Interactive Titanic dashboard using Bokeh](https://www.kaggle.com/pavlofesenko/interactive-titanic-dashboard-using-bokeh)\n- [ðŸ‘ª Titanic extended dataset (Kaggle + Wikipedia)](https://www.kaggle.com/pavlofesenko/titanic-extended)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}