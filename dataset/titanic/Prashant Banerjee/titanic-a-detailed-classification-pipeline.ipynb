{"cells":[{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0\"></a>\n# **Titanic Survival : A Detailed Classification Pipeline** \n\n\n\n## **Introduction**\n\n\nPrashant Banerjee\n\n\nApril 2020\n\n\nMachine Learning model development is a complicated process. It is a difficult task for beginners. Most beginners get lost in the process. So, in this notebook, I will try to provide a framework for model development to solve a binary classification problem. It will teach you how to approach a problem, how to think like a data scientist and what to code.\n\nI have used the famous titanic dataset for the illustration purposes. I will try to provide clear explanations and clean code so that the reader can solve any classification problem.\n\n\nThe reader is welcome to use, comment, experiment and fork this notebook for their personnal use.\n\n\nSo, let's dive in."},{"metadata":{},"cell_type":"markdown","source":"**I hope you find this notebook useful and your <font color=\"red\"><b>UPVOTES</b></font> would be very much appreciated**\n\n"},{"metadata":{},"cell_type":"markdown","source":"<a class=\"anchor\" id=\"0.1\"></a>\n# **Notebook Contents**\n\n- [Part 1 - The Problem Statement](#1)\n- [Part 2 - Basic Set Up](#2)\n    - [2.1  Import libraries](#2.1)\n    - [2.2 Read dataset](#2.2)\n- [Part 3 - Data Exploration](#3)\n    - [3.1\tExplore the training set](#3.1)\n    - [3.2\tExplore the test set](#3.2)\n- [Part 4 - Data Visualization](#4)\n    - [4.1\tMissing values](#4.1)\n    - [4.2\tSurvived](#4.2)\n    - [4.3\tSex](#4.3)\n    - [4.4\tPclass](#4.4)\n    - [4.5\tEmbarked](#4.5)\n    - [4.6\tAge](#4.6)\n- [Part 5 - Data Preprocessing](#5)\n    - [5.1\tRemove redundant features](#5.1)\n    - [5.2\tImputation of missing values in Age](#5.2)\n    - [5.3\tImputation of missing values in Cabin](#5.3)\n    - [5.4\tImputation of missing values in Embarked](#5.4)\n    - [5.5\tOutlier detection](#5.5)\n- [Part 6 - Feature Engineering](#6)\n    - [6.1\tCategorize passengers as male, female or child](#6.1)\n    - [6.2\tMake additional variable : travel alone](#6.2)\n    - [6.3\tCorrelation of features with target](#6.3)\n- [Part 7 - Categorical Variable Encoding](#7)\n- [Part 8 - Feature Scaling](#8)\n- [Part 9 - Modeling](#9)\n    - [9.1\tPredict accuracy with different algorithms](#9.1)\n    - [9.2\tPlot the classifier accuracy scores](#9.2)\n- [Part 10 - Feature Selection](#10)\n    - [10.1\tFeature Importance with Random Forest Model](#10.1)\n    - [10.2\tVisualize feature scores](#10.2)\n    - [10.3\tDrop least important feature](#10.3)\n- [Part 11 - Confusion Matrix](#11)\n- [Part 12 - Classification Metrices](#12)\n    - [12.1\tClassification Report](#12.1)\n    - [12.2\tClassification Accuracy](#12.2)\n    - [12.3\tClassification Error](#12.3)\n    - [12.4\tPrecision](#12.4)\n    - [12.5\tRecall](#12.5)\n    - [12.6\tTrue Positive Rate](#12.6)\n    - [12.7\tFalse Positive Rate](#12.7)\n    - [12.8\tSpecificity or True Negative Rate](#12.8)\n    - [12.9\tF1 score](#12.9)\n   -  [12.10\tSupport](#12.10)\n- [Part 13 - Cross-Validation](#13)\n- [Part 14 - Hyperparameter Optimization using Grid Search CV](#14)\n- [Part 15 - Ensemble Modeling](#15)\n- [Part 16 - Submission](#16)\n- [Part 17 - Conclusion](#17)\n- [Part 18 - Credits](#18)\n"},{"metadata":{},"cell_type":"markdown","source":"# **1. The Problem Statement** <a class=\"anchor\" id=\"1\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- The first step in a machine learning model development is to define the problem statement. It is a necessary step as it will help us to stay focused and move in the right direction.\n\n- So, in this case the problem statement is to predict how many people survive the titanic shipwreck disaster.\n"},{"metadata":{},"cell_type":"markdown","source":"# **2. Basic Set Up** <a class=\"anchor\" id=\"2\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"## **2.1 Import Libraries** <a class=\"anchor\" id=\"2.1\"></a>\n\n[Notebook Contents](#0.1)\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Ignore warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Data processing and analysis\nimport numpy as np\nimport pandas as pd\nimport math \nimport re\n\n\n# Data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n\n# Configure visualisations\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nsns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)\n\n\n# Classification algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\n\n\n# Data preprocessing :\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, scale, LabelEncoder, OneHotEncoder\n\n\n# Modeling helper functions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score\n\n\n\n# Classification metrices\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report, precision_score,recall_score,f1_score \n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **2.2 Read Dataset** <a class=\"anchor\" id=\"2.2\"></a>\n\n[Notebook Contents](#0.1)\n\n"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load train and Test set\n\n%time\n\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\nIDtest = test['PassengerId']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **3. Data Exploration** <a class=\"anchor\" id=\"3\"></a>\n\n[Notebook Contents](#0.1)\n"},{"metadata":{},"cell_type":"markdown","source":"## **3.1 Explore the training set** <a class=\"anchor\" id=\"3.1\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### **View shape of training set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The shape of the training set : {} '.format(train.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **View profile report of training set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas_profiling as pp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pp.ProfileReport(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Preview training set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **View concise summary of training set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It seems that several of the variables - `Age`, `Cabin` and `Embarked` contain missing values. Let's check it."},{"metadata":{},"cell_type":"markdown","source":"### **Print variables containing missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"var1 = [col for col in train.columns if train[col].isnull().sum() != 0]\n\nprint(train[var1].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- So, we are right that `Age`, `Cabin` and `Embarked` contain missing values."},{"metadata":{},"cell_type":"markdown","source":"### **View statistical properties of training set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Types of Variables**\n\n\n- Now, we will classify the variables into categorical and numerical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# find categorical variables\n\ncategorical = [var for var in train.columns if train[var].dtype =='O']\n\nprint('There are {} categorical variables in training set.\\n'.format(len(categorical)))\n\nprint('The categorical variables are :', categorical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find numerical variables\n\nnumerical = [var for var in train.columns if train[var].dtype !='O']\n\nprint('There are {} numerical variables in training set.\\n'.format(len(numerical)))\n\nprint('The numerical variables are :', numerical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## **3.2 Explore the test set** <a class=\"anchor\" id=\"3.2\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"### **View shape of test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The shape of the test set : {} '.format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **View profile report of test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pp.ProfileReport(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Preview test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **View concise summary of test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Print variables containing missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"var2 = [col for col in test.columns if test[col].isnull().sum() != 0]\n\nprint(test[var2].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- So, we are right that `Age`, `Cabin` and `Embarked` contain missing values."},{"metadata":{},"cell_type":"markdown","source":"### **View statistical properties of test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Types of Variables**\n\n\n- Now, we will classify the variables into categorical and numerical variables."},{"metadata":{"trusted":true},"cell_type":"code","source":"# find categorical variables\n\ncategorical = [var for var in test.columns if test[var].dtype =='O']\n\nprint('There are {} categorical variables in test set.\\n'.format(len(categorical)))\n\nprint('The categorical variables are :', categorical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find numerical variables\n\nnumerical = [var for var in test.columns if test[var].dtype !='O']\n\nprint('There are {} numerical variables in test set.\\n'.format(len(numerical)))\n\nprint('The numerical variables are :', numerical)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Observations about dataset**"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# **4. Data Visualization** <a class=\"anchor\" id=\"4\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"## **4.1 Missing values** <a class=\"anchor\" id=\"4.1\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# view missing values in training set\nmsno.matrix(train, figsize = (30,10))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4.2 Survived ** <a class=\"anchor\" id=\"4.2\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Survived'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Here `0 stands for not survived` and `1 stands for survived`.\n\n- So, 549 people survived and 342 people did not survive.\n\n- Let's visualize it by plotting."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Survived'], data = train, palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people who survived', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Now females have higher probability of survival than males.\n- Let' check it"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('Survived')['Sex'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Survived'], data = train, hue='Sex', palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people who survived', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let's check the percentage of survival for males and females separately."},{"metadata":{"trusted":true},"cell_type":"code","source":"females = train[train['Sex'] == 'female']\nfemales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"females['Survived'].value_counts()/len(females)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"males = train[train['Sex'] == 'male']\nmales.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"males['Survived'].value_counts()/len(males)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- As expected females have higher probability of survival (value 1) 74.20% than males 18.89%."},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the first of two pie-charts and set current axis\nplt.figure(figsize=(8,6))\nplt.subplot(1, 2, 1)   # (rows, columns, panel number)\nlabels1 = females['Survived'].value_counts().index\nsize1 = females['Survived'].value_counts()\ncolors1=['cyan','pink']\nplt.pie(size1, labels = labels1, colors = colors1, shadow = True, autopct='%1.1f%%',startangle = 90)\nplt.title('Percentage of females who survived', fontsize = 20)\nplt.legend(['1:Survived', '0:Not Survived'], loc=0)\nplt.show()\n\n# create the second of two pie-charts and set current axis\nplt.figure(figsize=(8,6))\nplt.subplot(1, 2, 2)   # (rows, columns, panel number)\nlabels2 = males['Survived'].value_counts().index\nsize2 = males['Survived'].value_counts()\ncolors2=['pink','cyan']\nplt.pie(size2, labels = labels2, colors = colors2, shadow = True, autopct='%1.1f%%',startangle = 90)\nplt.title('Percentage of males who survived', fontsize = 20)\nplt.legend(['0:Not Survived','1:Survived'])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- From the above pie-charts, we can deduce that females probability of survival is 74.2% (cyan color) while males probability of survival is 18.9% (cyan color)."},{"metadata":{},"cell_type":"markdown","source":"## **4.3 Sex** <a class=\"anchor\" id=\"4.3\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Sex'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Sex'], data=train, palette = 'bone')\ngraph.set_title('Distribution of sex among passengers', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Sex'].value_counts()/len(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(8,6))\nlabels = train['Sex'].value_counts().index\nsize = train['Sex'].value_counts()\ncolors=['cyan','pink']\nplt.pie(size, labels = labels, shadow = True, colors=colors, autopct='%1.1f%%',startangle = 90)\nplt.title('Percentage distribution of sex among passengers', fontsize = 20)\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.groupby('Pclass')['Sex'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **4.4 Pclass** <a class=\"anchor\" id=\"4.4\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train['Pclass'], data=train, palette = 'bone')\ngraph.set_title('Number of people in different classes', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train['Pclass'], data=train, hue='Survived', palette = 'bone')\ngraph.set_title('Distribution of people segregated by survival', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Here `0 stands for not survived` and `1 stands for survived`.\n\n- So, we can see that `Pclass` plays a major role in survival.\n\n- Majority of people survived in `Pclass 1` while a large number of people do not survive in `Pclass 3`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage of survivors per class\nsns.factorplot('Pclass', 'Survived', data = train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above plot indicates the percentage of survivors per class."},{"metadata":{},"cell_type":"markdown","source":"## **4.5 Embarked** <a class=\"anchor\" id=\"4.5\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train['Embarked'], data=train, palette = 'bone')\ngraph.set_title('Number of people across different embarkment', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train['Embarked'], data=train, hue='Survived', palette = 'bone')\ngraph.set_title('Number of people across different embarkment', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that port of embarkment plays a major role in survival probability."},{"metadata":{},"cell_type":"markdown","source":"## **4.6 Age** <a class=\"anchor\" id=\"4.6\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = train['Age']\nplt.figure(figsize=(8,6))\nplt.hist(x, bins=25, color='g')\nplt.xlabel('Age')\nplt.ylabel('Number of passengers')\nplt.title('Age distribution of passengers', fontsize = 20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that majority of passengers are aged between 20 and 40."},{"metadata":{},"cell_type":"markdown","source":"- We will again visit this data visualization section in Feature Engineering section."},{"metadata":{},"cell_type":"markdown","source":"# **5. Data Preprocessing** <a class=\"anchor\" id=\"5\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## **5.1 Remove redundant features** <a class=\"anchor\" id=\"5.1\"></a>\n\n[Notebook Contents](#0.1)\n\n- The `Ticket` and `PassengerId` are redundant features. So, we will remove them from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop(['Ticket', 'PassengerId'], axis = 1, inplace = True)\ntest.drop(['Ticket','PassengerId'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **5.2 Imputation of missing values in Age** <a class=\"anchor\" id=\"5.2\"></a>\n\n[Notebook Contents](#0.1)\n\n- We will make additional column with the title of the person (Mr, Mrs, Miss, etc).\n\n- Then, we impute the missing values in age with the median age for each title.\n\n- Let's first make a function to extract title from `Name` feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to extract title from Name feature\ndef passenger_title(passenger):\n    line = passenger\n    if re.search('Mrs', line):\n        return 'Mrs'\n    elif re.search('Mr', line):\n        return 'Mr'\n    elif re.search('Miss', line):\n        return 'Miss'\n    elif re.search('Master', line):\n        return 'Master'\n    else:\n        return 'Other'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# extract title  \ntrain['Title'] = train['Name'].apply(passenger_title)\ntest['Title'] = test['Name'].apply(passenger_title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill missing age, with median from title segregation: funtion\ndef fill_age(passenger):\n    \n    # determine age by group \n    temp = train.groupby(train.Title).median()\n    \n    age, title = passenger\n    \n    if age == age:\n        return age\n    else:\n        if title == 'Mr':\n            return temp.Age['Mr']\n        elif title == 'Miss':\n            return temp.Age['Miss']\n        elif title == ['Mrs']:\n            return temp.Age['Mrs']\n        elif title == 'Master':\n            return temp.Age['Master']\n        else:\n            return temp.Age['Other']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fill age according to title\ntrain['Age'] = train[['Age', 'Title']].apply(fill_age, axis = 1)\ntest['Age'] = test[['Age', 'Title']].apply(fill_age, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove column Name, it is not useful for predictions and we extracted the title already\ntrain.drop('Name', axis = 1, inplace = True)\ntest.drop('Name', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove column Title, it is not useful for predictions and we imputed the age already\ntrain.drop('Title', axis = 1, inplace = True)\ntest.drop('Title', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **5.3 Imputation of missing values in Cabin** <a class=\"anchor\" id=\"5.3\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- To extract missing values in Cabin, we extract Deck from Cabin and add 'Unknown' where NA.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def isNaN(num):\n    return num != num # checks if cell is NaN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the first letter of cabin \ndef first_letter_of_cabin(cabin):\n    if not isNaN(cabin):\n        return cabin[0]\n    else:\n        return 'Unknown'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Deck'] = train['Cabin'].apply(first_letter_of_cabin)\ntest['Deck'] = test['Cabin'].apply(first_letter_of_cabin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop old variable Cabin\ntrain.drop('Cabin', axis = 1, inplace = True)\ntest.drop('Cabin', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **5.4 Imputation of missing values in Embarked** <a class=\"anchor\" id=\"5.4\"></a>\n\n[Notebook Contents](#0.1)\n\n- We impute Embarked with the most frequent port (S)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train[\"Embarked\"].fillna(\"S\", inplace = True)\ntest['Embarked'].fillna(\"S\", inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Let's again check for missing values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can replace missing value in fare by taking median of all fares of those passengers \n#who share 3rd Passenger class and Embarked from 'S' \ntest['Fare'].fillna(test['Fare'].median(), inplace = True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **5.5 Outlier Detection** <a class=\"anchor\" id=\"5.5\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- The `Age` and `Fare` variable contain putliers. Now, let's check for outliers in `Age` and `Fare`.\n\n- Let's draw boxplots to visualise outliers in the above variables.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# draw boxplots to visualize outliers\n\nplt.figure(figsize=(15,10))\n\n\nplt.subplot(1, 2, 1)\nfig = train.boxplot(column='Age')\nfig.set_title('')\nfig.set_ylabel('Age')\n\n\nplt.subplot(1, 2, 2)\nfig = train.boxplot(column='Fare')\nfig.set_title('')\nfig.set_ylabel('Fare')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find outliers in Age variable\n\nIQR = train.Age.quantile(0.75) - train.Age.quantile(0.25)\nLower_fence = train.Age.quantile(0.25) - (IQR * 3)\nUpper_fence = train.Age.quantile(0.75) + (IQR * 3)\nprint('Age outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=max(0, Lower_fence), upperboundary=Upper_fence))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find outliers in Fare variable\n\nIQR = train.Fare.quantile(0.75) - train.Fare.quantile(0.25)\nLower_fence = train.Fare.quantile(0.25) - (IQR * 3)\nUpper_fence = train.Fare.quantile(0.75) + (IQR * 3)\nprint('Fare outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=max(0, Lower_fence), upperboundary=Upper_fence))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Since, `Age` and `Fare` do not have values less than 0. So, we assume their minimum values to be 0.\n\n- I will use top-coding approach to cap maximum values and remove outliers from the above variables.\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def max_value(df, variable, top):\n    return np.where(df[variable]>top, top, df[variable])\n\nfor df in [train, test]:\n    df['Age'] = max_value(df, 'Age', 81.0)\n    df['Fare'] = max_value(df, 'Fare', 100.2688)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Let's check that the above variables are capped at their maximum values."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Age.max(), test.Age.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Fare.max(), test.Fare.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# **6. Feature Engineering** <a class=\"anchor\" id=\"6\"></a>\n\n[Notebook Contents](#0.1)\n\n- In this section, we will make additional columns for future analysis."},{"metadata":{},"cell_type":"markdown","source":"## **6.1 Categorize passengers as male, female or child** <a class=\"anchor\" id=\"6.1\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- Children have much larger probability of survival than men or women. So, we will categorize the passengers as men, women or child."},{"metadata":{"trusted":true},"cell_type":"code","source":"# label minors as child, and remaining people as female or male\ndef male_female_child(passenger):\n    # take the age and sex\n    age, sex = passenger\n    \n    # compare age, return child if under 16, otherwise leave sex\n    if age < 16:\n        return 'child'\n    else:\n        return sex","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# new columns called person specifying if the person was female, male or child\ntrain['Person'] = train[['Age', 'Sex']].apply(male_female_child, axis = 1)\ntest['Person'] = test[['Age', 'Sex']].apply(male_female_child, axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of male, female and children on board\ntrain['Person'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# age segregated by class\nfig = sns.FacetGrid(train, hue = 'Person', aspect = 4)\nfig.map(sns.kdeplot, 'Age', shade = True)\nfig.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# age segregated by class\nfig = sns.FacetGrid(train, hue = 'Pclass', aspect = 4)\nfig.map(sns.kdeplot, 'Age', shade = True)\nfig.add_legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that the peak over 0 age for classes 2 and 3, coincides with the classes that had children. Class 1 did not have a lot of children, unsurprisingly. Note also, that older people were high class."},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.factorplot('Pclass', 'Survived', hue = 'Person', data = train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that males have lower probability of survival than females and children, regardless of the class they were in. \n- As for women and children, being in class 3 meant that their chances of survival were lower."},{"metadata":{},"cell_type":"markdown","source":"## **6.2 Make additional variable : travel alone** <a class=\"anchor\" id=\"6.2\"></a>\n\n[Notebook Contents](#0.1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def travel_alone(df):\n    df['Alone'] = df.Parch + df.SibSp\n    df['Alone'].loc[df['Alone'] > 0] = 'With Family'\n    df['Alone'].loc[df['Alone'] == 0] = 'Alone'\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- `0` indicates that person is travelling with family and `1` indicates that he is travelling alone."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = travel_alone(train)\ntest = travel_alone(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many passengers are travelling with family and alone\ntrain['Alone'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- So, 537 people are travelling alone and 354 people are travelling with family."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Alone'], data = train, palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people travelling alone or with family', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Alone'], data = train, hue = 'Survived', palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people travelling alone or with family', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that almost half number of people who are travelling with family survived whereas large number of people travelling alone did not survive.\n\n- So, travelling alone or with family plays a major role in deciding the survival probability."},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage of survivors depending on traveling alone or with family\nsns.factorplot('Alone', 'Survived', hue = 'Person', data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Deck'], data = train[train.Deck != 'Unknown'], hue = 'Survived', palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people on each deck', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()/2., height + 0.1,height ,ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The people who are on deck `C` and `B` have larger probability of survival."},{"metadata":{},"cell_type":"markdown","source":"## **6.3 Correlation of features with target** <a class=\"anchor\" id=\"6.3\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.corr()['Survived']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that `Survived` is negatively correlated with `Pclass`,`Age`,`SibSp`,`Embarked`,`Deck`,`Person`,`Alone` and positively correlated with `Parch` and `Fare`.\n\n- We can also plot a heatmap to visualize the relationship between features."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr=train.corr()#[\"Survived\"]\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr, vmax=.8, linewidths=0.01, square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\nplt.title('Correlation between features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **7. Categorical Variable Encoding** <a class=\"anchor\" id=\"7\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"- Now, let's take a look at train and test set."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- Drop the `Sex` variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"train.drop('Sex', axis=1, inplace=True)\ntest.drop('Sex', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- There are 4 variables that need to be categorical encoded. \n\n- They are `Embarked`,`Deck`,`Person` and `Alone`"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['Alone'] = pd.get_dummies(train['Alone'])\ntest['Alone'] = pd.get_dummies(test['Alone'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"labelenc=LabelEncoder()\n\ncategorical=['Embarked','Deck','Person']\nfor col in categorical:\n    train[col]=labelenc.fit_transform(train[col])\n    test[col]=labelenc.fit_transform(test[col])\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **8. Feature Scaling** <a class=\"anchor\" id=\"8\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- We need to do Feature Scaling first before proceeding with modeling."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_cols = train.columns\ntest_cols = test.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler = StandardScaler()\ntrain[['Age', 'Fare']] = scaler.fit_transform(train[['Age', 'Fare']])\ntest[['Age', 'Fare']] = scaler.transform(test[['Age', 'Fare']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **9. Modelling** <a class=\"anchor\" id=\"9\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Declare feature vector and target variable\nX = train.drop(labels = ['Survived'],axis = 1)\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **9.1 Predict accuracy with different algorithms** <a class=\"anchor\" id=\"9.1\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- I predict accuracy with 15 popular classifiers and evaluate their performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"names = [\"Logistic Regression\", \"Nearest Neighbors\", \"Naive Bayes\", \"Linear SVM\", \"RBF SVM\", \n         \"Gaussian Process\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\", \"Gradient Boosting\", \n         \"LDA\", \"QDA\", \"Neural Net\", \"LightGBM\", \"XGBoost\" ]    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers = [\n    LogisticRegression(),\n    KNeighborsClassifier(5),\n    GaussianNB(),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(kernel = \"rbf\", gamma=2, C=1),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    MLPClassifier(alpha=1, max_iter=1000),\n    lgb.LGBMClassifier(),    \n    xgb.XGBClassifier()\n   ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\naccuracy_scores = []\n\n# iterate over classifiers and predict accuracy\nfor name, clf in zip(names, classifiers):\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    score = round(score, 4)\n    accuracy_scores.append(score)\n    print(name ,' : ' , score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers_performance = pd.DataFrame({\"Classifiers\": names, \"Accuracy Scores\": accuracy_scores})\nclassifiers_performance","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- The accuracy score of top performing algorithms in descending order is given below -"},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers_performance.sort_values(by = 'Accuracy Scores' , ascending = False)[['Classifiers', 'Accuracy Scores']]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **9.2 Plot the classifier accuracy scores** <a class=\"anchor\" id=\"9.2\"></a>\n\n[Notebook Contents](#0.1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(8,6))\nx = classifiers_performance['Accuracy Scores']\ny = classifiers_performance['Classifiers']\nax.barh(y, x, align='center', color='green')\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Accuracy Scores')\nax.set_ylabel('Classifiers', rotation=0)\nax.set_title('Classifier Accuracy Scores')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **10. Feature Selection** <a class=\"anchor\" id=\"10\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- In this section, we will see how to improve model performance by feature selection.\n\n- We will visualize feature importance with random forest classifier and drop the least important feature, rebuild the model and check effect on accuracy.  \n \n- For a comprehensive overview on feature selection techniques, please see the kernel - \n\n   - [A Reference Guide to Feature Selection Methods](https://www.kaggle.com/prashant111/a-reference-guide-to-feature-selection-methods)\n"},{"metadata":{},"cell_type":"markdown","source":"## **10.1 Feature Importance with Random Forest model** <a class=\"anchor\" id=\"10.1\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- Until now, I have used all the features given in the model. Now, I will select only the important features, build the model using these features and see its effect on accuracy.\n\n- First, I will create the Random Forest model as follows:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate the classifier with n_estimators = 100\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n# fit the classifier to the training set\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, I will use the feature importance variable to see feature importance scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"# view the feature scores\nfeature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nfeature_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_scores.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_scores.index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that the most important feature is `Person` and least important feature is `Alone`."},{"metadata":{},"cell_type":"markdown","source":"## **10.2 Visualize feature scores** <a class=\"anchor\" id=\"10.2\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a seaborn bar plot to visualize feature scores\nf, ax = plt.subplots(figsize=(8,6))\nax = sns.barplot(x=feature_scores.values, y=feature_scores.index, palette='spring')\nax.set_title(\"Visualize feature scores of the features\")\nax.set_yticklabels(feature_scores.index)\nax.set_xlabel(\"Feature importance score\")\nax.set_ylabel(\"Features\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **10.3 Drop least important feature** <a class=\"anchor\" id=\"10.3\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"- Now, I will drop the least important feature `Alone` from the model, rebuild the model and check its effect on accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the least important feature from X_train, X_test and test set for further analysis\nX1_train = X_train.drop(['Alone'], axis=1)\nX1_test = X_test.drop(['Alone'], axis=1)\ntest = test.drop(['Alone'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy_scores1 = []\n\n# iterate over classifiers and predict accuracy\nfor name, clf in zip(names, classifiers):\n    clf.fit(X1_train, y_train)\n    score = clf.score(X1_test, y_test)\n    score = round(score, 4)\n    accuracy_scores1.append(score)\n    print(name ,' : ' , score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifiers_performance1 = pd.DataFrame({\"Classifiers\": names, \"Accuracy Scores\": accuracy_scores, \n                                         \"Accuracy Scores1\": accuracy_scores1})\nclassifiers_performance1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We can see that `Gaussian Process` has the maximum accuracy of 0.8441.\n\n- We will use the `Gaussian Process Classifier` to plot the confusion-matrix.\n"},{"metadata":{},"cell_type":"markdown","source":"# **11. Confusion matrix** <a class=\"anchor\" id=\"11\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- A **confusion matrix** is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n\n- Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n\n- **True Positives (TP)** – True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n\n- **True Negatives (TN)** – True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n\n- **False Positives (FP)** – False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called **Type I error**.\n\n- **False Negatives (FN)** – False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error**.\n\n- These four outcomes are summarized in a confusion matrix.\n\n- We will use the `Gaussian Process Classifier` to plot the confusion-matrix.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiate the XGBoost classifier\ngpc_clf = GaussianProcessClassifier(1.0 * RBF(1.0))\n\n\n# fit the classifier to the modified training set\ngpc_clf.fit(X1_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predict on the test set\ny1_pred = gpc_clf.predict(X1_test)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the accuracy\nprint('Gaussian Process Classifier model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y1_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print confusion-matrix\n\ncm = confusion_matrix(y_test, y1_pred)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The confusion matrix shows 156 + 93 = 249 correct predictions and 19 + 27 = 46 incorrect predictions.\n\nIn this case, we have\n\n- True Positives (Actual Positive:1 and Predict Positive:1) - 156\n- True Negatives (Actual Negative:0 and Predict Negative:0) - 93\n- False Positives (Actual Negative:0 but Predict Positive:1) - 19 (Type I error)\n- False Negatives (Actual Positive:1 but Predict Negative:0) - 27 (Type II error)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **12. Classification Metrices** <a class=\"anchor\" id=\"12\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{},"cell_type":"markdown","source":"## **12.1 Classification Report** <a class=\"anchor\" id=\"12.1\"></a>\n\n[Notebook Contents](#0.1)\n\n- **Classification Report** is another way to evaluate the classification model performance. \n- It displays the **precision**, **recall**, **f1** and **support** scores for the model.\n- We can print a classification report as follows:-"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(y_test, y1_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **12.2 Classification Accuracy** <a class=\"anchor\" id=\"12.2\"></a>\n\n[Notebook Contents](#0.1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"TP = cm[0,0]\nTN = cm[1,1]\nFP = cm[0,1]\nFN = cm[1,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print classification accuracy\n\nclassification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.4f}'.format(classification_accuracy))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **12.3 Classification Error** <a class=\"anchor\" id=\"12.3\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print classification error\n\nclassification_error = (FP + FN) / float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **12.4 Precision** <a class=\"anchor\" id=\"12.4\"></a>\n\n[Notebook Contents](#0.1)\n\n- **Precision** can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP).\n\n- So, Precision identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n\n- Mathematically, precision can be defined as the ratio of TP to (TP + FP)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# print precision score\n\nprecision = TP / float(TP + FP)\n\nprint('Precision : {0:0.4f}'.format(precision))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **12.5 Recall** <a class=\"anchor\" id=\"12.5\"></a>\n\n[Notebook Contents](#0.1)\n\n- **Recall** can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). \n\n- **Recall** is also called **Sensitivity**.\n\n- Recall identifies the proportion of correctly predicted actual positives.\n\n- Mathematically, Recall can be given as the ratio of TP to (TP + FN)."},{"metadata":{"trusted":true},"cell_type":"code","source":"recall = TP / float(TP + FN)\n\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **12.6 True Positive Rate** <a class=\"anchor\" id=\"12.6\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- **True Positive Rate** is synonymous with **Recall**."},{"metadata":{"trusted":true},"cell_type":"code","source":"true_positive_rate = TP / float(TP + FN)\n\nprint('True Positive Rate : {0:0.4f}'.format(true_positive_rate))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **12.7 False Positive Rate** <a class=\"anchor\" id=\"12.7\"></a>\n\n[Notebook Contents](#0.1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"false_positive_rate = FP / float(FP + TN)\n\nprint('False Positive Rate : {0:0.4f}'.format(false_positive_rate))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **12.8 Specificity (True Negative Rate)** <a class=\"anchor\" id=\"12.8\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- **Specificity** is also called **True Negative Rate**."},{"metadata":{"trusted":true},"cell_type":"code","source":"specificity = TN / (TN + FP)\n\nprint('Specificity : {0:0.4f}'.format(specificity))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **12.9 f1-score** <a class=\"anchor\" id=\"12.9\"></a>\n\n[Notebook Contents](#0.1)\n\n- **f1-score** is the weighted harmonic mean of precision and recall. \n- The best possible f1-score would be 1.0 and the worst would be 0.0. \n- f1-score is the harmonic mean of precision and recall. \n- So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. \n- The weighted average of f1-score should be used to compare classifier models, not global accuracy."},{"metadata":{},"cell_type":"markdown","source":"## **12.10 Support** <a class=\"anchor\" id=\"12.10\"></a>\n\n[Notebook Contents](#0.1)\n\n- Support is the actual number of occurrences of the class in our dataset."},{"metadata":{},"cell_type":"markdown","source":"# **13. Cross Validation** <a class=\"anchor\" id=\"13\"></a>\n\n[Notebook Contents](#0.1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# iterate over classifiers and calculate cross-validation score\nfor name, clf in zip(names, classifiers):\n    scores = cross_val_score(clf, X1_train, y_train, cv = 10, scoring='accuracy')\n    print(name , ':{:.4f}'.format(scores.mean()))\n   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- It can be seen that cross-validation does not result in improved performance."},{"metadata":{},"cell_type":"markdown","source":"# **14. Hyperparameter Optimization using GridSearch CV** <a class=\"anchor\" id=\"14\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- I choose the top 3 classifiers with maximum accuracy for ensemble modeling.\n\n- They are `AdaBoost`, `LightGBM` and `Gradient Boosting`.\n\n- So, we will tune the hyperparameters of these models before proceeding.\n"},{"metadata":{},"cell_type":"markdown","source":"### **AdaBoost Classifier Parameters tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"abc_params = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"n_estimators\": [1, 2]\n             }\n\ndtc_clf = DecisionTreeClassifier(random_state = 0, max_features = \"auto\", class_weight = \"balanced\", max_depth = None)\n\nabc_clf = AdaBoostClassifier(base_estimator = dtc_clf)\n\n\nabc_grid_search = GridSearchCV(estimator = abc_clf,  \n                               param_grid = abc_params,\n                               scoring = 'accuracy',\n                               cv = 5,\n                               verbose=0)\n\n\nabc_grid_search.fit(X1_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the best model\n\n# best score achieved during the GridSearchCV\nprint('AdaBoost GridSearch CV best score : {:.4f}\\n\\n'.format(abc_grid_search.best_score_))\n\n# print parameters that give the best results\nprint('AdaBoost Parameters that give the best results :','\\n\\n', (abc_grid_search.best_params_))\n\n# print estimator that was chosen by the GridSearch\nabc_best = abc_grid_search.best_estimator_\nprint('\\n\\nXGBoost Estimator that was chosen by the search :','\\n\\n', (abc_best))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **LightGBM Parameters tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_clf = lgb.LGBMClassifier()\n\n\nlgb_params={'learning_rate': [0.005],\n    'num_leaves': [6,8,12,16],\n    'objective' : ['binary'],\n    'colsample_bytree' : [0.5, 0.6],\n    'subsample' : [0.65,0.66],\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2,1.4],\n    }\n\n\nlgb_grid_search = GridSearchCV(estimator = lgb_clf,  \n                               param_grid = lgb_params,\n                               scoring = 'accuracy',\n                               cv = 5,\n                               verbose=0)\n\n\nlgb_grid_search.fit(X1_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the best model\n\n# best score achieved during the GridSearchCV\nprint('LightGBM GridSearch CV best score : {:.4f}\\n\\n'.format(lgb_grid_search.best_score_))\n\n# print parameters that give the best results\nprint('LightGBM Parameters that give the best results :','\\n\\n', (lgb_grid_search.best_params_))\n\n# print estimator that was chosen by the GridSearch\nlgb_best = lgb_grid_search.best_estimator_\nprint('\\n\\nLightGBM Estimator that was chosen by the search :','\\n\\n', (lgb_best))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Gradient Boost Parameters tuning**"},{"metadata":{"trusted":true},"cell_type":"code","source":"gbc_clf = GradientBoostingClassifier()\n\ngbc_params = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngbc_grid_search = GridSearchCV(estimator = gbc_clf, \n                               param_grid = gbc_params, \n                               scoring = \"accuracy\", \n                               cv = 5,\n                               verbose = 0)\n\ngbc_grid_search.fit(X1_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# examine the best model\n\n# best score achieved during the GridSearchCV\nprint('Gradient Boosting GridSearch CV best score : {:.4f}\\n\\n'.format(gbc_grid_search.best_score_))\n\n# print parameters that give the best results\nprint('Gradient Boosting Parameters that give the best results :','\\n\\n', (gbc_grid_search.best_params_))\n\n# print estimator that was chosen by the GridSearch\ngbc_best = gbc_grid_search.best_estimator_\nprint('\\n\\nGradient Boosting Estimator that was chosen by the search :','\\n\\n', (gbc_best))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **15. Ensemble Modeling** <a class=\"anchor\" id=\"15\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- I decided to choose a voting classifier to combine the predictions coming from the above 3 classifiers.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"votingC = VotingClassifier(estimators=[('abc', abc_best), ('lgb',lgb_best), ('gbc',gbc_best)], voting='soft')\n\nvotingC = votingC.fit(X1_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **16. Submission** <a class=\"anchor\" id=\"16\"></a>\n\n[Notebook Contents](#0.1)\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nsubmission = pd.concat([IDtest,test_Survived],axis=1)\n\n\nsubmission.to_csv(\"titanic_submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **17. Conclusion** <a class=\"anchor\" id=\"17\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- In this notebook, we have build a classification model on the famous titanic dataset.\n\n- We have used a voting ensemble classifier for making predictions."},{"metadata":{},"cell_type":"markdown","source":"# **18. Credits** <a class=\"anchor\" id=\"18\"></a>\n\n[Notebook Contents](#0.1)\n\n\n- This notebook is based on couple of excellent notebooks on titanic dataset. These are -\n\n  - [Titanic top 4 % ensemble modeling](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling)\n\n  - [Titanic Survival Prediction End to End ML Pipeline](https://www.kaggle.com/poonaml/titanic-survival-prediction-end-to-end-ml-pipeline)\n  \n\n- We have adapted several lines of code from above notebooks.\n"},{"metadata":{},"cell_type":"markdown","source":"So, now we will come to the end of this notebook.\n\nI hope you find this kernel useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\nThank you\n"},{"metadata":{},"cell_type":"markdown","source":"[Go to Top](#0)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}