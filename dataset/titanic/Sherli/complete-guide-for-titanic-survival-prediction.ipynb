{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Titanic Survival Prediction**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"*The RMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after it collided with an iceberg during its maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard the ship, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. The RMS Titanic was the largest ship afloat at the time it entered service and was the second of three Olympic-class ocean liners operated by the White Star Line.*\n\n**The goal of this project is to complete the analysis of what sorts of people were likely to survive.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#  Table of Contents\n\n  1. Missing Data Analysis\n  2. Exploratory Data Analysis\n  3. Feature Engineering\n  4. Feature Selection\n  5. Feature Importance\n  6. Classification algorithmsand metrics explained\n  7. Hyperparameter Tuning\n  8. Ensemble Techniques for Prediction\n  9. Model Evaluation\n  10. Submission","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport  pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('../input/titanic/train.csv')\ntest_df=pd.read_csv('../input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()\n# Age, Cabin, Embarked has null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Missing values Imputation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"Gend_male = pd.DataFrame(train_df[train_df[\"Sex\"] == \"male\"])\nmean_age_male = Gend_male['Age'].mean()\n\nGend_female = pd.DataFrame(train_df[train_df[\"Sex\"] == \"female\"])\nmean_age_female = Gend_female['Age'].mean()\ntrain_df.loc[(train_df['Age'].isna()) & (train_df['Sex']=='male'), 'Age']=mean_age_male\ntrain_df.loc[(train_df['Age'].isna()) & (train_df['Sex']=='female'), 'Age']=mean_age_female\n\nGend_male_test = pd.DataFrame(test_df[test_df[\"Sex\"] == \"male\"])\nmean_age_male_test = Gend_male_test['Age'].mean()\n\nGend_female_test = pd.DataFrame(test_df[test_df[\"Sex\"] == \"female\"])\nmean_age_female_test = Gend_female_test['Age'].mean()\ntest_df.loc[(test_df['Age'].isna()) & (test_df['Sex']=='male'), 'Age']=mean_age_male\ntest_df.loc[(test_df['Age'].isna()) & (test_df['Sex']=='female'), 'Age']=mean_age_female\n\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Cabin'].fillna('NaN',inplace=True)\ntest_df['Cabin'].fillna('NaN',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Embarked'].fillna('Unknown',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Total number of survived and not survived**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nSurvival_rate = {'Survived_count': [0],\n                'Not_Survived_count' : [0],\n                'Total' : [0]}\n\nSurvival_rate['Survived_count'] =  train_df.Survived.value_counts()[0]\nSurvival_rate['Not_Survived_count'] =  train_df.Survived.value_counts()[1]\nSurvival_rate['Total'] =  Survival_rate['Survived_count'] + Survival_rate['Not_Survived_count']\n\n# Create the index \nindex_ = ['Survival_Rate'] \n  \n# Set the index \n\nSurvival=pd.DataFrame([Survival_rate])\nSurvival.index = index_ \nSurvival.transpose()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Survival.hist(figsize=(10,10),grid=False)\n#plt.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Survived and Not Survived by Age and Embarked**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ns_count = {'Not_Survived':[0],\n          'Survived':[0]}\nsf_count = {'Not_Survived':[0],\n          'Survived':[0]}\nc_count = {'Not_Survived':[0],\n          'Survived':[0]}\ncf_count = {'Not_Survived':[0],\n          'Survived':[0]}\nq_count = {'Not_Survived':[0],\n          'Survived':[0]}\nqf_count = {'Not_Survived':[0],\n          'Survived':[0]}\ns_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='S'),'Survived'].value_counts()[0]\ns_count['Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='S'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['Male passengers : Embarked S'] \n# Set the index \nmale_S=pd.DataFrame([s_count])\nmale_S.index = index_ \n#male_S.transpose()\n#male_S\nsf_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='S'),'Survived'].value_counts()[0]\nsf_count['Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='S'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['Female passengers : Embarked S'] \n# Set the index \nfemale_S=pd.DataFrame([sf_count])\nfemale_S.index = index_\nc_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='C'),'Survived'].value_counts()[0]\nc_count['Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='C'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['Male passengers : Embarked C'] \n# Set the index \nmale_C=pd.DataFrame([c_count])\nmale_C.index = index_ \n#male_C\ncf_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='C'),'Survived'].value_counts()[0]\ncf_count['Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='C'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['Female passengers : Embarked C'] \n# Set the index \nfemale_C=pd.DataFrame([cf_count])\nfemale_C.index = index_ \nq_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='Q'),'Survived'].value_counts()[0]\nq_count['Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='Q'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['Male passengers : Embarked Q'] \n# Set the index \nmale_Q=pd.DataFrame([q_count])\nmale_Q.index = index_ \n#male_Q\nqf_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='Q'),'Survived'].value_counts()[0]\nqf_count['Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='Q'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['female passengers : Embarked Q'] \n# Set the index \nfemale_Q=pd.DataFrame([qf_count])\nfemale_Q.index = index_ \n#male_Q\nframes=[male_S,female_S,male_C,female_C,male_Q,female_Q]\nresult=pd.concat(frames)\nGender_Embarked = pd.DataFrame(result)\nGender_Embarked\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Gender_Embarked[[\"Not_Survived\",\"Survived\"]].plot(kind=\"bar\",stacked=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Survived and Non Survived male and female by PClass**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\na = {'Not_Survived':[0],\n          'Survived':[0]}\nb = {'Not_Survived':[0],\n          'Survived':[0]}\nc = {'Not_Survived':[0],\n          'Survived':[0]}\nd = {'Not_Survived':[0],\n          'Survived':[0]}\ne = {'Not_Survived':[0],\n          'Survived':[0]}\nf = {'Not_Survived':[0],\n          'Survived':[0]}\na=train_df.loc[(train_df['Sex']=='female') & (train_df['Pclass']== 1),'Survived'].value_counts()\n#Create the index \nindex_ = ['Female passengers : PClass 1'] \n# Set the index \nfemale_1=pd.DataFrame([a])\nfemale_1.index = index_ \nb=train_df.loc[(train_df['Sex']=='female') & (train_df['Pclass']== 2),'Survived'].value_counts()\n#Create the index \nindex_ = ['Female passengers : PClass 2'] \n# Set the index \nfemale_2=pd.DataFrame([b])\nfemale_2.index = index_ \nc=train_df.loc[(train_df['Sex']=='female') & (train_df['Pclass']== 3),'Survived'].value_counts()\n#Create the index \nindex_ = ['Female passengers : PClass 3'] \n# Set the index \nfemale_3=pd.DataFrame([c])\nfemale_3.index = index_ \nd=train_df.loc[(train_df['Sex']=='male') & (train_df['Pclass']== 1),'Survived'].value_counts()\n#Create the index \nindex_ = ['Male passengers : PClass 1'] \n# Set the index \nmale_1=pd.DataFrame([d])\nmale_1.index = index_ \ne=train_df.loc[(train_df['Sex']=='male') & (train_df['Pclass']== 2),'Survived'].value_counts()\n#Create the index \nindex_ = ['Male passengers : PClass 2'] \n# Set the index \nmale_2=pd.DataFrame([e])\nmale_2.index = index_ \nf=train_df.loc[(train_df['Sex']=='male') & (train_df['Pclass']== 3),'Survived'].value_counts()\n#Create the index \nindex_ = ['Male passengers : PClass 3'] \n# Set the index \nmale_3=pd.DataFrame([f])\nmale_3.index = index_ \n#male_Q\nframes=[male_1,female_1,male_2,female_2,male_1,female_2,male_3,female_3]\nresult=pd.concat(frames)\nGender_PClass = pd.DataFrame(result)\nGender_PClass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ax2 = Gender_PClass.plot.pie(subplots=True,figsize=(20,20), autopct='%1.1f%%',shadow=True)\nplt.legend(loc='center left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Survived and not survived by Pclass**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x='Pclass',hue='Survived',data=train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Survived and Not Survived by Embarked","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.FacetGrid(train_df,col=\"Survived\")\nax=ax.map(plt.hist,'Age',color=\"g\",bins=10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorize by Age bins, PClass and Sex**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.catplot(x=\"Pclass\",y=\"Age\",hue=\"Sex\",data=train_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"The feature engineering process includes :\n    1. Testing features.\n    2. Deciding what features to create.\n    3. Creating features.\n    4. Checking how the features work with your model.\n    5. Improving your features if needed.\n    6. Create more features until the work is done.\nFeature Engineering techniques include imputation, handling outliers, binning, log transform one-hot encoding, grouping operations, feature split, scaling and extracting date.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"***Computation of Age Bins*******","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['age_bins'] = pd.cut(x=train_df['Age'], bins=8, labels=False, retbins=False, include_lowest=True)\ntest_df['age_bins'] = pd.cut(x=test_df['Age'], bins=8, labels=False, retbins=False, include_lowest=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Computation of Fare Range****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Fare_cat']=0\ntrain_df.loc[train_df['Fare']<=7.91,'Fare_cat']=0\ntrain_df.loc[(train_df['Fare']>7.91)&(train_df['Fare']<=14.454),'Fare_cat']=1\ntrain_df.loc[(train_df['Fare']>14.454)&(train_df['Fare']<=31),'Fare_cat']=2\ntrain_df.loc[(train_df['Fare']>31)&(train_df['Fare']<=93.5),'Fare_cat']=3\ntrain_df.loc[(train_df['Fare']>93.5)&(train_df['Fare']<=164.8667),'Fare_cat']=4\ntrain_df.loc[(train_df['Fare']>164.8667)&(train_df['Fare']<=512.3292),'Fare_cat']=5\n\ntest_df['Fare_cat']=0\ntest_df.loc[test_df['Fare']<=7.91,'Fare_cat']=0\ntest_df.loc[(test_df['Fare']>7.91)&(test_df['Fare']<=14.454),'Fare_cat']=1\ntest_df.loc[(test_df['Fare']>14.454)&(test_df['Fare']<=31),'Fare_cat']=2\ntest_df.loc[(test_df['Fare']>31)&(test_df['Fare']<=93.5),'Fare_cat']=3\ntest_df.loc[(test_df['Fare']>93.5)&(test_df['Fare']<=164.8667),'Fare_cat']=4\ntest_df.loc[(test_df['Fare']>164.8667)&(test_df['Fare']<=512.3292),'Fare_cat']=5","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extract Initials from the Name feature. Categorize the Initials by different values**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"name = train_df['Name']\n#Extract the initials\ntrain_df['Title'] = name.str.extract(pat = \"(Mr|Master|Mrs|Miss|Major|Rev|Lady|Dr|Mme|Mlle|Col|Capt)\\\\.\")\ntest_df['Title'] = name.str.extract(pat = \"(Mr|Master|Mrs|Miss|Major|Rev|Lady|Dr|Mme|Mlle|Col|Capt)\\\\.\")\ntrain_df['Title'].astype(str)\ntest_df['Title'].astype(str)\n#Assign Rare for the rare initials\ntrain_df.Title[train_df.Title == 'Rev'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Major'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Lady'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Dr'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Mme'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Mlle'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Col'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Capt'] = 'Rare'\n\ntest_df.Title[test_df.Title == 'Rev'] = 'Rare'\ntest_df.Title[test_df.Title == 'Major'] = 'Rare'\ntest_df.Title[test_df.Title == 'Lady'] = 'Rare'\ntest_df.Title[test_df.Title == 'Dr'] = 'Rare'\ntest_df.Title[test_df.Title == 'Mme'] = 'Rare'\ntest_df.Title[test_df.Title == 'Mlle'] = 'Rare'\ntest_df.Title[test_df.Title == 'Col'] = 'Rare'\ntest_df.Title[test_df.Title == 'Capt'] = 'Rare'\n# Categorize the Initial\ntrain_df['Title'].replace(['Mr','Mrs','Miss','Master','Rare'],[1,2,3,4,5],inplace=True)\ntest_df['Title'].replace(['Mr','Mrs','Miss','Master','Rare'],[1,2,3,4,5],inplace=True)\n#train_df\n\n# Missing values Imputation\ntrain_df['Title'].fillna(0,inplace=True)\ntest_df['Title'].fillna(0,inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorize sex to numeric variable**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Sex'].replace(['male','female'],[0,1],inplace=True)\ntest_df['Sex'].replace(['male','female'],[0,1],inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Categorize Embarked to numeric variable**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df['Embarked'].unique()\ntrain_df['Embarked'] = train_df['Embarked'].map({'S': 1, 'C': 2, 'Q': 3,'Unknown':0} ).astype(int)\ntest_df['Embarked'] = test_df['Embarked'].map({'S': 1, 'C': 2, 'Q': 3,'Unknown':0} ).astype(int)\n#dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Family Size Computation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntrain_df['Family_Size']=0\ntrain_df['Family_Size'] = train_df['SibSp'] + train_df['Parch']\ntrain_df['IsAlone']=0\ntrain_df.loc[(train_df['Family_Size']==1),'IsAlone']=1\ntrain_df.loc[(train_df['Family_Size']==0) | (train_df['Family_Size']>1),'IsAlone']=0\n\ntest_df['Family_Size']=0\ntest_df['Family_Size'] = test_df['SibSp'] + test_df['Parch']\ntest_df['IsAlone']=0\ntest_df.loc[(test_df['Family_Size']==1),'IsAlone']=1\ntest_df.loc[(test_df['Family_Size']==0) | (test_df['Family_Size']>1),'IsAlone']=0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Feature selection is one of the core concepts in machine learning which hugely impacts the performance of the model. Irrelavant or partially relavant features can negatively impact the performance of the model. Feature selection and data cleaning should be the first and most important step of your model designing.\n**Benefits of Feature Selection :**\n    1. Reduces Overfitting\n    2. Improves accuracy\n    3. Reduces training time\n**Feature Selection Methods :**\n    1. Intrinsic\n    2. Wrapper methods\n    3. Filter methods\nThe **intrinsic method** uses an algorithm ExtraTrees classifier.\nThe **Wrapper method** uses techniques such as Forward feature selection, Backward elimination, Recursive feature elimination.\nThe **filter method** is divided into two types. Statistical approach and Feature Importance. The statistical approaches are Pearson Coefficient, Spearman Coefficient, ANOVA, Chisquared Test and mutual information test. Corelation Heatmap is drawn to identify the feature importances.\nThe **embedded method** is a combination of wrapper and filter methods. The techniques in embedded methods are Ridge regression and Lasso regression.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Corelation of all the attributes by Heatmap**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train_df.corr(), annot=True).set_title(\"Corelation of attributes\")\nfig=plt.gcf()\nfig.set_size_inches(15,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The positive corelated attributes are age : age_bins (0.97), SibSp : Family_size (0.89), Parch : Family_size (0.78), Sex:Title (0.58), Survived : Sex(0.54). Negative corelated : Name, Ticket, Cabin, Passenger_Id, Fare, Fare_Cat, Age","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(test_df.corr(), annot=True).set_title(\"Corelation of attributes\")\nfig=plt.gcf()\nfig.set_size_inches(15,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Importance by ExtraTreeClassifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\ny = train_df['Survived'] \n#X = pd.DataFrame(train_df)\n#df.drop(['A'], axis = 1)\nX = train_df.drop(['Survived','Name','Ticket','Cabin'],axis=1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the model \nextra_tree_forest = ExtraTreesClassifier(n_estimators = 5,criterion ='entropy', max_features = 5) \n  \n# Training the model \nextra_tree_forest.fit(X, y) \n  \n# Computing the importance of each feature \nfeature_importance = extra_tree_forest.feature_importances_ \n  \n# Normalizing the individual importances \nfeature_importance_normalized = np.std([tree.feature_importances_ for tree in extra_tree_forest.estimators_], axis = 0) \n\n# Plotting a Bar Graph to compare the models \nplt.figure(figsize=(20,6))\nplt.bar(X.columns,feature_importance_normalized,align='edge', width=0.3) \nplt.xlabel('Feature Labels') \nplt.ylabel('Feature Importances') \nplt.title('Comparison of different Feature Importances') \n\nplt.show() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Chisquare Test for Feature Selection**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2 \nX.shape\n# Two features with highest chi-squared statistics are selected \nchi2_features = SelectKBest(chi2, k = 9) \nX_kbest_features = chi2_features.fit_transform(X, y) \n  \n# Reduced features \nprint('Original feature number:', X.shape[1]) \nprint('Reduced feature number:', X_kbest_features.shape[1])\n#X\nX_kbest_features\n#PClass, Age, Sex, Fare, Fare_cat, Title, Family Size - Top 7 features\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction - Classification Algorithms","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the feature selection techniques, retaining the most important features in the dataset for prediction and dropping the unnecessary features.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Drop unnecessary columns in train and test set before predictions**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(\"Name\", axis=1)\ntrain_df = train_df.drop(\"Ticket\", axis=1)\ntrain_df = train_df.drop(\"Cabin\", axis=1)\ntrain_df = train_df.drop(\"Fare\", axis=1)\n#train_df = train_df.drop(\"Embarked\", axis=1)\ntrain_df = train_df.drop(\"IsAlone\", axis=1)\n#train_df = train_df.drop(\"SibSp\", axis=1)\n#train_df = train_df.drop(\"Parch\", axis=1)\n#train_df = train_df.drop(\"Pclass\", axis=1)\n#train_df = train_df.drop(\"Age\", axis=1)\ntrain_df = train_df.drop(\"age_bins\", axis=1)\ntrain_df = train_df.drop(\"PassengerId\", axis=1)\n\n\ntest_df = test_df.drop(\"Name\", axis=1)\ntest_df = test_df.drop(\"Ticket\", axis=1)\ntest_df = test_df.drop(\"Cabin\", axis=1)\ntest_df = test_df.drop(\"Fare\", axis=1)\n#test_df = test_df.drop(\"Embarked\", axis=1)\ntest_df = test_df.drop(\"IsAlone\", axis=1)\n#test_df = test_df.drop(\"SibSp\", axis=1)\n#test_df = test_df.drop(\"Parch\", axis=1)\n#test_df = test_df.drop(\"Pclass\", axis=1)\n#test_df = test_df.drop(\"Age\", axis=1)\ntest_df = test_df.drop(\"age_bins\", axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Corelation map (Heatmap) after Feature Selection and dropping columns for prediction**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(train_df.corr(), annot=True).set_title(\"Corelation of attributes\")\nfig=plt.gcf()\nfig.set_size_inches(15,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Split the data into train and test set for classifcation predictions**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = train_df.drop(\"Survived\", axis=1)\ny_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Logistic Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n#acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nlogistic_score = round(model.score(X_train,y_train) * 100,2)\nprint(logistic_score)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SVM Classification**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"RBF SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\nmodel = svm.SVC(kernel='rbf',C=1,gamma=0.1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nacc_svc = round(model.score(X_train, y_train) * 100, 2)\nacc_svc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Linear SVC**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nacc_l_svc = round(model.score(X_train, y_train) * 100, 2)\nacc_l_svc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KNN Classification**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#KNN Classification\na_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i)\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    a=a.append(pd.Series(model.score(X_train,y_train)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())\nacc_knn = round(model.score(X_train,y_train)*100,2)\nacc_knn","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gaussian Naive Bayes Classification**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model=GaussianNB()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nacc_gaus = round(model.score(X_train,y_train)*100,2)\nacc_gaus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Decision Tree Classification**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nacc_dec = round(model.score(X_train, y_train) * 100, 2)\nacc_dec","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Feature Selection using RFE (Recursive Feature Elimination)**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nrfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\nrfe.fit(X_train, y_train)\ny_pred = rfe.predict(X_test)\nrfe.score(X_train, y_train)\nrfe_scc = round(rfe.score(X_train, y_train) * 100, 2)\n\nfor i in range(X_train.shape[1]):\n    print('Column: %d, Name: %s, Selected %s, Rank: %.3f' % (i, X_train.columns[i],rfe.support_[i], rfe.ranking_[i]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lists of the 6 input features and whether or not they were selected as well as their relative ranking of importance.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Classification**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\ny_pred_ran = model.predict(X_test)\nmodel.score(X_train, y_train)\nacc_forest = round(model.score(X_train, y_train) * 100, 2)\nacc_forest","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Cross Validation**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(12,5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Confusion Matrix**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nf,ax=plt.subplots(3,3,figsize=(12,10))\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\n\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\n\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\nax[0,2].set_title('Matrix for KNN')\n\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\n\ny_pred = cross_val_predict(LogisticRegression(),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\nax[1,1].set_title('Matrix for Logistic Regression')\n\ny_pred = cross_val_predict(DecisionTreeClassifier(),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\n\ny_pred = cross_val_predict(GaussianNB(),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\n\nplt.subplots_adjust(hspace=0.2,wspace=0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Hyperparameter Tuning**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A hyperparameter is a parameter whose value is set before the learning process begins. Hyperparameter tuning is choosing a set of optimal parameters for a learning algorithm. Two different methods for optimizing hyperparameters : **GridSearch** and **RandomSearch**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hyperparameters tuning for Kernel SVM, Decision Tree and Random Forest.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Kernel SVM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nC=[2,2.1,2.5]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel,'C':C,'gamma':gamma}\ngd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\ngd.fit(X_train,y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random Forest Classification","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"n_estimators=range(100,1000,1100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\ngd.fit(X_train,y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best score for RBF-SVM is 82.2697 with C=1 and gamma = 0.5. The best score for Random Forest is 82.0456 with n_estimators=200","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Ensemble Algorithms**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Ensemble methods are tecghniques that create multiple models and then combine them to produce improved results. Ensemble methods usually produce more accurate solutions than a single model would.\nIn ensemble algorithms, **bagging methods** form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction.\n**Boosting** is an ensemble meta-algorithm for primarily reducing bias, and also variance.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Voting Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(C=0.05)),\n                                              ('DT',DecisionTreeClassifier(random_state=0)),\n                                              ('NB',GaussianNB()),\n                                              ('svm',svm.SVC(kernel='linear',probability=True))\n                                             ], \n                       voting='soft').fit(X_train,y_train)\nprint('The accuracy for ensembled model is:',round(ensemble_lin_rbf.score(X_train,y_train)*100,2))\nvot = round(ensemble_lin_rbf.score(X_train,y_train)*100,2)\ncross=cross_val_score(ensemble_lin_rbf,X_train,y_train, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bagging**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Bagging also known as Bootstrap aggregation is a way to decrease the variance in the prediction and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method.\nBagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of n_neighbours, as small value of n_neighbours.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Bagged KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(X_train,y_train)\nprediction=model.predict(X_test)\nprint('The accuracy for bagged KNN is:',round(model.score(X_train,y_train)*100,2))\nbag_knn = round(model.score(X_train,y_train)*100,2)\nresult=cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',round(result.mean()*100,2))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Bagged DecisionTree","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\nmodel.fit(X_train,y_train)\nprediction=model.predict(X_test)\nprint('The accuracy for bagged Decision Tree is:',round(model.score(X_train,y_train)*100,2))\nbag_ran = round(model.score(X_train,y_train)*100,2)\nresult=cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:',round(result.mean()*100,2))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Boosting","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Boosting is an ensemble method for improving the model predictions of any given learning algorithm. The idea of boosting is to train weak learners sequentially, each trying to correct its predecessor.\nBoosting is an iterative technique which adjusts the weight of an observation based on the last classification.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Ada Boost Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ada = AdaBoostClassifier(random_state=1,n_estimators=1000)\nada.fit(X_train, y_train)\ny_pred = ada.predict(X_test)\nada.score(X_train, y_train)\nada_boost = round(ada.score(X_train, y_train) * 100, 2)\nada_boost\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient Boosting**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"grad = GradientBoostingClassifier(n_estimators=1000,learning_rate=0.01,random_state=0)\ngrad.fit(X_train, y_train)\ny_pred = grad.predict(X_test)\ngrad.score(X_train, y_train)\ngrad_boost = round(grad.score(X_train, y_train) * 100, 2)\ngrad_boost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extreme Gradient Boosting**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"extreme = xgb.XGBClassifier(n_estimators=1000,learning_rate=0.1)\nextreme.fit(X_train,y_train)\ny_pred = extreme.predict(X_test)\nextreme.score(X_train, y_train)\nextreme_boost = round(extreme.score(X_train, y_train) * 100, 2)\nextreme_boost","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Confusion matrix for the best model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb=XGBClassifier(n_estimators=1000,random_state=0,learning_rate=0.01)\nresult=cross_val_predict(xgb,X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,result),cmap='winter',annot=True,fmt='2.0f')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Feature Importance","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nf,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=1000,random_state=0)\nmodel.fit(X_train,y_train)\npd.Series(model.feature_importances_,X_train.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\n\nmodel=AdaBoostClassifier(n_estimators=1000,learning_rate=0.01,random_state=0)\nmodel.fit(X_train,y_train)\npd.Series(model.feature_importances_,X_train.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\n\nmodel=GradientBoostingClassifier(n_estimators=1000,learning_rate=0.1,random_state=0)\nmodel.fit(X_train,y_train)\npd.Series(model.feature_importances_,X_train.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\n\nmodel=xgb.XGBClassifier(n_estimators=1000,learning_rate=0.1)\n#extreme = xgb.XGBClassifier(n_estimators=1000,learning_rate=0.1)\nmodel.fit(X_train,y_train)\npd.Series(model.feature_importances_,X_train.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"],\"Survived\": y_pred_ran})\nsubmission\nsubmission.to_csv('submission.csv',index=False)\n#test_df=pd.read_csv('../input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = pd.DataFrame({'Model': ['Radial SVC', 'KNN', 'Logistic Regression','Random Forest', 'Naive Bayes', 'Linear SVC', 'Decision Tree','VotingClassifier','Bagged KNN','Bagged DecisionTree','AdaBoost','GradientBoost','XGBoost'],\n    'Score': [acc_svc, acc_knn, logistic_score,acc_forest, acc_gaus, acc_l_svc, acc_dec,vot,bag_knn,bag_ran,ada_boost,grad_boost,extreme_boost]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}