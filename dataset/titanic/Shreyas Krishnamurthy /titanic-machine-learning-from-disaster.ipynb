{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly import tools\n\n# matplotlib\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"735af64f014ce264bfae3729127765d6e240b67f"},"cell_type":"markdown","source":"## Titanic: Machine Learning from Disaster\n\nThis kernel uses solve the prediction of survival of a person on the Titanic, using a Neural Network of Sequential Model from Keras API for TensorFlow.\n\nFurther, I will be using Grid Search for Hyperparameter tuning of the model."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## 1. Data Understanding\n\n[](http://)First let us take a look at how our data looks like, to get more details on the characteristics of the attributes/features."},{"metadata":{"_uuid":"2414bbc36cdb7bdfd26f51ac77be8d4da90ec92e"},"cell_type":"markdown","source":"**1.1 Peek into the Data:**"},{"metadata":{"trusted":true,"_uuid":"6f38d8cef32b49a8e0d733e8b9fc5b67ea0d6d4c","scrolled":true},"cell_type":"code","source":"# Reading the train and the test data.\ntrainData = pd.read_csv('../input/train.csv')\ntestData = pd.read_csv('../input/test.csv')\n\n# Displaying a sample of the train data to get more detailed info\ntrainData.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d90878191b339f5bbf351a18303da5a7eb9ea775"},"cell_type":"markdown","source":"**1.2  Summary of the data:**"},{"metadata":{"trusted":true,"_uuid":"2c7bb8c4f8044e894e337bbe9ed00c1508b02c9e"},"cell_type":"code","source":"trainData.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"656c84136252b9ca840fb82b43efea7493f639d2"},"cell_type":"markdown","source":"**1.3 Some insights :**\n* The data set consists of 12 attributes (PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked)\n    1. PassengerId - Numeric data, it is unique for each passenger, not useful.\n    2. Survived - Target data to be predicted, with incators 0 (Passenger Died) or 1 (Passenger Survived).\n    3. Pclass - Numeric data, has 3 categories (1st, 2nd or 3rd), useful.\n    4. Name - String data type, mostly unique for each passenger, not useful.\n    5. Sex - Categorical data, has 2 categories (male, female), useful.\n    6. Age - Numeric data, max age is 80, it contains few nulls (can be imputed), useful.\n    7. SibSp - Numeric data, max number of siblings / spouses aboard is 8, useful.\n    8. Parch - Numeric data, max number of parents / children aboard is 6, useful.\n    9. Ticket - Categorical data, it has a prefix to the number,  it can be useful.\n    10. Fare - Numeric data, higher the class, higher is the fare (need to check), max fare is 512,32 (currency is $ I guess) , useful.\n    11. Cabin - Categorical data, it contains few nulls (can be imputed), it can be useful.\n    12. Embarked - Categorical data, with 3 categories, C = Cherbourg, Q = Queenstown, S = Southampton, it contains few nulls (can be imputed), useful. \n* The attribute 'Survived' would be the attribute to be predicted, i.e, target attribute.\n* Based on the values in the target attribute, i.e., 0/1 which states if the passenger survived or not, it is a Binary Classification."},{"metadata":{"_uuid":"aa5d840dc5628ce3a3abe9014cb0c9fae4ba92a9"},"cell_type":"markdown","source":"## 2. Exploratory Data Analysis"},{"metadata":{"_uuid":"590288a17d5117b51d53371d960697f001d629e9"},"cell_type":"markdown","source":"**2.1 Data Types:**\n\nLet us take a look at the type of data being handled."},{"metadata":{"trusted":true,"_uuid":"9ac5cabf6d3191f896e678af075e43200b0e7c8f"},"cell_type":"code","source":"trainData.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9654acb9d1ac246fbc5b987dffcec6b5a675cfbf"},"cell_type":"markdown","source":"**2.2 Missing Data:**\n\nChecking and calculating the amount of missing values in the dataset.\n\nThese missing values will be handled later."},{"metadata":{"trusted":true,"_uuid":"20d8074d411818242af483eeeabab7bf118e5a84"},"cell_type":"code","source":"trainData.apply(lambda x: x.isnull().any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ba9232c55452f875c998faffc69a178ba79df4a"},"cell_type":"code","source":"pd.DataFrame({'percent_missing': trainData.isnull().sum() * 100 / len(trainData)})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd043cffba321a12165212fda83c5dbdc093fdd3"},"cell_type":"markdown","source":"**2.3 Unique Values:**\n\nChecking for the amount of unique values in each feature."},{"metadata":{"trusted":true,"_uuid":"8aa1f6fa076e91e6301207147edf9ecfad85444c"},"cell_type":"code","source":"pd.DataFrame({'percent_unique': trainData.apply(lambda x: x.unique().size/x.size*100)})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82d660ddc74125520bd2fc2c810549e47871d1c7"},"cell_type":"markdown","source":"From the results of sections 2.2 and 2.3, we see that the features/attributes Cabin, PassengerId and Name can be eliminated from the required features to build the model since, the feature Cabin has 77.1% of missing values, and the features PassengerId/Name has 100% unique features."},{"metadata":{"trusted":true,"_uuid":"a0b9ea4ddd46d4d07fa4e4acbec4dd1d2b0aecae"},"cell_type":"code","source":"# Names of the features extarcted from the data\nselFeatures = list(trainData.columns.values)\n# Removing the target variable from the column values\ntargetCol = 'Survived'\nselFeatures.remove(targetCol)\n\n# Removing features with unique values\nfor i in selFeatures:\n    if trainData.shape[0] == len(pd.Series(trainData[i]).unique()) :\n        selFeatures.remove(i)\n        \n# Removing features with high percentage of missing values\nselFeatures.remove('Cabin')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5213c82d4b187e9b42f23d9e9f700849bf16ec5e"},"cell_type":"markdown","source":"**2.4 Visualizations:**\n\nVisualizing the data using interactive plots. Starting with a matrix scatter plot showing the relation between the features that will be used for the training. Followed by plots to check the survival rate based on features."},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"92b6bc6fb6f0438009fd15766fe3e7ca5044e10e","scrolled":true},"cell_type":"code","source":"import seaborn as sns\nsns.set(style=\"ticks\")\nplotFeatures = [x for x in selFeatures]\nplotFeatures.append(\"Survived\")\nsns.pairplot(trainData[plotFeatures], hue=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f12cc8eb8ad88a3022bd13a9c7f239c0c826162f"},"cell_type":"code","source":"targetClass = trainData.Survived.value_counts().values.tolist()\ndata = [go.Pie(labels=['Died','Survived'], values=targetClass,\n              hoverinfo='label+percent', textinfo='value')]\nlayout = dict(\n        title = \"Comparison of Classes (Died/Survived)\",\n        autosize=False,\n        width=500,\n        height=500\n    )\n\nfig = dict(data=data, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"155bda2b13e7d0347bfac3c1d61b4a03ace14062"},"cell_type":"code","source":"def plotGraph(plotData,msg):\n    trace1 = go.Bar(\n    x=plotData.columns.values,\n    y=plotData.values[0],\n    name='No'\n    )\n    trace2 = go.Bar(\n        x=plotData.columns.values,\n        y=plotData.values[1],\n        name='Yes'\n    )\n    data = [trace1, trace2]\n    layout = dict(\n        title = msg,\n        xaxis= dict(title = plotData.columns.name),\n        yaxis= dict(title= 'Number of people'),\n        barmode='group',\n        autosize=False,\n        width=800,\n        height=500\n    )\n    fig = dict(data=data, layout=layout)\n    iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34fee116f43a3cf194500e309db4673370c93173"},"cell_type":"code","source":"pclass = pd.crosstab([trainData.Survived], trainData.Pclass)\nplotGraph(pclass,'Survived based on Pclass')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d95317ac0e030e5464f71ec5583dd1e5bf130c6"},"cell_type":"code","source":"sex = pd.crosstab([trainData.Survived], trainData.Sex)\nplotGraph(sex, 'Survived based on sex')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93875078bb049438079a73e3c2ea63d10dff4401"},"cell_type":"code","source":"embarked = pd.crosstab([trainData.Survived], trainData.Embarked)\nplotGraph(embarked, 'Survived based on embarked')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a83dae2a65e836b41963c8fc0469de96f6ca2e65"},"cell_type":"code","source":"SibSp = pd.crosstab([trainData.Survived], trainData.SibSp)\nplotGraph(SibSp, 'Survived based on SibSp')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ea7caa665e49f6ebb78c138b2722b9e8b632d7d"},"cell_type":"code","source":"Parch = pd.crosstab([trainData.Survived], trainData.Parch)\nplotGraph(Parch, 'Survived based on Parch')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7a0212549372cea0da312ba9bdb2aea9c68dcd89","_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"def plotLine(plotData,msg):\n    trace1 = go.Scatter(\n    x=plotData.columns.values,\n    y=plotData.values[0],\n    mode='lines',\n    name='No'\n    )\n    trace2 = go.Scatter(\n        x=plotData.columns.values,\n        y=plotData.values[1],\n        mode='lines',\n        name='Yes'\n    )\n    data = [trace1, trace2]\n    layout = dict(\n        title = msg,\n        xaxis= dict(title = plotData.columns.name),\n        yaxis= dict(title= 'Number of people'),\n        autosize=False,\n        width=800,\n        height=500\n    )\n    fig = dict(data=data, layout=layout)\n    iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23f050f613f9f3e6522268f1a1ec88c4c6e1aba6"},"cell_type":"code","source":"Age = pd.crosstab([trainData.Survived],trainData.Age)\nplotLine(Age,'Survival based on Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69811b64181c02a50e01b16ff0b7d2ef5f59196f"},"cell_type":"code","source":"Fare = pd.crosstab([trainData.Survived],trainData.Fare)\nplotLine(Fare,'Survival based on Fare')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07ed161b88ba5cb6e1b83bee89b39be51e2b9f87"},"cell_type":"markdown","source":"## 3. Data Preparation"},{"metadata":{"_uuid":"5cab4d617cbd75cedca1dc2a37d94a8530465a8f"},"cell_type":"markdown","source":"**3.1 Feature Selection**\n\nSelecting the columns required for building the model based on EDA."},{"metadata":{"trusted":true,"_uuid":"befb00eb9e6fde5ee19a7078fd2e7b418f093f9b"},"cell_type":"code","source":"# Also removing cabin and ticket features for the initial run.\nselFeatures.remove('Ticket')\n        \nprint(\"Target Class: '\"+ targetCol + \"'\")\nprint('Features to be investigated: ')\nprint(selFeatures)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3f945393b5bfcabc92a1cdf0b4af026fe0cd347"},"cell_type":"code","source":"def handle_categorical_na(df):\n    ## replacing the null/na/nan values in 'Cabin' attribute with 'X'\n#     df.Cabin = df.Cabin.fillna(value='X')\n#     ## Stripping the string data in 'Cabin' and 'Ticket' features of numeric values and duplicated characters\n#     df.Cabin = [''.join(set(filter(str.isalpha, s))) for s in df.Cabin]\n#     df.Ticket = [''.join(set(filter(str.isalpha, s))) for s in df.Ticket]\n#     ## replacing the '' values in 'Ticket' attribute with 'X'\n#     df.Ticket.replace(to_replace='',value='X',inplace=True)\n    ## Imputing the null/na/nan values in 'Age' attribute with its mean value \n    df.Age.fillna(value=df.Age.mean(),inplace=True)\n    ## replacing the null/na/nan values in 'Embarked' attribute with 'X'\n    df.Embarked.fillna(value='X',inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"737aa1c24f07833e7d0d398021a6ce3550cead08"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nseed = 7\nnp.random.seed(seed)\nX_train, X_test, Y_train, Y_test = train_test_split(trainData[selFeatures], trainData.Survived, test_size=0.2)\n\nX_train = handle_categorical_na(X_train)\nX_test = handle_categorical_na(X_test)\n\n## using One Hot Encoding for handling categorical data\nX_train = pd.get_dummies(X_train,columns=['Embarked','Sex'],prefix=['Embarked','Sex'])\nX_test = pd.get_dummies(X_test,columns=['Embarked','Sex'],prefix=['Embarked','Sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33bc4ffb6acf4ee753040ac147322e292f310e41"},"cell_type":"code","source":"common_col = [x for x in X_test.columns if x in X_train.columns]\nX_test = X_test[common_col]\n\nmissing_col = [x for x in X_train.columns if x not in X_test.columns]\n## Inserting missing columns in test data\nfor val in missing_col:\n    X_test.insert(X_test.shape[1], val, pd.Series(np.zeros(X_test.shape[0])))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9168f4459f77102deae432c99b7c70cb3e4fa6aa"},"cell_type":"markdown","source":"## 4. Modeling"},{"metadata":{"_uuid":"8c2fb40b64bd690f97d5daf95507d2ac146fbcf6"},"cell_type":"markdown","source":"**4.1 Neural Network**\n\nUsing Sequential model of Keras API to build a neural network. The model is tuned and optimezed using Grid Serach.\n\nThe optimized parameters are:\n* Number of neurons in input and hidden layers\n* Activation type\n* Optimizer \n* Epochs\n* Batch size\n* Dropout ratio\n\nDue to increased processing time, the best parameters were found and used. "},{"metadata":{"trusted":true,"_uuid":"fc9d3c1ad91f8cacdd70708bd438abaa90909ea8"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.constraints import maxnorm\n\nin_shape = X_train.shape[1]\n\ndef create_model(optimizer='Adam', neurons=50):\n    # Initialize the constructor\n    model = Sequential()\n    # Input - Layer\n    model.add(Dense(neurons, input_dim=in_shape, activation=activation))\n    # Hidden - Layers\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(neurons, activation = activation))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(neurons, activation = activation))\n    # Output- Layer\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model\n\ndef nn_hyperparameter_optimization():\n    model = KerasClassifier(build_fn=create_model, verbose=0)\n    # defining the grid search parameters\n    neurons = [65, 75, 85]\n    batch_size= [10, 20, 30, 40]\n    epochs= [10, 20, 30, 40]\n    optimizer = ['RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n    activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n    dropout_rate = [0.1, 0.2, 0.3, 0.4, 0.5]\n    param_grid = dict(neurons=neurons,\n                      optimizer=optimizer,\n                      batch_size=batch_size,\n                      epochs=epochs,\n                      activation=activation,\n                      dropout_rate=dropout_rate)\n    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=5)\n    grid_result = grid.fit(X_train, Y_train)\n\n    # summarize results\n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    means = grid_result.cv_results_['mean_test_score']\n    stds = grid_result.cv_results_['std_test_score']\n    params = grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n    return grid_result\n\n## based on the hyper parameter optimization, the below model is built.\nnn_model = Sequential()\n# Input - Layer\nnn_model.add(Dense(65, input_dim=in_shape, activation='relu'))\n# Hidden - Layers\nnn_model.add(Dropout(0.2))\nnn_model.add(Dense(65, activation = 'relu'))\nnn_model.add(Dropout(0.2))\nnn_model.add(Dense(65, activation = 'relu'))\n# Output- Layer\nnn_model.add(Dense(1, activation='sigmoid'))\nnn_model.compile(loss='binary_crossentropy', optimizer='Nadam', metrics=['accuracy'])\nnn_model.fit(X_train, Y_train,\n          batch_size=20,\n          epochs=20,\n          verbose=1,\n          validation_data=(X_test, Y_test))\n\nscore = nn_model.evaluate(X_test, Y_test, verbose=2)\nscore_nn = score[1]\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30366622a41bb3e3bd7f985d0fdd32c07f7a5cbf"},"cell_type":"markdown","source":"**4.2 Random Forest**\n\nUsing Random Forest to build a model. The model uses Random Search to find the best hyperparameter values.\n\nThe optimized parameters are:\n* Number of trees\n* Number of features to consider at every split\n* Maximum number of levels in tree\n* Minimum number of samples required to split a node\n* Minimum number of samples required at each leaf node\n* Method of selecting samples for training each tree\n"},{"metadata":{"trusted":true,"_uuid":"e55a4e20ee820e103b78f2072e2f6e10ebc2041a"},"cell_type":"code","source":" def rf_hyperparameter_optimization():\n    from sklearn.model_selection import RandomizedSearchCV\n    # Number of trees in random forest\n    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n    # Number of features to consider at every split\n    max_features = ['auto', 'sqrt']\n    # Maximum number of levels in tree\n    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n    max_depth.append(None)\n    # Minimum number of samples required to split a node\n    min_samples_split = [2, 5, 10]\n    # Minimum number of samples required at each leaf node\n    min_samples_leaf = [1, 2, 4]\n    # Method of selecting samples for training each tree\n    bootstrap = [True, False]\n    # Create the random grid\n    random_grid = {'n_estimators': n_estimators,\n                   'max_features': max_features,\n                   'max_depth': max_depth,\n                   'min_samples_split': min_samples_split,\n                   'min_samples_leaf': min_samples_leaf,\n                   'bootstrap': bootstrap}\n\n    from sklearn.ensemble import RandomForestRegressor\n    # Using the random grid to search for best hyperparameters\n    # Creating the base model to tune\n    rf = RandomForestRegressor()\n    # Random search of parameters, using 5 fold cross validation, \n    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n    # Fit the random search model\n    rf_random.fit(X_train, Y_train)\n    return rf_random.best_params_\n\n## based on the hyper parameter optimization, the below model is built.\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators=1400, min_samples_split=5, min_samples_leaf=4, max_features= 'sqrt', max_depth= 80, bootstrap= True)\nrf_model.fit(X_train,Y_train)\n# Fetching predictions\nY_pred = rf_model.predict(X_test)\n# Calculating the test accuracy\nfrom sklearn import metrics\nscore_rf = metrics.accuracy_score(Y_test, Y_pred)\nprint(\"Test Accuracy:\",score_rf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75c5d92a9f2d42c697cac4436f60365cca9f58be"},"cell_type":"markdown","source":"**4.3 Submit Predictions**\n\nComparing the accuracy of neural network and random forest models and submitting the predictions from the best model."},{"metadata":{"trusted":true,"_uuid":"b8b1b2685013b45f9f8a0bd437f0aa392be0b373"},"cell_type":"code","source":"xTest = testData[selFeatures]\nxTest = handle_categorical_na(xTest)\n## using One Hot Encoding for handling categorical data\nxTest = pd.get_dummies(xTest,columns=['Embarked','Sex'],prefix=['Embarked','Sex'])\ncommon_col = [x for x in xTest.columns if x in X_train.columns]\nxTest = xTest[common_col]\nmissing_col = [x for x in X_train.columns if x not in xTest.columns]\n## Inserting missing columns in test data\nfor val in missing_col:\n    xTest.insert(xTest.shape[1], val, pd.Series(np.zeros(xTest.shape[0])))\ncol_names = xTest.columns\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nxTest = my_imputer.fit_transform(xTest)\nxTest = pd.DataFrame(xTest)\nxTest.columns = col_names\n\nsubmission = pd.DataFrame()\n## Comparing and submitting the best result\nif score_nn>score_rf:\n    predictions = nn_model.predict_classes(xTest)\n    predictions = [x[0] for x in predictions]\nelse:\n    predictions = rf_model.predict(xTest)\nsubmission = pd.DataFrame({'PassengerId': testData.PassengerId, 'Survived': predictions})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}