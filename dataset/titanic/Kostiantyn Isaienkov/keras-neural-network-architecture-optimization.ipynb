{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1><center>Titanic: Keras Neural Network architecture optimization</center></h1>\n\n<center><img src=\"https://www.dlt.travel/immagine/33923/magazine-titanic2.jpg\"></center>"},{"metadata":{},"cell_type":"markdown","source":"#### In this kernel I present a simple approach to optimize keras neural network architecture using optuna. Method is similar to one presented here: <a href=\"https://www.kaggle.com/isaienkov/top-10-efficient-ensembling-in-few-lines-of-code\">Top 10%. Efficient ensembling in few lines of code</a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Sequential\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First let's do some feature engineering."},{"metadata":{"trusted":true},"cell_type":"code","source":"train['LastName'] = train['Name'].str.split(',', expand=True)[0]\ntest['LastName'] = test['Name'].str.split(',', expand=True)[0]\nds = pd.concat([train, test])\n\nsur = []\ndied = []\nfor index, row in ds.iterrows():\n    s = ds[(ds['LastName']==row['LastName']) & (ds['Survived']==1)]\n    d = ds[(ds['LastName']==row['LastName']) & (ds['Survived']==0)]\n    s=len(s)\n    if row['Survived'] == 1:\n        s-=1\n    d=len(d)\n    if row['Survived'] == 0:\n        d-=1\n    sur.append(s)\n    died.append(d)\nds['FamilySurvived'] = sur\nds['FamilyDied'] = died\n\nds['FamilySize'] = ds['SibSp'] + ds['Parch'] + 1\nds['IsAlone'] = 0\nds.loc[ds['FamilySize'] == 1, 'IsAlone'] = 1\nds['Fare'] = ds['Fare'].fillna(train['Fare'].median())\nds['Embarked'] = ds['Embarked'].fillna('Q')\n\ntrain = ds[ds['Survived'].notnull()]\ntest = ds[ds['Survived'].isnull()]\ntest = test.drop(['Survived'], axis=1)\n\ntrain['rich_woman'] = 0\ntest['rich_woman'] = 0\ntrain['men_3'] = 0\ntest['men_3'] = 0\n\ntrain.loc[(train['Pclass']<=2) & (train['Sex']=='female'), 'rich_woman'] = 1\ntest.loc[(test['Pclass']<=2) & (test['Sex']=='female'), 'rich_woman'] = 1\ntrain.loc[(train['Pclass']==3) & (train['Sex']=='male'), 'men_3'] = 1\ntest.loc[(test['Pclass']==3) & (test['Sex']=='male'), 'men_3'] = 1\n\ntrain['rich_woman'] = train['rich_woman'].astype(np.int8)\ntest['rich_woman'] = test['rich_woman'].astype(np.int8)\n\ntrain[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train['Cabin']])\ntest['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in test['Cabin']])\n\ntrain = train.drop(['PassengerId', 'Ticket', 'LastName', 'SibSp', 'Parch'], axis=1)\ntest = test.drop(['PassengerId', 'Ticket', 'LastName', 'SibSp', 'Parch'], axis=1)\n\ncategorical = ['Pclass', 'Sex', 'Embarked', 'Cabin']\nfor cat in categorical:\n    train = pd.concat([train, pd.get_dummies(train[cat], prefix=cat)], axis=1)\n    train = train.drop([cat], axis=1)\n    test = pd.concat([test, pd.get_dummies(test[cat], prefix=cat)], axis=1)\n    test = test.drop([cat], axis=1)\n    \ntrain = train.drop(['Sex_male', 'Name'], axis=1)\ntest =  test.drop(['Sex_male', 'Name'], axis=1)\n\ntrain = train.fillna(-1)\ntest = test.fillna(-1)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we specify default keras parameters for initial model"},{"metadata":{"trusted":true},"cell_type":"code","source":"EPOCHS = 15\n\ninitial_keras_params = {\n    'layers_number': 1,\n    'n_units_l_0': 128,\n    'activation_l_0': 'relu',\n    'dropout_l_0': 0.5,\n    'lr': 0.001\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we build our keras classifier that will be used for optimization"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def keras_classifier(parameters):\n    \n    model = Sequential()\n    layers_number = int(parameters['layers_number'])\n    \n    for i in range(layers_number):\n        model.add(Dense(int(parameters['n_units_l_' + str(i)]), activation=parameters['activation_l_' + str(i)]))\n        model.add(Dropout(int(parameters['dropout_l_' + str(i)])))\n    model.add(Dense(2, activation='softmax'))\n    model.compile(\n        loss='categorical_crossentropy', \n        optimizer=tf.keras.optimizers.Adam(lr=float(parameters['lr'])), \n        metrics=['accuracy']\n    )\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check our initial model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras_classifier(initial_keras_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train['Survived']\ny = tf.keras.utils.to_categorical(y, num_classes=2, dtype='float32')\nX = train.drop(['Survived', 'Cabin_T'], axis=1)\nX_test = test.copy()\n\nX, X_val, y, y_val = train_test_split(X, y, random_state=0, test_size=0.2, shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X, y, validation_split=0.2, epochs=EPOCHS, batch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = model.predict(X_val)\npreds = np.argmax(preds, axis=1)\n\nprint('accuracy: ', accuracy_score(np.argmax(y_val, axis=1), preds))\nprint('f1-score: ', f1_score(np.argmax(y_val, axis=1), preds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start optimization process"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model(trial):\n    n_layers = trial.suggest_int(\"layers_number\", 1, 2)\n    model = Sequential()\n    for i in range(n_layers):\n        num_hidden = trial.suggest_int(\"n_units_l_{}\".format(i), 2, 16)\n        activation = trial.suggest_categorical('activation_l_{}'.format(i), ['relu', 'sigmoid', 'tanh', 'elu'])\n        model.add(Dense(num_hidden, activation=activation))\n        dropout = trial.suggest_uniform(\"dropout_l_{}\".format(i), 0.1, 0.4)\n        model.add(Dropout(dropout))\n    model.add(Dense(2, activation='softmax'))\n\n    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n\n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer=tf.keras.optimizers.Adam(lr=lr),\n        metrics=['accuracy']\n    )\n\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def objective(trial):\n    model = create_model(trial)\n    \n    epochs = trial.suggest_int(\"epochs\", 3, 20)\n    batch = trial.suggest_int(\"batch\", 1, X.shape[0] / 4)\n    \n    model.fit(\n        X, \n        y, \n        batch_size=batch, \n        epochs=epochs, \n        verbose=0\n    )\n    preds = model.predict(X_val)\n    return accuracy_score(np.argmax(y_val, axis=1), np.argmax(preds, axis=1))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def optimize():\n    sampler = TPESampler(seed=666)\n    study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n    study.optimize(objective, n_trials=80)\n    return study.best_params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = optimize()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = params['epochs']\nbatch = params['batch']\ndel params['epochs']\ndel params['batch']\n\nopt_model = keras_classifier(params)\nopt_model.fit(X, y, validation_split=0.2, epochs=epochs, batch_size=batch)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = opt_model.predict(X_val)\npreds = np.argmax(preds, axis=1)\nprint('accuracy: ', accuracy_score(np.argmax(y_val, axis=1), preds))\nprint('f1-score: ', f1_score(np.argmax(y_val, axis=1), preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = opt_model.predict(X_test)\npreds = np.argmax(preds, axis=1)\npreds = preds.astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/titanic/gender_submission.csv')\nsubmission['Survived'] = preds\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}