{"cells":[{"metadata":{},"cell_type":"markdown","source":"# In this Notebook, you will find the following\n\n- SelfSupervised TabNet Implementation Examples\n- TabNet Feature Importance, Mask, output examples\n- Comparison of accuracy with TabNet with pretrain, TabNet without pretrain, LighGBM, and NN\n- Comparison of feature importance with TabNet with pretrain, TabNet without pretrain, LighGBM\n- Comparison of Output distribution for TabNet with pretrain, TabNet without pretrain, LighGBM, NN"},{"metadata":{},"cell_type":"markdown","source":"# Summary of the Results"},{"metadata":{},"cell_type":"markdown","source":"### Comparison OOF_ROC-AUC, OOF_Accuracy, LB_Accuracy\n\n | - | TabNet with Pretrain| TabNet without Pretrain| LightGBM | NN |\n| ------------ | ------------- | ------------- | ------------- | ------------- |\n| OOF ROC-AUC | 0.8620 | 0.8354 | 0.8750 | 0.8643 |\n| OOF Accuracy | 0.8114 | 0.7564 | 0.8260 | 0.8249 |\n| LB Accuracy | 0.7775 | 0.7560 | 0.7608 |0.7656 |\n| Time(s) | 34.6 | 37.3 | 0.24 | 6.86 | "},{"metadata":{},"cell_type":"markdown","source":"- TabNet is lower than LightGBM and NN for the accuracy of OOF.\n- TabNet is higher than LightGBM and NN for the accuracy of LB.\n- TabNet has a smaller difference between OOF and LB accuracy than LightGBM and NN.\n- TabNet with Pretrain has a higher accuracy of OOF and a lower accuracy of LB than TabNet without Pretrain.\n- When using CPUs, TabNet is more time consuming"},{"metadata":{},"cell_type":"markdown","source":"### Comparison Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"![Feature Importance for each Model](https://storage.googleapis.com/zenn-user-upload/ezsqiabborqn8jmil0odvrevipbc)"},{"metadata":{},"cell_type":"markdown","source":"- TabNet (with Pretrain or without Pretrain) has a smaller difference in importance between features than LightGBM.\n\n- In both TabNet with Pretrain and LightGBM, Fare ranks first. On the other hand, there is a rlarge variance for features between TabNet with Pretrain and LightGBM"},{"metadata":{},"cell_type":"markdown","source":"### TabNet's Mask\n"},{"metadata":{},"cell_type":"markdown","source":"![Feature Importance for each Model](https://storage.googleapis.com/zenn-user-upload/k7wzkbishovexicgah2hex4mwoig)"},{"metadata":{},"cell_type":"markdown","source":"- This Mask is created each time you make a decision (deciding which features to use), and the number of times you make a decision is determined by\nIt can be specified by n_steps. In this case, n_steps=3, so the number of Masks is 3.\n\n- The horizontal axis is the features (0='Pclass', 1='Sex', 2='Age', 3='SibSp', 4='Parch', 5='Fare', 6='Embarked') and the vertical axis is the number of test lines (25 lines from the top).\n\n- At a glance, we can see that 5='Fare' is strongly colored for mask2. This seems to be a reasonable result, considering that Fare came first in the feature importance."},{"metadata":{},"cell_type":"markdown","source":"### Comparison Predict Distribution"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://storage.googleapis.com/zenn-user-upload/mts8q51v792anu3e0cwkbsq0dob6\" width=75%>"},{"metadata":{},"cell_type":"markdown","source":"- The output distribution of TabNet has a shape similar to that of NN."},{"metadata":{},"cell_type":"markdown","source":"### Summary"},{"metadata":{},"cell_type":"markdown","source":"- TabNet is a highly interpretive model by using Mask and Feature Importance.\n\n- In Titanic data, the accuracy of TabNet was equivalent to that of LightBM and NN. Furthermore, since the dissociation between OOF and LB values is small, it may be a model that is less prone to overlearning.\n\n- The top features in TabNet's Feature Importance were different from those of LightGBM. Therefore, TabNet can be expected to be useful for Ensemble."},{"metadata":{},"cell_type":"markdown","source":"### Original Article(Japanese)\nhttps://zenn.dev/sinchir0/articles/9228eccebfbf579bfdf4"},{"metadata":{},"cell_type":"markdown","source":"# Implementation"},{"metadata":{},"cell_type":"markdown","source":"# Ref"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"https://www.kaggle.com/optimo/selfsupervisedtabnet/data?select=pytorch_tabnet-2.0.1-py3-none-any.whl\n\nhttps://github.com/dreamquark-ai/tabnet/blob/develop/census_example.ipynb"},{"metadata":{},"cell_type":"markdown","source":"# Install"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/tabnet/pytorch_tabnet-2.0.1-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Setting"},{"metadata":{"trusted":true},"cell_type":"code","source":"N_FOLDS = 5\nSEED = 33","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os, random\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.tab_model import TabNetClassifier\n\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')\nsub = pd.read_csv('../input/titanic/gender_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fix seed"},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(SEED)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocess"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([train, test], sort=False)\n\ndata['Sex'] = data['Sex'].replace(['male','female'], [0, 1])\ndata['Embarked'] = data['Embarked'].fillna(('S'))\ndata['Embarked'] = data['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndata['Fare'] = data['Fare'].fillna(np.mean(data['Fare']))\ndata['Age'] = data['Age'].fillna(data['Age'].median())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del_col = ['Name', 'PassengerId','Ticket', 'Cabin']\ndata.drop(del_col, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = data[:len(train)]\ntest = data[len(train):]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature cols"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_col = [col for col in train.columns.tolist() if col != 'Survived']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define categorical features for categorical embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_columns = ['Pclass','Sex','SibSp','Parch','Embarked']\n\n# class num\ncategorical_dims = {}\n\nfor col in categorical_columns:\n    categorical_dims[col] = train[col].nunique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_idxs = [ i for i, f in enumerate(feature_col) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(feature_col) if f in categorical_columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TabNet Pretraining"},{"metadata":{"trusted":true},"cell_type":"code","source":"# comment out when Unsupervised\n\ntabnet_params = dict(n_d=8, n_a=8, n_steps=3, gamma=1.3,\n                     n_independent=2, n_shared=2,\n                     seed=SEED, lambda_sparse=1e-3, \n                     optimizer_fn=torch.optim.Adam, \n                     optimizer_params=dict(lr=2e-2),\n                     mask_type=\"entmax\",\n                     scheduler_params=dict(mode=\"min\",\n                                           patience=5,\n                                           min_lr=1e-5,\n                                           factor=0.9,),\n                     scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n                     verbose=10\n                    )\n\npretrainer = TabNetPretrainer(**tabnet_params)\n\npretrainer.fit(\n    X_train=train.drop('Survived',axis=1).values,\n    eval_set=[train.drop('Survived',axis=1).values],\n    max_epochs=200,\n    patience=20, batch_size=256, virtual_batch_size=128,\n    num_workers=1, drop_last=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Make Folds"},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = StratifiedKFold(n_splits=N_FOLDS)\nfor f, (t_idx, v_idx) in enumerate(skf.split(train.drop('Survived',axis=1), train['Survived'])):\n    train.loc[v_idx, 'fold'] = int(f)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# TabNet Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\noof = np.zeros((len(train),))\ntest_preds_all = np.zeros((len(test),))\nmodels = []\n\nfor fold_num in range(N_FOLDS):\n    train_idx = train[train.fold != fold_num].index\n    valid_idx = train[train.fold == fold_num].index\n\n    print(\"FOLDS : \", fold_num)\n\n    ## model\n    X_train, y_train = train[feature_col].values[train_idx,], train['Survived'].values[train_idx,].astype(float)\n    X_valid, y_valid = train[feature_col].values[valid_idx,], train['Survived'].values[valid_idx,].astype(float)\n    \n    tabnet_params = dict(n_d=8, n_a=8, n_steps=3, gamma=1.3,\n                         n_independent=2, n_shared=2,\n                         seed=SEED, lambda_sparse=1e-3,\n                         optimizer_fn=torch.optim.Adam,\n                         optimizer_params=dict(lr=2e-2,\n                                               weight_decay=1e-5\n                                              ),\n                         mask_type=\"entmax\",\n                         scheduler_params=dict(max_lr=0.05,\n                                               steps_per_epoch=int(X_train.shape[0] / 256),\n                                               epochs=200,\n                                               is_batch_level=True\n                                              ),\n                         scheduler_fn=torch.optim.lr_scheduler.OneCycleLR,\n                         verbose=10,\n                         cat_idxs=cat_idxs, # comment out when Unsupervised\n                         cat_dims=cat_dims, # comment out when Unsupervised\n                         cat_emb_dim=1 # comment out when Unsupervised\n                        )\n\n    model = TabNetClassifier(**tabnet_params)\n\n    model.fit(X_train=X_train,\n              y_train=y_train,\n              eval_set=[(X_valid, y_valid)],\n              eval_name = [\"valid\"],\n              eval_metric = [\"auc\"],\n              max_epochs=200,\n              patience=20, batch_size=256, virtual_batch_size=128,\n              num_workers=0, drop_last=False,\n              from_unsupervised=pretrainer # comment out when Unsupervised\n             )\n    \n    # Make Oof\n    oof[valid_idx] = model.predict_proba(X_valid)[:,1]\n    # Model\n    models.append(model)\n\n    # for save weight\n    # name = f\"fold{fold_num}\"\n    # model.save_model(name)    \n\n    # preds on test\n    preds_test = model.predict_proba(test[feature_col].values)[:,1]\n    test_preds_all += preds_test / N_FOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('ROC-AUC')\nprint(roc_auc_score(train['Survived'].ravel(), oof.ravel()))\nprint('Accuracy')\nprint(accuracy_score(train['Survived'].ravel(), (oof > 0.5).astype('int').ravel()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Global Explainability : Feature Importance"},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold_num, model in enumerate(models):\n    # Feature Importance\n    feat_imp_fold = pd.DataFrame(model.feature_importances_,index=feature_col, columns= [f'imp_{fold_num}'])\n    if fold_num == 0:\n        feature_importance = feat_imp_fold.copy()\n    else:\n        feature_importance = pd.concat([feature_importance, feat_imp_fold], axis=1)\n        \nfeature_importance['imp_mean'] = feature_importance.mean(axis=1)\nfeature_importance = feature_importance.sort_values('imp_mean')\n\nplt.tick_params(labelsize=18)\nplt.barh(feature_importance.index.values,feature_importance['imp_mean']);\nplt.title('feature_importance',fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Local explainability and masks"},{"metadata":{"trusted":true},"cell_type":"code","source":"explain_matrix, masks = model.explain(test[feature_col].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(1, 3, figsize=(10,7))\n\nfor i in range(3):\n    axs[i].imshow(masks[i][:25])\n    axs[i].set_title(f\"mask {i}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 0 = 'Pclass', 1 = 'Sex', 2 = 'Age',3 =  'SibSp', 4 = 'Parch', 5 = 'Fare', 6 = 'Embarked'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(test_preds_all);\nplt.xlim(0.0,1.0)\nplt.title('Output proba hist');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub['Survived'] = (test_preds_all > 0.5).astype('int')\nsub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# LightGBM(For comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# change to category\nfor col in categorical_columns:\n    train[col] = train[col].astype('category')\n    test[col] = test[col].astype('category')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\noof = np.zeros((len(train),))\ntest_preds_all = np.zeros((len(test),))\nmodels = []\n\nfor fold_num in range(N_FOLDS):\n    train_idx = train[train.fold != fold_num].index\n    valid_idx = train[train.fold == fold_num].index\n\n    print(\"FOLDS : \", fold_num)\n\n    ## model\n    X_train, y_train = train[feature_col].values[train_idx,], train['Survived'].values[train_idx,].astype(float)\n    X_valid, y_valid = train[feature_col].values[valid_idx,], train['Survived'].values[valid_idx,].astype(float)\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n    params = {\n        'objective': 'binary'\n    }\n\n    model = lgb.train(\n        params, lgb_train,\n        valid_sets=[lgb_train, lgb_eval],\n        verbose_eval=10,\n        num_boost_round=1000,\n        early_stopping_rounds=10\n    )\n    \n    # Calc Oof\n    oof[valid_idx] = model.predict(X_valid, num_iteration=model.best_iteration)\n    # Model\n    models.append(model)\n\n    # for save weight\n    # name = f\"fold{fold_num}\"\n    # model.save_model(name)    \n\n    # preds on test\n    test_preds_all += model.predict(test[feature_col], num_iteration=model.best_iteration) / N_FOLDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('ROC-AUC')\nprint(roc_auc_score(train['Survived'].ravel(), oof.ravel()))\nprint('Accuracy')\nprint(accuracy_score(train['Survived'].ravel(), (oof > 0.5).astype('int').ravel()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Feature Importance\n\nfor fold_num, model in enumerate(models):\n    # Feature Importance\n    feat_imp_fold = pd.DataFrame(model.feature_importance(), index=feature_col, columns= [f'imp_{fold_num}'])\n    if fold_num == 0:\n        feature_importance = feat_imp_fold.copy()\n    else:\n        feature_importance = pd.concat([feature_importance, feat_imp_fold], axis=1)\n        \nfeature_importance['imp_mean'] = feature_importance.mean(axis=1)\nfeature_importance = feature_importance.sort_values('imp_mean')\n\nplt.tick_params(labelsize=18)\nplt.barh(feature_importance.index.values,feature_importance['imp_mean']);\nplt.title('feature_importance',fontsize=18);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(test_preds_all);\nplt.xlim(0.0,1.0)\nplt.title('Output proba hist');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub['Survived'] = (test_preds_all > 0.5).astype('int')\n# sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NN(For comparison)"},{"metadata":{"trusted":true},"cell_type":"code","source":"target_cols = ['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class TitanicDataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss /= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nNFOLDS = 5\n\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = True\n\nnum_features=len(feature_col)\nnum_targets=1\nhidden_size =150","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.25)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.25)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_training(fold_num, seed, train, test, target_cols):\n    \n    seed_everything(seed)\n    \n    print(\"FOLDS : \", fold_num)\n    \n    train_idx = train[train.fold != fold_num].index\n    valid_idx = train[train.fold == fold_num].index\n\n    ## model\n    X_train, y_train = train[feature_col].values[train_idx,], train[['Survived']].values[train_idx,].astype(float)\n    X_valid, y_valid = train[feature_col].values[valid_idx,], train[['Survived']].values[valid_idx,].astype(float)\n    \n    train_dataset = TitanicDataset(X_train, y_train)\n    valid_dataset = TitanicDataset(X_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)        \n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                          max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n   \n    oof = np.zeros((len(train), num_targets))\n    predictions = np.zeros((len(test), num_targets))\n    \n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"seed{seed}, FOLD: {fold_num}, EPOCH: {epoch}, train_loss: {train_loss}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            print('Update best loss')\n            best_loss = valid_loss\n            oof[valid_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold_num}_seed{seed}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                print(f'Early Stop epoch{epoch} early_step{early_step}')\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test[feature_col].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n\n    )\n\n    model.load_state_dict(torch.load(f\"FOLD{fold_num}_seed{seed}_.pth\"))\n    model.to(DEVICE)\n\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n\n    for fold_num in range(NFOLDS):\n        oof_, pred_ = run_training(fold_num, seed, train, test, target_cols)\n\n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nSEED = [33]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nprint(\"######################## Training ############################\")\nfor seed in SEED:\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('ROC-AUC')\nprint(roc_auc_score(train['Survived'].ravel(), oof.ravel()))\nprint('Accuracy')\nprint(accuracy_score(train['Survived'].ravel(), (oof > 0.5).astype('int').ravel()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(predictions);\nplt.xlim(0.0,1.0)\nplt.title('Output proba hist');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sub['Survived'] = (predictions > 0.5).astype('int')\n# sub.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}