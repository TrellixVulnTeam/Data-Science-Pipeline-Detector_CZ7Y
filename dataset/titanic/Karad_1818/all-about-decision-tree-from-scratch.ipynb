{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Decision Tree**","metadata":{}},{"cell_type":"markdown","source":"# Decision Trees are one of the most powerful [supervised algorithm](https://en.wikipedia.org/wiki/Supervised_learning) that we have ever discovered.\n# Intuition behind decision tree is to divide datasets into smaller datasets based on some feature until we reached down to dataset that can be uniquely classified.\n# As a name suggest , In this algorithm we'll devide datasets based on some kind of decision and we'll create tree structure.\n![](http://i.ibb.co/ZTZcjbm/decision.png)\n# The decision to split at each node is made based on the metric called purity. \n# A node is 100% impure when a node is split evenly 50/50 and 100% pure when all of its data belongs to a single class.\n# In order to optimize our model we need to reach maximum purity and avoid impurity\n\n# Let's learn about some of the purity metric :\n\n# 1. gini : $G = \\sum_{i=0}^{n-1} p_{i} * (1 - p_{i})$ \n      \n# 2. Entropy (Information gain) : $E = - \\sum_{i=0}^{n-1} p_{i} * log(p_{i})$\n# where $p_{i}$ is the probability of ith class and $n$ is total number of samples\n\n# Let's implement decision Tree based on gini :","metadata":{}},{"cell_type":"code","source":"# import\nimport math\nimport numpy as np\nimport pandas as pd # for manipulating dataset\nfrom sklearn import tree # for decision tree and to plot tree\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-08-12T13:25:59.286075Z","iopub.execute_input":"2021-08-12T13:25:59.286832Z","iopub.status.idle":"2021-08-12T13:25:59.291416Z","shell.execute_reply.started":"2021-08-12T13:25:59.286766Z","shell.execute_reply":"2021-08-12T13:25:59.290614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# implement gini\ndef gini(samples):\n    sample_sum = sum(samples)\n    score = 0\n    probs = []\n    for sample in samples:\n        prob = sample / sample_sum\n        # probability of every sample\n        probs.append(prob)\n    \n    for prob in probs:\n        score += (prob * (1 -  prob))\n    return score","metadata":{"execution":{"iopub.status.busy":"2021-08-12T13:23:24.025909Z","iopub.execute_input":"2021-08-12T13:23:24.026232Z","iopub.status.idle":"2021-08-12T13:23:24.031654Z","shell.execute_reply.started":"2021-08-12T13:23:24.026202Z","shell.execute_reply":"2021-08-12T13:23:24.030628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# implement entroty (info. gain) function\ndef entropy(samples):\n    sample_sum = sum(samples)\n    score = 0\n    probs = []\n    for sample in samples:\n        prob = sample / sample_sum\n        probs.append(prob)\n    \n    for prob in probs:\n        if prob > 0:\n            score += (prob * math.log(prob))\n    return -1 * score","metadata":{"execution":{"iopub.status.busy":"2021-08-12T10:03:07.677084Z","iopub.execute_input":"2021-08-12T10:03:07.677472Z","iopub.status.idle":"2021-08-12T10:03:07.683346Z","shell.execute_reply.started":"2021-08-12T10:03:07.67744Z","shell.execute_reply":"2021-08-12T10:03:07.682261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we need weighted gini for node\ndef weighted(p1,p2,n1,n2):\n    # n1 = samples with p1 prob\n    # n2 = samples with p2 prob\n    return (p1 * n1 + p2 * n2) / (n1 + n2)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T13:30:42.93224Z","iopub.execute_input":"2021-08-12T13:30:42.932606Z","iopub.status.idle":"2021-08-12T13:30:42.937881Z","shell.execute_reply.started":"2021-08-12T13:30:42.932576Z","shell.execute_reply":"2021-08-12T13:30:42.936639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we're going to use titanic datasets\ndf = pd.read_csv('/kaggle/input/titanic/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T13:26:04.132707Z","iopub.execute_input":"2021-08-12T13:26:04.133345Z","iopub.status.idle":"2021-08-12T13:26:04.185448Z","shell.execute_reply.started":"2021-08-12T13:26:04.133308Z","shell.execute_reply":"2021-08-12T13:26:04.184329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we'll only use Sex , Pclass and Survived(target column) column\ncol = ['Survived' , 'Pclass' , 'Sex']\ndf = df[col]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T13:26:14.964764Z","iopub.execute_input":"2021-08-12T13:26:14.965318Z","iopub.status.idle":"2021-08-12T13:26:14.978876Z","shell.execute_reply.started":"2021-08-12T13:26:14.965264Z","shell.execute_reply":"2021-08-12T13:26:14.977595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sex column contains categorical value male and female so we'll convert male to 1 and female to 0\nmapping = {\n    'male' : 1,\n    'female' : 0\n}\ndf['Sex'] = df.Sex.map(mapping)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T13:28:06.221625Z","iopub.execute_input":"2021-08-12T13:28:06.221977Z","iopub.status.idle":"2021-08-12T13:28:06.235646Z","shell.execute_reply.started":"2021-08-12T13:28:06.221947Z","shell.execute_reply":"2021-08-12T13:28:06.234564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.Survived.value_counts() # total we have 549 data with 0 label and 342 data with 1 label","metadata":{"execution":{"iopub.status.busy":"2021-08-12T13:28:40.744469Z","iopub.execute_input":"2021-08-12T13:28:40.744838Z","iopub.status.idle":"2021-08-12T13:28:40.754106Z","shell.execute_reply.started":"2021-08-12T13:28:40.744807Z","shell.execute_reply":"2021-08-12T13:28:40.753359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# gini of whole data\n# starting gini\ngini([549 , 342]) ","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:11:56.38367Z","iopub.execute_input":"2021-08-12T14:11:56.384034Z","iopub.status.idle":"2021-08-12T14:11:56.390811Z","shell.execute_reply.started":"2021-08-12T14:11:56.384003Z","shell.execute_reply":"2021-08-12T14:11:56.389816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pclass contain only 3 value 1,2,3\ndf.Pclass.unique()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T13:28:49.823483Z","iopub.execute_input":"2021-08-12T13:28:49.824129Z","iopub.status.idle":"2021-08-12T13:28:49.833179Z","shell.execute_reply.started":"2021-08-12T13:28:49.824075Z","shell.execute_reply":"2021-08-12T13:28:49.831992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# so now what kinds of condition we can make ?\n\n# let's say we have pclass with : 1 , 2 , 3\n# so we can make split at :         |   |\n#                               (1.5)  (2.5)\n\n# and sex has 2 value : 0 , 1 so we can make split at 0.5\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](http://i.ibb.co/hdK8LCR/pclass.png)","metadata":{}},{"cell_type":"code","source":"# if the condition is true\nx = df[df.Pclass <= 1.5]\nprint(f\"total elements with pclass <= 1.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([80,136])}\\n\")\n\n# if the condition is false\nx = df[df.Pclass > 1.5]\nprint(f\"total elements with pclass > 1.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([469,206])}\\n\")\n\nprint(f\"Weighted Gini : {weighted(0.466 , 0.424 , 216 , 675)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T13:49:23.36825Z","iopub.execute_input":"2021-08-12T13:49:23.368659Z","iopub.status.idle":"2021-08-12T13:49:23.380841Z","shell.execute_reply.started":"2021-08-12T13:49:23.368622Z","shell.execute_reply":"2021-08-12T13:49:23.379839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](http://i.ibb.co/X8WSPWT/pclass2.png)","metadata":{}},{"cell_type":"code","source":"# if the condition is true\nx = df[df.Pclass <= 2.5]\nprint(f\"total elements with pclass <= 2.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([177,223])}\\n\")\n\n# if the condition is false\nx = df[df.Pclass > 2.5]\nprint(f\"total elements with pclass > 2.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([372,119])}\\n\")\n\nprint(f\"Weighted Gini : {weighted(0.493 , 0.367 , 400 , 491)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:00:54.701575Z","iopub.execute_input":"2021-08-12T14:00:54.70191Z","iopub.status.idle":"2021-08-12T14:00:54.715332Z","shell.execute_reply.started":"2021-08-12T14:00:54.701882Z","shell.execute_reply":"2021-08-12T14:00:54.714012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](http://i.ibb.co/DRHLCHJ/sex.png)","metadata":{}},{"cell_type":"code","source":"# if the condition is true\nx = df[df.Sex <= 0.5]\nprint(f\"total elements with sex <= 0.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([81,233])}\\n\")\n\n# if the condition is false\nx = df[df.Sex > 0.5]\nprint(f\"total elements with sex > 0.5  : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([468,109])}\\n\")\n\nprint(f\"Weighted Gini : {weighted(0.382 , 0.306 , 314 , 577)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:08:24.658578Z","iopub.execute_input":"2021-08-12T14:08:24.658955Z","iopub.status.idle":"2021-08-12T14:08:24.671823Z","shell.execute_reply.started":"2021-08-12T14:08:24.658921Z","shell.execute_reply":"2021-08-12T14:08:24.670618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# so sex has minimum weighted gini so it becomes our starting condition\n\n# gini : 0.383 , (81,233)\n# gini : 0.306 , (468,109)\n\n# now lets check for sex <= 0.5 and pclass <= 1.5\nx = df[ (df.Sex <= 0.5) & (df.Pclass <= 1.5)]\nprint(f\"total elements with sex <= 0.5 and pclass <= 1.5 : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([3,91])}\\n\")\n\n# now lets check for sex <= 0.5 and pclass > 1.5\nx = df[ (df.Sex <= 0.5) & (df.Pclass > 1.5)]\nprint(f\"total elements with sex <= 0.5 and pclass > 1.5 : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([78,142])}\\n\")\n\nprint(f\"weighted gini : {weighted(0.061 , 0.457 , 94 , 220)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:17:21.03379Z","iopub.execute_input":"2021-08-12T14:17:21.03418Z","iopub.status.idle":"2021-08-12T14:17:21.048637Z","shell.execute_reply.started":"2021-08-12T14:17:21.034148Z","shell.execute_reply":"2021-08-12T14:17:21.047672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now lets check for sex <= 0.5 and pclass <= 2.5\nx = df[ (df.Sex <= 0.5) & (df.Pclass <= 2.5)]\nprint(f\"total elements with sex <= 0.5 and pclass <= 2.5 : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([9,161])}\\n\")\n\n# now lets check for sex <= 0.5 and pclass > 2.5\nx = df[ (df.Sex <= 0.5) & (df.Pclass > 2.5)]\nprint(f\"total elements with sex <= 0.5 and pclass > 2.5 : {x.shape[0]}\")\nprint(x.Survived.value_counts())\nprint(f\"Gini : {gini([72,72])}\\n\")\n\nprint(f\"weighted gini : {weighted(0.1 , 0.5 , 170 , 144)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:18:52.401371Z","iopub.execute_input":"2021-08-12T14:18:52.401734Z","iopub.status.idle":"2021-08-12T14:18:52.41539Z","shell.execute_reply.started":"2021-08-12T14:18:52.401704Z","shell.execute_reply":"2021-08-12T14:18:52.414306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here weighted average for pclass<=2.5 is less then pclass<=1.5 we'll select pclass <= 2.5\n# and this process will last untill we reached leaf nodes\n\n# now let's implement whole tree with sklearn\n\nfeatures = ['Sex' , 'Pclass']\n# model intialization\nmodel = tree.DecisionTreeClassifier()\n# fitting model\nmodel.fit(df[features] , df.Survived)\n\nplt.figure(figsize = (20,10))\ntree.plot_tree(model , feature_names = features)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-12T14:20:46.925061Z","iopub.execute_input":"2021-08-12T14:20:46.925434Z","iopub.status.idle":"2021-08-12T14:20:47.65661Z","shell.execute_reply.started":"2021-08-12T14:20:46.925404Z","shell.execute_reply":"2021-08-12T14:20:47.655725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Problem with Decision Tree  : \n\n# As the number of split increases , complexity of DT will also increase.\n# In general simple DTs will be preferred over complex DTS.\n# main problem with DT is that if tree will become more complex and classify every data point with 100% accuracy then there might arises problem of overfitting\n# In Below image we'll prefer Black line over Green line.\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"![](http://i.ibb.co/09fqDcB/over.png)","metadata":{}},{"cell_type":"code","source":"# to handle problem of overfitting we'll try to contro the depth of the decision tree , it might decrease training accuracy \n# but it'll surely increase testing accuracy\n# In other words , do splitting until we get 96% or 99% pure class instead of 100% pure class\n\n# in sklearn we can provide max_depth , for e.g.\n\nmodel = tree.DecisionTreeClassifier(max_depth = 7)\n\n# we can figure out optimal max_depth by cross validation or may be using some grid search algorithm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This whole notebook is based on @abhishek thakur's [Youtube](https://youtu.be/1DMWkIJRivo) video.\n# reference : Blog from [towards data science](https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14)","metadata":{}}]}